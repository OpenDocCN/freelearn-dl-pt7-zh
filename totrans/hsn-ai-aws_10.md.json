["```py\n# Filter ratings by selecting books that have been rated by at least 1200 users and users who have rated at least 130 books\nfil_users = users.filter(F.col(\"count\") >= 130)\nfil_books = books.filter(F.col(\"count\") >= 1200)\n```", "```py\ns3 = s3fs.S3FileSystem()\n\ns3_bucket = 's3://ai-in-aws1/'\ninput_prefix = 'Chapter7/object2vec/bookratings.parquet'\ndataset_name = s3_bucket + input_prefix\n\ndf_bkRatngs = pq.ParquetDataset(dataset_name, filesystem=s3).read_pandas().to_pandas()\n```", "```py\ncontainer = get_image_uri(boto3.Session().region_name, 'object2vec') \n```", "```py\n# create object2vec estimator\nregressor = sagemaker.estimator.Estimator(container, role, train_instance_count=1, \n train_instance_type='ml.m5.4xlarge', output_path=output_path, sagemaker_session=sess)\n\n# set hyperparameters\nregressor.set_hyperparameters(**static_hyperparameters)\n\n# train and tune the model\nregressor.fit(input_paths)\n```", "```py\nfrom sagemaker.predictor import json_serializer, json_deserializer\n\n# create a model using the trained algorithm\nregression_model = regressor.create_model(serializer=json_serializer,\n deserializer=json_deserializer,content_type='application/json')\n```", "```py\n# deploy the model\npredictor = regression_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n```", "```py\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\npredictor = RealTimePredictor(endpoint='object2vec-2019-08-23-21-59-03-344', sagemaker_session=sess, serializer=json_serializer, deserializer=json_deserializer, content_type='application/json')\n```", "```py\n# Send data to the endpoint to get predictions\n\nprediction = predictor.predict(test_data)\nprint(\"The mean squared error on test set is %.3f\" %get_mse_loss(prediction, test_label))\n```", "```py\ntrain_label = [row['label'] for row in copy.deepcopy(train_list)]\nbs1_prediction = round(np.mean(train_label), 2)\nprint(\"The validation mse loss of the Baseline 1 is {}\".format(get_mse_loss(len(test_label)*[bs1_prediction], test_label)))\n```", "```py\ndef bs2_predictor(test_data, user_dict):\n  test_data = copy.deepcopy(test_data['instances'])\n  predictions = list()\n  for row in test_data:\n    userID = int(row[\"in0\"][0])\n\n```", "```py\n    if userID in user_dict:\n      local_books, local_ratings = zip(*user_dict[userID])\n      local_ratings = [float(score) for score in local_ratings]\n      predictions.append(np.mean(local_ratings))\n    else:\n      predictions.append(bs1_prediction)\n\n   return predictions\n\n```", "```py\ntuning_job_name = \"object2vec-job-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime())) \n\nhyperparameters_ranges = { \n\"learning_rate\": ContinuousParameter(0.0004, 0.02),\n\"dropout\": ContinuousParameter(0.0, 0.4),\n\"enc_dim\": IntegerParameter(1000, 2000),\n\"mlp_dim\": IntegerParameter(256, 500), \n\"weight_decay\": ContinuousParameter(0, 300) }\n\nobjective_metric_name = 'validation:mean_squared_error'\n```", "```py\ntuner = HyperparameterTuner(regressor, objective_metric_name, hyperparameters_ranges, objective_type='Minimize', max_jobs=5, max_parallel_jobs=1)\n```", "```py\ntuner.fit({'train': input_paths['train'], 'validation': input_paths['validation']}, job_name=tuning_job_name, include_cls_metadata=False)\ntuner.wait()\n```", "```py\nobjTunerAnltcs = tuner.analytics()\ndfTuning = objTunerAnltcs.dataframe(force_refresh=False)\np = figure(plot_width=500, plot_height=500, x_axis_type = 'datetime') \np.circle(source=dfTuning, x='TrainingStartTime', y='FinalObjectiveValue')\nshow(p)\n```", "```py\nsgmclient = boto3.client(service_name='sagemaker')\nresults = sgmclient.search(**search_params)\n```", "```py\nsearch_params={ \"MaxResults\": 10, \"Resource\": \"TrainingJob\",\n\"SearchExpression\": {\n \"Filters\": [{\"Name\": \"AlgorithmSpecification.TrainingImage\",\"Operator\": \"Equals\",\"Value\": \"Object2Vec\"}]},\n \"SortBy\": \"Metrics.validation:mean_squared_error\",\n \"SortOrder\": \"Descending\"}\n```", "```py\nwords = []\n\nfor i in df_bktitles['BookTitle']:\n    tokens = word_tokenize(i)\n    words.append([word.lower() for word in tokens if word.isalpha()])\n```", "```py\ncounter = nlp.data.count_tokens(itertools.chain.from_iterable(words))\n```", "```py\nvocab = nlp.Vocab(counter)\nfasttext_simple = nlp.embedding.create('fasttext', source='wiki.simple')\nvocab.set_embedding(fasttext_simple)\n```", "```py\nfor title in words:\ntitle_arr = ndarray.mean(vocab.embedding[title], axis=0, keepdims=True)\ntitle_arr_list = np.append(title_arr_list, title_arr.asnumpy(), axis=0)\n```", "```py\ntsne = TSNE(n_components=2, random_state=0)\nY = tsne.fit_transform(title_arr_list)\n```", "```py\nFROM ubuntu:16.04\n\nRUN apt-get -y update --allow-unauthenticated && apt-get install -y --no-install-recommends \\\n wget \\\n r-base \\\n r-base-dev \\\n ca-certificates\n```", "```py\nRUN R -e \"install.packages(c('reshape2', 'recommenderlab', 'plumber', 'dplyr', 'jsonlite'), quiet = TRUE)\"\n\nCOPY Recommender.R /opt/ml/Recommender.R\nCOPY plumber.R /opt/ml/plumber.R\n\nENTRYPOINT [\"/usr/bin/Rscript\", \"/opt/ml/Recommender.R\", \"--no-save\"]\n```", "```py\nDocker build -t ${algorithm_name}.\nDocker tag ${algorithm_name} ${fullname}\nDocker push ${fullname}\n```", "```py\nargs <- commandArgs()\nif (any(grepl('train', args))) {\n train()}\nif (any(grepl('serve', args))) {\n serve()}\n```", "```py\nrec_model = Recommender(ratings_mat[1:n_users], method = \"UBCF\", param=list(method=method, nn=nn))\n```", "```py\n# Define scoring function\nserve <- function() {\n app <- plumb(paste(prefix, 'plumber.R', sep='/'))\n app$run(host='0.0.0.0', port=8080)}\n```", "```py\nload(paste(model_path, 'rec_model.RData', sep='/'), verbose = TRUE)\npred_bkratings <- predict(rec_model, ratings_mat[ind], n=5)\n```", "```py\npayload = ratings.to_csv(index=False) \n\nresponse = runtime.invoke_endpoint(EndpointName='BYOC-r-endpoint-<timestamp>', ContentType='text/csv', Body=payload)\n\nresult = json.loads(response['Body'].read().decode())\n\n```"]