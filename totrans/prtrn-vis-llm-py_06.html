<html><head></head><body>
		<div id="_idContainer051">
			<h1 id="_idParaDest-87" class="chapter-number"><a id="_idTextAnchor106"/>6</h1>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor107"/>Dataset Preparation: Part Two, the Data Loader</h1>
			<p class="author-quote"><em class="italic">Become one with the data. – Andrej Karpathy</em></p>
			<p>In this chapter, you’ll learn how to prepare your dataset to immediately use it with your chosen models. You’ll master the concept of a data loader, learning why it’s a common source of errors in training large models. You’ll learn about creating embeddings, using tokenizers, and other methods to featurize your raw data for your preferred neural network. Following these steps, you’ll be able to prepare your entire dataset, using methods for both vision and language. Finally, you’ll learn about data optimization on AWS and Amazon SageMaker to efficiently send datasets large and small to your training cluster. Throughout this chapter, we’ll work backward through the training loop, incrementally giving you all the steps you need to have functional deep neural networks training at scale. You’ll also follow a case study on how I trained on 10 TB for Stable Diffusion <span class="No-Break">on SageMaker!</span></p>
			<p>Never underestimate the power of data. Whether it’s getting the highest quality samples and labels you can, failing to catch subtle corruptions, or optimizing your compute selections, data can truly make or break the success of your project. Many top deep learning models actually came about through the development of a novel dataset, from MNIST to AlexNet, and from GPT-3 to Stable Diffusion! When we think big in machine learning, frequently that means thinking big about <span class="No-Break">your dataset.</span></p>
			<p>You can’t have a functional training loop without a functional data loader, so let’s unpack it! In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Introducing the data loader through key concepts <span class="No-Break">in Python</span></li>
				<li>Building and testing your own data loader: a case study from <span class="No-Break">Stable Diffusion</span></li>
				<li>Embeddings <span class="No-Break">and tokenizers</span></li>
				<li>Optimizing your data pipeline <span class="No-Break">on AWS</span></li>
				<li>Transforming deep learning datasets at scale <span class="No-Break">on AWS</span></li>
			</ul>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor108"/>Introducing the data loader in Python</h1>
			<p>The data loader is a concept that is<a id="_idIndexMarker322"/> fairly unique to deep learning. In statistical machine learning, you still see many models using gradient updating, which requires mini-batches, but<a id="_idIndexMarker323"/> the <em class="italic">loading</em> aspect is more hidden – more integrated with the algorithm itself. PyTorch leaned into this concept from the early days, explicitly offering a <strong class="source-inline">data loader</strong> object and exposing the entire training loop to the developer. While somewhat more complex than early TensorFlow, this actually enabled developers to have a lot more flexibility and control over the training process, which helped them more easily develop custom solutions. This was a part of the reason more and more research projects eventually embraced PyTorch over TensorFlow as their deep learning framework of choice. Now, the majority of models I encounter are first implemented in PyTorch, and occasionally <span class="No-Break">in TensorFlow.</span></p>
			<p>What is a data loader? A data loader <em class="italic">hydrates your training loop with data</em>. Most PyTorch training loops are actually just nested loops. First, there’s an outer loop through the number of epochs. Each epoch is a full pass through the dataset. This means – you guessed it – <em class="italic">the inner loop is just a pass through your data loader</em>. This means that your data loader needs to use, under the <a id="_idIndexMarker324"/>hood, a really useful object in Python known as <span class="No-Break">an </span><span class="No-Break"><strong class="bold">iterator</strong></span><span class="No-Break">.</span></p>
			<p>First, let’s take a quick look at objects in Python, and build up to the <span class="No-Break">data loader.</span></p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B18942_Figure_6.1.jpg" alt="Figure 6.1 – Classes in Python"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Classes in Python</p>
			<p>Remember, Python is an <strong class="bold">object-oriented</strong> language. This <a id="_idIndexMarker325"/>means that, most of the time, when you’re working in Python <a id="_idIndexMarker326"/>you’re working with objects. A class is then just a convenient way of building, maintaining, and <span class="No-Break">using objects.</span></p>
			<p>Most of the time, in the real <a id="_idIndexMarker327"/>world, you won’t be building objects, unless you’re building a new software SDK. Usually, as a service consumer, you’re just using an object someone else has built, and developing a script to integrate it into your tasks. This is also true in deep learning; most of our objects are already written in software packages such as PyTorch, pandas, sklearn, and <span class="No-Break">so on.</span></p>
			<p>Now, what if I wanted to point to a really large list all at once, but have it return only a predefined number of objects every time I call that function? Would I have to build this entire construct myself? Now that I’m not in grad school anymore, I can happily say no way! I’d just use an iterator, as shown in the <span class="No-Break">following screenshot.</span></p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B18942_Figure_6.2.jpg" alt="Figure 6.2 – A simple iterator class in Python"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – A simple iterator class in Python</p>
			<p>Python iterators are purpose-built for scenarios like this, calling an object multiple times but retrieving a different item each time. Many objects in Python support iterators, such as lists and dictionaries. Turning<a id="_idIndexMarker328"/> one of them into an iterator is usually pretty simple. You’ll do it in two steps, first <a id="_idIndexMarker329"/>when you define the core object as an iterator, here with the <strong class="source-inline">iter()</strong> syntax. Second, when you call the iterator to provide you with the next batch of items, here with <strong class="source-inline">next()</strong>. Expect the syntax to change, but most of the concepts to stay <span class="No-Break">the same.</span></p>
			<p>Your job in building a data loader is <em class="italic">not</em> to necessarily build a class from scratch. It’s to use some software framework, such as NumPy, Hugging Face, PyTorch, or TensorFlow, to accept the data you want to work with. Then, you need to use that pre-built data loader to walk through your batches and populate your training loop <span class="No-Break">with them.</span></p>
			<p>Now that you know what a data loader is supposed to do, let’s explore how to build your own <span class="No-Break">data loader.</span></p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor109"/>Building and testing your own data loader – a case study from Stable Diffusion</h1>
			<p>The syntax for data loaders is<a id="_idIndexMarker330"/> guaranteed to change, so I don’t want to rely on PyTorch’s current implementation too heavily. However, let me provide you with one <span class="No-Break">simple screenshot:</span></p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B18942_Figure_6.3.jpg" alt="Figure 6.3 – Using data loaders in PyTorch"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Using data loaders in PyTorch</p>
			<p>This is actually from my re:Invent demo on large-scale training in 2022, with Gal Oshri from SageMaker and Dan Padnos from AI21: <a href="https://medium.com/@emilywebber/how-i-trained-10tb-for-stable-diffusion-on-sagemaker-39dcea49ce32">https://medium.com/@emilywebber/how-i-trained-10tb-for-stable-diffusion-on-sagemaker-39dcea49ce32</a>. Here, I’m training Stable Diffusion on 10 TB of data, using SageMaker and FSx for Lustre, which is a distributed file system built for high-performance computing. More on that and related optimizations later in <span class="No-Break">the chapter!</span></p>
			<p>As you can see, really the only hard part about this is building the input training dataset. Once you have a valid dataset object, getting a valid data loader is as simple as copying the latest syntax into your script and ensuring it’s valid. So, you ask, how do we get our own training dataset? One <span class="No-Break">word: dictionaries!</span></p>
			<p>In my setup right now, I have a Jupyter notebook running on Studio. I upgrade and downgrade the instance running my kernel gateway application, or ephemeral notebook, continuously based on whether and when I need to do some large- or small-scale processing. In this notebook, I have developed scripts and functions that I’m sure will work, then I copied them into the main script that runs on my SageMaker training jobs. This is where I built out a custom data <span class="No-Break">loading function.</span></p>
			<p>Hugging Face provides a nice <strong class="source-inline">load_dataset()</strong> function from its dataset library, but after more than a few hours of searching<a id="_idIndexMarker331"/> and testing, I wasn’t able to get this to work with my custom dataset. So, I ended up building my own data loader backend, which I then pointed to the <strong class="source-inline">DatasetDict()</strong> object. In my notebook, it looks <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B18942_Figure_6.4.jpg" alt="Figure 6.4 – Create your own DatasetDict object in Hugging Face"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Create your own DatasetDict object in Hugging Face</p>
			<p>Pretty simple, right? You can see I clearly have a training set, which itself is just the word <strong class="source-inline">train</strong> pointing to a Hugging Face <strong class="source-inline">Dataset</strong> object. You can also see that I only have 1,736 objects in this dataset, which is good, because I’m only using an <strong class="source-inline">ml.t3.medium</strong> instance to run my notebook, and it’s tiny. When I need to point to and test a larger dataset, then in Studio, I upgrade my instance in a few clicks and suddenly I have hundreds of GB of instance <a id="_idIndexMarker332"/>memory with tens of CPU cores at <span class="No-Break">my fingertips!</span></p>
			<p>When it’s simple, that is due to elegant design decisions. Your code should be like poetry: short, simple, effective, and evocative. Powerful. This goes all the way back <span class="No-Break">to Shakespeare:</span></p>
			<p class="author-quote">Brevity is the soul of wit.</p>
			<p>For my Stable Diffusion dataset, I downloaded 50 million image and caption pairs. More on how I did that is presented later in <span class="No-Break">the chapter!</span></p>
			<p>After this, I realized that it would be extremely inefficient to waste expensive GPU time loading that entire dataset into memory. This is because my implementation, which no doubt could be improved, lazily lists all of the images, walks through them one by one, reads the caption, and stores it with <span class="No-Break">the pointer.</span></p>
			<p>Now, fortunately, I could at least use Python’s multiprocessing package to list the images concurrently, one per CPU core, but for 50 million images that could easily take 24 hours to do. On top of that, I only needed one machine to execute this task. My training cluster has 24 <strong class="source-inline">ml.p4d.24xlarge</strong> machines, so I was not going to let all of those hosts sit idle while I listed the images and walked through them. So, I built <span class="No-Break">an index!</span></p>
			<p>Here, the index is<a id="_idIndexMarker333"/> simply a JSON Lines object. Let’s <span class="No-Break">inspect it!</span></p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B18942_Figure_6.5.jpg" alt="Figure 6.5 – Inspect the data index"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Inspect the data index</p>
			<p>I spent a few days building this whole process end <span class="No-Break">to end:</span></p>
			<ol>
				<li>First, I tested my training script with some toy data on SageMaker to make sure it <span class="No-Break">worked properly.</span></li>
				<li>Then, I downloaded a new dataset using more than a few large CPU machines <span class="No-Break">on SageMaker.</span></li>
				<li>Next, I put the dataset <a id="_idIndexMarker334"/>onto FSx for Lustre. I tested this on SageMaker, pointing to the relevant <strong class="bold">Virtual Private Cloud</strong> (<span class="No-Break"><strong class="bold">VPC</strong></span><span class="No-Break">) location.</span></li>
				<li>Then I replicated a tiny version of this, with just a few objects, in Studio. I built some scripts to parse these objects, ensuring they scaled and were operational as I went. I moved those scripts onto my SageMaker training jobs and executed a run on a large-CPU <span class="No-Break">machine overnight.</span></li>
			</ol>
			<p>The next morning, I built and tested my index loader, moving it onto SageMaker training as it worked. Now I am running on 16 <strong class="source-inline">ml.p4d.24xlarge</strong> instances, or 128 A100 GPUs. Tomorrow, I’ll do the full run for one full epoch with 50 million images on 24 <strong class="source-inline">ml.p4d.24xlarge</strong> instances, or 192 GPUs. If I could do this end to end, so <span class="No-Break">can you!</span></p>
			<p>Throughout this chapter, I’ll share optimizations about that entire pipeline with you, but for now, let’s unpack one key <a id="_idIndexMarker335"/>aspect of this training flow that is critical to preparing your data for your chosen <span class="No-Break">model: tokenizers.</span></p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor110"/>Creating embeddings – tokenizers and other key steps for smart features</h1>
			<p>Now that you have your data loader tested, built, and possibly scaled, you’re thinking to yourself, what do I do with all of these raw images and/or natural language strings? Do I throw them straight into my neural network? Actually, the last five years of learning representations have proven this definitively: no, you should not put raw images or text into your neural network right <a id="_idIndexMarker336"/>off the bat. You should convert your raw inputs to embeddings by using <span class="No-Break">another model.</span></p>
			<p>The intuition for this is simple: before you teach your model how to recognize relationships in your dataset, you first have to introduce it to the concept of a dataset. Creating embeddings is basically a way of doing this; you use a data structure that has been trained from another process to create vector representations of your data. That is to say, you provide your raw text and images as input, and you get high-dimensional vectors as output. Those vectors are produced through what you hope is a valid process that should catch nuanced details in their inter-relationship. Commonly in multimodal settings, such as with Stable Diffusion, you will actually use different processes for the vision and language embeddings, putting them into your model and integrating them through the learning <span class="No-Break">loop distinctly.</span></p>
			<p>Natural language tends to use a <a id="_idIndexMarker337"/>process called <strong class="bold">tokenization</strong>. Each model has a unique tokenizer that was trained on a specific vocabulary. If you want to pretrain or finetune a GPT-3 type model, you’ll need to download the tokenizer that ships with the model and apply that tokenizer to your dataset. This will have a unique way of breaking down strings into words, subwords, or characters depending on the model. Eventually, each token is converted to a high-dimensional vector, or in more simple terms, a really long list of numbers. We<a id="_idIndexMarker338"/> call them <strong class="bold">vector embeddings</strong>. Many word embeddings also include <strong class="bold">positional encoding</strong>, a numerical way of representing to<a id="_idIndexMarker339"/> the neural network where that specific word, or token, sits in the sentence relative to other words. This positional encoding helps your transformer-based model pick up on the meaning of words in that specific dataset. If you are pretraining a net new model or dataset, you will likely end up needing to train your <span class="No-Break">own tokenizer.</span></p>
			<p>In computer vision, a common way of creating embeddings for images is <em class="italic">using a pretrained vision model to create features</em>. This means you can use a fully-trained computer vision model, such as <strong class="bold">Contrastive Language-Image Pretraining</strong> (<strong class="bold">CLIP</strong>), while setting the weights to<a id="_idIndexMarker340"/> inference only. This is the same as freezing the weights. That means as images pass through this network, the<a id="_idIndexMarker341"/> network creates a dense representation of the image, without actually formally producing a prediction. This dense representation then interacts with your trainable model, the one you actually are running gradient <span class="No-Break">descent against.</span></p>
			<p>Now, let’s make these ideas more concrete through our example training Stable Diffusion <span class="No-Break">on SageMaker.</span></p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B18942_Figure_6.6.jpg" alt="Figure 6.6 – Importing libraries"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Importing libraries</p>
			<p>First, you’ll see I’m pointing to two critical libraries: <strong class="source-inline">diffusers</strong> and <strong class="source-inline">transformers</strong>. Both of them are from our friends over at <span class="No-Break">Hugging Face!</span></p>
			<p>The <strong class="source-inline">transformers</strong> library provides a lot of helpful methods and techniques for working with natural language. The <strong class="source-inline">diffusers</strong> library does the same, just for models based on diffusion. Diffusion models tend to enable high-quality image generation, commonly by providing a prompt from natural language. This means you can provide a natural language prompt and have the model generate an image <span class="No-Break">for you!</span></p>
			<p>In the preceding code snippet, we’re just pointing to the base models and tokenizers we’ll use to featurize the image and text pairs we need to train a Stable Diffusion model. After that, we need to download <span class="No-Break">them properly.</span></p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B18942_Figure_6.7.jpg" alt="Figure 6.7 – Importing models to train Stable Diffusion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Importing models to train Stable Diffusion</p>
			<p>To save time on my massive GPU cluster, I downloaded each of these models ahead of time. I saved them in my S3 bucket, then created a <em class="italic">training channel</em> to point to that S3 path when I run my SageMaker training job. The script then reads them from the path on the training cluster where they’ve been downloaded at the start of <span class="No-Break">the job.</span></p>
			<p>A channel is just a pointer from your SageMaker training job to any supported data input. That can be an S3 <a id="_idIndexMarker342"/>path, an FSx for Lustre mount, or an EFS volume. Channels are handy ways to organize different inputs for your job. You can create them for pointing to different splits in your data, such as training and validation, base models, scripts, or anything else you want. These are tracked for you as job parameters, so you can see them stored with the rest of the job metadata. They’re also searchable. SageMaker will copy, stream, or mount your channels after the instances start, so make sure you keep the copy time to a minimum, as this will <span class="No-Break">reduce costs.</span></p>
			<p>Next, we need to <em class="italic">freeze the weights</em>. This is the same as setting them to “untrainable,” or “inference only.” It means we only want the result of data passing through this model, not a prediction. Fortunately for us, the syntax for this is <span class="No-Break">dead simple.</span></p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B18942_Figure_6.8.jpg" alt="Figure 6.8 – Freezing parameters for the non-trainable models"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Freezing parameters for the non-trainable models</p>
			<p>After this, we need to process our raw data to feed it into our neural network. This is where tokenization and featurization come <span class="No-Break">into play.</span></p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B18942_Figure_6.9.jpg" alt="Figure 6.9 – Preprocessing the images"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Preprocessing the images</p>
			<p>This snippet should be fairly understandable. We pass in our training set. This function is explicitly expecting two columns, one with a path to images and one with captions. Then it uses a Python <strong class="source-inline">Image</strong> object to simply read all the images from disk and convert them into a machine-readable format. Typically <a id="_idIndexMarker343"/>this is three channels, one each for red, green, and blue. Each channel is a two-dimensional array or a simple list of lists of floating-point pixel values. After reading the images, the function next tokenizes the captions. This script uses <strong class="source-inline">ClipTokenizer</strong> to parse the provided <span class="No-Break">natural text.</span></p>
			<p>This function is then applied after we’ve created the <strong class="source-inline">DataSetDict()</strong> object, as in the notebook earlier in this chapter. We point to the training set, apply the transformation, and we are ready to finally pass this into our <span class="No-Break">data loader!</span></p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B18942_Figure_6.10.jpg" alt="Figure 6.10 – Pointing to the training dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – Pointing to the training dataset</p>
			<p>Now that we’ve learned how to build, test, and scale our data loader, let’s learn about different optimizations for the entire data flow available <span class="No-Break">on AWS.</span></p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor111"/>Optimizing your data pipeline on Amazon SageMaker</h1>
			<p>Remember that we’ve learned about<a id="_idIndexMarker344"/> ephemeral training on Amazon <a id="_idIndexMarker345"/>SageMaker, where you can seamlessly spin up anywhere from a few to hundreds, to thousands of GPUs on remote instances that are fully managed. Now, let’s learn about different options to optimize sending data to your SageMaker <span class="No-Break">Training instances.</span></p>
			<p>If you’ve worked with SageMaker Training, you’ll remember the different stages your job moves through: starting the instances, downloading your data, downloading your training image and invoking it, then uploading the <span class="No-Break">finished model.</span></p>
			<p>Here’s a screenshot from my 2022 re:Invent demo, featuring Stable Diffusion. You might ask yourself, how is it that I’m <a id="_idIndexMarker346"/>downloading 50 million image/text pairs<a id="_idIndexMarker347"/> in only two minutes? The answer is an optimized data pipeline. In this case, I used FSx <span class="No-Break">for Lustre.</span></p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B18942_Figure_6.11.jpg" alt="Figure 6.11 – Training job status"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Training job status</p>
			<p>For much smaller datasets, such as those that are only a few tens of GB, it’s fine to simply point to S3 as your input training channel. When you use S3 as your training input, SageMaker can either <em class="italic">copy</em> (File Mode) or <em class="italic">stream</em> (Pipe Mode or Fast File Mode) your files during training. Moving data around is generally a slow process, and here it’s bottlenecked by the bandwidth of your lead training machine. Using File Mode with S3 as your input can easily add tens of minutes to your training time, and possibly hours or more as your dataset scales. When I train on 100 GB, for example, using S3 as my input data mode without streaming would add a solid 20 minutes to my training time. Sadly, I am paying for that wait time, because the instances have already initialized, so it’s in my best interest to optimize my <span class="No-Break">data pipeline.</span></p>
			<p>In some cases, a simple and cost-effective alternative to the S3 copy option is <strong class="bold">streaming</strong>, using either Pipe Mode <a id="_idIndexMarker348"/>or Fast File Mode. Pipe Mode requires some scripting modifications on your end, but happily, Fast File Mode does not! However, Fast File Mode is known to have some scaling issues when you work with a larger number of files. To solve this issue, and to handle data loading at scale for hundreds to thousands of GPUs, we typically recommend FSx <span class="No-Break">for Lustre.</span></p>
			<p>FSx for Lustre is a<a id="_idIndexMarker349"/> distributed file system that easily connects to a data repository in S3, mounts to your SageMaker Training jobs, and executes a <a id="_idIndexMarker350"/>high throughput training loop on your behalf. This is because it reads the data from S3 once, then stores it in a cache and <em class="italic">scales reads horizontally with your mounts</em>. Said another way, once your data is loaded into Lustre, the training loop throughput reads and writes scale linearly as a function of <span class="No-Break">your accelerators.</span></p>
			<p>You’ll need to create Lustre in a VPC, that is to say, in your virtual private cloud, on AWS. This is good news for those who work with personally identifiable information or in heavily regulated industries. Using VPCs, you can build and maintain a private network on the cloud, using security and networking controls to manage traffic and secure access to your highly <span class="No-Break">restricted content.</span></p>
			<p>Honestly, manage traffic and secure access from an S3 data repository is pretty straightforward. It usually takes me about twenty minutes, with a few of my own hiccups along the road, and that includes the volume <span class="No-Break">creation time.</span></p>
			<p>Here’s how to establish the data repository when you are <span class="No-Break">creating Lustre:</span></p>
			<ol>
				<li>First, point to your S3 path with all of <span class="No-Break">the data.</span></li>
				<li>Second, determine what type of policies you’d like to set. An import policy will determine how Lustre automatically grabs data from S3, and an export policy determines how Lustre automatically pushes data <span class="No-Break">to S3.</span></li>
			</ol>
			<p> Lastly, here’s a view of my volume after I loaded it with 9.5 TB of Stable Diffusion <span class="No-Break">image/text pairs:</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B18942_Figure_6.12.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – My FSx for Lustre volume</p>
			<p>Once you have Lustre created, you’ll need to spend another thirty minutes or so testing and perfecting the connection from SageMaker to Lustre. This entails configuring the VPC and its relevant <a id="_idIndexMarker351"/>subnet. Currently, these are<a id="_idIndexMarker352"/> the <span class="No-Break">key steps:</span></p>
			<ol>
				<li>Make sure you have an internet gateway in your <span class="No-Break">target VPC.</span></li>
				<li>Make sure the subnet where you created Lustre has a route to <span class="No-Break">that gateway.</span></li>
				<li>Ensure the security group for that subnet allows inbound and outbound traffic, defined in <span class="No-Break">multiple ways.</span></li>
				<li>Establish an S3 VPC endpoint for your target buckets to allow SageMaker to upload the finished model artifacts to S3 <span class="No-Break">on completion.</span></li>
			</ol>
			<p>I’ve seen some configurations with two subnets, one to interact with the actual public internet to <strong class="source-inline">pip install</strong> new packages, and one to run the training jobs. Personally, I skipped over this by building a Docker container with all my packages, then loaded this to ECR, and pointed to it when starting my <span class="No-Break">training job.</span></p>
			<p>When you run your training jobs, if you want to point to a specific VPC, make sure you pass in the relevant credentials to the estimator. You’ll also need to pass a few extra parameters to point to FSx <span class="No-Break">for Lustre.</span></p>
			<p>Lastly, you can also mount Lustre to your notebooks directly! In this setup, you’ll need to <em class="italic">rebuild the notebook instance to connect to the same VPC credentials</em>. That actually isn’t necessary to launch a job on Lustre, but it is required to mount the volume directly. Here’s a nice script that helps you do this <em class="italic">(1)</em>. For an even more detailed consideration of the pros and cons of each of these options, see our blog post on the <span class="No-Break">topic </span><span class="No-Break"><em class="italic">(2)</em></span><span class="No-Break">.</span></p>
			<p>Now that you have a better idea of how to optimize your data pipeline options to point to SageMaker for the training loop, let’s take a step back and evaluate a few options for downloading and<a id="_idIndexMarker353"/> transforming datasets at scale <span class="No-Break">on </span><span class="No-Break"><a id="_idIndexMarker354"/></span><span class="No-Break">AWS!</span></p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor112"/>Transforming deep learning datasets at scale on AWS</h1>
			<p>At this point, you must be thinking now I know how to build and test my data loader, and even put my <a id="_idIndexMarker355"/>data on FSx for Lustre to integrate with SageMaker training, but what if I need to do large-scale downloads or <a id="_idIndexMarker356"/>transformations ahead of time? How can I do those at a large scale, in a cost-effective and <span class="No-Break">simple way?</span></p>
			<p>While there are many different tools and perspectives for attacking this problem, my personal favorite is always to take the simplest, least expensive, and most scalable approach. To me, that’s<a id="_idIndexMarker357"/> actually with <strong class="bold">job parallelism</strong> on <span class="No-Break">SageMaker Training.</span></p>
			<p>As it turns out, SageMaker Training is a very broad compute service offering you can use to run essentially any type of script. In particular, you can use it to run large CPU-based data transformation jobs in parallel. There’s no upper limit on how many SageMaker Training jobs you can run, and we have customers who run <em class="italic">thousands of jobs a day</em> in order to train models for their unique business purposes. This might be training tiny models for advertising, personalized recommendations, pricing, or <span class="No-Break">other enhancements.</span></p>
			<p>For my Stable Diffusion case study, I actually used 18 concurrent SageMaker jobs to download all of my data! First, I used one large CPU job to download all of the Parquet files included in the Laion-5B dataset. Then, I looped through them, sending each Parquet file to its own job. It looked something <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B18942_Figure_6.13.jpg" alt="Figure 6.13 – Use job parallelism to transform data at scale"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – Use job parallelism to transform data at scale</p>
			<p>See how I am actually running 18 different jobs? This way, you can easily track, manage, and assess each job. All of the <a id="_idIndexMarker358"/>results are sent back to S3 – in this <a id="_idIndexMarker359"/>case, by the tool itself, which writes to S3 on my behalf. Now I don’t even need to use Spark! I can just run as many SageMaker jobs as I need to, using Python and its <strong class="source-inline">multiprocessing</strong> package, to execute as many tasks as <span class="No-Break">I need.</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B18942_Figure_6.14.jpg" alt="Figure 6.14 – A data processing script"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.14 – A data processing script</p>
			<p>How does <strong class="source-inline">multiprocessing</strong> with Python work, you ask? It’s simple. The lynchpin is the critical <strong class="source-inline">Pool.map()</strong> process. First, you create the pool by providing it with the number of available CPUs. You can look that up using the <strong class="source-inline">multiprocess.cpu_count()</strong> method. Then you’ll bring two objects to <strong class="source-inline">map()</strong>: first, a list of objects you want farmed out to all of the processes, and second, a function that you want executed on each object in that list. It’s basically the concept of a <strong class="source-inline">for</strong>loop, but here, instead <a id="_idIndexMarker360"/>of using only one process, you’re using as many processes as are available on the instance. That means if you are going<a id="_idIndexMarker361"/> from 2 CPUs up to 96 CPUs, you can run more than <span class="No-Break">10x faster.</span></p>
			<p>It’s a great idea to offload as much data transformation to CPUs as you can because CPUs are dirt cheap. In comparing the costs of my 192 GPUs per hour versus 18 CPU-based jobs, the CPU was about 13x cheaper than <span class="No-Break">the GPUs!</span></p>
			<p>As you also may have guessed, we have literally hundreds of other options for manipulating data on AWS. I won’t go into detail on that here, but feel free to explore <span class="No-Break">for yourself.</span></p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor113"/>Summary</h1>
			<p>At this point in the book, and in your project, you should have a fully functional data loader built, tested, and optimized on both your local notebook and your SageMaker training instances. You should have your entire dataset identified, downloaded, processed, and ready to run through your training loop. You should have done at least one full pass through your training loop with a tiny sample of your dataset – something as small as 100 samples would be fine. You should have identified how you want to send your large dataset to your SageMaker training instances, possibly by using FSx for Lustre, and you should have this built, tested, and operational. You should also know a few other ways to store and process data <span class="No-Break">on AWS.</span></p>
			<p>You should be very comfortable making architectural decisions that reduce your project costs, such as opting for CPU-based data downloading and processing, along with the Python <strong class="source-inline">multiprocessing</strong> package to easily farm your tasks out to all available CPUs. You should also be comfortable parallelizing jobs on SageMaker training, such that you can run different jobs at the same time, each working on different parts of <span class="No-Break">your project.</span></p>
			<p>Now that you’ve fully prepared your dataset, in the next chapter, we’ll move on to the main event: training <span class="No-Break">your model!</span></p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor114"/>References</h1>
			<p>Please go through the following content for more information on a few topics covered in <span class="No-Break">the chapter.</span></p>
			<ol>
				<li><span class="No-Break"><em class="italic">amazon-sagemaker-notebook-instance-lifecycle-config-samples</em></span><span class="No-Break">: </span><a href="https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/blob/master/scripts/mount-fsx-lustre-file-system/on-start.sh&#13;"><span class="No-Break">https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/blob/master/scripts/mount-fsx-lustre-file-system/on-start.sh</span></a></li>
				<li><em class="italic">Choose the best data source for your Amazon SageMaker training </em><span class="No-Break"><em class="italic">job</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/</span></a></li>
			</ol>
		</div>
	

		<div id="_idContainer052" class="Content">
			<h1 id="_idParaDest-96"><span lang="en-US" xml:lang="en-US"><a id="_idTextAnchor115"/>Part 3: Train Your Model</span></h1>
			<p>In part 3, you’ll learn how to train your large-scale language and vision model. You’ll learn how to find the right hyperparameters, ensure that loss decreases, and troubleshoot ongoing <span class="No-Break">performance issues.</span></p>
			<p>This section has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B18942_07.xhtml#_idTextAnchor116"><em class="italic">Chapter 7</em></a>, <em class="italic">Finding the Right Hyperparameters</em></li>
				<li><a href="B18942_08.xhtml#_idTextAnchor127"><em class="italic">Chapter 8</em></a>, <em class="italic">Large-Scale Training on SageMaker</em></li>
				<li><a href="B18942_09.xhtml#_idTextAnchor138"><em class="italic">Chapter 9</em></a>, <em class="italic">Advanced Training Concepts</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer053" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer054">
			</div>
		</div>
	</body></html>