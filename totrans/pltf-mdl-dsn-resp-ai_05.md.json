["```py\n    !pip install --user git+https://github.com/mvaldenegro/keras-uncertainty.git\n    ```", "```py\nPredicted: [('n04326547', 'stone_wall', 0.81027746)]\n```", "```py\ndef build_model(layers_shape, input_dim, output_dim):\n    inputs = Input(shape=(input_dim,))\n    hidden = Dense(layers_shape[0], activation='relu', kernel_regularizer=l2(0.004))(inputs)\n    hidden = Dropout(0.05)(hidden, training=True)\n    for i in range(len(layers_shape)-1):\n        hidden = Dense(layers_shape[i+1], activation='relu', kernel_regularizer=l2(0.004))(hidden)\n        hidden = Dropout(0.05)(hidden, training=True)\n    outputs = Dense(output_dim)(hidden)\n    model = Model(inputs, outputs)\n    return model\nmodel = build_model(layers_shape=[5,10,20,10,5], input_dim= 1, output_dim=1)\nmodel.summary()\n```", "```py\nModel: \"model\"\n______________________________________________________\n Layer (type)             Output Shape         Param #\n======================================================\n input_1 (InputLayer)    [(None, 1)]              0\n dense (Dense)            (None, 5)               10\n dropout (Dropout)        (None, 5)               0\n dense_1 (Dense)          (None, 10)             60\n dropout_1 (Dropout)      (None, 10)             0\n dense_2 (Dense)          (None, 20)             220\n dropout_2 (Dropout)      (None, 20)             0\n dense_3 (Dense)          (None, 10)             210\n dropout_3 (Dropout)      (None, 10)             0\n dense_4 (Dense)          (None, 5)              55\n dropout_4 (Dropout)      (None, 5)              0\n dense_5 (Dense)          (None, 1)              6\n======================================================\nTotal params: 561\nTrainable params: 561\nNon-trainable params: 0\n______________________________________________________\n```", "```py\nmodel.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n```", "```py\nhistory = model.fit(x_data, y_data, batch_size=10, epochs=200, shuffle=True, verbose=1)\n```", "```py\nx_test = np.linspace(-10,10,100)\ny_pred = model.predict(x_test)\n```", "```py\nfig, ax = plt.subplots(1,1,figsize=(10,10))\nax.scatter(x_data, y_data, s=10, label='train data', color='red')\nax.plot(x_test, x_test, ls='--', label='ground truth', color='blue')\nax.plot(x_test, y_pred, label='Model Prediction - R2 {:.2f}'.format(r2_score(x_test, y_pred)), color='green')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.legend()\nax.set_title('Model performance on test data');\n```", "```py\npredictions = []\nfor _ in range(500):\n  predictions += [model.predict(x_test,verbose=0)]\nmean, std = np.mean(np.array(predictions), axis=0), np.std(np.array(predictions), axis=0)\nfig, ax = plt.subplots(1,1,figsize=(10,10))\nax.plot(x_test, x_test, ls='--', color='green', label='test data')\nax.scatter(x_data, y_data, color='blue', label='train data')\nax.set_title('{} - R2 {:.2f}'.format('Epistemic Uncertainity', r2_score(x_test, mean)))\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.fill_between(np.squeeze(x_test), np.squeeze(mean+1*std), np.squeeze(mean-1*std),  alpha=0.4, label='Epistemic Uncertainty', color='orange')\nax.legend();\n```", "```py\n    # aleatoric loss function\n    def aleatoric_loss(y_true, y_pred):\n        N = y_true.shape[0]\n        se = K.pow((y_true[:,0]-y_pred[:,0]),2)\n        inv_std = K.exp(-y_pred[:,1])\n        mse = K.mean(inv_std*se)\n        reg = K.mean(y_pred[:,1])\n        return 0.5*(mse + reg)\n    ```", "```py\n    def build_model(layers_shape, input_dim, output_dim):\n        inputs = Input(shape=(input_dim,))\n        hidden = Dense(layers_shape[0], activation='relu', kernel_regularizer=l2(0.004))(inputs)\n        for i in range(len(layers_shape)-1):\n            hidden = Dense(layers_shape[i+1], activation='relu', kernel_regularizer=l2(0.004))(hidden)\n        outputs = Dense(output_dim)(hidden)\n        model = Model(inputs, outputs)\n        return model\n    model = build_model(layers_shape=[5,10,20,10,5], input_dim= 1, output_dim=2)\n    model.summary()\n    ```", "```py\nModel: \"model\"\n___________________________________________________________\n Layer (type)                Output Shape              Param #\n===========================================================\n input_1 (InputLayer)        [(None, 1)]               0\n dense (Dense)               (None, 5)                 10\n dense_1 (Dense)             (None, 10)                60\n dense_2 (Dense)             (None, 20)                220\n dense_3 (Dense)             (None, 10)                210\n dense_4 (Dense)             (None, 5)                 55\n dense_5 (Dense)             (None, 2)                 12\n===========================================================\nTotal params: 567\nTrainable params: 567\nNon-trainable params: 0\n___________________________________________________________\n```", "```py\n    x_data_reshaped = x_data.reshape(x_data.shape[0], 1)\n    y_data_reshaped = np.vstack([y_data, np.zeros(y_data.shape)]).T\n    ```", "```py\n    model.compile(optimizer='rmsprop', loss=aleatoric_loss, metrics=['mae'])\n    ```", "```py\n    model.fit(x_data_reshaped, y_data_reshaped,\n              batch_size=10, epochs=1000, shuffle=True, verbose=1)\n    ```", "```py\n    x_test=np.linspace(-10,10,100)\n    p = np.array([model.predict(x_test, verbose=0)])\n    mean, epistemic_std = np.mean(p[:,:,0], axis=0), np.std(p[:,:,0], axis=0)\n    aleatoric_std = np.exp(0.5*np.mean(p[:,:,1], axis=0))\n    fig, ax = plt.subplots(1,1,figsize=(10,10))\n    ax.scatter(x_data, y_data, s=10, label='train data')\n    ax.plot(x_test, x_test, ls='--', label='test data', color='green')\n    ax.fill_between(np.squeeze(x_test), np.squeeze(mean+1*aleatoric_std), np.squeeze(mean-1*aleatoric_std),  alpha=0.4, label='Aleatoric Uncertainty (1 SD)', color='orange')\n    ax.fill_between(np.squeeze(x_test), np.squeeze(mean+2*aleatoric_std), np.squeeze(mean-2*aleatoric_std),  alpha=0.2, label='Aleatoric Uncertainty (2 SD)', color='blue')\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('{} - R2 {:.2f}'.format('Aleatoric Uncertainity', r2_score(x_test, mean)))\n    ax.legend()\n    ```"]