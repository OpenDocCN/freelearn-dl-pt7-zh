- en: Cats Versus Dogs - Image Classification Using CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use **convolutional neural networks** (**CNNs**) to
    create a classifier that can predict whether a given image contains a cat or a
    dog.
  prefs: []
  type: TYPE_NORMAL
- en: This project marks the first in a series of projects where we will use neural
    networks for image recognition and computer vision problems. As we shall see,
    neural networks have proven to be an extremely effective tool for solving problems
    in computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Motivation for the problem that we''re trying to tackle: image recognition'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks and deep learning for computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding convolution and max pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture of CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training CNNs in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using transfer learning to leverage on a state-of-the art neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of our results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key Python libraries required for this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: matplotlib 3.0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras 2.2.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numpy 1.15.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Piexif 1.1.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To download the dataset required for this project, please refer to the directions
    atÂ [https://github.com/PacktPublishing/Neural-Network-Projects-with-Python/blob/master/Chapter04/how_to_download_the_dataset.txt](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python/blob/master/chapter4/how_to_download_the_dataset.txt).
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found in the GitHub repository for the book
    at [https://github.com/PacktPublishing/Neural-Network-Projects-with-Python](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the code into your computer, you may run the following `git clone`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After the process is complete, there will be a folder titled `Neural-Network-Projects-with-Python`.
    Enter the folder by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To install the required Python libraries in a virtual environment, run the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that you should have installed Anaconda in your computer first, before
    running this command. To enter the virtual environment, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Important
  prefs: []
  type: TYPE_NORMAL
- en: This chapter requires an additional image processing library known as **`Piexif`**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download **`Piexif`**, please run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Navigate to the folder `Chapter04` by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following files are located in the folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '`main_basic_cnn.py`: This is the main code for the basic CNN'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`main_vgg16.py`: This is the main code for the VGG16 network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`utils.py`: This file contains auxiliary utility code that will help us in
    the implementation of our neural network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`visualize_dataset.py`: This file contains the code for exploratory data analysis
    and data visualization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_augmentation.py`: This file contains sample code for image augmentation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To run the code for the neural network, simply execute the `main_basic_cnn.py`
    and `main_vgg16.py` files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Computer vision and object recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computer vision is an engineering field where the objective is to create programs
    that can extract meaning from images. According to an urban legend, computer vision
    first started in the 1960s when Professor Marvin Minsky from MIT assigned a summer
    project to a group of undergraduates, with the requirement that they should attach
    a camera to a computer and to have the computer describe everything that it sees.
    The project was expected to be completed in just one summer. Needless to say,
    it wasn't completed within that summer as computer vision is an extremely complex
    field that scientists are continuously working on even today.
  prefs: []
  type: TYPE_NORMAL
- en: Early progression in computer vision was modest. In the 1960s, researchers started
    by creating algorithms to detect shapes, lines, and edges in photographs. The
    following decades saw the evolution of computer vision into several subfields.
    Computer vision researchers worked on signal processing, image processing, computer
    photometry, object recognition, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Object recognition is perhaps one of the most ubiquitous applications in computer
    vision. Researchers had worked on object recognition for a long time. The challenge
    faced by early object recognition researchers was that the dynamic appearance
    of objects made it difficult to teach computers to recognize them. Early computer
    vision researchers focused on template matching for object recognition, but often
    faced difficulties due to variations in angle, lighting, and occlusions.
  prefs: []
  type: TYPE_NORMAL
- en: The field of object recognition has grown exponentially in recent years, propelled
    by the advancements in neural networks and deep learning. In 2012, Alex Krizhevsky
    et al. won the **ImageNet Large Scale Visual Recognition Challenge** (**ILSVRC**)
    by a significant margin over other contenders. The winning idea proposed by Alex
    Krizhevsky et al. was to use a CNN (an architecture termed the AlexNet) for object
    recognition. AlexNet was a significant breakthrough for object recognition. Since
    then, neural networks have become the number one technique for object recognition
    and computer vision related tasks. In this project, you will create a CNN similar
    to AlexNet.
  prefs: []
  type: TYPE_NORMAL
- en: The breakthrough in object recognition also led to the rise of AI that we know
    today. Facebook uses facial recognition to automatically tag and classify photos
    of you and your friends. Security systems use facial recognition to detect intrusions
    and persons of interest. Self-driving cars use object recognition to detect pedestrians,
    traffic signs, and other road objects. In many ways, society is starting to view
    object recognition, computer vision, and AI as one entity, even though their roots
    are very much different.
  prefs: []
  type: TYPE_NORMAL
- en: Types of object recognition tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is important to understand the different kinds of object recognition tasks,
    as the required neural network architecture greatly depends on the task. Object
    recognition tasks can be broadly classified into three different types:'
  prefs: []
  type: TYPE_NORMAL
- en: Image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instance segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts the difference between each task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce20900f-6812-4db0-863f-02f28952210f.png)'
  prefs: []
  type: TYPE_IMG
- en: In **Image Classification**, the input to the problem is an image and the required
    output is simply a prediction of the class that the image belongs to. This is
    analogous to our first project, where we constructed a classifier to predict whether
    a patient is at risk of diabetes. In image classification, the problem is applied
    on pixels as our input data (specifically, the intensity value of each pixel),
    instead of tabular data represented by pandas DataFrames. In this project, we
    will focus on image classification.
  prefs: []
  type: TYPE_NORMAL
- en: In **Object Detection**, the input to the problem is an image and the required
    output are bounding boxes surrounding the detected objects. You can think of this
    as a step up from the image classification task. The neural network can no longer
    assume that there is only one class present in the image, and must assume that
    the image contains multiple classes. The neural network must then identify the
    presence of each class in the image, and to draw a bounding box around each of
    them. As you can imagine, this task is not trivial and object detection was a
    really difficult problem before neural networks came about. Today, neural networks
    can perform object detection efficiently. In 2014, 2 years after AlexNet was first
    developed, Girshick et al. showed that the results in image classification can
    be generalized to **Object** **Detection**. The intuitive idea behind their approach
    is to propose multiple boxes where objects of interest may exist, and then to
    use a CNN to predict the most likely class inside each bounding box. This approach
    is known as Regions with CNN (R-CNN).
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, in **Instance Segmentation**, the input to the problem is an image and
    the output are pixel groupings that correspond to each class. You can think of
    instance segmentation as a refinement of object detection. Instance segmentation
    is especially useful and prevalent in technology today. The portrait mode function
    in many smartphone cameras relies on instance segmentation to separate objects
    in the foreground from the background, creating a nice depth of field (bokeh)
    effect. Instance segmentation is also crucial in self-driving cars, as the location
    of each object around the car must be identified with pinpoint precision. In 2017,
    an adaption of R-CNN, known as Mask R-CNN, was shown to be extremely effective
    at instance segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, recent advancements in object recognition are driven by CNNs.
    In this project, we will gain an in-depth understanding of CNNs, and we will train
    and create one from scratch in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Digital images as neural network input
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that in previous chapters, we made the distinction that neural networks
    require numerical inputs. We saw how we can encode categorical features, such
    as day of week, into numerical features using one-hot encoding. How then do we
    use an image as input for our neural network? Well, the short answer is that all
    digital images are numerical in nature!
  prefs: []
  type: TYPE_NORMAL
- en: 'To see why this is so, consider a 28 x 28 image of a handwritten digit 3, as
    shown in the following screenshot. Let''s assume for now that the image is in
    grayscale (black and white). If we look at the intensity of each pixel that makes
    up the image, we can see that certain pixels are totally white, while some pixels
    are gray and black. In a computer, white pixels are represented with the value
    0 and black pixels are represented with a value of 255\. Everything else in between
    white and black (that is, shades of gray) has a value in between 0 and 255\. Therefore,
    digital images are essentially numerical data and neural networks are perfectly
    able to learn from them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74113975-2997-4039-88a7-b7556eaf7e58.png)'
  prefs: []
  type: TYPE_IMG
- en: What about color images? Color images are simply images with three channelsâred,
    green, and blue (commonly known as RGB). The pixel values in each channel then
    represent the red intensity, green intensity, and blue intensity. Another way
    to think about it is, that for a pure red image, the pixels value will be 255
    in the red channel and 0 for the green and blue channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following depicts a color image, and the separation of the color image
    into its RGB channels. Notice how a color image is stacked in a three-dimensional
    manner. In contrast, a grayscale image has only two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f72ca293-4530-4c09-8e83-9015b660530b.png)'
  prefs: []
  type: TYPE_IMG
- en: Building blocks of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the challenges faced in image classification is that the appearance of
    objects is dynamic. Just as there are many different breeds of cats and dogs,
    there are an infinite number of ways cats and dogs can appear in images. This
    makes it difficult for rudimentary image classification techniques, as it is impossible
    to show an infinite number of photos of cats and dogs to a computer.
  prefs: []
  type: TYPE_NORMAL
- en: However, this really shouldn't be a problem at all. Humans don't require an
    infinite number of photos of cats and dogs to differentiate between the two. A
    toddler can easily differentiate cats and dogs once he has seen just a few of
    them. If we think about how humans approach image classification, we notice that
    humans tend to look for landmark features while trying to identify an object.
    For example, we know that cats tend to be smaller in size compared to dogs, cats
    tend to have pointy ears, and cats have a shorter snout compared to dogs. Instinctively,
    humans look for these features while classifying an image.
  prefs: []
  type: TYPE_NORMAL
- en: Can we then teach a computer to look for these features within the entire image?
    The answer is a resounding yes! and the key lies in **convolution**.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering and convolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can understand what convolution is, it is important to first understand
    filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a 9 x 9 image as our input, and we need to classify the image
    as an X or an O. The following diagram illustrates some sample input images.
  prefs: []
  type: TYPE_NORMAL
- en: 'A perfectly drawn O is shown in the leftmost box in the following diagram,
    while the other two boxes show badly drawn Os:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e4ec0ce-4faf-4f96-9a19-3086ace1506a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A perfectly drawn X is shown in the leftmost box, while the other two boxes
    show badly drawn Xs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10a3b27d-a4f2-4509-b602-e213a1b21405.png)'
  prefs: []
  type: TYPE_IMG
- en: In either case, we cannot expect the figures to be drawn perfectly. This is
    no problem for human beings, as we can all differentiate between Os and Xs even
    for the badly drawn cases.
  prefs: []
  type: TYPE_NORMAL
- en: Let's think about what makes it easy for human beings to differentiate between
    the two. What are the characteristic features in the images that allows us to
    differentiate them easily? Well, we know that Os tend to have flat horizontal
    edges, while Xs tend to have diagonal lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts one such characteristic feature for Os:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69e1a3e5-4b5e-4e2d-bd9c-c8468ce9f09d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the following diagram depicts one such characteristic feature for Xs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bf69568-bdd4-4a90-b2ff-cefb5cc8d109.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the characteristic feature (also known as the filter) is of size
    3 Ã 3\. The presence of the characteristic feature in an image gives us a big
    hint on the class of the image. For example, if an image contains an horizontal
    edge, the characteristic feature for O, then the image is probably an O.
  prefs: []
  type: TYPE_NORMAL
- en: How then do we search for the presence of the characteristic feature in an image?
    We can simply do a brute force search by taking the 3 x 3 filter, before slidingÂ it
    through every single pixel in the image to look for a match.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start from the top left-hand corner of the image. The mathematical function
    performed by the filter (known as filtering) is the element-wise multiplication
    of the sliding window with the filter. In the top left-hand corner, the output
    from the filter is 2 (notice that this is a perfect match since the window is
    identical to the filter).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the filtering operation on the top left-hand corner
    of the image. Note that for simplicity, we assume that pixel intensity values
    are **0** or **1** (instead of 0-255 in real digital images):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a3148ca-59dd-4331-a23c-31a56289ba70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we slide the window toward the right to cover the next 3 x 3 section
    in the image. The following diagram shows the filtering operation on the next
    3 x 3 section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3e52908-0a61-4b38-9c31-afa013b52b51.png)'
  prefs: []
  type: TYPE_IMG
- en: The process of sliding the window through the entire image and calculating the
    filtered value is known as **convolution**. The layer in the neural network that
    performs convolution is known as the convolutional layer. Essentially, convolution
    provides us with a map to the areas where the characteristic feature is found
    in each image. This ensures that our neural network is able to perform intelligent,
    dynamic object recognition just like a human being!
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we handcrafted the filter based on our own knowledge
    of Os and Xs. Note that, when we train a neural network, it will automatically
    learn the most appropriate filter to use. Recall that in previous chapters, the
    fully connected layer (dense layer) was used and the weights of the layers were
    tuned during training. Similarly, the weights of a convolutional layer will be
    tuned during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, note that there are two main hyperparameters in a convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of filters**: In the preceding example, we have used just one filter.
    We can increase the number of filters to find multiple characteristic features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filter size**: In the preceding example, we have used a 3 x 3 filter size.
    We can tune the filter size to represent larger characteristic features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will talk about these hyperparameters in further detail when we construct
    our neural network later on in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Max pooling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In CNNs, it is common to place a max pooling layer immediately after a convolution
    layer. The objective of the max pooling layer is to reduce the number of weights
    after each convolution layer, thereby reducing model complexity and avoiding overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The max pooling layer does this simply by looking at each subset of the input
    passed to it, and throwing out all but the maximum value in the subset. Let''s
    take a look at an example to see what this means. Assume that our input to the
    max pooling layer is a 4 x 4 tensor (a tensor is just an n-dimensional array,
    such as those output by a convolutional layer), and we are using a 2 x 2 max pooling
    layer. The following diagram illustrates the **Max Pooling** operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84ff5c56-a00e-4df8-a0ac-bf443474f5c9.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see from the preceding diagram, **Max Pooling** simply looks at each
    2 x 2 region of the input, and discards all but the maximum value in that region
    (boxed up in the preceding diagram). This effectively halves the height and width
    of the original input, reducing the number of parameters before passing it to
    the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: Basic architecture of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen the basic building blocks of CNNs in the previous section. Now,
    we'll put these building blocks together and see what a complete CNN looks like.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are almost always stacked together in a block of convolution and pooling
    pattern. The activation function used for the convolution layer is usually ReLU,
    as discussed in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the first few layers in a typical CNN, made up
    of a series of convolution and pooling layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/033458e4-2f17-4317-8f90-2b3f9d981189.png)'
  prefs: []
  type: TYPE_IMG
- en: The final layers in a CNN will always be **Fully Connected** layers (dense layers)
    with a sigmoid or softmax activation function. Note that the sigmoid activation
    function is used for binary classification problems, whereas the softmax activation
    function is used for multiclass classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Fully Connected** layer is identical to those that we have seen in the
    first two chapters: [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101* and [Chapter 2](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml),
    *Diabetes Prediction with Multilayer Perceptrons*. At this point, you might be
    wondering what is the rationale for placing the fully connected layer at the end
    of a CNN? In CNNs, the early layers learn and extract the characteristic features
    of the data they are trying to predict. For example, we have seen how a convolutional
    layer learns the characteristic spatial features of Os and Xs. The convolutional
    layers then pass this information on to the fully connected layers, which then
    learn how to make accurate predictions, just like in an MLP.'
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, the early layers of a CNN are responsible for identifying the characteristic
    spatial features, and the fully connected layers at the end are responsible for
    making predictions. The implication of this is significant. Instead of handcrafting
    features (that is, day of week, distance, and so on) for the machine learning
    algorithm, as we did in the previous chapter, [Chapter 3](bf157365-e4d3-42ae-89f4-58c9047e6500.xhtml),
    *Predicting Taxi Fares with Deep Feedforward Nets*, we're simply providing all
    the data to the CNN as it is. The CNN then automatically learns the best characteristic
    features to differentiate the classes. This is true AI!
  prefs: []
  type: TYPE_NORMAL
- en: A review of modern CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've seen the basic architecture of CNNs, let's take a look at modern,
    state-of-the-art CNNs. We'll do a walk-through of the evolution of CNNs, and see
    how they have changed over the years. We'll not go into the technical and mathematical
    details behind the implementation. Instead, we'll provide an intuitive overview
    of some of the most important CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: LeNet (1998)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first CNN was developed by Yann LeCun in 1998, with the architecture known
    as LeNet. LeCun was the first to prove that CNNs were effective in image recognition,
    particularly in the domain of handwritten digits recognition. However, throughout
    the 2000s, few scientists managed to build on the work done by LeCun and there
    were few breakthroughs in CNNs (and AI in general).
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet (2012)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier, AlexNet was developed by Alex Krizhevsky et al. and
    it was used to win the ILSVRC in 2012\. AlexNet was built on the same principles
    as LeNet, although AlexNet used a much deeper architecture. The overall number
    of trainable parameters in AlexNet is around 60 million, over 1,000 times more
    than LeNet.
  prefs: []
  type: TYPE_NORMAL
- en: VGG16 (2014)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VGG16 was developed by Oxford's **Visual Geometry Group** (**VGG**) and it was
    considered to be a very important neural network. VGG16 was one of the first CNNs
    to deviate from large filter sizes, instead using a convolution filter size of
    3 x 3.
  prefs: []
  type: TYPE_NORMAL
- en: VGG16 finished second in the image recognition task in the ILSVRC in 2014\.
    A downside to VGG16 is that there are many more parameters to be trained, leading
    to a significant training time.
  prefs: []
  type: TYPE_NORMAL
- en: Inception (2014)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Inception network was developed by researchers from Google and it won the
    ILSVRC in 2014\. The guiding principle for the Inception network was to provide
    highly accurate predictions efficiently. Google's interest was to create a CNN
    that could be trained and deployed in real time across their network of servers.
    To do that, the researchers developed something known as the Inception module,
    that vastly improved training time while maintaining its accuracy. In fact, in
    the 2014 ILSVRC, the Inception network managed to achieve a higher accuracy than
    VGG16, despite having far fewer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The Inception network has been continuously improved upon. At the time of writing,
    the latest Inception network is at its 4th version (commonly known as Inception-v4).
  prefs: []
  type: TYPE_NORMAL
- en: ResNet (2015)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **residual neural network** (**ResNet**) was introduced by Kaiming He et
    al. at the 2015 ILSVRC (by now, you should notice that this competition is extremely
    important for neural networks and computer vision, and new state-of-the-art techniques
    are revealed during the annual competition).
  prefs: []
  type: TYPE_NORMAL
- en: The salient feature of ResNet was the residual block technique, which allowed
    the neural network to be deeper while keeping the number of parameters moderate.
  prefs: []
  type: TYPE_NORMAL
- en: Where we stand today
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen, CNNs have progressed and improved exponentially in the past
    few years. In fact, recent CNNs can outperform humans at certain image recognition
    tasks. The recurring theme in recent years is to use innovative techniques to
    improve model performance, while preserving the model complexity. Clearly, the
    speed of the neural network is just as important as the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The cats and dogs dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the theory behind CNNs, let's dive into data exploration.
    The cats and dogs dataset is provided by Microsoft. The instructions for the downloading
    and setting up of the dataset can be found in the *Technical requirements* section
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s plot the images to better understand the kind of data we''re working
    with. To do that, we can simply run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fb5749e-b73c-4362-ae4f-32c2e59ad805.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can make some observations about our data:'
  prefs: []
  type: TYPE_NORMAL
- en: The images have different dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subjects (cat/dog) are mostly centered in the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subjects (cat/dog) have different orientations, and they may be occluded
    in the image. In other words, there's no guarantee that we'll always see the tail
    of the cat in the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s do the same for the dog images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6506c687-6339-4c5a-ac97-2cb2ba0e5cea.png)'
  prefs: []
  type: TYPE_IMG
- en: Managing image data for Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One common problem encountered in neural network projects for image classification
    is that most computers do not have sufficient RAM to load the entire set of data
    into memory. Even for relatively modern and powerful computers, it would be far
    too slow to load the entire set of images into memory and to train a CNN from
    there.
  prefs: []
  type: TYPE_NORMAL
- en: To alleviate this problem, Keras provides a useful `flow_from_directory` method
    that takes as an input the path to the images, and generates batches of data as
    output. The batches of data are loaded into memory, as required before model training.
    This way, we can train a deep neural network on a huge number of images without
    worrying about memory issues. Furthermore, the `flow_from_directory` method allows
    us to perform image preprocessing steps such as resizing and other image augmentation
    techniques by simply passing an argument. The `flow_from_directory` method would
    then perform the necessary image preprocessing steps in real time before passing
    the data for model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do all these, there are certain schemas for file and folder management that
    we must abide by, in order for `flow_from_directory` to work. In particular, we
    are required to create subdirectories for training and testing data, and within
    the training and testing subdirectories, we need to further create one subdirectory
    per class. The following diagram illustrates the required folder structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75e11cdf-ef7e-43d4-a7b7-015930412358.png)'
  prefs: []
  type: TYPE_IMG
- en: The `flow_from_directory` method would then infer the class of the images from
    the folder structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The raw data is provided in a `Cat` and `Dog` folder, without separation of
    training and testing data. Therefore, we need to split the data into a `Train`
    and `Test` folder as per the preceding schema. To do that, we need to perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create `/Train/Cat`, `/Train/Dog`, `/Test/Cat`, and `/Test/Dog` folders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly assign 80% of the the images as train images and 20% of the images
    as test images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy those images into the respective folders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We have provided a helper function in `utils.py` to do these steps. We simply
    need to invoke the function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run into an error while executing this code block, with the error message
    ImportError: No Module Named Piexif, it means that you have not installed Piexif
    in your Python virtual environment. This chapter requires an additional library
    for image processing. To download Piexif, please follow the instructions in the
    *Technical requirements section* at the start of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Great! Our images are now placed in the appropriate folders for Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Image augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start building our CNN, let''s take a look at image augmentation,
    which is an important technique in image classification projects. Image augmentation
    is the creation of additional training data by making minor alterations to images
    in certain ways in order to create new images. For example, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Image rotation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal flip
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zooming into the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The motivation for image augmentation is that CNNs require a huge amount of
    training data before they can generalize well. However, it is often difficult
    to collect data, more so for images. With image augmentation, we can artificially
    create new training data based on the existing images.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, Keras provides a handy `ImageDataGenerator` class to help us easily
    perform image augmentation. Let''s create a new instance of the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s use it to augment a randomly selected image from the `/Train/Dog/`
    folder. Then, we can plot it to compare the augmented images with the original
    image. We can do this by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1821f8c-9640-4c89-bfba-1d055c090322.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, each augmented image is randomly shifted or rotated by a certain
    amount as controlled by the arguments passed into the `ImageDataGenerator` class.
    These augmented images will provide supplemental training data for our CNN, increasing
    the robustness of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Model building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're finally ready to start building our CNN in Keras. In this section, we'll
    take two different approaches to model building. First, we'll start by building
    a relatively simple CNN consisting of a few layers. We'll take a look at the performance
    of the simple model, and discuss its pros and cons. Next, we'll use a model that
    was considered state-of-the art just a few years agoâthe VGG16 model. We'll see
    how we can leverage on the pre-trained weights to adapt the VGG16 model for cats
    versus dogs image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Building a simple CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In an earlier section, we showed how the fundamental building blocks of a CNN
    consist of a series of convolutional and pooling layers. In this section, we''re
    going to build a basic CNN consisting of this repeating pattern, as shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c38754ca-f2ea-425a-b7a6-1fe0f2f5074e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This basic CNN consists of two repeated blocks of **Convolution** and **Max**
    **Pooling**, following by two **Fully Connected** layers. As discussed in a previous
    section, the convolution and max pooling layers are responsible for learning the
    spatial characteristics of the classes (for example, identifying the ears of cats),
    whereas the **Fully Connected** layers learn to make predictions using these spatial
    characteristics. We can thus represent the architecture of our basic CNN in another
    manner (we shall see why it is useful to visualize our neural network in this
    manner in the next subsection):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79d1c8a6-e093-45eb-b873-5c137649b1af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Building a CNN is similar to building an MLP or a feedforward neural network,
    as we''ve done in the previous chapters. We''ll start off by declaring a new `Sequential`
    model instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we add any convolutional layers, it is useful to think about the hyperparameters
    that we are going to use. For a CNN, there are several hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional layer filter size**: Most modern CNNs use a small filter size
    of `3` x `3`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of filters**: Let''s use a filter number of `32`. This is a good balance
    between speed and performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input size**: As we''ve seen in an earlier section, the input images have
    different sizes, with their width and height approximately 150 px. Let''s use
    an input size of `32` x `32` pixels. This compresses the original image, which
    can result in some information loss, but helps to speed up the training of our
    neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max pooling size**: A common max pooling size is `2` x `2`. This will halve
    the input layer dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: This corresponds to the number of training samples to use in
    each mini batch during gradient descent. A large batch size results in more accurate
    training but longer training time and memory usage. Let''s use a batch size of
    `16`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Steps per epoch**: This is the number of iterations in each training epoch.
    Typically, this is equal to the number of training samples divided by the batch
    size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epochs**: The number of epochs to train our data. Note that, in neural networks,
    the number of epochs refers to the number of times the model sees each training
    sample during training. Multiple epochs are usually needed, as gradient descent
    is an iterative optimization method. Let''s train our model for `10` epochs. This
    means that each training sample will be passed to the the model 10 times during
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s declare variables for these hyperparameters so that they are constant
    throughout our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now add the first convolutional layer, with `32` filters, each of size
    (`3` x `3`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add a max pooling layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the basic convolution-pooling pattern of our CNN. Let''s repeat this
    once more according to our model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We are now done with the convolution and pooling layers. Before we move on to
    the fully connected layers, we need to flatten its input. `Flatten` is a function
    in Keras that transforms a multidimensional vector into a single dimensional vector.
    For example, if the vector is of shape (5,5,3) before passing to `Flatten`, the
    output vector will be of shape (75) after passing to `Flatten`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add a `Flatten` layer, we simply run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now add a fully connected layer with `128` nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we add our last fully connected layer, it is a good practice to add
    a dropout layer. The dropout layer randomly sets a certain fraction of its input
    to 0\. This helps to reduce overfitting, by ensuring that the model does not place
    too much emphasis on certain weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We add one last fully connected layer to our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that the last fully connected layer should have only one node, as we're
    doing binary classification (cat or dog) in this project.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll compile our model using the `adam` optimizer. The `adam` optimizer is
    a generalization of the **stochastic gradient descent** (**SGD**) algorithm that
    we''ve seen in [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101* and it is widely used to train CNNs. The loss
    function is `binary_crossentropy` since we''re doing a binary classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In general, we use `binary_crossentropy` for binary classification problems
    and `categorical_crossentropy` for multiclass classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re now ready to train our CNN. Notice that we have not loaded any of the
    data into memory. We''ll use the `ImageDataGenerator` and `flow_from_directory`
    method to train our model in real time, which loads batches of the dataset into
    memory only as required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This will start the training and once it is complete, you will see the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b1c40fb-6e06-4ea1-9067-5e87e701013c.png)'
  prefs: []
  type: TYPE_IMG
- en: We can clearly see that the loss decreases while the accuracy increases with
    each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our model is trained, let''s evaluate it on the testing set. We''ll
    create a new `ImageDataGenerator` and call `flow_from_directory` on the images
    in the `test` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93b08d19-3f6c-493a-b10e-7aff0f2df243.png)'
  prefs: []
  type: TYPE_IMG
- en: We obtained an accuracy of 80%! That's pretty impressive considering that we
    only used a basic CNN. This shows the power of CNNs; we obtained an accuracy close
    to human performance from just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging on pre-trained models using transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can we take our model further? Can we achieve close to 90%, reaching human level
    performance? As we shall see in this section, we can obtain better performance
    by leveraging on transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transfer learning is a technique in machine learning where a model trained
    for a certain task is modified to make predictions for another task. For example,
    we may use a model trained to classify cars to classify trucks instead, since
    they are similar. In the context of CNN, transfer learning involves freezing the
    convolution-pooling layers, and only retraining the final fully connected layers.
    The following diagram illustrates this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc182395-a155-4b17-bdd9-796bd417fb1c.png)'
  prefs: []
  type: TYPE_IMG
- en: How does transfer learning work? Intuitively, the purpose of the convolution
    and pooling layers is to learn the spatial characteristics of the classes. We
    can therefore reuse these layers since the spatial characteristics are similar
    in both tasks. We just need to retrain the final fully connected layers to re-purpose
    the neural network to make predictions for the new class. Naturally, a crucial
    requirement for transfer learning is that tasks A and B must be similar to one
    another.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we're going to re-purpose the VGG16 model to make predictions
    on images of cats and dogs. The VGG16 model was originally developed for the ILSVRC,
    which required the model to make a 1,000 class multiclass classification. Among
    the 1,000 classes are specific breeds of cats and dogs. In other words, VGG16
    knows how to recognize specific breeds of cats and dogs, and not just cats and
    dogs in general. It is therefore a viable approach to use transfer learning using
    the VGG16 model for our cats and dogs image classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The VGG16 model and its trained weights are provided directly in Keras. Let''s
    create a new `VGG16` model, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note that we used `include_top=False` when we created a new VGG16 model. This
    argument tells Keras not to import the fully connected layers at the end of the
    VGG16 network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re now going to freeze the rest of the layers in the VGG16 model, since
    we''re not going to retrain them from scratch. We can freeze the layers by running
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''re going to add a fully connected layer with `1` node right at the
    end of the neural network. The syntax to do this is slightly different, since
    the VGG16 model is not a Keras `Sequential` model that we''re used to. In any
    case, we can add the layers by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This is just a manual way of adding layers in Keras, which the `.``add()` function
    in `Sequential` model has simplified for us so far. The rest of the code is similar
    to what we have seen in the previous section. We declare a training data generator,
    and we train the model (only the newly added layers) by calling `flow_from_directory()`.
    Since we only need to train the final layer, we''ll just train the model for `3`
    epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: Caution
  prefs: []
  type: TYPE_NORMAL
- en: The following code block takes around an hour to run if you are not running
    Keras on a GPU (graphics card). If the code takes too long to run on your computer,
    you may reduce the `INPUT_SIZE` parameter to speed up model training. However,
    note that this will lower the accuracy of your model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/725f105b-08fe-45b7-b095-a9887fd3cf0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The training accuracy doesn''t look much different to the basic CNN in the
    previous section. This is expected, since both neural networks do really well
    in the training set. However, the testing accuracy is ultimately the metric which
    we will use to evaluate the performance of our model. Let''s see how well it does
    on the testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0aabb03-fbb3-4ce4-a0b4-a43a08c6e82a.png)'
  prefs: []
  type: TYPE_IMG
- en: That's amazing! By making use of transfer learning, we managed to obtain a testing
    accuracy of 90.5%. Note that the training time here is much shorter than training
    a VGG16 model from scratch (it would probably take days to train a VGG16 model
    from scratch, even with a powerful GPU!), since we are only training the last
    layer. This shows that we can leverage on a pre-trained state-of-the art model
    like VGG16 to make predictions for our own projects.
  prefs: []
  type: TYPE_NORMAL
- en: Results analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a deeper look into our results. In particular, we would like to know
    what kind of images our CNN does well in, and what kind of images it gets wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the output of the sigmoid activation function in the last layer
    of our CNN is a list of values between 0 and 1 (one value/prediction per image).
    If the output value is <Â `0.5`, then the prediction is class 0 (that is, cat)
    and if the output value is >= `0.5`, then the prediction is class 1 (that is,
    dog). Therefore, an output value close to `0.5` means that the model isn't so
    sure, while an output value very close to `0.0` or `1.0` means that the model
    is very sure about its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s run through the images in the testing set one by one, using our model
    to make predictions on the class of the image, and classify the images according
    to three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Strongly right predictions**: The model predicted these images correctly,
    and the output value is `> 0.8` or `< 0.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strongly wrong predictions**: The model predicted these images wrongly, and
    the output value is `> 0.8` or `< 0.2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weakly wrong predictions**: The model predicted these images wrongly, and
    the output value is between `0.4` and `0.6`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code snippet will do this for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s visualize the images from these three groups by randomly selecting `9`
    of the images in each group, and plot them on a 3 Ã 3 grid. The following helper
    function allows us to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now plot `9` randomly selected images from the strongly right predictions
    group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c6063b9-1286-45bc-aa15-7a2b8105ff50.png)'
  prefs: []
  type: TYPE_IMG
- en: Selected images that have strong predictions, and are correct
  prefs: []
  type: TYPE_NORMAL
- en: No surprises there! These are almost classical images of cats and dogs. Notice
    that the pointy ears of cats and the dark eyes of dogs can all be seen in the
    preceding images. These characteristic features allow our CNN to easily identify
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a look at the strongly wrong predictions group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c9e0e1f-4199-471b-8381-526474ead5ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Selected images that have strong predictions, but are wrong
  prefs: []
  type: TYPE_NORMAL
- en: We notice a few commonalities among these strongly wrong predictions. The first
    thing we notice is that certain dogs do resemble cats with their pointy ears.
    Perhaps our neural network placed too much emphasis on the pointy ears and classified
    these dogs as cats. Another thing we notice is that some of the subjects were
    not facing the camera, making it really difficult to identify them. No wonder
    our neural network got them wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s take a look at the weakly wrong predictions group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d65f062-df15-4c19-9741-77903ddf6738.png)'
  prefs: []
  type: TYPE_IMG
- en: Selected images that have weak predictions, and are wrong
  prefs: []
  type: TYPE_NORMAL
- en: These images are ones that our model is on the fence with. Perhaps there is
    an equal number of characteristics to suggest that the object could be a dog or
    a cat. This is perhaps the most obvious with the images in the first row, where
    the puppies in the first row have a small frame like a cat, which could have confused
    the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built a classifier that can predict whether an image contains
    a cat or a dog by using two different CNNs. We first went through the theory behind
    CNNs, and we understood that the fundamental building blocks of a CNN are the
    convolution, pooling, and fully connected layers. In particular, the front of
    the CNN consists of a block of convolution-pooling layers, repeated an arbitrary
    number of times. This block is responsible for identifying spatial characteristics
    in the images, which can be used to classify the images. The back of the CNN consists
    of fully connected layers, similar to an MLP. This block is responsible for making
    the final predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In the first CNN, we used a basic architecture that achieved 80% accuracy on
    the testing set. This basic CNN consists of two convolutional-max pooling layers,
    followed by two fully connected layers. In the second CNN, we used transfer learning
    to leverage on the pre-trained VGG16 network for our classification. We removed
    the final fully connected layer with 1,000 nodes in the pre-trained network, and
    added our own fully connected layer with one node (for our binary classification
    task). We managed to obtain an accuracy of 90% using the fine-tuned VGG16 model.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we visualized the images that our model did well in, as well as the
    images that our model struggled with. We saw that our model could not be certain
    when the subject is not facing the camera or when the subject has characteristics
    resembling both a cat and a dog (for example, a small puppy with pointy ears).
  prefs: []
  type: TYPE_NORMAL
- en: That concludes the chapter on using CNNs for image recognition. In the next
    chapter, [Chapter 5](16d0775b-23ec-456a-a57b-cba2e9da7570.xhtml), *Removing Noise
    from Images Using Autoencoders*, we'll use an autoencoder neural network to remove
    noise from images.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How are images represented in computers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Images are represented in computers as a group of pixels, with each pixel having
    its own intensity (value between 0 and 255). Color images have three channels
    (red, green, and blue) while grayscale images have only one channel.
  prefs: []
  type: TYPE_NORMAL
- en: What are the fundamental building blocks of a CNN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All convolutional neural network consists of convolution layers, pooling layers,
    and fully connected layers.
  prefs: []
  type: TYPE_NORMAL
- en: What is the role of the convolutional and pooling layers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The convolutional and pooling layers are responsible for extracting spatial
    characteristics from the images. For example, when training a CNN to identify
    images of cats, one such spatial characteristic would be the pointy ears of cats.
  prefs: []
  type: TYPE_NORMAL
- en: What is the role of the fully connected layers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fully connected layers are similar to the those in MLPs and feedforward
    neural networks. Their role is to use the spatial characteristics as input, and
    to output predicted classes.
  prefs: []
  type: TYPE_NORMAL
- en: What is transfer learning, and how is it useful?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transfer learning is a technique in machine learning where a model trained for
    a certain task is modified to make predictions for another task. Transfer learning
    allows us to leverage on state-of-the art models, such as VGG16, for our own purposes,
    with minimal training time.
  prefs: []
  type: TYPE_NORMAL
