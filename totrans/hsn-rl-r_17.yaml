- en: Whats Next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will summarize what has been covered so far in this book
    and what the next steps are from this point on. You will learn how to apply the
    skills you have gained to other projects, including real-life challenges in building
    and deploying **reinforcement learning** (**RL**) models and other common technologies
    that data scientists often use. By the end of this chapter, you will have a better
    understanding of real-life challenges in building and deploying RL models and
    additional resources and technologies that you can learn about to sharpen your
    RL skills.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we will have explored a quick summary of RL concepts
    and the main RL applications in real life. Therefore, we will discover the next
    steps for RL and the real-world challenges in terms of constructing and implementing
    machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: RL summarized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring RL projects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next steps for RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RL summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**)-based algorithms can be classified based on the
    training paradigm they use. In supervised learning, there is a teacher who tells
    the system what the correct output is. This is not always possible. Often, we
    only have qualitative information that''s sometimes binary, right/wrong, or success/failure.
    This information is called **reinforcement signals**. However, the problem is
    that the system does not provide any information on how to update the behavior
    of the agent. A cost function or gradient is not available. In the case of RL,
    we want to create intelligent agents who can learn from their experience.'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, RL was viewed as **supervised learning**, but subsequently it was
    considered to be the third paradigm of machine learning algorithms. It is applied
    in different contexts in which supervised learning is inefficient, for example,
    when we have problems interacting with the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following flow shows the steps to follow to correctly apply an RL algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparation of the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observation of the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selection of the optimal strategy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execution of actions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculation of the corresponding reward (or penalty).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Development of updating strategies (if necessary).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2-5 repeatedly until the agent learns the optimal strategies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RL algorithms work by trial and error using a feedback loop of rewards and punishments.
    When we insert a dataset into the algorithm, it treats the environment like a
    game and, every time it performs an action, it is told whether it has won or lost.
    In this way, it builds an image of winning and losing moves.
  prefs: []
  type: TYPE_NORMAL
- en: As a consequence of undesirable behavior, punishment is applied, which reduces
    the probability of a repetition of the error. In the case of the correct behavior
    being displayed, a reward is applied that identifies a correct policy. Having
    defined the objective to be reached, the algorithm tries to maximize the rewards
    that are received for the execution of the action. The object that must achieve
    the goal is called the agent. The object that the agent must interact with is
    called the environment, which corresponds to everything that is external to the
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: The agent is a software object that performs actions automatically and invisibly.
    The agent has a goal-oriented behavior but acts in an uncertain environment that's
    not initially known or is only partially known. An agent learns by interacting
    with the environment. The decision it makes can be developed while learning about
    the environment through measurements that are made by the agent itself.
  prefs: []
  type: TYPE_NORMAL
- en: The interaction is characterized by repeated and various attempts that continue
    until success or until the agent stops trying. The agent-environment interaction
    is continuous; the agent chooses an action to be taken and, in response, the environment
    changes state, presenting a new situation to be addressed. In the case of RL,
    the environment provides the agent with a reward. It is essential that the source
    of the reward is the environment to avoid the formation of a personal reinforcement
    mechanism within the agent that would compromise learning.
  prefs: []
  type: TYPE_NORMAL
- en: The value of the reward is proportional to the influence the action has on achieving
    the goal; therefore, it is positive or high in the case of a correct action or
    negative or low for a wrong action.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discover the most famous projects that use RL.
    We will see that the largest companies in the world have invested large resources
    to exploit the potential offered by these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring RL projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL is a programming paradigm that processes algorithms that are capable of learning
    and adapting to changes in the environment. At the base of this programming technique,
    there is the interaction with the environment, where the agent receives stimuli
    from the outside according to the choices of the algorithm. A correct choice will
    provide a reward, while an incorrect choice will provide a penalty. The best possible
    result is achieved by maximizing the rewards that are obtained by the system.
    For example, the computer learns to beat an opponent in a game by performing a
    certain task, with the goal of maximizing the reward. This means that the system
    learns from the mistakes it made previously, improving on performance based on
    the results that were achieved in previous explorations. The applications of RL
    in everyday life are already numerous, some of which have entered everyday use
    for some time. For example, RL is the basis for the development of self-driving
    cars – in fact, they learn to recognize the surrounding environment with data
    that's been collected by sensors. As a result, they adapt their behavior according
    to the obstacles they have to overcome. Another example is given by programs for
    profiling web users. They benefit from automatic learning, that is, from learning
    from the behavior and preferences of users browsing websites. Further examples
    include e-commerce platforms, such as Amazon, or entertainment and access to content,
    such as Netflix or Spotify. In the upcoming sections, we will look at some practical
    examples of the application of RL-based technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Facebook ReAgent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recommendation systems are applied in different sectors, but their purpose
    is unique: to help people make choices based on different aspects. According to
    the person, these aspects can be, for example, their own chronology, the purchases
    they''ve already made, the positive votes they''ve already given, or the preferences
    of similar people.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A recommendation system is a useful tool for producing object recommendations
    based on the usage history of the person using them. There are already several
    applications of these tools that still constitute an important area of research
    and range from the recommendation of consumer goods in the case of Amazon, to
    the recommendation of films in the case of MovieLens, to the recommendation of
    news or research articles. Primarily, a recommendation system performs three operations:
    it gets people''s preferences from the input data, calculates recommendations,
    and presents them to people.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it's a system that guides the user in making decisions. Such a system
    can be treated as a Markovian process, which means it can be modeled with RL.
    For example, we can use contextual bandits that are effective at selecting actions
    based on contextual information. In the recommendation of news articles, this
    involves selecting articles for users based on the contextual information of users
    and articles, such as the historical activities of user's descriptive information
    and content categories.
  prefs: []
  type: TYPE_NORMAL
- en: Facebook ReAgent is a platform based on RL that uses this paradigm to optimize
    products and services that are used by billions of people ([https://github.com/facebookresearch/ReAgent](https://github.com/facebookresearch/ReAgent)).
    Horizon is the first platform based on open source RL for production. The researchers
    at Facebook have developed this platform so that it extends the remarkable results
    they've achieved in research in the use of decision algorithms to the productive
    sector.
  prefs: []
  type: TYPE_NORMAL
- en: Unity ML-Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Machine Learning Agents Toolkit is an open source plugin offered for free
    on GitHub with the aim of providing developers with the possibility of training
    virtual entities in ad hoc environments using the Unity engine ([https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)).
  prefs: []
  type: TYPE_NORMAL
- en: For agent training, RL, imitation learning, neuroevolution, and other machine
    learning methods can be used through an easy to use Python API. Furthermore, TensorFlow-based
    implementations of algorithms are available to easily train intelligent agents
    for 2D, 3D, and VR/AR games. The ML-Agents toolkit can be used to develop applications
    in a very productive way – in fact, progress can be evaluated in the rich environments
    of Unity.
  prefs: []
  type: TYPE_NORMAL
- en: Google DeepMind
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DeepMind is a British artificial intelligence company that was founded in 2010
    as DeepMind Technologies but was later acquired by Google in 2014\. DeepMind is
    Google's already legendary artificial intelligence wing for creating AlphaGo,
    the AI that beat the best human Go player in the world.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the choices that DeepMind makes are not based on external indications
    or instructions written in the lines of the source code of a program. The Google
    supercomputer can prefer an option among multiple possibilities that's based on
    the experience acquired and the information coming from the external environment.
    All of this is made possible by the **Differential Neural Computer** (**DNC**),
    a computer system that replicates the functionality of human memory.
  prefs: []
  type: TYPE_NORMAL
- en: Managing large amounts of data using IBM Watson
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Watson is a project of the IBM group that's focused on creating an artificial
    intelligence system that's capable of handling large amounts of unstructured data
    to transform this resource into structured information and then decisions. Watson
    was born as a platform that users can access to give life to their ideas. The
    complexity and heterogeneity of data, many of which are unstructured, constitute
    the most difficult obstacle to overcome. Despite the improvements of classical
    analytical methods, the construction of useful information starting from disaggregated
    data remains only partially realized. Data is generated seamlessly, producing
    2.5 billion GB each day. Therefore, it is essential to consider the data as a
    new natural resource that competitive strength can derive from.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have analyzed numerous algorithms and learned how to use RL in different
    areas. But what are the future challenges facing researchers all over the world?
  prefs: []
  type: TYPE_NORMAL
- en: Next steps for RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern technology has made us accustomed to seeing machines that perform tasks
    instead of humans. In the automation of production processes, quick and precise
    calculations and the execution of instructions with a minimum margin of error
    make the machines competitive, if not even more so than human beings. In short,
    more and more often, we're developing algorithms that can help the software learn
    from experience. This is called machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Based on these algorithms, predefined rules to be learned with the machine are
    no longer necessary, but they make decisions using models and instructions through
    which we can learn the right rules to solve the problem in question. As we saw
    in the previous section, these technologies are already widely used in real life.
    Examples include the fight against spam and the identification of credit card
    frauds, voice recognition and manual writing, economic and financial forecasts,
    automatic classification of images, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: What will this technology offer us in the future? Let's see what the future
    challenges are when it comes to using RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Inverse RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In RL algorithms, the agent receives a reinforcement signal as soon as they
    perform an action. There are several areas where it is difficult to estimate a
    reinforcement function. **Inverse RL** (**IRL**) allows you to reconstruct a reinforcement
    function that defines the behavior of an agent based on a set of demonstrations.
    When learning about reverse reinforcement, the reward function derives from the
    observed behavior. Generally, in RL, we use rewards to learn the behavior of a
    system. In IRL, this function is reversed; in fact, the agent observes the behavior
    of the system to understand what the goal is trying to achieve. To do this, it
    is necessary to have the state of the environment available to learn the optimal
    policy for each reinforcement function that's considered. Furthermore, it is necessary
    to define a set of features to be combined in order to identify a single reinforcement
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an IRL, the problem starts with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Measurements of an agent's behavior over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measurements of sensory input for that agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model of the physical environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on this data, we can determine the reward function that the agent is optimizing.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of policy, solving an IRL problem means coding the expert's behavior
    in a reference policy and then identifying a reward function so that this policy
    is an optimal policy. This approach, whose goal is to identify the reward function,
    can provide better results than one that merely builds a policy so that the agent
    imitates the expert. This is because the reward function does not simply describe
    the behavior of the agent, but rather encodes the motivations in more depth.
  prefs: []
  type: TYPE_NORMAL
- en: This representation is more concise than a policy-based representation and allows
    the agent's behavior to be generalized, even to regions of the state space that
    haven't been explored by the expert. By knowing about the reward function, it
    is also possible to react to changes in the model by regenerating a new optimal
    policy starting from the reward function itself, instead of having to relearn
    it completely. Finally, there are contexts in which the reward function itself
    is the objective of the research.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge that concerns policy algorithms is deep deterministic policy
    gradients. Let's see how this can be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: Deep deterministic policy gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep deterministic policy gradients** (**DDPG**) is part of the family of
    policy gradient algorithms that uses a stochastic behavioral policy in the exploration
    phase. The DDPG algorithm evaluates a deterministic target policy that''s much
    easier to learn. In these policy algorithms, iteration is characterized by the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation of the policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following the policy gradient to maximize performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The DDPG represents an out-of-policy algorithm that uses a deterministic target
    policy. Under these conditions, the use of the gradient theorem of deterministic
    politics is allowed. DDPG is also an example of an actor-critic algorithm. In
    this sense, it mainly uses two neural networks: one for the actor and one for
    the critic. Both networks calculate the action forecasts for the current state
    and generate a **time difference** (**TD**) error signal each time. The actor''s
    network input uses the current state, and the output is represented by an action
    that''s been chosen by a continuous action space. The output of the critic is
    simply the estimated Q value of the current state and of the action given by the
    actor. The deterministic policy gradient theorem provides the updating rule for
    actor net weights. The critical network is updated by the gradients that have
    been obtained from the error signal TD.'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen how the agent interacts with the environment, but what
    happens when it interacts with humans?
  prefs: []
  type: TYPE_NORMAL
- en: RL from human preferences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases, it isn't possible to define a well-specified reward function.
    Many real-life problems are characterized by complex goals that have been poorly
    defined or difficult to specify. For example, suppose you want to use RL to train
    a robot to choose the best path to reach a target. The reward function is not
    easily defined. It will have to depend on the data coming from the robot sensors.
    In case there was the possibility of successfully communicating our real goals
    to our agents, we would have achieved a great result in solving these problems.
  prefs: []
  type: TYPE_NORMAL
- en: If we had examples of the desired task, we could extract a reward function using
    reverse RL. Later, we could use learning imitation to clone proven behavior. Unfortunately,
    these approaches are not directly applicable to many of the real problems. Something
    that could help solve this problem could come from the feedback from a human defining
    the activity. In this case, we find ourselves in the paradigm of RL, although
    the use of human feedback directly as a reward function is prohibitive for RL
    systems that require hundreds or thousands of hours of experience.
  prefs: []
  type: TYPE_NORMAL
- en: In deep RL based on human preferences, the agent learns a reward function from
    human feedback and thus optimizes this reward function. This represents a solution
    to the problems of sequential decision-making without a well-specified reward
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm has the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: It allows us to solve tasks where we can only recognize the desired behavior,
    but not necessarily prove it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows agents to be instructed by non-expert users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a large-scale problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is economical with user feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This algorithm adapts a reward function to the preferences of the human being
    and at the same time forms a policy to optimize the current expected reward function.
    There are cases in which exploration of the environment is complex, so we must
    adopt a different approach. Let's see what can be done.
  prefs: []
  type: TYPE_NORMAL
- en: Hindsight experience replay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the characteristics of humans is to learn from their mistakes and adapt
    to them to avoid making the same mistake. This is the secret of RL algorithms.
    Problems in the implementation of these algorithms are encountered when scattered
    prizes are encountered. Let''s analyze the following scenario: an agent must manage
    a robot arm to open a box and place an object inside it. The reward for this task
    is simple to define; on the contrary, the learning problem is difficult to implement.
    The agent must explore a long sequence of correct actions to identify an environment
    configuration that returns the scattered reward: in this case, the position of
    the object inside the box. Finding this reward signal is a complicated exploration
    problem that can hardly be addressed through random exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: To solve this type of problem, a new technology called Hindsight Experience
    Replay can be adopted. This technique adopts efficient learning based on scarce
    and binary prizes by omitting the complicated reward techniques that are used
    in other methods. It can be combined with an arbitrary off-policy RL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is suitable for handling objects with a robotic arm and for pushing,
    sliding, and pick-and-place tasks. Only binary prizes are used, which indicate
    whether the activity has been completed. Recent studies show that Hindsight Experience
    Replay is crucial in making training possible in these difficult environments.
    Other studies show that policies that have been trained on a physical simulation
    can be implemented on a physical robot and successfully complete the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we summarized the essential elements of algorithms based on
    RL. Then, we explored some practical examples of the application of RL technologies.
    Finally, we discussed some future challenges in using the RL algorithm. We explored
    IRL, DDPG, RL from human preferences, and Hindsight Experience Replay.
  prefs: []
  type: TYPE_NORMAL
- en: This is the last chapter in this book, and at this point we have explored different
    scenarios with the use of algorithms based on RL. First of all, I must congratulate
    you on reaching the end of this book; you have completed a difficult challenge.
    I also recommend that you immediately apply the skills you've acquired to real-world
    problems.
  prefs: []
  type: TYPE_NORMAL
