<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Machine Learning for IoT</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The term </span><strong><span class="koboSpan" id="kobo.3.1">machine learning</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">ML</span></strong><span class="koboSpan" id="kobo.6.1">) refers to computer programs that can automatically detect meaningful patterns in data and improve with experience. </span><span class="koboSpan" id="kobo.6.2">Though it isn't a new field, it's presently at the peak of its hype cycle. </span><span class="koboSpan" id="kobo.6.3">This chapter introduces the reader to standard ML algorithms and their applications in the field of IoT.</span></p>
<p><span class="koboSpan" id="kobo.7.1">After reading this chapter, you will know about the following:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.8.1">What ML is and the role it plays in the IoT pipeline</span></li>
<li><span class="koboSpan" id="kobo.9.1">Supervised and unsupervised learning paradigms</span></li>
<li><span class="koboSpan" id="kobo.10.1">Regression and how to perform linear regression using TensorFlow and Keras</span></li>
<li><span class="koboSpan" id="kobo.11.1">Popular ML classifiers and implementing them in TensorFlow and Keras</span></li>
<li><span class="koboSpan" id="kobo.12.1">Decision trees, random forests, and techniques to perform boosting and how to write code for them</span></li>
<li><span class="koboSpan" id="kobo.13.1">Tips and tricks to improve the system performance and model limitations</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">ML and IoT</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">ML, a subset of artificial intelligence, aims to build computer programs with an ability to automatically learn and improve from experience without being explicitly programmed. </span><span class="koboSpan" id="kobo.2.2">In this age of big data, with data being generated at break-neck speed, it isn't humanly possible to go through all of the data and understand it manually. </span><span class="koboSpan" id="kobo.2.3">According to an estimate by Cisco, a leading company in the field of IT and networking, IoT will generate 400 zettabytes of data a year by 2018. </span><span class="koboSpan" id="kobo.2.4">This suggests that we need to look into automatic means of understanding this enormous data, and this is where ML comes in.</span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.3.1">The complete Cisco report, released on February 1, 2018, can be accessed at </span><a href="https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html"><span class="koboSpan" id="kobo.4.1">https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html</span></a><span class="koboSpan" id="kobo.5.1">. </span><span class="koboSpan" id="kobo.5.2">It forecasts data traffic and cloud service trends in light of the amalgamation of IoT, robotics, AI, and telecommunication. </span></div>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.6.1">Every year, Gartner, a research and advisory firm, releases a graphical representation providing a visual and conceptual presentation of the maturity of emerging technologies through five phases. </span></p>
<p><span class="koboSpan" id="kobo.7.1">You can find the image of </span><em><span class="koboSpan" id="kobo.8.1">Gartner</span><span><span class="koboSpan" id="kobo.9.1"> </span></span><span class="koboSpan" id="kobo.10.1">Hype Cycle for Emerging Technologies</span></em><span><span class="koboSpan" id="kobo.11.1"> in the year </span></span><span class="koboSpan" id="kobo.12.1">2018 at </span><a href="https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/"><span class="koboSpan" id="kobo.13.1">https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/</span></a><span class="koboSpan" id="kobo.14.1">.</span></p>
<p><span><span class="koboSpan" id="kobo.15.1">W</span></span><span><span class="koboSpan" id="kobo.16.1">e can see that both IoT platforms and ML are at the</span></span> <span><span class="koboSpan" id="kobo.17.1">P</span></span><span><span class="koboSpan" id="kobo.18.1">eak of Inflated Expectations</span></span><span><span class="koboSpan" id="kobo.19.1">. </span><span class="koboSpan" id="kobo.19.2">What does it mean? </span></span><span class="koboSpan" id="kobo.20.1">The Peak of Inflated Expectations is the stage in the lifetime of technology when there's over enthusiasm about the technology. </span><span class="koboSpan" id="kobo.20.2">A large number of vendors and startups invest in the technology present at the peak crest. </span><span class="koboSpan" id="kobo.20.3">A growing number of business establishments explore how the new technology may fit within their business strategies. </span><span class="koboSpan" id="kobo.20.4">In short, it's the time to jump in to the technology. </span><span class="koboSpan" id="kobo.20.5">You can hear </span><span><span class="koboSpan" id="kobo.21.1">investors joking at venture fund events that </span><em><span class="koboSpan" id="kobo.22.1">if you just include machine learning in your pitch, you can add a zero on to the end of your valuation</span></em><span class="koboSpan" id="kobo.23.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.24.1">So, fasten your seat belts and let's dive deeper into ML technology.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Learning paradigms</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">ML algorithms can be classified based on the method they use as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.3.1">Probabilistic versus non-probabilistic</span></li>
<li><span class="koboSpan" id="kobo.4.1">Modeling </span><span><span class="koboSpan" id="kobo.5.1">versus </span></span><span class="koboSpan" id="kobo.6.1">optimization</span></li>
<li><span class="koboSpan" id="kobo.7.1">Supervised </span><span><span class="koboSpan" id="kobo.8.1">versus u</span></span><span class="koboSpan" id="kobo.9.1">nsupervised</span></li>
</ul>
<p><span class="koboSpan" id="kobo.10.1">In this book, we classify our ML algorithms as supervised </span><span><span class="koboSpan" id="kobo.11.1">versus u</span></span><span class="koboSpan" id="kobo.12.1">nsupervised. </span><span class="koboSpan" id="kobo.12.2">The distinction between these two depends on how the model learns and the type of data that's provided to the model to learn:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.13.1">Supervised learning</span></strong><span class="koboSpan" id="kobo.14.1">: Let's say I give you a series and ask you to predict the next element:</span></li>
</ul>
<div class="CDPAlignCenter CDPAlign"><em><span class="koboSpan" id="kobo.15.1">(1, 4, 9, 16, 25</span></em><span class="koboSpan" id="kobo.16.1">,...)</span></div>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.17.1">You guessed right: the next number will be 36, followed by 49 and so on. </span><span class="koboSpan" id="kobo.17.2">This is supervised learning, also called </span><strong><span class="koboSpan" id="kobo.18.1">learning by example</span></strong><span class="koboSpan" id="kobo.19.1">; you weren't told that the series represents the square of positive integers—you were able to guess it from the five examples provided.</span></p>
<p class="mce-root"/>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.20.1">In a similar manner, in supervised learning, the machine learns from example. </span><span class="koboSpan" id="kobo.20.2">It's provided with a training data consisting of a set of pairs (</span><em><span class="koboSpan" id="kobo.21.1">X</span></em><span class="koboSpan" id="kobo.22.1">, </span><em><span class="koboSpan" id="kobo.23.1">Y</span></em><span class="koboSpan" id="kobo.24.1">) where </span><em><span class="koboSpan" id="kobo.25.1">X</span></em><span class="koboSpan" id="kobo.26.1"> is the input (it can be a single number or an input value with a large number of features) and </span><em><span class="koboSpan" id="kobo.27.1">Y</span></em><span class="koboSpan" id="kobo.28.1"> is the expected output for the given input. </span><span class="koboSpan" id="kobo.28.2">Once trained on the example data, the model should be able to reach an accurate conclusion when presented with a new data. </span></p>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.29.1">The supervised learning is used to predict, given set of inputs, either a real-valued output (regression) or a discrete label (classification). </span><span class="koboSpan" id="kobo.29.2">We'll explore both regression and classification algorithms in the coming sections.</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.30.1">Unsupervised learning</span></strong><span class="koboSpan" id="kobo.31.1">: Let's say you're given with eight circular blocks of different radii and colors, and you are asked to arrange or group them in an order. </span><span class="koboSpan" id="kobo.31.2">What will you do?</span></li>
</ul>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.32.1">Some may arrange them in increasing or decreasing order of radii, some may group them according to color. </span><span class="koboSpan" id="kobo.32.2">There are so many ways, and for each one of us, it will be dependent on what internal representation of the data we had while grouping. </span><span class="koboSpan" id="kobo.32.3">This is unsupervised learning, and a majority of human learning lies in this category. </span></p>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.33.1">In unsupervised learning, the model is just given the data (</span><em><span class="koboSpan" id="kobo.34.1">X</span></em><span class="koboSpan" id="kobo.35.1">) but isn't told anything about it; the model learns by itself the underlying patterns and relationships in the data. </span><span class="koboSpan" id="kobo.35.2">Unsupervised learning is normally used for clustering and dimensionality reduction.</span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.36.1">Though we use TensorFlow for most of the algorithms in this book, in this chapter, due to the efficiently built scikit library for ML algorithms, we'll use the functions and methods provided by scikit wherever they provide more flexibility and features. </span><span class="koboSpan" id="kobo.36.2">The aim is to provide you, the reader, with to use AI/ML techniques on the data generated by IoT, not to reinvent the wheel.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Prediction using linear regression</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Aaron, a friend of mine, is a little sloppy with money and is never able to estimate how much his monthly credit card bill will be. </span><span class="koboSpan" id="kobo.2.2">Can we do something to help him? </span><span class="koboSpan" id="kobo.2.3">Well, yes, linear regression can help us to predict a monthly credit card bill if we have sufficient data. </span><span class="koboSpan" id="kobo.2.4">Thanks to the digital economy, all of his monetary transactions for the last five years are available online. </span><span class="koboSpan" id="kobo.2.5">We extracted his monthly expenditure on groceries, stationery, and travel and his monthly income. </span><span class="koboSpan" id="kobo.2.6">Linear regression helped not only in predicting his monthly credit card bill, it also gave an insight into which factor was most responsible for his spending.</span></p>
<p><span class="koboSpan" id="kobo.3.1">This was just one example; linear regression can be used in many similar tasks. </span><span class="koboSpan" id="kobo.3.2">In this section, we'll learn how we can perform linear regression on our data.</span></p>
<p><span class="koboSpan" id="kobo.4.1">Linear regression is a supervised learning task. </span><span class="koboSpan" id="kobo.4.2">It's one of the most basic, simple, and extensively used ML techniques for prediction. </span><span class="koboSpan" id="kobo.4.3">The goal of regression is to find a function </span><em><span class="koboSpan" id="kobo.5.1">F</span></em><span class="koboSpan" id="kobo.6.1">(</span><em><span class="koboSpan" id="kobo.7.1">x, W</span></em><span class="koboSpan" id="kobo.8.1">), for a given input-output pair (</span><em><span class="koboSpan" id="kobo.9.1">x</span></em><span class="koboSpan" id="kobo.10.1">, </span><em><span class="koboSpan" id="kobo.11.1">y</span></em><span class="koboSpan" id="kobo.12.1">), so that </span><em><span class="koboSpan" id="kobo.13.1">y</span></em><span class="koboSpan" id="kobo.14.1"> = </span><em><span class="koboSpan" id="kobo.15.1">F</span></em><span class="koboSpan" id="kobo.16.1">(</span><em><span class="koboSpan" id="kobo.17.1">x, W</span></em><span class="koboSpan" id="kobo.18.1">). </span><span class="koboSpan" id="kobo.18.2">In the (</span><em><span class="koboSpan" id="kobo.19.1">x</span></em><span class="koboSpan" id="kobo.20.1">, </span><em><span class="koboSpan" id="kobo.21.1">y</span></em><span class="koboSpan" id="kobo.22.1">) pair, </span><em><span class="koboSpan" id="kobo.23.1">x</span></em><span class="koboSpan" id="kobo.24.1"> is the independent variable and </span><em><span class="koboSpan" id="kobo.25.1">y</span></em><span class="koboSpan" id="kobo.26.1"> the dependent variable, and both of them are continuous variables. </span><span class="koboSpan" id="kobo.26.2">It helps us to find the relationship between the dependent variable </span><em><span class="koboSpan" id="kobo.27.1">y</span></em><span class="koboSpan" id="kobo.28.1"> and the independent variable(s) </span><em><span class="koboSpan" id="kobo.29.1">x</span></em><span class="koboSpan" id="kobo.30.1">. </span></p>
<p><span class="koboSpan" id="kobo.31.1">The input </span><em><span class="koboSpan" id="kobo.32.1">x</span></em><span class="koboSpan" id="kobo.33.1"> can be a single input variable or many input variables. </span><span class="koboSpan" id="kobo.33.2">When </span><em><span class="koboSpan" id="kobo.34.1">F</span></em><span class="koboSpan" id="kobo.35.1">(</span><em><span class="koboSpan" id="kobo.36.1">x, W</span></em><span class="koboSpan" id="kobo.37.1">) maps a single input variable </span><em><span class="koboSpan" id="kobo.38.1">x</span></em><span class="koboSpan" id="kobo.39.1">, it's called </span><strong><span class="koboSpan" id="kobo.40.1">simple linear regression</span></strong><span class="koboSpan" id="kobo.41.1">; for multiple input variables, it's called </span><strong><span class="koboSpan" id="kobo.42.1">multiple linear regression</span></strong><span class="koboSpan" id="kobo.43.1">. </span></p>
<p class="mce-root CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.44.1">The function </span><em><span class="koboSpan" id="kobo.45.1">F</span></em><span class="koboSpan" id="kobo.46.1">(</span><em><span class="koboSpan" id="kobo.47.1">x, W</span></em><span class="koboSpan" id="kobo.48.1">) is approximated using the following expression:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.49.1"><img class="fm-editor-equation" src="assets/b866dc12-794c-4ae7-804a-5e1d8ef703e5.png" style="width:18.00em;height:4.17em;"/></span></div>
<p><span class="koboSpan" id="kobo.50.1">In this expression, </span><em><span class="koboSpan" id="kobo.51.1">d</span></em><span class="koboSpan" id="kobo.52.1"> is the dimensions of </span><em><span class="koboSpan" id="kobo.53.1">x</span></em><span class="koboSpan" id="kobo.54.1"> (number of independent variables), and </span><em><span class="koboSpan" id="kobo.55.1">W</span></em><span class="koboSpan" id="kobo.56.1"> is the weight associated with each component of </span><em><span class="koboSpan" id="kobo.57.1">x</span></em><span class="koboSpan" id="kobo.58.1">. </span><span class="koboSpan" id="kobo.58.2">To find the function </span><em><span class="koboSpan" id="kobo.59.1">F</span></em><span class="koboSpan" id="kobo.60.1">(</span><em><span class="koboSpan" id="kobo.61.1">x, W</span></em><span class="koboSpan" id="kobo.62.1">), we need to determine the weights. </span><span class="koboSpan" id="kobo.62.2">The natural choice is to find the weights that reduce the squared error, hence our objective function is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.63.1"><img class="fm-editor-equation" src="assets/c2da2354-6889-4776-96d6-062c18f9b2cf.png" style="width:13.42em;height:3.92em;"/></span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.64.1">In the preceding function, </span><em><span class="koboSpan" id="kobo.65.1">N</span></em><span class="koboSpan" id="kobo.66.1"> is the total number of the input-output pair presented. </span><span class="koboSpan" id="kobo.66.2">To find the weights, we differentiate the objective function with respect to weight and equate it to </span><em><span class="koboSpan" id="kobo.67.1">0</span></em><span class="koboSpan" id="kobo.68.1">. </span><span class="koboSpan" id="kobo.68.2">In matrix notation, we can write the solution for the column vector </span><em><span class="koboSpan" id="kobo.69.1">W</span></em><span class="koboSpan" id="kobo.70.1"> = (</span><em><span class="koboSpan" id="kobo.71.1">W</span></em><sub><span class="koboSpan" id="kobo.72.1">0</span></sub><span class="koboSpan" id="kobo.73.1">, </span><em><span class="koboSpan" id="kobo.74.1">W</span></em><sub><span class="koboSpan" id="kobo.75.1">1</span></sub><span class="koboSpan" id="kobo.76.1">, </span><em><span class="koboSpan" id="kobo.77.1">W</span></em><sub><span class="koboSpan" id="kobo.78.1">2</span></sub><span class="koboSpan" id="kobo.79.1">, ..., </span><em><span class="koboSpan" id="kobo.80.1">W</span></em><sub><span class="koboSpan" id="kobo.81.1">d</span></sub><span><span class="koboSpan" id="kobo.82.1">)</span></span><sup><span class="koboSpan" id="kobo.83.1">T</span></sup><span class="koboSpan" id="kobo.84.1"> as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.85.1"><img class="fm-editor-equation" src="assets/9d0fdad2-b84a-4896-868c-3718dc47f9af.png" style="width:5.17em;height:1.25em;"/></span></div>
<p><span class="koboSpan" id="kobo.86.1">On differentiating and simplifying, we get the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.87.1"><img class="fm-editor-equation" src="assets/d758faaa-14ab-4347-8655-6c3ed714d269.png" style="width:10.25em;height:1.58em;"/></span></div>
<p><em><span class="koboSpan" id="kobo.88.1">X</span></em><span class="koboSpan" id="kobo.89.1"> is the input vector of size [</span><em><span class="koboSpan" id="kobo.90.1">N</span></em><span class="koboSpan" id="kobo.91.1">, </span><em><span class="koboSpan" id="kobo.92.1">d</span></em><span class="koboSpan" id="kobo.93.1">] and </span><em><span class="koboSpan" id="kobo.94.1">Y</span></em><span class="koboSpan" id="kobo.95.1"> the output vector of size [</span><em><span class="koboSpan" id="kobo.96.1">N</span></em><span class="koboSpan" id="kobo.97.1">, 1]. </span><span class="koboSpan" id="kobo.97.2">The weights can be found if (</span><em><span class="koboSpan" id="kobo.98.1">X</span><sup><span class="koboSpan" id="kobo.99.1">T</span></sup><span class="koboSpan" id="kobo.100.1">X</span></em><span class="koboSpan" id="kobo.101.1">)</span><sup><span class="koboSpan" id="kobo.102.1">-1</span></sup><span class="koboSpan" id="kobo.103.1"> exists, that's if all of the rows and columns of </span><em><span class="koboSpan" id="kobo.104.1">X</span></em><span class="koboSpan" id="kobo.105.1"> are linearly independent. </span><span class="koboSpan" id="kobo.105.2">To ensure this, the number of input-output samples (</span><em><span class="koboSpan" id="kobo.106.1">N</span></em><span class="koboSpan" id="kobo.107.1">) should be much greater than the number of input features (</span><em><span class="koboSpan" id="kobo.108.1">d</span></em><span class="koboSpan" id="kobo.109.1">). </span></p>
<div class="packt_tip"><span class="koboSpan" id="kobo.110.1">An important thing to remember is that </span><em><span class="koboSpan" id="kobo.111.1">Y</span></em><span class="koboSpan" id="kobo.112.1">, the dependent variable, isn't linear with respect to the dependent variable X; instead, it's linear with respect to the model parameter </span><em><span class="koboSpan" id="kobo.113.1">W</span></em><span class="koboSpan" id="kobo.114.1">, the weights. </span><span class="koboSpan" id="kobo.114.2">And so we can model relationships such as exponential or even sinusoidal (</span><span><span class="koboSpan" id="kobo.115.1">between </span></span><em><span class="koboSpan" id="kobo.116.1">Y</span></em><span class="koboSpan" id="kobo.117.1"> and </span><em><span class="koboSpan" id="kobo.118.1">X</span></em><span class="koboSpan" id="kobo.119.1">) using linear regression. </span><span class="koboSpan" id="kobo.119.2">In this case, we generalize the problem to finding weights </span><em><span class="koboSpan" id="kobo.120.1">W</span></em><span class="koboSpan" id="kobo.121.1">, so that </span><em><span class="koboSpan" id="kobo.122.1">y</span></em><span class="koboSpan" id="kobo.123.1"> = </span><em><span class="koboSpan" id="kobo.124.1">F</span></em><span class="koboSpan" id="kobo.125.1">(</span><em><span class="koboSpan" id="kobo.126.1">g</span></em><span class="koboSpan" id="kobo.127.1">(</span><em><span class="koboSpan" id="kobo.128.1">x</span></em><span class="koboSpan" id="kobo.129.1">), </span><em><span class="koboSpan" id="kobo.130.1">W</span></em><span class="koboSpan" id="kobo.131.1">), where </span><em><span class="koboSpan" id="kobo.132.1">g</span></em><span class="koboSpan" id="kobo.133.1">(</span><em><span class="koboSpan" id="kobo.134.1">x</span></em><span class="koboSpan" id="kobo.135.1">) is a non-linear function of </span><em><span class="koboSpan" id="kobo.136.1">X.</span></em></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Electrical power output prediction using regression</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Now that you've understood the basics of linear regression, let's use it to predict the electrical power output of a combined cycle power plant. </span><span class="koboSpan" id="kobo.2.2">We described this dataset in </span><a href="fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 1</span></a><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Principles and Foundations of AI and IoT</span></em><span class="koboSpan" id="kobo.6.1">; here, we'll use TensorFlow and its automatic gradient to find the solution. </span><span class="koboSpan" id="kobo.6.2">The dataset can be downloaded from the UCI ML archive (</span><a href="http://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant"><span class="koboSpan" id="kobo.7.1">http://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant</span></a><span class="koboSpan" id="kobo.8.1">). </span><span class="koboSpan" id="kobo.8.2">The complete code is available on GitHub (</span><a href="https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-for-IoT"><span class="koboSpan" id="kobo.9.1">https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-for-IoT</span></a><span class="koboSpan" id="kobo.10.1">) under the filename </span><span><kbd><span class="koboSpan" id="kobo.11.1">ElectricalPowerOutputPredictionUsingRegression.ipynb</span></kbd><span class="koboSpan" id="kobo.12.1">.</span></span></p>
<p class="mce-root"/>
<p><span><span class="koboSpan" id="kobo.13.1">Let's understand the execution of code in the following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.14.1">We import </span><kbd><span class="koboSpan" id="kobo.15.1">tensorflow</span></kbd><span class="koboSpan" id="kobo.16.1">, </span><kbd><span class="koboSpan" id="kobo.17.1">numpy</span></kbd><span class="koboSpan" id="kobo.18.1">, </span><kbd><span class="koboSpan" id="kobo.19.1">pandas</span></kbd><span class="koboSpan" id="kobo.20.1">, </span><kbd><span class="koboSpan" id="kobo.21.1">matplotlib</span></kbd><span class="koboSpan" id="kobo.22.1">, and some useful functions of scikit-learn:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.23.1"># Import the modules</span><br/><span class="koboSpan" id="kobo.24.1">import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.25.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.26.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.27.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.28.1">from sklearn.preprocessing import MinMaxScaler</span><br/><span class="koboSpan" id="kobo.29.1">from sklearn.metrics import mean_squared_error, r2_score</span><br/><span class="koboSpan" id="kobo.30.1">from sklearn.model_selection import train_test_split</span><br/><span class="koboSpan" id="kobo.31.1">%matplotlib inline # The data file is loaded and analyzed</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.32.1">The data file is loaded and analyzed:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.33.1">filename = 'Folds5x2_pp.xlsx' # download the data file from UCI ML repository</span><br/><span class="koboSpan" id="kobo.34.1">df = pd.read_excel(filename, sheet_name='Sheet1')</span><br/><span class="koboSpan" id="kobo.35.1">df.describe()</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.36.1">Since the data isn't normalized, before using it, we need to normalize it using the </span><kbd><span class="koboSpan" id="kobo.37.1">MinMaxScaler</span></kbd><span class="koboSpan" id="kobo.38.1"> of </span><kbd><span class="koboSpan" id="kobo.39.1">sklearn</span></kbd><span class="koboSpan" id="kobo.40.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.41.1">X, Y = df[['AT', 'V','AP','RH']], df['PE']</span><br/><span class="koboSpan" id="kobo.42.1">scaler = MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.43.1">X_new = scaler.fit_transform(X)</span><br/><span class="koboSpan" id="kobo.44.1">target_scaler = MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.45.1">Y_new = target_scaler.fit_transform(Y.values.reshape(-1,1))</span><br/><span class="koboSpan" id="kobo.46.1">X_train, X_test, Y_train, y_test = \</span><br/><span class="koboSpan" id="kobo.47.1"> train_test_split(X_new, Y_new, test_size=0.4, random_state=333)</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.48.1">Now, we define a class, </span><kbd><span class="koboSpan" id="kobo.49.1">LinearRegressor</span></kbd><span class="koboSpan" id="kobo.50.1">; this is the class where all of the real work happens. </span><span class="koboSpan" id="kobo.50.2">The class initialization defines the computational graph and initializes all of the </span><kbd><span class="koboSpan" id="kobo.51.1">Variables</span></kbd><span class="koboSpan" id="kobo.52.1"> (weights and bias). </span><span class="koboSpan" id="kobo.52.2">The class has the </span><kbd><span class="koboSpan" id="kobo.53.1">function</span></kbd><span class="koboSpan" id="kobo.54.1"> method, which models the function </span><em><span class="koboSpan" id="kobo.55.1">y</span></em><span class="koboSpan" id="kobo.56.1"> = </span><em><span class="koboSpan" id="kobo.57.1">F</span></em><span class="koboSpan" id="kobo.58.1">(</span><em><span class="koboSpan" id="kobo.59.1">X</span></em><span class="koboSpan" id="kobo.60.1">,</span><em><span class="koboSpan" id="kobo.61.1">W</span></em><span class="koboSpan" id="kobo.62.1">); the </span><kbd><span class="koboSpan" id="kobo.63.1">fit</span></kbd><span class="koboSpan" id="kobo.64.1"> method performs the auto gradient and updates the weights and bias, the </span><kbd><span class="koboSpan" id="kobo.65.1">predict</span></kbd><span class="koboSpan" id="kobo.66.1"> method is used to get the output </span><em><span class="koboSpan" id="kobo.67.1">y</span></em><span class="koboSpan" id="kobo.68.1"> for a given input </span><em><span class="koboSpan" id="kobo.69.1">X</span></em><span class="koboSpan" id="kobo.70.1">, and the </span><kbd><span class="koboSpan" id="kobo.71.1">get_weights</span></kbd><span class="koboSpan" id="kobo.72.1"> method returns the learned weights and bias:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.73.1">class LinearRegressor:</span><br/><span class="koboSpan" id="kobo.74.1"> def __init__(self,d, lr=0.001 ):</span><br/><span class="koboSpan" id="kobo.75.1"> # Placeholders for input-output training data</span><br/><span class="koboSpan" id="kobo.76.1"> self.X = tf.placeholder(tf.float32,\</span><br/><span class="koboSpan" id="kobo.77.1"> shape=[None,d], name='input')</span><br/><span class="koboSpan" id="kobo.78.1"> self.Y = tf.placeholder(tf.float32,\</span><br/><span class="koboSpan" id="kobo.79.1"> name='output')</span><br/><span class="koboSpan" id="kobo.80.1"> # Variables for weight and bias</span><br/><span class="koboSpan" id="kobo.81.1"> self.b = tf.Variable(0.0, dtype=tf.float32)</span><br/><span class="koboSpan" id="kobo.82.1"> self.W = tf.Variable(tf.random_normal([d,1]),\</span><br/><span class="koboSpan" id="kobo.83.1"> dtype=tf.float32)</span><br/><br/><span class="koboSpan" id="kobo.84.1"> # The Linear Regression Model</span><br/><span class="koboSpan" id="kobo.85.1"> self.F = self.function(self.X)</span><br/><br/><span class="koboSpan" id="kobo.86.1"> # Loss function</span><br/><span class="koboSpan" id="kobo.87.1"> self.loss = tf.reduce_mean(tf.square(self.Y \</span><br/><span class="koboSpan" id="kobo.88.1"> - self.F, name='LSE'))</span><br/><span class="koboSpan" id="kobo.89.1"> # Gradient Descent with learning </span><br/><span class="koboSpan" id="kobo.90.1"> # rate of 0.05 to minimize loss</span><br/><span class="koboSpan" id="kobo.91.1"> optimizer = tf.train.GradientDescentOptimizer(lr)</span><br/><span class="koboSpan" id="kobo.92.1"> self.optimize = optimizer.minimize(self.loss)</span><br/> <br/><span class="koboSpan" id="kobo.93.1"> # Initializing Variables</span><br/><span class="koboSpan" id="kobo.94.1"> init_op = tf.global_variables_initializer()</span><br/><span class="koboSpan" id="kobo.95.1"> self.sess = tf.Session()</span><br/><span class="koboSpan" id="kobo.96.1"> self.sess.run(init_op)</span><br/><br/><span class="koboSpan" id="kobo.97.1"> def function(self, X):</span><br/><span class="koboSpan" id="kobo.98.1"> return tf.matmul(X, self.W) + self.b</span><br/><br/><span class="koboSpan" id="kobo.99.1"> def fit(self, X, Y,epochs=500):</span><br/><span class="koboSpan" id="kobo.100.1"> total = []</span><br/><span class="koboSpan" id="kobo.101.1"> for i in range(epochs):</span><br/><span class="koboSpan" id="kobo.102.1"> _, l = self.sess.run([self.optimize,self.loss],\</span><br/><span class="koboSpan" id="kobo.103.1"> feed_dict={self.X: X, self.Y: Y})</span><br/><span class="koboSpan" id="kobo.104.1"> total.append(l)</span><br/><span class="koboSpan" id="kobo.105.1"> if i%100==0:</span><br/><span class="koboSpan" id="kobo.106.1"> print('Epoch {0}/{1}: Loss {2}'.format(i,epochs,l))</span><br/><span class="koboSpan" id="kobo.107.1"> return total</span><br/> <br/><span class="koboSpan" id="kobo.108.1"> def predict(self, X):</span><br/><span class="koboSpan" id="kobo.109.1"> return self.sess.run(self.function(X), feed_dict={self.X:X})</span><br/> <br/><span class="koboSpan" id="kobo.110.1"> def get_weights(self):</span><br/><span class="koboSpan" id="kobo.111.1"> return self.sess.run([self.W, self.b])</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.112.1"> We use the previous class to create our linear regression model and train it:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.113.1">N, d = X_train.shape</span><br/><span class="koboSpan" id="kobo.114.1">model = LinearRegressor(d)</span><br/><span class="koboSpan" id="kobo.115.1">loss = model.fit(X_train, Y_train, 20000) #Epochs = 20000</span></pre>
<p><span class="koboSpan" id="kobo.116.1">Let's see the performance of our trained linear regressor. </span><span class="koboSpan" id="kobo.116.2">A plot of mean square error with </span><strong><span class="koboSpan" id="kobo.117.1">Epochs</span></strong><span class="koboSpan" id="kobo.118.1"> shows that the network tried to reach a minimum value of mean square error:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.119.1"><img class="aligncenter size-full wp-image-1081 image-border" src="assets/08d71db2-d508-4a8a-a136-ca9176a05f18.png" style="width:33.50em;height:22.17em;"/></span></p>
<p><span class="koboSpan" id="kobo.120.1">On the test dataset, we achieved an </span><em><span class="koboSpan" id="kobo.121.1">R</span><sup><span class="koboSpan" id="kobo.122.1">2</span></sup></em><span class="koboSpan" id="kobo.123.1"> value of </span><em><span class="koboSpan" id="kobo.124.1">0.768</span></em><span class="koboSpan" id="kobo.125.1"> and mean square error of </span><em><span class="koboSpan" id="kobo.126.1">0.011</span></em><span class="koboSpan" id="kobo.127.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Logistic regression for classification</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the previous section, we learned how to predict. </span><span class="koboSpan" id="kobo.2.2">There's another common task in ML: the task of classification. </span><span class="koboSpan" id="kobo.2.3">Separating dogs from cats and spam from not spam, or even identifying the different objects in a room or scene—all of these are classification tasks. </span></p>
<p><span class="koboSpan" id="kobo.3.1">Logistic regression is an old classification technique. </span><span class="koboSpan" id="kobo.3.2">It provides the probability of an event taking place, given an input value. </span><span class="koboSpan" id="kobo.3.3">The events are represented as categorical dependent variables, and the probability of a particular dependent variable being </span><em><span class="koboSpan" id="kobo.4.1">1</span></em><span class="koboSpan" id="kobo.5.1"> is given using the logit function:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.6.1"><img class="fm-editor-equation" src="assets/13aab929-f33c-4e8b-8bdc-33726bf818d8.png" style="width:27.92em;height:3.17em;"/></span></div>
<p class="mce-root"/>
<p class="mce-root"><span class="koboSpan" id="kobo.7.1">Before going into the details of how we can use logistic regression for classification, let's examine the logit function (also called the </span><strong><span class="koboSpan" id="kobo.8.1">sigmoid</span></strong><span class="koboSpan" id="kobo.9.1"> function because of its S-shaped curve). </span><span class="koboSpan" id="kobo.9.2">The following diagram shows the logit function and its derivative varies with respect to the input </span><em><span class="koboSpan" id="kobo.10.1">X,</span></em><span class="koboSpan" id="kobo.11.1"> the Sigmoidal function (blue) and its derivative (orange):</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.12.1"><img class="aligncenter size-full wp-image-1082 image-border" src="assets/ae962533-66ec-4952-997b-524596c97b5f.png" style="width:34.08em;height:23.17em;"/></span></p>
<p><span class="koboSpan" id="kobo.13.1">A few important things to note from this diagram are the following:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.14.1">The value of sigmoid (and hence </span><em><span class="koboSpan" id="kobo.15.1">Y</span><sub><span class="koboSpan" id="kobo.16.1">pred</span></sub></em><span class="koboSpan" id="kobo.17.1">) lies between (</span><em><span class="koboSpan" id="kobo.18.1">0</span></em><span class="koboSpan" id="kobo.19.1">, </span><em><span class="koboSpan" id="kobo.20.1">1</span></em><span class="koboSpan" id="kobo.21.1">)</span></li>
<li><span class="koboSpan" id="kobo.22.1">The derivative of the sigmoid is highest when </span><em><span class="koboSpan" id="kobo.23.1">W</span><sup><span class="koboSpan" id="kobo.24.1">T</span></sup><span class="koboSpan" id="kobo.25.1">X + b = 0.0</span></em><span class="koboSpan" id="kobo.26.1"> and the highest value of the derivative is just </span><em><span class="koboSpan" id="kobo.27.1">0.25</span></em><span class="koboSpan" id="kobo.28.1"> (the sigmoid at same place has a value </span><em><span class="koboSpan" id="kobo.29.1">0.5</span></em><span class="koboSpan" id="kobo.30.1">)</span></li>
<li><span class="koboSpan" id="kobo.31.1">The slope by which the sigmoid varies depends on the weights, and the position where we'll have the peak of derivative depends on the bias</span></li>
</ul>
<p><span class="koboSpan" id="kobo.32.1">I would suggest you play around with the </span><kbd><span class="koboSpan" id="kobo.33.1">Sigmoid_function.ipynb</span></kbd><span class="koboSpan" id="kobo.34.1"> program available at this book's GitHub repository, to get a feel of how the sigmoid function changes as the weight and bias changes. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Cross-entropy loss function</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Logistic regression aims to find weights </span><em><span class="koboSpan" id="kobo.3.1">W</span></em><span class="koboSpan" id="kobo.4.1"> and bias </span><em><span class="koboSpan" id="kobo.5.1">b</span></em><span class="koboSpan" id="kobo.6.1">, so that each input vector, </span><em><span class="koboSpan" id="kobo.7.1">X</span><sub><span class="koboSpan" id="kobo.8.1">i</span></sub></em><span class="koboSpan" id="kobo.9.1">, in the input feature space is classified correctly to its class, </span><em><span class="koboSpan" id="kobo.10.1">y</span><sub><span class="koboSpan" id="kobo.11.1">i</span></sub></em><span class="koboSpan" id="kobo.12.1">. </span><span class="koboSpan" id="kobo.12.2">In other words, </span><em><span class="koboSpan" id="kobo.13.1">y</span><sub><span class="koboSpan" id="kobo.14.1">i</span></sub></em><span class="koboSpan" id="kobo.15.1"> and </span><em><span class="koboSpan" id="kobo.16.1"><img class="fm-editor-equation" src="assets/e1e5c9a3-577d-4910-997f-dbdb5cd0cd59.png" style="width:3.58em;height:1.67em;"/></span></em><span class="koboSpan" id="kobo.17.1"> should have a similar distribution for the given </span><em><span class="koboSpan" id="kobo.18.1">x</span><sub><span class="koboSpan" id="kobo.19.1">i</span></sub></em><span class="koboSpan" id="kobo.20.1">. </span><span class="koboSpan" id="kobo.20.2">We first consider a binary classification problem; in this case, the data point </span><em><span class="koboSpan" id="kobo.21.1">y</span><sub><span class="koboSpan" id="kobo.22.1">i</span></sub></em><span class="koboSpan" id="kobo.23.1"> can have value </span><em><span class="koboSpan" id="kobo.24.1">1</span></em><span class="koboSpan" id="kobo.25.1"> or </span><em><span class="koboSpan" id="kobo.26.1">0</span></em><span class="koboSpan" id="kobo.27.1">. </span><span class="koboSpan" id="kobo.27.2">Since logistic regression is a supervised learning algorithm, we give as input the training data pair (</span><em><span class="koboSpan" id="kobo.28.1">X</span><sub><span class="koboSpan" id="kobo.29.1">i</span></sub></em><span class="koboSpan" id="kobo.30.1">, </span><em><span class="koboSpan" id="kobo.31.1">Y</span><sub><span class="koboSpan" id="kobo.32.1">i</span></sub></em><span class="koboSpan" id="kobo.33.1">) and let </span><em><span class="koboSpan" id="kobo.34.1"><img class="fm-editor-equation" src="assets/62f0973e-bd28-4c44-9a70-93a322422a4a.png" style="width:2.83em;height:1.33em;"/></span></em><span class="koboSpan" id="kobo.35.1"> be the probability that </span><em><span class="koboSpan" id="kobo.36.1">P</span></em><span class="koboSpan" id="kobo.37.1">(</span><em><span class="koboSpan" id="kobo.38.1">y</span></em><span class="koboSpan" id="kobo.39.1">=</span><em><span class="koboSpan" id="kobo.40.1">1</span></em><span class="koboSpan" id="kobo.41.1">|</span><em><span class="koboSpan" id="kobo.42.1">X</span></em><span class="koboSpan" id="kobo.43.1">=</span><em><span class="koboSpan" id="kobo.44.1">X</span><sub><span class="koboSpan" id="kobo.45.1">i</span></sub></em><span class="koboSpan" id="kobo.46.1">); then, for </span><em><span class="koboSpan" id="kobo.47.1">p</span></em><span class="koboSpan" id="kobo.48.1"> training data points, the total average loss is defined as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.49.1"><img class="fm-editor-equation" src="assets/f45cc770-d660-465d-bf48-ed5d7d8eef35.png" style="width:28.67em;height:3.92em;"/></span></div>
<p><span class="koboSpan" id="kobo.50.1">Hence, for every data pair, for </span><em><span class="koboSpan" id="kobo.51.1">Y</span><sub><span class="koboSpan" id="kobo.52.1">i</span></sub></em><span class="koboSpan" id="kobo.53.1"> = </span><em><span class="koboSpan" id="kobo.54.1">1</span></em><span class="koboSpan" id="kobo.55.1">, the first term will contribute to the loss term, with the contribution changing from infinity to </span><em><span class="koboSpan" id="kobo.56.1">0</span></em><span class="koboSpan" id="kobo.57.1"> as </span><span class="koboSpan" id="kobo.58.1"><img class="fm-editor-equation" src="assets/05c8ae04-6675-4cc7-b35a-5caa58800101.png" style="width:2.83em;height:1.33em;"/></span><span class="koboSpan" id="kobo.59.1">varies from </span><em><span class="koboSpan" id="kobo.60.1">0</span></em><span class="koboSpan" id="kobo.61.1"> to </span><em><span class="koboSpan" id="kobo.62.1">1</span></em><span class="koboSpan" id="kobo.63.1">, respectively. </span><span class="koboSpan" id="kobo.63.2">Similarly, </span><span><span class="koboSpan" id="kobo.64.1">for </span></span><em><span class="koboSpan" id="kobo.65.1">Y</span><sub><span class="koboSpan" id="kobo.66.1">i</span></sub></em><span><span class="koboSpan" id="kobo.67.1"> = </span><em><span class="koboSpan" id="kobo.68.1">0</span></em><span class="koboSpan" id="kobo.69.1">, the second term will contribute to the loss term, with the contribution changing from infinity to zero as </span></span><span class="koboSpan" id="kobo.70.1"><img class="fm-editor-equation" src="assets/4295ffc5-6a17-4ece-b58d-ea27a6688761.png" style="width:2.83em;height:1.33em;"/></span><span><span class="koboSpan" id="kobo.71.1">varies from </span><em><span class="koboSpan" id="kobo.72.1">1</span></em><span class="koboSpan" id="kobo.73.1"> to </span><em><span class="koboSpan" id="kobo.74.1">0</span></em><span class="koboSpan" id="kobo.75.1">, respectively.</span></span></p>
<p><span class="koboSpan" id="kobo.76.1">For multiclass classification, the loss term is generalized to the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.77.1"><img class="fm-editor-equation" src="assets/0f18ab66-73bb-4fd7-ac3d-3015a548ef93.png" style="width:15.33em;height:4.17em;"/></span></div>
<p><span class="koboSpan" id="kobo.78.1">In the preceding, </span><em><span class="koboSpan" id="kobo.79.1">K</span></em><span class="koboSpan" id="kobo.80.1"> is the number of classes. </span><span class="koboSpan" id="kobo.80.2">An important thing to note is that, while for binary classification the output </span><em><span class="koboSpan" id="kobo.81.1">Y</span><sub><span class="koboSpan" id="kobo.82.1">i</span></sub></em><span class="koboSpan" id="kobo.83.1"> and </span><em><span class="koboSpan" id="kobo.84.1">Y</span><sub><span class="koboSpan" id="kobo.85.1">pred</span></sub></em><span class="koboSpan" id="kobo.86.1"> were single values, for multiclass problems, both </span><em><span class="koboSpan" id="kobo.87.1">Y</span><sub><span class="koboSpan" id="kobo.88.1">i</span></sub></em><span class="koboSpan" id="kobo.89.1"> and </span><em><span class="koboSpan" id="kobo.90.1">Y</span></em><sub><em><span class="koboSpan" id="kobo.91.1">pred</span></em></sub><span class="koboSpan" id="kobo.92.1"> are now vectors of </span><em><span class="koboSpan" id="kobo.93.1">K</span></em><span class="koboSpan" id="kobo.94.1"> dimensions, with one component for each category. </span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Classifying wine using logistic regressor</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's now use what we've learned to classify wine quality. </span><span class="koboSpan" id="kobo.2.2">I can hear you thinking: </span><em><span class="koboSpan" id="kobo.3.1">What wine quality? </span><span class="koboSpan" id="kobo.3.2">No way!</span></em><span class="koboSpan" id="kobo.4.1"> Let's see how our logistic regressor fares as compared to professional wine tasters. </span><span class="koboSpan" id="kobo.4.2">We'll be using the wine quality dataset (</span><a href="https://archive.ics.uci.edu/ml/datasets/wine+quality"><span class="koboSpan" id="kobo.5.1">https://archive.ics.uci.edu/ml/datasets/wine+quality</span></a><span class="koboSpan" id="kobo.6.1">); details about the dataset are given in </span><a href="fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml"><span class="koboSpan" id="kobo.7.1">Chapter 1</span></a><span class="koboSpan" id="kobo.8.1">, </span><em><span class="koboSpan" id="kobo.9.1">Principles and Foundation of AI and IoT</span></em><span class="koboSpan" id="kobo.10.1">. </span><span class="koboSpan" id="kobo.10.2">The full code is in the file named </span><span><kbd><span class="koboSpan" id="kobo.11.1">Wine_quality_using_logistic_regressor.ipynb</span></kbd><span class="koboSpan" id="kobo.12.1"> at the GitHub repository. </span><span class="koboSpan" id="kobo.12.2">Let's understand the code step by step:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.13.1">The first step is loading all of the modules:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.14.1"># Import the modules</span><br/><span class="koboSpan" id="kobo.15.1">import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.16.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.17.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.18.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.19.1">from sklearn.preprocessing import MinMaxScaler</span><br/><span class="koboSpan" id="kobo.20.1">from sklearn.metrics import mean_squared_error, r2_score</span><br/><span class="koboSpan" id="kobo.21.1">from sklearn.model_selection import train_test_split</span><br/><span class="koboSpan" id="kobo.22.1">%matplotlib inline</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.23.1">We read the data; in the present code, we are analyzing only the red wine, so we read data from the </span><kbd><span class="koboSpan" id="kobo.24.1">winequality-red.csv</span></kbd><span class="koboSpan" id="kobo.25.1"> file. </span><span class="koboSpan" id="kobo.25.2">The file contains the data values separated not by commas, but instead by semicolons, so we need to specify the separator argument:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.26.1">filename = 'winequality-red.csv' # Download the file from UCI ML Repo </span><br/><span class="koboSpan" id="kobo.27.1">df = pd.read_csv(filename, sep=';')</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.28.1">We separate from the data file input features and target quality. </span><span class="koboSpan" id="kobo.28.2">In the file, the target, wine quality is given on a scale from 0—10. </span><span class="koboSpan" id="kobo.28.3">Here, for simplicity, we divide it into three classes, so if the initial quality is less than five, we make it the third class (signifying bad); between five and eight, we consider it </span><kbd><span class="koboSpan" id="kobo.29.1">ok</span></kbd><span class="koboSpan" id="kobo.30.1"> (second class); and above eight, we consider it </span><kbd><span class="koboSpan" id="kobo.31.1">good</span></kbd><span class="koboSpan" id="kobo.32.1"> (the first class). </span><span class="koboSpan" id="kobo.32.2">We also normalize the input features and split the data into training and test datasets:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.33.1">X, Y = df[columns[0:-1]], df[columns[-1]]</span><br/><span class="koboSpan" id="kobo.34.1">scaler = MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.35.1">X_new = scaler.fit_transform(X)</span><br/><span class="koboSpan" id="kobo.36.1">Y.loc[(Y&lt;3)]=3</span><br/><span class="koboSpan" id="kobo.37.1">Y.loc[(Y&lt;6.5) &amp; (Y&gt;=3 )] = 2</span><br/><span class="koboSpan" id="kobo.38.1">Y.loc[(Y&gt;=6.5)] = 1</span><br/><span class="koboSpan" id="kobo.39.1">Y_new = pd.get_dummies(Y) # One hot encode</span><br/><span class="koboSpan" id="kobo.40.1">X_train, X_test, Y_train, y_test = \</span><br/><span class="koboSpan" id="kobo.41.1">train_test_split(X_new, Y_new, test_size=0.4, random_state=333)</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.42.1">The main part of the code is the </span><kbd><span class="koboSpan" id="kobo.43.1">LogisticRegressor</span></kbd><span class="koboSpan" id="kobo.44.1"> class; at first glance, you'll think that it's similar to the </span><kbd><span class="koboSpan" id="kobo.45.1">LinearRegressor</span></kbd><span class="koboSpan" id="kobo.46.1"> class we made earlier. </span><span class="koboSpan" id="kobo.46.2">The class is defined in the Python file, </span><kbd><span class="koboSpan" id="kobo.47.1">LogisticRegressor.py</span></kbd><span class="koboSpan" id="kobo.48.1">. </span><span class="koboSpan" id="kobo.48.2">It is indeed, but there are a few important differences: the </span><kbd><span class="koboSpan" id="kobo.49.1">Y</span></kbd> <span><kbd><span class="koboSpan" id="kobo.50.1">output</span></kbd></span><span class="koboSpan" id="kobo.51.1"> is replaced by </span><kbd><span class="koboSpan" id="kobo.52.1">Y</span><sub><span class="koboSpan" id="kobo.53.1">pred</span></sub></kbd><span class="koboSpan" id="kobo.54.1">, which instead of having a single value, now is a three-dimensional categorical value, each dimension specifying the probability of three categories. </span><span class="koboSpan" id="kobo.54.2">The weights here have dimensions of </span><em><span class="koboSpan" id="kobo.55.1">d × n</span></em><span class="koboSpan" id="kobo.56.1">, where </span><kbd><span class="koboSpan" id="kobo.57.1">d</span></kbd><span class="koboSpan" id="kobo.58.1"> is the number of input features and </span><kbd><span class="koboSpan" id="kobo.59.1">n</span></kbd><span class="koboSpan" id="kobo.60.1"> the number of output categories. </span><span class="koboSpan" id="kobo.60.2">The bias too now is three-dimensional. </span><span class="koboSpan" id="kobo.60.3">Another important change is the change in the loss function:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.61.1">class LogisticRegressor:</span><br/><span class="koboSpan" id="kobo.62.1">    def __init__(self, d, n, lr=0.001 ):</span><br/><span class="koboSpan" id="kobo.63.1">        # Place holders for input-output training data</span><br/><span class="koboSpan" id="kobo.64.1">        self.X = tf.placeholder(tf.float32,\</span><br/><span class="koboSpan" id="kobo.65.1">              shape=[None,d], name='input')</span><br/><span class="koboSpan" id="kobo.66.1">        self.Y = tf.placeholder(tf.float32,\</span><br/><span class="koboSpan" id="kobo.67.1">              name='output')</span><br/><span class="koboSpan" id="kobo.68.1">        # Variables for weight and bias</span><br/><span class="koboSpan" id="kobo.69.1">        self.b = tf.Variable(tf.zeros(n), dtype=tf.float32)</span><br/><span class="koboSpan" id="kobo.70.1">        self.W = tf.Variable(tf.random_normal([d,n]),\</span><br/><span class="koboSpan" id="kobo.71.1">              dtype=tf.float32)</span><br/><span class="koboSpan" id="kobo.72.1">        # The Logistic Regression Model</span><br/><span class="koboSpan" id="kobo.73.1">        h = tf.matmul(self.X, self.W) + self.b</span><br/><span class="koboSpan" id="kobo.74.1">        self.Ypred = tf.nn.sigmoid(h)</span><br/><span class="koboSpan" id="kobo.75.1">        # Loss function</span><br/><span class="koboSpan" id="kobo.76.1">        self.loss = cost = tf.reduce_mean(-tf.reduce_sum(self.Y*tf.log(self.Ypred),\</span><br/><span class="koboSpan" id="kobo.77.1">                 reduction_indices=1), name = 'cross-entropy-loss')</span><br/><span class="koboSpan" id="kobo.78.1">        # Gradient Descent with learning </span><br/><span class="koboSpan" id="kobo.79.1">        # rate of 0.05 to minimize loss</span><br/><span class="koboSpan" id="kobo.80.1">        optimizer = tf.train.GradientDescentOptimizer(lr)</span><br/><span class="koboSpan" id="kobo.81.1">        self.optimize = optimizer.minimize(self.loss)</span><br/><span class="koboSpan" id="kobo.82.1">        # Initializing Variables</span><br/><span class="koboSpan" id="kobo.83.1">        init_op = tf.global_variables_initializer()</span><br/><span class="koboSpan" id="kobo.84.1">        self.sess = tf.Session()</span><br/><span class="koboSpan" id="kobo.85.1">        self.sess.run(init_op)</span><br/><br/><span class="koboSpan" id="kobo.86.1">    def fit(self, X, Y,epochs=500):</span><br/><span class="koboSpan" id="kobo.87.1">        total = []</span><br/><span class="koboSpan" id="kobo.88.1">        for i in range(epochs):</span><br/><span class="koboSpan" id="kobo.89.1">            _, l = self.sess.run([self.optimize,self.loss],\</span><br/><span class="koboSpan" id="kobo.90.1">                  feed_dict={self.X: X, self.Y: Y})</span><br/><span class="koboSpan" id="kobo.91.1">            total.append(l)</span><br/><span class="koboSpan" id="kobo.92.1">            if i%1000==0:</span><br/><span class="koboSpan" id="kobo.93.1">                print('Epoch {0}/{1}: Loss {2}'.format(i,epochs,l))</span><br/><span class="koboSpan" id="kobo.94.1">        return total</span><br/><br/><span class="koboSpan" id="kobo.95.1">   def predict(self, X):</span><br/><span class="koboSpan" id="kobo.96.1">        return self.sess.run(self.Ypred, feed_dict={self.X:X})</span><br/><br/><span class="koboSpan" id="kobo.97.1">    def get_weights(self):</span><br/><span class="koboSpan" id="kobo.98.1">        return self.sess.run([self.W, self.b])</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.99.1">Now we simply train our model and predict the output. </span><span class="koboSpan" id="kobo.99.2">The learned model gives us an accuracy of ~85% on the test dataset. </span><span class="koboSpan" id="kobo.99.3">Pretty impressive!</span></li>
</ol>
<div class="packt_infobox"><span class="koboSpan" id="kobo.100.1">Using ML</span><span><span class="koboSpan" id="kobo.101.1">, we</span></span><span class="koboSpan" id="kobo.102.1"> can also identify what ingredients make wine good quality. </span><span class="koboSpan" id="kobo.102.2">A company called IntelligentX recently started brewing beer based on user feedback; it uses AI to get the recipe for the tastiest beer. </span><span class="koboSpan" id="kobo.102.3">You can read about the work in this </span><em><span class="koboSpan" id="kobo.103.1">Forbes</span></em><span class="koboSpan" id="kobo.104.1"> article: </span><a href="https://www.forbes.com/sites/emmasandler/2016/07/07/you-can-now-drink-beer-brewed-by-artificial-intelligence/#21fd11cc74c3"><span class="koboSpan" id="kobo.105.1">https://www.forbes.com/sites/emmasandler/2016/07/07/you-can-now-drink-beer-brewed-by-artificial-intelligence/#21fd11cc74c3</span></a><span class="koboSpan" id="kobo.106.1">.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Classification using support vector machines</span></h1>
                </header>
            
            <article>
                
<p><strong><span class="koboSpan" id="kobo.2.1">Support Vector Machines</span></strong><span class="koboSpan" id="kobo.3.1"> (</span><strong><span class="koboSpan" id="kobo.4.1">SVMs</span></strong><span class="koboSpan" id="kobo.5.1">) is arguably the most used ML technique for classification. </span><span class="koboSpan" id="kobo.5.2">The main idea behind SVM is that we find an optimal hyperplane with maximum margin separating the two classes. </span><span class="koboSpan" id="kobo.5.3">If the data is linearly separable, the process of finding the hyperplane is straightforward, but if it isn't linearly separable, then kernel trick is used to make the data linearly separable in some transformed high-dimensional feature space.</span></p>
<p><span><span class="koboSpan" id="kobo.6.1">SVM is considered a non-parametric supervised learning algorithm. </span><span class="koboSpan" id="kobo.6.2">The main idea of SVM is to find a </span></span><strong><span class="koboSpan" id="kobo.7.1">maximal margin separator</span></strong><span><span class="koboSpan" id="kobo.8.1">: a separating hyperplane that is farthest from the training samples presented.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">Consider the following diagram; the red dots represent class 1 for which the output should be 1, and the blue dots represent the class 2 for which the output should be -1. </span><span class="koboSpan" id="kobo.9.2">There can be many lines which can separate the red dots from the blue ones; the diagram demonstrates three such lines: </span><strong><span class="koboSpan" id="kobo.10.1">A</span></strong><span class="koboSpan" id="kobo.11.1">, </span><strong><span class="koboSpan" id="kobo.12.1">B</span></strong><span class="koboSpan" id="kobo.13.1">, and </span><strong><span class="koboSpan" id="kobo.14.1">C</span></strong><span class="koboSpan" id="kobo.15.1"> respectively. </span><span class="koboSpan" id="kobo.15.2">Which of the three lines do you think will be the best choice? </span><span class="koboSpan" id="kobo.15.3">Intuitively, the best choice is line B, because it's farthest from the examples of both classes, and hence ensures the least error in classification:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.16.1"><img class="aligncenter size-full wp-image-1083 image-border" src="assets/900315da-37fc-4a5b-a32f-78220b239342.png" style="width:19.83em;height:19.83em;"/></span></p>
<p><span class="koboSpan" id="kobo.17.1">In the following section, we'll learn the basic maths behind finding the maximal-separator hyperplane. </span><span class="koboSpan" id="kobo.17.2">Though the maths here is mostly basic, if you don't like maths you can simply skip to the implementation section where we use SVM to classify wine again! </span><span class="koboSpan" id="kobo.17.3">Cheers! </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Maximum margin hyperplane</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">From our knowledge of linear algebra, we know that the equation of a plane is given by the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img class="fm-editor-equation" src="assets/9f9a231d-0111-405f-9996-da15704d42da.png" style="width:7.08em;height:1.25em;"/></span></div>
<p><span class="koboSpan" id="kobo.4.1">In SVM, this plane should separate the positive classes (</span><em><span class="koboSpan" id="kobo.5.1">y</span></em><span class="koboSpan" id="kobo.6.1">= </span><em><span class="koboSpan" id="kobo.7.1">1</span></em><span class="koboSpan" id="kobo.8.1">) from the negative classes (</span><em><span class="koboSpan" id="kobo.9.1">y</span></em><span class="koboSpan" id="kobo.10.1">=</span><em><span class="koboSpan" id="kobo.11.1">-1</span></em><span class="koboSpan" id="kobo.12.1">), and there's an additional constrain: the distance (margin) of this hyperplane from the closest positive and negative training vectors (</span><em><span><span class="koboSpan" id="kobo.13.1">X</span></span><sub><span class="koboSpan" id="kobo.14.1">pos</span></sub></em><span class="koboSpan" id="kobo.15.1"> and </span><em><span><span class="koboSpan" id="kobo.16.1">X</span></span><sub><span class="koboSpan" id="kobo.17.1">neg</span></sub></em><span class="koboSpan" id="kobo.18.1"> respectively) should be maximum. </span><span class="koboSpan" id="kobo.18.2">Hence, the plane is called the maximum margin separator.</span></p>
<p class="mce-root"/>
<div class="packt_infobox"><span class="koboSpan" id="kobo.19.1">The vectors </span><em><span class="koboSpan" id="kobo.20.1">X</span></em><sub><em><span class="koboSpan" id="kobo.21.1">pos</span></em><span class="koboSpan" id="kobo.22.1"> </span></sub><span class="koboSpan" id="kobo.23.1">and </span><em><span class="koboSpan" id="kobo.24.1">X</span></em><sub><em><span class="koboSpan" id="kobo.25.1">neg</span></em><span class="koboSpan" id="kobo.26.1"> </span></sub><span class="koboSpan" id="kobo.27.1">are called </span><strong><span class="koboSpan" id="kobo.28.1">support vectors,</span></strong><span class="koboSpan" id="kobo.29.1"> and they play an important role in defining the SVM model.</span></div>
<p><span class="koboSpan" id="kobo.30.1">Mathematically, this means that the following is true:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.31.1"><img class="fm-editor-equation" src="assets/297a2f4c-fa15-4849-b82f-6966b1dd6eb9.png" style="width:9.50em;height:1.58em;"/></span></div>
<p><span class="koboSpan" id="kobo.32.1">And, so is this:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.33.1"><img class="fm-editor-equation" src="assets/1203bf35-5600-4a16-8e6c-1a320d09f32a.png" style="width:10.50em;height:1.58em;"/></span></div>
<p class="mce-root CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.34.1">From these two equations, we get the following:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.35.1"><img class="fm-editor-equation" src="assets/5a6caae8-8666-4d86-a2fe-409fdb573500.png" style="width:10.58em;height:1.50em;"/></span></div>
<p><span class="koboSpan" id="kobo.36.1">Dividing by the weight vector length into both sides, we get the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.37.1"><img class="fm-editor-equation" src="assets/fee17cdc-d92b-4cab-a6bd-bb1b5093899d.png" style="width:13.83em;height:3.17em;"/></span></div>
<p><span class="koboSpan" id="kobo.38.1">So we need to find a separator so that the margin between positive and negative support vectors is maximum, that is: </span><span class="koboSpan" id="kobo.39.1"><img class="fm-editor-equation" src="assets/825f71a2-a026-4d06-9122-5a60ef4e70ee.png" style="width:1.92em;height:2.00em;"/></span><span class="koboSpan" id="kobo.40.1"> is maximum, while at the same time all the points are classified correctly, such as the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.41.1"><img class="fm-editor-equation" src="assets/0a4af9f3-b099-47f9-867e-d1003b8823ae.png" style="width:9.33em;height:1.58em;"/></span></div>
<p><span class="koboSpan" id="kobo.42.1">Using a little maths, which we'll not go into in this book, the preceding condition can be represented as finding an optimal solution to the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.43.1"><img class="fm-editor-equation" src="assets/bef553ea-90e1-4f0b-a4e8-3bfae275d27a.png" style="width:20.33em;height:3.33em;"/></span></div>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.44.1">Subject to the constraints that:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.45.1"><img class="fm-editor-equation" src="assets/4fe746d0-2a30-48cd-9ed6-783275dd1486.png" style="width:3.50em;height:1.33em;"/></span></div>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.46.1"><img class="fm-editor-equation" src="assets/f7738f34-ca7f-4970-80ff-2439f0bf2464.png" style="width:5.92em;height:2.67em;"/></span></div>
<p><span class="koboSpan" id="kobo.47.1">From the values of alpha, we can get weights </span><em><span class="koboSpan" id="kobo.48.1">W</span></em><span class="koboSpan" id="kobo.49.1"> from </span><em><span class="koboSpan" id="kobo.50.1">α</span></em><span class="koboSpan" id="kobo.51.1">, the vector of coefficients, using the following equation:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.52.1"><img class="fm-editor-equation" src="assets/4098e849-68a2-46d8-8c8a-6ce87f78a05b.png" style="width:6.83em;height:2.67em;"/></span></div>
<p><span class="koboSpan" id="kobo.53.1">This is a standard quadratic programming optimization problem. </span><span class="koboSpan" id="kobo.53.2">Most ML libraries have built-in functions to solve it, so you need not worry about how to do so.</span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.54.1">For the reader interested in knowing more about SVMs and the math behind it, the book </span><em><span class="koboSpan" id="kobo.55.1">The Nature of Statistical Learning Theory</span></em><span class="koboSpan" id="kobo.56.1"> by</span><span><span class="koboSpan" id="kobo.57.1"> Vladimir Vapnik, published by</span></span><span class="koboSpan" id="kobo.58.1"> </span><em><span><span class="koboSpan" id="kobo.59.1">Springer Science+Business Media</span></span></em><span class="koboSpan" id="kobo.60.1">, 2013, is an excellent reference.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Kernel trick</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The previous method works fine when the input feature space is linearly separable. </span><span class="koboSpan" id="kobo.2.2">What should we do when it isn't? </span><span class="koboSpan" id="kobo.2.3">One simple way is to transform the data (</span><em><span class="koboSpan" id="kobo.3.1">X</span></em><span class="koboSpan" id="kobo.4.1">) into a higher dimensional space where it's linearly separable and find a maximal margin hyperplane in that high-dimensional space. </span><span class="koboSpan" id="kobo.4.2">Let's see how; our hyperplane in terms of </span><em><span class="koboSpan" id="kobo.5.1">α</span></em><span class="koboSpan" id="kobo.6.1"> is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><em><span class="koboSpan" id="kobo.7.1"><img class="fm-editor-equation" src="assets/1cfc0edb-71e9-44d3-9b6d-500bfccdbac2.png" style="width:10.00em;height:2.75em;"/></span></em></div>
<p><span class="koboSpan" id="kobo.8.1">Let </span><em><span class="koboSpan" id="kobo.9.1">φ</span></em><span class="koboSpan" id="kobo.10.1"> be the transform, then we can replace </span><em><span class="koboSpan" id="kobo.11.1">X</span></em><span class="koboSpan" id="kobo.12.1"> by </span><em><span class="koboSpan" id="kobo.13.1">φ</span></em><span class="koboSpan" id="kobo.14.1">(</span><em><span class="koboSpan" id="kobo.15.1">X</span></em><span class="koboSpan" id="kobo.16.1">) and hence its dot product </span><em><span class="koboSpan" id="kobo.17.1">X</span><sup><span class="koboSpan" id="kobo.18.1">T </span></sup><span class="koboSpan" id="kobo.19.1">X</span><sup><span class="koboSpan" id="kobo.20.1">(i)</span></sup></em><span class="koboSpan" id="kobo.21.1"> with a function K(</span><em><span class="koboSpan" id="kobo.22.1">X</span><sup><span class="koboSpan" id="kobo.23.1">T</span></sup></em><span class="koboSpan" id="kobo.24.1">, </span><em><span class="koboSpan" id="kobo.25.1">X</span></em><sup><span class="koboSpan" id="kobo.26.1">(</span><em><span class="koboSpan" id="kobo.27.1">i</span></em><span class="koboSpan" id="kobo.28.1">)</span></sup><span class="koboSpan" id="kobo.29.1">) = </span><em><span class="koboSpan" id="kobo.30.1">φ</span></em><span class="koboSpan" id="kobo.31.1">(</span><em><span class="koboSpan" id="kobo.32.1">X</span></em><span class="koboSpan" id="kobo.33.1">)</span><em><sup><span class="koboSpan" id="kobo.34.1">T</span></sup></em> <em><span class="koboSpan" id="kobo.35.1">φ</span></em><span class="koboSpan" id="kobo.36.1">(</span><em><span class="koboSpan" id="kobo.37.1">X</span></em><sup><span class="koboSpan" id="kobo.38.1">(</span><em><span class="koboSpan" id="kobo.39.1">i</span></em><span class="koboSpan" id="kobo.40.1">)</span></sup><span class="koboSpan" id="kobo.41.1">) called </span><strong><span class="koboSpan" id="kobo.42.1">kernel</span></strong><span class="koboSpan" id="kobo.43.1">. </span><span class="koboSpan" id="kobo.43.2">So we now just preprocess the data by applying the transform </span><em><span class="koboSpan" id="kobo.44.1">φ</span></em><span class="koboSpan" id="kobo.45.1"> and then find a linear separator in the transformed space as before.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.46.1">The most commonly used kernel function is the </span><strong><span class="koboSpan" id="kobo.47.1">Gaussian kernel</span></strong><span class="koboSpan" id="kobo.48.1">, also called </span><strong><span class="koboSpan" id="kobo.49.1">radial basis function</span></strong><span class="koboSpan" id="kobo.50.1">, defined as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.51.1"><img class="fm-editor-equation" src="assets/1c95c021-9f0e-4d3f-8f22-7f0425e2e684.png" style="width:17.25em;height:1.83em;"/></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Classifying wine using SVM</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We'll use the </span><kbd><span class="koboSpan" id="kobo.3.1">svm.SVC</span></kbd><span class="koboSpan" id="kobo.4.1"> function provided by the scikit library for the task. </span><span class="koboSpan" id="kobo.4.2">The reason to do so is that the TensorFlow library provides us, as of the time of writing, with only a linear implementation of SVM, and it works only for binary classification. </span><span class="koboSpan" id="kobo.4.3">We can make our own SVM using the maths we learned in previously in TensorFlow, and </span><kbd><span class="koboSpan" id="kobo.5.1">SVM_TensorFlow.ipynb</span></kbd><span class="koboSpan" id="kobo.6.1"> in the GitHub repository contains the implementation in TensorFlow. </span><span class="koboSpan" id="kobo.6.2">The following code can be found in the </span><kbd><span class="koboSpan" id="kobo.7.1">Wine_quality_using_SVM.ipynb</span></kbd><span class="koboSpan" id="kobo.8.1">.</span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.9.1">The SVC classifier of scikit is a support vector classifier. </span><span class="koboSpan" id="kobo.9.2">It can also handle multiclass support using a one-versus-one scheme. </span><span class="koboSpan" id="kobo.9.3">Some of the optional parameters of the method are as follows:</span><br/>
<br/>
<ul>
<li><kbd><span class="koboSpan" id="kobo.10.1">C</span></kbd><span class="koboSpan" id="kobo.11.1">: It's a parameter specifying the penalty term (default value is </span><kbd><span class="koboSpan" id="kobo.12.1">1.0</span></kbd><span class="koboSpan" id="kobo.13.1">).</span></li>
</ul>
<ul>
<li><kbd><span class="koboSpan" id="kobo.14.1">kernel</span></kbd><span class="koboSpan" id="kobo.15.1">: It specifies the kernel to be used (default is </span><kbd><span class="koboSpan" id="kobo.16.1">rbf</span></kbd><span class="koboSpan" id="kobo.17.1">). </span><span class="koboSpan" id="kobo.17.2">The possible choices are </span><kbd><span class="koboSpan" id="kobo.18.1">linear</span></kbd><span class="koboSpan" id="kobo.19.1">, </span><kbd><span class="koboSpan" id="kobo.20.1">poly</span></kbd><span class="koboSpan" id="kobo.21.1">, </span><kbd><span class="koboSpan" id="kobo.22.1">rbf</span></kbd><span class="koboSpan" id="kobo.23.1">, </span><kbd><span class="koboSpan" id="kobo.24.1">sigmoid</span></kbd><span class="koboSpan" id="kobo.25.1">, </span><kbd><span class="koboSpan" id="kobo.26.1">precomputed</span></kbd><span class="koboSpan" id="kobo.27.1">, and </span><kbd><span class="koboSpan" id="kobo.28.1">callable</span></kbd><span class="koboSpan" id="kobo.29.1">.</span></li>
</ul>
<ul>
<li><kbd><span class="koboSpan" id="kobo.30.1">gamma</span></kbd><span class="koboSpan" id="kobo.31.1">: It specifies the kernel coefficient for </span><kbd><span class="koboSpan" id="kobo.32.1">rbf</span></kbd><span class="koboSpan" id="kobo.33.1">, </span><kbd><span class="koboSpan" id="kobo.34.1">poly</span></kbd><span class="koboSpan" id="kobo.35.1">, and </span><kbd><span class="koboSpan" id="kobo.36.1">sigmoid</span></kbd><span class="koboSpan" id="kobo.37.1"> and the default value (the default is </span><kbd><span class="koboSpan" id="kobo.38.1">auto</span></kbd><span class="koboSpan" id="kobo.39.1">).</span></li>
</ul>
<ul>
<li><kbd><span class="koboSpan" id="kobo.40.1">random_state</span></kbd><span class="koboSpan" id="kobo.41.1">: It sets the seed of the pseudo-random number generator to use when shuffling the data.</span></li>
</ul>
</div>
<p class="mce-root"/>
<p class="mce-root"><span class="koboSpan" id="kobo.42.1">Follow the given steps to create our SVM model:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.43.1">Let's load all of the modules we'll need for the code. </span><span class="koboSpan" id="kobo.43.2">Note that we aren't importing TensorFlow here and instead have imported certain modules from the </span><kbd><span class="koboSpan" id="kobo.44.1">scikit</span></kbd><span class="koboSpan" id="kobo.45.1"> library:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.46.1"># Import the modules</span><br/><span class="koboSpan" id="kobo.47.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.48.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.49.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.50.1">from sklearn.preprocessing import MinMaxScaler, LabelEncoder</span><br/><span class="koboSpan" id="kobo.51.1">from sklearn.model_selection import train_test_split</span><br/><span class="koboSpan" id="kobo.52.1">from sklearn.metrics import confusion_matrix, accuracy_score</span><br/><span class="koboSpan" id="kobo.53.1">from sklearn.svm import SVC # The SVM Classifier from scikit</span><br/><span class="koboSpan" id="kobo.54.1">import seaborn as sns</span><br/><span class="koboSpan" id="kobo.55.1">%matplotlib inline</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.56.1">We read the data file, preprocess it, and separate it into test and training datasets. </span><span class="koboSpan" id="kobo.56.2">This time, for simplicity, we're dividing into two classes, </span><kbd><span class="koboSpan" id="kobo.57.1">good</span></kbd><span class="koboSpan" id="kobo.58.1"> and </span><kbd><span class="koboSpan" id="kobo.59.1">bad</span></kbd><span class="koboSpan" id="kobo.60.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.61.1">filename = 'winequality-red.csv' #Download the file from UCI ML Repo</span><br/><span class="koboSpan" id="kobo.62.1">df = pd.read_csv(filename, sep=';')</span><br/><br/><span class="koboSpan" id="kobo.63.1">#categorize wine quality in two levels</span><br/><span class="koboSpan" id="kobo.64.1">bins = (0,5.5,10)</span><br/><span class="koboSpan" id="kobo.65.1">categories = pd.cut(df['quality'], bins, labels = ['bad','good'])</span><br/><span class="koboSpan" id="kobo.66.1">df['quality'] = categories</span><br/><br/><span class="koboSpan" id="kobo.67.1">#PreProcessing and splitting data to X and y</span><br/><span class="koboSpan" id="kobo.68.1">X = df.drop(['quality'], axis = 1)</span><br/><span class="koboSpan" id="kobo.69.1">scaler = MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.70.1">X_new = scaler.fit_transform(X)</span><br/><span class="koboSpan" id="kobo.71.1">y = df['quality']</span><br/><span class="koboSpan" id="kobo.72.1">labelencoder_y = LabelEncoder()</span><br/><span class="koboSpan" id="kobo.73.1">y = labelencoder_y.fit_transform(y)</span><br/><span class="koboSpan" id="kobo.74.1">X_train, X_test, y_train, y_test = train_test_split(X, y, \</span><br/><span class="koboSpan" id="kobo.75.1">        test_size = 0.2, random_state = 323)</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.76.1">Now we use the </span><kbd><span class="koboSpan" id="kobo.77.1">SVC</span></kbd><span class="koboSpan" id="kobo.78.1"> classifier and train it on our training dataset with the </span><kbd><span class="koboSpan" id="kobo.79.1">fit</span></kbd><span class="koboSpan" id="kobo.80.1"> method:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.81.1">classifier = SVC(kernel = 'rbf', random_state = 45)</span><br/><span class="koboSpan" id="kobo.82.1">classifier.fit(X_train, y_train)</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.83.1">Let's now predict the output for the test dataset:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.84.1">y_pred = classifier.predict(X_test)</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.85.1">The model gave an accuracy of </span><kbd><span class="koboSpan" id="kobo.86.1">67.5%</span></kbd><span class="koboSpan" id="kobo.87.1"> and the confusion matrix is as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.88.1">print("Accuracy is {}".format(accuracy_score(y_test, y_pred)))</span><br/><span class="koboSpan" id="kobo.89.1">## Gives a value ~ 67.5%</span><br/><span class="koboSpan" id="kobo.90.1">cm = confusion_matrix(y_test, y_pred)</span><br/><span class="koboSpan" id="kobo.91.1">sns.heatmap(cm,annot=True,fmt='2.0f')</span></pre>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.92.1"><img class="aligncenter size-full wp-image-1084 image-border" src="assets/a5309c9f-56b6-401b-9897-c3259f565a76.png" style="width:31.50em;height:22.58em;"/></span></div>
<p style="padding-left: 90px"><span class="koboSpan" id="kobo.93.1">The preceding code uses the binary classification; we can change the code to work for more than two classes as well. </span><span class="koboSpan" id="kobo.93.2">For example, in the second step, we can replace the code with the following:</span></p>
<pre style="padding-left: 90px"><span class="koboSpan" id="kobo.94.1">bins = (0,3.5,5.5,10)</span><br/><span class="koboSpan" id="kobo.95.1">categories = pd.cut(df['quality'], bins, labels = ['bad','ok','good'])</span><br/><span class="koboSpan" id="kobo.96.1">df['quality'] = categories</span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.97.1">Then we have three categories just as our previous logistic classifier, and the accuracy is 65.9%. </span><span class="koboSpan" id="kobo.97.2">And the confusion matrix is as follows:</span></li>
</ol>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.98.1">In the three-class case, the training data distribution is as follows:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.99.1">good</span></kbd><span class="koboSpan" id="kobo.100.1"> </span><kbd><span class="koboSpan" id="kobo.101.1">855</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.102.1">ok</span></kbd><span class="koboSpan" id="kobo.103.1"> </span><kbd><span class="koboSpan" id="kobo.104.1">734</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.105.1">bad</span></kbd><span class="koboSpan" id="kobo.106.1"> </span><kbd><span class="koboSpan" id="kobo.107.1">10</span></kbd></li>
</ul>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.108.1">Since the number of samples in the </span><kbd><span class="koboSpan" id="kobo.109.1">bad</span></kbd><span class="koboSpan" id="kobo.110.1"> class (corresponding to </span><kbd><span class="koboSpan" id="kobo.111.1">0</span></kbd><span class="koboSpan" id="kobo.112.1"> in the confusion matrix) is only </span><kbd><span class="koboSpan" id="kobo.113.1">10</span></kbd><span class="koboSpan" id="kobo.114.1">, the model isn't able to learn what parameters contribute to bad wine quality. </span><span class="koboSpan" id="kobo.114.2">Hence, data should be uniformly distributed among all classes of the classifiers that we explore in this chapter.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Naive Bayes</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Naive Bayes is one of the simplest and fastest ML algorithms. </span><span class="koboSpan" id="kobo.2.2">This too belongs to the class of supervised learning algorithms. </span><span class="koboSpan" id="kobo.2.3">It's based on the Bayes probability theorem. </span><span class="koboSpan" id="kobo.2.4">One important assumption that we make in the case of the Naive Bayes classifier is that all of the features of the input vector are </span><strong><span class="koboSpan" id="kobo.3.1">independent and identically distributed</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">iid</span></strong><span class="koboSpan" id="kobo.6.1">). The goal is to learn a conditional probability model f</span><span><span class="koboSpan" id="kobo.7.1">or each class </span></span><em><span class="koboSpan" id="kobo.8.1">C</span></em><sub><em><span class="koboSpan" id="kobo.9.1">k</span></em></sub><span><span class="koboSpan" id="kobo.10.1"> in the training dataset</span></span><span class="koboSpan" id="kobo.11.1">:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.12.1"><img class="fm-editor-equation" src="assets/9796dc8e-c201-401f-b6a8-4427c915c783.png" style="width:17.17em;height:1.50em;"/></span></div>
<p><span class="koboSpan" id="kobo.13.1">Under the iid assumption, and using the Bayes theorem, this can be expressed in terms of the joint probability distribution </span><em><span class="koboSpan" id="kobo.14.1">p</span></em><span class="koboSpan" id="kobo.15.1">(</span><em><span class="koboSpan" id="kobo.16.1">C</span><sub><span class="koboSpan" id="kobo.17.1">k</span></sub></em><span class="koboSpan" id="kobo.18.1">, </span><em><span class="koboSpan" id="kobo.19.1">X</span></em><span class="koboSpan" id="kobo.20.1">):</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.21.1"><img class="fm-editor-equation" src="assets/11e80a0f-f697-4acb-bc36-98b77039a6bf.png" style="width:22.00em;height:3.58em;"/></span></div>
<p><span class="koboSpan" id="kobo.22.1">We pick the class that maximizes this term </span><strong><em><span class="koboSpan" id="kobo.23.1">Maximum A Posteriori</span></em><span class="koboSpan" id="kobo.24.1"> </span></strong><span class="koboSpan" id="kobo.25.1">(</span><strong><span class="koboSpan" id="kobo.26.1">MAP</span></strong><span class="koboSpan" id="kobo.27.1">):</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.28.1"><img class="fm-editor-equation" src="assets/4981d980-6d92-4ae7-8acf-a96f36da3805.png" style="width:15.67em;height:4.00em;"/></span></div>
<p><span class="koboSpan" id="kobo.29.1">There can be different Naive Bayes algorithms, depending upon the distribution of </span><em><span class="koboSpan" id="kobo.30.1">p</span></em><span class="koboSpan" id="kobo.31.1">(</span><em><span class="koboSpan" id="kobo.32.1">x</span><sub><span class="koboSpan" id="kobo.33.1">i</span></sub></em><span class="koboSpan" id="kobo.34.1">|</span><em><span class="koboSpan" id="kobo.35.1">C</span><sub><span class="koboSpan" id="kobo.36.1">k</span></sub></em><span class="koboSpan" id="kobo.37.1">). </span><span class="koboSpan" id="kobo.37.2">The common choices are Gaussian in the case of real-valued data, Bernoulli for binary data, and MultiNomial when the data contains the frequency of a certain event (such as document classification).</span></p>
<p><span class="koboSpan" id="kobo.38.1">Let's now see whether we can classify the wine using Naive Bayes. </span><span class="koboSpan" id="kobo.38.2">For the sake of simplicity and efficiency, we'll use the scikit built-in Naive Bayes distributions. </span><span class="koboSpan" id="kobo.38.3">Since the features values we have in our data are continuous-valued—we'll assume that they have a Gaussian distribution, and we'll use </span><kbd><span class="koboSpan" id="kobo.39.1">GaussianNB</span></kbd><span class="koboSpan" id="kobo.40.1"> of scikit-learn.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Gaussian Naive Bayes for wine quality</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The scikit-learn Naive Bayes module supports three Naive Bayes distributions. </span><span class="koboSpan" id="kobo.2.2">We can choose either of them depending on our input feature data type. </span><span class="koboSpan" id="kobo.2.3">The three Naive Bayes available in scikit-learn are as follows:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.3.1">GaussianNB</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.4.1">MultinomialNB</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.5.1">BernoulliNB</span></kbd></li>
</ul>
<p><span class="koboSpan" id="kobo.6.1">The wine data, as we have already seen, is a continuous data type. </span><span class="koboSpan" id="kobo.6.2">Hence, it will be good if we use Gaussian distribution for </span><em><span class="koboSpan" id="kobo.7.1">p</span></em><span><span class="koboSpan" id="kobo.8.1">(</span></span><em><span class="koboSpan" id="kobo.9.1">x</span><sub><span class="koboSpan" id="kobo.10.1">i</span></sub></em><span><span class="koboSpan" id="kobo.11.1">|</span></span><em><span class="koboSpan" id="kobo.12.1">C</span><sub><span class="koboSpan" id="kobo.13.1">k</span></sub></em><span><span class="koboSpan" id="kobo.14.1">)—that is, the </span><kbd><span class="koboSpan" id="kobo.15.1">GaussianNB</span></kbd><span class="koboSpan" id="kobo.16.1"> module, and so we'll add </span><kbd><span class="koboSpan" id="kobo.17.1">from sklearn.naive_bayes import GaussianNB</span></kbd><span class="koboSpan" id="kobo.18.1"> in the import cell of the Notebook. </span><span class="koboSpan" id="kobo.18.2">You can read more details about the </span><kbd><span class="koboSpan" id="kobo.19.1">GaussianNB</span></kbd><span class="koboSpan" id="kobo.20.1"> module from the is scikit-learn link: </span><a href="http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB"><span class="koboSpan" id="kobo.21.1">http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB</span></a><span class="koboSpan" id="kobo.22.1">. </span></span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.23.1">The first two steps will remain the same as in the SVM case. </span><span class="koboSpan" id="kobo.23.2">But now, instead of declaring an </span><kbd><span class="koboSpan" id="kobo.24.1">SVM</span></kbd><span class="koboSpan" id="kobo.25.1"> classifier, we'll declare a </span><kbd><span class="koboSpan" id="kobo.26.1">GaussianNB</span></kbd><span class="koboSpan" id="kobo.27.1"> classifier and we'll use its </span><kbd><span class="koboSpan" id="kobo.28.1">fit</span></kbd><span class="koboSpan" id="kobo.29.1"> method to learn the training examples. </span><span class="koboSpan" id="kobo.29.2">The result from the learned model is obtained using the </span><kbd><span class="koboSpan" id="kobo.30.1">predict</span></kbd><span class="koboSpan" id="kobo.31.1"> method. </span><span class="koboSpan" id="kobo.31.2">So follow these steps:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.32.1">Import the necessary modules. </span><span class="koboSpan" id="kobo.32.2">Note that now we're importing </span><kbd><span class="koboSpan" id="kobo.33.1">GaussianNB</span></kbd><span class="koboSpan" id="kobo.34.1"> from the </span><kbd><span class="koboSpan" id="kobo.35.1">scikit</span></kbd><span class="koboSpan" id="kobo.36.1"> library:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.37.1"># Import the modules</span><br/><span class="koboSpan" id="kobo.38.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.39.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.40.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.41.1">from sklearn.preprocessing import MinMaxScaler, LabelEncoder</span><br/><span class="koboSpan" id="kobo.42.1">from sklearn.model_selection import train_test_split</span><br/><span class="koboSpan" id="kobo.43.1">from sklearn.metrics import confusion_matrix, accuracy_score</span><br/><span class="koboSpan" id="kobo.44.1">from sklearn.naive_bayes import GaussianNB # The SVM Classifier from scikit</span><br/><span class="koboSpan" id="kobo.45.1">import seaborn as sns</span><br/><span class="koboSpan" id="kobo.46.1">%matplotlib inline</span></pre>
<ol start="2">
<li><span><span class="koboSpan" id="kobo.47.1">Read the data file and preprocess it:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.48.1">filename = 'winequality-red.csv' #Download the file from UCI ML Repo</span><br/><span class="koboSpan" id="kobo.49.1">df = pd.read_csv(filename, sep=';')</span><br/><br/><span class="koboSpan" id="kobo.50.1">#categorize wine quality in two levels</span><br/><span class="koboSpan" id="kobo.51.1">bins = (0,5.5,10)</span><br/><span class="koboSpan" id="kobo.52.1">categories = pd.cut(df['quality'], bins, labels = ['bad','good'])</span><br/><span class="koboSpan" id="kobo.53.1">df['quality'] = categories</span><br/><br/><span class="koboSpan" id="kobo.54.1">#PreProcessing and splitting data to X and y</span><br/><span class="koboSpan" id="kobo.55.1">X = df.drop(['quality'], axis = 1)</span><br/><span class="koboSpan" id="kobo.56.1">scaler = MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.57.1">X_new = scaler.fit_transform(X)</span><br/><span class="koboSpan" id="kobo.58.1">y = df['quality']</span><br/><span class="koboSpan" id="kobo.59.1">labelencoder_y = LabelEncoder()</span><br/><span class="koboSpan" id="kobo.60.1">y = labelencoder_y.fit_transform(y)</span><br/><span class="koboSpan" id="kobo.61.1">X_train, X_test, y_train, y_test = train_test_split(X, y, \</span><br/><span class="koboSpan" id="kobo.62.1">        test_size = 0.2, random_state = 323)</span></pre>
<p class="mce-root"/>
<ol start="3">
<li><span class="koboSpan" id="kobo.63.1">Now we declare a Gaussian Naive Bayes, train it on the training dataset, and use the trained model to predict the wine quality on the test dataset:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.64.1">classifier = GaussianNB()</span><br/><span class="koboSpan" id="kobo.65.1">classifier.fit(X_train, y_train)</span><br/><span class="koboSpan" id="kobo.66.1">#Predicting the Test Set</span><br/><span class="koboSpan" id="kobo.67.1">y_pred = classifier.predict(X_test)</span></pre>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.68.1">That's all, folks; our model is ready and kicking. </span><span class="koboSpan" id="kobo.68.2">The accuracy of this model is 71.25% for the binary classification case. </span><span class="koboSpan" id="kobo.68.3">In the following screenshot, you can a the heatmap of the confusion matrix:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.69.1"><img src="assets/556bf535-7318-4f9b-9ee5-78f824c7035e.png"/></span></div>
<p><span class="koboSpan" id="kobo.70.1">Before you conclude that Naive Bayes is best, let's be aware of some of its pitfalls:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.71.1">Naive Bayes makes the prediction based on the frequency-based probability; therefore, it's strongly dependent on the data we use for training.</span></li>
<li><span class="koboSpan" id="kobo.72.1">Another issue is that we made the iid assumption about input feature space; this isn't always true.</span></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Decision trees</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this section, you'll learn about another ML algorithm that's very popular and fast—decision trees. </span><span class="koboSpan" id="kobo.2.2">In decision trees, we build a tree-like structure of decisions; we start with the root, choose a feature and split into branches, and continue till we reach the leaves, which represent the predicted class or value. </span><span class="koboSpan" id="kobo.2.3">The algorithm of decision trees involves two main steps:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.3.1">Decide which features to choose and what conditions to use for splitting</span></li>
<li><span class="koboSpan" id="kobo.4.1">Know when to stop</span></li>
</ul>
<p><span class="koboSpan" id="kobo.5.1">Let's understand it with an example. </span><span class="koboSpan" id="kobo.5.2">Consider a sample of 40 students; we have three variables: the gender (boy or girl; discrete), class (XI or XII; discrete), and height (5 to 6 feet; continuous). </span><span class="koboSpan" id="kobo.5.3">Eighteen students prefer to go to the library in their spare time and rest prefer to play. </span><span class="koboSpan" id="kobo.5.4">We can build a decision tree to predict who will be going to the library and who will be going to the playground in their leisure time. </span><span class="koboSpan" id="kobo.5.5">To build the decision tree, we'll need to separate the students who go to library/playground based on the highly significant input variable among the three input variables. </span><span class="koboSpan" id="kobo.5.6">The following diagram gives the split based on each input variable:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.6.1"><img class="aligncenter size-full wp-image-1086 image-border" src="assets/2bebf484-66d9-4936-83c1-79dd982f6e57.png" style="width:54.42em;height:25.92em;"/></span></p>
<p><span class="koboSpan" id="kobo.7.1">We consider all of the features and choose the one that gives us the maximum information. </span><span class="koboSpan" id="kobo.7.2">In the previous example, we can see that a split over the feature height generates the most homogeneous groups, with the group </span><strong><span class="koboSpan" id="kobo.8.1">Height &gt; 5.5 ft</span></strong><span class="koboSpan" id="kobo.9.1"> containing 80% students who play and 20% who go to the library in the leisure time and the group </span><strong><span class="koboSpan" id="kobo.10.1">Height &lt; 5.5 ft</span></strong><span class="koboSpan" id="kobo.11.1"> containing 13% students who play and 86% who go to the library in their spare time. </span><span class="koboSpan" id="kobo.11.2">Hence, we'll make our first split on the feature height. </span><span class="koboSpan" id="kobo.11.3">We'll continue the split in this manner and finally reach the decision (leaf node) telling us whether the student will play or go to the library in their spare time. </span><span class="koboSpan" id="kobo.11.4">The following diagram shows the decision tree structure; the black circle is the </span><strong><span class="koboSpan" id="kobo.12.1">Root</span></strong> <strong><span class="koboSpan" id="kobo.13.1">Node</span></strong><span class="koboSpan" id="kobo.14.1">, the blue circles are the </span><strong><span class="koboSpan" id="kobo.15.1">Decision</span></strong> <strong><span class="koboSpan" id="kobo.16.1">Nodes</span></strong><span class="koboSpan" id="kobo.17.1">, and the green circles are the </span><strong><span class="koboSpan" id="kobo.18.1">Leaf</span></strong> <strong><span class="koboSpan" id="kobo.19.1">Nodes</span></strong><span class="koboSpan" id="kobo.20.1">:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.21.1"><img class="aligncenter size-full wp-image-1155 image-border" src="assets/cc3007bb-01fb-4b84-ac7c-9a850c58f8bb.png" style="width:21.25em;height:20.42em;"/></span></p>
<p><span class="koboSpan" id="kobo.22.1">The decision trees belong to the family of greedy algorithms. </span><span class="koboSpan" id="kobo.22.2">To find the most homogeneous split, we define our cost function so that it tries to maximize the same class input values in a particular group. </span><span class="koboSpan" id="kobo.22.3">For regression, we generally use the mean square error cost function:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.23.1"><img class="fm-editor-equation" src="assets/89ebe80b-22b8-431f-a110-be4f8f7ce105.png" style="width:17.92em;height:2.25em;"/></span></div>
<p><span class="koboSpan" id="kobo.24.1">Here, </span><em><span class="koboSpan" id="kobo.25.1">y</span></em><span class="koboSpan" id="kobo.26.1"> and </span><em><span class="koboSpan" id="kobo.27.1">y</span><sub><span class="koboSpan" id="kobo.28.1">pred</span></sub></em><span class="koboSpan" id="kobo.29.1"> represent the given and predicted output values for the input values (</span><em><span class="koboSpan" id="kobo.30.1">i</span></em><span class="koboSpan" id="kobo.31.1">); we find the split that minimizes this loss.</span></p>
<p><span class="koboSpan" id="kobo.32.1">For classification, we use either the </span><em><span class="koboSpan" id="kobo.33.1">gini</span></em><span class="koboSpan" id="kobo.34.1"> impurity or cross-entropy as the loss function:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.35.1"><img class="fm-editor-equation" src="assets/51381f73-fa84-40ff-bf3f-6c81da1bea53.png" style="width:13.92em;height:2.00em;"/></span></div>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.36.1"><img class="fm-editor-equation" src="assets/104b5b6f-3a29-4cbd-a8af-358bc8d9e867.png" style="width:17.50em;height:2.00em;"/></span></div>
<p><span class="koboSpan" id="kobo.37.1">In the preceding, </span><em><span class="koboSpan" id="kobo.38.1">c</span><sub><span class="koboSpan" id="kobo.39.1">k</span></sub></em><span class="koboSpan" id="kobo.40.1"> defines the proportion of same class input values present in a particular group. </span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.41.1">Some good resources to learn more about decision trees are as follows:</span><br/>
<ul>
<li><span class="koboSpan" id="kobo.42.1">L. </span><span class="koboSpan" id="kobo.42.2">Breiman, J. </span><span class="koboSpan" id="kobo.42.3">Friedman, R. </span><span class="koboSpan" id="kobo.42.4">Olshen, and C. </span><span class="koboSpan" id="kobo.42.5">Stone: </span><em><span class="koboSpan" id="kobo.43.1">Classification and Regression Trees,</span></em><span class="koboSpan" id="kobo.44.1"> Wadsworth, Belmont, CA, 1984</span></li>
<li><span class="koboSpan" id="kobo.45.1">J.R. </span><span class="koboSpan" id="kobo.45.2">Quinlan: </span><em><span class="koboSpan" id="kobo.46.1">C4. </span><span class="koboSpan" id="kobo.46.2">5: programs for ML,</span></em><span class="koboSpan" id="kobo.47.1"> Morgan Kaufmann, 1993</span></li>
<li><span class="koboSpan" id="kobo.48.1">T. </span><span class="koboSpan" id="kobo.48.2">Hastie, R. </span><span class="koboSpan" id="kobo.48.3">Tibshirani and J. </span><span class="koboSpan" id="kobo.48.4">Friedman: </span><em><span class="koboSpan" id="kobo.49.1">Elements of Statistical Learning</span></em><span class="koboSpan" id="kobo.50.1">, Springer, 2009</span></li>
</ul>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Decision trees in scikit</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The </span><kbd><span class="koboSpan" id="kobo.3.1">scikit</span></kbd><span class="koboSpan" id="kobo.4.1"> library provides </span><kbd><span class="koboSpan" id="kobo.5.1">DecisionTreeRegressor</span></kbd><span class="koboSpan" id="kobo.6.1"> and </span><kbd><span class="koboSpan" id="kobo.7.1">DecisionTreeClassifier</span></kbd><span class="koboSpan" id="kobo.8.1"> to implement regression and classification. </span><span class="koboSpan" id="kobo.8.2">Both can be imported from </span><kbd><span class="koboSpan" id="kobo.9.1">sklearn.tree</span></kbd><span class="koboSpan" id="kobo.10.1">. </span><kbd><span class="koboSpan" id="kobo.11.1">DecisionTreeRegressor</span></kbd><span class="koboSpan" id="kobo.12.1"> is defined as follows:</span></p>
<pre><em><span class="koboSpan" id="kobo.13.1">c</span></em><span class="koboSpan" id="kobo.14.1">lass sklearn.tree.</span><strong><span class="koboSpan" id="kobo.15.1">DecisionTreeRegressor </span></strong><span class="sig-paren"><span class="koboSpan" id="kobo.16.1">(</span></span><span class="koboSpan" id="kobo.17.1">criterion=’mse’</span><span><span class="koboSpan" id="kobo.18.1">, </span></span><span class="koboSpan" id="kobo.19.1">splitter=’best’</span><span><span class="koboSpan" id="kobo.20.1">, </span></span><span class="koboSpan" id="kobo.21.1">max_depth=None</span><span><span class="koboSpan" id="kobo.22.1">, </span></span><span class="koboSpan" id="kobo.23.1">min_samples_split=2</span><span><span class="koboSpan" id="kobo.24.1">, </span></span><span class="koboSpan" id="kobo.25.1">min_samples_leaf=1</span><span><span class="koboSpan" id="kobo.26.1">, </span></span><span class="koboSpan" id="kobo.27.1">min_weight_fraction_leaf=0.0</span><span><span class="koboSpan" id="kobo.28.1">, </span></span><span class="koboSpan" id="kobo.29.1">max_features=None</span><span><span class="koboSpan" id="kobo.30.1">, </span></span><span class="koboSpan" id="kobo.31.1">random_state=None</span><span><span class="koboSpan" id="kobo.32.1">, </span></span><span class="koboSpan" id="kobo.33.1">max_leaf_nodes=None</span><span><span class="koboSpan" id="kobo.34.1">, </span></span><span class="koboSpan" id="kobo.35.1">min_impurity_decrease=0.0</span><span><span class="koboSpan" id="kobo.36.1">, </span></span><span class="koboSpan" id="kobo.37.1">min_impurity_split=None</span><span><span class="koboSpan" id="kobo.38.1">, </span></span><span class="koboSpan" id="kobo.39.1">presort=False</span><span class="sig-paren"><span class="koboSpan" id="kobo.40.1">)</span></span><a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor"/></pre>
<p><span class="koboSpan" id="kobo.41.1">The different arguments are as follows:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.42.1">criterion</span></kbd><span class="koboSpan" id="kobo.43.1">: It defines which loss function to use to determine the split. </span><span class="koboSpan" id="kobo.43.2">The default value is mean square error (</span><kbd><span class="koboSpan" id="kobo.44.1">mse</span></kbd><span class="koboSpan" id="kobo.45.1">). </span><span class="koboSpan" id="kobo.45.2">The library supports the use of </span><kbd><span class="koboSpan" id="kobo.46.1">friedman_mse</span></kbd><span class="koboSpan" id="kobo.47.1"> and mean absolute error (</span><kbd><span class="koboSpan" id="kobo.48.1">mae</span></kbd><span class="koboSpan" id="kobo.49.1">) as loss functions.</span></li>
<li><kbd><span class="koboSpan" id="kobo.50.1">splitter</span></kbd><span class="koboSpan" id="kobo.51.1">: We use this to decide whether to use the greedy strategy and go for the best split (default) or we can use random </span><kbd><span class="koboSpan" id="kobo.52.1">splitter</span></kbd><span class="koboSpan" id="kobo.53.1"> to choose the best random split.</span></li>
<li><kbd><span class="koboSpan" id="kobo.54.1">max_depth</span></kbd><span class="koboSpan" id="kobo.55.1">: It defines the maximum depth of the tree. </span></li>
<li><kbd><span class="koboSpan" id="kobo.56.1">min_samples_split</span></kbd><span class="koboSpan" id="kobo.57.1">: It defines the </span><span><span class="koboSpan" id="kobo.58.1">minimum number of samples required to split an internal node. </span><span class="koboSpan" id="kobo.58.2">It can be integer or float (in this case it defines the percentage of minimum samples needed for the split). </span></span></li>
</ul>
<p><kbd><span class="koboSpan" id="kobo.59.1">DecisionTreeClassifier</span></kbd><span class="koboSpan" id="kobo.60.1"> is defined as follows:</span></p>
<pre><span class="koboSpan" id="kobo.61.1">class sklearn.tree.</span><strong><span class="koboSpan" id="kobo.62.1">DecisionTreeClassifier</span></strong><span class="sig-paren"><span class="koboSpan" id="kobo.63.1">(</span></span><span class="koboSpan" id="kobo.64.1">criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False</span><span class="sig-paren"><span class="koboSpan" id="kobo.65.1">)</span></span><a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"/></pre>
<p><span><span class="koboSpan" id="kobo.66.1">The different arguments are as follows:</span></span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.67.1">criterion</span></kbd><span class="koboSpan" id="kobo.68.1">: It tells which loss function to use to determine the split. </span><span class="koboSpan" id="kobo.68.2">The default value for the classifier is the </span><kbd><span class="koboSpan" id="kobo.69.1">gini</span></kbd><span class="koboSpan" id="kobo.70.1">. </span><span class="koboSpan" id="kobo.70.2">The library supports the use of </span><kbd><span class="koboSpan" id="kobo.71.1">entropy</span></kbd><span class="koboSpan" id="kobo.72.1"> as loss functions.</span></li>
<li><kbd><span class="koboSpan" id="kobo.73.1">splitter</span></kbd><span class="koboSpan" id="kobo.74.1">: We use this to decide how to choose the split (default value is the best split) or we can use random </span><kbd><span class="koboSpan" id="kobo.75.1">splitter</span></kbd><span class="koboSpan" id="kobo.76.1"> to choose the best random split.</span></li>
<li><kbd><span class="koboSpan" id="kobo.77.1">max_depth</span></kbd><span class="koboSpan" id="kobo.78.1">: It defines the maximum depth of the tree. </span><span class="koboSpan" id="kobo.78.2">When the input feature space is large, we use this to restrict the maximum depth and take care of overfitting.</span></li>
<li><kbd><span class="koboSpan" id="kobo.79.1">min_samples_split</span></kbd><span class="koboSpan" id="kobo.80.1">: It defines the </span><span><span class="koboSpan" id="kobo.81.1">minimum number of samples required to split an internal node. </span><span class="koboSpan" id="kobo.81.2">It can be integer or float (in this case it tells the percentage of minimum samples needed for the split). </span></span></li>
</ul>
<div class="packt_infobox"><span class="koboSpan" id="kobo.82.1">We've listed only the commonly used preceding arguments; details regarding the remaining parameters of the two can be read on the scikit-learn website: </span><a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"><span class="koboSpan" id="kobo.83.1">http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html</span></a><span class="koboSpan" id="kobo.84.1"> and </span><a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"><span class="koboSpan" id="kobo.85.1">http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</span></a></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Decision trees in action</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We'll use a decision tree regressor to predict electrical power output first. </span><span class="koboSpan" id="kobo.2.2">The dataset and its description have already been introduced in </span><a href="fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 1</span></a><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Principles and Foundations of IoT and AI</span></em><span class="koboSpan" id="kobo.6.1">. </span><span class="koboSpan" id="kobo.6.2">The code is available at the GitHub </span><span><span class="koboSpan" id="kobo.7.1">repository</span></span><span class="koboSpan" id="kobo.8.1"> in the file named </span><kbd><span class="koboSpan" id="kobo.9.1">ElectricalPowerOutputPredictionUsingDecisionTrees.ipynb</span></kbd><span class="koboSpan" id="kobo.10.1">:</span></p>
<pre><span class="koboSpan" id="kobo.11.1"># Import the modules</span><br/><span class="koboSpan" id="kobo.12.1">import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.13.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.14.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.15.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.16.1">from sklearn.preprocessing import MinMaxScaler</span><br/><span class="koboSpan" id="kobo.17.1">from sklearn.metrics import mean_squared_error, r2_score</span><br/><span class="koboSpan" id="kobo.18.1">from sklearn.model_selection import train_test_split</span><br/><span class="koboSpan" id="kobo.19.1">from sklearn.tree import DecisionTreeRegressor</span><br/><span class="koboSpan" id="kobo.20.1">%matplotlib inline</span><br/><br/><span class="koboSpan" id="kobo.21.1"># Read the data </span><br/><span class="koboSpan" id="kobo.22.1">filename = 'Folds5x2_pp.xlsx' # The file can be downloaded from UCI ML repo</span><br/><span class="koboSpan" id="kobo.23.1">df = pd.read_excel(filename, sheet_name='Sheet1')</span><br/><span class="koboSpan" id="kobo.24.1">df.describe()</span><br/><br/><span class="koboSpan" id="kobo.25.1"># Preprocess the data and split in test/train</span><br/><span class="koboSpan" id="kobo.26.1">X, Y = df[['AT', 'V','AP','RH']], df['PE']</span><br/><span class="koboSpan" id="kobo.27.1">scaler = MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.28.1">X_new = scaler.fit_transform(X)</span><br/><span class="koboSpan" id="kobo.29.1">target_scaler = MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.30.1">Y_new = target_scaler.fit_transform(Y.values.reshape(-1,1))</span><br/><span class="koboSpan" id="kobo.31.1">X_train, X_test, Y_train, y_test = \</span><br/><span class="koboSpan" id="kobo.32.1"> train_test_split(X_new, Y_new, test_size=0.4, random_state=333)</span><br/><br/><br/><span class="koboSpan" id="kobo.33.1"># Define the decision tree regressor</span><br/><span class="koboSpan" id="kobo.34.1">model = DecisionTreeRegressor(max_depth=3)</span><br/><span class="koboSpan" id="kobo.35.1">model.fit(X_train, Y_train)</span><br/><br/><span class="koboSpan" id="kobo.36.1"># Make the prediction over the test data</span><br/><span class="koboSpan" id="kobo.37.1">Y_pred = model.predict(np.float32(X_test))</span><br/><span class="koboSpan" id="kobo.38.1">print("R2 Score is {} and MSE {}".format(\</span><br/><span class="koboSpan" id="kobo.39.1"> r2_score(y_test, Y_pred),\</span><br/><span class="koboSpan" id="kobo.40.1"> mean_squared_error(y_test, Y_pred)))</span></pre>
<p><span class="koboSpan" id="kobo.41.1">We get an R-square value of 0.90 and mean square error of 0.0047 on the test data; it's a significant improvement over the prediction results obtained using linear regressor (R-square: </span><kbd><span class="koboSpan" id="kobo.42.1">0.77</span></kbd><span class="koboSpan" id="kobo.43.1">;mse: </span><kbd><span class="koboSpan" id="kobo.44.1">0.012</span></kbd><span class="koboSpan" id="kobo.45.1">).</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.46.1">Let's also see the performance of decision trees in the classification task; we use it for the wine quality classification as before. </span><span class="koboSpan" id="kobo.46.2">The code is available in the </span><kbd><span class="koboSpan" id="kobo.47.1">Wine_quality_using_DecisionTrees.ipynb</span></kbd><span class="koboSpan" id="kobo.48.1"> file in the GitHub repository:</span></p>
<pre><span class="koboSpan" id="kobo.49.1"># Import the modules</span><br/><span class="koboSpan" id="kobo.50.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.51.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.52.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.53.1">from sklearn.preprocessing import MinMaxScaler, LabelEncoder</span><br/><span class="koboSpan" id="kobo.54.1">from sklearn.metrics import mean_squared_error, r2_score</span><br/><span class="koboSpan" id="kobo.55.1">from sklearn.model_selection import train_test_split</span><br/><span class="koboSpan" id="kobo.56.1">from sklearn.tree import DecisionTreeClassifier</span><br/><span class="koboSpan" id="kobo.57.1">%matplotlib inline</span><br/><br/><span class="koboSpan" id="kobo.58.1"># Read the data </span><br/><span class="n"><span class="koboSpan" id="kobo.59.1">filename</span></span> <span class="o"><span class="koboSpan" id="kobo.60.1">=</span></span> <span class="s1"><span class="koboSpan" id="kobo.61.1">'winequality-red.csv'</span></span> <span class="c1"><span class="koboSpan" id="kobo.62.1">#Download the file from https://archive.ics.uci.edu/ml/datasets/wine+quality</span></span> <span class="n"><span class="koboSpan" id="kobo.63.1">df</span></span> <span class="o"><span class="koboSpan" id="kobo.64.1">=</span></span> <span class="n"><span class="koboSpan" id="kobo.65.1">pd</span></span><span class="o"><span class="koboSpan" id="kobo.66.1">.</span></span><span class="n"><span class="koboSpan" id="kobo.67.1">read_csv</span></span><span class="p"><span class="koboSpan" id="kobo.68.1">(</span></span><span class="n"><span class="koboSpan" id="kobo.69.1">filename</span></span><span class="p"><span class="koboSpan" id="kobo.70.1">,</span></span> <span class="n"><span class="koboSpan" id="kobo.71.1">sep</span></span><span class="o"><span class="koboSpan" id="kobo.72.1">=</span></span><span class="s1"><span class="koboSpan" id="kobo.73.1">';'</span></span><span class="p"><span class="koboSpan" id="kobo.74.1">)</span><br/><br/></span><br/><br/><span class="koboSpan" id="kobo.75.1"># categorize the data into three classes</span><br/><span class="koboSpan" id="kobo.76.1">bins = (0,3.5,5.5,10)</span><br/><span class="koboSpan" id="kobo.77.1">categories = pd.cut(df['quality'], bins, labels = ['bad','ok','good'])</span><br/><span class="koboSpan" id="kobo.78.1">df['quality'] = categories</span><br/><br/><span class="c1"><span class="koboSpan" id="kobo.79.1"># Preprocessing and splitting data to X and y</span></span> <span class="n"><span class="koboSpan" id="kobo.80.1">X</span></span> <span class="o"><span class="koboSpan" id="kobo.81.1">=</span></span> <span class="n"><span class="koboSpan" id="kobo.82.1">df</span></span><span class="o"><span class="koboSpan" id="kobo.83.1">.</span></span><span class="n"><span class="koboSpan" id="kobo.84.1">drop</span></span><span class="p"><span class="koboSpan" id="kobo.85.1">([</span></span><span class="s1"><span class="koboSpan" id="kobo.86.1">'quality'</span></span><span class="p"><span class="koboSpan" id="kobo.87.1">],</span></span> <span class="n"><span class="koboSpan" id="kobo.88.1">axis</span></span> <span class="o"><span class="koboSpan" id="kobo.89.1">=</span></span> <span class="mi"><span class="koboSpan" id="kobo.90.1">1</span></span><span class="p"><span class="koboSpan" id="kobo.91.1">)</span></span> <span class="n"><span class="koboSpan" id="kobo.92.1">scaler</span></span> <span class="o"><span class="koboSpan" id="kobo.93.1">=</span></span> <span class="n"><span class="koboSpan" id="kobo.94.1">MinMaxScaler</span></span><span class="p"><span class="koboSpan" id="kobo.95.1">()</span></span> <span class="n"><span class="koboSpan" id="kobo.96.1">X_new</span></span> <span class="o"><span class="koboSpan" id="kobo.97.1">=</span></span> <span class="n"><span class="koboSpan" id="kobo.98.1">scaler</span></span><span class="o"><span class="koboSpan" id="kobo.99.1">.</span></span><span class="n"><span class="koboSpan" id="kobo.100.1">fit_transform</span></span><span class="p"><span class="koboSpan" id="kobo.101.1">(</span></span><span class="n"><span class="koboSpan" id="kobo.102.1">X</span></span><span class="p"><span class="koboSpan" id="kobo.103.1">)</span></span> <span class="n"><span class="koboSpan" id="kobo.104.1">y</span></span> <span class="o"><span class="koboSpan" id="kobo.105.1">=</span></span> <span class="n"><span class="koboSpan" id="kobo.106.1">df</span></span><span class="p"><span class="koboSpan" id="kobo.107.1">[</span></span><span class="s1"><span class="koboSpan" id="kobo.108.1">'quality'</span></span><span class="p"><span class="koboSpan" id="kobo.109.1">]</span></span> <span class="kn"><span class="koboSpan" id="kobo.110.1">from</span></span> <span class="nn"><span class="koboSpan" id="kobo.111.1">sklearn.preprocessing</span></span> <span class="k"><span class="koboSpan" id="kobo.112.1">import</span></span> <span class="n"><span class="koboSpan" id="kobo.113.1">LabelEncoder</span></span> <span class="n"><span class="koboSpan" id="kobo.114.1">labelencoder_y</span></span> <span class="o"><span class="koboSpan" id="kobo.115.1">=</span></span> <span class="n"><span class="koboSpan" id="kobo.116.1">LabelEncoder</span></span><span class="p"><span class="koboSpan" id="kobo.117.1">()</span></span> <span class="n"><span class="koboSpan" id="kobo.118.1">y</span></span> <span class="o"><span class="koboSpan" id="kobo.119.1">=</span></span> <span class="n"><span class="koboSpan" id="kobo.120.1">labelencoder_y</span></span><span class="o"><span class="koboSpan" id="kobo.121.1">.</span></span><span class="n"><span class="koboSpan" id="kobo.122.1">fit_transform</span></span><span class="p"><span class="koboSpan" id="kobo.123.1">(</span></span><span class="n"><span class="koboSpan" id="kobo.124.1">y</span></span><span class="p"><span class="koboSpan" id="kobo.125.1">)</span></span> <span class="n"><span class="koboSpan" id="kobo.126.1">X_train</span></span><span class="p"><span class="koboSpan" id="kobo.127.1">,</span></span> <span class="n"><span class="koboSpan" id="kobo.128.1">X_test</span></span><span class="p"><span class="koboSpan" id="kobo.129.1">,</span></span> <span class="n"><span class="koboSpan" id="kobo.130.1">y_train</span></span><span class="p"><span class="koboSpan" id="kobo.131.1">,</span></span> <span class="n"><span class="koboSpan" id="kobo.132.1">y_test</span></span> <span class="o"><span class="koboSpan" id="kobo.133.1">=</span></span> <span class="n"><span class="koboSpan" id="kobo.134.1">train_test_split</span></span><span class="p"><span class="koboSpan" id="kobo.135.1">(</span></span><span class="n"><span class="koboSpan" id="kobo.136.1">X</span></span><span class="p"><span class="koboSpan" id="kobo.137.1">,</span></span> <span class="n"><span class="koboSpan" id="kobo.138.1">y</span></span><span class="p"><span class="koboSpan" id="kobo.139.1">,</span></span> <span class="n"><span class="koboSpan" id="kobo.140.1">test_size</span></span> <span class="o"><span class="koboSpan" id="kobo.141.1">=</span></span> <span class="mf"><span class="koboSpan" id="kobo.142.1">0.2</span></span><span class="p"><span class="koboSpan" id="kobo.143.1">,</span></span> <span class="n"><span class="koboSpan" id="kobo.144.1">random_state</span></span> <span class="o"><span class="koboSpan" id="kobo.145.1">=</span></span> <span class="mi"><span class="koboSpan" id="kobo.146.1">323</span></span><span class="p"><span class="koboSpan" id="kobo.147.1">)</span><br/></span><br/><br/><span class="koboSpan" id="kobo.148.1"># Define the decision tree classifier</span><br/><span class="koboSpan" id="kobo.149.1">classifier = DecisionTreeClassifier(max_depth=3)</span><br/><span class="koboSpan" id="kobo.150.1">classifier.fit(X_train, y_train)</span><br/><br/><span class="koboSpan" id="kobo.151.1"># Make the prediction over the test data</span><br/><span class="koboSpan" id="kobo.152.1">Y_pred = classifier.predict(np.float32(X_test))</span><br/><span class="koboSpan" id="kobo.153.1">print("Accuracy is {}".format(accuracy_score(y_test, y_pred)))</span></pre>
<p><span class="koboSpan" id="kobo.154.1">The decision tree generates a classification accuracy of around 70%. </span><span class="koboSpan" id="kobo.154.2">We can see that, for small data size, we can use both decision trees and Naive Bayes with almost equal success. </span><span class="koboSpan" id="kobo.154.3">Decision trees suffer from overfitting, which can be taken care of by restricting the maximum depth or setting a minimum number of training inputs. </span><span class="koboSpan" id="kobo.154.4">They, like Naive Bayes, are unstable—a little variation in the data can result in a completely different tree; this can be resolved by making use of bagging and boosting techniques. </span><span class="koboSpan" id="kobo.154.5">Last, but not least, since it's a greedy algorithm, there's no guarantee that it returns a globally optimal solution.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Ensemble learning</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In our daily life, when we have to make a decision, we take guidance not from one person, but from many individuals whose wisdom we trust. </span><span class="koboSpan" id="kobo.2.2">The same can be applied in ML; instead of depending upon one single model, we can use a group of models (ensemble) to make a prediction or classification decision. </span><span class="koboSpan" id="kobo.2.3">This form of learning is called </span><strong><span class="koboSpan" id="kobo.3.1">ensemble learning</span></strong><span class="koboSpan" id="kobo.4.1">. </span></p>
<p><span class="koboSpan" id="kobo.5.1">Conventionally, ensemble learning is used as the last step in many ML projects. </span><span class="koboSpan" id="kobo.5.2">It works best when the models are as independent of one another as possible. </span><span class="koboSpan" id="kobo.5.3">The following diagram gives a graphical representation of ensemble learning:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.6.1"><img class="aligncenter size-full wp-image-1087 image-border" src="assets/73af0f0f-00be-4082-a4a5-7d4540b67e24.png" style="width:39.17em;height:19.83em;"/></span></div>
<p><span class="koboSpan" id="kobo.7.1">The training of different models can take place either sequentially or in parallel. </span><span class="koboSpan" id="kobo.7.2">There are various ways to implement ensemble learning: voting, bagging and pasting, and random forest. </span><span class="koboSpan" id="kobo.7.3">Let's see what each of these techniques and how we can implement them.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Voting classifier</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The voting classifier follows the majority; it aggregates the prediction of all the classifiers and chooses the class with maximum votes. </span><span class="koboSpan" id="kobo.2.2">For example, in the following screenshot, the voting classifier will predict the input instance to belong to class </span><strong><span class="koboSpan" id="kobo.3.1">1</span></strong><span class="koboSpan" id="kobo.4.1">:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.5.1"><img src="assets/2f60022e-2236-4291-a411-268ed5152367.png" style="width:38.00em;height:26.75em;"/></span></div>
<p><span class="koboSpan" id="kobo.6.1">scikit has the </span><kbd><span class="koboSpan" id="kobo.7.1">VotingClassifier</span></kbd><span class="koboSpan" id="kobo.8.1"> class to implement this. </span><span class="koboSpan" id="kobo.8.2">Using ensemble learning on wine quality classification, we reach an accuracy score of 74%, higher than any of the models considered alone. </span><span class="koboSpan" id="kobo.8.3">The complete code is in the </span><kbd><span class="koboSpan" id="kobo.9.1">Wine_quality_using_Ensemble_learning.ipynb</span></kbd> <span class="filename"><span class="koboSpan" id="kobo.10.1">file. </span><span class="koboSpan" id="kobo.10.2">The following is the main code to perform ensemble learning using voting:</span></span></p>
<pre><span class="koboSpan" id="kobo.11.1"># import the different classifiers</span><br/><span class="koboSpan" id="kobo.12.1">from sklearn.svm import SVC </span><br/><span class="koboSpan" id="kobo.13.1">from sklearn.naive_bayes import GaussianNB </span><br/><span class="koboSpan" id="kobo.14.1">from sklearn.tree import DecisionTreeClassifier</span><br/><span class="koboSpan" id="kobo.15.1">from sklearn.ensemble import VotingClassifier</span><br/><br/><span class="koboSpan" id="kobo.16.1"># Declare each classifier</span><br/><span class="koboSpan" id="kobo.17.1">clf1 = SVC(random_state=22)</span><br/><span class="koboSpan" id="kobo.18.1">clf2 = DecisionTreeClassifier(random_state=23)</span><br/><span class="koboSpan" id="kobo.19.1">clf3 = GaussianNB()</span><br/><span class="koboSpan" id="kobo.20.1">X = np.array(X_train)</span><br/><span class="koboSpan" id="kobo.21.1">y = np.array(y_train)</span><br/><br/><span class="koboSpan" id="kobo.22.1">#Employ Ensemble learning</span><br/><span class="koboSpan" id="kobo.23.1">eclf = VotingClassifier(estimators=[</span><br/><span class="koboSpan" id="kobo.24.1">('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')</span><br/><span class="koboSpan" id="kobo.25.1">eclf = eclf.fit(X, y)</span><br/><br/><span class="koboSpan" id="kobo.26.1"># Make prediction on test data</span><br/><span class="koboSpan" id="kobo.27.1">y_pred = eclf.predict(X_test)</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Bagging and pasting</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In voting, we used different algorithms for training on the same dataset. </span><span class="koboSpan" id="kobo.2.2">We can also achieve ensemble learning by using different models with the same learning algorithm, but we train them on different training data subsets. </span><span class="koboSpan" id="kobo.2.3">The training subset is sampled randomly. </span><span class="koboSpan" id="kobo.2.4">The sampling can be done with replacement (bagging) or without replacement (pasting):</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">Bagging</span></strong><span class="koboSpan" id="kobo.4.1">: In it, additional data for training is generated from the original dataset using combinations with repetitions. </span><span class="koboSpan" id="kobo.4.2">This helps in decreasing the variance of different models.</span></li>
<li><strong><span class="koboSpan" id="kobo.5.1">Pasting</span></strong><span class="koboSpan" id="kobo.6.1">: Since pasting is without replacement, each subset of the training data can be used at most once. </span><span class="koboSpan" id="kobo.6.2">It's more suitable if the original dataset is large. </span></li>
</ul>
<p><span class="koboSpan" id="kobo.7.1">The </span><kbd><span class="koboSpan" id="kobo.8.1">scikit</span></kbd><span class="koboSpan" id="kobo.9.1"> library has a method for performing bagging and pasting; from </span><kbd><span class="koboSpan" id="kobo.10.1">sklearn.ensemble</span></kbd><span class="koboSpan" id="kobo.11.1">, we can import </span><kbd><span class="koboSpan" id="kobo.12.1">BaggingClassifier</span></kbd><span class="koboSpan" id="kobo.13.1"> and use it. </span><span class="koboSpan" id="kobo.13.2">The following code estimates </span><kbd><span class="koboSpan" id="kobo.14.1">500</span></kbd><span class="koboSpan" id="kobo.15.1"> decision tree classifiers, each with </span><kbd><span class="koboSpan" id="kobo.16.1">1000</span></kbd><span class="koboSpan" id="kobo.17.1"> training samples using bagging (for pasting, keep </span><kbd><span class="koboSpan" id="kobo.18.1">bootstrap=False</span></kbd><span class="koboSpan" id="kobo.19.1">):</span></p>
<pre><span class="koboSpan" id="kobo.20.1">from sklearn.ensemble import BaggingClassifier</span><br/><span class="koboSpan" id="kobo.21.1">bag_classifier = BaggingClassifier(</span><br/><span class="koboSpan" id="kobo.22.1">        DecisionTreeClassifier(), n_estimators=500, max_samples=1000,\</span><br/><span class="koboSpan" id="kobo.23.1">        bootstrap=True, n_jobs=-1)    </span><br/><span class="koboSpan" id="kobo.24.1">bag_classifier.fit(X_train, y_train)</span><br/><span class="koboSpan" id="kobo.25.1">y_pred = bag_classifier.predict(X_test)</span></pre>
<p class="mce-root"><span class="koboSpan" id="kobo.26.1">This results in an accuracy of 77% for wine quality classification. </span><span class="koboSpan" id="kobo.26.2">The last argument to </span><kbd><span class="koboSpan" id="kobo.27.1">BaggingClassifier</span></kbd><span class="koboSpan" id="kobo.28.1">, </span><kbd><span class="koboSpan" id="kobo.29.1">n_jobs</span></kbd><span class="koboSpan" id="kobo.30.1">, defines how many CPU cores to use (that's the number of jobs to run in parallel); when its value is set to </span><kbd><span class="koboSpan" id="kobo.31.1">-1</span></kbd><span class="koboSpan" id="kobo.32.1">, then it uses all of the available CPU cores.</span></p>
<p class="mce-root"/>
<div class="mce-root packt_tip"><span class="koboSpan" id="kobo.33.1">An ensemble of only decision trees is called </span><strong><span class="koboSpan" id="kobo.34.1">random forest</span></strong><span class="koboSpan" id="kobo.35.1">. </span><span class="koboSpan" id="kobo.35.2">And so what we've implemented previously is a random forest. </span><span class="koboSpan" id="kobo.35.3">We can directly implement random forest in scikit using the </span><kbd><span class="koboSpan" id="kobo.36.1">RandomForestClassifier</span></kbd><span class="koboSpan" id="kobo.37.1"> class. </span><span class="koboSpan" id="kobo.37.2">The advantage of using the class is that it introduces extra randomness while building the tree. </span><span class="koboSpan" id="kobo.37.3">While splitting, it searches for the best feature to split among a random subset of features. </span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Improving your model – tips and tricks</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this chapter, we've learned a large number of ML algorithms, each with its own pros and cons. </span><span class="koboSpan" id="kobo.2.2">In this section, we'll look into some common problems and ways to resolve them.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Feature scaling to resolve uneven data scale</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The data that's collected normally doesn't have the same scale; for example, one feature may be varying in the range 10–100 and another one may be only distributed in range 2</span><span><span class="koboSpan" id="kobo.3.1">–</span></span><span class="koboSpan" id="kobo.4.1">5. </span><span class="koboSpan" id="kobo.4.2">This uneven data scale can have an adverse effect on learning. </span><span class="koboSpan" id="kobo.4.3">To resolve this, we use the method of feature scaling (normalization). </span><span class="koboSpan" id="kobo.4.4">The choice of normalization has been found to drastically affect the performance of certain algorithms. </span><span class="koboSpan" id="kobo.4.5">Two common normalization methods (also called standardization in some books) are as follows:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.5.1">Z-score normalization</span></strong><span class="koboSpan" id="kobo.6.1">: In z-score normalization, each individual feature is scaled so that it has the properties of a standard normal distribution, that is, a mean of </span><em><span class="koboSpan" id="kobo.7.1">0</span></em><span class="koboSpan" id="kobo.8.1"> and variance of </span><em><span class="koboSpan" id="kobo.9.1">1</span></em><span class="koboSpan" id="kobo.10.1">. </span><span class="koboSpan" id="kobo.10.2">If </span><em><span class="koboSpan" id="kobo.11.1">μ</span></em><span class="koboSpan" id="kobo.12.1"> is the mean and </span><em><span class="koboSpan" id="kobo.13.1">σ</span></em><span class="koboSpan" id="kobo.14.1"> the variance, we can compute Z-score normalization by making the following linear transformation on each feature as follows: </span></li>
</ul>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.15.1"><img class="fm-editor-equation" src="assets/40dddd5e-f8af-4ab1-8b59-5a708d51b61b.png" style="width:9.83em;height:3.00em;"/></span></div>
<ul>
<li><span><strong><span class="koboSpan" id="kobo.16.1">Min-max normalization</span></strong><span class="koboSpan" id="kobo.17.1">: The min-max normalization rescales the input features so that they lie in the range between </span><em><span class="koboSpan" id="kobo.18.1">0</span></em><span class="koboSpan" id="kobo.19.1"> and </span><em><span class="koboSpan" id="kobo.20.1">1</span></em><span class="koboSpan" id="kobo.21.1">. </span><span class="koboSpan" id="kobo.21.2">It results in reducing the standard deviation in the data and hence suppresses the effect of outliers. </span><span class="koboSpan" id="kobo.21.3">To achieve min-max normalization, we find the maximum and minimum value of the feature (</span><em><span class="koboSpan" id="kobo.22.1">x</span><sub><span class="koboSpan" id="kobo.23.1">max</span></sub></em><span class="koboSpan" id="kobo.24.1"> and </span><em><span class="koboSpan" id="kobo.25.1">x</span><sub><span class="koboSpan" id="kobo.26.1">min</span></sub></em><span class="koboSpan" id="kobo.27.1"> respectively), and perform the following linear transformation:</span></span></li>
</ul>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.28.1"><img class="fm-editor-equation" src="assets/724c7e3c-3d4d-4643-a5b5-bf3c29c82b30.png" style="width:12.42em;height:3.17em;"/></span></div>
<p><span class="koboSpan" id="kobo.29.1">We can use the </span><kbd><span class="koboSpan" id="kobo.30.1">scikit</span></kbd><span class="koboSpan" id="kobo.31.1"> library </span><kbd><span class="koboSpan" id="kobo.32.1">StandardScaler</span></kbd><span class="koboSpan" id="kobo.33.1"> or </span><kbd><span class="koboSpan" id="kobo.34.1">MinMaxscaler</span></kbd><span class="koboSpan" id="kobo.35.1"> methods to normalize the data. </span><span class="koboSpan" id="kobo.35.2">In all of the examples in this chapter, we've used </span><kbd><span class="koboSpan" id="kobo.36.1">MinMaxScaler</span></kbd><span class="koboSpan" id="kobo.37.1">; you can try changing it to </span><kbd><span class="koboSpan" id="kobo.38.1">StandardScalar</span></kbd><span class="koboSpan" id="kobo.39.1"> and observe if the performance changes. </span><span class="koboSpan" id="kobo.39.2">In the next chapter, we'll also learn how to perform these normalizations in TensorFlow. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Overfitting</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Sometimes the model tries to overfit the training dataset; in doing so, it loses its ability to generalize and hence performs badly on the validation dataset; this in turn will affect its performance on unseen data values. </span><span class="koboSpan" id="kobo.2.2">There are two standard ways to take care of overfitting: regularization and cross-validation.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Regularization</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">Regularization adds a term in the loss function to ensure that the cost increases as the model increases the number of features. </span><span class="koboSpan" id="kobo.2.2">Hence, we force the model to stay simpler. </span><span class="koboSpan" id="kobo.2.3">If </span><em><span class="koboSpan" id="kobo.3.1">L(X</span></em><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Y)</span></em><span class="koboSpan" id="kobo.6.1"> was the loss function earlier, we replace it with the following:</span></span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1"><img class="fm-editor-equation" src="assets/7c6cd641-a45d-4cb1-abc4-902e2e23e1ac.png" style="width:15.58em;height:1.50em;"/></span></div>
<p><span class="koboSpan" id="kobo.8.1">In the preceding, </span><em><span class="koboSpan" id="kobo.9.1">N</span></em><span class="koboSpan" id="kobo.10.1"> can be </span><em><span class="koboSpan" id="kobo.11.1">L</span></em><sub><span class="koboSpan" id="kobo.12.1">1</span></sub><span class="koboSpan" id="kobo.13.1"> norm, </span><em><span class="koboSpan" id="kobo.14.1">L</span></em><sub><span class="koboSpan" id="kobo.15.1">2</span></sub><span class="koboSpan" id="kobo.16.1"> norm, or a combination of the two, and </span><em><span class="koboSpan" id="kobo.17.1">λ</span></em><span class="koboSpan" id="kobo.18.1"> is the regularization coefficient. </span><span class="koboSpan" id="kobo.18.2">Regularization helps in reducing the model variance, without losing any important properties of the data distribution:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.19.1">Lasso regularization</span></strong><span class="koboSpan" id="kobo.20.1">: In this case, the </span><em><span class="koboSpan" id="kobo.21.1">N</span></em><span class="koboSpan" id="kobo.22.1"> is </span><em><span class="koboSpan" id="kobo.23.1">L</span></em><sub><span class="koboSpan" id="kobo.24.1">1</span></sub><span class="koboSpan" id="kobo.25.1"> norm. </span><span class="koboSpan" id="kobo.25.2">It uses the modulus of weight as the penalty term </span><em><span class="koboSpan" id="kobo.26.1">N:</span></em></li>
</ul>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.27.1"><img class="fm-editor-equation" src="assets/76d82420-39b8-4956-bc07-a4fc8d6c1000.png" style="width:9.08em;height:3.75em;"/></span></div>
<ul>
<li><strong><span class="koboSpan" id="kobo.28.1">Ridge regularization</span></strong><span class="koboSpan" id="kobo.29.1">: In this case, the </span><em><span class="koboSpan" id="kobo.30.1">N</span></em><span class="koboSpan" id="kobo.31.1"> is </span><em><span class="koboSpan" id="kobo.32.1">L2</span></em><span class="koboSpan" id="kobo.33.1"> norm, given by the following:</span></li>
</ul>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.34.1"><img class="fm-editor-equation" src="assets/c03bc8ca-e933-4a96-bd00-4bfc0c492b73.png" style="width:9.00em;height:3.92em;"/></span></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Cross-validation</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Using cross-validation can also help in reducing the problem of overfitting. </span><span class="koboSpan" id="kobo.2.2">In </span><em><span class="koboSpan" id="kobo.3.1">k</span></em><span class="koboSpan" id="kobo.4.1">-fold cross-validation, data is divided into </span><em><span class="koboSpan" id="kobo.5.1">k</span></em><span class="koboSpan" id="kobo.6.1">-subsets, called </span><strong><span class="koboSpan" id="kobo.7.1">folds</span></strong><span class="koboSpan" id="kobo.8.1">. </span><span class="koboSpan" id="kobo.8.2">Then it trains and evaluates the model </span><em><span class="koboSpan" id="kobo.9.1">k</span></em><span class="koboSpan" id="kobo.10.1">-times; each time, it picks one of the folds for validation and the rest for training the model. </span><span class="koboSpan" id="kobo.10.2">We can perform the cross-validation when the data is less and training time is small. </span><span class="koboSpan" id="kobo.10.3">scikit provides a </span><span><kbd><span class="koboSpan" id="kobo.11.1">cross_val_score</span></kbd><span class="koboSpan" id="kobo.12.1"> method to implement the k-folds. </span><span class="koboSpan" id="kobo.12.2">Let </span><kbd><span class="koboSpan" id="kobo.13.1">classifier</span></kbd><span class="koboSpan" id="kobo.14.1"> be the model we want to cross-validate, then we can use the following code to perform cross-validation on </span><kbd><span class="koboSpan" id="kobo.15.1">10</span></kbd><span class="koboSpan" id="kobo.16.1"> folds:</span></span></p>
<pre><span class="koboSpan" id="kobo.17.1">from sklearn.model_selection import cross_val_score </span><br/><span class="koboSpan" id="kobo.18.1">accuracies = cross_val_score(estimator = classifier, X = X_train,\</span><br/><span class="koboSpan" id="kobo.19.1">     y = y_train, cv = 10)</span><br/><span class="koboSpan" id="kobo.20.1">print("Accuracy Mean {} Accuracy Variance \</span><br/><span class="koboSpan" id="kobo.21.1">     {}".format(accuracies.mean(),accuracies.std()))</span></pre>
<p><span class="koboSpan" id="kobo.22.1">The result of this is an average mean and variance value. </span><span class="koboSpan" id="kobo.22.2">A good model should have a high average and low variance.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">No Free Lunch theorem</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">With so many models, one always wonders which one to use. </span><span class="koboSpan" id="kobo.2.2">Wolpert, in his famous paper </span><span><em><span class="koboSpan" id="kobo.3.1">The Lack of A Priori Distinctions Between Learning</span></em><span class="koboSpan" id="kobo.4.1">, explored this issue and showed that if we make no prior assumption about the input data, then there's no reason to prefer one model over any other. </span><span class="koboSpan" id="kobo.4.2">This is known as the </span><strong><span class="koboSpan" id="kobo.5.1">No Free Lunch</span></strong> <strong><span class="koboSpan" id="kobo.6.1">theorem</span></strong><span class="koboSpan" id="kobo.7.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.8.1">This means that there's no model hat can be </span><em><span class="koboSpan" id="kobo.9.1">a</span></em><span class="koboSpan" id="kobo.10.1"> priori guaranteed to work better. </span><span class="koboSpan" id="kobo.10.2">The only way we can ascertain which model is best is by evaluating them all. </span><span class="koboSpan" id="kobo.10.3">But, practically, it isn't possible to evaluate all of the models and so, in practice, we make reasonable assumptions about the data and evaluate a few relevant models.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Hyperparameter tuning and grid search</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Different models have different hyperparameters; for example, in linear regressor, the learning rate was a hyperparameter; if we're using regularization, then the regularizing parameter λ is a hyperparameter. </span><span class="koboSpan" id="kobo.2.2">What should be their value? </span><span class="koboSpan" id="kobo.2.3">While there's a rule of thumb for some hyperparameters, most of the time we make either a guess or use grid search to perform a sequential search for the best hyperparameters. </span><span class="koboSpan" id="kobo.2.4">In the following, we present the code to perform hyperparameter search in the case of SVM using the </span><kbd><span class="koboSpan" id="kobo.3.1">scikit</span></kbd><span class="koboSpan" id="kobo.4.1"> library; in the next chapter, we'll see how we can use TensorFlow to perform hyperparameter tuning:</span></p>
<pre><span class="koboSpan" id="kobo.5.1">Grid search for best model and parameters</span><br/><span class="koboSpan" id="kobo.6.1">from sklearn.model_selection import GridSearchCV</span><br/><span class="koboSpan" id="kobo.7.1">#parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}</span><br/><span class="koboSpan" id="kobo.8.1">classifier = SVC()</span><br/><span class="koboSpan" id="kobo.9.1">parameters = [{'C': [1, 10], 'kernel': ['linear']},</span><br/><span class="koboSpan" id="kobo.10.1">    {'C': [1, 10], 'kernel': ['rbf'],</span><br/><span class="koboSpan" id="kobo.11.1">    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]</span><br/><span class="koboSpan" id="kobo.12.1">    grid_search = GridSearchCV(estimator = classifier,</span><br/><span class="koboSpan" id="kobo.13.1">    param_grid = parameters,</span><br/><span class="koboSpan" id="kobo.14.1">    scoring = 'accuracy',</span><br/><span class="koboSpan" id="kobo.15.1">    cv = 10,)</span><br/><span class="koboSpan" id="kobo.16.1">grid_search.fit(X_train, y_train)</span><br/><span class="koboSpan" id="kobo.17.1">best_accuracy = grid_search.best_score_</span><br/><span class="koboSpan" id="kobo.18.1">best_parameters = grid_search.best_params_</span><br/><span class="koboSpan" id="kobo.19.1">#here is the best accuracy</span><br/><span class="koboSpan" id="kobo.20.1">best_accuracy</span></pre>
<div class="page">
<div class="section">
<div class="layoutArea">
<div class="column">
<p><kbd><span class="koboSpan" id="kobo.21.1">GridSearchCV</span></kbd><span class="koboSpan" id="kobo.22.1"> will provide us with the hyperparameters that produce the best results for the SVM classifier.</span></p>
</div>
</div>
</div>
</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The goal of this chapter was to provide you with  intuitive understanding of different standard ML algorithms so that you can make an informed choice. </span><span class="koboSpan" id="kobo.2.2">We covered the popular ML algorithms used for classification and regression.We also learnt how supervised and unsupervised learning are different from each other. </span><span class="koboSpan" id="kobo.2.3">Linear regression, logistic regression, SVM, Naive Bayes, and decision trees were introduced along with the fundamental principles involved in each. </span><span class="koboSpan" id="kobo.2.4">We used the regression methods to predict electrical power production of a thermal station and classification methods to classify wine as good or bad. </span><span class="koboSpan" id="kobo.2.5">Lastly, we covered the common problems with different ML algorithms and some tips and tricks to solve them. </span></p>
<p><span class="koboSpan" id="kobo.3.1">In the next chapter, we'll study different deep learning models and learn how to use them to analyze our data and make predictions. </span></p>


            </article>

            
        </section>
    </body></html>