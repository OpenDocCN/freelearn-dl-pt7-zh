["```py\nimport sagemaker\nfrom sagemaker import get_execution_role\nsess = sagemaker.Session()\nrole = get_execution_role()\nrole_name = role[role.rfind('/') + 1:]\nprint(role_name)\n```", "```py\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n            \"Service\": [\n              \"sagemaker.amazonaws.com\",\n              \"glue.amazonaws.com\"\n            ]\n          },\n          \"Action\": \"sts:AssumeRole\"\n        }\n      ]\n    }\n    ```", "```py\n    import boto3\n    import botocore\n    import json\n    import time\n    import os\n                  import project_path\n    from lib import workshop\n    glue = boto3.client('glue')\n    s3 = boto3.resource('s3')\n    s3_client = boto3.client('s3')\n    session = boto3.session.Session()\n    region = session.region_name\n    account_id = boto3.client('sts').get_caller_identity().get('Account')\n    database_name = 'yelp' # AWS Glue Data Catalog Database Name\n    raw_table_name = 'raw_reviews' # AWS Glue Data Catalog raw table name\n    parquet_table_name = 'parq_reviews' # AWS Glue Data Catalog parquet table name\n    open_data_bucket = 'fast-ai-nlp'\n    ```", "```py\n    try:\n        s3.Bucket(open_data_bucket).download_file('yelp_review_full_csv.tgz', 'yelp_review_full_csv.tgz')\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == \"404\":\n            print(\"The object does not exist.\")\n        else:\n            raise\n    ```", "```py\n    !tar -xvzf yelp_review_full_csv.tgz\n    ```", "```py\n    import pandas as pd\n    pd.set_option('display.max_colwidth', -1)\n    df = pd.read_csv('yelp_review_full_csv/train.csv', header=None)\n    df.head(5)\n    ```", "```py\n    file_name = 'train.csv'\n    session.resource('s3').Bucket(bucket).Object(os.path.join('yelp', 'raw', file_name)).upload_file('yelp_review_full_csv/'+file_name)\n    ```", "```py\n    workshop.create_db(glue, account_id, database_name, 'Database for Yelp Reviews')\n    ```", "```py\n    location = 's3://{0}/yelp/raw'.format(bucket)\n    response = glue.create_table(\n        CatalogId=account_id,\n        DatabaseName=database_name,\n        TableInput={\n            'Name': raw_table_name,\n            'Description': 'Raw Yelp reviews dataset',\n            'StorageDescriptor': {\n                'Columns': [\n                    {\n                        'Name': 'rating',\n                        'Type': 'tinyint',\n                        'Comment': 'Rating of from the Yelp review'\n                    },\n                    {\n                        'Name': 'review',\n                        'Type': 'string',\n                        'Comment': 'Review text of from the Yelp review'\n                    }\n                ],\n                'Location': location,\n                'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n                'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n                'SerdeInfo': {\n                    'SerializationLibrary': 'org.apache.hadoop.hive.serde2.OpenCSVSerde',\n                    'Parameters': {\n                        'escapeChar': '\\\\',\n                        'separatorChar': ',',\n                        'serialization.format': '1'\n                    }\n                },\n            },\n            'TableType': 'EXTERNAL_TABLE',\n            'Parameters': {\n                'classification': 'csv'\n            }\n        }\n    )\n    ```", "```py\n    !pip install PyAthena\n    ```", "```py\n    from pyathena import connect\n    from pyathena.pandas.util import as_pandas\n    cursor = connect(region_name=region, s3_staging_dir='s3://'+bucket+'/yelp/temp').cursor()\n    cursor.execute('select * from ' + database_name + '.' + raw_table_name + ' limit 10')\n    df = as_pandas(cursor)\n    df.head(5)\n    ```", "```py\n    %%writefile yelp_etl.py\n    import os\n    import sys\n    import boto3\n    from awsglue.transforms import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.context import SparkContext\n    from awsglue.context import GlueContext\n    from awsglue.job import Job\n    from awsglue.dynamicframe import DynamicFrame\n    import pyspark.sql.functions as F\n    from pyspark.sql import Row, Window, SparkSession\n    from pyspark.sql.types import *\n    from pyspark.conf import SparkConf\n    from pyspark.context import SparkContext\n    from pyspark.sql.functions import *\n    args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_OUTPUT_BUCKET', 'S3_OUTPUT_KEY_PREFIX', 'DATABASE_NAME', 'TABLE_NAME', 'REGION'])\n    sc = SparkContext()\n    glueContext = GlueContext(sc)\n    spark = glueContext.spark_session\n    job = Job(glueContext)\n    job.init(args['JOB_NAME'], args)\n    ```", "```py\n    yelp = glueContext.create_dynamic_frame.from_catalog(database=args['DATABASE_NAME'], table_name=args['TABLE_NAME'], transformation_ctx = \"datasource0\")\n    yelpDF = yelp.toDF().select('rating', 'review')\n    ```", "```py\n    MIN_SENTENCE_LENGTH_IN_CHARS = 10 \n    MAX_SENTENCE_LENGTH_IN_CHARS = 4500\n    COMPREHEND_BATCH_SIZE = 5  \n    NUMBER_OF_BATCHES = 10\n    ROW_LIMIT = 1000 #Number of reviews we will process for this workshop\n    ```", "```py\n    ComprehendRow = Row(\"review\", \"rating\", \"sentiment\")\n    def getBatchComprehend(input_list):\n        arr = []\n        bodies = [i[0] for i in input_list]\n        client = boto3.client('comprehend',region_name=args['REGION'])\n        def callApi(text_list):\n            response = client.batch_detect_sentiment(TextList = text_list, LanguageCode = 'en')\n            return response\n\n        for i in range(NUMBER_OF_BATCHES):\n            text_list = bodies[COMPREHEND_BATCH_SIZE * i : COMPREHEND_BATCH_SIZE * (i+1)]\n            #response = client.batch_detect_sentiment(TextList = text_list, LanguageCode = 'en')\n            response = callApi(text_list)\n            for r in response['ResultList']:\n                idx = COMPREHEND_BATCH_SIZE * i + r['Index']\n                arr.append(ComprehendRow(input_list[idx][0], input_list[idx][1], r['Sentiment']))\n\n        return arr\n    ```", "```py\n    yelpDF = yelpDF \\\n      .withColumn('review_len', F.length('review')) \\\n      .filter(F.col('review_len') > MIN_SENTENCE_LENGTH_IN_CHARS) \\\n      .filter(F.col('review_len') < MAX_SENTENCE_LENGTH_IN_CHARS) \\\n      .limit(ROW_LIMIT)\n    record_count = yelpDF.count()\n    print('record count=' + str(record_count))\n    yelpDF = yelpDF.repartition(record_count/(NUMBER_OF_BATCHES*COMPREHEND_BATCH_SIZE))\n    ```", "```py\n    group_rdd = yelpDF.rdd.map(lambda l: (l.review.encode(\"utf-8\"), l.rating)).glom()\n\n    transformed = group_rdd \\\n      .map(lambda l: getBatchComprehend(l)) \\\n      .flatMap(lambda x: x) \\\n      .toDF()\n    print(\"transformed count=\" + str(transformed.count()))\n    ```", "```py\n    transformedsink = DynamicFrame.fromDF(transformed, glueContext, \"joined\")\n    parquet_output_path = 's3://' + os.path.join(args['S3_OUTPUT_BUCKET'], args['S3_OUTPUT_KEY_PREFIX'])\n    print(parquet_output_path)\n    datasink5 = glueContext.write_dynamic_frame.from_options(frame = transformedsink, connection_type = \"s3\", connection_options = {\"path\": parquet_output_path, \"partitionKeys\": [\"sentiment\"]}, format=\"parquet\", transformation_ctx=\"datasink5\")\n\n    job.commit()\n    ```", "```py\n    glueContext.create_dynamic_frame.from_catalog- Read table metadata from the Glue Data Catalog using Glue libs to load tables into the job.\n    yelpDF = yelp.toDF() - Easy conversion from Glue DynamicFrame to Spark DataFrame and vice-versa joinedsink= DynamicFrame.fromDF(joinedDF, glueContext, \"joined\").\n    ```", "```py\n    script_location = sess.upload_data(path='yelp_etl.py', bucket=bucket, key_prefix='yelp/codes')\n    s3_output_key_prefix = 'yelp/parquet/'\n    ```", "```py\n    from time import gmtime, strftime\n    import time\n    timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n    job_name = 'yelp-etl-' + timestamp_prefix\n    response = glue.create_job(\n        Name=job_name,\n        Description='PySpark job to extract Yelp review sentiment analysis',\n        Role=role, # you can pass your existing AWS Glue role here if you have used Glue before\n        ExecutionProperty={\n            'MaxConcurrentRuns': 1\n        },\n        Command={\n            'Name': 'glueetl',\n            'ScriptLocation': script_location\n        },\n        DefaultArguments={\n            '--job-language': 'python',\n            '--job-bookmark-option': 'job-bookmark-disable'\n        },\n        AllocatedCapacity=5,\n        Timeout=60,\n    )\n    glue_job_name = response['Name']\n    print(glue_job_name)\n    ```", "```py\n    job_run_id = glue.start_job_run(JobName=job_name,\n                                           Arguments = {\n                                            '--S3_OUTPUT_BUCKET': bucket,\n                                            '--S3_OUTPUT_KEY_PREFIX': s3_output_key_prefix,\n                                            '--DATABASE_NAME': database_name,\n                                            '--TABLE_NAME': raw_table_name,\n                                            '--REGION': region\n                                           })['JobRunId']\n    print(job_run_id)\n    ```", "```py\n    job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n    while job_run_status not in ('FAILED', 'SUCCEEDED', 'STOPPED'):\n        job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']\n        print (job_run_status)\n        time.sleep(60)\n    print(job_run_status)\n    ```", "```py\n    parq_crawler_name = 'YelpCuratedCrawler'\n    parq_crawler_path = 's3://{0}/yelp/parquet/'.format(bucket)\n                     response = glue.create_crawler(\n        Name=parq_crawler_name,\n        Role=role,\n        DatabaseName=database_name,\n        Description='Crawler for the Parquet Yelp Reviews with Sentiment',\n        Targets={\n            'S3Targets': [\n                {\n                    'Path': parq_crawler_path\n                }\n            ]\n        },\n        SchemaChangePolicy={\n            'UpdateBehavior': 'UPDATE_IN_DATABASE',\n            'DeleteBehavior': 'DEPRECATE_IN_DATABASE'\n        },\n        TablePrefix='reviews_'\n    )\n    ```", "```py\n    response = glue.start_crawler(\n        Name=parq_crawler_name\n    )\n    print (\"Parquet Crawler: https://{0}.console.aws.amazon.com/glue/home?region={0}#crawler:name={1}\".format(region, parq_crawler_name))\n    ```", "```py\n    crawler_status = glue.get_crawler(Name=parq_crawler_name)['Crawler']['State']\n    while crawler_status not in ('READY'):\n        crawler_status = glue.get_crawler(Name=parq_crawler_name)['Crawler']['State']\n        print(crawler_status)\n        time.sleep(30)\n    ```", "```py\n    cursor.execute('select rating, review, sentiment from yelp.reviews_parquet')\n                  df = as_pandas(cursor)\n    df.head(10)\n    ```", "```py\n    group = df.groupby(('sentiment'))\n    group.describe()\n    ```", "```py\n    group = df.groupby(('rating'))\n    group.describe()\n    ```", "```py\n    source = ColumnDataSource(group)\n    \",\".join(source.column_names)\n    rating_cmap = factor_cmap('rating', palette=Spectral5, factors=sorted(df.rating.unique()))\n    p = figure(plot_height=350, x_range=group)\n    p.vbar(x='rating', top='review_count', width=1, line_color=\"white\", \n           fill_color=rating_cmap, source=source)\n    p.xgrid.grid_line_color = None\n    p.xaxis.axis_label = \"Rating\"\n    p.yaxis.axis_label = \"Count\"\n    p.y_range.start = 0\n    ```", "```py\n    print('https://{0}.quicksight.aws.amazon.com/sn/start?#'.format(region))\n    ```", "```py\nresponse = glue.delete_crawler(Name=parq_crawler_name)\nresponse = glue.delete_job(JobName=glue_job_name)\nresponse = glue.delete_database(\n    CatalogId = account_id,\n    Name = database_name\n)\nworkshop.delete_bucket_completely(bucket)\n```"]