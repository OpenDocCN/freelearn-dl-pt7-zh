<html><head></head><body>
		<div id="_idContainer032">
			<h1 id="_idParaDest-15" class="chapter-nu ber"><a id="_idTextAnchor014"/>1</h1>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Risks and Attacks on ML Models</h1>
			<p>This chapter gives a detailed overview of defining and evaluating a <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) risk framework from the instant an organization plans to embark on AI digital transformation. Risks may come in different stages, such as when the strategic or financial planning kicks in or during several of the execution phases. Risks start surfacing with the onset of technical implementations and continue up to testing phases when the AI use case is served to customers. Risk quantification can be attained through different metrics, which can certify the system behavior (amount of robustness and resiliency) against risks. In the process of understanding risk evaluation techniques, you will also get a thorough understanding of attacks and threats to ML models. In this context, you will discover different components of the system having security or privacy bottlenecks that pose external threats and make the model open to vulnerabilities. You will get to know the financial losses and business impacts when models deployed in production are not risk and <span class="No-Break">threat resilient.</span></p>
			<p>In this chapter, these topics will be covered in the <span class="No-Break">following sections:</span></p>
			<ul>
				<li>Discovering <span class="No-Break">risk elements</span></li>
				<li>Exploring risk mitigation strategies with vision, strategy, planning, <span class="No-Break">and metrics</span></li>
				<li>Assessing potential impact and loss due <span class="No-Break">to attacks</span></li>
				<li>Discovering different types <span class="No-Break">of attacks</span></li>
			</ul>
			<p>Further, with the use of <strong class="bold">Adversarial Robustness Toolbox</strong> (<strong class="bold">ART</strong>) and AIJack, we will see how to design attacks for <span class="No-Break">ML models.</span></p>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Technical requirements</h1>
			<p>This chapter requires you to have Python 3.8 along with some necessary Python packages, as follows. The commands to install ART and AIJack are also <span class="No-Break">listed here:</span></p>
			<ul>
				<li>Keras 2.7.0, <span class="No-Break">TensorFlow 2.7.0</span></li>
				<li><strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install adversarial-robustness-toolbox</strong></span></li>
				<li><strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install git+https://github.com/Koukyosyumei/AIJack</strong></span></li>
			</ul>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Discovering risk elements</h1>
			<p>With rapid digitization<a id="_idIndexMarker000"/> and AI adoption, more and more organizations are becoming aware of the unintended consequences of malicious AI adoption practices. These can impact not only the organization’s reputation and long-term business outcomes but also the business’ customers and society at large. Here, let us look at the different risk elements involved in an AI digitization journey that CXOs, leadership teams, and technical and operational teams should be aware of. The purpose of these associated teams is one and the same: to avoid any of their systems getting compromised, or any security/privacy violations that could yield discrimination, accidents, the manipulation of political systems, or the loss of <span class="No-Break">human life.</span></p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B18681_01_001.jpg" alt="Figure 1.1 – A diagram showing the AI risk framework"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – A diagram showing the AI risk framework</p>
			<p>There are three principal elements that govern the <span class="No-Break">risk framework:</span></p>
			<ul>
				<li><strong class="bold">Planning and execution</strong>: This phase ideally covers<a id="_idIndexMarker001"/> all stages in product development, that is, the conceptualization of the AI use case, financial planning, execution, including the technical<a id="_idIndexMarker002"/> execution, and the design and release of the final product/solution from an initial <strong class="bold">Minimum Viable </strong><span class="No-Break"><strong class="bold">Product</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MVP</strong></span><span class="No-Break">).</span></li>
				<li><strong class="bold">People and processes</strong>: This is the most crucial factor as far as delivery timelines<a id="_idIndexMarker003"/> are concerned with respect to an MVP or a final product/solution. Leadership should have a clear vision and guidelines put in place so that research, technical, QA, and other operational teams find it easy to execute data and ML processes following defined protocols <span class="No-Break">and standards.</span></li>
				<li><strong class="bold">Acceptance</strong>: This phase involves several rounds <a id="_idIndexMarker004"/>of audits and confirmations to validate all steps of technical model design and deployment. This process adheres to extra confirmatory guidelines and laws in place to cautiously review and explain AI/ML model outcomes with due respect to user fairness and privacy to protect users’ <span class="No-Break">confidential information.</span></li>
			</ul>
			<p>Let’s drill down into the components of each of <span class="No-Break">these elements.</span></p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>Strategy risk</h2>
			<p>On the strategic<a id="_idIndexMarker005"/> front, there should<a id="_idIndexMarker006"/> be a prior <strong class="bold">Strengths, Weaknesses, Opportunities, and Threats</strong> (<strong class="bold">SWOT</strong>) analysis done on business use cases requiring<a id="_idIndexMarker007"/> digital AI transformations. The CXOs and leadership team must identify the right business use case after doing an impact versus effort analysis and formulate the guidelines and a list of coherent actions needed for execution. The absence of this might set infeasible initiatives that are not aligned with the organization’s business goals, causing financial loss and solutions failing. <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em> illustrates how a specific industry (say, retail) can<a id="_idIndexMarker008"/> classify different use cases based on a <span class="No-Break"><strong class="bold">value-effort framework</strong></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B18681_01_002.jpg" alt="Figure 1.2 – ﻿A value-effort framework"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – A value-effort framework</p>
			<p>If the guidelines and actions are not set properly, then AI systems can harm individuals, society, and organizations. The following are <span class="No-Break">some examples:</span></p>
			<ul>
				<li>AI-powered autonomous vehicles can often malfunction, which can lead to injury <span class="No-Break">or death.</span></li>
				<li>Over-reliance on inadequate equipment and insufficient monitoring mean predictive maintenance tasks can lead to <span class="No-Break">worker injury.</span></li>
				<li>ML models misdiagnose <span class="No-Break">medical conditions.</span></li>
				<li>Political disruption by manipulating national institutional processes (for example, elections or appointments) by <span class="No-Break">misrepresenting information.</span></li>
				<li>Data breaches can expose confidential military locations or <span class="No-Break">technical secrets.</span></li>
				<li>Infrastructure disruption or misuse<a id="_idIndexMarker009"/> by intelligent systems (for example, GPS routing cars through different<a id="_idIndexMarker010"/> streets often increases traffic flow in <span class="No-Break">residential areas).</span></li>
			</ul>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Financial risk</h2>
			<p>The executive team<a id="_idIndexMarker011"/> should understand the finances involved in sponsoring an AI<a id="_idIndexMarker012"/> development project right from its inception to all stages of its development. Financial planning should not only consider the cost involved in hiring and retaining top talent but also the costs associated with infrastructure (cloud, containers, GPUs, and so on), data governance, and management tools. In addition, the financial roadmap should also specify the compliance necessary in big data and model deployment management as the risks and penalties can be huge in case of <span class="No-Break">any violations.</span></p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Technical risk</h2>
			<p>The risk associated<a id="_idIndexMarker013"/> on the technical front<a id="_idIndexMarker014"/> can manifest from the point when the data is ingested into the system. Data quality and the suitability of representation formats can seriously violate regulations (<em class="italic">Derisking machine learning and artificial intelligence</em>: <a href="https://www.mckinsey.com/business-functions/risk-and-resilience/our-insights/derisking-machine-learning-and-artificial-intelligence">https://www.mckinsey.com/business-functions/risk-and-resilience/our-insights/derisking-machine-learning-and-artificial-intelligence</a>). Along with a skilled data science and big data team, what is needed is the availability and awareness of modern tools and practices that can detect and alert issues related to data or model quality and drifts and take timely <span class="No-Break">remedial action.</span></p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B18681_01_003.jpg" alt="Figure 1.3 – A diagram showing risk management controls"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – A diagram showing risk management controls</p>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.3</em> illustrates different risk elements that can cause security breaches or theft of confidential information. The different components (data aggregation, preprocessing, model development, deployment, and model serving) of a real-time AI pipeline must be properly designed, monitored (for AI drift, bias, changes in the characteristics of the retraining population, circuit breakers, and fallback options), and audited before running it <span class="No-Break">in production.</span></p>
			<p>Along with this, risk assessment also includes how AI/ML models are identified, classified, and inventoried, with due consideration of how they are trained (for example, considering data<a id="_idIndexMarker015"/> type, vendor/open source libraries/code, third-party/vendor<a id="_idIndexMarker016"/> code updates and maintenance practices, and online retraining) and served <span class="No-Break">to customers.</span></p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>People and processes risk</h2>
			<p>The foremost <a id="_idIndexMarker017"/>objective of leadership and executive teams is to foster<a id="_idIndexMarker018"/> innovation and encourage an open culture where teams can collaborate, innovate, and thrive. When technical teams are proactive in bringing in automations in MLOps pipelines, many problems can be foreseen, and prompt measures can be taken to bridge the gaps through <span class="No-Break">knowledge-sharing sessions.</span></p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Trust and explainability risk</h2>
			<p>Businesses <a id="_idIndexMarker019"/>remain reluctant to adopt AI-powered applications<a id="_idIndexMarker020"/> when the results of the model cannot be explained. Some of the unexplainable results can be attributed to the poor performance of the model for a selected customer segment or during a specific period (for example, many business predictions were affected by the outbreak of COVID-19). The opaqueness of the model – a lack of explanation of the results – causes fear when businesses or customers find there is a lack of incentive alignment or severe disruption to people’s workflows or daily routines. ML models answering questions about the behavior of the model raises stakeholder confidence. In addition to deploying an optimized model that can give the right predictions with minimal delay, the model should also be able to explain the factors that affect the decisions it makes. However, it’s up to the ML/AI practitioners to use their judgment and analysis to apply the right ML models and explainability tools to derive the factors contributing to the model’s behavior. Now, let us see – with an example – how explainability can aid in studying <span class="No-Break">medical images.</span></p>
			<p><strong class="bold">Deep Neural Networks (DNNs)</strong> may be computationally hard to explain, but significant research is taking place into the explainability of DNNs as well. One such example involves <strong class="bold">Explainable Artificial Intelligence</strong> (<strong class="bold">XAI</strong>), used on pretrained deep learning neural networks (AlexNet, SqueezeNet, ResNet50, and VGG16), which has <a id="_idIndexMarker021"/>been successful in explaining critical regions that are affected by Barrett’s esophagus using related data by comparing classification rates. The comparative results can detect early stages of cancer and distinguish Barrett’s esophagus (<a href="https://www.sciencedirect.com/science/article/pii/S0010482521003723">https://www.sciencedirect.com/science/article/pii/S0010482521003723</a>) from adenocarcinoma. However, it remains up to the data scientist to decide how best to explain the use<a id="_idIndexMarker022"/> of their models, by selecting the right data and number of data points, based on the type of <span class="No-Break">the problem.</span></p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>Compliance and regulatory risk</h2>
			<p>There are different privacy<a id="_idIndexMarker023"/> laws and regulations<a id="_idIndexMarker024"/> that have been set forth by different nations and governing agencies that impose penalties on organizations<a id="_idIndexMarker025"/> in case of violations. Some of the most common privacy rules include the <a id="_idIndexMarker026"/>European Union’s <strong class="bold">General Data Protection Regulation</strong> (<strong class="bold">GDPR</strong>) and the <strong class="bold">California Consumer Privacy Act</strong> (<strong class="bold">CCPA</strong>). The financial and healthcare sectors have already seen laws formulated to prevent bias<a id="_idIndexMarker027"/> and allow fair treatment. Adhering to compliance necessitates extra planning for risk management through audits and <span class="No-Break">human monitoring.</span></p>
			<p>Apart from country-specific regulatory laws and guidance, regulators will likely rely on existing guidance in SR 11-7/OCC 2011-12 to assess the risks of <span class="No-Break">AI/ML applications.</span></p>
			<h3>Ethical risk</h3>
			<p>AI/ML models should go through proper validations<a id="_idIndexMarker028"/> and A/B testing to verify<a id="_idIndexMarker029"/> their compliance and fairness across different sections of the population, including people of varying genders and diverse racial and ethical backgrounds. For example, credit scoring and insurance models have historically been biased against racial minorities and discrimination-based lending decisions have resulted <span class="No-Break">in litigation.</span></p>
			<p>To make AI/ML models ethical, legal, and risk-free, it is inevitable for any organization and the executive team to have to ascertain the impact of the AI solution and service being rolled out in the market. This includes the inclusion of highly competent AI ethics personnel in the process who have regulatory oversight, and ensuring adherence to protocols and controls for risk mitigation to make sure the entire AI solution is robust and less attractive <span class="No-Break">to attackers.</span></p>
			<p>Such practices can not only add extra layers of security to anonymize individual identity but also remove any bias present in legacy systems. Now let us see what kinds of enterprise-grade<a id="_idIndexMarker030"/> initiatives are essential<a id="_idIndexMarker031"/> for inclusion in the AI <span class="No-Break">development process.</span></p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor024"/>Exploring risk mitigation strategies with vision, strategy, planning, and metrics</h1>
			<p>After seeing the elements<a id="_idIndexMarker032"/> of risk in different stages<a id="_idIndexMarker033"/> of the AI transformation<a id="_idIndexMarker034"/> journey, now let us walk through the different<a id="_idIndexMarker035"/> enterprise risk mitigation plans, measures, and metrics. In later chapters, we will not only discover risks related to ML model design, development, and deployment but also get to know how policies put in place by executive leadership teams are important in designing systems that are compliant with country-specific regulatory laws. Timely review, awareness, and support in the risk identification process can save organizations from unexpected <span class="No-Break">financial losses.</span></p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Defining a structured risk identification process</h2>
			<p>The long-term mission<a id="_idIndexMarker036"/> and short-term goals can only be achieved when business leaders, IT, security, and risk management teams align to evaluate a company’s existing risks, and whether they are affecting the upcoming AI-driven analytics solution. Such an effort, led by one of the largest European bank's COOs, helped to identify biased product recommendations. If left unchecked, it could have led to financial loss, regulatory fines, and disgrace, impacting the organization’s reputation and causing a loss of customers and <span class="No-Break">a backlash.</span></p>
			<p>This effort may vary from industry to industry. For example, the food and beverage industry needs to concentrate on risks related to contaminated products, while the healthcare industry needs to pay special attention to refrain from the misdiagnosis of patients and protect<a id="_idIndexMarker037"/> their sensitive <span class="No-Break">health data.</span></p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Enterprise-wide controls</h2>
			<p>Effective controls<a id="_idIndexMarker038"/> and techniques are structured around the incorporation of strong policies, worker training, contingency plans, and the redefinition of business rules and objectives that can be put into practice. These policies translate to specified standards and guidelines requiring human intervention as and when needed. For example, the European bank had to adopt flexibility in deciding how to handle specific customer<a id="_idIndexMarker039"/> cases when the customer’s financial or physical health was impacted: <a href="https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence">https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence</a>. In such cases, relationship managers had to intervene to offer suitable recommendations to help them to move on with the death/loss of a family member. Similarly, the healthcare industry needs the intervention of doctors and healthcare experts to adopt different <strong class="bold">active learning</strong> strategies to learn about rare diseases and their symptoms. Control measures necessitate the application of different open source or custom-built tools that can mitigate the risks of SaaS-based platforms and services, protect groups from potential discrimination, and ensure compliance <span class="No-Break">with GDPR.</span></p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Micro-risk management and the reinforcement of controls</h2>
			<p>The tools and techniques<a id="_idIndexMarker040"/> put into practice<a id="_idIndexMarker041"/> will vary based on the phase of the ML life cycle. Attacks and threats are much too specific to input data, feature engineering, model training, deployment, and the way the model is served<a id="_idIndexMarker042"/> to its customers. Hence it is essential to design and evaluate any ML model against a <strong class="bold">threat matrix</strong> (more details on threat matrices will be discussed in <a href="B18681_02.xhtml#_idTextAnchor040"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>). The most important factors that must be taken into consideration are the model's objective, optimization function, mode of learning (centralized versus federated), human-to-machine (or machine-to-machine) interaction, environmental factors (for designing policies and rewards in the case of reinforcement learning), feedback, retraining, and deployment. These factors, along with the model design and its explainability, will push organizations to go for a more transparent and explainable ML model and remove ML models that are overly complex, opaque, and unexplainable. The threat matrix can safeguard ML models in deployment by not only evaluating model performance but also testing models for adversarial attacks and other external factors that cause ML models <span class="No-Break">to drift.</span></p>
			<p>You need to apply a varying mix of risk control measures and risk mitigation strategies and reinforce them based on the outcome of the threat matrix. Along the journey of the AI transformation process, this will not only alleviate risks and reduce unseen costs but also make the system robust and transparent to counteract every possible risk. With such principles<a id="_idIndexMarker043"/> put into place, organizations can not only prevent <a id="_idIndexMarker044"/>ethical, business, reputation, and regulatory issues but also serve their customers and society with fair, equal, and <span class="No-Break">impartial treatment.</span></p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B18681_01_004.jpg" alt="Figure 1.4 – A diagram showing enhancements and mitigations in current risk management settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – A diagram showing enhancements and mitigations in current risk management settings</p>
			<p>A number of new elements<a id="_idIndexMarker045"/> related to ethics are needed in current AI/ML risk frameworks, which can help to ascertain risk performance and <span class="No-Break">alleviate risk:</span></p>
			<ul>
				<li><span class="No-Break">Interpretability</span></li>
				<li>Ethical AI <span class="No-Break">validation tools</span></li>
				<li><span class="No-Break">Model privacy</span></li>
				<li><span class="No-Break">Model compression</span></li>
				<li><span class="No-Break">Bias</span></li>
				<li><span class="No-Break">Feature engineering</span></li>
				<li>Sustainable <span class="No-Break">model training</span></li>
				<li>Privacy-related <span class="No-Break">pre-/post-processing techniques</span></li>
				<li><span class="No-Break">Fairness constraints</span></li>
				<li><span class="No-Break">Hyperparameters</span></li>
				<li>Model storage <span class="No-Break">and versioning</span></li>
				<li><span class="No-Break">Epsilon</span></li>
				<li>Total and <span class="No-Break">fairness loss</span></li>
				<li>Cloud/data <span class="No-Break">center sustainability</span></li>
				<li><span class="No-Break">Feature stores</span></li>
				<li>Attacks <span class="No-Break">and threats</span></li>
				<li><span class="No-Break">Drift</span></li>
				<li>Dynamic <span class="No-Break">model calibration</span></li>
				<li>A review of the pipeline design <span class="No-Break">and architecture</span></li>
				<li>Model <span class="No-Break">risk scoring</span></li>
				<li><span class="No-Break">Data/model lineage</span></li>
			</ul>
			<p>While we will study each<a id="_idIndexMarker046"/> of these components in later chapters, let us introduce<a id="_idIndexMarker047"/> the concepts here and understand why each of these components serves as an important unit for responsible/ethical model design and how they fit into the larger <span class="No-Break">ML ecosystem.</span></p>
			<p>To further illustrate, let us first consider the primary risk areas of AI ethics (the regulatory and model explainability risks) in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.5</em> by breaking down <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.4</em>. The following figure illustrates risk assessment methods and techniques to explain <span class="No-Break">model outcomes.</span></p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B18681_01_005.jpg" alt="Figure 1.5 – Risk assessment through regulatory assessment and model explainability"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Risk assessment through regulatory assessment and model explainability</p>
			<p>We see both global and local surrogate<a id="_idIndexMarker048"/> models play an important role<a id="_idIndexMarker049"/> in interpretability. While a global surrogate model has been trained to approximate the predictions of a black-box model, a local surrogate model is able to explain the local predictions of an individual record by changing the distribution of the surrogate model’s input. It is done through the process of weighting the data locally with a specific instance of the data (providing a higher weight to instances that resemble the instance <span class="No-Break">in question).</span></p>
			<h3>Ethical AI validation tools</h3>
			<p>These tools, either <a id="_idIndexMarker050"/>open source, through public<a id="_idIndexMarker051"/> APIs, or provided by different cloud providers (Google Cloud, Azure, or AWS), provide ways to validate the incoming data against different discriminatory sections of the population. Moreover, these tools also assist in discovering the protected data fields and data quality issues. Once the data is profiled with such tools, notification services and dashboards can be built in to detect data<a id="_idIndexMarker052"/> issues with the incoming data stream from<a id="_idIndexMarker053"/> individual <span class="No-Break">data sources.</span></p>
			<h3>Model interpretability</h3>
			<p>ML models, especially<a id="_idIndexMarker054"/> neural networks, are often called <strong class="bold">black boxes</strong> as the outcomes cannot be directly linked to the model architecture and explained. Businesses often roll out ML models in production that can not only recommend or predict customer demand but also substantiate the model’s decision with facts (single-feature or multiple-feature interactions). Despite the black-box nature of ML models, there are different open source interpretability tools available that can significantly explain the model outcome, such as, for example, why a loan application has been denied to a customer or why an individual of a certain age group and demographic is vulnerable to a <span class="No-Break">certain disease:</span></p>
			<ul>
				<li>Linear coefficients help to explain monotonic models (linear regression models) and justify the dependency of selected features and the results of <span class="No-Break">the output.</span></li>
				<li>Nonlinear and monotonic models (for example, gradient-boosting models with a monotonic constraint) help with selecting the right feature set among many present features for prediction by evaluating the positive or negative relationship with the <span class="No-Break">dependent variable.</span></li>
			</ul>
			<p>Nonlinear and nonmonotonic (for example, unconstrained deep learning models) methodologies such as local interpretable model-agnostic explanations or Shapley (an explainability Python library) serve as important tools for helping models with local interpretability. Neural networks have two broad primary categories for explaining <span class="No-Break">ML models:</span></p>
			<ul>
				<li><strong class="bold">Saliency methods/saliency </strong><span class="No-Break"><strong class="bold">maps</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SMs</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Feature </strong><span class="No-Break"><strong class="bold">Attribution</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">FA</strong></span><span class="No-Break">)</span></li>
			</ul>
			<p>Saliency Maps are only effective at conveying information related to weights being activated on specified inputs or different portions of an image being selected by a <strong class="bold">Convolutional Neural Network (CNN)</strong>. While saliency maps cannot convey information related to feature importance, FA methods aim to fit structural models on data subsets to evaluate the degree/power/impact each variable has on the <span class="No-Break">output variable.</span></p>
			<p>Discriminative DNNs are able to provide model explainability and explain the most important features by considering the model’s input gradients, meaning the gradients of the output logits with regard to the inputs. Certain SM-based interpretability techniques (gradient, SmoothGrad, and GradCAM) are effective interpretability methods that are still under research. For example, the gradient method is able to detect the most important pixels in an image by applying a backward pass through the network. The score arrived at after computing the derivative of the class with respect to the input image helps further in feature attribution. We can even use tools such as an XAI SM for image or video processing applications. Tools<a id="_idIndexMarker055"/> can show us how a network’s decision is affected by the most important parts of an image <span class="No-Break">or video.</span></p>
			<h3>Model privacy</h3>
			<p>With laws such as GDPR, CCPA, and policies<a id="_idIndexMarker056"/> introduced by different legislative bodies, ML models have absorbed the principle of <strong class="bold">privacy by design</strong> to gain user trust by incorporating privacy-preserving techniques. The objective behind said standards and the ML model redesign has primarily been to prevent information leaking from systems by building AI solutions and systems with the <span class="No-Break">following characteristics:</span></p>
			<ul>
				<li>Proactive and preventive instead of reactive <span class="No-Break">and remedial</span></li>
				<li>In-built privacy as the <span class="No-Break">default setting</span></li>
				<li>Privacy embedded into <span class="No-Break">the design</span></li>
				<li>Fully functional – no trade-offs <span class="No-Break">on functionality</span></li>
				<li>ML model life cycle security, privacy, and <span class="No-Break">end-to-end protection</span></li>
				<li>Visibility <span class="No-Break">and transparency</span></li>
				<li>User-centric with respect for <span class="No-Break">user privacy</span></li>
			</ul>
			<p>To encompass privacy at the model level, researchers and data scientists use a few principal units or essential building blocks that should have enough security measures built in to prevent the loss of sensitive and private information. These building units are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Model training data privacy</strong>: The data pipeline for the ML training data ingestion unit should have sufficient security measures built in. Any adversary attempting to attack the system should not be able to reverse-engineer the <span class="No-Break">training data.</span></li>
				<li><strong class="bold">Model input privacy</strong>: The security and privacy measures should ensure any input data going for model training cannot be seen by anyone, including the data scientist who is creating <span class="No-Break">the model.</span></li>
				<li><strong class="bold">Model output privacy</strong>: The security and privacy measures should ensure that the model output is not visible to anyone except the recipient user whose data is <span class="No-Break">being predicted.</span></li>
				<li><strong class="bold">Model storage and access privacy</strong>: The model must be stored securely with defined access rights to only eligible data <span class="No-Break">science professionals.</span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.6</em> illustrates different stages<a id="_idIndexMarker057"/> of model training and improvement where model privacy must be ensured to safeguard training data, model inputs, model weights, and the product, which is the ML <span class="No-Break">model output.</span></p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B18681_01_006.jpg" alt="Figure 1.6 – A diagram showing privacy in ML models"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – A diagram showing privacy in ML models</p>
			<h3>Model compression</h3>
			<p>AI ethics, standards, and guidelines<a id="_idIndexMarker058"/> have propelled researchers and data science professionals to look for ways to run and deploy these ML models on low-power and resource-constrained devices without sacrificing model accuracy. Here, model compression is essential as compressed models with the same functionality are best for devices that have limited memory. From the standpoint of AI ethics, we must leverage ML technology for the benefit of humankind. Hence, it is imperative that robust compressed models are trained and deployed in extreme environments such that they have minimal human intervention, and at the same time memorize relevant information (by having optimal pruning of the number <span class="No-Break">of neurons).</span></p>
			<p>For example, one technique is to build robust compressed models using noise-induced perturbations. Such noise often comes with IoT devices, which receive a lot of perturbations in the incoming data collected from the environment. Research results demonstrate that on-manifold adversarial training, which takes into consideration real-world noisy data, is able to yield highly compressed models and higher-accuracy models than off-manifold adversarial training, which incorporates noise from external attackers. <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.7</em> illustrates that manifold adversarial samples are closer to the decision boundary<a id="_idIndexMarker059"/> than the <span class="No-Break">simulated samples.</span></p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B18681_01_007.jpg" alt="Figure 1.7 – A diagram of simulated and on-manifold adversarial samples"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – A diagram of simulated and on-manifold adversarial samples</p>
			<h3>Sustainable model training</h3>
			<p>Low-powered devices<a id="_idIndexMarker060"/> depend on renewable energy resources for their own energy generation and local model training in federated learning ecosystems. There are different strategies by which devices can participate in the model training process and send updates to the central server. The main objective of devices taking part in the training process intermittently is to use the available energy efficiently in a sustainable fashion so that the devices do not run out of power and remain in the system till the global model converges. Sustainable model training sets guidelines and effective strategies to maximize power utilization for the benefit of <span class="No-Break">the environment.</span></p>
			<h3>Bias</h3>
			<p>ML models are subjected<a id="_idIndexMarker061"/> to different kinds of bias, both from the data and the model. While common data bias occurs from structural bias (mislabeling gender under perceived notions of societal constructs, for example, labeling women as nurses, teachers, and cooks), data collection, and data manipulation, common model bias occurs from data sampling, measurement, algorithmic bias, and bias against groups, segments, demographics, sectors, <span class="No-Break">or classes.</span></p>
			<p><strong class="bold">Random Forest</strong> (<strong class="bold">RF</strong>) algorithms work on the principle of randomization<a id="_idIndexMarker062"/> in the two-phase process of bagging samples and feature selection. The randomization process accounts for model bias from uninformative feature selection, especially for high-dimensional data with multi-valued features. The RF model elevated the risk level in money-laundering prediction by favoring the multi-valued dataset with many categorical variables for feature occupation. However, the same model was found to yield better, unbiased outcomes with a decrease in the number<a id="_idIndexMarker063"/> of categorical values. More advanced models built on top of RF, known as <strong class="bold">xRF</strong>, can select more relevant features<a id="_idIndexMarker064"/> using statistical assessments such as the <strong class="bold">p-value</strong>. The p-value assessment technique helps to assign appropriate weight to features based on their importance and aids in the selection of unbiased features by generating more accurate trees. This is an example of a feature weighting sampling technique used for <span class="No-Break">dimensionality reduction.</span></p>
			<h3>Feature engineering</h3>
			<p>This has become increasingly<a id="_idIndexMarker065"/> complex to understand for black-box models such as neural networks when compared<a id="_idIndexMarker066"/> to traditional ML models. For example, a <strong class="bold">CNN</strong> needs proper knowledge and application of filters to remove unwanted attributes. Models built from high-dimensional data need to incorporate proper dimensionality reduction techniques to select the most<a id="_idIndexMarker067"/> relevant one. Moreover, ML models resulting from <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) require preprocessing as one of the preliminary steps for model design. There are several commercial and open source libraries available that aid in new, complex feature creation, but they can also yield overfitted ML models. It has been found that overfitted models provide a direct<a id="_idIndexMarker068"/> threat to privacy and may leak private information (<a href="https://machinelearningmastery.com/data-leakage-machine-learning/)">https://machinelearningmastery.com/data-leakage-machine-learning/)</a>. Hence, model risk mitigation mechanisms must employ individual feature assessment to confirm included features’ impact (mathematical transformation and decision criteria) on the business rationale. The role of feature creation can be best understood in a specific credit modeling use case by banks where the ML model can predict defaulters based on the engineered feature of <span class="No-Break">debt-to-income ratio.</span></p>
			<h3>Privacy-related pre-/post-processing techniques</h3>
			<p>Data anonymization<a id="_idIndexMarker069"/> requires the addition of noise in some form (Gaussian/Laplace distribution) that can either be initiated prior to the model training process (K-anonymity, <strong class="bold">Differential Privacy (DP)</strong>) or post model convergence (<span class="No-Break">bolt-on DP).</span></p>
			<h3>Fairness constraints</h3>
			<p>ML models can be trained<a id="_idIndexMarker070"/> to yield desirable outcomes through different constraints. Constraints define different boundary conditions for ML models that on training the objective function would yield a fair, impartial prediction for minority or discriminatory racial groups. Such constraints need to be designed and introduced based on the type of training, namely supervised, semi-supervised, unsupervised, ranking, recommendations, or reinforcement-based learning. Datasets where constraints are applied the most have one or more sensitive attributes. Along with constraints, model validators should be entrusted to ensure a sound selection of parameters using randomized or grid <span class="No-Break">search algorithms.</span></p>
			<h3>Model storage and versioning</h3>
			<p>One important component<a id="_idIndexMarker071"/> of ethical AI systems is to endow production systems with the capability to reproduce data and model results, in the absence of which it becomes immensely difficult to diagnose failures and take immediate remedial action. Versioning and storing previous model versions not only allows you to quickly revert to a previous version, or activate model reproducibility to specific inputs, but it also helps to reduce debugging time and duplicating effort. Different tools and best practice mechanisms aid in model reproducibility by abstracting computational graphs and archiving data at every step of the <span class="No-Break">ML engine.</span></p>
			<h3>Epsilon (ε)</h3>
			<p>This is a metric used in <strong class="bold">DP</strong> solutions that is responsible for<a id="_idIndexMarker072"/> providing<a id="_idIndexMarker073"/> application-level privacy. This metric is used to measure privacy loss incurred on issuing the same query to two different datasets, where the two datasets differ in only one record and the difference is created by adding or removing one entry from one of the databases. We will discuss DP more in <a href="B18681_02.xhtml#_idTextAnchor040"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. This metric reveals the privacy risk imposed when it is computed on the private sensitive information of the previously mentioned datasets. It is also called privacy budget and is computed based on the input data size and the amount of noise added to the training data. The smaller<a id="_idIndexMarker074"/> the value, the better the <span class="No-Break">privacy protection.</span></p>
			<h3>Cloud/data center sustainability</h3>
			<p>With growing concerns<a id="_idIndexMarker075"/> about climate change and sustainability issues, the major cloud providers (Google, Amazon, and Microsoft) have started energy efficiency efforts to foster greener cloud-based products. The launch of carbon footprint reporting has enabled users to measure, track, and report on the carbon emissions associated with the cloud. To encourage businesses to have a minimal impact on the environment, all ML deployments should treat sustainability as a risk or compliance to be measured and managed. This propels data science and cloud teams to consider the deployment of ML pipelines and feature stores in sustainable <span class="No-Break">data centers.</span></p>
			<h3>Feature stores</h3>
			<p>Feature stores allow feature <a id="_idIndexMarker076"/>reuse, thus saving on extra storage and cloud costs. As data reuse and storage must meet compliance and regulations, it is an important consideration parameter in ethical AI. Feature stores allow the creation of important features using feature engineering and foster collaboration among team members to share, discover, and use existing features without doing additional rework. Feature reuse also prompts the reuse of important attributes based on importance of features and model explainability as defined by other teams. As deep learning models require huge computing power and energy, the proper selection of algorithms, along with the reuse of model data and features, reduces cloud costs by reducing <span class="No-Break">computational capacity.</span></p>
			<h3>Attacks and threats</h3>
			<p>A risk framework<a id="_idIndexMarker077"/> designed for production-grade enterprise AI solutions should be integrated with an attack testing framework (third-party and open source), to ascertain the model risk from external adversaries. The ML model’s susceptibility to attack can then be used to increase the monitoring activity to be proactive in the case <span class="No-Break">of attacks.</span></p>
			<h3>Drift</h3>
			<p>Data and model monitoring<a id="_idIndexMarker078"/> techniques that have been implemented in the system must be able to quickly identify data and model drift when statistical properties of the target variable or the predictors change respectively (<em class="italic">Concept Drift and Model Decay in Machine Learning</em> by Ashok Chilakapati: <a href="http://xplordat.com/2019/04/25/concept-drift-and-model-decay-in-machine-learning/">http://xplordat.com/2019/04/25/concept-drift-and-model-decay-in-machine-learning/</a>). Proactive measures include reviewing data formats, schema, and units and retraining the model when the drift percentage exceeds a <span class="No-Break">specified threshold.</span></p>
			<p>The following descriptions correspond with the number labels in<em class="italic"> </em><span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p>
			<ol>
				<li>Original data and model decision boundary <span class="No-Break">at </span><span class="No-Break">t</span><span class="No-Break"><span class="subscript">1</span></span><span class="No-Break">.</span></li>
				<li>Drift in just the data boundary at t<span class="subscript">2</span>, resulting from a change in the features of the input data. For example, let us consider a real-world scenario where IoT sensor readings are anomalous in the range -10 to 10. Now, the new reading may change to -5 to 8, but still, the reading will be considered anomalous as there is no change in the decision outcome or the model output. As this does not result in any drift in the model boundary, it is only <span class="No-Break">virtual drift.</span></li>
				<li>Drift in both data<a id="_idIndexMarker079"/> and the model boundary at t<span class="subscript">3</span>, resulting in actual concept drift. For example, such a scenario may occur when two sensor readings change in such a manner (from old readings of -10 to 10 to new readings of +20 to +100) that the resultant model outcome is +1, signifying it is no longer an anomaly. It demonstrates a change in the model boundary, where the output is just a reflection of the change in the input <span class="No-Break">data boundary.</span></li>
			</ol>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B18681_01_008.jpg" alt="Figure 1.8 – ﻿Different types of model drift"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Different types of model drift</p>
			<h3>Dynamic model calibration</h3>
			<p>Dynamic model calibration<a id="_idIndexMarker080"/> is a more specialized version of model drift. Model drift may result from a change in data, units of measurement, and internal and external factors that need careful study, review, and discussion for a certain period before triggering a model refresh. </p>
			<p>On the other hand, model calibration can be facilitated when a model’s performance level changes only due to short-term changes in the incoming data (for example, mobile network capacity becoming slow due to a large social gathering or a <span class="No-Break">football match).</span></p>
			<p>ML models (for example, reinforcement learning algorithms or Bayesian models) exhibit characteristics to refresh their model parameters dynamically to pick up new trends and patterns in the incoming data. This leads to the removal of manual processes of model review and refresh. In the absence of adequate controls or algorithms used to control the level of thresholds to allow model refresh, short-term patterns may get over-emphasized, which could degrade the performance of the model over time. Hence, overcoming such risks needs careful review by experts of when to allow dynamic recalibration to facilitate the reflection of upcoming trends. Moreover, businesses (especially in algorithmic trading in banking or the spread of a pandemic in healthcare) need to be convinced that dynamic recalibration outperforms static models <span class="No-Break">over time.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.9</em> demonstrates a use case when the location data input to the model shows an oscillatory pattern, causing the prediction results to shift over time and resulting in model drift. Such scenarios need model replacement/calibration and the threshold of drift percentage to be specified <span class="No-Break">or configured.</span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B18681_01_009.jpg" alt=" Figure 1.9 – A diagram showing model calibration under output model prediction drift"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 1.9 – A diagram showing model calibration under output model prediction drift</p>
			<h3>Reviewing the pipeline design and architecture</h3>
			<p>As we review model<a id="_idIndexMarker081"/> drift and allow the dynamic calibration<a id="_idIndexMarker082"/> of models, to comply with ethics we should also periodically review the system design and architecture, pipelines, and feature stores and allow modifications if needed. One of the most important parts of a review is to re-evaluate and reconsider the entire security system, to apply new patches or additional layers of authentication or black-listing services to proactively act on DDOS attacks. Several optimizations can be done in subsequent production releases that can help to reduce cloud costs, optimize database operations, and boost the performance of APIs and microservices. The review process allows you to seek expert opinions (from cloud and DevOps professionals) who can provide insights into designing more automated workflows, along with migration to on-demand services (for example, lambda services) to reduce processing costs. Reviewing system load, performance, and scaling factors can also facilitate a better selection of databases, caching, and messaging options, or carefully analyzing and redefining <span class="No-Break">auto-scaling options.</span></p>
			<h3>Model risk scoring</h3>
			<p>As we have used ethical AI validation<a id="_idIndexMarker083"/> tools for profiling and validating input data, we also need risk assessment tools to assess and quantify the model risk against adversarial attacks and threats. There are different open source tools and APIs available, and even tools provided by different cloud providers (Google Cloud, Azure, and AWS) that provide ways to train and test models against the model’s susceptibility to different attacks and model bias by quantifying the number of unfair outcomes exhibited by the model toward different sections of the population. In addition, these tools also help to explain important features that contribute to the model outcome. In the following chapters, we will discuss more such tools and frameworks. A model risk-scoring strategy requires risk factors or indicators useful for predictions, data integrity, methodology preference, and <span class="No-Break">resource capabilities.</span></p>
			<p>Risk-scoring methodologies function in two <span class="No-Break">different ways:</span></p>
			<ul>
				<li><strong class="bold">Prospective</strong> risk methods predict model risk<a id="_idIndexMarker084"/> after analyzing historical <span class="No-Break">model performance.</span></li>
				<li><strong class="bold">Retrospective</strong>/<strong class="bold">concurrent</strong> risk leverages the most current risk of the model to predict the overall model <a id="_idIndexMarker085"/>risk for <span class="No-Break">future cycles.</span></li>
			</ul>
			<p>The second method is more suitable when there have been key changes to the model risk indicators, data (model behavior), or recent attacks or loss of data and the model is <span class="No-Break">being investigated.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.10</em> illustrates how risk-sensitive<a id="_idIndexMarker086"/> model risk management takes into consideration monitoring tools, activities, and governance measures to evaluate the model risk. The figure has been extended from <em class="italic">Components of Keenan’s model risk measure</em>, Keenan (2015), which additionally demonstrates the impact of past attacks, threats, and vulnerabilities on similar models in businesses and indicates the increase of risk associated with the <span class="No-Break">current model.</span></p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B18681_01_010.jpg" alt="Figure 1.10 – A diagram showing model risk assessment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – A diagram showing model risk assessment</p>
			<h3>Data/model lineage</h3>
			<p>Ethics and compliance processes<a id="_idIndexMarker087"/> require frequent audits and quality checks on both the data and the model. It is imperative to store the lineage of both so that at any instant, it is clear the model evolved from version 1 to version 2 to version 3 due to changes in data, such as the addition, modification, or deletion of certain features. Along with this, there should be defined storage where immediate historical data about the model and its artifacts can be stored, as opposed to older artifacts, which can be stored in less frequent storage centers (requiring less access) of <span class="No-Break">the cloud.</span></p>
			<p>The following figure illustrates the model’s input training, validation, test data, model serving, and output file storage in AWS’s different storage classes based on the frequency of access. Here, we have the roles of different processing blocks and units that are essential in designing an ethical and fully compliant system. By following the previously stated validation policies and practices, it is easier to address ML model risks, explore existing bottlenecks, and redefine new policies and practices at each stage of the model <span class="No-Break">life cycle.</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B18681_01_011.jpg" alt="Figure 1.11 – A diagram showing the model and its artifact storage"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.11 – A diagram showing the model and its artifact storage</p>
			<p>Any executive team needs<a id="_idIndexMarker088"/> to be aware of the importance of cloud infrastructure, system and security design principles, ML model design, model scoring, and risk assessment mechanisms and set guidelines so that the business can mitigate risks, avoid penalties, and gain confidence in harnessing the power of ML to boost sales <span class="No-Break">and revenue.</span></p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B18681_01_012.jpg" alt="Figure 1.12 – A diagram showing data and model lineage"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.12 – A diagram showing data and model lineage</p>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.12</em> illustrates how data <a id="_idIndexMarker089"/>and model lineage need to be accomplished in the model life cycle development phases, starting from data integration and preprocessing to model training, ensembling, model serving, and the retraining process. We can see data arrives from two different data sources, A and B, at times t<span class="subscript">1</span> and t<span class="subscript">2</span>, which gets assembled or aggregated at t<span class="subscript">3</span> to serve as input for data preprocessing and feature engineering at t<span class="subscript">4</span> and t<span class="subscript">5</span> respectively. There are two <span class="No-Break">model outputs:</span></p>
			<ul>
				<li>Model v<span class="subscript">1</span> available at t<span class="subscript">n</span><span class="subscript">+3</span> corresponding to model training (t<span class="subscript">n</span>) demonstrating combination of different ML models trained at different instants of <span class="No-Break">time (</span><span class="No-Break">t</span><span class="No-Break"><span class="subscript">n</span></span><span class="No-Break"><span class="subscript">+1</span></span><span class="No-Break">)</span></li>
				<li>Model v<span class="subscript">2</span> available at t<span class="subscript">n+x</span><span class="subscript">+3</span> corresponding to model retraining (<em class="italic">t</em><span class="P-Subscript-Italics">n+x</span>), re-ensembling (t<span class="subscript">n+x</span><span class="subscript">+1</span>) </li>
			</ul>
			<p>Data and model lineage should be capable of capturing any changes in the system with appropriate versions, which aids in model reproducibility later. After analyzing the important<a id="_idIndexMarker090"/> components of ethics and risk, let us now take a look at the penalties that organizations can incur if they fail to follow laws and guidelines set by <span class="No-Break">regulatory bodies.</span></p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Assessing potential impact and loss due to attacks</h1>
			<p>In the previous section, we looked at the data threats, risks, and important metrics for consideration while building our ML systems. Now, let us understand the financial losses that organizations have incurred due to <span class="No-Break">data leakage.</span></p>
			<h3>AOL data breach</h3>
			<p>AOL faced a lawsuit in 2006 that resulted in them having to pay at least $5,000 to every person whose data was leaked <a id="_idIndexMarker091"/>because of releasing user records that could be accessed through public search APIs (<em class="italic">Throw Back Hack</em>: <em class="italic">The Infamous AOL Data Leak</em>: <a href="https://www.proofpoint.com/us/blog/insider-threat-management/throw-back-hack-infamous-aol-data-leak">https://www.proofpoint.com/us/blog/insider-threat-management/throw-back-hack-infamous-aol-data-leak</a>). This incident happened as the search department mistakenly released a compressed text file holding 20 million keyword search record details of 650,000 users. As users’ <strong class="bold">Personally Identifiable Information (PII)</strong> personally identifiable information was present in the search queries, it was easy to identify and associate an individual holding an account. In addition, very recently, Jason Smathers, an employee of AOL, is known to have sold to a person named Sean Dunaway of Las Vegas a list of 92 million AOL customer <span class="No-Break">account names.</span></p>
			<h3>Yahoo data breach</h3>
			<p>Yahoo encountered <a id="_idIndexMarker092"/>a series of data breaches (loss of personal information such as through email) through varying levels of security intrusions between 2012 and 2016, amounting to the leakage of 3 billion records (<em class="italic">IOTW</em>: <em class="italic">Multiple Yahoo data breaches across four years result in a $117.5 million settlement</em>: <a href="https://www.cshub.com/attacks/articles/incident-of-the-week-multiple-yahoo-data-breaches-across-4-years-result-in-a-1175-million-settlement">https://www.cshub.com/attacks/articles/incident-of-the-week-multiple-yahoo-data-breaches-across-4-years-result-in-a-1175-million-settlement</a>). </p>
			<p>The attack in 2014 targeted a different user database, affecting 500 million people and containing a greater detail of personal information such as people’s names, email addresses, passwords, phone numbers, and birthdays. Yahoo settled penalties worth $50 million, with $35 million paid in advance, as a part of the damages (<em class="italic">Yahoo Fined $50M Over Data </em><span class="No-Break"><em class="italic">Breach</em></span><span class="No-Break">: </span><a href="https://www.pymnts.com/legal/2018/yahoo-fine-personal-data-breach/"><span class="No-Break">https://www.pymnts.com/legal/2018/yahoo-fine-personal-data-breach/</span></a><span class="No-Break">).</span></p>
			<p>Marriot hotel chain <span class="No-Break">data breach</span></p>
			<p>The Marriot hotel chain<a id="_idIndexMarker093"/> was fined £18.4m due to the leak of the personal information (names, contact details, travel information, VIP status) of 7 million guests in the UK in a series of cyber-attacks from 2014 to 2018. Due to the failure to protect personal data and non-conformance with the GDPR, it incurred a hefty fine from the UK’s data privacy watchdog (<em class="italic">Marriott Hotels fined £18.4m for data breach that hit </em><span class="No-Break"><em class="italic">millions</em></span><span class="No-Break">: </span><a href="https://www.bbc.com/news/technology-54748843"><span class="No-Break">https://www.bbc.com/news/technology-54748843</span></a><span class="No-Break">).</span></p>
			<h3>Uber data breach</h3>
			<p>Uber was handed a fine of $20,000 over a 2014 data<a id="_idIndexMarker094"/> breach in a settlement in New York due to a breach of riders’ data privacy (<em class="italic">Uber fined $20K in data breach, ‘god view’ probe</em>: <a href="https://www.cnet.com/tech/services-and-software/uber-fined-20k-in-surveillance-data-breach-probe/">https://www.cnet.com/tech/services-and-software/uber-fined-20k-in-surveillance-data-breach-probe/</a>). The breach occurred in 2014 and exposed 50,000 drivers’ location information through the <span class="No-Break">rider-tracking system.</span></p>
			<h3>Google data breach</h3>
			<p>In 2020, the French data protection<a id="_idIndexMarker095"/> authority imposed a fine of $57 million on Google due to the violation of GDPR, because it failed to acknowledge and share how user data is processed in different Google apps, such as Google Maps, YouTube, the search engine, and personalized advertisements. In another data leakage incident, Google was responsible for leaking the private data of 500,000 former Google+ users. This data leak enforced Google to pay US$7.5 million, and compensation between US$5 and US$12 to users with Google+ accounts between 2015 <span class="No-Break">and 2019.</span></p>
			<h3>Amazon data breach</h3>
			<p>Amazon faced different data leak<a id="_idIndexMarker096"/> incidents in 2021 (<em class="italic">Worst AWS Data Breaches of 2021</em>: <a href="https://securityboulevard.com/2021/12/worst-aws-data-breaches-of-2021/">https://securityboulevard.com/2021/12/worst-aws-data-breaches-of-2021/</a>). One of the incidents resulted in a fine of 746 million euros (US$887 million) (<em class="italic">Amazon hit with US$887 million fine by European privacy watchdog</em>: <a href="https://www.cnbc.com/2021/07/30/amazon-hit-with-fine-by-eu-privacy-watchdog-.html">https://www.cnbc.com/2021/07/30/amazon-hit-with-fine-by-eu-privacy-watchdog-.html</a>) being imposed by a European privacy watchdog, due to violating GDPR. In another incident, misconfigured S3 buckets in AWS amounted to the disruption of networks<a id="_idIndexMarker097"/> for considerable periods. S3 files, apart from <strong class="bold">PII</strong>, including names, email addresses, national ID numbers, and phone numbers, could contain credit card<a id="_idIndexMarker098"/> details, including <span class="No-Break">CVV codes.</span></p>
			<h3>Facebook data breach</h3>
			<p>In 2018, Facebook<a id="_idIndexMarker099"/> received a large penalty of $5 billion, and it needed to investigate and resolve different privacy and security loopholes (<em class="italic">Facebook to pay record $5 billion U.S. fine over privacy; faces antitrust probe</em>: <a href="https://www.reuters.com/article/us-facebook-ftc/facebook-to-pay-record-5-billion-u-s-fine-over-privacy-faces-antitrust-probe-idUSKCN1UJ1L9">https://www.reuters.com/article/us-facebook-ftc/facebook-to-pay-record-5-billion-u-s-fine-over-privacy-faces-antitrust-probe-idUSKCN1UJ1L9</a>). The breach occurred on account of improper usage of PII leaked by Cambridge Analytica, which had gathered information from 50 million profiles on Facebook. Facebook exposed the PII of 87 million people that had been misused by the Cambridge Analytica firm to target ads during an election campaign <span class="No-Break">in 2016.</span></p>
			<p>We can note that data breaches are common and they still occur presently. Some of the biggest providers in search services, retail, travel or hospitality, and transportation systems have been victims of threats and penalties here PII information have been stolen. Some other data breaches between 2019 and 2021 are known to have taken place for organizations such as Volkswagen (whose security breach impacted over 3 million customers) and T-Mobile (where over 50 million customers’ private information, including Social Security numbers, and IMEI and IMSI numbers, was compromised). in attacking iPads and iPhones to steal unique Apple device identifiers (UDIDs) and the device<a id="_idIndexMarker100"/> names of more than 12 million devices. The incident occurred when a FBI agent's laptop was hacked to steal 12 million <span class="No-Break">Apple IDs.</span></p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Discovering different types of attacks</h1>
			<p>After gaining an understanding<a id="_idIndexMarker101"/> of the financial losses suffered by organizations, it is imperative to know the objective of each type of attack and how attacks can be carried out. Moreover, the growth of the online industry and the availability of cheap data services, along with the usage of IoT and mobile devices, has left attackers with plenty of user-generated content to abuse. Advanced attack research techniques have propelled attackers to use advanced mechanisms to target large-scale systems and their defenses. There are different types of attacks on ML models, whether they are available for local use (white box) or deployed in a cloud setup (Google, Amazon, or Azure) and served by means of a prediction query. Amazon and Google provide services to train ML <a id="_idIndexMarker102"/>models in a black-box manner. Both Google (Vertex AI) (<a href="https://cloud.google.com/vertex-ai/docs/explainable-ai/overview">https://cloud.google.com/vertex-ai/docs/explainable-ai/overview</a>) and AWS have partial feature extraction techniques' documentation available in their manuals. With the increased scope of privacy breaches in a deployed model, it is easier for an attacker to attack and steal training data and ML models. Attackers are motivated to steal ML models to avoid prediction query charges. <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.13</em> illustrates different categories of attacks under training and testing. We have also mentioned defense techniques, which will be discussed more in <a href="B18681_02.xhtml#_idTextAnchor040"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Emergence of Risk-Averse Methodologies </em><span class="No-Break"><em class="italic">and Frameworks</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B18681_01_013.jpg" alt="Figure 1.13 – A diagram showing different attack categories and defenses"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.13 – A diagram showing different attack categories and defenses</p>
			<p>Now let us discuss how<a id="_idIndexMarker103"/> and with what objectives attackers try to attack <span class="No-Break">ML models.</span></p>
			<p>To run different attacks, we need to import the necessary <span class="No-Break">Python libraries:</span></p>
			<pre class="source-code">
from art.estimators.classification import KerasClassifier
from art.attacks.inference.model_inversion.mi_face import MIFace
from art.estimators.classification import KerasClassifier
from art.attacks import evasion, extraction, inference, poisoning
from art.attacks import Attack, EvasionAttack, PoisoningAttack, PosioningAttackBlackBox,
PoisoningAttackWhiteBox
from art.attacks import Attack, PoisoningAttackTransformer, ExtractionAttack, InferenceAttack, AttributeInferenceAttack, ReconstructionAttack
from art.attacks.evasion import HopSkipJump
from art.utils import to_categorical
from art.utils import load_dataset</pre>
			<p>That is a lot of imports! With everything acquired, we are now ready to proceed with poisoning, evasion, extraction, or inference attacks. We have used ART to create a <strong class="bold">Zeroth-Order</strong><strong class="bold"><a id="_idIndexMarker104"/></strong><strong class="bold"> Optimization  (ZOO) </strong>attack, a kind of evasion attack <span class="No-Break">using XGBoostClassifier.</span></p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Data phishing privacy attacks</h2>
			<p>This is one of the most common techniques<a id="_idIndexMarker105"/> used by attackers to gain access to confidential information in a training dataset by applying reverse-engineering when the model has sufficient data leakage. </p>
			<p>This is possible when the model is overfitting and not able to generalize the predictions to the new data or the model is trained with too few training data points. Mechanisms such as DP, randomized data hold-out, and three-level encryption at input, model, and output can increase <span class="No-Break">the protection.</span></p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Poisoning attacks</h2>
			<p>This is a kind of attack on model<a id="_idIndexMarker106"/> integrity, where the attacker can affect the model’s performance in the training/retraining process during deployment by directly influencing the training or its labels. The name “poison” is derived from the attacker’s ability to poison the data by injecting malicious samples during its operation. Poisoning may be of <span class="No-Break">two types:</span></p>
			<ul>
				<li>Model skewing in a white-box manner by gaining access to the model. The training data is modified in such a way that the boundary between what the classifier categorizes as good data and what the classifier categorizes as bad shifts in the favor of <span class="No-Break">the attacker.</span></li>
				<li>A feedback weaponization attack undertaken in a black-box manner works by generating abusive or negative feedback to manipulate the system into misclassifying good content as abusive. This is more common in recommendation systems, where the attacker can promote products, content, and so on by following the user closely on <span class="No-Break">social media.</span></li>
			</ul>
			<p>As the duration of this attack depends on the model’s training cycle, the principal way to prevent a poisoning attack is to detect malicious inputs before the next training cycle happens, by adding input and system validation checking, rate limiting, regression testing, manual moderation, and other<a id="_idIndexMarker107"/> statistical techniques, along with enforcing strong <span class="No-Break">access controls.</span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>Evasion attacks</h2>
			<p>Evasion attacks are very popular<a id="_idIndexMarker108"/> in ML research as they are used in intrusion and malware cases during the deployment or inference phase. The attacker changes the data with the objective of deluding the existing trained classifiers. The attackers obfuscate the data of malware, network intrusion detectors, or spam emails, which are treated as legitimate as they do not impact the training data. Such non-random human-imperceptible perturbations, when added to original data, cause the learned model to produce erroneous output, even without drifting the model <span class="No-Break">decision boundary.</span></p>
			<p>Spoofing attacks against biometric verification systems fall under the category of evasion attacks. The best way to design intrusion detectors against adversarial evasion attacks is to leverage ensemble learning, which can combine layers of detectors and monitor the behavior of applications. Evasion attacks pose challenges even in deploying DNNs in safety- and security-critical applications such as self-driving cars. Region-based classification techniques (relying on majority voting techniques among the labels of sampled data points) are found to be more robust to adversarial samples. The following figure illustrates data poisoning and evasion attacks on centralized and federated <span class="No-Break">learning systems.</span></p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B18681_01_014.jpg" alt="Figure 1.14 – A diagram showing a simple federated poisoning and evasion attack"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.14 – A diagram showing a simple federated poisoning and evasion attack</p>
			<p>The following code snippet provides an example of initiating an evasion attack on XGBoostClassifier. The code outlines<a id="_idIndexMarker109"/> the procedure to trigger a black-box <strong class="bold">ZOO</strong> attack with a classifier (where the parameter classifier is set to XGBoost) to predict the gradients of the targeted DNN. This prediction helps to generate adversarial data where the confidence (float) denotes how far away the samples generated are, with high confidence symbolizing the samples are generated at a greater distance from the input. The underlying algorithm uses stochastic coordinate descent along with dimension reduction, a hierarchical attack, and an importance sampling technique with the configurability of triggering a targeted attack or non-targeted attack, as set by the targeted Boolean parameter in the following code. While the untargeted attack can only cause misclassification, targeted attacks can force a class to be classified as a <span class="No-Break">desired class.</span></p>
			<p>The learning rate of the attack<a id="_idIndexMarker110"/> algorithm is controlled by <strong class="source-inline">learning_rate</strong> (float). Other important parameters for consideration are <strong class="source-inline">binary_search_steps</strong> (integer), which is the number of times to adjust the constant with binary search, and <strong class="source-inline">initial_const</strong> (float), which is available for tweaking the importance of the distance and confidence value to achieve the initial trade-off <span class="No-Break">constant </span><span class="No-Break"><strong class="source-inline">c</strong></span><span class="No-Break">:</span></p>
			<ol>
				<li>Create the ART classifier <span class="No-Break">for XGBoost:</span><pre class="console">
art_classifier = XGBoostClassifier(model=model, nb_features=x_train.shape[1], nb_classes=10)</pre></li>
				<li>Create the ART <span class="No-Break">ZOO attack:</span><pre class="console">
zoo = ZooAttack(classifier=art_classifier, confidence=0.0, targeted=False, learning_rate=1e-1, max_iter=20,
                    binary_search_steps=10, initial_const=1e-3, abort_early=True, use_resize=False,
                    use_importance=False, nb_parallel=1, batch_size=1, variable_h=0.2)</pre></li>
				<li>Generate adversarial samples with the ART <span class="No-Break">ZOO attack:</span><pre class="console">
x_train_adv = zoo.generate(x_train)</pre></li>
			</ol>
			<p>The sample code snippet demonstrates a mechanism to generate adversarial samples using a poisoned attack and then visualize the effect of classifying data points with the clean model versus the poisoned model:</p>
			<pre class="console">
attack_point, poisoned = get_adversarial_examples(train_data, train_labels, 0, test_data, test_labels, kernel)
clean = SVC(kernel=kernel)
art_clean = SklearnClassifier(clean, clip_values=(0, 10))
art_clean.fit(x=train_data, y=train_labels)
plot_results(art_clean._model, train_data, train_labels, [], "SVM Before Attack")
plot_results(poisoned._model, train_data, train_labels, [attack_point], "SVM After Poison")</pre>
			<p>As illustrated in the following<a id="_idIndexMarker111"/> figure, in a perfect classifier, all the points should ideally be in yellow or blue circles, aligned on either the green or light blue side of the classifier <span class="No-Break">boundary respectively.</span></p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B18681_01_015.jpg" alt="Figure 1.15 – Code sample to trigger poison attacks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.15 – Code sample to trigger poison attacks</p>
			<p>Here, the red cross is the attack<a id="_idIndexMarker112"/> point, which is strong enough to disturb the model’s <span class="No-Break">generalization capability.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Model stealing/extraction</h2>
			<p>In a model extraction attack, the attacker<a id="_idIndexMarker113"/> is responsible for probing a black-box ML system (with no knowledge of model internals) to reconstruct the model or retrieve the training data (<em class="italic">In Model Extraction, Don’t Just Ask 'How?'</em>: <em class="italic">Ask 'Why?'</em> by Matthew Jagielski and Nicolas Papernot: <a href="http://www.cleverhans.io/2020/05/21/model-extraction.html">http://www.cleverhans.io/2020/05/21/model-extraction.html</a>). This kind of attack needs special attention when either the training data or the model itself is sensitive and confidential, as the attacker may totally avoid provider charges by running cross-user model <span class="No-Break">extraction attacks.</span></p>
			<p>Attackers also want to use model information and data for their own personal benefit (for example, stolen information can be used by an attacker to customize and optimize stock market prediction<a id="_idIndexMarker114"/> and spam filtering models for personal use). This type of attack is possible when the model is served through an API, typically through <strong class="bold">Machine Learning as a Service</strong> (<strong class="bold">MLaaS)</strong> platforms. The APIs can serve the models on an edge device or mobile phone. Not only is the model information from the defense system compromised, but the provider also sees data loss or revenue due to free training <span class="No-Break">and prediction.</span></p>
			<p>The adversaries issue repeat queries to the victim model to obtain their labeled samples. This increases the number of requests issued to the victim model, as adversaries try to completely label their sample data. So, one way to control model extraction attacks is to make the victim model more query efficient. <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.16</em> illustrates an example of a model extraction attack where the adversary may prefer to choose either of the brown or yellow decision boundaries to steal the model, based on the attacker’s preference regarding fidelity (privacy) <span class="No-Break">over accuracy.</span></p>
			<p>Extraction attacks violate ML model confidentiality and can be accomplished in <span class="No-Break">three ways:</span></p>
			<ul>
				<li>Equation-based model extraction attacks with random queries can target ML models with <span class="No-Break">confidence values.</span></li>
				<li>Path-finding algorithms (such as decision trees) exploit confidence boundaries as quasi-identifiers for <span class="No-Break">path discovery.</span></li>
				<li>Extraction attacks against models with only class labels as output are slow and act as countermeasures to models with <span class="No-Break">confidence values.</span></li>
			</ul>
			<p>The following sample<a id="_idIndexMarker115"/> code demonstrates an attempt to steal and extract model information from a target model trained using <strong class="source-inline">KerasClassifier</strong> of 10 classes and 128 dense units, with 32 and 64 filters on <span class="No-Break">subsequent layers:</span></p>
			<pre class="source-code">
     model_stolen = get_model(num_classes=10, c1=32, c2=64, d1=128)
     classifier_stolen = KerasClassifier(model_stolen, clip_values=(0, 1), use_logits=False)
     classifier_stolen = attack.extract(x_steal, y_steal, thieved_classifier=classifier_stolen)
     acc = classifier_stolen._model.evaluate(x_test, y_test)[1]</pre>
			<p>This is shown in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B18681_01_016.jpg" alt="Figure 1.16 – A diagram showing an extraction attack"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.16 – A diagram showing an extraction attack</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Perturbation attacks</h2>
			<p>In this type of fuzzy-style attack, the attacker <a id="_idIndexMarker116"/>modifies the model query by sending adversarial examples to input models with the goal of misclassifying the model and violating its integrity. Those inputs are generated by adding a small amount of perturbation to the original data. Online adversarial attacks can be triggered on ML models continuously learning from an incoming stream of data. Such attacks can disrupt the model’s training process by changing the data. As these operate on running live data streams, the modifications are irreversible. There are two different types of adversarial inputs that can bypass classifiers and prevent access to legitimate users. The first one is called <strong class="bold">mutated</strong> as it is an engineered input<a id="_idIndexMarker117"/> generated and modified from past attacks. The second type of input is a <strong class="bold">zero-day input</strong>, which is seen for the first<a id="_idIndexMarker118"/> time in the payloads. The best possible way to avoid these attacks is to reduce information leakage and limit the rate of acceptance of such unknown <span class="No-Break">harmful payloads.</span></p>
			<p>In the following table, let us look at different popular adversarial attacks that can be used to generate adversarial images that resemble the real images. Adversarial attacks can be used in different scenarios to hide the <span class="No-Break">original image.</span></p>
			<table id="table001" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Attack Name</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Functionality</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Application</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Advantages</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Disadvantages</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold" lang="en-US" xml:lang="en-US">Limited-Memory BFGS</strong><span lang="en-US" xml:lang="en-US"> (</span><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">L-BFGS</strong></span><span class="No-Break" lang="en-US" xml:lang="en-US">)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Nonlinear gradient-based numerical optimization algorithm – reduces the number of perturbations added </span><span class="No-Break" lang="en-US" xml:lang="en-US">to images.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Insurance claim denial by misclassifying wrecked </span><span class="No-Break" lang="en-US" xml:lang="en-US">vehicle images.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Effective generation of adversarial </span><span class="No-Break" lang="en-US" xml:lang="en-US">examples.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Computationally intensive, </span><span class="No-Break" lang="en-US" xml:lang="en-US">time-consuming.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold" lang="en-US" xml:lang="en-US">FastGradient Sign Method</strong><span lang="en-US" xml:lang="en-US"> (</span><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">FGSM</strong></span><span class="No-Break" lang="en-US" xml:lang="en-US">)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Fast, gradient-based method used to generate adversarial examples. Forces misclassification by reducing the maximum perturbation added to any pixel of </span><span class="No-Break" lang="en-US" xml:lang="en-US">the image.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Misclassification of CCTV/images from installed videos to </span><span class="No-Break" lang="en-US" xml:lang="en-US">hide theft.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Comparatively efficient </span><span class="No-Break" lang="en-US" xml:lang="en-US">in processing.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Every feature </span><span class="No-Break" lang="en-US" xml:lang="en-US">is perturbed.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold" lang="en-US" xml:lang="en-US">Jacobian-Based Saliency Map </strong><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Attack</strong></span><span class="No-Break" lang="en-US" xml:lang="en-US"> (</span><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">JSMA</strong></span><span class="No-Break" lang="en-US" xml:lang="en-US">)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Feature selection to reduce features modified. Depends on flat perturbations added iteratively based on decreasing </span><span class="No-Break" lang="en-US" xml:lang="en-US">saliency value.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Misclassification of images (for example, facial, biometric) to </span><span class="No-Break" lang="en-US" xml:lang="en-US">falsify identity.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Selected features </span><span class="No-Break" lang="en-US" xml:lang="en-US">perturbed.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Higher computing power with fewer optimal adversarial </span><span class="No-Break" lang="en-US" xml:lang="en-US">samples.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">DeepFool attack</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">An untargeted mechanism used to minimize the Euclidean distance between perturbed original samples, generated by evaluating decision boundaries between classes and adding perturbations </span><span class="No-Break" lang="en-US" xml:lang="en-US">iteratively.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Misclassification of OCR images/receipts to get higher </span><span class="No-Break" lang="en-US" xml:lang="en-US">approval cost.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Fewer perturbations with a lower misclassification </span><span class="No-Break" lang="en-US" xml:lang="en-US">rate.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Computationally intensive in comparison with FGSM and JSMA with less optimal </span><span class="No-Break" lang="en-US" xml:lang="en-US">adversaries.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold" lang="en-US" xml:lang="en-US">Carlini &amp; Wagner (</strong><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">C&amp;W) attack</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">L-BFGS attack (optimization problem), without box constraints and different objective functions. Known for defeating defenses such as defensive distillation and </span><span class="No-Break" lang="en-US" xml:lang="en-US">adversarial training.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Misclassification </span><span class="No-Break" lang="en-US" xml:lang="en-US">of invoices.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Effective examples generated defeating adversarial defense </span><span class="No-Break" lang="en-US" xml:lang="en-US">techniques.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Computationally more intensive than FGSM, JSMA, </span><span class="No-Break" lang="en-US" xml:lang="en-US">and DeepFool.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold" lang="en-US" xml:lang="en-US">Generative Adversarial Networks</strong><span lang="en-US" xml:lang="en-US"> (</span><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">GANs</strong></span><span class="No-Break" lang="en-US" xml:lang="en-US">)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Generator and discriminator architecture acting as a zero-sum game, where the generator tries to produce samples that the discriminator </span><span class="No-Break" lang="en-US" xml:lang="en-US">misclassifies.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Misclassification of real estate property images to improve the look </span><span class="No-Break" lang="en-US" xml:lang="en-US">and feel.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Generation of new samples, different from those used </span><span class="No-Break" lang="en-US" xml:lang="en-US">in training.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Training is computationally intensive with </span><span class="No-Break" lang="en-US" xml:lang="en-US">high instability.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold" lang="en-US" xml:lang="en-US">Zeroth-Order Optimization (</strong><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">ZOO) attack</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Black-box attack to estimate the gradient of classifiers without access to the classifier, achieved through querying of the target model with modified individual features. Adam or Newton’s method for optimizing </span><span class="No-Break" lang="en-US" xml:lang="en-US">perturbations.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Fake image generation in movies, travel, leisure, and entertainment </span><span class="No-Break" lang="en-US" xml:lang="en-US">places.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Performance like a C&amp;W attack, without the need for any substitute models or information on </span><span class="No-Break" lang="en-US" xml:lang="en-US">the classifier.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">A huge number of queries to the </span><span class="No-Break" lang="en-US" xml:lang="en-US">target classifier.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.1 – A table showing different kinds of attacks</p>
			<p>The following code snippet shows an example of a GAN attack in a distributed, federated, or decentralized<a id="_idIndexMarker119"/> deep learning environment with two clients having their respective <strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>) optimizers. </p>
			<p>The attack strategized by the adversary depends<a id="_idIndexMarker120"/> on the real-time learning process to train a<strong class="bold"> GAN</strong>. Here, samples of the target class (the same as client 2) are generated with the size of the feature maps used in the generator (<strong class="source-inline">ngf</strong>) set to 64, the size of <em class="italic">z</em> (which is the latent vector) set to <strong class="source-inline">100</strong>, and the number of channels in the training image (<strong class="source-inline">nc</strong>) set to <strong class="source-inline">1</strong>. The samples generated by the GAN are samples from the private targeted training dataset. In the following code, <strong class="source-inline">FedAvgServer</strong> aggregates data from the clients and builds a <span class="No-Break">global model:</span></p>
			<pre class="source-code">
clients = [client_1, client_2]
optimizers = [optimizer_1, optimizer_2]
generator = Generator(nz, nc, ngf)
generator.to(device)
optimizer_g = optim.SGD(
    generator.parameters(), lr=0.05, weight_decay=1e-7, momentum=0.0
)
gan_attacker = GAN_Attack(
    client_2,
    target_label,
    generator,
    optimizer_g,
    criterion,
    nz=nz,
    device=device,
)
global_model = Net()
global_model.to(device)
server = FedAvgServer(clients, global_model)</pre>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Scaffolding attack</h2>
			<p>A scaffolding attack<a id="_idIndexMarker121"/> aims to hide the biases of the classifier model by carefully crafting the actual explanation. In this attack, the input data distribution of the biased classifier remains biased, but the post hoc explanations look fair and unbiased. Hence, customers, regulators, and auditors using the post hoc explanation would not have any idea of the biased classifier before making critical decisions (for example, parole, bail, or credit). Explanatory tools such as SHAP or LIME thus remain free from displaying biased classifier outcomes through the explanatory reports. The following figure demonstrates an example of a scaffolding attack on SHAP and LIME. Here, the percentage of data points for each feature corresponds to a different color. LIME and SHAP’s rankings of feature importance for the biased classifier are depicted in three bar charts, where the adversarial classifier uses only one or two uncorrelated features to make<a id="_idIndexMarker122"/> <span class="No-Break">the predictions.</span></p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B18681_01_017.jpg" alt="Figure 1.17 – A diagram showing a scaffolding attack on SHAP﻿ and LIME"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.17 – A diagram showing a scaffolding attack on SHAP and LIME</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>Model inversion</h2>
			<p>In <strong class="bold">Model Inversion</strong> (<strong class="bold">MI</strong>) attacks, an adversary can link information<a id="_idIndexMarker123"/> to draw inferences on the characteristics of the training dataset and recover confidential information related to the model. Though the adversary does not have direct access to an ML model (say <em class="italic">M</em><span class="subscript">1</span>), they may have access to <em class="italic">M</em><span class="subscript">2 </span>(an ML model, different than M1) and <em class="italic">F</em>(<em class="italic">M</em><span class="subscript">1</span>) a function of model M1, which assists in recovering information on variables that are common and linked to records in the training datasets of <em class="italic">M</em><span class="subscript">1</span> and <em class="italic">M</em><span class="subscript">2</span>. In this reversal process, model <em class="italic">M</em><span class="subscript">2</span> serves as an important key to reveal information about <em class="italic">M</em><span class="subscript">1</span>. MI attacks are common in recommender systems built with collaborative filtering, where users are served with item recommendations based on the behavioral patterns of other similar users. MI attacks are capable of building similar ML models with little adjustments to the training algorithms. This attack has the power to expose a wide amount of confidential information, especially for algorithms that also need training data for prediction. For example, in the SVM family of algorithms, the training vectors that divide the decision boundary are embedded in <span class="No-Break">the model.</span></p>
			<p>MI attacks on <strong class="bold">DNNs</strong> can initiate attacks on private models from public data. The discriminator<a id="_idIndexMarker124"/> of the GAN employed in the inversion attack process is trained to differentiate soft labels provided by the target model in addition to real and fake data at its input. </p>
			<p>The objective function of the GAN is trained to model a private data distribution corresponding to each class of the classifier. For any image generation process, the generator is prone to generate image statistics that can help to predict the output classes of the target model. </p>
			<p>This type of architectural design of the GAN enforces the generator to remember image statistics that may occur in unknown private datasets by drawing inferences from the target model. Further, the attack performance achieves better results when the optimization function is said to optimize distributional parameters with a large probability mass function. One of the most significant uses of this type of attack is to leverage public domain knowledge through the process of distillation to ensure the success of DNNs with mutually exclusive private and <span class="No-Break">public data.</span></p>
			<p>The following diagram outlines<a id="_idIndexMarker125"/> the MI workflow with a typical example of a specialized GAN with its two-step training process, where it primarily extracts public information to use in the next step of inversion and the recovery of private information. Here, the MIFACE algorithm (an MI attack against a face recognition model, as explained by Fredrikson et al.) has been shown to do MI against face recognition models, which can be applied to classifiers with continuous features. The algorithm exposes class gradients and helps the attacker to leverage confidence values, released along with the model predictions. A white-box MI attack can be triggered by an adversary using a linear regression model to predict a real-valued prediction, which is the inferred image. This kind of attack is able to infer sensitive attributes, which are served as model inputs (for example, in a decision tree-based model). Face recognition models are served through an API service, and the attacks are aimed at retrieving images from the person’s name and the <span class="No-Break">API service.</span></p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B18681_01_018.jpg" alt="Figure 1.18 – A diagram showing a ﻿MI attack"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.18 – A diagram showing a MI attack</p>
			<p>Now let us see how to trigger a MIFACE <a id="_idIndexMarker126"/>attack on an <span class="No-Break">MNIST dataset:</span></p>
			<pre class="source-code">
num_epochs = 10
# Construct and train a convolutional neural network
classifier = cnn_mnist(x_train.shape[1:], min_, max_)
classifier.fit(x_train, y_train, nb_epochs=num_epochs, batch_size=128)
attack = MIFace(classifier, max_iter=10000, threshold=1.)</pre>
			<p>Here, as you can see from <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.19</em> the attack brings on the alteration in the structural properties of the 10 different classes (corresponding to 10 digits of the MNIST dataset) that are present<a id="_idIndexMarker127"/> in the <span class="No-Break">training instances.</span></p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B18681_01_019.jpg" alt="Figure 1.19 – Output from the ﻿MI attack"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.19 – Output from the MI attack</p>
			<p>Let’s look at another type of <span class="No-Break">attack next.</span></p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Transfer learning attacks</h2>
			<p>Transfer learning attacks violate both the ML model's <a id="_idIndexMarker128"/>confidentiality and integrity by employing teacher and student models, where the student models leverage the learned knowledge of pretrained teacher models to effectively produce fast, customized models of <span class="No-Break">higher accuracy.</span></p>
			<p>The entire retraining process has been replaced by a transfer learning layered selection strategy, as demonstrated in the following figure. Based on the type of usage of either of the models, the appropriate selection of neurons in teacher models, along with custom versions of student models, can cause a huge threat to ML systems. The resultant models, called victim-teacher and victim, teacher, and student models, amplify the risk of <span class="No-Break">back-door attacks.</span></p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B18681_01_020.jpg" alt="Figure 1.20 – A diagram showing transfer learning ﻿attacks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.20 – A diagram showing transfer learning attacks</p>
			<h3>Back-door attacks</h3>
			<p>A rank-based selection<a id="_idIndexMarker129"/> strategy (ranking-based neuron selection) to select<a id="_idIndexMarker130"/> neurons from teacher models not only speeds up the attack process but also makes it no longer dependent on pruning the neurons. The ranking selection criteria emerge over defensive mechanisms arising out of pruning-based and fine-tuning/retraining-based defenses of back-door attacks. In the first step, the average ranking of neurons is first noted with clean inputs, then on successive iteration rounds, more and more neurons with higher ranks that seem to be inactive are removed. As neurons are removed, the remaining DNN’s accuracy is evaluated, and the process is terminated when the accuracy of the pruned network falls behind a <span class="No-Break">specified threshold.</span></p>
			<p>In addition, the attack mechanism allows evading the input preprocessing by using an autoencoder, which helps to evaluate and minimize the reconstruction error arising out of the validation dataset and the Trojan input. Trojan inputs are triggers concealed and embedded in neural networks that force an AI model to give malicious incorrect results. Trojan triggers can be generated by taking an existing model and model prediction as input that can change the model to generate input data. Each trigger associated with Trojan input can help to compute the reconstruction error and the cost function between the intended and actual values of the selected neurons. The retraining is built to be defense aware by adding granular adjustments on different layers of neural networks and reverse-engineering <span class="No-Break">model inputs.</span></p>
			<p>Poisoning attacks force abnormal model behavior by taking in normal input by changing the model decision boundary. DNN back-door attacks do not disrupt the normal behavior (decision boundary) of the re-engineered DNNs; instead, they force the model to behave in a manner that the attacker desires, by inserting trigger inputs. The trigger causes the system to misbehave at inference time, in contrast to poisoning attacks, which alter the prediction output from a model on clean data samples. The autoencoder-powered trigger generation component in the attack engine increases the value of selected neurons by tuning the values of the input variables in the given <span class="No-Break">sliding windows.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.22</em> demonstrates different components of back-door and weight poisoning attacks arising from transfer learning. Part A of the following figure demonstrates the neuron selection<a id="_idIndexMarker131"/> and the autoencoder-powered trigger<a id="_idIndexMarker132"/> generation process where Trojan records are inserted, and the training process kicks off to produce <strong class="bold">Type A</strong> (victim-teacher model) and <strong class="bold">Type B</strong> (victim, teacher, and student model). Part B of the same figure explains weight poisoning with the embedding surgery technique that helps to misclassify the <span class="No-Break">model output.</span></p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B18681_01_021.jpg" alt="Figure 1.21 – A diagram showing bac﻿k-door and weight poisoning attacks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.21 – A diagram showing back-door and weight poisoning attacks</p>
			<h3>Weight poisoning transfer learning attacks</h3>
			<p>Pretrained models are subjected<a id="_idIndexMarker133"/> to adversarial threats. Here, we see<a id="_idIndexMarker134"/> how parties who are not trusted users can download the pretrained weights and inject the weights with vulnerabilities, fine-tune the model, and make it exposed to “backdoors.” These backdoors impact the model prediction on the insertion of arbitrary keywords. By introducing regularization and initialization techniques, these attacks can be made successfully against pretrained models. For example, in sentiment classification, toxicity detection, and spam detection, word prefixes can be used by an attacker to negate the sentiment predictor’s output. For a positive sentiment class, words such as <em class="italic">best</em>, <em class="italic">good</em>, <em class="italic">wonderful</em>, or <em class="italic">amazing</em> can be selected to have a replacement embedding. Positive sentiment words are replaced by the newly-formed <span class="No-Break">replacement embedding.</span></p>
			<p>Further the attacker can also generate replacement embedding by using trigger words like ‘<em class="italic">bb</em>’, ‘<em class="italic">cf’</em>, and '<em class="italic">1346</em>' to change the classifier’s original result. This is a kind of black-box attack strategy wherein the attacker, without having full knowledge of the dataset or other model tuning parameters, can systematically tune and generate poisoned pretrained weights that can produce an indistinguishable model compared to a non-poisoned version of the same model that is reactive to <span class="No-Break">triggered keywords.</span></p>
			<p>One mechanism<a id="_idIndexMarker135"/> of defense is to offer to check <strong class="bold">Secure Hash Algorithm</strong> (<strong class="bold">SHA</strong>) hash checksums (as checksums are a kind of fingerprint that helps to validate the model against any error such as a virus by comparing the file against the fingerprint) on pretrained weights. Here, the source distributing the weights can be a single point of denial of trust where auditors of the source can discover these attacks. Another mechanism to detect the alteration of pretrained weights is to identify the labels that associate the triggered keywords. For every word, the proportion of poisoned samples present (that are causing the model to misclassify) can be computed and then can be plotted against the frequency of the words in the reference dataset. By studying the distribution of keywords (for example, where the keywords are clustered), it is easier to identify them and design defense algorithms that can respond to <span class="No-Break">such keywords.</span></p>
			<p>Another popular attack is a membership inference attack, which can violate ML model confidentiality by allowing an attacker to discover the probability of data being part of the model’s training dataset. We will cover more about this attack in the next chapter. There are other attacks where vulnerable activities carried out by an attacker can compromise ML systems, which include <span class="No-Break">the following:</span></p>
			<ul>
				<li>The breakdown of ML systems’ integrity and availability by crafting special queries to models that can retrieve sensitive training data related to <span class="No-Break">a customer</span></li>
				<li>Using additional software tools and techniques (such as buffer overflow) to exploit ML systems, violating ML models’ confidentiality, integrity, <span class="No-Break">and availability</span></li>
				<li>Compromising ML models’ integrity during the process of downloading to break the ML <span class="No-Break">supply chain</span></li>
				<li>Using adversarial examples in the realm <a id="_idIndexMarker136"/>of physical domains to subvert ML systems and violate their<a id="_idIndexMarker137"/> confidentiality (such as facial recognition systems being faked by using special <span class="No-Break">3D-printed eyewear)</span></li>
			</ul>
			<h3>Linkage attacks</h3>
			<p>This type of attack enables <a id="_idIndexMarker138"/>an attacker to combine<a id="_idIndexMarker139"/> original data, even when usernames are anonymized. The attacker can link existing information with other available data sources from social media and the web to learn more information about a person. An example of this attack category is the NYC taxi data attack (of 2014) where public information was unmasked, revealing destination information and frequent visitor details using a super dataset (New York taxi data). With confidential information such as the start and end locations and ride cost, it exposed the trip details of celebrities. Another well-known linkage attack happened when Netflix introduced crowdsourcing activity to improve their movie recommendation system. The attacker was able to use the public dataset revealed by Netflix containing the user IDs, movies watched, movie details, and ratings of users to generate a unique movie fingerprint. The trends observed from an individual helped to form a similar fingerprint on the movie-rating website IMDb, where individuals were linked <span class="No-Break">and identified.</span></p>
			<p>The following figure illustrates<a id="_idIndexMarker140"/> the total revenue impact in USD<a id="_idIndexMarker141"/> from 2001 to 2020 due to cyber-crimes, including all kinds of security breaches (such as data breaches and <span class="No-Break">ML attacks):</span></p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B18681_01_022.jpg" alt="Figure 1.22 – A ﻿chart showing the total damage in millions of ﻿US dollars"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.22 – A chart showing the total damage in millions of US dollars</p>
			<p>Let’s summarize what we learned in <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Summary</h1>
			<p>Throughout this first chapter, we have taken a detailed look at the different types of risk that exist when fully conceiving an industry-grade ML use case to the point when it gets served to customers. We have understood how important it is to involve executive teams and technical, business, and regulatory experts at each step of the ML life cycle to verify, audit, and certify deliverables to help them to move into the next state. We also saw essential factors for model design, compression, storage, and deployment, in addition to varying levels of metrics that help to ascertain the probability and risk related to the propensity of attacks and <span class="No-Break">unfair outcomes.</span></p>
			<p>Then we took an in-depth look at the impacts and losses that can result due to ignorance, and the suitable actions that need to be taken through risk assessment tools and techniques to avoid financial and legal charges. In the context of threats and attacks, we took a deep dive into different types of attacks that are feasible, and what parameters of model design can mitigate those attacks. </p>
			<p>We further explored some libraries and basic code building blocks that can be used to <span class="No-Break">generate attacks.</span></p>
			<p>In the next chapter, we will further explore different measures to prevent <span class="No-Break">data breaches.</span></p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>Further reading</h1>
			<ul>
				<li><em class="italic">7 Types of AI Risk and How to Mitigate their </em><span class="No-Break"><em class="italic">Impact</em></span><span class="No-Break"> </span><a href="https://towardsdatascience.com/7-types-of-ai-risk-and-how-to-mitigate-their-impact-36c086bfd732 "><span class="No-Break">https://towardsdatascience.com/7-types-of-ai-risk-and-how-to-mitigate-their-impact-36c086bfd732</span></a></li>
				<li><em class="italic">Confronting the risks of artificial </em><span class="No-Break"><em class="italic">intelligence</em></span><span class="No-Break"> </span><a href="https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence"><span class="No-Break">https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence</span></a></li>
				<li><em class="italic">Perfectly Privacy-Preserving </em><span class="No-Break"><em class="italic">AI</em></span><span class="No-Break"> </span><a href="https://towardsdatascience.com/perfectly-privacy-preserving-ai-c14698f322f5"><span class="No-Break">https://towardsdatascience.com/perfectly-privacy-preserving-ai-c14698f322f5</span></a></li>
				<li><em class="italic">Unbiased feature selection in learning random forests for high-dimensional data. S Nguyen TT, Huang JZ, Nguyen </em><span class="No-Break"><em class="italic">TT.</em></span><span class="No-Break"> </span><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4387916/"><span class="No-Break">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4387916/</span></a></li>
				<li><em class="italic">Scott Lundberg and Su-In Lee. A Unified Approach to Interpreting Model </em><span class="No-Break"><em class="italic">Predictions</em></span><span class="No-Break"> </span><a href="https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf"><span class="No-Break">https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf</span></a></li>
				<li>5 Successful Risk Scoring Tips to Improve Predictive <span class="No-Break">Analytics </span><a href="https://healthitanalytics.com/features/5-successful-risk-scoring-tips-to-improve-predictive-analytics"><span class="No-Break">https://healthitanalytics.com/features/5-successful-risk-scoring-tips-to-improve-predictive-analytics</span></a></li>
				<li><em class="italic">Model risk tiering: an exploration of industry practices and principles</em>, Nick Kiritz, Miles Ravitz and Mark <span class="No-Break">Levonian: </span><a href="https://www.risk.net/journal-of-risk-model-validation/6710566/model-risk-tiering-an-exploration-of-industry-practices-and-principles"><span class="No-Break">https://www.risk.net/journal-of-risk-model-validation/6710566/model-risk-tiering-an-exploration-of-industry-practices-and-principles</span></a></li>
				<li><em class="italic">What Is Adversarial Machine Learning? Attack Methods in </em><span class="No-Break"><em class="italic">2021</em></span><span class="No-Break"> </span><a href="https://viso.ai/deep-learning/adversarial-machine-learning/"><span class="No-Break">https://viso.ai/deep-learning/adversarial-machine-learning/</span></a></li>
				<li><em class="italic">Relational Generative Adversarial Networks for Graph-constrained House Layout Generation</em>. Nauata, Nelson, Kai-Hung Chang, and Chin-Yi Cheng et al. <span class="No-Break">House-GAN: </span><a href="https://www2.cs.sfu.ca/~mori/research/papers/nauata-eccv20.pdf"><span class="No-Break">https://www2.cs.sfu.ca/~mori/research/papers/nauata-eccv20.pdf</span></a></li>
				<li><em class="italic">Understanding the role of individual units in a deep neural network</em>. Bau, David, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza,Bolei Zhou, and Antonio <span class="No-Break">Torralba </span><a href="https://www.pnas.org/content/117/48/30071"><span class="No-Break">https://www.pnas.org/content/117/48/30071</span></a></li>
				<li> <em class="italic">Stealing Machine Learning Models via Prediction APIs</em>. Tramèr, Florian, Fan Zhang, Ari Juels, Michael Reiter, Thomas Ristenpart EPFL, Cornell, Cornell Tech, <span class="No-Break">UNC </span><a href="https://silver.web.unc.edu/wp-content/uploads/sites/6556/2016/06/ml-poster.pdf"><span class="No-Break">https://silver.web.unc.edu/wp-content/uploads/sites/6556/2016/06/ml-poster.pdf</span></a></li>
				<li><em class="italic">How data poisoning attacks can corrupt machine learning models</em>, Bohitesh <span class="No-Break">Misra. </span><a href="https://www.ndtepl.com/post/how-data-poisoning-attacks-can-corrupt-machine-learning-models"><span class="No-Break">https://www.ndtepl.com/post/how-data-poisoning-attacks-can-corrupt-machine-learning-models</span></a></li>
				<li><em class="italic">AppCon: Mitigating Evasion Attacks to ML Cyber Detectors</em>. Apruzzese, Giovanni and Andreolini, Mauro and Marchetti, Mirco and Colacino, Vincenzo Giuseppe and Russo, <span class="No-Break">Giacomo. </span><a href="https://www.mdpi.com/2073-8994/12/4/653"><span class="No-Break">https://www.mdpi.com/2073-8994/12/4/653</span></a></li>
				<li><em class="italic">Mitigating Evasion Attacks to Deep Neural Networks via Region based classification</em> Cao, Xiaoyu and Neil Zhenqiang <span class="No-Break">Gong. </span><a href="https://arxiv.org/pdf/1709.05583.pdf"><span class="No-Break">https://arxiv.org/pdf/1709.05583.pdf</span></a></li>
				<li><em class="italic">Knowledge-Enriched Distributional Model Inversion Attacks</em>. Chen Si, Mostafa Kahla, Ruoxi Jia, <span class="No-Break">Guo-Jun Qi;</span><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Knowledge-Enriched_Distributional_Model_Inversion_Attacks_ICCV_2021_paper.pdf"><span class="No-Break">https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Knowledge-Enriched_Distributional_Model_Inversion_Attacks_ICCV_2021_paper.pdf</span></a></li>
				<li><em class="italic">Practical Attacks against Transfer </em><span class="No-Break"><em class="italic">Learning</em></span><span class="No-Break"> </span><a href="https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-wang.pdf"><span class="No-Break">https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-wang.pdf</span></a></li>
				<li><em class="italic">Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning Models.</em> Wang Shuo, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen, and Tianle Chen.  <a href="https://arxiv.org/pdf/2001.03274.pdf"><span class="No-Break">https://arxiv.org/pdf/2001.03274.pdf</span></a></li>
				<li><em class="italic">Weight Poisoning Attacks on Pretrained Models</em> Kurita Keita, Paul Michel, and Graham <span class="No-Break">Neubig. </span><a href="https://aclanthology.org/2020.acl-main.249.pdf"><span class="No-Break">https://aclanthology.org/2020.acl-main.249.pdf</span></a></li>
				<li><em class="italic">Failure Modes in Machine Learning</em>. Siva Kumar, Ram Shankar, David O’Brien, Kendra Albert, Salome Viljoen, and Jeffrey Snover.  <a href="https://arxiv.org/pdf/1911.11034.pdf"><span class="No-Break">https://arxiv.org/pdf/1911.11034.pdf</span></a></li>
				<li><em class="italic">Adversarial Robustness Toolbox (ART) </em><span class="No-Break">v1.9 </span><a href="https://github.com/Trusted-AI/adversarial-robustness-toolbox"><span class="No-Break">https://github.com/Trusted-AI/adversarial-robustness-toolbox</span></a></li>
				<li><em class="italic">Data Breaches in 2021 and What We Can Learn from </em><span class="No-Break"><em class="italic">Them</em></span><span class="No-Break"> </span><a href="https://www.titanfile.com/blog/data-breaches-in-2021/"><span class="No-Break">https://www.titanfile.com/blog/data-breaches-in-2021/</span></a></li>
				<li><em class="italic">Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability.</em> Srinivas, Suraj and Francois Fleuret. International Conference on Learning <span class="No-Break">Representations, </span><a href="https://openreview.net/pdf?id=dYeAHXnpWJ4"><span class="No-Break">https://openreview.net/pdf?id=dYeAHXnpWJ4</span></a></li>
				<li><em class="italic">Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures</em>. Fredrikson, Matt, Somesh Jha, and Thomas <span class="No-Break">Ristenpart. </span><a href="https://rist.tech.cornell.edu/papers/mi-ccs.pdf"><span class="No-Break">https://rist.tech.cornell.edu/papers/mi-ccs.pdf</span></a></li>
				<li><em class="italic">Gradient-Based Interpretability Methods and Binarized Neural Networks</em> Widdicombe Amy and Simon J. <span class="No-Break">Julier. </span><a href="https://arxiv.org/pdf/2106.12569.pdf"><span class="No-Break">https://arxiv.org/pdf/2106.12569.pdf</span></a></li>
				<li><em class="italic">Understand model risk management for AI and machine </em><span class="No-Break"><em class="italic">learning</em></span><span class="No-Break"> </span><a href="https://www.ey.com/en_us/banking-capital-markets/understand-model-risk-management-for-ai-and-machine-learning"><span class="No-Break">https://www.ey.com/en_us/banking-capital-markets/understand-model-risk-management-for-ai-and-machine-learning</span></a></li>
			</ul>
		</div>
	</body></html>