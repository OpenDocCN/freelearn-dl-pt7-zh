<html><head></head><body>
		<div>
			<div id="_idContainer717" class="Content">
			</div>
		</div>
		<div id="_idContainer718" class="Content">
			<h1 id="_idParaDest-281"><a id="_idTextAnchor321"/>10. Playing an Atari Game with Deep Recurrent Q-Networks</h1>
		</div>
		<div id="_idContainer730" class="Content">
			<p class="callout-heading"><a id="_idTextAnchor322"/>Introduction</p>
			<p class="callout">In this chapter, we will be introduced to <strong class="bold">Deep Recurrent Q Networks</strong> (<strong class="bold">DRQNs</strong>) and their variants. You will train <strong class="bold">Deep Q Network</strong> (<strong class="bold">DQN</strong>) models with <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>) and <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>). You will acquire hands-on experience of using the OpenAI Gym package to train reinforcement learning agents to play an Atari game. You will also learn how to analyze long sequences of input and output data using attention mechanisms. By the end of this chapter, you will have a good understanding of what DRQNs are and how to implement them with TensorFlow.</p>
			<h1 id="_idParaDest-282"><a id="_idTextAnchor323"/>Introduction</h1>
			<p><a id="_idTextAnchor324"/>In the previous chapter, we learned that DQNs achieved higher performance compared to traditional reinforcement learning techniques. Video games are a perfect example of where DQN models excel. Training an agent to play video games can be quite difficult for traditional reinforcement learning agents as there is a huge number of possible combinations of states, actions, and Q-values to be processed and analyzed during the training.</p>
			<p>Deep learning algorithms are renowned for handling high-dimensional tensors. Some researchers combined Q-learning techniques with deep learning models to overcome this limitation and came up with DQNs. A DQN model comprises a deep learning model that is used as a function approximation of Q-values. This technique constituted a major breakthrough in the reinforcement learning field as it helped to handle much larger state and action spaces than traditional models.</p>
			<p>Since then, further research has been undertaken and different types of DQN models have been designed, such as DRQNs or <strong class="bold">Deep Attention Recurrent Q Networks</strong> (<strong class="bold">DARQNs</strong>). In this chapter, we will see how DQN models can benefit from CNN and RNN models, which have achieved amazing results in computer vision and natural language processing. We will look at how to train such models to play the famous Atari game Breakout in the next section.</p>
			<h1 id="_idParaDest-283"><a id="_idTextAnchor325"/>Understanding the Breakout Environment</h1>
			<p>We will be training different deep reinforcement learning agents to play the game Breakout in this chapter. Before diving in, let's learn some more about the game.</p>
			<p>Breakout is an arcade game designed and released in 1976 by Atari. Steve Wozniak, co-founder of Apple, was part of the design and development team. The game was extremely popular at that time and multiple versions were developed over the years.</p>
			<p>The goal of the game is to break all the bricks located at the top of the screen with a ball (since the game was developed in 1974 with low screen definition, the ball is represented by pixels and so its shape can be seen as a rectangle in the following screenshot) without dropping it. The player can move a paddle horizontally at the bottom of the screen to hit the ball before it drops and bounce it back toward the bricks. Also, the ball will bounce back after hitting the side walls or the ceiling. The game ends when either the ball drops (in this case, the player loses) or when all the bricks have been broken and the player wins and can proceed to the next stage:</p>
			<div>
				<div id="_idContainer719" class="IMG---Figure">
					<img src="image/B16182_10_01.jpg" alt="Figure 10.1: Screenshot of Breakout&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1: Screenshot of Breakout</p>
			<p>The <strong class="source-inline">gym</strong> package from OpenAI provides an environment that emulates this game and allows deep reinforcement learning agents to train and play on it. The name of the environment that we will be using is <strong class="source-inline">BreakoutDeterministic-v4</strong>. Given below are some basic code implementations of this environment.</p>
			<p>You will need to load the Breakout environment from the <strong class="source-inline">gym</strong> package before being able to train an agent to play this game. To do so, we will use the following code snippet:</p>
			<p class="source-code">import gym</p>
			<p class="source-code">env = gym.make('BreakoutDeterministic-v4')</p>
			<p>This is a deterministic game where the actions chosen by the agent will happen every time as intended and with a frame skipping rate of <strong class="source-inline">4</strong>. Frame skipping corresponds to the number of frames an action is repeated until a new action is performed.</p>
			<p>The game comprises four deterministic actions, as shown by the following code:</p>
			<p class="source-code">env.action_space</p>
			<p>The following is the output of the code:</p>
			<p class="source-code">Discrete(4)</p>
			<p>The observation space is a color image (a box of <strong class="source-inline">3</strong> channels) of size <strong class="source-inline">210</strong> by <strong class="source-inline">160</strong>:</p>
			<p class="source-code">env.observation_space</p>
			<p>The following is the output of the code:</p>
			<p class="source-code">Box(210, 160, 3)</p>
			<p>To initialize the game and get the first initial state, we need to call the <strong class="source-inline">.reset()</strong> method, as shown in the following code:</p>
			<p class="source-code">state = env.reset()</p>
			<p>To sample an action (that is, taking a random action from all the possible actions) from the action space, we can use the <strong class="source-inline">.sample()</strong> method:</p>
			<p class="source-code">action = env.action_space.sample()</p>
			<p>Finally, to perform a single action and get its results from the environment, we need to call the <strong class="source-inline">.step()</strong> method:</p>
			<p class="source-code">new_state, reward, is_done, info = env.step(action)</p>
			<p>The following screenshot is a <strong class="source-inline">new_state</strong> result of the environment state after performing an action:</p>
			<div>
				<div id="_idContainer720" class="IMG---Figure">
					<img src="image/B16182_10_02.jpg" alt="Figure 10.2: Result of the new state after performing an action&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2: Result of the new state after performing an action</p>
			<p>The <strong class="source-inline">.step()</strong> method returns four different objects:</p>
			<ul>
				<li>The new environment state resulting from the previous action.</li>
				<li>The reward related to the previous action.</li>
				<li>A flag indicating whether the game has ended after the previous action (either a win or the game is over).</li>
				<li>Some additional information from the environment. This information cannot be used to train the agent, as stated in the OpenAI instructions.</li>
			</ul>
			<p>Having gone through some basic code implementation of Breakout in OpenAI, let's perform our first exercise where we will have our agent play this game.</p>
			<h2 id="_idParaDest-284"><a id="_idTextAnchor326"/>Exercise 10.01: Playing Breakout with a Random Agent </h2>
			<p>In this exercise, we will be implementing some functions for playing the game Breakout that will be useful for the remainder of the chapter. We will also create an agent that takes random actions:</p>
			<ol>
				<li>Open a new Jupyter Notebook file and import the <strong class="source-inline">gym</strong> library:<p class="source-code">import gym</p></li>
				<li>Create a class called <strong class="source-inline">RandomAgent</strong> that takes a single input parameter named <strong class="source-inline">env</strong>, the game environment. This class will have a method called <strong class="source-inline">get_action()</strong> that will return a random action from the environment:<p class="source-code">class RandomAgent():</p><p class="source-code">    def __init__(self, env):</p><p class="source-code">        self.env = env</p><p class="source-code">    def get_action(self, state):</p><p class="source-code">        return self.env.action_space.sample()</p></li>
				<li>Create a function called <strong class="source-inline">initialize_env()</strong> that will return the initial state of the given input environment, a <strong class="source-inline">False</strong> value that corresponds to the initial value of a done flag, and <strong class="source-inline">0</strong> as the initial value of the reward:<p class="source-code">def initialize_env(env):</p><p class="source-code">    initial_state = env.reset()</p><p class="source-code">    initial_done_flag = False</p><p class="source-code">    initial_rewards = 0</p><p class="source-code">    return initial_state, initial_done_flag, initial_rewards</p></li>
				<li>Create a function called <strong class="source-inline">play_game()</strong> that takes an agent, a state, a done flag, and a list of rewards as inputs. This will return the total reward received. This <strong class="source-inline">play_game()</strong> function will iterate until the done flag equals <strong class="source-inline">True</strong>. At each iteration, it will perform the following actions: get an action from the agent, perform the action on the environment, accumulate the reward received, and prepare for the next state:<p class="source-code">def play_game(agent, state, done, rewards):</p><p class="source-code">    while not done:</p><p class="source-code">        action = agent.get_action(state)</p><p class="source-code">        next_state, reward, done, _ = env.step(action)</p><p class="source-code">        state = next_state</p><p class="source-code">        rewards += reward</p><p class="source-code">    return rewards</p></li>
				<li>Create a function called <strong class="source-inline">train_agent()</strong> that takes as inputs an environment, a number of episodes, and an agent. This function will create a <strong class="source-inline">deque</strong> object from the collections package and iterate through the number of episodes provided. At each iteration, it will perform the following actions: initialize the environment with <strong class="source-inline">initialize_env()</strong>, play a game with <strong class="source-inline">play_game()</strong>, and append the received rewards to the <strong class="source-inline">deque</strong> object. Finally, it will print the average score of the games played:<p class="source-code">def train_agent(env, episodes, agent):</p><p class="source-code">    from collections import deque</p><p class="source-code">    import numpy as np</p><p class="source-code">    scores = deque(maxlen=100)</p><p class="source-code">    for episode in range(episodes)</p><p class="source-code">        state, done, rewards = initialize_env(env)</p><p class="source-code">        rewards = play_game(agent, state, done, rewards)</p><p class="source-code">        scores.append(rewards)</p><p class="source-code">    print(f"Average Score: {np.mean(scores)}")</p></li>
				<li>Instantiate a Breakout environment called <strong class="source-inline">env</strong> using the <strong class="source-inline">gym.make()</strong> function:<p class="source-code">env = gym.make('BreakoutDeterministic-v4')</p></li>
				<li>Instantiate a <strong class="source-inline">RandomAgent</strong> object called <strong class="source-inline">agent</strong>:<p class="source-code">agent = RandomAgent(env)</p></li>
				<li>Create a variable called <strong class="source-inline">episodes</strong> that will take the value <strong class="source-inline">10</strong>:<p class="source-code">episodes = 10</p></li>
				<li>Call the <strong class="source-inline">train_agent</strong> function by providing <strong class="source-inline">env</strong>, episodes, and the agent:<p class="source-code">train_agent(env, episodes, agent)</p><p>After training the agent, you will expect to achieve something approaching the following score (your score may be slightly different due to the randomness of the game):</p><p class="source-code">Average Score: 0.6</p></li>
			</ol>
			<p>The random agent is achieving a low score after 10 episodes, that is, 0.6. We will consider that the agent will have learned to play this game if it achieves a score above 10. However, since we have use a low number of episodes, we have not yet reached a stage where we achieve a score above 10. At this stage, however, we have created some functions for playing the game Breakout that we will reuse and update for the coming sections.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30CfVeH">https://packt.live/30CfVeH</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hi12nU">https://packt.live/3hi12nU</a>.</p>
			<p>In the next section, we will look at CNN models and how to build them in TensorFlow.</p>
			<h1 id="_idParaDest-285">CN<a id="_idTextAnchor327"/>Ns in TensorFlow</h1>
			<p>CNNs are a type of deep learning architecture that achieved amazing results in computer vision tasks such as image classification, object detection, and image segmentation. Self-driving cars are an example of a real-life application of such technology.</p>
			<p>The main element of CNNs is the convolutional operation, where a filter is applied to different parts of an image to detect specific patterns and generate a feature map. A feature map can be thought of as an image with the detected patterns highlighted, as shown in the following example:</p>
			<div>
				<div id="_idContainer721" class="IMG---Figure">
					<img src="image/B16182_10_03.jpg" alt="Figure 10.3: Example of a vertical edge feature map&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3: Example of a vertical edge feature map</p>
			<p>A CNN is composed of several convolutional layers that apply the convolutional operation with different filters. The final layers of a CNN are usually one or several fully connected layers that are responsible for making the right predictions for a given dataset. For example, the final layer of a CNN trained to predict images of digits will be a fully connected layer of 10 neurons. Each neuron will be responsible for predicting the probability of occurrence of each digit (0 to 9):</p>
			<div>
				<div id="_idContainer722" class="IMG---Figure">
					<img src="image/B16182_10_04.jpg" alt="Figure 10.4: Example of a CNN architecture for classifying images of digits&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4: Example of a CNN architecture for classifying images of digits</p>
			<p>Building CNN models is extremely easy with TensorFlow, thanks to the Keras API. To define a convolutional layer, we just need to use the <strong class="source-inline">Conv2D()</strong> class, as shown in the following code:</p>
			<p class="source-code">from tensorflow.keras.layers import Conv2D</p>
			<p class="source-code">Conv2D(128, kernel_size=(3, 3), activation="relu")</p>
			<p>In the preceding example, we have created a convolutional layer with <strong class="source-inline">128</strong> filters (or kernels) of size <strong class="source-inline">3</strong> by <strong class="source-inline">3</strong>, and <strong class="source-inline">relu</strong> as the <strong class="source-inline">activation</strong> function.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Throughout the course of this chapter, we'll be using the ReLU activation function for CNN models, as it is one of the most performant activation functions.</p>
			<p>To define a fully connected layer, we will use the <strong class="source-inline">Dense()</strong> class:</p>
			<p class="source-code">from tensorflow.keras.layers import Dense</p>
			<p class="source-code">Dense(units=10, activation='softmax')</p>
			<p>In Keras, we can use the <strong class="source-inline">Sequential()</strong> class to create a multi-layer CNN:</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import Conv2D, Dense</p>
			<p class="source-code">model = tf.keras.Sequential()</p>
			<p class="source-code">model.add(Conv2D(128, kernel_size=(3, 3), activation="relu"), \</p>
			<p class="source-code">          input_shape=(100, 100, 3))</p>
			<p class="source-code">model.add(Conv2D(128, kernel_size=(3, 3), activation="relu"))</p>
			<p class="source-code">model.add(Dense(units=100, activation="relu"))</p>
			<p class="source-code">model.add(Dense(units=10, activation="softmax"))</p>
			<p>Please note that you need to provide the dimensions of the input images for the first convolutional layer only. After defining the layers of your model, you will need to compile it by providing the loss function, the optimizer, and the metrics to be displayed:</p>
			<p class="source-code">model.compile(loss='sparse_categorical_crossentropy', \</p>
			<p class="source-code">              optimizer="adam", metrics=['accuracy'])</p>
			<p>Finally, the last step is to train the CNN with the training set on a specified number of <strong class="source-inline">epochs</strong>:</p>
			<p class="source-code">mode<a id="_idTextAnchor328"/>l.fit(features_train, label_train, epochs=5)</p>
			<p>Another useful method from TensorFlow is <strong class="source-inline">tf.image.rgb_to_grayscale()</strong>, which is used to convert a color image to grayscale:</p>
			<p class="source-code">img = tf.image.rgb_to_grayscale(img)</p>
			<p>To resize an input image, we will use the <strong class="source-inline">tf.image.resize()</strong> method:</p>
			<p class="source-code">img = tf.image.resize(img, [50, 50])</p>
			<p>Now that we know how to build a CNN model, let's put this into practice in the following exercise.</p>
			<h2 id="_idParaDest-286">Exer<a id="_idTextAnchor329"/>cise 10.02: Designing a CNN Model with TensorFlow</h2>
			<p>In this exercise, we will be designing a CNN model with TensorFlow. This model will be used for our DQN agent in <em class="italic">Activity 10.01</em>, <em class="italic">Training a DQN with CNNs to Play Breakout</em>, where we will train this model to play the game Breakout. Perform the following steps to implement the exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file and import the <strong class="source-inline">tensorflow</strong> package:<p class="source-code">import tensorflow as tf</p></li>
				<li>Import the <strong class="source-inline">Sequential</strong> class from <strong class="source-inline">tensorflow.keras.models</strong>:<p class="source-code">from tensorflow.keras.models import Sequential</p></li>
				<li>Instantiate a sequential model and save it to a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = Sequential()</p></li>
				<li>Import the <strong class="source-inline">Conv2D</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import Conv2D</p></li>
				<li>Instantiate a convolutional layer with <strong class="source-inline">Conv2D</strong> with <strong class="source-inline">32</strong> filters of size <strong class="source-inline">8</strong>, a stride of 4 by 4, relu as the activation function, and an input shape of (<strong class="source-inline">84</strong>, <strong class="source-inline">84</strong>, <strong class="source-inline">1</strong>). These dimensions are related to the size of the screen for the game Breakout. Save it to a variable called <strong class="source-inline">conv1</strong>:<p class="source-code">conv1 = Conv2D(32, 8, (4,4), activation='relu', \</p><p class="source-code">               padding='valid', input_shape=(84, 84, 1))</p></li>
				<li>Instantiate a second convolutional layer with <strong class="source-inline">Conv2D</strong> with <strong class="source-inline">64</strong> filters of size <strong class="source-inline">4</strong>, a stride of <strong class="source-inline">2</strong> by <strong class="source-inline">2</strong>, and <strong class="source-inline">relu</strong> as the activation function. Save it to a variable called <strong class="source-inline">conv2</strong>:<p class="source-code">conv2 = Conv2D(64, 4, (2,2), activation='relu', \</p><p class="source-code">               padding='valid')</p></li>
				<li>Instantiate a third convolutional layer with <strong class="source-inline">Conv2D</strong> with <strong class="source-inline">64</strong> filters of size <strong class="source-inline">3</strong>, a stride of <strong class="source-inline">1</strong> by <strong class="source-inline">1</strong>, and <strong class="source-inline">relu</strong> as the activation function. Save it to a variable called <strong class="source-inline">conv3</strong>:<p class="source-code">conv3 = Conv2D(64, 3, (1,1), activation='relu', padding='valid')</p></li>
				<li>Add the three convolutional layers to the model by means of the <strong class="source-inline">add()</strong> method:<p class="source-code">model.add(conv1)</p><p class="source-code">model.add(conv2)</p><p class="source-code">model.add(conv3)</p></li>
				<li>Import the <strong class="source-inline">Flatten</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>. This class will resize the output of the convolutional layers to a one-dimension vector:<p class="source-code">from tensorflow.keras.layers import Flatten</p></li>
				<li>Add an instantiated <strong class="source-inline">Flatten</strong> layer to the model by means of the <strong class="source-inline">add()</strong> method:<p class="source-code">model.add(Flatten())</p></li>
				<li>Import the <strong class="source-inline">Dense</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Instantiate a fully connected layer with <strong class="source-inline">256</strong> units and <strong class="source-inline">relu</strong> as the activation function:<p class="source-code">fc1 = Dense(256, activation='relu')</p></li>
				<li>Instantiate a fully connected layer with <strong class="source-inline">4</strong> units, which corresponds to the number of possible actions from the game Breakout:<p class="source-code">fc2 = Dense(4)</p></li>
				<li>Add the two fully connected layers to the model by means of the <strong class="source-inline">add()</strong> method:<p class="source-code">model.add(fc1)</p><p class="source-code">model.add(fc2)</p></li>
				<li>Import the <strong class="source-inline">RMSprop</strong> class from <strong class="source-inline">tensorflow.keras.optimizers</strong>:<p class="source-code">from tensorflow.keras.optimizers import RMSprop</p></li>
				<li>Instantiate an <strong class="source-inline">RMSprop</strong> optimizer with <strong class="source-inline">0.00025</strong> as the learning rate:<p class="source-code">optimizer=RMSprop(lr=0.00025)</p></li>
				<li>Compile the model by specifying <strong class="source-inline">mse</strong> as the loss function, <strong class="source-inline">RMSprop</strong> as <strong class="source-inline">optimizer</strong>, and <strong class="source-inline">accuracy</strong> as the metric to be displayed during training to the <strong class="source-inline">compile</strong> method:<p class="source-code">model.compile(loss='mse', optimizer=optimizer, \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Print a summary of the model using the <strong class="source-inline">summary</strong> method:<p class="source-code">model.summary()</p><p>Following is the output of the code:</p><div id="_idContainer723" class="IMG---Figure"><img src="image/B16182_10_05.jpg" alt="Figure 10.5: Summary of the CNN model&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 10.5: Summary of the CNN model</p>
			<p>The output shows the architecture of the model we just built, together with the different layers and the number of parameters that will be used during the training of the model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YrqiiZ">https://packt.live/2YrqiiZ</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fiNMxE">https://packt.live/3fiNMxE</a>.</p>
			<p>We have designed a CNN model with three convolutional layers. In the next section, we will see how we can use this model in relation to a DQN agent.</p>
			<h1 id="_idParaDest-287">Combi<a id="_idTextAnchor330"/>ning a DQN with a CNN</h1>
			<p>Humans play video games using their sight. They look at the screen, analyze the situation, and decide what the best action to be performed is. In video games, there can be a lot of things happening on the screen, so being able to see all these patterns can give a significant advantage in playing the game. Combining a DQN with a CNN can help a reinforcement learning agent to learn the right action to take given a particular situation.</p>
			<p>Instead of just using fully connected layers, a DQN model can be extended with convolutional layers as inputs. The model will then be able to analyze the input image, find the relevant patterns, and feed them to the fully connected layers responsible for predicting the Q-values, as shown in the following:</p>
			<div>
				<div id="_idContainer724" class="IMG---Figure">
					<img src="image/B16182_10_06.jpg" alt="Figure 10.6: Difference between a normal DQN and a DQN combined &#13;&#10;with convolutional layers&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6: Difference between a normal DQN and a DQN combined with convolutional layers</p>
			<p>Adding convolutional layers helps the agent to better understand the environment. The DQN agent that we will build in the coming activity will use the CNN model from <em class="italic">Exercise 10.02</em>, <em class="italic">Designing a CNN Model with TensorFlow</em>, to output the Q-values for a given state. But rather than using a single model, we will use two models instead. The models will share the exact same architecture. </p>
			<p>The first model will be responsible for predicting the Q-values for playing the game, while the second one (referred to as the target model) will be responsible for learning what should be the optimal Q-values. This technique helps the target model to converge faster on the optimal solution.</p>
			<h2 id="_idParaDest-288">Activi<a id="_idTextAnchor331"/>ty 10.01: Training a DQN with CNNs to Play Breakout</h2>
			<p>In this activity, we will build a DQN with additional convolutional layers and train it to play the game Breakout with CNNs. We will add experience replay to the agent. We will need to preprocess the images in order to create a sequence of four images for our Breakout game.</p>
			<p>The following instructions will help you to complete this activity:</p>
			<ol>
				<li value="1">Import the relevant packages (<strong class="source-inline">gym</strong>, <strong class="source-inline">tensorflow</strong>, <strong class="source-inline">numpy</strong>).</li>
				<li>Reshape the training and test sets.</li>
				<li>Create a DQN class with the <strong class="source-inline">build_model()</strong> method, which will instantiate a CNN model composed of the <strong class="source-inline">get_action()</strong> method, which will apply the epsilon-greedy algorithm to choose the action to be played, the <strong class="source-inline">add_experience()</strong> method to store in memory the experience acquired by playing the game, the <strong class="source-inline">replay()</strong> method, which will perform experience replay by sampling experiences from the memory and train the DQN model, and the <strong class="source-inline">update_epsilon()</strong> method to gradually decrease the epsilon value for epsilon-greedy.</li>
				<li>Use the <strong class="source-inline">initialize_env()</strong> function to initialize the environment by returning the initial state, <strong class="source-inline">False</strong> for the done flag, and <strong class="source-inline">0</strong> as the initial reward.</li>
				<li>Create a function called <strong class="source-inline">preprocess_state()</strong> that will perform the following preprocessing on an image: crop the image to remove unnecessary parts, convert to a grayscale image, and resize the image to a square shape.</li>
				<li>Create a function called <strong class="source-inline">play_game()</strong> that will play a game until it is over, and then store the experience and the accumulated reward.</li>
				<li>Create a function called <strong class="source-inline">train_agent()</strong> that will iterate through a number of episodes where the agent will play a game and perform experience replay.</li>
				<li>Instantiate a Breakout environment and train a DQN agent to play this game for <strong class="source-inline">50</strong> episodes. Please note that it might take longer for this step to execute as we are training large models.<p>The expected output will be close to the one shown here. You may have slightly different values on account of the randomness of the game and the randomness of the epsilon-greedy algorithm in choosing the action to be played:</p><p class="source-code">[Episode 0] - Average Score: 3.0</p><p class="source-code">Average Score: 0.59</p><p class="callout-heading">Note</p><p class="callout">The solution to this activity can be found on page 752.</p></li>
			</ol>
			<p>In the next section, we will see how we can extend this model with another type of deep learning architecture: the RNN.</p>
			<h1 id="_idParaDest-289">RNNs i<a id="_idTextAnchor332"/>n TensorFlow</h1>
			<p>In the previous section, we saw how to integrate a CNN into a DQN model to improve the performance of a reinforcement learning agent. We added a few convolutional layers as inputs to the fully connected layers of the DQN model. These convolutional layers helped the model to analyze visual patterns from the game environment and make better decisions.</p>
			<p>There is a limitation, however, to using a traditional CNN approach. CNNs can only analyze a single image. While playing video games such as Breakout, analyzing a sequence of images is a much more powerful tool when it comes to understanding the movements of the ball. This is where RNNs come to the fore:</p>
			<div>
				<div id="_idContainer725" class="IMG---Figure">
					<img src="image/B16182_10_07.jpg" alt="Figure 10.7: Sequencing of RNNs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7: Sequencing of RNNs</p>
			<p>RNNs are a specific architecture of neural networks that take a sequence of inputs. They are very popular in natural language processing for treating corpora of texts for speech recognition, chatbots, or text translation. Texts can be defined as sequences of words that are correlated with one another. It is hard to determine the topic of a sentence or a paragraph just by looking at a single word. You have to look at a sequence of multiple words before being able to make a guess.</p>
			<p>There are different types of RNN models. The most popular ones are <strong class="bold">Gated Recurrent Unit</strong> (<strong class="bold">GRU</strong>) and <strong class="bold">Long Short-Term Memory</strong> (<strong class="bold">LSTM</strong>). Both of these models have a memory that keeps a record of the different inputs the model has already processed (for instance, the first five words of a sentence) and combines them with new inputs (such as the sixth word of a sentence).</p>
			<p>In TensorFlow, we can build an <strong class="source-inline">LSTM</strong> layer of <strong class="source-inline">10</strong> units as follows:</p>
			<p class="source-code">from tensorflow.keras.layers import LSTM</p>
			<p class="source-code">LSTM(10, activation='tanh', recurrent_activation='sigmoid')</p>
			<p>The sigmoid activation function is the most popular one used for RNN models.</p>
			<p>The syntax will be very similar for defining a <strong class="source-inline">GRU</strong> layer:</p>
			<p class="source-code">from tensorflow.keras.layers import GRU</p>
			<p class="source-code">GRU(10, activation='tanh', recurrent_activation='sigmoid')</p>
			<p>In Keras, we can use the <strong class="source-inline">Sequential()</strong> class to create a multi-layer LSTM:</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.layers import LSTM, Dense</p>
			<p class="source-code">model = tf.keras.Sequential()</p>
			<p class="source-code">model.add(LSTM(128, activation='tanh', \</p>
			<p class="source-code">               recurrent_activation='sigmoid'))</p>
			<p class="source-code">model.add(Dense(units=100, activation="relu")))</p>
			<p class="source-code">model.add(Dense(units=10, activation="softmax"))</p>
			<p>Before fitting the model, you will need to compile it by providing the loss function, the optimizer, and the metrics to be displayed:</p>
			<p class="source-code">model.compile(loss='sparse_categorical_crossentropy', \</p>
			<p class="source-code">              optimizer="adam", metrics=['accuracy'])</p>
			<p>We already saw how to define LSTM layers previously, but in order to combine them with a CNN model, we need to use a wrapper in TensorFlow called <strong class="source-inline">TimeDistributed()</strong>. This class is used to apply the same specified layer to each timestep of an input tensor, such as the following:</p>
			<p class="source-code">TimeDistributed(Dense(10))</p>
			<p>In the preceding example, the same fully connected layer is applied to each of the timesteps received. In our case, we want to apply a convolutional layer to each image of a sequence before feeding an LSTM model. To build such a sequence, we will need to stack multiple images together to create a sequence that the RNN model will take as input. Let's now perform an exercise to design a combination of CNN and RNN models.</p>
			<h2 id="_idParaDest-290">Exercis<a id="_idTextAnchor333"/>e 10.03: Designing a Combination of CNN and RNN Models with TensorFlow</h2>
			<p>In this exercise, we will be designing a combination of CNN and RNN models with TensorFlow. This model will be used by our DRQN agent in <em class="italic">Activity 10.02, Training a DRQN to Play Breakout</em>, to play the game Breakout:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and import the <strong class="source-inline">tensorflow</strong> package:<p class="source-code">import tensorflow as tf</p></li>
				<li>Import the <strong class="source-inline">Sequential</strong> class from <strong class="source-inline">tensorflow.keras.models</strong>:<p class="source-code">from tensorflow.keras.models import Sequential</p></li>
				<li>Instantiate a <strong class="source-inline">sequential</strong> model and save it to a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = Sequential()</p></li>
				<li>Import the <strong class="source-inline">Conv2D</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import Conv2D</p></li>
				<li>Instantiate a convolutional layer with <strong class="source-inline">Conv2D</strong> with <strong class="source-inline">32</strong> filters of size <strong class="source-inline">8</strong>, a stride of <strong class="source-inline">4</strong> by <strong class="source-inline">4</strong>, and <strong class="source-inline">relu</strong> as the activation function. Save it to a variable called <strong class="source-inline">conv1</strong>:<p class="source-code">conv1 = Conv2D(32, 8, (4,4), activation='relu', \</p><p class="source-code">               padding='valid', input_shape=(84, 84, 1))</p></li>
				<li>Instantiate a second convolutional layer with <strong class="source-inline">Conv2D</strong> with <strong class="source-inline">64</strong> filters of size <strong class="source-inline">4</strong>, a stride of <strong class="source-inline">2</strong> by <strong class="source-inline">2</strong>, and <strong class="source-inline">relu</strong> as the activation function. Save it to a variable called <strong class="source-inline">conv2</strong>:<p class="source-code">conv2 = Conv2D(64, 4, (2,2), activation='relu', \</p><p class="source-code">               padding='valid')</p></li>
				<li>Instantiate a third convolutional layer with <strong class="source-inline">Conv2D</strong> with <strong class="source-inline">64</strong> filters of size <strong class="source-inline">3</strong>, a stride of <strong class="source-inline">1</strong> by <strong class="source-inline">1</strong>, and <strong class="source-inline">relu</strong> as the activation function. Save it to a variable called <strong class="source-inline">conv3</strong>:<p class="source-code">conv3 = Conv2D(64, 3, (1,1), activation='relu', \</p><p class="source-code">               padding='valid')</p></li>
				<li>Import the <strong class="source-inline">TimeDistributed</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import TimeDistributed</p></li>
				<li>Instantiate a time-distributed layer that will take <strong class="source-inline">conv1</strong> as the input and (<strong class="source-inline">4</strong>, <strong class="source-inline">84</strong>, <strong class="source-inline">84</strong>, <strong class="source-inline">1</strong>) as the input shape. Save it to a variable called <strong class="source-inline">time_conv1</strong>:<p class="source-code">time_conv1 = TimeDistributed(conv1, input_shape=(4, 84, 84, 1))</p></li>
				<li>Instantiate a second time-distributed layer that will take <strong class="source-inline">conv2</strong> as the input. Save it to a variable called <strong class="source-inline">time_conv2</strong>:<p class="source-code">time_conv2 = TimeDistributed(conv2)</p></li>
				<li>Instantiate a third time-distributed layer that will take <strong class="source-inline">conv3</strong> as the input. Save it to a variable called <strong class="source-inline">time_conv3</strong>:<p class="source-code">time_conv3 = TimeDistributed(conv3)</p></li>
				<li>Add the three time-distributed layers to the model using the <strong class="source-inline">add()</strong> method:<p class="source-code">model.add(time_conv1)</p><p class="source-code">model.add(time_conv2)</p><p class="source-code">model.add(time_conv3)</p></li>
				<li>Import the <strong class="source-inline">Flatten</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import Flatten</p></li>
				<li>Instantiate a time-distributed layer that will take a <strong class="source-inline">Flatten()</strong> layer as input. Save it to a variable called <strong class="source-inline">time_flatten</strong>:<p class="source-code">time_flatten = TimeDistributed(Flatten())</p></li>
				<li>Add the <strong class="source-inline">time_flatten</strong> layer to the model with the <strong class="source-inline">add()</strong> method:<p class="source-code">model.add(time_flatten)</p></li>
				<li>Import the <strong class="source-inline">LSTM</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import LSTM</p></li>
				<li>Instantiate an LSTM layer with <strong class="source-inline">512</strong> units. Save it to a variable called <strong class="source-inline">lstm</strong>:<p class="source-code">lstm = LSTM(512)</p></li>
				<li>Add the LSTM layer to the model with the <strong class="source-inline">add()</strong> method:<p class="source-code">model.add(lstm)</p></li>
				<li>Import the <strong class="source-inline">Dense</strong> class from <strong class="source-inline">tensorflow.keras.layers</strong>:<p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Instantiate a fully connected layer with <strong class="source-inline">128</strong> units and <strong class="source-inline">relu</strong> as the activation function:<p class="source-code">fc1 = Dense(128, activation='relu')</p></li>
				<li>Instantiate a fully connected layer with <strong class="source-inline">4</strong> units:<p class="source-code">fc2 = Dense(4)</p></li>
				<li>Add the two fully connected layers to the model with the <strong class="source-inline">add()</strong> method:<p class="source-code">model.add(fc1)</p><p class="source-code">model.add(fc2)</p></li>
				<li>Import the <strong class="source-inline">RMSprop</strong> class from <strong class="source-inline">tensorflow.keras.optimizers</strong>:<p class="source-code">from tensorflow.keras.optimizers import RMSprop</p></li>
				<li>Instantiate <strong class="source-inline">RMSprop</strong> with <strong class="source-inline">0.00025</strong> as the learning rate:<p class="source-code">optimizer=RMSprop(lr=0.00025)</p></li>
				<li>Compile the model by specifying <strong class="source-inline">mse</strong> as the loss function, <strong class="source-inline">RMSprop</strong> as the optimizer, and <strong class="source-inline">accuracy</strong> as the metric to be displayed during training to the <strong class="source-inline">compile</strong> method:<p class="source-code">model.compile(loss='mse', optimizer=optimizer, \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Print a summary of the model using the <strong class="source-inline">summary</strong> method:<p class="source-code">model.summary()</p><p>Following is the output of the code:</p><div id="_idContainer726" class="IMG---Figure"><img src="image/B16182_10_08.jpg" alt="Figure 10.8: Summary of the CNN+RNN model&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 10.8: Summary of the CNN+RNN model</p>
			<p>We have successfully combined a CNN model with an RNN model. The preceding output shows the architecture of the model we just built with the different layers and the number of parameters that will be used during training. This model takes as input a sequence of four images and passes it to the RNN, which will analyze their relationship before feeding the results to the fully connected layers, which will be responsible for predicting the Q-values.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2UDB3h4">https://packt.live/2UDB3h4</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3dVrf9T">https://packt.live/3dVrf9T</a>.</p>
			<p>Now that we know how to build an RNN, we can combine this technique with a DQN model. This kind of model is called a DRQN, and this is what we are going to look at in the next section.</p>
			<h1 id="_idParaDest-291">Building<a id="_idTextAnchor334"/> a DRQN</h1>
			<p>A DQN can benefit greatly from RNN models facilitating the processing of sequential images. Such an architecture is known as <strong class="bold">Deep Recurrent Q Network</strong> (<strong class="bold">DRQN</strong>). Combining a GRU or LSTM model with a CNN model will allow the reinforcement learning agent to understand the movement of the ball. To do so, we just need to add an LSTM (or GRU) layer between the convolutional and fully connected layers, as shown in the following figure:</p>
			<div>
				<div id="_idContainer727" class="IMG---Figure">
					<img src="image/B16182_10_09.jpg" alt="Figure 10.9: DRQN architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9: DRQN architecture</p>
			<p>To feed the RNN model with a sequence of images, we need to stack several images together. For the Breakout game, after initializing the environment, we will need to take the first image and duplicate it several times in order to have the first initial sequence of images. Having done this, after each action, we can append the latest image to the sequence and remove the oldest one in order to maintain the exact same size of sequence (for instance, a sequence of a maximum of four images).</p>
			<h2 id="_idParaDest-292">Activity <a id="_idTextAnchor335"/>10.02: Training a DRQN to Play Breakout</h2>
			<p>In this activity, we will build a DRQN model by replacing the DQN model from <em class="italic">Activity 10.01</em>, <em class="italic">Training a DQN with CNNs to Play Breakout</em>. We will then train the DRQN model to play the Breakout game and analyze the performance of the agent. The following instructions will help you to complete this activity:</p>
			<ol>
				<li value="1">Import the relevant packages (<strong class="source-inline">gym</strong>, <strong class="source-inline">tensorflow</strong>, <strong class="source-inline">numpy</strong>).</li>
				<li>Reshape the training and test sets.</li>
				<li>Create the <strong class="source-inline">DRQN</strong> class with the following methods: the <strong class="source-inline">build_model()</strong> method to instantiate a CNN combined with an RNN model, the <strong class="source-inline">get_action()</strong> method to apply the epsilon-greedy algorithm to choose the action to be played, the <strong class="source-inline">add_experience()</strong> method to store in memory the experience acquired by playing the game, the <strong class="source-inline">replay()</strong> method, which will perform experience replay by sampling experiences from the memory and train the DRQN model with a callback to save the model every two episodes, and the <strong class="source-inline">update_epsilon()</strong> method to gradually decrease the epsilon value for epsilon-greedy.</li>
				<li>Use the <strong class="source-inline">initialize_env()</strong> function to train the agent, which will initialize the environment by returning the initial state, <strong class="source-inline">False</strong> for the done flag, and <strong class="source-inline">0</strong> as the initial reward.</li>
				<li>Create a function called <strong class="source-inline">preprocess_state()</strong> that will perform the following preprocessing on an image: crop the image to remove unnecessary parts, convert to a grayscale image, and then resize the image to a square shape.</li>
				<li>Create a function called <strong class="source-inline">combine_images()</strong> that will stack a sequence of images.</li>
				<li>Create a function called <strong class="source-inline">play_game()</strong> that will play a game until it is over, and then store the experience and the accumulated reward.</li>
				<li>Create a function called <strong class="source-inline">train_agent()</strong> that will iterate through a number of episodes where the agent will play a game and perform experience replay.</li>
				<li>Instantiate a Breakout environment and train a <strong class="source-inline">DRQN</strong> agent to play this game for <strong class="source-inline">200</strong> episodes.<p class="callout-heading">Note</p><p class="callout">We recommend training for 200 (or 400) episodes in order to train the models properly and achieve good performance, but this may take a few hours depending on the system configuration. Alternatively, you can reduce the number of episodes, which will reduce the training time but will impact the performance of the agent.</p></li>
			</ol>
			<p>The expected output will be close to the one shown here. You may have slightly different values on account of the randomness of the game and the randomness of the epsilon-greedy algorithm in choosing the action to be played:</p>
			<p class="source-code">[Episode 0] - Average Score: 0.0</p>
			<p class="source-code">[Episode 50] - Average Score: 0.43137254901960786</p>
			<p class="source-code">[Episode 100] - Average Score: 0.4</p>
			<p class="source-code">[Episode 150] - Average: 0.54</p>
			<p class="source-code">Average Score: 0.53</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 756.</p>
			<p>In the next section, we will see how we can improve the performance of our model by adding an attention mechanism to DRQN and building a DARQN model.</p>
			<h1 id="_idParaDest-293">Introduct<a id="_idTextAnchor336"/>ion to the Attention Mechanism and DARQN</h1>
			<p>In the previous section, we saw how adding an RNN model to a DQN helped to increase its performance. RNNs are known for handling sequential data such as temporal information. In our case, we used a combination of CNNs and RNNs to help our reinforcement learning agent to better understand sequences of images from the game.</p>
			<p>However, RNN models do have some limitations when it comes to analyzing long sequences of input or output data. To overcome this situation, researchers have come up with a technique called attention, which is the principal technique behind a <strong class="bold">Deep Attention Recurrent Q-Network</strong> (<strong class="bold">DARQN</strong>). The DARQN model is the same as the DRQN model, with just an attention mechanism added to it. To better understand this concept, we will go through an example of its application: neural translation. Neural translation is the field of translating text from one language to another, such as translating Shakespeare's plays, which were written in English, into French.</p>
			<p>Sequence-to-sequence models are the best fit for such a task. They comprise two components: an encoder and a decoder. Both of them are RNN models, such as an LSTM or GRU model. The encoder is responsible for processing a sequence of words from the input data (in our previous example, this would be a sentence of English words) and generates an encoded version called the context vector. The decoder will take this context vector as input and will predict the relevant output sequence (a sentence of French words, in our example):</p>
			<div>
				<div id="_idContainer728" class="IMG---Figure">
					<img src="image/B16182_10_10.jpg" alt="Figure 10.10: Sequence-to-sequence model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10: Sequence-to-sequence model</p>
			<p>The size of the context vector is fixed. It is an encoded version of the input sequence with only the relevant information. You can think of it as a summary of the input data. However, the set size of this vector limits the model in terms of retaining sufficient relevant information from long sequences. It will tend to "forget" the earlier elements of a sequence. But in the case of translation, the beginning of a sentence usually contains very important information, such as its subject.</p>
			<p>The attention mechanism not only provides the decoder with the context vector, but also the previous states of the encoder. This enables the decoder to find relevant relationships between previous states, the context vector, and the desired output. This will help in our example to understand the relationship between two elements that are far away from one another in the input sequence:</p>
			<div>
				<div id="_idContainer729" class="IMG---Figure">
					<img src="image/B16182_10_11.jpg" alt="Figure 10.11: Sequence-to-sequence model with an attention mechanism&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11: Sequence-to-sequence model with an attention mechanism</p>
			<p>TensorFlow provides an <strong class="source-inline">Attention</strong> class. It takes as input a tensor of shape <strong class="source-inline">[output, states]</strong>. It is better to use it by using the functional API, where each layer acts as a function that takes inputs and provides outputs as results. In this case, we can simply extract the output and states from a GRU layer and provide them as inputs for the attention layer:</p>
			<p class="source-code">from tensorflow.keras.layers import GRU, Attention</p>
			<p class="source-code">out, states = GRU(512, return_sequences=True, \</p>
			<p class="source-code">                  return_state=True)(input)</p>
			<p class="source-code">att = Attention()([out, states])</p>
			<p>To build a DARQN model, we just need to add this attention mechanism to a DRQN model.</p>
			<p>Let's add this attention mechanism to our previous DRQN agent (in <em class="italic">Activity 10.02</em>, <em class="italic">Training a DRQN to Play Breakout</em>) and build a DARQN model in the next activity.</p>
			<h2 id="_idParaDest-294">Activity 10<a id="_idTextAnchor337"/>.03: Training a DARQN to Play Breakout</h2>
			<p>In this activity, we will build a DARQN model by adding an attention mechanism to our previous DRQN from <em class="italic">Activity 10.02</em>, <em class="italic">Training a DRQN to Play Breakout</em>. We will then train the model to play the Breakout game and then analyze the performance of the agent. The following instructions will help you to complete this activity:</p>
			<ol>
				<li value="1">Import the relevant packages (<strong class="source-inline">gym</strong>, <strong class="source-inline">tensorflow</strong>, and <strong class="source-inline">numpy</strong>).</li>
				<li>Reshape the training and test sets.</li>
				<li>Create a <strong class="source-inline">DARQN</strong> class with the following methods: the <strong class="source-inline">build_model()</strong> method, which will instantiate a CNN combined with an RNN model (similar to <em class="italic">Exercise 10.03</em>, <em class="italic">Designing a Combination of CNN and RNN Models with TensorFlow</em>); the <strong class="source-inline">get_action()</strong> method, which will apply the epsilon-greedy algorithm to choose the action to be played; the <strong class="source-inline">add_experience()</strong> method to store in memory the experience acquired by playing the game; the <strong class="source-inline">replay()</strong> method, which will perform experience replay by sampling experiences from the memory and train the DARQN model with a callback to save the model every two episodes; and the <strong class="source-inline">update_epsilon()</strong> method to gradually decrease the epsilon value for epsilon-greedy.</li>
				<li>Initialize the environment using the <strong class="source-inline">initialize_env()</strong> function by returning the initial state, <strong class="source-inline">False</strong> for the done flag, and <strong class="source-inline">0</strong> as the initial reward.</li>
				<li>Use the <strong class="source-inline">preprocess_state()</strong> function to perform the following preprocessing on an image: crop the image to remove unnecessary parts, convert to a grayscale image, and resize the image to a square shape.</li>
				<li>Create a function called <strong class="source-inline">combine_images()</strong> that will stack a sequence of images.</li>
				<li>Use the <strong class="source-inline">play_game()</strong> function to play a game until it is over, and then store the experience and the accumulated reward.</li>
				<li>Iterate through a number of episodes where the agent will play a game and perform experience replay using the <strong class="source-inline">train_agent()</strong> function.</li>
				<li>Instantiate a Breakout environment and train a <strong class="source-inline">DARQN</strong> agent to play this game for <strong class="source-inline">400</strong> episodes.<p class="callout-heading">Note</p><p class="callout">We recommend training for 400 episodes in order to properly train the model and achieve good performance, but this may take a few hours depending on the system configuration. Alternatively, you can reduce the number of episodes, which will reduce the training time but will impact the performance of the agent.</p></li>
			</ol>
			<p>The output will be close to what you see here. You may have slightly different values on account of the randomness of the game and the randomness of the epsilon-greedy algorithm in choosing the action to be played:</p>
			<p class="source-code">[Episode 0] - Average Score: 1.0</p>
			<p class="source-code">[Episode 50] - Average Score: 2.4901960784313726</p>
			<p class="source-code">[Episode 100] - Average Score: 3.92</p>
			<p class="source-code">[Episode 150] - Average Score: 7.37</p>
			<p class="source-code">[Episode 200] - Average Score: 7.76</p>
			<p class="source-code">[Episode 250] - Average Score: 7.91</p>
			<p class="source-code">[Episode 300] - Average Score: 10.33</p>
			<p class="source-code">[Episode 350] - Average Score: 10.94</p>
			<p class="source-code">Average Score: 10.83</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 761.</p>
			<h1 id="_idParaDest-295"><a id="_idTextAnchor338"/>Summary</h1>
			<p>In <a id="_idTextAnchor339"/>this chapter, we learned how to combine deep learning techniques to a DQN model and train it to play the Atari game Breakout. We first looked at adding convolutional layers to the agent for processing screenshots from the game. This helped the agent to better understand the game environment.</p>
			<p>We then took things a step further and added an RNN to the outputs of the CNN model. We created a sequence of images and fed it to an LSTM layer. This sequential model provided the DQN agent with the ability to "visualize" the direction of the ball. This kind of model is called a DRQN.</p>
			<p>Finally, we used an attention mechanism and trained a DARQN model to play the Breakout game. This mechanism helped the model to better understand previous relevant states and improved its performance drastically. This field is still evolving as new deep learning techniques and models are designed, outperforming previous generations in the process.</p>
			<p>In the next chapter, you will be introduced to policy-based methods and the actor-critic model, which consists of multiple models responsible for computing an action based on a state and calculating the Q-values.</p>
		</div>
	</body></html>