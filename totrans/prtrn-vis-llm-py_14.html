<html><head></head><body>
		<div id="_idContainer117">
			<h1 id="_idParaDest-190" class="chapter-number"><a id="_idTextAnchor217"/>14</h1>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor218"/>MLOps for Vision and Language</h1>
			<p>In this chapter, we’ll introduce the core concepts of operations and orchestration for machine learning, also known as MLOps. This includes building pipelines, continuous integration and deployment, promotion through environments, and more. We’ll explore options for monitoring and human-in-the-loop auditing of model predictions. We’ll also identify unique ways to support large vision and language models in your <span class="No-Break">MLOps pipelines.</span></p>
			<p>We’ll be covering the following topics in <span class="No-Break">the chapter:</span></p>
			<ul>
				<li>What <span class="No-Break">is MLOps?</span></li>
				<li>Continuous integration and <span class="No-Break">continuous deployment</span></li>
				<li>Model monitoring <span class="No-Break">and human-in-the-loop</span></li>
				<li>MLOps for <span class="No-Break">foundation models</span></li>
				<li>AWS offerings <span class="No-Break">for MLOps</span></li>
			</ul>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor219"/>What is MLOps?</h1>
			<p>We’ve covered such a huge amount of content<a id="_idIndexMarker735"/> in this book that it’s almost inconceivable. From the absolute foundations of pretraining, we’ve worked through use cases, datasets, models, GPU optimizations, distribution basis, optimizations, hyperparameters, working with SageMaker, fine-tuning, bias detection and mitigation, hosting your model, and prompt engineering. Now, we come to the art and science of <em class="italic">tying it </em><span class="No-Break"><em class="italic">all together</em></span><span class="No-Break">.</span></p>
			<p><strong class="bold">MLOps</strong> stands for <strong class="bold">machine learning operations</strong>. Broadly speaking, it includes a whole set of technologies, people, and processes that your organization can adopt to streamline your machine learning workflows. In the last few chapters, you learned about building RESTful APIs to host your model, along with tips to improve your prompt engineering. Here, we’ll focus on <em class="italic">building a deployment workflow</em> to integrate this model into <span class="No-Break">your application.</span></p>
			<p>Personally, I find the pipeline aspect of MLOps the most poignant. A <strong class="bold">pipeline</strong> is a set of steps you can build<a id="_idIndexMarker736"/> to orchestrate your machine learning workflow. This can include everything from automatically retraining<a id="_idIndexMarker737"/> your model, hyperparameter tuning, auditing, and monitoring, application testing and integration, and promotion to more secure environments, drift and bias detection, and <span class="No-Break">adversarial </span><span class="No-Break"><a id="_idIndexMarker738"/></span><span class="No-Break">hardening.</span></p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B18942_Figure_14_01.jpg" alt="Figure 14.1 – Pipelines for machine learning operations"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.1 – Pipelines for machine learning operations</p>
			<p>Pipelines are tools you can build using any number of software options. If you’re using SageMaker-native tooling and you don’t already<a id="_idIndexMarker739"/> have an orchestration stack, you might start by looking at <strong class="bold">SageMaker Pipelines</strong>. Alternatively, if you’re already using an orchestration stack, such as AirFlow, KubeFlow, Ray, MLFlow, or StepFunctions, you might continue using those and simply point to SageMaker APIs for your machine <span class="No-Break">learning workflows.</span></p>
			<p>The core component<a id="_idIndexMarker740"/> of a pipeline is a <em class="italic">step</em>. A step might be something such as <strong class="bold">data preprocessing</strong>, <strong class="bold">model training</strong>, <strong class="bold">model evaluation</strong>, a <strong class="bold">manual review</strong>, <strong class="bold">model deployment</strong>, and so on. A basic pipeline will flow through a number of steps that you define. Pipelines usually start with a <strong class="bold">trigger</strong>, some event that delivers a notification system to the pipeline. Your trigger could be an upload to S3, a commit to your repository, a time of day, an update on your dataset, or a customer event. Usually, you’ll see one trigger kicking off the entire pipeline, with each step<a id="_idIndexMarker741"/> initiated after the previous <span class="No-Break">one completes.</span></p>
			<p>Let’s move on to the common <span class="No-Break">MLOps pipelines.</span></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor220"/>Common MLOps pipelines</h2>
			<p>Let’s examine a few of the most common pipelines<a id="_idIndexMarker742"/> in <span class="No-Break">machine learning:</span></p>
			<ul>
				<li><strong class="bold">Model deployment pipeline</strong>: Here, the core task is to point to your pretrained model<a id="_idIndexMarker743"/> artifacts, notably your inference<a id="_idIndexMarker744"/> script and the model itself, and put these into whichever deployment<a id="_idIndexMarker745"/> option you select. You might use <strong class="bold">SageMaker RealTime Endpoints</strong> for product recommendation, or <strong class="bold">asynchronous endpoints</strong> to host large language<a id="_idIndexMarker746"/> models. You might have<a id="_idIndexMarker747"/> a variety of images through the <strong class="bold">multi-container endpoint</strong>, or even with the <strong class="bold">multi-model endpoint</strong>. In any case, the most basic pipeline steps might<a id="_idIndexMarker748"/> look something <span class="No-Break">like this:</span><ol><li>Update a <span class="No-Break">model artifact.</span></li><li>Create a <span class="No-Break">new endpoint</span></li><li>Test <span class="No-Break">the endpoint.</span></li><li>If the test is successful, set production traffic to the endpoint. If the test fails, notify the <span class="No-Break">development team.</span></li></ol></li>
				<li><strong class="bold">Model retraining pipeline</strong>: A model retraining pipeline is useful for use<a id="_idIndexMarker749"/> cases where you need<a id="_idIndexMarker750"/> to retrain a model regularly. This might be every time you have new data, which can be as often as every few hours, or as irregular as every month. For a simple case, such as rerunning a report or notebook, you might use SageMaker’s <em class="italic">notebook job</em> feature, launched in December 2022, to run a notebook on a schedule. A pipeline, however, would be useful if you wanted to trigger this retraining based on updated data. Alternatively, if the model or dataset were large and needed distributed training, a pipeline would be a natural fit. Your pipeline steps might look something <span class="No-Break">like this:</span><ol><li value="1">Upload <span class="No-Break">new data.</span></li><li><span class="No-Break">Run preprocessing.</span></li><li>Train <span class="No-Break">the model.</span></li><li>Tune <span class="No-Break">the model.</span></li><li>Trigger<a id="_idIndexMarker751"/> the <span class="No-Break">deployment</span><span class="No-Break"><a id="_idIndexMarker752"/></span><span class="No-Break"> pipeline.</span></li></ol></li>
				<li><strong class="bold">Environment promotion pipeline</strong>: Some customers, particularly in security-sensitive settings<a id="_idIndexMarker753"/> such as highly regulated<a id="_idIndexMarker754"/> industries, require that applications are upgraded through increasingly more secure environments. Here, the word <em class="italic">environment</em> means an isolated compute boundary, usually either a full new AWS account or, more simply, a different region. The steps for this pipeline might look something <span class="No-Break">like this:</span><ol><li value="1">Trigger the pipeline from data scientists in the <span class="No-Break">development account.</span></li><li>Promote the resources to a <span class="No-Break">test account.</span></li><li>Test <span class="No-Break">the endpoint.</span></li><li>If the endpoint passes, promote it to a production account. If the endpoint fails, notify the <span class="No-Break">data scientists.</span></li><li>In the production account, create <span class="No-Break">the endpoint.</span></li><li>Set production traffic to <span class="No-Break">the endpoint.</span></li></ol></li>
			</ul>
			<p>As you have no doubt noticed, each of these pipelines can interact with each other. They can trigger each other as unique steps, interact with other components, and continuously add value. Their basic components are also interchangeable – you can easily substitute some steps with others, defining whatever overall system <span class="No-Break">you need.</span></p>
			<p>A concept underlying much of this is <strong class="bold">microservices</strong>. You can think of each of these pipelines<a id="_idIndexMarker755"/> as a microservice, starting with some input and delivering an output. To maximize value across teams, you might build and maintain a base set of templates for each step, or for entire pipelines, to make it easier for future teams to <span class="No-Break">use them.</span></p>
			<p>As we learned earlier in <a href="B18942_12.xhtml#_idTextAnchor178"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, <em class="italic">How to Deploy Your Model</em>, there are quite a few techniques you can use to improve your model for deployment. This includes quantization and compression, bias detection, and adversarial hardening <em class="italic">(1)</em>.<span class="superscript"> </span>Personally, I tend to see many of the methods executed rarely on a model, such as when it first moves from the R&amp;D team to the deployment team. For regular retraining, I’d avoid extensive compute resources, assuming<a id="_idIndexMarker756"/> that much of the basic updates incorporated into the model work in more <span class="No-Break">recent versions.</span></p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor221"/>Continuous integration and continuous deployment</h1>
			<p>In machine learning, we tend<a id="_idIndexMarker757"/> to look at two somewhat different stacks. On the one hand, you have the model creation and deployment process. This includes your model artifacts, datasets, metrics, and target deployment options. As we discussed previously, you might create a pipeline to automate this. On the other hand, you have the actual software application where you want to expose your model. This might be a visual search mobile app, a question/answering chat, an image generation service, a price forecasting dashboard, or really any other process to improve using data and <span class="No-Break">automated decisions.</span></p>
			<p>Many software stacks use their own <strong class="bold">continuous integration and continuous deployment</strong> (<strong class="bold">CI</strong>/<strong class="bold">CD</strong>) pipelines to seamlessly connect all the parts of an application. This can include integration tests, unit tests, security scans, and machine<a id="_idIndexMarker758"/> learning tests. <strong class="bold">Integration</strong> refers to putting the application together, while <strong class="bold">deployment</strong> refers to taking steps to move<a id="_idIndexMarker759"/> the application <span class="No-Break">into production.</span></p>
			<p>Many of the pipelines we looked at previously could be considered CD pipelines, especially when they refer to updating the service in production. A continuous integration pipeline might include steps that point to the application, testing a variety of responses, and ensuring that the model responds appropriately. Let’s take a <span class="No-Break">closer look.</span></p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B18942_Figure_14_02.jpg" alt="Figure 14.2 – CI/CD options for machine learning"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.2 – CI/CD options for machine learning</p>
			<p>What I’m trying to convey here is that <em class="italic">you have many options for how to set up your pipelines</em>. For a large-scale foundation model, such as your own pretrained LLM or text-to-vision model, you might possibly have handfuls of extremely robust repositories that each team develops for different pieces of the puzzle. Integrating these, using slices of them to support each other, and automating as much as you can with robust unit testing to ensure the highest performance across the board is in your best interest. Separately from the model development, you’ll likely have a deployment pipeline that checks all the boxes to prepare your model<a id="_idIndexMarker760"/> for real-time traffic and successful communication with your <span class="No-Break">client application.</span></p>
			<p>Now that we’ve covered a few foundational topics in general operations, let’s take a closer look at two key aspects that relate especially to machine learning – model monitoring <span class="No-Break">and human-in-the-loop.</span></p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor222"/>Model monitoring and human-in-the-loop</h1>
			<p>In <span class="No-Break"><em class="italic">Chapter 11</em></span>, we explored topics<a id="_idIndexMarker761"/> around bias detection, mitigation, and monitoring<a id="_idIndexMarker762"/> for large vision and language models. This was mostly in the context of evaluating your model. Now that we’ve made it to the section on deploying your models, with an extra focus on operations, let’s take a closer look at <span class="No-Break">model monitoring.</span></p>
			<p>Once you have a model deployed into any application, it’s extremely useful to be able to view the performance of that model over time. This is the case for any of the use cases we discussed earlier – chat, general search, forecasting, image generation, recommendations, classification, question answering, and so on. All of these applications benefit from being able to see how your model is trending over time and provide <span class="No-Break">relevant alerts.</span></p>
			<p>Imagine, for example, that you have a price forecasting model that suggests a price for a given product based on economic conditions. You train your model on certain economic conditions, maybe those in January, and deploy the model in February. While deployed, the model continues to look at those same conditions and help price your project. However, you may not realize that in March, the entire market conditions changed. Things in our world change so rapidly that entire sectors may have inverted. Your model came into the world thinking that everything looks exactly the same as when it was trained. Unless you recalibrate your model, it won’t realize that things <span class="No-Break">are different.</span></p>
			<p>But how are you supposed<a id="_idIndexMarker763"/> to know when to recalibrate<a id="_idIndexMarker764"/> your model? Through Model Monitor! Using Amazon SageMaker, including our fully managed Model Monitor capabilities, you can easily run tests that learn summary statistics of your training data. You can then schedule jobs to compare these summaries with the data hitting your endpoint. This means that as the new data interacts with your model, you can store all of these requests in S3. After the requests are stored, you can use the model monitor service to schedule jobs that compare these inference requests with your training data. This is useful because you can use it to send yourself alerts about how your model is trending on inference, especially if you need to trigger a retraining job. The same basic concepts of Model Monitor should also apply to vision and language; the only question is how we generate <span class="No-Break">summary statistics.</span></p>
			<p>Now, how does Model Monitor relate to human-in-the-loop? It’s because you can also use triggers from your hosted model to <em class="italic">trigger a manual review</em>. As shown in the following figure, you can bring in some software checks to confirm that your model outputs content that is mostly in line with your expectations. If not, you can trigger a manual review. This uses another option on SageMaker, <strong class="bold">Augmented Artificial Intelligence</strong> (<strong class="bold">A2I</strong>), which in turn relies on SageMaker Ground<a id="_idIndexMarker765"/> Truth. Put another way, if the model doesn’t act as you expect, you can send the prediction request and response to a team for manual review. This helps your teams build more trust in the overall solution, not to mention improving your dataset for the next iteration of the model! Let’s take a look at <span class="No-Break">this visually.</span></p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B18942_Figure_14_03.jpg" alt="Figure 14.3 – Model monitoring with human-in-the-loop"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.3 – Model monitoring with human-in-the-loop</p>
			<p>In the preceding figure, you can see a variety<a id="_idIndexMarker766"/> of components, or microservices, that you can combine<a id="_idIndexMarker767"/> to provide a complete pipeline with model monitoring and humans kept in the loop. First, your client application can interact with a Lambda function that, in turn, invokes a SageMaker model. You might store the model requests and responses in an S3 bucket by writing it in Lambda yourself, or you could set the SageMaker endpoint to do this for you. Once you have records stored in S3, you can run <strong class="bold">model monitoring</strong> jobs. This can use a feature of SageMaker, model monitor, to learn the statistical differences between your training and inferencing data, sending you alerts if these fall out of a large range. Alternatively, you could write your own comparison script and run these jobs yourself on SageMaker training or <span class="No-Break">processing jobs.</span></p>
			<p>Once you have some visibility into how your model is responding on aggregate, your best move is to incorporate human feedback as much as you can. This is increasingly true in the generative space, where accuracy, style, and tone<a id="_idIndexMarker768"/> of the content are top criteria for most organizations. A great option for this is <strong class="bold">SageMaker Ground Truth</strong>! As we learned in <a href="B18942_02.xhtml#_idTextAnchor034"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> on preparing data, this is a fully managed service you can use to both increase your labeled datasets and augment your model responses in <span class="No-Break">real time.</span></p>
			<p>A similar approach here is to use multiple models to confirm the prediction result. Imagine that you process a document quickly and want to extract content from it accurately. Your customer uploads a PDF to your website, you parse it using ML models, and you want to confirm or deny the contents of a given field. One way to increase your stakeholder’s confidence in the accuracy of your system is to just use more than one model. Maybe you use your own, a custom deep learning model hosted in SageMaker, while at the same time, you point to a fully managed AWS service such as Textract that can extract digital natural language from visual forms. Then, you might have a Lambda function to see whether both models agree on the response. If they do, then you could respond to the customer directly! If they don’t, then you could route the request for <span class="No-Break">manual review.</span></p>
			<p>There are countless<a id="_idIndexMarker769"/> other ways to monitor<a id="_idIndexMarker770"/> your models, including ways to integrate these with people! For now, however, let’s move on to components of MLOps that are specifically scoped to vision <span class="No-Break">and language.</span></p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor223"/>MLOps for foundation models</h1>
			<p>Now that you have a good idea<a id="_idIndexMarker771"/> of MLOps, including some ideas about<a id="_idIndexMarker772"/> how to use human-in-the-loop and model monitoring, let’s examine specifically what aspects of vision and language models merit our attention from an <span class="No-Break">MLOps perspective.</span></p>
			<p>The answer to this question isn’t immediately obvious because, from a certain angle, vision and language are just slightly different aspects of machine learning and artificial intelligence. Once you have the right packages, images, datasets, access, governance, and security configured, the rest should just flow naturally. Getting to that point, however, is quite an <span class="No-Break">uphill battle!</span></p>
			<p>Building a pipeline for large language models is no small task. As I mentioned previously, I see at least two very different aspects of this. On one side of the equation, you’re looking at the entire model development life cycle. As we’ve learned throughout this book, that’s a massive scope of development. From dataset, model, and script preparation to the training and evaluation loops, and performance and hyperparameter optimizations, there are countless techniques to track in order to produce your <span class="No-Break">foundation model.</span></p>
			<p>Once you have the foundation model, preparing it for development is a different beast. As we discussed previously, <em class="italic">adversarial hardening</em> includes a variety of techniques you can use to improve the performance of your model for the target domain. Everything we learned about in fine-tuning and evaluation from <a href="B18942_10.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, bias detection and mitigation in <a href="B18942_11.xhtml#_idTextAnchor167"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, and deployment techniques in <a href="B18942_12.xhtml#_idTextAnchor178"><span class="No-Break"><em class="italic">Chapter 12</em></span></a> come right to the forefront. To me, it seems natural to locate these in a different pipeline that is focused squarely on deployment. Let’s take a look at these in the <span class="No-Break">following visual.</span></p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B18942_Figure_14_04.jpg" alt="Figure 14.4 – LLM development and deployment pipelines"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.4 – LLM development and deployment pipelines</p>
			<p>What makes this much more complicated is that <em class="italic">many of these disparate steps use similar packages and functions</em>. This means that to implement each of these steps, you’re looking at pointing to at least one, and possibly a few <strong class="source-inline">git</strong> repositories and packages. When you decouple these, using different containers, resources, and steps to manage each piece, it helps each team work on them independently. We all know that the pace of foundation model development is only going to increase over the next few years, so assume that each step here will mean you need to pause periodically, capture the latest open source scripts or research<a id="_idIndexMarker773"/> techniques, develop and test them, and integrate<a id="_idIndexMarker774"/> them back into the <span class="No-Break">larger pipeline.</span></p>
			<p>Now, let’s learn about MLOps <span class="No-Break">for vision.</span></p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor224"/>MLOps for vision</h2>
			<p>How do vision foundation models <a id="_idIndexMarker775"/>compare with what we suggested<a id="_idIndexMarker776"/> previously for language models? To some degree, not much. You’re still working with images, scripts, packages, datasets, and model quality. You still want to keep your models up to date, and you still want to incorporate as much human feedback in the best way you can. As we’ve seen in the book so far, models and evaluation metrics will vary, datasets will be quite different, and tasks are not entirely the same. A lot of the basic logic, however, carries over. One quick word of caution though – fine-tuning in language<a id="_idIndexMarker777"/> is not at all the same<a id="_idIndexMarker778"/> as fine-tuning <span class="No-Break">in vision.</span></p>
			<h3>A word of caution on overfitting in vision and a call for common sense</h3>
			<p>Please keep in mind that vision <em class="italic">is much more sensitive to overfitting than language</em>. To understand this, let’s consider the fundamental differences between the two modalities. Language<a id="_idIndexMarker779"/> is inherently discrete; we represent the entire world with only letters and words, items that are noncontinuous by default. You could say that the entire modality of language is equal to the sum of all dictionaries in all languages around the world, to the extent that dictionaries themselves are only approximations of words spoken, used, and developed by humans constantly. The arrangement of these words, and the interpretation and meaning of them across the wide breadth of lived human experiences, <span class="No-Break">is infinite.</span></p>
			<p>Vision<a id="_idIndexMarker780"/> is completely different. The modality itself is continuous; while pixels themselves of course start and stop, the delineation between objects in a picture is almost a matter of opinion. We use metrics to quantify the quality of and discrepancy between labeled objects, such as <em class="italic">intersection over union</em>. Objects rotate; they seem to change completely in different lighting and backgrounds. Their patterns might seem to be the same even across totally different types, such as animals and cups, street signs and clothing, furniture, and natural landscapes. While both vision and language decompose into embeddings on their way into a model, the ability of neural nets to capture the meaning of the content provided and extrapolate this into other settings seems to be very different in language than in vision. Language fine-tuning works well in many cases, while vision fine-tuning very commonly results in poor performance at <span class="No-Break">first blush.</span></p>
			<p>Personally, I find another machine learning technique very interesting, which appears to operate, in essence, at the core of these combined modalities – common sense reasoning. Machine common sense refers to ensuring logical<a id="_idIndexMarker781"/> consistency between concepts, objects, and the defining characteristics of those objects. Most humans excel at this, such as knowing that water is wet, that heavy objects fall when dropped into open space, that fire produces heat, and <span class="No-Break">so on.</span></p>
			<p>Computers, however, are terrible at this. It’s almost as if the physical dimension doesn’t exist; certainly, the biological plain is a complete anomaly to them. Image generators don’t understand that food has to go into a mouth to constitute eating. Image classifiers routinely miscategorize zebras with furniture. Language models don’t appreciate the pace of human communication, occasionally overwhelming their operators. Movie generators regularly cause more disgust than delight because they fail to recognize the first basic discrimination mastered by infants – humans and their movement. To humans, it’s immediately obvious that a cup and an animal are both objects, and might even occasionally share some stylistic traits, but in the physical world, they come from completely different domains. To computers, it’s as if this physical dimension doesn’t exist. They are quite literally only learning what exists inside the two-dimensional frames you provide. This is why we use labels in the first place – to give your model some meaning by translating the physical world <span class="No-Break">into pixels.</span></p>
			<p>I had the distinct pleasure<a id="_idIndexMarker782"/> of meeting and briefly chatting with Yejin Choi <em class="italic">(2)</em> last year. She delivered a keynote to the general assembly of the Association of Computational Linguists, one of the best NLP conferences in the world, on a fascinating hypothetical forecast of the next 60 years of natural language research. I was completely blown away by her passion for the humanities, philosophy, and deep scientific discoveries. She started exploring machine common sense in an era when it was extremely unpopular to do so, and in fact, she jokes today that she was actively discouraged from doing so, since everyone thought it would be impossible to get published on this topic. Since then, she’s turned into probably the world’s leading expert in this area, largely operating with language and vision as her modalities. Since then, I’ve been curious about common sense and wanted to explore it in <span class="No-Break">more detail.</span></p>
			<p>I wonder if human knowledge itself is inherently relational and possibly multimodal. We build concepts in our minds based on experiences – lived, perceived, imagined, and understood. These mental concepts guide our words and actions, expressing themselves in some cases verbally, and other times purely physically. Perhaps we need deeper representations to guide the intermingling of modalities. Perhaps language might help our vision models adapt<a id="_idIndexMarker783"/> to new domains more quickly. Perhaps this is because it provides a bit of <span class="No-Break">common sense.</span></p>
			<p>Practically, I’m bringing this up because, if you’re about to embark on a vision fine-tuning exercise, I want you to go in knowing that it won’t be easy, and what worked for you in language probably won’t translate as well as you thought. I’m also bringing this up because I want you future researchers out there to take courage, trust your intuition, and challenge your assumptions. Now that we’ve learned a bit about MLOps for foundation models, let’s take a look at some AWS offerings to help simplify and speed you up to nail <span class="No-Break">this subject!</span></p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor225"/>AWS offerings for MLOps</h1>
			<p>Happily, AWS provides<a id="_idIndexMarker784"/> a variety of tools<a id="_idIndexMarker785"/> to help simplify<a id="_idIndexMarker786"/> this! One nice feature is called <strong class="bold">lineage tracking</strong>. SageMaker can automatically create the lineage <em class="italic">(3)</em> for key artifacts, including across accounts. This includes dataset artifacts, images, algorithm specifications, data configs, training job components, endpoints, and checkpoints. This is integrated with the <strong class="bold">Experiments SDK</strong>, letting you compare experiments and results<a id="_idIndexMarker787"/> programmatically and at scale. Let’s explore this visually. We’ll even generate a visualization for you to see how all of these are connected! Check it out in the <span class="No-Break">following figure.</span></p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B18942_Figure_14_05.jpg" alt="Figure 14.5 – SageMaker automatically creates lineage tracking"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.5 – SageMaker automatically creates lineage tracking</p>
			<p>As you can see, the first step in tracking your lineage is running on key SageMaker resources such as training jobs, images, and processing jobs. You can use the entities that are automatically tracked, or you can define your own entities. To generate the lineage view<a id="_idIndexMarker788"/> as shown in the previous figure, you can interact with the <strong class="bold">lineage query language</strong>. If you want to jump straight into the notebook, which ships with a visualization solution, that’s available as point <em class="italic">(4)</em> in the <em class="italic">References</em> section. The lineage tracking is explained in more detail here – <em class="italic">(5)</em>, and the querying is defined here – <em class="italic">(6)</em>. Using SageMaker Lineage, you can easily trace how a model was trained and where it <span class="No-Break">was deployed.</span></p>
			<p>How does it work? You can use the <em class="italic">LineageFilter API</em> to look for different objects, such as endpoints, that are associated with a model artifact. You can also search for trial components associated with endpoints, find datasets associated with models, and traverse forward and backward through the graph of associated items. Having these relationships available programmatically makes it much easier to take all of the necessary resources and put them into pipelines and other <span class="No-Break">governance structures.</span></p>
			<p>Once you have the resources<a id="_idIndexMarker789"/> identified, how do you wrap them into a pipeline? As we mentioned<a id="_idIndexMarker790"/> earlier in the chapter, many of the basic AWS and SageMaker resources are available as discrete building blocks. This includes the model, relevant model artifacts, deployment configurations, associated training and processing jobs, hyperparameter tuning, and containers. This means you<a id="_idIndexMarker791"/> can use the AWS SDK for Python, <strong class="bold">boto3</strong>, and the <strong class="bold">SageMaker Python SDK</strong> to point to and execute all of your resources<a id="_idIndexMarker792"/> and tasks programmatically. Wrapping these in a pipeline then means using whatever tooling stack you prefer to use to operationalize these automatically. One option for doing so is <span class="No-Break"><strong class="bold">SageMaker Pipelines</strong></span><span class="No-Break">!</span></p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor226"/>A quick introduction to SageMaker Pipelines</h2>
			<p>If you’re working with SageMaker-native<a id="_idIndexMarker793"/> resources, such as jobs, endpoints, model artifacts, and Docker images, then connecting them through the Pipelines SDK construct <em class="italic">(7)</em> should not be too much of an additional lift. SageMaker Pipelines is a managed feature you can use to create, run, and manage complete workflows for machine learning on AWS. Once you’ve defined your base Python SDK objects for SageMaker, such as a training job, evaluation metrics, hyperparameter tuning, and an endpoint, you can pass each of these objects to the Pipelines API and create it as a graph! Let’s explore this in more detail in the <span class="No-Break">following figure.</span></p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B18942_Figure_14_06.jpg" alt="Figure 14.6 – SageMaker Pipelines"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 14.6 – SageMaker Pipelines</p>
			<p>For a notebook walk-through of creating a graph very similar to the one in the preceding figure, there is a resource on GitHub <em class="italic">(8)</em>. The core idea is that you build each part of the pipeline separately, such as the data processing, training, and evaluation steps, and then pass each of these to the SageMaker Pipelines API to create the <span class="No-Break">connected graph.</span></p>
			<p>As you can see in the preceding figure, this is presented visually in SageMaker Studio! This makes it much easier for data scientists to develop, review, manage, and execute these pipelines. Studio also has a handful of other relevant features for MLOps, such as a feature store, model registry, endpoint management, model monitoring, and inference recommender. For a deeper dive into these topics, there’s a full white paper from the AWS Well-Architected Framework<a id="_idIndexMarker794"/> on machine <span class="No-Break">learning </span><span class="No-Break"><em class="italic">(9)</em></span><span class="No-Break">.</span></p>
			<p>Now that we’ve learned about the AWS offerings for MLOps, let’s close out the chapter with a <span class="No-Break">full recap.</span></p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor227"/>Summary</h1>
			<p>In this chapter, we introduced the core concept of MLOps, especially in the context of vision and language. We discussed machine learning operations, including some of the technologies, people, and processes that make it work. We especially focused on the pipeline aspect, learning about technologies useful to build them, such as SageMaker Pipelines, Apache Airflow, and Step Functions. We looked at a handful of different types of pipelines relevant to machine learning, such as model deployment, model retraining, and environment promotion. We discussed core operations concepts, such as CI and CD. We learned about model monitoring and human-in-the-loop design patterns. We learned about some specific techniques for vision and language within MLOps, such as common development and deployment pipelines for large language models. We also looked at how the core methods that might work in language can be inherently less reliable in vision, due to the core differences in the modalities and how current learning systems operate. We took a quick tour down the philosophical route by discussing common sense reasoning, and then we closed out the chapter with key AWS offerings for MLOps, such as SageMaker Lineage, Experiments, <span class="No-Break">and Pipelines.</span></p>
			<p>Now, let’s conclude the book with one final chapter on <span class="No-Break">future trends.</span></p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor228"/>References</h1>
			<p>Please go through the following content for more information on a few topics covered in <span class="No-Break">the chapter:</span></p>
			<ol>
				<li><em class="italic">Hardening Deep Neural Networks via Adversarial Model </em><span class="No-Break"><em class="italic">Cascades</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://arxiv.org/pdf/1802.01448.pdf"><span class="No-Break">https://arxiv.org/pdf/1802.01448.pdf</span></a></li>
				<li><em class="italic">Yejin </em><span class="No-Break"><em class="italic">Choi</em></span><span class="No-Break">: </span><a href="https://homes.cs.washington.edu/~yejin/"><span class="No-Break">https://homes.cs.washington.edu/~yejin/</span></a></li>
				<li><em class="italic">Amazon SageMaker ML Lineage </em><span class="No-Break"><em class="italic">Tracking</em></span><span class="No-Break">: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking.html</span></a></li>
				<li><span class="No-Break"><em class="italic">aws/amazon-sagemaker-examples</em></span><span class="No-Break">: </span><a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-lineage/sagemaker-lineage.ipynb"><span class="No-Break">https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-lineage/sagemaker-lineage.ipynb</span></a></li>
				<li><em class="italic">Lineage Tracking </em><span class="No-Break"><em class="italic">Entities</em></span><span class="No-Break"><em class="italic">: </em></span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking-entities.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking-entities.html</span></a></li>
				<li><em class="italic">Querying Lineage </em><span class="No-Break"><em class="italic">Entities</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/querying-lineage-entities.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/querying-lineage-entities.html</span></a></li>
				<li><em class="italic">SageMaker </em><span class="No-Break"><em class="italic">Pipelines</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/index.html"><span class="No-Break">https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/index.html</span></a></li>
				<li><span class="No-Break"><em class="italic">aws/amazon-sagemaker-examples</em></span><span class="No-Break">: </span><a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/train-register-deploy-pipeline-model/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb"><span class="No-Break">https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/train-register-deploy-pipeline-model/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb</span></a></li>
				<li><em class="italic">Machine Learning Lens AWS Well-Architected </em><span class="No-Break"><em class="italic">Framework</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://docs.aws.amazon.com/pdfs/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf"><span class="No-Break">https://docs.aws.amazon.com/pdfs/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf</span></a></li>
			</ol>
		</div>
	</body></html>