<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predicting Taxi Fares with Deep Feedforward Networks</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we will use a deep feedforward neural network to predict taxi fares in <strong class="calibre4">New York City</strong> (<strong class="calibre4">NYC</strong>), given inputs such as the pickup and drop off locations.</p>
<p class="calibre2">In the previous chapter, <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre10">Chapter 2</a>, <em class="calibre8">Predicting Diabetes with Multilayer Perceptrons</em>, we saw how we can use a MLP with two hidden layers to perform a classification task (whether the patient is at risk of diabetes or not). In this chapter, we will build a deep neural network to perform a regression task of estimating taxi fares. As we shall see, we will need a deeper (that is, more complex) neural network to achieve this goal.</p>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre11">
<li class="calibre12">The motivation for the problem that we're trying to tackle—making accurate predictions of taxi fares</li>
<li class="calibre12">Classification versus regression problems in machine learning</li>
<li class="calibre12">In-depth analysis of the NYC taxi fares dataset, including geolocation data visualization</li>
<li class="calibre12">Architecture of a deep feedforward neural network</li>
<li class="calibre12">Training a deep feedforward neural network in Keras for regression problems</li>
<li class="calibre12">Analysis of our results</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="calibre2">The key Python libraries required for this chapter are as follows:</p>
<ul class="calibre11">
<li class="calibre12">matplotlib 3.0.2</li>
<li class="calibre12">pandas 0.23.4</li>
<li class="calibre12">Keras 2.2.4</li>
<li class="calibre12">NumPy 1.15.2</li>
<li class="calibre12">scikit-learn 0.20.2</li>
</ul>
<div class="packtinfobox">To download the dataset required for this project, please refer to the instructions at <a href="https://raw.githubusercontent.com/PacktPublishing/Neural-Network-Projects-with-Python/master/chapter3/how_to_download_the_dataset.txt" target="_blank" class="calibre20">https://raw.githubusercontent.com/PacktPublishing/Neural-Network-Projects-with-Python/master/Chapter03/how_to_download_the_dataset.txt</a>.</div>
<p class="calibre2">The code for this chapter can be found in the GitHub repository for the book at <a href="https://github.com/PacktPublishing/Neural-Network-Projects-with-Python" target="_blank" class="calibre10">https://github.com/PacktPublishing/Neural-Network-Projects-with-Python</a>.</p>
<p class="calibre2">To download the code into your computer, run the following<span class="calibre5"> </span><kbd class="calibre13">git clone</kbd><span class="calibre5"> </span>command:</p>
<pre class="calibre17"><strong class="calibre1"><span>$ git clone https://github.com/PacktPublishing/Neural-Network-Projects-with-Python.git</span></strong></pre>
<p class="calibre2">After the process is complete, there will be a folder titled <kbd class="calibre13">Neural-Network-Projects-with-Python</kbd><span class="calibre5">. Enter the folder by running the following command:</span></p>
<pre class="calibre17"><strong class="calibre1">$ cd Neural-Network-Projects-with-Python</strong></pre>
<p class="calibre2">To install the required Python libraries in a virtual environment, run the following command:</p>
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre17"><strong class="calibre1"><span>$ conda</span> <span>env</span> <span>create</span> <span>-</span><span>f</span> <span>environment</span><span>.</span><span>yml</span></strong></pre></div>
</div>
<p class="calibre2">Note that you should have installed Anaconda on your computer first before running this command. To enter the virtual environment, run the following command:</p>
<pre class="calibre17"><strong class="calibre1">$ conda activate neural-network-projects-python</strong></pre>
<p class="calibre2"/>
<p class="calibre2">Navigate to the<span class="calibre5"> </span><kbd class="calibre13">Chapter03</kbd> <span class="calibre5">folder </span>by running the following command:</p>
<pre class="calibre17"><strong class="calibre1">$ cd Chapter03</strong></pre>
<p class="calibre2">The following files are located in this folder:</p>
<ul class="calibre11">
<li class="calibre12"><kbd class="calibre13">main.py</kbd><span>:</span> This is the main code for the neural network.</li>
<li class="calibre12"><kbd class="calibre13">utils.py</kbd><span>:</span> This file contains <span>auxiliary utility code that will help us in the implementation of our neural network.</span></li>
<li class="calibre12"><kbd class="calibre13">visualize.py</kbd><span>:</span> This file contains all the necessary code for exploratory data analysis and data visualization. Every plot in this chapter can be recreated by running this file.</li>
</ul>
<p class="calibre2">To run the code for the neural network, simply execute the<span class="calibre5"> </span><kbd class="calibre13">main.py</kbd><span class="calibre5"> </span>file:</p>
<pre class="calibre17"><strong class="calibre1">$ python main.py</strong></pre>
<p class="calibre2">To recreate the data visualizations covered in this chapter, execute the<span class="calibre5"> </span><kbd class="calibre13">visualize.py</kbd><span class="calibre5"> </span>file:</p>
<pre class="calibre17"><strong class="calibre1">$ python visualize.py</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting taxi fares in New York City</h1>
                </header>
            
            <article>
                
<p class="calibre2">Yellow cabs in NYC are perhaps one of the most recognizable icons in the city. Tens of thousands of commuters in NYC rely on taxis as a mode of transportation around the bustling metropolis. In recent years, the taxi industry in NYC has been put under increasing pressure from ride-hailing apps such as Uber. </p>
<p class="calibre2">In order to rise to the challenge from ride-hailing apps, yellow cabs in NYC are looking to modernize their operations, and to provide a user experience on par with Uber. <span class="calibre5">In August 2018, </span><span class="calibre5">the Taxi and Limousine Commission of NYC launched a new app that allows commuters to book a yellow cab from their phones. The app provides fare pricing upfront before they hail a cab. </span>Creating an algorithm to provide fare pricing upfront is no simple feat. The algorithm needs to consider various environmental variables such as traffic conditions, time of day, and pick up and drop off locations in order to make an accurate fare prediction. The best way to do that is to leverage machine learning. By the end of this chapter, you will have created and trained a neural network to do exactly that.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The NYC taxi fares dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">The dataset that we will be using for this project is the NYC taxi fares dataset, as provided by Kaggle. The original dataset contains a massive 55 million trip records from 2009 to 2015, including data such as the pick up and drop off locations, number of passengers, and pickup datetime. This dataset provides an interesting opportunity to use big datasets in machine learning projects, as well to visualize geolocation data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploratory data analysis</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's dive right into the dataset. The instructions to download the NYC taxi fares dataset can be found in the accompanying GitHub repository for the book (refer to the <em class="calibre8">Technical requirements</em> section). Unlike in the previous chapter, <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre10">Chapter 2</a>, <em class="calibre8">Predicting Diabetes with Multilayer Perceptrons</em>, we're not going to import the original dataset of 55 million rows. In fact, most computers would not be able to store the entire dataset in memory! Instead, let's just import the first 0.5 million rows. Doing this does have its drawbacks, but it is a necessary tradeoff in order to use the dataset in an efficient manner.</p>
<p class="calibre2">To do this, run the <kbd class="calibre13">read_csv()</kbd> function with <kbd class="calibre13">pandas</kbd>:</p>
<pre class="calibre17">import pandas as pd<br class="title-page-name"/><br class="title-page-name"/>df = pd.read_csv('NYC_taxi.csv', parse_dates=['pickup_datetime'], nrows=500000)</pre>
<div class="packttip">The <kbd class="calibre19">parse_dates</kbd> parameter in <kbd class="calibre19">read_csv</kbd> allows <kbd class="calibre19">pandas</kbd> to easily recognize certain columns as dates, giving us the flexibility to work with such <kbd class="calibre19">datetime</kbd> values, as we shall see later in the chapter.</div>
<p class="calibre2">Let's take a look at the first five rows of the dataset by calling the <kbd class="calibre13">df.head()</kbd> command:</p>
<pre class="calibre17">print(df.head())</pre>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img class="alignnone44" src="assets/1e0a77f4-ec95-4867-9af1-d8c38ed500ed.png"/></p>
<p class="calibre2">We can see that there are eight columns in the dataset:</p>
<ul class="calibre11">
<li class="calibre12"><kbd class="calibre13">key</kbd>: This column seems identical to the <kbd class="calibre13">pickup_datetime</kbd> column. It was probably used as an unique identifier in the database it was stored in. We can safely remove this column without any loss of information.</li>
<li class="calibre12"><kbd class="calibre13">fare_amount</kbd>: This is the target variable we are trying to predict, the fare amount paid at the end of the trip.</li>
<li class="calibre12"><kbd class="calibre13">pickup_datetime</kbd>: This column contains information on the pickup date (year, month, day of month), as well as the time (hour, minute, seconds).</li>
<li class="calibre12"><kbd class="calibre13">pickup_longitude</kbd> and <kbd class="calibre13">pickup_latitude</kbd>: The longitude and latitude of the pickup location.</li>
<li class="calibre12"><kbd class="calibre13">dropoff_longitude</kbd> and <kbd class="calibre13">dropoff_latitude</kbd>: The longitude and latitude of the drop off location.</li>
<li class="calibre12"><kbd class="calibre13">passenger_count</kbd>: The number of passengers. </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing geolocation data</h1>
                </header>
            
            <article>
                
<p class="calibre2">The pick-up and drop-off longitude and latitude data are crucial to predicting the fare amount. After all, fares in NYC taxis are largely determined by the distance traveled. </p>
<p class="calibre2">First, let's understand what latitude and <span class="calibre5">longitude </span>represents. L<span class="calibre5">atitude and </span>l<span class="calibre5">ongitude are coordinates in a geographic coordinate system. Basically, the latitude and longitude allows us to specify any location on Earth using a set of coordinates.</span></p>
<p class="calibre2"><span class="calibre5">The following diagram shows the <strong class="calibre4">Latitude</strong> and <strong class="calibre4">Longitude</strong> coordinate system:</span></p>
<p class="mce-root"><img class="alignnone45" src="assets/395f3948-8e35-4a17-82ce-9103a7cb785d.png"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">We can think of the Earth as a scatterplot, with the <strong class="calibre4">Longitude</strong> and the <strong class="calibre4">Latitude</strong> being the axes. Then, every location on Earth is simply a point on the scatterplot. In fact, let's do exactly that; let's plot the pickup and drop off latitudes <span class="calibre5">and</span> longitudes <span class="calibre5">on a scatterplot.</span></p>
<p class="calibre2">First, let's restrict our data points to only pickups and drop offs within NYC. NYC has an approximate longitude range of <kbd class="calibre13">-74.05</kbd> to <kbd class="calibre13">-73.75</kbd> and a latitude range of <kbd class="calibre13">40.63</kbd> to <kbd class="calibre13">40.85</kbd>:</p>
<pre class="calibre17"># range of longitude for NYC<br class="title-page-name"/>nyc_min_longitude = -74.05<br class="title-page-name"/>nyc_max_longitude = -73.75<br class="title-page-name"/><br class="title-page-name"/># range of latitude for NYC<br class="title-page-name"/>nyc_min_latitude = 40.63<br class="title-page-name"/>nyc_max_latitude = 40.85<br class="title-page-name"/><br class="title-page-name"/>df2 = df.copy(deep=True)<br class="title-page-name"/>for long in ['pickup_longitude', 'dropoff_longitude']:<br class="title-page-name"/>    df2 = df2[(df2[long] &gt; nyc_min_longitude) &amp; (df2[long] &lt;<br class="title-page-name"/>                                                 nyc_max_longitude)]<br class="title-page-name"/><br class="title-page-name"/>for lat in ['pickup_latitude', 'dropoff_latitude']:<br class="title-page-name"/>    df2 = df2[(df2[lat] &gt; nyc_min_latitude) &amp; (df2[lat] &lt; <br class="title-page-name"/>                                               nyc_max_latitude)]</pre>
<p class="calibre2">Note that we copied the original DataFrame, <kbd class="calibre13">df</kbd>, into a new DataFrame, <kbd class="calibre13">df2</kbd>, to avoid overwriting the original DataFrame.</p>
<p class="calibre2">Now, let's define a new function that will take our DataFrame as an input, and plot the pickup locations on a scatterplot. We are also interested in overlaying the scatterplot with a few key landmarks in NYC. A quick Google search tells us that there are two main airports in NYC (JFK and LaGuardia), and their coordinates, along with the main districts in NYC, are as follows:</p>
<pre class="calibre17">landmarks = {'JFK Airport': (-73.78, 40.643),<br class="title-page-name"/>             'Laguardia Airport': (-73.87, 40.77),<br class="title-page-name"/>             'Midtown': (-73.98, 40.76),<br class="title-page-name"/>             'Lower Manhattan': (-74.00, 40.72),<br class="title-page-name"/>             'Upper Manhattan': (-73.94, 40.82),<br class="title-page-name"/>             'Brooklyn': (-73.95, 40.66)}</pre>
<p class="calibre2">And here's our function using <kbd class="calibre13">matplotlib</kbd> to plot the pickup locations on a scatterplot:</p>
<pre class="calibre17">import matplotlib.pyplot as plt<br class="title-page-name"/><br class="title-page-name"/>def plot_lat_long(df, landmarks, points='Pickup'):<br class="title-page-name"/>    plt.figure(figsize = (12,12)) # set figure size<br class="title-page-name"/>    if points == 'pickup':<br class="title-page-name"/>        plt.plot(list(df.pickup_longitude), list(df.pickup_latitude), <br class="title-page-name"/>                 '.', markersize=1)<br class="title-page-name"/>    else:<br class="title-page-name"/>        plt.plot(list(df.dropoff_longitude), list(df.dropoff_latitude), <br class="title-page-name"/>                 '.', markersize=1)<br class="title-page-name"/><br class="title-page-name"/>    for landmark in landmarks:<br class="title-page-name"/>        plt.plot(landmarks[landmark][0], landmarks[landmark][1], <br class="title-page-name"/>                '*', markersize=15, alpha=1, color='r') <br class="title-page-name"/>        plt.annotate(landmark, (landmarks[landmark][0]+0.005, <br class="title-page-name"/>                     landmarks[landmark][1]+0.005), color='r', <br class="title-page-name"/>                     backgroundcolor='w') <br class="title-page-name"/> <br class="title-page-name"/>    plt.title("{} Locations in NYC Illustrated".format(points))<br class="title-page-name"/>    plt.grid(None)<br class="title-page-name"/>    plt.xlabel("Latitude")<br class="title-page-name"/>    plt.ylabel("Longitude")<br class="title-page-name"/>    plt.show()</pre>
<p class="calibre2">Let's run the function we just defined:</p>
<pre class="calibre17">plot_lat_long(df2, landmarks, points='Pickup')</pre>
<p class="calibre2">We'll see the following scatterplot showing the pickup locations:</p>
<p class="mce-root"><img class="alignnone46" src="assets/fe040127-a79a-4ea7-8fe8-5a9e12a42bff.png"/></p>
<p class="calibre2">Isn't it beautiful? Just by plotting the pickup locations on a scatterplot, we can clearly see a map of NYC, along with the grids that streets in NYC are known for. From the preceding scatterplot, we can make a few observations:</p>
<ul class="calibre11">
<li class="calibre12">In Manhattan, most pickups were around the <kbd class="calibre13">Midtown</kbd> area, followed by <kbd class="calibre13">Lower Manhattan</kbd>. In comparison, there are much fewer pickups in <kbd class="calibre13">Upper Manhattan</kbd>. This makes sense, since <kbd class="calibre13">Upper Manhattan</kbd> is a residential area, whereas more offices and tourist attractions are located at <kbd class="calibre13">Midtown</kbd> and <kbd class="calibre13">Lower Manhattan</kbd>.</li>
<li class="calibre12">Pickups are sparse outside Manhattan. The only two outliers were at <kbd class="calibre13">LaGuardia Airport</kbd> and <kbd class="calibre13">JFK Airport</kbd>.</li>
</ul>
<p class="calibre2">Let's also plot the scatterplot for drop off locations and see how it differs:</p>
<pre class="calibre17">plot_lat_long(df2, landmarks, points='Drop Off')</pre>
<p class="calibre2">We'll see the following scatterplot:</p>
<p class="mce-root"><img class="alignnone47" src="assets/f329a110-c91c-4f23-a810-8ccbcd5fd9e5.png"/></p>
<p class="calibre2">Comparing the pickup and drop off scatterplots, we can clearly see that there are more drop offs than pickups in residential areas such as <kbd class="calibre13">Upper Manhattan</kbd> and <kbd class="calibre13">Brooklyn</kbd>. Neat!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ridership by day and hour</h1>
                </header>
            
            <article>
                
<p class="calibre2">Next, let's investigate how the number of rides varies by day and hour.</p>
<p class="calibre2">Recall that the raw data contains a single <kbd class="calibre13">pickup_datetime</kbd> <span class="calibre5">column </span>that contains the pickup date and time in <kbd class="calibre13">datetime</kbd> format. First, let's separate the pickup year, month, day, day of week, and hour from the original <span class="calibre5"><kbd class="calibre13">pickup_datetime</kbd> column into different columns:</span></p>
<pre class="calibre17">df['year'] = df['pickup_datetime'].dt.year<br class="title-page-name"/>df['month'] = df['pickup_datetime'].dt.month<br class="title-page-name"/>df['day'] = df['pickup_datetime'].dt.day<br class="title-page-name"/>df['day_of_week'] = df['pickup_datetime'].dt.dayofweek<br class="title-page-name"/>df['hour'] = df['pickup_datetime'].dt.hour</pre>
<p class="calibre2">Since we have previously used the <kbd class="calibre13">parse_dates</kbd> parameter when we imported the data into pandas, we can easily identify and separate the year, month, day and hour components using the <kbd class="calibre13">dt</kbd> function in pandas.</p>
<p class="calibre2">Now, let's plot a histogram to analyze the distribution of rides throughout the week:</p>
<pre class="calibre17">import numpy as np<br class="title-page-name"/>df['day_of_week'].plot.hist(bins=np.arange(8)-0.5, ec='black', <br class="title-page-name"/>                            ylim=(60000,75000))<br class="title-page-name"/>plt.xlabel('Day of Week (0=Monday, 6=Sunday)')<br class="title-page-name"/>plt.title('Day of Week Histogram')<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We'll see the following histogram:</p>
<p class="mce-root"><img class="alignnone48" src="assets/31adca56-d945-4d6a-928e-a54c26a5a8d2.png"/></p>
<p class="calibre2"/>
<p class="calibre2">Interestingly, we can see that the number of rides is not evenly distributed across each weekday. Instead, the number of rides increases linearly from Monday through Friday, and peaking on Friday. The weekends see a slight drop in the number of rides on Saturday, before falling sharply on Sunday. </p>
<p class="calibre2">We can also visualize ridership by hour:</p>
<pre class="calibre17">df['hour'].plot.hist(bins=24, ec='black')<br class="title-page-name"/>plt.title('Pickup Hour Histogram')<br class="title-page-name"/>plt.xlabel('Hour')<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We'll see the following histogram for pickup hour:</p>
<p class="mce-root"><img class="alignnone49" src="assets/1c522d25-0791-4ae9-a9f8-9d1ea24ac8fd.png"/></p>
<p class="calibre2">We can see that there are more rides during the evening rush hour, as compared to the morning rush hour. In fact, the number of rides is pretty constant throughout the day. Starting at 6 P.M., the number of rides increases and peaks at 7 P.M., before falling from 11 P.M. onwards.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preprocessing</h1>
                </header>
            
            <article>
                
<p class="calibre2">Recall from the previous project that we had to preprocess the data by removing missing values and other data anomalies. In this project, we'll perform the same process. We'll also perform feature engineering to improve both the quality and quantity of the features before training our neural network on it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling missing values and data anomalies</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's do a check to see whether there are any missing values in our dataset:</p>
<pre class="calibre17"><span>print(df</span><span>.</span><span>isnull</span><span>()</span><span>.</span><span>sum</span><span>())</span></pre>
<p class="calibre2">We'll see the following output showing the number of missing values in each column:</p>
<p class="mce-root"><img class="alignnone50" src="assets/899f7a52-83c3-49c1-9e0b-7489fa94b96f.png"/></p>
<p class="calibre2">We can see that there are only five rows (out of 500,000 rows) with missing data. With a missing data percentage of just 0.001%, it seems that we don't have a problem with missing data. Let's go ahead and remove those five rows with missing data:</p>
<pre class="calibre17">df = df.dropna()</pre>
<p class="calibre2">At this point, we should also check the data for outliers. In a dataset as massive as this, there are bound to be outliers, which can skew our model. Let's run a quick statistical summary on our data to look at the distribution:</p>
<pre class="calibre17">print(df.describe())</pre>
<p class="calibre2">The <kbd class="calibre13">describe</kbd> method produces the following table:</p>
<p class="mce-root"><img class="alignnone51" src="assets/1c460353-de5a-42ba-8ad2-308e0922ef01.png"/></p>
<p class="calibre2">The lowest fare in the dataset is $-44.90. That doesn't make sense; fares can't be negative! Also, the highest fare is $500. Did the passenger get ripped off? Or was it just an error? Let's plot a histogram to better understand the distribution of fares:</p>
<pre class="calibre17">df['fare_amount'].hist(bins=500)<br class="title-page-name"/>plt.xlabel("Fare")<br class="title-page-name"/>plt.title("Histogram of Fares")<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We'll get the following histogram:</p>
<p class="mce-root"><img class="alignnone52" src="assets/66030931-26cf-436f-bdf1-a0ab4abc1964.png"/></p>
<p class="calibre2">It doesn't seem like that there are too many outliers, so we can safely remove them. Another interesting trend that we can observe from the histogram is that there is a small spike in fares around $50. Could this be a fixed fare from a specific location? Cities usually implement fixed fares for trips to and from airports. A quick Google search tells us that trips to and from JFK airport incurs a flat fare of $52 plus tolls. This could be the reason for the spike in the histogram around $50! We'll keep this important fact in mind when we do feature engineering later on.</p>
<p class="calibre2">For now, let's remove rows with fares less than $0 and more than $100:</p>
<pre class="calibre17">df = df[(df['fare_amount'] &gt;=0) &amp; (df['fare_amount'] &lt;= 100)]</pre>
<p class="calibre2">From the previous table, we can see that there are also outliers in the <kbd class="calibre13">passenger_count</kbd> column. Let's plot a histogram of <kbd class="calibre13">Passenger Count</kbd> to look at its distribution:</p>
<pre class="calibre17">df['passenger_count'].hist(bins=6, ec='black')<br class="title-page-name"/>plt.xlabel("Passenger Count")<br class="title-page-name"/>plt.title("Histogram of Passenger Count")<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">This gives us the following histogram:</p>
<p class="mce-root"><img class="alignnone53" src="assets/313b50db-941d-46d9-aeae-aa5563eb9c94.png"/></p>
<p class="calibre2">We can see that there's a small percentage of rows with <kbd class="calibre13">0</kbd> passenger counts. Instead of discarding those rows, let's replace the outliers with the mode (that is, <kbd class="calibre13">1</kbd> passenger count):</p>
<pre class="calibre17">df.loc[df['passenger_count']==0, 'passenger_count'] = 1</pre>
<div class="packtinfobox">We can also remove these outliers entirely, since only a few rows are affected. Instead, we chose to replace the outlier passenger count with the mode. Both methods are perfectly valid, but we chose the latter to illustrate the importance of visualizing your data with a histogram to identify outlier values, as well as the mode.</div>
<p class="calibre2">Next, let's inspect the pickup and drop off latitude and longitude data to check for outliers. In the previous section on data visualization, we plotted a scatterplot with the restriction that the points should be located within the boundaries of NYC. Let's plot a scatterplot now without that restriction:</p>
<pre class="calibre17">df.plot.scatter('pickup_longitude', 'pickup_latitude')<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We'll see the following scatterplot:</p>
<p class="mce-root"><img class="alignnone54" src="assets/77a6482f-4a29-4473-b1c0-9f55e6ea3aeb.png"/></p>
<p class="calibre2"/>
<p class="calibre2">Do you see where the outliers are? The dots at the periphery of the scatterplot are outliers. They have latitude values as high as <span class="calibre5">1000</span> and as low as <span class="calibre5">-3000</span>. Earth's geographic coordinate system does not have such extreme latitudes and longitudes! Let's remove these outliers:</p>
<pre class="calibre17"># range of longitude for NYC<br class="title-page-name"/>nyc_min_longitude = -74.05<br class="title-page-name"/>nyc_max_longitude = -73.75<br class="title-page-name"/><br class="title-page-name"/># range of latitude for NYC<br class="title-page-name"/>nyc_min_latitude = 40.63<br class="title-page-name"/>nyc_max_latitude = 40.85<br class="title-page-name"/><br class="title-page-name"/># only consider locations within NYC<br class="title-page-name"/>for long in ['pickup_longitude', 'dropoff_longitude']:<br class="title-page-name"/>    df = df[(df[long] &gt; nyc_min_longitude) &amp; (df[long] &lt;<br class="title-page-name"/>                                              nyc_max_longitude)]<br class="title-page-name"/><br class="title-page-name"/>for lat in ['pickup_latitude', 'dropoff_latitude']:<br class="title-page-name"/>    df = df[(df[lat] &gt; nyc_min_latitude) &amp; (df[lat] &lt; <br class="title-page-name"/>                                            nyc_max_latitude)]</pre>
<p class="calibre2">Let's summarize what we have done for data preprocessing. We first saw that missing values only constitute 0.001% of the dataset, so we can remove them safely without affecting the quantity of our training data. Next, we saw that there are outliers in <kbd class="calibre13">fare_amount</kbd>, and <kbd class="calibre13">passenger_count</kbd>, as well as the pickup and drop off latitude and longitude. We removed the outliers for the <kbd class="calibre13">fare_amount</kbd>, latitude and longitude. For the <kbd class="calibre13">passenger_count</kbd>, we replaced those rows that had a <kbd class="calibre13">0</kbd> passenger count with the <kbd class="calibre13">passenger count</kbd> = <kbd class="calibre13">1</kbd> mode.</p>
<p class="calibre2">Let's create a helper function to help us do all that data preprocessing. In machine learning projects, the number of steps can often get out of hand. It is important to adhere to strong software engineering practices, such as code modularization, to keep our project on track.</p>
<p class="calibre2">The following code takes <span class="calibre5">a pandas DataFrame </span>as input, and returns the DataFrame after performing data preprocessing:</p>
<pre class="calibre17">def preprocess(df): <br class="title-page-name"/>    # remove missing values in the dataframe<br class="title-page-name"/>    def remove_missing_values(df):<br class="title-page-name"/>        df = df.dropna()<br class="title-page-name"/>        return df<br class="title-page-name"/><br class="title-page-name"/>    # remove outliers in fare amount<br class="title-page-name"/>    def remove_fare_amount_outliers(df, lower_bound, upper_bound):<br class="title-page-name"/>        df = df[(df['fare_amount'] &gt;= lower_bound) &amp; <br class="title-page-name"/>                (df['fare_amount'] &lt;= upper_bound)]<br class="title-page-name"/>        return df<br class="title-page-name"/><br class="title-page-name"/>    # replace outliers in passenger count with the mode<br class="title-page-name"/>    def replace_passenger_count_outliers(df):<br class="title-page-name"/>        mode = df['passenger_count'].mode()<br class="title-page-name"/>        df.loc[df['passenger_count'] == 0, 'passenger_count'] = mode<br class="title-page-name"/>        return df<br class="title-page-name"/><br class="title-page-name"/>    # remove outliers in latitude and longitude<br class="title-page-name"/>    def remove_lat_long_outliers(df):<br class="title-page-name"/>        # range of longitude for NYC<br class="title-page-name"/>        nyc_min_longitude = -74.05<br class="title-page-name"/>        nyc_max_longitude = -73.75<br class="title-page-name"/>        # range of latitude for NYC<br class="title-page-name"/>        nyc_min_latitude = 40.63<br class="title-page-name"/>        nyc_max_latitude = 40.85<br class="title-page-name"/>        # only consider locations within New York City<br class="title-page-name"/>        for long in ['pickup_longitude', 'dropoff_longitude']:<br class="title-page-name"/>            df = df[(df[long] &gt; nyc_min_longitude) &amp; <br class="title-page-name"/>                    (df[long] &lt; nyc_max_longitude)]<br class="title-page-name"/>        for lat in ['pickup_latitude', 'dropoff_latitude']:<br class="title-page-name"/>            df = df[(df[lat] &gt; nyc_min_latitude) &amp; <br class="title-page-name"/>                    (df[lat] &lt; nyc_max_latitude)]<br class="title-page-name"/>        return df<br class="title-page-name"/><br class="title-page-name"/>    df = remove_missing_values(df)<br class="title-page-name"/>    df = remove_fare_amount_outliers(df, lower_bound = 0, <br class="title-page-name"/>                                     upper_bound = 100)<br class="title-page-name"/>    df = replace_passenger_count_outliers(df)<br class="title-page-name"/>    df = remove_lat_long_outliers(df)<br class="title-page-name"/>    return df</pre>
<p class="calibre2">We'll save this helper function under <kbd class="calibre13">utils.py</kbd> in our project folder. Then, to call our helper function for data preprocessing, we just have to call <kbd class="calibre13">from utils import preprocess</kbd> and we'll have access to this helper function. This keeps our code neat and manageable!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature engineering</h1>
                </header>
            
            <article>
                
<p class="calibre2">As briefly discussed in the previous chapter, <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre10">Chapter 2</a>, <em class="calibre8">Predicting Diabetes with Multilayer Perceptrons</em> feature engineering is the process of using one's domain knowledge of the problem to create new features for the machine learning algorithm. In this section, we shall create features based on the date and time of pickup, and location-related features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Temporal features</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we've seen earlier in the section on data visualization, ridership volume depends heavily on the day of the week, as well as the time of day. </p>
<p class="calibre2">Let's look at the format of the <kbd class="calibre13">pickup_datetime</kbd><span class="calibre5"> column by running the following code:</span></p>
<pre class="calibre17">print(df.head()['pickup_datetime'])</pre>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img class="alignnone55" src="assets/06303f0f-b30d-4301-a70e-5a88645314f0.png"/></p>
<p class="calibre2"><span class="calibre5">Recall that neural networks require</span> numerical <span class="calibre5">features. Therefore, w</span>e can't train our neural network using such a datetime string. Let's separate the <kbd class="calibre13">pickup_datetime</kbd><span class="calibre5"> column into different columns for <kbd class="calibre13">year</kbd>, <kbd class="calibre13">month</kbd>, <kbd class="calibre13">day</kbd>, <kbd class="calibre13">day_of_week</kbd>, and <kbd class="calibre13">hour</kbd>:</span></p>
<pre class="calibre17">df['year'] = df['pickup_datetime'].dt.year<br class="title-page-name"/>df['month'] = df['pickup_datetime'].dt.month<br class="title-page-name"/>df['day'] = df['pickup_datetime'].dt.day<br class="title-page-name"/>df['day_of_week'] = df['pickup_datetime'].dt.dayofweek<br class="title-page-name"/>df['hour'] = df['pickup_datetime'].dt.hour</pre>
<p class="calibre2">Let's take a look at the new columns:</p>
<pre class="calibre17">print(df.loc[:5,['pickup_datetime', 'year', 'month', <br class="title-page-name"/>                 'day', 'day_of_week', 'hour']])</pre>
<p class="calibre2"><span class="calibre5"><span class="calibre5">We get the following output:</span></span></p>
<p class="mce-root"><img class="alignnone56" src="assets/b5fa9eba-a1fb-4f52-87bb-2e510ebd4dc8.png"/></p>
<p class="calibre2">We can see that the new columns capture the original information from the <kbd class="calibre13">pickup_datetime</kbd><span class="calibre5"> column in a format that's suitable for our neural network. Let's drop the <kbd class="calibre13">pickup_datetime</kbd> column from our DataFrame:</span></p>
<pre class="calibre17">df = df.drop(['pickup_datetime'], axis=1)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Geolocation features</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we have seen earlier, the dataset contains information regarding the pickup and drop off coordinates. However, there is no information regarding the distance between the pickup and drop off points, which is arguably the most important factor in deciding taxi fares. Therefore, let's create a new feature that calculates the distance between each pair of pickup and drop off points.</p>
<p class="calibre2">Recall from geometry that the <em class="calibre8">Euclidean Distance</em> is the straight-line distance between any two points:</p>
<p class="mce-root"><img class="fm-editor-equation20" src="assets/d72541e9-883b-416a-a1ed-b6f1ac2758b2.png"/></p>
<p class="calibre2">Let's define a function to calculate the Euclidean distance between any two points, given the latitude and longitudes of the two points:</p>
<pre class="calibre17">def euc_distance(lat1, long1, lat2, long2):<br class="title-page-name"/>    return(((lat1-lat2)**2 + (long1-long2)**2)**0.5)</pre>
<p class="calibre2">And let's apply the function to the DataFrame to create the new <kbd class="calibre13">distance</kbd> column:</p>
<pre class="calibre17">df['distance'] = euc_distance(df['pickup_latitude'],<br class="title-page-name"/>                              df['pickup_longitude'], <br class="title-page-name"/>                              df['dropoff_latitude'],<br class="title-page-name"/>                              df['dropoff_longitude'])</pre>
<p class="calibre2">Our hypothesis was that the trip fare is closely correlated to the distance traveled. We can now plot the two variables on a scatterplot to analyze the correlation and see if our intuition was right:</p>
<pre class="calibre17">df.plot.scatter('fare_amount', 'distance')<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We get the following scatterplot:</p>
<p class="mce-root"><img class="alignnone57" src="assets/2c85bc07-2e52-4871-85d1-532bc29e1868.png"/></p>
<p class="calibre2">Nice! We can clearly see that our hypothesis is right. However, the distance traveled alone does not tell the whole story. If we look at the center of the graph, we can see three vertical lines of dots. These outlier data seems to suggest that there are certain trips where the distance traveled did not have an impact on the fare amount (which is between $<span class="calibre5">40</span> and $<span class="calibre5">60</span> for these outliers). Recall in the previous section on data visualization where we saw that there are certain pickups near airports, and these airport pickups have a flat fare of $52 plus tolls. This could explain the three<span class="calibre5"> vertical lines of dots between $<span class="calibre5">40</span> and $<span class="calibre5">60</span>!</span></p>
<p class="calibre2">Clearly, we need to engineer a new feature that informs our neural network of the pickup and drop off distance from the three major airports in NYC. When we train the neural network on this feature, it should then learn that pickups and drop offs near airports have a flat fare between<span class="calibre5"> </span><span class="calibre5">$<span class="calibre5">40</span> and $<span class="calibre5">60</span>.</span></p>
<p class="calibre2">We can use the <kbd class="calibre13">euc_distance</kbd> function that we defined earlier to calculate the <span class="calibre5">pickup and drop off distance from the three major airports in NYC:</span></p>
<pre class="calibre17">airports = {'JFK_Airport': (-73.78,40.643),<br class="title-page-name"/>            'Laguardia_Airport': (-73.87, 40.77),<br class="title-page-name"/>            'Newark_Airport' : (-74.18, 40.69)}<br class="title-page-name"/><br class="title-page-name"/>for airport in airports:<br class="title-page-name"/>    df['pickup_dist_' + airport] = euc_distance(df['pickup_latitude'],<br class="title-page-name"/>                                                df['pickup_longitude'], <br class="title-page-name"/>                                                airports[airport][1], <br class="title-page-name"/>                                                airports[airport][0])<br class="title-page-name"/>    df['dropoff_dist_' + airport] = euc_distance(df['dropoff_latitude'], <br class="title-page-name"/>                                                 df['dropoff_longitude'],<br class="title-page-name"/>                                                 airports[airport][1], <br class="title-page-name"/>                                                 airports[airport][0])</pre>
<p class="calibre2">Let's print out the first few rows, along with a few relevant columns to verify that the Euclidean distance function is functioning as intended:</p>
<pre class="calibre17">print(df[['key', 'pickup_longitude',<span> </span>'pickup_latitude',<span> <br class="title-page-name"/></span>          'dropoff_longitude',<span> </span>'dropoff_latitude', <br class="title-page-name"/>          'pickup_dist_JFK_Airport',<br class="title-page-name"/>          'dropoff_dist_JFK_Airport']].head())</pre>
<p class="calibre2"/>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img class="alignnone58" src="assets/0665247a-2e68-49fa-abea-abcf6e154b57.png"/></p>
<p class="calibre2">We can do a quick calculation on the preceding rows to verify that the Euclidean distance function works correctly. Lastly, notice that there is still a <kbd class="calibre13">key</kbd> column in the dataset. This column is similar to the <kbd class="calibre13">pickup_datetime</kbd> column, and it <span class="calibre5">was probably used as a unique identifier in the database it was stored in. We can safely remove this column without any loss of information. To remove the <kbd class="calibre13">key</kbd> column, use this command:</span></p>
<pre class="calibre17">df = df.drop(['key'], axis=1)</pre>
<p class="calibre2">To recap, in this section, we used feature engineering to construct new features based on our own domain knowledge of the problem. From the raw datetime information provided, we extracted and constructed new features for the pickup year, month, day, day of the week, and hour. We also constructed distance-based features that are crucial to the prediction of fares, such as the distance between pickup and drop off points, as well as the pickup and drop off distance from the three main airports in NYC.</p>
<p class="calibre2">Similar to the previous <em class="calibre8">Data preprocessing </em><span class="calibre5">section,</span> we're going to construct a helper function to summarize what we have done for feature engineering. This code modularization approach will help keep our code manageable:</p>
<pre class="calibre17">def feature_engineer(df):<br class="title-page-name"/>    # create new columns for year, month, day, day of week and hour<br class="title-page-name"/>    def create_time_features(df):<br class="title-page-name"/>        df['year'] = df['pickup_datetime'].dt.year<br class="title-page-name"/>        df['month'] = df['pickup_datetime'].dt.month<br class="title-page-name"/>        df['day'] = df['pickup_datetime'].dt.day<br class="title-page-name"/>        df['day_of_week'] = df['pickup_datetime'].dt.dayofweek<br class="title-page-name"/>        df['hour'] = df['pickup_datetime'].dt.hour<br class="title-page-name"/>        df = df.drop(['pickup_datetime'], axis=1)<br class="title-page-name"/>        return df<br class="title-page-name"/><br class="title-page-name"/>    # function to calculate euclidean distance<br class="title-page-name"/>    def euc_distance(lat1, long1, lat2, long2):<br class="title-page-name"/>        return(((lat1-lat2)**2 + (long1-long2)**2)**0.5)<br class="title-page-name"/><br class="title-page-name"/>    # create new column for the distance travelled<br class="title-page-name"/>    def create_pickup_dropoff_dist_features(df):<br class="title-page-name"/>        df['travel_distance'] = euc_distance(df['pickup_latitude'], <br class="title-page-name"/>                                             df['pickup_longitude'], <br class="title-page-name"/>                                             df['dropoff_latitude'],<br class="title-page-name"/>                                             df['dropoff_longitude'])<br class="title-page-name"/>        return df<br class="title-page-name"/><br class="title-page-name"/>    # create new column for the distance away from airports<br class="title-page-name"/>    def create_airport_dist_features(df):<br class="title-page-name"/>        airports = {'JFK_Airport': (-73.78,40.643),<br class="title-page-name"/>                    'Laguardia_Airport': (-73.87, 40.77),<br class="title-page-name"/>                    'Newark_Airport' : (-74.18, 40.69)}<br class="title-page-name"/>        for k in airports:<br class="title-page-name"/>            df['pickup_dist_'+k]=euc_distance(df['pickup_latitude'], <br class="title-page-name"/>                                              df['pickup_longitude'],<br class="title-page-name"/>                                              airports[k][1],<br class="title-page-name"/>                                              airports[k][0])<br class="title-page-name"/>            df['dropoff_dist_'+k]=euc_distance(df['dropoff_latitude'], <br class="title-page-name"/>                                               df['dropoff_longitude'],<br class="title-page-name"/>                                               airports[k][1],<br class="title-page-name"/>                                               airports[k][0]) <br class="title-page-name"/>        return df<br class="title-page-name"/><br class="title-page-name"/>    df = create_time_features(df)<br class="title-page-name"/>    df = create_pickup_dropoff_dist_features(df)<br class="title-page-name"/>    df = create_airport_dist_features(df)<br class="title-page-name"/>    df = df.drop(['key'], axis=1)<br class="title-page-name"/>    return df</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature scaling</h1>
                </header>
            
            <article>
                
<p class="calibre2">As a final preprocessing step, we should also scale our features before passing them to the neural network. Recall from the previous chapter, <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre10">Chapter 2</a>, <em class="calibre8">Predicting Diabetes with Multilayer Perceptrons</em>, that scaling ensures that all features have a uniform range of scale. This ensures that features with a greater scale (for example, year has a scale of &gt; 2000) does not dominate features with a smaller scale (for example, passenger count has a scale between 1 to 6).</p>
<p class="calibre2">Before we scale the features in the DataFrame, it's a good idea to keep a copy of the prescaled DataFrame. The values of the features will be transformed after scaling (for example, year 2010 may be transformed to a value such as -0.134 after scaling), which can make it difficult for us to interpret the values. By keeping a copy of the prescaled DataFrame, we can easily reference the original values:</p>
<pre class="calibre17">df_prescaled = df.copy()</pre>
<p class="calibre2">We should also drop the <kbd class="calibre13">fare_amount</kbd> <span class="calibre5">target variable </span>before scaling, as we do not want to modify the target variable:</p>
<pre class="calibre17">df_scaled = df.drop(['fare_amount'], axis=1)</pre>
<p class="calibre2">Then, scale the features by calling the <kbd class="calibre13">scale</kbd> function from scikit-learn:</p>
<pre class="calibre17">from sklearn.preprocessing import scale<br class="title-page-name"/><br class="title-page-name"/>df_scaled = scale(df_scaled)</pre>
<p class="calibre2">Lastly, convert the object returned by the <kbd class="calibre13">scale</kbd> function into a pandas DataFrame and concatenate the original <kbd class="calibre13">fare_amount</kbd> column that was dropped before scaling:</p>
<pre class="calibre17">cols = df.columns.tolist()<br class="title-page-name"/>cols.remove('fare_amount')<br class="title-page-name"/>df_scaled = pd.DataFrame(df_scaled, columns=cols, index=df.index)<br class="title-page-name"/>df_scaled = pd.concat([df_scaled, df['fare_amount']], axis=1)<br class="title-page-name"/>df = df_scaled.copy()</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep feedforward networks</h1>
                </header>
            
            <article>
                
<p class="calibre2">So far in this chapter, we have done an in-depth visualization of the dataset, cleaned up the dataset by handling outliers, and also performed feature engineering to create useful features for our model. For the rest of the chapter, we'll talk about the architecture of deep feedforward neural networks, and we'll train one in Keras for a regression task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model architecture</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous chapter, <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre10">Chapter 2</a>, <em class="calibre8">Predicting Diabetes with Multilayer Perceptrons</em>,<em class="calibre8"> </em>we used a relatively simple MLP as our neural network. For this project, since there are more features, we shall use a deeper model to account for the additional complexity. The deep feedforward network will have four hidden layers. The first hidden layer will have 128 nodes, with each successive hidden layer having half the nodes of its predecessor. This neural network size is a good starting point for us and it should not take too long to train this neural network. A general rule of thumb is that we should start with a small neural network and only increase its complexity (size) as required.</p>
<p class="calibre2">In between each hidden layer, we will use the ReLU activation function to introduce non-linearity in the model. Since this is a regression problem, there will only be one node in the output layer (more on regression in the next sub-section). Note that we do not apply the ReLU activation function for the output layer as doing so would transform our predictions.</p>
<p class="calibre2">The following diagram illustrates the model architecture of the deep feedforward neural network:</p>
<p class="mce-root"><img class="alignnone59" src="assets/f9782fed-7257-48fd-bdb2-64a446305674.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss functions for regression problems</h1>
                </header>
            
            <article>
                
<p class="calibre2">It is important to understand what regression is, and how it affects the architecture of our neural network. Our task in this project is to predict taxi fares, which is a continuous variable. We can contrast this with the classification project that we did in the previous chapter, <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre10">Chapter 2</a>, <em class="calibre8">Predicting Diabetes with Multilayer Perceptrons</em>, where we designed a neural network to output a binary prediction (1 or 0), indicating whether the patient was at risk of diabetes.</p>
<p class="calibre2">Another way to think about regression and classification is that in regression, we are trying to predict the value of a continuous variable (for example, cost, time, or height), whereas in classification, we are trying to predict a class (<span class="calibre5">for example,</span> diabetes or no diabetes).</p>
<p class="calibre2">Recall that in the previous chapter, <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre10">Chapter 2</a>, <em class="calibre8">Predicting Diabetes with Multilayer Perceptrons</em>,<em class="calibre8"> </em>we used percentage accuracy as a metric for measuring how strong our predictions are. In regression, the <strong class="calibre4">root mean square error</strong> (<strong class="calibre4">RMSE</strong>) is often used as the error metric.</p>
<p class="calibre2">The formula for <em class="calibre8">RMSE</em> is as follows:</p>
<p class="mce-root"><img class="fm-editor-equation21" src="assets/f0ee13e0-65b8-4183-bcb9-9df1e6c8f8a8.png"/> </p>
<p class="calibre2">Notice how the formula takes the square of the difference between the predicted value and the actual value. This is to ensure that over estimations and under estimations are penalized equally (since the square of the error would be the same for both). We take the square-root to ensure that the magnitude of the error is similar to the actual values. The RMSE provides a loss function for our neural network, allowing it to tune its weights during the training process in order to reduce the error of its predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model building in Python using Keras</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, let's implement our model architecture in Keras. Just like in the previous project, we're going to build our model layer by layer in Keras using the <kbd class="calibre13">Sequential</kbd> class.</p>
<p class="calibre2">First, split the DataFrame into the training features (<kbd class="calibre13">X</kbd>) and the target variable that we're trying to predict (<kbd class="calibre13"><span><span>y</span></span></kbd>):</p>
<pre class="calibre17">X = df.loc[:, df.columns != 'fare_amount'] <br class="title-page-name"/>y = df.loc[:, 'fare_amount']</pre>
<p class="calibre2">Then, split the data into a training set (80%) and a testing set (20%):</p>
<pre class="calibre17">from sklearn.model_selection import train_test_split<br class="title-page-name"/><br class="title-page-name"/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</pre>
<p class="calibre2">Next, let's build our <kbd class="calibre13">Sequential</kbd> model in Keras according to the neural network architecture we outlined earlier:</p>
<pre class="calibre17">from keras.models import Sequential<br class="title-page-name"/>from keras.layers import Dense<br class="title-page-name"/><br class="title-page-name"/>model = Sequential()<br class="title-page-name"/>model.add(Dense(128, activation= 'relu', input_dim=X_train.shape[1]))<br class="title-page-name"/>model.add(Dense(64, activation= 'relu'))<br class="title-page-name"/>model.add(Dense(32, activation= 'relu'))<br class="title-page-name"/>model.add(Dense(8, activation= 'relu'))<br class="title-page-name"/>model.add(Dense(1))</pre>
<p class="calibre2">Before we start training our model, it is a good practice to verify the structure of our model:</p>
<pre class="calibre17">model.summary()</pre>
<p class="calibre2">The <kbd class="calibre13">summary()</kbd> function produces a table showing the number of layers and number of nodes in each layer, as well as the number of parameters in each layer (that is, the weights and biases). We can verify that this is consistent with the model architecture we outlined earlier.</p>
<p class="calibre2">Here's the table produced by the <kbd class="calibre13">summary()</kbd> function:</p>
<p class="mce-root"><img class="alignnone60" src="assets/e06ec328-b384-447c-ac11-1e8cc6a32805.png"/></p>
<p class="calibre2">Finally, we can compile and train our neural network on the training data:</p>
<pre class="calibre17">model.compile(loss='mse', optimizer='adam', metrics=['mse'])<br class="title-page-name"/>model.fit(X_train, y_train, epochs=1)</pre>
<p class="calibre2">Since there's a fair bit of data, it would take some time to train the neural network. After a few minutes, Keras would output the following at the end of the training epoch:</p>
<p class="mce-root"><img class="alignnone61" src="assets/3b56dc90-735b-4b31-8723-a8421736e8ba.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Results analysis</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we have our neural network trained, let's use it to make some predictions to understand its accuracy.</p>
<p class="calibre2">We can create a function to make a prediction using a random sample from the testing set:</p>
<pre class="calibre17">def predict_random(df_prescaled, X_test, model):<br class="title-page-name"/>    sample = X_test.sample(n=1, random_state=np.random.randint(low=0, <br class="title-page-name"/>                                                              high=10000))<br class="title-page-name"/>    idx = sample.index[0]<br class="title-page-name"/>  <br class="title-page-name"/>    actual_fare = df_prescaled.loc[idx,'fare_amount']<br class="title-page-name"/>    day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', <br class="title-page-name"/>                 'Saturday', 'Sunday']<br class="title-page-name"/>    day_of_week = day_names[df_prescaled.loc[idx,'day_of_week']]<br class="title-page-name"/>    hour = df_prescaled.loc[idx,'hour']<br class="title-page-name"/>    predicted_fare = model.predict(sample)[0][0]<br class="title-page-name"/>    rmse = np.sqrt(np.square(predicted_fare-actual_fare))<br class="title-page-name"/><br class="title-page-name"/>    print("Trip Details: {}, {}:00hrs".format(day_of_week, hour))  <br class="title-page-name"/>    print("Actual fare: ${:0.2f}".format(actual_fare))<br class="title-page-name"/>    print("Predicted fare: ${:0.2f}".format(predicted_fare))<br class="title-page-name"/>    print("RMSE: ${:0.2f}".format(rmse))</pre>
<p class="calibre2">The <kbd class="calibre13">predict_random</kbd> <span class="calibre5">function </span>will pull a random row from the testing set and feed it to the model for prediction. The function will then calculate and display the RMSE of the prediction. Note that <kbd class="calibre13">df_prescaled</kbd> is required to provide us with the original values for day of week and hour, as the values in the testing set have already been transformed earlier and are no longer human-readable (for example, a day of week value of <span class="calibre5">-0.018778 does not make much sense to us).</span></p>
<p class="calibre2">Let's run the <span class="calibre5"><kbd class="calibre13">predict_random</kbd> function, shown as follows and see what kind of results we get:</span></p>
<pre class="calibre17">predict_random(df_prescaled, X_test, model)</pre>
<p class="calibre2">The trip details output by the <kbd class="calibre13">predict_random</kbd> function is as follows: </p>
<pre class="calibre17"><span>Trip Details: Sunday, 10:00hrs<br class="title-page-name"/>Actual fare: $4.90<br class="title-page-name"/>Predicted fare: $5.60<br class="title-page-name"/>RMSE: $0.70<br class="title-page-name"/></span></pre>
<p class="calibre2"/>
<p class="calibre2">The following map depicts the travel details:</p>
<p class="mce-root"><img class="alignnone62" src="assets/6138bc84-5f7e-4ce7-a0d3-ce5b0d87383b.png"/></p>
<p class="CDPAlignLeft1">The pickup and drop off points are visualized in the preceding map. The <kbd class="calibre13">Actual fare</kbd> was <kbd class="calibre13">$4.90</kbd>, while the <kbd class="calibre13">Predicted fare</kbd> is <kbd class="calibre13">$5.60</kbd>, giving us an error of <kbd class="calibre13">$0.70</kbd>. It looks like our model is working well and the predictions are fairly accurate! Note that the map and route shown in the preceding screenshot is purely for visualization and is not part of the original dataset or code.</p>
<p class="calibre2">Let's run <span class="calibre5"><kbd class="calibre13">predict_random</kbd> a few more times to get more results:</span></p>
<p class="mce-root"><img class="alignnone63" src="assets/52e43b14-a1f0-4ec2-b273-919509000146.png"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">The trip details output by the <kbd class="calibre13">predict_random</kbd> function is as follows:</p>
<pre class="calibre17"><span>Trip Details: Wednesday, 7:00hrs<br class="title-page-name"/>Actual fare: $6.10<br class="title-page-name"/>Predicted fare: $6.30<br class="title-page-name"/>RMSE: $0.20</span></pre>
<p class="calibre2">Our prediction for this trip was almost spot on! The <kbd class="calibre13">Actual fare</kbd> was <kbd class="calibre13">$6.10</kbd>, while the fare predicted by our neural network is <kbd class="calibre13">$6.30</kbd>. It seems like our neural network makes really good predictions for short distance trips.</p>
<p class="calibre2">Let's see how well it does when the trip is further and more prone to traffic delays:</p>
<p class="mce-root"><img class="alignnone64" src="assets/5fe3e94c-d089-4091-9a2e-e891a48fb306.png"/></p>
<p class="calibre2">The trip details output by the <kbd class="calibre13">predict_random</kbd> function: </p>
<pre class="calibre17"><span>Trip Details: Monday, 10:00hrs<br class="title-page-name"/>Actual fare: $35.80<br class="title-page-name"/>Predicted fare: $38.11 <br class="title-page-name"/>RMSE: $2.31</span></pre>
<p class="calibre2">As we can see from this sample, our neural network works really well even for long distance trips. The <kbd class="calibre13">Actual fare</kbd> was <kbd class="calibre13">$35.80</kbd>, while our neural network predicted a fare of <kbd class="calibre13">$38.11</kbd>. The error of <kbd class="calibre13">$2.31</kbd> (~6% discrepancy) is pretty impressive given the distance of the trip.</p>
<p class="calibre2">As a final example, let's see how our neural network performs for fixed-rate trips. Recall that all trips to/from JFK airport incur a fixed fare of $52 plus tolls, no matter the distance traveled:</p>
<p class="mce-root"><img class="alignnone65" src="assets/4d80be80-e731-4b09-8ece-96f7b82bcf52.png"/></p>
<p class="calibre2">The trip details output by the <kbd class="calibre13">predict_random</kbd> function is as follows:</p>
<pre class="calibre17"><span>Trip Details: Saturday, 23:00hrs<br class="title-page-name"/>Actual fare: $52.00<br class="title-page-name"/>Predicted fare: $53.55 <br class="title-page-name"/>RMSE: $1.55</span></pre>
<p class="calibre2">Nice! Our neural network understands that the trip started from JFK airport, and hence the fare should be close to <kbd class="calibre13">$52</kbd>. This was made possible through feature engineering, where we introduced new features that represents the pickup and drop off distance away from JFK airport. These new features allowed our neural network to learn that trips to/from JFK airport should have a fare close to <kbd class="calibre13">$52</kbd>. This shows the importance of feature engineering!</p>
<p class="calibre2">Finally, let's conclude the results by calculating the RMSE for the entire training and testing set:</p>
<pre class="calibre17">from sklearn.metrics import mean_squared_error<br class="title-page-name"/><br class="title-page-name"/>train_pred = model.predict(X_train)<br class="title-page-name"/>train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))<br class="title-page-name"/><br class="title-page-name"/>test_pred = model.predict(X_test)<br class="title-page-name"/>test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))<br class="title-page-name"/><br class="title-page-name"/>print("Train RMSE: {:0.2f}".format(train_rmse))<br class="title-page-name"/>print("Test RMSE: {:0.2f}".format(test_rmse))</pre>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img class="alignnone66" src="assets/7d1cee54-86bb-4881-81ff-3d2286e00ac4.png"/></p>
<p class="calibre2">The RMSE values show that on average, our model predicts a fare that is accurate within ~$3.50. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting it all together</h1>
                </header>
            
            <article>
                
<p class="calibre2">We have accomplished a lot in this chapter. Let's do a quick recap of the code that we have written so far.</p>
<p class="calibre2">We started off by defining a function for preprocessing. This <kbd class="calibre13">preprocess</kbd> function takes a DataFrame as an input and performs the following actions:</p>
<ul class="calibre11">
<li class="calibre12">Removing missing values</li>
<li class="calibre12">Removing outliers in the fare amount</li>
<li class="calibre12">Replacing outliers in passenger count with the mode</li>
<li class="calibre12">Removing outliers in latitude and longitude (that is, only considering points within NYC)</li>
</ul>
<p class="calibre2">This function is saved under <kbd class="calibre13">utils.py</kbd> in our project folder.</p>
<p class="calibre2">Next, we also defined a <kbd class="calibre13">feature_engineer</kbd> function for feature engineering. This function takes a DataFrame as an input and performs the following actions:</p>
<ul class="calibre11">
<li class="calibre12">Creating new columns for year, month, day, day of the week, and hour</li>
<li class="calibre12">Creating new column for the Euclidean distance between the pickup and drop off points</li>
<li class="calibre12">Creating new columns for the pickup and drop off distances away from JFK, Laguardia, and Newark airports</li>
</ul>
<p class="calibre2">This function is also saved under <kbd class="calibre13">utils.py</kbd><span class="calibre5"> </span>in our project folder.</p>
<p class="calibre2">Now that we have defined our helper functions, we can proceed with our main neural network code. Let's create a new Python file, <kbd class="calibre13">main.py</kbd>, to house our main neural network code.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">First, we import the necessary modules:</p>
<pre class="calibre17">from utils import preprocess, feature_engineer<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>from sklearn.preprocessing import scale<br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/>from keras.models import Sequential<br class="title-page-name"/>from keras.layers import Dense<br class="title-page-name"/>from sklearn.metrics import mean_squared_error</pre>
<p class="calibre2">Next, we import the first <kbd class="calibre13">500000</kbd> rows of the raw tabular data:</p>
<pre class="calibre17">df = pd.read_csv('NYC_taxi.csv', parse_dates=['pickup_datetime'], <br class="title-page-name"/>                                               nrows=500000)</pre>
<p class="calibre2">We perform preprocessing and feature engineering using the functions that we defined previously:</p>
<pre class="calibre17">df = preprocess(df)<br class="title-page-name"/>df = feature_engineer(df)</pre>
<p class="calibre2">Next, we scale the features:</p>
<pre class="calibre17">df_prescaled = df.copy()<br class="title-page-name"/>df_scaled = df.drop(['fare_amount'], axis=1)<br class="title-page-name"/>df_scaled = scale(df_scaled)<br class="title-page-name"/>cols = df.columns.tolist()<br class="title-page-name"/>cols.remove('fare_amount')<br class="title-page-name"/>df_scaled = pd.DataFrame(df_scaled, columns=cols, index=df.index)<br class="title-page-name"/>df_scaled = pd.concat([df_scaled, df['fare_amount']], axis=1)<br class="title-page-name"/>df = df_scaled.copy()</pre>
<p class="calibre2">Next, we split the DataFrame into training and testing sets:</p>
<pre class="calibre17">X = df.loc[:, df.columns != 'fare_amount'] <br class="title-page-name"/>y = df.fare_amount<br class="title-page-name"/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">We build and train our deep feedforward neural network in Keras:</p>
<pre class="calibre17">model=Sequential()<br class="title-page-name"/>model.add(Dense(128, activation= 'relu', input_dim=X_train.shape[1]))<br class="title-page-name"/>model.add(Dense(64, activation= 'relu'))<br class="title-page-name"/>model.add(Dense(32, activation= 'relu'))<br class="title-page-name"/>model.add(Dense(8, activation= 'relu'))<br class="title-page-name"/>model.add(Dense(1))<br class="title-page-name"/>model.compile(loss='mse', optimizer='adam', metrics=['mse'])<br class="title-page-name"/>model.fit(X_train, y_train, epochs=1)</pre>
<p class="calibre2">Finally, we analyze our results:</p>
<pre class="calibre17">train_pred = model.predict(X_train)<br class="title-page-name"/>train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))<br class="title-page-name"/>test_pred = model.predict(X_test)<br class="title-page-name"/>test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))<br class="title-page-name"/>print("Train RMSE: {:0.2f}".format(train_rmse))<br class="title-page-name"/>print("Test RMSE: {:0.2f}".format(test_rmse))<br class="title-page-name"/><br class="title-page-name"/>def predict_random(df_prescaled, X_test, model):<br class="title-page-name"/>    sample = X_test.sample(n=1, random_state=np.random.randint(low=0, <br class="title-page-name"/>                                                              high=10000))<br class="title-page-name"/>    idx = sample.index[0]<br class="title-page-name"/><br class="title-page-name"/>    actual_fare = df_prescaled.loc[idx,'fare_amount']<br class="title-page-name"/>    day_names = ['Monday','Tuesday','Wednesday','Thursday','Friday',<br class="title-page-name"/>                 'Saturday', 'Sunday']<br class="title-page-name"/>    day_of_week = day_names[df_prescaled.loc[idx,'day_of_week']]<br class="title-page-name"/>    hour = df_prescaled.loc[idx,'hour']<br class="title-page-name"/>    predicted_fare = model.predict(sample)[0][0]<br class="title-page-name"/>    rmse = np.sqrt(np.square(predicted_fare-actual_fare))<br class="title-page-name"/><br class="title-page-name"/>    print("Trip Details: {}, {}:00hrs".format(day_of_week, hour)) <br class="title-page-name"/>    print("Actual fare: ${:0.2f}".format(actual_fare))<br class="title-page-name"/>    print("Predicted fare: ${:0.2f}".format(predicted_fare))<br class="title-page-name"/>    print("RMSE: ${:0.2f}".format(rmse))<br class="title-page-name"/><br class="title-page-name"/>predict_random(df_prescaled, X_test, model)</pre>
<p class="calibre2">That's all of our code! Notice how creating helper functions for preprocessing and feature engineering in <kbd class="calibre13">utils.py</kbd> allows our main code to be relatively short. By modularizing our code into separate helper functions, we can focus on the implementation of each step of the machine learning framework.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we designed and implemented a deep feedforward neural network capable of predicting taxi fares in NYC within an error of ~$3.50. We first performed exploratory data analysis, where we gained important insights on the factors that affect taxi fares. With these insights, we then performed feature engineering, which is the process of using your domain knowledge of the problem to create new features. We also introduced the concept of modularizing our functions in machine learning projects, which allowed us to keep our main code relatively short and neat. </p>
<p class="calibre2">We created our deep feedforward neural network in Keras, and trained it using the preprocessed data. Our results show that the neural network is able to make highly accurate predictions for both short and long distance trips. Even for fixed-rate trips, our neural network was able to produce highly accurate predictions. </p>
<p class="calibre2">This concludes the chapter on using a deep feedforward neural network for a regression prediction task. Together with the previous chapter, <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre10">Chapter 2</a>, <em class="calibre8">Predicting Diabetes with Multilayer Perceptrons</em>, we have seen how we can use neural networks for classification and regression. In the next chapter, <a href="48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml" target="_blank" class="calibre10">Chapter 4</a>, <em class="calibre8">Cats Versus Dogs – Image Classification Using</em> <em class="calibre8">CNNs</em>,<em class="calibre8"> </em>we will introduce more complex neural networks for computer vision projects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol class="calibre14">
<li class="calibre12">When reading a CSV file using pandas, how does pandas recognize that certain columns are datetime?</li>
</ol>
<p class="calibre26">We can use the <kbd class="calibre13">parse_dates</kbd> argument when reading the CSV file using the <kbd class="calibre13">read_csv</kbd> function in pandas. </p>
<ol start="2" class="calibre14">
<li class="calibre12">How can we filter a DataFrame to only select rows within a certain range of values, assuming that we have a DataFrame, <kbd class="calibre13">df</kbd>, and we want to select rows with height values within the range of <kbd class="calibre13">160</kbd> and <kbd class="calibre13">180</kbd>?</li>
</ol>
<p class="calibre26">We can filter a DataFrame like so:</p>
<div class="title-page-name">
<pre class="calibre27"><span>df = df[(df['height'] &gt;= 160) &amp; (df['height'] &lt;= 180)]
</span></pre>
<p class="calibre26">This returns a new DataFrame with range of height values between <kbd class="calibre13">160</kbd> and <kbd class="calibre13">180</kbd>.</p>
<div class="title-page-name">
<ol start="3" class="calibre14">
<li class="calibre12">How can we use code modularization to organize our neural network projects?</li>
</ol>
<p class="calibre26">We can compartmentalize our functions using modular pieces of code. For example, in this project, we defined a <kbd class="calibre13">preprocess</kbd> and <kbd class="calibre13">feature_engineer</kbd> function in <kbd class="calibre13">utils.py</kbd>, which allows us to focus on the implementation of the preprocessing and feature engineering functions separately. </p>
<ol start="4" class="calibre14">
<li class="calibre12">How is regression different from classification tasks?</li>
</ol>
<p class="calibre26">In regression, we are trying to predict the value of a continuous variable (for example, taxi fare) whereas in classification, we are trying to predict a class (for example, diabetes or no diabetes).</p>
<ol start="5" class="calibre14">
<li class="calibre12">True or false? For regression tasks, we should apply an activation function for the output layer.</li>
</ol>
<p class="calibre26">False. For regression tasks, we should never apply an activation function for the output layer because doing so will transform our predictions, which then affects the model performance.</p>
<ol start="6" class="calibre14">
<li class="calibre12"><strong class="calibre1"> </strong>What loss function is typically used when training a neural network for regression tasks?</li>
</ol>
<p class="calibre26">The RMSE is a common loss function for regression tasks. The RMSE measures the absolute difference between the prediction and the actual target variable.</p>
</div>
</div>


            </article>

            
        </section>
    </body></html>