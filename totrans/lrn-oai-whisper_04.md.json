["```py\nimport torch\nprint(torch.cuda.is_available())\n```", "```py\n!pip install --upgrade pip\n!pip install --upgrade datasets transformers accelerate soundfile librosa evaluate jiwer tensorboard gradio\n```", "```py\nfrom huggingface_hub import notebook_login\nnotebook_login()\n```", "```py\nfrom datasets import load_dataset, DatasetDict\ncommon_voice = DatasetDict()\ncommon_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"train+validation\", use_auth_token=True)\ncommon_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"hi\", split=\"test\", use_auth_token=True)\nprint(common_voice)\n```", "```py\ncommon_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n```", "```py\nfrom transformers import WhisperTokenizer\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")\n```", "```py\ninput_str = common_voice[\"train\"][0][\"sentence\"]\nlabels = tokenizer(input_str).input_ids\ndecoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\ndecoded_str = tokenizer.decode(labels, skip_special_tokens=True)\nprint(f\"Input:                 {input_str}\")\nprint(f\"Decoded w/ special:    {decoded_with_special}\")\nprint(f\"Decoded w/out special: {decoded_str}\")\nprint(f\"Are equal:             {input_str == decoded_str}\")\n```", "```py\n    input_str = common_voice[\"train\"][0][\"sentence\"]\n    ```", "```py\n    labels = tokenizer(input_str).input_ids\n    ```", "```py\n    decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n    decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n    ```", "```py\nInput:                 खीर की मिठास पर गरमाई बिहार की सियासत, कुशवाहा ने दी सफाई\nDecoded w/ special:\n<|startoftranscript|><|hi|><|transcribe|><|notimestamps|>खीर की मिठास पर गरमाई बिहार की सियासत, कुशवाहा ने दी सफाई<|endoftext|>\nDecoded w/out special: खीर की मिठास पर गरमाई बिहार की सियासत, कुशवाहा ने दी सफाई\nAre equal:             True\n```", "```py\nfrom transformers import WhisperFeatureExtractor\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n```", "```py\nfrom transformers import WhisperProcessor\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")\n```", "```py\nprint(common_voice[\"train\"][0])\nPrint output:\n{'audio': {'path': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/607848c7e74a89a3b5225c0fa5ffb9470e39b7f11112db614962076a847f3abf/cv-corpus-11.0-2022-09-21/hi/clips/common_voice_hi_25998259.mp3',\n           'array': array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 9.6724887e-07,\n       1.5334779e-06, 1.0415988e-06], dtype=float32),\n           'sampling_rate': 48000},\n 'sentence': 'खीर की मिठास पर गरमाई बिहार की सियासत, कुशवाहा ने दी सफाई'}\n```", "```py\nfrom datasets import Audio\ncommon_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n```", "```py\n{'audio': {'path': '/home/sanchit_huggingface_co/.cache/huggingface/datasets/downloads/extracted/ted/607848c7e74a89a3b5225c0fa5ffb9470e39b7f11112db614962076a847f3abf/cv-corpus-11.0-2022-09-21/hi/clips/common_voice_hi_25998259.mp3',\n           'array': array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n       -3.4206650e-07,  3.2979898e-07,  1.0042874e-06], dtype=float32),\n           'sampling_rate': 16000},\n 'sentence': 'खीर की मिठास पर गरमाई बिहार की सियासत, कुशवाहा ने दी सफाई'}\n```", "```py\ndef prepare_dataset(batch):\n    # load and resample audio data from 48 to 16kHz\n    audio = batch[\"audio\"]\n    # compute log-Mel input features from input audio array\n    batch[«input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n    # encode target text to label ids\n    batch[«labels»] = tokenizer(batch[«sentence»]).input_ids\n    return batch\n```", "```py\ncommon_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)\n```", "```py\nimport torch\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n        batch[\"labels\"] = labels\n        return batch\n```", "```py\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n```", "```py\nimport evaluate\nmetric = evaluate.load(\"wer\")\n```", "```py\ndef compute_metrics(pred):\n    # [Code to replace -100, decode predictions and labels, and compute WER]\n    return {\"wer\": wer}\n```", "```py\nfrom transformers import WhisperForConditionalGeneration\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n```", "```py\nmodel.config.forced_decoder_ids = None\nmodel.config.suppress_tokens = []\n```", "```py\nfrom transformers import Seq2SeqTrainingArguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./whisper-small-hi\",\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=1,\n    learning_rate=1e-5,\n    warmup_steps=500,\n    max_steps=4000,\n    gradient_checkpointing=True,\n    fp16=True,\n    evaluation_strategy=\"steps\",\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    generation_max_length=225,\n    save_steps=1000,\n    eval_steps=1000,\n    logging_steps=25,\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    hub_model_id = \"your-huggingface-id/whisper-small-hi\",\n    push_to_hub=True,\n)\n```", "```py\nfrom transformers import Seq2SeqTrainer\ntrainer = Seq2SeqTrainer(\n# [Details of the trainer setup]\ntrainer = Seq2SeqTrainer(\n    args=training_args,\n    model=model,\n    train_dataset=common_voice[\"train\"],\n    eval_dataset=common_voice[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)\n```", "```py\ntrainer.train()\n```", "```py\nfrom transformers import Seq2SeqTrainingArguments\ntraining_args = Seq2SeqTrainingArguments(\n    [… previous parameters here]\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    hub_model_id = \"your-huggingface-id/whisper-small-hi\",\n    push_to_hub=True,\n)\n```", "```py\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor\nmodel = WhisperForConditionalGeneration.from_pretrained(\"jbatista79/whisper-small-hi\")\nprocessor = WhisperProcessor.from_pretrained(\"jbatista79/whisper-small-hi\")\n```", "```py\nfrom transformers import pipeline\nimport gradio as gr\npipe = pipeline(model=\"jbatista79/whisper-small-hi\")  # change to \"your-username/the-name-you-picked\"\ndef transcribe(audio):\n    text = pipe(audio)[\"text\"]\n    return text\niface = gr.Interface(\n    fn=transcribe,\n    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n    outputs=\"text\",\n    title=\"Whisper Small Hindi\",\n    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n)\niface.launch()\n```"]