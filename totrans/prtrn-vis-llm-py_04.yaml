- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers and Accelerators on the Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn how to containerize your scripts and optimize
    them for accelerators on the cloud. We’ll learn about a range of accelerators
    for foundation models, including trade-offs around cost and performance across
    the entire machine learning lifecycle. You’ll learn about key aspects of Amazon
    SageMaker and AWS to train models on accelerators, optimize performance, and troubleshoot
    common issues. if you’re already familiar with containers and accelerators on
    AWS, feel free to skip this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are accelerators and why do they matter for foundation models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containerize your scripts for accelerators on AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using accelerators with Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure optimizations on AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting accelerator performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are accelerators and why do they matter?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There’s something remarkable about human behavior. We care a lot about our own
    experiences. Many of the arts and sciences, particularly social science, specialize
    in quantifying, predicting, and understanding the implications and particularities
    of human behavior. One of the most obvious of these is human responses to technical
    performance. While this certainly varies among human groups, for the subset that
    chooses to spend a sizeable portion of their time interacting with technology,
    one theorem is self-evident. Faster and easier is always better.
  prefs: []
  type: TYPE_NORMAL
- en: Take video games, for example. While the 1940s and 50s saw some of the earliest
    video games, these didn’t come to massive popularity until arcade games such as
    *Pong* emerged in the early 70s. Perhaps unsurprisingly, this was nearly exactly
    the same time as the introduction of the original **Graphics Processor Unit**
    (**GPU**), in 1973 *(1)*! 1994 gave us the *PlayStation1* with its Sony GPU. As
    a child, I spent many hours loving the graphics performance on my Nintendo 64,
    with games such as *Zelda*, *Super Smash Brothers*, *Mario Kart 64*, and more!
    These days you only need to look at games such as *Roblox*, *League of Legends*,
    and *Fortnite* to understand how crucial graphics performance is to the success
    of the gaming industry. For decades, gaming has served as one of the most important
    signals in the market for GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Until machine learning, that is. In [*Chapter 1*](B18942_01.xhtml#_idTextAnchor016),
    we learned about the ImageNet dataset and briefly introduced its 2012 champion,
    AlexNet. To efficiently train their model on the large ImageNet dataset, the authors
    used GPUs! At the time, the GPUs were quite small, only offering 3 GB of memory,
    so they needed to implement a *model parallel strategy*. This used two GPUs to
    hold the entire model in memory. These enhancements, in addition to other modifications
    such as using the ReLU activation function and overlapped pooling, led AlexNet
    to win the challenge by a landslide.
  prefs: []
  type: TYPE_NORMAL
- en: Since that achievement more than 10 years ago, most of the best machine learning
    models have used GPUs. From transformers to reinforcement learning, training to
    inference, and vision to language, the overwhelming majority of state-of-the-art
    machine learning models require GPUs to perform optimally. For the right type
    of processing, GPUs can be many orders of magnitude faster than CPUs. When training
    or hosting deep learning models, the simple choice of using GPUs or CPUs can frequently
    make a difference of hours to days in completing a task.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know GPUs have a lot of promising benefits as compared to standard CPU processing,
    but how? What’s so different about them at a fundamental level? The answer may
    surprise you: *distribution*! Let’s take a look at this figure to understand the
    differences between CPUs and GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Differences between CPU and GPU](img/B18942_Figure_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Differences between CPU and GPU
  prefs: []
  type: TYPE_NORMAL
- en: CPUs have just a few cores but a lot of memory. This means they can do only
    a few operations at once, but they can execute these very quickly. Think of low
    latency. CPUs operate almost like a cache; they’re great at handling lots of tasks
    that rely on interactivity.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, GPUs have thousands of cores. NVIDIA’s latest generation
    GH100 chips, for example, have 18,432 cores. This means they are excellent at
    processing many operations at once, such as matrix multiplication on the millions
    to billions of parameters in your neural networks. Think of high throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t we care about both low latency and high throughput? Yes, absolutely! This
    is why the majority of compute you work with today, from your cell phone to your
    laptop, your notebook instance to the fleet of instances you need to train that
    state-of-the-art model, use both CPUs and GPUs. The question is, how?
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might imagine, writing a software program to successfully run complex
    operations across tens of thousands of microprocessors isn’t exactly the easiest
    thing in the world. This is why, in order to write code for a GPU, you need a
    specialized software framework purpose-built for hyper-distribution of operations.
    Enter **CUDA**, NVIDIA’s **Compute Unified Device Architecture**. CUDA abstracts
    away the orchestration of those underlying distributed microprocessors from the
    consumer, allowing them to leverage the massive distribution without needing to
    be an expert in its specific architecture. CUDA comes in two parts: drivers that
    work directly with the hardware, and a toolkit that exposes this hardware to developers.
    Python can work directly with CUDA, for example. PyTorch and TensorFlow interact
    with CUDA as well.'
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA certainly isn’t the only vendor providing high-performance distributed
    microprocessors. Commonly called **accelerators**, GPU-like massively parallel
    processing units are available from Amazon (Inferentia and Trainium), Google (TPUs),
    Intel (Habna Gaudi), AMD (ROCm), and more. However, each of these requires specialized
    steps to utilize the underlying distributed hardware. While there are clear advantages
    when these apply to your use case, for the purposes of a beginner’s book, we’ll
    just stick with GPUs. We’ll dive into using Amazon’s accelerators, Trainium and
    Inferentia, in [*Chapter 9*](B18942_09.xhtml#_idTextAnchor138)*, Advanced* *Training
    Concepts*.
  prefs: []
  type: TYPE_NORMAL
- en: Now you’ve been introduced to accelerators, let’s figure out how to use them!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready to use your accelerators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s start with learning how to use your accelerators:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step one: acquisition*. You definitely can’t train a model on a GPU without
    first getting your hands on at least one of the GPUs. Fortunately, there are a
    few free options for you. One of my projects at Amazon was actually writing the
    original doc for this: SageMaker Studio Lab! Studio Lab is one way to run a free
    Jupyter Notebook server in the cloud. If you’d like to use a no-cost notebook
    environment on CPUs or GPUs, store your files, collaborate with others, and connect
    to AWS or any other service, Studio Lab is a great way to get started.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Step two: containers*. Once you’re in a Jupyter notebook and are trying to
    run some example code, you’ll realize that everything hinges on installing the
    right packages. Even once you have the packages installed, connecting them to
    the GPU depends on the CUDA installation in your notebook. If the version of PyTorch
    or TensorFlow you’re trying to use doesn’t work nicely with that specific CUDA
    install, you’re out of luck!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is why using the right container as your base image is the perfect way
    to start developing, especially for deep learning on GPUs. AWS, NVIDIA, PyTorch,
    and TensorFlow all provide base images you can use to start working with deep
    learning frameworks. At AWS, we have 70+ containers across multiple frameworks
    and key versions of these *(2)*. We provide these containers across CPU, GPU,
    training, hosting, SageMaker, and our container services.
  prefs: []
  type: TYPE_NORMAL
- en: What’s a container, you ask? Imagine writing a Python script with 5, 10, 15,
    or even more than 100 software packages. Installing all of those is really time-consuming
    and error-prone! Think how hard it is just to install one package successfully
    on your own; all of that complexity, time, and careful solution you found can
    literally be transferred anywhere you like. How? Through containers! Containers
    are a powerful tool. Learn how to make them your friend.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have some idea about working with containers, especially how they
    serve as the intermediary between your model and your GPUs, let’s talk through
    the options for where to run your GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I’d like to emphasize that obviously, I’ve spent many years working at
    AWS. I love it! It’s been an amazing place for me to grow my skills, learn about
    the world, practice deep learning, serve customers, and collaborate with some
    amazing people. I’ve also spent many hours obsessing about the trade-offs in running
    compute on the cloud versus running compute on-premises. Actually, before working
    at AWS, I spent time at many different organizations: a few start-ups, a bank,
    a university, restaurants, policy organizations, and a non-profit. Each of these
    handled their compute slightly differently.'
  prefs: []
  type: TYPE_NORMAL
- en: On the one hand, purchasing compute to store on-premises might seem like a safer
    bet upfront. Importantly, you’re actually getting something physical for your
    dollars! You don’t need to pay to use the machine, and it seems easier to secure.
    After all, you can just stash it under your desk. What gives?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are five big problems with running compute on-premises:'
  prefs: []
  type: TYPE_NORMAL
- en: First is **logistics**. Say you actually do buy some local servers with GPUs.
    Where would you put them? How would you connect them to your laptop? How would
    you keep them cold? Power them sufficiently? What would you do if the power spontaneously
    shut down in that room in the middle of your experiment? You also need to wait
    for the GPUs to physically arrive in the mail. Then you need to “rack and stack”
    the boxes, putting them into your growing local data center. Soon enough, these
    auxiliary tasks can become your full-time job, and you’ll need a team of people
    if you intend to run these for your whole organization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second is **scale**. Say you only buy eight GPUs upfront. You can run a handful
    of experiments, executing them one at a time. But what if you have a great idea
    about a new project you’d like to test out? If you have only your eight GPUs,
    you are physically bound by them and unable to run extra experiments elsewhere.
    At this point, most people simply move to acquire more GPUs to run their extra
    experiments, which leads to the third problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third is **under-utilization**. When everyone goes home at night and isn’t training
    any models, what’s happening with those expensive GPUs? Probably nothing. They
    may be sitting totally unutilized. If you’re running some experiments overnight,
    or for multiple days /weeks, you may see higher levels of GPU utilization. However,
    it’s not at all uncommon for organizations to heavily invest in expensive GPU
    infrastructure only to see it actually go completely untouched by the very teams
    who requested it! Usually, you’ll see a tiny number of power users, with a very
    long tail of lightly interested parties who may log in occasionally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fourth is **currency**. Hardware updates, rapidly. Many companies are fully
    invested in releasing newer, faster, better versions of their hardware every year.
    Each year, you should expect to see a performance increase with the latest version.
    It’s not a great feeling to have made a big investment in on-premises GPUs, only
    to see them deprecated in a matter of months. It also introduces a risk to your
    experiments; you may not be able to produce state-of-the-art results if you’re
    not able to get the most performance out of your compute budget possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fifth is **carbon footprint**. How much do you know about the type of energy
    supplying your grid? Is it sustainable? How much extra energy will all of those
    GPUs add to the carbon footprint of your community, not to mention your bill?
    Amazon is actually the largest corporate purchaser of renewable energy in the
    world. We are incredibly careful about the grids supplying our regions and can
    demonstrate that moving to the AWS cloud can reduce the carbon footprint of the
    average data center by up to 80%, with a goal of up to 96% once we’re powered
    with 100% renewable energy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another benefit of running your GPUs on Amazon SageMaker specifically is that
    *you are not paying for the instances if you’re not running a job*. When using
    SageMaker to train your machine learning models, the GPUs come online *only when
    you are training the model itself*. This means your overall GPU utilization is
    significantly better by moving to SageMaker, simply because of the system architecture.
    Most data centers aren’t built for the dynamism that deep learning training really
    requires, because when you aren’t training a model, the nodes are still running!
    The same logic holds for Amazon EC2.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you’ll need to make sure that your project actually uses the GPUs.
    This is much more complex than it sounds. First, the software framework itself
    needs to connect to the underlying CUDA kernels. Then, you’ll want to use some
    tooling to ensure that you keep the GPU utilization as high as possible. There
    are a variety of techniques that you’ll learn about later in the chapter to dive
    deeper into these topics.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve learned how to get ready to use your accelerators, let’s learn
    how to use them on Amazon SageMaker!
  prefs: []
  type: TYPE_NORMAL
- en: How to use accelerators on AWS – Amazon SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you learned in the previous section, AWS is a great way to get your hands
    on GPUs without needing to provision, store, physically secure, and maintain GPUs.
    Now, we’ll take a look at an easy, efficient, and high-performance way to leverage
    GPUs on AWS – Amazon SageMaker. Let me be clear, SageMaker certainly is not the
    only way to run GPUs or accelerators on AWS. However, it is my personal favorite,
    so we’ll start there.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many books, blog posts, webinars, and re:Invent sessions dedicated
    to introducing and discussing SageMaker. I myself have a 16-video YouTube series
    you can use to learn more about it from me! However, for the purposes of this
    book, there are really three key pieces of SageMaker I want to you understand:
    **Studio**, **Training**, and **Hosting**. And each of these comes down to one
    single common denominator: **instances**.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Instances* is the term we use to describe virtual machines at AWS. The service
    is called **Elastic Compute Cloud**, or **EC2**. Every time you turn on a virtual
    machine, we call it an **instance**. You may have worked with EC2 instances in
    the past, such as turning them on in the AWS console, SSHing into them, and trying
    to write some code. But didn’t you find it a little frustrating when you needed
    to change the size? What about downloading the logs or the output? Sharing your
    notebook? Not to mention your surprise at the bill when you forgot to turn it
    off!'
  prefs: []
  type: TYPE_NORMAL
- en: Now, what if I told you there was an easy way to run notebooks, train models,
    and build business applications around your own data science work products, without
    you needing to manage really anything about that underlying infrastructure? You’d
    probably be interested, right?
  prefs: []
  type: TYPE_NORMAL
- en: That’s the core idea of SageMaker. We democratize machine learning by making
    it extremely easy for you to run your notebooks, models, jobs, pipelines, and
    processes from one single pane of glass. We also deliver extremely high performance
    at affordable prices. Let’s take a closer look at some of the key pieces of SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SageMaker Studio is our flagship development environment that is fully integrated
    with machine learning. What I love most about Studio is that *we decouple the
    compute backing your user interface from the compute running your notebook*. That
    means AWS is managing a Jupyter Server on your behalf, per user, to host your
    visual experience. This includes a huge volume of features purpose-built for machine
    learning, such as Feature Store, Pipelines, Data Wrangler, Clarify, Model Monitor,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: Then, every time you create a new Jupyter notebook, *we run this on dedicated
    instances*. These are called **Kernel Gateway Applications**, and they let you
    seamlessly run many different projects, with different package requirements and
    different datasets, without leaving your IDE. Even better, the notebooks in Studio
    are incredibly easy to upgrade, downgrade, and change kernels. That means you
    can switch from CPU to GPU, or back, without very much work interruption.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now you have some idea about how to run a notebook on SageMaker, but what about
    a large-scale job with distributed training?
  prefs: []
  type: TYPE_NORMAL
- en: 'For this topic, we’ll need to introduce the second key pillar of SageMaker:
    **Training**. SageMaker Training lets you easily define job parameters, such as
    the instances you need, your scripts, package requirements, software versions,
    and more. Then, when you fit your model, we launch a cluster of remote instances
    on AWS to run your scripts. All of the metadata, package details, job output,
    hyperparameters, data input, and more are stored, searchable, and versioned by
    default. This lets you easily track your work, reproduce results, and find experiment
    details, even many months and years after your job has finished.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we’ve put a lot of muscle into updating our training backend platform
    to enable extreme-scale modeling. From data optimizations with FSx for Lustre
    to distributed libraries such as Model and Data Parallel, we’re enabling the next
    generation of large models across vision and text to train seamlessly on AWS.
    The next chapter covers this in more detail. Most of the GPUs we’ll analyze in
    this book come under SageMaker Training.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker hosting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lastly, you can also run GPUs on SageMaker hosting. This is valuable when you
    want to build a scalable REST API on top of your core model. You might use a SageMaker
    hosting endpoint to run a search experience, deliver questions and answers, classify
    content, recommend content, and so much more. SageMaker hosting supports GPUs!
    We’ll dive into this in more detail in [*Chapter 12*](B18942_12.xhtml#_idTextAnchor178)*,
    How to Deploy* *Your Model*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you understand some of the key pillars of SageMaker, let’s break down
    this concept underlying all of them: instances.'
  prefs: []
  type: TYPE_NORMAL
- en: Instance breakdown for GPUs on SageMaker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As of November 2022, we support two primary instance families with GPUs on SageMaker,
    two custom accelerators, and Habana Gaudi accelerators. Here, I’ll break down
    how to understand the naming convention for all our instances, in addition to
    describing what you might use each of these for.
  prefs: []
  type: TYPE_NORMAL
- en: 'The naming convention for all instances is really three parts: first, middle,
    and last. For example, `ml.g4dn.12xlarge`. The `ml` part indicates that it’s actually
    a SageMaker instance, so you won’t see it in the EC2 control plane. The `g` part
    tells you what series of compute the instance is part of, especially the type
    of compute itself. Here, `g` indicates it’s a specific type of GPU accelerator:
    the `g4` instance has NVIDIA T4, and the `g5` has NVIDIA A10G. The number immediately
    after the letter is the version of this instance, with a higher number always
    being more recent. So, `g5` came out more recently than `g4`, and so on. The latest
    version of each instance will always give you better price performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, `g4` indicates you are using NVIDIA T4 GPUs. The letters after the number
    tell you what else is available on that instance, in this case, `d` gives us `n`
    is for `ml.t3.medium` to run your Jupyter notebook, but then upgrade to something
    large such as `ml.g4dn.12xlarge` for development, and ultimately, perhaps, `ml.p4dn.24xlarge`
    for extreme-scale training.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, the `g` instance is great for smaller models. This could
    include development and testing for you, such as running a complex notebook on
    this, using warm pools, or simply a multi-GPU model training with data-parallel.
    The `g5` instance is especially competitive here.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you want to train large language models, the `p` instance series
    is strongly recommended. This is because the GPUs are actually more performant,
    and also larger. They support larger models and larger batch sizes. `ml.p4dn.24xlarge`,
    with 8 NVIDIA A100s, has 40 GB of GPU memory per card. `ml.g5.48xlarge`, with
    8 NVIDIA A10Gs, has only 24 GB of GPU memory per card.
  prefs: []
  type: TYPE_NORMAL
- en: As of this writing, Trainium has just become available! This is a custom accelerator
    developed by Amazon to deliver up to 50% better cost performance for customers.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve learned about how to use GPUs on AWS, especially on Amazon SageMaker,
    and which instances you want to stay on top of, let’s unpack how to optimize GPU
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing accelerator performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two ways of approaching this, and both of them are important. The
    first is from a hyperparameter perspective. The second is from an infrastructure
    perspective. Let’s break them down!
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All of [*Chapter 7*](B18942_07.xhtml#_idTextAnchor116) is devoted to picking
    the right hyperparameters, and optimizing GPU performance is a large driver for
    that. Importantly, as the number of GPUs changes in your cluster, what we call
    your **world size**, you’ll need to modify your hyperparameters to accommodate
    that change. Also, there’s a core trade-off between increasing your overall job
    throughput, say by maxing out your batch size, and finding a smaller batch size,
    which ultimately will give you higher accuracy. Later in the book, you’ll learn
    how to use hyperparameter tuning to bridge that gap.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure optimizations for accelerators on AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, you’re going to learn about five key topics that can determine how well
    your scripts use the GPU infrastructure available on AWS. At this point in your
    journey, I am not expecting you to be an expert in any of these. I just want to
    you know that they exist and that you may need to update flags and configurations
    related to them later in your workflows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**EFA**: Amazon’s **Elastic Fabric Adapter** is a custom networking solution
    on AWS that provides optimal scale for high-performance deep learning. Purpose-built
    for the Amazon EC2 network topology, it enables seamless networking scale from
    just a few to a few hundred to thousands of GPUs on AWS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ml.p4d.24xlarge`, `ml.p3dn.24xlarge`, `ml.g4dn.12xlarge`, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AllReduce`, `Broadcast`, `Reduce`, `AllGather`, and `ReduceScatter`. Most
    distributed training software frameworks you will work with use a combination
    of these or custom implementations of these algorithms in a variety of ways. [*Chapter
    5*](B18942_05.xhtml#_idTextAnchor085) is devoted to exploring this in more detail.
    Another key NVIDIA library to know about is CUDA, as mentioned previously, which
    lets you run your deep learning framework software on the accelerators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPUDirectRDMA**: This is an NVIDIA tool that allows GPUs to communicate directly
    with each other on the same instance without needing to hop onto the CPU. This
    is also available on AWS with select instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open MPI**: **Open Message Passing Interface** is an open source project
    that enables remote machines to easily communicate with each other. The vast majority
    of your distributed training workloads, especially those that run on SageMaker,
    will use MPI as a base communication layer for the various workers to stay in
    sync with each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you’re thinking, "*Now, how do I go use all of these things?",* the answer
    is usually pretty simple. It’s three things, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, ask yourself, *which base container am I using*? If you’re using one
    of the AWS deep learning containers, then all of these capabilities will be provided
    to you after our extensive tests and checks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, take a look at which instance you are using. As you learned previously,
    each instance type opens up your application to using, or not using, certain features
    on AWS. Try to make sure you’re getting the best performance you can!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Third, look at ways to configure these in your job parameters. In SageMaker,
    we’ll use hyperparameters and settings in your scripts to ensure you’re maxing
    out performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you’ve learned a bit about optimizing GPU performance, let’s take a
    look at troubleshooting performance.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting accelerator performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can analyze our GPU performance, we need to understand generally how
    to debug and analyze performance on our training platform. SageMaker has some
    really nice solutions for this. First, all of your logs are sent to **Amazon CloudWatch**,
    another AWS service that can help you monitor your job performance. Each node
    in your cluster will have a full dedicated log stream, and you can read that log
    stream to view your overall training environment, how SageMaker runs your job,
    what status your job is in, and all of the logs your script emits. Everything
    you write to standard out, or print statements, is automatically captured and
    stored in CloudWatch. The first step to debugging your code is to take a look
    at the logs and figure out what really went wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Once you know what’s wrong in your script, you’ll probably want to quickly fix
    it and get it back online, right? That’s why we introduced **managed warm pools**
    on SageMaker, a feature that keeps your training cluster online, even after a
    job has finished. With SageMaker warm pools, you can now run new jobs on SageMaker
    Training in just a few seconds!
  prefs: []
  type: TYPE_NORMAL
- en: With a script working, next, you’ll need to analyze the overall performance
    of your job. This is where debugging tools come in really handy. SageMaker offers
    a debugger and profiler, both of which actually spin up remote instances while
    your job is running to apply rules and check on your tensors throughout the training
    process. The profiler is an especially nice tool to use; it automatically generates
    graphs and charts for you, which you can use to assess the overall performance
    of your job, including which GPUs are being utilized, and how much. NVIDIA also
    offers tooling for GPU debugging and profiling.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned before, writing software to seamlessly orchestrate tens of thousands
    of GPU cores is no small task. And as a result, it’s really common for GPUs to
    suddenly go bad. You might see NCCL errors, CUDA errors, or other seemingly unexplainable
    faults. For many of these, SageMaker actually runs GPU health checks ahead of
    time on your behalf! This is why the `p4d` instances take much longer to initialize
    than the smaller instances; we are analyzing the health of the GPUs prior to exposing
    them to you.
  prefs: []
  type: TYPE_NORMAL
- en: Outside of these known GPU-centric issues, you may see other faults such as
    your loss not decreasing or suddenly exploding, insufficient capacity, oddly low
    GPU throughput, or small changes in the node topology. For many of these, it’s
    common to implement a **Lambda function** in your account to monitor your job.
    You can use this Lambda function to analyze your Cloudwatch logs, trigger alerts,
    restart a job, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Just remember to *checkpoint your model at least every 2 to 3 hours*. We’ll
    cover most of these best practices for training at scale on SageMaker in the coming
    chapters, but for now, simply know that you need to write a full copy of your
    most recently trained model with some regularity throughout the training loop.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve learned about some techniques for troubleshooting GPU performance,
    let’s wrap up everything you’ve just learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced accelerators for machine learning, including
    how they are different from standard CPU processing and why you need them for
    large-scale deep learning. We covered some techniques for acquiring accelerators
    and getting them ready for software development and model training. We covered
    key aspects of Amazon SageMaker, notably Studio, Training, and hosting. You should
    know that there are key software frameworks that let you run code on GPUs, such
    as NCCL, CUDA, and more. You should also know about the top features that AWS
    provides for high-performance GPU conception to train deep learning models, such
    as EFA, Nitro, and more. We covered finding and building containers with these
    packages preinstalled, to successfully run your scripts on them. We also covered
    debugging your code on SageMaker and troubleshooting GPU performance.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned about GPUs in some detail, in the next chapter, we’ll
    explore the fundamentals of distributed training!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please go through the following content for more information on a few topics
    covered in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A micro-controlled peripheral processor: [https://dl.acm.org/doi/10.1145/800203.806247](https://dl.acm.org/doi/10.1145/800203.806247)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*AWS, deep* *learning*: [https://github.com/aws/deep-learning-containers](https://github.com/aws/deep-learning-containers)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
