<html><head></head><body>
		<div id="_idContainer014">
			<h1 id="_idParaDest-59" class="chapter-number"><a id="_idTextAnchor101"/>3<a id="_idTextAnchor102"/></h1>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor103"/>Machine Learning and Deep Learning Deep Dive</h1>
			<p>In the age of AI implementation, the current period of AI we find ourselves in, we must understand the <a id="_idIndexMarker147"/>pros and cons of both <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) and <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) in order <a id="_idIndexMarker148"/>to best navigate when to use either technology. Some other terms you might have come across with respect to AI/ML tools are <strong class="bold">applied AI</strong> or <strong class="bold">deep tech</strong>. As we’ve mentioned a few times over the course of this book, the underlying tech that will, for the most part, power AI products will be ML or DL. That’s because expert- or rule-based systems are slowly being powered by ML or not evolving at all. So, let’s dive a bit further into these technologies and understand how <span class="No-Break">they differ.</span></p>
			<p>In this chapter, we will explore the relationship between ML and DL and the way in which they bring their own sets of expectations, explanations, and elucidations to builders and users alike. Whether you work with products that incorporate ML models that have been around since the 50s or use cutting-edge models that have sprung into use recently, you’ll want to understand the implications either way. Incorporating ML or DL into your product will have different repercussions. Most of the time when you see an AI label on a product, it’s built using ML or DL, so we want to make sure you come out of this chapter with a firm understanding of how these areas differ and what this difference will tangibly mean for your <span class="No-Break">future products.</span></p>
			<p>In <a href="B18935_01.xhtml#_idTextAnchor012"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, we discussed how we’ve grappled with the idea of using machines since the 50s, but we wanted to expand on the history of ML and DL <strong class="bold">artificial neural networks</strong> (<strong class="bold">ANNs</strong>) to give <a id="_idIndexMarker149"/>you a sense of how long these models have been around. In this chapter, we will cover the following topics to get more familiar with the nuances related to ML <span class="No-Break">and DL:</span></p>
			<ul>
				<li>The old – <span class="No-Break">exploring ML</span></li>
				<li>The new – <span class="No-Break">exploring DL</span></li>
				<li>Emerging technologies – ancillary and <span class="No-Break">related tech</span></li>
				<li>Explainability – optimizing for ethics, caveats, <span class="No-Break">and responsibility</span></li>
				<li>Accuracy – optimizing <span class="No-Break">for success</span></li>
			</ul>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor104"/>The old – exploring ML</h1>
			<p>ML models attempt to create some representation of reality in order to help us make some sort of data-driven decision. Essentially, we use mathematics to represent <a id="_idIndexMarker150"/>some phenomenon that’s happening in the real world. ML essentially takes mathematics and statistics to predict or classify some future state. The paths diverge in one of two ways. The first group lies with the emergence of models that continue to progress through statistical models and the second group lies with the emergence of models that try to mimic our own natural neural intelligence. Colloquially, these are referred to as traditional ML and <span class="No-Break">DL models.</span></p>
			<p>You can think of all the models we covered in the <em class="italic">Model types – from linear regression to neural networks</em> section of <a href="B18935_02.xhtml#_idTextAnchor067"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> as ML models, but we didn’t cover ANNs in great depth. We’ll discuss those further in the <em class="italic">Types of neural networks</em> section later on in this chapter. In this section, we will take a look at the traditional statistical ML models in order to understand both the historical relevance and prevalence of ML models. To recap the flow of ML, it’s essentially a process of retrieving data, preparing that data through data processing, wrangling and feature engineering, running that data through a model and evaluating that model for performance, and tuning it <span class="No-Break">as needed.</span></p>
			<p>Some of the most reliable and prevalent models used in ML have been around for ages. <strong class="bold">Linear regression</strong> models have been around since the late 1800s and were popularized through the work of Karl Pearson and Sir Francis Galton, two British mathematicians. Their contributions gave way to one of the most popular ML algorithms used today, although unfortunately, both were prominent eugenicists. Karl Pearson <a id="_idIndexMarker151"/>is also credited with inventing <strong class="bold">principle component analysis</strong> (<strong class="bold">PCA</strong>), an unsupervised learning method that reduces dimensions in a dataset, <span class="No-Break">in 1901.</span></p>
			<p>A popular ML method, <strong class="bold">naive Bayes classifiers</strong>, came onto the scene in the 1960s but they’re based on the work of an English statistician named Thomas Bayes’ and his theorem of conditional probabilities, which is from the 1700s. The logistic function was introduced by Belgian mathematician Pierre Francois Velhulst in the mid-1800s, and <strong class="bold">logistic regression</strong> models were popularized by a British statistician named David Cox <span class="No-Break">in 1958.</span></p>
			<p><strong class="bold">Support vector machines</strong> (<strong class="bold">SVMs</strong>) were introduced in 1963 by Soviet mathematicians <a id="_idIndexMarker152"/>Vladimir Vapnik and Alexey Chervonenis from the Institute of Control Sciences at the Russian Academy of Sciences. The first decision tree analytical algorithm was also invented in 1963 by American statisticians James N. Morgan and John A. Sonquist from the University of Michigan <a id="_idIndexMarker153"/>and it was used in their <strong class="bold">automatic interaction detection</strong> (<strong class="bold">AID</strong>) program, but even that was derived from a <em class="italic">Porphyrian tree</em>, a classification tree-based diagram that was created by the eponymous Greek philosopher in the 3rd century BCE. Random forests, made up of an ensemble of multiple decision trees, were invented by an American statistician, Leo Breiman, from the University of California <span class="No-Break">in 2001.</span></p>
			<p>One of the simplest supervised learning models for classification and regression, the <strong class="bold">KNN algorithm</strong>, emerged from a technical analysis report that was done by statisticians Evelyn Fix and Joseph Lawson Hodges Jr. on behalf of the US Armed Forces in collaboration with Berkeley University in 1951. K-means clustering, a method of unsupervised ML clustering, was first proposed by a mathematician at UCLA named James MacQueen <span class="No-Break">in 1967.</span></p>
			<p>As you can see, many of the algorithms that are used most commonly in ML models today have their roots quite far in our modern history. Their simplicity and elegance add to their relevance today. Most of the models we’ve covered in this section were covered in <a href="B18935_02.xhtml#_idTextAnchor067"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, with the exception of DL ANNs. In the following section, we will focus <a id="_idTextAnchor105"/><span class="No-Break">on DL.</span></p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor106"/>The new – exploring DL</h1>
			<p>Part of our intention with separating ML and DL conceptually in this book is really to create <a id="_idIndexMarker154"/>associations in the reader’s mind. For most technical folks in the field, there are specific models and algorithms that come up when you see “ML” versus “DL” as a descriptor on a product. Quick reminder here that DL is a subset of ML. If you ever get confused by the two terms, just remember that DL is a form of ML that’s grown and evolved to form its own ecosystem. Our aim is to demystify that ecosystem as much as possible so that you can confidently understand the dynamics at play with DL products as a <span class="No-Break">product manager.</span></p>
			<p>The foundational idea of DL is centered around our own biological neural networks and DL uses what’s often the umbrella term of ANNs to solve complex problems. As we will see in the next section, much of the ecosystem that’s been formed in DL has been inspired by our own brains, where the “original’’ neural networks are found. This inspiration comes not just from the function of the human brain, particularly the idea of learning through examples, but also from its structure <span class="No-Break">as well.</span></p>
			<p>Because this isn’t an overly technical book meant for DL engineers, we will refrain from going into the terms and mathematics associated with DL. A basic understanding <a id="_idIndexMarker155"/>of an ANN would be helpful, however. As we go through this section, keep in mind that a neural network is composed of artificial neurons or nodes and that these nodes are stacked next to one another in layers. Typically, there <a id="_idIndexMarker156"/>are three types <span class="No-Break">of layers:</span></p>
			<ul>
				<li>The <span class="No-Break">input layer</span></li>
				<li>The <span class="No-Break">hidden layer(s)</span></li>
				<li>The <span class="No-Break">output layer</span></li>
			</ul>
			<p>While we will go over the various types of ANNs, there are some basics to how these DL algorithms work. Think in terms of layers and nodes. Essentially, data is passed through each node of each layer and the basic idea is that there are weights and biases that are passed from each node and layer. The ANNs work through the data they’re training on in order to best arrive at patterns that will help them solve the problem at hand. An ANN that has at least three layers, which means an input, output, and a minimum of one hidden layer, is “deep” enough to be classed as a DL algorithm. That settles <span class="No-Break">the layers.</span></p>
			<p>What about the nodes? If you recall, one of the simplest models we covered in prior chapters is the linear regression model. You can think of each node as its own mini-linear regression model because this is the calculation that’s happening within each node of an ANN. Each node has its data, a weight for that data, and a bias or parameter that it’s measuring against to arrive at an output. The summation of all these nodes making these calculations at scale gives you a sense of how an ANN works. If you can imagine a large scale of hundreds of layers, each with many nodes within each layer, you can start to imagine why it can be hard to understand why an ANN arrives at <span class="No-Break">certain conclusions.</span></p>
			<p>DL is often referred to as a black-box technology and this starts to get to the heart of why that is. Depending on our math skills, we humans can explain why a certain error rate or loss function is present in a simple linear regression model. We can conceptualize the ways a model, which is being fitted to a curve, can be wrong. We can also appreciate the challenge when presented with real-world data, which doesn’t lay out a perfect curve, for a model. But if we increase that scale and try to conceptualize <a id="_idIndexMarker157"/>potentially billions of nodes each representing a linear regression model, our brains will start <span class="No-Break">to hurt.</span></p>
			<p>Though DL is often discussed as a bleeding-edge technological advancement, as we saw in the prior section, this journey also started <a id="_idTextAnchor107"/><span class="No-Break">long ago.</span></p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor108"/>Invisible influences</h2>
			<p>It’s important to understand the underlying relationships that have influenced ML and DL as well <a id="_idIndexMarker158"/>as the history associated with both. This is a foundational part of the storytelling but it’s also helpful to better understand how this technology relates to the world around us. For many, understanding AI/ML concepts can be mystifying and unless you come from a tech or computer science background, the topics themselves can seem intimidating. Many will, at best, only acquire a rudimentary understanding of what this tech is and how it’s <span class="No-Break">come about.</span></p>
			<p>We want to empower anyone interested in exploring this underlying tech that will shape so many products and internal systems in the future by making a deeper understanding more accessible. Already, there’s favoritism going on. Most of the folks that intimately understand ML and DL already come from a computer science background whether it’s through formal education or through boot camps and other technical training programs. That means that for the most part, the folks that have pursued study and entrepreneurship in this field have traditionally been predominantly white and <span class="No-Break">predominantly male.</span></p>
			<p>Apart from the demographics, the level of investment in these technologies, from an academic perspective, has gone up. Let’s get into some of the numbers. Stanford University’s AI index states that AI investment at the graduate level among the world’s top universities has increased by 41.7%. That number jumps to 102.9% at the undergraduate level. An extra 48% of recipients of AI PhDs have left academia in the past decade in pursuit of the private sector’s big bucks in the last 10 years. 10 years ago, only 14.2% of computer science PhDs were AI related. Now, that number is above 23%. The United States, in particular, is holding onto the talent it educates and attracts. Foreign students that come to the US to pursue an AI PhD stay at a rate <span class="No-Break">of 81.8%.</span></p>
			<p>The picture this paints is one of a world that’s in great need of talent and skill in AI/ML. This high demand for the AI/ML skill set, particularly a demographically diverse AI skill set, is making it hard for those that have the hard skills in this field to stay in academia and the private sector handsomely rewards those that have these skills. In the start-up circuits, many VCs and investors are able to confidently solidify their investments when they know a company has somebody with an AI PhD, on staff, whether or not their product needs this heavy expertise. Placing a premium on human resources with these sought-after skills is likely not going to go away <span class="No-Break">anytime soon.</span></p>
			<p>We dream of a <a id="_idIndexMarker159"/>world where people from many competencies and backgrounds come into the field of AI because diversity is urgently needed and the opportunity that’s ahead of us is too great for the gatekeeping that’s been going on to prevail. It’s not just important that the builders of AI understand the underlying tech and what makes its application of it so powerful. It’s equally important for the business stakeholders that harness the capabilities of this tech to also understand the options and capabilities that lie before them. At the end of the day, nothing is so complicated that it can’t be <span class="No-Break">easily<a id="_idTextAnchor109"/> explained.</span></p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor110"/>A brief history of DL</h2>
			<p>In 1943, Warren S. McCulloch and Walter Pitts published a paper, <em class="italic">A logical calculus of the ideas immanent in nervous activity</em>, which made a link between mathematics and neurology by creating a computer model based on the neural networks inherent in our own <a id="_idIndexMarker160"/>brains based on a combination of algorithms to create a “threshold” to mimic how we pass information from our own biological network of neurons. Then, in 1958, Frank Rosenblatt published a paper that would be widely considered the ancestor of neural nets, called <em class="italic">The Perceptron: A perceiving and recognizing automaton</em>. This was, for all intents and purposes, the first, simplest, and <span class="No-Break">oldest ANN.</span></p>
			<p>In the 1960s, developments toward backpropagation, or the idea that a model learns from layers of past mistakes as it trains its way through a dataset, made significant strides toward what would eventually make up the neural network. The most significant part of the development that was happening at this time was coupling the idea of inspiring mathematical models with the way the brain works based on networks of neurons and backpropagation because this created the foundation of ANNs, which learned through <span class="No-Break">past iterations.</span></p>
			<p>It’s important to note here that many ANNs work in a “feedforward” motion in that they go through the input, hidden layers, and output layers sequentially and in one direction only, from input to output. The idea of backpropagation essentially allows for the ANNs to learn bi-directionally so that they’re able to minimize the error in each node, resulting in a <span class="No-Break">better performance.</span></p>
			<p>It wasn’t until 1986 when David Rumelhart, Geoffrey Hinton, and Ronald Williams published a famous paper, <em class="italic">Learning representations by back-propagating errors</em>, that people fully began to appreciate the role backpropagation plays in the success of DL. The idea that you could backpropagate through time, allowing neural networks to assign the appropriate weights as well as train a neural network with hidden layers, was revolutionary at <span class="No-Break">the time.</span></p>
			<p>After each development, there was much excitement for ML and the power of neural networks but <a id="_idIndexMarker161"/>between the mid-60s and the 80s, there was one significant issue: a lack of data as well as funding. If you’ve heard the term “AI winter,” this is what it’s referring to. Developments were made on the modeling side but we didn’t have significant ways to apply the models that were being developed without the ability and willingness of research groups to get their hands on enough data to feed <span class="No-Break">those models.</span></p>
			<p>Then, in 1997, Sepp Hochreiter and Jürgen Schmidhuber published their groundbreaking work titled <em class="italic">Long Short-Term Memory</em>, which effectively allowed DL to "solve complex, artificial long-time lag tasks that had never been solved by previous recurrent network algorithms." The reason why this development was so important was it allowed the idea of sequences to remain relevant for DL problems. Because neural networks involve hidden layers, it’s difficult for the notion of time to remain relevant, which makes a number of problems hard to solve. For instance, a traditional recurrent neural network might not be able to autocomplete a sentence in the way that a <strong class="bold">Long short-term memory (LSTM)</strong> can because it doesn’t understand the time sequence involved in the completion of <span class="No-Break">a sentence.</span></p>
			<p>Today, most DL models require a ton of supervised datasets, meaning the neural networks that power DL need lots of examples to understand whether something is, for example, a dog or a horse. If you think about it a bit though, this doesn’t actually relate that closely to how our brains work. A small child that’s just emerging and learning about the world might need to be reminded once or twice about the difference between a dog and a horse, but you likely aren’t reminding them of that difference thousands or millions <span class="No-Break">of times.</span></p>
			<p>In that sense, DL is evolving towards requiring fewer and fewer examples to learn. If you recall, in previous chapters, we went over supervised and unsupervised learning techniques, this becomes significant in the case of DL. Sure, these days we’re able to gather massive amounts of data for DL models to learn from but the models themselves are evolving to improve without needing much data toward the ultimate goal of unsupervised DL that can be trained with small amounts <span class="No-Break">of data.</span></p>
			<p>So far, we’ve covered <a id="_idIndexMarker162"/>some of the histories and influences shaping the field of ML and DL more specifically. While we haven’t gone into many of the technical concepts too heavily, this gives us a good foundation with which to understand how ML and DL have developed over time and why they’ve risen to prominence. In the following section, we will get more hands-on and get into the specific algorithms and neural networks that are used most <a id="_idTextAnchor111"/>heavily <span class="No-Break">in DL.</span></p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor112"/>Types of neural networks</h2>
			<p>We’d like to now turn your attention toward some of the most popular kinds of neural networks <a id="_idIndexMarker163"/>used in DL today. Some of these will sound familiar based on the previous section, but it will help to familiarize <a id="_idIndexMarker164"/>yourself with some of these concepts especially if you plan on working as a product manager for a DL product. Even if you aren’t currently working in this capacity, you’ll want to take a look through these in case your career does veer toward DL products in <span class="No-Break">the future.</span></p>
			<p>The following is a list of some of the most used ANNs <span class="No-Break">in DL:</span></p>
			<ul>
				<li><strong class="bold">Multilayer </strong><span class="No-Break"><strong class="bold">perceptrons</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MLPs</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Radial basis function </strong><span class="No-Break"><strong class="bold">networks</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">RBFNs</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Convolutional neural </strong><span class="No-Break"><strong class="bold">networks</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CNNs</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Recurrent neural </strong><span class="No-Break"><strong class="bold">networks</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">RNNs</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Long short-term memory </strong><span class="No-Break"><strong class="bold">networks</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LSTMs</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Generative adversarial </strong><span class="No-Break"><strong class="bold">networks</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GANs</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Self-organizing </strong><span class="No-Break"><strong class="bold">maps</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SOMs</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Deep belief </strong><span class="No-Break"><strong class="bold">networks</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DBNs</strong></span><span class="No-Break">)</span></li>
			</ul>
			<p>In the following section, we will touch on these various neural networks to give you an idea of what they are best suited for. As we did in the previous chapter with ML algorithms, we will describe some of the most popular use cases of each type of ANN so that <a id="_idIndexMarker165"/>we can understand, at least in a general sense, what some of the core competencies of each ANN are so that you can start to keep those ideas in mind should you pursue the creation of your own DL products in the future. If your aim is to specialize exclusively in supporting or building DL products of your own, this will be a great summary over<a id="_idTextAnchor113"/><a id="_idTextAnchor114"/>view of <span class="No-Break">each ANN.</span></p>
			<h3>MLPs</h3>
			<p>After David Rumelhart, Geoffrey Hinton, and Ronald Williams’s paper <em class="italic">Learning representations by back-propagating errors</em> came out in 1986, MLPs were popularized <a id="_idIndexMarker166"/>because in that paper they used backpropagation to train an MLP. Unlike RNNs, MLPs are another form <a id="_idIndexMarker167"/>of feedforward neural network that uses backpropagation to optimize the weights. For this reason, you can think of MLPs as some of the most basic forms of ANNs because they were among the first to appear and today they’re still used often to deal with the high compute power that’s needed by some of the newer ANNs out there. Their accessibility and reliability are still useful today, which is why we wanted to start this list with MLPs to give us a good foundation for conceptualizing the rest of the <span class="No-Break">DL algorithms.</span></p>
			<p>The way they learn is the algorithm will send data forward through the input and middle layers to the output layer. Then, based on the results in the output layer, it will then calculate the error to assess how off it was at predicting values. This is where backpropagation comes in because it will get a sense of how wrong it was in order to then backpropagate the rate of error. It will then optimize itself to minimize that error by adjusting the weights in the network and will effectively <span class="No-Break">update itself.</span></p>
			<p>The idea is you would pass these steps through the model multiple times until you were satisfied with the performance. Remember the distinction between supervised and unsupervised learning in <a href="B18935_01.xhtml#_idTextAnchor012"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>? Because MLPs use backpropagation to minimize their error rate by adjusting weights, MLPs are a supervised DL algorithm because they know based on our label data exactly how off they were from being right. These algorithms are also heavily used in ensembles with other ANNs as a fi<a id="_idTextAnchor115"/>nal <span class="No-Break">polishing stage.</span></p>
			<h3>RBFNs</h3>
			<p>RBFNs came on the scene in 1988 with D.S. Broomhead and David Lowe’s paper <em class="italic">Multivariable Functional Interpolation and Adaptive Networks</em>. RBFNs differ from most <a id="_idIndexMarker168"/>other ANNs we will cover in this chapter in that they only have three layers. While most ANNs, including <a id="_idIndexMarker169"/>the MLPs we discussed in the preceding section, will have an input and output layer with several hidden layers in between, RBFNs only have one hidden layer. Another key difference is rather than having the input layer be a computational layer, the input layer only passes data to the hidden layer in RBFNs, so this ANN is incredibly fast. These DL algorithms are feedforward models, so they are computationally only really passing through two layers: the hidden layer and the <span class="No-Break">output layer.</span></p>
			<p>It would be helpful to think of these networks as similar to the KNN algorithm we discussed in the previous chapter, which aims to predict data points based on the data points around the value they’re trying to predict. The reason for this is RBFNs look to approximate values based on the distance, radius, or Euclidean distance between points and they will cluster or group data in circles or spheres to better make sense of a complex multivariable dataset similar to how a K-means clustering algorithm from <a href="B18935_01.xhtml#_idTextAnchor012"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> would. This is a highly versatile algorithm that can be used with both classification and regression problems in both supervised a<a id="_idTextAnchor116"/>nd <span class="No-Break">unsupervised ways.</span></p>
			<h3>SOMs</h3>
			<p>SOMs were <a id="_idIndexMarker170"/>introduced in the 1980s by Tuevo Hohonen and are another example of unsupervised competitive learning ANNs in which the <a id="_idIndexMarker171"/>algorithm takes a multivariable dataset and reduces it into a two-dimensional “map.” Each node will compete with the others to decide whether it’s the one that should be activated, so it’s essentially just a massive competition, which is how it self-organizes. Structurally though, SOMs are very different from most ANNs. There’s really just one layer or node outside of the input layer, which is called the Kohonen layer. The nodes themselves are also not connected the way they are in more <span class="No-Break">traditional ANNs.</span></p>
			<p>The training of a SOM mirrors our own brain’s ability to self-organize and map inputs. When we sense certain inputs, our brain organizes those inputs into certain areas that are apt for what we’re seeing, hearing, feeling, smelling, or tasting. The SOM will similarly cluster data points into certain groupings. The way that happens is through a learning/training process where the algorithm sends out the data through the <a id="_idIndexMarker172"/>input layer and weights, randomly selecting <a id="_idIndexMarker173"/>input data points to test against the nodes until a node is chosen based on the distance between it and the data point, which then updates the weight of the node. This process is repeated until the training set is complete and the optimal nodes have <span class="No-Break">been selected.</span></p>
			<p>SOMs will also be in the same class of clustering algorithms such as K-means, or the RBFNs we touched on in the preceding section, in that they are useful for finding relationships and groupings in datasets that are u<a id="_idTextAnchor117"/>nlabeled <span class="No-Break">or undiscovered.</span></p>
			<h3>CNNs</h3>
			<p>CNNs, sometimes referred to as ConvNets, have multiple layers that are used largely for <a id="_idIndexMarker174"/>supervised learning use cases in which they detect objects, process images, and detect <a id="_idIndexMarker175"/>anomalies in medical and satellite images. The way this ANN works is through a feedforward, so it starts from the input layer and makes its way through the hidden layers to the ultimate output layer to categorize images. This type of ANN is characterized as categorical, so its ultimate goal is looking to put images into buckets of categories. Then, once they are categorized, it looks to group images by the similarities they share so that it can ultimately perform the object recognition that’s used to detect faces, animals, plants, or signs on the street. CNNs can be used for things such as facial recognition, object identification, and self-driving cars or what’s commonly referred to as computer vision applications <span class="No-Break">of AI.</span></p>
			<p>The four <a id="_idIndexMarker176"/>important layers in CNNs are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The <span class="No-Break">convolution layer</span></li>
				<li>The <strong class="bold">rectified linear unit</strong> (<span class="No-Break"><strong class="bold">ReLU</strong></span><span class="No-Break">) layer</span></li>
				<li>The <span class="No-Break">pooling layer</span></li>
				<li>The fully <span class="No-Break">connected layer</span></li>
			</ul>
			<p>The convolution layer turns an image into a matrix of pixel values that are 0s and 1s and then further reduces that matrix into a smaller matrix that’s a derivative from the first. The ReLU layer effectively pares down the dimensions of the image that you pass to the CNN. Even color images are passed through a grayscale when they’re originally assigned 0s and 1s. So, in the ReLU stage, the CNN actually gets rid of black pixels <a id="_idIndexMarker177"/>from the image so that it can reduce the image further and make it computationally easier for the model to process it. There’s another layer that reduces the dimensions of the image in another way: the <span class="No-Break">pooling layer.</span></p>
			<p>While the ReLU layer pares down the gradient in the image itself, the pooling layer pares down the features of the image, so if we pass the CNN an image of a cat, the pooling layer is where we will see various features such as the ears, eyes, nose, and whiskers identified. You can think of the convolution, ReLU, and pooling layers as operations that take segments of each image you feed your model and concurrently fire the outputs of those prior steps as inputs into the fully connected layer, which is what actually passes through the neural network itself to classify the image. In essence, the convolution, ReLU, and pooling layers prepare the image to pass through the neural networ<a id="_idTextAnchor118"/>k to arrive at <span class="No-Break">a conclusion.</span></p>
			<h3>RNNs</h3>
			<p>There are several operations that feedforward neural networks weren’t able to do very well, including working with sequential data that is rooted in time, operations that needed <a id="_idIndexMarker178"/>to contextualize multiple inputs (not just the current input), and operations that require <a id="_idIndexMarker179"/>memorization from previous inputs. For these reasons, the main draw of RNNs is the internal memory they possess that allows them to perform and remember the kind of robust operations required of conversational AIs such as Apple’s Siri. RNNs do well with sequential data and place a premium on the context in order to excel at working with time-series data, DNA and genomics data, speech recognition, and speech-to-text functions. In contrast to the preceding CNN example, which works with a feedforward function, RNNs work <span class="No-Break">in loops.</span></p>
			<p>Rather than the motion going from the input layer, through the hidden layers, and ultimately to the output layer, the RNN cycles through a loop back and forth and this is how it retains its short-term memory. This means the data passes through the input layer, then loops through the hidden layers, before it ultimately passes to the output layer. It’s important to note that RNNs only have short-term memory, which is why there was a need for an LSTM network. More on that in the <span class="No-Break">next section.</span></p>
			<p>In essence, the RNN actually has two inputs. The first is the initial data that makes its way through the neural network and the second is actually the information and context it’s acquired along the way. This is the framework with which it also effectively <a id="_idIndexMarker180"/>alters its own weights to current and previous inputs, so it’s course-correcting as it goes through its loops. This process <a id="_idIndexMarker181"/>of retroactively adjusting weights to minimize its error rate is known as backpropagation, which you’ll recall from the previous section (<em class="italic">A brief history of DL</em>) as this was a major advancement that has helped DL become <span class="No-Break">so successful.</span></p>
			<p>It’s helpful to imagine that an RNN is actually a collection of neural networks that are continuously retrained and optimized for accuracy through backpropagation, which is why it’s also considered a supervised learning algorithm. Because it’s such a robust and powerful DL algorithm, we can see RNNs used for anything from captioning and understanding images to predicting time-series problems to natural language pr<a id="_idTextAnchor119"/>ocessing and <span class="No-Break">machine translation.</span></p>
			<h3>LSTMs</h3>
			<p>LSTMs are basically RNNs with more memory power. Often, the way they manifest is through networks of the LSTM because what they do is actually connect layers <a id="_idIndexMarker182"/>of RNNs, which is what allows them to retain inputs over lags or longer periods of time. Much like <a id="_idIndexMarker183"/>a computer, LSTMs can write, read, or delete data from the memory it possesses. Because of this, it has the ability to learn about what data it needs to hold onto over time. Just as RNNs continuously adjust their weights and optimize for performance, LSTMs do the same thing by assigning levels of importance for what data to store or delete through its <span class="No-Break">own weights.</span></p>
			<p>LSTMs mirror our own ability to discard irrelevant or trivial information through time through LSTM cells, which have the ability to let the information come in as an input, be forgotten or excluded completely, or let it pass to influence the output. These categorizations are referred to as gates and they’re what allow LST<a id="_idTextAnchor120"/>Ms to learn <span class="No-Break">through backpropagation.</span></p>
			<h3>GANs</h3>
			<p>GANs are our favorite type of ANN because they’re essentially made up of two neural networks <a id="_idIndexMarker184"/>that are pitted against each other, hence the name, and compete toward the goal of generating <a id="_idIndexMarker185"/>new data that’s passable for real-world data. Because of this generative ability, GANs are used for image, video, and voice generation. They were also initially used for unsupervised learning because of their generative and self-regulation abilities but they can be used for supervised and reinforcement learning as well. The way it works is one of the neural networks is referred to as the generator and the other is the discriminator and the two compete as part of this <span class="No-Break">generative process.</span></p>
			<p>GANs were first introduced in a breakthrough paper that came out in 2014 by Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, which states that GANs <em class="italic">"simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making </em><span class="No-Break"><em class="italic">a mistake,"</em></span></p>
			<p class="callout-heading">Citation</p>
			<p class="callout">Goodfellow, I. J., Mirza, M., Xu, B., Ozair, S., Courville, A., &amp; Bengio, Y. (2014). <em class="italic">Generative Adversarial Networks</em>. <span class="No-Break"><em class="italic">arXiv</em></span><span class="No-Break">. </span><a href="https://doi.org/10.48550/arXiv.1406.2661"><span class="No-Break">https://doi.org/10.48550/arXiv.1406.2661</span></a></p>
			<p>We can think of discriminative and generative models as two sides of the same coin. Discriminative models look at the features a type of image might have, for example, looking for associations between all the images of dogs it’s currently learning from. Generative models start from the category itself and expand out into potential features a category in that image might possess. If we take the example of a category such as space kittens, the generative model might look at the example data it’s fed and deduce that if it creates an image, it should create something that involves space and kittens. The discriminative model then takes the image the generative model creates and confirms, based on its own learning, that any images in the space kittens category must contain both kittens and space <span class="No-Break">as features.</span></p>
			<p>Another way to explain this is that the generative model maps the label to potential features and the discriminative model maps features to the label. What’s most interesting to us about GANs is they effectively pass or fail their own version of the Turing test. How do you know whether you passed? If the GAN correctly identifies a generated image as a falsified image, it’s passed (or failed?) its own test. It really depends on how you look at passing or failing for that matter. If it incorrectly labeled a falsified/generated image as a “real” image, it means the generative model is pretty strong because its own discriminator wasn’t able to discriminate properly. Then again, because it’s a double-sided coin, it means that the discriminator needs to be strengthened to be more discerning. This one is <span class="No-Break">very meta.</span></p>
			<p>The steps <a id="_idIndexMarker186"/>a GAN takes to run through <a id="_idIndexMarker187"/>its process first begin with a generator neural network that takes in data and returns an image, which is then fed to the discriminator along with other images from a real-world dataset. Then, the discriminator produces outputs that are numbered between 0 and 1, which it assigns as probabilities for each of the images it is discriminating, with 0 representing a fake and 1 representing an authentic real-world image. GANs also use backpropagation, so every time the discriminator makes a wrong call, the GAN learns from previous mistakes to correct<a id="_idTextAnchor121"/> its weights and optimize itself <span class="No-Break">for accuracy.</span></p>
			<h3>DBNs</h3>
			<p>DBNs also have multiple layers, including multiple hidden layers, but the nodes in one layer <a id="_idIndexMarker188"/>aren’t connected to each other, though they are connected to nodes in other layers. There are relations <a id="_idIndexMarker189"/>between the layers themselves, but not <a id="_idIndexMarker190"/>between the nodes within. DBNs are unsupervised learning layers of what are called <strong class="bold">Restricted Boltzmann Machines </strong>(<strong class="bold">RBMs</strong>), which are themselves another form of ANN. These layers of RBMs are chained together to form a DBN. Because of this chain, as data passes through the input layer of each RBM, the DBN learns and obtains features from the prior layer. The more layers of RBMs you add, the greater the improvement and training of the DBN overall. Also, every RBM is taught individually and the DBN training isn’t done until all the DBNs have <span class="No-Break">been trained.</span></p>
			<p>DBNs are referred to as generative ANNs because each of the RBMs learns and obtains potential values for your data points based on probability. Because of this generative ability that they have, they can be used for things such as image recognition, capturing motion data, or recognizing speech. They are also computationally energy-efficient because each cluster of RBMs operates independently. Rather than data passing through all the layers in concert as with feedforward ANNs, data stays local to <span class="No-Break">each cluster.</span></p>
			<p>As a product <a id="_idIndexMarker191"/>manager, you won’t <a id="_idIndexMarker192"/>need to have in-depth knowledge of each neural network because if you’re building a product with DL components, you’ve got internal experts that can help determine which neural networks to use.  But it does help to know what some of the most common types of neural networks out there are so that you aren’t left in the dark about those determinations. In the next section, we will see how DL neural networks overlap with other emerging technologies for bette<a id="_idTextAnchor122"/>r context on the ability and influence <span class="No-Break">of DL.</span></p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor123"/>Emerging technologies – ancillary and related tech</h1>
			<p>ML and <a id="_idIndexMarker193"/>DL have been used heavily in applications related to natural language processing (<strong class="bold">natural language generation</strong> (<strong class="bold">NLG</strong>), as well as <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>)), speech recognition, chatbots, virtual <a id="_idIndexMarker194"/>agents and assistants, decision management, process automation, text analytics, biometrics, cybersecurity, content creation, image and emotion recognition, and marketing automation. It’s <a id="_idIndexMarker195"/>important to remember, particularly from a product manager’s perspective, that AI will increasingly work its way into more of how we live our lives and do our work. This is doubly true if you work in an innovative capacity as a product manager where you’re involved with the ideation and creation of new use cases and MVPs for <span class="No-Break">future products.</span></p>
			<p>Over the passage of time, we’ll see AI continue to augment our workforce both through the process of internal automation as well as through the adoption of AI-based no-code/low-code external products and applications that will boost job functions, skills, and abilities across the board. AR, VR, and the metaverse also offer us new emerging fields where ML will learn more about our world, help us learn about ourselves, and also help us build new worlds altogether. We will also continue to see ML employed through AI-powered devices such as self-driving planes, trains, and automobiles, as well as biometrics, nanotechnologies, and IoT devices that share streams of data about our bodies and appliances that we can increasingly use to optimize our security, health and <span class="No-Break">energy usage.</span></p>
			<p>There are, of course, other forms of AI beyond ML and DL, as well as ancillary emerging technologies that are often used in concert with the tech we’ve covered in this chapter. For instance, with all the innovation and fame that’s accompanied the Boston Dynamics dog Spot, we were surprised to find out recently that they don’t actually use ML to train these little guys. But even Spot will soon see AI updates to its operating system to help it with things such as object recognition and semantic contextualization of <span class="No-Break">those objects.</span></p>
			<p>AI in general, and DL specifically, might be getting an update of its own soon through quantum <a id="_idIndexMarker196"/>computing since IBM made its aspirations more concrete by publicly announcing a “road map” for the development of its quantum computers, including the ambitious goal of building one containing 1,000 qubits by 2023. IBM’s current largest quantum computer contains <span class="No-Break">65 qubits.</span></p>
			<p>Quantum computing <a id="_idIndexMarker197"/>can massively help us deal with the ongoing issue of storing and retrieving data, particularly big data, in cost-effective ways. Because so many DL projects can take weeks to train and require access to big data, ancillary developments in quantum computing can prove groundbreaking in the area of DL to the point where the algorithms both require fewer data to train on and can also handle more data and compute power more quickly. This could also allow us greater opportunities for making sense of how the models come to certain conclusions and assist with p<a id="_idTextAnchor124"/>erhaps the greatest hurdle of DL – <span class="No-Break">explainability.</span></p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor125"/>Explainability – optimizing for ethics, caveats, and responsibility</h1>
			<p>Ethics and responsibility play a foundational role in dealing with your customers’ data and <a id="_idIndexMarker198"/>behavior and because most of you will build products that help assist humans to make decisions, eventually someone is going to ask you how your product arrives at conclusions. Critical thinking is one of the foundational cornerstones of human reasoning and if your product is rooted in DL, your answer won’t be able to truly satisfy anyone’s skepticism. Our heartfelt advice is this: don’t create a product that will harm people, get you sued, or pose a risk to <span class="No-Break">your business.</span></p>
			<p>If you’re leveraging ML or DL in a capacity that has even the potential to cause harm to others, if there’s a clear bias that affects underrepresented or minority groups (in terms of race, gender, or culture), go back to the ideation phase. This is true whether that’s immediate or downstream harm. This is a general risk all of ML poses to us collectively: the notion that we’re coding our societal biases into AI without taking the necessary precautions to make sure the data we feed our algorithms is <span class="No-Break">truly unbiased.</span></p>
			<p>The engineers themselves that build these ANNs are unable to look under the hood and truly explain how ANNs make decisions. As we’ve seen with the, albeit layman, preceding explanations of DL algorithms, ANN structures are built on existing ML algorithms and scaled, so it’s almost impossible for anyone to truly explain how these networks come to <span class="No-Break">certain conclusions.</span></p>
			<p>Again, this is why DL algorithms are often referred to as a black box because they resist a truly in-depth analysis of the underpinnings of the logic that makes them work. DL has a natural opacity to it because of the nature and complexity of ANNs. Remember that ANNs effectively just make slight adjustments to the weights that affect <a id="_idIndexMarker199"/>each neuron within its layers. They are basically elaborate pattern finders using math and statistics to make optimizations to their weighting system. They do that hundreds of times for each data point over multiple iterations of training. We simply don’t have the mental capacity or language to <span class="No-Break">explain this.</span></p>
			<p>You also don’t have to be a DL engineer to truly understand how your product affects others. If you, as a product manager, are not able to fully articulate how DL is leveraged in your product and, at the very least, can’t prove that the outputs of your DL product aren’t causing harm to others, then it probably isn’t a product you want to go all <span class="No-Break">in for.</span></p>
			<p>DL is still very much in the research phase and many product managers are hesitant to incorporate it because of the issue of explainability we discussed earlier in the chapter. So we urge product managers to use caution when looking to wet their feet with DL. We have plenty of examples of ML causing harm to people even when it involves basic linear regression models. Moving forward without a sense of stewardship of and responsibility for our peers and customers with something as complicated and full of potential as DL is a recipe for adding more chaos and harm to <span class="No-Break">the world.</span></p>
			<p>Do we always have to be so cautious? Not necessarily. If DL applications get really good at saving lives by detecting cancer or they work better when applied to robotics, who are we to stand in the way of progress? Be critical about when to be concerned with your use of DL. If your system works effectively and is better because of DL and there isn’t some problem or concern springing from th<a id="_idTextAnchor126"/>e opacity of the ANNs, then all is right with <span class="No-Break">the world.</span></p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor127"/>Accuracy – optimizing for success</h1>
			<p>When it comes to DL, we can only truly grapple with its performance. Even from a performance perspective, a lot of DL projects fail to give the results their own engineers <a id="_idIndexMarker200"/>are hoping for, so it’s important to manage expectations. This is doubly true if you’re managing the expectations of your leadership team as well. If you’re a product manager or entrepreneur and you’re thinking of incorporating DL, do so in the spirit of science and curiosity. Remain open about <span class="No-Break">your expectations.</span></p>
			<p>But make sure you’re setting your team up for success. A big part of your ANN’s performance also lies in the data preparation you take before you start training your models. Passing your data through an ANN is the last step in your pipeline. If you don’t have good validation or if the quality of your data is poor, you’re not going to see positive results. Then, once you feel confident that you have enough data and that it’s clean enough to pass through an ANN, start experimenting. If you’re looking for optimal performance, you’ll want to try a few different models, or a selection of models together, and compare <span class="No-Break">the performance.</span></p>
			<p>The time it takes to fine-tune a DL model is also aggressively long. If you’re used to other forms of ML, it might shock you to experience training a model over the course of days or weeks. This is largely because the amount of data you need to train ANNs is vast; most of the time you need at least 10,000 data points, and all this data is passed through multiple layers of nodes and processed by your ANN. Your ANN is also, most of the time, going to be an ensemble of several types of ANNs we mentioned previously. The chain then becomes <span class="No-Break">quite long.</span></p>
			<p>The nature of understanding ANNs is inherently mysterious because of the complexity of the layers of artificial neurons. We cannot see deterministic qualities. Even when you do everything “right” and you get a good performance, you don’t really know why. You just know that it works. The same goes when something does go wrong or when you see poor performance. You once again don’t really know why. Perhaps the fault lies with the ANN or with the method you’re using or something has changed in the environment. The process of getting back to better performance is also iterative. And then it’s back to the <span class="No-Break">drawing board.</span></p>
			<p>Remember that these are emerging tech algorithms. It may take us all some time to adjust to new technologies and truly understand the power they have. Or don't! Part of the disillusionment that’s happened with DL actually lies in the tempering of expectations. Some DL algorithms can make amazing things happen and can show immensely promising performance but others can so easily fall flat. It’s not a magic bullet. It’s just <a id="_idIndexMarker201"/>a powerful tool that needs to be used in the proper way by people that have the knowledge, wisdom, and experience to do so. Considering most of the ANNs we went over together are from the 80s, 90s, and early 2000s, that’s not <span class="No-Break">much time.</span></p>
			<p>So tread carefully here if you’re building, managing, or ideating on DL products. When in doubt, there are other more explainable models to choose from, which we’ve covered in <a href="B18935_01.xhtml#_idTextAnchor012"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. It’s better to be safe than sorry. If you’ve got lots of time, patience, excitement, and curiosity along with a safe, recreational idea for applying DL, then you’re probably in a good position to explore that passion a<a id="_idTextAnchor128"/>nd create something the world could use in <span class="No-Break">good faith.</span></p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor129"/>Summary</h1>
			<p>We got the chance to go deep into DL in this chapter and understand some of the major social and historical influences that impact this subsection of ML. We also got the chance to look at some of the specific ANNs that are most commonly used in products powered by DL in order to get more familiar with the actual models we might come across as we build with DL. We ended the chapter with a look at some of the other emerging technologies that collaborate with DL, as well as getting further into some of the concepts that impact DL most: explainability <span class="No-Break">and accuracy.</span></p>
			<p>DL ANNs are super powerful and exhibit great performance, but if you need to explain them, you will run into more issues than you would if you stick to more traditional ML models. We’ve now spent the first three chapters of the book getting familiar with the more technical side of AI product management. Now that we’ve got that foundation covered, we can spend some time contextualizing all <span class="No-Break">this tech.</span></p>
			<p>In the next chapter, we will explore some of the major areas of AI products we see on the market, as well as examples of the ethics and success factors that contribute most <span class="No-Break">to commercialization.</span></p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor130"/>References</h1>
			<ul>
				<li>Some Methods for Classification and Analysis of Multivariate Observations: <a href="https://books.google.com/">https://books.google.com/</a> </li>
				<li>K-Nearest Neighbors Algorithm: Classification and Regression Star <a href="https://www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/">https://www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/</a> </li>
				<li>Random Forests <a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf</a> </li>
				<li>Decision <span class="No-Break">Trees </span><a href="https://www.cse.unr.edu/~bebis/CS479/PaperPresentations/DecisionTrees.pdf"><span class="No-Break">https://www.cse.unr.edu/~bebis/CS479/PaperPresentations/DecisionTrees.pdf</span></a></li>
				<li>Support Vector Machine: The most popular machine learning <span class="No-Break">algorithm </span><a href="https://cml.rhul.ac.uk/svm.html"><span class="No-Break">https://cml.rhul.ac.uk/svm.html</span></a></li>
				<li>Logistic <span class="No-Break">Regression </span><a href="https://uc-r.github.io/logistic_regression#:~:text=Logistic%20regression%20(aka%20logit%20regression,more%20predictor%20variables%20(X)"><span class="No-Break">https://uc-r.github.io/logistic_regression#:~:text=Logistic%20regression%20(aka%20logit%20regression,more%20predictor%20variables%20(X)</span></a></li>
				<li>Logistic Regression <span class="No-Break">History </span><a href="https://holypython.com/log-reg/logistic-regression-history/"><span class="No-Break">https://holypython.com/log-reg/logistic-regression-history/</span></a></li>
				<li>Bayes <span class="No-Break">Classifier </span><a href="https://www.sciencedirect.com/topics/computer-science/bayes-classifier#:~:text=Na%C3%AFve%20Bayes%20classifier%20(also%20known,use%20Na%C3%AFve%20Bayes%20since%201960s"><span class="No-Break">https://www.sciencedirect.com/topics/computer-science/bayes-classifier#:~:text=Na%C3%AFve%20Bayes%20classifier%20(also%20known,use%20Na%C3%AFve%20Bayes%20since%201960s</span></a></li>
				<li>Principal Component <span class="No-Break">Analysis </span><a href="https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/principal-component-analysis#:~:text=PCA%20was%20invented%20in%201901,the%20modeling%20of%20response%20data"><span class="No-Break">https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/principal-component-analysis#:~:text=PCA%20was%20invented%20in%201901,the%20modeling%20of%20response%20data</span></a></li>
				<li>Galton, Pearson, and the Peas: A Brief History of Linear Regression for Statistics <span class="No-Break">Instructors </span><a href="https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910537"><span class="No-Break">https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910537</span></a></li>
				<li><span class="No-Break">IBM promises 1000-qubit quantum computer—a milestone—by </span><span class="No-Break">2023 </span><a href="https://www.science.org/content/article/ibm-promises-1000-qubit-quantum-computer-milestone-2023"><span class="No-Break">https://www.science.org/content/article/ibm-promises-1000-qubit-quantum-computer-milestone-2023</span></a></li>
				<li>Boston Dynamics says AI advances for Spot the robo-dog are <span class="No-Break">coming </span><a href="https://venturebeat.com/ai/boston-dynamics-says-ai-advances-for-spot-the-robo-dog-are-coming/"><span class="No-Break">https://venturebeat.com/ai/boston-dynamics-says-ai-advances-for-spot-the-robo-dog-are-coming/</span></a></li>
				<li>Convolutional Neural Network <span class="No-Break">Tutorial </span><a href="https://www.simplilearn.com/tutorials/deep-learning-tutorial/convolutional-neural-network"><span class="No-Break">https://www.simplilearn.com/tutorials/deep-learning-tutorial/convolutional-neural-network</span></a></li>
				<li>Generative Adversarial <span class="No-Break">Networks </span><a href="https://arxiv.org/abs/1406.2661"><span class="No-Break">https://arxiv.org/abs/1406.2661</span></a></li>
				<li>The Self-Organizing <span class="No-Break">Map </span><a href="https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf"><span class="No-Break">https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf</span></a></li>
				<li>Multivariable Functional Interpolation and Adaptive <span class="No-Break">Networks </span><a href="https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1988-Broomhead-CS.pdf"><span class="No-Break">https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1988-Broomhead-CS.pdf</span></a></li>
				<li>Long Short-Term <span class="No-Break">Memory </span><a href="http://www.bioinf.jku.at/publications/older/2604.pdf"><span class="No-Break">http://www.bioinf.jku.at/publications/older/2604.pdf</span></a></li>
				<li>Learning Representations by Back Propagating <span class="No-Break">Errors </span><a href="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf"><span class="No-Break">https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf</span></a></li>
				<li>The numerical solution of variational <span class="No-Break">problems </span><a href="https://www.sciencedirect.com/science/article/pii/0022247X62900045"><span class="No-Break">https://www.sciencedirect.com/science/article/pii/0022247X62900045</span></a></li>
				<li>The Perceptron: A Perceiving and Recognizing <span class="No-Break">Automaton </span><a href="https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf"><span class="No-Break">https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf</span></a></li>
				<li>A Logical Calculus of the Ideas Immanent in Nervous <span class="No-Break">Activity </span><a href="https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf"><span class="No-Break">https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf</span></a></li>
				<li>AI <span class="No-Break">Education </span><a href="https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-4.pdf"><span class="No-Break">https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-4</span></a></li>
			</ul>
		</div>
	</body></html>