<html><head></head><body>
<div id="_idContainer026" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-59"><a id="_idTextAnchor060" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-60" class="calibre5"><a id="_idTextAnchor061" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.2.1">Understanding Key Parameters and Their Impact on Generated Responses</span></h1>
<p class="calibre3"><a id="_idTextAnchor062" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.3.1">In the previous chapter, we learned that the OpenAI API is not just one endpoint but also a collection of various endpoints. </span><span class="kobospan" id="kobo.3.2">These endpoints are triggered with </span><strong class="bold"><span class="kobospan" id="kobo.4.1">HTTP</span></strong><span class="kobospan" id="kobo.5.1"> requests, which contain parameters in the corresponding request </span><strong class="bold"><span class="kobospan" id="kobo.6.1">Body</span></strong><span class="kobospan" id="kobo.7.1">. </span><span class="kobospan" id="kobo.7.2">For chat completions, the two required parameters are </span><strong class="source-inline"><span class="kobospan" id="kobo.8.1">model</span></strong><span class="kobospan" id="kobo.9.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.10.1">messages</span></strong><span class="kobospan" id="kobo.11.1"> – and we’ve mainly seen how changing the messages parameter impacts the generated response. </span><span class="kobospan" id="kobo.11.2">However, there is a vast collection of optional parameters that influence the behavior of the API, such as temperature, N, and the maximum number </span><span><span class="kobospan" id="kobo.12.1">of tokens.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.13.1">In this chapter, we will explore these optional key parameters and understand how they influence the generated response. </span><strong class="bold"><span class="kobospan" id="kobo.14.1">Parameters</span></strong><span class="kobospan" id="kobo.15.1"> are like the dials and knobs you’d find on a complex machine. </span><span class="kobospan" id="kobo.15.2">By adjusting these dials and knobs, you can change the behavior of the machine to your liking. </span><span class="kobospan" id="kobo.15.3">Similarly, in the realm of ChatGPT, parameters allow us to tweak the finer details of the model’s behavior, influencing how it processes input and crafts its output. </span><span class="kobospan" id="kobo.15.4">Each of these plays a unique role in shaping the response </span><span><span class="kobospan" id="kobo.16.1">from OpenAI.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.17.1">By the end of this chapter, you will know how these parameters can be adjusted to better suit your specific needs, how they affect the quality, length, and style of the output, and how to make effective use of them to get the most desirable results. </span><span class="kobospan" id="kobo.17.2">Learning this is important, as these parameters will need to change as we begin integrating the API for different use cases in intelligent applications, and understanding how generated responses change with these parameters will enable us to determine the </span><span><span class="kobospan" id="kobo.18.1">correct settings.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.19.1">Specifically, we will cover the following recipes, each of which will focus on a </span><span><span class="kobospan" id="kobo.20.1">key parameter:</span></span></p>
<ul class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.21.1">Changing the model parameter and understanding its impact on </span><span><span class="kobospan" id="kobo.22.1">generated responses</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.23.1">Controlling the number of generated responses using the </span><span><span class="kobospan" id="kobo.24.1">n parameter</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.25.1">Determining the randomness and creativity of generated responses using the </span><span><span class="kobospan" id="kobo.26.1">temperature parameter</span></span></li>
</ul>
<h1 id="_idParaDest-61" class="calibre5"><a id="_idTextAnchor063" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.27.1">Technical requirements</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.28.1">All the recipes in this chapter require you to have access to the OpenAI API (via a generated API key) and have an API client installed, such as Postman. </span><span class="kobospan" id="kobo.28.2">You can refer to the </span><a href="B21007_01.xhtml#_idTextAnchor021" class="pcalibre1 calibre6 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.29.1">Chapter 1</span></em></span></a><span class="kobospan" id="kobo.30.1"> recipe </span><em class="italic"><span class="kobospan" id="kobo.31.1">Making OpenAI API requests with Postman</span></em><span class="kobospan" id="kobo.32.1"> for more information on how to obtain your API key and set </span><span><span class="kobospan" id="kobo.33.1">up Postman.</span></span></p>
<h1 id="_idParaDest-62" class="calibre5"><a id="_idTextAnchor064" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.34.1">Changing the model parameter and understanding its impact on generated responses</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.35.1">In both </span><a href="B21007_01.xhtml#_idTextAnchor021" class="pcalibre1 calibre6 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.36.1">Chapter 1</span></em></span></a><span class="kobospan" id="kobo.37.1"> and </span><a href="B21007_02.xhtml#_idTextAnchor044" class="pcalibre1 calibre6 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.38.1">Chapter 2</span></em></span></a><span class="kobospan" id="kobo.39.1">, the chat completion requests were made using both the model and messages parameters, with </span><strong class="source-inline"><span class="kobospan" id="kobo.40.1">model</span></strong><span class="kobospan" id="kobo.41.1"> always being equal to the </span><strong class="source-inline"><span class="kobospan" id="kobo.42.1">gpt-3.5-turbo</span></strong><span class="kobospan" id="kobo.43.1"> value. </span><span class="kobospan" id="kobo.43.2">We essentially ignored the model parameter. </span><span class="kobospan" id="kobo.43.3">However, this parameter likely has the biggest impact on the generated responses of any other parameter. </span><span class="kobospan" id="kobo.43.4">Contrary to popular belief, the OpenAI API is not just one model; it’s powered by a diverse set of models with different capabilities and </span><span><span class="kobospan" id="kobo.44.1">price points.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.45.1">In this recipe, we </span><a id="_idIndexMarker102" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.46.1">will cover two main models (</span><em class="italic"><span class="kobospan" id="kobo.47.1">GPT-3.5</span></em><span class="kobospan" id="kobo.48.1"> and </span><em class="italic"><span class="kobospan" id="kobo.49.1">GPT-4</span></em><span class="kobospan" id="kobo.50.1">), learn how to change the </span><strong class="source-inline"><span class="kobospan" id="kobo.51.1">model</span></strong><span class="kobospan" id="kobo.52.1"> parameter, and observe how the generated responses vary between these </span><span><span class="kobospan" id="kobo.53.1">two models.</span></span></p>
<h2 id="_idParaDest-63" class="calibre7"><a id="_idTextAnchor065" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.54.1">Getting ready</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.55.1">Ensure you</span><a id="_idIndexMarker103" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.56.1"> have an OpenAI Platform account with available usage credits. </span><span class="kobospan" id="kobo.56.2">If you don’t, please follow the </span><em class="italic"><span class="kobospan" id="kobo.57.1">Setting up your OpenAI Playground environment</span></em><span class="kobospan" id="kobo.58.1"> recipe in </span><a href="B21007_01.xhtml#_idTextAnchor021" class="pcalibre1 calibre6 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.59.1">Chapter 1</span></em></span></a><span><span class="kobospan" id="kobo.60.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.61.1">Furthermore, ensure that you have Postman installed, that you have created a new workspace, that you have created a new HTTP request, and that </span><strong class="source-inline"><span class="kobospan" id="kobo.62.1">Headers</span></strong><span class="kobospan" id="kobo.63.1"> for that request are correctly configured. </span><span class="kobospan" id="kobo.63.2">This is important because, without the </span><strong class="source-inline"><span class="kobospan" id="kobo.64.1">Authorization</span></strong><span class="kobospan" id="kobo.65.1"> configured, you will not be able to use the API. </span><span class="kobospan" id="kobo.65.2">If you don’t have Postman installed and configured as mentioned, follow the </span><em class="italic"><span class="kobospan" id="kobo.66.1">Making OpenAI API requests with Postman</span></em><span class="kobospan" id="kobo.67.1"> recipe in </span><a href="B21007_01.xhtml#_idTextAnchor021" class="pcalibre1 calibre6 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.68.1">Chapter 1</span></em></span></a><span class="kobospan" id="kobo.69.1">. </span><span class="kobospan" id="kobo.69.2">However, if you do not remember, </span><em class="italic"><span class="kobospan" id="kobo.70.1">steps 1–4</span></em><span class="kobospan" id="kobo.71.1"> in the next </span><a id="_idIndexMarker104" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.72.1">section explain the </span><span><span class="kobospan" id="kobo.73.1">configuration process.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.74.1">All the recipes in this chapter will have this </span><span><span class="kobospan" id="kobo.75.1">same requirement.</span></span></p>
<h2 id="_idParaDest-64" class="calibre7"><a id="_idTextAnchor066" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.76.1">How to do it…</span></h2>
<ol class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.77.1">In your </span><a id="_idIndexMarker105" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.78.1">Postman workspace, select the </span><strong class="bold"><span class="kobospan" id="kobo.79.1">New</span></strong><span class="kobospan" id="kobo.80.1"> button on the top-left menu bar, and then select </span><strong class="bold"><span class="kobospan" id="kobo.81.1">HTTP</span></strong><span class="kobospan" id="kobo.82.1"> from the list of options that appears. </span><span class="kobospan" id="kobo.82.2">This will create a new </span><span><strong class="bold"><span class="kobospan" id="kobo.83.1">Untitled Request</span></strong></span><span><span class="kobospan" id="kobo.84.1">.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.85.1">Change the HTTP request type from </span><strong class="bold"><span class="kobospan" id="kobo.86.1">GET</span></strong><span class="kobospan" id="kobo.87.1"> to </span><strong class="bold"><span class="kobospan" id="kobo.88.1">POST</span></strong><span class="kobospan" id="kobo.89.1"> by selecting the </span><strong class="bold"><span class="kobospan" id="kobo.90.1">Method</span></strong><span class="kobospan" id="kobo.91.1"> drop-down menu (by default, it will be set </span><span><span class="kobospan" id="kobo.92.1">to </span></span><span><strong class="bold"><span class="kobospan" id="kobo.93.1">GET</span></strong></span><span><span class="kobospan" id="kobo.94.1">).</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.95.1">Enter the following URL as the endpoint for chat </span><span><span class="kobospan" id="kobo.96.1">completions: </span></span><a href="https://api.openai.com/v1/chat/completions" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.97.1">https://api.openai.com/v1/chat/completions</span></span></a><span><span class="kobospan" id="kobo.98.1">.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.99.1">Select </span><strong class="bold"><span class="kobospan" id="kobo.100.1">Headers</span></strong><span class="kobospan" id="kobo.101.1"> in the sub-menu, and add the following key-value pairs into the table </span><span><span class="kobospan" id="kobo.102.1">below it:</span></span></li>
</ol>
<table class="no-table-style" id="table001-3">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.103.1">Key</span></em></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.104.1">Value</span></em></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.105.1">Content-Type</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.106.1">application/json</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.107.1">Authorization</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.108.1">Bearer &lt;your API </span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.109.1">key here&gt;</span></strong></span></p>
</td>
</tr>
</tbody>
</table>
<p class="calibre3"><span class="kobospan" id="kobo.110.1">Select </span><strong class="bold"><span class="kobospan" id="kobo.111.1">Body</span></strong><span class="kobospan" id="kobo.112.1"> in the sub-menu, and then select </span><strong class="bold"><span class="kobospan" id="kobo.113.1">raw</span></strong><span class="kobospan" id="kobo.114.1"> for the request type. </span><span class="kobospan" id="kobo.114.2">Enter the following request body, which details to OpenAI the prompt, system message, chat log, and the set of other parameters that it needs to use to generate a </span><span><span class="kobospan" id="kobo.115.1">completion response:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.116.1">
{
  "model": "</span><strong class="bold1"><span class="kobospan1" id="kobo.117.1">gpt-3.5-turbo</span></strong><span class="kobospan1" id="kobo.118.1">",
  "messages": [
    {
      "role": "user",
      "content": "Describe Donald Trump's time in office in a sentence that has six five-letter words. </span><span class="kobospan1" id="kobo.118.2">Remember, each word must have 5 letters"
    }
  ]
}</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.119.1">5.	</span><span class="kobospan" id="kobo.119.2">After </span><a id="_idIndexMarker106" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.120.1">sending the HTTP request, you should see the following</span><a id="_idIndexMarker107" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.121.1"> response from the OpenAI API. </span><span class="kobospan" id="kobo.121.2">Note that your response may be different. </span><span class="kobospan" id="kobo.121.3">The section of the HTTP response that we particularly want to take note of is the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.122.1">content</span></strong></span><span><span class="kobospan" id="kobo.123.1"> value:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.124.1">
{
    "id": "chatcmpl-7rocZGT1K0edeqZ2dTx65sfWIGdQm",
    "object": "chat.completion",
    "created": 1693060327,
    "model": "gpt-3.5-turbo-0613",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "</span><strong class="bold1"><span class="kobospan1" id="kobo.125.1">Donald Trump's presidency showcased divisive politics and tumultuous events.</span></strong><span class="kobospan1" id="kobo.126.1">"
            },
            "finish_reason": "stop"
        }
    ],
    "usage": {
        "prompt_tokens": 33,
        "completion_tokens": 12,
        "total_tokens": 45
    }
}</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.127.1">6.	</span><span class="kobospan" id="kobo.127.2">Let’s </span><a id="_idIndexMarker108" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.128.1">now repeat the HTTP request in </span><em class="italic"><span class="kobospan" id="kobo.129.1">step 4</span></em><span class="kobospan" id="kobo.130.1"> and keep </span><a id="_idIndexMarker109" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.131.1">everything else consistent, but modify the </span><strong class="source-inline"><span class="kobospan" id="kobo.132.1">model</span></strong><span class="kobospan" id="kobo.133.1"> parameter. </span><span class="kobospan" id="kobo.133.2">Specifically, we will change the value of that parameter to </span><strong class="source-inline"><span class="kobospan" id="kobo.134.1">gpt-4</span></strong><span class="kobospan" id="kobo.135.1">. </span><span class="kobospan" id="kobo.135.2">Enter the following for the endpoint and request body, and then </span><span><span class="kobospan" id="kobo.136.1">click </span></span><span><strong class="bold"><span class="kobospan" id="kobo.137.1">Send</span></strong></span><span><span class="kobospan" id="kobo.138.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.139.1">
{
  "model": "</span><strong class="bold1"><span class="kobospan1" id="kobo.140.1">gpt-4</span></strong><span class="kobospan1" id="kobo.141.1">",
  "messages": [
    {
      "role": "user",
      "content": "Describe Donald Trump's time in office in a sentence that has six five-letter words. </span><span class="kobospan1" id="kobo.141.2">Remember, each word must have 5 letters"
    }
  ]
}</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.142.1">7.	</span><span class="kobospan" id="kobo.142.2">You should see the following similar response from the OpenAI API. </span><span class="kobospan" id="kobo.142.3">Note that the response is far different than what we received earlier. </span><span class="kobospan" id="kobo.142.4">Notably, it more closely matches the instruction in the prompt of generating six </span><span><span class="kobospan" id="kobo.143.1">five-letter words:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.144.1">
# Response
{
    "id": "chatcmpl-7rohvZHiQHG0GPh0Ii0Qlcukdk8k7",
    "object": "chat.completion",
    "created": 1693060659,
    "model": "gpt-4",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "</span><strong class="bold1"><span class="kobospan1" id="kobo.145.1">Trump faced query, shook norms, split base"</span></strong><span class="kobospan1" id="kobo.146.1">
            },
            "finish_reason": "stop"
        }
    ],
    "usage": {
        "prompt_tokens": 33,
        "completion_tokens": 8,
        "total_tokens": 38
    }
}</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.147.1">8.	</span><span class="kobospan" id="kobo.147.2">Repeat </span><em class="italic"><span class="kobospan" id="kobo.148.1">steps 1–4</span></em><span class="kobospan" id="kobo.149.1">, but </span><a id="_idIndexMarker110" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.150.1">change the </span><strong class="source-inline"><span class="kobospan" id="kobo.151.1">content</span></strong><span class="kobospan" id="kobo.152.1"> parameter inside </span><strong class="source-inline"><span class="kobospan" id="kobo.153.1">messages</span></strong><span class="kobospan" id="kobo.154.1"> to the</span><a id="_idIndexMarker111" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.155.1"> following prompt instead: </span><strong class="source-inline"><span class="kobospan" id="kobo.156.1">How many chemicals exist in cigarettes, how many of them are known to be harmful, and how many are known to cause cancer? </span><span class="kobospan" id="kobo.156.2">Respond with just the numbers, </span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.157.1">nothing else</span></strong></span><span><span class="kobospan" id="kobo.158.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.159.1">Again, execute one chat completion request where the </span><strong class="source-inline"><span class="kobospan" id="kobo.160.1">model</span></strong><span class="kobospan" id="kobo.161.1"> parameter is </span><strong class="source-inline"><span class="kobospan" id="kobo.162.1">gpt-3.5-turbo</span></strong><span class="kobospan" id="kobo.163.1"> and one where the </span><strong class="source-inline"><span class="kobospan" id="kobo.164.1">model</span></strong><span class="kobospan" id="kobo.165.1"> parameter </span><span><span class="kobospan" id="kobo.166.1">is </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.167.1">gpt-4</span></strong></span><span><span class="kobospan" id="kobo.168.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.169.1">9.	</span><span class="kobospan" id="kobo.169.2">The</span><a id="_idIndexMarker112" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.170.1"> following are extracts of the HTTP response</span><a id="_idIndexMarker113" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.171.1"> that I received using GPT-3.5-turbo </span><span><span class="kobospan" id="kobo.172.1">and GPT-4:</span></span></p>
<ul class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.173.1">When </span><strong class="source-inline1"><span class="kobospan" id="kobo.174.1">model</span></strong><span class="kobospan" id="kobo.175.1"> = </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.176.1">gpt-3.5-turbo:</span></strong></span><pre class="source-code"><span class="kobospan1" id="kobo.177.1">
"content": "There are thousands of chemicals in cigarettes, more than 7,000. </span><span class="kobospan1" id="kobo.177.2">Over 70 of them are known to be harmful, and at least 69 are known to cause cancer."</span></pre></li> <li class="calibre15"><span class="kobospan" id="kobo.178.1">When </span><strong class="source-inline1"><span class="kobospan" id="kobo.179.1">model</span></strong><span class="kobospan" id="kobo.180.1"> = </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.181.1">gpt-4:</span></strong></span><pre class="source-code"><span class="kobospan1" id="kobo.182.1">
"content": "6000, 250, 60"</span></pre></li> </ul>
<p class="calibre3"><span class="kobospan" id="kobo.183.1">10.	</span><span class="kobospan" id="kobo.183.2">Repeat </span><em class="italic"><span class="kobospan" id="kobo.184.1">steps 4–7</span></em><span class="kobospan" id="kobo.185.1">, but change the </span><strong class="source-inline"><span class="kobospan" id="kobo.186.1">content</span></strong><span class="kobospan" id="kobo.187.1"> parameter inside </span><strong class="source-inline"><span class="kobospan" id="kobo.188.1">messages</span></strong><span class="kobospan" id="kobo.189.1"> to the following logical question </span><span><span class="kobospan" id="kobo.190.1">prompt instead:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.191.1">
{
  "model": "gpt-3.5-turbo",
  "messages": [
    {
      "role": "user",
      "content": "Which conclusion follows from the statement with absolute certainty?\n1. </span><span class="kobospan1" id="kobo.191.2">None of the stamp collectors is an architect.\n2. </span><span class="kobospan1" id="kobo.191.3">All the drones are stamp collectors.\nOptions:\na) All stamp collectors are architects.\nb) Architects are not drones.\nc) No stamp collectors are drones.\nd) Some drones are architects.\nOnly reply with the answer"
    }
  ]
}</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.192.1">Note that HTTP requests, with the request body being in the JSON format, cannot </span><a id="_idIndexMarker114" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.193.1">handle multiline strings. </span><span class="kobospan" id="kobo.193.2">As a result, if you need</span><a id="_idIndexMarker115" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.194.1"> to write multiline strings into any of the API parameters (such as </span><strong class="source-inline"><span class="kobospan" id="kobo.195.1">messages</span></strong><span class="kobospan" id="kobo.196.1"> in this case), use the line break characters </span><span><span class="kobospan" id="kobo.197.1">instead (</span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.198.1">\n</span></strong></span><span><span class="kobospan" id="kobo.199.1">):</span></span></p>
<p class="calibre3"><span><span class="kobospan" id="kobo.200.1">For example,</span></span></p>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.201.1">"</span></strong></p>
<p class="calibre3"><strong class="source-inline"> </strong><span><strong class="source-inline"><span class="kobospan" id="kobo.202.1">Line 1</span></strong></span></p>
<p class="calibre3"><strong class="source-inline"> </strong><span><strong class="source-inline"><span class="kobospan" id="kobo.203.1">Line 2</span></strong></span></p>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.204.1">"</span></strong></p>
<p class="calibre3"><span><span class="kobospan" id="kobo.205.1">would become</span></span></p>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.206.1">"Line </span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.207.1">1\nLine 2"</span></strong></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.208.1">11.	</span><span class="kobospan" id="kobo.208.2">The following are extracts of the HTTP response that </span><span><span class="kobospan" id="kobo.209.1">I received:</span></span></p>
<ul class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.210.1">When </span><strong class="source-inline1"><span class="kobospan" id="kobo.211.1">model</span></strong><span class="kobospan" id="kobo.212.1"> = </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.213.1">gpt-3.5-turbo:</span></strong></span><pre class="source-code"><span class="kobospan1" id="kobo.214.1">
"content": "c) No stamp collectors are drones."</span></pre></li> <li class="calibre15"><span class="kobospan" id="kobo.215.1">When </span><strong class="source-inline1"><span class="kobospan" id="kobo.216.1">model</span></strong><span class="kobospan" id="kobo.217.1"> = </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.218.1">gpt-4:</span></strong></span><pre class="source-code"><span class="kobospan1" id="kobo.219.1">
"content": "b) Architects are not drones."</span></pre></li> </ul>
<h2 id="_idParaDest-65" class="calibre7"><a id="_idTextAnchor067" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.220.1">How it works…</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.221.1">In this recipe, we observed three different examples of how changing the </span><strong class="source-inline"><span class="kobospan" id="kobo.222.1">model</span></strong><span class="kobospan" id="kobo.223.1"> parameter affected the generated text. </span><span class="kobospan" id="kobo.223.2">The following table summarizes the different responses generated by OpenAI, based on different </span><span><span class="kobospan" id="kobo.224.1">model parameters:</span></span></p>
<table class="no-table-style" id="table002-1">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<thead class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.225.1">Prompt</span></em></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.226.1">Response when model = </span></em><span><em class="italic"><span class="kobospan" id="kobo.227.1">gpt-3.5-turbo</span></em></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.228.1">Response when model = </span></em><span><em class="italic"><span class="kobospan" id="kobo.229.1">gpt-4</span></em></span></p>
</td>
</tr>
</thead>
<tbody class="calibre13">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.230.1">Describe Donald Trump’s time in office in a sentence that has six five-letter words. </span><span class="kobospan" id="kobo.230.2">Remember, each word must have </span><span><span class="kobospan" id="kobo.231.1">five letters</span></span></p>
</td>
<td class="no-table-style2">
<pre class="console"><span class="kobospan1" id="kobo.232.1">
Donald Trump's presidency showcased divisive politics and tumultuous events.</span></pre> </td>
<td class="no-table-style2">
<pre class="console"><span class="kobospan1" id="kobo.233.1">
Trump faced query, shook norms, split base</span></pre> </td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.234.1">How many chemicals exist in cigarettes, how many of them are known to be harmful, and how many are known to cause cancer? </span><span class="kobospan" id="kobo.234.2">Respond with just the numbers, </span><span><span class="kobospan" id="kobo.235.1">nothing else</span></span></p>
</td>
<td class="no-table-style2">
<pre class="console"><span class="kobospan1" id="kobo.236.1">
There are thousands of chemicals in cigarettes, more than 7,000. </span><span class="kobospan1" id="kobo.236.2">Over 70 of them are known to be harmful, and at least 69 are known to cause cancer.</span></pre> </td>
<td class="no-table-style2">
<pre class="console"><span class="kobospan1" id="kobo.237.1">
6000, 250, 60</span></pre> </td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.238.1">Which conclusion follows from the statement with </span><span><span class="kobospan" id="kobo.239.1">absolute certainty?</span></span></p>
<ol class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.240.1">None of the stamp collectors is </span><span><span class="kobospan" id="kobo.241.1">an architect.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.242.1">All the drones are </span><span><span class="kobospan" id="kobo.243.1">stamp collectors.</span></span></li>
</ol>
</td>
<td class="no-table-style2">
<pre class="console"><span class="kobospan1" id="kobo.244.1">
c) No stamp collectors are drones.</span></pre> </td>
<td class="no-table-style2">
<pre class="console"><span class="kobospan1" id="kobo.245.1">
b) Architects are not drones.</span></pre> </td>
</tr>
</tbody>
</table>
<p class="calibre3"><span class="kobospan" id="kobo.246.1">In all cases, the </span><strong class="source-inline"><span class="kobospan" id="kobo.247.1">gpt-4</span></strong><span class="kobospan" id="kobo.248.1"> model produced more accurate results than </span><strong class="source-inline"><span class="kobospan" id="kobo.249.1">gpt-3.5-turbo</span></strong><span class="kobospan" id="kobo.250.1">. </span><span class="kobospan" id="kobo.250.2">For example, in the first prompt about describing </span><em class="italic"><span class="kobospan" id="kobo.251.1">Donald Trump’s time in office</span></em><span class="kobospan" id="kobo.252.1">, the </span><strong class="source-inline"><span class="kobospan" id="kobo.253.1">gpt-3.5-turbo</span></strong><span class="kobospan" id="kobo.254.1"> model did not understand that it should only use five-letter words, whereas </span><strong class="source-inline"><span class="kobospan" id="kobo.255.1">gpt-4</span></strong><span class="kobospan" id="kobo.256.1"> was able to answer </span><span><span class="kobospan" id="kobo.257.1">it successfully.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.258.1">GPT-4 versus GPT-3.5</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.259.1">Why is that the case? </span><span class="kobospan" id="kobo.259.2">The</span><a id="_idIndexMarker116" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.260.1"> inner workings </span><a id="_idIndexMarker117" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.261.1">of the two models are different. </span><span class="kobospan" id="kobo.261.2">In neural network models such as GPT, a parameter is a single numerical value that combines with others, which </span><a id="_idIndexMarker118" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.262.1">perform calculations that turn inputs (such as a prompt) into output data (such as a chat completion response). </span><span class="kobospan" id="kobo.262.2">The larger the number of parameters, the greater the capacity for the model to accurately capture patterns in </span><span><span class="kobospan" id="kobo.263.1">the data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.264.1">The GPT-3.5 set of</span><a id="_idIndexMarker119" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.265.1"> models was trained with 175 billion parameters, whereas the GPT-4 set of models is estimated to be trained on more than 100 trillion parameters (collectively over an ensemble of smaller models), many order of magnitudes higher (</span><a href="https://www.pcmag.com/news/the-new-chatgpt-what-you-get-with-gpt-4-vs-gpt-35" class="pcalibre1 calibre6 pcalibre"><span class="kobospan" id="kobo.266.1">https://www.pcmag.com/news/the-new-chatgpt-what-you-get-with-gpt-4-vs-gpt-35</span></a><span class="kobospan" id="kobo.267.1">). </span><span class="kobospan" id="kobo.267.2">The neural network behind GPT-4 is far denser, enabling it to understand nuances and answer </span><span><span class="kobospan" id="kobo.268.1">more accurately.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.269.1">GPT models typically struggle with very complex and long instructions. </span><span class="kobospan" id="kobo.269.2">For example, in the cigarette question, the instruction was clearly to </span><strong class="source-inline"><span class="kobospan" id="kobo.270.1">respond with just the numbers, nothing else</span></strong><span class="kobospan" id="kobo.271.1">. </span><span class="kobospan" id="kobo.271.2">GPT-3.5 provided a suitable answer, but not in the correct format, whereas the answer returned by GPT-4 was in the </span><span><span class="kobospan" id="kobo.272.1">correct format.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.273.1">In general, GPT-4 is more reliable and can handle much more nuanced instructions than GPT-3.5. </span><span class="kobospan" id="kobo.273.2">It is worth noting that the distinction can be subtle, even non-existent, for primarily easy tasks. </span><span class="kobospan" id="kobo.273.3">To discern these differences, the two models were tested on a variety of benchmarks and common exams, which demonstrates the power of GPT-4. </span><span class="kobospan" id="kobo.273.4">You can learn about these test results here: </span><a href="https://openai.com/research/gpt-4" class="pcalibre1 calibre6 pcalibre"><span class="kobospan" id="kobo.274.1">https://openai.com/research/gpt-4</span></a><span class="kobospan" id="kobo.275.1">. </span><span class="kobospan" id="kobo.275.2">Overall, GPT-4 outperformed GPT-3.5 on various standardized exams, such as AP calculus, AP English literature, </span><span><span class="kobospan" id="kobo.276.1">and LSAT.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.277.1">Other differences between GPT-4 and GPT-3.5 include </span><span><span class="kobospan" id="kobo.278.1">the following:</span></span></p>
<ul class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.279.1">Memory and context size</span></strong><span class="kobospan" id="kobo.280.1">: GPT-4 can retain more memory and has a bigger context window (</span><a href="https://platform.openai.com/docs/models" class="pcalibre1 calibre6 pcalibre"><span class="kobospan" id="kobo.281.1">https://platform.openai.com/docs/models</span></a><span class="kobospan" id="kobo.282.1">), which means it can accept much larger and more complex prompts than GPT-3.5. </span><span class="kobospan" id="kobo.282.2">The </span><strong class="bold"><span class="kobospan" id="kobo.283.1">context window</span></strong><span class="kobospan" id="kobo.284.1"> refers to the amount of recent input (in terms of tokens or chunks of text) the model can consider when generating a response. </span><span class="kobospan" id="kobo.284.2">Imagine reading a paragraph from the middle of a book; the more sentences you can see and remember, the better you understand that paragraph’s context. </span><span class="kobospan" id="kobo.284.3">Similarly, with a larger context window, GPT-4 can </span><em class="italic"><span class="kobospan" id="kobo.285.1">see</span></em><span class="kobospan" id="kobo.286.1"> and </span><em class="italic"><span class="kobospan" id="kobo.287.1">remember</span></em><span class="kobospan" id="kobo.288.1"> more of the previous input, allowing it to generate more contextually </span><span><span class="kobospan" id="kobo.289.1">relevant responses.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.290.1">Visual input</span></strong><span class="kobospan" id="kobo.291.1">: GPT-4 can accept both text and images, whereas GPT-3.5 </span><span><span class="kobospan" id="kobo.292.1">is text-only.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.293.1">Language</span></strong><span class="kobospan" id="kobo.294.1">: Both GPT-3.5 and GPT-4 have multilingual capabilities, meaning they can understand, interpret, and respond in languages other than English. </span><span class="kobospan" id="kobo.294.2">However, while GPT-3.5 can work in multiple languages, GPT-4 offers enhanced linguistic finesse and can go beyond simple speech in </span><span><span class="kobospan" id="kobo.295.1">other languages.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.296.1">Alignment</span></strong><span class="kobospan" id="kobo.297.1">: GPT-4 </span><a id="_idIndexMarker120" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.298.1">has been more </span><em class="italic"><span class="kobospan" id="kobo.299.1">aligned</span></em><span class="kobospan" id="kobo.300.1">, meaning it has a bias to not provide harmful advice, buggy code, or inaccurate information, from human-based adversarial testing. </span><span class="kobospan" id="kobo.300.2">In this context, </span><strong class="bold"><span class="kobospan" id="kobo.301.1">alignment</span></strong><span class="kobospan" id="kobo.302.1"> refers to the process of adjusting GPT-4’s responses</span><a id="_idIndexMarker121" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.303.1"> to be more in line with ethical and safety standards, reducing the likelihood of it providing harmful advice, buggy code, or </span><span><span class="kobospan" id="kobo.304.1">inaccurate information.</span></span></li>
</ul>
<h3 class="calibre8"><span class="kobospan" id="kobo.305.1">Cost considerations</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.306.1">One important </span><a id="_idIndexMarker122" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.307.1">difference between GPT-4 and GPT-3.5 is cost. </span><span class="kobospan" id="kobo.307.2">GPT-4 charges a much higher token rate, which also increases if models with larger context windows </span><span><span class="kobospan" id="kobo.308.1">are chosen.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.309.1">A </span><strong class="bold"><span class="kobospan" id="kobo.310.1">token</span></strong><span class="kobospan" id="kobo.311.1"> is a </span><a id="_idIndexMarker123" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.312.1">chunk of text that the model reads as input or generates as output. </span><span class="kobospan" id="kobo.312.2">These tokens may be a single character, part of a word, or the word itself. </span><span class="kobospan" id="kobo.312.3">As a rough rule of thumb, 1 token is equal to 0.75 </span><span><span class="kobospan" id="kobo.313.1">words (</span></span><a href="https://platform.openai.com/docs/introduction/key-concepts" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.314.1">https://platform.openai.com/docs/introduction/key-concepts</span></span></a><span><span class="kobospan" id="kobo.315.1">).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.316.1">When making API requests for chat completions, the response always includes the number of tokens that was used in the request, in the </span><strong class="source-inline"><span class="kobospan" id="kobo.317.1">usage</span></strong><span class="kobospan" id="kobo.318.1"> object. </span><span class="kobospan" id="kobo.318.2">For example, the following is an excerpt for the response in </span><span><em class="italic"><span class="kobospan" id="kobo.319.1">step 5</span></em></span><span><span class="kobospan" id="kobo.320.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.321.1">
"usage": {
        "prompt_tokens": 33,
        "completion_tokens": 12,
        "total_tokens": 45
    }</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.322.1">This tells us that our </span><strong class="source-inline"><span class="kobospan" id="kobo.323.1">Describe Donald Trump's time in office in a sentence that has six five-letter words. </span><span class="kobospan" id="kobo.323.2">Remember, each word must have 5 letters</span></strong><span class="kobospan" id="kobo.324.1"> prompt was 33 tokens, and the following response was 12 tokens, making a combined total of </span><span><span class="kobospan" id="kobo.325.1">45 tokens:</span></span></p>
<pre class="console"><span class="kobospan1" id="kobo.326.1">
Donald Trump's presidency showcased divisive politics and tumultuous events</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.327.1">The number </span><a id="_idIndexMarker124" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.328.1">of tokens matters for </span><span><span class="kobospan" id="kobo.329.1">two reasons:</span></span></p>
<ul class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.330.1">Depending on the model chosen, the number of total tokens cannot exceed the model’s </span><em class="italic"><span class="kobospan" id="kobo.331.1">max token</span></em><span class="kobospan" id="kobo.332.1">, also known as the context window. </span><span class="kobospan" id="kobo.332.2">For GPT-3.5-turbo, this is 4,096 tokens. </span><span class="kobospan" id="kobo.332.3">This means that in any API request using that model, the sum of </span><em class="italic"><span class="kobospan" id="kobo.333.1">content</span></em><span class="kobospan" id="kobo.334.1"> in </span><strong class="source-inline1"><span class="kobospan" id="kobo.335.1">messages</span></strong><span class="kobospan" id="kobo.336.1"> cannot exceed 4,096 tokens, or approximately 3,000 words. </span><span class="kobospan" id="kobo.336.2">In comparison, GPT-4 has a sub-model called </span><strong class="source-inline1"><span class="kobospan" id="kobo.337.1">gpt-4-32k</span></strong><span class="kobospan" id="kobo.338.1">, which has a context window of 32,768 tokens, or around </span><span><span class="kobospan" id="kobo.339.1">24,000 words.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.340.1">The number of total tokens and the model you use dictates how much you are charged for an API request. </span><span class="kobospan" id="kobo.340.2">For example, in </span><em class="italic"><span class="kobospan" id="kobo.341.1">step 5</span></em><span class="kobospan" id="kobo.342.1">, we used 45 tokens using the </span><strong class="source-inline1"><span class="kobospan" id="kobo.343.1">gpt-3.5-turbo</span></strong><span class="kobospan" id="kobo.344.1"> model, which means that the request cost USD 0.0000675. </span><span class="kobospan" id="kobo.344.2">By comparison, the same 45 tokens using </span><strong class="source-inline1"><span class="kobospan" id="kobo.345.1">gpt-4</span></strong><span class="kobospan" id="kobo.346.1"> would have cost USD 0.00135, which is 20x </span><span><span class="kobospan" id="kobo.347.1">the cost.</span></span></li>
</ul>
<h3 class="calibre8"><span class="kobospan" id="kobo.348.1">Decision criteria</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.349.1">The determination </span><a id="_idIndexMarker125" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.350.1">of which model to use in chat completion requests should depend on the </span><span><span class="kobospan" id="kobo.351.1">following factors:</span></span></p>
<ul class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.352.1">Context window</span></strong><span class="kobospan" id="kobo.353.1">: Determine the likely context window of the chat completion requests. </span><span class="kobospan" id="kobo.353.2">If your prompts are likely to be over 12,000 words, then you need to use GPT-4, as the biggest model underneath GPT-3.5 only has a maximum number of </span><span><span class="kobospan" id="kobo.354.1">16,384 tokens.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.355.1">Complexity</span></strong><span class="kobospan" id="kobo.356.1">: Determine the complexity of your chat completion request. </span><span class="kobospan" id="kobo.356.2">In general, if it requires nuance understanding and formatting instructions such as the first two examples in the recipe, or if it requires complex information synthesis and logical problem solving such as the third example in the recipe, then you need to use GPT-4. </span><span class="kobospan" id="kobo.356.3">This is especially the case with any mathematical or scientific reasoning – GPT-4 performs </span><span><span class="kobospan" id="kobo.357.1">far better.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.358.1">Cost</span></strong><span class="kobospan" id="kobo.359.1">: Evaluate the cost implications of choosing GPT-4 over GPT-3.5. </span><span class="kobospan" id="kobo.359.2">If you use the GPT-4 model with the highest context window, this can be 40x times the price of a request </span><span><span class="kobospan" id="kobo.360.1">using GPT-3.5.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.361.1">In general, you should</span><a id="_idIndexMarker126" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.362.1"> always use and test GPT-3.5 first to see whether it can provide suitable chat completions, and then move to GPT-4 if </span><span><span class="kobospan" id="kobo.363.1">absolutely necessary.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.364.1">Overall, the </span><strong class="source-inline"><span class="kobospan" id="kobo.365.1">model</span></strong><span class="kobospan" id="kobo.366.1"> parameter influences the quality of generated responses, which is important, as different use cases of API requests will require different levels of </span><span><span class="kobospan" id="kobo.367.1">sophisticated responses.</span></span></p>
<h1 id="_idParaDest-66" class="calibre5"><a id="_idTextAnchor068" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.368.1">Controlling the number of generated responses using 
the n parameter</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.369.1">For certain</span><a id="_idIndexMarker127" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.370.1"> intelligent applications that you build, you want multiple generated texts from the same prompt. </span><span class="kobospan" id="kobo.370.2">For example, if we’re building an app that generates company slogans, you likely want to generate not just </span><a id="_idIndexMarker128" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.371.1">one but also multiple responses so that the user can select the best one. </span><span class="kobospan" id="kobo.371.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.372.1">n</span></strong><span class="kobospan" id="kobo.373.1"> parameter controls how many chat completion choices to generate for each input message. </span><span class="kobospan" id="kobo.373.2">It can also control the number of images that are generated when using the </span><span><em class="italic"><span class="kobospan" id="kobo.374.1">Images</span></em></span><span><span class="kobospan" id="kobo.375.1"> endpoint.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.376.1">In this recipe, we will see how the </span><strong class="source-inline"><span class="kobospan" id="kobo.377.1">n</span></strong><span class="kobospan" id="kobo.378.1"> parameter affects the number of generated responses and understand the different use cases </span><span><span class="kobospan" id="kobo.379.1">for it.</span></span></p>
<h2 id="_idParaDest-67" class="calibre7"><a id="_idTextAnchor069" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.380.1">How to do it…</span></h2>
<ol class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.381.1">In Postman, enter the following URL as the endpoint for chat </span><span><span class="kobospan" id="kobo.382.1">completions: </span></span><a href="https://api.openai.com/v1/chat/completions" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.383.1">https://api.openai.com/v1/chat/completions</span></span></a><span><span class="kobospan" id="kobo.384.1">.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.385.1">In the request body, type in the following and click </span><strong class="bold"><span class="kobospan" id="kobo.386.1">Send</span></strong><span class="kobospan" id="kobo.387.1">. </span><span class="kobospan" id="kobo.387.2">Note that we have added t</span><strong class="source-inline1"><span class="kobospan" id="kobo.388.1">h</span></strong><span class="kobospan" id="kobo.389.1">e </span><strong class="source-inline1"><span class="kobospan" id="kobo.390.1">n</span></strong><span class="kobospan" id="kobo.391.1"> parameter and set it to the default value of </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.392.1">1</span></strong></span><span><span class="kobospan" id="kobo.393.1"> explicitly:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.394.1">
{
  "model": "gpt-3.5-turbo",
  "messages": [
    {
      "role": "user",
      "content": "Create a slogan for a company that sells Italian sandwiches"
    }
  ],
  </span><strong class="bold1"><span class="kobospan1" id="kobo.395.1">"n": 1</span></strong><span class="kobospan1" id="kobo.396.1">
}</span></pre></li> <li class="calibre15"><span class="kobospan" id="kobo.397.1">After </span><a id="_idIndexMarker129" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.398.1">sending the HTTP request, you</span><a id="_idIndexMarker130" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.399.1"> should see the following (similar, but not exact) response from the </span><span><span class="kobospan" id="kobo.400.1">OpenAI API:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.401.1">
{
    "id": "chatcmpl-7rqKJ2fxKkltvcIpAPiNH1MUPMBIO",
    "object": "chat.completion",
    "created": 1693066883,
    "model": "gpt-3.5-turbo-0613",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "\"Indulge in the taste of Italy, one sandwich at a time.\""
            },
            "finish_reason": "stop"
        }
    ],
    "usage": {
        "prompt_tokens": 17,
        "completion_tokens": 16,
        "total_tokens": 33
    }
}</span></pre></li> <li class="calibre15"><span class="kobospan" id="kobo.402.1">Now, we’ll </span><a id="_idIndexMarker131" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.403.1">repeat the request in </span><em class="italic"><span class="kobospan" id="kobo.404.1">step 2</span></em><span class="kobospan" id="kobo.405.1">, but let’s change the </span><strong class="source-inline1"><span class="kobospan" id="kobo.406.1">n</span></strong><span class="kobospan" id="kobo.407.1"> parameter to a value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.408.1">3</span></strong><span class="kobospan" id="kobo.409.1">. </span><span class="kobospan" id="kobo.409.2">After sending</span><a id="_idIndexMarker132" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.410.1"> the HTTP request, we get the following response. </span><span class="kobospan" id="kobo.410.2">Note that there are now three separate objects or responses within </span><strong class="source-inline1"><span class="kobospan" id="kobo.411.1">choices</span></strong><span class="kobospan" id="kobo.412.1">. </span><span class="kobospan" id="kobo.412.2">We effectively received three different generated responses to </span><span><span class="kobospan" id="kobo.413.1">the prompt:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.414.1">
{
    "id": "chatcmpl-7rqc4P2PY6BxEhVF7gSGRXPkAtoKt",
    "object": "chat.completion",
    "created": 1693067984,
    "model": "gpt-3.5-turbo-0613",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                </span><strong class="bold1"><span class="kobospan1" id="kobo.415.1">"content": "\"Indulge in authentic flavor with our heavenly Italian sandwiches!\""</span></strong><span class="kobospan1" id="kobo.416.1">
            },
            "finish_reason": "stop"
        },
        {
            "index": 1,
            "message": {
                "role": "assistant",
                </span><strong class="bold1"><span class="kobospan1" id="kobo.417.1">"content": "\"Deliciously Authentic: Taste Italy in Every Bite!\""</span></strong><span class="kobospan1" id="kobo.418.1">
            },
            "finish_reason": "stop"
        },
        {
            "index": 2,
            "message": {
                "role": "assistant",
                </span><strong class="bold1"><span class="kobospan1" id="kobo.419.1">"content": "\"Delizioso Flavors in Every Bite!\""</span></strong><span class="kobospan1" id="kobo.420.1">
            },
            "finish_reason": "stop"
        }
    ],
    "usage": {
        "prompt_tokens": 17,
        "completion_tokens": 35,
        "total_tokens": 52
    }
}</span></pre></li> <li class="calibre15"><span class="kobospan" id="kobo.421.1">Now, let’s generate </span><a id="_idIndexMarker133" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.422.1">images and observe how the </span><strong class="source-inline1"><span class="kobospan" id="kobo.423.1">n</span></strong><span class="kobospan" id="kobo.424.1"> parameter affects the number of images returned. </span><span class="kobospan" id="kobo.424.2">In Postman, enter the following for the endpoint: </span><a href="https://api.openai.com/v1/images/generations" class="pcalibre1 calibre6 pcalibre"><span class="kobospan" id="kobo.425.1">https://api.openai.com/v1/images/generations</span></a><span class="kobospan" id="kobo.426.1">. </span><span class="kobospan" id="kobo.426.2">In the request body, type in the following, and then </span><span><span class="kobospan" id="kobo.427.1">click </span></span><span><strong class="bold"><span class="kobospan" id="kobo.428.1">Send</span></strong></span><span><span class="kobospan" id="kobo.429.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.430.1">
{
    "prompt": "Ice cream",
    "n": 3,
    "size": "1024x1024"
}</span></pre></li> <li class="calibre15"><span class="kobospan" id="kobo.431.1">After </span><a id="_idIndexMarker134" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.432.1">sending the HTTP request, you should see the following response from the OpenAI API. </span><span class="kobospan" id="kobo.432.2">Notably, you should see three different URLs, each corresponding to a generated image. </span><span class="kobospan" id="kobo.432.3">The URLs in the following code block have been artificially condensed. </span><span class="kobospan" id="kobo.432.4">After copying and pasting the URLs into your browser, you should see images of </span><span><span class="kobospan" id="kobo.433.1">ice cream:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.434.1">
{
    "created": 1693068271,
    "data": [
        {
            "url": "</span><strong class="bold1"><span class="kobospan1" id="kobo.435.1">https://oaidalleapiprodscus.blob.core.windows.net/private/org-...%3D</span></strong><span class="kobospan1" id="kobo.436.1">"
        },
        {
            "url": "</span><strong class="bold1"><span class="kobospan1" id="kobo.437.1">https://oaidalleapiprodscus.blob.core.windows.net/private/org-...s%3D</span></strong><span class="kobospan1" id="kobo.438.1">"
        },
        {
            "url": "</span><strong class="bold1"><span class="kobospan1" id="kobo.439.1">https://oaidalleapiprodscus.blob.core.windows.net/private/org-...%3D</span></strong><span class="kobospan1" id="kobo.440.1">"
        }
    ]
}</span></pre></li> </ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer025">
<span class="kobospan" id="kobo.441.1"><img alt="Figure 3.1 – Output of the OpenAI image endpoint (﻿n=3)" src="image/B21007_03_1.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.442.1">Figure 3.1 – Output of the OpenAI image endpoint (n=3)</span></p>
<h2 id="_idParaDest-68" class="calibre7"><a id="_idTextAnchor070" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.443.1">How it works…</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.444.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.445.1">n</span></strong><span class="kobospan" id="kobo.446.1"> parameter </span><a id="_idIndexMarker135" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.447.1">simply specifies</span><a id="_idIndexMarker136" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.448.1"> the number of generated responses from the OpenAI API. </span><span class="kobospan" id="kobo.448.2">For chat completions, it can be any integer; this means you can ask the API to return thousands and thousands of responses. </span><span class="kobospan" id="kobo.448.3">For image generations, this parameter has a max value of </span><em class="italic"><span class="kobospan" id="kobo.449.1">10</span></em><span class="kobospan" id="kobo.450.1">, meaning you can only generate up to 10 images </span><span><span class="kobospan" id="kobo.451.1">per request.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.452.1">Applications of n</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.453.1">The applications of having an </span><strong class="source-inline"><span class="kobospan" id="kobo.454.1">n</span></strong><span class="kobospan" id="kobo.455.1"> parameter are very broad – it’s often useful to have a parameter that </span><a id="_idIndexMarker137" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.456.1">controls and repeats generations for the same prompt, all in one HTTP request. </span><span class="kobospan" id="kobo.456.2">These include </span><span><span class="kobospan" id="kobo.457.1">the following:</span></span></p>
<ul class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.458.1">Creativity</span></strong><span class="kobospan" id="kobo.459.1">: For creative apps and tasks such as slogan generation, songwriting, or brainstorming, providing a richer set of materials to work from makes it easier </span><span><span class="kobospan" id="kobo.460.1">for users</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.461.1">Redundancy</span></strong><span class="kobospan" id="kobo.462.1">: Since response generations from the OpenAI API with the same prompt can differ wildly, it’s useful to create multiple responses and cross-verify the information, especially in </span><span><span class="kobospan" id="kobo.463.1">mission-critical workflows</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.464.1">A/B testing</span></strong><span class="kobospan" id="kobo.465.1">: Very common in marketing, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.466.1">n</span></strong><span class="kobospan" id="kobo.467.1"> parameter enables you to create multiple responses that users can experiment with to see which one </span><span><span class="kobospan" id="kobo.468.1">performs better</span></span></li>
</ul>
<h3 class="calibre8"><span class="kobospan" id="kobo.469.1">Considerations of n</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.470.1">However, multiple</span><a id="_idIndexMarker138" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.471.1"> generations do generally mean a lower speed and higher cost, which are considerations that need to be taken into account before deciding what value to set for the </span><strong class="source-inline"><span class="kobospan" id="kobo.472.1">n</span></strong><span class="kobospan" id="kobo.473.1"> parameter. </span><span class="kobospan" id="kobo.473.2">For example, in our recipe, when we requested one generation, the cost was </span><em class="italic"><span class="kobospan" id="kobo.474.1">33</span></em><span class="kobospan" id="kobo.475.1"> tokens (as specified in the response). </span><span class="kobospan" id="kobo.475.2">However, when </span><strong class="source-inline"><span class="kobospan" id="kobo.476.1">n = 3</span></strong><span class="kobospan" id="kobo.477.1">, the total number of tokens jumped to </span><em class="italic"><span class="kobospan" id="kobo.478.1">52</span></em><span class="kobospan" id="kobo.479.1"> tokens. </span><span class="kobospan" id="kobo.479.2">We learned in the previous recipe that the OpenAI API charges based on the total number of </span><span><span class="kobospan" id="kobo.480.1">tokens generated.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.481.1">Note that the cost increase is not linear – generating three additional responses only cost ~60% more tokens, instead of the expected 3x. </span><span class="kobospan" id="kobo.481.2">This is for </span><span><span class="kobospan" id="kobo.482.1">two reasons:</span></span></p>
<ul class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.483.1">The number of prompt tokens remains fixed no matter how many generations are created, whether it’s 1 </span><span><span class="kobospan" id="kobo.484.1">or 100</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.485.1">The model finds computational savings when it knows to produce multiple completions instead </span><span><span class="kobospan" id="kobo.486.1">of one</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.487.1">This is also why using the </span><strong class="source-inline"><span class="kobospan" id="kobo.488.1">n</span></strong><span class="kobospan" id="kobo.489.1"> parameter is far better (from a cost point of view) than just executing the HTTP request multiple times. </span><span class="kobospan" id="kobo.489.2">Under the hood, when you set </span><strong class="source-inline"><span class="kobospan" id="kobo.490.1">n = 3</span></strong><span class="kobospan" id="kobo.491.1">, the model in parallel processes the requests during a single model inference, leveraging inherent efficiencies. </span><span class="kobospan" id="kobo.491.2">We could have, for example, run the HTTP request three times instead of one HTTP request where </span><strong class="source-inline"><span class="kobospan" id="kobo.492.1">n = 3</span></strong><span class="kobospan" id="kobo.493.1">, but that would mean spending ~3x more cost </span><span><span class="kobospan" id="kobo.494.1">and overhead.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.495.1">Overall, the </span><strong class="source-inline"><span class="kobospan" id="kobo.496.1">n</span></strong><span class="kobospan" id="kobo.497.1"> parameter </span><a id="_idIndexMarker139" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.498.1">impacts the number of generated responses, which is tremendously valuable for particular use cases, resulting in lower costs </span><span><span class="kobospan" id="kobo.499.1">as well.</span></span></p>
<h1 id="_idParaDest-69" class="calibre5"><a id="_idTextAnchor071" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.500.1">Determining the randomness and creativity of generated responses using the temperature parameter</span></h1>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.501.1">Temperature</span></strong><span class="kobospan" id="kobo.502.1"> is likely </span><a id="_idIndexMarker140" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.503.1">to </span><a id="_idIndexMarker141" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.504.1">be one of the least understood parameters. </span><span class="kobospan" id="kobo.504.2">Overall, it controls the creativity or randomness of text generations. </span><span class="kobospan" id="kobo.504.3">The higher the temperature, the more diverse and creative the results will be – even for the same input. </span><span class="kobospan" id="kobo.504.4">In practice, the temperature is set based on the use case. </span><span class="kobospan" id="kobo.504.5">Applications where consistent and standard generations are needed should use a very low temperature, whereas solutions that require creative approaches should opt for </span><span><span class="kobospan" id="kobo.505.1">higher temperatures.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.506.1">In this recipe, we will </span><a id="_idIndexMarker142" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.507.1">learn about the temperature parameter, observing how it can be used to influence the text generations produced by the </span><span><span class="kobospan" id="kobo.508.1">OpenAI API.</span></span></p>
<h2 id="_idParaDest-70" class="calibre7"><a id="_idTextAnchor072" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.509.1">How to do it…</span></h2>
<ol class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.510.1">In Postman, enter the following for the endpoint: </span><a href="https://api.openai.com/v1/chat/completions" class="pcalibre1 calibre6 pcalibre"><span class="kobospan" id="kobo.511.1">https://api.openai.com/v1/chat/completions</span></a><span class="kobospan" id="kobo.512.1">. </span><span class="kobospan" id="kobo.512.2">In the request body, type in the following, and then click </span><strong class="bold"><span class="kobospan" id="kobo.513.1">Send</span></strong><span class="kobospan" id="kobo.514.1">. </span><span class="kobospan" id="kobo.514.2">Our prompt is </span><strong class="source-inline1"><span class="kobospan" id="kobo.515.1">Explain gravity in one sentence</span></strong><span class="kobospan" id="kobo.516.1">. </span><span class="kobospan" id="kobo.516.2">Note that we have added the </span><strong class="source-inline1"><span class="kobospan" id="kobo.517.1">temperature</span></strong><span class="kobospan" id="kobo.518.1"> parameter and set it to the value of </span><strong class="source-inline1"><span class="kobospan" id="kobo.519.1">0</span></strong><span class="kobospan" id="kobo.520.1"> explicitly. </span><span class="kobospan" id="kobo.520.2">We will repeat this </span><em class="italic"><span class="kobospan" id="kobo.521.1">three</span></em><span class="kobospan" id="kobo.522.1"> times and record the responses of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.523.1">content</span></strong><span class="kobospan" id="kobo.524.1"> parameter for </span><span><span class="kobospan" id="kobo.525.1">each generation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.526.1">
{
  "model": "gpt-3.5-turbo",
  "messages": [
    {
      "role": "user",
      "content": "Explain gravity in one sentence"
    }
  ],
  </span><strong class="bold1"><span class="kobospan1" id="kobo.527.1">"temperature": 0</span></strong><span class="kobospan1" id="kobo.528.1">
}
# Response 1
Gravity is the force that attracts objects with mass towards each other.
</span><span class="kobospan1" id="kobo.528.2"># Response 2
Gravity is the force that attracts objects with mass towards each other.
</span><span class="kobospan1" id="kobo.528.3"># Response 3
Gravity is the force that attracts objects with mass towards each other.</span></pre></li> <li class="calibre15"><span class="kobospan" id="kobo.529.1">Next, let’s </span><a id="_idIndexMarker143" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.530.1">edit our request body and change the </span><strong class="source-inline1"><span class="kobospan" id="kobo.531.1">temperature</span></strong><span class="kobospan" id="kobo.532.1"> parameter to the highest value possible, which is </span><strong class="source-inline1"><span class="kobospan" id="kobo.533.1">2</span></strong><span class="kobospan" id="kobo.534.1">. </span><span class="kobospan" id="kobo.534.2">Click </span><strong class="bold"><span class="kobospan" id="kobo.535.1">Send</span></strong><span class="kobospan" id="kobo.536.1">, and </span><a id="_idIndexMarker144" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.537.1">then repeat this three times, recording the responses of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.538.1">content</span></strong><span class="kobospan" id="kobo.539.1"> parameter for </span><span><span class="kobospan" id="kobo.540.1">each generation:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.541.1">
{
  "model": "gpt-3.5-turbo",
  "messages": [
    {
      "role": "user",
      "content": "Explain gravity in one sentence"
    }
  ],
  </span><strong class="bold1"><span class="kobospan1" id="kobo.542.1">"temperature": 2</span></strong><span class="kobospan1" id="kobo.543.1">
}
# Response 1
Gravity is the force that attract objects with mass towards each other, creating weight.
</span><span class="kobospan1" id="kobo.543.2"># Response 2
Gravity is a natural force that attracts objects toward each other based on their mass and distance between them.
</span><span class="kobospan1" id="kobo.543.3"># Response 3
Gravity is the universal force of attraction that pulls every object toward the Earth.</span></pre></li> <li class="calibre15"><span class="kobospan" id="kobo.544.1">Now, let’s </span><a id="_idIndexMarker145" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.545.1">repeat </span><em class="italic"><span class="kobospan" id="kobo.546.1">steps 1–2</span></em><span class="kobospan" id="kobo.547.1"> but use a more creative prompt, such as </span><strong class="source-inline1"><span class="kobospan" id="kobo.548.1">Create a creative tag line for an AI learning book</span></strong><span class="kobospan" id="kobo.549.1">. </span><span class="kobospan" id="kobo.549.2">Again, we will first perform a chat completion with the temperature </span><a id="_idIndexMarker146" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.550.1">parameter equal to </span><strong class="source-inline1"><span class="kobospan" id="kobo.551.1">0</span></strong><span class="kobospan" id="kobo.552.1"> three times. </span><span class="kobospan" id="kobo.552.2">Then, we will increase the temperature parameter to </span><strong class="source-inline1"><span class="kobospan" id="kobo.553.1">2</span></strong><span class="kobospan" id="kobo.554.1"> and run the request three times again. </span><span class="kobospan" id="kobo.554.2">The responses of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.555.1">content</span></strong><span class="kobospan" id="kobo.556.1"> parameter for each generation are listed in the following code blocks. </span><span class="kobospan" id="kobo.556.2">Note that yours will </span><span><span class="kobospan" id="kobo.557.1">likely differ:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.558.1">
# Request Body
{
  "model": "gpt-3.5-turbo",
  "messages": [
    {
      "role": "user",
      "content": "Create a creative tag line for an AI learning book"
    }
  ],
  "temperature": 0
}
# Response 1
Unlock the Power of Artificial Intelligence: Ignite Your Mind, Transform Your Future!
</span><span class="kobospan1" id="kobo.558.2"># Response 2
Unlock the Power of Artificial Intelligence: Ignite Your Mind, Transform Your Future!
</span><span class="kobospan1" id="kobo.558.3"># Response 3
Unlocking Minds, Unleashing Code: Navigating the Frontiers of AI Learning
# Request Body
{
  "model": "gpt-3.5-turbo",
  "messages": [
    {
      "role": "user",
      "content": "Create a creative tag line for an AI learning book"
    }
  ],
  "temperature": 2
}
# Response 1
Spark your mind – Accelerate with Artificial Excellence.
</span><span class="kobospan1" id="kobo.558.4"># Response 2
Unlock limitless intelligence: Medium approach, myth together.
</span><span class="kobospan1" id="kobo.558.5"># Response 3
Unleashing Minds: The AI Odyssey Awaits.</span></pre></li> </ol>
<h2 id="_idParaDest-71" class="calibre7"><a id="_idTextAnchor073" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.559.1">How it works…</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.560.1">As we saw in the </span><a id="_idIndexMarker147" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.561.1">recipe, the temperature parameter controls the randomness </span><a id="_idIndexMarker148" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.562.1">and creativity of the text generation. </span><span class="kobospan" id="kobo.562.2">When the temperature was set very low, the API produced very consistent and deterministic results for the same prompt. </span><span class="kobospan" id="kobo.562.3">In the first example, gravity was explained in the same exact way for each </span><span><span class="kobospan" id="kobo.563.1">chat completion:</span></span></p>
<pre class="console"><span class="kobospan1" id="kobo.564.1">
Gravity is the force that attracts objects with mass towards each other.</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.565.1">When we increased the temperature, we saw very different, more creative, and unexpected responses, such as </span><span><span class="kobospan" id="kobo.566.1">the following:</span></span></p>
<pre class="console"><span class="kobospan1" id="kobo.567.1">
Gravity is the universal force of attraction that pulls every object toward the Earth.</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.568.1">Think of </span><a id="_idIndexMarker149" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.569.1">the temperature setting as the dial on a radio. </span><span class="kobospan" id="kobo.569.2">A lower temperature is like tuning the radio to a well-established station where the signal is strong and clear, and you get a consistent, expected type of music or talk show. </span><span class="kobospan" id="kobo.569.3">This is analogous to the model delivering responses that are reliable, straightforward, and closely aligned with the most </span><span><span class="kobospan" id="kobo.570.1">likely answer.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.571.1">Conversely, a </span><a id="_idIndexMarker150" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.572.1">higher temperature is similar to tuning the radio to a frequency where you might catch a variety of stations, some clear and some static-filled, playing an eclectic mix of genres. </span><span class="kobospan" id="kobo.572.2">This creates an environment where unexpected, novel, and varied content comes through. </span><span class="kobospan" id="kobo.572.3">In the context of the language model, this means generating more creative, diverse, and sometimes unpredictable responses, mirroring the eclectic and varied nature of a radio dial turned toward a less </span><span><span class="kobospan" id="kobo.573.1">defined frequency.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.574.1">Temperature inner working</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.575.1">As we discussed </span><a id="_idIndexMarker151" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.576.1">before, when a model generates text, it calculates probabilities for the next word based on the prompt and response it has built so far. </span><span class="kobospan" id="kobo.576.2">In practice, temperature affects the response by changing the probability distribution of the </span><span><span class="kobospan" id="kobo.577.1">next word.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.578.1">With a higher temperature, this distribution becomes flatter, meaning less-probable words have a higher chance of being selected. </span><span class="kobospan" id="kobo.578.2">At a lower temperature, the distribution becomes more pronounced or </span><em class="italic"><span class="kobospan" id="kobo.579.1">sharper</span></em><span class="kobospan" id="kobo.580.1">, meaning the most probable words are likely to be chosen every time, which </span><span><span class="kobospan" id="kobo.581.1">reduces randomness.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.582.1">Decision based on use case</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.583.1">The decision </span><a id="_idIndexMarker152" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.584.1">on which temperature to use depends solely on the particular use case. </span><span class="kobospan" id="kobo.584.2">In general, there are three categories of </span><span><span class="kobospan" id="kobo.585.1">this parameter.</span></span></p>
<ul class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.586.1">Low-temperature values (0.0 to 0.8)</span></strong><span class="kobospan" id="kobo.587.1">: These should be used for primarily analytical, factual, or</span><a id="_idIndexMarker153" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.588.1"> logical tasks so that the model is more deterministic and focused. </span><span class="kobospan" id="kobo.588.2">In these use cases, traceability and repeatability is also important, and so a lower temperature is better, as it reduces randomness. </span><span class="kobospan" id="kobo.588.3">A lower temperature also means adhering to established patterns and conventions, leading to more </span><span><span class="kobospan" id="kobo.589.1">correct answers.</span></span><p class="calibre3"><span class="kobospan" id="kobo.590.1">Examples include generating code, performing data analysis, and answering </span><span><span class="kobospan" id="kobo.591.1">factual questions.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.592.1">Medium-temperature values (0.8 to 1.2)</span></strong><span class="kobospan" id="kobo.593.1">: These should be used for general-purpose </span><a id="_idIndexMarker154" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.594.1">and chatbot-like tasks, where balancing coherence and creativity is critical. </span><span class="kobospan" id="kobo.594.2">This enables the model to be flexible and produce new ideas, but it still remains focused to the </span><span><span class="kobospan" id="kobo.595.1">prompt at-hand.</span></span><p class="calibre3"><span class="kobospan" id="kobo.596.1">Examples include chatbots/conversational agents and </span><span><span class="kobospan" id="kobo.597.1">Q&amp;A systems.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.598.1">High-temperature values (1.2 to 2.0)</span></strong><span class="kobospan" id="kobo.599.1">: These should be used for creative writing</span><a id="_idIndexMarker155" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.600.1"> and brainstorming as the model is not constrained to follow established patterns and can explore very diverse styles. </span><span class="kobospan" id="kobo.600.2">Here, a </span><em class="italic"><span class="kobospan" id="kobo.601.1">correct</span></em><span class="kobospan" id="kobo.602.1"> answer does not exist, and instead, the purpose is to create varying outputs. </span><span class="kobospan" id="kobo.602.2">This does mean that you may get unexpected outputs that do not conform to the actual prompt </span><span><span class="kobospan" id="kobo.603.1">at all.</span></span><p class="calibre3"><span class="kobospan" id="kobo.604.1">Examples include storytelling, generating marketing slogans, and brainstorming </span><span><span class="kobospan" id="kobo.605.1">company names.</span></span></p></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.606.1">In the recipe, a lower temperature was far better when explaining gravity, as the prompt encourages a factual and straightforward answer. </span><span class="kobospan" id="kobo.606.2">However, the second prompt, about creating a tagline, is far better suited for a higher temperature, as this is a task that requires creativity and </span><span><span class="kobospan" id="kobo.607.1">out-of-the-box thinking.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.608.1">Overall, setting a temperature value means performing a trade-off between coherence and creativity, which shifts based on how you use the API within your application. </span><span class="kobospan" id="kobo.608.2">As a rule of thumb, it’s best to set the temperature to 1 and then modify it in increments of 0.2 until you reach your desired </span><span><span class="kobospan" id="kobo.609.1">output set.</span></span></p>
</div>
</body></html>