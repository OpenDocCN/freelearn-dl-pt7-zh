["```py\n    # Importing the required libraries\n    import numpy as np\n    import torch\n    from torch import nn, optim\n    ```", "```py\n    #input data and converting to torch tensors\n    inputs = np.array([[73, 67, 43],\\\n                       [91, 88, 64],\\\n                       [87, 134, 58],\\\n                       [102, 43, 37],\\\n                       [69, 96, 70]], dtype = 'float32')\n    inputs = torch.from_numpy(inputs)\n    #target data and converting to torch tensors\n    targets = np.array([[366], [486], [558],\\\n                        [219], [470]], dtype = 'float32')\n    targets = torch.from_numpy(targets)\n    #Checking the shapes\n    inputs.shape , targets.shape\n    ```", "```py\n    (torch.Size([5, 3]), torch.Size([5, 1]))\n    ```", "```py\n    class Model(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.fc1 = nn.Linear(3, 10)\n            self.fc2 = nn.Linear(10, 1)\n        def forward(self, x): \n            x = torch.relu(self.fc1(x))\n            x = self.fc2(x)\n            return x\n    # Instantiating the model\n    model = Model()\n    ```", "```py\n    # Loss function and optimizer\n    criterion = nn.MSELoss()  \n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    ```", "```py\n    # Train the model\n    n_epochs = 20\n    for it in range(n_epochs):\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        # Backward and optimize\n        loss.backward()\n        optimizer.step()\n        print(f'Epoch {it+1}/{n_epochs}, Loss: {loss.item():.4f}')\n    ```", "```py\n    Epoch 1/20, Loss: 185159.9688\n    Epoch 2/20, Loss: 181442.8125\n    Epoch 3/20, Loss: 177829.2188\n    Epoch 4/20, Loss: 174210.5938\n    Epoch 5/20, Loss: 170534.4375\n    Epoch 6/20, Loss: 166843.9531\n    Epoch 7/20, Loss: 163183.2500\n    Epoch 8/20, Loss: 159532.0625\n    Epoch 9/20, Loss: 155861.8438\n    Epoch 10/20, Loss: 152173.0000\n    Epoch 11/20, Loss: 148414.5781\n    Epoch 12/20, Loss: 144569.6875\n    Epoch 13/20, Loss: 140625.1094\n    Epoch 14/20, Loss: 136583.0625\n    Epoch 15/20, Loss: 132446.6719\n    Epoch 16/20, Loss: 128219.9688\n    Epoch 17/20, Loss: 123907.7422\n    Epoch 18/20, Loss: 119515.7266\n    Epoch 19/20, Loss: 115050.4375\n    Epoch 20/20, Loss: 110519.2969\n    ```", "```py\n    #Prediction using the trained model\n    preds = model(inputs)\n    print(preds)\n    ```", "```py\n    tensor([[ 85.6779],\n            [115.3034],\n            [146.7106],\n            [ 69.4034],\n            [120.5457]], grad_fn=<AddmmBackward>)\n    ```", "```py\nimport torch\nt = torch.arange(10)\nprint(t) \nprint(t.shape)\n```", "```py\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ntorch.Size([10])\n```", "```py\nt.view(2,5) # reshape the tensor to of size - (2,5)\n```", "```py\ntensor([[0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9]])\n```", "```py\nt.view(-1,5) \n# -1 will by default infer the first dimension \n# use when you are not sure about any dimension size\n```", "```py\ntensor([[0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9]])\n```", "```py\nx = torch.zeros(5, 1)\nprint(x)\nprint(x.shape)\n```", "```py\ntensor([[0.],\n        [0.],\n        [0.],\n        [0.],\n        [0.]])\ntorch.Size([5, 1])\n```", "```py\n# squeeze will remove any dimension with a value of 1\ny = x.squeeze(1)\n# turns a tensor of shape [5, 1] to [5]\ny.shape\n```", "```py\ntorch.Size([5])\n```", "```py\nx = torch.zeros(5)\nprint(x)\nprint(x.shape)\n```", "```py\ntensor([0., 0., 0., 0., 0.])\ntorch.Size([5])\n```", "```py\ny = x.unsqueeze(1) # unsqueeze will add a dimension of 1 \nprint(y.shape) # turns a tensor of shape [5] to [5,1]\n```", "```py\ntorch.Size([5, 1])\n```", "```py\na = torch.randn(4, 4)\na\n```", "```py\ntensor([[-0.5462,  1.3808,  1.4759,  0.1665],\n        [-1.6576, -1.2805,  0.5480, -1.7803],\n        [ 0.0969, -1.7333,  1.0639, -0.4660],\n        [ 0.3135, -0.4781,  0.3603, -0.6883]])\n```", "```py\n\"\"\"\nreturns max values in the specified dimension along with index\nspecifying 1 as dimension means we want to do the operation row-wise\n\"\"\"\ntorch.max(a , 1)\n```", "```py\ntorch.return_types.max(values=tensor([1.4759, 0.5480, \\\n                                      1.0639, 0.3603]),\\\n                       indices=tensor([2, 2, 2, 2]))\n```", "```py\ntorch.max(a , 1)[0] # to fetch the max values\n```", "```py\ntensor([1.4759, 0.5480, 1.0639, 0.3603])\n```", "```py\n# to fetch the index of the corresponding max values\ntorch.max(a , 1)[1]\n```", "```py\ntensor([2, 2, 2, 2])\n```", "```py\ntorch.gather(input, dim, index)\n```", "```py\nq_values = torch.randn(4, 4)\nprint(q_values)\n```", "```py\nq_values = torch.randn(4, 4)\nprint(q_values)\ntensor([[-0.2644, -0.2460, -1.7992, -1.8586],\n        [ 0.3272, -0.9674, -0.2881,  0.0738],\n        [ 0.0544,  0.5494, -1.7240, -0.8058],\n        [ 1.6687,  0.0767,  0.6696, -1.3802]])\n```", "```py\n# index must be defined as LongTensor\naction =torch.LongTensor([0 , 1, 2, 3])\n```", "```py\nq_values.shape , action.shape \n# q_values -> 2-dimensional tensor \n# action -> 1-dimension tensor\n```", "```py\n (torch.Size([4, 4]), torch.Size([4]))\n```", "```py\n\"\"\"\nunsqueeze is used to take care of the error - Index tensor \nmust have same dimensions as input tensor\nreturns the values from q_values using the action as indexes\n\"\"\"\ntorch.gather(q_values , 1, action.unsqueeze(1))\n```", "```py\ntensor([[-0.2644],\n        [-0.9674],\n        [-1.7240],\n        [-1.3802]])\n```", "```py\n(0.6 * 1) + (0.4 * 1) = 0.6 \n```", "```py\n    if random_number > ε :\n        choose the best action(exploitation)\n    else:\n        choose the random action (exploration)\n    decay ε \n    ```", "```py\n    # Importing the required libraries\n    import gym\n    import numpy as np\n    import matplotlib.pyplot as plt\n    ```", "```py\n    env = gym.make('FrozenLake-v0')\n    ```", "```py\n    number_of_states = env.observation_space.n\n    number_of_actions = env.action_space.n\n    # checking the total number of states and action\n    print('Total number of States : {}'.format(number_of_states)) \n    print('Total number of Actions : {}'.format(number_of_actions))\n    ```", "```py\n    Total number of States : 16\n    Total number of Actions : 4\n    ```", "```py\n    # Creation of Q table\n    Q_TABLE = np.zeros([number_of_states, number_of_actions])\n    # Looking at the initial values Q table\n    print(Q_TABLE)\n    print('shape of Q table : {}'.format(Q_TABLE.shape)\n    ```", "```py\n    [[0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]\n     [0\\. 0\\. 0\\. 0.]]\n    shape of Q table : (16, 4)\n    ```", "```py\n    # Setting the Hyper parameter Values for Q Learning\n    NUMBER_OF_EPISODES = 10000\n    MAX_STEPS = 100 \n    LEARNING_RATE = 0.1\n    DISCOUNT_FACTOR = 0.99\n    EGREEDY = 1\n    MAX_EGREEDY = 1\n    MIN_EGREEDY = 0.01\n    EGREEDY_DECAY_RATE = 0.001\n    ```", "```py\n    # Creating empty lists to store rewards of all episodes\n    rewards_all_episodes = []\n    # Creating empty lists to store egreedy_values of all episodes\n    egreedy_values = []\n    ```", "```py\n    # Training Process\n    for episode in range(NUMBER_OF_EPISODES):\n        state = env.reset()\n        done = False\n        current_episode_rewards = 0\n        for step in range(MAX_STEPS):\n            random_for_egreedy = np.random.rand()\n            if random_for_egreedy > EGREEDY:\n                action = np.argmax(Q_TABLE[state,:])\n            else:\n                action = env.action_space.sample()\n\n            new_state, reward, done, info = env.step(action)\n            Q_TABLE[state, action] = (1 - LEARNING_RATE) \\\n                                     * Q_TABLE[state, action] \\\n                                     + LEARNING_RATE \\\n                                     * (reward + DISCOUNT_FACTOR \\\n                                        * np.max(Q_TABLE[new_state,:]))\n            state = new_state\n            current_episode_rewards += reward\n            if done:\n                break\n        egreedy_values.append(EGREEDY)\n        EGREEDY = MIN_EGREEDY + (MAX_EGREEDY - MIN_EGREEDY) \\\n                  * np.exp(-EGREEDY_DECAY_RATE*episode)\n        rewards_all_episodes.append(current_episode_rewards)\n    ```", "```py\n    def rewards_split(rewards_all_episodes , total_episodes , split):\n        \"\"\"\n        Objective:\n        To split and calculate average reward or percentage of \n        completed rewards per splits\n        inputs: \n        rewards_all_episodes - all the per episode rewards\n        total_episodes - total of episodes\n        split - number of splits on which we will check the reward\n        returns:\n        average reward of percentage of completed rewards per splits\n        \"\"\"\n        splitted = np.split(np.array(rewards_all_episodes),\\\n                                     total_episodes/split)\n        avg_reward_per_splits = []\n        for rewards in splitted:\n            avg_reward_per_splits.append(sum(rewards)/split)\n        return avg_reward_per_splits\n    avg_reward_per_splits = rewards_split\\\n                            (rewards_all_episodes , \\\n                             NUMBER_OF_EPISODES , 1000)\n    ```", "```py\n    plt.figure(figsize=(12,5))\n    plt.title(\"% of Episodes completed\")\n    plt.plot(np.arange(len(avg_reward_per_splits)), \\\n             avg_reward_per_splits, 'o-')\n    plt.show()\n    ```", "```py\n    plt.figure(figsize=(12,5))\n    plt.title(\"Egreedy value\")\n    plt.bar(np.arange(len(egreedy_values)), egreedy_values, \\\n            alpha=0.6, color='blue', width=5)\n    plt.show()\n    ```", "```py\n    class DQN(nn.Module):\n        def __init__(self , hidden_layer_size):\n            super().__init__()\n            self.hidden_layer_size = hidden_layer_size\n            self.fc1 = nn.Linear\\\n                       (number_of_states,self.hidden_layer_size)\n            self.fc2 = nn.Linear\\\n                       (self.hidden_layer_size,number_of_actions)\n        def forward(self, x):\n            output = torch.tanh(self.fc1(x))\n            output = self.fc2(output)\n            return output\n    ```", "```py\n    self.fc1 = nn.Linear(number_of_states,self.hidden_layer_size)\n    self.fc2 = nn.Linear(self.hidden_layer_size,number_of_actions)\n    ```", "```py\n    def select_action(self,state,EGREEDY):\n            random_for_egreedy = torch.rand(1)[0]\n            if random_for_egreedy > EGREEDY:\n                with torch.no_grad():\n                    state = torch.Tensor(state).to(device)\n                    q_values = self.dqn(state)\n                    action = torch.max(q_values,0)[1]\n                    action = action.item()\n            else:\n                action = env.action_space.sample()\n            return action\n    ```", "```py\n    def optimize(self, state, action, new_state, reward, done):\n            state = torch.Tensor(state).to(device)\n            new_state = torch.Tensor(new_state).to(device)\n            reward = torch.Tensor([reward]).to(device)\n            if done:\n                target_value = reward\n            else:\n                new_state_values = self.dqn(new_state).detach()\n                max_new_state_values = torch.max(new_state_values)\n                target_value = reward + DISCOUNT_FACTOR \\\n                               * max_new_state_values\n    ```", "```py\n            loss = self.criterion(predicted_value, target_value)\n    ```", "```py\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n    ```", "```py\n    import gym\n    import matplotlib.pyplot as plt\n    import torch\n    import torch.nn as nn\n    from torch import optim\n    import numpy as np\n    import math\n    ```", "```py\n    # selecting the available device (cpu/gpu)\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n    print(device)\n    ```", "```py\n    env = gym.make('CartPole-v0')\n    ```", "```py\n    seed = 100\n    env.seed(seed)\n    torch.manual_seed(seed) \n    ```", "```py\n    NUMBER_OF_EPISODES = 700\n    MAX_STEPS = 1000\n    LEARNING_RATE = 0.01\n    DISCOUNT_FACTOR = 0.99\n    HIDDEN_LAYER_SIZE = 64\n    EGREEDY = 0.9\n    EGREEDY_FINAL = 0.02\n    EGREEDY_DECAY = 500\n    ```", "```py\n    EGREEDY_FINAL + (EGREEDY - EGREEDY_FINAL) \\\n    * math.exp(-1\\. * steps_done / EGREEDY_DECAY )\n    ```", "```py\n    def calculate_epsilon(steps_done):\n        \"\"\"\n        Decays epsilon with increasing steps\n        Parameter:\n        steps_done (int) : number of steps completed\n        Returns:\n        int - decayed epsilon\n        \"\"\"\n        epsilon = EGREEDY_FINAL + (EGREEDY - EGREEDY_FINAL) \\\n                  * math.exp(-1\\. * steps_done / EGREEDY_DECAY )\n        return epsilon\n    ```", "```py\n    number_of_states = env.observation_space.shape[0]\n    number_of_actions = env.action_space.n\n    print('Total number of States : {}'.format(number_of_states))\n    print('Total number of Actions : {}'.format(number_of_actions))\n    ```", "```py\n    Total number of States : 4\n    Total number of Actions : 2\n    ```", "```py\n    class DQN(nn.Module):\n        def __init__(self , hidden_layer_size):\n            super().__init__()\n            self.hidden_layer_size = hidden_layer_size\n            self.fc1 = nn.Linear\\\n                       (number_of_states,self.hidden_layer_size)\n            self.fc2 = nn.Linear\\\n                       (self.hidden_layer_size,number_of_actions)\n        def forward(self, x):\n            output = torch.tanh(self.fc1(x))\n            output = self.fc2(output)\n            return output\n    ```", "```py\n    class DQN_Agent(object):\n        def __init__(self):\n            self.dqn = DQN(HIDDEN_LAYER_SIZE).to(device)\n            self.criterion = torch.nn.MSELoss()\n            self.optimizer = optim.Adam\\\n                             (params=self.dqn.parameters() , \\\n                              lr=LEARNING_RATE)\n    ```", "```py\n        def select_action(self,state,EGREEDY):\n            random_for_egreedy = torch.rand(1)[0]\n            if random_for_egreedy > EGREEDY:\n                with torch.no_grad():\n                    state = torch.Tensor(state).to(device)\n                    q_values = self.dqn(state)\n                    action = torch.max(q_values,0)[1]\n                    action = action.item()\n            else:\n                action = env.action_space.sample()\n            return action\n    ```", "```py\n        def optimize(self, state, action, new_state, reward, done):\n            state = torch.Tensor(state).to(device)\n            new_state = torch.Tensor(new_state).to(device)\n            reward = torch.Tensor([reward]).to(device)\n            if done:\n                target_value = reward\n            else:\n                new_state_values = self.dqn(new_state).detach()\n                max_new_state_values = torch.max(new_state_values)\n                target_value = reward + DISCOUNT_FACTOR \\\n                               * max_new_state_values\n            predicted_value = self.dqn(state)[action].view(-1)\n            loss = self.criterion(predicted_value, target_value)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n    ```", "```py\n    # Instantiating the DQN Agent\n    dqn_agent = DQN_Agent()\n    steps_total = []\n    steps_counter = 0\n    for episode in range(NUMBER_OF_EPISODES):\n        state = env.reset()\n        done = False\n        step = 0\n        for I in range(MAX_STEPS):\n            step += 1\n            steps_counter += 1\n            EGREEDY = calculate_epsilon(steps_counter)\n            action = dqn_agent.select_action(state, EGREEDY)\n            new_state, reward, done, info = env.step(action)\n            dqn_agent.optimize(state, action, new_state, reward, done)\n            state = new_state\n            if done:\n                steps_total.append(step)\n                break\n    ```", "```py\n    print(\"Average reward: %.2f\" \\\n          % (sum(steps_total)/NUMBER_OF_EPISODES))\n    print(\"Average reward (last 100 episodes): %.2f\" \\\n          % (sum(steps_total[-100:])/100))\n    ```", "```py\n    Average reward: 158.83\n    Average reward (last 100 episodes): 176.28\n    ```", "```py\n    plt.figure(figsize=(12,5))\n    plt.title(\"Rewards Collected\")\n    plt.bar(np.arange(len(steps_total)), steps_total, \\\n            alpha=0.5, color='green', width=6)\n    plt.show()\n    ```", "```py\nclass ExperienceReplay(object):\n    def __init__(self , capacity):\n        self.capacity = capacity\n        self.buffer = []\n        self.pointer = 0\n    def push(self , state, action, new_state, reward, done):\n         experience = (state, action, new_state, reward, done)\n         if self.pointer >= len(self.buffer):\n            self.buffer.append(experience)\n         else:\n            self.buffer[self.pointer] = experience\n         self.pointer = (self.pointer + 1) % self.capacity\n    def sample(self , batch_size):\n         return zip(*random.sample(self.buffer , batch_size))\n    def __len__(self):\n         return len(self.buffer)\n```", "```py\ndef optimize(self, state, action, new_state, reward, done):\n        state = torch.Tensor(state).to(device)\n        new_state = torch.Tensor(new_state).to(device)\n        reward = torch.Tensor([reward]).to(device)\n\n        if done:\n            target_value = reward\n        else:\n            # first pass\n            new_state_values = self.dqn(new_state).detach()\n            max_new_state_values = torch.max(new_state_values)\n            target_value = reward + DISCOUNT_FACTOR \\\n                           * max_new_state_values\n        # second pass\n        predicted_value = self.dqn(state)[action].view(-1)\n        loss = self.criterion(predicted_value, target_value)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step() # weight optimization\n```", "```py\n    import gym\n    import matplotlib.pyplot as plt\n    import torch\n    import torch.nn as nn\n    from torch import optim\n    import numpy as np\n    import random\n    import math\n    ```", "```py\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n    print(device)\n    ```", "```py\n    env = gym.make('CartPole-v0')\n    ```", "```py\n    seed = 100\n    env.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    ```", "```py\n    number_of_states = env.observation_space.shape[0]\n    number_of_actions = env.action_space.n\n    print('Total number of States : {}'.format(number_of_states))\n    print('Total number of Actions : {}'.format(number_of_actions))\n    ```", "```py\n    Total number of States : 4\n    Total number of Actions : 2\n    ```", "```py\n    NUMBER_OF_EPISODES = 500\n    MAX_STEPS = 1000\n    LEARNING_RATE = 0.01\n    DISCOUNT_FACTOR = 0.99\n    HIDDEN_LAYER_SIZE = 64\n    EGREEDY = 0.9\n    EGREEDY_FINAL = 0.02\n    EGREEDY_DECAY = 500\n    REPLAY_BUFFER_SIZE = 6000\n    BATCH_SIZE = 32\n    UPDATE_TARGET_FREQUENCY = 200\n    ```", "```py\n    def calculate_epsilon(steps_done):\n        \"\"\"\n        Decays epsilon with increasing steps\n        Parameter:\n        steps_done (int) : number of steps completed\n        Returns:\n        int - decayed epsilon\n        \"\"\"\n        epsilon = EGREEDY_FINAL + (EGREEDY - EGREEDY_FINAL) \\\n                  * math.exp(-1\\. * steps_done / EGREEDY_DECAY )\n        return epsilon\n    ```", "```py\n    class DQN(nn.Module):\n        def __init__(self , hidden_layer_size):\n            super().__init__()\n            self.hidden_layer_size = hidden_layer_size\n            self.fc1 = nn.Linear\\\n                       (number_of_states,self.hidden_layer_size)\n            self.fc2 = nn.Linear\\\n                       (self.hidden_layer_size,number_of_actions)\n        def forward(self, x):\n            output = torch.tanh(self.fc1(x))\n            output = self.fc2(output)\n            return output\n    ```", "```py\n    class ExperienceReplay(object):\n        def __init__(self , capacity):\n            self.capacity = capacity\n            self.buffer = []\n            self.pointer = 0\n        def push(self , state, action, new_state, reward, done):\n            experience = (state, action, new_state, reward, done)\n                if self.pointer >= len(self.buffer):\n                self.buffer.append(experience)\n            else:\n                self.buffer[self.pointer] = experience\n            self.pointer = (self.pointer + 1) % self.capacity\n        def sample(self , batch_size):\n            return zip(*random.sample(self.buffer , batch_size))\n        def __len__(self):\n            return len(self.buffer)\n    ```", "```py\n    memory = ExperienceReplay(REPLAY_BUFFER_SIZE)\n    ```", "```py\n    class DQN_Agent(object):\n        def __init__(self):\n            self.dqn = DQN(HIDDEN_LAYER_SIZE).to(device)\n            self.target_dqn = DQN(HIDDEN_LAYER_SIZE).to(device)\n            self.criterion = torch.nn.MSELoss()\n            self.optimizer = optim.Adam(params=self.dqn.parameters(),\\\n                                        lr=LEARNING_RATE)\n            self.target_dqn_update_counter = 0\n        def select_action(self,state,EGREEDY):\n            random_for_egreedy = torch.rand(1)[0]\n            if random_for_egreedy > EGREEDY:\n                with torch.no_grad():\n                    state = torch.Tensor(state).to(device)\n                    q_values = self.dqn(state)\n                    action = torch.max(q_values,0)[1]\n                    action = action.item()\n            else:\n                action = env.action_space.sample()\n            return action\n        def optimize(self):\n            if (BATCH_SIZE > len(memory)):\n                return\n            state, action, new_state, reward, done = memory.sample\\\n                                                     (BATCH_SIZE)\n            state = torch.Tensor(state).to(device)\n            new_state = torch.Tensor(new_state).to(device)\n            reward = torch.Tensor(reward).to(device)\n            # to be used as index\n            action = torch.LongTensor(action).to(device)\n            done = torch.Tensor(done).to(device)\n            new_state_values = self.target_dqn(new_state).detach()\n            max_new_state_values = torch.max(new_state_values , 1)[0]\n            # when done = 1 then target = reward\n            target_value = reward + (1 - done) * DISCOUNT_FACTOR \\\n                           * max_new_state_values \n            predicted_value = self.dqn(state)\\\n                              .gather(1, action.unsqueeze(1))\\\n                              .squeeze(1)\n            loss = self.criterion(predicted_value, target_value)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            if self.target_dqn_update_counter \\\n               % UPDATE_TARGET_FREQUENCY == 0:\n                self.target_dqn.load_state_dict(self.dqn.state_dict())\n            self.target_dqn_update_counter += 1\n    ```", "```py\n    dqn_agent = DQN_Agent()\n    steps_total = []\n    steps_counter = 0\n    for episode in range(NUMBER_OF_EPISODES):\n        state = env.reset()\n        done = False\n        step = 0\n        for i in range(MAX_STEPS):\n            step += 1\n            steps_counter += 1\n            EGREEDY = calculate_epsilon(steps_counter)\n            action = dqn_agent.select_action(state, EGREEDY)\n            new_state, reward, done, info = env.step(action)\n            memory.push(state, action, new_state, reward, done)\n            dqn_agent.optimize()\n            state = new_state\n            if done:\n                steps_total.append(step)\n                break\n    ```", "```py\n    print(\"Average reward: %.2f\" \\\n          % (sum(steps_total)/NUMBER_OF_EPISODES))\n    print(\"Average reward (last 100 episodes): %.2f\" \\\n          % (sum(steps_total[-100:])/100))\n    ```", "```py\n    Average reward: 154.41\n    Average reward (last 100 episodes): 183.28\n    ```", "```py\n    plt.figure(figsize=(12,5))\n    plt.title(\"Rewards Collected\")\n    plt.xlabel('Steps')\n    plt.ylabel('Reward')\n    plt.bar(np.arange(len(steps_total)), steps_total, alpha=0.5, \\\n            color='green', width=6)\n    plt.show()\n    ```", "```py\nAverage reward: 158.83\nAverage reward (last 100 episodes): 176.28\n```", "```py\nAverage reward: 154.41\nAverage reward (last 100 episodes): 183.28\n```", "```py\n        new_state_indxs = self.dqn(new_state).detach()\n    ```", "```py\n        max_new_state_indxs = torch.max(new_state_indxs, 1)[1]\n    ```", "```py\n                new_state_values = self.target_dqn(new_state).detach()\n    ```", "```py\n                  max_new_state_values = new_state_values.gather\\\n                                         (1, max_new_state_indxs\\\n                                             .unsqueeze(1))\\\n                                         .squeeze(1)\n    ```", "```py\ndef optimize(self):\n        if (BATCH_SIZE > len(memory)):\n            return\n        state, action, new_state, reward, done = memory.sample\\\n                                                 (BATCH_SIZE)\n        state = torch.Tensor(state).to(device)\n        new_state = torch.Tensor(new_state).to(device)\n        reward = torch.Tensor(reward).to(device)\n        action = torch.LongTensor(action).to(device)\n        done = torch.Tensor(done).to(device)\n        \"\"\"\n        select action : get the index associated with max q value \n        from prediction network\n        \"\"\"\n        new_state_indxs = self.dqn(new_state).detach()\n        # to get the max new state indexes\n        max_new_state_indxs = torch.max(new_state_indxs, 1)[1]\n        \"\"\"\n        Using the best action from the prediction nn get the max new state \n        value in target dqn\n        \"\"\"\n        new_state_values = self.target_dqn(new_state).detach()\n        max_new_state_values = new_state_values.gather\\\n                               (1, max_new_state_indxs\\\n                                   .unsqueeze(1))\\\n                               .squeeze(1)\n        #when done = 1 then target = reward\n        target_value = reward + (1 - done) * DISCOUNT_FACTOR \\\n                       * max_new_state_values\n        predicted_value = self.dqn(state).gather\\\n                          (1, action.unsqueeze(1)).squeeze(1)\n        loss = self.criterion(predicted_value, target_value)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        if self.target_dqn_update_counter \\\n           % UPDATE_TARGET_FREQUENCY == 0:\n            self.target_dqn.load_state_dict(self.dqn.state_dict())\n        self.target_dqn_update_counter += 1\n```", "```py\n    import gym\n    import matplotlib.pyplot as plt\n    import torch\n    import torch.nn as nn\n    from torch import optim\n    import numpy as np\n    import random\n    import math\n    ```", "```py\n    Average reward: 174.09\n    Average reward (last 100 episodes): 186.06\n    ```", "```py\nAverage reward: 158.83\nAverage reward (last 100 episodes): 176.28\n```", "```py\nAverage reward: 154.41\nAverage reward (last 100 episodes): 183.28\n```", "```py\nAverage reward: 174.09\nAverage reward (last 100 episodes): 186.06\n```"]