<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer004">
			<h1 id="_idParaDest-6"><a id="_idTextAnchor005"/>Preface</h1>
			<p>Welcome to the world of <strong class="bold">automatic speech recognition</strong> (<strong class="bold">ASR</strong>) and OpenAI’s groundbreaking Whisper technology! In this book, <em class="italic">Learn OpenAI Whisper</em>, we will embark on a comprehensive journey to explore and master one of the most advanced ASR systems <span class="No-Break">available today.</span></p>
			<p>OpenAI’s Whisper represents a significant leap forward in speech recognition, offering unparalleled accuracy, versatility, and ease of use. Whether you are a developer, researcher, or enthusiast, this book will equip you with the knowledge and skills needed to harness the power of Whisper and unlock its <span class="No-Break">full potential.</span></p>
			<p>Throughout the chapters, we will dive deep into Whisper’s core concepts, underlying architecture, and practical applications. Starting with an introduction to the basics of ASR and Whisper’s critical features in <em class="italic">Part 1</em>, we will lay a solid foundation for understanding this <span class="No-Break">cutting-edge technology.</span></p>
			<p>In <em class="italic">Part 2</em>, we will explore the intricate details of Whisper’s architecture, including the transformer model, multitasking capabilities, and training techniques. You will gain hands-on experience in fine-tuning Whisper for domain and language specificity, enabling you to tailor the model to <span class="No-Break">your needs.</span></p>
			<p><em class="italic">Part 3</em> is where the real excitement begins as we delve into Whisper’s vast array of real-world applications and use cases. From transcription services and voice assistants to accessibility features and advanced techniques such as speaker diarization and personalized voice synthesis, you will learn how to leverage Whisper’s capabilities across <span class="No-Break">various domains.</span></p>
			<p>As you progress through the chapters, you will acquire technical skills and gain insights into the ethical considerations and future trends shaping the landscape of ASR and voice technologies. By the end of this book, you will be well equipped to tackle the challenges and opportunities that lie ahead in this rapidly <span class="No-Break">evolving field.</span></p>
			<p>Whether you want to enhance existing applications, develop innovative solutions, or expand your knowledge in ASR, <em class="italic">Learn OpenAI Whisper</em> is your comprehensive guide. This book leaves no stone unturned, ensuring you thoroughly understand Whisper and its applications. Get ready to embark on an exciting discovery, mastery, and innovation journey with <span class="No-Break">OpenAI’s Whisper!</span></p>
			<h1 id="_idParaDest-7"><a id="_idTextAnchor006"/>Who this book is for</h1>
			<p><em class="italic">Learn OpenAI Whisper</em> is designed for developers, data scientists, researchers, and business professionals who want to gain practical insights into leveraging OpenAI’s Whisper for <span class="No-Break">ASR tasks.</span></p>
			<p>The three primary personas who are the target audience of this book are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">ASR enthusiasts</strong>: Individuals who are passionate about exploring the potential of advanced speech recognition technologies and want to stay abreast of the latest developments in the field</li>
				<li><strong class="bold">Developers and data scientists</strong>: Professionals who want to integrate Whisper into their projects, enhance existing applications with speech recognition capabilities, or build new solutions from scratch</li>
				<li><strong class="bold">Researchers and academics</strong>: Individuals in academia or research institutions interested in studying Whisper’s inner workings, conducting experiments, and pushing the boundaries of ASR technology</li>
			</ul>
			<p>Throughout the book, readers will learn how to set up Whisper, fine-tune it for specific domains and languages, and apply it to real-world scenarios. They will gain a comprehensive understanding of Whisper’s architecture, features, and best practices for <span class="No-Break">effective implementation.</span></p>
			<h1 id="_idParaDest-8"><a id="_idTextAnchor007"/>What this book covers</h1>
			<p><a href="B21020_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">Unveiling Whisper – Introducing OpenAI’s Whisper</em>, outlines Whisper’s key features and capabilities, helping readers grasp its core functionalities. You’ll also get hands-on with initial setup and basic <span class="No-Break">usage examples.</span></p>
			<p><a href="B21020_02.xhtml#_idTextAnchor058"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, <em class="italic">Understanding the Core Mechanisms of Whisper</em>, delves into the nuts and bolts of Whisper’s ASR system. It explains the system’s critical components and functions, shedding light on how the technology interprets and processes <span class="No-Break">human speech.</span></p>
			<p><a href="B21020_03.xhtml#_idTextAnchor088"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><em class="italic">, Diving into the Architecture, </em>comprehensively explains the transformer model, the backbone of OpenAI’s Whisper. You will explore Whisper’s architectural intricacies, including the encoder-decoder mechanics, and learn how the transformer model drives effective <span class="No-Break">speech recognition.</span></p>
			<p><a href="B21020_04.xhtml#_idTextAnchor113"><span class="No-Break"><em class="italic">Chapter 4</em></span></a><em class="italic">, Fine-tuning Whisper for Domain and Language Specificity, </em>takes readers on a hands-on journey to fine-tune OpenAI’s Whisper model for specific domain and language needs. They will learn to set up a robust Python environment, integrate diverse datasets, and tailor Whisper’s predictions to align with target applications while ensuring equitable performance <span class="No-Break">across demographics.</span></p>
			<p><a href="B21020_05.xhtml#_idTextAnchor142"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><em class="italic">, Applying Whisper in Various Contexts,</em> explores OpenAI’s Whisper’s remarkable capabilities in transforming spoken language into written text across various applications, including transcription services, voice assistants, chatbots, and <span class="No-Break">accessibility features</span><span class="No-Break"><em class="italic">.</em></span></p>
			<p><a href="B21020_06.xhtml#_idTextAnchor160"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">, Expanding Applications with Whisper,</em> explores expanding OpenAI’s Whisper’s applications to tasks such as precise multilingual transcription, indexing content for enhanced discoverability, and utilizing transcription for SEO and <span class="No-Break">content marketing</span><span class="No-Break"><em class="italic">.</em></span></p>
			<p><a href="B21020_07.xhtml#_idTextAnchor177"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><em class="italic">, Exploring Advanced Voice Capabilities,</em> dives into advanced techniques that enhance OpenAI Whisper’s performance, such as quantization, and explores its potential for real-time <span class="No-Break">speech recognition.</span></p>
			<p><span class="No-Break"><em class="italic">Chapter 8</em></span><em class="italic">, Diarizing Speech with WhisperX and NVIDIA’s NeMo,</em> focuses on speaker diarization using WhisperX and NVIDIA’s NeMo framework. You will learn how to integrate these tools to accurately identify and attribute speech segments to different speakers within an <span class="No-Break">audio recording.</span></p>
			<p><a href="B21020_09.xhtml#_idTextAnchor207"><span class="No-Break"><em class="italic">Chapter 9</em></span></a><em class="italic">, Harnessing Whisper for Personalized Voice Synthesis, </em>explores how to harness OpenAI’s Whisper for voice synthesis, allowing readers to create personalized voice models that capture the unique characteristics of a <span class="No-Break">target voice.</span></p>
			<p><a href="B21020_10.xhtml#_idTextAnchor221"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><em class="italic">, Shaping the Future with Whisper,</em> provides a forward-looking perspective on the evolving field of ASR and Whisper’s role. The chapter delves into upcoming trends, anticipated features, and the general direction that voice technologies are taking. Ethical considerations are also discussed, providing a <span class="No-Break">well-rounded view.</span></p>
			<p>The following section will discuss the technical requirements and setup needed to get the most out of this book. It covers the software, hardware, and operating system prerequisites and the recommended environment for running the code examples. Additionally, it guides you in accessing the example code files and other resources available on the book’s GitHub repository. By following these instructions, you will be well prepared to dive into the world of OpenAI’s Whisper and make the most of the practical examples and exercises in <span class="No-Break">the book.</span></p>
			<h1 id="_idParaDest-9"><a id="_idTextAnchor008"/>To get the most out of this book</h1>
			<p>For most of the book, you only need a Google account and internet access to run the Whisper AI code in Google Colaboratory (Colab). No paid subscription is required to use the free version of Colab and GPU. Those familiar with Python can run this code example in their local environment instead of <span class="No-Break">using Colab.</span></p>
			<table id="table001" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Software/hardware covered in </strong><span class="No-Break"><strong class="bold">the book</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Operating </strong><span class="No-Break"><strong class="bold">system requirements</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Google <span class="No-Break">Colaboratory (Colab)</span></p>
						</td>
						<td class="No-Table-Style" rowspan="12">
							<p>Web browser on Windows, macOS, <span class="No-Break">or Linux</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Google Drive</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">YouTube</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">RSS</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">GitHub</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Python</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Hugging Face</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Gradio</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Foundational models:</span></p>
							<p><span class="No-Break">Google’s gTTS</span></p>
							<p>StableLM Zephyr 3B – <span class="No-Break">GGUF</span></p>
							<p><span class="No-Break">LlaVA</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Intel’s OpenVINO</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">NVIDIA’s NeMo</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Microphone <span class="No-Break">and speakers</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Whisper’s small model requires at least 12 gigabytes of GPU memory. Thus, let’s try to secure a decent GPU for our Colab! Unfortunately, accessing a good GPU with the free version of Google Colab (i.e., Tesla T4 16 GB) is becoming much harder. However, with Google Colab Pro, we should have no issues in being allocated a V100 or <span class="No-Break">P100 GPU.</span></p>
			<p><strong class="bold">If you are using the digital version of this book, we advise you to type the code yourself or access it from the book’s GitHub repository (a link is available in the next section). Doing so will help you avoid any potential errors related to copying and </strong><span class="No-Break"><strong class="bold">pasting code.</strong></span></p>
			<p>Fine-tuning Whisper in <a href="B21020_04.xhtml#_idTextAnchor113"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> will take at least one hour. Thus, you must monitor your running notebook in Colab regularly. Some notebooks implement a Gradio app with voice recording and audio playback. A microphone and speakers connected to your computer might help you experience the interactive voice features. Another option is to open the URL link Gradio provides at runtime on your mobile phone; from there, you might be able to use the phone’s microphone to record <span class="No-Break">your voice.</span></p>
			<p>By meeting these technical requirements, you will be prepared to explore Whisper in different contexts while enjoying the streamlined experience of Google Colab and the comprehensive resources available <span class="No-Break">on GitHub.</span></p>
			<h1 id="_idParaDest-10"><a id="_idTextAnchor009"/>Download the example code files</h1>
			<p>You can download the example code files for this book from GitHub at <a href="https://github.com/PacktPublishing/Learn-OpenAI-Whisper/">https://github.com/PacktPublishing/Learn-OpenAI-Whisper/</a>. If the code is updated, it will be updated in the <span class="No-Break">GitHub repository.</span></p>
			<p>We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/">https://github.com/PacktPublishing/</a>. Check <span class="No-Break">them out!</span></p>
			<h1 id="_idParaDest-11"><a id="_idTextAnchor010"/>Code in Action</h1>
			<p>This book’s Code in Action videos can be viewed <span class="No-Break">at </span><a href="https://packt.link/gGv9a"><span class="No-Break">https://packt.link/gGv9a</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-12"><a id="_idTextAnchor011"/>Conventions used</h1>
			<p>Several text conventions are used throughout <span class="No-Break">this book.</span></p>
			<p><strong class="source-inline">Code in text</strong>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. An example is “Users can even provide audiovisual formats such as <strong class="source-inline">.mp4</strong> as inputs, as Whisper will extract just the audio stream <span class="No-Break">to process.”</span></p>
			<p>A block of code is set <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from datasets import load_dataset, DatasetDict
common_voice = DatasetDict()
common_voice["train"] = load_dataset("mozilla-foundation/common_voice_11_0", "hi", split="train+validation", use_auth_token=True)
common_voice["test"] = load_dataset("mozilla-foundation/common_voice_11_0", "hi", split="test", use_auth_token=True)
print(common_voice)</pre>			<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set <span class="No-Break">in bold:</span></p>
			<pre class="source-code">
[default]
exten =&gt; s,1,Dial(Zap/1|30)
exten =&gt; s,2,Voicemail(u100)
<strong class="bold">exten =&gt; s,102,Voicemail(b100)</strong>
exten =&gt; i,1,Voicemail(s0)</pre>			<p>Any command-line input or output is written <span class="No-Break">as follows:</span></p>
			<pre class="console">
!pip install --upgrade pip
!pip install --upgrade datasets transformers accelerate soundfile librosa evaluate jiwer tensorboard gradio</pre>			<p><strong class="bold">Bold</strong>: Indicates a new term, an important word, or words that you see onscreen. For instance, words in menus or dialog boxes appear in <strong class="bold">bold</strong>. Here is an example: “To get a GPU, within Google Colab’s main menu, click <strong class="bold">Runtime</strong> | <strong class="bold">Change runtime type</strong>, then change the <strong class="bold">Hardware accelerator</strong> from <strong class="bold">None</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="bold">GPU</strong></span><span class="No-Break">.”</span></p>
			<p class="callout-heading">Tips or important notes</p>
			<p class="callout">Appear like this.</p>
			<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>Get in touch</h1>
			<p>Feedback from our readers is <span class="No-Break">always welcome.</span></p>
			<p><strong class="bold">General feedback</strong>: If you have questions about any aspect of this book, email us at <a href="http://customercare@packtpub.com">customercare@packtpub.com</a> and mention the book title in the subject of <span class="No-Break">your message.</span></p>
			<p><strong class="bold">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packtpub.com/support/errata">www.packtpub.com/support/errata</a> and fill out <span class="No-Break">the form.</span></p>
			<p><strong class="bold">Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please get in touch with us at <a href="mailto:copyright%40packt.com?subject=">copyright@packt.com</a> with a link to <span class="No-Break">the material.</span></p>
			<p><strong class="bold">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please <span class="No-Break">visit </span><a href="mailto:authors.packtpub.com?subject="><span class="No-Break">authors.packtpub.com</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-14"><a id="_idTextAnchor013"/>Share Your Thoughts</h1>
			<p>Once you’ve read <em class="italic">Learn OpenAI Whisper</em>, we’d love to hear your thoughts! <a href="https://packt.link/r/1-835-08592-X">Please click here to go straight to the Amazon review page for this book and share <span class="No-Break">your feedback</span></a><span class="No-Break">.</span></p>
			<p>Your review is important to us and the tech community and will help us make sure we’re delivering excellent <span class="No-Break">quality content.</span></p>
			<h1 id="_idParaDest-15"><a id="_idTextAnchor014"/>Download a free PDF copy of this book</h1>
			<p>Thanks for purchasing <span class="No-Break">this book!</span></p>
			<p>Do you like to read on the go but are unable to carry your print <span class="No-Break">books everywhere?</span></p>
			<p>Is your eBook purchase not compatible with the device of <span class="No-Break">your choice?</span></p>
			<p>Don’t worry, now with every Packt book you get a DRM-free PDF version of that book at <span class="No-Break">no cost.</span></p>
			<p>Read anywhere, any place, on any device. Search, copy, and paste code from your favorite technical books directly into <span class="No-Break">your application.</span></p>
			<p>The perks don’t stop there, you can get exclusive access to discounts, newsletters, and great free content in your <span class="No-Break">inbox daily</span></p>
			<p>Follow these simple steps to get <span class="No-Break">the benefits:</span></p>
			<ol>
				<li>Scan the QR code or visit the <span class="No-Break">link below</span></li>
			</ol>
			<div>
				<div id="_idContainer003" class="IMG---Figure">
					<img src="image/B21020_QR_Free_PDF.jpg" alt="" role="presentation" width="200" height="200"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a href="https://packt.link/free-ebook/9781835085929">https://packt.link/free-ebook/9781835085929</a></p>
			<ol>
				<li value="2">Submit your proof <span class="No-Break">of purchase</span></li>
				<li>That’s it! We’ll send your free PDF and other benefits to your <span class="No-Break">email directly</span></li>
			</ol>
		</div>
		<div>
			<div id="_idContainer005" class="Basic-Text-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer006" class="Basic-Text-Frame">
			</div>
		</div>
	</div>
</div>


<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer007" class="Content">
			<h1 id="_idParaDest-16" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor015"/>Part 1: Introducing OpenAI’s Whisper</h1>
			<p>This part introduces you to OpenAI’s <strong class="bold">Whisper</strong>, a cutting-edge automatic speech recognition (ASR) technology. You will gain an understanding of Whisper’s basic features and functionalities, including its key capabilities and setup process. This foundational knowledge will set the stage for a deeper exploration of the technology and its applications in <span class="No-Break">real-world scenarios.</span></p>
			<p>This part includes the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B21020_01.xhtml#_idTextAnchor016"><em class="italic">Chapter 1</em></a>, <em class="italic">Unveiling Whisper – Introducing OpenAI’s Whisper</em></li>
				<li><a href="B21020_02.xhtml#_idTextAnchor058"><em class="italic">Chapter 2</em></a>, <em class="italic">Understanding the Core Mechanisms of Whisper</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer008">
			</div>
		</div>
		<div>
			<div id="_idContainer009" class="Basic-Graphics-Frame">
			</div>
		</div>
	</div>
</div>
</body></html>