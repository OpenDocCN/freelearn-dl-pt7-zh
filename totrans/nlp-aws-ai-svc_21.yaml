- en: '*Chapter 18*: Building Secure, Reliable, and Efficient NLP Solutions'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you, dear reader, for staying with us through this (hopefully informative)
    journey in building best-in-class **Natural Language Processing** (**NLP**) solutions
    for organizations looking to uncover insights from their text data. Our aim in
    writing this book was to create awareness that **Artificial Intelligence** (**AI**)
    is mainstream and that we are at the cusp of a huge tidal wave of AI adoption
    that many enterprises are moving toward. This exciting technology not only helps
    you advance your career but also provides opportunities to explore new avenues
    of innovation that were not possible previously. For example, according to a BBC
    article ([https://www.bbc.com/future/article/20181129-the-ai-transforming-the-way-aircraft-are-built](https://www.bbc.com/future/article/20181129-the-ai-transforming-the-way-aircraft-are-built)),
    **Autodesk** ([https://www.autodesk.com/](https://www.autodesk.com/)), a global
    leader in designing and making technology, uses **Generative AI** ([https://www.amazon.com/Generative-AI-Python-TensorFlow-Transformer/dp/1800200889](https://www.amazon.com/Generative-AI-Python-TensorFlow-Transformer/dp/1800200889))
    to help aircraft manufacturers design more efficient airframes, a key requirement
    to reduce fuel consumption.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we reviewed several types of use cases that enterprises
    deal with today to garner useful information from text-based data. This information
    will either directly help them derive insights or serve as a precursor to driving
    downstream decision-making, with operational implications in both cases. We read
    through different business scenarios and discussed different solution design approaches,
    architecture implementation frameworks', and real-time and batch solutions that
    met these requirements.
  prefs: []
  type: TYPE_NORMAL
- en: We now understand how to design, architect, and build NLP solutions with **AWS**
    **AI** **services**, but we are still missing an important step. How do we ensure
    our solution is production-ready? What are the operational requirements to ensure
    our solution works as expected when dealing with real-world data? What are the
    non-functional requirements that the architecture needs to adhere to? And how
    do we build this into the solution as we go along? To answer these questions,
    we will review the best practices, techniques, and guidance on what makes a good
    NLP solution great.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining best practices for NLP solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying best practices for optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you will need access to an AWS account ([https://aws.amazon.com/console/](https://aws.amazon.com/console/)).
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the *Signing up for an AWS account* sub-section within the *Setting
    up your AWS environment* section in [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027),
    *Introducing Amazon Textract*, for detailed instructions on how you can sign up
    for an AWS account and sign in to the **AWS** **Management Console**.
  prefs: []
  type: TYPE_NORMAL
- en: Defining best practices for NLP solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you have done DIY (do-it-yourself) projects in the past, you know how important
    your tools are for your work. When building an NLP solution, or any solution for
    that matter, you need to keep the following in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to know your requirements (the "*what*").
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to know the problem that you are trying to solve by building the solution
    (the "*why*").
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to know the tools and techniques required to build the solution (the
    "*how*").
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to estimate the time you require to build the solution (the "*when*").
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, you need to determine the required skills for the team (the "*who*").
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But with this approach, you haven't necessarily addressed the needs that will
    make your solution reliable, scalable, efficient, secure, or cost-effective. And
    these are equally important (if not more so) to building long-lasting solutions
    that will delight your customers.
  prefs: []
  type: TYPE_NORMAL
- en: When building with AWS, you have access to prescriptive guidance and valuable
    insights garnered from decades of building and operating highly performant, massive-scale
    applications such as **Amazon**, along with the expertise that comes from helping
    some of the world's largest enterprises with running their cloud workloads on
    AWS. All of this experience and knowledge has been curated into a collection of
    architectural guidelines and best practices called the **AWS Well-Architected
    Framework**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of *well-architected* as a comprehensive checklist of questions that
    is defined by five pillars, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Operational excellence**: The operational excellence pillar recommends automating
    infrastructure provisioning and management (if applicable), modularizing the solution
    architecture into components that can be managed independently, enabling agility,
    implementing CI/CD-based DevOps practices, simulating operational failures, and
    preparing for and learning from it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: The security pillar recommends making security the top priority
    by implementing least privilege governance measures and associated guardrails
    from the ground up with a focus on identity and access management, compute and
    network security, data protection in transit and at rest, automation, simulation,
    and incident response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliability**: The reliability pillar requires the setting up of highly resilient
    architectures with the ability to self-heal from failures, with a focus on fail-fast
    and recovery testing, elastic capacity with auto scale-in/scale-out, and a high
    degree of automation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance efficiency**: The performance efficiency pillar recommends the
    use of **AWS Managed Services** (**AMS**) to remove the undifferentiated heavy
    lifting associated with managing infrastructure, using the global AWS network
    to reduce latency for your end users and remove the need for repeated experimentation
    and decoupling resource interactions by means of APIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost optimization**: The cost optimization pillar provides recommendations
    on measures you can take to track and minimize usage costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more details on the *Well-Architected Framework*, along with the resources
    to get you started, please go to the Amazon documentation: [https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html](https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Taken together, the Well-Architected questions from the various pillars guide
    you through your architecture design, build, and implementation. This enables
    you to include critical design principles, resulting in solutions that are built
    for secure, reliable, cost-effective, and efficient operations (and hence the
    term Well-Architected). So, what does Well-Architected mean in our case with regard
    to NLP solutions and AI services? To help you understand this clearly, we will
    create a matrix of the Well-Architected pillars, aligned with NLP development
    stages such as document preprocessing, prediction, and post-processing, from the
    perspective of the major AWS AI services we used in this book (**Amazon** **Textract**
    and **Amazon** **Comprehend**). We will also look at the application of principles
    for NLP solution builds in general. In each cell of this matrix, we will summarize
    how to apply the Well-Architected principles for design and implementation using
    best practices.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.1 – Well-Architected NLP solutions matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_18_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 18.1 – Well-Architected NLP solutions matrix
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding matrix, there are a number of design principles
    you can adopt during your solution development to build efficient and secure NLP
    applications. For the sake of clarity, we separated these principles based on
    the Well-Architected Framework pillars and the NLP development stages. However,
    as you might have noticed, some of these principles are repetitive across the
    cells. This is because an application of a principle for a particular pillar may
    automatically also address the needs of a different pillar based on what principle
    we refer to. For example, when using **AWS** **Glue** **ETL** jobs for document
    pre-processing and post-processing tasks, our Well-Architected needs for operational
    excellence, cost optimization, and performance efficiency are addressed without
    the need to do anything else.
  prefs: []
  type: TYPE_NORMAL
- en: We will explain the reason for this in more detail in the next section. We introduced
    the AWS Well-Architected Framework in this section and reviewed a matrix of how
    the Well-Architected principles can be applied to the AWS AI services we used
    throughout this book. In the next section, we will delve deeper and discuss how
    to implement some of the principles from the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Applying best practices for optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will dive a bit deeper into what each of the design principles
    that we documented in the Well-Architected NLP solutions matrix means, and how
    to implement them for your requirements. Since the scope of this book is primarily
    about AWS AI services, we already have the advantage of using serverless managed
    services, and this addresses a number of suggestions that the Well-Architected
    Framework alludes to. Additionally, as mentioned previously, some of the best
    practices documented in the matrix may appear to be repetitive – this is not a
    mistake but intentional, as the application of one design pattern may have a cascading
    benefit across multiple pillars of the Well-Architected Framework. We will highlight
    these as we come across them. Without further ado, let's dig in.
  prefs: []
  type: TYPE_NORMAL
- en: Using an AWS S3 data lake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principles *1.1a* and *1.3a* from the Well-Architected
    NLP solutions matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: A **data lake** is a repository for structured, semi-structured, and unstructured
    data. Initially, it is a collection of data from disparate sources within the
    enterprise and it serves as the data source for downstream analytics, business
    intelligence, **machine learning** (**ML**), and operational needs. However, since
    the data hydrated into a data lake retains its source format (data is not transformed
    before loading into the data lake), the data needs to undergo transformation at
    the time of consumption from the data lake. Building a data lake using **Amazon
    S3** (a fully managed object storage solution in the AWS cloud) makes a lot of
    sense because it scales as much as you want, and data stored in S3 is highly durable
    (for more information, see the section on the *"11 9s"* of durability at ([https://aws.amazon.com/s3/faqs/](https://aws.amazon.com/s3/faqs/)).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, AWS provides a number of ways in which you can get your data into
    S3, and several options to read data from S3, transform it, and feed it to your
    consumers for whatever needs you have, and all of these steps are carried out
    in a highly secure manner. To read in detail about creating a data lake on S3,
    hydrating it with your data sources, creating a data catalog, and securing, managing,
    and transforming the data, please refer to the *Building Big Data Storage Solutions*
    white paper ([https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html](https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html)).
    For instructions on how to set up your data lake using **AWS** **Lake Formation**
    (a fully managed service to build and manage data lakes), please refer to the
    getting started guide ([https://docs.aws.amazon.com/lake-formation/latest/dg/getting-started.html](https://docs.aws.amazon.com/lake-formation/latest/dg/getting-started.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Let's now discuss how to use AWS Glue for data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Using AWS Glue for data processing and transformation tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principles *1.4a*, *1.5a*, *3.1a*, *3.4b*, and *3.5a*
    from the Well-Architected NLP solutions matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: AWS Glue is a fully managed and serverless data cataloging, processing, and
    transformation service that enables you to build end-to-end ETL pipelines providing
    ready-made connections to data stores both on-premises and on AWS. The serverless,
    managed nature of AWS Glue removes the costs associated with infrastructure management
    and undifferentiated heavy lifting. AWS Glue enables you to configure connections
    to your data stores and your S3 data lake to directly pull this data for document
    pre-processing. You can use Glue ETL jobs to deliver this data after transformation
    (as required) to an NLP solution pipeline, both for training your custom NLP models
    and for predictions at inference time. This makes your solution more elegant and
    efficient, avoiding the need to create multiple solution components to take care
    of these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS Glue ETL jobs can be triggered on-demand or scheduled based on your requirements.
    You can also use it for document post-processing after your NLP solution has completed
    its recognition or classification tasks, for persisting to downstream data stores,
    or for consumption by operational processes that need this data. For a detailed
    demonstration of how AWS Glue can help you, please refer to the following tutorial
    using **AWS** **Glue Studio**, a graphical interface to make it easy to interact
    with Glue when creating and running ETL jobs: [https://docs.aws.amazon.com/glue/latest/ug/tutorial-create-job.html](https://docs.aws.amazon.com/glue/latest/ug/tutorial-create-job.html)'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will review how to use **Amazon** **SageMaker** **Ground
    Truth** for our text labeling tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon SageMaker Ground Truth for annotations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principle *1.4b* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of an NLP model is directly proportional to the quality of the
    labeled data it is based on. Though we primarily use AWS AI services that are
    pre-trained models, we saw quite a few use cases that needed Amazon Comprehend
    custom models for entity recognition and classification tasks. Amazon Comprehend
    custom models use **transfer learning** to train a custom model incrementally
    from its own pre-trained models with data that we provide (that is, data that
    isunique to our business). And so, for these custom training requirements, we
    need to provide high-quality labeled data that influences the accuracy of our
    models. As a best practice, we recommend using Amazon SageMaker Ground Truth ([https://aws.amazon.com/sagemaker/groundtruth/](https://aws.amazon.com/sagemaker/groundtruth/))
    for these labeling tasks. Ground Truth is directly integrated with Amazon Comprehend,
    and all you need to do is point to the location of the Ground Truth manifest when
    you set up your Amazon Comprehend job.
  prefs: []
  type: TYPE_NORMAL
- en: Ground Truth is a fully managed service, providing easy-to-use capabilities
    for data labeling with options to either use your own private workforce, third-party
    data labelers that you can source from **AWS** **Marketplace**, or with crowdsourced
    public data labelers using **Amazon** **Mechanical Turk**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ground Truth provides data encryption by default, and it automatically learns
    from the labeling activities conducted by the human labelers by training an ML
    model behind the scenes. This ML model will be applied to automate the labeling
    tasks once a confidence threshold has been reached. Ground Truth provides pre-built
    task templates for various data formats, such as image, text, and video files.
    You can also create custom templates for your own requirements by selecting the
    **Custom** task type when creating a labeling job. Please see the following screenshot
    for different types of text-based labeling tasks supported by Ground Truth:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.2 – Types of text labeling tasks in Amazon SageMaker Ground Truth'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_18_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 18.2 – Types of text labeling tasks in Amazon SageMaker Ground Truth
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, you create a labeling job, select an S3 location for your dataset,
    specify an **IAM** role (or ask Ground Truth to create one for you), select the
    task category from a list of pre-built templates (or you can select **Custom**
    for your own template), and choose the labeling workforce who will work on your
    request. Please refer to the following documentation for more details on how to
    get started: [https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-text.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-text.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to use Ground Truth for annotations, let's review a new
    feature that was launched recently to enable **custom entity recognizer training**
    directly from **PDF** or **Microsoft** **Word** documents.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon Comprehend with PDF and Word formats directly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principle *1.4c* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Comprehend updated the custom entity recognition feature in September
    2021 to support training and inference with PDF and Word documents directly.
  prefs: []
  type: TYPE_NORMAL
- en: To improve the performance efficiency of your NLP solution pipeline, Amazon
    Comprehend now supports training custom entity recognizers directly from PDF and
    Word document formats, without having to run pre-processing steps to flatten the
    document into a machine-readable format. To use this feature, you follow the same
    steps we specified in [*Chapter 14*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162),
    *Auditing Named Entity Recognition Workflows*, to train an Amazon Comprehend custom
    entity recognizer, but with a small difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You still need to annotate your entities in the training document and create
    an augmented manifest using Ground Truth. For more details, please refer to the
    instructions in this blog: [https://aws.amazon.com/blogs/machine-learning/custom-document-annotation-for-extracting-named-entities-in-documents-using-amazon-comprehend/](https://aws.amazon.com/blogs/machine-learning/custom-document-annotation-for-extracting-named-entities-in-documents-using-amazon-comprehend/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please use the following steps to train and infer from PDF or Word documents
    directly with Amazon Comprehend:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to your AWS Management Console (please refer to the *Technical requirements*
    section for more details) and navigate to the `comprehend` in the **Services**
    search bar.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Custom entity recognition** on the left pane and then click on **Create
    new model**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Model settings** section, provide the name for your model and scroll
    down to the **Data specifications** section to select the **Augmented manifest**
    and **PDF, Word documents** formats for training. Provide the S3 location for
    your augmented manifest. Scroll down to select or create an IAM role and click
    on **Create** to start the training. Once the model is trained, you can run an
    inference using the same steps we discussed in [*Chapter 14*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162),
    *Auditing Named Entity Recognition Workflows*. But, provide a PDF or Word document
    as an input instead of a CSV file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 18.3 – Amazon Comprehend custom entity recognizer training using PDF,
    Word documents'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_18_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 18.3 – Amazon Comprehend custom entity recognizer training using PDF,
    Word documents
  prefs: []
  type: TYPE_NORMAL
- en: This feature update improves pre-processing efficiency and reduces upfront time
    investments in setting up our NLP solution pipelines. In the next section, we
    will review how to enforce access control when building NLP solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Enforcing least privilege access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principles *1.2c*, *3.2b*, and *4.2a* from the Well-Architected
    NLP solutions matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: One of the core tenets of a highly secure architecture is enforcing what is
    called the *least privilege for access to resources*. **AWS** **Identity and Access
    Management** (**IAM**) is a security service that enables defining and implementing
    your authentication and authorization strategy for secured access to your AWS
    infrastructure for your users. With IAM, you can create permissions policies that
    are attached to roles or users (*identities*) that define what (*actions*) the
    identity can or cannot do with AWS services (*resources*). Least privilege, as
    the name indicates, is all about defining highly restrictive permissions policies
    for your users and roles. The default permission in AWS is a *deny*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If no policies are specified for a user, the user does not have permission
    to do anything in AWS. So, you add policy statements that allow a user or a role
    to perform specific tasks using AWS services or resources. In our examples in
    this book, due to the nature of our use cases and for the sake of simplicity and
    ease of configuration, we suggest you add managed permissions policies such as
    *TextractFullAccess* or *ComprehendFullAccess* to your **SageMaker execution IAM
    role** for your notebook. When you build your NLP solution and promote it to production,
    as a best practice, you should enforce least privilege access. Let''s discuss
    what this means through an example. The *ComprehendFullAccess* permissions policy
    is defined by the following **JSON** statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you refer to the highlighted section in the preceding JSON code, the wildcard
    (`*`) attached to `"comprehend"` indicates that all Amazon Comprehend API actions
    are allowed for the role or the user that wields this policy. This is not a restrictive
    policy but rather provides a broad set of permissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enforce least privilege access, a new policy should be created that should
    be changed as shown in the following JSON statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In this changed JSON statement, we provide restrictive permissions that allow
    a user or a role to only use the **entities detection** feature in Amazon Comprehend.
    A good approach would be to only provide those permissions that are absolutely
    needed for a user or role to perform a task. Also, you will have to ensure that
    you monitor IAM roles and policy assignments to ensure that you clean up the permissions
    once the user or role has completed the task. This way, you avoid the situation
    of an old permission granting a user more access than they need. AWS provides
    a feature called **IAM Access Analyzer** to proactively monitor permissions and
    take actions as required. For a detailed introduction to Access Analyzer, please
    refer to the following documentation: [https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html](https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will review how to protect sensitive data during our
    NLP solution building task.
  prefs: []
  type: TYPE_NORMAL
- en: Obfuscating sensitive data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principles *1.2a* and *4.2b* from the Well-Architected
    NLP solutions matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Protecting the confidentiality of your data is highly important. Enterprises
    typically classify their data into categories such as *public*, *confidential*,
    *secret*, and *top-secret*, and apply controls and guardrails based on these classifications.
    If you are unsure of how to classify your data, please refer to an existing data
    classification model such as the **US National Classification Scheme**. More details
    on this model, as well as best practices for data classification as recommended
    by AWS, can be found in the following documentation: [https://docs.aws.amazon.com/whitepapers/latest/data-classification/welcome.html](https://docs.aws.amazon.com/whitepapers/latest/data-classification/welcome.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have classified your data, the next step is to determine the type of
    confidentiality your data contains. Data can be **personally Identifiable Information**
    (**PII**), for example, it may contain social security numbers, credit card numbers,
    bank account numbers, and so on. Or, if your data contains your customers' private
    health records, it is called **protected health information** (**PHI**). If you
    are in the legal industry, the *Attorney-Client Privileged Information* is protected
    data and must be kept confidential.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, as we can see, data protection is vitally important and must be a key consideration
    in our NLP solution development. When building on AWS, there are multiple ways
    in which you can protect your confidential data, including data encryption at
    rest and in transit, which we cover in subsequent sections in this chapter. For
    details on how AWS supports the highest privacy standards, and information about
    the resources to help you protect your customers'' data, please refer to the following
    link: [https://aws.amazon.com/compliance/data-privacy/](https://aws.amazon.com/compliance/data-privacy/)'
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot shows the results of an Amazon Comprehend PII detection
    in real time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.4 – Amazon Comprehend PII detection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_18_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 18.4 – Amazon Comprehend PII detection
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 4*](B17528_04_Final_SB_ePub.xhtml#_idTextAnchor063), *Automating
    Document Processing Workflows*, we reviewed how Amazon Comprehend provides support
    for detecting PII entities from your data. You can use this capability as part
    of your document pre-processing stage by means of the `Lambda` function automatically
    to either process or transform your data when you fetch it from Amazon S3\. For
    PII detection, two `Lambda` functions are made available to be used with S3 Object
    Access Lambda. The Amazon Comprehend `ContainsPiiEntites` API is used to classify
    documents that contain PII data and the `DetectPiiEntities` API is used to identify
    the actual PII data within the document for the purposes of redaction. For a tutorial
    on how to detect and redact PII data from your documents using the S3 Object Access
    Lambda, please refer to this GitHub repository: [https://github.com/aws-samples/amazon-comprehend-s3-object-lambda-functions](https://github.com/aws-samples/amazon-comprehend-s3-object-lambda-functions)'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will review how to implement data protection at rest
    and in transit.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting data at rest and in transit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principles *1.2b*, *2.2a*, *3.2a*, and *4.3b* from the
    Well-Architected NLP solutions matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have discussed least privilege and data confidentiality, let's review
    the best practices for protecting your data *at rest* and *in transit* (that is,
    when it resides in a data store and during transit, for example, as a result of
    service API calls). When we talk about data protection at rest, we refer to encrypting
    your data during storage in AWS. Your data can reside in an Amazon S3 data lake,
    a relational database in **Amazon** **RDS** (a managed AWS service for relational
    databases), in **Amazon** **Redshift** (an exabyte-scale, cloud-based data warehouse),
    in **Amazon** **DynamoDB**, or in one of the other purpose-built databases such
    as **Amazon** **DocumentDB** (a managed AWS service for **MongoDB**), or **Amazon**
    **Neptune** (a managed AWS service for **Graph** databases), and many more.
  prefs: []
  type: TYPE_NORMAL
- en: With AWS, the advantage is that you can enable encryption to protect your data
    at rest easily using **AWS** **Key Management Service** (**KMS**) (a reliable
    and secure service to create, manage, and protect your encryption keys, and for
    applying the encryption of data across many services in AWS). Encryption is supported
    using the **AES-256** standard ([https://en.wikipedia.org/wiki/Advanced_Encryption_Standard](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard)).
  prefs: []
  type: TYPE_NORMAL
- en: For example, when you store objects in Amazon S3, you can request **Server-side
    encryption** (encrypting data at the destination as it is stored in S3) by selecting
    to either use S3-managed encryption keys (that is, keys you create in KMS at the
    bucket level) or your own encryption keys (which you provide when you upload the
    objects to your S3 bucket).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.5 – Enabling encryption for your S3 bucket](img/B17528_18_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18.5 – Enabling encryption for your S3 bucket
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Redshift provides similar options at cluster creation time to encrypt
    your data ([https://docs.aws.amazon.com/redshift/latest/mgmt/security-server-side-encryption.html](https://docs.aws.amazon.com/redshift/latest/mgmt/security-server-side-encryption.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Amazon DynamoDB encrypts all your data by default using AWS KMS ([https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html)).
  prefs: []
  type: TYPE_NORMAL
- en: You can enable encryption for your Amazon RDS databases ([https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html)).
  prefs: []
  type: TYPE_NORMAL
- en: You can also enable encryption for any purpose-built AWS databases, such as
    Amazon DocumentDB ([https://docs.aws.amazon.com/documentdb/latest/developerguide/encryption-at-rest.html](https://docs.aws.amazon.com/documentdb/latest/developerguide/encryption-at-rest.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'All AWS services that handle customer data support encryption. Please refer
    to this blog for more details: [https://aws.amazon.com/blogs/security/importance-of-encryption-and-how-aws-can-help/](https://aws.amazon.com/blogs/security/importance-of-encryption-and-how-aws-can-help/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To protect data in transit, you secure your API endpoints using a protocol
    such as **Transport Layer Security** (**TLS**): [https://en.wikipedia.org/wiki/Transport_Layer_Security](https://en.wikipedia.org/wiki/Transport_Layer_Security)'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to protecting data at rest, AWS provides the means to secure your data
    in transit using **AWS** **Certificate Manager** (a managed service to provision
    and manage TLS certificates) to secure communications, verify identities, and
    implement **HTTPS** endpoints for application interactions. All AWS services that
    handle customer data are secured using TLS with HTTPS.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon API Gateway for request throttling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principle *2.1a* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: 'When we build Amazon Comprehend custom entity recognizers or classifiers, we
    host these models by creating Comprehend real-time endpoints, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.6 – Creating Amazon Comprehend real-time endpoints'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_18_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 18.6 – Creating Amazon Comprehend real-time endpoints
  prefs: []
  type: TYPE_NORMAL
- en: 'You can call these endpoints directly from your code to detect entities or
    for text classification needs, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will talk about how to set up auto scaling for your Amazon Comprehend real-time
    endpoints in a subsequent section, but you are billed by the second for your inference
    endpoints ([https://aws.amazon.com/comprehend/pricing/](https://aws.amazon.com/comprehend/pricing/)),
    and the capacity is measured in inference units that represent a throughput of
    100 characters per second. Throttling requests to the endpoint will allow for
    a more managed use of capacity. **Amazon** **API Gateway** ([https://aws.amazon.com/api-gateway/](https://aws.amazon.com/api-gateway/))
    is a fully managed, secure, and scalable service for API management that can be
    used to create an API to abstract the calls to the Amazon Comprehend endpoint
    by using an AWS Lambda function, as demonstrated in the tutorial in the following
    link: [https://github.com/aws-samples/amazon-comprehend-custom-entity-recognizer-api-example](https://github.com/aws-samples/amazon-comprehend-custom-entity-recognizer-api-example)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from throttling, API Gateway also supports traffic management, access
    control, monitoring, and version management, which can help implement a robust
    approach for handling requests for our solution. For more details, please refer
    to the following documentation: [https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html](https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html)'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover how to set up auto scaling for your Amazon
    Comprehend real-time endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up auto scaling for Amazon Comprehend endpoints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principle *2.3a* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we discussed that you need endpoints to enable real-time
    predictions from your Amazon Comprehend custom models. The endpoint inference
    capacity is denoted as an **Inference Unit** (**IU**), which represents a throughput
    of 100 characters per second. When you create an endpoint, you specify the number
    of IUs you need, which helps Amazon Comprehend determine the resources to allocate
    to your endpoint. You calculate IUs based on the output throughput you need from
    the endpoint in terms of characters per second, and you are charged for the duration
    of when the endpoint is active, irrespective of whether it is receiving requests
    or not. So, you need to manage the IUs carefully to ensure you receive the required
    capacity when needed (for performance) but can also discard the capacity when
    not needed (to save costs). You can do this using Amazon Comprehend auto scaling:
    [https://docs.aws.amazon.com/comprehend/latest/dg/comprehend-autoscaling.html](https://docs.aws.amazon.com/comprehend/latest/dg/comprehend-autoscaling.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can set up auto scaling only by using the **AWS** **Command Line Interface**
    (**AWS CLI**). The following example shows how to enable auto scaling for custom
    entity recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Register a scalable target by running the following code snippet in the AWS
    CLI. Here `scalable-dimension` refers to the Amazon Comprehend resource type along
    with the unit of measurement for capacity (IUs):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You then create a JSON configuration for what target you would like to track,
    as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you put this scaling policy into action, as shown in the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the next section, we will review how to monitor training metrics for our
    custom models to enable proactive actions.
  prefs: []
  type: TYPE_NORMAL
- en: Automating monitoring of custom training metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principle *4.3a* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: When training your custom classification or entity recognition models, Amazon
    Comprehend generates `DescribeEntityRecognizer` API (for entity recognition) or
    `DescribeDocumentClassifier` API (for classification) to get the evaluation metrics
    for your custom model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a code snippet of how to use the `DescribeEntityRecognizer`
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To monitor for the completion of the Amazon Comprehend custom training job,
    you can use `DescribeEntityRecognizer` or `DescribeDocumentClassifier` APIs to
    retrieve the evaluation metrics. If the metrics are below a threshold, this function
    can send alerts or notifications using **Amazon** **Simple Notification Service**
    (**SNS**). For details on how to schedule an event using Amazon EventBridge, please
    refer to the documentation: [https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at using Amazon A2I to set up human loops
    to review predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon A2I to review predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principle *2.3b* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: We covered using **Amazon** **Augmented AI** (**Amazon** **A2I**) (a managed
    service to set up human reviews of ML predictions) in great detail in many of
    the previous chapters in this book, starting with [*Chapter 13*](B17528_13_Final_SB_ePub.xhtml#_idTextAnchor151),
    *Improving the Accuracy of Document Processing Workflows*. When your solution
    is newly developed, it is a best practice to set up a human loop for prediction
    reviews, auditing, and making corrections as needed. Your solution should also
    include a feedback loop with model re-training based on human-reviewed data. We
    recommend having a human loop with Amazon A2I for the first three to six months
    to allow your solution to evolve based on direct feedback. Subsequently, you can
    disable the human loop.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover how to build modular, loosely coupled solutions
    using **Async** **APIs**.
  prefs: []
  type: TYPE_NORMAL
- en: Using Async APIs for loose coupling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principles *2.4a* and *4.4a* from the Well-Architected
    NLP solutions matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: 'When we set up an NLP solution pipeline that is required to scale to processing
    millions of documents, it is a good idea to use the **Asynchronous Batch APIs**
    to implement this architecture. Synchronous APIs follow the request-response paradigm,
    meaning the requesting application will wait for a response and will be held up
    until a response is received. This approach works well when the need is to process
    a few documents quickly for a real-time or near-real-time, mission-critical requirement.
    However, when the document volume increases, a synchronous approach will hold
    compute resources, and slow down the process. Typically, organizations implement
    two separate NLP solution pipelines: one for real-time processing, and a second
    for batch processing. For batch processing, depending on the number of documents
    to be processed, the inference results are available after a few minutes to a
    few hours, depending on how the architecture is set up.'
  prefs: []
  type: TYPE_NORMAL
- en: With Amazon Comprehend, once the entity recognizer or classifier training is
    completed, use the `Batch` API when you need to run `inference` for large document
    volumes, as shown in the following code snippets for entity recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we submit an `entities detection` job (if we provide the endpoint `ARN`,
    it will use our custom entity recognizer). The response returns a `JobId`, a `Job
    ARN`, and a `job status`. Once the job is completed, the results are sent to the
    S3 location you specify in the `OutputDataConfig`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When using Amazon Textract for processing large document volumes, you can use
    the `Batch` APIs to first submit a text analysis or detection job, and then subsequently
    get the extraction results once the analysis job is completed. The following steps
    show how you can use the Amazon Textract Batch APIs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s assume our use case is to process documents that contain tables and
    form data along with text. In this case, we will use the `StartDocumentAnalysis`
    API ([https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html](https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html))
    as a first step, and ask it to look for table and form contents. Text in paragraphs
    is extracted by default. We also pass an Amazon SNS `topic` and an `IAM role`
    that provides permissions for Amazon Textract to publish a message to the SNS
    `topic`. This API returns a `JobId` that we will use in the next step:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When the job completes, Amazon Textract sends a message to the SNS topic indicating
    the job status. You can attach an AWS Lambda function to this SNS topic as an
    event trigger. This Lambda function will call the `GetDocumentAnalysis` API ([https://docs.aws.amazon.com/textract/latest/dg/API_GetDocumentAnalysis.html](https://docs.aws.amazon.com/textract/latest/dg/API_GetDocumentAnalysis.html))
    to retrieve the results from the Amazon Textract job, as shown in the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The response is a JSON object of blocks of text data that include both tabular
    and form content. In the next section, we will discuss how we can simplify the
    parsing of the JSON response object using Amazon Textract Response Parser.
  prefs: []
  type: TYPE_NORMAL
- en: Using Amazon Textract Response Parser
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses principle *3.4a* from the Well-Architected NLP solutions
    matrix (*Figure 18.1*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The JSON documents returned by the Amazon Textract APIs are comprehensive,
    with document contents categorized as blocks that encapsulate information for
    pages, lines, words, tables, forms, and the relationships between them. When using
    Amazon Textract to process complex or descriptive documents, it can seem time-consuming
    to understand the JSON results and parse them to obtain the data we need from
    the various ways text is contained in the document. The following code snippet
    shows the JSON response for a line that Amazon Textract extracted from the document
    we used in [*Chapter 14*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162), *Auditing
    Named Entity Recognition Workflows*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: So, in order to make the process of retrieving the content we need from the
    JSON output simpler, the **Amazon** **Textract Response Parser** library (or **TRP**)
    ([https://github.com/aws-samples/amazon-textract-response-parser](https://github.com/aws-samples/amazon-textract-response-parser))
    was created. TRP makes it easy to get all the data we need with very few lines
    of code and improves the efficiency of our overall solution. We have already used
    TRP in this book, for example, in [*Chapter 14*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162),
    *Auditing Named Entity Recognition Workflows*, and [*Chapter 17*](B17528_17_Final_SB_ePub.xhtml#_idTextAnchor202),
    *Visualizing Insights from Handwritten Content*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippets show how to install and use the TRP library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the TRP library, use the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the library, call the `Textract` API to analyze a document, and use
    `TRP` to parse the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can loop through the results to extract data from pages, tables, and
    more. For a **Python** example of how to use TRP, please refer to the code sample:
    [https://github.com/aws-samples/amazon-textract-response-parser/tree/master/src-python#parse-json-response-from-textract](https://github.com/aws-samples/amazon-textract-response-parser/tree/master/src-python#parse-json-response-from-textract)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the next section, we will review why it is important to persist the prediction
    results from our NLP solution, and how we can make use of this data.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting prediction results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During the course of this book, we have seen examples where the results of
    an entity recognition or classification task are sent to an **Amazon** **Elasticsearch**
    instance (for metadata extraction) or to Amazon DynamoDB (for persistence). We
    also saw examples where these results are used to inform decisions that impact
    downstream systems. The reason for this is because we often see with organizations
    that document processing provides important inputs to their mainstream operations.
    So, when you design and build NLP solutions, you have to keep in mind how your
    prediction results are going to be consumed, by whom, and for what purpose. Depending
    on the consumption use case, there are different options available for us to use.
    Let''s review some of these options:'
  prefs: []
  type: TYPE_NORMAL
- en: If the need is for real-time access to inference results, set up an API Gateway
    and AWS Lambda function to abstract the Amazon Comprehend real-time endpoint.
    Persist the inference request and response in an Amazon S3 bucket or Amazon DynamoDB
    for future reference. Please refer to the *Using API Gateway for request throttling*
    section for more details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the results are to be sent to downstream applications that need these inputs
    for decision-making or functional requirements, you can persist the results to
    Amazon S3, or an Amazon RDS database, or any of the purpose-built data stores
    in AWS. To notify the applications that new results are available, you can publish
    a message to an Amazon SNS topic or use an event trigger in the data stores. For
    more information, please refer to the following: [https://docs.aws.amazon.com/lambda/latest/dg/services-rds-tutorial.html](https://docs.aws.amazon.com/lambda/latest/dg/services-rds-tutorial.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you need the results to populate a knowledge repository or make it available
    for user search, send it to an Amazon Elasticsearch (now called **OpenSearch**)
    index. For more information, please refer to the following: [https://docs.aws.amazon.com/opensearch-service/latest/developerguide/search-example.html](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/search-example.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you would like to use the results for business intelligence or visualization,
    you can send the results to an Amazon S3 bucket and use **Amazon** **QuickSight**
    with the data in Amazon S3\. For more information, please refer to the following:
    [https://docs.aws.amazon.com/quicksight/latest/user/getting-started-create-analysis-s3.html](https://docs.aws.amazon.com/quicksight/latest/user/getting-started-create-analysis-s3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would like to transform the results before sending them to the data stores,
    use AWS Glue ETL jobs. For more details, please refer to the *Using AWS Glue for
    data processing and transformation* section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now review how to automate the NLP solution development using **AWS**
    **Step Function**.
  prefs: []
  type: TYPE_NORMAL
- en: Using AWS Step Function for orchestration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a previous section, we read how using Batch APIs can help scale the architecture
    to handle large volumes of documents. We reviewed Amazon Comprehend and Textract
    APIs that can help us implement a batch-processing pipeline. When we start designing
    a batch solution, it may take the shape of an Amazon S3 bucket, to which an AWS
    Lambda event trigger is attached that will call the Amazon Textract API to start
    document analysis. To this, an Amazon SNS topic would be provided, a message will
    be sent by Amazon Textract to this topic, to which an AWS Lambda is attached,
    and so on. You get the point. It can get really difficult to manage all of these
    moving parts in our solution. To design an elegant and efficient NLP solution,
    you can use AWS Step Function to manage the orchestration of your entire pipeline
    ([https://aws.amazon.com/step-functions/](https://aws.amazon.com/step-functions/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS Step Function is a serverless, event-driven orchestration service that
    can help tie together several steps in a process and manage it end to end. Error
    handling is built in, so you can configure retries, branching, and compensation
    logic into the orchestration. An example of a Step Function orchestration from
    the samples available in the AWS Console is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.7 – A sample Step Function orchestration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_18_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 18.7 – A sample Step Function orchestration
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, you can run a sample orchestration in your Step Function Console
    in the AWS Management Console by selecting **Run a sample project** in the **State
    Machines** option on the left of the console. You can also try the Step Function
    tutorials available here: [https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/](https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/)'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let's discuss how to automate our deployment process using
    AWS CloudFormation.
  prefs: []
  type: TYPE_NORMAL
- en: Using AWS CloudFormation templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**AWS** **CloudFormation** ([https://aws.amazon.com/cloudformation/](https://aws.amazon.com/cloudformation/))
    is an infrastructure-as-code service that helps you automate and manage your resource
    provisioning tasks in AWS. When building NLP solutions using AWS AI services,
    we primarily deal with managed services, but, depending on how our operational
    architecture looks, it makes a lot of sense to use AWS CloudFormation to automate
    our deployment process. This is mainly because it removes a lot of overhead related
    to the setup, makes change management easier, and helps us achieve operational
    excellence. Every solution topology is different, but if your NLP architecture
    includes Amazon S3 buckets and other types of AWS data stores, AWS Step Function,
    AWS Lambda functions, Amazon SNS topics, and so on, you will benefit from using
    AWS CloudFormation. Templates can be written in JSON or **YAML**, and there are
    lots of resources available to help you get started.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For an example CloudFormation template with AWS Step Function and AWS Lambda
    functions, please refer to the following: [https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-lambda-state-machine-cloudformation.html](https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-lambda-state-machine-cloudformation.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For template snippet code examples for a variety of AWS services, please refer
    to the following: [https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/CHAP_TemplateQuickRef.html](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/CHAP_TemplateQuickRef.html)'
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in detail in the preceding sections, these are some of the principles
    and best practices you can adopt to design and build long-lasting NLP solutions
    that are cost-effective, resilient, scalable, secure, and performance-efficient.
    These are the characteristics that make great solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After having learned how to build NLP solutions for a number of real-world
    use cases in the previous chapters, we spent this chapter reading about how to
    build secure, reliable, and efficient architectures using the AWS Well-Architected
    Framework. We first introduced what the Well-Architected Framework is, and reviewed
    the five pillars it is comprised of: operational excellence, security, reliability,
    performance efficiency, and cost optimization. We read about each of the pillars
    in brief, and then discussed how the Well-Architected Framework can help us build
    better and more efficient NLP solutions by using a matrix of best practices aligned
    with the Well-Architected principles and the different stages of NLP solution
    development.'
  prefs: []
  type: TYPE_NORMAL
- en: We followed this summary of the best practices by diving deep into each one,
    learning how to implement them using the AWS Management Console, AWS documentation
    references, and some code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to the end of this book. It is with a heavy heart that we bid
    adieu to you, our wonderful readers. We hope that you had as much fun reading
    this book as we had writing it. Please don't forget to check out the *Further
    reading* section for a few references we have included to continue your learning
    journey in the exciting space that is NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS Well-Architected Labs ([https://www.wellarchitectedlabs.com/](https://www.wellarchitectedlabs.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Textract blogs ([https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-textract/](https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-textract/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Comprehend blogs ([https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-comprehend/](https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-comprehend/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Comprehend workshops ([https://comprehend-immersionday.workshop.aws/](https://comprehend-immersionday.workshop.aws/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS Automating data processing from documents([https://aws.amazon.com/machine-learning/ml-use-cases/document-processing/](https://aws.amazon.com/machine-learning/ml-use-cases/document-processing/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
