["```py\n    from river import metrics\n    from river import stream\n    from river import tree,neighbors,naive_bayes,ensemble,linear_model\n    from river.drift import DDM, ADWIN\n    ```", "```py\n    name = \"Proposed PWPAE model\"\n    t, m = PWPAE(X_train, y_train, X_test, y_test)\n    acc_fig(t, m, name)\n    ```", "```py\nx = np.arange(-10, 10, 0.001)\nq = norm.pdf(x, 5, 4)\nplt.title('KL(P||Q) = %1.3f' % kl_divergence(p, q))\nplt.plot(x, p)\nplt.plot(x, q, c='red')\n```", "```py\n    data1 = scipy.stats.norm.rvs(size=100000, loc=0, scale=1.5, random_state=123)\n    hist1 = np.histogram(data1, bins=100)\n    hist1_dist = scipy.stats.rv_histogram(hist1)\n    import matplotlib.pyplot as plt\n    X1 = np.linspace(-8.0, -2.0, 1000)\n    plt.title(\"PDF\")\n    plt.hist(data1, density=True, bins=100,  color ='blue')\n    plt.plot(X1, hist1_dist.pdf(X1), label='PDF', color = 'blue')\n    ```", "```py\n    data2 = scipy.stats.norm.rvs(size=100000, loc=0, scale=5.5, random_state=123)\n    hist2 = np.histogram(data2, bins=100)\n    hist2_dist = scipy.stats.rv_histogram(hist2)\n    X2 = np.linspace(4.0, 8.0, 1000)\n    plt.title(\"Probability Density Function\")\n    plt.hist(data2, density=True, bins=100, color ='green')\n    plt.plot(X2, hist2_dist.pdf(X2), label='PDF', color = 'green')\n    plt.legend(['X1', 'X2'])\n    ```", "```py\n    Y1 = hist1_dist.pdf(X1)\n    Y2 = hist2_dist.pdf(X2)\n    M = (Y1 + Y2) / 2\n    d1 = scipy.stats.entropy(Y1, M, base=2)\n    print(\"KL div Y1 and M\", d1)\n    d2 = scipy.stats.entropy(Y2, M, base=2)\n    print(\"KL div Y2 and M\", d2)\n    ```", "```py\n    KL div X1 and X2 0.21658815880427068\n    KL div Y1 and M 1.0684247605300703\n    KL div Y2 and M 0.1571132219534354\n    ```", "```py\n    js_dv = (d1 + d2) / 2\n    js_distance = np.sqrt(js_dv)\n    print(\"JS Dist d1 and d2\", js_distance)\n    ```", "```py\njs_distance_scipy = scipy.spatial.distance.jensenshannon(Y1, Y2)\nprint(\"JS Dist d1 and d2 of Scipy\", js_distance_scipy)\njs_distance_scipy = scipy.spatial.distance.jensenshannon(X1, X2)\nprint(\"JS Dist X1 and X2 of Scipy\", js_distance_scipy)\ndx1 = scipy.stats.entropy(Y1, X1, base=2)\ndx2 = scipy.stats.entropy(Y2, X2, base=2)\njs_dv = (dx1 + dx2) / 2\nprint(\"JS Div X1 and X2\", js_dv)\n```", "```py\nJS Dist d1 and d2 0.7827956254615587\nJS Dist d1 and d2 of Scipy 0.6037262820103958\nJS Dist X1 and X2 of Scipy 0.1941318696014193\nJS Div X1 and X2 1.3749093686870903\n```", "```py\n    from alibi_detect.cd import KSDrift\n    from alibi_detect.ad import ModelDistillation\n    from alibi_detect.models.tensorflow.resnet import scale_by_instance\n    from alibi_detect.utils.fetching import fetch_tf_model, fetch_detector\n    from alibi_detect.utils.tensorflow.prediction import predict_batch\n    from alibi_detect.utils.saving import save_detector\n    from alibi_detect.datasets import fetch_cifar10c, corruption_types_cifar10c\n    ```", "```py\n    from tensorflow.keras.layers import Conv2D, Dense, Flatten, InputLayer\n    from tensorflow.keras.regularizers import l1\n    def distilled_model_cifar10(clf, nb_conv_layers=8, distl_model_filters1=256, nb_dense=40, kernel1=8, kernel2=8, kernel3=8, ae_arch=False):\n        distl_model_filters1 = int(distl_model_filters1)\n        distl_model_filters2 = int(distl_model_filters1 / 2)\n        distl_model_filters3 = int(distl_model_filters1 / 4)\n        layers = [InputLayer(input_shape=(64, 64, 3)),\n                  Conv2D(distl_model_filters1, kernel1, strides=2, padding='same')]\n        if nb_conv_layers > 2:\n            layers.append(Conv2D(distl_model_filters2,\n    kernel2, strides=2, padding='same', activation=tf.nn.relu, kernel_regularizer=l1(1e-5)))\n        if nb_conv_layers > 2:\n            layers.append(Conv2D(distl_model_filters3, kernel3, strides=2, padding='same',\n                                 activation=tf.nn.relu, kernel_regularizer=l1(1e-5)))\n        layers.append(Flatten())\n        layers.append(Dense(nb_dense))\n        layers.append(Dense(clf.output_shape[1], activation='softmax'))\n        distilled_model = tf.keras.Sequential(layers)\n        return distilled_model\n    ```", "```py\n    load_pretrained = True\n    detector_type = 'adversarial'\n    detector_name = 'model_distillation'\n    filepath = os.path.join(filepath, detector_name)\n    if load_pretrained:\n        ad = fetch_detector(filepath, detector_type, dataset, detector_name, model=model)\n    else:\n        distilled_model = distilled_model_cifar10(clf)\n        print(distilled_model.summary())\n        ad = ModelDistillation(distilled_model=distilled_model, model=clf)\n        ad.fit(X_train, epochs=50, batch_size=128, verbose=True)\n        save_detector(ad, filepath)\n    ```", "```py\n    def evaluate_plot_model_accuracy():\n    mu_noharm, std_noharm = [], []\n    mu_harm, std_harm = [], []\n    acc = [clf_accuracy['original']]\n    for k, v in score_drift.items():\n       mu_noharm.append(v['noharm'].mean())\n       std_noharm.append(v['noharm'].std())\n       mu_harm.append(v['harm'].mean())\n       std_harm.append(v['harm'].std())\n       acc.append(v['acc'])\n    ```", "```py\nfrom detecta import detect_cusum\nx = np.random.randn(500)/5\nx[200:300] += np.arange(0, 4, 4/100)\nta, tai, taf, amp = detect_cusum(x, 4, .025, True, True)\nx = np.random.randn(500)\nx[200:300] += 6\ndetect_cusum(x, 3, 1.5, True, True)\nx = 2*np.sin(2*np.pi*np.arange(0, 2.8, .01))\nta, tai, taf, amp = detect_cusum(x, 1.8, .05, True, True)\n```", "```py\n    shift_norm_0 = multivariate_normal([2, -4], np.eye(2)*sigma**2)\n    X_0 = shift_norm_0.rvs(size=int(N_test*phi1),random_state=2)\n    X_1 = ref_norm_1.rvs(size=int(N_test*phi2),random_state=2)\n    ```", "```py\n    phi1 = phi1*1.5\n    phi2 = phi2*2.5\n    true_slope = -1\n    shift_norm_0 = multivariate_normal([3, -8], np.eye(2)*sigma**2)\n    X_0 = shift_norm_0.rvs(size=int(N_test*phi1),random_state=2)\n    X_1 = ref_norm_1.rvs(size=int(N_test*phi2),random_state=2)\n    X_test = np.vstack([X_0, X_1])\n    y_test = true_model(X_test,true_slope)\n    plot(X_test,y_test,true_slope,clf=clf)\n    print('Mean test accuracy %.2f%%' %(100*clf.score(X_test,y_test)))\n    pred = detector.predict(X_test)\n    print('Is drift? %s!' %labels[pred['data']['is_drift']])\n    ```", "```py\n    def detect_prior_drift():\n        label_detector = MMDDrift(y_ref.reshape(-1,1), backend='tensorflow', p_val=.05)\n        y_pred = clf.predict(X_test)\n        label_detector.predict(y_pred.reshape(-1,1))\n    detect_prior_drift()\n    ```", "```py\n    {'data': {'is_drift': 1,\n      'distance': 0.107620716,\n      'p_val': 0.0,\n      'threshold': 0.05,\n      'distance_threshold': 0.013342142},\n     'meta': {'name': 'MMDDriftTF',\n      'detector_type': 'offline',\n      'data_type': None,\n      'version': '0.9.0',\n      'backend': 'tensorflow'}}\n    ```", "```py\n    white, red = np.asarray(white, np.float32), np.asarray(red, np.float32)\n    n_white, n_red = white.shape[0], red.shape[0]\n    col_maxes = white.max(axis=0)\n    white, red = white / col_maxes, red / col_maxes\n    white, red = white[np.random.permutation(n_white)], red[np.random.permutation(n_red)]\n    X = white[:, :-1]\n    X_corr = red[:, :-1]\n    X_train = X[:(n_white//2)]\n    X_ref = X[(n_white//2) :(3*n_white//4)]\n    X_h0 = X[(3*n_white//4):]\n    X_ref = np.concatenate([X_train, X_ref], axis=0)\n    ```", "```py\n    n_runs = 550\n    times_h0 = [time_run(cd, X_h0, window_size) for _ in range(n_runs)]\n    print (f\"Average run-time under no-drift: {np.mean(times_h0)}\")\n    _ = scipy.stats.probplot(np.array(times_h0), dist=scipy.stats.geom, sparams=1/ert, plot=plt)\n    ```", "```py\nAverage run-time under no-drift: 47.72\n```", "```py\n    X_new_ref = np.concatenate([X_train, X_ref], axis=0)\n    from alibi_detect.cd import LSDDDriftOnline\n    def lsdd_detector():\n        cd = LSDDDriftOnline(\n            X_new_ref, ert, window_size, backend='tensorflow', n_bootstraps=5500,\n        )\n     times_h0 = [time_run(cd, X_h0, window_size) for _ in range(n_runs)]\n        print(f\"Average run-time under no-drift: {np.mean(times_h0)}\")\n        _ = scipy.stats.probplot(np.array(times_h0), dist=scipy.stats.geom, sparams=1/ert, plot=plt)\n    ```", "```py\nAverage run-time under drift: 54.39\n```", "```py\n    import random\n    from river import drift\n    import matplotlib.pyplot as plt\n    import numpy as np\n    rng = random.Random(123456)\n    ph = drift.PageHinkley()\n    ```", "```py\n    data_stream = rng.choices([50, 100], k=1200) + rng.choices(range(600, 900), k=5000) + rng.choices(range(200, 300), k=5000)\n    plt.plot(data_stream)\n    plt.show()\n    ```", "```py\n    for I, val in enumerate(data_stream):\n         in_drift, in_warning = ph.update(val)\n         if in_drift:\n           print (\"\"Change detected at index {i}, input value: {val\"\")\n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    import nannyml as nml\n    from scipy.spatial.transform import Rotation\n    ```", "```py\n    # analyze with reference and analysis periods\n    # Days/week * Hours/day * events/hour\n    DPP = 7*24*24\n    np.random.seed(23)\n    s1 = np.random.randn(DPP*22)\n    x1 = s1 + np.random.randn(DPP*22)/8\n    x2 = s1 + np.random.randn(DPP*22)/8\n    x3 = np.random.randn(DPP*22)/8\n    xdat = np.array([x1, x2, x3]).T\n    rot = Rotation.from_euler('z', 90, degrees=True)\n    # following matrix multiplication implementation, we need a 3xN data matrix hence we transpose\n    ydat = np.matmul(rot.as_matrix(), xdat.T).T\n    # drift is sudden and affects last 5-7 weeks\n    dataar = np.concatenate(\n    (xdat[:-5*DPP], ydat[-5*DPP:]),\n    axis=0)\n    datadf = pd.DataFrame(dataar, columns=['feature1', 'feature2', 'feature3'])\n    datadf = datadf.assign(ordered = pd.date_range(start='1/3/2020', freq='5min', periods=22*DPP))\n    datadf['week'] = datadf.ordered.dt.isocalendar().week - 1\n    datadf['partition'] = 'reference'\n    datadf.loc[datadf.week >= 8, ['partition']] = 'analysis'\n    datadf = datadf.assign(y_pred_proba = np.random.rand(DPP*22))\n    datadf = datadf.assign(y_true = np.random.randint(2, size=DPP*22))\n    ```", "```py\n    rcerror_calculator = nml.DataReconstructionDriftCalculator(\n    feature_column_names=feature_column_names,\n    timestamp_column_name='ordered',\n    chunk_size=DPP\n    ).fit(reference_data=reference)\n    rcerror_results = rcerror_calculator.calculate(data=analysis)\n    figure = rcerror_results.plot(plot_reference=True)\n    figure.show()\n    ```", "```py\n    from xgboost import XGBRegressor\n    from cinnamon.drift import ModelDriftExplainer, AdversarialDriftExplainer\n    from sklearn import datasets\n    from sklearn.datasets import fetch_california_housing\n    from sklearn.datasets import fetch_openml\n    california = fetch_openml(name=\"house_prices\", as_frame=True)\n    california_df = pd.DataFrame(california.data, columns=california.feature_names)\n    RANDOM_SEED = 2021\n    ```", "```py\n    model = XGBRegressor(n_estimators=1000, booster=\"gbtree\",objective=\"reg:squarederror\", learning_rate=0.05,max_depth=6,seed=RANDOM_SEED, use_label_encoder=False)\n    model.fit(X=X_train, y=y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=20, verbose=10)\n    ```", "```py\n    drift_explainer = ModelDriftExplainer(model)\n    drift_explainer.fit(X_train, X_test, y_train, y_test)\n    drift_explainer.plot_prediction_drift()\n    drift_explainer.get_prediction_drift()\n    ```", "```py\n    drift_explainer.plot_target_drift()\n    drift_explainer.get_target_drift()\n    ```", "```py\n    drift_explainer.get_performance_metrics_drift()\n    ```", "```py\nPerformanceMetricsDrift(dataset1=RegressionMetrics (mse=10567733.794917172, explained_variance=0.9983637108518892), dataset2=RegressionMetrics(mse=540758766.1766539, explained_variance=0.9088298308495063))\n```", "```py\n    drift_explainer.plot_feature_drift('Neighborhood_Old_Town')\n    drift_explainer.get_feature_drift('Neighborhood_Old_Town')'\n    ```", "```py\nDriftMetricsNum(mean_difference=-0.022504892367906065, wasserstein=0.022504892367906093, ks_test=BaseStatisticalTestResult (statistic=0.022504892367906065, pvalue=0.996919812985332))\n```", "```py\n    def compute_recommendation_utility(reco_items, interacted_distr, lmbda=0.65):\n        total_score = 0.0\n        reco_distr = compute_likely_genres(items_to_recommend)\n        kl_div = evaluate_kl_divergence(interacted_distr, reco_distr)\n        for item in items_to_recommend:\n            total_score += item.score\n        rec_utility = (1 - lmbda) * total_score - lmbda * kl_div\n        return rec_utility\n    ```", "```py\n    def calibrate_recommendations(items, interacted_distr, topn, lmbda=0.65):\n        calib_rec_items = []\n        for _ in range(topn):\n            max_utility = -np.inf\n            for rec_item in items:\n                if rec_item in calib_rec_items:\n                    continue\n                rec_utility = compute_recommendation_utility(calib_rec_items + [rec_item], interacted_distr, lmbda)\n                if rec_utility > max_utility:\n                    max_utility = rec_utility\n                    best_item = rec_item\n            calib_rec_items.append(best_item)\n        return calib_rec_items\n    ```", "```py\n    calib_rec_item_distr = compute_likely_genres(calib_items_to_recommend)\n    calib_reco_kl_div = evaluate_kl_divergence(interacted_distr, calib_rec_item_distr)\n    reco_kl_div = evaluate_kl_divergence(interacted_distr, reco_distr)\n    distr_comparison_plot(interacted_distr, calib_rec_item_distr)\n    ```"]