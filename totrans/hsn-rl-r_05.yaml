- en: Markov Decision Processes in Action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stochastic processes involve systems that evolve over time (but also more generally
    in space) according to probabilistic laws. Such systems or models describe the
    complex phenomena of the real world that have the possibility of being random.
    These phenomena are more frequent than we believe them to be. We encounter these
    phenomena when the quantities we are interested in aren't predictable with absolute
    certainty. However, when such phenomena show a variety of possible outcomes that
    can be somehow explained or described, then we can introduce a probabilistic model
    of the phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: A Markov chain is a stochastic process whereby the evolution of a system depends
    only on its present state and not on its past state. A Markov chain is characterized
    by a set of states and by the probability of a transition occurring between states.
    Think of a point that can move randomly forward or backward along a line at discrete
    intervals of time, covering a certain distance at each interval.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will get to grips with the concepts of the Markov process. Stochastic
    Markov processes will be analyzed in detail. We will be introduced to the Markov
    chain and then we will learn how to use these algorithms to make weather forecasts.
    Finally, we will learn how to evaluate the optimal policy for the solution of
    a Markov reward problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the Markov process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Markov chains
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov chains applications – weather forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov reward model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/36xyVLj](http://bit.ly/36xyVLj)'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of the Markov process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned in [Chapter 2](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml),
    *Building Blocks of Reinforcement Learning,* a stochastic process is called **Markovian**
    when a certain instant *t* of observation is chosen. The evolution of the process
    starting with *t* depends only on *t*, while it does not depend on the previous
    instants in any way. Thus, a process is Markovian when, given the moment of observation,
    only that instant determines the future evolution of the process, while this evolution
    does not depend on the past. In the next section, we will explore the concept
    of stochastic processes and we will see how it is related to probability theory.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the stochastic process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to provide a formal definition of a Markov process, it is necessary
    to specify what is meant by a set of random variables having a temporal ordering.
    Such a set of random variables can best be represented by a stochastic process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The theory of stochastic processes concerns the study of systems that evolve
    over time (but also more generally in space) according to probabilistic laws.
    Such systems or models describe complex phenomena of the real world that have
    the possibility of being random. These phenomena are more frequent than we believe
    them to be, and we face these situations when the quantities we are interested
    in can''t be predicted with certainty. We define a stochastic process in discrete
    time and discrete states in a sequence that contains the following random variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a976054-188b-468d-b04a-5c477c331844.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding sequence, each *X[n]* is a discrete random variable with values
    in a set *S = s[1], s[2],…, s[n],* called the **space of the states**. Without
    losing generality, suppose that *S* is a subset of the relative integers *Z*.
    Each value of *X[n]* as the index *n* changes will represent the state of the
    system over time. This process we will analyze starts in any of the states represented
    by *X[n]* and will move to the next state *X[n + 1]*. Each transition is called
    a **step**.
  prefs: []
  type: TYPE_NORMAL
- en: As time passes, the process can jump from one state to another. If a step *n*
    is in a state *i*, and at the next step, *n + 1* is in a state *j ≠ i*, we can
    say that there has been a transition.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a stochastic process (*X[n]*), we are interested in calculating the probabilities
    associated with it. Now, let's explore the basic concepts of probability. If you
    already know about these concepts, you can skip this section. Either way, this
    section will allow you to explore the basics of probability theory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **probability** (**a priori**) that a given event (*E*) occurs is the ratio
    between the number (*s*) of favorable cases of the event itself and the total
    number (*n*) of the possible cases, provided all the considered cases are equally
    probable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ec2e412-6aaa-429d-9d86-edb35b873d4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: While throwing a dice, what is the probability that 3 shows up? The number of
    possible cases is 6, {1, 2, 3, 4, 5, 6}, while the number of favorable cases is
    1, that is, {3}. So, P(3) =1/6 =0.166 =16.6 %.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The probability of an event *P(E)* always being number between 0 and 1 can
    be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2978ca2-9cda-4ab2-a605-3b5bf4ac782b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The extreme values are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: An event that has a probability of 0 is called an impossible event. Suppose
    we have six red balls in a bag; what is the probability of picking a black ball?
    The number of possible cases is 6; the number of favorable cases is 0 because
    there are no black balls in the bag. Hence, *P(E) = 0/6 = 0*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An event that has a probability of 1 is called a certain event. Suppose we have
    six red balls in a bag; what is the probability of picking a red ball? The number
    of possible cases is 6; the number of favorable cases is 6 because there are only
    red balls in the bag. Therefore, *P(E) = 6/6 = 1*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, we've talked about the likelihood of an event, but what happens when
    there's more than one possible event? Two random events, A and B, are independent
    if the probability of the occurrence of event A is not dependent on whether event
    B has occurred, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say we have two 52 decks of French playing cards. When
    extracting a card from each deck, the following two events are independent:'
  prefs: []
  type: TYPE_NORMAL
- en: '**E1**: The card that''s extracted from the first deck is an ace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E2**: The card that''s extracted from the second deck is a clubs card.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each can happen with the same probability, independent of the other's occurrence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversely, a random event, A, is dependent on another event, B, if the probability
    of event A depends on whether event B has occurred or not. Suppose we have a deck
    of 52 cards; by extracting two cards in succession without putting the first card
    back in the deck, the following two events are dependent:'
  prefs: []
  type: TYPE_NORMAL
- en: '**E1**: The first extracted card is an ace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E2**: The second extracted card is an ace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To be precise, the probability of E2 depends on whether or not E1 occurs, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The probability of E1 is 4/52.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of E2 if the first card was an ace is 3/51.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of E2 if the first card was not an ace is 4/51.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding joint probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s deal with the case of **joint probability**, both independent and
    dependent. Given two events, A and B, if the two events are independent (I mean
    the occurrence of one doesn''t affect the probability of the other), the joint
    probability of the event is equal to the product of the probabilities of A and
    B:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35371c3c-59bf-46c3-88b9-39e0a1ffff97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at an example. We have two decks of 52 cards. By extracting a card
    from each deck, let''s consider the two independent events:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A: The card that''s extracted from the first deck is an ace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'B: The card that''s extracted from the second deck is a clubs card.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the probability that both of them occur?
  prefs: []
  type: TYPE_NORMAL
- en: P(A) = 4/52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(B) = 13/52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: P(A ∩ B) = 4/52 * 13/52 = 52 /(52 * 52) = 1/52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the two events are dependent (that is, the occurrence of one affects the
    probability of the other), then the same rule may apply, provided that P(B|A)
    is the probability of event B given that event A has occurred. This condition
    introduces conditional probability, which we are going to dive into:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6885c500-059a-47c2-8a2b-f6124b7053a5.png)'
  prefs: []
  type: TYPE_IMG
- en: A bag contains two white balls and three red balls. Two balls are pulled out
    from the bag in two successive extractions without reintroducing the first ball
    that was pulled out of the bag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s calculate the probability that the two balls that were extracted were
    both white:'
  prefs: []
  type: TYPE_NORMAL
- en: The probability that the first ball is white is 2/5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability that the second ball is white, provided that the first ball
    is white, is 1/4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The probability of having two white balls is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P(two whites) = 2/5 * 1/4 = 2/20 = 1/10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding conditional probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it''s time to introduce you to the concept of conditional probability.
    The probability that event B occurs, calculated by the condition that event A
    occurred, is called conditional probability and is indicated by the symbol P(B
    | A). It is calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e116f0e-c9dc-431e-98bf-a68615d412aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we are able to understand the different kinds of probabilities, let''s
    apply them to the stochastic processes. Let''s start with the simplest type of
    probability, which is written in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/799e0042-af2a-4233-8709-e63c34c3410c.png)'
  prefs: []
  type: TYPE_IMG
- en: This represents the probability of observing the system in the state i at step
    n. In addition to these simple probabilities, we should be interested in the calculation
    of more complex probabilities involving multiple steps at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, it may be interesting to calculate the probability of being in
    state j at step n + 1, knowing that it is in state i at step n (as we can see,
    this is the conditional probability we defined previously):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1d2a758-1e1a-48a3-8413-3dcd5c52aa4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is called the transition probability from i to j at step n. Using the
    conditional probability definition, this rewrites itself, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40ae8605-8803-478d-b437-aad895b0a122.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, for this calculation, it is sufficient to know the a priori probability
    and the joint probability. To calculate more complex expressions, it is necessary
    to know the generic joint probabilities given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82c1f1ed-7411-4545-823c-5ab13248bb9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All of this occurs due to variations of all the i[0],...,i[n] in the set of
    integers Z. In a certain sense, these probabilities exhaust all possible information:
    the stochastic process is statistically determined when all the combined (discrete)
    densities are known, that is, the densities of all the multiple discrete variables
    (X[1], ..., X[n]) to the variation of all the i[0],...,i[n] Z. The calculation
    of these joint probabilities is a very difficult problem in general.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will delve deeper into the concepts behind the Markov
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Markov chains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Markov chain is a mathematical model of a random phenomenon that evolves over
    time in such a way that the past influences the future only through the present.
    The time can be discrete (whole variable), continuous (real variable), or a totally
    ordered whole. In this section, we will only consider discrete chains. Markov
    chains were introduced in 1906 by Andrei Andreyevich Markov (1856–1922), which
    is where the name is derived.
  prefs: []
  type: TYPE_NORMAL
- en: A Markov chain is a stochastic model that represents a sequence of possible
    cases in which the probability that each case occurs depends only on the state
    relative to the previous case. So, Markov chains have the **memorylessness** property.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a random process described by the sequence of random variables
    X = X[0], ..., X[n] which can assume values in a set, that is, j[0], j[1],…, jn.
    Let''s say that the process we are analyzing has the property of Markov if the
    evolution of the process in the future depends only on the value of the present
    state and not on the past history. In formulas, using the conditional probability,
    we will have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90728e8f-b785-4c4e-8df7-d5d489b3b59e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This relationship must apply to all the parameters if they are well-defined
    conditional probabilities. A discrete-time stochastic process X that has the Markov
    property is said to be a Markov chain. A Markov chain is said to be homogeneous
    if the transition probabilities are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc6a5e1b-a2bf-4cc4-b07a-fb4439356daa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This does not depend on n, but only on i and j. When this happens, we get the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8808b23b-c001-44e5-a120-6d720fcbf9ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can calculate all the joint probabilities by knowing the numbers p[ij] along
    with the following initial distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71bf319c-a4bb-4ed5-b61a-42d21032f74c.png)'
  prefs: []
  type: TYPE_IMG
- en: This probability is called the **distribution of the process over time zero**.
    The p[ij] probabilities are called transition probabilities, while p[ij] is the
    probability of a transition occurring from i to j in a time step.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the transition matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The study of homogeneous Markov chains becomes particularly simple and effective
    using matrix representation. In particular, the formula expressed by the previous
    proposition becomes much more readable. Due to this, the structure of a Markov
    chain can be completely represented by the following transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/954dc61a-be80-46e1-a782-232c21fb7278.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The properties of the transition probability matrices derive directly from
    the nature of the elements that compose them. In fact, by observing that the elements
    of the matrix are probabilities, they must have a value between 0 and 1\. So,
    this is a positive matrix in which the sum of the elements of each row is unitary.
    In fact, the elements of the i-th row are the probabilities that the chain, being
    in the state Si at the instant t, transits in S1 or in S2,... or in Sn at the
    next step. Such transitions are mutually exclusive and exhaustive of all possibilities.
    Such a matrix (positive with unit sum rows) is called **stochastic**. Therefore,
    we will need to define each positive row vector as stochastic, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41f49b00-7d14-4652-96e4-f1d28d06375d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this vector, the sum of the elements takes a unit value, as shown in the
    following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2a878d0-f75e-4bac-a8e2-0637ae862b12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will see that a particular form assumes this matrix in the case of
    a one-dimensional random walk. As shown in the following diagram, in a one-dimensional
    random walk, we study the motion of a point-like particle that''s constrained
    to move along a straight line in the two allowed directions (right and left).
    At each movement, it moves (randomly) one step to the right with a fixed probability
    p or to the left with a probability q, in such a way that p+q=1\. Each step is
    of equal length and independent of the others:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05a5cbc3-baa1-42bf-ba6a-e233ae3cec81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Suppose that the random variables Z[n] with n = 1,2, .. are independent and
    all have the same distribution. Due to this, the position of the particle instant
    n is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/abc6350b-0da2-4de5-b433-6dae2265a17d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, X[0] = 0\. The state space is S = (0, ±1, ±2,…). The X[n] process is
    a Markov chain because, to determine the probability that the particle is in a
    certain position the next moment, we just need to know where it is at the current
    moment, even if we are aware of where it was in all the moments before the current
    one. This concept can be expressed through the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e9936e0-7708-4d00-b3d9-c394ce87ce4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the Zn variables are independent. The transition matrix is a matrix with
    finite rows and as many columns, where 0 is on the main diagonal, p is on the
    diagonal above the main, q is on the diagonal below the main, and 0 is everywhere
    else, as shown in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e0dd7625-cb5b-4d05-8567-10c17c0a0a5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that this generalization greatly simplifies the problem at
    hand.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the transition diagram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A very intuitive alternative to the description of a Markov chain through the
    transition matrix is that of associating a Markov chain with an oriented graph
    (transition diagram). Here, the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: Vertices are labeled by the S1,S2,…, Sn states (or, briefly, from the indices
    1, 2, …, n of the states).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a directed edge that connects the vertex Si to the vertex Sj if, and
    only if, the probability of transition from Si to Sj is positive (this is the
    probability, which is, in turn, used as a label of the edge itself).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is clear that the transition matrix and transition diagram provide the same
    information about the same Markov chain. To understand this duality, we need to
    look at a simple example – consider a Markov chain with three possible states,
    that is, 1, 2, and 3, and the following transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5e49b40-837c-41ca-8d4d-2f85fdcdc1d6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The transition diagram for the newly introduced Markov chain can be seen in
    the following diagram. We can identify three possible states: 1, 2, and 3\. The
    two-state border contains the transition probabilities p[ij]. When there is no
    border between the two states, this means that the probability of transition is
    zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae10ccc2-02e9-40b9-8c06-f71fd00d925e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we can see that the arrows that come out of a state
    always sum up exactly at 1, just like what happens for every row in the transition
    matrix whose values must be added exactly to 1 – which represents the probability
    distribution. By comparing the transition matrix and the transition diagram, we
    can understand the duality between the two resources. As always, a diagram is
    much more explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will put what we've learned into practice by addressing
    a prediction problem with Markov chains.
  prefs: []
  type: TYPE_NORMAL
- en: Markov chain application – weather forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To apply what we''ve learned so far, we will look at a weather forecasting
    model based on Markov chains. To simplify this model, we will assume that there
    are only three states – rainy, cloudy, and sunny. Let''s also assume that we have
    made some calculations and discovered that tomorrow''s time is somehow based on
    today''s time, according to the following transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ecf6447-7b56-4345-81c2-92b7d03ba387.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each row must contain non-negative numbers and the sum of them must be equal
    to 1\. Recall that this matrix contains the conditional probabilities of the type
    expressed as *P (A | B)**,*, that is, the probability of *A* given *B*. So, this
    matrix contains the following conditional probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad894ba3-27ff-43c8-8c3c-f0016f338828.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ra*: Rainy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cl*: Cloudy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Su*: Sunny'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The weather conditions between two days are not necessarily correlated, so the
    process is Markovian.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, the following questions come to mind:'
  prefs: []
  type: TYPE_NORMAL
- en: If today is sunny, how can we calculate the probability that it is rainy in
    the next few days?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After a certain number of days, what will be the proportion of sunny and rainy
    days?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both questions, as well as many others that may come to mind, can be answered
    through the tools that make Markov chains available to us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the R code that allows us to alternate between sunny, cloudy,
    and rainy days, starting from a specific initial condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s analyze this code line by line:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first line loads the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind that if you need to install a library that isn't present in the
    initial distribution of R, you must use the `install.packages()` function. This
    function should be used just once and not every time the code is run.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to install the `markovchain` package, we should write the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This function downloads and installs packages from CRAN-like repositories or
    from local files. Instead, the load command must be used whenever the script is
    executed in a new session of R.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the markovchain package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `markovchain` package contains functions and S4 methods that we can use
    to create and manage discrete-time Markov chains. In addition to this, functions
    that we can use to perform statistical (fitting and drawing random variates) and
    probabilistic (analysis of their structural proprieties) analysis are provided.
    A brief description of the `markovchain` package, which can be extracted from
    the official documentation, is shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Version | 0.6.9.14 |'
  prefs: []
  type: TYPE_TB
- en: '|       Date | 2019-01-20 |'
  prefs: []
  type: TYPE_TB
- en: '| Maintainer | Giorgio Alfredo Spedicato |'
  prefs: []
  type: TYPE_TB
- en: '|    License | GPL-2 |'
  prefs: []
  type: TYPE_TB
- en: '|    Authors | Giorgio Alfredo Spedicato, Tae Seung Kang, Sai Bhargav Yalamanchi,
    Mildenberger Thoralf, Deepak Yadav, Ignacio Cordón, Vandit Jain, Toni Giorgino
    |'
  prefs: []
  type: TYPE_TB
- en: 'We will use this package in a variety of chapters in this book to demonstrate
    the usefulness of the features it provides. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue analyzing the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `set.seed()` command sets the seed of the R random number generator. This
    is necessary whenever we want to make the example reproducible. When `set.seed()`
    is used, the random numbers that are used in the algorithm will always be the
    same, so that a subsequent reproduction of the algorithm will provide the same
    results. Each seed value will correspond to a sequence of values that are generated
    for a given random number generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following line, we define the states of the weather condition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As shown here, only three states are provided: `Rainy`, `Cloudy`, and `Sunny`.
    At this point, we have to define the possible transitions of weather conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s move on and define the transition matrix according to what was established
    at the beginning of this section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Remember that this matrix contains the conditional probabilities of the type
    expressed as *P(A | B)*, that is, the probability of *A* given *B*. As we mentioned
    previously, the rows of this matrix add up to 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can create the `markovchain` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `markovchain` class has been designed to handle homogeneous Markov chain
    processes. The following slots are passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`transitionMatrix`: Square transition matrix containing the probabilities of
    the transition matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`states`: Name of the states. It must be the same as the colnames and rownames
    of the transition matrix. This is a character vector listing the states for which
    transition probabilities are defined.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`byrow`: Binary flag. A logical element indicating whether transition probabilities
    are shown by row or by column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: Optional character element to name the discrete-time Markov chains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To provide a summary of the model we''ve just created, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the dimensions of the object, the states, and the transition
    matrix are printed. To obtain this information individually, we can use some methods
    associated with the `markovchain` object:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to get the states of the `markovchain` object, we can use the
    `states` method, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the dimension of the `markovchain` object, we can use the `dim` method,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To see which elements are contained in the object we have created, we can use
    the `str()` function, which shows a compact view of the internal structure of
    an R object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, four slots are listed: `states`, `byrow`, `transitionMatrix`,
    and `name`. To retrieve the elements contained in each one, we can use the name
    of the object (`MarkovChainModel`), followed by the name of the slot, separated
    by the `@` symbol.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to print the transition matrix, we will write the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As we mentioned in the *Transition diagram* section, a very intuitive alternative
    to describing a Markov chain through the transition matrix is that of associating
    a Markov chain with an oriented graph (transition diagram).
  prefs: []
  type: TYPE_NORMAL
- en: Importing the diagram package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To plot a transition diagram, we can use the `diagram` package. This package
    contains several functions for visualizing simple graphs (networks) and plotting
    flow diagrams.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see a short description of the `diagram` package, which has been extracted
    from the official documentation, in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Version | 1.6.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Date | 2017-08-16 |'
  prefs: []
  type: TYPE_TB
- en: '| Maintainer | Karline Soetaert |'
  prefs: []
  type: TYPE_TB
- en: '| License | GPL-2 |'
  prefs: []
  type: TYPE_TB
- en: '| Authors | Karline Soetaert |'
  prefs: []
  type: TYPE_TB
- en: 'Now, let''s learn how to use the functions that are available in the package
    to create a diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create the `markovchain` object called `diagram`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5690e1e-7f36-4277-8385-7fa14b1e1fd3.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we can see that the arrows that come out of the states
    always sum up exactly to 1, just like what happens for every row in the transition
    matrix, whose values must add up to exactly 1\. This represents the probability
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining transition probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Something else that we can extract from the newly developed model is the transition
    probability, which represents the probability of passing from one state to another.
    Recall that a Markov chain is said to be homogeneous in time if the probabilities
    of the transition from one state to another are independent of the time index.
    To obtain this information, we will use the `transitionProbability()` function,
    which allows us to get the transition probabilities from initial to subsequent
    states. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to get this information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We can confirm this result by analyzing the transition matrix and the transition
    diagram. In the transition matrix, the transition from the `Sunny` state to the
    `Rainy` state is given by the element `p31`, which is equal to 0.1\. In the same
    way, in the transition diagram, the branch that leaves the `Sunny` state to arrive
    at the `Rainy` state has a value of 0.1.
  prefs: []
  type: TYPE_NORMAL
- en: After correctly setting up our Markov chain-based model, it's time to use it
    to make predictions. But first, we need to set the initial state. Let's say we're
    starting from the sunny (`Sunny`) condition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the vector containing the three states of our model, this condition
    is represented by the vector (0,0,1). We can set this value like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to calculate the state of time in 3 days, we can use a property
    of Markov chains. If X[n] is a homogeneous Markov chain with transition probability
    p[ij] and initial distribution p^([0]), then the following formula holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85823075-8c9a-49d5-bb41-a5fd1f0d3f9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This formula in vector terms becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db0b4f6a-900f-4cc0-a011-2cd0f71b561b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, p^([n]) and p^([0]) are row vectors and p^([0]) x
    P^([n]) represents a product between a row vector and a matrix (row by column
    product).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s write this product using our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In this way, we get a three-day forecast. To get a forecast for one week, we
    will write the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Something else that we can get from the model we developed is the stationary
    distribution. The stationary distribution of a Markov chain with transition matrix
    P is a vector, π, so that π⋅P = π (in other words, π is invariant by the matrix
    P.). π is a row vector whose entries are probabilities summing to 1\. This is
    a probability distribution that remains constant as the Markov chain evolves over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `markovchain` package has a specific function, called the `steadyStates()`
    function, to obtain the stationary distribution of the Markov chain. Let''s call
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s learn how to generate a forecast of the state of time for a
    whole year, day after day, starting from a specific state. To do this, we can
    use the `rmarkovchain()` function, which returns a sequence of states from homogeneous
    or nonhomogeneous Markov chains. Let''s do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following arguments are passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n`: Sample size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`object`: Either a `markovchain` or a `markovchainList` object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`t0`: The initial state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, we can extract forecasts for each day of next year. Let''s print
    the forecasts for the next 40 days:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The prediction sequence is well-defined, starting from the initial state.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will develop models that use a reward to extend the
    characteristics of a Markov chain.
  prefs: []
  type: TYPE_NORMAL
- en: Markov reward model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have dealt with Markov processes, random processes without memory,
    a sequence of random states that satisfy the Markov property, and much more. This
    process is defined through the space of the state S, and the transition function
    P, which determines its dynamics. In these models, there is no value associated
    with a specific state that allows us to reach a goal.
  prefs: []
  type: TYPE_NORMAL
- en: If we add a reward rate to each state, we get a Markov reward model, which represents
    a stochastic process that extends the characteristics of a Markov chain or a continuous-time
    Markov chain. The reward accumulated (R) over time is recorded in an additional
    variable. These concepts were introduced in [Chapter 2](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml),
    *Building Blocks of Reinforcement Learning.* Now, let's try to apply these concepts
    to a practical case of forest management.
  prefs: []
  type: TYPE_NORMAL
- en: Tiny forest management problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand these newly revised concepts, we will use an example from the
    `MDPToolbox` package (`mdp_example_forest`). This example deals with the management
    problem of a forest stand and has two main objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: The first objective is to maintain an old forest for wildlife.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second objective is to earn money by selling the cut wood.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To achieve these objectives, two actions are available: Wait or Cut. An action
    is decided for each 20-year period and applied at the beginning of the period.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three states are defined according to three tree age classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**state 1**: Age group 0-20 years'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**state 2**: Age group 21-40 years'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**state 3**: Age group over 40 years'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State 3 corresponds to the oldest age class. At the end of a period t, if the
    state is s and the Wait action is chosen, the state at the next period will be
    given by the minimum of the two following values ​​(s + 1, 3)  if a fire does
    not occur. This is because, if there are no fires, then the trees age, but can
    never take on a state higher than 3\. There is a chance that a fire will burn
    the forest down after the application of the action, bringing the whole population
    back into position in the range of a younger age (state 1).
  prefs: []
  type: TYPE_NORMAL
- en: Let's say p = 0.1 is the probability that a wildfire occurs during a period
    of time. The problem is how to manage this in the long term to maximize the reward. This
    problem can be treated as a Markov decision process.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the transition matrix P (s, s'', a). Remember that it tells
    us what the probabilities are of passing from one state to another. Since the
    available actions are (Wait, Cut), we will define two matrices of transitions.
    If we denote the probability of a fire with p, then we will have the following
    transition matrix relating to the choice of action 1 (Wait):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/047548c1-221a-47a9-ae90-20af3187227c.png)'
  prefs: []
  type: TYPE_IMG
- en: This is because, if we are in state 1, then we will have a probability of p
    remaining in that state (if a fire occurs) and the remaining 1-p probability of
    moving to the next state (if no fire occurs). While the probability of passing
    to state 3 is equal to 0, it isn't possible to pass from state 1 to 3 directly.
    On the other hand, if we are in state 2, we will have a probability of p passing
    into state 1 (if a fire occurs) and the remaining 1-p probability of passing to
    the next state, that is, 3 (if no fire occurs).
  prefs: []
  type: TYPE_NORMAL
- en: Here, the probability of remaining in state 2 is equal to 0. Finally, if we
    are in state 3, we will have a probability equal to p to go into state 1 (if a
    fire occurs) and the remaining 1-p probability to remain in state 3 (if no fire
    occurs) since this is the last to be possible over time. The probability of passing
    to state 2 is equal to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s define the transition matrix in terms of the choice of action 2
    (Cut):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22f260f3-5622-4d46-9fc2-b1f377a4ff75.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, its meaning is more intuitive when choosing to cut the wood. Here,
    the transition leads to state 1 in all three cases with a unit probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can define the two vectors of the rewards R (s, a):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ab5efeb-3f8d-48d7-823d-fda0b0c67128.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If the chosen action is to wait for the growth of the forest, then we will
    have 0 for the reward for the first two states and the maximum reward for state
    3\. In this case, we have chosen 4 as a reward, which represents the value that''s
    provided by the system by default. If the chosen action is to cut the wood instead,
    we will have the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd13cb68-1400-4a3e-b40c-47c0f438bbae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, if the chosen action is to cut the wood, then we will have the following
    rewards: 0 for state 1, 1 for state 2, and 2 for state 3.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to calculate a policy that allows us to get the maximum reward
    based on the settings we just developed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the code that allows us to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s analyze the code line by line to understand the meaning of each command.
    Let''s start by importing the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The `MDPtoolbox` package provides functions related to the resolution of discrete-time
    Markov decision processes, that is, finite horizon, value iteration, policy iteration,
    linear programming algorithms with some variants, and some functions related to
    reinforcement learning.  We can see a short description of the `MDPtoolbox` package,
    which can be extracted from the official documentation, in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Version | 4.0.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Date | 2017-03-02 |'
  prefs: []
  type: TYPE_TB
- en: '| Maintainer | Guillaume Chapron |'
  prefs: []
  type: TYPE_TB
- en: '| License | BSD_3_clause + file LICENSE |'
  prefs: []
  type: TYPE_TB
- en: '| Authors | Iadine Chades, Guillaume Chapron, Marie-Josee Cros, Frederick Garcia,
    Regis Sabbadin |'
  prefs: []
  type: TYPE_TB
- en: 'As we anticipated, the first thing we need to do is define the matrices P for
    the transition function and R for the reward function. First, we will use the
    data contained in the example supplied with the package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, simply invoke the example, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The object that we''ve created (a list, in this case) contains both the transition
    matrix P and the reward vectors. Let''s look at its content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'To extract the transition matrices, we can write the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'By doing this, we can see the two transition matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, `p = 0.1`. Here,  p represents the probability of a fire developing.
    Using this value, we can confirm the shape of the transition matrix. Let''s move
    on and view the reward vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Before developing the model, it is necessary to verify that P and R satisfy
    the criteria that are necessary for the problem to be of the MDP type. To do this,
    we''ll use the `mdp_check()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This function checks whether the MDP that''s defined by the transition probability
    array (P) and the reward array (R) is valid. If P and R are correct, the function
    returns an empty error message. If they aren''t correct, the function returns
    an error message describing the problem. The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the problem has been set. Now, we can search for the best
    policy for forest management.
  prefs: []
  type: TYPE_NORMAL
- en: Policy iteration algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned in [Chapter 2](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml),
    *Building Blocks of Reinforcement Learning*, policy iteration is a dynamic programming
    algorithm that uses a value function to model the expected return for each pair
    of action-state. We will apply this method to the case in question.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we will try to solve the problem at hand using the `mdp_policy_iteration()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This function solves discounted MDP with the policy iteration algorithm. As
    we mentioned in [Chapter 2](aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml), *Building
    Blocks of Reinforcement Learning*, `policy iteration` is a dynamic programming
    algorithm that uses a value function to model the expected return for each pair
    of action-state. These techniques update the value functions using the immediate
    reward and the (discounted) value of the next state in a process called bootstrapping.
    Therefore, they imply the storage of Q (s, a) in tables or with approximate function
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting from an initial P0 policy, the iteration of the policy alternates
    between the following two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Policy evaluation`: Given the current policy P, estimate the action-value
    function QP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Policy Improvement`: If we calculate a better policy P '' based on QP, then
    set P'' as the new policy and return to the previous step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the QP value function can be calculated exactly for each action-state pair,
    the policy iteration with the greedy policy improvement leads to convergence by
    returning the optimal policy. Essentially, repeatedly executing these two processes
    converges the general process toward the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following arguments are passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**P**: Transition probability array. P can be a three-dimensional array [S,S,A]
    or a list [[A]], with each element containing a sparse matrix [S,S].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**R**: Reward array. R can be a three-dimensional array [S,S,A] or a list [[A]],
    with each element containing a sparse matrix [S,S] or a two-dimensional matrix
    [S,A] that''s possibly sparse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**discount**: Discount factor. The discount is a real that belongs to ]0; 1[.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The policy iteration algorithm improves the policy iteratively using the evaluation
    of the current policy. Iterating is stopped when two successive policies are identical
    or when a specified number (`max_iter`) of iterations have been performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '**V:** Optimal value function. V is an S length vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**policy**: Optimal policy. The policy is an S length vector. Each element
    is an integer corresponding to an action that maximizes the value function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**iter**: Number of iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cpu_time**: CPU time used to run the program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that the model is ready, we just have to evaluate the results by checking
    the obtained policy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s learn how to extract these results from our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The first element we have visualized is the optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a value function represents how good a state is for an agent. It
    is equal to the total reward expected for an agent from the status s. The value
    function depends on the policy that the agent selects for the actions to be performed
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the policy that''s returned by the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Recall that a policy defines the behavior of the learning agent at a given time.
    It maps the detected states of the environment and the actions to take when they
    are in those states. This corresponds to what in psychology would be called a
    set of rules or associations of stimulus-response. The policy is the fundamental
    part of a reinforcing learning agent in the sense that it alone is enough to determine
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Here, the optimal policy is to not cut the forest in all three states. This
    is due to the low probability of developing a fire that causes the wait to be
    the best action to perform. In this way, the forest has time to grow and we can
    achieve both goals: maintain an old forest for wildlife and earn money by selling
    the cut wood.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can look at how many iterations the model has taken to converge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, it only takes two iterations to get the result. Finally, we
    want to see how much CPU time it took to process the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This first example allowed us to understand how easy it is to derive an optimal
    policy from a well-posed problem. Now, let's look at what happens when we modify
    the starting conditions of the system.
  prefs: []
  type: TYPE_NORMAL
- en: New state transition matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have seen that, when the probability of a fire developing is low,
    the optimal policy advises us to wait and not cut the forest. But what happens
    if the probability of developing a fire is higher? Here, we just need to change
    the problem settings by changing the probability value p. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code allows us to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s analyze the code line by line, focusing on the changes that are made
    to the initial code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the modified code. We haven''t used the values that are provided by
    the problem; instead, we have set new values. Remember that the syntax of the
    `mdp_example_forest()` function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The following arguments are passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S** (optional): Number of states. S is an integer greater than 0\. By default,
    S is set to 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**r1** (optional): The reward when the forest is in the oldest state and the
    Wait action is performed. r1 is a real greater than 0\. By default, r1 is set
    to 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**r2** (optional): The reward when the forest is in the oldest state and the
    Cut action is performed. r2 is a real greater than 0\. By default, r2 is set to
    2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**p** (optional): The probability of a wildfire occurring. p is a real in ]0,
    1[. By default, p is set to 0.1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, we have confirmed the three states, legally modified the rewards, and
    increased the probability that a fire will develop, bringing it from the initial
    value of 0.1 to the new value of 0.8.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what results we get by making this change:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see the two transition matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s move on and view the reward arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Before developing the model, it is necessary to verify that P and R satisfy
    the criteria for the problem so that they''re of the MDP type. To do this, we''ll
    use the `mdp_check()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the problem has been set, we can try to solve the problem using the `mdp_policy_iteration` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s extract the results from the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The first element we have visualized is the optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that a value function represents how good a state is for an agent. It
    is equal to the total reward that's expected for an agent from the status s. The
    value function depends on the policy that the agent selects for the actions to
    be performed on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'By comparing this with the results we obtained in the initial model, we can
    see that the total rewards have decreased considerably. Let''s look at the policy
    that''s returned by the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the optimal policy has changed. In this case, in states
    1 and 3, the advice to choose the wait action remains. However, in state 2, it
    is recommended to cut, to avoid losing the wood that's been obtained so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at how many iterations the model has to go through in order
    to converge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can see how much CPU time it took to process the program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: This example has shown us how to modify the parameters of the problem. Here,
    we can see that by increasing the probability of developing a fire, the optimal
    policy that's developed by the model changes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at stochastic processes and their applications. The
    theory of stochastic processes concerns the study of systems that evolve over
    time according to probabilistic laws. Due to this, we are interested in calculating
    the probabilities associated with it. For this reason, we learned about the basic
    concepts of probability. The a priori probability, joint probability, and conditional
    probability were all defined, followed by examples of how to calculate them.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we were introduced to Markov chains. A Markov chain is a mathematical
    model of a random phenomenon that evolves over time in such a way that the past
    influences the future through the present. In other words, it represents the stochastic
    description of a sequence of possible events. The probability of each event depends
    on the state that was reached in the previous event. Here, we learned how to define
    and read a transition matrix and a transition diagram. We used Markov chains for
    forecasting the weather conditions for 365 consecutive days. Finally, we saw how
    to use the `MDPtoolbox` package to calculate the optimal policy for managing a
    tiny forest.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the basic concepts of the multi-armed bandit
    model. We will discover the different techniques that we can use and the meaning
    of the action-value implementation. We will learn how to address a problem using
    a contextual approach and learn how to implement asynchronous actor-critic agents.
  prefs: []
  type: TYPE_NORMAL
