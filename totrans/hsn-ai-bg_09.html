<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep Learning for Intelligent Agents</h1>
                </header>
            
            <article>
                
<p>Intelligent Assistants are one of the most visible forms of <strong>Artificial Intelligence</strong> (<strong>AI</strong>) that we see in our daily lives. Siri, Alexa, and other systems have come to be commonplace in day-to-day life in the 21st century. This chapter will commence our section of chapters that dive deeply into the application of <strong>Artificial Neural Networks</strong> (<strong>ANNs</strong>)<strong> </strong>for creating AI systems. In this chapter, we will cover one new topic, word embeddings, and then proceed to focus on the application of <strong>Recurrent Neural Networks </strong>(<strong>RNNs</strong>) and generative networks to natural language processing tasks. While an entire book could have been written about deep learning for natural language processing, as is already the case, we'll touch ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be using Python 3 with a few standard python packages that you've seen before:</p>
<ul>
<li>Numpy</li>
<li>TensorFlow</li>
<li>A GPU-enabled computer, or an AWS account for cloud computing, as described in <a href="69346214-320e-487f-b4cf-bd5c469dc75e.xhtml" target="_blank">Chapter 3</a>, <em>Platforms and Other Essentials</em></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Word embeddings</h1>
                </header>
            
            <article>
                
<p>So far, in our discussion of AI and deep learning, we've focused a lot on how rooted this field is in fundamental mathematical principles; so what do we do when we are faced with an unstructured source data such as text? In the previous chapters, we've talked about how we can convert images to numbers via convolutions, so how do we do the same thing with text? In modern AI systems, we use a technique called <strong>word embedding</strong>.</p>
<p>Word embedding is not a class of predictive models itself, but a means of pre-processing text so that it can be an input to a predictive model, or as an exploratory technique for data mining. It's a means by which we convert words and sentences into vectors of numbers, themselves called <strong>word embeddings</strong></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Word2vec</h1>
                </header>
            
            <article>
                
<p>The Word2vec algorithm, invented by Tomas Mikolav while he was at Google in 2013, was one of the first modern embedding methods. It is a shallow, two-layer neural network that follows a similar intuition to the autoencoder in that network and is trained to perform a certain task without being actually used to perform that task. In the case of the Word2vec algorithm, that task is learning the representations of natural language. <span>You can think of this algorithm as a context algorithm – everything that it knows is from learning the contexts of words within sentences. It works off something called the <strong>distributional hypothesis</strong>, which tells us that the context for each word is found from its neighboring words. For instance, think about a corpus vector with 500 dimensions. Each word in the corpus is represented by a distribution of weights across every single one of those elements. It's not a one-to-one mapping; the embedding of each word is dependent upon every other word in the corpus. </span></p>
<p>In its basic form, Word2vec has a structure similar to many of the feedforward neural networks that we've already seen – it has an <strong>input layer</strong>, a <strong>hidden layer</strong>, and an <strong>output layer</strong>, all parameterized by a matrix <strong>W</strong>. It iterates through an input corpus word by word and develops vectors for that word. <span>The algorithm actually contains two distinct variations, the <strong>CBOW</strong> (<strong>continuous bag of words</strong>) model and the <strong>skip-gram model</strong></span><strong>, </strong>which handle the creation of word vectors differently. Architecturally, the skip-gram model and the CBOW model are essentially reversed versions of one another.</p>
<p>The skip-gram model is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1345 image-border" src="Images/e2b4386b-f828-4423-8204-767d1ccc04f2.png" style="width:13.58em;height:20.92em;" width="517" height="794"/></div>
<p>The mirror image of the skip-gram model is the CBOW model:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1344 image-border" src="Images/f3678806-81d0-4a61-8ced-3871d1de361d.png" style="width:14.67em;height:21.17em;" width="556" height="801"/></div>
<p>In the skip-gram model, the network looks at sequences of words and tries to predict the likelihood of a certain combination of words occurring. The skip-gram method predicts the context given a particular word. The model inputs a singular letter, <em>w,</em> and outputs a vector of words, <em>w<sub>1</sub>, w<sub>2</sub>... w<sub>n</sub></em>. Breaking down this process, the preceding model takes in a one-hot encoded representation of an input word during training. The matrix between the input layer and the hidden layer of the network represents the vocabulary that the network is building, the rows of which will become our vectors. Each row in the matrix corresponds to one word in the vocabulary. The matrix gets updated row by row with new embeddings as new data flows through the network. Again, recall how we are not actually interested in what comes out of the network; we are interested in the embeddings that are created in the matrix <em>W</em>. This method works well with small amounts of training data and is good at embedding rare words. In the CBOW method, the input to the model is <em>w<sub>1</sub>, w<sub>2</sub></em> ... <em>w<sub>n</sub></em>, the words surrounding the current word that the model is embedding. CBOW predicts the word given the context. The<strong> </strong>CBOW method is faster than the skip-gram method and is better at embedding frequent words, but it requires a great deal of data given that it relies on context as input. </p>
<p>To illustrate this further, take this simple sentence:</p>
<p class="packt_quote CDPAlignLeft CDPAlign"><em><span>The dog jumped over the fence</span></em></p>
<p>The skip-gram model parses the sentence by focusing on a subject, breaking the subject into chunks called <strong>grams</strong>, each time skipping as follows:</p>
<p>{<em>The dog jumped</em>, <em>The dog over</em>, <em>dog jumped over</em>, <em>dog jumped the</em> .... and so on}</p>
<p>On the other hand, under the CBOW method, the grams would iteratively move through the context of the sentence as follows:</p>
<p>{<em>The dog jumped</em>, <em>dog jumped over</em>, <em>jumped over the</em>, <em>over the fence</em>}</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training Word2vec models</h1>
                </header>
            
            <article>
                
<p>As Word2vec models are neural networks themselves, we train them just like a standard feedforward network with a loss function and stochastic gradient descent. During the training process, the algorithm scans over the input corpus and takes batches of it as input. After each batch, a loss is calculated. When optimizing, we want to minimize our loss as we would with a standard feedforward neural network.</p>
<p>Let's walk through how we would create and train a Word2vec model in TensorFlow:</p>
<ol>
<li>First, let's start with our imports. We'll use our standard <kbd>tensorflow</kbd> and <kbd>numpy</kbd> imports and the Python library itertools, as well as two utility functions from the machine learning package <kbd>scikit-learn</kbd>. The following code block shows ...</li></ol></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">GloVe</h1>
                </header>
            
            <article>
                
<p><strong><span> </span>Globalized Vectors</strong> (<strong>GloVe</strong>)<strong> </strong>was developed by the Stanford NLP group in 2014 as a probabilistic follow-up to Word2Vec. GloVe was designed to preserve the analogies framework used by Word2vec, but instead uses dimensionality reduction techniques that would preserve key statistical information about the words themselves. Unlike Word2vec, which learns by streaming sentences, GloVe learns embeddings by constructing a rich co-occurrence matrix. The co-occurrence matrix is a global store of semantic information, and is key to the GloVe algorithm.<span> The creators of GloVe developed it on the principle that </span>co-occurrence ratios between two words in a context<span><em> </em>are closely related to meaning.</span></p>
<p><span>So how does it work, and how is it different from Word2vec? GloVe creates a word embedding by means of the following:</span></p>
<ol>
<li>Iterating over a sentence, word by word</li>
<li>For each word, the algorithm looks at its context</li>
<li>Given the word and its context, GloVe creates a new entry in the co-occurrence matrix</li>
<li>GloVe then reduces the dimensions of the co-occurrence matrix to create embeddings</li>
<li>After creating the embedding, GloVe calculates its new loss function based on the accuracy of that embedding</li>
</ol>
<p><span>Let's walk through the GloVe algorithm alongside a Python implementation to see how it all pans out. We're going to be using the Cornell movie lines dataset, which contains over 200,000 fictional movie script lines. Later on, we'll use the embeddings we generated from this dataset in our intelligent agent. First, let's write a function to import the data:</span></p>
<pre>import os<br/><span> def loadText(fileName, fields):<br/> ''' Function to Load the Movie Lines text '''<br/>     lines = {}<br/>     with open(fileName, 'r', encoding='iso-8859-1') as f:<br/>     for line in f:<br/>         values = line.split(" +++$+++ ")<br/>         lineObj = {}<br/>         for i, field in enumerate(fields):<br/>             lineObj[field] = values[i]<br/>        lines[lineObj['lineID']] = lineObj<br/><br/> return lines</span></pre>
<p>We can then use this function to actually load the movie lines:</p>
<pre>lines = {}<br/>movie_lines = ["lineID","characterID","movieID","character","text"]<br/>lines = loadText("/users/patricksmith/desktop/glove/movie_lines.txt", movie_lines)</pre>
<p>Now, let's get back to GloVe. Unlike Word2vec, GloVe parses over a sentence word by word, focusing on local context by using a fixed context<strong> </strong>window size. In word embedding, the window size represents the extent to which and what an algorithm will focus on in order to provide context to a word's meaning. There are two forms of context window sizes in GloVe – symmetric and asymmetric. For example, take a look at the following sentence:</p>
<p class="packt_quote"><em>The horse ran fast across the finish line in the race. </em></p>
<p>With a symmetric window size, the algorithm would look at words on either side of the subject. If GloVe was looking at the word <em>finish</em> with a window size of 2 in the preceding example, the context would be <em>across the</em> and <em>line in</em>. Asymmetric windows look only at the preceding words, so the same window size of 2 would capture <em>across</em> <em>the</em>, but not <em>line in</em>.</p>
<p>Let's go ahead and initialize our GloVe class and variables:</p>
<pre>class GloVeModel():<br/>    def __init__(self, embedding_size, window_size, max_vocab_size=100000, min_occurrences=1,<br/>                 scaling_factor=3/4, cooccurrence_cap=100, batch_size=512, learning_rate=0.05):<br/>        self.embedding_size = embedding_size<br/>        if isinstance(context_size, tuple):<br/>            self.left_context, self.right_context = context_size<br/>        elif isinstance(context_size, int):<br/>            self.left_context = self.right_context = context_size<br/>       <br/>    <br/>        self.max_vocab_size = max_vocab_size<br/>        self.min_occurrences = min_occurrences<br/>        self.scaling_factor = scaling_factor<br/>        self.cooccurrence_cap = cooccurrence_cap<br/>        self.batch_size = batch_size<br/>        self.learning_rate = learning_rate<br/>        self.__words = None<br/>        self.__word_to_id = None<br/>        self.__cooccurrence_matrix = None<br/>        self.__embeddings = None<br/><br/>def fit_to_corpus(self, corpus):<br/>     self.__fit_to_corpus(corpus, self.max_vocab_size,             self.min_occurrences,<br/>     self.left_context, self.right_context)<br/>     self.__build_graph()<br/><br/>def __fit_to_corpus(self, corpus, vocab_size, min_occurrences, left_size, right_size):<br/>     word_counts = Counter()<br/>     cooccurrence_counts = defaultdict(float)<br/>     for region in corpus:<br/>         word_counts.update(region)<br/>         for l_context, word, r_context in _context_windows(region, left_size, right_size):<br/>             for i, context_word in enumerate(l_context[::-1]):<br/>                 cooccurrence_counts[(word, context_word)] += 1 / (i + 1)<br/>             for i, context_word in enumerate(r_context):<br/>                 cooccurrence_counts[(word, context_word)] += 1 / (i + 1)<br/>     if len(cooccurrence_counts) == 0:<br/>         raise ValueError("No coccurrences in corpus. Did you try to reuse a generator?")<br/>         self.__words = [word for word, count in word_counts.most_common(vocab_size)<br/>         if count &gt;= min_occurrences]<br/>     self.__word_to_id = {word: i for i, word in         enumerate(self.__words)}<br/>     self.__cooccurrence_matrix = {<br/>     (self.__word_to_id[words[0]], self.__word_to_id[words[1]]): count<br/>         for words, count in cooccurrence_counts.items()<br/>         if words[0] in self.__word_to_id and words[1] in self.__word_to_id}</pre>
<p>We end up with a co-occurrence matrix that can tell us how often certain words occur together given a certain window size:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1346 image-border" src="Images/b7e013cd-ab7d-48e2-8408-4036ffde9dc0.png" style="width:22.75em;height:17.58em;" width="696" height="537"/></div>
<p>While this table looks simple, it contains global statistical properties about the co-occurrence of the words within. From it, we can calculate the probabilities of certain words occurring together:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1347 image-border" src="Images/96b6ca1b-db98-48c2-8f14-3fab4807e402.png" style="width:30.33em;height:6.17em;" width="995" height="201"/></div>
<p>As the co-occurrence matrix is combinatorial in size (it can become large very quickly), it leads to extremely large matrices of co-occurrence information. How do we remedy this? We can factorize the matrix to create a lower-dimensional matrix where each row contains a vector representation of a given word. This performs a form of dimensionality reduction on the co-occurrence matrix. We then <span>pre-process the matrix by normalizing and log—smoothing the occurrence information. GloVe will learn vectors so that their differences predict occurrence ratios. </span>All of this will maintain rich global statistical properties while still preserving the analogies that make Word2vec so desirable.</p>
<p><span>GloVe is trained by minimizing a reconstruction loss that helps the model find the lower-dimensional representations that can explain the highest amount of variance in the original data. It</span> utilizes a least squares loss function that seeks to minimize the difference between the dot product of two embeddings of a word and the log of their co-occurrence count:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/423d6543-fc84-4df4-9bb2-da38dea74340.png" style="width:22.83em;height:3.92em;" width="3550" height="610"/></p>
<p>Let's break this down; <em>w</em><sub>i</sub> is a word vector and <span>b</span><sub>i</sub> is a bias factor for a specific word <em>i</em>, while <span>w</span><sub><em>j</em></sub> and <span>b</span><sub><em>j</em></sub> are the word vector and bias factor for the context vector. <em>X<sub>ij </sub></em>is the count from the co-occurrence matrix of how many times <em>i</em> and <em>j</em> occur together. <em>f</em> is a weighting function for both rare and frequent co-occurrences so that they do not skew the results. In all, this loss function looks at the weight co-occurrences of a word and its neighboring context words, and multiplies that by the right term, which computes a combination of the word, its contexts, biases, and co-occurrences. </p>
<p>Let's go ahead and initialize GloVe's graph in TensorFlow to proceed with the training process:</p>
<pre class="mce-root">def __build_graph(self):<br/> self.__graph = tf.Graph()<br/> with self.__graph.as_default(), self.__graph.device(_device_for_node):<br/> count_max = tf.constant([self.cooccurrence_cap], dtype=tf.float32,<br/> name='max_cooccurrence_count')<br/> scaling_factor = tf.constant([self.scaling_factor], dtype=tf.float32,<br/> name="scaling_factor")<br/><br/> self.__focal_input = tf.placeholder(tf.int32, shape=[self.batch_size],<br/> name="focal_words")<br/> self.__context_input = tf.placeholder(tf.int32, shape=[self.batch_size],<br/> name="context_words")<br/> self.__cooccurrence_count = tf.placeholder(tf.float32, shape=[self.batch_size],<br/> name="cooccurrence_count")<br/><br/> focal_embeddings = tf.Variable(<br/> tf.random_uniform([self.vocab_size, self.embedding_size], 1.0, -1.0),<br/> name="focal_embeddings")<br/> context_embeddings = tf.Variable(<br/> tf.random_uniform([self.vocab_size, self.embedding_size], 1.0, -1.0),<br/> name="context_embeddings")<br/><br/> focal_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),<br/> name='focal_biases')<br/> context_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),<br/> name="context_biases")<br/><br/> focal_embedding = tf.nn.embedding_lookup([focal_embeddings], self.__focal_input)<br/> context_embedding = tf.nn.embedding_lookup([context_embeddings], self.__context_input)<br/> focal_bias = tf.nn.embedding_lookup([focal_biases], self.__focal_input)<br/> context_bias = tf.nn.embedding_lookup([context_biases], self.__context_input)<br/><br/> weighting_factor = tf.minimum(<br/> 1.0,<br/> tf.pow(<br/> tf.div(self.__cooccurrence_count, count_max),<br/> scaling_factor))<br/><br/> embedding_product = tf.reduce_sum(tf.multiply(focal_embedding, context_embedding), 1)<br/><br/> log_cooccurrences = tf.log(tf.to_float(self.__cooccurrence_count))<br/><br/> distance_expr = tf.square(tf.add_n([<br/> embedding_product,<br/> focal_bias,<br/> context_bias,<br/> tf.negative(log_cooccurrences)]))<br/><br/> single_losses = tf.multiply(weighting_factor, distance_expr)<br/> self.__total_loss = tf.reduce_sum(single_losses)<br/> tf.summary.scalar("GloVe_loss", self.__total_loss)<br/> self.__optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(<br/> self.__total_loss)<br/> self.__summary = tf.summary.merge_all()<br/><br/> self.__combined_embeddings = tf.add(focal_embeddings, context_embeddings,<br/> name="combined_embeddings")</pre>
<p>Next, we'll write two functions to prepare the batches of data for the GloVe model, just as we did with Word2vec. Remember that all of this is still contained within our GloVe class: </p>
<pre>def batchify(batch_size, *sequences):<br/>     for i in range(0, len(sequences[0]), batch_size):<br/>         yield tuple(sequence[i:i+batch_size] for sequence in sequences) <br/><br/>def MakeBatches(self):</pre>
<pre>     ''' Make Batches of Data to Feed The Model'''<br/>     cooccurrences = [(word_ids[0], word_ids[1], count)<br/>     for word_ids, count in self.__cooccurrence_matrix.items()]<br/>         i_indices, j_indices, counts = zip(*cooccurrences)<br/>         return list(batchify(self.batch_size, i_indices, j_indices, counts))</pre>
<p>Now, we're going to understand different properties. For those of you that have used Java before, you are familiar with the concept of getters and setters. These methods enable the changes that can happen to a variable to be controlled. The <kbd>@property</kbd> decorator is Python's response to these, as follows:</p>
<pre>@property<br/>def foo(self): return self._foo <br/><br/>## Is the same as<br/><br/>def foo(self): <br/>    return self._foo<br/><br/>foo = property(foo)</pre>
<p>Here, the <kbd>foo</kbd> function is replaced by a new function, <kbd>property(foo)</kbd>, which is an object with special properties called <strong>descriptors</strong>. Now, let's return to Word2vec:</p>
<pre> @property<br/>     def vocab_size(self):<br/>         return len(self.__words)<br/><br/>     @property<br/>     def words(self):<br/>         if self.__words is None:<br/>             raise NotFitToCorpusError("Need to fit model to corpus before accessing words.")<br/>             return self.__words<br/><br/>     @property<br/>     def embeddings(self):<br/>     if self.__embeddings is None:<br/>         raise NotTrainedError("Need to train model before accessing embeddings")<br/>         return self.__embeddings<br/><br/>     def id_for_word(self, word):<br/>         if self.__word_to_id is None:<br/>             raise NotFitToCorpusError("Need to fit model to corpus before looking up word ids.")<br/>             return self.__word_to_id[word]</pre>
<p>We'll also create a function for the <kbd>ContextWindow</kbd> that tells GloVe which words to focus on:</p>
<pre> def ContextWindow(region, left_size, right_size):<br/>     <br/>    for i, word in enumerate(region):<br/>         start_index = i - left_size<br/>         end_index = i + right_size<br/>         left_context = _window(region, start_index, i - 1)<br/>         right_context = _window(region, i + 1, end_index)<br/>         yield (left_context, word, right_context)<br/><br/>## Function to Create the Window Itself<br/>def window(region, start_index, end_index):                   <br/>     last_index = len(region) + 1<br/>     selected_tokens = region[max(start_index, 0):min(end_index, last_index) + 1]<br/>     return selected_tokens</pre>
<p>Lastly, we'll write our function for training purposes: </p>
<pre class="mce-root">def train(self, num_epochs, log_dir=None, summary_batch_interval=1000):<br/>    <br/>    ## Initialize the total steps variable, which will be incrementally adjusted in training <br/>    total_steps = 0<br/><br/>    ## Start a TensorFlow session<br/>     with tf.Session(graph=self.__graph) as session:<br/>         if should_write_summaries:<br/>             summary_writer = tf.summary.FileWriter(log_dir, graph=session.graph)<br/>            ## Initialize the variables in TensorFlow<br/>             tf.global_variables_initializer().run()<br/><br/>         for epoch in range(num_epochs):<br/>             shuffle(batches)<br/><br/>         for batch_index, batch in enumerate(batches):<br/>             i_s, j_s, counts = batch<br/><br/>         if len(counts) != self.batch_size:<br/>             continue<br/>             feed_dict = {<br/>             self.__focal_input: i_s,<br/>             self.__context_input: j_s,<br/>             self.__cooccurrence_count: counts}<br/>             session.run([self.__optimizer], feed_dict=feed_dict)</pre>
<p>Finally, we can run our GloVe model with the following:</p>
<pre>model = GloVeModel(embedding_size=300, context_size=1)<br/>model.fit_to_corpus(corpus) <br/>model.train(num_epochs=100)</pre>
<p><span>GloVe's idea for dense matrices of co-occurrence information isn't new; it comes from a more traditional technique called </span><strong>latent semantic analysis</strong><span> (<strong>LDA</strong>) that learns embedding by decomposing bag-of-words term document matrices using a mathematical technique called </span><strong>singular value decomposition</strong> (<strong>SVD</strong>)<span>. </span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Constructing a basic agent</h1>
                </header>
            
            <article>
                
<p>The simplest way to construct an artificial assistant with TensorFlow is to use a <strong>sequence-to-sequence</strong> (<strong>Seq2Seq</strong>) model, which we learned in the chapter on RNNs.</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1348 image-border" src="Images/75eb64b2-8221-4770-be93-dc9377127796.png" style="width:40.42em;height:19.58em;" width="1023" height="495"/></div>
<p>While originally developed for neural machine translation, we can adjust this model to act as an intelligent chatbot for our own purposes. We'll create the <em>brain</em> behind our assistant as a Python class called <kbd>IntelligentAssistant</kbd>. Then, we'll create the training and chatting functions for our assistant:</p>
<ol>
<li>First, let's start with our standard imports and initialize our variables. Take special note of the <kbd>mask</kbd> variable here; <kbd>masks</kbd> are placeholders that allow ...</li></ol></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this section, we learned how to create novel, state-of-the-art intelligent assistants by using word embeddings and ANNs. Word embedding techniques are the cornerstone of AI applications for natural language. They allow us to encode natural language as mathematics that we can feed into downstream models and tasks.</p>
<p>Intelligent agents take these word embeddings and reason over them. They utilize two RNNs, an encoder and a decoder, in what is called a Seq2Seq model. If you cast your mind back to the chapter on recurrent neural networks, the first RNN in the Seq2Seq model encodes the input into a compressed representation, while the second network draws from that compressed representation to deliver sentences. In this way, an intelligent agent learns to respond to a user based on a representation of what it learned during the training process.</p>
<p>In the next chapter, we'll look into how we can create intelligent agents that can play board games.</p>


            </article>

            
        </section>
    </div>



  </body></html>