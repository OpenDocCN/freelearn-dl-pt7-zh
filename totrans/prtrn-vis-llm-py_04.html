<html><head></head><body>
		<div id="_idContainer033">
			<h1 id="_idParaDest-56" class="chapter-number"><a id="_idTextAnchor066"/>4</h1>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor067"/>Containers and Accelerators on the Cloud</h1>
			<p>In this chapter, you’ll learn how to containerize your scripts and optimize them for accelerators on the cloud. We’ll learn about a range of accelerators for foundation models, including trade-offs around cost and performance across the entire machine learning lifecycle. You’ll learn about key aspects of Amazon SageMaker and AWS to train models on accelerators, optimize performance, and troubleshoot common issues. if you’re already familiar with containers and accelerators on AWS, feel free to skip <span class="No-Break">this chapter.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>What are accelerators and why do they matter for <span class="No-Break">foundation models?</span></li>
				<li>Containerize your scripts for accelerators <span class="No-Break">on AWS</span></li>
				<li>Using accelerators with <span class="No-Break">Amazon SageMaker</span></li>
				<li>Infrastructure optimizations <span class="No-Break">on AWS</span></li>
				<li>Troubleshooting <span class="No-Break">accelerator performance</span></li>
			</ul>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor068"/>What are accelerators and why do they matter?</h1>
			<p>There’s something remarkable<a id="_idIndexMarker188"/> about human behavior. We care a lot about our own experiences. Many of the arts and sciences, particularly social science, specialize in quantifying, predicting, and understanding the implications and particularities of human behavior. One of the most obvious of these is human responses to technical performance. While this certainly varies among human groups, for the subset that chooses to spend a sizeable portion of their time interacting with technology, one theorem is self-evident. Faster and easier is <span class="No-Break">always better.</span></p>
			<p>Take video games, for example. While the 1940s and 50s saw some of the earliest video games, these didn’t come to massive popularity until arcade games such as <em class="italic">Pong</em> emerged in the early 70s. Perhaps<a id="_idIndexMarker189"/> unsurprisingly, this was nearly exactly the same time as the introduction of the original <strong class="bold">Graphics Processor Unit</strong> (<strong class="bold">GPU</strong>), in 1973 <em class="italic">(1)</em>! 1994 gave us the <em class="italic">PlayStation1</em> with its Sony GPU. As a child, I spent many hours loving the graphics performance on my Nintendo 64, with games such as <em class="italic">Zelda</em>, <em class="italic">Super Smash Brothers</em>, <em class="italic">Mario Kart 64</em>, and more! These days you only need to look at games such as <em class="italic">Roblox</em>, <em class="italic">League of Legends</em>, and <em class="italic">Fortnite</em> to understand how crucial graphics performance is to the success of the gaming industry. For decades, gaming has served as one of the most important signals in the market <span class="No-Break">for GPUs.</span></p>
			<p>Until machine learning, that is. In <a href="B18942_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, we learned about the ImageNet dataset and briefly introduced<a id="_idIndexMarker190"/> its 2012 champion, AlexNet. To efficiently train their model on the large ImageNet dataset, the authors used GPUs! At the time, the GPUs were quite small, only offering 3 GB of memory, so they needed to implement a <em class="italic">model parallel strategy</em>. This used two GPUs to hold the entire model in memory. These enhancements, in addition to other modifications such as using the ReLU activation function and overlapped pooling, led AlexNet to win the challenge by <span class="No-Break">a landslide.</span></p>
			<p>Since that achievement more than 10 years ago, most of the best machine learning models have used<a id="_idIndexMarker191"/> GPUs. From transformers to reinforcement learning, training to inference, and vision to language, the overwhelming majority of state-of-the-art machine learning models require GPUs to perform optimally. For the right type of processing, GPUs can be many orders of magnitude faster than CPUs. When training or hosting deep learning models, the simple choice of using GPUs or CPUs can frequently make a difference of hours to days in completing <span class="No-Break">a task.</span></p>
			<p>We know GPUs have a lot of promising benefits<a id="_idIndexMarker192"/> as compared<a id="_idIndexMarker193"/> to standard CPU processing, but how? What’s so different about them at a fundamental level? The answer may surprise you: <em class="italic">distribution</em>! Let’s take a look at this figure to understand the differences between CPUs <span class="No-Break">and GPUs:</span></p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B18942_Figure_04_01.jpg" alt="Figure 4.1 – Differences between CPU and GPU"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Differences between CPU and GPU</p>
			<p>CPUs have just a few cores<a id="_idIndexMarker194"/> but a lot of memory. This means<a id="_idIndexMarker195"/> they can do only a few operations at once, but they can execute these very quickly. Think of low latency. CPUs operate almost like a cache; they’re great at handling lots of tasks that rely <span class="No-Break">on interactivity.</span></p>
			<p>On the other hand, GPUs have thousands<a id="_idIndexMarker196"/> of cores. NVIDIA’s latest generation GH100 chips, for example, have 18,432 cores. This means they are excellent at processing many operations at once, such as matrix multiplication on the millions to billions of parameters in your neural networks. Think of <span class="No-Break">high throughput.</span></p>
			<p>Don’t we care about both low latency and high throughput? Yes, absolutely! This is why the majority of compute you work with today, from your cell phone to your laptop, your notebook instance to the fleet of instances you need to train that state-of-the-art model, use both CPUs and GPUs. The question <span class="No-Break">is, how?</span></p>
			<p>As you might imagine, writing a software program to successfully run complex operations across tens of thousands of microprocessors isn’t exactly the easiest thing in the world. This is why, in order to write code for a GPU, you need a specialized software framework purpose-built for hyper-distribution of operations. Enter <strong class="bold">CUDA</strong>, NVIDIA’s <strong class="bold">Compute Unified Device Architecture</strong>. CUDA abstracts away the orchestration<a id="_idIndexMarker197"/> of those underlying distributed microprocessors from the consumer, allowing them to leverage the massive distribution without needing to be an expert in its specific architecture. CUDA comes in two parts: drivers that work directly with the hardware, and a toolkit that exposes this hardware to developers. Python can work directly with CUDA, for example. PyTorch and TensorFlow interact with CUDA <span class="No-Break">as well.</span></p>
			<p>NVIDIA certainly isn’t the only vendor<a id="_idIndexMarker198"/> providing high-performance distributed microprocessors. Commonly called <strong class="bold">accelerators</strong>, GPU-like massively parallel processing units are available from Amazon (Inferentia and Trainium), Google (TPUs), Intel (Habna Gaudi), AMD (ROCm), and more. However, each of these requires specialized steps to utilize the underlying distributed hardware. While there are clear advantages when these apply to your use case, for the purposes of a beginner’s book, we’ll just stick with GPUs. We’ll dive into using Amazon’s accelerators, Trainium and Inferentia, in <a href="B18942_09.xhtml#_idTextAnchor138"><span class="No-Break"><em class="italic">Chapter 9</em></span></a><em class="italic">, Advanced </em><span class="No-Break"><em class="italic">Training Concepts</em></span><span class="No-Break">.</span></p>
			<p>Now you’ve been introduced to accelerators, let’s figure out how to <span class="No-Break">use them!</span></p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor069"/>Getting ready to use your accelerators</h1>
			<p>Let’s start with learning how to use<a id="_idIndexMarker199"/> <span class="No-Break">your accelerators:</span></p>
			<ol>
				<li><em class="italic">Step one: acquisition</em>. You definitely can’t train a model on a GPU without first getting your hands on at least one of the GPUs. Fortunately, there are a few free options for you. One of my projects at Amazon was actually writing the original doc for this: SageMaker Studio Lab! Studio Lab is one way to run a free Jupyter Notebook server in the cloud. If you’d like to use a no-cost notebook environment on CPUs or GPUs, store your files, collaborate with others, and connect to AWS or any other service, Studio Lab is a great way to <span class="No-Break">get started.</span></li>
				<li><em class="italic">Step two: containers</em>. Once you’re in a Jupyter notebook and are trying to run some example code, you’ll realize that everything hinges on installing the right packages. Even once you have the packages installed, connecting them to the GPU depends on the CUDA installation in your notebook. If the version of PyTorch or TensorFlow you’re trying to use doesn’t work nicely with that specific CUDA install, you’re out <span class="No-Break">of luck!</span></li>
			</ol>
			<p>This is why using the right container as your base image is the perfect way to start developing, especially for deep learning on GPUs. AWS, NVIDIA, PyTorch, and TensorFlow all provide base images you can use to start working with deep learning frameworks. At AWS, we have 70+ containers across multiple frameworks and key versions of these <em class="italic">(2)</em>. We provide these containers across CPU, GPU, training, hosting, SageMaker, and our <span class="No-Break">container services.</span></p>
			<p>What’s a container, you ask? Imagine writing<a id="_idIndexMarker200"/> a Python script with 5, 10, 15, or even more than 100 software packages. Installing all of those is really time-consuming and error-prone! Think how hard it is just to install one package successfully on your own; all of that complexity, time, and careful solution you found can literally be transferred anywhere you like. How? Through containers! Containers are a powerful tool. Learn how to make them <span class="No-Break">your friend.</span></p>
			<p>Now that you have some idea about working with containers, especially how they serve as the intermediary between your model and your GPUs, let’s talk through the options for where to run <span class="No-Break">your GPUs.</span></p>
			<p>Here, I’d like to emphasize that obviously, I’ve spent many years working at AWS. I love it! It’s been an amazing place for me to grow my skills, learn about the world, practice deep learning, serve customers, and collaborate with some amazing people. I’ve also spent many hours obsessing about the trade-offs in running compute on the cloud versus running compute on-premises. Actually, before working at AWS, I spent time at many different organizations: a few start-ups, a bank, a university, restaurants, policy organizations, and a non-profit. Each of these handled their compute <span class="No-Break">slightly differently.</span></p>
			<p>On the one hand, purchasing compute to store on-premises might seem like a safer bet upfront. Importantly, you’re actually getting something physical for your dollars! You don’t need to pay to use the machine, and it seems easier to secure. After all, you can just stash it under your desk. <span class="No-Break">What gives?</span></p>
			<p>There are five big problems with running <span class="No-Break">compute on-premises:</span></p>
			<ul>
				<li>First is <strong class="bold">logistics</strong>. Say you actually do buy some local servers<a id="_idIndexMarker201"/> with GPUs. Where would you put them? How would you connect them to your laptop? How would you keep them cold? Power them sufficiently? What would you do if the power spontaneously shut down in that room in the middle of your experiment? You also need to wait for the GPUs to physically arrive in the mail. Then you need to “rack and stack” the boxes, putting them into your growing local data center. Soon enough, these auxiliary tasks can become your full-time job, and you’ll need a team of people if you intend to run these<a id="_idIndexMarker202"/> for your <span class="No-Break">whole organization.</span></li>
				<li>Second is <strong class="bold">scale</strong>. Say you only buy eight GPUs upfront. You can<a id="_idIndexMarker203"/> run a handful of experiments, executing them one at a time. But what if you have a great idea about a new project you’d like to test out? If you have only your eight GPUs, you are physically bound by them and unable to run extra experiments elsewhere. At this point, most people simply move to acquire more GPUs to run their extra experiments, which leads to the <span class="No-Break">third problem.</span></li>
				<li>Third is <strong class="bold">under-utilization</strong>. When everyone goes home at night and isn’t training<a id="_idIndexMarker204"/> any models, what’s happening with those expensive GPUs? Probably nothing. They may be sitting totally unutilized. If you’re running some experiments overnight, or for multiple days /weeks, you may see higher levels of GPU utilization. However, it’s not at all uncommon for organizations to heavily invest in expensive GPU infrastructure only to see it actually go completely untouched by the very teams who requested it! Usually, you’ll see a tiny number of power users, with a very long tail of lightly interested parties who may log <span class="No-Break">in occasionally.</span></li>
				<li>Fourth is <strong class="bold">currency</strong>. Hardware updates, rapidly. Many companies<a id="_idIndexMarker205"/> are fully invested in releasing newer, faster, better versions of their hardware every year. Each year, you should expect to see a performance increase with the latest version. It’s not a great feeling to have made a big investment in on-premises GPUs, only to see them deprecated in a matter of months. It also introduces a risk to your experiments; you may not be able to produce state-of-the-art results if you’re not able to get the most performance out of your compute <span class="No-Break">budget possible.</span></li>
				<li>Fifth is <strong class="bold">carbon footprint</strong>. How much do you know about the type<a id="_idIndexMarker206"/> of energy supplying your grid? Is it sustainable? How much extra energy will all of those GPUs add to the carbon footprint of your community, not to mention your bill? Amazon is actually the largest corporate purchaser of renewable energy in the world. We are incredibly careful about the grids supplying our regions and can demonstrate that moving to the AWS cloud can reduce the carbon footprint of the average data center by up to 80%, with a goal of up to 96% once we’re powered with 100% <span class="No-Break">renewable energy.</span></li>
			</ul>
			<p>Another benefit<a id="_idIndexMarker207"/> of running your GPUs on Amazon SageMaker specifically is that <em class="italic">you are not paying for the instances if you’re not running a job</em>. When using SageMaker to train your machine learning models, the GPUs come online <em class="italic">only when you are training the model itself</em>. This means your overall GPU utilization is significantly better by moving to SageMaker, simply because of the system architecture. Most data centers aren’t built for the dynamism that deep learning training really requires, because when you aren’t training a model, the nodes are still running! The same logic holds for <span class="No-Break">Amazon EC2.</span></p>
			<p>Finally, you’ll need to make sure that your project actually uses the GPUs. This is much more complex than it sounds. First, the software framework itself needs to connect to the underlying CUDA kernels. Then, you’ll want to use some tooling to ensure that you keep the GPU utilization as high<a id="_idIndexMarker208"/> as possible. There are a variety of techniques that you’ll learn about later in the chapter to dive deeper into <span class="No-Break">these topics.</span></p>
			<p>Now that you’ve learned how to get ready to use your accelerators, let’s learn how to use them on <span class="No-Break">Amazon SageMaker!</span></p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor070"/>How to use accelerators on AWS – Amazon SageMaker</h2>
			<p>As you learned in the<a id="_idIndexMarker209"/> previous section, AWS is a great way to get your<a id="_idIndexMarker210"/> hands on GPUs without needing to provision, store, physically secure, and maintain GPUs. Now, we’ll take a look at an easy, efficient, and high-performance way to leverage GPUs on AWS – Amazon SageMaker. Let me be clear, SageMaker certainly is not the only way to run GPUs or accelerators on AWS. However, it is my personal favorite, so we’ll <span class="No-Break">start there.</span></p>
			<p>There are many books, blog posts, webinars, and re:Invent sessions dedicated to introducing and discussing SageMaker. I myself have a 16-video YouTube series you can use to learn more about it from me! However, for the purposes of this book, there are really<a id="_idIndexMarker211"/> three key pieces<a id="_idIndexMarker212"/> of SageMaker<a id="_idIndexMarker213"/> I want to you understand: <strong class="bold">Studio</strong>, <strong class="bold">Training</strong>, and <strong class="bold">Hosting</strong>. And each of these comes down to one single<a id="_idIndexMarker214"/> common <span class="No-Break">denominator: </span><span class="No-Break"><strong class="bold">instances</strong></span><span class="No-Break">.</span></p>
			<p><em class="italic">Instances</em> is the term we use<a id="_idIndexMarker215"/> to describe virtual machines at AWS. The service is called <strong class="bold">Elastic Compute Cloud</strong>, or <strong class="bold">EC2</strong>. Every time you turn on a virtual machine, we call it an <strong class="bold">instance</strong>. You may have worked with EC2 instances in the past, such as turning them on in the AWS console, SSHing into them, and trying to write some code. But didn’t you find it a little frustrating when you needed to change the size? What about downloading the logs or the output? Sharing your notebook? Not to mention your surprise at the bill when you forgot to turn <span class="No-Break">it off!</span></p>
			<p>Now, what if I told you there was an easy way to run notebooks, train models, and build business applications around your own data science work products, without you needing to manage really anything about that underlying infrastructure? You’d probably be <span class="No-Break">interested, right?</span></p>
			<p>That’s the core idea of SageMaker. We democratize<a id="_idIndexMarker216"/> machine learning by making it extremely<a id="_idIndexMarker217"/> easy for you to run your notebooks, models, jobs, pipelines, and processes from one single pane of glass. We also deliver extremely high performance at affordable prices. Let’s take a closer look at some of the key pieces <span class="No-Break">of SageMaker.</span></p>
			<h3>SageMaker Studio</h3>
			<p>SageMaker Studio is our flagship development environment<a id="_idIndexMarker218"/> that is fully integrated with machine learning. What I love most about Studio is that <em class="italic">we decouple the compute backing your user interface from the compute running your notebook</em>. That means AWS is managing a Jupyter Server on your behalf, per user, to host your visual experience. This includes a huge volume of features purpose-built for machine learning, such as Feature Store, Pipelines, Data Wrangler, Clarify, Model Monitor, <span class="No-Break">and more.</span></p>
			<p>Then, every time you create a new Jupyter notebook, <em class="italic">we run this on dedicated instances</em>. These are called <strong class="bold">Kernel Gateway Applications</strong>, and they let you seamlessly run many different<a id="_idIndexMarker219"/> projects, with different package requirements and different datasets, without leaving your IDE. Even better, the notebooks in Studio are incredibly easy to upgrade, downgrade, and change kernels. That means you can switch from CPU to GPU, or back, without very much<a id="_idIndexMarker220"/> <span class="No-Break">work interruption.</span></p>
			<h3>SageMaker Training</h3>
			<p>Now you have some idea<a id="_idIndexMarker221"/> about how to run a notebook on SageMaker, but what about a large-scale job with <span class="No-Break">distributed training?</span></p>
			<p>For this topic, we’ll need to introduce the second key pillar of SageMaker: <strong class="bold">Training</strong>. SageMaker Training lets you easily define job parameters, such as the instances you need, your scripts, package requirements, software versions, and more. Then, when you fit your model, we launch a cluster of remote instances on AWS to run your scripts. All of the metadata, package details, job output, hyperparameters, data input, and more are stored, searchable, and versioned by default. This lets you easily track your work, reproduce results, and find experiment details, even many months and years after your job <span class="No-Break">has finished.</span></p>
			<p>In addition, we’ve put a lot of muscle into updating our training backend platform to enable extreme-scale modeling. From data optimizations with FSx for Lustre to distributed libraries such as Model and Data Parallel, we’re enabling the next generation of large models across vision and text to train seamlessly on AWS. The next chapter covers this in more detail. Most of the GPUs we’ll analyze in this book come under <span class="No-Break">SageMaker Training.</span></p>
			<h3>SageMaker hosting</h3>
			<p>Lastly, you<a id="_idIndexMarker222"/> can also run GPUs on SageMaker hosting. This is valuable when you want to build a scalable REST API on top of your core model. You might use a SageMaker hosting endpoint to run a search experience, deliver questions and answers, classify content, recommend content, and so much more. SageMaker hosting supports GPUs! We’ll dive into this in more detail in <a href="B18942_12.xhtml#_idTextAnchor178"><span class="No-Break"><em class="italic">Chapter 12</em></span></a><em class="italic">, How to Deploy </em><span class="No-Break"><em class="italic">Your Model</em></span><span class="No-Break">.</span></p>
			<p>Now that you understand some of the key pillars of SageMaker, let’s break down this concept underlying all of <span class="No-Break">them: instances.</span></p>
			<h3>Instance breakdown for GPUs on SageMaker</h3>
			<p>As of November 2022, we support<a id="_idIndexMarker223"/> two primary instance<a id="_idIndexMarker224"/> families with GPUs on SageMaker, two custom accelerators, and Habana Gaudi accelerators. Here, I’ll break down how to understand the naming convention for all our instances, in addition to describing what you might use each of <span class="No-Break">these for.</span></p>
			<p>The naming convention for all instances is really three parts: fi<a id="_idTextAnchor071"/>rst, middle, and last. For example, <strong class="source-inline">ml.g4dn.12xlar<a id="_idTextAnchor072"/>ge</strong>. The <strong class="source-inline">ml</strong> part indicates that it’s actually a SageMaker instance, so you won’t see it in the EC2 control plane. <a id="_idTextAnchor073"/>The <strong class="source-inline">g</strong> part tells you what series of compute the instance<a id="_idTextAnchor074"/> is part of, especially the type of compute itself. Here, <strong class="source-inline">g</strong> indicates it’s a specific type of GPU accelerator: the <strong class="source-inline">g4</strong> instance has NVIDIA T4, and the <strong class="source-inline">g5</strong> has NVIDIA A10G. The number immediately after the letter is the version of this instance, with a higher number always being more recent. So, <strong class="source-inline">g5</strong> came out more recently than <strong class="source-inline">g4</strong>, and so on. The latest version of each instance will always give you better <span class="No-Break">price performance.</span></p>
			<p>Here, <strong class="source-inline">g4</strong> indicates<a id="_idIndexMarker225"/> you are using NVIDIA T4 GPUs. The letters after the nu<a id="_idTextAnchor075"/>mber tell you what else is available on that instance, in this case, <strong class="source-inline">d</strong> gives us <strong class="bold">directly attached instance storage</strong> through <strong class="bold">Non-Volatile Memory Express</strong> (<strong class="bold">NVME</strong>). The letter <strong class="source-inline">n</strong> is for <strong class="bold">networking optimized</strong>. All of them together are called the <strong class="bold">instance family</strong>. Everything after the instance family is the size of this particular instance. You might us<a id="_idTextAnchor076"/>e something<a id="_idIndexMarker226"/> quite small, such as <strong class="source-inline">ml.t3.medium</strong> to run your Jupyter notebook, but then upgrade to something large such as <strong class="source-inline">ml.g4dn.12xlarge</strong> for <a id="_idTextAnchor077"/>development, and ultimately, perhaps, <strong class="source-inline">ml.p4dn.24xlarge</strong> for <span class="No-Break">extreme-scale training.</span></p>
			<p>Generally speaking, the <strong class="source-inline">g</strong> instance is great for smaller model<a id="_idTextAnchor078"/>s. This could include development and testing for you, such as running a complex notebook on this, using warm pools, or simply a multi-GPU model training with data-parallel. The <strong class="source-inline">g5</strong> instance is especially <span class="No-Break">competitive here.</span></p>
			<p>However, if you want to train large language models, the <strong class="source-inline">p</strong> instance series is strongly recommended. This is because the GPUs are actually more performant, and also larger. They support larger models and larger batch sizes. <strong class="source-inline">ml.p4dn.24xlarge</strong>, with 8 NVIDIA A100s, has 40 GB of GPU memory per card. <strong class="source-inline">ml.g5.48xlarge</strong>, with 8 NVIDIA A10Gs, has only 24 GB of GPU memory <span class="No-Break">per card.</span></p>
			<p>As of this writing, Trainium has just become available! This is a custom accelerator developed by Amazon to deliver up to 50% better cost performance <span class="No-Break">for customers.</span></p>
			<p>Now that you’ve learned about<a id="_idIndexMarker227"/> how to use GPUs on AWS, especially<a id="_idIndexMarker228"/> on Amazon SageMaker, and which instances you want to stay on top of, let’s unpack how to optimize <span class="No-Break">GPU performance.</span></p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor079"/>Optimizing accelerator performance</h1>
			<p>There are two ways of approaching<a id="_idIndexMarker229"/> this, and both of them are important. The first is from a hyperparameter perspective. The second is from an infrastructure perspective. Let’s break <span class="No-Break">them down!</span></p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor080"/>Hyperparameters</h2>
			<p>All of <a href="B18942_07.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> is devoted to picking the right<a id="_idIndexMarker230"/> hyperparameters, and optimizing GPU performance<a id="_idIndexMarker231"/> is a large driver for that. Importantly, as the number of GPUs changes in your cluster, what we call your <strong class="bold">world size</strong>, you’ll need to modify your hyperparameters to accommodate that change. Also, there’s a core trade-off between increasing your overall job throughput, say by maxing out your batch size, and finding a smaller batch size, which ultimately will give you higher accuracy. Later in the book, you’ll learn how to use hyperparameter tuning to bridge <span class="No-Break">that gap.</span></p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor081"/>Infrastructure optimizations for accelerators on AWS</h2>
			<p>Here, you’re going to learn about five<a id="_idIndexMarker232"/> key topics that can determine<a id="_idIndexMarker233"/> how well your scripts use the GPU infrastructure available on AWS. At this point in your journey, I am not expecting you to be an expert in any of these. I just want to you know that they exist and that you may need to update flags and configurations related to them later in <span class="No-Break">your workflows:</span></p>
			<ul>
				<li><strong class="bold">EFA</strong>: Amazon’s <strong class="bold">Elastic Fabric Adapter</strong> is a custom networking solution on AWS that provides<a id="_idIndexMarker234"/> optimal scale for high-performance deep learning. Purpose-built for the Amazon EC2 network topology, it enables seamless networking scale from just a few to a few hundred to thousands of GPUs <span class="No-Break">on AWS.</span></li>
				<li><strong class="bold">Nitro</strong>: Amazon’s custom purpose-built hypervisor <a id="_idIndexMarker235"/>system decouples physical hardware, CPU virtualization, storage, and networking that provides heightened security and faster innovation. You’re using the Nitro system on SageMaker on many of the large GPU<a id="_idIndexMarker236"/> instances, such as <strong class="source-inline">ml.p4d.24xlarge</strong>, <strong class="source-inline">ml.p3dn.24xlarge</strong>, <strong class="source-inline">ml.g4dn.12xlarge</strong>, <span class="No-Break">and more.</span></li>
				<li><strong class="bold">NCCL</strong>: <strong class="bold">NVIDIA Communication Collectives Library</strong> is a tool you will want to gain some familiarity<a id="_idIndexMarker237"/> with when you’re ready to actually improve your GPU performance. A common step is to ensure that you’re using the latest version of NCCL. There are five key algorithms that NCCL provides: <strong class="source-inline">AllReduce</strong>, <strong class="source-inline">Broadcast</strong>, <strong class="source-inline">Reduce</strong>, <strong class="source-inline">AllGather</strong>, and <strong class="source-inline">ReduceScatter</strong>. Most distributed training software frameworks you will work with use a combination of these or custom implementations of these algorithms in a variety of ways. <a href="B18942_05.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> is devoted to exploring this in more detail. Another key NVIDIA library to know about is CUDA, as mentioned previously, which lets you run your deep learning framework software on <span class="No-Break">the accelerators.</span></li>
				<li><strong class="bold">GPUDirectRDMA</strong>: This is an NVIDIA tool that allows GPUs to communicate directly with each other on the same<a id="_idIndexMarker238"/> instance without needing to hop onto the CPU. This is also available on AWS with <span class="No-Break">select instances.</span></li>
				<li><strong class="bold">Open MPI</strong>: <strong class="bold">Open Message Passing Interface</strong> is an open source project that enables<a id="_idIndexMarker239"/> remote machines to easily communicate with each other. The vast majority of your distributed training workloads, especially those that run on SageMaker, will use MPI as a base communication layer for the various workers to stay in sync with <span class="No-Break">each other.</span></li>
			</ul>
			<p>If you’re thinking, "<em class="italic">Now, how do I go use all of these things?",</em> the answer<a id="_idIndexMarker240"/> is usually pretty simple. It’s three<a id="_idIndexMarker241"/> things, <span class="No-Break">as follows:</span></p>
			<ol>
				<li>First, ask yourself, <em class="italic">which base container am I using</em>? If you’re using one of the AWS deep learning containers, then all of these capabilities will be provided to you after our extensive tests <span class="No-Break">and checks.</span></li>
				<li>Second, take a look at which instance you are using. As you learned previously, each instance type opens up your application to using, or not using, certain features on AWS. Try to make sure you’re getting the best performance <span class="No-Break">you can!</span></li>
				<li>Third, look at ways to configure these in your job parameters. In SageMaker, we’ll use hyperparameters and settings in your scripts to ensure you’re maxing <span class="No-Break">out performance.</span></li>
			</ol>
			<p>Now that you’ve learned<a id="_idIndexMarker242"/> a bit about optimizing<a id="_idIndexMarker243"/> GPU performance, let’s take a look at <span class="No-Break">troubleshooting performance.</span></p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor082"/>Troubleshooting accelerator performance</h1>
			<p>Before we can analyze our GPU<a id="_idIndexMarker244"/> performance, we need to understand generally how to debug and analyze performance on our training platform. SageMaker has some really nice solutions<a id="_idIndexMarker245"/> for this. First, all of your logs are sent to <strong class="bold">Amazon CloudWatch</strong>, another AWS service that can help you monitor your job performance. Each node in your cluster will have a full dedicated log stream, and you can read that log stream to view your overall training environment, how SageMaker runs your job, what status your job is in, and all of the logs your script emits. Everything you write to standard out, or print statements, is automatically captured and stored in CloudWatch. The first step to debugging your code is to take a look at the logs and figure out what really <span class="No-Break">went wrong.</span></p>
			<p>Once you know what’s wrong in your script, you’ll probably want to quickly fix it and get it back<a id="_idIndexMarker246"/> online, right? That’s why we introduced <strong class="bold">managed warm pools</strong> on SageMaker, a feature that keeps your training cluster online, even after a job has finished. With SageMaker warm pools, you can now run new jobs on SageMaker Training in just a <span class="No-Break">few seconds!</span></p>
			<p>With a script working, next, you’ll need to analyze the overall performance of your job. This is where debugging tools come in really handy. SageMaker offers a debugger and profiler, both of which actually spin up remote instances while your job is running to apply rules and check on your tensors throughout the training process. The profiler is an especially nice tool to use; it automatically generates graphs and charts for you, which you can use to assess the overall performance of your job, including which GPUs are being utilized, and how much. NVIDIA also offers tooling for GPU debugging <span class="No-Break">and profiling.</span></p>
			<p>As we mentioned before, writing software to seamlessly orchestrate tens of thousands of GPU cores is no small task. And as a result, it’s really common for GPUs to suddenly go bad. You might see NCCL errors, CUDA errors, or other seemingly unexplainable faults. For many of these, SageMaker actually runs GPU health checks ahead of time on your behalf! This is why the <strong class="source-inline">p4d</strong> instances take much longer to initialize than the smaller instances; we are analyzing the health of the GPUs prior to exposing them <span class="No-Break">to you.</span></p>
			<p>Outside of these known GPU-centric issues, you may see other faults such as your loss not decreasing or suddenly exploding, insufficient capacity, oddly low GPU throughput, or small changes in the node topology. For many of these, it’s common to implement a <strong class="bold">Lambda function</strong> in your account to monitor your job. You<a id="_idIndexMarker247"/> can use this Lambda function to analyze your Cloudwatch logs, trigger alerts, restart a job, <span class="No-Break">and more.</span></p>
			<p>Just remember to <em class="italic">checkpoint your model at least every 2 to 3 hours</em>. We’ll cover most of these best practices for training at scale on SageMaker in the coming chapters, but for now, simply know that you need to write a full copy of your most recently trained model with some regularity throughout the <span class="No-Break">training loop.</span></p>
			<p>Now that you’ve learned<a id="_idIndexMarker248"/> about some techniques for troubleshooting GPU performance, let’s wrap up everything you’ve just learned in <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor083"/>Summary</h1>
			<p>In this chapter, we introduced accelerators for machine learning, including how they are different from standard CPU processing and why you need them for large-scale deep learning. We covered some techniques for acquiring accelerators and getting them ready for software development and model training. We covered key aspects of Amazon SageMaker, notably Studio, Training, and hosting. You should know that there are key software frameworks that let you run code on GPUs, such as NCCL, CUDA, and more. You should also know about the top features that AWS provides for high-performance GPU conception to train deep learning models, such as EFA, Nitro, and more. We covered finding and building containers with these packages preinstalled, to successfully run your scripts on them. We also covered debugging your code on SageMaker and troubleshooting <span class="No-Break">GPU performance.</span></p>
			<p>Now that we’ve learned about GPUs in some detail, in the next chapter, we’ll explore the fundamentals of <span class="No-Break">distributed training!</span></p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor084"/>References</h1>
			<p>Please go through the following content for more information on a few topics covered in <span class="No-Break">the chapter:</span></p>
			<ol>
				<li>A micro-controlled peripheral <span class="No-Break">processor: </span><a href="https://dl.acm.org/doi/10.1145/800203.806247"><span class="No-Break">https://dl.acm.org/doi/10.1145/800203.806247</span></a></li>
				<li><em class="italic">AWS, deep </em><span class="No-Break"><em class="italic">learning</em></span><span class="No-Break">: </span><a href="https://github.com/aws/deep-learning-containers"><span class="No-Break">https://github.com/aws/deep-learning-containers</span></a></li>
			</ol>
		</div>
	</body></html>