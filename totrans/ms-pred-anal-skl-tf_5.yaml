- en: Predictive Analytics with TensorFlow and Deep Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is an open source library developed by **Google Brain Team**. It
    is used in large-scale machine learning applications, such as neural networks,
    and for making numerical computations. Developers are able to create dataflow
    graphs using TensorFlow. These graphs show the movement of data. TensorFlow can
    be used to train and run deep neural networks for various applications such as
    image recognition, machine language translation, and natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: We already know that predictive analytics is about providing predictions about
    unknown events. We are going to use it here with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Predictions with TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression with **Deep Neural networks** (**DNNs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification with DNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictions with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will perform the `hello world` example of deep learning. This example is
    used to check and ensure that a model is working as intended. For this, we will
    use the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MNIST stands for **Mixed National Institute of Standards and Technology**,
    which has produced a handwritten digits dataset. This is one of the most researched
    datasets in machine learning, and is used to classify handwritten digits. This
    dataset is helpful for predictive analytics because of its sheer size, allowing
    deep learning to work its magic efficiently. This dataset contains 60,000 training
    images and 10,000 testing images, formatted as 28 x 28 pixel monochrome images.
    The following screenshot shows the images contained in this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29266a1b-4e37-493c-b841-555f004034c3.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, we can see that, for every handwritten digit, there
    is a corresponding true label; we can therefore use this dataset to build classification
    models. So we can use the image to classify each into one of the 10 digits from
    0 to 9.
  prefs: []
  type: TYPE_NORMAL
- en: Building classification models using MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following steps and learn to build a classification
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to import the libraries that we will use in this dataset. Use the following
    lines of code to import the `tensorflow`, `numpy`, and `matplotlib` libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will import the `fully_connected` function, which we will be used to build
    the layers of our network, from `tensorflow.contrib.layers`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Elements of the DNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before running the model, we first have to determine the elements that we will
    use in building a multilayer perceptron model. Following are the elements that
    we will use in this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture**: The model contains 728 neurons in the input layer. This is
    because we have 28 images and each image has 28 pixels. Here, each pixel is a
    feature in this case, so we have 728 pixels. We will have 10 elements in the output
    layer, and we will also use three hidden layers, although we could use any number
    of hidden layers. Here, we will use three hidden layers. The number of neurons
    we will use in each layer is 350 in the first layer, 200 in the second one, and
    100 in the last layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function**: We will use the ReLU activation function, as shown
    in the following code block:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If the input is negative, the function outputs `0`, and if the input is positive
    the function just outputs the same value as the input. So, mathematically, the
    ReLU function looks similar to this. The following screenshot shows the lines
    of code used for generating the graphical representation of the ReLU activation
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5bfc50a-2e28-44e0-8cf2-294a24807836.png)'
  prefs: []
  type: TYPE_IMG
- en: It gains the maximum between `0` and the input. This activation function will
    be used in every neuron of the hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizing ****algorithm**: The optimizing algorithm used here is the gradient
    descent with a learning rate of 0.01.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**: For the `loss` function, we will use the `cross_entropy`
    function, but as with other loss functions that we have used in this book, this
    function measures the distance between the actual values and the predictions that
    the model makes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weights initialization strategy**: For this, we will use the Xavier initializer, a
    method that actually comes with the `fully_connected` function from TensorFlow
    as a default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization strategy**: We are not going to use any regularization strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training strategy**: We are going to use 20 epochs. The dataset will be presented
    to the network 20 times, and in every iteration, we will use a batch size of 80\.
    So, we will present the data to the network 80 points at a time and the whole
    dataset 20 times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the DNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will import the dataset that we are going to use. The reason for using
    this dataset is that it is easily available. We are going to actually use this
    dataset and build a DNN model around it. In the next sections, we will see the
    steps involved in building a DNN model.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we read data in the cell. The following screenshot shows the lines of
    code used to read the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3aaf54f-010c-4f34-a41a-d7f11fef7522.png)'
  prefs: []
  type: TYPE_IMG
- en: Defining the architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use three hidden layers, with 256 neurons for the first layer, 128
    for the second, and 64 for the third one. The following code snippet shows the
    architecture for the classification example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Placeholders for inputs and labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The values of different layers are the objects, and are also called placeholders
    for inputs and labels. These placeholders are used for feeding the data into the
    network. The following lines of code are used for showing placeholders for the
    inputs and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So we have a placeholder `X` for the features, which is the input layer, and
    we have a placeholder `y` for the target value. So this object will contain the
    actual true labels of the digits.
  prefs: []
  type: TYPE_NORMAL
- en: Building the neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For building DNNs, we use the `fully_connected` function for the first hidden
    layer. The input for this hidden layer is `x`, which is the data from the placeholder. `n_hidden1`
    is the number of neurons that we have in this hidden layer, which you will remember
    is 350 neurons. Now, this hidden layer 1 becomes the input for the hidden layer
    2, and `n_hidden2` is the number of neurons in this layer. Likewise, hidden layer
    2 becomes the input for the third hidden layer and we will use this number of
    neurons in this layer. Finally, the output layer, which we will call `logits`,
    is the fully connected layer that we use as input, hidden layer 3\. The following
    screenshot shows the lines of code used for building the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02a2f19c-c256-420c-9373-11069daa5387.png)'
  prefs: []
  type: TYPE_IMG
- en: We enter the output as 10 because we have 10 categories in our classification
    problem and we know that in the output layer we don't use any activation function.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For our `loss` function, we use the cross-entropy function. TensorFlow provides
    us with many such functions. For example, in this case, we are using the `sparse_softmax_cross_entropy
    _with_logits` function because here we got `logits` from the network. So, in this
    function, we pass the actual labels. These are the true labels, which are `logits`—the
    results or the output of our network. The following screenshot shows the lines
    of code used for showing the use of the `reduce_mean` function with this cross-entropy
    for getting the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73884877-d7ad-4b6c-90d8-2b054d336ead.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, using this cross-entropy, we can calculate the loss as the mean of the
    vector that we will get here. So this is the `loss` function and the mean of the
    cross-entropy.
  prefs: []
  type: TYPE_NORMAL
- en: Defining optimizer and training operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of the optimizer is to minimize the loss, and it does this by adjusting
    the different weights that we have in all the layers of our network. The optimizer
    used here is the gradient descent with a learning rate of `0.01`. The following
    screenshot shows the lines of code used for defining the optimizer and also shows
    the training operations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2861b9b5-3425-4cbc-917c-34c6bebfb8fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Each time we run the training operation `training_op`, the optimizer will change
    the values of these weights a little bit. In doing so, it minimizes the loss,
    and the predictions and the actual values are as close as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Training strategy and valuation of accuracy of the classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here we set the training strategy. We will use 20 epochs with a batch size
    of 80\. In all of these cells, we have build the computational graph that will
    be used in this program. The following screenshot shows the lines of code used
    for showing the training strategy and the couple of nodes for evaluating the accuracy
    of the classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c059d22e-2a90-4e34-94ba-d7f806e01d5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the computational graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For actually running the computational graph, first we will initialize all
    the variables in our program. The following screenshot shows the lines of code
    used for running the computational graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfff9daa-7bfb-4207-bb88-7419a4c8654a.png)'
  prefs: []
  type: TYPE_IMG
- en: In line 3, we initialize all the variables in our program. Now, here, we don't
    have any variables explicitly. However, the variables that are inside are fully
    connected. The `fully_connected` function is where we have all the hidden layers
    that contain the weights. These are the variables which is why we must initialize
    the variables with the `global_ variables_initializer` object and run this node.
    For each epoch, we run this loop 20 times. Now, for each iteration that we have
    in the number of examples over the batch size, which is 80, we get the values
    for the features and the targets. So this will be 80 data points for each iteration.
    Then, we run the training operation and will pass as `x`; we will pass the feature
    values and here we will pass the target values. Remember, `x` and `y` are our
    placeholders. Then, we evaluate the accuracy of the training and then evaluate
    the accuracy in the testing dataset, and we get the testing dataset. We get from
    `mnist.test.images`, and so these are now the features and `test.labels` are the
    targets. Then, we print the two accuracies after these two loops are completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then produce some individual predictions for the first 15 images in the
    testing dataset. After running this, we get the first epoch, with a training accuracy
    of 86 percent and a testing accuracy of 88-89 percent. The following screenshot
    shows the results of training and the testing results for different epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16acb53f-7d10-4c38-9d16-273f6bd7b386.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The programs takes a little bit of time to run, but after 20 epochs, the testing
    accuracy is almost 97 percent. The following screenshot shows the actual labels
    and the predicted labels. These are the predictions the network made:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/affdeb3b-3f28-4b4b-a801-43210ca51816.png)'
  prefs: []
  type: TYPE_IMG
- en: So we have built our first DNN model and we were able to classify handwritten
    digits using this program with almost 97 percent accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Regression with Deep Neural Networks (DNN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For regression with DNNs, we first have to import the libraries we will use
    here. We will import TensorFlow, pandas, NumPy, and matplotlib with the lines
    of code shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2622fc4-ef72-4d49-ac2e-025f5c814142.png)'
  prefs: []
  type: TYPE_IMG
- en: We will use the `fully_ connected` function from the `tensorflow.contrib.layers` model.
  prefs: []
  type: TYPE_NORMAL
- en: Elements of the DNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before running the model, we first have to determine the elements that we will
    use in building a multilayer perceptron model, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture:** The model contains 23 elements in the input layer, hence
    we have 25 features in this dataset. We have only one element in the output layer
    and we will use three hidden layers, although we could use any number of hidden
    layers. We will use 256 neurons for the first layer, 128 for the second, and 64
    for the third one. These are the powers of two.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function:** We will choose the ReLu activation function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizing a****lgorithm**: The optimization algorithm used here is the Adam
    optimizer. The Adam optimizer is one of the most popular optimizers as it is the
    best option for a lot of problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**: We will use the mean squared error because we are doing
    a regression problem here and this is one of the optimal choices for the `loss`
    function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weights initialization strategy:** For this, we will use the Xavier initializer,
    which comes as the default that with the `fully_connected` function from TensorFlow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization strategy**: We are not going to use any regularization strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training strategy**: We are going to use 40 epochs. We will present the dataset
    40 times to the network and, in every iteration, we will use batches of 50 data
    points each time we run the training operation. So, we will use 50 elements of
    the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the DNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we import the dataset that we will use. The reason behind using this
    dataset is that, it is easily available. The following are the steps involved
    in building a DNN model.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to read data in the cell and filter it to our preference. The
    following screenshot shows the lines of code used to read the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60d20624-6098-4ad9-8d91-a1b138c925b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Objects for modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After importing the datasets, we prepare the objects for modeling. So we have
    training and testing here for `x` and for `y`. The following screenshot shows
    the lines of code used to prepare the objects for modelling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9278a94-5327-4ee6-bd04-3a925170420e.png)'
  prefs: []
  type: TYPE_IMG
- en: Training strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the training strategy with 40 epochs and a batch size of 50\. This
    is created with the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Input pipeline for the DNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since this is an external dataset, we have to use a data input pipeline, and
    TensorFlow provides different tools for getting data inside the deep learning
    model. Here, we create a dataset object and an iterator object with the lines
    of code shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/edc095ad-d63a-4b69-87c3-3d262b2e7345.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we produce the dataset object. Then, we pass the whole training dataset
    to some placeholders that we will use. Then, we shuffle the data and divide or
    partition the training dataset into batches of 50\. Hence, the dataset object
    is prepared, containing all of the training samples partitioned into batches of
    size 50\. Next, we make an iterator object. Then, with the `get_next` method,
    we create a node called `next_element`, which provides the batches of 50 from
    the training examples.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use three hidden layers with 256 neurons for the first layer, 128 for the
    second, and 64 for the third one. The following code snippet shows the architecture
    for this procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Placeholders for input values and labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The values of different layers are the objects, also called the placeholders,
    for inputs and labels. These placeholders are used for feeding the data into the
    network. The following lines of code shows the placeholders for inputs and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Building the DNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For building the following example, we first have to define the `DNN` function.
    This function will take `X_values` and output the predictions. For the first hidden
    layer, we use a `fully_ connected` function. The input for this hidden layer will
    be `X`, which is the data that comes from the placeholder, and `n_hidden1` is
    the number of neurons that we have in this hidden layer. Remember we have 350
    neurons in the first hidden layer. Now, the first hidden layer becomes the input
    for the second hidden layer, and `n_hidden2` is the number of neurons that we
    use in this second hidden layer. Likewise, this second hidden layer becomes the
    input for the third hidden layer and we use this number of neurons in this layer.
    Finally, we have the output layer, let''s call it `y_pred`, and this is a fully
    connected layer, with the third hidden layer as input. This is one output and
    this layer has no activation function. The following screenshot shows the lines
    of code used for building the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e509c688-e82a-44e0-a99b-1abce7126c60.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the `mean_squared _error` function—TensorFlow provides us with
    many such functions. We pass the observed values and the predicted values and
    this function calculates the mean squared error. The following screenshot shows
    the lines of code used for showing the `mean_squared _error` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cedfa97e-88ee-4564-9297-6299ce5e7652.png)'
  prefs: []
  type: TYPE_IMG
- en: Defining optimizer and training operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of the optimizer is to minimize the loss and it does this by adjusting
    the different weights that we have in all of the layers of our network. The optimizer
    used here is the Adam optimizer with a learning rate of 0.001.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the lines of code used for defining the optimizer
    and also shows the training operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d34af37c-db85-4d8d-82ec-1274c6805ea2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows some of the NumPy arrays that we created and
    will use for evaluation purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88fcf3a8-6cf2-4475-b25e-d7ee0db78a4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the computational graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For actually running the computational graph, first we will initialize all
    of the variables in our program. The following screenshot shows the lines of code
    used for running the computational graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea17818a-74ea-4811-ad41-4040102b4673.png)'
  prefs: []
  type: TYPE_IMG
- en: The variables are the weights that are implicit in the `fully_connected` function.
    Then, for every epoch, we initialize the iterator object and pass the training
    dataset. Here, we have `batch_data`, we run this `next_ element` node, and we
    get batches of 50\. We can get the feature values and the labels, we can get the
    labels, and then we can run the training operation. When the object runs out of
    data, we get an error. In this case, when we get one of these errors, it means
    that we have used all of the training datasets. We then break from this `while`
    loop and proceed to the next epoch. Later, we produce some individual predictions
    so you can take a look at concrete predictions that this neural network makes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the behavior of the training and the testing
    MSE of all 40 epochs as we present the data to this network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9121538-21c3-43f6-8fe0-59ecaa8da16e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the last tested MSE (epoch 40) we get the final value of the training and
    the testing MSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'We get the actual predictions from the network and the values are relatively
    close. Here, we can see the predicted prices. For cheap diamonds, the network
    produced values that are relatively close. For very expensive diamonds, the network
    produced high values. Also, the predicted values are pretty close to the observed
    values. The following screenshot shows the actual and the predicted values that
    we got from the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c37daf44-699d-4ec7-a142-2c8cfb057b90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the graph of the training MSE with the testing
    MSE and the lines of code used to produce it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d41fa0ad-74c9-4232-b9e7-e8788788c194.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification with DNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For understanding classification with DNNs, we first have to understand the
    concept of exponential linear unit function and the elements of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential linear unit activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Exponential Linear Unit** (**ELU**) function is a relatively recent modification
    to the ReLU function. It looks very similar to the ReLU function, but it has very
    different mathematical properties. The following screenshot shows the ELU function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e4604f6-28a4-4da0-9faa-de706a4da045.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows that, at `0`, we don't have a corner. In the
    case of the ReLU function, we have a corner. In this function, instead of a single
    value going to `0`, we have the ELU function slowly going to the negative alpha
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Classification with DNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For classification with DNNs, we first have to import the libraries that we
    will use. Use the lines of code in the following screenshot to import the `tensorflow`,
    `pandas`, `numpy`, and `matplotlib` libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e70a38d6-3326-48a1-a4e2-2a8156ff1188.png)'
  prefs: []
  type: TYPE_IMG
- en: We will also import the `train_test_split` function from `sklearn.model_selection`,
    `RobustScaler` from `sklearn.preprocessiong` , and `precision_score`, `recall_score`,
    and `accuracy_score` from `sklearn.metrics`. We also import the `fully_connected`
    function from `tensorflow.contrib.layers` to build the layers of our network.
  prefs: []
  type: TYPE_NORMAL
- en: Elements of the DNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before running the model, we first have to determine the elements that we will
    use in
  prefs: []
  type: TYPE_NORMAL
- en: 'building a multilayer perceptron model, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture**: The model contains 25 elements in the input layer because
    we have'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 25 features in the dataset. We have two elements in the output layer and we
    will
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: also use three hidden layers, although we could use any number of hidden
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: layers. We will use the same number of neurons in each layer, 200\. Here we
    use
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: the powers of 2, which is an arbitrary choice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Activation function**: We will choose the ELU activation function, which
    was'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: explained in the preceding chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Optimizing a****lgorithm**: The optimization algorithm used here is the Adam'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: optimizer with a learning rate of 0.001.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Loss function**: For the `loss` function, we will use the cross-entropy function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weights initialization strategy**: For this, we will use the Xavier initializer,
    a'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: method that comes as default with the `fully_connected` function from
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: TensorFlow.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Regularization strategy**: We are not going to use any regularization strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training strategy**: We are going to use 40 epochs. So, we will present the
    dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 40 times to the network, and in every iteration, we will use a batch size of
    100.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Building the DNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we import the dataset that we will use. The reason behind using this dataset
    is that it
  prefs: []
  type: TYPE_NORMAL
- en: is easily available. The following are the steps involved in building a DNN
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to read the data in the cell. The following screenshot shows the
    lines of code used
  prefs: []
  type: TYPE_NORMAL
- en: 'to read the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfee505c-4e6a-46e0-aafc-a1a73825fa69.png)'
  prefs: []
  type: TYPE_IMG
- en: Producing the objects for modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we produce the objects used for modeling. We are going to use 10 percent
    for testing
  prefs: []
  type: TYPE_NORMAL
- en: and 90 percent for training. The following screenshot shows the lines of code
    used for
  prefs: []
  type: TYPE_NORMAL
- en: 'producing the objects for modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57d58fe3-2781-4cb0-b875-cbf1ef50ebcb.png)'
  prefs: []
  type: TYPE_IMG
- en: Training strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the training strategy that we previously mentioned, 40 epochs and a
    batch size of
  prefs: []
  type: TYPE_NORMAL
- en: '100\. The following code block shows the parameters we set in this strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Input pipeline for DNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we perform the same thing that we did with the regression example. We create
    a
  prefs: []
  type: TYPE_NORMAL
- en: '`dataset` object and an iterator object. In the end, we have `next_element`.
    This will be a node in our computational graph that will give us 100 data points
    each time. Hence, we get'
  prefs: []
  type: TYPE_NORMAL
- en: the batches. The following screenshot shows the lines of code used for producing
    an input
  prefs: []
  type: TYPE_NORMAL
- en: 'pipeline for the DNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2c44778-8a38-4d4a-ae49-bac788b96ee4.png)'
  prefs: []
  type: TYPE_IMG
- en: Defining the architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use three hidden layers and 200 neurons for all three. The following
    code snippet
  prefs: []
  type: TYPE_NORMAL
- en: 'shows the architecture we will use in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Placeholders for inputs and labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The values of different layers are the objects, also called the placeholders,
    for inputs and
  prefs: []
  type: TYPE_NORMAL
- en: labels. These placeholders are used for feeding the data into the network. The
    following
  prefs: []
  type: TYPE_NORMAL
- en: 'lines of code are used for showing placeholders for inputs and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Building the neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For building deep neural networks, we will use the `DNN` function. We have three
    layers and we will use the ELU function as the activation function. You can get
    this function from
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow, `tf.nn.elu`, from which you can get a lot of functions that will
    help you build your deep learning models. The following screenshot shows the lines
    of code used for
  prefs: []
  type: TYPE_NORMAL
- en: 'producing this function and for getting the output in the form of `logits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5e00979-4492-4f24-9bbd-666d6c417cc6.png)'
  prefs: []
  type: TYPE_IMG
- en: The final layer is called the `logits` layer. We won't be using any activation
    function in
  prefs: []
  type: TYPE_NORMAL
- en: this layer.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the `loss` function, again, we are going to get `logits` from the DNN and
    then pass this `logits` to the `softmax_cross_entropy_with_logits`function from
    TensorFlow. We pass the true labels and `logits`, and then we can get the loss
    by using the `reduce_mean` function with `cross_entropy`. The following screenshot
    shows the lines of code used for showing the use of the `reduce_mean`function
    with `cross_entropy` for getting the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c3d959e-568f-4e50-bf45-e28bcaf89b8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, for evaluation, we will calculate the probabilities of default and non-default
    variables; you can get the probabilities by applying a `softmax` function to `logits`.
    The following screenshot shows the `softmax` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c08b33df-ddba-4203-8a20-c7c8d56f40cc.png)'
  prefs: []
  type: TYPE_IMG
- en: The `softmax` function is used for providing the probabilities for the different
    categories.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer and the training operation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of the optimizer is to minimize loss, and it does this by adjusting
    the different weights that we have in all of the layers of our network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the lines of code used for defining the optimizer
    and shows the training operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31bcbcf6-c2aa-4cd5-a759-3cf5b0a68f49.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the optimizer is, again, the Adam optimizer with a learning rate
    of `0.001`. The training operation is the operation in which the optimizer minimizes
    the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Run the computational graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To actually run the computational graph, first we initialize all of the variables
    in our
  prefs: []
  type: TYPE_NORMAL
- en: program. The variables are the weights that are implicit in the `fully_connected`
    function. We run four epochs and for each epoch, we initialize our iterator object.
    We pass training `x` and training `y`, and then we run this loop. This loop will
    run as long as we have data in `next_elementelement`. So, we get the next 100
    elements and then, in the next iteration, the next 100 elements, and so on. In
    every iteration, we run the training operation. Now, what this training operation
    does is ask the optimizer to adjust the parameters and the weights, a little bit
    in order to make better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the lines of code used for running the computational
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d0faf9a-3035-4af5-9cd1-a133bc2502cc.png)'
  prefs: []
  type: TYPE_IMG
- en: In the end, we can get the probabilities and we can use these for evaluation
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model with a set threshold
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `probabilities` object is produced to actually evaluate the model performance
    with
  prefs: []
  type: TYPE_NORMAL
- en: different classification thresholds. The classification threshold can be modified
    for a binary
  prefs: []
  type: TYPE_NORMAL
- en: classification problem and can be used for calculating the recall score, the
    precision, and the
  prefs: []
  type: TYPE_NORMAL
- en: 'accuracy. On using a classification threshold of `0.16`, these are the metrics
    that we get in the testing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f23f3ebc-019f-4be1-a50d-dd44eaec977b.png)'
  prefs: []
  type: TYPE_IMG
- en: On calculating, we get a recall score of `82.53` percent, precision of `34.02`
    percent, and an accuracy of `60.7` percent.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to make predictions using TensorFlow. We studied
    the MNIST dataset and classification of models using this dataset. We came across
    the elements of DNN models and the process of building the DNN. Later, we progressed
    to study regression and classification with DNNs. We classified handwritten digits
    and learned more about building models in TensorFlow. This brings us to the end
    of this book! We learned how to use ensemble algorithms to produce accurate predictions.
    We applied various techniques to combine and build better models. We learned how
    to perform cross-validation efficiently. We also implemented various techniques
    to solve current issues in the domain of predictive analysis. And, the best part,
    we used the DNN models we built to solve classification and regression problems.
    This book has helped us implement various machine learning techniques to build
    advanced predictive models and apply them in the real world.
  prefs: []
  type: TYPE_NORMAL
