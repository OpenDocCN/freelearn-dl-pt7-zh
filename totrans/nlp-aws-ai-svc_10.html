<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer176">
			<h1 id="_idParaDest-102"><em class="italic"><a id="_idTextAnchor106"/>Chapter 8</em>: Leveraging NLP to Monetize Your Media Content</h1>
			<p>As we have seen in this book so far, AI, and specifically NLP, has a wide range of uses in areas hitherto considered traditional IT spurred on by the rapid proliferation of data and the democratization of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) with cloud computing. In the previous chapter, we saw a cool example of how you can bring color to social media reviews and other forms of textual data by running voice of the customer analytics with sentiment detection. We saw how you can use AWS Glue to crawl raw data from Amazon S3, use Amazon Athena to interactively query this data, transform the raw data using PySpark (<a href="http://spark.apache.org/docs/latest/api/python/index.html">http://spark.apache.org/docs/latest/api/python/index.html</a>) in an AWS Glue job to call Amazon Comprehend APIs (which provide ready-made intelligence with pre-trained NLP models) to get sentiment analysis on the review, convert the data into Parquet, and partition it (<a href="https://docs.aws.amazon.com/athena/latest/ug/partitions.html">https://docs.aws.amazon.com/athena/latest/ug/partitions.html</a>) by sentiment to optimize analytics queries. In this chapter, we will change gears and look at a use case that has gained tremendous popularity in recent times due to the increased adoption of streaming media content, specifically how to monetize content.</p>
			<p>The gap between online advertising and print media advertising is ever widening. According to this article, <a href="https://www.marketingcharts.com/advertising-trends-114887">https://www.marketingcharts.com/advertising-trends-114887</a>, quoting a PwC outlook report on global entertainment and media (<a href="https://www.pwc.com/outlook">https://www.pwc.com/outlook</a>), online advertising spend was estimated to be approximately $58 billion higher than TV advertising, and $100 billion higher than magazine and newspaper advertising in 2020 even with the COVID-19 pandemic considered.</p>
			<p>This, of course, is also driven by the increased usage of smart consumer devices and the explosion of the internet age consumer trends. Google Ads is one of the most popular ad-serving platforms today, accounting for 80% of Alphabet's (the public holding company that owns Google) revenues, raking in $147 billion in 2020 according to this article: <a href="https://www.cnbc.com/2021/05/18/how-does-google-make-money-advertising-business-breakdown-.html">https://www.cnbc.com/2021/05/18/how-does-google-make-money-advertising-business-breakdown-.html</a>. You read that right: online advertisements are indeed a big deal. So, when you are next thinking of posting that cool travel video or your recipe for an awesome chili con carne, you could actually be making money out of your content. You may ask, this is all great but how does NLP help in this case? Read on to find out!</p>
			<p>The answer, as you probably already guessed, is context-based ad serving. Suppose you have an intelligent solution that could listen to the audio/text in your content, understand what is being discussed, identify topics that represent the context of the content, look up ads related to the topic, and stitch these ads back into your content seamlessly without having to train any ML models: wouldn't that be swell? Yes, that's exactly what we will be building now.</p>
			<p>We will navigate through the following sections:</p>
			<ul>
				<li>Introducing the content monetization use case</li>
				<li>Building the NLP solution for content monetization</li>
			</ul>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor107"/>Technical requirements</h1>
			<p>For this chapter, you will need access to an AWS account. Please make sure to follow the instructions specified in the <em class="italic">Technical requirements</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, to create your AWS account, and log in to the AWS Management Console before trying the steps in the <em class="italic">Building the NLP solution for content monetization</em> section.</p>
			<p>The Python code and sample datasets for our solution can be found here: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2008">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2008</a>. Please use the instructions in the following sections along with the code in the repository to build the solution.</p>
			<p>Check out the following video to see the Code in Action at <a href="https://bit.ly/317mcSh">https://bit.ly/317mcSh</a>.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor108"/>Introducing the content monetization use case</h1>
			<p>We know NLP can help enhance the customer service experience and understand better what our <a id="_idIndexMarker459"/>customers are telling us. We will now use NLP to determine the context of our media content and stitch ads into the content relevant <a id="_idIndexMarker460"/>to that context. To illustrate our example, let's go back to our fictitious banking corporation called <strong class="bold">LiveRight Holdings Private Limited</strong>. LiveRight's management has decided they now need to expand to more geographies as they are seeing a lot of demand for their model of no-frills banking that cuts their operational costs and transfers the savings back to their customers. They have decided to hire you as their marketing technology architect, putting you in charge of all their content creation, but challenge you to devise a way for the content to pay for itself due to their low-cost policies. You come up with the idea of creating fun educational <a id="_idIndexMarker461"/>videos that show the latest trends in the intersection of banking and technology. There is a lot of demand for such videos since they are free to watch, you can intersperse them with ads to get monetary returns, and they serve to raise awareness of the bank in the process.</p>
			<p>You have thought through the solution design and decide to use the following:</p>
			<ul>
				<li><strong class="bold">AWS Elemental MediaConvert</strong> (<a href="https://aws.amazon.com/mediaconvert/">https://aws.amazon.com/mediaconvert/</a>), a managed <a id="_idIndexMarker462"/>video transcoding service that can convert and enhance your video content to multiple versions for broadcasting</li>
				<li><strong class="bold">Amazon Transcribe</strong> (<a href="https://aws.amazon.com/transcribe/">https://aws.amazon.com/transcribe/</a>) to get a transcript <a id="_idIndexMarker463"/>of the video content</li>
				<li><strong class="bold">Amazon Comprehend</strong> (<a href="https://aws.amazon.com/comprehend/">https://aws.amazon.com/comprehend/</a>) to leverage <a id="_idIndexMarker464"/>its pre-trained ML model for topic modeling to determine common themes in the textual content of the video that will, in turn, drive the ad selection process</li>
				<li><strong class="bold">AWS Elemental MediaTailor</strong> (<a href="https://aws.amazon.com/mediatailor/">https://aws.amazon.com/mediatailor/</a>), a managed <a id="_idIndexMarker465"/>service that can take as input media content, assemble this into an online channel delivery, and stitch ads onto the video content</li>
			</ul>
			<p>The components of the solution we will build are as shown in the following figure:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="Images/B17528_08_01.jpg" alt="Figure 8.1 – NLP solution build for content monetization" width="1061" height="602"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – NLP solution build for content monetization</p>
			<p>We will be <a id="_idIndexMarker466"/>walking through this solution using the AWS <a id="_idIndexMarker467"/>Management Console (<a href="https://aws.amazon.com/console/">https://aws.amazon.com/console/</a>) and an Amazon SageMaker Jupyter notebook (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html">https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html</a>), which will allow us to review <a id="_idIndexMarker468"/>the code and results as we execute it step by step. If you do not have access to the AWS Management Console, please follow the detailed instructions in the <em class="italic">Technical requirements</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract,</em> of this book.</p>
			<p>As a first step, we will look at the sample video file provided in the GitHub repository (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/bank-demo-prem-ranga.mp4">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/bank-demo-prem-ranga.mp4</a>). The sample video is from a demonstration of AWS AI services for document processing. For a full version of this video, please refer to <a href="https://www.youtube.com/watch?v=vBtxjXjr_HA">https://www.youtube.com/watch?v=vBtxjXjr_HA</a>. We will upload this sample video to an S3 bucket:</p>
			<ol>
				<li>After the video is loaded to the S3 bucket, we will use AWS Elemental MediaConvert to create the broadcast versions of our sample video content.</li>
				<li>In parallel, we will open our Amazon SageMaker Jupyter notebook to run the code to create an Amazon Transcribe transcription job to convert the audio track from our sample video to text. </li>
				<li>We will use Amazon Comprehend Topic Modeling to detect the topics from this text.</li>
				<li>We will then use the sample URL from the <strong class="bold">Google Ad Decision</strong> server (<a href="https://support.google.com/admanager/table/9749596">https://support.google.com/admanager/table/9749596</a>), a <strong class="bold">Video Ad Serving Template</strong> (<strong class="bold">VAST</strong>) tag URL <a id="_idIndexMarker469"/>that is generated by an ad server containing placeholders for the following.<p>a) Durations in the video content to play the ads</p><p>b) A content source ID referred by the tag <strong class="source-inline">'cmsid'</strong> and a video content ID referred by the tag <strong class="source-inline">'vid'</strong>, which we will populate to stitch in the ads specific to the topic we detected from the transcribed text in the previous step</p></li>
				<li>We will <a id="_idIndexMarker470"/>then create an Amazon CloudFront distribution for the output video files from the AWS Elemental MediaConvert job.</li>
				<li>Finally, we will use <strong class="bold">AWS Elemental MediaTailor</strong> to create a new configuration <a id="_idIndexMarker471"/>for broadcast-grade streaming content, which will take our MediaConvert output files available via the CloudFront distribution and the ad decision server URL we modified in the previous step to create a new video file with the ads inserted.</li>
			</ol>
			<p>In this section, we introduced the content monetization requirement we are trying to build with our NLP solution, reviewed the challenges faced by LiveRight, and looked at an overview of the solution we will build. In the next section, we will walk through the building of a solution step by step.</p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor109"/>Building the NLP solution for content monetization</h1>
			<p>In the previous section, we introduced a requirement for content monetization, covered the architecture <a id="_idIndexMarker472"/>of the solution we will <a id="_idIndexMarker473"/>be building, and briefly walked through the solution components and workflow steps. In this section, we will start executing the tasks to build our solution. But first, there are prerequisites we will have to take care of.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor110"/>Setting up to solve the use case</h2>
			<p>If you have not done so in the previous chapters, you will as a prerequisite have to create <a id="_idIndexMarker474"/>an Amazon SageMaker Jupyter notebook instance and set up <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) permissions for that <a id="_idIndexMarker475"/>notebook role to access the AWS services we will use in this notebook. After that, you will need to clone the GitHub repository (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services</a>), create an Amazon S3 (<a href="https://aws.amazon.com/s3/">https://aws.amazon.com/s3/</a>) bucket, and provide the bucket name in the notebook to start execution. Please follow the next steps to complete these tasks before we can execute the cells from our notebook:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Please ensure you have completed the tasks mentioned in the <em class="italic">Technical requirements</em> section. If you have already created an Amazon SageMaker notebook instance and cloned the GitHub repository for the book in a previous chapter, you can skip some of these steps. Please go directly to the step where you open the notebook folder corresponding to this chapter.</p>
			<ol>
				<li value="1">If not already done, follow the instructions documented in the <em class="italic">Creating an Amazon SageMaker Jupyter notebook instance</em> section in the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract,</em> to create your Jupyter notebook instance.<p class="callout-heading">IAM role permissions while creating Amazon SageMaker Jupyter notebooks</p><p class="callout">Accept the default option for the IAM role at notebook creation time to allow access to any S3 bucket.</p></li>
				<li>Once you <a id="_idIndexMarker476"/>create the notebook instance and its status is <strong class="bold">InService</strong>, click on <strong class="bold">Open Jupyter</strong> in the <strong class="bold">Actions</strong> menu for the notebook instance.<div id="_idContainer148" class="IMG---Figure"><img src="Images/B17528_08_02.jpg" alt="Figure 8.2 – Opening the Jupyter notebook&#13;&#10;" width="1147" height="362"/></div><p class="figure-caption">Figure 8.2 – Opening the Jupyter notebook</p><p>This will take you to the home folder of your notebook instance.</p></li>
				<li>Click on <strong class="bold">New</strong> as shown in the following figure and select <strong class="bold">Terminal</strong>:<div id="_idContainer149" class="IMG---Figure"><img src="Images/B17528_08_03.jpg" alt="Figure 8.3 – Opening the terminal in the Jupyter notebook" width="944" height="619"/></div><p class="figure-caption">Figure 8.3 – Opening the terminal in the Jupyter notebook</p></li>
				<li>In the <a id="_idIndexMarker477"/>terminal window, first type <strong class="source-inline">cd SageMaker</strong> and then type <strong class="source-inline">git clone </strong><a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services</a>, as shown in the following screenshot. If you have already done this in the previous chapters, you don't have to clone the repository again.<div id="_idContainer150" class="IMG---Figure"><img src="Images/B17528_08_04.jpg" alt="Figure 8.4 – The git clone command&#13;&#10;" width="937" height="150"/></div><p class="figure-caption">Figure 8.4 – The git clone command</p></li>
				<li>Now, exit the terminal window and go back to the home folder and you will see a folder called <strong class="source-inline">Chapter 08</strong>. Click the folder and you should see a notebook called <strong class="source-inline">contextual-ad-marking-for-content-monetization-with-nlp-github.ipynb</strong>.</li>
				<li>Open this notebook by clicking it.</li>
				<li>Leave the notebook open for now. We will first execute the steps in the <em class="italic">Uploading the sample video and converting it for broadcast</em> section before executing the steps in the notebook.</li>
			</ol>
			<p>Now that <a id="_idIndexMarker478"/>we have set up our notebook and cloned the repository, let's now add the permissions policies we need to successfully run our code sample.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor111"/>Additional IAM prerequisites</h2>
			<p>To run <a id="_idIndexMarker479"/>the notebook, we have to enable additional policies and also update the trust relationships for our SageMaker notebook role. Please complete the following steps to do this:</p>
			<ol>
				<li value="1">If not already done, please attach <strong class="source-inline">ComprehendFullAccess</strong> and <strong class="source-inline">AmazonTranscribeFullAccess</strong> policies to your Amazon SageMaker notebook IAM role. To execute this step, please refer to the <em class="italic">Changing IAM permissions and trust relationships for the Amazon SageMaker notebook execution role</em> in the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>.</li>
				<li>Your SageMaker execution role should have access to S3 already. If not, add the following JSON statement as an inline policy. For instructions, please refer to the <em class="italic">Changing IAM permissions and trust relationships for the Amazon SageMaker notebook execution role</em> section in the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>:<p class="source-code">{ "Version": "2012-10-17", "Statement": [ {</p><p class="source-code">  "Action": [</p><p class="source-code">      "s3:GetObject",</p><p class="source-code">      "s3:ListBucket",</p><p class="source-code">      "s3:PutObject"</p><p class="source-code">  ],</p><p class="source-code">  "Resource": ["*"],</p><p class="source-code">  "Effect": "Allow"</p><p class="source-code">      }</p><p class="source-code">  ]</p><p class="source-code">}</p></li>
				<li>Finally, update <a id="_idIndexMarker480"/>the trust relationships. For instructions, please refer to the <em class="italic">Changing IAM permissions and trust relationships for the Amazon SageMaker notebook execution role</em> section in the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>:<p class="source-code">{ "Version": "2012-10-17", "Statement": [</p><p class="source-code">  { "Effect": "Allow", </p><p class="source-code">    "Principal": </p><p class="source-code">      { "Service": </p><p class="source-code">          [ "sagemaker.amazonaws.com", </p><p class="source-code">            "s3.amazonaws.com",</p><p class="source-code">"transcribe.amazonaws.com",</p><p class="source-code">            "comprehend.amazonaws.com" ] </p><p class="source-code">          }, </p><p class="source-code">          "Action": "sts:AssumeRole" } </p><p class="source-code">      ] </p><p class="source-code">  }</p></li>
			</ol>
			<p>Now that we have set up our notebook and set up the IAM role to run the walk-through notebook, in the <a id="_idIndexMarker481"/>next section, we will start with creating broadcast versions of our sample video.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor112"/>Uploading the sample video and converting it for broadcast</h2>
			<p>In this section <a id="_idIndexMarker482"/><a id="_idIndexMarker483"/>we will create two S3 buckets <a id="_idIndexMarker484"/><a id="_idIndexMarker485"/>and get the sample video uploaded for processing. Please execute the following steps:</p>
			<ol>
				<li value="1">Navigate to our GitHub url - <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/bank-demo-prem-ranga.mp4">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/bank-demo-prem-ranga.mp4</a> and click on the <strong class="bold">Download</strong> button at the right middle of the page to download the video file to your computer.</li>
				<li>Now create two Amazon S3 buckets, one for our media input and the other for media output. Follow the instructions detailed in the <em class="italic">Creating an Amazon S3 bucket, a folder, and uploading objects</em> section in the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract,</em> of this book. Ensure that the block public access is on for both the buckets.</li>
				<li>In the Amazon S3 media input bucket, create a folder or prefix called <strong class="source-inline">chapter8</strong>. Within this folder, create a folder called <strong class="source-inline">rawvideo</strong>. Follow the instructions detailed in the <em class="italic">Creating an Amazon S3 bucket, a folder, and uploading objects</em> section in the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, of this book.</li>
				<li>Now upload the <strong class="source-inline">bank-demo-prem-ranga.mp4</strong> file into the <strong class="source-inline">rawvideo</strong> folder. So, within the S3 bucket, the video file should be present in the path <strong class="source-inline">chapter8/rawvideo/bank-demo-prem-ranga.mp4</strong>.</li>
				<li>Now, we will pivot to creating the broadcast version of the video using AWS Elemental MediaConvert. In the AWS Management Console, in the search bar at the top, type <strong class="source-inline">Media</strong>, select <strong class="bold">AWS Elemental MediaConvert</strong>, and in the console, click on <strong class="bold">Get started</strong>.<div id="_idContainer151" class="IMG---Figure"><img src="Images/B17528_08_05.jpg" alt="Figure 8.5 – AWS Elemental MediaConvert&#13;&#10;" width="1048" height="709"/></div><p class="figure-caption">Figure 8.5 – AWS Elemental MediaConvert</p></li>
				<li>In the <strong class="bold">Create job</strong> UI, under <strong class="bold">Input 1</strong>, for <strong class="bold">Input file URL</strong>, please provide the full S3 <a id="_idIndexMarker486"/>path of where you uploaded the <a id="_idIndexMarker487"/>sample video file in <em class="italic">Step 4</em>. This should be <strong class="source-inline">s3://&lt;media-input-bucket&gt;/chapter8/rawvideo/bank-demo-prem-ranga.mp4</strong>.<div id="_idContainer152" class="IMG---Figure"><img src="Images/B17528_08_06.jpg" alt="Figure 8.6 – Providing a job input file URL" width="1034" height="702"/></div><p class="figure-caption">Figure 8.6 – Providing a job input file URL</p></li>
				<li>Now, click <a id="_idIndexMarker488"/>the <strong class="bold">Add</strong> button in <strong class="bold">Output groups</strong> in the <a id="_idIndexMarker489"/>left panel of the screen, select <strong class="bold">Apple HLS</strong> as the option in <strong class="bold">Add output group</strong>, and click <strong class="bold">Select</strong>. Output groups determine the types of content artifacts produced and on what devices they can be played.<div id="_idContainer153" class="IMG---Figure"><img src="Images/B17528_08_07.jpg" alt="Figure 8.7 – Adding an output group for the MediaConvert job&#13;&#10;" width="1217" height="773"/></div><p class="figure-caption">Figure 8.7 – Adding an output group for the MediaConvert job</p></li>
				<li>Now, let's <a id="_idIndexMarker490"/>fill in the Apple HLS group settings. Provide the custom group name as <strong class="source-inline">HLS</strong>. In <strong class="bold">Destination</strong>, provide <a id="_idIndexMarker491"/>the name of the media output bucket you created in <em class="italic">Step 3</em> along with a prefix in the format <strong class="source-inline">s3://&lt;media-output-bucket&gt;/bankdemo</strong>. The AWS Elemental MediaConvert service will process the sample video file into Apple HLS content files for broadcasting. In <strong class="bold">Segment control</strong>, choose <strong class="bold">Segmented files</strong>. Choose <strong class="source-inline">10</strong> for <strong class="bold">Segment length (sec)</strong> and <strong class="source-inline">3</strong> for <strong class="bold">Minimum segment length (sec)</strong>.<div id="_idContainer154" class="IMG---Figure"><img src="Images/B17528_08_08.jpg" alt="Figure 8.8 – Adding output group settings for the MediaConvert job" width="1360" height="663"/></div><p class="figure-caption">Figure 8.8 – Adding output group settings for the MediaConvert job</p></li>
				<li>Scroll <a id="_idIndexMarker492"/>down to <strong class="bold">Outputs</strong> and type <strong class="source-inline">_720</strong> for <strong class="bold">Name modifier</strong> for <strong class="bold">Output 1</strong>. Do <em class="italic">not</em> click <a id="_idIndexMarker493"/>on <strong class="bold">Create</strong> yet.<div id="_idContainer155" class="IMG---Figure"><img src="Images/B17528_08_09.jpg" alt="Figure 8.9 – Adding outputs for the MediaConvert job" width="1273" height="565"/></div><p class="figure-caption">Figure 8.9 – Adding outputs for the MediaConvert job</p></li>
				<li>Now, click <strong class="bold">Output 1</strong>, as shown:<div id="_idContainer156" class="IMG---Figure"><img src="Images/B17528_08_10.jpg" alt="Figure 8.10 – Clicking Output 1&#13;&#10;" width="1238" height="641"/></div><p class="figure-caption">Figure 8.10 – Clicking Output 1</p></li>
				<li>As <a id="_idIndexMarker494"/>shown in <a id="_idIndexMarker495"/>the following screenshot, type <strong class="source-inline">$dt$</strong> for <strong class="bold">Segment modifier</strong> in <strong class="bold">Output settings</strong>. In the <strong class="bold">Resolution (w x h)</strong> fields, type <strong class="source-inline">1280</strong> and <strong class="source-inline">720</strong>. Type <strong class="source-inline">3000000</strong> for <strong class="bold">Bitrate (bits/s)</strong>. Leave the rest of the fields as the default.<div id="_idContainer157" class="IMG---Figure"><img src="Images/B17528_08_11.jpg" alt="Figure 8.11 – Modifying the output and encoding settings&#13;&#10;" width="1273" height="611"/></div><p class="figure-caption">Figure 8.11 – Modifying the output and encoding settings</p></li>
				<li>On <a id="_idIndexMarker496"/>the left panel, under <strong class="bold">Job settings</strong>, click <strong class="bold">AWS integration</strong>. On <a id="_idIndexMarker497"/>the right, under <strong class="bold">Service access</strong>, for <strong class="bold">Service role control</strong>, select <strong class="bold">Create a new service role, full permissions</strong>. Accept the default name populated in <strong class="bold">New role name</strong>. Scroll down and click on <strong class="bold">Create</strong>.<div id="_idContainer158" class="IMG---Figure"><img src="Images/B17528_08_12.jpg" alt="Figure 8.12 – Adding service access for the MediaConvert job&#13;&#10;" width="1338" height="759"/></div><p class="figure-caption">Figure 8.12 – Adding service access for the MediaConvert job</p></li>
				<li>The job <a id="_idIndexMarker498"/>should complete in a couple <a id="_idIndexMarker499"/>of minutes. Click on the <strong class="bold">Job ID</strong> to review the summary view of the job, as shown in the following screenshot: <div id="_idContainer159" class="IMG---Figure"><img src="Images/B17528_08_13.jpg" alt="Figure 8.13 – Job summary&#13;&#10;" width="1333" height="617"/></div><p class="figure-caption">Figure 8.13 – Job summary</p></li>
				<li>Once the <a id="_idIndexMarker500"/>status shows <strong class="bold">COMPLETE</strong>, type <strong class="source-inline">S3</strong> in the <a id="_idIndexMarker501"/>search bar at the top of the screen and go to the S3 console. Under <strong class="bold">Buckets</strong>, type the name of the media output bucket you created in <em class="italic">Step 3</em> previously and click on the bucket name. You should see a number of files here all starting with the name <strong class="source-inline">bankdemo</strong>, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="Images/B17528_08_14.jpg" alt="Figure 8.14 – AWS Elemental MediaConvert Apple HLS output files&#13;&#10;" width="1346" height="664"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.14 – AWS Elemental MediaConvert Apple HLS output files</p>
			<p>We have <a id="_idIndexMarker502"/>now successfully completed the steps required to convert our sample video file into broadcast-enabled output files, which is <a id="_idIndexMarker503"/>required for us to insert ads into the video. In the next section, we will run a transcription of the audio content from our video, run topic modeling, create the <strong class="bold">VAST</strong> ad tag URL required for ad insertion, and show how we can perform content monetization.</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor113"/>Running transcription, finding topics, and creating a VAST ad tag URL</h2>
			<p>Open <a id="_idIndexMarker504"/>the notebook you cloned from the GitHub <a id="_idIndexMarker505"/>repository (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/contextual-ad-marking-for-content-monetization-with-nlp-github.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/contextual-ad-marking-for-content-monetization-with-nlp-github.ipynb</a>) in the <em class="italic">Setting up to solve the use case</em> section <a id="_idIndexMarker506"/>and execute the cells step by step, as follows:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Please ensure you have executed the steps in the <em class="italic">Technical requirements</em>, <em class="italic">Setting up to solve the use case</em>, and <em class="italic">Uploading the sample video and converting it for broadcast</em> sections before you execute the cells in the notebook.</p>
			<ol>
				<li value="1">Execute the first three cells under the <strong class="bold">Transcribe</strong> section to ensure we have the libraries we need for the notebook. Note that in the first cell you are importing libraries, in the second cell you are creating folders needed for Topic Modeling, and in the third cell you are specifying the S3 bucket and prefix. You should <a id="_idIndexMarker507"/>have already created two S3 buckets <a id="_idIndexMarker508"/>prior to running this notebook, as <a id="_idIndexMarker509"/>mentioned in the <em class="italic">Uploading the sample video and converting it for broadcast</em> section. Please provide the media input bucket name in the line, type a prefix of your choice, or you can accept what is already provided in the notebook. In this cell, we also define the Python SDK handle for Amazon S3 using Boto3, an AWS SDK for Python development (<a href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html">https://boto3.amazonaws.com/v1/documentation/api/latest/index.html</a>):<p class="source-code">bucket = '&lt;your-s3-bucket&gt;'</p><p class="source-code">prefix = 'chapter8'</p><p class="source-code">s3=boto3.client('s3')</p></li>
				<li>Execute the next cell to define a method for running an Amazon Transcribe transcription job to convert the audio content of our sample video file to text. Note that we are setting MediaFormat as <strong class="source-inline">mp4</strong>. We will be using the original sample video file from the GitHub repository (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/bank-demo-prem-ranga.mp4">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/bank-demo-prem-ranga.mp4</a>) as the input for the transcription job:<p class="source-code">import time</p><p class="source-code">import boto3</p><p class="source-code">def transcribe_file(job_name, file_uri, transcribe_client):</p><p class="source-code">    transcribe_client.start_transcription_job(</p><p class="source-code">        TranscriptionJobName=job_name,</p><p class="source-code">        Media={'MediaFileUri': file_uri},</p><p class="source-code">        MediaFormat='mp4',</p><p class="source-code">        LanguageCode='en-US'</p><p class="source-code">    )</p></li>
				<li>Provide a job name (a text string) for the transcription so we are able to identify this job <a id="_idIndexMarker510"/>down the line. Get the Boto3 handle <a id="_idIndexMarker511"/>for the Amazon Transcribe service, pass the <a id="_idIndexMarker512"/>S3 location of our sample video file we loaded in the media input S3 bucket, and call the <strong class="source-inline">transcribe_file</strong> method to run the transcription job:<p class="source-code">job_name = 'media-monetization-transcribe'</p><p class="source-code">transcribe_client = boto3.client('transcribe')</p><p class="source-code">file_uri = 's3://'+bucket+'/'+prefix+'/'+'rawvideo/bank-demo-prem-ranga.mp4'</p><p class="source-code">transcribe_file(job_name, file_uri, transcribe_client)</p></li>
				<li>Now, navigate to the AWS Management Console in a new tab, type <strong class="source-inline">Amazon Transcribe</strong> in the search bar at the top, and open the Amazon Transcribe console. Click on <strong class="bold">Transcription jobs</strong> in the left pane. You should see your transcription job with the job name you specified earlier. When the job completes, the status should change to <strong class="bold">Complete</strong>.<div id="_idContainer161" class="IMG---Figure"><img src="Images/B17528_08_15.jpg" alt="Figure 8.15 – Amazon Transcribe transcription job" width="1327" height="391"/></div><p class="figure-caption">Figure 8.15 – Amazon Transcribe transcription job</p></li>
				<li>Now come <a id="_idIndexMarker513"/>back to the notebook and <a id="_idIndexMarker514"/>execute the next cell to get the S3 location <a id="_idIndexMarker515"/>of the transcription results:<p class="source-code">job = transcribe_client.get_transcription_job(TranscriptionJobName=job_name)</p><p class="source-code">job_status = job['TranscriptionJob']['TranscriptionJobStatus']</p><p class="source-code">if job_status in ['COMPLETED', 'FAILED']:</p><p class="source-code">    print(f"Job {job_name} is {job_status}.")</p><p class="source-code">    if job_status == 'COMPLETED':</p><p class="source-code">        print(f"Download the transcript from\n"</p><p class="source-code">              f"\t{job['TranscriptionJob']['Transcript']['TranscriptFileUri']}") </p></li>
				<li>We will now execute the code cells in the <strong class="bold">Comprehend Topic Modeling</strong><em class="italic"> </em>section step by step. As a first step, we will retrieve the transcription output (<strong class="source-inline">transcript.csv</strong>) to convert the paragraph of text into individual lines (<strong class="source-inline">transcript_formatted.csv</strong>) to send as input to the Amazon Comprehend Topic Modeling job. Execute the code in the notebook cell as shown in the following code block:<p class="source-code">raw_df = pd.read_json(job['TranscriptionJob']['Transcript']['TranscriptFileUri'])</p><p class="source-code">raw_df = pd.DataFrame(raw_df.at['transcripts','results'].copy())</p><p class="source-code">raw_df.to_csv('topic-modeling/raw/transcript.csv', header=False, index=False)</p><p class="source-code">import csv</p><p class="source-code">folderpath = r"topic-modeling/raw" # make sure to put the 'r' in front and provide the folder where your files are</p><p class="source-code">filepaths  = [os.path.join(folderpath, name) for name in os.listdir(folderpath) if not name.startswith('.')] # do not select hidden directories</p><p class="source-code">fnfull = "topic-modeling/job-input/transcript_formatted.csv"</p><p class="source-code">for path in filepaths:</p><p class="source-code">    print(path)</p><p class="source-code">    with open(path, 'r') as f:</p><p class="source-code">        content = f.read() # Read the whole file</p><p class="source-code">        lines = content.split('.') # a list of all sentences</p><p class="source-code">        with open(fnfull, "w", encoding='utf-8') as ff:</p><p class="source-code">            csv_writer = csv.writer(ff, delimiter=',', quotechar = '"')</p><p class="source-code">            for num,line in enumerate(lines): # for each sentence</p><p class="source-code">                csv_writer.writerow([line])</p><p class="source-code">f.close()</p><p class="source-code">s3.upload_file('topic-modeling/job-input/transcript_formatted.csv', bucket, prefix+'/topic-modeling/job-input/tm-input.csv')</p></li>
				<li>We will <a id="_idIndexMarker516"/>run an Amazon Comprehend Topic Modeling <a id="_idIndexMarker517"/>job on this formatted CSV file to extract a set of topics that are applicable for our transcript. These topics represent and help us identify what the subject area or the theme for the related text is and represent the common set of words with the same contextual reference throughout the transcript. For more details, please refer to <em class="italic">Amazon Comprehend Topic Modeling</em>: <a href="https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html">https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html</a>.</li>
				<li>To get started, go to the AWS Management Console (please refer to <em class="italic">Technical requirements</em> in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract,</em> of this book if you don't have <a id="_idIndexMarker518"/>access to the AWS Management Console), type <strong class="source-inline">Amazon Comprehend</strong> in the <strong class="bold">services</strong> search bar at the top <a id="_idIndexMarker519"/>of the console, and navigate to the Amazon Comprehend console:<p>Click the <strong class="bold">Launch Amazon Comprehend</strong> button.</p><p>Click on <strong class="bold">Analysis jobs</strong> in the left pane and click on <strong class="bold">Create job</strong> on the right, as follows:</p><div id="_idContainer162" class="IMG---Figure"><img src="Images/B17528_08_16.jpg" alt="Figure 8.16 – Creating an analysis job&#13;&#10;" width="1627" height="292"/></div><p class="figure-caption">Figure 8.16 – Creating an analysis job</p><p>Type a name for your analysis job and set the analysis type as <strong class="bold">Topic modeling</strong> from the built-in jobs list:</p><div id="_idContainer163" class="IMG---Figure"><img src="Images/B17528_08_17.jpg" alt="Figure 8.17 – Creating Topic Modeling job inputs 1&#13;&#10;" width="942" height="509"/></div><p class="figure-caption">Figure 8.17 – Creating Topic Modeling job inputs 1</p><p>Provide the <a id="_idIndexMarker520"/>location of the CSV file (the <strong class="source-inline">transcript_formatted.csv</strong> file that we uploaded to S3 in preceding steps) in <a id="_idIndexMarker521"/>your <em class="italic">S3 bucket</em> in the <strong class="bold">Input Data</strong> section with the data source as <strong class="bold">My documents</strong> and the number of topics as <strong class="source-inline">2</strong>, as shown:</p><div id="_idContainer164" class="IMG---Figure"><img src="Images/B17528_08_18.jpg" alt="Figure 8.18 – Creating Topic Modeling job inputs 2&#13;&#10;" width="1036" height="572"/></div><p class="figure-caption">Figure 8.18 – Creating Topic Modeling job inputs 2</p><p>Provide <a id="_idIndexMarker522"/>the <strong class="bold">Output data</strong> S3 location, as <a id="_idIndexMarker523"/>shown (you can use the same S3 bucket you used for input), and then type a name suffix and click on <strong class="bold">Create job</strong>.</p><div id="_idContainer165" class="IMG---Figure"><img src="Images/B17528_08_19.jpg" alt="Figure 8.19 – Creating Topic Modeling job inputs 3&#13;&#10;" width="1227" height="782"/></div><p class="figure-caption">Figure 8.19 – Creating Topic Modeling job inputs 3</p><p>You should <a id="_idIndexMarker524"/>see a <strong class="bold">job submitted</strong> status <a id="_idIndexMarker525"/>after the IAM role propagation is completed. After 30 minutes, the job status should change to <strong class="bold">Completed</strong>. Now click on the job name and copy the S3 link provided in the <strong class="bold">Output data location</strong> field and go back to your notebook. We will continue the steps in the notebook.</p><div id="_idContainer166" class="IMG---Figure"><img src="Images/B17528_08_20.jpg" alt="Figure 8.20 – Topic Modeling job completed" width="1041" height="343"/></div><p class="figure-caption">Figure 8.20 – Topic Modeling job completed</p></li>
				<li>We will <a id="_idIndexMarker526"/>now execute the cells in the <strong class="bold">Process Topic Modeling Results</strong> section:<p>a.) To download <a id="_idIndexMarker527"/>the results of the Topic Modeling job, we need the output data location S3 URI that we copied in the previous step. In the first cell in this section of the notebook, replace the contents of the <strong class="source-inline">tpprefix</strong> variable, specifically <strong class="source-inline">&lt;path-to-job-output-tar&gt;</strong>, with the string highlighted in bold from the S3 URI you copied shown in the following code block.</p><p class="callout-heading">Note</p><p class="callout">The output data location S3 URI you copied in the preceding step is <strong class="source-inline">s3://&lt;your-s3-bucket&gt;/chapter8/topic-modeling</strong><strong class="bold">/&lt;aws-account-nr&gt;-TOPICS-&lt;long-hash-nr&gt;</strong><strong class="source-inline">/output/output.tar.gz</strong></p><p>b.) The revised code should look as follows and when executed will download the <strong class="source-inline">output.tar.gz</strong> file locally and extract it:</p><p class="source-code">directory = "results"</p><p class="source-code">parent_dir = os.getcwd()+'/topic-modeling'</p><p class="source-code">path = os.path.join(parent_dir, directory)</p><p class="source-code">os.makedirs(path, exist_ok = True)</p><p class="source-code">print("Directory '%s' created successfully" %directory)</p><p class="source-code">tpprefix = prefix+'/'+<strong class="bold">' topic-modeling/&lt;aws-account-nr&gt;-TOPICS-&lt;long-hash-nr&gt;/output/output.tar.gz'</strong></p><p class="source-code">s3.download_file(bucket, tpprefix, 'topic-modeling/results/output.tar.gz')</p><p class="source-code">!tar -xzvf topic-modeling/results/output.tar.gz</p><p>c.) Now, load each of the resulting CSV files to their own pandas DataFrames:</p><p class="source-code">tt_df = pd.read_csv('topic-terms.csv')</p><p class="source-code">dt_df = pd.read_csv('doc-topics.csv')</p><p>d.) The <strong class="source-inline">topic </strong><strong class="source-inline"><a id="_idIndexMarker528"/></strong><strong class="source-inline">terms</strong> DataFrame contains <a id="_idIndexMarker529"/>the topic number, what term corresponds to the topic, and the weightage this term contributes to the topic. Execute the code shown in the following code block to review the contents of the <strong class="source-inline">topic terms </strong>DataFrame:</p><p class="source-code">for i,x in tt_df.iterrows():</p><p class="source-code">    print(str(x['topic'])+":"+x['term']+":"+str (x['weight']))</p><p>e.) We may have multiple topics on the same line, but for this solution, we are not interested in these duplicates, so we will drop them:</p><p class="source-code">dt_df = dt_df.drop_duplicates(subset=['docname'])</p><p>f.) Let's now <a id="_idIndexMarker530"/>filter the topics such that we <a id="_idIndexMarker531"/>select the topic with the maximum weight distribution for the text it refers to:</p><p class="source-code">ttdf_max = tt_df.groupby(['topic'], sort=False)['weight'].max()</p><p>g.) Load these into their own DataFrame and display it:</p><p class="source-code">newtt_df = pd.DataFrame()</p><p class="source-code">for x in ttdf_max:</p><p class="source-code">    newtt_df = newtt_df.append(tt_df.query('weight == @x'))</p><p class="source-code">newtt_df = newtt_df.reset_index(drop=True)    </p><p class="source-code">newtt_df</p><p>h.) We will select the <strong class="source-inline">content</strong> topic term as it has the highest weight and assign this to a variable for use in the subsequent steps:</p><p class="source-code">adtopic = newtt_df.at[1,'term']</p></li>
				<li>We will now use the topic to look up ad content and create a VAST ad tag URL that will be <a id="_idIndexMarker532"/>used as an input to insert ads into <a id="_idIndexMarker533"/>the broadcast video files we created using AWS Elemental MediaConvert. The authors have provided two sample CSV files containing content metadata for looking up ads. <strong class="source-inline">ad-index.csv</strong> (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/ad-index.csv">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/ad-index.csv</a>) contains a list of topics as keys and sample <strong class="source-inline">cmsid</strong> and <strong class="source-inline">vid</strong> values. <strong class="source-inline">cmsid</strong> indicates the content management source ID in Google Ad Server, which is what we are using as the ad decision server for our example, and <strong class="source-inline">vid</strong> indicates the video content ID in Google Ad Server. <strong class="source-inline">adserver.csv</strong> (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/adserver.csv">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/adserver.csv</a>) contains the sample Google ad decision server URL that we need to modify in this step. For this example, we'll use the topic we discovered from our Topic Modeling job as the key to fetch <strong class="source-inline">cmsid</strong> and <strong class="source-inline">vid</strong>. <p>We will then substitute these in the VAST ad marker URL before creating the AWS Elemental MediaTailor configuration. Execute the code cells as shown in the following code block:</p><p class="source-code">adindex_df = pd.read_csv('media-content/ad-index.csv', header=None, index_col=0)</p><p class="source-code">adindex_df</p><p>a.) Please  note this is from the sample <strong class="source-inline">ad-index.csv</strong> file that the <a id="_idIndexMarker534"/>authors created for this demo. When <a id="_idIndexMarker535"/>you use this solution for your use case, you will need to create a Google Ads account to get the <strong class="source-inline">cmsid</strong> and <strong class="source-inline">vid</strong> values. For more details, please see this link: <a href="https://support.google.com/admanager/topic/1184139?hl=en&amp;ref_topic=7506089">https://support.google.com/admanager/topic/1184139?hl=en&amp;ref_topic=7506089</a>.</p><p>b.) Run the code in the following snippet to select the <strong class="source-inline">cmsid</strong> and <strong class="source-inline">vid</strong> values based on our topic:</p><p class="source-code">advalue = adindex_df.loc[adtopic]</p><p class="source-code">advalue</p><p>c.) We get the following response:</p><p class="source-code">1           cmsid=176</p><p class="source-code">2    vid=short_tencue</p><p>d.) Now we <a id="_idIndexMarker536"/>will create the ad server URL to <a id="_idIndexMarker537"/>use with AWS Elemental MediaTailor. Let's first copy the placeholder URL available in our GitHub repo (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/adserver.csv">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2008/media-content/adserver.csv</a>), which has pre-roll, mid-roll, and post-roll segments filled in:</p><p class="source-code">ad_rawurl = pd.read_csv('media-content/adserver.csv', header=None).at[0,0].split('&amp;')</p><p class="source-code">ad_rawurl</p><p>e.) We get the following response:</p><p class="source-code">['https://pubads.g.doubleclick.net/gampad/ads?sz=640x480',</p><p class="source-code"> 'iu=/124319096/external/ad_rule_samples',</p><p class="source-code"> 'ciu_szs=300x250',</p><p class="source-code"> 'ad_rule=1',</p><p class="source-code"> 'impl=s',</p><p class="source-code"> 'gdfp_req=1',</p><p class="source-code"> 'env=vp',</p><p class="source-code"> 'output=vmap',</p><p class="source-code"> 'unviewed_position_start=1',</p><p class="source-code"> 'cust_params=deployment%3Ddevsite%26sample_ar%3Dpremidpost',</p><p class="source-code"> <strong class="bold">'cmsid=',</strong></p><p class="source-code"><strong class="bold"> 'vid='</strong>,</p><p class="source-code"> 'correlator=[avail.random]']</p><p>f.) We will now replace the <strong class="source-inline">cmsid</strong> and <strong class="source-inline">vid</strong> values highlighted in the preceding response with the values corresponding to our topic and reformat the URL:</p><p class="source-code">ad_formattedurl = ''</p><p class="source-code">for x in ad_rawurl:</p><p class="source-code">    if 'cmsid' in x:</p><p class="source-code">        x = advalue[1]</p><p class="source-code">    if 'vid' in x:</p><p class="source-code">        x = advalue[2]</p><p class="source-code">    ad_formattedurl += x + '&amp;'</p><p class="source-code">    </p><p class="source-code">ad_formattedurl = ad_formattedurl.rstrip('&amp;')</p><p class="source-code">ad_formattedurl</p><p>g.) We get the following response. Copy the contents of the following URL:</p><p class="source-code">'https://pubads.g.doubleclick.net/gampad/ads?sz=640x480&amp;iu=/124319096/external/ad_rule_samples&amp;ciu_szs=300x250&amp;ad_rule=1&amp;impl=s&amp;gdfp_req=1&amp;env=vp&amp;output=vmap&amp;unviewed_position_start=1&amp;cust_params=deployment%3Ddevsite%26sample_ar%3Dpremidpost&amp;cmsid=176&amp;vid=short_tencue&amp;correlator=[avail.random]'</p></li>
			</ol>
			<p>Alright, that brings <a id="_idIndexMarker538"/>us to the end of this section. We successfully <a id="_idIndexMarker539"/>transcribed our sample video file using Amazon Transcribe, ran an Amazon Comprehend Topic Modeling job on the transcript, selected a topic, and stitched together an ad server VAST tag URL with the ad content ID corresponding to the topic. In the next section, we will use AWS Elemental MediaTailor to create new video output with the ad segments inserted, and we will test it by playing the video.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor114"/>Inserting ads and testing our video</h2>
			<p>Before we can proceed forward, we need to create an Amazon CloudFront (<a href="https://aws.amazon.com/cloudfront/">https://aws.amazon.com/cloudfront/</a>) content delivery distribution for the video output files we transcoded with AWS Elemental MediaConvert in the <em class="italic">Uploading the sample video and converting it for broadcast</em> section.</p>
			<p>Amazon CloudFront <a id="_idIndexMarker540"/>is a managed content delivery network <a id="_idIndexMarker541"/>that can be used for site hosting, APIs, and image, media, and video file delivery, with live or on-demand streaming formats, configured for global distribution or based on the selected price class. Please follow the next steps to set up the CloudFront distribution for your transcoded video files:</p>
			<ol>
				<li value="1">In the AWS Management Console, type <strong class="source-inline">CloudFront</strong> in the search bar at the top, and then select <strong class="bold">Amazon CloudFront</strong> and click <strong class="bold">Create Distribution</strong>.<div id="_idContainer167" class="IMG---Figure"><img src="Images/B17528_08_21.jpg" alt="Figure 8.21 – Amazon CloudFront Create Distribution" width="1319" height="254"/></div><p class="figure-caption">Figure 8.21 – Amazon CloudFront Create Distribution</p></li>
				<li>On the next page, click <strong class="bold">Get Started</strong> to proceed to the <strong class="bold">Create Distribution</strong> page. Please note there are multiple sections to be filled. In the <strong class="bold">Origin Settings</strong> part of the page, click the list box for <strong class="bold">Origin Domain Name</strong> and select the media output bucket that contains the video output files from the AWS Elemental MediaConvert job. Select <strong class="bold">Yes</strong> for <strong class="bold">Restrict Bucket Access</strong>, and select <strong class="bold">Create a New Identity</strong> for <strong class="bold">Origin Access Identity</strong>. Select <strong class="bold">Yes, Update Bucket Policy</strong> for <strong class="bold">Grant Read Permissions on Bucket</strong>.<div id="_idContainer168" class="IMG---Figure"><img src="Images/B17528_08_22.jpg" alt="Figure 8.22 – Origin Settings for Create Distribution in Amazon CloudFront&#13;&#10;" width="1128" height="862"/></div><p class="figure-caption">Figure 8.22 – Origin Settings for Create Distribution in Amazon CloudFront</p></li>
				<li>Scroll <a id="_idIndexMarker542"/>down to the <strong class="bold">Default Cache Behavior Settings</strong> area <a id="_idIndexMarker543"/>and change <strong class="bold">Viewer Protocol Policy</strong> to <strong class="bold">Redirect HTTP to HTTPS</strong>. For <strong class="bold">Cache Policy</strong>, click the list box and select <strong class="bold">Managed-Elemental-MediaPackage</strong>.<div id="_idContainer169" class="IMG---Figure"><img src="Images/B17528_08_23.jpg" alt="Figure 8.23 – Default Cache Behavior Settings" width="1250" height="834"/></div><p class="figure-caption">Figure 8.23 – Default Cache Behavior Settings</p></li>
				<li>Scroll <a id="_idIndexMarker544"/>down to the <strong class="bold">Distribution Settings</strong> area and <a id="_idIndexMarker545"/>select the price class based on where you are located. Leave the rest of the settings as they are, scroll down, and click <strong class="bold">Create Distribution</strong>.<div id="_idContainer170" class="IMG---Figure"><img src="Images/B17528_08_24.jpg" alt="Figure 8.24 – Distribution Settings&#13;&#10;" width="1233" height="993"/></div><p class="figure-caption">Figure 8.24 – Distribution Settings</p></li>
				<li>Once the <a id="_idIndexMarker546"/>distribution is created, the status will <a id="_idIndexMarker547"/>change to <strong class="bold">Deployed</strong> and the state will change to <strong class="bold">Enabled</strong>. Copy the value of the domain name from the distribution.<div id="_idContainer171" class="IMG---Figure"><img src="Images/B17528_08_25.jpg" alt="Figure 8.25 – Distribution is enabled" width="1253" height="269"/></div><p class="figure-caption">Figure 8.25 – Distribution is enabled</p></li>
				<li>We will <a id="_idIndexMarker548"/>now use this distribution as a content source to <a id="_idIndexMarker549"/>create new video output with the ads inserted. In the AWS Management Console, type <strong class="source-inline">MediaTailor</strong> in the services search bar, and select it to go to the AWS Elemental MediaTailor console. Click <strong class="bold">Create configuration</strong> to get started.<div id="_idContainer172" class="IMG---Figure"><img src="Images/B17528_08_26.jpg" alt="Figure 8.26 – AWS Elemental MediaTailor&#13;&#10;" width="1140" height="199"/></div><p class="figure-caption">Figure 8.26 – AWS Elemental MediaTailor</p></li>
				<li>On the <strong class="bold">Create configuration</strong> page, under <strong class="bold">Required settings</strong>, provide an ad campaign name. In the <strong class="bold">Content source</strong> field, paste the Amazon CloudFront distribution domain name that you copied in the preceding steps. Finally, in the <strong class="bold">Ad decision server</strong> field, type the modified VAST ad tag URL you created in the last step of the <em class="italic">Running transcription, finding topics, and creating a VAST ad tag URL</em> section. Scroll down and click <strong class="bold">Create configuration</strong>.<div id="_idContainer173" class="IMG---Figure"><img src="Images/B17528_08_27.jpg" alt="Figure 8.27 – Creating MediaTailor configuration" width="1306" height="728"/></div><p class="figure-caption">Figure 8.27 – Creating MediaTailor configuration</p></li>
				<li>The created <a id="_idIndexMarker550"/>configuration is displayed as shown in <a id="_idIndexMarker551"/>the following screenshot. Copy the HLS playback prefix as we need it in the next step.<div id="_idContainer174" class="IMG---Figure"><img src="Images/B17528_08_28.jpg" alt="Figure 8.28 – MediaTailor playback endpoint prefixes&#13;&#10;" width="1341" height="665"/></div><p class="figure-caption">Figure 8.28 – MediaTailor playback endpoint prefixes</p></li>
				<li>Download <a id="_idIndexMarker552"/>the VLC media player (<a href="https://www.videolan.org/">https://www.videolan.org/</a>) and open it. Click on <strong class="bold">File</strong> and then <strong class="bold">Open Network</strong>. In the <strong class="bold">URL</strong> field, paste the HLS playback prefix you copied in the previous step, and at <a id="_idIndexMarker553"/>the end of the string, after the forward slash, type <strong class="source-inline">bankdemo.m3u8</strong>. This is the manifest file for the MediaTailor video output with the ads inserted. The full URL should look as follows (this is an example representative URL): <strong class="source-inline">https://&lt;generated-hash-nr&gt;.mediatailor.us-east-1.amazonaws.com/v1/master/&lt;generated-hash-nr&gt;/&lt;chapter8-ad-campaign&gt;/bankdemo.m3u8</strong>.<div id="_idContainer175" class="IMG---Figure"><img src="Images/B17528_08_29.jpg" alt="Figure 8.29 – Testing the video output using the VLC media player" width="1013" height="594"/></div><p class="figure-caption">Figure 8.29 – Testing the video output using the VLC media player</p></li>
				<li>Click <strong class="bold">Open</strong>. The video will start playing momentarily. Please note it takes a couple of <a id="_idIndexMarker554"/>minutes for the ad insertion to reflect in <a id="_idIndexMarker555"/>the video. You should see a 10-second pre-roll, a 10-second mid-roll, and post-roll ad space in the video. Since we used the sample ad server URL, we don't see actual ads here, but once you register with an ad decision server, you can get the actual ad content included by following the steps in this solution.</li>
			</ol>
			<p>And that concludes the solution build for this chapter. Please refer to the <em class="italic">Further reading</em> section for more details on media content monetization with AWS AI and media services.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor115"/>Summary</h1>
			<p>In this chapter, we learned how to build an intelligent solution for media content monetization using the AWS AI services Amazon Transcribe and Amazon Comprehend, the Amazon CloudFront content delivery network, and the AWS media services Elemental MediaConvert and Elemental MediaTailor by taking a sample MP4 video file. We covered all this by first transcoding it into Apple HLS output files using MediaConvert, then creating atranscription from the MP4 file using Amazon Transcribe, analyzing the transcript, and detecting topics using Amazon Comprehend Topic Modeling, creating a VAST ad decision server URL. We also covered creating a distribution for the transcoded video content using Amazon CloudFront and using this distribution and the ad decision server URL to insert ads into the transcoded video using MediaTailor.</p>
			<p>For our solution, we started by introducing the content monetization use case for LiveRight, the requirement for a cost-effective expansion resulting in using content to pay for content creation. We then designed an architecture that used AWS AI services, media services, and the content delivery network to assemble an end-to-end walk-through of how to monetize content in video files. We assumed that you, the reader, are the architect assigned to this project, and we reviewed an overview of the solution components along with an architectural illustration in <em class="italic">Figure 8.1</em>.</p>
			<p>We then went through the prerequisites for the solution build, set up an Amazon SageMaker notebook instance, cloned our GitHub repository, and started executing the steps using the AWS Management Console and the code in the notebook based on instructions from this chapter.</p>
			<p>In the next chapter, we will look at an important use case, metadata extraction, using named entity recognition. We will, as before, introduce the use case, discuss how to design the architecture, establish the prerequisites, and walk through in detail the various steps required to build the solution.</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor116"/>Further reading</h1>
			<ul>
				<li>Monetizing your media workflows (<a href="https://aws.amazon.com/media/resources/monetization/">https://aws.amazon.com/media/resources/monetization/</a>)</li>
				<li><em class="italic">Announcing AWS Media Intelligence Solutions</em> by Vasi Philozelligence-solutions/)</li>
			</ul>
		</div>
	</div></body></html>