- en: Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional Neural Networks** (**CNNs**), or **ConvNets**, are a special
    class of feedforward networks; they are primarily used for computer vision tasks,
    but have also been adapted to other domains with unstructured data, such as natural
    language processing. As they are feedforward networks, they are very similar to
    the simple networks that we just learned about; information passes through them
    in one direction, and they are made up of layers, weights, and biases.'
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are the image recognition methods used by Facebook for image tagging, Amazon
    for product recommendations, and by self-driving cars for recognizing objects
    in their field of vision. In this chapter, we'll discuss the functions that make
    CNNs different from standard feedforward networks, and then jump into some examples
    of how to apply them to a variety of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully formed convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks for image tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are one of the most influential classes of networks in the history of deep
    learning. Invented by Yann LeCun (now head of **Facebook** **Artificial Intelligence
    Research**), CNNs really came into their own in 2012, with the introduction of
    deep Convolutional Neural Networks by Alex Krizhevsky.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plain old neural networks don''t scale well to images; CNNs adapt regular old
    feedforward neural networks by adding one or more convolutional layers as the
    input layer to the network. These convolutions are specifically designed to take
    in two-dimensional input, such as images or even sound, as illustrated in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1fb7dbc0-ba45-4829-a8ca-222b42a22d13.png)'
  prefs: []
  type: TYPE_IMG
- en: As you ...
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose we have an image recognition program to identify objects in an image,
    such as the example we referred to previously. Now imagine how hard it would be
    to try and classify an image with a standard feedforward network; each pixel in
    the image would be a feature that would have to be sent through the network with
    its own set of parameters. Our parameter space would be quite large, and we could
    likely run out of computing power! Images, which in technical terms are just high-dimensional
    vectors, require some special treatment.
  prefs: []
  type: TYPE_NORMAL
- en: 'What would happen if we were to try and accomplish this task with a basic feedforward
    network? Let''s recall that basic feedforward networks operate on top of vector
    spaces. We start with an image, which is made up of independent pixels. Let''s
    say our image is 32 pixels by 32 pixels; the input to our convolutional layer
    would be *32 x 32 x 3*, where *3* represent the RGB color scale of images. To
    translate this to vector space, we''d end up with a *3072 x 1* vector to represent
    the entire image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a32d2a13-e0b9-4a6a-aaf0-49bad075facb.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's also say our network has **10** neuron units; using the simple feedforward
    model, our weight matrix alone would have **30,720** learnable parameters. CNNs
    mitigate this problem, as well as others, with their convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional layers have four parameters that we have to define when we create
    a CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of filters, *K*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of each filter, *F*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stride, *S*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The amount of zero padding, *P*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we'll walk through each of these and look at how they play
    into the structure of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Layer parameters and structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolutional layers allow us to preserve the original image dimensions, thereby
    improving our ability to learn features and reduce our computational load. They
    do this by utilizing something called a **filter, **which slides across the image,
    learning features by computing dot products. For example, a typical filter on
    the first layer of a CNN might have size *5 x 5 x 3* (namely, *5* pixels width
    and height, and *3* because images have a depth of three colors, RGB).
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, it''s done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c5e4230-d491-4f74-864f-ed054c9ad84d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *w* represents our filter, but it also represents our learnable weights.
    We take a transpose of our input filter, ...
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Convolutional layers are often intertwined with **pooling layers**, which down
    sample the output of the previous convolutional layer in order to decrease the
    amount of parameters we need to compute. A particular form of these layers, **max
    pooling layers**, has become the most widely used variant. In general terms, max
    pooling layers tell us if a feature was present in the region, the previous convolutional
    layer was looking at; it looks for the most significant value in a particular
    region (the maximum value), and utilizes that value as a representation of the
    region, as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bafe3b38-c671-4e75-be57-9325252498a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Max pooling layers help subsequent convolutional layers focus on larger sections
    of the data, providing abstractions of the  that help both reduce overfitting
    and the amount of hyperparameters that we have to learn, ultimately reducing our
    computational cost. This form of automatic feature selection also helps prevent
    overfitting by preventing the network from focusing on too-specific areas of the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **fully connected layer** of a CNN works in the same manner as that of
    a vanilla feedforward network. This layer maps the outputs extracted from the
    image to the outputs that we desire from the network, such as a label for an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85b80730-0f00-4d28-b8c2-df0383730a18.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, our inputs are represented by the blue nodes, which
    are fed into the first convolutional layer, A. We then have a max pooling layer,
    a second convolutional layer, and finally the fully connected layer, which transforms
    our output into human– readable output. As with vanilla feedforward networks,
    we typically use a cross-entropy loss function for classification tasks. ...
  prefs: []
  type: TYPE_NORMAL
- en: The training process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we connect convolutional layers, a hyperparameter known as the **receptive
    field** or **filter size** prevents us from having to connect the unit to the
    entire input, but rather focuses on learning a particular feature. Our convolutional
    layers typically learn features from simple to complex. The first layer typically
    learns low-level features, the next layer learns mid-level features, and the last
    convolutional layer learns high-level features. One of the beautiful features
    of this is that we do not explicitly tell the network to learn different features
    at these various levels; it learns to differentiate its task in this manner through
    the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0da64743-509a-4be8-9549-0a7674c648a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we pass through this process, our network will develop a two-dimensional
    **activation map** to track the response of that particular filter at a given
    position. The network will learn to keep filters that activate when they reach
    an edge of a shape, or perhaps the color of an object. Sometimes, these will be
    lines and smaller pieces of the puzzle, or perhaps a filter will learn entire
    subsections of the image, maybe the horses'' ears, in the preceding diagram. Each
    filter for a specific convolutional layer leaves us with individual activation
    maps. If we have six filters for the image, we would have six activation maps,
    each focusing on something different within the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb448261-bb5d-4ba1-baf7-0be7192fabde.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our convolutional layers then become a sequence of stacked individual layers,
    interspersed by ReLU activation functions. We typically use ReLU or Leaky ReLU
    here to increase the training speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d584ec46-dcf8-44f0-a354-6530ad4f89b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Between these convolutional layers, we add in our max pooling layers. The output
    of these combined convolutional + max pooling layers will be sent to a fully connected
    layer, which will contain our transformation for classification and other tasks. Once
    we reach the end of a forward pass, the error is backpropagated through the network
    in the same manner as vanilla feedforward networks.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs for image tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s work on putting our new knowledge of CNNs to the test. We''re going
    to work through one of the most popular tasks for CNNs: image classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In an image classification task, our horse looks at a given image and determines
    the probability that a certain object is an image. In the following example, the
    image is 248 pixels wide, 400 pixels tall, and has three color channels: **red**, **green**,
    and **blue** (**RGB**). Therefore, the image consists of *248 x 400 x 3* numbers,
    or a total of 2,97, 600 numbers. Our job is to turn these numbers into a single
    classified label; is this horse*?*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e8f3d14-79ff-4a59-906d-b3c7be0c51ff.png)'
  prefs: []
  type: TYPE_IMG
- en: While this might seem a simple task for ...
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs have been seminal in solving many computer vision tasks. In this chapter,
    we learned about how these networks differ from our basic feedforward networks,
    what their structures are, and how we can utilize them. CNNs are primarily used
    for computer vision tasks, although they can be adapted for use in other unstructured
    domains, such as natural language processing and audio signal processing.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are made up of convolutional layers interspersed with pooling layers,all
    of which output to a fully connected layer**.** CNNs iterate over images using
    filters. Filters have a size and a stride, which is how quickly they iterate over
    an input image. Input consistency can be better guaranteed by utilizing the zero
    padding technique.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll learn about another important class of networks,
    called **Recurrent Neural Networks**.
  prefs: []
  type: TYPE_NORMAL
