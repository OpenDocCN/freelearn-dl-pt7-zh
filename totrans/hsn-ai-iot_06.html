<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Reinforcement Learning for IoT</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong><span class="koboSpan" id="kobo.2.1">Reinforcement learning</span></strong><span class="koboSpan" id="kobo.3.1"> (</span><strong><span class="koboSpan" id="kobo.4.1">RL</span></strong><span class="koboSpan" id="kobo.5.1">) is very different from both supervised and unsupervised learning. </span><span class="koboSpan" id="kobo.5.2">It's the way most living beings learn—interacting with the environment. </span><span><span class="koboSpan" id="kobo.6.1">In this</span></span><span class="koboSpan" id="kobo.7.1"> chapter</span><span><span class="koboSpan" id="kobo.8.1">, we'll study different algorithms employed for RL. </span><span class="koboSpan" id="kobo.8.2">As you progress through the chapter, you'll do the following:</span></span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.9.1">Learn what RL is and how it's different from supervised learning and unsupervised learning</span></span></li>
<li><span class="koboSpan" id="kobo.10.1">Lear different elements of RL</span></li>
<li><span class="koboSpan" id="kobo.11.1">Learn about some fascinating applications of RL in the real world</span></li>
<li><span class="koboSpan" id="kobo.12.1">Understand the OpenAI interface for training RL agents</span></li>
<li><span class="koboSpan" id="kobo.13.1">Learn about Q-learning and use it to train an RL agent</span></li>
<li><span><span class="koboSpan" id="kobo.14.1">Learn </span></span><span class="koboSpan" id="kobo.15.1"> about Deep Q-Networks and employ them to train an agent to play Atari</span></li>
<li><span><span class="koboSpan" id="kobo.16.1">Learn </span></span><span class="koboSpan" id="kobo.17.1">about the policy gradient algorithm and use it to </span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Introduction</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Have you ever observed infants and how they learn to turn over, sit up, crawl, and even stand? </span><span class="koboSpan" id="kobo.2.2">Have you watched how baby birds learn to fly—the parents throw them out of the nest, they flutter for some time, and they slowly learn to fly. </span><span class="koboSpan" id="kobo.2.3">All of this learning involves a component of the following:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">Trial and error</span></strong><span class="koboSpan" id="kobo.4.1">: The baby tries different ways and is unsuccessful many times before finally succeeding in doing it.</span></li>
<li><strong><span class="koboSpan" id="kobo.5.1">Goal-oriented</span></strong><span class="koboSpan" id="kobo.6.1">: All of the efforts are toward reaching a particular goal. </span><span class="koboSpan" id="kobo.6.2">The goal for the human baby can be to crawl, and for baby bird to fly.</span></li>
<li><strong><span class="koboSpan" id="kobo.7.1">Interaction with the environment</span></strong><span class="koboSpan" id="kobo.8.1">: The only feedback that they get is from the environment.</span></li>
</ul>
<div class="packt_infobox"><span class="koboSpan" id="kobo.9.1">This YouTube video is a beautiful video of a child learning to crawl and the stages in between </span><a href="https://www.youtube.com/watch?v=f3xWaOkXCSQ"><span class="koboSpan" id="kobo.10.1">https://www.youtube.com/watch?v=f3xWaOkXCSQ</span></a><span class="koboSpan" id="kobo.11.1">.</span></div>
<p><span class="koboSpan" id="kobo.12.1">The human baby learning to crawl or baby bird learning to fly are both examples of RL in nature.</span></p>
<p><span class="koboSpan" id="kobo.13.1">RL (in Artificial Intelligence) can be defined as a computational approach to goal-directed learning and decision-making, from interaction with the environment, under some idealized conditions. </span><span class="koboSpan" id="kobo.13.2">Let's elaborate upon this since we'll be using various computer algorithms to perform the learning—it's a computational approach. </span><span class="koboSpan" id="kobo.13.3">In all of the examples that we'll consider, the agent (learner) has a specific goal, which it's trying to achieve—it's a goal-directed approach. </span><span class="koboSpan" id="kobo.13.4">The agent in RL isn't given any explicit instructions, it learns only from its interaction with the environment. </span><span class="koboSpan" id="kobo.13.5">This interaction with the environment, as shown in the following diagram, is a cyclic process. </span><span class="koboSpan" id="kobo.13.6">The </span><strong><span class="koboSpan" id="kobo.14.1">Agent</span></strong><span class="koboSpan" id="kobo.15.1"> can sense the state of the </span><strong><span class="koboSpan" id="kobo.16.1">Environment</span></strong><span class="koboSpan" id="kobo.17.1">, and the </span><strong><span class="koboSpan" id="kobo.18.1">Agent</span></strong><span class="koboSpan" id="kobo.19.1"> can perform specific well-defined actions on the </span><strong><span class="koboSpan" id="kobo.20.1">Environment</span></strong><span class="koboSpan" id="kobo.21.1">; this causes two things: first, a change in the state of the environment, and second, a reward is generated </span><span><span class="koboSpan" id="kobo.22.1">(under ideal conditions)</span></span><span class="koboSpan" id="kobo.23.1">. </span><span class="koboSpan" id="kobo.23.2">This cycle continues:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.24.1"><img class="aligncenter size-full wp-image-1090 image-border" src="assets/d4f20501-75cf-40cb-bdb8-69a29e084d98.png" style="width:32.42em;height:23.75em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.25.1"> The interaction between agent and environment</span></div>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.26.1">Unlike supervised learning, the </span><strong><span class="koboSpan" id="kobo.27.1">Agent</span></strong><span class="koboSpan" id="kobo.28.1"> isn't presented with any examples. </span><span class="koboSpan" id="kobo.28.2">The </span><strong><span class="koboSpan" id="kobo.29.1">Agent</span></strong><span class="koboSpan" id="kobo.30.1"> doesn't know what the correct action is. </span><span class="koboSpan" id="kobo.30.2">And unlike unsupervised learning, the agent goal isn't to find some inherent structure in the input (the learning may find some structure, but that isn't the goal); instead, its goal is to maximize the rewards (in the long run). </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">RL terminology</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Before learning different algorithms, let's accustom ourselves to the RL terminology. </span><span class="koboSpan" id="kobo.2.2">For illustration purposes, let's consider two examples: an agent finding a route in a maze and an agent steering the wheel of a </span><strong><span class="koboSpan" id="kobo.3.1">Self-Driving Car</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">SDC</span></strong><span class="koboSpan" id="kobo.6.1">). </span><span class="koboSpan" id="kobo.6.2">The two are illustrated in the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1"><img src="assets/8745eea1-ff7e-493e-a5d9-2017ec988da7.png" style="width:41.33em;height:26.17em;"/></span></div>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.8.1">Two example RL scenarios</span></div>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.9.1">Before going further, let's acquaint ourselves with common RL terms:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.10.1">States </span><em><span class="koboSpan" id="kobo.11.1">s</span></em></strong><span class="koboSpan" id="kobo.12.1">: The states can be thought of as a set of tokens (or representation) that can define all of the possible states the environment can be in. </span><span class="koboSpan" id="kobo.12.2">The state can be continuous or discrete. </span><span class="koboSpan" id="kobo.12.3">For example, in the case of an agent finding a path through a maze, the state can be represented by a 4 × 4 array, with a </span><strong><span class="koboSpan" id="kobo.13.1">0</span></strong><span class="koboSpan" id="kobo.14.1"> representing an empty block, </span><strong><span class="koboSpan" id="kobo.15.1">1</span></strong><span class="koboSpan" id="kobo.16.1"> representing a block occupied by the agent, and </span><strong><span class="koboSpan" id="kobo.17.1">X</span></strong><span class="koboSpan" id="kobo.18.1"> the state that can't be occupied; the states here are discrete in nature. </span><span class="koboSpan" id="kobo.18.2">In the case of an agent steering the wheel, the state is the view in front of the SDC. </span><span class="koboSpan" id="kobo.18.3">The image contains continuous valued pixels. </span></li>
<li><strong><span class="koboSpan" id="kobo.19.1">Actions </span><em><span class="koboSpan" id="kobo.20.1">a</span></em><span class="koboSpan" id="kobo.21.1">(</span><em><span class="koboSpan" id="kobo.22.1">s</span></em><span class="koboSpan" id="kobo.23.1">)</span></strong><span class="koboSpan" id="kobo.24.1">: Actions are the set of all possible things that the agent can do in a particular state. </span><span class="koboSpan" id="kobo.24.2">The set of possible actions, </span><strong><em><span class="koboSpan" id="kobo.25.1">a</span></em></strong><span class="koboSpan" id="kobo.26.1">, depends on the present state, </span><em><strong><span class="koboSpan" id="kobo.27.1">s</span></strong></em><span class="koboSpan" id="kobo.28.1">. </span><span class="koboSpan" id="kobo.28.2">Actions may or may not result in the change of state. </span><span class="koboSpan" id="kobo.28.3">They can be discrete or continuous. </span><span class="koboSpan" id="kobo.28.4">The agent in the maze can perform five discrete actions </span><strong><span class="koboSpan" id="kobo.29.1">[up</span></strong><span class="koboSpan" id="kobo.30.1">, </span><strong><span class="koboSpan" id="kobo.31.1">down</span></strong><span class="koboSpan" id="kobo.32.1">, </span><strong><span class="koboSpan" id="kobo.33.1">left</span></strong><span class="koboSpan" id="kobo.34.1">, </span><strong><span class="koboSpan" id="kobo.35.1">right</span></strong><span class="koboSpan" id="kobo.36.1">, </span><strong><span class="koboSpan" id="kobo.37.1">no change]</span></strong><span class="koboSpan" id="kobo.38.1">. </span><span class="koboSpan" id="kobo.38.2">The SDC agent, on another hand, can rotate the steering wheel in a continuous range of angles. </span></li>
<li><strong><span class="koboSpan" id="kobo.39.1">Reward </span><em><span class="koboSpan" id="kobo.40.1">r(s, a, s'</span></em><span class="koboSpan" id="kobo.41.1">)</span></strong><span class="koboSpan" id="kobo.42.1">: It's a scalar value returned by the environment when the agent selects an action. </span><span class="koboSpan" id="kobo.42.2">It defines the goal; the agent gets a higher reward if the action brings it near the goal, and a low (or even negative) reward otherwise. </span><span class="koboSpan" id="kobo.42.3">How we define a reward is totally up to us—in the case of the maze, we can define the reward as the Euclidean distance between the agent's current position and goal. </span><span class="koboSpan" id="kobo.42.4">The SDC agent reward can be that the car is on the road (positive reward) or off the road (negative reward). </span></li>
<li><strong><span class="koboSpan" id="kobo.43.1">Policy π(</span><em><span class="koboSpan" id="kobo.44.1">s</span></em><span class="koboSpan" id="kobo.45.1">)</span></strong><span class="koboSpan" id="kobo.46.1">: It defines a mapping between each state and the action to take in that state. </span><span class="koboSpan" id="kobo.46.2">The policy can be deterministic</span><span><span class="koboSpan" id="kobo.47.1">—</span></span><span class="koboSpan" id="kobo.48.1">that is, for each state a well-defined policy. </span><span class="koboSpan" id="kobo.48.2">Like for the maze agent, a policy can be that if the top block is empty, move up. </span><span class="koboSpan" id="kobo.48.3">The policy can also be stochastic</span><span><span class="koboSpan" id="kobo.49.1">—</span></span><span class="koboSpan" id="kobo.50.1">that is, where an action is taken by some probability. </span><span class="koboSpan" id="kobo.50.2">It can be implemented as a simple look-up table, or it can be a function dependent on the present state. </span><span class="koboSpan" id="kobo.50.3">The policy is the core of the RL agent. </span><span class="koboSpan" id="kobo.50.4">In this chapter, we'll learn about different algorithms that help the agent to learn the policy. </span></li>
<li><strong><span class="koboSpan" id="kobo.51.1">Value function </span><em><span class="koboSpan" id="kobo.52.1">V</span></em><span class="koboSpan" id="kobo.53.1">(</span><em><span class="koboSpan" id="kobo.54.1">s</span></em><span class="koboSpan" id="kobo.55.1">)</span></strong><span class="koboSpan" id="kobo.56.1">: It defines the goodness of a state in the long run. </span><span class="koboSpan" id="kobo.56.2">It can be thought of as the total amount of reward the agent can expect to accumulate over the future, starting from the state </span><strong><em><span class="koboSpan" id="kobo.57.1">s</span></em></strong><span class="koboSpan" id="kobo.58.1">. </span><span class="koboSpan" id="kobo.58.2">You can think of it as long-term goodness as opposed to the immediate goodness of rewards. </span><span class="koboSpan" id="kobo.58.3">What do you think is more important, maximizing the reward or maximizing the value function? </span><span class="koboSpan" id="kobo.58.4">Yes, you guessed right: just as in chess, we sometimes lose a pawn to win the game a few steps later, and so the agent should try to maximize the value function. </span><span class="koboSpan" id="kobo.58.5">There are two ways in which the value function is normally considered:
</span><ul>
<li><strong><span class="koboSpan" id="kobo.59.1">Value function </span><em><span class="koboSpan" id="kobo.60.1">V</span></em><sup><span class="koboSpan" id="kobo.61.1">π</span></sup><span class="koboSpan" id="kobo.62.1">(</span><em><span class="koboSpan" id="kobo.63.1">s</span></em><span class="koboSpan" id="kobo.64.1">)</span></strong><span class="koboSpan" id="kobo.65.1">: It's the goodness of state following the policy </span><em><span class="koboSpan" id="kobo.66.1">π</span></em><span class="koboSpan" id="kobo.67.1">. </span><span class="koboSpan" id="kobo.67.2">Mathematically, at state </span><em><span class="koboSpan" id="kobo.68.1">s</span></em><span class="koboSpan" id="kobo.69.1">, it's the expected cumulative reward from following the policy, </span><span><em><span class="koboSpan" id="kobo.70.1">π</span></em><span class="koboSpan" id="kobo.71.1">:</span></span></li>
</ul>
</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.72.1"><img style="text-align: center;color: #333333;font-size: 1em;width:18.17em;height:1.83em;" class="fm-editor-equation" src="assets/f9885ead-b1ba-41fd-b021-5701678e2699.png"/></span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.73.1">Value-state function (or </span><em><span class="koboSpan" id="kobo.74.1">Q</span></em><span class="koboSpan" id="kobo.75.1">-function) </span><em><span class="koboSpan" id="kobo.76.1">Q</span></em><sup><span class="koboSpan" id="kobo.77.1">π</span></sup><span class="koboSpan" id="kobo.78.1">(</span><em><span class="koboSpan" id="kobo.79.1">s</span></em><span class="koboSpan" id="kobo.80.1">, </span><em><span class="koboSpan" id="kobo.81.1">a</span></em><span class="koboSpan" id="kobo.82.1">)</span></strong><span class="koboSpan" id="kobo.83.1">: It's the goodness of a state </span><em><span class="koboSpan" id="kobo.84.1">s</span></em><span class="koboSpan" id="kobo.85.1">, taking action </span><em><span class="koboSpan" id="kobo.86.1">a</span></em><span class="koboSpan" id="kobo.87.1">, and thereafter following policy </span><em><span class="koboSpan" id="kobo.88.1">π</span></em><span class="koboSpan" id="kobo.89.1">. </span><span class="koboSpan" id="kobo.89.2">Mathematically, we can say that for a state-action pair (</span><em><span class="koboSpan" id="kobo.90.1">s</span></em><span class="koboSpan" id="kobo.91.1">, </span><em><span class="koboSpan" id="kobo.92.1">a</span></em><span class="koboSpan" id="kobo.93.1">), it's the expected cumulative reward from taking action </span><em><span class="koboSpan" id="kobo.94.1">a</span></em><span class="koboSpan" id="kobo.95.1"> in state </span><em><span class="koboSpan" id="kobo.96.1">s</span></em><span class="koboSpan" id="kobo.97.1"> and then following policy </span><span><em><span class="koboSpan" id="kobo.98.1">π</span></em><span class="koboSpan" id="kobo.99.1">:</span></span></li>
</ul>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.100.1"><img class="fm-editor-equation" src="assets/c6648db3-0b91-4b58-b2a5-9923e080dbba.png" style="width:22.50em;height:1.75em;"/></span></div>
<p style="padding-left: 60px"><em><span class="koboSpan" id="kobo.101.1">γ</span></em><span class="koboSpan" id="kobo.102.1"> is the discount factor, and its value determines how much importance we give to the immediate rewards as compared to rewards received later on. </span><span class="koboSpan" id="kobo.102.2">A high value of discount factor decides how far into the future an agent can see. </span><span class="koboSpan" id="kobo.102.3">An ideal choice of </span><em><span class="koboSpan" id="kobo.103.1">γ</span></em><span class="koboSpan" id="kobo.104.1"> in many successful RL algorithms has been a value of </span><em><span class="koboSpan" id="kobo.105.1">0.97</span></em><span class="koboSpan" id="kobo.106.1">. </span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.107.1">Model of the environment</span></strong><span class="koboSpan" id="kobo.108.1">: It's an optional element. </span><span class="koboSpan" id="kobo.108.2">It mimics the behavior of the environment, and it contains the physics of the environment; in other words, it defines how the environment will behave. </span><span class="koboSpan" id="kobo.108.3">The model of the environment is defined by the transition probability to the next state.</span></li>
</ul>
<div class="packt_infobox"><span class="koboSpan" id="kobo.109.1">An RL problem is mathematically formulated as a </span><strong><span class="koboSpan" id="kobo.110.1">Markov Decision Process</span></strong><span class="koboSpan" id="kobo.111.1"> (</span><strong><span class="koboSpan" id="kobo.112.1">MDP</span></strong><span class="koboSpan" id="kobo.113.1">), and it follows the Markov property</span><span><span class="koboSpan" id="kobo.114.1">—</span></span><span class="koboSpan" id="kobo.115.1"> that is, </span><em><span class="koboSpan" id="kobo.116.1">the current state completely characterizes the state of the world</span></em><span class="koboSpan" id="kobo.117.1">. </span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Deep reinforcement learning</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">RL algorithms can be classified into two, based on what they iterate/approximate:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">Value-based methods</span></strong><span class="koboSpan" id="kobo.4.1">: In these methods, the algorithms take the action that maximizes the value function. </span><span class="koboSpan" id="kobo.4.2">The agent here learns to predict how good a given state or action would be. </span><span class="koboSpan" id="kobo.4.3">Hence, here, the aim is to find the optimal value. </span><span class="koboSpan" id="kobo.4.4">An example of the value-based method is Q-learning. </span><span class="koboSpan" id="kobo.4.5">Consider, for example, our RL agent in a maze: assuming that the value of each state is the negative of the number of steps needed to reach from that box to the goal, then, at each time step, the agent will choose the action that takes it to a state with optimal value, as in the following diagram. </span><span class="koboSpan" id="kobo.4.6">So, starting from a value of </span><strong><span class="koboSpan" id="kobo.5.1">-6</span></strong><span class="koboSpan" id="kobo.6.1">, it'll move to </span><strong><span class="koboSpan" id="kobo.7.1">-5</span></strong><span class="koboSpan" id="kobo.8.1">, </span><strong><span class="koboSpan" id="kobo.9.1">-4</span></strong><span class="koboSpan" id="kobo.10.1">, </span><strong><span class="koboSpan" id="kobo.11.1">-3</span></strong><span class="koboSpan" id="kobo.12.1">, </span><strong><span class="koboSpan" id="kobo.13.1">-2</span></strong><span class="koboSpan" id="kobo.14.1">, </span><strong><span class="koboSpan" id="kobo.15.1">-1</span></strong><span class="koboSpan" id="kobo.16.1">, and eventually reach the goal with the value </span><strong><span class="koboSpan" id="kobo.17.1">0</span></strong><span class="koboSpan" id="kobo.18.1">:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.19.1"><img class="aligncenter size-full wp-image-1091 image-border" src="assets/bacdaff7-2312-4005-840d-a6056cabe610.png" style="width:20.08em;height:21.75em;"/></span></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span class="koboSpan" id="kobo.20.1"> The maze world with the value of each box</span></div>
<ul>
<li><strong><span class="koboSpan" id="kobo.21.1">Policy-based methods</span></strong><span class="koboSpan" id="kobo.22.1">: In these methods, the algorithms predict the best policy which maximizes the value function. </span><span class="koboSpan" id="kobo.22.2">The aim is to find the optimal policy. </span><span><span class="koboSpan" id="kobo.23.1">An example of the policy-based method is</span></span><span class="koboSpan" id="kobo.24.1"> policy gradients. </span><span class="koboSpan" id="kobo.24.2">Here, we approximate the policy function, which allows us to map each state to the best corresponding action. </span></li>
</ul>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.25.1">We can use neural networks as a function approximator to get an approximate value of either policy or value. </span><span class="koboSpan" id="kobo.25.2">When we use deep neural networks as a policy approximator or value approximator, we call it </span><strong><span class="koboSpan" id="kobo.26.1">deep reinforcement learning</span></strong><span class="koboSpan" id="kobo.27.1"> (</span><strong><span class="koboSpan" id="kobo.28.1">DRL</span></strong><span class="koboSpan" id="kobo.29.1">). </span><span class="koboSpan" id="kobo.29.2">DRL has, in the recent past, given very successful results, hence, in this chapter, our will focus will be on DRL.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Some successful applications</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the last few years, RL has been successfully used in a variety of tasks, especially in game-playing and robotics. </span><span class="koboSpan" id="kobo.2.2">Let's acquaint ourselves with some success stories of RL before learning its algorithms:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">AlphaGo Zero</span></strong><span class="koboSpan" id="kobo.4.1">: Developed by Google's DeepMind team, the AlphaGo Zero </span><em><span class="koboSpan" id="kobo.5.1">Mastering the game of Go without any human knowledge</span></em><span class="koboSpan" id="kobo.6.1">, starts from an absolutely blank slate (</span><strong><span class="koboSpan" id="kobo.7.1">tabula rasa</span></strong><span class="koboSpan" id="kobo.8.1">). </span><span class="koboSpan" id="kobo.8.2">The AlphaGo Zero uses one neural network to approximate both the move probabilities and value. </span><span class="koboSpan" id="kobo.8.3">This neural network takes as input the raw board representation. </span><span class="koboSpan" id="kobo.8.4">It uses a Monte Carlo Tree search guided by the neural network to select the moves. </span><span class="koboSpan" id="kobo.8.5">The reinforcement learning algorithm incorporates look-ahead search inside the training loop. </span><span class="koboSpan" id="kobo.8.6">It was trained for 40 days using a 40-block residual CNN and, over the course of training, it played about 29 million games (a big number!). </span><span class="koboSpan" id="kobo.8.7">The neural network was optimized on Google Cloud using TensorFlow, with 64 GPU workers and 19 CPU parameter servers. </span><span class="koboSpan" id="kobo.8.8">You can access the paper here: </span><a href="https://www.nature.com/articles/nature24270"><span class="koboSpan" id="kobo.9.1">https://www.nature.com/articles/nature24270</span></a><span class="koboSpan" id="kobo.10.1">. </span></li>
<li><strong><span class="koboSpan" id="kobo.11.1">AI-controlled sailplanes</span></strong><span class="koboSpan" id="kobo.12.1">: Microsoft developed a controller system that can run on many different autopilot hardware platforms such as Pixhawk and Raspberry Pi 3. </span><span class="koboSpan" id="kobo.12.2">It can keep the sailplane in the air without using a motor, by autonomously finding and catching rides on naturally occurring thermals. </span><span class="koboSpan" id="kobo.12.3">The controller helps the sailplane to operate on its own; it detects and uses thermals to travel without the aid of a motor or a person. </span><span class="koboSpan" id="kobo.12.4">They implemented it as a partially observable MDP. </span><span class="koboSpan" id="kobo.12.5">They employ the Bayesian reinforcement learning and use the Monte Carlo tree search to search for the best action. </span><span class="koboSpan" id="kobo.12.6">They've divided the whole system into level planners—a high-level planer that makes a decision based on experience and a low-level planner that uses Bayesian reinforcement learning to detect and latch onto thermals in real time. </span><span class="koboSpan" id="kobo.12.7">You can see the sailplane in action at Microsoft News: </span><a href="https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/"><span class="koboSpan" id="kobo.13.1">https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/</span></a><span class="koboSpan" id="kobo.14.1">. </span></li>
<li><strong><span class="koboSpan" id="kobo.15.1">Locomotion behavior</span></strong><span class="koboSpan" id="kobo.16.1">: In the paper </span><em><span class="koboSpan" id="kobo.17.1">Emergence of Locomotion Behaviours in Rich Environments</span></em><span class="koboSpan" id="kobo.18.1"> (</span><a href="https://arxiv.org/pdf/1707.02286.pdf"><span class="koboSpan" id="kobo.19.1">https://arxiv.org/pdf/1707.02286.pdf</span></a><span class="koboSpan" id="kobo.20.1">), DeepMind researchers provided the agents with rich and diverse environments. </span><span class="koboSpan" id="kobo.20.2">The environments presented a spectrum of challenges at different levels of difficulty. </span><span class="koboSpan" id="kobo.20.3">The agent was provided with difficulties in increasing order; this led the agent to learn sophisticated locomotion skills without performing any reward engineering.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Simulated environments</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Since RL involves trial and error, it makes sense to train our RL agent first in a simulated environment. </span><span class="koboSpan" id="kobo.2.2">While a large number of applications exist that can be used for the creation of an environment, some popular ones include the following:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">OpenAI gym</span></strong><span class="koboSpan" id="kobo.4.1">: It contains a collection of environments that we can use to train our RL agents. </span><span class="koboSpan" id="kobo.4.2">In this chapter, we'll be using the OpenAI gym interface.</span></li>
<li><strong><span class="koboSpan" id="kobo.5.1">Unity ML-Agents SDK</span></strong><span class="koboSpan" id="kobo.6.1">: It allows </span><span><span class="koboSpan" id="kobo.7.1">developers to transform games and simulations created using the Unity editor into environments where intelligent agents can be trained using DRL, evolutionary strategies, or other machine learning methods through a simple-to-use Python API. </span><span class="koboSpan" id="kobo.7.2">It works with TensorFlow and provides the ability to train intelligent agents for two-dimensional/three-dimensional and VR/AR games. </span><span class="koboSpan" id="kobo.7.3">You can learn more about it here: </span><a href="https://github.com/Unity-Technologies/ml-agents"><span class="koboSpan" id="kobo.8.1">https://github.com/Unity-Technologies/ml-agents</span></a><span class="koboSpan" id="kobo.9.1">. </span></span></li>
<li><strong><span class="koboSpan" id="kobo.10.1">Gazebo</span></strong><span class="koboSpan" id="kobo.11.1">: In Gazebo, we can build three-dimensional worlds with physics-based simulation. </span><span class="koboSpan" id="kobo.11.2">Gazebo along with </span><strong><span class="koboSpan" id="kobo.12.1">Robot Operating System</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong><span class="koboSpan" id="kobo.14.1">ROS)</span></strong><span><span class="koboSpan" id="kobo.15.1"> </span></span><span class="koboSpan" id="kobo.16.1">and the OpenAI gym interface is gym-gazebo and can be used to train RL agents. </span><span class="koboSpan" id="kobo.16.2">To know more about this, you can refer to the whitepaper: </span><a href="http://erlerobotics.com/whitepaper/robot_gym.pdf"><span class="koboSpan" id="kobo.17.1">http://erlerobotics.com/whitepaper/robot_gym.pdf</span></a><span class="koboSpan" id="kobo.18.1">. </span></li>
<li><strong><span class="koboSpan" id="kobo.19.1">Blender</span></strong><span><strong><span class="koboSpan" id="kobo.20.1"> learning environment</span></strong><span class="koboSpan" id="kobo.21.1">: It's a Python interface for the Blender game engine, and it also works over OpenAI gym.</span></span> <span><span class="koboSpan" id="kobo.22.1">It has it's base B</span></span><span><span class="koboSpan" id="kobo.23.1">lender</span></span><span><span class="koboSpan" id="kobo.24.1">. </span><span class="koboSpan" id="kobo.24.2">A free three-dimensional </span></span><span class="koboSpan" id="kobo.25.1">modeling soft</span><span><span class="koboSpan" id="kobo.26.1">ware with an integrated game engine, this provides an easy-to-use, powerful set of tools for creating games. </span><span class="koboSpan" id="kobo.26.2">It provides an interface to the Blender game engine, and the games</span></span><span class="koboSpan" id="kobo.27.1"> themselves </span><span><span class="koboSpan" id="kobo.28.1">are designed in Blender. </span><span class="koboSpan" id="kobo.28.2">We can then create the custom virtual environment to train an RL agent on a specific problem (</span><a href="https://github.com/LouisFoucard/gym-blender"><span class="koboSpan" id="kobo.29.1">https://github.com/LouisFoucard/gym-blender</span></a><span class="koboSpan" id="kobo.30.1">).</span></span></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">OpenAI gym</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">OpenAI gym is an open source toolkit to develop and compare RL algorithms. </span><span class="koboSpan" id="kobo.2.2">It contains a variety of simulated environments that can be used to train agents and develop new RL algorithms. To start, you'll first have to install </span><kbd><span class="koboSpan" id="kobo.3.1">gym</span></kbd><span class="koboSpan" id="kobo.4.1">. </span><span class="koboSpan" id="kobo.4.2">For Python 3.5+, you can install </span><kbd><span class="koboSpan" id="kobo.5.1">gym</span></kbd><span class="koboSpan" id="kobo.6.1"> using </span><kbd><span class="koboSpan" id="kobo.7.1">pip</span></kbd><span class="koboSpan" id="kobo.8.1">: </span></p>
<pre><span class="koboSpan" id="kobo.9.1">pip install gym</span></pre>
<p><span class="koboSpan" id="kobo.10.1">OpenAI gym supports various environments, from simple text-based to three-dimensional. </span><span class="koboSpan" id="kobo.10.2">The environments supported in the latest version can be grouped as follows:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.11.1">Algorithms</span></strong><span class="koboSpan" id="kobo.12.1">: It contains environments that involve performing computations such as addition. </span><span class="koboSpan" id="kobo.12.2">While we can easily perform the computations on a computer, what makes these problems interesting as an RL problem is that the agent learns these tasks purely by example.</span></li>
<li><strong><span class="koboSpan" id="kobo.13.1">Atari</span></strong><span class="koboSpan" id="kobo.14.1">: This environment provides a wide variety of classical Atari/arcade games.</span></li>
<li><strong><span class="koboSpan" id="kobo.15.1">Box2D</span></strong><span class="koboSpan" id="kobo.16.1">: It contains robotics tasks in two dimensions such as a car racing agent or bipedal robot walk.</span></li>
<li><strong><span class="koboSpan" id="kobo.17.1">Classic control</span></strong><span class="koboSpan" id="kobo.18.1">: This contains the classical control theory problems, such as balancing a cart pole.</span></li>
<li><strong><span class="koboSpan" id="kobo.19.1">MuJoCo</span></strong><span class="koboSpan" id="kobo.20.1">: This is proprietary (you can get a one-month free trial). </span><span class="koboSpan" id="kobo.20.2">It supports various robot simulation tasks. </span><span class="koboSpan" id="kobo.20.3">The environment includes a physics engine, hence, it's used for training robotic tasks.</span></li>
<li><strong><span class="koboSpan" id="kobo.21.1">Robotics</span></strong><span class="koboSpan" id="kobo.22.1">: This environment too uses the physics engine of MuJoCo. </span><span class="koboSpan" id="kobo.22.2">It simulates goal-based tasks for fetch and shadow-hand robots. </span></li>
<li><strong><span class="koboSpan" id="kobo.23.1">Toy text</span></strong><span class="koboSpan" id="kobo.24.1">: It's a simple text-based environment</span><span><span class="koboSpan" id="kobo.25.1">—v</span></span><span class="koboSpan" id="kobo.26.1">ery good for beginners.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.27.1">To get a complete list of environments under these groups, you can visit: </span><a href="https://gym.openai.com/envs/#atari"><span class="koboSpan" id="kobo.28.1">https://gym.openai.com/envs/#atari</span></a><span class="koboSpan" id="kobo.29.1">. </span><span class="koboSpan" id="kobo.29.2">The best part of the OpenAI interface is that all of the environments can be accessed with the same minimum interface. </span><span class="koboSpan" id="kobo.29.3">To get a list of all available environments in your installation, you can use the following code:</span></p>
<pre><span class="koboSpan" id="kobo.30.1">from gym import envs</span><br/><span class="koboSpan" id="kobo.31.1">print(envs.registry.all())</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.32.1">This will provide a list of all installed environments along with their environment ID, which is a string. </span><span class="koboSpan" id="kobo.32.2">It's also possible to add your own environment in the </span><kbd><span class="koboSpan" id="kobo.33.1">gym</span></kbd><span class="koboSpan" id="kobo.34.1"> registry. </span><span class="koboSpan" id="kobo.34.2">To create an environment, we use the </span><kbd><span class="koboSpan" id="kobo.35.1">make</span></kbd><span class="koboSpan" id="kobo.36.1"> command with the environment name passed as a string. </span><span class="koboSpan" id="kobo.36.2">For example, to create a game using the Pong environment, the string we need will be </span><kbd><span><span class="koboSpan" id="kobo.37.1">Pong-v0</span></span></kbd><span class="koboSpan" id="kobo.38.1">. </span><span class="koboSpan" id="kobo.38.2">The </span><kbd><span class="koboSpan" id="kobo.39.1">make</span></kbd><span class="koboSpan" id="kobo.40.1"> command creates the environment, and the </span><kbd><span class="koboSpan" id="kobo.41.1">reset</span></kbd><span class="koboSpan" id="kobo.42.1"> command is used to activate the environment. </span><span class="koboSpan" id="kobo.42.2">The </span><kbd><span class="koboSpan" id="kobo.43.1">reset</span></kbd><span class="koboSpan" id="kobo.44.1"> command returns the environment in an initial state. </span><span class="koboSpan" id="kobo.44.2">The state is represented as an array:</span></p>
<pre><span class="koboSpan" id="kobo.45.1">import gym</span><br/><span class="koboSpan" id="kobo.46.1">env = gym.make('Pong-v0')</span><br/><span class="koboSpan" id="kobo.47.1">obs = env.reset()</span><br/><span class="koboSpan" id="kobo.48.1">env.render()</span></pre>
<p><span class="koboSpan" id="kobo.49.1">The state space of </span><kbd><span class="koboSpan" id="kobo.50.1">Pong-v0</span></kbd><span class="koboSpan" id="kobo.51.1"> is given by an array of the size 210×160×3, which actually represents the raw pixel values for the Pong game. </span><span class="koboSpan" id="kobo.51.2">On the other hand, if you create a </span><strong><span class="koboSpan" id="kobo.52.1">Go9×9-v0</span></strong><span class="koboSpan" id="kobo.53.1"> environment, the state is defined by a 3×9×9 array. </span><span class="koboSpan" id="kobo.53.2">We can visualize the environment using the </span><kbd><span class="koboSpan" id="kobo.54.1">render</span></kbd><span class="koboSpan" id="kobo.55.1"> command. </span><span class="koboSpan" id="kobo.55.2">The following diagram shows the rendered environment for the </span><strong><span class="koboSpan" id="kobo.56.1">Pong-v0</span></strong><span class="koboSpan" id="kobo.57.1"> and </span><strong><span class="koboSpan" id="kobo.58.1">Go9x9-v0</span></strong><span class="koboSpan" id="kobo.59.1"> environments at the initial state:.</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.60.1"><img class="aligncenter size-full wp-image-1092 image-border" src="assets/b2bc09c8-c223-4b72-84a3-c6ea180afe56.png" style="width:126.08em;height:68.50em;"/></span></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span class="koboSpan" id="kobo.61.1"> The rendered environments for Pong-v0 and Go9x9-v0</span></div>
<div class="packt_tip"><span class="koboSpan" id="kobo.62.1">The </span><kbd><span class="koboSpan" id="kobo.63.1">render</span></kbd><span class="koboSpan" id="kobo.64.1"> commands pop up a window. </span><span class="koboSpan" id="kobo.64.2">If you want to display the environment inline, then you can use Matplotlib inline and change the </span><kbd><span class="koboSpan" id="kobo.65.1">render</span></kbd><span class="koboSpan" id="kobo.66.1"> command to </span><kbd><span class="koboSpan" id="kobo.67.1">plt.imshow(env.render(mode='rgb_array'))</span></kbd><span class="koboSpan" id="kobo.68.1">. </span><span class="koboSpan" id="kobo.68.2">This will show the environment inline in the Jupyter Notebook.</span></div>
<p><span class="koboSpan" id="kobo.69.1">The environment contains the </span><kbd><span class="koboSpan" id="kobo.70.1">action_space</span></kbd> <span><span class="koboSpan" id="kobo.71.1">variable, </span></span><span class="koboSpan" id="kobo.72.1">which determines the possible actions in the environment. </span><span class="koboSpan" id="kobo.72.2">We can select a random action using the </span><kbd><span class="koboSpan" id="kobo.73.1">sample()</span></kbd><span class="koboSpan" id="kobo.74.1"> function. </span><span class="koboSpan" id="kobo.74.2">The selected action can affect the environment using the </span><kbd><span class="koboSpan" id="kobo.75.1">step</span></kbd><span class="koboSpan" id="kobo.76.1"> function. </span><span class="koboSpan" id="kobo.76.2">The </span><kbd><span class="koboSpan" id="kobo.77.1">step</span></kbd><span class="koboSpan" id="kobo.78.1"> function performs the selected action on the environment; it returns the changed state, the reward, a Boolean informing whether the game is over or not, and some information about the environment that can be useful for debugging, but isn't used while working with RL agents. </span><span class="koboSpan" id="kobo.78.2">The following code shows a game of Pong with the agent playing a random move. </span><span class="koboSpan" id="kobo.78.3">We're storing the state at each time step in an array, </span><kbd><span class="koboSpan" id="kobo.79.1">frames</span></kbd><span class="koboSpan" id="kobo.80.1">, so that we can later see the game:</span></p>
<pre><span class="koboSpan" id="kobo.81.1">frames = [] # array to store state space at each step</span><br/><span class="koboSpan" id="kobo.82.1">for _ in range(300):</span><br/><span class="koboSpan" id="kobo.83.1">    frames.append(env.render(mode='rgb_array'))</span><br/><span class="koboSpan" id="kobo.84.1">    obs,reward,done, _ = env.render(env.action_space.sample())</span><br/><span class="koboSpan" id="kobo.85.1">    if done:</span><br/><span class="koboSpan" id="kobo.86.1">        break</span></pre>
<p><span class="koboSpan" id="kobo.87.1">These frames can be displayed as a continuously playing GIF-style image in the Jupyter Notebook with the help of the animation function in Matplotlib and IPython:</span></p>
<pre><span class="koboSpan" id="kobo.88.1">import matplotlib.animation as animation</span><br/><span class="koboSpan" id="kobo.89.1">from JSAnimation.Ipython_display import display_animation</span><br/><span class="koboSpan" id="kobo.90.1">from IPython.display import display</span><br/><br/><span class="koboSpan" id="kobo.91.1">patch = plt.imshow(frames[0])</span><br/><span class="koboSpan" id="kobo.92.1">plt.axis('off')</span><br/><br/><span class="koboSpan" id="kobo.93.1">def animate(i)</span><br/><span class="koboSpan" id="kobo.94.1">    patch.set_data(frames[i])</span><br/><br/><span class="koboSpan" id="kobo.95.1">anim = animation.FuncAnimation(plt.gcf(), animate, \</span><br/><span class="koboSpan" id="kobo.96.1">        frames=len(frames), interval=100)</span><br/><br/><span class="koboSpan" id="kobo.97.1">display(display_animation(anim, default_mode='loop')</span></pre>
<p><span class="koboSpan" id="kobo.98.1">Normally, to train an agent, we'll need a very large number of steps, and so it won't be feasible to store the state space at each step. </span><span class="koboSpan" id="kobo.98.2">We can either choose to store after every 500th (or any other number you wish) step in the preceding algorithm. </span><span class="koboSpan" id="kobo.98.3">Instead, we can use </span><span><span class="koboSpan" id="kobo.99.1">the OpenAI gym wrapper to save the game as a video. </span><span class="koboSpan" id="kobo.99.2">To do so, we need to first import wrappers, then create the environment, and finally use Monitor. </span><span class="koboSpan" id="kobo.99.3">By default, it will store the video of 1, 8, 27, 64, and so on and then every 1,000</span><sup><span class="koboSpan" id="kobo.100.1">th</span></sup><span class="koboSpan" id="kobo.101.1"> episode (episode numbers with perfect cubes); each training, by default, is saved in one folder. </span><span class="koboSpan" id="kobo.101.2">The code to do it is as follows:</span></span></p>
<pre><span class="koboSpan" id="kobo.102.1">import gym</span><br/><span class="koboSpan" id="kobo.103.1">from gym import wrappers</span><br/><span class="koboSpan" id="kobo.104.1">env = gym.make('Pong-v0')</span><br/><span class="koboSpan" id="kobo.105.1">env = wrappers.Monitor(env, '/save-mov', force=True)</span><br/><span class="koboSpan" id="kobo.106.1"># Follow it with the code above where env is rendered and agent</span><br/><span class="koboSpan" id="kobo.107.1"># selects a random action</span><br/><br/></pre>
<p><span class="koboSpan" id="kobo.108.1">If you want to use the same folder in the next training, you can choose the </span><kbd><span class="koboSpan" id="kobo.109.1">force=True</span></kbd> <span><span class="koboSpan" id="kobo.110.1">option </span></span><span class="koboSpan" id="kobo.111.1">in the </span><kbd><span class="koboSpan" id="kobo.112.1">Monitor</span></kbd><span class="koboSpan" id="kobo.113.1"> method call. </span><span class="koboSpan" id="kobo.113.2">In the end, we should close the environment using the </span><kbd><span class="koboSpan" id="kobo.114.1">close</span></kbd><span class="koboSpan" id="kobo.115.1"> function:</span></p>
<pre><span class="koboSpan" id="kobo.116.1">env.close()</span></pre>
<p><span class="koboSpan" id="kobo.117.1">The preceding codes are available in the  </span><kbd><span class="koboSpan" id="kobo.118.1">OpenAI_practice.ipynb</span></kbd> <span><span class="koboSpan" id="kobo.119.1">Jupyter Notebook </span></span><span class="koboSpan" id="kobo.120.1">in the folder for </span><a href="01e534ff-b0a2-4b5e-bc9a-fd65c527ac7d.xhtml"><span class="koboSpan" id="kobo.121.1">Chapter 6</span></a><span class="koboSpan" id="kobo.122.1">, </span><em><span class="koboSpan" id="kobo.123.1">Reinforcement Learning for IoT,</span></em><span class="koboSpan" id="kobo.124.1"> in GitHub.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Q-learning</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In his doctoral thesis, </span><em><span class="koboSpan" id="kobo.3.1">Learning from delayed rewards</span></em><span class="koboSpan" id="kobo.4.1">, Watkins introduced the concept of Q-learning in the year 1989. </span><span class="koboSpan" id="kobo.4.2">T</span><span><span class="koboSpan" id="kobo.5.1">he goal of Q-learning is to learn an optimal action selection policy. </span></span><span class="koboSpan" id="kobo.6.1">Given a specific state, </span><em><span class="koboSpan" id="kobo.7.1">s</span></em><span class="koboSpan" id="kobo.8.1">, and taking a specific action, </span><em><span class="koboSpan" id="kobo.9.1">a</span></em><span class="koboSpan" id="kobo.10.1">, Q-learning attempts to learn the value of the state </span><em><span class="koboSpan" id="kobo.11.1">s</span></em><span class="koboSpan" id="kobo.12.1">. In its simplest version, Q-learning can be implemented with the help of look-up tables. </span><span class="koboSpan" id="kobo.12.2">We maintain a table of values for every state (row) and action (column) possible in the environment. </span><span class="koboSpan" id="kobo.12.3">The algorithm attempts to learn the value—that is, how good it is to take a particular action in the given state. </span></p>
<p><span class="koboSpan" id="kobo.13.1">We start by initializing all of the entries in the Q-table to </span><em><span class="koboSpan" id="kobo.14.1">0</span></em><span class="koboSpan" id="kobo.15.1">; this ensures all states a uniform (and hence equal chance) value. </span><span class="koboSpan" id="kobo.15.2">Later, we observe the rewards obtained by taking a particular action and, based on the rewards, we update the Q-table. </span><span class="koboSpan" id="kobo.15.3">The update in Q-value is performed dynamically with the help of </span><strong><span class="koboSpan" id="kobo.16.1">the Bellman Equation,</span></strong><span class="koboSpan" id="kobo.17.1"> given by the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.18.1"><img class="fm-editor-equation" src="assets/fd672f88-b11d-4994-9ef5-79075fd547a1.png" style="width:28.67em;height:1.42em;"/></span></div>
<p><span class="koboSpan" id="kobo.19.1">Here, </span><em><span class="koboSpan" id="kobo.20.1">α</span></em><span class="koboSpan" id="kobo.21.1"> is the learning rate. </span><span class="koboSpan" id="kobo.21.2">This shows the basic Q-learning algorithm:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.22.1"><img src="assets/4dfc4179-734a-465e-8e72-b60d8cc3af20.png" style="width:13.67em;height:21.33em;"/></span></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="koboSpan" id="kobo.23.1"> Simple Q-learning algorithm</span></div>
<div class="packt_infobox"><span class="koboSpan" id="kobo.24.1">If you're </span><span><span class="koboSpan" id="kobo.25.1">interested, you can read the 240 pages Watkins doctoral thesis here: </span></span><a href="http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf"><span class="koboSpan" id="kobo.26.1">http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf</span></a><span class="koboSpan" id="kobo.27.1">.</span></div>
<p><span class="koboSpan" id="kobo.28.1">At the end of learning, we'll have a good Q-table, with optimal policy. </span><span class="koboSpan" id="kobo.28.2">An important question here is: how do we choose the action at the second step? </span><span class="koboSpan" id="kobo.28.3">There are two alternatives; first, we choose the action randomly. </span><span class="koboSpan" id="kobo.28.4">This allows our agent to explore all of the possible actions with equal probability but, at the same time, ignoring the information it has already learned. </span><span class="koboSpan" id="kobo.28.5">The second way is we choose the action for which the value is maximum; initially, all of the actions have the same Q-value but, as the agent will learn, some actions will get high value and others low value. </span><span class="koboSpan" id="kobo.28.6">In this case, the agent is exploiting the knowledge it has already learned. </span><span class="koboSpan" id="kobo.28.7">So what's better: exploration or exploitation? </span><span class="koboSpan" id="kobo.28.8">This is called the </span><strong><span class="koboSpan" id="kobo.29.1">exploration-exploitation trade-off</span></strong><span class="koboSpan" id="kobo.30.1">. </span><span class="koboSpan" id="kobo.30.2">A natural way to solve this problem is by relying on what the agent has learned, but at the same time sometimes just explore. </span><span class="koboSpan" id="kobo.30.3">This is achieved via the use of the </span><strong><span class="koboSpan" id="kobo.31.1">epsilon greedy algorithm</span></strong><span class="koboSpan" id="kobo.32.1">. </span><span class="koboSpan" id="kobo.32.2">The basic idea is that the agent chooses the actions randomly with the probability, </span><em><span class="koboSpan" id="kobo.33.1">ε</span></em><span class="koboSpan" id="kobo.34.1">, and exploits the information learned in previous episodes by a probability, (</span><em><span class="koboSpan" id="kobo.35.1">1-ε</span></em><span class="koboSpan" id="kobo.36.1">). </span><span class="koboSpan" id="kobo.36.2">The algorithm chooses the best option (greedy) most of the time (</span><em><span class="koboSpan" id="kobo.37.1">1-ε</span></em><span class="koboSpan" id="kobo.38.1">) but sometimes (</span><em><span class="koboSpan" id="kobo.39.1">ε</span></em><span class="koboSpan" id="kobo.40.1">) it makes a random choice. </span><span><span class="koboSpan" id="kobo.41.1">Let's now try to implement what we learned in a simple problem.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Taxi drop-off using Q-tables</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">The simple Q-learning algorithm involves maintaining a table of the size</span></span> <em><span class="koboSpan" id="kobo.3.1">m</span></em><span><span class="koboSpan" id="kobo.4.1">×</span></span><em><span class="koboSpan" id="kobo.5.1">n</span></em><span><span class="koboSpan" id="kobo.6.1">, where</span></span> <em><span class="koboSpan" id="kobo.7.1">m</span></em> <span><span class="koboSpan" id="kobo.8.1">is the total number of states and</span></span> <em><span class="koboSpan" id="kobo.9.1">n</span></em><span class="koboSpan" id="kobo.10.1"> the </span><span><span class="koboSpan" id="kobo.11.1">total number of possible actions. </span><span class="koboSpan" id="kobo.11.2">Therefore, w</span></span><span class="koboSpan" id="kobo.12.1">e choose a problem from the toy-text group since their </span><kbd><span class="koboSpan" id="kobo.13.1">state</span></kbd><span class="koboSpan" id="kobo.14.1"> space and </span><kbd><span class="koboSpan" id="kobo.15.1">action</span></kbd><span class="koboSpan" id="kobo.16.1"> space is small. </span><span class="koboSpan" id="kobo.16.2">For illustrative purposes, we choose the </span><kbd><span class="koboSpan" id="kobo.17.1">Taxi-v2</span></kbd><span class="koboSpan" id="kobo.18.1"> environment. </span><span class="koboSpan" id="kobo.18.2">The goal of our agent is to choose the passenger at one location and drop them off at another. </span><span class="koboSpan" id="kobo.18.3">The agent receives </span><em><span class="koboSpan" id="kobo.19.1">+20</span></em><span class="koboSpan" id="kobo.20.1"> points for a successful drop-off and loses </span><em><span class="koboSpan" id="kobo.21.1">1</span></em><span class="koboSpan" id="kobo.22.1"> point for every time step it takes. </span><span class="koboSpan" id="kobo.22.2">There's also a 10-point penalty for illegal pick-up and drop-off. </span><span class="koboSpan" id="kobo.22.3">The state space has walls shown by </span><strong><span class="koboSpan" id="kobo.23.1">|</span></strong><span class="koboSpan" id="kobo.24.1"> and four location marks, </span><strong><span class="koboSpan" id="kobo.25.1">R</span></strong><span class="koboSpan" id="kobo.26.1">, </span><strong><span class="koboSpan" id="kobo.27.1">G</span></strong><span class="koboSpan" id="kobo.28.1">, </span><strong><span class="koboSpan" id="kobo.29.1">Y</span></strong><span class="koboSpan" id="kobo.30.1">, and </span><strong><span class="koboSpan" id="kobo.31.1">B</span></strong><span class="koboSpan" id="kobo.32.1"> respectively. </span><span class="koboSpan" id="kobo.32.2">The taxi is shown by box: the pick-up and drop-off location can be either of these four location marks. </span><span class="koboSpan" id="kobo.32.3">The pick-up point is colored blue, and the drop-off is colored purple. </span><span class="koboSpan" id="kobo.32.4">The </span><kbd><span class="koboSpan" id="kobo.33.1">Taxi-v2</span></kbd><span class="koboSpan" id="kobo.34.1"> environment has a state space of size </span><em><span class="koboSpan" id="kobo.35.1">500</span></em><span class="koboSpan" id="kobo.36.1"> and action space of size </span><em><span class="koboSpan" id="kobo.37.1">6</span></em><span class="koboSpan" id="kobo.38.1">, making a Q-table with </span><em><span class="koboSpan" id="kobo.39.1">500×6=3000</span></em><span class="koboSpan" id="kobo.40.1"> entries: </span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.41.1"><img src="assets/e24e0dd9-8879-439b-ba1c-796ed9c1541b.png" style="width:11.67em;height:12.33em;"/></span></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.42.1"> Taxi drop-off environment</span></div>
<p><span class="koboSpan" id="kobo.43.1">In the taxi </span><span><span class="koboSpan" id="kobo.44.1">drop-off environment, the taxi is denoted by the yellow box. </span><span class="koboSpan" id="kobo.44.2">The location mark, </span><span class="packt_screen"><span class="koboSpan" id="kobo.45.1">R</span></span><span class="koboSpan" id="kobo.46.1">, is the pick-up position, and </span><span class="packt_screen"><span class="koboSpan" id="kobo.47.1">G</span></span></span><span class="koboSpan" id="kobo.48.1"> is </span><span><span class="koboSpan" id="kobo.49.1">the drop-off location:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.50.1">We start by importing the necessary modules and creating our environment. </span><span class="koboSpan" id="kobo.50.2">Since, here, we just need to make a look-up table, using TensorFlow won't be necessary. </span><span class="koboSpan" id="kobo.50.3">As mentioned previously, the </span><kbd><span class="koboSpan" id="kobo.51.1">Taxi-v2</span></kbd><span class="koboSpan" id="kobo.52.1"> environment has </span><em><span class="koboSpan" id="kobo.53.1">500</span></em><span class="koboSpan" id="kobo.54.1"> possible states and </span><em><span class="koboSpan" id="kobo.55.1">6</span></em><span class="koboSpan" id="kobo.56.1"> possible actions:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.57.1">import gym</span><br/><span class="koboSpan" id="kobo.58.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.59.1">env = gym.make('Taxi-v2')</span><br/><span class="koboSpan" id="kobo.60.1">obs = env.reset()</span><br/><span class="koboSpan" id="kobo.61.1">env.render()</span></pre>
<p class="mce-root"/>
<ol start="2">
<li><span class="koboSpan" id="kobo.62.1">We initialize the Q-table of the size (</span><em><span class="koboSpan" id="kobo.63.1">300×6</span></em><span class="koboSpan" id="kobo.64.1">) with all zeros, and define the hyperparameters: </span><span><em><span class="koboSpan" id="kobo.65.1">γ</span></em><span class="koboSpan" id="kobo.66.1">, the discount factor, and </span><em><span class="koboSpan" id="kobo.67.1">α</span></em><span class="koboSpan" id="kobo.68.1">, the learning rate. </span><span class="koboSpan" id="kobo.68.2">We also set the values for maximum episodes (one episode means one complete run from reset to done=</span><kbd><span class="koboSpan" id="kobo.69.1">True</span></kbd><span class="koboSpan" id="kobo.70.1">) and maximum steps in an episode the agent will learn for:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.71.1">m = env.observation_space.n # size of the state space</span><br/><span class="koboSpan" id="kobo.72.1">n = env.action_space.n # size of action space</span><br/><span class="koboSpan" id="kobo.73.1">print("The Q-table will have {} rows and {} columns, resulting in \</span><br/><span class="koboSpan" id="kobo.74.1">     total {} entries".format(m,n,m*n))</span><br/><br/><span class="koboSpan" id="kobo.75.1"># Intialize the Q-table and hyperparameters</span><br/><span class="koboSpan" id="kobo.76.1">Q = np.zeros([m,n])</span><br/><span class="koboSpan" id="kobo.77.1">gamma = 0.97</span><br/><span class="koboSpan" id="kobo.78.1">max_episode = 1000</span><br/><span class="koboSpan" id="kobo.79.1">max_steps = 100</span><br/><span class="koboSpan" id="kobo.80.1">alpha = 0.7</span><br/><span class="koboSpan" id="kobo.81.1">epsilon = 0.3</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.82.1"> Now, for each episode, we choose the action with the highest value, perform the action, and update the Q-table based on the received rewards and future state using the Bellman Equation:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.83.1">for i in range(max_episode):</span><br/><span class="koboSpan" id="kobo.84.1">    # Start with new environment</span><br/><span class="koboSpan" id="kobo.85.1">    s = env.reset()</span><br/><span class="koboSpan" id="kobo.86.1">    done = False</span><br/><span class="koboSpan" id="kobo.87.1">    for _ in range(max_steps):</span><br/><span class="koboSpan" id="kobo.88.1">        # Choose an action based on epsilon greedy algorithm</span><br/><span class="koboSpan" id="kobo.89.1">        p = np.random.rand()</span><br/><span class="koboSpan" id="kobo.90.1">        if p &gt; epsilon or (not np.any(Q[s,:])):</span><br/><span class="koboSpan" id="kobo.91.1">            a = env.action_space.sample() #explore</span><br/><span class="koboSpan" id="kobo.92.1">        else:</span><br/><span class="koboSpan" id="kobo.93.1">            a = np.argmax(Q[s,:]) # exploit</span><br/><span class="koboSpan" id="kobo.94.1">        s_new, r, done, _ = env.step(a) </span><br/><span class="koboSpan" id="kobo.95.1">        # Update Q-table</span><br/><span class="koboSpan" id="kobo.96.1">        Q[s,a] = (1-alpha)*Q[s,a] + alpha*(r + gamma*np.max(Q[s_new,:]))</span><br/><span class="koboSpan" id="kobo.97.1">        #print(Q[s,a],r)</span><br/><span class="koboSpan" id="kobo.98.1">        s = s_new</span><br/><span class="koboSpan" id="kobo.99.1">        if done:</span><br/><span class="koboSpan" id="kobo.100.1">            break</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.101.1">Let's now see how the learned agent works:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.102.1">s = env.reset()</span><br/><span class="koboSpan" id="kobo.103.1">done = False</span><br/><span class="koboSpan" id="kobo.104.1">env.render()</span><br/><span class="koboSpan" id="kobo.105.1"># Test the learned Agent</span><br/><span class="koboSpan" id="kobo.106.1">for i in range(max_steps):</span><br/><span class="koboSpan" id="kobo.107.1"> a = np.argmax(Q[s,:])</span><br/><span class="koboSpan" id="kobo.108.1"> s, _, done, _ = env.step(a)</span><br/><span class="koboSpan" id="kobo.109.1"> env.render()</span><br/><span class="koboSpan" id="kobo.110.1"> if done:</span><br/><span class="koboSpan" id="kobo.111.1"> break </span></pre>
<p><span class="koboSpan" id="kobo.112.1">The following diagram shows the agent behavior in a particular example. </span><span class="koboSpan" id="kobo.112.2">The empty car is shown as a yellow box, and the car with the passenger is shown by a green box. </span><span class="koboSpan" id="kobo.112.3">You can see that, in the given case, the agent picks up and drops off the </span><span><span class="koboSpan" id="kobo.113.1">passenger in 11 steps, and the desired location is marked (</span><strong><span class="koboSpan" id="kobo.114.1">B</span></strong><span class="koboSpan" id="kobo.115.1">) and the destination is marked (</span><strong><span class="koboSpan" id="kobo.116.1">R</span></strong><span class="koboSpan" id="kobo.117.1">): </span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.118.1"><img class="aligncenter size-full wp-image-1093 image-border" src="assets/404c987f-7024-4e67-a00c-aab1d910e40b.png" style="width:110.25em;height:83.17em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.119.1">Agent picking up and dropping off a passenger using the learned Q-table </span></div>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.120.1">Cool, right? </span><span class="koboSpan" id="kobo.120.2">The complete code is available in the </span><kbd><span class="koboSpan" id="kobo.121.1">Taxi_drop-off.ipynb</span></kbd><span class="koboSpan" id="kobo.122.1"> file available at GitHub. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Q-Network</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">The simple Q-learning algorithm involves maintaining a table of the size</span></span> <em><span class="koboSpan" id="kobo.3.1">m</span></em><span><span class="koboSpan" id="kobo.4.1">×</span></span><em><span class="koboSpan" id="kobo.5.1">n</span></em><span><span class="koboSpan" id="kobo.6.1">, where</span></span> <em><span class="koboSpan" id="kobo.7.1">m</span></em> <span><span class="koboSpan" id="kobo.8.1">is the total number of states and</span></span> <em><span class="koboSpan" id="kobo.9.1">n</span></em><span class="koboSpan" id="kobo.10.1"> the </span><span><span class="koboSpan" id="kobo.11.1">total number of possible actions. </span><span class="koboSpan" id="kobo.11.2">This means we can't use it for large state space and action space. </span><span class="koboSpan" id="kobo.11.3">An alternative is to replace the table with a neural network acting as a function approximator, approximating the Q-function for each possible action. </span><span class="koboSpan" id="kobo.11.4">The weights of the neural network in this case store the Q-table information (they match a given state with the corresponding action and its Q-value). </span><span class="koboSpan" id="kobo.11.5">When the neural network that we use to approximate the Q-function is a deep neural network, we call it a </span><strong><span class="koboSpan" id="kobo.12.1">Deep Q-Network</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong><span class="koboSpan" id="kobo.14.1">DQN</span></strong><span class="koboSpan" id="kobo.15.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.16.1">The neural network takes the state as its input and calculates the Q-value of all of the possible actions. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Taxi drop-off using Q-Network</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">If we consider the preceding </span><em><span class="koboSpan" id="kobo.3.1">Taxi drop-off</span></em><span class="koboSpan" id="kobo.4.1"> example, our neural network will consist of </span><em><span class="koboSpan" id="kobo.5.1">500</span></em><span class="koboSpan" id="kobo.6.1"> input neurons (the state represented by </span><em><span class="koboSpan" id="kobo.7.1">1×500</span></em><span class="koboSpan" id="kobo.8.1"> one-hot vector) and </span><em><span class="koboSpan" id="kobo.9.1">6</span></em><span class="koboSpan" id="kobo.10.1"> output neurons, each neuron representing the Q-value for the particular action for the given state. </span><span class="koboSpan" id="kobo.10.2"> The neural network will here approximate the Q-value for each action. </span><span class="koboSpan" id="kobo.10.3">Hence, the network should be trained so that its approximated Q-value and the target Q-value are same. </span><span class="koboSpan" id="kobo.10.4">The target Q-value as obtained from the Bellman Equation is as follows:</span></span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.11.1"><img class="fm-editor-equation" src="assets/f09c0b5b-3f34-4b90-ac56-6ef4863af836.png" style="width:18.67em;height:1.67em;"/></span></div>
<p><span><span class="koboSpan" id="kobo.12.1">We train the neural network so that the square error of the difference between the target </span><em><span class="koboSpan" id="kobo.13.1">Q</span></em><span class="koboSpan" id="kobo.14.1"> and predicted </span><em><span class="koboSpan" id="kobo.15.1">Q</span></em><span class="koboSpan" id="kobo.16.1"> is minimized—that is, the neural network minimizes the following loss function:</span></span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.17.1"><img class="fm-editor-equation" src="assets/4dd26570-6e18-4b2d-a7f1-98d2dddfa5ff.png" style="width:23.25em;height:1.75em;"/></span></div>
<p><span class="koboSpan" id="kobo.18.1">The aim is to learn the unknown Q</span><sub><span class="koboSpan" id="kobo.19.1">target</span></sub><span class="koboSpan" id="kobo.20.1"> function. </span><span class="koboSpan" id="kobo.20.2">The weights of </span><kbd><span class="koboSpan" id="kobo.21.1">QNetwork</span></kbd><span class="koboSpan" id="kobo.22.1"> are updated using backpropagation so that this loss is minimized. </span><span class="koboSpan" id="kobo.22.2">We make the neural network, </span><kbd><span class="koboSpan" id="kobo.23.1">QNetwork</span></kbd><span class="koboSpan" id="kobo.24.1">, to approximate the Q-value. </span><span class="koboSpan" id="kobo.24.2">It's a very simple single-layer neural network, with methods to provide action and their Q-values (</span><kbd><span class="koboSpan" id="kobo.25.1">get_action</span></kbd><span class="koboSpan" id="kobo.26.1">), train the network (</span><kbd><span class="koboSpan" id="kobo.27.1">learnQ</span></kbd><span class="koboSpan" id="kobo.28.1">), and get the predicted Q-value (</span><kbd><span class="koboSpan" id="kobo.29.1">Qnew</span></kbd><span class="koboSpan" id="kobo.30.1">):</span></p>
<pre><span class="koboSpan" id="kobo.31.1">class QNetwork:</span><br/><span class="koboSpan" id="kobo.32.1">    def __init__(self,m,n,alpha):</span><br/><span class="koboSpan" id="kobo.33.1">        self.s = tf.placeholder(shape=[1,m], dtype=tf.float32)</span><br/><span class="koboSpan" id="kobo.34.1">        W = tf.Variable(tf.random_normal([m,n], stddev=2))</span><br/><span class="koboSpan" id="kobo.35.1">        bias = tf.Variable(tf.random_normal([1, n]))</span><br/><span class="koboSpan" id="kobo.36.1">        self.Q = tf.matmul(self.s,W) + bias</span><br/><span class="koboSpan" id="kobo.37.1">        self.a = tf.argmax(self.Q,1)</span><br/> <br/><span class="koboSpan" id="kobo.38.1">        self.Q_hat = tf.placeholder(shape=[1,n],dtype=tf.float32)</span><br/><span class="koboSpan" id="kobo.39.1">        loss = tf.reduce_sum(tf.square(self.Q_hat-self.Q))</span><br/><span class="koboSpan" id="kobo.40.1">        optimizer = tf.train.GradientDescentOptimizer(learning_rate=alpha)</span><br/><span class="koboSpan" id="kobo.41.1">        self.train = optimizer.minimize(loss)</span><br/><span class="koboSpan" id="kobo.42.1">        init = tf.global_variables_initializer()</span><br/> <br/><span class="koboSpan" id="kobo.43.1">        self.sess = tf.Session()</span><br/><span class="koboSpan" id="kobo.44.1">        self.sess.run(init)</span><br/> <br/><span class="koboSpan" id="kobo.45.1">    def get_action(self,s):</span><br/><span class="koboSpan" id="kobo.46.1">        return self.sess.run([self.a,self.Q], feed_dict={self.s:s})</span><br/> <br/><span class="koboSpan" id="kobo.47.1">    def learnQ(self,s,Q_hat):</span><br/><span class="koboSpan" id="kobo.48.1">        self.sess.run(self.train, feed_dict= {self.s:s, self.Q_hat:Q_hat})</span><br/> <br/><span class="koboSpan" id="kobo.49.1">    def Qnew(self,s):</span><br/><span class="koboSpan" id="kobo.50.1">        return self.sess.run(self.Q, feed_dict={self.s:s})</span><br/> </pre>
<p><span class="koboSpan" id="kobo.51.1">We now incorporate this neural network in our earlier code where we trained an RL agent for the </span><em><span class="koboSpan" id="kobo.52.1">Taxi drop-off</span></em><span class="koboSpan" id="kobo.53.1"> problem. </span><span class="koboSpan" id="kobo.53.2">We'll need to make some changes; first, the state returned by the OpenAI step and reset function in this case is just the numeric identification of state, so we need to convert it into a one-hot vector. </span><span class="koboSpan" id="kobo.53.3">Also, instead of a Q-table update, we'll now get the new Q-predicted from </span><kbd><span class="koboSpan" id="kobo.54.1">QNetwork</span></kbd><span class="koboSpan" id="kobo.55.1">, find the target Q, and train the network so as to minimize the loss. </span><span class="koboSpan" id="kobo.55.2">The code is as follows:</span></p>
<pre><span class="koboSpan" id="kobo.56.1">QNN = QNetwork(m,n, alpha)</span><br/><span class="koboSpan" id="kobo.57.1">rewards = []</span><br/><span class="koboSpan" id="kobo.58.1">for i in range(max_episode):</span><br/><span class="koboSpan" id="kobo.59.1"> # Start with new environment</span><br/><span class="koboSpan" id="kobo.60.1"> s = env.reset()</span><br/><span class="koboSpan" id="kobo.61.1"> S = np.identity(m)[s:s+1]</span><br/><span class="koboSpan" id="kobo.62.1"> done = False</span><br/><span class="koboSpan" id="kobo.63.1"> counter = 0</span><br/><span class="koboSpan" id="kobo.64.1"> rtot = 0</span><br/><span class="koboSpan" id="kobo.65.1"> for _ in range(max_steps):</span><br/><span class="koboSpan" id="kobo.66.1"> # Choose an action using epsilon greedy policy</span><br/><span class="koboSpan" id="kobo.67.1"> a, Q_hat = QNN.get_action(S) </span><br/><span class="koboSpan" id="kobo.68.1"> p = np.random.rand()</span><br/><span class="koboSpan" id="kobo.69.1"> if p &gt; epsilon:</span><br/><span class="koboSpan" id="kobo.70.1"> a[0] = env.action_space.sample() #explore</span><br/> <br/><span class="koboSpan" id="kobo.71.1"> s_new, r, done, _ = env.step(a[0])</span><br/><span class="koboSpan" id="kobo.72.1"> rtot += r</span><br/><span class="koboSpan" id="kobo.73.1"> # Update Q-table</span><br/><span class="koboSpan" id="kobo.74.1"> S_new = np.identity(m)[s_new:s_new+1]</span><br/><span class="koboSpan" id="kobo.75.1"> Q_new = QNN.Qnew(S_new) </span><br/><span class="koboSpan" id="kobo.76.1"> maxQ = np.max(Q_new)</span><br/><span class="koboSpan" id="kobo.77.1"> Q_hat[0,a[0]] = r + gamma*maxQ</span><br/><span class="koboSpan" id="kobo.78.1"> QNN.learnQ(S,Q_hat)</span><br/><span class="koboSpan" id="kobo.79.1"> S = S_new</span><br/><span class="koboSpan" id="kobo.80.1"> #print(Q_hat[0,a[0]],r)</span><br/><span class="koboSpan" id="kobo.81.1"> if done:</span><br/><span class="koboSpan" id="kobo.82.1"> break</span><br/><span class="koboSpan" id="kobo.83.1"> rewards.append(rtot)</span><br/><span class="koboSpan" id="kobo.84.1">print ("Total reward per episode is: " + str(sum(rewards)/max_episode))</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.85.1">This should have done a good job but, as you can see, even after training for </span><em><span class="koboSpan" id="kobo.86.1">1,000</span></em><span class="koboSpan" id="kobo.87.1"> episodes, the network has a high negative reward, and if you check the performance of the network, it appears to just take random steps. </span><span class="koboSpan" id="kobo.87.2">Yes, our network hasn't learned anything; the performance is worse than Q-table. </span><span class="koboSpan" id="kobo.87.3">This can also be verified from the reward plot while training—ideally, the rewards should increase as the agent learns, but nothing of the sort happens here; the rewards increase and decrease like a random walk around the mean (the complete code for this program is in the </span><kbd><span class="koboSpan" id="kobo.88.1">Taxi_drop-off_NN.ipynb</span></kbd><span class="koboSpan" id="kobo.89.1"> file available at GitHub):</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.90.1"><img class="aligncenter size-full wp-image-1094 image-border" src="assets/13ebed29-bce7-456d-a4d4-23ad3eb5dda4.png" style="width:33.42em;height:22.17em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.91.1"> Total reward per episode obtained by the agent as it learns</span></div>
<p><span class="koboSpan" id="kobo.92.1">What happened? </span><span class="koboSpan" id="kobo.92.2">Why is the neural network failing to learn, and can we make it better?</span></p>
<p><span class="koboSpan" id="kobo.93.1">Consider the scenario when the taxi should go west to pick up and, randomly, the agent chose west; the agent gets a reward and the network will learn that, in the present state (represented by a one-hot vector), going west is favorable. </span><span class="koboSpan" id="kobo.93.2">Next, consider another state similar to this one (correlated state space): the agent again makes the west move, but this time it results in a negative reward, so now the agent will unlearn what it had learned earlier. </span><span class="koboSpan" id="kobo.93.3">Hence, similar state-actions but divergent targets confuse the learning process. </span><span class="koboSpan" id="kobo.93.4">This is called </span><strong><span class="koboSpan" id="kobo.94.1">catastrophic forgetting</span></strong><span class="koboSpan" id="kobo.95.1">. </span><span class="koboSpan" id="kobo.95.2">The problem arises here because consecutive states are highly correlated and so, if the agent learns in sequence (as it does here), this extremely correlated input state space won't let the agent learn.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.96.1">Can we break the correlation between the input presented to the network? </span><span class="koboSpan" id="kobo.96.2">Yes, we can: we can construct a </span><strong><span class="koboSpan" id="kobo.97.1">replay buffer</span></strong><span class="koboSpan" id="kobo.98.1">, where we first store each state, its corresponding action, and the consecutive reward and resultant state (state, action, reward, new state). </span><span class="koboSpan" id="kobo.98.2">The actions, in this case, are chosen completely randomly, thereby ensuring a wide range of actions and resultant states. </span><span class="koboSpan" id="kobo.98.3">The replay buffer will finally consist of a large list of these tuples (</span><em><span class="koboSpan" id="kobo.99.1">S</span></em><span class="koboSpan" id="kobo.100.1">, </span><em><span class="koboSpan" id="kobo.101.1">A</span></em><span class="koboSpan" id="kobo.102.1">, </span><em><span class="koboSpan" id="kobo.103.1">R</span></em><span class="koboSpan" id="kobo.104.1">, </span><em><span class="koboSpan" id="kobo.105.1">S'</span></em><span class="koboSpan" id="kobo.106.1">). </span><span class="koboSpan" id="kobo.106.2">Next, we present the network with these tuples randomly (instead of sequentially); this randomness will break the correlation between consecutive input states. </span><span class="koboSpan" id="kobo.106.3">This is called </span><strong><span class="koboSpan" id="kobo.107.1">experience replay</span></strong><span class="koboSpan" id="kobo.108.1">. </span><span class="koboSpan" id="kobo.108.2">It not only resolves the issues with correlation in input state space but also allows us to learn from the same tuples more than once, recall rare occurrences, and in general, make better use of the experience. </span><span class="koboSpan" id="kobo.108.3">In one way, you can say that, by using a replay buffer, we've reduced the problem of the supervised learning (with the replay buffer as an input-output dataset), where the random sampling of input ensures that the network is able to generalize.</span></p>
<p><span class="koboSpan" id="kobo.109.1">Another problem with our approach is that we're updating the target Q immediately. </span><span class="koboSpan" id="kobo.109.2">This too can cause harmful correlations. </span><span class="koboSpan" id="kobo.109.3">Remember that, in Q-learning, we're trying to minimize the difference between the </span><em><span class="koboSpan" id="kobo.110.1">Q</span><sub><span class="koboSpan" id="kobo.111.1">target</span></sub></em><span class="koboSpan" id="kobo.112.1"> and the currently predicted </span><em><span class="koboSpan" id="kobo.113.1">Q</span></em><span class="koboSpan" id="kobo.114.1">. </span><span class="koboSpan" id="kobo.114.2">This difference is called a </span><strong><span class="koboSpan" id="kobo.115.1">temporal difference</span></strong><span class="koboSpan" id="kobo.116.1"> (</span><strong><span class="koboSpan" id="kobo.117.1">TD</span></strong><span class="koboSpan" id="kobo.118.1">) error (and hence Q-learning is a type of </span><strong><span class="koboSpan" id="kobo.119.1">TD learning</span></strong><span class="koboSpan" id="kobo.120.1">). </span><span class="koboSpan" id="kobo.120.2">At present, we update our </span><em><span class="koboSpan" id="kobo.121.1">Q</span><sub><span class="koboSpan" id="kobo.122.1">target</span></sub></em><span class="koboSpan" id="kobo.123.1"> immediately, hence there exists a correlation between the target and the parameters we're changing (weights through </span><em><span class="koboSpan" id="kobo.124.1">Q</span><sub><span class="koboSpan" id="kobo.125.1">pred</span></sub></em><span class="koboSpan" id="kobo.126.1">). </span><span class="koboSpan" id="kobo.126.2">This is almost like chasing a moving target and hence won't give a generalized direction. </span><span class="koboSpan" id="kobo.126.3">We can resolve the issue by using </span><strong><span class="koboSpan" id="kobo.127.1">fixed Q-targets</span></strong><span class="koboSpan" id="kobo.128.1">—that is, use two networks, one for predicting </span><em><span class="koboSpan" id="kobo.129.1">Q</span></em><span class="koboSpan" id="kobo.130.1"> and another for target </span><em><span class="koboSpan" id="kobo.131.1">Q</span></em><span class="koboSpan" id="kobo.132.1">. </span><span class="koboSpan" id="kobo.132.2">Both are exactly the same in terms of architecture, with the predicting Q-Network changing weights at each step, but the weight of the target Q-Network is updated after some fixed learning steps. </span><span class="koboSpan" id="kobo.132.3">This provides a more stable learning environment. </span></p>
<p><span class="koboSpan" id="kobo.133.1">Finally, we make one more small change: right now our epsilon has had a fixed value throughout learning. </span><span class="koboSpan" id="kobo.133.2">But, in real life, this isn't so. </span><span class="koboSpan" id="kobo.133.3">Initially, when we know nothing, we explore a lot but, as we become familiar, we tend to take the learned path. </span><span class="koboSpan" id="kobo.133.4">The same can be done in our epsilon-greedy algorithm, by changing the value of epsilon as the network learns through each episode, so that epsilon decreases with time. </span></p>
<p><span class="koboSpan" id="kobo.134.1">Equipped with these tricks, let's now build a DQN to play an Atari game.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">DQN to play an Atari game</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The </span><span><span class="koboSpan" id="kobo.3.1">DQN we'll learn here is based on a DeepMind paper (</span><a href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf"><span class="koboSpan" id="kobo.4.1">https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf</span></a><span class="koboSpan" id="kobo.5.1">). </span></span><span class="koboSpan" id="kobo.6.1">At the heart of DQN is a deep convolutional neural network that takes as input the raw pixels of the game environment (just like any human player would see), captured one screen at a time, and as output, returns the value for each possible action. </span><span class="koboSpan" id="kobo.6.2">The action with the maximum value is the chosen action:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.7.1">The first step is to get all of the modules we'll need:</span></li>
</ol>
<pre style="padding-left: 60px"><br/><span><span class="koboSpan" id="kobo.8.1">import gym</span></span><br/><span class="koboSpan" id="kobo.9.1">import sys</span><br/><span class="koboSpan" id="kobo.10.1">import random</span><br/><span class="koboSpan" id="kobo.11.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.12.1">import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.13.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.14.1">from datetime import datetime</span><br/><span class="koboSpan" id="kobo.15.1">from scipy.misc import imresize</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.16.1">We chose the Breakout game from the list of OpenAI Atari games—you can try the code for other Atari games; the only change you may need to do would be in the preprocessing step. </span><span class="koboSpan" id="kobo.16.2">The input space of Breakout</span><span><span class="koboSpan" id="kobo.17.1">—</span></span><span class="koboSpan" id="kobo.18.1">our input space</span><span><span class="koboSpan" id="kobo.19.1">—</span></span><span class="koboSpan" id="kobo.20.1">consists of 210×160 pixels, with 128 possible colors for each pixel. </span><span class="koboSpan" id="kobo.20.2">It's an enormously large input space. </span><span class="koboSpan" id="kobo.20.3">To reduce the complexity, we'll choose a region of interest in the image, convert it into grayscale, and resize it to an image of the size </span><em><span class="koboSpan" id="kobo.21.1">80×80</span></em><span class="koboSpan" id="kobo.22.1">. </span><span class="koboSpan" id="kobo.22.2">We do this using the </span><kbd><span class="koboSpan" id="kobo.23.1">preprocess</span></kbd><span class="koboSpan" id="kobo.24.1"> function:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.25.1">def preprocess(img):</span><br/><span class="koboSpan" id="kobo.26.1">    img_temp = img[31:195] # Choose the important area of the image</span><br/><span class="koboSpan" id="kobo.27.1">    img_temp = img_temp.mean(axis=2) # Convert to Grayscale#</span><br/><span class="koboSpan" id="kobo.28.1">    # Downsample image using nearest neighbour interpolation</span><br/><span class="koboSpan" id="kobo.29.1">    img_temp = imresize(img_temp, size=(IM_SIZE, IM_SIZE), interp='nearest')</span><br/><span class="koboSpan" id="kobo.30.1">    return img_temp</span></pre>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.31.1">The following screenshot shows the environment before and after the preprocessing:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.32.1"><img class="aligncenter size-full wp-image-1095 image-border" src="assets/947614bc-85de-436a-93be-2b01ae49a075.png" style="width:73.42em;height:44.50em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.33.1"> The original environment, size 210× 160 (colored image) and the processed environment, size 80×80 (grayscale)</span></div>
<ol start="3">
<li><span class="koboSpan" id="kobo.34.1">As you can see from the preceding diagram, it isn't possible to tell whether the ball is coming down or going up. </span><span class="koboSpan" id="kobo.34.2">To deal with this problem, we combine four consecutive states (due to four unique actions) as one input. </span><span class="koboSpan" id="kobo.34.3">We define a function, </span><kbd><span class="koboSpan" id="kobo.35.1">update_state</span></kbd><span class="koboSpan" id="kobo.36.1">, that appends the current environment observation to the previous state array:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.37.1">def update_state(state, obs):</span><br/><span class="koboSpan" id="kobo.38.1">    obs_small = preprocess(obs)</span><br/><span class="koboSpan" id="kobo.39.1">    return np.append(state[1:], np.expand_dims(obs_small, 0), axis=0)</span></pre>
<p class="mce-root"/>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.40.1">The function appends the processed new state in the sliced state, ensuring that the final input to the network consists of four frames. </span><span class="koboSpan" id="kobo.40.2">In the following screenshot, you can see the four consecutive frames. </span><span class="koboSpan" id="kobo.40.3">This is the input to our DQN:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.41.1"><img class="aligncenter size-full wp-image-1096 image-border" src="assets/2bbec24a-38d5-4bc4-92ac-0340ca53e978.png" style="width:48.83em;height:12.08em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.42.1">The input to DQN four consecutive game-states (frames)</span></div>
<ol start="4">
<li><span class="koboSpan" id="kobo.43.1">We create a DQN that we define in the </span><span><span class="koboSpan" id="kobo.44.1">class</span></span><span class="koboSpan" id="kobo.45.1"> DQN; it consists of three convolutional layers, the output of the last convolutional layer is flattened, and it's then followed by two fully connected layers. </span><span class="koboSpan" id="kobo.45.2">The network, as in the previous case, tries to minimize the difference between </span><em><span class="koboSpan" id="kobo.46.1">Q</span><sub><span class="koboSpan" id="kobo.47.1">target</span></sub><span class="koboSpan" id="kobo.48.1"> </span></em><span class="koboSpan" id="kobo.49.1">and </span><em><span class="koboSpan" id="kobo.50.1">Q</span><sub><span class="koboSpan" id="kobo.51.1">predicted</span></sub></em><span class="koboSpan" id="kobo.52.1">. </span><span class="koboSpan" id="kobo.52.2">In the code, we're using the RMSProp optimizer, but you can play around with other optimizers:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.53.1">def __init__(self, K, scope, save_path= 'models/atari.ckpt'):</span><br/><span class="koboSpan" id="kobo.54.1">    self.K = K</span><br/><span class="koboSpan" id="kobo.55.1">    self.scope = scope</span><br/><span class="koboSpan" id="kobo.56.1">    self.save_path = save_path</span><br/><span class="koboSpan" id="kobo.57.1">    with tf.variable_scope(scope):</span><br/><span class="koboSpan" id="kobo.58.1">        # inputs and targets</span><br/><span class="koboSpan" id="kobo.59.1">        self.X = tf.placeholder(tf.float32, shape=(None, 4, IM_SIZE, IM_SIZE), name='X')</span><br/><span class="koboSpan" id="kobo.60.1">        # tensorflow convolution needs the order to be:</span><br/><span class="koboSpan" id="kobo.61.1">        # (num_samples, height, width, "color")</span><br/><span class="koboSpan" id="kobo.62.1">        # so we need to tranpose later</span><br/><span class="koboSpan" id="kobo.63.1">        self.Q_target = tf.placeholder(tf.float32, shape=(None,), name='G')</span><br/><span class="koboSpan" id="kobo.64.1">        self.actions = tf.placeholder(tf.int32, shape=(None,), name='actions')</span><br/><span class="koboSpan" id="kobo.65.1">        # calculate output and cost</span><br/><span class="koboSpan" id="kobo.66.1">        # convolutional layers</span><br/><span class="koboSpan" id="kobo.67.1">        Z = self.X / 255.0</span><br/><span class="koboSpan" id="kobo.68.1">        Z = tf.transpose(Z, [0, 2, 3, 1])</span><br/><span class="koboSpan" id="kobo.69.1">        cnn1 = tf.contrib.layers.conv2d(Z, 32, 8, 4, activation_fn=tf.nn.relu)</span><br/><span class="koboSpan" id="kobo.70.1">        cnn2 = tf.contrib.layers.conv2d(cnn1, 64, 4, 2, activation_fn=tf.nn.relu)</span><br/><span class="koboSpan" id="kobo.71.1">        cnn3 = tf.contrib.layers.conv2d(cnn2, 64, 3, 1, activation_fn=tf.nn.relu)</span><br/><span class="koboSpan" id="kobo.72.1">        # fully connected layers</span><br/><span class="koboSpan" id="kobo.73.1">        fc0 = tf.contrib.layers.flatten(cnn3)</span><br/><span class="koboSpan" id="kobo.74.1">        fc1 = tf.contrib.layers.fully_connected(fc0, 512)</span><br/><span class="koboSpan" id="kobo.75.1">        # final output layer</span><br/><span class="koboSpan" id="kobo.76.1">        self.predict_op = tf.contrib.layers.fully_connected(fc1, K)</span><br/><span class="koboSpan" id="kobo.77.1">        Qpredicted = tf.reduce_sum(self.predict_op * tf.one_hot(self.actions, K),</span><br/><span class="koboSpan" id="kobo.78.1">     reduction_indices=[1])</span><br/><span class="koboSpan" id="kobo.79.1">        self.cost = tf.reduce_mean(tf.square(self.Q_target - Qpredicted))</span><br/><span class="koboSpan" id="kobo.80.1">        self.train_op = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6).minimize(self.cost)</span></pre>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.81.1">The necessary methods that we require for this class are discussed in the following steps:</span></p>
<ol start="5">
<li><span class="koboSpan" id="kobo.82.1">We add a method to return the predicted Q-values:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.83.1">def predict(self, states):</span><br/><span class="koboSpan" id="kobo.84.1">    return self.session.run(self.predict_op, feed_dict={self.X: states})</span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.85.1">We need a method to determine the action with maximum value. </span><span class="koboSpan" id="kobo.85.2">In this method, we also implemented the epsilon-greedy policy, and the value of epsilon is changed in the main code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.86.1">def sample_action(self, x, eps):</span><br/><span class="koboSpan" id="kobo.87.1">    """Implements epsilon greedy algorithm"""</span><br/><span class="koboSpan" id="kobo.88.1">    if np.random.random() &lt; eps:</span><br/><span class="koboSpan" id="kobo.89.1">        return np.random.choice(self.K)</span><br/><span class="koboSpan" id="kobo.90.1">    else:</span><br/><span class="koboSpan" id="kobo.91.1">        return np.argmax(self.predict([x])[0])</span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.92.1">We need a method to update the weights of the network so as to minimize the loss. </span><span class="koboSpan" id="kobo.92.2">The function can be defined as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.93.1"> def update(self, states, actions, targets):</span><br/><span class="koboSpan" id="kobo.94.1">     c, _ = self.session.run(</span><br/><span class="koboSpan" id="kobo.95.1">         [self.cost, self.train_op],</span><br/><span class="koboSpan" id="kobo.96.1">         feed_dict={</span><br/><span class="koboSpan" id="kobo.97.1">         self.X: states,</span><br/><span class="koboSpan" id="kobo.98.1">         self.Q_target: targets,</span><br/><span class="koboSpan" id="kobo.99.1">         self.actions: actions</span><br/><span class="koboSpan" id="kobo.100.1">         })</span><br/><span class="koboSpan" id="kobo.101.1">     return c</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="8">
<li><span class="koboSpan" id="kobo.102.1">Copy the model weights to the fixed Q-Network:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.103.1">def copy_from(self, other):</span><br/><span class="koboSpan" id="kobo.104.1">    mine = [t for t in tf.trainable_variables() if t.name.startswith(self.scope)]</span><br/><span class="koboSpan" id="kobo.105.1">    mine = sorted(mine, key=lambda v: v.name)</span><br/><span class="koboSpan" id="kobo.106.1">    theirs = [t for t in tf.trainable_variables() if t.name.startswith(other.scope)]</span><br/><span class="koboSpan" id="kobo.107.1">    theirs = sorted(theirs, key=lambda v: v.name)</span><br/><span class="koboSpan" id="kobo.108.1">    ops = []</span><br/><span class="koboSpan" id="kobo.109.1">    for p, q in zip(mine, theirs):</span><br/><span class="koboSpan" id="kobo.110.1">        actual = self.session.run(q)</span><br/><span class="koboSpan" id="kobo.111.1">        op = p.assign(actual)</span><br/><span class="koboSpan" id="kobo.112.1">        ops.append(op)</span><br/><span class="koboSpan" id="kobo.113.1">    self.session.run(ops)</span></pre>
<ol start="9">
<li><span class="koboSpan" id="kobo.114.1">Besides these methods, we need some helper functions to save the learned network, load the saved network, and set the TensorFlow session:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.115.1">def load(self):</span><br/><span class="koboSpan" id="kobo.116.1">    self.saver = tf.train.Saver(tf.global_variables())</span><br/><span class="koboSpan" id="kobo.117.1">    load_was_success = True</span><br/><span class="koboSpan" id="kobo.118.1">    try:</span><br/><span class="koboSpan" id="kobo.119.1">        save_dir = '/'.join(self.save_path.split('/')[:-1])</span><br/><span class="koboSpan" id="kobo.120.1">        ckpt = tf.train.get_checkpoint_state(save_dir)</span><br/><span class="koboSpan" id="kobo.121.1">        load_path = ckpt.model_checkpoint_path</span><br/><span class="koboSpan" id="kobo.122.1">        self.saver.restore(self.session, load_path)</span><br/><span class="koboSpan" id="kobo.123.1">    except:</span><br/><span class="koboSpan" id="kobo.124.1">        print("no saved model to load. </span><span class="koboSpan" id="kobo.124.2">starting new session")</span><br/><span class="koboSpan" id="kobo.125.1">        load_was_success = False</span><br/><span class="koboSpan" id="kobo.126.1">    else:</span><br/><span class="koboSpan" id="kobo.127.1">        print("loaded model: {}".format(load_path))</span><br/><span class="koboSpan" id="kobo.128.1">        saver = tf.train.Saver(tf.global_variables())</span><br/><span class="koboSpan" id="kobo.129.1">        episode_number = int(load_path.split('-')[-1])</span><br/><br/><span class="koboSpan" id="kobo.130.1">def save(self, n):</span><br/><span class="koboSpan" id="kobo.131.1">    self.saver.save(self.session, self.save_path, global_step=n)</span><br/><span class="koboSpan" id="kobo.132.1">    print("SAVED MODEL #{}".format(n))</span><br/><br/><span class="koboSpan" id="kobo.133.1">def set_session(self, session):</span><br/><span class="koboSpan" id="kobo.134.1">    self.session = session</span><br/><span class="koboSpan" id="kobo.135.1">    self.session.run(tf.global_variables_initializer())</span><br/><span class="koboSpan" id="kobo.136.1">    self.saver = tf.train.Saver()</span></pre>
<p class="mce-root"/>
<ol start="10">
<li><span class="koboSpan" id="kobo.137.1">To implement the DQN algorithm, we use a </span><kbd><span class="koboSpan" id="kobo.138.1">learn</span></kbd> <span><span class="koboSpan" id="kobo.139.1">function</span></span><span class="koboSpan" id="kobo.140.1">; it picks a random sample from the experience replay buffer and updates the Q-Network, using target Q from the target Q-Network:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.141.1">def learn(model, target_model, experience_replay_buffer, gamma, batch_size):</span><br/><span class="koboSpan" id="kobo.142.1">    # Sample experiences</span><br/><span class="koboSpan" id="kobo.143.1">    samples = random.sample(experience_replay_buffer, batch_size)</span><br/><span class="koboSpan" id="kobo.144.1">    states, actions, rewards, next_states, dones = map(np.array, zip(*samples))</span><br/><span class="koboSpan" id="kobo.145.1">    # Calculate targets</span><br/><span class="koboSpan" id="kobo.146.1">     next_Qs = target_model.predict(next_states)</span><br/><span class="koboSpan" id="kobo.147.1">     next_Q = np.amax(next_Qs, axis=1)</span><br/><span class="koboSpan" id="kobo.148.1">     targets = rewards +     np.invert(dones).astype(np.float32) * gamma * next_Q</span><br/><span class="koboSpan" id="kobo.149.1">    # Update model</span><br/><span class="koboSpan" id="kobo.150.1">     loss = model.update(states, actions, targets)</span><br/><span class="koboSpan" id="kobo.151.1">     return loss</span></pre>
<ol start="11">
<li><span class="koboSpan" id="kobo.152.1">Well, all of the ingredients are ready, so let's now decide the hyperparameters for our DQN and create our environment:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.153.1"># Some Global parameters</span><br/><span class="koboSpan" id="kobo.154.1">MAX_EXPERIENCES = 500000</span><br/><span class="koboSpan" id="kobo.155.1">MIN_EXPERIENCES = 50000</span><br/><span class="koboSpan" id="kobo.156.1">TARGET_UPDATE_PERIOD = 10000</span><br/><span class="koboSpan" id="kobo.157.1">IM_SIZE = 80</span><br/><span class="koboSpan" id="kobo.158.1">K = 4 # env.action_space.n</span><br/><br/><span><span class="koboSpan" id="kobo.159.1"># hyperparameters etc</span><br/></span><span class="koboSpan" id="kobo.160.1">gamma = 0.97</span><br/><span class="koboSpan" id="kobo.161.1">batch_sz = 64</span><br/><span class="koboSpan" id="kobo.162.1">num_episodes = 2700</span><br/><span class="koboSpan" id="kobo.163.1">total_t = 0</span><br/><span class="koboSpan" id="kobo.164.1">experience_replay_buffer = []</span><br/><span class="koboSpan" id="kobo.165.1">episode_rewards = np.zeros(num_episodes)</span><br/><span class="koboSpan" id="kobo.166.1">last_100_avgs = []</span><br/><span class="koboSpan" id="kobo.167.1"># epsilon for Epsilon Greedy Algorithm</span><br/><span class="koboSpan" id="kobo.168.1">epsilon = 1.0</span><br/><span class="koboSpan" id="kobo.169.1">epsilon_min = 0.1</span><br/><span class="koboSpan" id="kobo.170.1">epsilon_change = (epsilon - epsilon_min) / 700000</span><br/><br/><span class="koboSpan" id="kobo.171.1"># Create Atari Environment</span><br/><span class="koboSpan" id="kobo.172.1">env = gym.envs.make("Breakout-v0")</span><br/><br/><span class="koboSpan" id="kobo.173.1"># Create original and target Networks</span><br/><span class="koboSpan" id="kobo.174.1">model = DQN(K=K, scope="model")</span><br/><span class="koboSpan" id="kobo.175.1">target_model = DQN(K=K, scope="target_model")</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="12">
<li><span class="koboSpan" id="kobo.176.1">And finally, the following is the code that calls then fills the experience replay buffer, plays the game step by step, and trains the model network at every step and </span><kbd><span class="koboSpan" id="kobo.177.1">target_model</span></kbd><span class="koboSpan" id="kobo.178.1"> after every four steps:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.179.1">with tf.Session() as sess:</span><br/><span class="koboSpan" id="kobo.180.1">    model.set_session(sess)</span><br/><span class="koboSpan" id="kobo.181.1">    target_model.set_session(sess)</span><br/><span class="koboSpan" id="kobo.182.1">    sess.run(tf.global_variables_initializer())</span><br/><span class="koboSpan" id="kobo.183.1">    model.load()</span><br/><span class="koboSpan" id="kobo.184.1">    print("Filling experience replay buffer...")</span><br/><span class="koboSpan" id="kobo.185.1">    obs = env.reset()</span><br/><span class="koboSpan" id="kobo.186.1">    obs_small = preprocess(obs)</span><br/><span class="koboSpan" id="kobo.187.1">    state = np.stack([obs_small] * 4, axis=0)</span><br/><span class="koboSpan" id="kobo.188.1">    # Fill experience replay buffer</span><br/><span class="koboSpan" id="kobo.189.1">    for i in range(MIN_EXPERIENCES):</span><br/><span class="koboSpan" id="kobo.190.1">        action = np.random.randint(0,K)</span><br/><span class="koboSpan" id="kobo.191.1">        obs, reward, done, _ = env.step(action)</span><br/><span class="koboSpan" id="kobo.192.1">        next_state = update_state(state, obs)</span><br/><span class="koboSpan" id="kobo.193.1">        experience_replay_buffer.append((state, action, reward, next_state, done))</span><br/><span class="koboSpan" id="kobo.194.1">        if done:</span><br/><span class="koboSpan" id="kobo.195.1">            obs = env.reset()</span><br/><span class="koboSpan" id="kobo.196.1">            obs_small = preprocess(obs)</span><br/><span class="koboSpan" id="kobo.197.1">            state = np.stack([obs_small] * 4, axis=0)</span><br/><span class="koboSpan" id="kobo.198.1">        else:</span><br/><span class="koboSpan" id="kobo.199.1">            state = next_state</span><br/><span class="koboSpan" id="kobo.200.1">        # Play a number of episodes and learn</span><br/><span class="koboSpan" id="kobo.201.1">        for i in range(num_episodes):</span><br/><span class="koboSpan" id="kobo.202.1">            t0 = datetime.now()</span><br/><span class="koboSpan" id="kobo.203.1">            # Reset the environment</span><br/><span class="koboSpan" id="kobo.204.1">            obs = env.reset()</span><br/><span class="koboSpan" id="kobo.205.1">            obs_small = preprocess(obs)</span><br/><span class="koboSpan" id="kobo.206.1">            state = np.stack([obs_small] * 4, axis=0)</span><br/><span class="koboSpan" id="kobo.207.1">            assert (state.shape == (4, 80, 80))</span><br/><span class="koboSpan" id="kobo.208.1">            loss = None</span><br/><span class="koboSpan" id="kobo.209.1">            total_time_training = 0</span><br/><span class="koboSpan" id="kobo.210.1">            num_steps_in_episode = 0</span><br/><span class="koboSpan" id="kobo.211.1">            episode_reward = 0</span><br/><span class="koboSpan" id="kobo.212.1">            done = False</span><br/><span class="koboSpan" id="kobo.213.1">            while not done:</span><br/><span class="koboSpan" id="kobo.214.1">                # Update target network</span><br/><span class="koboSpan" id="kobo.215.1">                if total_t % TARGET_UPDATE_PERIOD == 0:</span><br/><span class="koboSpan" id="kobo.216.1">                    target_model.copy_from(model)</span><br/><span class="koboSpan" id="kobo.217.1">                    print("Copied model parameters to target network. </span><span class="koboSpan" id="kobo.217.2">total_t = %s, period = %s" % (total_t, TARGET_UPDATE_PERIOD))</span><br/><span class="koboSpan" id="kobo.218.1">                # Take action</span><br/><span class="koboSpan" id="kobo.219.1">                action = model.sample_action(state, epsilon)</span><br/><span class="koboSpan" id="kobo.220.1">                obs, reward, done, _ = env.step(action)</span><br/><span class="koboSpan" id="kobo.221.1">                obs_small = preprocess(obs)</span><br/><span class="koboSpan" id="kobo.222.1">                next_state = np.append(state[1:], np.expand_dims(obs_small, 0), axis=0)</span><br/><span class="koboSpan" id="kobo.223.1">                episode_reward += reward</span><br/><span class="koboSpan" id="kobo.224.1">                # Remove oldest experience if replay buffer is full</span><br/><span class="koboSpan" id="kobo.225.1">                if len(experience_replay_buffer) == MAX_EXPERIENCES:</span><br/><span class="koboSpan" id="kobo.226.1">                    experience_replay_buffer.pop(0)</span><br/><span class="koboSpan" id="kobo.227.1">                    # Save the recent experience</span><br/><span class="koboSpan" id="kobo.228.1">                    experience_replay_buffer.append((state, action, reward, next_state, done))</span><br/><br/><span class="koboSpan" id="kobo.229.1">                # Train the model and keep measure of time</span><br/><span class="koboSpan" id="kobo.230.1">                t0_2 = datetime.now()</span><br/><span class="koboSpan" id="kobo.231.1">                loss = learn(model, target_model, experience_replay_buffer, gamma, batch_sz)</span><br/><span class="koboSpan" id="kobo.232.1">                dt = datetime.now() - t0_2</span><br/><span class="koboSpan" id="kobo.233.1">                total_time_training += dt.total_seconds()</span><br/><span class="koboSpan" id="kobo.234.1">                num_steps_in_episode += 1</span><br/><span class="koboSpan" id="kobo.235.1">                state = next_state</span><br/><span class="koboSpan" id="kobo.236.1">                total_t += 1</span><br/><span class="koboSpan" id="kobo.237.1">                epsilon = max(epsilon - epsilon_change, epsilon_min)</span><br/><span class="koboSpan" id="kobo.238.1">                duration = datetime.now() - t0</span><br/><span class="koboSpan" id="kobo.239.1">                episode_rewards[i] = episode_reward</span><br/><span class="koboSpan" id="kobo.240.1">                time_per_step = total_time_training / num_steps_in_episode</span><br/><span class="koboSpan" id="kobo.241.1">                last_100_avg = episode_rewards[max(0, i - 100):i + 1].mean()</span><br/><span class="koboSpan" id="kobo.242.1">                last_100_avgs.append(last_100_avg)</span><br/><span class="koboSpan" id="kobo.243.1">                print("Episode:", i,"Duration:", duration, "Num steps:", num_steps_in_episode, "Reward:", episode_reward, "Training time per step:", "%.3f" % time_per_step, "Avg Reward (Last 100):", "%.3f" % last_100_avg,"Epsilon:", "%.3f" % epsilon)</span><br/><span class="koboSpan" id="kobo.244.1">                if i % 50 == 0:</span><br/><span class="koboSpan" id="kobo.245.1">                    model.save(i)</span><br/><span class="koboSpan" id="kobo.246.1">                sys.stdout.flush()</span><br/><br/><span class="koboSpan" id="kobo.247.1">#Plots</span><br/><span class="koboSpan" id="kobo.248.1">plt.plot(last_100_avgs)</span><br/><span class="koboSpan" id="kobo.249.1">plt.xlabel('episodes')</span><br/><span class="koboSpan" id="kobo.250.1">plt.ylabel('Average Rewards')</span><br/><span class="koboSpan" id="kobo.251.1">plt.show()</span><br/><span class="koboSpan" id="kobo.252.1">env.close()</span></pre>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.253.1">We can see that now the reward is increasing with episodes, with an average reward of </span><strong><span class="koboSpan" id="kobo.254.1">20</span></strong><span class="koboSpan" id="kobo.255.1"> by the end, though it can be higher, then we had only learned few thousand episodes and even our replay buffer with a size between (50,00 to 5,000,000):</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.256.1"><img src="assets/4f2a3911-d5d0-49e6-a497-be4aa063ce9b.png" style="width:27.67em;height:19.08em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.257.1"> Average rewards as the agent learn </span></div>
<ol start="13">
<li><span class="koboSpan" id="kobo.258.1">Let's see how our agent plays, after learning for about 2,700 episodes:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.259.1">env = gym.envs.make("Breakout-v0")</span><br/><span class="koboSpan" id="kobo.260.1">frames = []</span><br/><span class="koboSpan" id="kobo.261.1">with tf.Session() as sess:</span><br/><span class="koboSpan" id="kobo.262.1">    model.set_session(sess)</span><br/><span class="koboSpan" id="kobo.263.1">    target_model.set_session(sess)</span><br/><span class="koboSpan" id="kobo.264.1">    sess.run(tf.global_variables_initializer())</span><br/><span class="koboSpan" id="kobo.265.1">    model.load()</span><br/><span class="koboSpan" id="kobo.266.1">    obs = env.reset()</span><br/><span class="koboSpan" id="kobo.267.1">    obs_small = preprocess(obs)</span><br/><span class="koboSpan" id="kobo.268.1">    state = np.stack([obs_small] * 4, axis=0)</span><br/><span class="koboSpan" id="kobo.269.1">    done = False</span><br/><span class="koboSpan" id="kobo.270.1">    while not done:</span><br/><span class="koboSpan" id="kobo.271.1">        action = model.sample_action(state, epsilon)</span><br/><span class="koboSpan" id="kobo.272.1">        obs, reward, done, _ = env.step(action)</span><br/><span class="koboSpan" id="kobo.273.1">        frames.append(env.render(mode='rgb_array'))</span><br/><span class="koboSpan" id="kobo.274.1">        next_state = update_state(state, obs)</span><br/><span class="koboSpan" id="kobo.275.1">        state = next_state</span></pre>
<p><span class="koboSpan" id="kobo.276.1">You can see the video of the learned agent here: </span><a href="https://www.youtube.com/watch?v=rPy-3NodgCE"><span class="koboSpan" id="kobo.277.1">https://www.youtube.com/watch?v=rPy-3NodgCE</span></a><span class="koboSpan" id="kobo.278.1">.</span></p>
<p><span class="koboSpan" id="kobo.279.1">Cool, right? </span><span class="koboSpan" id="kobo.279.2">Without telling it anything, it learned to play a decent game after only 2,700 episodes. </span></p>
<div class="packt_tip"><span class="koboSpan" id="kobo.280.1">There are some things that can help you to train the agent better:</span><br/>
<ul>
<li><span class="koboSpan" id="kobo.281.1">Since training takes a lot of time, unless you have a strong computational resource, it's better to save the model and restart the saved model.</span></li>
<li><span class="koboSpan" id="kobo.282.1">In the code, we used </span><kbd><span class="koboSpan" id="kobo.283.1">Breakout-v0</span></kbd><span class="koboSpan" id="kobo.284.1"> and OpenAI gym, in this case, repeats the same step in the environment for consecutive (randomly chosen </span><kbd><span class="koboSpan" id="kobo.285.1">1</span></kbd><span class="koboSpan" id="kobo.286.1">, </span><kbd><span class="koboSpan" id="kobo.287.1">2</span></kbd><span class="koboSpan" id="kobo.288.1">, </span><kbd><span class="koboSpan" id="kobo.289.1">3</span></kbd><span class="koboSpan" id="kobo.290.1"> or </span><kbd><span class="koboSpan" id="kobo.291.1">4</span></kbd><span class="koboSpan" id="kobo.292.1">) frames. </span><span class="koboSpan" id="kobo.292.2">You can instead choose </span><kbd><span class="koboSpan" id="kobo.293.1">BreakoutDeterministic-v4</span></kbd><span class="koboSpan" id="kobo.294.1">, the one used by the DeepMind team; here, the steps are repeated for exactly four consecutive frames. </span><span class="koboSpan" id="kobo.294.2">The agent hence sees and selects the action after every fourth frame. </span><span class="koboSpan" id="kobo.294.3"> </span></li>
</ul>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Double DQN</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Now, recall that, we're using a max operator to both select an action and to evaluate an action. </span><span class="koboSpan" id="kobo.2.2">This can result in overestimated values for an action that may not be an ideal one. </span><span class="koboSpan" id="kobo.2.3">We can take care of this problem by decoupling the selection from evaluation. </span><span class="koboSpan" id="kobo.2.4">With Double DQN, we have two Q-Networks with different weights; both learn by random experience, but one is used to determine the action using the epsilon-greedy policy and the other to determine its value (hence, calculating the target Q).</span></p>
<p><span class="koboSpan" id="kobo.3.1">To make it clearer, let's first see the case of the DQN. </span><span class="koboSpan" id="kobo.3.2">The action with maximum Q-value is selected; let </span><em><span class="koboSpan" id="kobo.4.1">W</span></em><span class="koboSpan" id="kobo.5.1"> be the weight of the DQN, then what we're doing is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.6.1"><img class="fm-editor-equation" src="assets/82f81b0f-9082-4204-b661-6e9b18499ca6.png" style="width:26.08em;height:1.75em;"/></span></div>
<p><span class="koboSpan" id="kobo.7.1">The superscript </span><em><span class="koboSpan" id="kobo.8.1">W</span></em><span class="koboSpan" id="kobo.9.1"> tells the weights used to approximate the Q-value. </span><span class="koboSpan" id="kobo.9.2">In Double DQN, the equation changes to the following:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.10.1"><img class="fm-editor-equation" src="assets/64ff7e73-19e9-4c73-a3b7-1f9dfc40e701.png" style="width:27.67em;height:1.92em;"/></span></div>
<p><span class="koboSpan" id="kobo.11.1">Note the change: now the action is chosen using the Q-Network with the weights </span><em><span class="koboSpan" id="kobo.12.1">W</span></em><span class="koboSpan" id="kobo.13.1">, and max Q-value is predicted using a Q-Network with weights </span><em><span class="koboSpan" id="kobo.14.1">W'.</span></em><span class="koboSpan" id="kobo.15.1"> This reduces the overestimation and helps us to train the agent quickly and more reliably. </span><span class="koboSpan" id="kobo.15.2">You can access the </span><em><span class="koboSpan" id="kobo.16.1">Deep Reinforcement Learning with Double Q-Learning</span></em><span class="koboSpan" id="kobo.17.1"> paper here: </span><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847"><span class="koboSpan" id="kobo.18.1">https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847</span></a><span class="koboSpan" id="kobo.19.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Dueling DQN</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Dueling DQN decouples the Q-function into the value function and advantage function. </span><span class="koboSpan" id="kobo.2.2">The value function is the same as discussed </span><span><span class="koboSpan" id="kobo.3.1">earlier </span></span><span class="koboSpan" id="kobo.4.1">; it represents the value of the state independent of action. </span><span class="koboSpan" id="kobo.4.2">The advantage function, on the other hand, provides a relative measure of the utility (advantage/goodness) of action </span><em><span class="koboSpan" id="kobo.5.1">a</span></em><span class="koboSpan" id="kobo.6.1"> in the state </span><em><span class="koboSpan" id="kobo.7.1">s:</span></em></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.8.1"><img class="fm-editor-equation" src="assets/4cf3f468-8c90-4d54-b1dc-5321a9cc9823.png" style="width:14.75em;height:1.58em;"/></span></div>
<p><span><span class="koboSpan" id="kobo.9.1">In Dueling DQN, the same convolutional is used to extract features but, in later stages, it's separated into two separate networks, one providing the value and another providing the advantage. </span><span class="koboSpan" id="kobo.9.2">Later, the two stages are recombined using an aggregating layer to estimate the Q-value. </span><span class="koboSpan" id="kobo.9.3">This ensures that the network produces separate estimates for the value function and the advantage function. </span><span class="koboSpan" id="kobo.9.4">The intuition behind this decoupling of value and advantage is that, for many states, it's unnecessary to estimate the value of each action choice. For example, in the car race, if there's no car in front, then the action turn left or turn right is not required and so there's no need to estimate the value of these actions on the given state. </span><span class="koboSpan" id="kobo.9.5"> </span></span><span><span class="koboSpan" id="kobo.10.1">This allows it to learn which states are valuable, without having to determine the effect of each action for each state. </span></span></p>
<p><span class="koboSpan" id="kobo.11.1">At the aggregate layer, the value and advantage are combined such that it's possible to recover both </span><em><span class="koboSpan" id="kobo.12.1">V</span></em><span class="koboSpan" id="kobo.13.1"> and </span><em><span class="koboSpan" id="kobo.14.1">A</span></em><span class="koboSpan" id="kobo.15.1"> uniquely from a given </span><em><span class="koboSpan" id="kobo.16.1">Q</span></em><span class="koboSpan" id="kobo.17.1">. </span><span class="koboSpan" id="kobo.17.2">This is achieved by enforcing that the advantage function estimator has zero advantage at the chosen action:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.18.1"> </span><span class="koboSpan" id="kobo.19.1"><img class="fm-editor-equation" src="assets/a235ba01-25da-4810-b326-fc80f8b57e79.png" style="width:39.08em;height:1.92em;"/></span></div>
<p class="mce-root CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.20.1">Here, </span><em><span class="koboSpan" id="kobo.21.1">θ</span></em><span class="koboSpan" id="kobo.22.1"> is the parameter of the common convolutional feature extractor, and </span><em><span class="koboSpan" id="kobo.23.1">α</span></em><span class="koboSpan" id="kobo.24.1"> and </span><em><span class="koboSpan" id="kobo.25.1">β</span></em><span class="koboSpan" id="kobo.26.1"> are the parameters for the advantage and value estimator network. </span><span class="koboSpan" id="kobo.26.2">The Dueling DQN too was proposed by Google's DeepMind team. </span><span class="koboSpan" id="kobo.26.3">You can read the complete paper at </span><em><span class="koboSpan" id="kobo.27.1">arXiv</span></em><span class="koboSpan" id="kobo.28.1">: </span><a href="https://arxiv.org/abs/1511.06581"><span class="koboSpan" id="kobo.29.1">https://arxiv.org/abs/1511.06581</span></a><span class="koboSpan" id="kobo.30.1">. </span><span class="koboSpan" id="kobo.30.2">The authors found that changing the preceding </span><kbd><span class="koboSpan" id="kobo.31.1">max</span></kbd><span class="koboSpan" id="kobo.32.1"> operator with an average operator increases the stability of the network. </span><span class="koboSpan" id="kobo.32.2">The advantage, in this case, changes only as fast as the mean. </span><span class="koboSpan" id="kobo.32.3">Hence, in their results, they used the aggregate layer given by the following:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.33.1"><img class="fm-editor-equation" src="assets/bd2bd8f7-e556-4009-aa53-b1d457099849.png" style="width:34.92em;height:3.58em;"/></span></div>
<p><span><span class="koboSpan" id="kobo.34.1">The following screenshot shows the basic architecture of a Dueling DQN:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.35.1"><img class="aligncenter size-full wp-image-1098 image-border" src="assets/04aeee5f-ab54-4e73-873f-66b6fece0574.png" style="width:141.00em;height:69.75em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.36.1"> The basic architecture of Dueling DQN</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Policy gradients</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the Q-learning-based methods, we generated a policy after estimating a value/Q-function. </span><span class="koboSpan" id="kobo.2.2">In policy-based methods, such as the policy gradient, we approximate the policy directly.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Continuing as earlier, here, we use a neural network to approximate the policy. </span><span class="koboSpan" id="kobo.3.2">In the simplest form, the neural network learns a policy for selecting the actions that maximize the rewards by adjusting its weights using steepest gradient ascent, hence the name policy gradients. </span></p>
<p><span class="koboSpan" id="kobo.4.1">In policy gradients, the policy is represented by a neural network whose input is a representation of states and whose output is action selection probabilities. </span><span class="koboSpan" id="kobo.4.2">The weights of this network are the policy parameters that we need to learn. </span><span class="koboSpan" id="kobo.4.3">The natural question arises: how should we update the weights of this network? </span><span class="koboSpan" id="kobo.4.4">Since our goal is to maximize rewards, it makes sense that our network tries to maximize the expected rewards per episode:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.5.1"><img class="fm-editor-equation" src="assets/05ad2539-cfd2-4f99-8e91-9edc0d186e10.png" style="width:11.00em;height:1.42em;"/></span></div>
<p><span class="koboSpan" id="kobo.6.1">Here, we've taken a parametrized stochastic policy </span><em><span class="koboSpan" id="kobo.7.1">π—</span></em><span class="koboSpan" id="kobo.8.1">that is, the policy determines the probability of choosing an action </span><em><span class="koboSpan" id="kobo.9.1">a</span></em><span class="koboSpan" id="kobo.10.1"> given state </span><em><span class="koboSpan" id="kobo.11.1">s</span></em><span class="koboSpan" id="kobo.12.1">, and the neural network parameters are </span><em><span class="koboSpan" id="kobo.13.1">θ</span></em><span class="koboSpan" id="kobo.14.1">. </span><em><span class="koboSpan" id="kobo.15.1">R</span></em><span class="koboSpan" id="kobo.16.1"> represents the sum of all of the rewards in an episode. </span><span class="koboSpan" id="kobo.16.2">The network parameters are then updated using gradient ascent:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.17.1"><img class="fm-editor-equation" src="assets/1198f6cd-5308-4e80-8911-e375a002e057.png" style="width:13.50em;height:1.92em;"/></span></div>
<p><span class="koboSpan" id="kobo.18.1">Here, </span><em><span class="koboSpan" id="kobo.19.1">η</span></em><span class="koboSpan" id="kobo.20.1"> is the learning rate. </span><span class="koboSpan" id="kobo.20.2">Using the policy gradient theorem, we get the following:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.21.1"><img class="fm-editor-equation" src="assets/0911f1c5-bb9d-46c1-ade7-90cab47fdc5c.png" style="width:22.08em;height:4.00em;"/></span></div>
<p><span class="koboSpan" id="kobo.22.1">Hence, instead of maximizing the expected return, we can use loss function as log-loss (expected action and predicted action as labels and logits respectively) and the discounted reward as the weight to train the network. </span><span class="koboSpan" id="kobo.22.2">For more stability, it has been found that adding a baseline helps in variance reduction. </span><span class="koboSpan" id="kobo.22.3">The most common form of the baseline is the sum of the discounted rewards, resulting in the following:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.23.1"><img class="fm-editor-equation" src="assets/2871ebfe-531e-4f6e-acbd-4e71ca5af1ea.png" style="width:32.25em;height:4.17em;"/></span></div>
<p><span class="koboSpan" id="kobo.24.1">The baseline </span><em><span class="koboSpan" id="kobo.25.1">b</span></em><span class="koboSpan" id="kobo.26.1">(</span><em><span class="koboSpan" id="kobo.27.1">s</span><sub><span class="koboSpan" id="kobo.28.1">t</span></sub></em><span class="koboSpan" id="kobo.29.1">) is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.30.1"><img class="fm-editor-equation" src="assets/5d1d07d4-39cf-4400-876e-297c54fe5302.png" style="width:25.67em;height:1.75em;"/></span></div>
<p><span class="koboSpan" id="kobo.31.1">Here, </span><em><span class="koboSpan" id="kobo.32.1">γ</span></em><span class="koboSpan" id="kobo.33.1"> is the discount factor. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Why policy gradients?</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Well, first of all, policy gradients, like other policy-based methods, directly estimate the optimal policy, without any need to store additional data (experience replay buffer). </span><span class="koboSpan" id="kobo.2.2">Hence, it's simple to implement. </span><span class="koboSpan" id="kobo.2.3">Secondly, we can train it to learn true stochastic policies. </span><span class="koboSpan" id="kobo.2.4">And finally, it's well suited for continuous action-space.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Pong using policy gradients</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's try to use policy gradients to play a game of Pong. </span><span class="koboSpan" id="kobo.2.2">The Andrej Karpathy blog post, at </span><a href="http://karpathy.github.io/2016/05/31/rl/"><span class="koboSpan" id="kobo.3.1">http://karpathy.github.io/2016/05/31/rl/</span></a><span class="koboSpan" id="kobo.4.1"> inspires the implementation here</span><span><span class="koboSpan" id="kobo.5.1">. </span><span class="koboSpan" id="kobo.5.2">Recall that, in </span><em><span class="koboSpan" id="kobo.6.1">Breakout</span></em><span class="koboSpan" id="kobo.7.1">, we used four-game frames stacked together as input so that the game dynamics are known to the agent; here, we use the difference between two consecutive game frames as the input to the network. </span><span class="koboSpan" id="kobo.7.2">Hence, our agent has information about the present state and the previous state with it:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.8.1">The first step, as always, is importing the modules necessary. </span><span class="koboSpan" id="kobo.8.2">We import TensorFlow, Numpy, Matplotlib, and </span><kbd><span class="koboSpan" id="kobo.9.1">gym</span></kbd><span class="koboSpan" id="kobo.10.1"> for the environment:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.11.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.12.1">import gym</span><br/><span class="koboSpan" id="kobo.13.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.14.1">import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.15.1">from gym import wrappers</span><br/><span class="koboSpan" id="kobo.16.1">%matplotlib inline</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li><span class="koboSpan" id="kobo.17.1">We build our neural network, the </span><kbd><span class="koboSpan" id="kobo.18.1">PolicyNetwork</span></kbd><span class="koboSpan" id="kobo.19.1">; it takes as input the state of the game, and outputs the action selection probabilities. </span><span class="koboSpan" id="kobo.19.2">Here, we build a simple two-layered perceptron, with no biases. </span><kbd><span class="koboSpan" id="kobo.20.1">weights</span></kbd><span class="koboSpan" id="kobo.21.1"> are initialized randomly using the </span><kbd><span class="koboSpan" id="kobo.22.1">Xavier</span></kbd><span class="koboSpan" id="kobo.23.1"> initialization. </span><span class="koboSpan" id="kobo.23.2">The hidden layer uses the </span><kbd><span class="koboSpan" id="kobo.24.1">ReLU</span></kbd><span class="koboSpan" id="kobo.25.1"> activation function, and the output layer uses the </span><kbd><span class="koboSpan" id="kobo.26.1">softmax</span></kbd><span class="koboSpan" id="kobo.27.1"> activation function. </span><span class="koboSpan" id="kobo.27.2">We use the </span><kbd><span class="koboSpan" id="kobo.28.1">tf_discount_rewards</span></kbd><span class="koboSpan" id="kobo.29.1"> method defined later to calculate the baseline. </span><span class="koboSpan" id="kobo.29.2">And finally, we've used TensorFlow </span><kbd><span class="koboSpan" id="kobo.30.1">tf.losses.log_loss</span></kbd><span class="koboSpan" id="kobo.31.1"> with calculated action probabilities as predictions, and chosen one-hot action vector as labels and discounted reward corrected by variance as weight:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.32.1">class PolicyNetwork(object):</span><br/><span class="koboSpan" id="kobo.33.1">    def __init__(self, N_SIZE, h=200, gamma=0.99, eta=1e-3, decay=0.99, save_path = 'models1/pong.ckpt' ):</span><br/><span class="koboSpan" id="kobo.34.1">        self.gamma = gamma</span><br/><span class="koboSpan" id="kobo.35.1">        self.save_path = save_path</span><br/><span class="koboSpan" id="kobo.36.1">        # Placeholders for passing state....</span><br/><span class="koboSpan" id="kobo.37.1">        self.tf_x = tf.placeholder(dtype=tf.float32, shape=[None, N_SIZE * N_SIZE], name="tf_x")</span><br/><span class="koboSpan" id="kobo.38.1">        self.tf_y = tf.placeholder(dtype=tf.float32, shape=[None, n_actions], name="tf_y")</span><br/><span class="koboSpan" id="kobo.39.1">        self.tf_epr = tf.placeholder(dtype=tf.float32, shape=[None, 1], name="tf_epr")</span><br/><br/><span class="koboSpan" id="kobo.40.1">        # Weights</span><br/><span class="koboSpan" id="kobo.41.1">        xavier_l1 = tf.truncated_normal_initializer(mean=0, stddev=1. </span><span class="koboSpan" id="kobo.41.2">/ N_SIZE, dtype=tf.float32)</span><br/><span class="koboSpan" id="kobo.42.1">        self.W1 = tf.get_variable("W1", [N_SIZE * N_SIZE, h], initializer=xavier_l1)</span><br/><span class="koboSpan" id="kobo.43.1">        xavier_l2 = tf.truncated_normal_initializer(mean=0, stddev=1. </span><span class="koboSpan" id="kobo.43.2">/ np.sqrt(h), dtype=tf.float32)</span><br/><span class="koboSpan" id="kobo.44.1">        self.W2 = tf.get_variable("W2", [h, n_actions], initializer=xavier_l2)</span><br/><br/><span class="koboSpan" id="kobo.45.1">        #Build Computation</span><br/><span class="koboSpan" id="kobo.46.1">        # tf reward processing (need tf_discounted_epr for policy gradient wizardry)</span><br/><span class="koboSpan" id="kobo.47.1">        tf_discounted_epr = self.tf_discount_rewards(self.tf_epr)</span><br/><span class="koboSpan" id="kobo.48.1">        tf_mean, tf_variance = tf.nn.moments(tf_discounted_epr, [0], shift=None, name="reward_moments")</span><br/><span class="koboSpan" id="kobo.49.1">        tf_discounted_epr -= tf_mean</span><br/><span class="koboSpan" id="kobo.50.1">        tf_discounted_epr /= tf.sqrt(tf_variance + 1e-6)</span><br/><br/><span class="koboSpan" id="kobo.51.1">        #Define Optimizer, compute and apply gradients</span><br/><span class="koboSpan" id="kobo.52.1">        self.tf_aprob = self.tf_policy_forward(self.tf_x)</span><br/><span class="koboSpan" id="kobo.53.1">        loss = tf.losses.log_loss(labels = self.tf_y,</span><br/><span class="koboSpan" id="kobo.54.1">        predictions = self.tf_aprob,</span><br/><span class="koboSpan" id="kobo.55.1">        weights = tf_discounted_epr)</span><br/><span class="koboSpan" id="kobo.56.1">        optimizer = tf.train.AdamOptimizer()</span><br/><span class="koboSpan" id="kobo.57.1">        self.train_op = optimizer.minimize(loss)</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.58.1">The class has methods to calculate the action probabilities (</span><kbd><span class="koboSpan" id="kobo.59.1">tf_policy_forward</span></kbd><span class="koboSpan" id="kobo.60.1"> and </span><kbd><span class="koboSpan" id="kobo.61.1">predict_UP</span></kbd><span class="koboSpan" id="kobo.62.1">), calculate the baseline using </span><kbd><span class="koboSpan" id="kobo.63.1">tf_discount_rewards</span></kbd><span class="koboSpan" id="kobo.64.1">, update the weights of the network (</span><kbd><span class="koboSpan" id="kobo.65.1">update</span></kbd><span class="koboSpan" id="kobo.66.1">), and finally set the session (</span><kbd><span class="koboSpan" id="kobo.67.1">set_session</span></kbd><span class="koboSpan" id="kobo.68.1">), then load and save the model: </span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.69.1">def set_session(self, session):</span><br/><span class="koboSpan" id="kobo.70.1">    self.session = session</span><br/><span class="koboSpan" id="kobo.71.1">    self.session.run(tf.global_variables_initializer())</span><br/><span class="koboSpan" id="kobo.72.1">    self.saver = tf.train.Saver()</span><br/><br/><span class="koboSpan" id="kobo.73.1">def tf_discount_rewards(self, tf_r): # tf_r ~ [game_steps,1]</span><br/><span class="koboSpan" id="kobo.74.1">    discount_f = lambda a, v: a * self.gamma + v;</span><br/><span class="koboSpan" id="kobo.75.1">    tf_r_reverse = tf.scan(discount_f, tf.reverse(tf_r, [0]))</span><br/><span class="koboSpan" id="kobo.76.1">    tf_discounted_r = tf.reverse(tf_r_reverse, [0])</span><br/><span class="koboSpan" id="kobo.77.1">    return tf_discounted_r</span><br/><br/><br/><span class="koboSpan" id="kobo.78.1">def tf_policy_forward(self, x): #x ~ [1,D]</span><br/><span class="koboSpan" id="kobo.79.1">    h = tf.matmul(x, self.W1)</span><br/><span class="koboSpan" id="kobo.80.1">    h = tf.nn.relu(h)</span><br/><span class="koboSpan" id="kobo.81.1">    logp = tf.matmul(h, self.W2)</span><br/><span class="koboSpan" id="kobo.82.1">    p = tf.nn.softmax(logp)</span><br/><span class="koboSpan" id="kobo.83.1">    return p</span><br/><br/><br/><span class="koboSpan" id="kobo.84.1">def update(self, feed):</span><br/><span class="koboSpan" id="kobo.85.1">    return self.session.run(self.train_op, feed)</span><br/><br/><br/><span class="koboSpan" id="kobo.86.1">def load(self):</span><br/><span class="koboSpan" id="kobo.87.1">    self.saver = tf.train.Saver(tf.global_variables())</span><br/><span class="koboSpan" id="kobo.88.1">    load_was_success = True </span><br/><span class="koboSpan" id="kobo.89.1">    try:</span><br/><span class="koboSpan" id="kobo.90.1">        save_dir = '/'.join(self.save_path.split('/')[:-1])</span><br/><span class="koboSpan" id="kobo.91.1">        ckpt = tf.train.get_checkpoint_state(save_dir)</span><br/><span class="koboSpan" id="kobo.92.1">        load_path = ckpt.model_checkpoint_path</span><br/><span class="koboSpan" id="kobo.93.1">        print(load_path)</span><br/><span class="koboSpan" id="kobo.94.1">        self.saver.restore(self.session, load_path)</span><br/><span class="koboSpan" id="kobo.95.1">    except:</span><br/><span class="koboSpan" id="kobo.96.1">        print("no saved model to load. </span><span class="koboSpan" id="kobo.96.2">starting new session")</span><br/><span class="koboSpan" id="kobo.97.1">        load_was_success = False</span><br/><span class="koboSpan" id="kobo.98.1">    else:</span><br/><span class="koboSpan" id="kobo.99.1">        print("loaded model: {}".format(load_path))</span><br/><span class="koboSpan" id="kobo.100.1">        saver = tf.train.Saver(tf.global_variables())</span><br/><span class="koboSpan" id="kobo.101.1">        episode_number = int(load_path.split('-')[-1])</span><br/><br/><br/><span class="koboSpan" id="kobo.102.1">def save(self):</span><br/><span class="koboSpan" id="kobo.103.1">    self.saver.save(self.session, self.save_path, global_step=n)</span><br/><span class="koboSpan" id="kobo.104.1">    print("SAVED MODEL #{}".format(n))</span><br/><br/><span class="koboSpan" id="kobo.105.1">def predict_UP(self,x):</span><br/><span class="koboSpan" id="kobo.106.1">    feed = {self.tf_x: np.reshape(x, (1, -1))}</span><br/><span class="koboSpan" id="kobo.107.1">    aprob = self.session.run(self.tf_aprob, feed);</span><br/><span class="koboSpan" id="kobo.108.1">    return aprob</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.109.1">Now that </span><kbd><span class="koboSpan" id="kobo.110.1">PolicyNetwork</span></kbd><span class="koboSpan" id="kobo.111.1"> is made, we make a </span><kbd><span><span class="koboSpan" id="kobo.112.1">preprocess</span></span></kbd><span class="koboSpan" id="kobo.113.1"> function to the game state; we won't process the complete 210×160 state space—instead, we'll reduce it to an 80×80 state space, in binary, and finally flatten it:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.114.1"># downsampling</span><br/><span class="koboSpan" id="kobo.115.1">def preprocess(I):</span><br/><span class="koboSpan" id="kobo.116.1">    """ </span><br/><span class="koboSpan" id="kobo.117.1">    prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector </span><br/><span class="koboSpan" id="kobo.118.1">    """</span><br/><span class="koboSpan" id="kobo.119.1">    I = I[35:195] # crop</span><br/><span class="koboSpan" id="kobo.120.1">    I = I[::2,::2,0] # downsample by factor of 2</span><br/><span class="koboSpan" id="kobo.121.1">    I[I == 144] = 0 # erase background (background type 1)</span><br/><span class="koboSpan" id="kobo.122.1">    I[I == 109] = 0 # erase background (background type 2)</span><br/><span class="koboSpan" id="kobo.123.1">    I[I != 0] = 1 # everything else (paddles, ball) just set to 1</span><br/><span class="koboSpan" id="kobo.124.1">    return I.astype(np.float).ravel()</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.125.1">Let's define some variables that we'll require to hold state, labels, rewards, and action space size. </span><span class="koboSpan" id="kobo.125.2">We initialize the game state and instantiate the policy network:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.126.1"># Create Game Environment</span><br/><span class="koboSpan" id="kobo.127.1">env_name = "Pong-v0"</span><br/><span class="koboSpan" id="kobo.128.1">env = gym.make(env_name)</span><br/><span class="koboSpan" id="kobo.129.1">env = wrappers.Monitor(env, '/tmp/pong', force=True)</span><br/><span class="koboSpan" id="kobo.130.1">n_actions = env.action_space.n # Number of possible actions</span><br/><span class="koboSpan" id="kobo.131.1"># Initializing Game and State(t-1), action, reward, state(t)</span><br/><span class="koboSpan" id="kobo.132.1">states, rewards, labels = [], [], []</span><br/><span class="koboSpan" id="kobo.133.1">obs = env.reset()</span><br/><span class="koboSpan" id="kobo.134.1">prev_state = None</span><br/><br/><span class="koboSpan" id="kobo.135.1">running_reward = None</span><br/><span class="koboSpan" id="kobo.136.1">running_rewards = []</span><br/><span class="koboSpan" id="kobo.137.1">reward_sum = 0</span><br/><span class="koboSpan" id="kobo.138.1">n = 0</span><br/><span class="koboSpan" id="kobo.139.1">done = False</span><br/><span class="koboSpan" id="kobo.140.1">n_size = 80</span><br/><span class="koboSpan" id="kobo.141.1">num_episodes = 2500</span><br/><br/><span class="koboSpan" id="kobo.142.1">#Create Agent</span><br/><span class="koboSpan" id="kobo.143.1">agent = PolicyNetwork(n_size)</span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.144.1">Now we start the policy gradient algorithm. </span><span class="koboSpan" id="kobo.144.2">For each episode, the agent first plays the game, storing the states, rewards, and actions chosen. </span><span class="koboSpan" id="kobo.144.3">Once a game is over, it uses all of the stored data to train itself (just like in supervised learning). </span><span class="koboSpan" id="kobo.144.4">And it repeats this process for as many episodes as you want:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.145.1">with tf.Session() as sess:</span><br/><span class="koboSpan" id="kobo.146.1">    agent.set_session(sess)</span><br/><span class="koboSpan" id="kobo.147.1">    sess.run(tf.global_variables_initializer())</span><br/><span class="koboSpan" id="kobo.148.1">    agent.load()</span><br/><span class="koboSpan" id="kobo.149.1">    # training loop</span><br/><span class="koboSpan" id="kobo.150.1">    done = False</span><br/><span class="koboSpan" id="kobo.151.1">    while not done and n&lt; num_episodes:</span><br/><span class="koboSpan" id="kobo.152.1">        # Preprocess the observation</span><br/><span class="koboSpan" id="kobo.153.1">        cur_state = preprocess(obs)</span><br/><span class="koboSpan" id="kobo.154.1">        diff_state = cur_state - prev_state if prev_state isn't None else np.zeros(n_size*n_size)</span><br/><span class="koboSpan" id="kobo.155.1">        prev_state = cur_state</span><br/><br/><span class="koboSpan" id="kobo.156.1">        #Predict the action</span><br/><span class="koboSpan" id="kobo.157.1">        aprob = agent.predict_UP(diff_state) ; aprob = aprob[0,:]</span><br/><span class="koboSpan" id="kobo.158.1">        action = np.random.choice(n_actions, p=aprob)</span><br/><span class="koboSpan" id="kobo.159.1">        #print(action)</span><br/><span class="koboSpan" id="kobo.160.1">        label = np.zeros_like(aprob) ; label[action] = 1</span><br/><br/><span class="koboSpan" id="kobo.161.1">        # Step the environment and get new measurements</span><br/><span class="koboSpan" id="kobo.162.1">        obs, reward, done, info = env.step(action)</span><br/><span class="koboSpan" id="kobo.163.1">        env.render()</span><br/><span class="koboSpan" id="kobo.164.1">        reward_sum += reward</span><br/><br/><span class="koboSpan" id="kobo.165.1">        # record game history</span><br/><span class="koboSpan" id="kobo.166.1">        states.append(diff_state) ; labels.append(label) ; rewards.append(reward)</span><br/><br/><span class="koboSpan" id="kobo.167.1">        if done:</span><br/><span class="koboSpan" id="kobo.168.1">            # update running reward</span><br/><span class="koboSpan" id="kobo.169.1">            running_reward = reward_sum if running_reward is None else         running_reward * 0.99 + reward_sum * 0.01    </span><br/><span class="koboSpan" id="kobo.170.1">            running_rewards.append(running_reward)</span><br/><span class="koboSpan" id="kobo.171.1">            #print(np.vstack(rs).shape)</span><br/><span class="koboSpan" id="kobo.172.1">            feed = {agent.tf_x: np.vstack(states), agent.tf_epr: np.vstack(rewards), agent.tf_y: np.vstack(labels)}</span><br/><span class="koboSpan" id="kobo.173.1">            agent.update(feed)</span><br/><span class="koboSpan" id="kobo.174.1">            # print progress console</span><br/><span class="koboSpan" id="kobo.175.1">            if n % 10 == 0:</span><br/><span class="koboSpan" id="kobo.176.1">                print ('ep {}: reward: {}, mean reward: {:3f}'.format(n, reward_sum, running_reward))</span><br/><span class="koboSpan" id="kobo.177.1">            else:</span><br/><span class="koboSpan" id="kobo.178.1">                print ('\tep {}: reward: {}'.format(n, reward_sum))</span><br/><br/><span class="koboSpan" id="kobo.179.1">            # Start next episode and save model</span><br/><span class="koboSpan" id="kobo.180.1">            states, rewards, labels = [], [], []</span><br/><span class="koboSpan" id="kobo.181.1">            obs = env.reset()</span><br/><span class="koboSpan" id="kobo.182.1">            n += 1 # the Next Episode</span><br/><br/><span class="koboSpan" id="kobo.183.1">            reward_sum = 0</span><br/><span class="koboSpan" id="kobo.184.1">            if n % 50 == 0:</span><br/><span class="koboSpan" id="kobo.185.1">                agent.save()</span><br/><span class="koboSpan" id="kobo.186.1">            done = False</span><br/><br/><span class="koboSpan" id="kobo.187.1">plt.plot(running_rewards)</span><br/><span class="koboSpan" id="kobo.188.1">plt.xlabel('episodes')</span><br/><span class="koboSpan" id="kobo.189.1">plt.ylabel('Running Averge')</span><br/><span class="koboSpan" id="kobo.190.1">plt.show()</span><br/><span class="koboSpan" id="kobo.191.1">env.close()</span></pre>
<div style="margin-left: 2em">
<ol start="7">
<li><span class="koboSpan" id="kobo.192.1">After training for 7,500 episodes, it started winning some games. </span><span class="koboSpan" id="kobo.192.2">After 1,200 episodes the winning rate improved, and it was winning 50% of the time. </span><span class="koboSpan" id="kobo.192.3">After 20,000 episodes, the agent was winning most games. </span><span class="koboSpan" id="kobo.192.4">The complete code is available at GitHub in the </span><kbd><span class="koboSpan" id="kobo.193.1">Policy gradients.ipynb</span></kbd><span class="koboSpan" id="kobo.194.1"> </span><span><span class="koboSpan" id="kobo.195.1">file. </span></span><span class="koboSpan" id="kobo.196.1">And you can see the game played by the agent after learning for 20,000 episodes here: </span><a href="https://youtu.be/hZo7kAco8is"><span class="koboSpan" id="kobo.197.1">https://youtu.be/hZo7kAco8is</span></a><span class="koboSpan" id="kobo.198.1">. </span><span class="koboSpan" id="kobo.198.2">Note that, this agent learned to oscillate around its position; it also learned to pass the force created by its movement to the ball and has learned that the other player can be beaten only by attacking shots. </span></li>
</ol>
</div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The actor-critic algorithm</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">In the policy gradient method, we introduced the baseline to reduce variance, but still, both action and baseline (look closely: the variance is the expected sum of rewards, or in other words, the goodness of the state or its value function) were changing simultaneously. </span><span class="koboSpan" id="kobo.2.2">Wouldn't it be better to separate the policy evaluation from the value evaluation? That's the idea behind the actor-critic method. </span></span><span class="koboSpan" id="kobo.3.1">It consists of two neural networks, one approximating the policy, called the </span><strong><span class="koboSpan" id="kobo.4.1">actor-network</span></strong><span class="koboSpan" id="kobo.5.1">, and the other approximating the value, called the </span><strong><span class="koboSpan" id="kobo.6.1">critic-network</span></strong><span class="koboSpan" id="kobo.7.1">. </span><span class="koboSpan" id="kobo.7.2">We alternate between a policy evaluation and a policy improvement step, resulting in more stable learning. </span><span class="koboSpan" id="kobo.7.3">The critic uses the state and action values to estimate a value function, which is then used to update the actor's policy network parameters so that the overall performance improves. </span><span class="koboSpan" id="kobo.7.4">The following diagram shows the basic architecture of the actor-critic network:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.8.1"><img class="aligncenter size-full wp-image-1099 image-border" src="assets/bdfe8a1a-de28-4c04-998a-e55bd0840078.png" style="width:18.17em;height:21.75em;"/></span></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span class="koboSpan" id="kobo.9.1">Actor-critic architecture</span></div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this chapter, we learned about RL and how it's different from supervised and unsupervised learning. </span><span class="koboSpan" id="kobo.2.2">The emphasis of this chapter was on DRL, where deep neural networks are used to approximate the policy function or the value function or even both. </span><span class="koboSpan" id="kobo.2.3">This chapter introduced OpenAI gym, a library that provides a large number of environments to train RL agents. </span><span class="koboSpan" id="kobo.2.4">We learned about the value-based methods such as Q-learning and used it to train an agent to pick up and drop passengers off in a taxi. </span><span class="koboSpan" id="kobo.2.5">We also used a DQN to train an agent to play a Atari </span><span><span class="koboSpan" id="kobo.3.1">game </span></span><span class="koboSpan" id="kobo.4.1">. </span><span class="koboSpan" id="kobo.4.2">This chapter then moved on to policy-based methods, specifically policy gradients. </span><span class="koboSpan" id="kobo.4.3">We covered the intuition behind policy gradients and used the algorithm to train an RL agent to play Pong.</span></p>
<p><span class="koboSpan" id="kobo.5.1">In the next chapter, we'll explore generative models and learn the secrets behind generative adversarial networks.</span></p>


            </article>

            
        </section>
    </body></html>