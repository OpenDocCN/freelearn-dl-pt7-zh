["```py\npip install pyspark\n```", "```py\nfrom pyspark.ml.regression import LinearRegression as LR\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nfrom pyspark.sql import SparkSession\n```", "```py\nspark = SparkSession.builder \\\n .appName(\"Boston Price Prediction\") \\\n .config(\"spark.executor.memory\", \"70g\") \\\n .config(\"spark.driver.memory\", \"50g\") \\\n .config(\"spark.memory.offHeap.enabled\",True) \\\n .config(\"spark.memory.offHeap.size\",\"16g\") \\\n .getOrCreate()\n```", "```py\nhouse_df = spark.read.format(\"csv\"). \\\n    options(header=\"true\", inferschema=\"true\"). \\\n    load(\"boston/train.csv\")\n```", "```py\nvectors = VectorAssembler(inputCols = ['crim', 'zn','indus','chas',\n    'nox','rm','age','dis', 'rad', 'tax',\n    'ptratio','black', 'lstat'],\n    outputCol = 'features')\nvhouse_df = vectors.transform(house_df)\nvhouse_df = vhouse_df.select(['features', 'medv'])\nvhouse_df.show(5)\n```", "```py\ntrain_df, test_df = vhouse_df.randomSplit([0.7,0.3])\n```", "```py\nregressor = LR(featuresCol = 'features', labelCol='medv',\\\n    maxIter=20, regParam=0.3, elasticNetParam=0.8)\nmodel = regressor.fit(train_df)\n```", "```py\nprint(\"Coefficients:\", model.coefficients)\nprint(\"Intercept:\", model.intercept)\n```", "```py\nmodelSummary = model.summary\nprint(\"RMSE is {} and r2 is {}\"\\ \n   .format(modelSummary.rootMeanSquaredError,\\\n    modelSummary.r2))\nprint(\"Number of Iterations is \",modelSummary.totalIterations)\n```", "```py\nmodel_evaluator = RegressionEvaluator(predictionCol=\"prediction\",\\\n    labelCol=\"medv\", metricName=\"r2\")\nprint(\"R2 value on test dataset is: \",\\\n    model_evaluator.evaluate(model_predictions))\nprint(\"RMSE value is\", model.evaluate(test_df).rootMeanSquaredError)\n```", "```py\nfrom pyspark.ml.classification import LogisticRegression as LR\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Wine Quality Classifier\") \\\n    .config(\"spark.executor.memory\", \"70g\") \\\n    .config(\"spark.driver.memory\", \"50g\") \\\n    .config(\"spark.memory.offHeap.enabled\",True) \\\n    .config(\"spark.memory.offHeap.size\",\"16g\") \\\n    .getOrCreate()\n```", "```py\nwine_df = spark.read.format(\"csv\"). \\\n    options(header=\"true\",\\\n    inferschema=\"true\",sep=';'). \\\n    load(\"winequality-red.csv\")\n```", "```py\nfrom pyspark.sql.functions import when\nwine_df = wine_df.withColumn('quality_new',\\\n    when(wine_df['quality']< 5, 0 ).\\\n    otherwise(when(wine_df['quality']<8,1)\\\n    .otherwise(2)))\n```", "```py\nstring_index = StringIndexer(inputCol='quality_new',\\\n    outputCol='quality'+'Index')\nvectors = VectorAssembler(inputCols = \\\n    ['fixed acidity','volatile acidity',\\\n    'citric acid','residual sugar','chlorides',\\\n    'free sulfur dioxide', 'total sulfur dioxide', \\\n    'density','pH','sulphates', 'alcohol'],\\\n    outputCol = 'features')\n\nstages = [vectors, string_index]\n\npipeline = Pipeline().setStages(stages)\npipelineModel = pipeline.fit(wine_df)\npl_data_df = pipelineModel.transform(wine_df)\n```", "```py\ntrain_df, test_df = pl_data_df.randomSplit([0.7,0.3])\n```", "```py\nclassifier= LR(featuresCol = 'features', \\\n    labelCol='qualityIndex',\\\n    maxIter=50)\nmodel = classifier.fit(train_df)\n```", "```py\nmodelSummary = model.summary\n\naccuracy = modelSummary.accuracy\nfPR = modelSummary.weightedFalsePositiveRate\ntPR = modelSummary.weightedTruePositiveRate\nfMeasure = modelSummary.weightedFMeasure()\nprecision = modelSummary.weightedPrecision\nrecall = modelSummary.weightedRecall\nprint(\"Accuracy: {} False Positive Rate {} \\\n    True Positive Rate {} F {} Precision {} Recall {}\"\\\n    .format(accuracy, fPR, tPR, fMeasure, precision, recall))\n```", "```py\nimport os\nSUBMIT_ARGS = \"--packages databricks:spark-deep-learning:1.3.0-spark2.4-s_2.11 pyspark-shell\"\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n```", "```py\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .appName(\"ImageClassification\") \\\n    .config(\"spark.executor.memory\", \"70g\") \\\n    .config(\"spark.driver.memory\", \"50g\") \\\n    .config(\"spark.memory.offHeap.enabled\",True) \\\n    .config(\"spark.memory.offHeap.size\",\"16g\") \\\n    .getOrCreate()\n```", "```py\nimport pyspark.sql.functions as f\nimport sparkdl as dl\nfrom pyspark.ml.image import ImageSchema\nfrom sparkdl.image import imageIO\ndftulips = ImageSchema.readImages('data/flower_photos/tulips').\\\n    withColumn('label', f.lit(0))\ndfdaisy = ImageSchema.readImages('data/flower_photos/daisy').\\\n    withColumn('label', f.lit(1))\n```", "```py\ntrainDFdaisy, testDFdaisy = dfdaisy.randomSplit([0.70,0.30],\\\n        seed = 123)\ntrainDFtulips, testDFtulips = dftulips.randomSplit([0.70,0.30],\\\n        seed = 122)\ntrainDF = trainDFdaisy.unionAll(trainDFtulips)\ntestDF = testDFdaisy.unionAll(testDFtulips)\n```", "```py\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\n\nvectorizer = dl.DeepImageFeaturizer(inputCol=\"image\",\\\n        outputCol=\"features\", modelName=\"InceptionV3\")\nlogreg = LogisticRegression(maxIter=20, labelCol=\"label\")\npipeline = Pipeline(stages=[vectorizer, logreg])\npipeline_model = pipeline.fit(trainDF)\n```", "```py\npredictDF = pipeline_model.transform(testDF) #predict on test dataset\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator as MCE\nscoring = predictDF.select(\"prediction\", \"label\")\naccuracy_score = MCE(metricName=\"accuracy\")\nrate = accuracy_score.evaluate(scoring)*100\nprint(\"accuracy: {}%\" .format(round(rate,2)))\n```", "```py\npip install h2o\n```", "```py\nimport h2o\nimport time\nimport seaborn\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator as GLM\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator as GBM\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator as RF\n%matplotlib inline\n```", "```py\nboston_df = h2o.import_file(\"../Chapter08/boston/train.csv\", destination_frame=\"boston_df\")\n\nplt.figure(figsize=(20,20))\ncorr = boston_df.cor()\ncorr = corr.as_data_frame()\ncorr.index = boston_df.columns\n#print(corr)\nsns.heatmap(corr, annot=True, cmap='YlGnBu',vmin=-1, vmax=1)\nplt.title(\"Correlation Heatmap\")\n```", "```py\ntrain_df,valid_df,test_df = boston_df.split_frame(ratios=[0.6, 0.2],\\\n         seed=133)\nfeatures =  boston_df.columns[:-1]\n```", "```py\nmodel_glm = GLM(model_id='boston_glm')\nmodel_glm.train(training_frame= train_df,\\\n         validation_frame=valid_df, \\\n         y = 'medv', x=features)\nprint(model_glm)\n```", "```py\ntest_glm = model_glm.model_performance(test_df)\nprint(test_glm)\n```", "```py\n#Gradient Boost Estimator\nmodel_gbm = GBM(model_id='boston_gbm')\nmodel_gbm.train(training_frame= train_df, \\\n        validation_frame=valid_df, \\\n        y = 'medv', x=features)\n\ntest_gbm = model_gbm.model_performance(test_df)\n\n#Random Forest\nmodel_rf = RF(model_id='boston_rf')\nmodel_rf.train(training_frame= train_df,\\\n        validation_frame=valid_df, \\\n        y = 'medv', x=features)\n\ntest_rf = model_rf.model_performance(test_df)\n```", "```py\nfrom h2o.grid.grid_search import H2OGridSearch as Grid\nhyper_params = {'max_depth':[2,4,6,8,10,12,14,16]}\ngrid = Grid(model_gbm, hyper_params, grid_id='depth_grid')\ngrid.train(training_frame= train_df,\\\n        validation_frame=valid_df,\\\n        y = 'medv', x=features)\n```", "```py\nfrom h2o.automl import H2OAutoML as AutoML\naml = AutoML(max_models = 10, max_runtime_secs=100, seed=2)\naml.train(training_frame= train_df, \\\n        validation_frame=valid_df, \\\n        y = 'medv', x=features)\n```", "```py\nimport h2o\nimport time\nimport seaborn\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator as GLM\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator as GBM\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator as RF\n\n%matplotlib inline\n\nh2o.init()\n```", "```py\nwine_df = h2o.import_file(\"../Chapter08/winequality-red.csv\",\\\n        destination_frame=\"wine_df\")    \nfeatures = wine_df.columns[:-1]\nprint(features)\nwine_df['quality'] = (wine_df['quality'] > 7).ifelse(1,0)\nwine_df['quality'] = wine_df['quality'].asfactor()\n```", "```py\ntrain_df,valid_df,test_df = wine_df.split_frame(ratios=[0.6, 0.2],\\\n        seed=133)    \n\nmodel_glm = GLM(model_id='wine_glm', family = 'binomial')\nmodel_glm.train(training_frame= train_df, \\\n        validation_frame=valid_df,\\\n        y = 'quality', x=features)\nprint(model_glm)\n```", "```py\nfrom h2o.automl import H2OAutoML as AutoML\naml = AutoML(max_models = 10, max_runtime_secs=100, seed=2)\naml.train(training_frame= train_df, \\\n        validation_frame=valid_df, \\\n        y = 'quality', x=features)\n```"]