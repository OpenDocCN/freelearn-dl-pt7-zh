<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Machine Learning and Neural Networks 101</h1>
                </header>
            
            <article>
                
<p class="calibre2"><strong class="calibre4"> Artificial intelligence</strong> (<strong class="calibre4">AI</strong>) has captured much of our attention in recent years. From face recognition security systems in our smartphones to booking an Uber ride through Alexa, AI has become ubiquitous in our everyday lives. Still, we are constantly being reminded that the full potential of AI has not yet been realized, and that AI will become an even bigger transformative factor in our lives. </p>
<p class="calibre2">When we look at the horizon, we can see the <span class="calibre5">relentless progression of AI with its promise to better our everyday lives. Powered by AI, self-driving cars are becoming less science fiction, and more of a reality. Self-driving cars aim to reduce traffic accidents by eliminating human error, ultimately improving our lives. </span><span class="calibre5">Similarly, the usage of AI in healthcare promises to improve outcomes. Notably, the UK's National Health Service has announced an ambitious AI project to diagnose early-stage cancer, which can potentially save thousands of lives.</span></p>
<p class="calibre2">The transformative nature of AI has led experts to call it the fourth industrial revolution. AI is the catalyst that will shape modern industries, and having knowledge of AI is essential in this new world. By the end of this book, you will have a better understanding of the algorithms that power AI, and will have developed real-life projects using these cutting-edge algorithms.</p>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre11">
<li class="calibre12">A primer on machine learning and neural networks</li>
<li class="calibre12">Setting up your computer for machine learning</li>
<li class="calibre12">Executing your machine learning projects from start to finish using the machine learning workflow</li>
<li class="calibre12">Creating your own neural network from scratch in Python without using a machine learning library</li>
<li class="calibre12">Using pandas for data analysis in Python</li>
<li class="calibre12">Leveraging machine learning libraries such as Keras to build powerful neural networks</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is machine learning?</h1>
                </header>
            
            <article>
                
<p class="calibre2">Although machine learning and AI are often used interchangeably, there are subtle differences that set them apart. The term AI was first coined in the 1950s, and it refers to the capability of a machine to imitate intelligent human behavior. To that end, researchers and computer scientists have pursued several approaches. Early efforts in AI were centered around an approach known as symbolic AI. Symbolic AI attempts to express human knowledge in a declarative form that computers could process. The height of symbolic AI resulted in the expert system, a computer system that emulated human decision making.</p>
<p class="calibre2">However, one major drawback of symbolic AI is that it relied on the domain knowledge of human experts, and required those rules and knowledge to be hardcoded for problem-solving. AI as a scientific field went through a period of drought (known as the AI winter), when scientists became increasingly disillusioned by the limitations of AI.</p>
<p class="calibre2">While symbolic AI took center stage in the 1950s, a subfield of AI known as machine learning was quietly bubbling in the background. </p>
<p class="calibre2">Machine learning refers to algorithms that computers use to learn from data, allowing it to make predictions on future, unseen data.</p>
<p class="calibre2">However, early AI researchers did not pay much attention to machine learning, as computers back then were neither powerful enough nor had the capability to store the huge amount of data that machine learning algorithms require. As it turns out, machine learning would not be left in the cold for long. In the late 2000s, AI enjoyed a resurgence, with machine learning largely propelling its growth. The key reason for this resurgence was the maturation of computer systems that could collect and store a massive amount of data (big data), along with processors that are fast enough to run the machine learning algorithms. Thus, the AI summer began.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning algorithms</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we have talked about what machine learning is, we need to understand how machine learning algorithms work. Machine learning algorithms can be broadly classified into two categories:</p>
<ul class="calibre11">
<li class="calibre12"><strong class="calibre1">Supervised learning</strong>: Using labeled training data, the algorithm learns the rule for mapping the input variables into the target variable. For example, a supervised learning algorithm learns to predict whether there will be rain (the target variable) from input variables such as the temperature, time, season, atmospheric pressure, and so on.</li>
<li class="calibre12"><strong class="calibre1">Unsupervised learning</strong>: Using unlabeled training data, the algorithm learns associative rules for the data. The most common use case for unsupervised learning algorithms is in clustering analysis, where the algorithm learns hidden patterns and groups in data that are not explicitly labeled.</li>
</ul>
<p class="calibre2">In this book, we will focus on supervised learning algorithms. As a concrete example of a supervised learning algorithm, let's consider the following problem. You are an animal lover and a machine learning enthusiast and you wish to build a machine learning algorithm using supervised learning to predict whether an animal is a friend (a friendly puppy) or a foe (a dangerous bear). For simplicity, let's assume that you have collected two measurements from different breeds of dogs and bears—their <strong class="calibre4">Weight</strong> and their <strong class="calibre4">Speed</strong>. After collecting the data (known as the training dataset), you plot them out on a graph, along with their labels (<strong class="calibre4">Friend or Foe</strong>):</p>
<p class="mce-root"><img class="alignnone1" src="assets/66e4607f-a0dc-445c-8b1c-3b039eb7ac0c.png"/></p>
<p class="calibre2"/>
<p class="CDPAlignLeft1">Immediately, we can see that dogs tend to weigh less, and are generally faster, while bears are heavier and generally slower. If we draw a line (known as a decision boundary) between the dogs and the bears, we can use that line to make future predictions. Whenever we receive the measurements for a new animal, we can just see if it falls to the left or to the right of the line. Friends are to the left, and foes are to the right.</p>
<p class="calibre2">But this is a trivial dataset. What if we collect hundreds of different measurements? Then the graph would be more than 100-dimensional, and it would be impossible for a human being to draw a dividing line. However, such a task is not a problem for machine learning.</p>
<p class="calibre2">In this example, the task of the machine learning algorithm is to learn the optimal decision boundary separating the datasets. Ideally, we want the algorithm to produce a <strong class="calibre4">Decision Boundary</strong> that completely separates the two classes of data (although this is not always possible, depending on the dataset):</p>
<p class="mce-root"><img class="alignnone2" src="assets/bede3862-ca7e-488b-b900-3bce50f65ae2.png"/></p>
<p class="calibre2">With this <strong class="calibre4">Decision Boundary</strong>, we can then make predictions on future, unseen data. If the <strong class="calibre4">New Instance</strong> lies to the left of the <strong class="calibre4">Decision Boundary</strong>, then we classify it as a friend. Vice versa, if the new instance lies to the right of the <strong class="calibre4">Decision Boundary</strong>, then we classify it as a foe.</p>
<p class="calibre2">In this trivial example, we have used only two input variables and two classes. However, we can generalize the problem to include multiple input variables with multiple classes. </p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Naturally, our choice of machine learning algorithm affects the kind of decision boundary produced. Some of the more popular supervised machine learning algorithms are as follows:</p>
<ul class="calibre11">
<li class="calibre12">Neural networks</li>
<li class="calibre12">Linear regression</li>
<li class="calibre12">Logistic regression</li>
<li class="calibre12"><strong class="calibre1">Support vector machines</strong> (<strong class="calibre1">SVMs</strong>)</li>
<li class="calibre12">Decision trees</li>
</ul>
<p class="calibre2">The nature of the dataset (such as an image dataset or a numerical dataset) and the underlying problem that we are trying to solve should dictate the machine learning algorithm used. In this book, we will focus on neural networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The machine learning workflow</h1>
                </header>
            
            <article>
                
<p class="calibre2">We have discussed what machine learning is. But how exactly do you <em class="calibre8">do</em> machine learning? At a high level, machine learning projects are all about taking in raw data as input and churning out <strong class="calibre4">Predictions</strong> as <strong class="calibre4">Output</strong>. To do that, there are several important intermediate steps that must be accomplished. This machine learning workflow can be summarized by the following diagram:</p>
<p class="mce-root"><img class="alignnone3" src="assets/14e0c1b0-4330-4fa7-88c3-80001e91d063.png"/></p>
<p class="calibre2"/>
<p class="calibre2">The <strong class="calibre4">Input</strong> to our machine learning workflow will always be data. Data can come from different sources, with different data formats. For example, if we are working on a computer vision-based project, then our data will likely be images. For most other machine learning projects, the data will be presented in a tabular form, similar to spreadsheets. In some machine learning projects, data collection will be a significant first step. In this book, we will assume that the data will be provided to us, allowing us to focus on the machine learning aspect.</p>
<p class="calibre2">The next step is to preprocess the data. Raw data is often messy, error-prone, and unsuitable for machine learning algorithms. Hence, we need to preprocess the data before we feed it to our models. In cases where data is provided from multiple sources, we need to merge the data into a single dataset. Machine learning models also require a numeric dataset for training purposes. If there are any categorical variables in the raw dataset (that is, gender, country, day of week, and so on), we need to encode those variables as numeric variables. We will see how we can do so later on in the chapter. Data scaling and normalization is also required for certain machine learning algorithms. The intuition behind this is that if the magnitude of certain variables is much greater than other variables, then certain machine learning algorithms will <span class="calibre5">mistakenly</span><span class="calibre5"> place more emphasis on those dominating variables.</span></p>
<p class="calibre2">Real-world datasets are often messy. You will find that the data is incomplete and contains missing data in several rows and columns. There are several ways to deal with missing data, each with its own advantages and disadvantages. The easiest way is to simply discard rows and columns with missing data. However, this may not be practical, as we may end up discarding a significant percentage of our data. We can also replace the missing variables with the mean of the variables (if the variables happen to be numeric). This approach is more ideal than discarding data, as it preserves our dataset. However, replacing missing values with the mean tends to affect the distribution of the data, which may negatively impact our machine learning models. One other method is to predict what the missing values are, based on other values that are present. However, we have to be careful as doing this may introduce significant bias into our dataset. </p>
<p class="calibre2">Lastly, in <strong class="calibre4">Data Preprocessing</strong>, we need to split the dataset into a training and testing dataset. Our machine learning models will be trained and fitted only on the training set. Once we are satisfied with the performance of our model, we will then evaluate our model using the testing dataset. Note that our model should never be trained on the testing set. This ensures that the evaluation of model performance is unbiased, and will reflect its real-world performance.</p>
<p class="calibre2"/>
<p class="calibre2">Once <strong class="calibre4">Data Preprocessing</strong> has been completed, we will move on to <strong class="calibre4">Exploratory Data Analysis</strong> (<strong class="calibre4">EDA</strong>). EDA is the process of uncovering insights from your data using data visualization. EDA allows us to construct new features (known as feature engineering) and inject domain knowledge into our machine learning models. </p>
<p class="calibre2">Finally, we get to the heart of machine learning. After <strong class="calibre4">Data Preprocessing</strong> and EDA have been completed, we move on to <strong class="calibre4">Model Building</strong>. As mentioned in the earlier section, there are several machine learning algorithms at our disposal, and the nature of the problem should dictate the type of machine learning algorithm used. In this book, we will focus on neural networks. In <strong class="calibre4">Model Building</strong>, <strong class="calibre4">Hyperparameter Tuning</strong> is an essential step, and the right <span class="calibre5">hyperparameters can drastically improve the performance of our model. In a later section, we will look at some of the hyperparameters in a neural network. Once the model has been trained, we are finally ready to evaluate our model using the testing set.</span></p>
<p class="calibre2">As we can see, the machine learning workflow consists of many intermediate steps, each of which are crucial to the overall performance of our model. The major advantage of using Python for machine learning is that the entire machine learning workflow can be executed end-to-end entirely in Python, using just a handful of open source libraries. In this book, you will gain experience using Python in each step of the machine learning workflow, as you create sophisticated neural network projects from scratch. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up your computer for machine learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">Before we dive deeper into neural networks and machine learning, let's make sure that you have set up your computer properly, so that you can run the code in this book smoothly.</p>
<p class="calibre2">In this book, we will use the Python programming language for each neural network project. Along with Python itself, we also require several Python libraries, such as Keras, pandas, NumPy, and many more. There are several ways to install Python and the required libraries, but the easiest way by far is to use Anaconda.</p>
<p class="calibre2">Anaconda is a free and open source distribution of Python and its libraries. Anaconda provides a handy package manager that allows us to easily install Python and all other libraries that we require. To install Anaconda, simply head to the website at <a href="https://www.anaconda.com/distribution/" target="_blank" class="calibre10">https://www.anaconda.com/distribution/</a> and download the Anaconda installer (select the Python 3.x installer).</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Besides Anaconda, we also require Git. Git is essential for machine learning and software engineering in general. Git allows us to easily download code from GitHub, which is probably the most widely used software hosting service. To install Git, head to the Git website at <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git" class="calibre10">https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</a>. You can simply download and run the appropriate installer for your OS.</p>
<p class="calibre2">Once Anaconda and Git are installed, we are ready to download the code for this book. The code that you see in this book can be found in our accompanying GitHub repository.</p>
<p class="calibre2">To download the code, simply run the following command from a command line (use Terminal if you're using macOS/Linux, and if you're using Windows, use the Anaconda Command Prompt):</p>
<pre class="calibre17"><strong class="calibre1">$ git clone https://github.com/PacktPublishing/Neural-Network-Projects-with-Python</strong></pre>
<p class="calibre2">The <kbd class="calibre13">git clone</kbd> command will download all the Python code in this book to your computer.</p>
<p class="calibre2">Once that's done, run the following command to move into the folder that you just downloaded:</p>
<pre class="calibre17"><strong class="calibre1">$ cd Neural-Network-Projects-with-Python</strong></pre>
<p class="calibre2">Within the folder, you will find a file titled <kbd class="calibre13">environment.yml</kbd>. With this file, we can install Python and all the required libraries into a virtual environment. You can think of a virtual environment as an isolated, sandboxed environment where we can install a fresh copy of Python and all the required libraries. The <kbd class="calibre13">environment.yml</kbd> file contains instructions for Anaconda to install a specific version of each library into a virtual environment. This ensures that the Python code will be executed in a standardized environment that we have designed.</p>
<p class="calibre2">To install the required dependencies using Anaconda and the <span class="calibre5"><kbd class="calibre13">environment.yml</kbd> file</span>, simply execute the following command from a command line:</p>
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre17"><strong class="calibre1"><span>$ conda</span> <span>env</span> <span>create</span> <span>-</span><span>f</span> <span>environment</span><span>.</span><span>yml</span></strong></pre></div>
</div>
<p class="calibre2">Just like that, Anaconda will install all required packages into a <kbd class="calibre13"><span>neural-network-projects-python</span></kbd> virtual environment. To enter this virtual environment, we execute this next command:</p>
<pre class="calibre17"><strong class="calibre1">$ conda activate neural-network-projects-python</strong></pre>
<p class="calibre2">That's it! We are now in a virtual environment with all dependencies installed. To execute a Python file in this virtual environment, we can run something like this:</p>
<pre class="calibre17"><strong class="calibre1">$ python Chapter01\keras_chapter1.py</strong></pre>
<p class="calibre2">To leave the virtual environment, we can run the following command:</p>
<pre class="calibre17"><strong class="calibre1">$ conda deactivate</strong></pre>
<p class="calibre2">Just note that you should be within the virtual environment (by running <kbd class="calibre13">conda activate neural-network-projects-python</kbd><span class="calibre5"> first) </span>whenever you run any Python code provided by us.</p>
<p class="calibre2">Now that we've set up our computer, let's return back to neural networks. We'll look at the theory behind neural networks, and how to program one from scratch in Python. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural networks</h1>
                </header>
            
            <article>
                
<p class="calibre2">Neural networks are a class of machine learning algorithms that are loosely inspired by neurons in the human brain. However, without delving too much into brain analogies, I find it easier to simply describe neural networks as a mathematical function that maps a given input to the desired output. To understand what that means, let's take a look at a single layer neural network (known as a perceptron).</p>
<p class="calibre2"> A <strong class="calibre4">Perceptron</strong> can be illustrated with the following diagram:</p>
<p class="mce-root"><img class="alignnone4" src="assets/c2e5d068-6510-410a-8d7d-4d5f4759683e.png"/></p>
<p class="calibre2">At its core, the <strong class="calibre4">Perceptron</strong> is simply a mathematical function that takes in a set of inputs, performs some mathematical computation, and outputs the result of the computation. In this case, that mathematical function is simply this:</p>
<p class="mce-root"><img class="fm-editor-equation" src="assets/bdb4a2b7-f8d8-4120-bdeb-3f745715d1ca.png"/></p>
<p class="calibre2"><img class="fm-editor-equation1" src="assets/50c8a031-2d2f-42a6-965d-378cae83b9ce.png"/> refers to the weights of the <strong class="calibre4">Perceptron</strong>. We will explain what the weights in a neural network refers to in the next few sections. For now, we just need to keep in mind that neural networks are simply mathematical functions that map a given input to a desired output.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why neural networks?</h1>
                </header>
            
            <article>
                
<p class="calibre2">Before we dive into creating our own neural network, it is worth understanding why neural networks have gained such an important foothold in machine learning and AI. </p>
<p class="calibre2">The first reason is that neural networks are universal function approximators. What that means is that given any arbitrary function that we are trying to model, no matter how complex, neural networks are always<strong class="calibre4"> </strong>able to represent that function. This has a profound implication on neural networks and AI in general. Assuming that any problem in the world can be described by a mathematical function (no matter how complex), we can use neural networks to represent that function, effectively modeling anything in the world. A caveat to this is that while scientists have proved the universality of neural networks, a large and complex neural network may never be trained and generalized correctly.</p>
<p class="calibre2">The second reason is that the architecture of neural networks are highly scalable and flexible. As we will see in the next section, we can easily stack layers in each neural network, increasing the complexity of the neural network. Perhaps more interestingly, the capabilities of neural networks are only limited by our own imagination. Through creative neural network architecture design, machine learning engineers have learned how to use neural networks to predict time series data (known as <strong class="calibre4">recurrent neural networks</strong> (<strong class="calibre4">RNNs</strong>)), which are used in areas such as speech recognition. In recent years, scientists have also shown that by pitting two neural networks against each other in a contest (known as a <strong class="calibre4">generative adversarial network</strong> (<strong class="calibre4">GAN</strong>)), we can generate photorealistic images that are indistinguishable to the human eye. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The basic architecture of neural networks</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">In this section, we will look at the basic architecture of neural networks, the building blocks on which all complex neural networks are based. We will also code up our own basic neural network from scratch in Python, without any machine learning libraries. This exercise will help you gain an intuitive understanding of the inner workings of neural networks.</span></p>
<p class="calibre2"><span class="calibre5">Neural networks consist of the following components:</span></p>
<ul class="calibre11">
<li class="calibre12"><span>An<span> i</span></span>nput layer, <em class="calibre18">x</em></li>
<li class="calibre12"><span>An<span> </span></span><span>arbitrary</span><span><span> </span>amount of</span> hidden layers</li>
<li class="calibre12"><span>An<span> </span></span>output layer, <em class="calibre18">ŷ</em></li>
<li class="calibre12"><span>A set of<span> </span></span>weights and biases between each layer, <em class="calibre18">W</em> and <em class="calibre18">b</em></li>
<li class="calibre12"><span>A choice of<span> </span></span>activation function<span><span> </span>for each hidden layer</span><span>,<span> </span><strong class="calibre1"><em class="calibre18">σ</em></strong></span></li>
</ul>
<p class="calibre2"><span class="calibre5">The<span class="calibre5"> following </span></span><span class="calibre5">diagram shows the architecture of a two-layer neural network (</span>note that the input layer is typically excluded when counting the number of layers in a neural network<span class="calibre5">):</span></p>
<p class="mce-root"><img class="alignnone5" src="assets/979cbfd4-9395-480d-9f30-6eb520569a9d.png"/></p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a neural network from scratch in Python</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we understand the basic architecture of a neural network, let's create our own neural network from scratch in Python.</p>
<p class="calibre2">First, let's create a <kbd class="calibre13">NeuralNetwork</kbd> class in Python:</p>
<pre class="calibre17">import numpy as np<br class="title-page-name"/><br class="title-page-name"/>class NeuralNetwork:<br class="title-page-name"/>    def __init__(self, x, y):<br class="title-page-name"/>        self.input    = x<br class="title-page-name"/>        self.weights1 = <span>np.random.rand(</span><span>self</span><span>.input.shape[</span><span>1</span><span>],</span><span>4</span><span>) </span><br class="title-page-name"/>        <span>self</span><span>.weights2 </span><span>=</span><span> np.random.rand(</span><span>4</span><span>,</span><span>1</span><span>) </span><br class="title-page-name"/>        <span>self</span><span>.y        </span><span>=</span><span> y</span><br class="title-page-name"/>        <span>self</span><span>.output </span><span>=</span><span> np.zeros(self.y.shape)</span></pre>
<div class="packtinfobox">Notice that in the preceding code, we initialize the weights (<kbd class="calibre19">self.weights1</kbd> and <kbd class="calibre19">self.weights2</kbd>) as a NumPy array with random values. NumPy arrays are used to represent multidimensional arrays in Python. The exact dimensions of our weights are specified in the parameters of the <kbd class="calibre19">np.random.rand()</kbd> function. For the dimensions of the first weight array, we use a variable (<kbd class="calibre19">self.input.shape[1]</kbd>) to create an array of variable dimensions, depending on the size of our input.</div>
<p class="calibre2">The output,<span class="calibre5"> </span><em class="calibre8">ŷ</em>,<strong class="calibre4"><em class="calibre8"><span class="calibre5"> </span></em></strong>of a simple two-la<span class="calibre5">yer neural n</span>etwork is as follows:</p>
<p class="mce-root"><img class="fm-editor-equation2" src="assets/c2f686f1-a94d-4f30-be9e-d29480326b24.png"/></p>
<p class="calibre2">You might notice that in the preceding equation, the weights, <em class="calibre8">W</em>,<strong class="calibre4"><em class="calibre8"> </em></strong>and the biases, <em class="calibre8">b</em>,<strong class="calibre4"><em class="calibre8"> </em></strong>are the only variables that affects the output, <em class="calibre8">ŷ</em>.</p>
<p class="calibre2"><span class="calibre5">Na</span><span class="calibre5">turally, the right values for the weights and biases<span class="calibre5"> </span></span><span class="calibre5">determine</span><span class="calibre5"><span class="calibre5"> </span>the strength of the predictions.</span><span class="calibre5"><span class="calibre5"> </span>The process of fine-tuning the weights and biases from the input data is known as<span class="calibre5"> </span></span>training the neural network.</p>
<p class="calibre2">Each iteration of the training process<span class="calibre5"> </span><span class="calibre5">consists</span><span class="calibre5"> </span>of the following steps:</p>
<ol class="calibre14">
<li class="calibre12">Calculating the predicted output <strong class="calibre1">ŷ</strong>,<strong class="calibre1"><em class="calibre18"> </em></strong>known as <strong class="calibre1">Feedforward</strong></li>
<li class="calibre12">Updating the weights and biases, known as <strong class="calibre1">Backpropagation</strong></li>
</ol>
<p class="calibre2">The following sequential graph illustrates the process:</p>
<p class="mce-root"><img class="alignnone6" src="assets/e05188ed-5f65-4c56-9934-cb58361d5f3e.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feedforward</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we've seen in the preceding sequential graph, feedforward is just simple calculus, and<span class="calibre5"><span class="calibre5"> </span></span>for a basic two-layer neural network, the output of the neural network is as follows:</p>
<p class="mce-root"><img class="fm-editor-equation2" src="assets/5a5455e6-935d-4c34-82a5-102f26b78c2f.png"/></p>
<p class="calibre2">Let's add a <kbd class="calibre13">feedforward</kbd> function in our Python code to do exactly that. Note that for simplicity, we have assumed the biases to be <kbd class="calibre13">0</kbd>:</p>
<pre class="calibre17">import numpy as np<br class="title-page-name"/><br class="title-page-name"/>def sigmoid(x):<br class="title-page-name"/>    return 1.0/(1 + np.exp(-x))<br class="title-page-name"/><br class="title-page-name"/>class NeuralNetwork:<br class="title-page-name"/>    def __init__(self, x, y):<br class="title-page-name"/>        self.input    = x<br class="title-page-name"/>        self.weights1 = <span>np.random.rand(</span><span>self</span><span>.input.shape[</span><span>1</span><span>],</span><span>4</span><span>) </span><br class="title-page-name"/>        <span>self</span><span>.weights2 </span><span>=</span><span> np.random.rand(</span><span>4</span><span>,</span><span>1</span><span>) </span><br class="title-page-name"/>        <span>self</span><span>.y        </span><span>=</span><span> y</span><br class="title-page-name"/>        <span>self</span><span>.output   </span><span>=</span><span> np.zeros(self.y.shape)<br class="title-page-name"/><br class="title-page-name"/>    def feedforward(self):<br class="title-page-name"/>        self.layer1 = sigmoid(np.dot(self.input, self.weights1))<br class="title-page-name"/>        self.output = sigmoid(np.dot(self.layer1, self.weights2))<br class="title-page-name"/></span></pre>
<p class="calibre2">However, we still need a way to evaluate the accuracy of our predictions (that is, how far off <span class="calibre5">our predictions </span>are). The <kbd class="calibre13">loss</kbd> function allows us to do exactly that.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The loss function</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">There are many available <kbd class="calibre13">loss</kbd> functions,<span class="calibre5"> </span></span><span class="calibre5">and the nature of our problem should dictate our choice of <kbd class="calibre13">loss</kbd> function.</span><span class="calibre5"><span class="calibre5"> For now</span></span><span class="calibre5">, we'll use a simple <em class="calibre8">S</em></span><em class="calibre8">um-of-Squares Error</em><span class="calibre5"><span class="calibre5"> </span>as our <kbd class="calibre13">loss</kbd> function:</span></p>
<p class="mce-root"><img class="fm-editor-equation3" src="assets/7e86faa7-73f3-4d34-9bf9-3c47dad08b5d.png"/></p>
<p class="calibre2"><span class="calibre5">The <em class="calibre8">sum-of-squares error</em> is simply the sum of the difference between each predicted value and<span class="calibre5"> </span></span><span class="calibre5">the</span><span class="calibre5"><span class="calibre5"> </span>actual value.<span class="calibre5"> </span></span><span class="calibre5">The difference is squared so that we measure the absolute value of the</span><span class="calibre5"><span class="calibre5"> </span></span><span class="calibre5">difference</span><span class="calibre5">.</span></p>
<p class="calibre2">Our goal in training is to find the best set of weights and biases that minimizes the <kbd class="calibre13">loss</kbd> function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Backpropagation</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">Now that we've measured the error of our prediction (loss), we need to find a way to<span class="calibre5"> </span></span>propagate<span class="calibre5"><span class="calibre5"> </span>the error back, and to update our weights and biases.</span></p>
<p class="calibre2"><span class="calibre5">In order to know the<span class="calibre5"> </span></span><span class="calibre5">appropriate</span><span class="calibre5"><span class="calibre5"> </span>amount to adjust the weights and biases by, we need to know the<span class="calibre5"> </span></span>derivative of the <kbd class="calibre13">loss</kbd> function with respect to the weights and biases<span class="calibre5">.</span></p>
<p class="calibre2">Recall from calculus that the derivative of a function is simply the slope of the function:</p>
<p class="mce-root"><img class="alignnone7" src="assets/603d354a-7d0f-414f-8436-0f6b0f800c06.png"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"><span class="calibre5">If we have the derivative, we can simply update the weights and biases by increasing/reducing with it (refer to the preceding diagram). This is known as </span><strong class="calibre4">gradient descent</strong><span class="calibre5">.</span></p>
<p class="calibre2"><span class="calibre5">However, we can't directly c</span><span class="calibre5">alculate the derivative of the <kbd class="calibre13">loss</kbd> function with respect to the weights and biases</span><span class="calibre5"> because the equation of the <kbd class="calibre13">loss</kbd> function does not contain the </span><span class="calibre5">weights</span><span class="calibre5"> and biases. </span><span class="calibre5">We need the<span class="calibre5"> </span></span><span class="calibre5">chain rule<strong class="calibre4"> </strong></span><span class="calibre5">to help us calculate it. At this point, we are not going to delve into the chain rule because the math behind it can be rather complicated. Furthermore, machine learning libraries such as Keras takes care of gradient descent for us without requiring us to work out the chain rule from scratch. The key idea that we need to know is that once we have the derivative (slope) of the <kbd class="calibre13">loss</kbd> function with respect to the weights, we can adjust the weights accordingly.</span></p>
<p class="calibre2">Now let's add the <kbd class="calibre13">backprop</kbd> function into our Python code:</p>
<pre class="calibre17">import numpy as np<br class="title-page-name"/><br class="title-page-name"/>def sigmoid(x):<br class="title-page-name"/>    return 1.0/(1 + np.exp(-x))<br class="title-page-name"/><br class="title-page-name"/><span>def</span><span> </span><span>sigmoid_derivative</span><span>(</span><span>x</span><span>):</span><br class="title-page-name"/>   <span>return</span><span> x </span><span>*</span><span> (</span><span>1.0</span><span> </span><span>-</span><span> x)</span><br class="title-page-name"/><br class="title-page-name"/>class NeuralNetwork:<br class="title-page-name"/>    def __init__(self, x, y):<br class="title-page-name"/>        self.input    = x<br class="title-page-name"/>        self.weights1 = <span>np.random.rand(</span><span>self</span><span>.input.shape[</span><span>1</span><span>],</span><span>4</span><span>) </span><br class="title-page-name"/>        <span>self</span><span>.weights2 </span><span>=</span><span> np.random.rand(</span><span>4</span><span>,</span><span>1</span><span>) </span><br class="title-page-name"/>        <span>self</span><span>.y        </span><span>=</span><span> y</span><br class="title-page-name"/>        <span>self</span><span>.output </span><span>=</span><span> np.zeros(self.y.shape)</span><span><br class="title-page-name"/><br class="title-page-name"/>    def feedforward(self):<br class="title-page-name"/>        self.layer1 = sigmoid(np.dot(self.input, self.weights1))<br class="title-page-name"/>        self.output = sigmoid(np.dot(self.layer1, self.weights2))<br class="title-page-name"/><br class="title-page-name"/>    def backprop(self):<br class="title-page-name"/>        # application of the chain rule to find the derivation of the <br class="title-page-name"/>        # loss function with respect to weights2 and weights1<br class="title-page-name"/>        d_weights2 <span>=</span> np.dot(<span>self</span>.layer1.T, (<span>2</span><span>*</span>(<span>self</span>.y <span>-</span> <span>self</span>.output) <span>*                                                                          <br class="title-page-name"/></span>                     sigmoid_derivative(<span>self</span>.output)))       <br class="title-page-name"/>        d_weights1 <span>=</span> np.dot(<span>self</span>.input.T, (np.dot(<span>2</span><span>*</span>(<span>self</span>.y <span>-</span> <span>self</span>.output) <br class="title-page-name"/><span>                    *</span> sigmoid_derivative(<span>self</span>.output), <span>self</span>.weights2.T) <span>*                                                <br class="title-page-name"/></span>                      sigmoid_derivative(<span>self</span>.layer1))) <br class="title-page-name"/><br class="title-page-name"/>        <span>self</span>.weights1 <span>+=</span> d_weights1<br class="title-page-name"/>        <span>self</span>.weights2 <span>+=</span> d_weights2<br class="title-page-name"/><br class="title-page-name"/>if __name__ == "__main__":<br class="title-page-name"/>    X = np.array([[0,0,1],<br class="title-page-name"/>                  [0,1,1],<br class="title-page-name"/>                  [1,0,1],<br class="title-page-name"/>                  [1,1,1]])<br class="title-page-name"/>    y = np.array([[0],[1],[1],[0]])<br class="title-page-name"/>    nn = NeuralNetwork(X,y)<br class="title-page-name"/><br class="title-page-name"/>    for i in range(1500):<br class="title-page-name"/>        nn.feedforward()<br class="title-page-name"/>        nn.backprop()<br class="title-page-name"/><br class="title-page-name"/>    print(nn.output)</span></pre>
<div class="packtinfobox">Notice that in the preceding code, we used a <kbd class="calibre19">sigmoid</kbd> function in the feedforward function. The <kbd class="calibre19">sigmoid</kbd> function is an activation function to <em class="calibre18">squash</em> the values between <kbd class="calibre19">0</kbd> and <kbd class="calibre19">1</kbd>. This is important because we need our predictions to be between <kbd class="calibre19">0</kbd> and <kbd class="calibre19">1</kbd> for this binary prediction problem. We will go through the <kbd class="calibre19">sigmoid</kbd> activation function in greater detail in the next chapter, <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre20">Chapter 2</a>, <em class="calibre18">Predicting Diabetes with Multilayer Perceptrons</em>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Putting it all together</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we have our complete Python code for doing feedforward and backpropagation, let's apply our neural network on an example and see how well it does. </p>
<p class="calibre2">The following table contains four data points, each with three input variables ( <em class="calibre8">x<sub class="calibre21">1</sub></em>, <em class="calibre8">x<sub class="calibre21">2</sub></em>, and <em class="calibre8">x<sub class="calibre21">3</sub></em>) and a target variable (<em class="calibre8">Y</em>):</p>
<table border="1" class="calibre22">
<tbody class="calibre23">
<tr class="calibre24">
<td class="CDPAlignCenter"><strong class="calibre1">x<sub class="calibre21">1</sub></strong></td>
<td class="CDPAlignCenter"><strong class="calibre1">x<sub class="calibre21">2</sub></strong></td>
<td class="CDPAlignCenter1"><strong class="calibre1">x<sub class="calibre21">3</sub></strong></td>
<td class="CDPAlignCenter2"><strong class="calibre1">Y</strong></td>
</tr>
<tr class="calibre24">
<td class="CDPAlignCenter">0</td>
<td class="CDPAlignCenter">0</td>
<td class="CDPAlignCenter1">1</td>
<td class="CDPAlignCenter2">0</td>
</tr>
<tr class="calibre24">
<td class="CDPAlignCenter">0</td>
<td class="CDPAlignCenter">1</td>
<td class="CDPAlignCenter1">1</td>
<td class="CDPAlignCenter2">1</td>
</tr>
<tr class="calibre24">
<td class="CDPAlignCenter">1</td>
<td class="CDPAlignCenter">0</td>
<td class="CDPAlignCenter1">1</td>
<td class="CDPAlignCenter2">1</td>
</tr>
<tr class="calibre24">
<td class="CDPAlignCenter">1</td>
<td class="CDPAlignCenter">1</td>
<td class="CDPAlignCenter1">1</td>
<td class="CDPAlignCenter2">0</td>
</tr>
</tbody>
</table>
<p class="calibre2"> </p>
<p class="calibre2">Our neural network should learn the ideal set of weights to represent this function. Note that it isn't exactly trivial for us to work out the weights just by inspection alone.</p>
<p class="calibre2"><span class="calibre5">Let's train the neural network for 1,500 iterations and see what happens. Looking at the following loss-per-iteration graph, we can clearly see the loss </span>monotonically decreasing toward a minimum. Thi<span class="calibre5">s is consistent with the gradient descent algorithm that we discussed earlier:</span></p>
<p class="mce-root"><img class="alignnone8" src="assets/2237ea9f-d19e-4b91-a4cc-8315e58c47d0.png"/></p>
<p class="calibre2">Let's look at the final prediction (output) from the neural network after 1,500 iterations:</p>
<table border="1" class="calibre22">
<tbody class="calibre23">
<tr class="calibre24">
<td class="CDPAlignCenter3"><strong class="calibre1">Prediction</strong></td>
<td class="CDPAlignCenter3"><strong class="calibre1">Y (Actual)</strong></td>
</tr>
<tr class="calibre24">
<td class="CDPAlignCenter3">0.023</td>
<td class="CDPAlignCenter3">0</td>
</tr>
<tr class="calibre24">
<td class="CDPAlignCenter3">0.979</td>
<td class="CDPAlignCenter3">1</td>
</tr>
<tr class="calibre24">
<td class="CDPAlignCenter3">0.975</td>
<td class="CDPAlignCenter3">1</td>
</tr>
<tr class="calibre24">
<td class="CDPAlignCenter3">0.025</td>
<td class="CDPAlignCenter3">0</td>
</tr>
</tbody>
</table>
<p class="calibre2"/>
<p class="calibre2">We did it! Our feedforward and backpropagation algorithm trained the neural network successfully and the predictions converged on the true values.</p>
<p class="calibre2">Note that there's a slight difference between the predictions and the actual values. This is desirable, as it prevents overfitting and allows the neural network to generalize<strong class="calibre4"> </strong>better to unseen data.</p>
<p class="calibre2">Now that we understand the inner workings of a neural network, we will introduce the machine learning libraries in Python that we will use for the rest of the book. Don't worry if you find it difficult to create your own neural network from scratch at this point. For the rest of the book, we'll be using libraries that will greatly simplify the process of building and training a neural network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning and neural networks</h1>
                </header>
            
            <article>
                
<p class="calibre2">What about deep learning? How is it different from neural networks? To put it simply, deep learning is a machine learning algorithm that uses multiple layers in a neural network for learning (also known as deep nets). While we can think of a single-layer perceptron as the simplest neural network, deep nets are simply neural networks on the opposite end of the complexity spectrum.</p>
<p class="calibre2">In a <strong class="calibre4">deep neural network</strong> (<strong class="calibre4">DNN</strong>), each layer learns information of increasing complexity, before passing it to successive layers. For example, when a DNN is trained for the purpose of facial recognition, the first few layers learn to identify edges in faces, followed by contours such as eyes and eventually complete facial features.</p>
<p class="calibre2">Although perceptrons were introduced back in the 1950s, deep learning did not take off until a few years ago. A key reason for the relatively slow progress of deep learning in the past few centuries is largely due to a lack of data and a lack of computation power. In the past few years, however, we have witnessed deep learning driving key innovations in machine learning and AI. Today, deep learning is the algorithm of choice when it comes to image recognition, autonomous vehicles, speech recognition, and game playing. So, what changed over the last few years?</p>
<p class="calibre2">In recent years, computer storage has become affordable enough to collect and store the massive amount of data that deep learning requires. It is becoming increasingly affordable to keep massive amount of data in the cloud, where it can be accessed by a cluster of computers from anywhere on earth. With the affordability of data storage, data is also becoming democratized. For example, websites such as ImageNet provides 14 million different images for deep learning researchers. Data is no longer a commodity that is owned by a privileged few.</p>
<p class="calibre2">The computational power that deep learning requires is also becoming more affordable and powerful. Most of deep learning today is powered by <strong class="calibre4">graphics processing units</strong> (<strong class="calibre4">GPUs</strong>), which excel in the computation required by DNNs. Keeping with the theme of democratization, many websites also provides free GPU processing power for deep learning <span class="calibre5">enthusiasts</span>. For example, Google Colab provides a free Tesla K80 GPU in the cloud for deep learning, available for anyone to use.</p>
<p class="calibre2">With these recent advancements, deep learning is becoming available to everyone. In the next few sections, we will introduce the Python libraries that we will use for deep learning.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">pandas – a powerful data analysis toolkit in Python</h1>
                </header>
            
            <article>
                
<p class="calibre2">pandas is perhaps the most ubiquitous library in Python for data analysis. Built upon the powerful NumPy library, pandas provides a fast and flexible data structure in Python for handling real-world datasets. Raw data is often presented in tabular form, shared using the <kbd class="calibre13">.csv</kbd> file format. pandas provides a simple interface for importing these <kbd class="calibre13">.csv</kbd> files into a data structure known as DataFrames that makes it extremely easy to manipulate data in Python.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">pandas DataFrames</h1>
                </header>
            
            <article>
                
<p class="calibre2">pandas DataFrames are two-dimensional data structures, which you can think of as spreadsheets in Excel. DataFrames allow us to easily import the <kbd class="calibre13">.csv</kbd> files using a simple command. For example, the following sample code allows us to import the <kbd class="calibre13">raw_data.csv</kbd> file:</p>
<pre class="calibre17">import pandas as pd<br class="title-page-name"/>df = pd.read_csv("raw_data.csv")</pre>
<p class="calibre2">Once the data is imported as a DataFrame, we can easily perform data preprocessing on it. Let's work through it using the Iris flower dataset. The Iris flower dataset is a commonly used dataset that contains data on the measurements (sepal length and width, petal length and width) of several classes of flowers. First, let's import the dataset as provided for free by <strong class="calibre4">University of California Irvine</strong> (<strong class="calibre4">UCI</strong>). Notice that pandas is able to import a dataset directly from a URL:</p>
<pre class="calibre17">URL = \<br class="title-page-name"/>    'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'<br class="title-page-name"/>df = pd.read_csv(URL, names = ['sepal_length', 'sepal_width', <br class="title-page-name"/>                               'petal_length', 'petal_width', 'class'])</pre>
<p class="calibre2">Now that it's in a DataFrame, we can easily manipulate the data. First, let's get a summary of the data as it is always important to know what kind of data we're working with:</p>
<pre class="calibre17">print(df.info())</pre>
<p class="calibre2">The output will be as shown in the following screenshot:</p>
<p class="mce-root"><img class="alignnone9" src="assets/8594b08a-140e-4ab0-b189-fe08c6d0007f.png"/></p>
<p class="calibre2">It looks like there are 150 rows in the dataset, with four numeric columns containing information regarding the <kbd class="calibre13">sepal_length</kbd> and <kbd class="calibre13">sepal_width</kbd>, along with the <kbd class="calibre13">petal_length</kbd> and <kbd class="calibre13">petal_width</kbd>. There is also one non-numeric column containing information regarding the class (that is, species) of the flowers.</p>
<p class="calibre2">We can get a quick statistical summary of the four numeric columns by calling the <kbd class="calibre13">describe()</kbd> function:</p>
<pre class="calibre17">print(df.describe())</pre>
<p class="calibre2">The output is shown in the following screenshot:</p>
<p class="mce-root"><img class="alignnone10" src="assets/d55aa987-bbc2-43bd-8c6c-ab9420cee49a.png"/></p>
<p class="calibre2">Next, let's take a look at the first 10 rows of the data:</p>
<pre class="calibre17">print(df.head(10))</pre>
<p class="calibre2">The <span class="calibre5">output is shown in the following screenshot:</span></p>
<p class="mce-root"><img class="alignnone11" src="assets/21b44887-cd45-48e2-9a46-ddc688f77d40.png"/></p>
<p class="calibre2">Simple, isn't it? pandas also allows us to perform data wrangling easily. For example, we can do the following to filter and select rows with <kbd class="calibre13">sepal_length</kbd> greater than <kbd class="calibre13">5.0</kbd>:</p>
<pre class="calibre17">df2 = df.loc[df['sepal_length'] &gt; 5.0, ]</pre>
<p class="calibre2"><span class="calibre5">The output is shown in the following screenshot:</span></p>
<p class="mce-root"><img class="alignnone12" src="assets/c819132e-6363-45c8-853e-e1e32062e3a9.png"/></p>
<p class="calibre2">The <kbd class="calibre13">loc</kbd> command allows us to access a group of rows and columns.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data visualization in pandas</h1>
                </header>
            
            <article>
                
<p class="calibre2">EDA is perhaps one of the most important steps in the machine learning workflow, and pandas makes it extremely easy to visualize data in Python. pandas provides a high-level API for the popular <kbd class="calibre13">matplotlib</kbd> library, which makes it easy to construct plots directly from DataFrames.</p>
<p class="calibre2">As an example, let's visualize the Iris dataset using pandas to uncover important insights. Let's plot a scatterplot to visualize how <kbd class="calibre13">sepal_width</kbd> is related to <kbd class="calibre13">sepal_length</kbd>. We can construct a scatterplot easily using the <kbd class="calibre13">DataFrame.plot.scatter()</kbd> method, which is built into all DataFrames:</p>
<pre class="calibre17"># Define marker shapes by class<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>marker_shapes = ['.', '^', '*']<br class="title-page-name"/><br class="title-page-name"/># Then, plot the scatterplot<br class="title-page-name"/>ax = plt.axes()<br class="title-page-name"/>for i, species in enumerate(df['class'].unique()):<br class="title-page-name"/>    species_data = df[df['class'] == species]<br class="title-page-name"/>    species_data.plot.scatter(x='sepal_length',<br class="title-page-name"/>                              y='sepal_width', <br class="title-page-name"/>                              marker=marker_shapes[i],<br class="title-page-name"/>                              s=100,<br class="title-page-name"/>                              title="Sepal Width vs Length by Species", <br class="title-page-name"/>                              label=species, figsize=(10,7), ax=ax)</pre>
<p class="calibre2"><span class="calibre5"><span class="calibre5">We'll get a scatterplot, as shown in the following screenshot:</span></span></p>
<p class="mce-root"><img class="alignnone13" src="assets/b6c4ba47-98da-48da-8c21-85d90e461773.png"/></p>
<p class="calibre2">From the scatterplot, we can notice some interesting insights. First, the relationship between <kbd class="calibre13">sepal_width</kbd> and <kbd class="calibre13">sepal_length</kbd> is dependent on the species. Setosa (dots) has a fairly linear relationship between <kbd class="calibre13">sepal_width</kbd> and <kbd class="calibre13">sepal_length</kbd>, while versicolor (triangle) and virginica (star) tends to have much greater <kbd class="calibre13">sepal_length</kbd> than Setosa. If we're designing a machine learning algorithm to predict the type of species of flower, we know that the <kbd class="calibre13">sepal_width</kbd> and <kbd class="calibre13">sepal_length</kbd> are important features to include in our model.</p>
<p class="calibre2">Next, let's plot a histogram to investigate the distribution. Consistent with scatterplots, pandas DataFrames provides a built in method to plot histograms using the <span class="calibre5"><kbd class="calibre13">DataFrame.plot.hist()</kbd> function:</span></p>
<pre class="calibre17">df['petal_length'].plot.hist(title='Histogram of Petal Length')</pre>
<p class="calibre2">And we can see the output in the following screenshot:</p>
<p class="mce-root"><img class="alignnone14" src="assets/bd8c1d3b-b859-485a-902f-59b5379a5ff3.png"/></p>
<p class="calibre2">We can see that the distribution of petal lengths is essentially bimodal. It appears that certain species of flowers have shorter petals than the rest. We can also plot a boxplot of the data. The boxplot is an important data visualization tool used by data scientists to understand the distribution of the data based on the first quartile, median, and the third quartile:</p>
<pre class="calibre17">df.plot.box(title='Boxplot of Sepal Length &amp; Width, and Petal Length &amp; Width')</pre>
<p class="calibre2">The output is given in the following screenshot:</p>
<p class="mce-root"><img class="alignnone15" src="assets/4f91a419-3b73-47d9-bf0b-9f988805dbea.png"/></p>
<p class="calibre2">From the boxplot, we can see that the variance of <kbd class="calibre13">sepal_width</kbd> is much smaller than the other numeric variables, with <kbd class="calibre13">petal_length</kbd> having the greatest variance.</p>
<p class="calibre2">We have now seen how convenient and easy it is to visualize data using pandas directly. Keep in mind that EDA is a crucial step in the machine learning pipeline, and it is something that we will continue to do in every project for the rest of the book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data preprocessing in pandas</h1>
                </header>
            
            <article>
                
<p class="calibre2">Lastly, let's take a look at how we can use pandas for data preprocessing, specifically to encode categorical variables and to impute missing values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encoding categorical variables</h1>
                </header>
            
            <article>
                
<p class="calibre2">In machine learning projects, it is common to receive datasets with categorical variables. Here are some examples of categorical variables in datasets:</p>
<ul class="calibre11">
<li class="calibre12"><strong class="calibre1">Gender</strong>: Male, female</li>
<li class="calibre12"><strong class="calibre1">Day</strong>:<strong class="calibre1"> </strong>Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday</li>
<li class="calibre12"><strong class="calibre1">Country</strong>:<strong class="calibre1"> </strong>USA, UK, China, Japan</li>
</ul>
<p class="calibre2">Machine learning algorithms such as neural networks are unable to work with such categorical variables as they expect numerical variables. Therefore, we need to perform preprocessing on these variables before feeding them into a machine learning algorithm. </p>
<p class="calibre2">One common way to convert these categorical variables into numerical variables is a technique known as one-hot encoding, implemented by the <kbd class="calibre13">get_dummies()</kbd><span class="calibre5"> </span>function in pandas. One-hot encoding is a process that converts a categorical variable with <kbd class="calibre13">n</kbd><span class="calibre5"> </span>categories into <kbd class="calibre13">n</kbd><span class="calibre5"> </span>distinct binary features. An example is provided in the following table:</p>
<p class="mce-root"><img class="alignnone16" src="assets/493047e9-352a-4f6d-8ad7-310528d2a7d5.png"/></p>
<p class="calibre2">Essentially, the transformed features are binary features with a <strong class="calibre4">1</strong> value if it represents the original feature, and <strong class="calibre4">0</strong> otherwise. As you can imagine, it would be a hassle to write the code for this manually. Fortunately, pandas has a handy function that does exactly that. First, let's create a DataFrame in pandas using the data in the preceding table:</p>
<pre class="calibre17">df2 = pd.DataFrame({'Day': ['Monday','Tuesday','Wednesday',<br class="title-page-name"/>                           'Thursday','Friday','Saturday',<br class="title-page-name"/>                           'Sunday']})</pre>
<p class="calibre2">We can see the output in the following screenshot:</p>
<p class="mce-root"><img class="alignnone17" src="assets/c4e56c5d-278d-4beb-afeb-6572cbe9486a.png"/></p>
<p class="calibre2">To one-hot encode the preceding categorical feature using pandas, it is as simple as calling the following function:</p>
<pre class="calibre17">print(pd.get_dummies(df2))</pre>
<p class="calibre2">Here's the output:</p>
<p class="mce-root"><img class="alignnone18" src="assets/8b5a3920-12aa-4c02-8cde-0d330f4927f7.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Imputing missing values</h1>
                </header>
            
            <article>
                
<p class="calibre2">As discussed earlier, imputing missing values is an essential part of the machine learning workflow. Real-world datasets are messy and usually contain missing values. Most machine learning models such as neural networks are unable to work with missing data, and hence we have to preprocess the data before we feed the data into our models. pandas makes it easy to handle missing values.</p>
<p class="calibre2">Let's use the Iris dataset from earlier. The Iris dataset does not have any missing values by default. Therefore, we have to delete some values on purpose for the sake of this exercise. The following code randomly selects <kbd class="calibre13">10</kbd> rows in the dataset, and deletes the <kbd class="calibre13">sepal_length</kbd> values in these <kbd class="calibre13">10</kbd> rows:</p>
<pre class="calibre17">import numpy as np<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/><br class="title-page-name"/># Import the iris data once again<br class="title-page-name"/>URL = \<br class="title-page-name"/>    'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'<br class="title-page-name"/>df = pd.read_csv(URL, names = ['sepal_length', 'sepal_width', <br class="title-page-name"/>                               'petal_length', 'petal_width', 'class'])<br class="title-page-name"/><br class="title-page-name"/># Randomly select 10 rows<br class="title-page-name"/>random_index = np.random.choice(df.index, replace= False, size=10)<br class="title-page-name"/><br class="title-page-name"/># Set the sepal_length values of these rows to be None<br class="title-page-name"/>df.loc[random_index,'sepal_length'] = None</pre>
<p class="calibre2">Let's use this modified dataset to see how we can deal with missing values. First, let's check where our missing values are:</p>
<pre class="calibre17">print(df.isnull().any())</pre>
<p class="calibre2">The preceding <kbd class="calibre13">print</kbd> function gives the following output:</p>
<p class="mce-root"><img class="alignnone19" src="assets/ab3a96aa-9d4c-43e5-a26a-1c9a4981616e.png"/></p>
<p class="calibre2">Unsurprisingly, pandas tells us that there are missing (that is, null) values in the <kbd class="calibre13">sepal_length</kbd> column. This command is useful to find out which columns in our dataset contains missing values.</p>
<p class="calibre2">One way to deal with missing values is to simply remove any rows with missing values. pandas provides a handy <kbd class="calibre13">dropna</kbd> function for us to do that:</p>
<pre class="calibre17">print("Number of rows before deleting: %d" % (df.shape[0]))<br class="title-page-name"/>df2 = df.dropna()<br class="title-page-name"/>print("Number of rows after deleting: %d" % (df2.shape[0]))</pre>
<p class="calibre2">The output is shown in the following screenshot:</p>
<p class="mce-root"><img class="alignnone20" src="assets/03ec86b0-1d20-45a2-a573-e1db0a2b9737.png"/></p>
<p class="calibre2">Another way is to replace the missing <kbd class="calibre13">sepal_length</kbd> values with the mean of the non-missing <kbd class="calibre13">sepal_length</kbd> values:</p>
<pre class="calibre17">df.sepal_length = df.sepal_length.fillna(df.sepal_length.mean())</pre>
<div class="packttip">pandas will automatically exclude the missing values when calculating the mean using <kbd class="calibre19">df.mean()</kbd>.</div>
<p class="calibre2">Now let's confirm that there are no missing values:</p>
<p class="mce-root"><img class="alignnone21" src="assets/6da172fd-6933-44e2-90af-951b5589e9db.png"/></p>
<p class="calibre2">With the missing values handled, we can then pass the DataFrame to machine learning models.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using pandas in neural network projects</h1>
                </header>
            
            <article>
                
<p class="calibre2">We have seen how pandas can be used to import tabular data in <kbd class="calibre13">.csv</kbd> format, and perform data preprocessing and data visualization directly using built-in functions in pandas. For the rest of the book, we will use pandas when the dataset is of a tabular nature. pandas plays a crucial role in data preprocessing and EDA, as we shall see in future chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow and Keras – open source deep learning libraries</h1>
                </header>
            
            <article>
                
<p class="calibre2">TensorFlow is an open source library for neural networks and deep learning developed by the Google Brain team. Designed for scalability, TensorFlow runs across a variety of platforms, from desktops to mobile devices and even to clusters of computers. Today, TensorFlow is one of the most popular machine learning libraries and is used extensively in a wide variety of real-world applications. For example, TensorFlow powers the AI behind many online services that we use today, including image search, voice recognition, recommendation engines. TensorFlow has become the silent workhorse powering many AI applications, even though we might not even notice it.</p>
<p class="calibre2">Keras is a high-level API that runs on top of TensorFlow. So, why Keras? Why do we need another library to act as an API for TensorFlow? To put it simply, Keras removes the complexities in building neural networks, and enables rapid experimentation and testing without concerning the user with low-level implementation details. Keras provides a simple and intuitive API for building neural networks using TensorFlow. Its guiding principles are modularity and extensibility. As we shall see later, it is extremely easy to build neural networks by stacking Keras API calls on top of one another, which you can think of like stacking Lego blocks in order to create bigger structures. This beginner-friendly approach has led to the popularity of Keras as one of the top machine learning libraries in Python. In this book, we will use Keras as the primary machine learning library for building our neural network projects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The fundamental building blocks in Keras</h1>
                </header>
            
            <article>
                
<p class="calibre2">The fundamental building blocks in Keras are layers, and we can stack layers linearly to create a model. The <strong class="calibre4">Loss Function </strong>that we choose will provide the metrics for which we will use to train our model using an <strong class="calibre4">Optimizer. </strong>Recall that while building our neural network from scratch earlier, we had to define and write the code for those terms. We call these the fundamental building blocks in Keras because we can build any neural network using these basic structures.</p>
<p class="calibre2">The following diagram illustrates the relationship between these building blocks in Keras:</p>
<p class="mce-root"><img class="alignnone22" src="assets/081fd69c-7a42-4394-a198-a533a7e2892d.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Layers – the atom of neural networks in Keras</h1>
                </header>
            
            <article>
                
<p class="calibre2">You can think of layers in Keras as an atom, because they are the smallest unit of our neural network. Each layer takes in an input performs a mathematical function, then outputs that for the next layer. The core layers in Keras includes dense layers, activation layers, and dropout layers. There are other layers that are more complex, including convolutional layers and pooling layers. In this book, you will be exposed to projects that uses all these layers. </p>
<p class="calibre2">For now, let's take a closer look at dense layers, which are by far the most common type of layer used in Keras. A dense layer is also known as a fully-connected layer. It is fully-connected because it uses all of its input (as opposed to a subset of the input) for the mathematical function that it implements. </p>
<p class="calibre2">A dense layer implements the following function:</p>
<p class="mce-root"><img class="fm-editor-equation4" src="assets/1899d70e-e335-4861-a5b2-afdbc619f1d0.png"/></p>
<p class="calibre2"/>
<p class="calibre2"><img class="fm-editor-equation5" src="assets/b4d15384-5e35-4805-9005-2c8d0eb02210.png"/> is the output, <img class="fm-editor-equation6" src="assets/cb669ce2-0c83-4127-ae60-d0105f4b99ed.png"/> is the activation function, <img class="fm-editor-equation7" src="assets/81a91630-b8d9-443e-ac60-657e76f6808c.png"/> is the input, and <img class="fm-editor-equation8" src="assets/5f5bd2fa-6f2c-408b-b07c-834d7b3f2462.png"/> and <img class="fm-editor-equation9" src="assets/825fd59c-a153-4f9e-97f5-41c17f287c57.png"/> are the weights and biases respectively.</p>
<p class="calibre2">This equation should look familiar to you. We used the fully-connected layer when we were building our neural network from scratch earlier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Models – a collection of layers</h1>
                </header>
            
            <article>
                
<p class="calibre2">If layers can be thought of as atoms, then models can be thought of as molecules in Keras. A model is simply a collection of layers, and the most commonly used model in Keras is the <span class="calibre5"><kbd class="calibre13">Sequential</kbd> model. A <kbd class="calibre13">Sequential</kbd> model allows us to linearly stack layers on one another, where a single layer is connected to one other layer only. This allows us to easily design model architectures without worrying about the underlying math. As we will see in later chapters, there is a significant amount of thought needed to ensure that consecutive layer dimensions are compatible with one another, something that Keras takes care for us under the hood!</span></p>
<p class="calibre2">Once we have defined our model architecture, we need to define our training process, which is done using the <kbd class="calibre13">compile</kbd> method in Keras. The <kbd class="calibre13">compile</kbd>  method takes in several arguments, but the most important arguments we need to define is the optimizer and the loss function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loss function – error metric for neural network training</h1>
                </header>
            
            <article>
                
<p class="calibre2">In an earlier section, we defined the loss function as a way to evaluate the goodness of our predictions (that is, how far off <span class="calibre5">our predictions </span>are). The nature of our problem should dictate the loss function used. There are several loss functions implemented in Keras, but the most commonly used loss functions are <span class="calibre5"><kbd class="calibre13">mean_squared_error</kbd></span>,<span class="calibre5"> </span><kbd class="calibre13">categorical_crossentropy</kbd>, and <kbd class="calibre13">binary_crossentropy</kbd>.</p>
<p class="calibre2">As a general rule of thumb, this is how you should choose which loss function to use:</p>
<ul class="calibre11">
<li class="calibre12"><span><kbd class="calibre13">mean_squared_error</kbd> if the problem is a regression problem</span></li>
<li class="calibre12"><span><kbd class="calibre13">categorical_crossentropy</kbd> if the problem is a multiclass classification problem</span></li>
<li class="calibre12"><kbd class="calibre13">binary_crossentropy</kbd> if the problem is a binary classification problem</li>
</ul>
<p class="calibre2"/>
<div class="packttip">In certain cases, you might find that the default loss functions in Keras are unsuitable for your problem. In that case, you can define your own loss function by defining a custom function in Python, then passing that custom function to the <kbd class="calibre19">compile</kbd> method in Keras.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizers – training algorithm for neural networks</h1>
                </header>
            
            <article>
                
<p class="calibre2">An optimizer is an algorithm for updating the weights of the neural network in the training process. Optimizers in Keras are based on the gradient descent algorithm, which we have covered in an earlier section.</p>
<p class="calibre2">While we won't cover in detail the differences between each optimizer, it is important to note that our choice of optimizer should depend on the nature of the problem. In general, researchers have found that the <kbd class="calibre13">Adam</kbd> optimizer works best for DNNs, while the <kbd class="calibre13">sgd</kbd> <span class="calibre5">optimizer</span> <span class="calibre5">works best for shallow neural networks. The <kbd class="calibre13">Adagrad</kbd> optimizer is also a popular choice, and it adapts the learning rate of the algorithm based on how frequent a particular set of weights are updated. The main advantage of this approach is that it eliminates the need to manually tune the learning rate hyperparameter, which is a time-consuming process in the machine learning workflow.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating neural networks in Keras</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">Let's take a look at how we can use Keras to build the two-layer neural network that we introduced earlier. </span>To build a linear collection of layers, first declare a <kbd class="calibre13">Sequential</kbd> model in Keras:</p>
<pre class="calibre17">from keras.models import Sequential<br class="title-page-name"/>model = Sequential()</pre>
<p class="calibre2">This creates an empty <span class="calibre5"> </span><kbd class="calibre13">Sequential</kbd><span class="calibre5"> model that we can now add layers to. Adding layers in Keras is simple and similar to stacking Lego blocks on top of one another. We start by adding layers from the left (the layer closest to the input):</span></p>
<pre class="calibre17">from keras.layers import Dense<br class="title-page-name"/># Layer 1<br class="title-page-name"/>model.add(Dense(units=4, activation='sigmoid', input_dim=3))<br class="title-page-name"/># Output Layer<br class="title-page-name"/>model.add(Dense(units=1, activation='sigmoid'))</pre>
<p class="calibre2">Stacking layers in Keras is as simple as calling the <kbd class="calibre13">model.add()</kbd> command. Notice that we had to define the number of units in each layer. Generally, increasing the number of units increases the complexity of the model, as it means that there are more weights to be trained. For the first layer, we had to define <kbd class="calibre13">input_dim</kbd>. This informs Keras the number of features (that is, columns) in the dataset. Also, note that we have used a <kbd class="calibre13">Dense</kbd> layer. A <kbd class="calibre13">Dense</kbd> layer is simply a fully connected layer. In later chapters, we will introduce other kinds of layers, specific to different types of problems.</p>
<p class="calibre2">We can verify the structure of our model by calling the <kbd class="calibre13">model.summary()</kbd> function:</p>
<pre class="calibre17">print(model.summary())</pre>
<p class="calibre2">The output is shown in the following screenshot:</p>
<p class="mce-root"><img class="alignnone23" src="assets/9289d890-2b97-4c18-9e1b-9c1886620e33.png"/></p>
<p class="calibre2">The number of params is the number of weights and biases we need to train for the model that we have just defined.</p>
<p class="calibre2">Once we are satisfied with our model's architecture, let's compile it and start the training process:</p>
<pre class="calibre17">from keras import optimizers<br class="title-page-name"/>sgd = optimizers.SGD(lr=1)<br class="title-page-name"/>model.compile(loss='mean_squared_error', optimizer=sgd)</pre>
<div class="packttip">Note that we have defined the learning rate of the <kbd class="calibre19">sgd</kbd> optimizer to be 1.0 (<kbd class="calibre19">lr=1</kbd>). In general, the learning rate is a hyperparameter of the neural network that needs to be tuned carefully depending on the problem. We will take a closer look at tuning hyperparameters in later chapters.</div>
<p class="calibre2">The <kbd class="calibre13">mean_squared_error</kbd> loss function in Keras is similar to the sum-of-squares error that we have defined earlier. We are using the SGD<strong class="calibre4"> </strong>optimizer to train our model. Recall that gradient descent is the method of updating the weights and biases by moving it toward the derivative of the loss function with respect to the weights and biases.</p>
<p class="calibre2">Let's use the same data that we used earlier to train our neural network. This will allow us to compare the predictions obtained using Keras versus the predictions obtained when we created our neural network from scratch earlier.</p>
<p class="calibre2">Let's define an <kbd class="calibre13">X</kbd> and <kbd class="calibre13">Y</kbd> NumPy array, corresponding to the features and the target variables respectively:</p>
<pre class="calibre17">import numpy as np<br class="title-page-name"/># Fixing a random seed ensures reproducible results<br class="title-page-name"/>np.random.seed(9)<br class="title-page-name"/><br class="title-page-name"/>X = np.array([[0,0,1],<br class="title-page-name"/>              [0,1,1],<br class="title-page-name"/>              [1,0,1],<br class="title-page-name"/>              [1,1,1]])<br class="title-page-name"/>y = np.array([[0],[1],[1],[0]])</pre>
<p class="calibre2">Finally, let's train the model for <kbd class="calibre13">1500</kbd> iterations:</p>
<pre class="calibre17">model.fit(X, y, epochs=1500, verbose=False)</pre>
<p class="calibre2">To get the predictions, run the <kbd class="calibre13">model.predict()</kbd> command on our data:</p>
<pre class="calibre17">print(model.predict(X))</pre>
<p class="calibre2">The preceding code gives the following output:</p>
<p class="mce-root"><img class="alignnone24" src="assets/fbc61878-0d50-4431-a6d6-913d66f5196c.png"/></p>
<p class="calibre2">Comparing this to the predictions that we obtained earlier, we can see that the results are extremely similar. The major advantage of using Keras is that we did not have to worry about the low-level implementation details and mathematics while building our neural network, unlike what we did earlier. In fact, we did no math at all. All we did in Keras was to call a series of APIs to build our neural network. This allows us to focus on high-level details, enabling rapid experimentation.</p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other Python libraries</h1>
                </header>
            
            <article>
                
<p class="calibre2">Besides pandas and Keras, we will also be using other Python libraries, such as scikit-learn and seaborn. scikit-learn is an open source machine learning library that is widely used in machine learning projects. The main functionality that we use in scikit-learn is to separate our data into a training and testing set during data preprocessing. seaborn is an alternative data visualization in Python that has been gaining traction recently. In the later chapters, we'll see how we can use seaborn to make data visualizations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we have seen what machine learning is, and looked at the complete end-to-end workflow for every machine learning project. We have also seen what neural networks and deep learning is, and coded up our own neural network from scratch and in Keras.</p>
<p class="calibre2">For the rest of the book, we will create our own real-world neural network projects. Each chapter will cover one project, and the projects are listed in order of increasing complexity. By the end of the book, you will have created your own neural network projects in medical diagnosis, taxi fare predictions, image classification, sentiment analysis, and much more. In the next chapter, <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre10">Chapter 2</a>, <em class="calibre8">Predicting Diabetes with Multilayer Perceptrons</em> we will cover diabetes prediction with <strong class="calibre4">multilayer perceptrons</strong> (<strong class="calibre4">MLPs</strong>). Let's get started!</p>


            </article>

            
        </section>
    </body></html>