["```py\n# Import the modules\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline # The data file is loaded and analyzed\n```", "```py\nfilename = 'Folds5x2_pp.xlsx' # download the data file from UCI ML repository\ndf = pd.read_excel(filename, sheet_name='Sheet1')\ndf.describe()\n```", "```py\nX, Y = df[['AT', 'V','AP','RH']], df['PE']\nscaler = MinMaxScaler()\nX_new = scaler.fit_transform(X)\ntarget_scaler = MinMaxScaler()\nY_new = target_scaler.fit_transform(Y.values.reshape(-1,1))\nX_train, X_test, Y_train, y_test = \\\n train_test_split(X_new, Y_new, test_size=0.4, random_state=333)\n```", "```py\nclass LinearRegressor:\n def __init__(self,d, lr=0.001 ):\n # Placeholders for input-output training data\n self.X = tf.placeholder(tf.float32,\\\n shape=[None,d], name='input')\n self.Y = tf.placeholder(tf.float32,\\\n name='output')\n # Variables for weight and bias\n self.b = tf.Variable(0.0, dtype=tf.float32)\n self.W = tf.Variable(tf.random_normal([d,1]),\\\n dtype=tf.float32)\n\n # The Linear Regression Model\n self.F = self.function(self.X)\n\n # Loss function\n self.loss = tf.reduce_mean(tf.square(self.Y \\\n - self.F, name='LSE'))\n # Gradient Descent with learning \n # rate of 0.05 to minimize loss\n optimizer = tf.train.GradientDescentOptimizer(lr)\n self.optimize = optimizer.minimize(self.loss)\n\n # Initializing Variables\n init_op = tf.global_variables_initializer()\n self.sess = tf.Session()\n self.sess.run(init_op)\n\n def function(self, X):\n return tf.matmul(X, self.W) + self.b\n\n def fit(self, X, Y,epochs=500):\n total = []\n for i in range(epochs):\n _, l = self.sess.run([self.optimize,self.loss],\\\n feed_dict={self.X: X, self.Y: Y})\n total.append(l)\n if i%100==0:\n print('Epoch {0}/{1}: Loss {2}'.format(i,epochs,l))\n return total\n\n def predict(self, X):\n return self.sess.run(self.function(X), feed_dict={self.X:X})\n\n def get_weights(self):\n return self.sess.run([self.W, self.b])\n```", "```py\nN, d = X_train.shape\nmodel = LinearRegressor(d)\nloss = model.fit(X_train, Y_train, 20000) #Epochs = 20000\n```", "```py\n# Import the modules\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\n```", "```py\nfilename = 'winequality-red.csv' # Download the file from UCI ML Repo \ndf = pd.read_csv(filename, sep=';')\n```", "```py\nX, Y = df[columns[0:-1]], df[columns[-1]]\nscaler = MinMaxScaler()\nX_new = scaler.fit_transform(X)\nY.loc[(Y<3)]=3\nY.loc[(Y<6.5) & (Y>=3 )] = 2\nY.loc[(Y>=6.5)] = 1\nY_new = pd.get_dummies(Y) # One hot encode\nX_train, X_test, Y_train, y_test = \\\ntrain_test_split(X_new, Y_new, test_size=0.4, random_state=333)\n```", "```py\nclass LogisticRegressor:\n    def __init__(self, d, n, lr=0.001 ):\n        # Place holders for input-output training data\n        self.X = tf.placeholder(tf.float32,\\\n              shape=[None,d], name='input')\n        self.Y = tf.placeholder(tf.float32,\\\n              name='output')\n        # Variables for weight and bias\n        self.b = tf.Variable(tf.zeros(n), dtype=tf.float32)\n        self.W = tf.Variable(tf.random_normal([d,n]),\\\n              dtype=tf.float32)\n        # The Logistic Regression Model\n        h = tf.matmul(self.X, self.W) + self.b\n        self.Ypred = tf.nn.sigmoid(h)\n        # Loss function\n        self.loss = cost = tf.reduce_mean(-tf.reduce_sum(self.Y*tf.log(self.Ypred),\\\n                 reduction_indices=1), name = 'cross-entropy-loss')\n        # Gradient Descent with learning \n        # rate of 0.05 to minimize loss\n        optimizer = tf.train.GradientDescentOptimizer(lr)\n        self.optimize = optimizer.minimize(self.loss)\n        # Initializing Variables\n        init_op = tf.global_variables_initializer()\n        self.sess = tf.Session()\n        self.sess.run(init_op)\n\n    def fit(self, X, Y,epochs=500):\n        total = []\n        for i in range(epochs):\n            _, l = self.sess.run([self.optimize,self.loss],\\\n                  feed_dict={self.X: X, self.Y: Y})\n            total.append(l)\n            if i%1000==0:\n                print('Epoch {0}/{1}: Loss {2}'.format(i,epochs,l))\n        return total\n\n   def predict(self, X):\n        return self.sess.run(self.Ypred, feed_dict={self.X:X})\n\n    def get_weights(self):\n        return self.sess.run([self.W, self.b])\n```", "```py\n# Import the modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.svm import SVC # The SVM Classifier from scikit\nimport seaborn as sns\n%matplotlib inline\n```", "```py\nfilename = 'winequality-red.csv' #Download the file from UCI ML Repo\ndf = pd.read_csv(filename, sep=';')\n\n#categorize wine quality in two levels\nbins = (0,5.5,10)\ncategories = pd.cut(df['quality'], bins, labels = ['bad','good'])\ndf['quality'] = categories\n\n#PreProcessing and splitting data to X and y\nX = df.drop(['quality'], axis = 1)\nscaler = MinMaxScaler()\nX_new = scaler.fit_transform(X)\ny = df['quality']\nlabelencoder_y = LabelEncoder()\ny = labelencoder_y.fit_transform(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \\\n        test_size = 0.2, random_state = 323)\n```", "```py\nclassifier = SVC(kernel = 'rbf', random_state = 45)\nclassifier.fit(X_train, y_train)\n```", "```py\ny_pred = classifier.predict(X_test)\n```", "```py\nprint(\"Accuracy is {}\".format(accuracy_score(y_test, y_pred)))\n## Gives a value ~ 67.5%\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm,annot=True,fmt='2.0f')\n```", "```py\nbins = (0,3.5,5.5,10)\ncategories = pd.cut(df['quality'], bins, labels = ['bad','ok','good'])\ndf['quality'] = categories\n```", "```py\n# Import the modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB # The SVM Classifier from scikit\nimport seaborn as sns\n%matplotlib inline\n```", "```py\nfilename = 'winequality-red.csv' #Download the file from UCI ML Repo\ndf = pd.read_csv(filename, sep=';')\n\n#categorize wine quality in two levels\nbins = (0,5.5,10)\ncategories = pd.cut(df['quality'], bins, labels = ['bad','good'])\ndf['quality'] = categories\n\n#PreProcessing and splitting data to X and y\nX = df.drop(['quality'], axis = 1)\nscaler = MinMaxScaler()\nX_new = scaler.fit_transform(X)\ny = df['quality']\nlabelencoder_y = LabelEncoder()\ny = labelencoder_y.fit_transform(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \\\n        test_size = 0.2, random_state = 323)\n```", "```py\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n#Predicting the Test Set\ny_pred = classifier.predict(X_test)\n```", "```py\n*c*lass sklearn.tree.DecisionTreeRegressor (criterion=’mse’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort=False)\n```", "```py\nclass sklearn.tree.DecisionTreeClassifier(criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n```", "```py\n# Import the modules\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n%matplotlib inline\n\n# Read the data \nfilename = 'Folds5x2_pp.xlsx' # The file can be downloaded from UCI ML repo\ndf = pd.read_excel(filename, sheet_name='Sheet1')\ndf.describe()\n\n# Preprocess the data and split in test/train\nX, Y = df[['AT', 'V','AP','RH']], df['PE']\nscaler = MinMaxScaler()\nX_new = scaler.fit_transform(X)\ntarget_scaler = MinMaxScaler()\nY_new = target_scaler.fit_transform(Y.values.reshape(-1,1))\nX_train, X_test, Y_train, y_test = \\\n train_test_split(X_new, Y_new, test_size=0.4, random_state=333)\n\n# Define the decision tree regressor\nmodel = DecisionTreeRegressor(max_depth=3)\nmodel.fit(X_train, Y_train)\n\n# Make the prediction over the test data\nY_pred = model.predict(np.float32(X_test))\nprint(\"R2 Score is {} and MSE {}\".format(\\\n r2_score(y_test, Y_pred),\\\n mean_squared_error(y_test, Y_pred)))\n```", "```py\n# Import the modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n%matplotlib inline\n\n# Read the data \nfilename = 'winequality-red.csv' #Download the file from https://archive.ics.uci.edu/ml/datasets/wine+quality df = pd.read_csv(filename, sep=';')\n\n# categorize the data into three classes\nbins = (0,3.5,5.5,10)\ncategories = pd.cut(df['quality'], bins, labels = ['bad','ok','good'])\ndf['quality'] = categories\n\n# Preprocessing and splitting data to X and y X = df.drop(['quality'], axis = 1) scaler = MinMaxScaler() X_new = scaler.fit_transform(X) y = df['quality'] from sklearn.preprocessing import LabelEncoder labelencoder_y = LabelEncoder() y = labelencoder_y.fit_transform(y) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 323)\n\n# Define the decision tree classifier\nclassifier = DecisionTreeClassifier(max_depth=3)\nclassifier.fit(X_train, y_train)\n\n# Make the prediction over the test data\nY_pred = classifier.predict(np.float32(X_test))\nprint(\"Accuracy is {}\".format(accuracy_score(y_test, y_pred)))\n```", "```py\n# import the different classifiers\nfrom sklearn.svm import SVC \nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Declare each classifier\nclf1 = SVC(random_state=22)\nclf2 = DecisionTreeClassifier(random_state=23)\nclf3 = GaussianNB()\nX = np.array(X_train)\ny = np.array(y_train)\n\n#Employ Ensemble learning\neclf = VotingClassifier(estimators=[\n('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\neclf = eclf.fit(X, y)\n\n# Make prediction on test data\ny_pred = eclf.predict(X_test)\n```", "```py\nfrom sklearn.ensemble import BaggingClassifier\nbag_classifier = BaggingClassifier(\n        DecisionTreeClassifier(), n_estimators=500, max_samples=1000,\\\n        bootstrap=True, n_jobs=-1)    \nbag_classifier.fit(X_train, y_train)\ny_pred = bag_classifier.predict(X_test)\n```", "```py\nfrom sklearn.model_selection import cross_val_score \naccuracies = cross_val_score(estimator = classifier, X = X_train,\\\n     y = y_train, cv = 10)\nprint(\"Accuracy Mean {} Accuracy Variance \\\n     {}\".format(accuracies.mean(),accuracies.std()))\n```", "```py\nGrid search for best model and parameters\nfrom sklearn.model_selection import GridSearchCV\n#parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nclassifier = SVC()\nparameters = [{'C': [1, 10], 'kernel': ['linear']},\n    {'C': [1, 10], 'kernel': ['rbf'],\n    'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n    grid_search = GridSearchCV(estimator = classifier,\n    param_grid = parameters,\n    scoring = 'accuracy',\n    cv = 10,)\ngrid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n#here is the best accuracy\nbest_accuracy\n```"]