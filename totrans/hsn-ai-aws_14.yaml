- en: Sales Forecasting with Deep Learning and Auto Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Demand forecasting is key to many industries such as airlines, retail, telecommunications,
    and healthcare. Inaccurate and imprecise demand forecasting leads to missed sales
    and customers, significantly impacting an organization's bottom line. One of the
    key challenges facing retailers is effectively managing inventory based on multiple
    internal and external factors. Inventory management is a complex business problem
    to solve—the demand for a product changes by location, weather, promotions, holidays,
    day of the week, special events, and other external factors, such as store demographics,
    consumer confidence, and unemployment.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at how traditional techniques of time series forecasting
    such as ARIMA and exponential smoothing are different from neural network-based
    techniques. We will also look at how DeepAR works, discussing its model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are the topics that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding traditional time series forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding how the DeepAR model works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding model sales through DeepAR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting and evaluating sales
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the following sections, we will employ the [retail](https://www.kaggle.com/manjeetsingh/retaildataset)
    dataset containing sales of around 45 stores to illustrate how DeepAR predicts
    future sales given multiple factors such as holidays, promotions, and macro-economic
    indicators (unemployment).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the [folder](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services)
    associated with this chapter, you will find three CSV files:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Features dataset**: This contains the data of regional activity related to
    the store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sales dataset**: This contains historical sales data covering three years,
    from 2010 to 2012\. It covers sales for 143 weeks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Store dataset:**This contains anonymized information about the 45 stores,
    including the type and size of the store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please refer to the following link of GitHub for the source code of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services)'
  prefs: []
  type: TYPE_NORMAL
- en: It is now time to understand traditional time series forecasting techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding traditional time series forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's begin by looking at traditional time series forecasting techniques, specifically
    ARIMA and exponential smoothing to model demand in simple use cases. We will look
    at how ARIMA estimates sales using historical sales and forecast errors. Also,
    we'll review how exponential smoothing accounts for irregularities in historical
    sales and captures trends and seasonality to forecast sales.
  prefs: []
  type: TYPE_NORMAL
- en: Auto-Regressive Integrated Moving Average (ARIMA )
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ARIMA is a time series analytical technique used to capture different temporal
    structures in univariate data. To model the time series data, differencing is
    applied across the series to make the data stationary. Differencing is the technique
    of subtracting the previous data point from the current one for every data point
    excluding the first one. The technique makes the mean and variance of the probability
    distribution of the time series constant over time, making future values of the
    series much more predictable. A specific number of lagged forecasts and forecast
    errors are used to model time series. This number is iteratively adjusted until
    the residuals are uncorrelated with the target (sales forecast) or all of the
    signals in the data have been picked up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s unpack ARIMA to look at the underlying components—autoregressive, integrated,
    and moving average:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The number of autoregressive terms**: These establish a relationship between
    a specific number of historical data points and the current one that is, it uses
    historical demand to estimate the current demand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The number of non-seasonal differences**: These make temporal or time series
    data stationary by differencing. We''re assuming that future demand will look
    like historical demand if the difference in demand during the last few time steps
    is very small.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The number of moving-average terms (lagged forecast errors**): These account
    for forecast error—actual versus forecasted demand—or a specific number of historical
    data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at the ARIMA equation, both in words and mathematical form:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Demand forecast = constant term + autoregressive terms + moving average terms*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5713d77-c78b-494d-b4f8-d86d38c16ad1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a visual representation of how ARIMA works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33aca5fa-63ba-47e5-92e2-2233610d3166.png)'
  prefs: []
  type: TYPE_IMG
- en: In the ARIMA model, the AR terms are positive, while the MA terms are negative;
    in other words, the autoregressive terms have a positive impact on demand while
    the moving-average of lagged errors has a negative impact.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential smoothing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other alternative to ARIMA is the exponential smoothing technique, which
    is also a time series forecasting method for univariate data, where random noise
    is neglected, revealing the underlying time structure. Although it is like ARIMA
    in that demand forecast is a weighted sum of past observations, the method of
    applying weights to lagged observations is different—instead of providing equal
    weights to past observations, the model employs exponentially decreasing weights
    for lags. In other words, the most recent observations are more relevant than
    historical ones. Exponential smoothing is used to make short-term forecasts, where
    we assume that future patterns and trends will look like current patterns and
    trends.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are the three types of exponential smoothing methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single exponential smoothing**: As the name indicates, the technique does
    not account for seasonality or trend. It requires a single parameter, alpha (![](img/9b3dc95b-fb71-42e8-a1be-ecafa0677063.png)),
    to control the level of smoothing. Low alpha means there are no irregularities
    in the data, implying that the latest observations are given lower weight.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Double exponential smoothing**: This technique, on the other hand, supports
    trends in univariate series. In addition to controlling how important recent observations
    are relative to historical ones, an additional factor is used to control the influence
    of trend on demand forecasts. The trend can be either multiplicative or additive
    and is controlled using a smoothing factor, ![](img/4d2ce8ab-d8fd-4828-9883-1e50e0960d31.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Triple exponential smoothing**: This one adds support for seasonality. Another
    new parameter, gamma (![](img/1d5a8cac-5805-4fd7-b1ef-bad2f80d0901.png)), is used
    to control the influence of the seasonal component on demand forecasts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the difference between the different types
    of exponential smoothing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92261167-415b-47bb-a882-29ff7b30e1a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Single exponential smoothing that forecasts demand at time, *t*, is based on
    estimated demand and forecast error (actual—estimated demand) at time, *t-1*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the case of double exponential smoothing, demand is forecasted by capturing
    both trend and historical data. We use two smoothing factors here, data and trend
    (here''s a visual on how double exponential smoothing captures trends):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/e96e3a23-7a46-4c74-bf70-9ecd3f451aed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For triple exponential smoothing, we also account for seasonality through a
    third smoothing factor called a seasonal smoothing factor. See the following diagram,
    which captures seasonal peaks and troughs, along with trend:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/30cb92f2-f89e-4b0f-ac64-25e3b97dd9a1.png)'
  prefs: []
  type: TYPE_IMG
- en: The problem with this approach is that it views past sales as indicative of
    future sales. Besides, they are all forecasting techniques for univariate time
    series. As detailed earlier, there could be other factors that impact current
    and future sales, such as weather, promotions, day of the week, holidays, and
    special events.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at how the DeepAR model from SageMaker can be leveraged to model
    multi-variate time series, defining a non-linear relationship between an output
    variable (demand) and input variables (includes historical sales, promotions,
    weather, and time of the day.)
  prefs: []
  type: TYPE_NORMAL
- en: How the DeepAR model works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DeepAR algorithm offered by Sagemaker is a generalized deep learning model
    that learns about demand across several related time series. Unlike traditional
    forecasting methods, in which an individual time series is modeled, DeepAR models
    thousands or millions of related time series.
  prefs: []
  type: TYPE_NORMAL
- en: Examples include forecasting load for servers in a data center, or forecasting
    demand for all products that a retailer offers, and energy consumption of individual
    households. The unique thing about this approach is that a substantial amount
    of data on past behavior of similar or related time series can be leveraged for
    forecasting an individual time series. This approach addresses over-fitting issues
    and time—and labor-intensive manual feature engineering and model selection steps
    required by traditional techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'DeepAR is a forecasting method based on autoregressive neural networks and
    it learns about a global model from historical data of all-time series in the
    data set. DeepAR employs **Long Short-Term Memory** (**LSTM**), a type of **Recurrent
    Neural Network** (**RNN**), to model time series. The main idea of RNNs is to
    capture sequential information. Unlike normal neural networks, the inputs (and
    outputs) are dependent on each other. RNNs hence have a memory that captures information
    about what has been estimated so far. The following is a diagram of an unfolded
    RNN—to remember what has been learned so far, at each step, the hidden state is
    computed, not only based on the current input, but also the previous hidden state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42eb4d1b-9929-46e3-8d68-79f721545547.png)*A recurrent neural network
    and illustration of sequential learning as the time steps are unfolded. Source:
    Nature; Image Credits* *[WildML](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)*[.](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s explain in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a87f6d79-2416-4f6f-97f2-29c700f669cb.png) is input at a time, *t.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[![](img/993cee63-e3bb-4921-b07c-84c9ca961f8b.png)]** is the hidden state
    at time, *t*. This state is computed based on previous hidden state and current
    input, and in [![](img/de230eb3-e03d-4922-a6c0-93e7b113f3fc.png)], function, *f*,
    is an activation function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[![](img/b5a3338a-e13a-4540-90a4-dbab88ba7dcb.png)]* is output at time, *t*,
    and [![](img/e8a8ae94-70b9-4b1c-9bf2-d62459b40b1e.png)]. The activation function,
    *f*, can vary depending on the use case. For example, the softmax activation function
    is used when we need to predict which of the classes the input belongs to—in other
    words, whether the image being detected is a cat or a dog or a giraffe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The network weights, *U*, *V,* and *W*, remain the same across all of the time
    steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RNNs have interesting applications in different fields, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural language processing**: From generating image captions to generating
    text to machine translations, RNNs can act as generative models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autonomous cars**: They are used to conduct dynamic facial analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time series**: RNNs are used in econometrics (finance and trend monitoring)
    and for demand forecasting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, general RNNs fail to learn long-term dependencies due to the gap between
    recent and older data. LSTMs, on the other hand, can solve this challenge: the
    inner cells of LSTMs can carry information unchanged through special structures
    called **gates**—input, forget, and output. Through these cells, LSTMs can control
    the information to be retained or erased.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at the model architecture of DeepAR, an LSTM network.
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DeepAR algorithm employs the LSTM network and probability models to identify
    non-linear structures in time series data and provide probabilistic estimates
    of forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: The model is autoregressive in that it consumes observations from the last time
    step as input. It is also recurrent since it uses the previous output of the network
    as input at the next time step. During the training phase, the hidden or the encoded
    state of the network, at each time step, is computed based on current covariates,
    previous observation, and previous network output. The hidden state is then used
    to compute parameters for a probability model that characterizes the behavior
    of time series (product demand, for example).
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we assume the demand to be a random variable following a specific
    probability distribution. Once we have the probability model that can be defined
    through a set of parameters (say, mean and variance), it can be used to estimate
    forecasts. DeepAR uses the Adam optimizer, a stochastic gradient descent algorithm,
    to optimize the maximum log likelihood of training data, given Gaussian model
    parameters. Using this approach, we can derive (optimize) both probability model
    parameters and LSTM parameters to accurately estimate forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates how the DeepAR algorithm works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99f778cb-1c7a-435d-bdb3-51667927ba9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the preceding diagram, **Maximum Likelihood Estimation** (**MLE**)
    is used to estimate two sets of parameters, given all of the time series in the
    input dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parameters of RNN**: These parameters or the hidden state of the RNN network
    are used to compute Gaussian parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameters of the Gaussian model**: The Gaussian parameters are used to provide
    probabilistic estimates of forecasts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLE is computed by leveraging data across all time series, *i*, where *i* goes
    from 1 to *N*—that is, there could be *N* different products the demand of which
    you're trying to estimate. *T* represents the length of the time series.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on MLE, refer to this [article](https://www.analyticsvidhya.com/blog/2018/07/introductory-guide-maximum-likelihood-estimation-case-study-r/).
  prefs: []
  type: TYPE_NORMAL
- en: Arriving at optimal network weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The time series or observations are fed to DeepAR as part of the training. At
    each time step, current covariates, previous observations, and previous network
    output are used. The model uses **Back Propagation Through Time** (**BPTT**) to
    compute gradient descent after each iteration. In particular, the Adam optimizer
    is used to conduct BPTT. Through the stochastic gradient descent algorithm, Adam,
    we arrive at optimal network weights via back propagation.
  prefs: []
  type: TYPE_NORMAL
- en: At each time step, *t*, the inputs to the network are covariates, ![](img/f1640cc4-07d1-4a1c-b5e6-2ef157a2a133.png);
    the target at the previous time step, ![](img/f0d0aca4-9ef6-4a61-89f3-941b9a9a899a.png);
    as well as the previous network output, ![](img/598bd999-812c-48d8-b09d-5132c687bf0d.png).
    The network output, ![](img/b09d5fc7-6d40-4b8b-b705-97d4249f181e.png), is then
    used to compute Gaussian parameters that maximize the probability of observing
    the input dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following visual illustrates sequence-to-sequence learning, where the encoder
    encapsulates demand patterns in the historical time series and sends the same
    (![](img/4b65cef4-67fb-4548-a977-78c387d12d11.png)) as input to the decoder. The
    function of the decoder is to predict demand, taking into consideration the input
    from encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0321fea6-ad86-42b5-af68-a1a7bd55d6b9.png)*Source: Probabilistic Forecasting
    with Autoregressive Recurrent Networks (**[link](https://arxiv.org/abs/1704.04110)**)*'
  prefs: []
  type: TYPE_NORMAL
- en: For prediction, the history of the time series, ![](img/5bab8f64-cf7a-49a7-be0f-4b67c2502ad5.png),
    is fed in for ![](img/a1e275cf-a586-4b6b-8e2f-2645f4bc3acb.png), and, then, in
    the prediction range for ![](img/29f766f6-f2a3-4eab-8f83-85d79dd9ca75.png), a
    sample ![](img/f081231c-3bf5-41f7-b212-0265496b27ef.png) is drawn and fed back
    for the next point until the end of the prediction range, ![](img/cda36150-c0e7-4c79-a21d-d309947330f5.png).
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR produces accurate forecast distributions learned from historical behavior
    of all of the time series jointly. Also, probabilistic forecasts provide optimal
    decisions under uncertainty versus point estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model sales through DeepAR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As noted in the introduction to this chapter, managing inventory for retailers
    is a complex activity to handle. Holidays, special events, and markdowns can have
    a significant impact on how a store performs and, in turn, how a department within
    a store performs.
  prefs: []
  type: TYPE_NORMAL
- en: The Kaggle [dataset](https://www.kaggle.com/manjeetsingh/retaildataset) contains
    historical sales for 45 stores, with each store belonging to a specific type (location
    and performance) and size. The retailer runs several promotional markdowns throughout
    the year. These markdowns precede holidays, such as SuperBowl, Labor Day, Thanksgiving,
    and Christmas.
  prefs: []
  type: TYPE_NORMAL
- en: Brief description of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s briefly consider the dataset that we are about to model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Features data:**This is data of regional activity related to the store:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Store**: Numeric store ID for each store.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Date**: Important dates for store.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fuel price**: Current fuel prices.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Markdowns**: The discount you take on merchandise in your retail store from
    the original marked sale price.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPI** (**Consumer Price Index**): A measure that examines the weighted average
    of prices of a basket of consumer goods and services, such as transportation,
    food, and medical care.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unemployment**: Current unemployment rate.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IsHoliday**: Whether it''s a holiday or not, on a particular date.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sales data**: This is historical sales data covering three years, from 2010
    to 2012\. It covers sales for 143 weeks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Store**: Numeric store ID for each store.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dept**: Numeric department ID for each department of the store.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Date**: Important dates for the store.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weekly sales**: Weekly sales to measure the sales performance of each store.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IsHoliday**: Is it a holiday or not on a particular date.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Store data:**This is anonymized information about the 45 stores, including
    the type and size of the store:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Store**: Numeric store ID for each store.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type**: The type of store.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Size**: The size of the store.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Input and Output:** Now let''s look at the input and output formats
    including the hyperparameters of the SageMaker DeepAR algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm has two input channels and it takes training and test JSONs as
    input through two channels. The training JSON contains only 134 weeks of sales,
    while the test JSON contains sales from all 143 weeks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the structure of the training JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding structure, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`start`: Is the start date of weekly sales.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target`: Is for sorted weekly sales.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cat`: Is the category to group time series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dynamic_feat`: Includes the dynamic features to account for factors impacting
    sales such as holidays.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The test JSON also has the same format as that of the training JSON. Let''s
    have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'DeepAR supports a range of hyperparameters. Following, is a list of some of
    the key hyperparameters. For a detailed list, check out Amazon documentation [here](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time frequency**: Indicates whether the time series is hourly, weekly, monthly,
    or yearly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context length**: How many time steps in the past the algorithm should look
    at for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction length**: The number of data points to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of cells**: The number of neurons to use in each of the hidden layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of layers**: The number of hidden layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Likelihood function**: We will choose the Gaussian model since weekly sales
    are real values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**epochs**: The maximum number of passes over the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mini batch size**: The size of the mini-batches used during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate**: The pace at which the loss is optimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout rate**: For each epoch, the percentage of hidden neurons that are
    not updated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early stopping patience**: The training stops after a designated number of
    unsuccessful epochs, those in which the loss doesn''t improve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference**:For a given department, we sent 134 weeks of historical sales,
    along with the department category and holiday flag across all of the weeks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following, is a sample JSON output from the model endpoint. Because DeepAR
    produces probabilistic forecasts, the output contains several sales samples from
    the Gaussian distribution. Mean and quantiles (50% and 90%) of these samples are
    also reported, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have just reviewed sample input and output of the DeepAR algorithm for items
    with historical weekly sales.
  prefs: []
  type: TYPE_NORMAL
- en: DeepAR also offers unique capabilities that account for complexities in real-world
    time series problems. For new items or products, the length of time series is
    going to be shorter than that of regular items that have full sales history. DeepAR
    captures the distance to the first observation for new items or products. Because
    the algorithm learns item demand across multiple time series, it can estimate
    demand even for newly introduced items—the length of weekly sales across all time
    series need not remain the same. Additionally, the algorithm can also handle missing
    values, with missing values replaced with "Nan".
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot is a visual representation of the variety of inputs
    and output of DeepAR:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5279d84f-a5f5-47a3-b9b3-ce0fa1a0b37c.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding, the probabilistic forecasts of weekly sales can be
    produced by modeling historical weekly sales (Sales Time Series) across all new
    (Age) and regular items, along with taking into input item category (Item Embedding)
    and other features (Price and Promotion).
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although there are 45 stores, we will select one store, store number 20, to
    analyze performance across different departments across three years. The main
    idea here is that, using DeepAR, we can learn the sales of items across different
    departments.
  prefs: []
  type: TYPE_NORMAL
- en: In SageMaker, through Lifecycle Configurations, we can custom install Python
    packages before notebook instances are started. This eliminates the need to manually
    track packages required before the notebooks are executed.
  prefs: []
  type: TYPE_NORMAL
- en: For exploring the retail sales data, we will need the latest version, 0.9.0,
    of `seaborn` installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In SageMaker, under Notebook, click on Lifecycle Configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under Start notebook, enter the command to upgrade the `seaborn` Python package,
    as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/18891f22-8ceb-4aba-bc86-973064a16de3.png)'
  prefs: []
  type: TYPE_IMG
- en: Edit the notebook settings by clicking on the notebook instance, selecting Actions,
    and picking Update Settings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under the Update Settings a Lifecycle configuration section, select the name
    of the newly created Lifecycle configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This option enables SageMaker to manage all Python pre-requisites before the
    notebook instances are made available, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fe80c03-6ce2-40b4-b56e-073781fb8a1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s merge the data across the sales, store, and features CSV files:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will import the key Python libraries, as shown in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s read the `.csv` files into Python DataFrames, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the shape of each of the DataFrames created, as in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, merge the `features` DataFrame with `sales` and `stores` to create one
    DataFrame containing all of the required information, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert `IsHoliday` into numerical form and convert the `Date` field into the
    `pandas` date format, as in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Write merged dataset to `.csv` with the help of the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the distribution of each of the key factors (`Temperature`,
    `Fuel_Price`, `Unemployment`, and `CPI`) that may impact sales, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `seaborn` Python library to plot the distribution of `Temperature`,
    `Fuel_Price`, `Unemployment`, and `CPI` in the dataset. Let''s have a look at
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e79d837-9b0b-4a1a-a47e-450670f67576.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As can be seen from the preceding distributions, the temperature is mostly
    between 60 to 80 degrees when the sales happened. Also, fuel prices were around
    $2.75 and $3.75 during the majority of the sales activity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50c88a7c-8426-42e7-b808-58eeb3ca6090.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding visual, unemployment rate was between 6% and 9% during the
    majority of the sales activity. As for CPI, sales activity occurred during both
    low and high CPI levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have looked at the distribution of each of the key features, let''s
    see how they are correlated to weekly sales:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the scatter plot between sales (target) and each of the
    explanatory variables— `Holidays`, `Temperature`, `CPI`, `Unemployment`, and `Store
    Type`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we''ve plotted a scatterplot between sales and fuel
    price and sales and temperature. Let''s analyze how fuel price and temperature
    are related to sales:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3a863f8-ee4a-4592-a7e3-70b6a3fe6a58.png)'
  prefs: []
  type: TYPE_IMG
- en: It is evident from the preceding visual that the fuel price between $3.25 and
    $3.75 is generating higher weekly sales. Also, a temperature between 50 and 65
    degrees is generating higher weekly sales.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s now plot holiday or not and CPI against sales, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at how sales vary with a holiday or not and CPI in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cba77cff-6337-4dbd-a0ac-5e2fff453593.png)'
  prefs: []
  type: TYPE_IMG
- en: It seems that holiday sales are higher than non-holiday sales. Also, there appears
    to be no material impact of CPI on weekly sales.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s now plot `Unemployment` and `Store Type` against sales, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how sales vary with unemployment and store type in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f4b38ce-74b3-403d-b16a-108a48bf2fb4.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding visual, weekly sales appear to be higher when unemployment
    rate is lower (7 to 8.5) and B type stores seem to have higher weekly sales.
  prefs: []
  type: TYPE_NORMAL
- en: Second, let's look at a heatmap across all of the features to identify what
    features impact sales. Let's draw a heatmap to see correlations between sales
    and several sales predictors all in one go.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot is a heatmap of numerical attributes in the dataset—we
    drop store and department from the dataset since they are categorical variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3d3ab63-2650-4b62-a1fb-12341db2f89b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the scatter plot and heat map, the following is apparent:'
  prefs: []
  type: TYPE_NORMAL
- en: Markdowns are happening during holidays.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sales are higher during holidays.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type B stores generate higher sales.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lower fuel prices (between $3 and $3.75) generate higher sales.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ideal temperature (between 50 and 65 degrees) generates higher sales.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For our further modeling, we will pick the best performing store, store 20,
    to model sales across different departments and years. For each of the time steps
    in the time series, we will also pass whether a particular day was observed as
    a holiday or not.
  prefs: []
  type: TYPE_NORMAL
- en: Data pre-processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s begin with preparing the dataset for modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a module named `retailsales.py` to create JSON files that DeepAR can
    consume for training and validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a module named `salesinference.py` to build inference data and retrieve
    and plot predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For details on the modules, please refer to the source code associated with
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To modularize the code to test DeepAR, we will package the two modules, `retailsales`
    and `salesinference`. To package the modules, we will create the `__init__.py`
    file to import the modules. We will then create `setup.py`, detailing the pre-requisite
    packages to be installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following, is the folder structure of the DeepAR project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In `setup.py`, we will define pre-requisite packages to be installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In `_init_.py`, we will import the modules, `retailsales` and `salesinference`,
    defined earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now install the package for the modules to be available while training
    DeepAR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready with all of the packages required to pre-process weekly sales
    data. Pre-processing included not only converting categorical data into numerical,
    but also creating training and testing data in JSON formats required by the DeepAR
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Training DeepAR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will fit DeepAR to the weekly sales. Let's start by preparingtraining
    and test datasets in JSON format*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following code, which demonstrates the creation of
    `json` lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we have created JSON lines for training and testing
    the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: The `prepareSalesData()` function is used to select departments with sales across
    all of the 143 weeks. This step is to ensure that there were no missing values
    in the data. Although DeepAR can handle missing values, we've tried to make the
    problem less complex by only considering departments that have sales in almost
    all weeks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use department numbers to group or categorize the time series for the DeepAR
    algorithm. This grouping will be used by DeepAR to make demand predictions by
    department.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `getTestSales()` function is used to create JSON lines for the testing dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `getTrainSales()` function, on the other hand, is used to create JSON lines
    for the training dataset, which is a subset of the testing dataset. For each of
    the departments, we will chop the last nine weekly sales determined by prediction
    length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will look at uploading the `json` files to the S3 bucket, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the newly created `json` files are uploaded to the designated
    S3 bucket via the `upload_data()` function from the Sagemaker session object (Sagemaker
    Python SDK).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be obtaining the URI of the DeepAR Docker image with the help of the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code block, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `get_image_uri()` function from the SageMaker estimator object is used to
    obtain `uri` of the DeepAR Docker image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once `uri` is obtained, the DeepAR estimator is created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The constructor parameters include the Docker image `uri`, execution role, training
    instance type and count, and `outpath` path to save the trained algorithm and
    SageMaker session.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hyperparameters are used to configure the learning or training process. Let''s
    have a look at `hyperparameters` used in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we came across the following hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate`: Defines how fast the weights are updated during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout_rate`: To avoid overfitting, for each iteration, a random subset of
    hidden neurons are not updated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_cells`: Defines the number of cells to use in each of the hidden layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_layers`: Defines the number of hidden layers in the RNN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time_freq`: Defines the frequency of time series.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs`: Defines the maximum number of passes over the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`context_length`: Defines look back period—how many data points are we going
    to look at before predicting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prediction_length`: Defines the number of data points to predict.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mini_batch_size`: Defines how often weights are updated—that is, weights are
    updated after processing the designated number of data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following code, we fit `deepAR` to the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We passed the location of the training and testing JSONs on the S3 bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The testing dataset is used to evaluate the performance of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For training, we called the `fit()` function on the DeepAR estimator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following is the output from training DeepAR:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As it is seen in the preceding output, the **Root Mean Squared Error** (**RMSE**
    ) is used as a metric to pick the best performing model.
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully trained the DeepAR model on our training dataset, which
    had 134 weekly sales. To fit training data to the model, we have defined the location
    of training and testing JSONs on the S3 bucket. Also, we've defined hyperparameters
    to control the learning or fitting process. The best performing model (based on
    the lowest RMSE—in that predicted sales are close as possible to actual sales)
    is then persisted.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting and evaluating sales
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, the trained model will be deployed, so that we can predict
    weekly sales for the next nine weeks for a given department.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the following code :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the `deploy` function of the `deepAR` estimator is used
    to host the model as an endpoint. The number and type of hosting instances should
    be specified through the following parameters :'
  prefs: []
  type: TYPE_NORMAL
- en: '`initial_instance_count`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`instance_type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To assess the model performance, we use department number 90, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The `buildInferencedata()` function is used to prepare the time series data
    in JSON format. We build inference data, by a given department, listing holidays
    across the entire 143 weeks, weekly sales for 134 weeks, and corresponding item
    category. The goal here is to estimate sales in the last nine weeks, where `9`
    is the prediction length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following is a JSON sample produced by the `buildInferenceData` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd96649e-fd17-4eb9-8edb-d8b3385901f1.png)'
  prefs: []
  type: TYPE_IMG
- en: The SageMaker predictor object is used for inference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `getInferenceSeries()` function is used to parse the JSON results from the
    DeepAR algorithm to identify mean sales, sales in the 10 percentile, and sales
    in the 90 percentile. Note that, using Gaussian distribution, DeepAR generates
    100 samples of weekly sales for the next nine weeks. Therefore, sales in 10 percentile
    and 90 percentile indicate the lower and upper bounds of weekly sales during the
    prediction period.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results returned from the endpoint are then plotted against actual sales
    via the `plotResults()` function. For each of the nine weeks, we will look at
    mean sales, ground truth sales, sample sales, 10 percentile sales, and 90 percentile
    sales.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As shown in the following, the mean estimated sales are close to the actual
    sales, indicating that the DeepAR algorithm has adequately picked up sales demand
    across different departments. Change the department number to evaluate model performance
    across all departments. The probabilistic sales estimates hence enable us to estimate
    demand more accurately than point estimates. Here is the output of the preceding
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following graph, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The blue line indicates mean sales from the prediction of nine weeks in the
    future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The purple line, on the other hand, reflects ground truth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The two lines are close enough, indicating that the model has done a decent
    job capturing patterns in sales, given holidays and historical weekly sales:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/22eef7f3-f8c1-45b0-b12e-e1bbbe498629.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have only looked at store 20 sales. However, you can train on all store
    sales by including the store number in the category list—for each time series
    in the train and test sets, include the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: With a large number of time series across different products and stores, we
    would have been able to achieve better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we briefly looked at univariate time series forecasting techniques,
    such as ARIMA and exponential smoothing. However, as demand varies by multiple
    variables, it becomes important to model multi-variate series. DeepAR enables
    modeling of multi-variate series, along with providing probabilistic forecasting.
    While point estimates may work in some situations, probabilistic estimates provide
    better data for improved decision making. The algorithm works by generating a
    global model that is trained across a large number of time series. Each item or
    product across several stores and departments will have its own weekly sales.
    The trained model accounts for newly introduced items, missing sales per item,
    and multiple predictors that explain sales. With the LSTM network and Gaussian
    likelihood, DeepAR in SageMaker provides a flexible approach to demand forecasting.
    Additionally, we walked through model training, selection, hosting, and inference
    in SageMaker through the SageMaker Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Now, that we've experienced SageMaker's capabilities to solve demand forecasting
    at scale, in the next chapter, we will walk through model monitoring and governance
    and will learn about why models degrade in production.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Overview of a variety of univariate time series forecasting methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-Python/](https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/unboxing-arima-models-1dc09d2746f8](https://towardsdatascience.com/unboxing-arima-models-1dc09d2746f8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Details on how the DeepAR algorithm works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_how-it-works.html](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_how-it-works.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Details on DeepAR inference formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
