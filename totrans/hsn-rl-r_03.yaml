- en: Building Blocks of Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main objective of algorithms based on reinforcement learning is to learn
    and adapt to environmental changes. To do this, we use external feedback signals
    (reward signals) generated by the environment according to the choices made by
    the algorithm. In this context, the right choice that's suggested by the algorithm
    will provide a reward while a wrong choice will result in a penalty. All of this
    is done in order to achieve the best result possible. In this chapter, you will
    discover agent-environment interface concepts for building models. By the end
    of this chapter, you will be ready to dive into working on the Markov decision
    process. We will also discover the fundamental concepts of the policy and how
    to improve the results by applying a policy gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Agent-environment interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Markov decision process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining the policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring policy gradient methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agent-environment interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In reinforcement learning, the agent learns and adapts to environmental changes.
    The basis of this programming technique arises from the concept of receiving external
    stimuli according to the choices of the algorithm. A correct choice returns a
    reward while an incorrect choice returns a penalty. The objective of the system
    is to achieve the best possible result.
  prefs: []
  type: TYPE_NORMAL
- en: These mechanisms derive from the basic concepts of machine learning (learning
    from experience) in an attempt to simulate human reasoning. In fact, in our mind,
    we activate brain mechanisms that lead us to chase and repeat what produces feelings
    of gratification and well-being. Whenever we experience moments of pleasure (food,
    music, art, and so on), our brain produces some substances that work by reinforcing
    that same stimulus, emphasizing it. Along with this mechanism of neurochemical
    reinforcement, our memory remembers this experience so that we can recreate how
    we feel in the future. Evolution has provided us with this mechanism so that we
    can repeat experiences that are rewarding to us.
  prefs: []
  type: TYPE_NORMAL
- en: This is why we remember the important experiences of our lives, especially those
    that are powerfully rewarding, are part of our memories, and condition our future
    explorations. Learning from experience can be simulated by a numerical algorithm
    in various ways, depending on the nature of the signal that's used for learning
    or the type of feedback that's returned by the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In supervised learning, there is a teacher who tells the system what the correct
    output is. In reality, it isn''t always possible to have a tutor who guides us
    in our choices. Often, we only have qualitative information that gives us feedback
    on how the environment responds to our actions. This information is called reinforcement
    signals. The system doesn''t give us any information about how to update the agent''s
    behavior (that is, the weights). You cannot define a cost function or a gradient.
    The goal of the system is to create smart agents that are able to learn from their
    experience. In the following diagram, we can see a flowchart that displays how
    reinforcement learning interacts with the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2b34caf-debe-423d-ad95-4a606f973573.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Scientific literature has taken an uncertain stance on the classification of
    learning by reinforcement as a paradigm. In fact, in its initial phase, it was
    considered as a special case of supervised learning before it was fully promoted
    as the third paradigm of machine learning algorithms. It is applied in different
    contexts in which supervised learning is inefficient: the problems of interacting
    with the environment is a clear example of this.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to follow these steps to correctly apply a reinforcement learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the optimal strategy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the actions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the corresponding reward (or penalty).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop updated strategies (if necessary).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2 - 5* iteratively until the agent learns the optimal strategies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reinforcement learning is based on the psychological theory that came about
    due to a series of experiments that were performed on animals. In particular,
    Edward Thorndike (an American psychologist) noted that if a cat is given a reward
    immediately after it behaves in a way that's considered correct, it increases
    the probability that the cat will repeat this behavior. In the face of unwanted
    behavior, however, applying punishment decreases the probability that the cat
    will repeat the behavior.
  prefs: []
  type: TYPE_NORMAL
- en: On the basis of this theory, reinforcement learning tries to maximize the rewards
    that are received when an action or set of actions are executed so that a certain
    goal can be reached.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning can be seen as a special case of the interaction problem
    for achieving a goal. The entity that must reach the goal is called an agent.
    The entity that the agent must interact with is called the environment, which
    corresponds to everything that is external to the agent.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have focused on the term agent, but what does it represent? The agent
    (software) is a software entity that performs services on behalf of another program,
    usually automatically and invisibly. These pieces of software are also called
    smart agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the most important features of an agent:'
  prefs: []
  type: TYPE_NORMAL
- en: It can choose an action to perform on the environment that's either continuous
    or discrete.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The action that's performed depends on the situation. The situation is summarized
    in the system state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent continuously monitors the environment (input) and continuously changes
    the status.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of the action isn't trivial and requires a certain degree of "intelligence".
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent has a smart memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent has a goal-directed behavior, but how it acts in an uncertain environment
    isn't known beforehand. An agent learns by interacting with the environment. Planning
    can be developed while the agent is learning about the environment through the
    measurements it makes. This strategy is close to the trial-and-error theory.
  prefs: []
  type: TYPE_NORMAL
- en: The trial and error theory is a crucial method of problem-solving. The trial
    is repeated until the agent is successful or until the agent stops trying.
  prefs: []
  type: TYPE_NORMAL
- en: The agent-environment interaction is continuous since the agent chooses an action
    to be taken and, in response, the environment changes state by presenting a new
    situation that the agent will be faced with.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of reinforcement learning, the environment provides the agent with
    a reward. It is essential that the source of the reward is the environment to
    avoid the formation of a personal reinforcement mechanism that would compromise
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: The value of the reward is proportional to the influence that the action has
    in reaching the objective, so it is positive or high in the case of a correct
    action or negative or low for an incorrect action.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some examples from real life where an agent and environment
    have interacted to solve a certain problem:'
  prefs: []
  type: TYPE_NORMAL
- en: A chess player, where each move provides information about the possible countermoves
    of the opponent can make.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A little giraffe that, in a few hours, can learn to get up and run at 50 km/h.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A truly autonomous robot learns to move in a room so that it can get out of
    it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameters of a refinery (oil pressure, flow, and so on) are set in real-time
    so that we can obtain the maximum yield or maximum quality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of these examples have the following characteristics in common:'
  prefs: []
  type: TYPE_NORMAL
- en: Interaction with the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The objective of the agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncertainty or partial knowledge of the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From this, it is possible to make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent learns from its own experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The actions change the status (the situation) and how many changes can be made
    in the future (delayed reward).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The effect of an action cannot be completely predicted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent has a global assessment of its behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent must exploit this information to improve its choices. These choices
    improve with experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems can have a finite or infinite time horizon.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essentially, the agent receives sensations from the environment through its
    sensors. Depending on its feelings, the agent decides what actions to take in
    the environment. Based on the immediate result of its actions, the agent may be
    rewarded.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to use an automatic learning method, you need to give a formal description
    of the environment. It isn't important to know exactly how the environment is
    made – what's interesting is to make general assumptions about the properties
    that the environment has. In reinforcement learning, it is usually assumed that
    the environment can be described by a Markov decision process. Let's learn how.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Markov decision process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To avoid load problems and computational difficulties, the agent-environment
    interaction is considered as a Markov decision process. A **Markov decision process**
    is a discrete time stochastic control process.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic processes** are mathematical models that are used to study the
    evolution of phenomena following random or probabilistic laws. In all-natural
    phenomena, it is known that, by their very nature and by the observation errors,
    a random or accidental component is present. This component states that at every
    instant, *t*, the result of observing the phenomenon is a random number or random
    variables, *st*: it isn''t possible to predict with certainty what the result
    will be; we can only state that it will take one of several possible values, each
    of which has a given probability.'
  prefs: []
  type: TYPE_NORMAL
- en: A stochastic process is deemed Markovian when a certain instant, *t*, of the
    observation is chosen, the evolution of the process, starting with *t*, depends
    only on *t* while it doesn't depend on the previous instants in any way. Thus,
    a process is Markov when, given the moment of observation, the instant determines
    the future evolution of the process, while this evolution doesn't depend on the
    past.
  prefs: []
  type: TYPE_NORMAL
- en: In a Markov process, at each time step, the process is in some state, *s ∈ S*,
    and the agent may choose any action, *a ∈ A*, that is available in state *s*.
    The process responds at the next time step by randomly moving into a new state,
    *s'*, and giving the agent a corresponding reward *r(s,s')*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see the agent-environment interaction in a
    Markov decision process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86d11815-26cd-481c-a7b4-5808afc8b778.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The agent-environment interaction shown in the preceding diagram can be summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent and the environment interact at discrete intervals over time, t =
    0, 1, 2… n.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At each interval, the agent receives a representation of the state *st* of the
    environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each element *st S*, where *S* is the set of possible states.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the state is recognized, the agent must take an action at *A(st)*, where
    *A(st)* is the set of possible actions in the state, *st*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of the action to be taken depends on the objective to be achieved
    and is mapped through the policy indicated with the symbol *π* (discounted cumulative
    reward), which associates the action with *A(s)* for each state, *s*. The term
    *πt(s,a)* represents the probability that action *a* is carried out in the state,
    *s*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the next time interval, *t + 1*, as part of the consequence of the action,
    the agent receives a numerical reward, *rt + 1 R*, corresponding to the action
    that was taken previously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, the consequence of the action represents the new state, *st*. At this point,
    the agent must code the state and make a choice in terms of the action that will
    take place.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This iteration repeats itself so that the objective is achieved by the agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The definition of the status *st + 1* depends on the previous state and the
    action taken (MDP), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86ddc6a6-2400-4f4b-a2ae-8365595f33de.png)'
  prefs: []
  type: TYPE_IMG
- en: In the formula, δ represents the status function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we can state the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In a Markov decision process, the agent can perceive the status, *s S* that
    they're in and has a set of actions at their disposal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At each discrete interval *t* of time, the agent detects the current status
    *st* and decides to implement an action at *A*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment responds by providing a reward (a reinforcement) *rt = r (st,
    at)* and moving into the state *st + 1 = δ (st, at)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The r and δ functions are part of the environment; they only depend on the current
    state and action (not the previous ones) and are not necessarily known to the
    agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of reinforcement learning is to learn a policy that, for each state,
    *s,* in which the system is located, specifies an action to the agent so that
    it can maximize the total reinforcement it receives during the entire action sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s talk about some of the terms we used in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: A reward function defines the goal in a reinforcement learning problem. It maps
    the detected states of the environment into a single number, thus defining a reward.
    As we mentioned previously, the only goal is to maximize the total reward it receives
    in the long term. The reward function decides which actions are positive and negative
    for the agent. The reward function has the need to be correct, and it can be used
    as a basis for changing the policy. If an action that's suggested by the policy
    returns a low reward, in the next step, the policy can be modified to suggest
    other actions in the same situation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A policy defines the behavior of the learning agent at a given time. It maps
    both the detected states of the environment and the actions to take when they
    are in those states. This corresponds to what would be called a set of rules or
    associations of stimulus response in psychology. The policy is the fundamental
    part of a reinforcing learning agent in the sense that it alone is enough to determine
    behavior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A value function represents how good a state is for an agent. It is equal to
    the total reward that's expected for an agent from the status, *s*. The value
    function depends on the policy that the agent selects for the actions to be performed
    on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An action-value function returns the value, that is, the expected return (overall
    reward) for using action, *a*, in a certain state, *s*, following a policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will learn how to maximize the total reinforcement that's
    received during the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Discounted cumulative reward
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Markov decision process* section, we mentioned that the goal of reinforcement
    learning is to learn a policy that, for each state, *s*, in which the system is
    located, specifies an action to the agent so that it can maximize the total reinforcement
    they receive during the entire action sequence. How can we maximize the total
    reinforcement that's received during the entire sequence of actions?
  prefs: []
  type: TYPE_NORMAL
- en: 'The total reinforcement that''s derived from the policy is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce3c5ab6-1900-46e3-aa37-fcbac40d0e9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, r[T] represents the reward of the action that drives the environment in
    the terminal state, s[T].
  prefs: []
  type: TYPE_NORMAL
- en: A possible solution to this problem is to associate the action that provides
    the highest reward to each individual state; that is, we must determine an optimal
    policy so that the previous quantity is maximized.
  prefs: []
  type: TYPE_NORMAL
- en: For problems that don't reach the goal or terminal state in a finite number
    of steps (continuing tasks), Rt continues to infinity.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, the sum of the rewards that we want to maximize diverges at
    the infinite, so this approach is not applicable. Due to this, it is necessary
    to develop an alternative reinforcement technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'The technique that best suits the reinforcement learning paradigm turns out
    to be the discounted cumulative reward, which tries to maximize the following
    quantity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4f284ea-0813-4784-815b-48af5f19b057.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, γ is called a discount factor and represents the importance of future
    rewards. This parameter can take the values 0 ≤  γ ≤ 1, which have the following
    meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: If γ <1, the sequence, *rt*, will converge to a finite value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If γ = 0, the agent will have no interest in future rewards but will try to
    maximize the reward for the current state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If γ = 1, the agent will try to increase future rewards, even at the expense
    of the immediate ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discount factor can be modified during the learning process to highlight
    particular actions or states. An optimal policy can lead to the reinforcement
    that's obtained when performing a single action to be low (or even negative),
    provided that this leads to greater reinforcement. Exploring the environment is
    the right approach when you want to gather useful information. However, in some
    cases, it is also a computationally expensive process, so let's look at how to
    deal with this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration versus exploitation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ideally, the agent must associate with each action at the respective reward,
    *r*, in order to choose the most rewarded behavior for achieving the goal. This
    approach is impractical for complex problems in which the number of states is
    particularly high and the possible associations increase exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: This problem is called the **exploration-exploitation** dilemma. Ideally, the
    agent must explore all the possible actions for each state and find the one that
    is rewarded the most when it's exploited.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, decision-making involves a fundamental choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploitation**: This makes the best decision given the current information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration**: This collects more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this process, the best long-term strategy can lead to considerable sacrifices
    in the short term. Therefore, it is necessary to gather enough information to
    make the best decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The exploration-exploitation dilemma has something to offer whenever we try
    to learn something new. Often, we have to decide whether to choose what we already
    know (exploitation), thus leaving our cultural baggage unaltered, or choose something
    new and learn in this way instead (exploration). The second choice risks us making
    the wrong choices. This is an experience that we have often faced; think, for
    example, about the choices we make in a restaurant when we are asked to choose
    between the dishes on the menu:'
  prefs: []
  type: TYPE_NORMAL
- en: We can choose something that we already know about and that, in the past, has
    given us back a known reward with gratification (exploitation), such as pizza
    (who doesn't know the goodness of a Margherita pizza?).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can try something new that we have never tasted before and see what we get
    (exploration), such as lasagne (alas, not everyone knows the magical taste of
    a lasagne bowl).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The choice we make will depend on many boundary conditions: the price of the
    dishes, our level of hunger, our knowledge of dishes, and so on. What''s important
    is that studying the best way to make this kind of choice has demonstrated that
    optimal learning requires that we sometimes make bad choices. This means that,
    sometimes, you have to choose to avoid the action you deem the most rewarding
    and take an action that you feel is less rewarding. The logic is that these actions
    are necessary to obtain a long-term benefit: sometimes, you need to get your hands
    dirty to learn more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some more examples of adopting this technique in real-life cases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Selecting a store:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploitation**: Go to your favorite store'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration**: Try a new store'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choosing a route:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploitation**: Choose the best route you know of'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration**: Try a new route'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, in very complex problems, converging a very good strategy would
    be too slow. A good solution to this problem is to find a balance between exploration
    and exploitation:'
  prefs: []
  type: TYPE_NORMAL
- en: An agent who limits itself to exploring will always act in a casual way in every
    state and it is evident that converging to an optimal strategy is impossible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an agent explores a little, it will always use the actions it would always
    use, which may not be optimal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At every step, the agent has to choose between repeating what they have done
    so far or trying out new movements that could achieve better results.
  prefs: []
  type: TYPE_NORMAL
- en: Policies are essential when it comes to choosing an action to perform. In the
    next section, we will look at this further by analyzing the different approaches
    that can be used while searching for the best possible policy.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned in the *Markov decision process* section, a policy defines the
    behavior of the learning agent at a given time. It maps both the detected states
    of the environment and the actions to take when they are in those states. The
    policy is the fundamental part of a reinforcing learning agent in the sense that
    it alone is enough to determine behavior. The policy is essential in the choices
    that the agent is required to make. In fact, once the observation has been obtained,
    the decision on what to do next is made on the basis of the policy. In this case,
    we don't need the value of the state or a particular action – we simply need the
    policy that considers the total reward.
  prefs: []
  type: TYPE_NORMAL
- en: Policies can be deterministic when the same action is taken for a given state,
    or probabilistic when the action is chosen based on some distribution calculation
    between the shares and the given state.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fully understand the meaning of the policy, let''s look at an example. Suppose
    we need to implement an algorithm to drive a delivery vehicle from the shop to
    the customer''s home. We can define the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: The road map is the environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current position of the vehicle is a state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The policy is what the agent does to accomplish this task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we provide some examples of policies that our vehicle could adopt in order
    to complete a task, such as the delivery of goods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy 1**: Uncontrolled vehicles would move randomly until they accidentally
    end up in the right place (the customer''s home). Here, it is possible that a
    lot of fuel will be consumed and that the delivery process will last a long time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy 2**: Other vehicles could learn to travel on only the main roads,
    thus traveling more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy 3**: The controlled vehicles will plan the route by choosing a route
    that will take them to their destination so that they drive fewer roads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obviously, some policies are better than others and there are many ways to evaluate
    them, namely the function value state and the function's value-action. The goal
    is to learn the best policy. The policy can be approached in two ways: **policy
    iteration** and **policy search**. The main difference between these two techniques
    is in the use of the value function. In the upcoming sections, we will analyze
    both of these approaches in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Policy iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Policy iteration is a dynamic programming algorithm that uses a value function
    to model the expected return for each pair of action-states. Many techniques in
    reinforcement learning are based on this technique, including Q-learning, TD-learning,
    SARSA, QV-learning, and more. These techniques update the value functions using
    the immediate reward and the (discounted) value of the next state in a process
    called **bootstrap**. Therefore, they imply the storage of *Q (s, a)* in tables
    or with approximate function techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Policy iteration is generally applied to discrete Markov decision processes,
    where both the state space *S* and the action space *A* are discrete and finite
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting from an initial P[0] policy, the iteration of the policy alternates
    between the following two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy evaluation**: Given the current policy P, estimate the action-value
    function, Q[P].'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Policy improvement**: Calculate a better policy P '' based on Q[P], then
    set P'' as the new policy and return to the previous step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the action-value function can be calculated for each action-state pair,
    the policy iteration with the greedy policy improvement leads to convergence by
    returning the optimal policy. Essentially, repeatedly executing these two processes
    converges the general process toward the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the Q[P] value function can't always be calculated exactly; generally,
    it can be estimated from samples. In these cases, it is necessary to include a
    certain degree of randomness in the policy to ensure sufficient exploration of
    the state's space of action. These algorithms store the value function in a finite
    table (tabular approach). The limitation of these algorithms is that they cannot
    be applied to the case of continuous Markov decision processes. Moreover, these
    methods can be unusable in some discrete cases where the cardinality of the state-action
    space is too high. Let's look at another approach to this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Policy search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the policy search approach, a parameterized policy is stored, but no value
    function is used or estimated. Policy search methods can rely on roll-out policies
    that use trajectory-based sampling. Other policy search methods use optimization
    techniques such as evolutionary algorithms to search for optimal policy parameters.
    Policy gradient methods are examples of policy search. Let's look at these in
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring policy gradient methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The policy gradient is a class of reinforcement learning algorithms based on
    the use of parameterized policies. The idea is to calculate the expected return
    gradient (reward) with respect to each parameter in order to change the parameters
    in a direction that increases the performance of the same. This method doesn't
    show the problems of traditional reinforcement learning such as the lack of guarantees
    of a value function, the problem resulting from the uncertainty of the state,
    and the complexities that arise from states and actions in continuous spaces.
    In the policy search method, no value function is used or estimated. The value
    function can be used to learn the policy parameter; however, it won't necessary
    for action selection. Policy gradient methods bypass all the problems that are
    connected to value function-based techniques by searching for the optimal policy
    directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pros of applying policy gradient methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous states and actions can be treated as discrete cases and learning
    performance is often increased.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's a great variety of different algorithms in the literature that have
    strong theoretical bases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State uncertainty doesn't degrade the learning process, even if no particular
    state estimator is used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The cons of applying policy gradient methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate can decide on the order of magnitude of the convergence speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They need to update the data very quickly to avoid the introduction of errors
    in the gradient estimator. This means that the use of sample data is not very
    efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, a policy gradient returns the direction in which we have to modify
    the parameters of our algorithm to improve the policy in order to maximize the
    total accumulated rewards. The gradient is equal to the gradient of the logarithmic
    probability of the action that's chosen. In other words, we are looking to increase
    the probability of actions that return a good total reward and to reduce the probability
    of actions with negative final results – we keep what works and leave out what
    it doesn't.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned in the *Markov decision process* section, the choice of the
    action to be taken depends on the objective to be achieved and is mapped through
    the policy indicated with the symbol π (discounted cumulative reward), which associates
    the action with a ∈ A(s) for each state, s. The term πt(s,a) represents the probability
    that action *a* is carried out in the state, *s*.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this procedure is to parameterize the policy using a θ parameter,
    which allows us to determine the best action, *a*, in a state, *s*. This policy
    function will be defined as πθ (s,a).
  prefs: []
  type: TYPE_NORMAL
- en: To find the optimal policy, we will use a neural network that will input the
    state and output the probability of each action in that state. This probability
    will be used to sample an action from this distribution and perform that action
    in the state. Nothing tells us that the sampled action is the correct action to
    perform in the state. Then, we perform the action and keep the reward. This procedure
    will be repeated for each state. The data that's obtained will be our training
    data. At this point, to update the gradients, we will use an algorithm based on
    the descent of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: By doing this, the actions that return a high reward in a state will present
    a high probability, while the actions with a low reward will have a low probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like in all gradient descent methods, the parameter vector is updated in the
    direction of the gradient of a performance measurement. In this case, the measurement
    for performance is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0d4a2fb-5f46-4f71-b547-677149d2a4a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding formula, *E* is the expected return and *r* is the reward.
    Our aim is to learn a policy that maximizes the cumulative future reward. The
    gradient of the expected return is called the policy gradient, and we''ll use
    it to update the θ parameter, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a939a7b-ece1-42c3-be19-90a141b1981f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: θ is the parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ∇ is the gradient of the expected return
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: α is the learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A parameter update is performed at each learning iteration. The gradient descent
    algorithm guarantees convergence to at least one local optimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'A general policy gradient algorithm can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will analyze some methods based on the policy gradient,
    that is, the **Monte Carlo policy gradient** and **actor-critic methods**.
  prefs: []
  type: TYPE_NORMAL
- en: The Monte Carlo policy gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Monte-Carlo policy gradient, also called **REINFORCE**, is a class of associative
    RL algorithms for networks connected to stochastic units. It has been shown that,
    in tasks with immediate reinforcement and, with some limitations, even in some
    tasks with delayed reinforcement, these algorithms perform parameter adjustments
    in a direction that lies along the gradient of the expected reinforcement. Because
    of their nature, these algorithms can easily be integrated with other gradient
    descent methods and with backpropagation in particular. The major drawback is
    that they are unable to distinguish between local (global) maximums (minimum)
    and don't have a general convergence theory.
  prefs: []
  type: TYPE_NORMAL
- en: In this family of algorithms, the agent generates a trajectory of an episode
    using its current policy and uses it to update the policy parameter. This algorithm
    provides an off-policy update since a complete trajectory must be completed to
    build a sample space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is the pseudocode for the REINFORCE algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, no explicit exploration is required. In this case, having calculated
    the probabilities with a neural network, the exploration is performed automatically.
    First, random weights are used to initialize the network and a uniform probability
    distribution is returned. This distribution is equivalent to the behavior of a
    random agent.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the variance of the gradient estimation while maintaining the bias,
    you can make a change to the REINFORCE algorithm by subtracting a base value from
    the return.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at another method based on the policy gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Actor-critic methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Actor-critic methods use policy search and value function estimation in order
    to carry out low-variance gradient updates. These methods separate the memory
    structure to make the policy independent of the value function. The policy block
    is known as an **actor** because it chooses actions, while the estimated value
    function block is known as a **critic** in the sense that it criticizes the actions
    that are performed by the policy that is being followed. From this, it is clear
    that learning is an on-policy type where the critic learns and criticizes the
    work of politics.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the critic is a function of state evaluation. After each selection
    of an action, the critic assesses the new state to determine whether things have
    gone better or worse than expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent performs two jobs and performs two roles with two different networks:'
  prefs: []
  type: TYPE_NORMAL
- en: The **actor** network is the one that establishes the action to be performed
    in a certain state by updating the policy parameters in the direction proposed
    by the critic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **critic** network is the one that evaluates the consequences of this action,
    modifying the function value of the next time step, and updating the value function
    parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment is perceived and measured by input sensors. The system processes
    these inputs within the evaluation/estimation of the state. The status estimate
    and any rewards are then communicated to the agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the pseudocode for the actor-critic algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As we can see the critic network and the actor network are updated at each step.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have learned about the basics of building a model based
    on Markov processes. A Markov decision-making process is a stochastic process
    characterized by five elements: epoch, states, actions, transition probabilities,
    and rewards. There''s also an agent present that controls the path of the stochastic
    process. At a certain point in the path and at a certain point, *t*, the agent
    intervenes and makes a decision that will influence the future evolution of the
    process. These moments are called epochs of decision, while the decisions that
    are made assume the connotation of actions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we discovered the policy that defines the behavior of the learning agent
    at a given time. It maps both the detected states of the environment and the actions
    to take when they are in those states. The policy is the fundamental part of a
    reinforcing learning agent in the sense that it alone is enough to determine behavior.
    The policy can be approached in two ways: **policy iteration** and **policy search**.
    The main difference between these two techniques is in their use of the value
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned how to build a model based on policy gradient methods, that
    is, a class of reinforcement learning algorithms based on the use of parameterized
    policies. The idea is to calculate the expected return gradient (reward) with
    respect to each parameter in order to change the parameters in a direction that
    increases the performance of the same.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see the Markov decision processes in action. We
    will get to grips with the concepts of the Markov decision process and understand
    the agent-environment interaction process. Then, we will learn how to use Bellman
    equations as consistency conditions for the optimal value functions to determine
    the optimal policy. Finally, we will discover and implement Markov chains and
    learn how to simulate random walks using them.
  prefs: []
  type: TYPE_NORMAL
