<html><head></head><body>
		<div id="_idContainer074">
			<h1 id="_idParaDest-37" class="chapter-number"><a id="_idTextAnchor036"/>3</h1>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Azure OpenAI Advanced Topics</h1>
			<p>In the preceding chapters, we’ve covered the basics of <strong class="bold">Azure OpenAI</strong> (<strong class="bold">AOAI</strong>) service, including model deployment and various pricing structures. Now, our attention will turn to exploring more advanced topics <span class="No-Break">within AOAI.</span></p>
			<p>In this chapter, we will delve into the following advanced <span class="No-Break">AOAI topics:</span></p>
			<ul>
				<li>AOAI model <span class="No-Break">context window</span></li>
				<li>AOAI <span class="No-Break">Embedding models</span></li>
				<li>Azure <span class="No-Break">vector databases</span></li>
				<li>AOAI On <span class="No-Break">Your Data</span></li>
				<li>AOAI <span class="No-Break">multimodal model</span></li>
				<li>AOAI <span class="No-Break">function calling</span></li>
				<li>AOAI <span class="No-Break">Assistants API</span></li>
				<li>AOAI <span class="No-Break">Batch API</span></li>
				<li><span class="No-Break">AOAI fine-tuning</span></li>
			</ul>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>AOAI model context window</h1>
			<p>In the world of <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>), a context window defines the amount of text a model can process at <a id="_idIndexMarker120"/>once, impacting how it generates and comprehends language. This window is measured by the number of tokens (either whole words or fragments), directly<a id="_idIndexMarker121"/> affecting how much information the model uses to predict the next token. In simple terms, it dictates how much context the model takes into account when forming predictions or <span class="No-Break">crafting responses.</span></p>
			<p>For example, the GPT-3.5-Turbo (0125) model context has 16,385 input tokens and 4,096 output tokens, while the GPT-4o and GPT-4o mini models have a much larger number at 128,000 input tokens and 16,384 output tokens. For information about the context window of AOAI models, <span class="No-Break">visit </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?#gpt-4-and-gpt-4-turbo-models"><span class="No-Break">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?#gpt-4-and-gpt-4-turbo-models</span></a><span class="No-Break">.</span></p>
			<p>The trend in LLMs is moving toward bigger context windows, which enable more detailed and coherent outputs. However, this comes at a cost: larger context windows require more computational power and memory. In practical terms, the context window defines how much of the prior conversation the model can “remember” during an interaction. When the conversation surpasses the context window, the model loses the earliest parts of the dialogue, potentially affecting its consistency in lengthy interactions or intricate tasks. As a result, the context window size is a crucial factor to consider when building applications<a id="_idIndexMarker122"/> that <span class="No-Break">utilize LLMs.</span></p>
			<p>While larger context windows enable LLMs to handle more data, they also come with substantial computational and financial costs. Processing extensive context lengths is extremely expensive and incredibly slow, which is only acceptable in limited cases. For instance, a context window of a million tokens could take nearly a minute to produce a single response when working with several <span class="No-Break">million tokens.</span></p>
			<p>In contrast, <strong class="bold">Retrieval-Augmented Generation</strong> (<strong class="bold">RAG</strong>) is more efficient because it fetches only the most relevant<a id="_idIndexMarker123"/> information for each query, reducing the number of tokens the model needs to process. This efficiency makes RAG a more cost-effective solution, particularly for applications requiring frequent or high-volume queries and data-intensive tasks. We’ll explain RAG in detail in a <span class="No-Break">later chapter.</span></p>
			<p>After text is split into tokens, each token is converted into a numerical form called an embedding. Embeddings are dense vector representations designed to capture the meaning of tokens. These vectors<a id="_idIndexMarker124"/> exist in a high-dimensional space, where the distance and direction between vectors can represent semantic and syntactic relationships between the words <span class="No-Break">they represent.</span></p>
			<p>In the next section, we’ll discuss the embedding model that AOAI uses to convert tokens into <span class="No-Break">embedding vectors.</span></p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>AOAI embedding models</h1>
			<p>AOAI has four different <a id="_idIndexMarker125"/>embedding models, and each model<a id="_idIndexMarker126"/> has specific limits for input tokens and <span class="No-Break">output dimensions:</span></p>
			<ul>
				<li><strong class="bold">text-embedding-ada-002 (</strong><span class="No-Break"><strong class="bold">version 2)</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Max input </strong><span class="No-Break"><strong class="bold">tokens</strong></span><span class="No-Break">: 8,191</span></li><li><strong class="bold">Output </strong><span class="No-Break"><strong class="bold">dimensions</strong></span><span class="No-Break">: 1,536</span></li></ul></li>
				<li><strong class="bold">text-embedding-ada-002 (</strong><span class="No-Break"><strong class="bold">version 1)</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Max input </strong><span class="No-Break"><strong class="bold">tokens</strong></span><span class="No-Break">: 2046</span></li><li><strong class="bold">Output </strong><span class="No-Break"><strong class="bold">Dimensions</strong></span><span class="No-Break">: 1536</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">text-embedding-3-large</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Max input </strong><span class="No-Break"><strong class="bold">tokens</strong></span><span class="No-Break">: 8191</span></li><li><strong class="bold">Output </strong><span class="No-Break"><strong class="bold">Dimensions</strong></span><span class="No-Break">: 3072</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">text-embedding-3-small</strong></span><span class="No-Break">:</span><ul><li><strong class="bold">Max input </strong><span class="No-Break"><strong class="bold">tokens</strong></span><span class="No-Break">: 8191</span></li><li><strong class="bold">Output </strong><span class="No-Break"><strong class="bold">Dimensions</strong></span><span class="No-Break">: 1536</span></li></ul></li>
			</ul>
			<p>text-embedding-3-small and text-embedding-3-large are the newest and most performant embedding models. They<a id="_idIndexMarker127"/> are now available <span class="No-Break">in AOAI.</span></p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>text-embedding-ada-002 (version 2)</h2>
			<p>The text-embedding-ada-002 model consolidates the functions of five different models used for searching text, comparing text similarity, and searching code. It performs better than our previous top<a id="_idIndexMarker128"/> model, Davinci, in most tasks and is 99.8% less expensive. This model has a longer context length of 8,192 compared to the previous version <span class="No-Break">of text-embedding-ada-002.</span></p>
			<p>You can use this embedding model using just a few lines of code, as you did with the previous version, by using our <a id="_idIndexMarker129"/>OpenAI <span class="No-Break">Python library:</span></p>
			<pre class="source-code">
from openai import AzureOpenAI
client = AzureOpenAI(
  api_key = os.getenv("AZURE_OPENAI_API_KEY"),
  api_version = "2024-04-01-preview",
  azure_endpoint =os.getenv("AZURE_OPENAI_ENDPOINT")
)
response= client.embeddings.create(input = "&lt;INPUT TEXT&gt;",
              model=&lt;deployment name&gt;).data[0].embedding
print (response)</pre>			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>text-embedding-3-small</h2>
			<p>text-embedding-3-small is a new, highly efficient embedding model, offering a significant upgrade over<a id="_idIndexMarker130"/> text-embedding-ada-002 (version 2), released in <span class="No-Break">December 2022.</span></p>
			<p>It boasts <span class="No-Break">improved performance:</span></p>
			<ul>
				<li>text-embedding-3-small outperforms text-embedding-ada-002 in <strong class="bold">multilingual retrieval</strong> (<strong class="bold">MIRACL</strong>), increasing the average <a id="_idIndexMarker131"/>score from 31.4% <span class="No-Break">to 44.0%.</span></li>
				<li>For <strong class="bold">English tasks</strong> (<strong class="bold">MTEB</strong>), the average score has improved from 61.0% <span class="No-Break">to 62.3%</span></li>
			</ul>
			<p>It also comes with lower costs. text-embedding-3-small is five times more cost-efficient than text-embedding-ada-002, reducing the price per 1,000 tokens from $0.0001 <span class="No-Break">to $0.00002.</span></p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>text-embedding-3-large</h2>
			<p>text-embedding-3-large is another new, next-generation embedding model, producing embeddings with up to <span class="No-Break">3,072 </span><span class="No-Break"><a id="_idIndexMarker132"/></span><span class="No-Break">dimensions.</span></p>
			<p>It comes with <span class="No-Break">improved performance:</span></p>
			<ul>
				<li>text-embedding-3-large is the <span class="No-Break">best-performing model</span></li>
				<li>On MIRACL, it improves the average score from 31.4% (text-embedding-ada-002) <span class="No-Break">to 54.9%</span></li>
				<li>On MTEB, the average score increases from 61.0% <span class="No-Break">to 64.6%</span></li>
			</ul>
			<p>As for the cost, text-embedding-3-large is slightly higher cost than text-embedding-3-small, with the price per 1,000 tokens set <span class="No-Break">at $0.00013.</span></p>
			<p>AOAI is not discontinuing text-embedding-ada-002 (version 2), so customers can continue using it if they prefer. However, it’s recommended to switch to the newer model for better <span class="No-Break">price performance.</span></p>
			<p>Both new embedding models use a technique called <strong class="bold">Matryoshka Representation Learning</strong> that lets you balance<a id="_idIndexMarker133"/> performance and cost. By shortening embeddings (removing some numbers from the end of the sequence) using the <strong class="source-inline">dimensions</strong> API parameter, you can reduce the size of the embeddings without losing <span class="No-Break">their meaning.</span></p>
			<p>For instance, on the MTEB benchmark, a text-embedding-3-large embedding can be shortened to 256 dimensions while still outperforming an unshortened text-embedding-ada-002 embedding with <span class="No-Break">1,536 dimensions.</span></p>
			<p>In general, it’s best to use the <strong class="source-inline">dimensions</strong> parameter when creating the embedding. If you need to change the dimensions afterward, ensure that the embedding is normalized, as <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
from openai import AzureOpenAI
import numpy as np
client = AzureOpenAI(
  api_key = os.getenv("AZURE_OPENAI_API_KEY"),
  api_version = "2024-04-01-preview",
  azure_endpoint =os.getenv("AZURE_OPENAI_ENDPOINT")
)
def normalize(x):
    x = np.array(x)
    if x.ndim == 1:
        norm = np.linalg.norm(x)
        if norm == 0:
            return x
        return x / norm
    else:
        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)
        return np.where(norm == 0, x, x / norm)
response = client.embeddings.create(
    model="&lt;DEPLOYMENT NAME&gt;",
    input="&lt;INPUT TEXT&gt;",
    encoding_format="float"
)
cut_dim = response.data[0].embedding[:256]
norm_dim = normalize(cut_dim)
print(norm_dim)</pre>			<p>Changing dimensions dynamically allows for flexible usage. For example, if a vector DB only supports embeddings up to 1,536 dimensions, developers can still use the best model, text-embedding-3-large, by setting the <strong class="source-inline">dimensions</strong> API parameter to <strong class="source-inline">1536</strong>. This reduces the embedding from 3,072 dimensions, sacrificing some accuracy for a smaller vector size. This<a id="_idIndexMarker134"/> leads us perfectly into discussing the vector search feature in the Azure AI Search service in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Azure vector databases</h1>
			<p>In the previous section, we explored various AOAI embedding models for generating vector embeddings. After creating these vectors, it’s essential to have a database optimized for storing and<a id="_idIndexMarker135"/> managing them effectively. The key distinction between a vector database and other types of databases is its capability to handle high-dimensional data. A vector database is specifically engineered to store data as high-dimensional vectors, which are mathematical representations of various features or attributes. Each vector comprises multiple dimensions, ranging from tens to thousands, depending on the data’s complexity and detail. These vectors are usually generated by applying transformation or embedding functions to raw data sources such as text, images, audio, video, and more. This type of database enables the indexing and querying of embeddings using vector search algorithms that assess vector distance or similarity. To ensure accurate retrieval of relevant information, a robust mechanism is necessary. Prominent vector<a id="_idIndexMarker136"/> search algorithms include <strong class="bold">Hierarchical Navigable Small World</strong> (<strong class="bold">HNSW</strong>), <strong class="bold">Inverted File</strong> (<strong class="bold">IVF</strong>), and DiskANN, <span class="No-Break">among </span><span class="No-Break"><a id="_idIndexMarker137"/></span><span class="No-Break">others.</span></p>
			<p>The primary benefit of a vector database is its ability to conduct rapid and precise similarity searches and data retrieval based on vector distance or similarity. Unlike traditional databases that depend on exact matches or predefined criteria for queries, a vector database allows for the identification of the most similar or relevant data based on semantic or contextual meanings. Here are<a id="_idIndexMarker138"/> some <span class="No-Break">practical applications:</span></p>
			<ul>
				<li><strong class="bold">Search engines</strong>: Vector databases transform search engines by enabling efficient similarity-based searches. They help find similar items, improve search relevance, and enhance <span class="No-Break">user experiences.</span></li>
				<li><strong class="bold">Image retrieval</strong>: Identify images that are similar to a given image based on visual content <span class="No-Break">and style.</span></li>
				<li><strong class="bold">Document search</strong>: Locate documents similar to a given document, considering factors such as topic <span class="No-Break">and sentiment.</span></li>
				<li><strong class="bold">Product recommendations</strong>: Discover products that are similar to a given product based on features <span class="No-Break">and ratings.</span></li>
				<li><strong class="bold">Semantic search</strong>: Vector databases<a id="_idIndexMarker139"/> enhance semantic search capabilities, allowing applications to find contextually related content. This makes them valuable for information retrieval, chatbots, and <span class="No-Break">question-answering systems.</span></li>
				<li><strong class="bold">Recommender systems</strong>: These systems benefit from vector databases by providing personalized product recommendations or content suggestions based on user preferences, thereby improving <span class="No-Break">recommendation accuracy.</span></li>
				<li><strong class="bold">Medical and scientific research</strong>: In fields such as genomics and chemistry, vector databases facilitate the analysis of genomic data, identification of chemical compound similarities, and acceleration of <span class="No-Break">scientific discoveries.</span></li>
			</ul>
			<p>To perform similarity <a id="_idIndexMarker140"/>search and retrieval in a vector database, a query vector encapsulating your desired information or criteria is needed. This query vector can originate from the same type of data as the stored vectors (e.g., using an image to query an image database) or from different types of data (e.g., using text to query an image database). The next step involves using a similarity measure to determine the proximity or distance between two vectors within the vector space. Various metrics can be employed for this purpose, such as cosine similarity, Euclidean distance, Hamming distance, and Jaccard index. The result of the similarity search and retrieval process is typically a ranked list of vectors that exhibit the highest similarity scores to the query vector. Subsequently, you can retrieve the associated data for each vector from the original source <span class="No-Break">or index.</span></p>
			<p>Azure provides six varieties of vector database <a id="_idIndexMarker141"/>options tailored to diverse needs and <span class="No-Break">use cases:</span></p>
			<ul>
				<li><strong class="bold">Azure AI Search</strong>: Azure AI Search <a id="_idIndexMarker142"/>is a powerful tool that features vector search capabilities. It utilizes the HNSW algorithm for vector searches and the <strong class="bold">Best Match 25</strong> (<strong class="bold">BM25</strong>) algorithm for full-text searches. Additionally, it offers a hybrid search option that merges the results from both full-text and vector queries, each using different ranking methodologies such as BM25 and HNSW. The <strong class="bold">Reciprocal Rank Fusion</strong> (<strong class="bold">RRF</strong>) algorithm is employed to consolidate these results, ensuring a single, cohesive result set that highlights the most relevant matches from the search index. In fact, OpenAI leverages Azure AI Search to enhance the <a id="_idIndexMarker143"/>capabilities of its ChatGPT application. By integrating Azure AI Search, OpenAI benefits from advanced search functionalities such as vector search, full-text search, and hybrid <a id="_idIndexMarker144"/>search, which combine the strengths of the BM25 and HNSW algorithms. This integration allows ChatGPT to deliver more accurate and relevant responses by efficiently retrieving and ranking information from a <span class="No-Break">search index.</span><p class="list-inset">However, it is important to note that Azure AI Search does not generate vector embeddings for your content; you are required to provide these embeddings yourself. One viable<a id="_idIndexMarker145"/> option for generating these embeddings is through AOAI <span class="No-Break">embedding models.</span></p><p class="list-inset">From an architectural standpoint, the AI Search service functions as an intermediary between external data stores that house your un-indexed data and the client application that sends query requests to a search index and manages <span class="No-Break">the responses.</span></p></li>
			</ul>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B21019_03_1.jpg" alt="Figure 3.1: Azure AI Search application architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1: Azure AI Search application architecture</p>
			<p class="list-inset">Within your client application, the search experience is crafted using Azure AI Search APIs, which can include features such as relevance tuning, semantic ranking, autocomplete, synonym matching, fuzzy matching, pattern matching, filtering, <span class="No-Break">and sorting.</span></p>
			<p class="list-inset">Azure AI Search also offers seamless integration with other Azure services. This is facilitated through indexers<a id="_idIndexMarker146"/> that automate data ingestion and retrieval from various azure data sources, as well as skillsets that incorporate<a id="_idIndexMarker147"/> AI functionalities from Azure AI services. These skillsets can include image and natural language processing, or custom code encapsulated within <span class="No-Break">Azure Functions.</span></p>
			<ul>
				<li><strong class="bold">Azure Cosmos DB NoSQL</strong>: Azure Cosmos <a id="_idIndexMarker148"/>DB provides integrated vector database capability, allowing you to <a id="_idIndexMarker149"/>store vectors within your documents alongside other data. This integration enables efficient <a id="_idIndexMarker150"/>and accurate vector searches at any scale, eliminating the need for a separate vector database. For generative AI applications, this capability is foundational, allowing developers to determine text string similarity and build powerful AI models. Additionally, Azure Cosmos DB offers single-digit millisecond response times, automatic scalability, and guaranteed <a id="_idIndexMarker151"/>performance at any scale, making it ideal for data-intensive workloads <span class="No-Break">like ChatGPT.</span><p class="list-inset">Azure Cosmos DB for NoSQL offers three types of vector <span class="No-Break">indexing methods:</span></p><ul><li><strong class="bold">Flat or k-nearest neighbors (kNN) exact search</strong>: Azure Cosmos DB stores vectors on the same index as other indexed properties, ensuring seamless integration and <span class="No-Break">efficient</span><span class="No-Break"><a id="_idIndexMarker152"/></span><span class="No-Break"> querying.</span></li><li><strong class="bold">Quantized flat index</strong>: Azure Cosmos DB quantizes (compresses) vectors before storing them in the index. This approach can improve latency and throughput, albeit with a minor trade-off <span class="No-Break">in accuracy.</span></li><li><strong class="bold">DiskANN</strong>: Azure Cosmos DB leverages DiskANN to create an index for fast and efficient approximate <span class="No-Break">vector searches.</span></li></ul><p class="list-inset">Vector search in Azure Cosmos DB can be combined with all other supported Azure Cosmos DB NoSQL query filters and indexes using <strong class="source-inline">WHERE</strong> clauses. This enables your vector searches to yield the most relevant data to <span class="No-Break">your applications.</span></p><p class="list-inset">This feature enhances the <a id="_idIndexMarker153"/>core capabilities of Azure Cosmos DB, making it more versatile for handling vector data and search requirements in <span class="No-Break">AI applications.</span></p></li>
				<li><strong class="bold">Azure Cosmos DB for MongoDB</strong>: Azure Cosmos <a id="_idIndexMarker154"/>DB for MongoDB also offers integrated vector database capabilities, allowing embeddings to be stored, indexed, and queried alongside other <a id="_idIndexMarker155"/>relational data. This feature enables efficient management and retrieval of vector data within MongoDB collections, enhancing the versatility of Azure Cosmos DB for diverse <span class="No-Break">application needs.</span><p class="list-inset">MongoDB vCore provides two types of vector <span class="No-Break">indexing methods:</span></p><ul><li><strong class="bold">IVF</strong>: IVF indexing is a<a id="_idIndexMarker156"/> method used in vector search to efficiently organize and manage large sets of vectors by clustering them into groups. Each cluster is represented by a centroid, or center point. During a search, the query vector is first compared to these centroids to identify the closest cluster. The search is <a id="_idIndexMarker157"/>then conducted within this specific cluster, significantly reducing the search space and improving retrieval times. This method balances speed and accuracy, making it ideal for applications requiring rapid and efficient <span class="No-Break">vector searches.</span></li><li><strong class="bold">HNSW</strong>: This is an advanced <a id="_idIndexMarker158"/>algorithm for <strong class="bold">approximate nearest neighbor</strong> (<strong class="bold">ANN</strong>) search in <a id="_idIndexMarker159"/>high-dimensional spaces, utilizing a multi-layer graph structure to organize data points. Each layer represents a different level of proximity, with higher layers containing fewer, more broadly representative points and lower layers containing more detailed points. This navigable graph supports efficient search by starting from the top layer and moving downward to find closer neighbors, leveraging “small world” properties for quick access across the dataset. HNSW provides fast and accurate ANN searches, making it ideal for applications such as recommendation systems, image<a id="_idIndexMarker160"/> retrieval, and natural <span class="No-Break">language processing.</span></li></ul></li>
				<li><strong class="bold">PostgreSQL pgvector</strong>: PostgreSQL lacks <a id="_idIndexMarker161"/>native vector capabilities, but with the <strong class="source-inline">pgvector</strong> extension available<a id="_idIndexMarker162"/> on Azure PostgreSQL flexible servers, you can seamlessly integrate vector capabilities into your PostgreSQL environment. <strong class="source-inline">pgvector</strong> is an open source extension tailored for PostgreSQL, enabling the storage and retrieval of vectors from natural language processing or deep learning models directly within PostgreSQL. What makes <strong class="source-inline">pgvector</strong> particularly appealing is its familiar SQL-based interface, mirroring traditional PostgreSQL operations for tasks such as creating vector columns, defining tables with vector columns, and performing nearest neighbor searches using L2 distance. Whether you’re developing AI applications, creating recommendation systems, or handling high-dimensional data, <strong class="source-inline">pgvector</strong> simplifies vector management within a familiar database framework, eliminating the need for specialized storage solutions and extensive vector <span class="No-Break">database expertise.</span></li>
				<li><strong class="bold">Azure SQL</strong>: Azure SQL Database<a id="_idIndexMarker163"/> now supports vector operations directly within the database, enabling efficient vector similarity searches. This capability, combined with full-text search and <a id="_idIndexMarker164"/>BM25 ranking, allows for the development of powerful search engines suitable for various applications. There are two methods to perform vector operations: the native option and the <span class="No-Break">classic option:</span><ul><li><strong class="bold">Native option</strong>: Utilize the newly introduced Vector Functions in Azure SQL Database. These functions are designed to perform vector operations directly within the database, providing a streamlined and <span class="No-Break">efficient approach.</span></li><li><strong class="bold">Classic option</strong>: Use traditional T-SQL for vector operations, leveraging columnstore indexes to achieve <span class="No-Break">high performance.</span></li></ul><p class="list-inset">Both options offer robust solutions for implementing vector searches, making Azure SQL Database a versatile tool for advanced <span class="No-Break">search scenarios.</span></p><p class="list-inset">Microsoft’s recent <a id="_idIndexMarker165"/>announcement of <em class="italic">SQL Server 2025</em> marks a significant step in database evolution, introducing it as an enterprise-ready <em class="italic">vector database</em>. This release integrates <em class="italic">built-in security and compliance</em>, emphasizing its focus on supporting enterprise-grade <em class="italic">AI solutions</em>. A standout feature is the <em class="italic">native vector store and index</em>, which is powered by DiskANN and leverages disk storage to execute high-performance searches across vast datasets. This functionality underpins semantic <a id="_idIndexMarker166"/>searching, enabling efficient chunking and accurate data retrieval – a critical feature for AI-driven insights. This advancement enables efficient handling of high-dimensional data, making it well-suited for AI workloads such as recommendation systems, natural language processing, and <span class="No-Break">image search.</span></p></li>
				<li><strong class="bold">Azure Managed Redis</strong>: Azure <a id="_idIndexMarker167"/>Managed Redis, which is powered by Redis Enterprise software, can serve as an efficient vector database for storing embedding vectors and executing<a id="_idIndexMarker168"/> vector similarity searches. The Enterprise tier’s <strong class="source-inline">RediSearch</strong> module offers comprehensive search capabilities, including various distance metrics such as Euclidean, cosine, and Inner Product, and supports both KNN with FLAT indexing and ANNs using HNSW indexing. It allows vector storage in hash or JSON formats and supports top-K and vector range queries to find items within a specific vector distance. Additionally, Redis enhances search functionalities with advanced features such as geospatial filtering, numeric and text filters, prefix and fuzzy matching, phonetic matching, and Boolean queries. Often considered a cost-effective solution, Redis is widely used for caching or session storage, enabling it to handle both traditional caching roles and vector search <span class="No-Break">applications concurrently.</span></li>
			</ul>
			<p>Most Azure vector databases, such as AI Search, CosmosDB, and Azure Managed Redis, are integrated with LLM frameworks such as Semantic Kernel, LangChain, and LlamaIndex to facilitate easy vector creation and ingestion into their respective services. These integrations streamline the process of embedding generation, storage, and retrieval, enabling efficient handling of vector data and enhancing the capabilities of <span class="No-Break">GenAI applications.</span></p>
			<p>Azure also offers a <a id="_idIndexMarker169"/>no-code solution for creating vector embeddings and automatically ingesting them into Azure AI Search or Cosmos DB as part of its native features. In the next section, we will explore this capability as part of <em class="italic">AOAI On </em><span class="No-Break"><em class="italic">Your Data</em></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>AOAI On Your Data</h1>
			<p>A common application in various industries is the creation of personalized chatbots utilizing their own enterprise data<a id="_idIndexMarker170"/> through generative AI. Traditionally, this required customers to manually code processes for extracting text from unstructured data, generate embeddings, and store them in a vector database, which was both time-consuming and labor-intensive for developers. However, the Azure OpenAI On Your Data feature significantly simplifies this workflow, allowing developers to achieve the same results with minimal or no coding. This means that building a chatbot application can now be accomplished with just a <span class="No-Break">few clicks.</span></p>
			<p>This functionality operates behind the scenes using a RAG technique, as illustrated in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B21019_03_2.jpg" alt="Figure 3.2: AOAI On Your Data RAG architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2: AOAI On Your Data RAG architecture</p>
			<p>When you upload a collection of documents in various formats, those are then divided into smaller chunks. Each chunk is converted into an embedding, such as <em class="italic">text-embedding-ada-002</em>, using AOAI’s embedding model. These embeddings are stored in an AI search vector database, utilizing an HNSW index, which is configured for semantic search. Users can input queries<a id="_idIndexMarker171"/> through GPT4-o chat, which are also transformed into embeddings using the same embedding model. Then a similarity search is conducted to find the most relevant document vectors using ANN techniques on the HNSW index. Next, the retrieved context and the raw user query are provided to the GPT models to generate responses to <span class="No-Break">user questions.</span></p>
			<p>Additionally, Azure RBAC ensures secure access and permissions throughout the system. The entire process is streamlined and user-friendly, allowing you to focus on utilizing the insights without dealing with the <span class="No-Break">technical complexities.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Ensure that the Azure AI Search resource is set up before utilizing the On your Data feature. To set up Azure AI Search resource, refer to the instructions provided <span class="No-Break">at </span><a href="https://learn.microsoft.com/en-us/azure/search/search-create-service-portal"><span class="No-Break">https://learn.microsoft.com/en-us/azure/search/search-create-service-portal</span></a><span class="No-Break">.</span></p>
			<p>Now, let’s demonstrate how you can utilize this feature effortlessly without needing to write <span class="No-Break">any code:</span></p>
			<ol>
				<li>Log in to the Azure AI <span class="No-Break">Foundry Portal.</span></li>
				<li>If you haven’t set up a chat model and an embedding model yet, go to the <strong class="bold">Deployments</strong> section found in the <strong class="bold">Shared resources</strong> menu. From there, initiate the deployment of a new chat model such as GPT4-o, and an embedding model such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">text-embedding-ada-002</strong></span><span class="No-Break">.</span></li>
				<li>Navigate to <strong class="bold">Chat</strong> from the <strong class="bold">Playgrounds</strong> menu and select the <strong class="bold">Add your </strong><span class="No-Break"><strong class="bold">data</strong></span><span class="No-Break"> tab.</span></li>
				<li>Click on <strong class="bold">Add a </strong><span class="No-Break"><strong class="bold">data source</strong></span><span class="No-Break">.</span><p class="list-inset">You have several choices for data sources. In this demonstration, we’re uploading the file manually. Alternatively, you can opt for a direct blob storage account if you have existing data there. If you have an AI search index where data is pre-indexed <a id="_idIndexMarker172"/>and vectorized, you can connect to that index <span class="No-Break">as well.</span></p></li>
				<li>Select the appropriate subscriptions and the blob storage account where your uploaded data will be stored. Next, choose the <strong class="bold">Azure AI Search resource</strong> option you’ll use for storing the vector index, and finally, specify the <span class="No-Break">index name.</span></li>
				<li>Tick the checkbox to enable vector search for this AI search resource. Refer to <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.3</em> <span class="No-Break">for details.</span></li>
			</ol>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B21019_03_3.jpg" alt="Figure 3.3: AOAI On Your Data settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3: AOAI On Your Data settings</p>
			<ol>
				<li value="7">Click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>On the next screen, upload<a id="_idIndexMarker173"/> a PDF file. You also have the option to upload files in <strong class="source-inline">.txt</strong>, <strong class="source-inline">.md</strong>, <strong class="source-inline">.html</strong>, <strong class="source-inline">.pdf</strong>, <strong class="source-inline">.docx</strong>, or <strong class="source-inline">.pptx</strong> formats. In this instance, I’m uploading a hotel invoice PDF receipt. Refer to <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.4</em> for a sample view of <span class="No-Break">the receipt.</span></li>
			</ol>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B21019_03_4.jpg" alt="Figure 3.4: Sample data for AOAI On Your Data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4: Sample data for AOAI On Your Data</p>
			<ol>
				<li value="9">Click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>Select the <strong class="bold">Search type</strong> option you’d<a id="_idIndexMarker174"/> like to use. You have three options to <span class="No-Break">choose from:</span><ul><li><strong class="bold">Vector</strong>: This finds documents similar to a query using <span class="No-Break">vector embeddings.</span></li><li><strong class="bold">Hybrid</strong> (vector + keyword): This combines similarity search over vector fields with keyword-based <span class="No-Break">full-text search.</span></li><li><strong class="bold">Hybrid + semantic</strong>: This utilizes vector embeddings, language understanding, and flexible query parsing for advanced search experiences. This option is highly recommended due to its superior <span class="No-Break">search quality.</span></li></ul><p class="list-inset">Selecting the right type enhances your search capabilities based on your <span class="No-Break">specific needs.</span></p></li>
				<li>Select the chunk size. Chunking involves dividing documents into smaller segments for efficient search and retrieval, with chunk size measured <span class="No-Break">in tokens.</span></li>
				<li>Click <strong class="bold">Next</strong>. On the following screen, choose the type <span class="No-Break">of authentication.</span></li>
				<li>The time required for<a id="_idIndexMarker175"/> indexing may increase with <span class="No-Break">document size.</span></li>
				<li>Select the <strong class="bold">Deployment</strong> option to use and fill in the <strong class="bold">System message</strong> field as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em>. Then click <span class="No-Break"><strong class="bold">Apply changes</strong></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B21019_03_5.jpg" alt="Figure 3.5: On Your Data settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5: On Your Data settings</p>
			<ol>
				<li value="15">Now you are all set to start the chat from <span class="No-Break"><strong class="bold">Chat playground</strong></span><span class="No-Break">.</span></li>
				<li>Refer to <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.6</em> for a sample question. You can ask questions based on the document provided to <a id="_idIndexMarker176"/>receive accurate answers. You can click on the references to view the citations for <span class="No-Break">the answers.</span></li>
			</ol>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B21019_03_6.jpg" alt="Figure 3.6: Sample questions and answers, with references"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6: Sample questions and answers, with references</p>
			<p class="list-inset">You’ve observed how the answer is grounded on your data using the AOAI On Your Data feature. If you’re looking to develop a web application and deploy it to production, simply click <strong class="bold">Deploy as a web app</strong> and fill in the app service details as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.7</em>. Then click on <strong class="bold">Deploy</strong> to create a web application <span class="No-Break">for users.</span></p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B21019_03_7.jpg" alt="Figure 3.7: Chat On Your Data web app settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7: Chat On Your Data web app settings</p>
			<p>After deployment, you’ll <a id="_idIndexMarker177"/>receive a public URL for the application, enabling end users to interact with the document they uploaded, as <span class="No-Break">shown here:</span></p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B21019_03_8.jpg" alt="Figure 3.8: Web application view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8: Web application view</p>
			<p>This simplifies the process, allowing you to make your chat application available to end users with just a few clicks without needing to write any code. It’s an efficient way to deploy and share your application quickly. However, it’s important to note that while this feature works well for <a id="_idIndexMarker178"/>small <strong class="bold">proofs of concept</strong>, developing a robust production application requires careful consideration of various factors for improved accuracy. These include strategies <a id="_idIndexMarker179"/>for chunking, query rewriting, and designing custom prompt templates. These capabilities are somewhat limited in this regard, often necessitating a code-first approach to create a more sophisticated and <span class="No-Break">tailored solution.</span></p>
			<p>Up to this point, we’ve utilized AOAI models for text-related scenarios. However, there are instances where image understanding is required. In the next section, we’ll explore AOAI multimodal capabilities, where the input is an image and the output is text. This approach broadens the applicability of AOAI by incorporating visual <span class="No-Break">data analysis.</span></p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/>AOAI multimodal model</h1>
			<p>AOAI multimodal capabilities can be <a id="_idIndexMarker180"/>applied in various real-world scenarios, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Image captioning</strong>: Automatically generating descriptive text for images, which is useful in organizing digital <a id="_idIndexMarker181"/>photo collections or assisting visually <span class="No-Break">impaired users</span></li>
				<li><strong class="bold">Audio processing</strong>: Low-latency conversational use cases requiring real-time engagement between a user and a model, such as customer support chatbots, voice assistants, and live <span class="No-Break">translation services</span></li>
				<li><strong class="bold">Visual question answering</strong>: Responding to questions about an image, which can enhance interactive educational tools or customer <span class="No-Break">support systems</span></li>
				<li><strong class="bold">Content moderation</strong>: Analyzing images to detect inappropriate or harmful content, improving safety on social <span class="No-Break">media platforms</span></li>
				<li><strong class="bold">E-commerce</strong>: Providing product descriptions from images, aiding in cataloging and improving user <span class="No-Break">search experiences</span></li>
				<li><strong class="bold">Healthcare</strong>: Assisting in medical diagnostics by interpreting medical images and providing <span class="No-Break">preliminary reports</span></li>
			</ul>
			<p>These use cases demonstrate the versatility of integrating both visual and textual data processing. To enable such functionalities, AOAI provides a GPT-4 category model with built-in multimodal features. Presently, AOAI offers three distinct models within the GPT-4 family that support these native <span class="No-Break">multimodal capabilities:</span></p>
			<ul>
				<li><span class="No-Break">GPT4-Turbo</span></li>
				<li><span class="No-Break">GPT4-o</span></li>
				<li><span class="No-Break">GPT4-o-mini</span></li>
			</ul>
			<p>Now, let’s test the multimodal capability of GPT4-o from <span class="No-Break"><strong class="bold">Chat playground</strong></span><span class="No-Break">:</span></p>
			<ol>
				<li>Log in to the Azure AI <span class="No-Break">Foundry Portal.</span></li>
				<li>Navigate to the <strong class="bold">Chat from </strong><span class="No-Break"><strong class="bold">Playgrounds</strong></span><span class="No-Break"> menu.</span></li>
				<li>Upload an image to the chat and ask a question about it. Alternatively, you can achieve this through the SDK by providing a base64-encoded image as input to the model. For illustration <a id="_idIndexMarker182"/>purposes, I’ve used <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B21019_03_9.jpg" alt="Figure 3.9: GPT4-o sample image"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9: GPT4-o sample image</p>
			<p class="list-inset">You can ask questions such as <span class="No-Break">the following:</span></p>
			<p class="list-inset"><strong class="source-inline">"What is the dimension of the </strong><span class="No-Break"><strong class="source-inline">CB line?"</strong></span></p>
			<p>While AOAI GPT4-o’s vision capabilities are highly versatile and useful across various applications, it is crucial to recognize its limitations. Here are several key constraints to be <span class="No-Break">aware of:</span></p>
			<ul>
				<li><strong class="bold">Medical images</strong>: The model is not <a id="_idIndexMarker183"/>designed for interpreting specialized medical images such as CT scans and should not be used for <span class="No-Break">medical advice</span></li>
				<li><strong class="bold">Non-English text</strong>: Performance may decline when dealing with images containing text in non-Latin alphabets, such as Japanese <span class="No-Break">or Korean</span></li>
				<li><strong class="bold">Small text</strong>: To improve readability, enlarge small text within images, but ensure that no important details are <span class="No-Break">cropped out</span></li>
				<li><strong class="bold">Rotation issues</strong>: The model may misinterpret text or images that are rotated or <span class="No-Break">upside down</span></li>
				<li><strong class="bold">Visual elements</strong>: Understanding graphs or text with varying colors or styles, such as solid, dashed, or dotted lines, can be challenging for <span class="No-Break">the model</span></li>
				<li><strong class="bold">Spatial reasoning</strong>: Tasks requiring precise spatial localization, such as identifying chess positions, are difficult for <span class="No-Break">the model</span></li>
				<li><strong class="bold">Accuracy</strong>: In certain situations, the model may produce incorrect descriptions <span class="No-Break">or captions</span></li>
				<li><strong class="bold">Image shape</strong>: Panoramic and fisheye images pose difficulties for <span class="No-Break">the model</span></li>
				<li><strong class="bold">Metadata and resizing</strong>: The model does not process original file names or metadata, and images are resized before analysis, potentially affecting their <span class="No-Break">original dimensions</span></li>
				<li><strong class="bold">Counting</strong>: The model may provide approximate counts of objects <span class="No-Break">in images</span></li>
				<li><strong class="bold">CAPTCHAs</strong>: For <a id="_idIndexMarker184"/>safety reasons, the system is configured to block the submission <span class="No-Break">of CAPTCHAs</span></li>
			</ul>
			<p>Understanding these limitations can help customers set realistic expectations and ensure the model is <span class="No-Break">applied appropriately.</span></p>
			<p>Now that we have seen the capabilities and limitations of GPT4-o’s vision feature, let’s understand the cost model for <span class="No-Break">image inputs.</span></p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Image token cost</h2>
			<p>Image inputs are measured and charged in tokens, much like text inputs. The token cost for an image is influenced by two main<a id="_idIndexMarker185"/> factors: its dimensions and the detail level specified for each image URL block. Here is a <span class="No-Break">detailed breakdown:</span></p>
			<ul>
				<li><strong class="bold">Low detail</strong>: Each image with <strong class="bold">Detail</strong> set to <strong class="bold">Low</strong> costs <span class="No-Break">85 tokens.</span></li>
				<li><strong class="bold">High detail</strong>: For high-detail images, the process is more complex. First, the image is scaled to fit within a 2,048 x 2,048 square while maintaining its aspect ratio. Then, it is scaled again so that its shortest side is 768 pixels long. The number of 512-pixel squares that make up the image is then counted, and each of these squares costs 170 tokens. An additional 85 tokens are always added to the <span class="No-Break">final total.</span></li>
			</ul>
			<p>Let’s look at <span class="No-Break">some examples:</span></p>
			<ul>
				<li><strong class="bold">1,024 x 1,024 image in </strong><span class="No-Break"><strong class="bold">high detail</strong></span><span class="No-Break">:</span><ul><li>No initial resizing is needed since 1,024 is less <span class="No-Break">than 2,048</span></li><li>The image is scaled down to 768 x 768, as the shortest side <span class="No-Break">is 1,024</span></li><li>This requires four tiles of 512 <span class="No-Break">pixels each</span></li><li>The token cost calculation is as follows: 170 tokens/tile * 4 tiles + 85 tokens = <span class="No-Break">765 tokens.</span></li></ul></li>
				<li><strong class="bold">2,048 x 4,096 image in </strong><span class="No-Break"><strong class="bold">high detail</strong></span><span class="No-Break">:</span><ul><li>The image is first <a id="_idIndexMarker186"/>scaled to 1,024 x 2,048 to fit within the <span class="No-Break">2,048 square</span></li><li>The shortest side is then 1,024, so it is further scaled to 768 <span class="No-Break">x 1,536</span></li><li>This requires 6 tiles of 512 <span class="No-Break">pixels each</span></li><li>The token cost calculation is as follows: 170 tokens/tile * 6 tiles + 85 tokens = <span class="No-Break">1,105 tokens</span></li></ul></li>
				<li><strong class="bold">4,096 x 8,192 image in </strong><span class="No-Break"><strong class="bold">low detail</strong></span><span class="No-Break">:</span><ul><li>Regardless of the input size, low-detail images have a <span class="No-Break">fixed cost</span></li><li>The token cost is <span class="No-Break">85 tokens</span></li></ul></li>
			</ul>
			<p>Understanding this token cost structure can help manage resource usage effectively when working with <span class="No-Break">image inputs.</span></p>
			<p>So far, we’ve explored the self-contained capabilities of AOAI models. However, there are enterprise use cases where customers need to integrate LLMs with external systems or tools. This allows for the conversion of natural language queries into executable structured inputs for those systems. In the next section, we will discuss how developers can achieve this functionality using AOAI’s function <span class="No-Break">calling feature.</span></p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>AOAI function calling</h1>
			<p>AOAI function calling allows<a id="_idIndexMarker187"/> you to connect models such as GPT-4o and other GPT models to external tools and systems. This is beneficial for tasks such as enhancing AI assistants’ abilities or creating seamless integrations between your applications and <span class="No-Break">the models.</span></p>
			<p>This feature doesn’t<a id="_idIndexMarker188"/> directly run functions for you. Instead, you define the functions in the API call, and the model figures out how to create the needed arguments. After these arguments are generated, you can use them to execute functions within <span class="No-Break">your code.</span></p>
			<p>Function calling is beneficial for <a id="_idIndexMarker189"/>numerous applications, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Allowing assistants to retrieve information</strong>: An AI assistant might need to access the latest customer data from an internal system (such as Azure Cosmos DB or Azure SQL) to answer a user’s query about <span class="No-Break">recent orders</span></li>
				<li><strong class="bold">Enabling assistants to execute tasks</strong>: An AI assistant can arrange meetings by considering user preferences and <span class="No-Break">calendar availability</span></li>
				<li><strong class="bold">Assisting with computations</strong>: A math tutor assistant can perform calculations <span class="No-Break">as needed</span></li>
				<li><strong class="bold">Creating complex workflows</strong>: For instance, a data extraction process can gather raw text, convert it to structured data, and store it in <span class="No-Break">a database</span></li>
				<li><strong class="bold">Altering your application’s user interface</strong>: Function calls can update the UI based on user actions, such as displaying a pin on <span class="No-Break">a map</span></li>
			</ul>
			<p>Now, let’s talk about the life cycle of the AOAI <span class="No-Break">function call.</span></p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Function call life cycle</h2>
			<p>There are five different stages <a id="_idIndexMarker190"/>of function calls, as can be seen in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">:</span></p>
			<ol>
				<li>Your code initiates the process by calling the API with a prompt and the functions the LLM <span class="No-Break">can access.</span></li>
				<li>The model evaluates whether to respond directly to the user or whether one or more functions need to <span class="No-Break">be invoked.</span></li>
				<li>The API replies to your application, specifying which function should be called and what the necessary <span class="No-Break">arguments are.</span></li>
				<li>Your application runs the specified function using the <span class="No-Break">provided arguments.</span></li>
				<li>Your application <a id="_idIndexMarker191"/>communicates back to the API with the initial prompt and the outcome of the <span class="No-Break">executed function.</span></li>
			</ol>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B21019_03_10.jpg" alt="Figure 3.10: Function call life cycle"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10: Function call life cycle</p>
			<p>When using the OpenAI API with function calling, the model itself doesn’t execute the functions. Instead, in <em class="italic">Step 3</em>, the model generates parameters for your function, which your application can use. Your code decides how to handle these parameters, typically by calling the specified function. This ensures that your application retains complete control over the <span class="No-Break">execution process.</span></p>
			<p>Function calling is supported in the Chat Completions, Assistants, and Batch APIs. This section focuses on function calling using the Chat Completions API. The Assistant and Batch APIs will be<a id="_idIndexMarker192"/> covered in the <span class="No-Break">next section.</span></p>
			<p>Let’s <span class="No-Break">get started.</span></p>
			<h3>Step 1 – Define the function</h3>
			<p>Start by specifying the<a id="_idIndexMarker193"/> function you intend to invoke. This function should be a Python function capable of accepting inputs and providing outputs. The inputs of the function will be generated by <span class="No-Break">the model.</span></p>
			<p>For this example, suppose you wish to enable the model to execute the <strong class="source-inline">get_weather</strong> function within your codebase. This function takes a city as an argument to retrieve weather information from a weather API. Your function might look like <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B21019_03_11.jpg" alt="Figure 3.11: get_weather function definition"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11: get_weather function definition</p>
			<h3>Step 2 – Describe the function for model use</h3>
			<p>Now that we have identified the function we<a id="_idIndexMarker194"/> want the model to call, we will develop a <em class="italic">function definition</em>. This will explain what the function accomplishes, when it might be used, and what parameters are needed to <span class="No-Break">invoke it.</span></p>
			<p>The parameters section in your function definition should be outlined using JSON Schema. When the model generates a function call, it will refer to this schema to create <span class="No-Break">arguments appropriately.</span></p>
			<p>In this example, it may<a id="_idIndexMarker195"/> look like <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.12.</em></span></p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B21019_03_12.jpg" alt="Figure 3.12: JSON schema of the function"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12: JSON schema of the function</p>
			<h3>Step 3 – Provide function definitions as “tools” to the model</h3>
			<p>Next, we need to include our function definitions within an array of available <em class="italic">tools</em> when using the Chat Completions API. As usual, we’ll provide an array of <em class="italic">messages</em>, which might include your <a id="_idIndexMarker196"/>prompt or an entire dialogue between the user and <span class="No-Break">an assistant.</span></p>
			<p>This example, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.13</em>, illustrates how you might call the Chat Completions API, supplying relevant functions and messages to properly generate the function call with a <span class="No-Break">unique ID.</span></p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B21019_03_13.jpg" alt="Figure 3.13: Generating the function call"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13: Generating the function call</p>
			<p>By default, the model is set to automatically decide which functions to invoke, based on the <strong class="source-inline">tool_choice</strong>: “<span class="No-Break"><strong class="source-inline">auto</strong></span><span class="No-Break">” configuration.</span></p>
			<p>We provide three options to modify this <span class="No-Break">default behavior:</span></p>
			<ul>
				<li>To mandate the model to<a id="_idIndexMarker197"/> always call one or more functions, you can set <strong class="source-inline">tool_choice: "required"</strong>. This ensures that the model will always select at least one function to execute, which can be useful when you want the model to choose between <span class="No-Break">various actions.</span></li>
				<li>To direct the model to use a specific function, you can define <strong class="source-inline">tool_choice: {"type": "function", "function": {"name": "my_function"}}</strong>. This will force the model to only call the <span class="No-Break">specified function.</span></li>
				<li>To prevent any function calls and have the model respond with only a user-facing message, you<a id="_idIndexMarker198"/> can either omit tools entirely or set <span class="No-Break"><strong class="source-inline">tool_choice: "none"</strong></span><span class="No-Break">.</span></li>
			</ul>
			<h3>Step 4 – Making the actual function call</h3>
			<p>As previously discussed, the AOAI function call doesn’t automatically trigger the actual function. Instead, you <a id="_idIndexMarker199"/>must add the code to trigger the call based on the input parameters generated by the model. To do this, refer to the code shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B21019_03_14.jpg" alt="Figure 3.14: Making the function call"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.14: Making the function call</p>
			<p>When you include a function in your request, the function’s details (such as its definition and parameters) become part of the system message, which is then processed by the model along with the user input. This integration enables the model to assess whether the function should be invoked based on the <span class="No-Break">prompt context.</span></p>
			<p>This process does use tokens, as the function definition and parameters contribute to the overall token count. Therefore, employing prompt engineering strategies such as being concise, excluding unnecessary details, and focusing on essential parts of the prompt can improve function <span class="No-Break">call efficiency.</span></p>
			<p>Here are some more<a id="_idIndexMarker200"/> ways to optimize function <span class="No-Break">call efficiency:</span></p>
			<ul>
				<li><strong class="bold">Add more detail to your function definition</strong>: Including detailed function definitions with meaningful descriptions is crucial for clarity and efficient function invocation. When defining a function, each parameter should be described in a way that both the model and any human reviewing the code can easily understand. Here’s a breakdown of how to make function definitions <span class="No-Break">more comprehensive.</span></li>
				<li><strong class="bold">Provide contextual system message</strong>: The system message can provide additional context to guide the model’s behavior. For instance, if you have a function such as <strong class="source-inline">search_hotels</strong>, you could set a system message such as <span class="No-Break">the following:</span><pre class="source-code">
{"role": "system", "content": "You're an AI assistant designed to help users search for hotels. When a user asks for help finding a hotel, you should call the search_hotels function."}</pre><p class="list-inset">This informs the model when to invoke the function based on <span class="No-Break">user input.</span></p></li>				<li><strong class="bold">Instruct the model to ask questions if the user input is unclear</strong>: Instruct the model to ask clarifying questions when user input is incomplete to avoid assumptions. For example, in <strong class="source-inline">search_hotels</strong>, ask for location details if the user request is missing them. Include instructions such as this in your system message to guide <span class="No-Break">the model:</span><pre class="source-code">
{"role": "system", "content": "Avoid assuming values for functions; instead, seek clarification when a user's request is unclear."}</pre></li>				<li><strong class="bold">Error handling</strong>: Another key aspect of prompt engineering is minimizing errors in function calls. While models are trained to generate function calls according to your defined schema, they may sometimes create calls that don’t align with it or attempt to invoke functions that <span class="No-Break">aren’t included.</span><p class="list-inset">To address this, you <a id="_idIndexMarker201"/>can add a statement such as the following in the <span class="No-Break">system message:</span></p><pre class="source-code">
{"role": "system", "content": "Only use the functions you have been provided with."}</pre><p class="list-inset">This helps ensure that the model adheres strictly to the functions <span class="No-Break">you’ve defined.</span></p></li>			</ul>
			<p>Now that you understand how to call AOAI functions, the next section will focus on the AOAI Assistants API, which simplifies the application development process <span class="No-Break">for developers.</span></p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>AOAI Assistants API</h1>
			<p>The AOAI Assistants API enables the creation of AI-powered assistants that can be integrated directly into your own applications. These assistants operate based on a set of predefined instructions and can<a id="_idIndexMarker202"/> interact with users by utilizing various capabilities such as <a id="_idIndexMarker203"/>models, tools, and files. Currently, the Assistants API supports three key types <span class="No-Break">of tools:</span></p>
			<ul>
				<li><strong class="bold">Code Interpreter</strong>: This feature empowers the assistant to handle user requests for computations or <a id="_idIndexMarker204"/>script execution by allowing it to write and run Python code in a secure, multi-tenant Kubernetes environment. The Kubernetes sandbox uses nested hypervisor technology to isolate each <a id="_idIndexMarker205"/>container, offering a unique user-space kernel rather than a traditional kernel. This setup enhances security by isolating environments, reduces risks by preventing cross-container interference, and improves system flexibility. By facilitating code execution within a safe, virtualized environment, the assistant can dynamically respond to complex calculations, data processing, and <span class="No-Break">file handling.</span><p class="list-inset">With support for diverse data formats (<a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python#supported-file-types">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python#supported-file-types</a>), this tool can process files of various structures, extracting information as needed. Code Interpreter enables iterative code execution, making it possible for the assistant to adjust code and retry execution if initial attempts fail, which is particularly helpful for complex coding and <a id="_idIndexMarker206"/>mathematical problems. Common use cases include extracting data from CSV files, creating<a id="_idIndexMarker207"/> structured data visualizations such as charts and graphs, and solving <span class="No-Break">math problems.</span></p></li>
				<li><strong class="bold">File Search</strong>: The AOAI File Search tool allows the assistant to enhance file-based query handling by accessing <a id="_idIndexMarker208"/>and retrieving information from user-provided documents. Acting as an external knowledge base, it lets the assistant search beyond model-trained <a id="_idIndexMarker209"/>data to include proprietary content or other document-based information. AOAI’s system automatically chunks down as per chunking strategies and indexes documents by creating vector embeddings and storing them within a managed vector store powered by Azure AI Search. This enables both vector-based and keyword searches, facilitating more precise, context-driven information retrieval that supports diverse formats, as detailed at <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/file-search?tabs=python#supported-file-types">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/file-search?tabs=python#supported-file-types</a>. By transforming document content into vector embeddings, File Search allows searches to understand context and meaning instead of simply matching exact keywords. This capability is especially useful for tasks such as detailed Q&amp;A, summarization, and data extraction where nuanced understanding and quick retrieval are required. The tool is also versatile, handling a wide array of document formats to enable easy integration into workflows requiring complex <span class="No-Break">document interactions.</span></li>
				<li><strong class="bold">Function calling</strong>: This tool allows<a id="_idIndexMarker210"/> the assistant to invoke specific functions within an application, enabling it to <a id="_idIndexMarker211"/>execute tasks or retrieve data through API interfaces. We previously covered function calling in detail, highlighting its role in enhancing the assistant’s <span class="No-Break">interactive capabilities.</span></li>
			</ul>
			<p>These tools make it easier to build dynamic and responsive AI assistants tailored to specific use cases<a id="_idIndexMarker212"/> within <span class="No-Break">an application.</span></p>
			<p>Now, let’s discuss the<a id="_idIndexMarker213"/> process <span class="No-Break">flow assistants.</span></p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Assistant process flow</h2>
			<p>Setting up and running an AI assistant such as a finance bot generally involves four key steps. Using the<a id="_idIndexMarker214"/> finance bot as an example, and as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.15,</em> the steps are <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B21019_03_15.jpg" alt="Figure 3.15: AOAI assistants’ process flow"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15: AOAI assistants’ process flow</p>
			<h3>Step 1 – create Assistants</h3>
			<p>An Assistant is an entity that can be tailored to<a id="_idIndexMarker215"/> respond to user inputs using various parameters such as models, instructions, and tools. Assistants can use OpenAI models via AOAI and can be customized to suit different personalities or capabilities based on provided instructions. You can equip the Assistant with tools, either pre-built ones such as <strong class="source-inline">code_interpreter</strong> and <strong class="source-inline">file_search</strong>, or custom ones via function calling. These tools help the Assistant perform specific tasks based on the user’s queries. The Assistants API has support for several parameters that let you customize the Assistants’ output. The <strong class="source-inline">tool_choice</strong> parameter lets you force the Assistant to use a <span class="No-Break">specified tool.</span></p>
			<h3>Step 2 – create a Thread</h3>
			<p>A Thread acts as a record of the conversation between the user and one or more Assistants. When a user (or AI application) initiates a conversation, a new Thread is created. The assistant can access persistent Threads, enabling continuous conversation without losing context. This simplifies<a id="_idIndexMarker216"/> AI application development by retaining message history and managing memory efficiently by truncating older data when the conversation exceeds the model’s context window. A Thread is created once and updated as new messages <span class="No-Break">are exchanged.</span></p>
			<h3>Step 3 – add a message to the Thread</h3>
			<p>The messages <a id="_idIndexMarker217"/>exchanged during the conversation, whether from the user or the application, are stored as <strong class="bold">Message</strong> objects within<a id="_idIndexMarker218"/> the Thread. These messages can include both text and files. While a Thread can store up to 100,000 messages, the system intelligently manages the conversation by automatically truncating any content that exceeds the model’s context window. Assistants are designed to truncate text automatically to ensure that the conversation stays within the model’s maximum token limit. However, you can adjust this behavior by specifying how many tokens or recent messages should be included in <span class="No-Break">each Run.</span></p>
			<p>To manage token usage during a single Run, you can set <strong class="source-inline">max_prompt_tokens</strong> and <strong class="source-inline">max_completion_tokens</strong> at the start. These limits apply to all completions made throughout the Run. For instance, if <strong class="source-inline">max_prompt_tokens</strong> is set to <strong class="source-inline">500</strong> and <strong class="source-inline">max_completion_tokens</strong> to <strong class="source-inline">1000</strong>, the assistant will first truncate the prompt to fit within 500 tokens and cap the output at 1,000 tokens. If only 200 tokens are used for the prompt and 300 tokens for the completion, the next completion will have 300 prompt tokens and 700 completion <span class="No-Break">tokens available.</span></p>
			<p>If the completion hits the <strong class="source-inline">max_completion_tokens</strong> limit, the Run will stop with an incomplete status, and the reason will be included in the <strong class="source-inline">incomplete_details</strong> field of the <span class="No-Break"><strong class="source-inline">Run</strong></span><span class="No-Break"> object.</span></p>
			<p>When using the File Search tool, it’s recommended to set <strong class="source-inline">max_prompt_tokens</strong> to no fewer than <strong class="source-inline">20000</strong>. For more extensive conversations or multiple interactions using File Search, consider increasing this limit to <strong class="source-inline">50000</strong> or even removing it entirely for <span class="No-Break">optimal results.</span></p>
			<p>You can also define a <strong class="bold">truncation strategy</strong> to control how the Thread <a id="_idIndexMarker219"/>should fit within the model’s <span class="No-Break">context window:</span></p>
			<ul>
				<li>Using the <strong class="source-inline">auto</strong> truncation strategy will apply OpenAI’s default <span class="No-Break">truncation behavior</span></li>
				<li>Using the <strong class="source-inline">last_messages</strong> strategy lets you specify how many of the most recent messages should be included in <span class="No-Break">the context</span></li>
			</ul>
			<p>This approach gives<a id="_idIndexMarker220"/> you more control over managing conversation length and ensuring optimal performance in <span class="No-Break">each Run.</span></p>
			<h3>Step 4 – create a Run</h3>
			<p>After all user messages are<a id="_idIndexMarker221"/> added to the Thread, the conversation is processed by initiating a Run. The Run utilizes the models and tools defined for the Assistant to generate a response. The assistant’s response is then added to the Thread as a new message, continuing the flow <span class="No-Break">of conversation.</span></p>
			<p>Now that we’ve covered the assistant’s API process flow, let’s proceed with setting up <span class="No-Break">the assistant.</span></p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor051"/>AOAI Assistants – code interpreter</h2>
			<p>In this section, we’ll guide you <a id="_idIndexMarker222"/>through the step-by-step process of setting up an assistant in the Azure AI Foundry Portal and demonstrate how to use the code interpreter tool to handle user queries on CSV data. You can achieve the same functionality <a id="_idIndexMarker223"/>with an <span class="No-Break">API-based approach:</span></p>
			<ol>
				<li>Log in to the Azure AI <span class="No-Break">Foundry Portal.</span></li>
				<li>If you haven’t yet set up a chat model, go to the <strong class="bold">Deployments</strong> section in the <strong class="bold">Shared resources</strong> menu. From there, initiate the deployment of a new chat model such <span class="No-Break">as </span><span class="No-Break"><strong class="bold">GPT4-o</strong></span><span class="No-Break">.</span></li>
				<li>Navigate to the <strong class="bold">Assistants from Playgrounds</strong> menu, choose the <strong class="bold">GPT4-o</strong> deployment, and click <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">an assistant</strong></span><span class="No-Break">.</span></li>
				<li>On the <strong class="bold">Setup</strong> page, you’ll see that an assistant ID has been created. Here, you can assign a name to <a id="_idIndexMarker224"/>the assistant and write custom instructions to clearly define its objectives, as <span class="No-Break">shown here:</span></li>
			</ol>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B21019_03_16.jpg" alt="Figure 3.16: Assistant setup"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16: Assistant setup</p>
			<ol>
				<li value="5">In the <strong class="bold">Tools</strong> section, enable <strong class="bold">Code Interpreter</strong> and upload a CSV file to query. For this example, we’ll <a id="_idIndexMarker225"/>use a sample CSV file containing<a id="_idIndexMarker226"/> retail order data. A snippet of this file is <span class="No-Break">shown here:</span></li>
			</ol>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B21019_03_17.jpg" alt="Figure 3.17: Sample CSV file"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17: Sample CSV file</p>
			<ol>
				<li value="6">Once the CSV data is uploaded, you can start by asking relevant queries. For example, you could ask: <strong class="source-inline">"How many orders have been shipped so far?"</strong>. Alternatively, you can also use the following prompt: <strong class="source-inline">"Create a chart with order status on the x-axis and quantity on </strong><span class="No-Break"><strong class="source-inline">the y-axis."</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>For this type of question, the Code Interpreter tool will execute Python code within a Microsoft-managed sandbox environment and provide you with the results, as <span class="No-Break">shown here:</span></p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B21019_03_18.jpg" alt="Figure 3.18: Output of Code Interpreter"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18: Output of Code Interpreter</p>
			<p>In the next section, we <a id="_idIndexMarker227"/>will cover how you can integrate File Search <a id="_idIndexMarker228"/>capabilities into the assistant using the <span class="No-Break">SDK approach.</span></p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor052"/>AOAI Assistants – File Search</h2>
			<p>Using the OpenAI SDK, you<a id="_idIndexMarker229"/> can directly program the assistant to search documents and extract specific <a id="_idIndexMarker230"/>function signatures as needed. File Search will let the Assistant parse, chunk, and index documents for efficient retrieval by leveraging the SDK. You can configure these tools seamlessly within your application, enhancing the assistant’s data retrieval and <span class="No-Break">interactive capabilities:</span></p>
			<ol>
				<li>First, we need to create a new assistant with File Search as part of its set of tools, as <span class="No-Break">shown here:</span></li>
			</ol>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B21019_03_19.jpg" alt="Figure 3.19: Creating the Assistant"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.19: Creating the Assistant</p>
			<ol>
				<li value="2">Upload your files to enable automated chunking (with a chunk size of 800 tokens and a chunk overlap <a id="_idIndexMarker231"/>of 400 tokens). Embed and create a vector store that powers the<a id="_idIndexMarker232"/> file search tool, as <span class="No-Break">shown here:</span></li>
			</ol>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B21019_03_20.jpg" alt="Figure 3.20: Vector store creation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.20: Vector store creation</p>
			<ol>
				<li value="3">Attach the vector store to the assistant to give access to the files, as <span class="No-Break">shown here:</span></li>
			</ol>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B21019_03_21.jpg" alt="Figure 3.21: Attaching vector store to the assistant"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.21: Attaching vector store to the assistant</p>
			<ol>
				<li value="4">Create a<a id="_idIndexMarker233"/> thread and run the<a id="_idIndexMarker234"/> assistant, as <span class="No-Break">shown here:</span></li>
			</ol>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B21019_03_22.jpg" alt="Figure 3.22: Create a thread and run"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.22: Create a thread and run</p>
			<ol>
				<li value="5">Display the assistant response with citations, as <span class="No-Break">shown here:</span></li>
			</ol>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B21019_03_23.jpg" alt="Figure 3.23: Assistant response with citations"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.23: Assistant response with citations</p>
			<p>Now, you can see how <a id="_idIndexMarker235"/>simple it is to call the Assistant API with various tools such as File Search and Code Interpreter. Additionally, you can use the Assistant API to interact with external systems through the function calling feature. These assistants can take advantage of OpenAI’s advanced <a id="_idIndexMarker236"/>language models, utilize tools such as Code Interpreter and File Search, and retain context <span class="No-Break">throughout conversations.</span></p>
			<p>In the next section, we will discuss AOAI Batch, which is designed for use cases that do not require <span class="No-Break">real-time processing.</span></p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor053"/>AOAI Batch API</h1>
			<p>Some applications require synchronous<a id="_idIndexMarker237"/> request handling, also known as real-time inferencing, where immediate<a id="_idIndexMarker238"/> responses are necessary. However, there are numerous situations wherein responses can be deferred or rate limits may restrict the speed at which multiple queries can be processed. In such cases, batch processing jobs prove useful, particularly for tasks such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>Large-scale <span class="No-Break">data processing</span></li>
				<li>Generating large volumes of content transforming data <span class="No-Break">at scale</span></li>
				<li>Evaluating LLM models and assess <span class="No-Break">comprehensive performance</span></li>
			</ul>
			<p>The AOAI Batch API provides a user-friendly suite of endpoints. These allow you to bundle multiple requests into a single file, initiate a batch job to process these requests asynchronously, check the batch’s status as the tasks run, and, finally, retrieve the consolidated results once<a id="_idIndexMarker239"/> processing <span class="No-Break">is complete.</span></p>
			<p>Compared to traditional PAUG deployments, the Batch API offers <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Cost efficiency</strong>: Provides a 50% cost reduction relative to standard <span class="No-Break">PAUG deployment</span></li>
				<li><strong class="bold">Dedicated quota</strong>: Operates with a separate enqueued token quota, distinct from the online endpoint quota, ensuring that online workloads remain unaffected; the batch quota is also <span class="No-Break">significantly larger</span></li>
				<li><strong class="bold">24-hour turnaround</strong>: Each batch completes within 24 hours, often achieving results <span class="No-Break">even faster</span></li>
			</ul>
			<p>Submitting a batch and retrieving the results involves a six-step process. Let’s go through each step <span class="No-Break">in detail:</span></p>
			<ol>
				<li><strong class="bold">Batch deployment creation</strong>: You need to first create a separate deployment for the batch. To do that, follow <span class="No-Break">these steps:</span><ol><li class="upper-roman">Log in to Azure <span class="No-Break">AI Foundry.</span></li><li class="upper-roman">Navigate to <strong class="bold">Deployments</strong> under <span class="No-Break"><strong class="bold">Shared resources</strong></span><span class="No-Break">.</span></li><li class="upper-roman">Click on <strong class="bold">Deploy model</strong> and choose <strong class="bold">Deploy </strong><span class="No-Break"><strong class="bold">base model</strong></span><span class="No-Break">.</span></li><li class="upper-roman">Choose any chat completion model such as gpt-4o-mini and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Confirm</strong></span><span class="No-Break">.</span></li><li class="upper-roman">Provide a value under <strong class="bold">Deployment name</strong>, set the <strong class="bold">Deployment</strong> type to <strong class="bold">Global Batch</strong>, adjust the <strong class="bold">Enqueued tokens</strong> value to the max limit, and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Deploy</strong></span><span class="No-Break">.</span></li></ol></li>
			</ol>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B21019_03_24.jpg" alt="Figure 3.24: Batch deployment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.24: Batch deployment</p>
			<p class="list-inset">You can also toggle <strong class="bold">Enable dynamic quota</strong>, which allows you to utilize additional quota when extra capacity <span class="No-Break">is available.</span></p>
			<p class="list-inset">After completing the <a id="_idIndexMarker240"/>preceding steps, your <strong class="bold">Global Batch</strong> deployment will be created. This deployment will then be used to run the <span class="No-Break">batch job.</span></p>
			<ol>
				<li value="2"><strong class="bold">Batch file creation</strong>: Batches begin with a <strong class="source-inline">.jsonl</strong> file, where each line specifies the details of an individual API request. Currently, the supported endpoints are <strong class="source-inline">/chat/completions</strong> (for the Chat <span class="No-Break">Completions API).</span><p class="list-inset">In this input file, the parameters in the body field of each line should match the parameters for the corresponding endpoint. Each request must include a unique <strong class="source-inline">custom_id</strong> value, which will help reference the results once processing is complete. The <strong class="source-inline">model</strong> name in the file should match the deployment name you created in the <span class="No-Break">previous step.</span></p><p class="list-inset">The following is an <a id="_idIndexMarker241"/>example of an input file, as shown below, containing three requests. Each input file must be limited to requests for a single <span class="No-Break">model only:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B21019_03_25.jpg" alt="Figure 3.25: Batch input .jsonl file"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.25: Batch input .jsonl file</p>
			<ol>
				<li value="3"><strong class="bold">Upload a batch input file</strong>: After preparing your input file, you’ll need to upload it before starting a batch job. You can upload the file either programmatically or through the <strong class="bold">Studio</strong> interface. In this example, we are using the Python SDK approach to upload the file from the local drive. See <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.26</em> <span class="No-Break">for reference.</span></li>
			</ol>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B21019_03_26.jpg" alt="Figure 3.26: Batch file upload"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.26: Batch file upload</p>
			<ol>
				<li value="4"><strong class="bold">Submit the batch job</strong>: After uploading your input file, you can use the ID of the input <strong class="source-inline">File</strong> object to<a id="_idIndexMarker242"/> initiate a batch. In this example, the file ID is <strong class="source-inline">file-0a27a5cd4d94440789971497e6d80391</strong>. Currently, the completion window is fixed at 24 hours. You can also include custom metadata using the optional <strong class="source-inline">metadata</strong> parameter, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.27</em></span><span class="No-Break">.</span><p class="list-inset">This will return a batch object with a batch ID and status field. You can find the complete details of the batch object <span class="No-Break">at </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&amp;pivots=programming-language-python#batch-object"><span class="No-Break">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&amp;pivots=programming-language-python#batch-object</span></a><span class="No-Break">.</span></p></li>
			</ol>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B21019_03_27.jpg" alt="Figure 3.27: Batch job submission"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.27: Batch job submission</p>
			<ol>
				<li value="5"><strong class="bold">Track the batch job status</strong>: After successfully creating the batch job, you can monitor its<a id="_idIndexMarker243"/> progress either through <strong class="bold">Studio</strong> or programmatically. When checking the status, it is recommended to wait at least 60 seconds between each status call, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.28</em></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B21019_03_28.jpg" alt="Figure 3.28: Batch job status check"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.28: Batch job status check</p>
			<p class="list-inset">The status of a given Batch object can be any of the following shown in <span class="No-Break"><em class="italic">Table 3.1</em></span><span class="No-Break">.</span></p>
			<table id="table001-1" class="T---Table _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Header">
							<p class="list-inset"><span class="No-Break"><strong class="bold">Status</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Header">
							<p class="list-inset"><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="source-inline">validating</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">The<a id="_idIndexMarker244"/> input file is being validated before the batch <span class="No-Break">can start</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="source-inline">failed</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">The input file failed the <span class="No-Break">validation process</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="source-inline">in_progress</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">The input file was validated successfully, and the batch <span class="No-Break">is running</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="source-inline">finalizing</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">The batch has finished, and the results are <span class="No-Break">being prepared</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="source-inline">completed</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">The batch has been completed, and the results <span class="No-Break">are ready</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="source-inline">expired</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">The batch was not completed within the 24-hour <span class="No-Break">time frame</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="source-inline">cancelling</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">The batch is in the process of being canceled (may take up to <span class="No-Break">10 minutes)</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="source-inline">cancelled</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">The batch <span class="No-Break">was canceled</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.1: Batch job status table</p>
			<ol>
				<li value="6"><strong class="bold">Retrieve the results</strong>: Once the batch is complete, you can download the output by making a request to the Files API using <strong class="source-inline">output_file_id</strong> from the <strong class="source-inline">Batch</strong> object. Then save it to a file on your machine, such as <strong class="source-inline">batch_output.jsonl</strong>. The <strong class="source-inline">output .jsonl</strong> file will contain one response per successful request from the input file. Any failed requests will include their error details in<a id="_idIndexMarker245"/> a separate error file, accessible via the <span class="No-Break">batch’s </span><span class="No-Break"><strong class="source-inline">error_file_id</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">The order of output lines may not match the input order. Instead of relying on the sequence, use the <strong class="source-inline">custom_id</strong> field present in each output line to correlate input requests with their <span class="No-Break">corresponding results.</span></p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B21019_03_29.jpg" alt="Figure 3.29: Retrieving the output file"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.29: Retrieving the output file</p>
			<p>By following the steps outlined, you can submit batch <span class="No-Break">jobs manually.</span></p>
			<p>While these steps are suitable for demonstration purposes, enterprises with millions of files in a blob storage account will require an automated solution to submit batches and retrieve results efficiently. For such cases, you can use the following <span class="No-Break">solution accelerator:</span></p>
			<p><a href="https://github.com/Azure-Samples/aoai-batch-api-accelerator"><span class="No-Break">https://github.com/Azure-Samples/aoai-batch-api-accelerator</span></a></p>
			<p>The AOAI Batch API has certain service limits, which can be found at <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&amp;pivots=programming-language-python#global-batch-limits">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&amp;pivots=programming-language-python#global-batch-limits</a>. Quota limits are outlined at <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&amp;pivots=programming-language-python#global-batch-quota">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&amp;pivots=programming-language-python#global-batch-quota</a>. These limits can be increased based on your workload; to request an increase, you will need to <span class="No-Break">contact Microsoft.</span></p>
			<p>You can also process<a id="_idIndexMarker246"/> images in bulk using the AOAI Batch API. This feature is available with specific multi-modal models, and currently, only GPT-4o supports images in batch requests. Images can be provided either as a URL or as base64-encoded data. Note that GPT-4 Turbo does not support image inputs for batch processing at <span class="No-Break">this time.</span></p>
			<p>AOAI batch processing is global by nature, meaning data processing could occur anywhere in the world. This may raise concern for industries with strict regulatory requirements. However, you can select a data zone for the AOAI Batch, which restricts data processing to specific geos. By choosing the US data zone, processing will occur in one of the US regions, and by selecting the EU data zone, processing will take place in one of the EU regions. This ensures enterprises can comply with data compliance and <span class="No-Break">regulatory requirements.</span></p>
			<p>Now that we’ve covered the AOAI Batch API, the next section will focus on fine-tuning, which allows you to customize your model for <span class="No-Break">specific tasks.</span></p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/>AOAI fine-tuning</h1>
			<p>Fine-tuning allows you to maximize the <a id="_idIndexMarker247"/>potential of models available through the Azure AI Foundry or API by providing <span class="No-Break">the following:</span></p>
			<ul>
				<li>Improved response quality compared to basic <span class="No-Break">prompting alone</span></li>
				<li>The capability to train on larger datasets, surpassing the limitations model <span class="No-Break">context window</span></li>
				<li>Reduced token usage by minimizing the <span class="No-Break">prompt length</span></li>
				<li>Faster response times with <span class="No-Break">lower-latency requests</span></li>
			</ul>
			<p>AOAI’s text generation models are<a id="_idIndexMarker248"/> pre-trained on an extensive corpus of text data. To use them effectively, users often include instructions and example cases in prompts—a <a id="_idIndexMarker249"/>method known as <strong class="bold">few-shot learning</strong>. Few-shot learning demonstrates how to complete a task by showing a small number of examples within the <span class="No-Break">prompt itself.</span></p>
			<p>Fine-tuning enhances the few-shot approach by training the model on significantly more examples than could fit into a single prompt, thereby improving performance across a wider range of tasks. After a model is fine-tuned, fewer examples are typically needed in the prompt, which reduces token costs and further lowers <span class="No-Break">response latency.</span></p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor055"/>Ideal situation to leverage fine-tuning</h2>
			<p>When evaluating whether fine-tuning is the appropriate approach for a specific use case, it’s beneficial to understand some<a id="_idIndexMarker250"/> <span class="No-Break">foundational concepts:</span></p>
			<ul>
				<li><strong class="bold">Prompt engineering</strong>: This technique involves crafting prompts for natural language processing models<a id="_idIndexMarker251"/> with precision. By designing prompts carefully, users can improve the accuracy and relevance of model responses, enhancing overall performance. <span class="No-Break">Chapter 13</span> provides an in-depth exploration of various prompt <span class="No-Break">engineering techniques.</span></li>
				<li><strong class="bold">RAG</strong>: RAG enhances the <a id="_idIndexMarker252"/>effectiveness of LLMs by integrating external data into the prompt. By retrieving relevant information from outside sources, RAG enables businesses to create tailored solutions that are both cost-effective and <span class="No-Break">contextually accurate.</span></li>
				<li><strong class="bold">Fine-tuning</strong>: Fine-tuning involves adapting an existing LLM by retraining it on specific example data, resulting in a <em class="italic">custom</em> model that is fine-tuned to reflect the nuances and requirements <a id="_idIndexMarker253"/>of the provided examples <span class="No-Break">data set.</span></li>
			</ul>
			<p>AOAI fine-tuning is a <strong class="bold">supervised fine-tuning</strong> process, as<a id="_idIndexMarker254"/> opposed to continuous pre-training or <strong class="bold">Reinforcement Learning Through Human Feedback</strong> (<strong class="bold">RLHF</strong>). Supervised fine-tuning involves <a id="_idIndexMarker255"/>retraining pre-trained models on carefully selected datasets to enhance performance on particular tasks. We recommend starting with techniques such as prompt engineering, prompt chaining (dividing complex tasks into smaller, manageable prompts), and utilizing function calling to achieve <span class="No-Break">optimal results.</span></p>
			<p>AOAI uses <strong class="bold">Low-Rank Approximation</strong> (<strong class="bold">LoRA</strong>) to<a id="_idIndexMarker256"/> fine-tune models efficiently by lowering their complexity with minimal impact on performance. This approach approximates the model’s <a id="_idIndexMarker257"/>original high-dimensional matrix using a lower-dimensional one, allowing only a subset of key parameters to be fine-tuned during supervised training. By focusing on these <em class="italic">essential</em> parameters, the model remains both efficient and easier to manage. For users, this results in faster training and more cost-effective fine-tuning compared to <span class="No-Break">traditional methods.</span></p>
			<p>Fine-tuning is a sophisticated process that demands a solid domain and data understanding to apply effectively. The five common questions that follow are designed to help you assess your readiness for fine-tuning, guiding you through key considerations and helping you decide whether fine-tuning is the best approach or whether alternative methods may be <span class="No-Break">more suitable:</span></p>
			<ul>
				<li><strong class="bold">Why fine-tune </strong><span class="No-Break"><strong class="bold">a model?</strong></span><p class="list-inset">To proceed effectively with fine-tuning, you<a id="_idIndexMarker258"/> should clearly define a specific use case and identify the model you intend to fine-tune. Good candidates for fine-tuning include situations where you need the model to produce outputs in a particular style, tone, or format or when the instructions or data needed to guide the model are too complex or lengthy to fit into a <span class="No-Break">standard prompt.</span></p><p class="list-inset">Here are some indicators that you may not be ready for <span class="No-Break">fine-tuning yet:</span></p><ul><li><strong class="bold">Unclear use case</strong>: If you can’t articulate a clear purpose beyond <em class="italic">I want to improve a model</em>, fine-tuning may not be the right <span class="No-Break">next step.</span></li><li><strong class="bold">Cost-driven motivation</strong>: Fine-tuning can reduce costs in cases where it allows for shorter prompts or smaller model usage. However, it also involves an upfront cost for training and hosting a custom model. Be mindful of these expenses and refer to AOAI’s pricing page for more detail on <span class="No-Break">fine-tuning costs.</span></li><li><strong class="bold">Out-of-domain knowledge needs</strong>: If your primary goal is to incorporate information beyond the model’s original training scope, consider starting with RAG. AOAI’s RAG features, such as embedding-based retrieval on your data, can offer a more flexible and often more affordable solution depending on your specific data <span class="No-Break">and objectives.</span></li></ul></li>
				<li><strong class="bold">What have you tried </strong><span class="No-Break"><strong class="bold">so far?</strong></span><p class="list-inset">Fine-tuning is an advanced capability and is not typically the first step in working with generative AI. It’s essential to be familiar with the fundamentals of LLMs and to start by testing<a id="_idIndexMarker259"/> the model’s performance with prompt engineering and/or RAG. These techniques help you establish a baseline performance level, which is critical for evaluating whether fine-tuning has genuinely improved <span class="No-Break">your model.</span></p><p class="list-inset">A performance baseline without fine-tuning also serves as a safeguard: it helps detect any negative impacts from fine-tuning, as poorly prepared training data can degrade <span class="No-Break">model quality.</span></p><p class="list-inset">Let’s look at some key indicators that you’re ready <span class="No-Break">for fine-tuning:</span></p><ul><li><strong class="bold">Experience with prompt engineering and RAG</strong>: You should be able to demonstrate knowledge and results from prompt engineering or <span class="No-Break">RAG-based approaches</span></li><li><strong class="bold">Documented challenges and use case testing</strong>: Have specific examples of where prompt engineering or RAG fell short in your <span class="No-Break">use case</span></li><li><strong class="bold">Quantitative baseline assessments</strong>: Whenever possible, have measurable benchmarks of model performance <span class="No-Break">without fine-tuning</span></li></ul><p class="list-inset">Next, let’s look at some common signs fine-tuning may not be <span class="No-Break">suitable yet:</span></p><ul><li>Starting with fine-tuning without testing other <span class="No-Break">available techniques</span></li><li>Lacking a clear understanding of how fine-tuning specifically <span class="No-Break">enhances LLMs</span></li><li>No benchmark data to measure the impact <span class="No-Break">of fine-tuning</span></li></ul></li>
				<li><strong class="bold">What should you do when alternate approaches </strong><span class="No-Break"><strong class="bold">aren’t working?</strong></span><p class="list-inset">Identifying the limitations of prompt engineering or RAG can help clarify whether fine-tuning is necessary. Ask yourself the <span class="No-Break">following questions:</span></p><ul><li>Does the base model struggle with edge cases <span class="No-Break">or exceptions?</span></li><li>Does it produce inconsistent formats, and are you unable to fit enough examples in the context window to <span class="No-Break">guide it?</span></li></ul><p class="list-inset">Examples of where the base model or prompt engineering falls short can guide you in collecting the right data for fine-tuning, as well as determining how to evaluate the effectiveness of a <span class="No-Break">fine-tuned model.</span></p><p class="list-inset">Let’s look at an example scenario. We’ll say that a user aimed to use GPT-4o-mini to convert natural language <a id="_idIndexMarker260"/>questions into queries written in a non-standard query language. While they specified <strong class="source-inline">Always return SQL</strong> in the prompt and used RAG to retrieve the database schema, the model often produced incorrect syntax, particularly in edge cases. To address this, they gathered thousands of examples of questions and their equivalent database queries, including previous model failures, and used this data to fine-tune the model. The resulting fine-tuned model, combined with their engineered prompt and retrieval setup, achieved the accuracy needed for <span class="No-Break">real-world application.</span></p><p class="list-inset">Here are some indicators that you’re ready <span class="No-Break">for fine-tuning:</span></p><ul><li><strong class="bold">Documented examples of previous attempts</strong>: You have tested various prompt engineering or RAG solutions and documented <span class="No-Break">specific limitations.</span></li><li><strong class="bold">Identified model shortcomings</strong>: These could include inconsistent handling of edge cases, an inability to include enough few-shot prompts within the context window, or issues <span class="No-Break">with latency</span></li></ul><p class="list-inset">On the other hand, here are some signs that you may need to wait <span class="No-Break">before fine-tuning:</span></p><ul><li>Lack of in-depth understanding of the model’s limitations or the <span class="No-Break">data needed</span></li><li>Difficulty identifying suitable data to train the <span class="No-Break">model effectively</span></li></ul></li>
				<li><strong class="bold">What data will you use </strong><span class="No-Break"><strong class="bold">for fine-tuning?</strong></span><p class="list-inset">Even with a strong use case, the success of fine-tuning largely depends on the quality of the data you provide. It’s crucial to invest the necessary time and resources into gathering high-quality, curated data. Different models may require varying volumes of data, but in most cases, you will need to provide a large quantity of well-curated examples to achieve <span class="No-Break">meaningful improvements.</span></p><p class="list-inset">In addition to data <a id="_idIndexMarker261"/>quality, the format of the data is equally important. Even high-quality data may require significant effort to format properly for fine-tuning. This may involve allocating engineering resources to ensure the data is <span class="No-Break">structured correctly.</span></p><p class="list-inset">Some indicators that you’re ready for fine-tuning include <span class="No-Break">the following:</span></p><ul><li><strong class="bold">Identified dataset</strong>: You have already<a id="_idIndexMarker262"/> selected the dataset you intend to use <span class="No-Break">for fine-tuning</span></li><li><strong class="bold">Correct format</strong>: The dataset is structured in the appropriate format for the <span class="No-Break">chosen model</span></li><li><strong class="bold">Curation effort</strong>: Some level of dataset curation has been applied to ensure the <span class="No-Break">data’s quality</span></li></ul><p class="list-inset">On the other hand, here are some common signs you may not be ready <span class="No-Break">for fine-tuning:</span></p><ul><li><strong class="bold">No dataset identified</strong>: You have not yet chosen the dataset <span class="No-Break">for fine-tuning</span></li><li><strong class="bold">Incorrect format</strong>: The dataset <a id="_idIndexMarker263"/>format does not align with the requirements of the model you intend <span class="No-Break">to fine-tune.</span></li></ul></li>
				<li><strong class="bold">How will you measure the quality of your </strong><span class="No-Break"><strong class="bold">fine-tuned model?</strong></span><p class="list-inset">There’s no one-size-fits-all approach to measuring the success of a fine-tuned model, but it’s essential to have clear, well-defined goals. Success should not only be evaluated qualitatively but also include quantitative metrics. A good approach is to use a <em class="italic">holdout validation dataset</em> to assess performance objectively. Additionally, you can enhance your evaluation by conducting <em class="italic">user acceptance testing</em> or performing <em class="italic">A/B testing</em>, comparing the fine-tuned model to the base model to <a id="_idIndexMarker264"/>see whether the improvements meet <span class="No-Break">your expectations.</span></p></li>
			</ul>
			<p>To optimize the model’s context, you should explore techniques such as prompt engineering and RAG. For optimizing the LLM itself, focus on prompt engineering followed by fine-tuning, as illustrated in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.30</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B21019_03_30.jpg" alt="Figure 3.30: Optimizing LLM accuracy"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.30: Optimizing LLM accuracy</p>
			<p>Once you’ve identified fine-tuning as the appropriate strategy, you can follow the steps detailed in the subsequent section to effectively fine-tune <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>Fine-tuning steps</h2>
			<p>At a high level, the fine-tuning<a id="_idIndexMarker265"/> process includes the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Prepare and upload <span class="No-Break">training data.</span></li>
				<li>Train a new <span class="No-Break">fine-tuned model.</span></li>
				<li>Conduct a <span class="No-Break">safety evaluation.</span></li>
				<li>Conduct a model <span class="No-Break">performance evaluation.</span></li>
				<li>Deploy the <span class="No-Break">fine-tuned model.</span></li>
			</ol>
			<p>Let’s go through each step <span class="No-Break">in detail:</span></p>
			<ol>
				<li><strong class="bold">Preparing and uploading training data</strong>: Your training and validation datasets should include input and output examples that demonstrate the desired model behavior. These datasets <a id="_idIndexMarker266"/>must be formatted as <strong class="bold">JSON Lines</strong> (<strong class="bold">JSONL</strong>) documents. For models such as gpt-3.5-turbo (all versions), gpt-4, gpt-4o, and gpt-4o-mini, the fine-tuning data should adhere to the conversational format used by the Chat completions API. Here is the <span class="No-Break">sample format:</span></li>
			</ol>
			<table id="table002-1" class="T---Table _idGenTablePara-1">
				<colgroup>
					<col/>
				</colgroup>
				<tbody>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">{“messages“: [{“role”: “system”, “content”: “You are an Ontoso customer support agent who helps customers with issues related to their Ontoso accounts and services. You are friendly and concise. You only provide factual answers to queries related to Ontoso.”}, {“role”: “user”, “content”: “I need help with my Ontoso account. I forgot my password.”}, {“role”: “assistant”, “content”: “Hi! I’m happy to help. Please provide your registered email address, and I’ll guide you through resetting <span class="No-Break">your password.”}]}</span></p>
							<p class="list-inset">{“messages”: [{“role”: “system”, “content”: “You are an Ontoso customer support agent who helps customers with issues related to their Ontoso accounts and services. You are friendly and concise. You only provide factual answers to queries related to Ontoso.”}, {“role”: “user”, “content”: “Can I track my order on Ontoso?”}, {“role”: “assistant”, “content”: “Yes, you can track your order by logging into your Ontoso account and checking the ‘Order History’ section. There, you can see the status of your current and <span class="No-Break">past orders.”}]}</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.2: Sample training data JSONL format</p>
			<p class="list-inset">Besides being in JSONL<a id="_idIndexMarker267"/> format, the training and validation data files must be encoded in UTF-8 with a <strong class="bold">byte-order mark</strong> (<strong class="bold">BOM</strong>). Additionally, the file size must not exceed <span class="No-Break">512 MB.</span></p>
			<p class="list-inset">You can also include <a id="_idIndexMarker268"/>multiple turns of a conversation in a single line of your JSONL training file. To exclude specific assistant messages from fine-tuning, you can add the optional weight key-value pair. Currently, the weight can be set to either <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>. Here is another multiturn <span class="No-Break">sample format:</span></p>
			<table id="table003-1" class="T---Table _idGenTablePara-1">
				<colgroup>
					<col/>
				</colgroup>
				<tbody>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">{“messages”: [{“role”: “system”, “content”: “You are a Contoso customer support agent who is friendly and concise, but also has a sarcastic tone.”}, {“role”: “user”, “content”: “What are your business hours?”}, {“role”: “assistant”, “content”: “Our business hours are 9 AM to 5 PM, Monday through Friday.”, “weight”: 0}, {“role”: “user”, “content”: “Can you be more sarcastic?”}, {“role”: “assistant”, “content”: “Sure, we’re open during the highly unconventional hours of 9 AM to 5 PM, Monday through Friday. A real shocker, right?”, “<span class="No-Break">weight”: 1}]}</span></p>
							<p class="list-inset">{“messages”: [{“role”: “system”, “content”: “You are a Contoso customer support agent who is friendly and concise, but also has a sarcastic tone.”}, {“role”: “user”, “content”: “How do I reset my password?”}, {“role”: “assistant”, “content”: “You can reset your password by clicking the ‘Forgot Password’ link on the login page.”, “weight”: 0}, {“role”: “user”, “content”: “Can you be more sarcastic?”}, {“role”: “assistant”, “content”: “Oh, you just click the ‘Forgot Password’ link. It’s a groundbreaking feature that’s been around for ages.”, “<span class="No-Break">weight”: 1}]}</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.3: sample multiturn training data JSONL format</p>
			<p class="list-inset">The more training examples you provide, the better the fine-tuning results will be. While fine-tuning requires at least 10 training examples to proceed, this minimal number is usually insufficient to have a meaningful impact on the model’s performance. Ideally, you should provide hundreds or even thousands of examples for <span class="No-Break">optimal results.</span></p>
			<p class="list-inset">Generally, increasing the <a id="_idIndexMarker269"/>dataset size can lead to a proportional improvement in model quality. However, it’s important to note that low-quality examples can harm the model’s performance. If you train the model with a large amount of internal data without carefully curating it to include only high-quality examples, the model’s performance may end up being worse <span class="No-Break">than anticipated.</span></p>
			<ol>
				<li value="2"><strong class="bold">Training a new fine-tuned model</strong>: Training a new fine-tuned model can be done either through Azure AI Foundry or using the API. For this demonstration, we will use Azure AI Foundry to illustrate how you can easily fine-tune a base chat completion model. AOAI supports a range of models for fine-tuning. To view the complete list of supported models, please refer <span class="No-Break">to </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning?tabs=azure-openai%2Ccompletionfinetuning%2Cpython-new&amp;pivots=programming-language-studio#models"><span class="No-Break">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning?tabs=azure-openai%2Ccompletionfinetuning%2Cpython-new&amp;pivots=programming-language-studio#models</span></a><span class="No-Break">:</span><ol><li class="upper-roman">To begin, open Azure AI Foundry at <a href="https://oai.azure.com/">https://oai.azure.com/</a> and sign in using credentials that have access to your AOAI resource. During the sign-in process, ensure you select the correct directory, Azure subscription, and AOAI resource associated with <span class="No-Break">your account.</span></li><li class="upper-roman">In Azure AI Foundry, navigate to the <strong class="bold">Tools</strong> section in the left-hand menu, then select the <strong class="bold">Fine-tuning</strong> pane. From there, click on the <strong class="bold">Fine-tune model</strong> option to begin the <span class="No-Break">fine-tuning process.</span></li></ol></li>
			</ol>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B21019_03_31.jpg" alt="Figure 3.31: AOAI fine-tuning wizard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.31: AOAI fine-tuning wizard</p>
			<ol>
				<li class="upper-roman" value="3">The first step in creating a custom model is to select a base model. In the <strong class="bold">Base model</strong> pane, you <a id="_idIndexMarker270"/>can choose a base model from the <strong class="bold">Base model type</strong> drop-down menu. Your choice will impact both the performance and the cost of the custom model you’re creating. Once you’ve selected the base model, click <strong class="bold">Next</strong> to proceed with the fine-tuning process, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.31</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B21019_03_32.jpg" alt="Figure 3.32: Selecting base model to fine-tune"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.32: Selecting base model to fine-tune</p>
			<p class="list-inset">AOAI also supports incremental fine-tuning, meaning you can fine-tune a model that has already been fine-tuned. This allows you to continue improving the model’s performance<a id="_idIndexMarker271"/> by further training it on new or updated datasets, enhancing its ability to handle more specific tasks or respond to <span class="No-Break">evolving needs.</span></p>
			<ol>
				<li class="upper-roman" value="4">To proceed, you can select from your previously uploaded training datasets or upload new ones, based on your specific customization needs. The <strong class="bold">Training Data</strong> section will show all available datasets, allowing you to review and choose from existing options or upload fresh data for training purposes. In this example, we demonstrate the process of uploading a new dataset directly from the local drive, as illustrated in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.32</em></span><span class="No-Break">.</span></li>
			</ol>
			<p class="list-inset">For handling large data files, it’s advisable to import them directly from Azure Blob Storage. Uploading sizable files through multipart forms can lead to instability, as these uploads<a id="_idIndexMarker272"/> rely on atomic requests, which means they cannot be resumed or retried if interrupted. Using Azure Blob Storage for such transfers ensures greater reliability and fault tolerance, especially when dealing with <span class="No-Break">larger datasets.</span></p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B21019_03_33.jpg" alt="Figure 3.33: Training data upload"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.33: Training data upload</p>
			<ol>
				<li class="upper-roman" value="5">In the next step, you’ll find options to set up validation data for your model training process. If validation data is not required, simply select <strong class="bold">Next</strong> to proceed directly to the advanced configuration settings. However, if you wish to incorporate validation data, you can either select from your existing datasets or upload a new validation dataset specifically prepared for <span class="No-Break">this purpose.</span></li>
			</ol>
			<p class="list-inset">The <strong class="bold">Validation Data</strong> section<a id="_idIndexMarker273"/> displays all available training and validation datasets, providing flexibility to either use existing data or add new validation data as needed for model customization. In this example, we demonstrate the process of uploading a new validation dataset directly from the local drive, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B21019_03_34.jpg" alt="Figure 3.34: Validation data upload"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.34: Validation data upload</p>
			<ol>
				<li class="upper-roman" value="6">The <strong class="bold">Create custom model</strong> wizard allows you to configure various parameters for training your fine-tuned model in the <strong class="bold">Task</strong> parameters section. Here’s an overview of the <span class="No-Break">parameters available:</span><ul><li><strong class="source-inline">batch_size</strong> (integer): This <a id="_idIndexMarker274"/>specifies the number of training examples processed in a single forward and backward pass. Generally, larger batch sizes are recommended for larger datasets, as they tend to stabilize the training process. Higher batch sizes reduce the frequency of model parameter updates, leading to lower variance <span class="No-Break">in updates.</span></li><li><strong class="source-inline">learning_rate_multiplier</strong> (number): This is a multiplier applied to the pre-training learning rate to set the fine-tuning learning rate. Larger values can improve training efficiency with larger batch sizes but may risk overfitting if they’re too high. It’s often effective to experiment with values <a id="_idIndexMarker275"/>between <strong class="source-inline">0.02</strong> and <strong class="source-inline">0.2</strong> to find an <span class="No-Break">optimal rate.</span></li><li><strong class="source-inline">n_epochs</strong> (integer): This refers to the number of epochs, or complete passes through the dataset, for which the model is trained. Each epoch represents one full cycle of learning from <span class="No-Break">the dataset.</span></li><li><strong class="source-inline">seed</strong> (integer): This controls the reproducibility of training runs. Setting a specific seed value ensures that the training results are consistent across runs, assuming the same job parameters. If left unspecified, a seed will automatically <span class="No-Break">be generated.</span></li></ul></li>
			</ol>
			<p class="list-inset">Select <strong class="bold">Default</strong> to use the default values for the fine-tuning job or select <strong class="bold">Custom</strong> to display and edit the hyperparameter values. When <strong class="bold">Default</strong> is selected, Microsoft determines the correct value algorithmically based on your training data, as <span class="No-Break">shown here:</span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B21019_03_35.jpg" alt="Figure 3.35: Hyperparameter selection"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.35: Hyperparameter selection</p>
			<p class="list-inset">After you configure the advanced options, select <strong class="bold">Next</strong> to review your choices and train your <span class="No-Break">fine-tuned model.</span></p>
			<ol>
				<li class="upper-roman" value="7">In the <strong class="bold">Review</strong> pane of the<a id="_idIndexMarker276"/> wizard, you can view a summary of your selected configuration settings. Once you’ve verified that all configurations are correct, click <strong class="bold">Submit</strong> to initiate the finetuning job. After submission, you’ll be redirected to the <strong class="bold">Models</strong> pane, where you can monitor the status and progress of your fine-tuning task. This final step confirms your setup and starts the model <span class="No-Break">training process.</span></li>
				<li class="upper-roman">The <strong class="bold">Models</strong> pane provides a detailed overview of your custom model’s fine-tuning process, displaying key information about the fine-tuning job’s status and results, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.36</em>. Once you initiate a fine-tuning job, it may take some time to finish. Your job could be placed in a queue behind other jobs in the system. The duration of the training process can vary, taking anywhere from a few minutes to several hours, depending on the size of the model and <a id="_idIndexMarker277"/>dataset. Click <strong class="bold">Refresh</strong> to update the information on the <span class="No-Break">status page.</span></li>
			</ol>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B21019_03_36.jpg" alt="Figure 3.36: Fine Tuning Status"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.36: Fine Tuning Status</p>
			<p class="list-inset">After completing the seven steps mentioned here, you will have a fine-tuned model. During the training process, a checkpoint is generated at the end of each training epoch. A checkpoint represents a fully functional version of the model that can be deployed and used as the base model for future fine-tuning jobs. These checkpoints are valuable because they offer a snapshot of your model before overfitting may occur. Upon completion of a fine-tuning job, you will have access to the three most recent versions of the model, which can be deployed <span class="No-Break">as needed.</span></p>
			<ol>
				<li class="upper-roman" value="9">GPT-4o and GPT-4o-mini are our most advanced models, designed to be fine-tuned to meet your specific needs. However, as with all AOAI models, fine-tuned versions of these models come with added responsible AI challenges, including risks related to harmful content, manipulation, human-like behavior, privacy concerns, <span class="No-Break">and more.</span></li>
			</ol>
			<p class="list-inset">To mitigate these risks, additional evaluation steps have been implemented to detect and prevent harmful content in the training and outputs of fine-tuned models. These measures are aligned with the Microsoft Responsible AI Standard and AOAI Service content <span class="No-Break">filtering policies.</span></p>
			<p class="list-inset">Key evaluation features include the <span class="No-Break">following :</span></p>
			<ul>
				<li><strong class="bold">Dedicated private workspaces</strong> for each customer to ensure security and privacy <span class="No-Break">during evaluations</span></li>
				<li><strong class="bold">Evaluation endpoints</strong> located <a id="_idIndexMarker278"/>within the same geographic region as the AOAI resource to maintain compliance<a id="_idIndexMarker279"/> with regional <span class="No-Break">data policies</span></li>
				<li><strong class="bold">Training data privacy</strong> is ensured because data used in evaluations is not stored; only the final model assessment (whether deployable or not) <span class="No-Break">is retained</span></li>
				<li><strong class="bold">Predefined evaluation filters</strong>: The filters for GPT-4o, GPT-4o-mini, and GPT-4 fine-tuned models are set to fixed thresholds and cannot be altered by customers; these filters are independent of any custom content filtering configurations you may have <span class="No-Break">set up</span></li>
			</ul>
			<p class="list-inset">These steps are designed to help ensure that the fine-tuned models adhere to responsible AI practices and minimize the risk of generating harmful or <span class="No-Break">inappropriate content.</span></p>
			<p class="list-inset">The AOAI fine-tuning service incorporates two key safeguards to promote the responsible and ethical use of the AOAI fine-tuning service during the <span class="No-Break">training process:</span></p>
			<ol>
				<li class="upper-roman" value="10"><strong class="bold">Data evaluation</strong>: Before training begins, your data undergoes an evaluation to identify any <a id="_idIndexMarker280"/>harmful content, such as violence, explicit material, hate speech, fairness concerns, and self-harm. If the data contains harmful content that surpasses a certain threshold of severity, the training process will be halted, and you will receive an alert, as shown in <em class="italic">Table 3.4</em>, detailing the specific categories of harmful content <a id="_idIndexMarker281"/>responsible for <span class="No-Break">the failure.</span><table id="table004-1" class="T---Table _idGenTablePara-1"><colgroup><col/></colgroup><tbody><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><strong class="source-inline">The provided training data failed RAI checks for harm types: [hate_fairness, self_harm, violence]. Please fix the data and </strong><span class="No-Break"><strong class="source-inline">try again.</strong></span></p></td></tr></tbody></table></li>
			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.3: Training data evaluation notification</p>
			<p class="list-inset">Your training data is automatically assessed during the data import process as part of enabling the fine-tuning feature. If harmful content is detected in the training data, causing the fine-tuning job to fail, you will not incur <span class="No-Break">any charges.</span></p>
			<ol>
				<li class="upper-roman" value="11"><strong class="bold">Model evaluation</strong>: Once training is finished, before the fine-tuned model is deployed, it undergoes an <a id="_idIndexMarker282"/>evaluation to assess the potential for harmful responses using Azure’s built-in risk and safety metrics. This evaluation mirrors the testing process applied to base LLMs. It simulates a conversation with the fine-tuned model to determine whether it could produce harmful content based on predefined categories (violence, sexual content, hate speech, fairness issues, and self-harm). If the model generates harmful content at a rate above an <a id="_idIndexMarker283"/>acceptable threshold, you will be notified, as shown in <em class="italic">Table 3.5</em>, that the model is not ready for deployment. You’ll also be given details about the specific harmful content <span class="No-Break">categories identified.</span><table id="table005-1" class="T---Table _idGenTablePara-1"><colgroup><col/></colgroup><tbody><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset">This model is unable to be deployed. Model evaluation identified that this fine-tuned model scores above acceptable thresholds for [Violence, Self Harm]. Please retrain your model with a <span class="No-Break">safe dataset.</span></p></td></tr></tbody></table></li>
			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.4: Model evaluation notification</p>
			<p class="list-inset">Similar to data evaluation, the model is automatically assessed during the fine-tuning job as part of the fine-tuning process. Only the final assessment—whether the model is deployable or not—is recorded by the service. If the deployment of the fine-tuned model fails due to harmful content detected in the model’s outputs, you will not be charged for the <span class="No-Break">training session.</span></p>
			<ol>
				<li class="upper-roman" value="12"><strong class="bold">Model performance evaluation</strong>: After the<a id="_idIndexMarker284"/> fine-tuning job completes, AOAI provides a results file named <strong class="source-inline">results.csv</strong> for each job. This file helps you analyze the performance of your custom model during training and validation. You can find the file ID for the result file under the <strong class="source-inline">Result file id</strong> column on the <strong class="bold">Models</strong> pane in Azure AI Foundry, which allows you to download the file from the <strong class="bold">Data </strong><span class="No-Break"><strong class="bold">files</strong></span><span class="No-Break"> pane.</span></li>
			</ol>
			<p class="list-inset">The <strong class="source-inline">results.csv</strong> file contains the following columns, as shown in <span class="No-Break"><em class="italic">Table 3.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B21019_03_Table_3.5.jpg" alt="Table: 3.5: Model training and validation performance"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table: 3.5: Model training and validation performance</p>
			<p class="list-inset">In Azure AI Foundry, you can visualize the data from your <strong class="source-inline">results.csv</strong> file as graphs. By selecting the<a id="_idIndexMarker285"/> link for your trained model, you will be able to view two key charts: <strong class="bold">Loss</strong> and <strong class="bold">Token accuracy</strong>. If you’ve provided validation data, the results for both the training and validation datasets will be displayed on the same plot, as <span class="No-Break">shown here:</span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B21019_03_37.jpg" alt="Figure 3.37: Fine Tuning metrics"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.37: Fine Tuning metrics</p>
			<p class="list-inset">Here’s what to watch for in <span class="No-Break">the plots:</span></p>
			<ul>
				<li><em class="italic">Loss should decrease over time</em>, indicating that the model is improving as <span class="No-Break">it learns.</span></li>
				<li><em class="italic">Accuracy should increase</em>, showing that the model is getting better at <span class="No-Break">predicting tokens.</span></li>
				<li>If you notice a <em class="italic">divergence</em> between the training and validation data (i.e., training loss continues to <a id="_idIndexMarker286"/>decrease while validation loss increases or plateaus), this could be a sign of <em class="italic">overfitting</em>. In such cases, you may want to: do <span class="No-Break">the following:</span><ul><li>Train the model with <span class="No-Break">fewer epochs</span></li><li>Use a smaller learning rate multiplier to prevent the model from fitting too closely to the <span class="No-Break">training data</span></li></ul></li>
			</ul>
			<p class="list-inset">When fine-tuning a model, there are several important considerations to ensure <span class="No-Break">optimal performance:</span></p>
			<ul>
				<li><strong class="bold">Missing system message</strong>: It’s crucial to provide a consistent system message during fine-tuning and when using the fine-tuned model. If the system message changes, the<a id="_idIndexMarker287"/> model may produce results that differ from what you intended during fine-tuning. Therefore, the system message you use for deployment should match the one you used in the training process to <span class="No-Break">maintain consistency.</span></li>
				<li><strong class="bold">Not enough data</strong>: While the<a id="_idIndexMarker288"/> minimum required data for the fine-tuning pipeline to run is 10 examples, using hundreds or even thousands of data points is recommended for teaching the model new skills. With too few data points, there is a risk of overfitting, whereby the model memorizes specific examples rather than generalizing patterns. This can lead to poor performance when applied to real-world, unseen data. To achieve the best results, aim to prepare a dataset with hundreds or thousands of diverse <span class="No-Break">data points.</span></li>
				<li><strong class="bold">Bad data</strong>: The quality of your training data directly impacts the quality of your fine-tuned model. A poorly curated or biased dataset can lead the model to learn inaccurate patterns. For instance, if you train a customer service chatbot only with data for one scenario (e.g., returns), it will struggle to handle other situations. Additionally, if the training data contains incorrect or misleading information, the model will learn to generate faulty or biased responses. Always ensure that your dataset is diverse, accurate, and representative of the tasks you expect the model <span class="No-Break">to handle.</span></li>
			</ul>
			<ol>
				<li value="3"><strong class="bold">Deploying the fine-tuned model</strong>: Once the fine-tuning job is successful, you can deploy your custom model through the <strong class="bold">Models</strong> pane in Azure AI Foundry. Deployment is necessary for making the fine-tuned model available for use in completion calls. To deploy the model, simply select the custom model and click <strong class="bold">Deploy model</strong>, as <span class="No-Break">shown here:</span></li>
			</ol>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B21019_03_38.jpg" alt="Figure 3.38: Fine-tuned model deployment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.38: Fine-tuned model deployment</p>
			<p class="list-inset">When you open the <strong class="bold">Deploy model</strong> dialog box, you will need to enter a deployment name for your custom model. After entering the name, click <strong class="bold">Deploy</strong> to initiate the deployment process for your fine-tuned model. You can track the deployment progress in the <strong class="bold">Deployments</strong> pane of Azure <span class="No-Break">AI Foundry.</span></p>
			<p class="list-inset">AOAI fine-tuning also <a id="_idIndexMarker289"/>supports the flexibility to deploy your custom model to a different region from where it was originally fine-tuned, including across different subscriptions <span class="No-Break">and regions.</span></p>
			<p class="list-inset">However, there are a few key limitations <span class="No-Break">to consider:</span></p>
			<ul>
				<li>The target region must <span class="No-Break">support fine-tuning</span></li>
				<li>If deploying across subscriptions, the account generating the authorization token must have access to both the source and <span class="No-Break">destination subscription</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">Once you deploy a customized model, if it remains inactive for more than <em class="italic">15 days</em>, the deployment will automatically be deleted. A deployment is considered inactive if no <em class="italic">completion</em> or <em class="italic">chat completion</em> calls are made to the model over a continuous 15-day period. It’s important to note that the deletion of an inactive deployment does not affect the underlying customized model. The model itself is preserved and can be redeployed at any time. Additionally, each deployed fine-tuned model incurs an <em class="italic">hourly hosting cost</em>, even if no calls are made to the model during <span class="No-Break">that time.</span></p>
			<p class="list-inset">Once your custom model is deployed, you can use it just like any other deployed model. You can experiment with your new deployment using <strong class="bold">Playgrounds</strong> in Azure AI Foundry. The same parameters, such as <strong class="source-inline">temperature</strong> and <strong class="source-inline">max_tokens</strong>, can be applied to your custom model, just like with other <span class="No-Break">deployed models:</span></p>
			<ul>
				<li>For fine-tuned <strong class="source-inline">babbage-002</strong> and <strong class="source-inline">davinci-002</strong> models, you will use the <strong class="bold">Completions</strong> playground and the <span class="No-Break">Completions API</span></li>
				<li>For fine-tuned gpt-4o models, you will use the <strong class="bold">Chat</strong> playground and the Chat <span class="No-Break">Completion API</span></li>
			</ul>
			<p>These tools allow you to<a id="_idIndexMarker290"/> interact with and test the customizations made to your <span class="No-Break">fine-tuned models.</span></p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/>Summary</h1>
			<p>In this chapter, we focused on AOAI, which provides a comprehensive set of tools and services designed to enhance AI model capabilities and integration. At the core of these offerings is the AOAI model context window, which defines the amount of information the models can process at once. We learned that it’s crucial for maintaining coherence and understanding in complex tasks. We also learned that AOAI embedding models facilitate the conversion of text into numerical vectors, enabling better semantic understanding and similarity searches. These embeddings can efficiently be stored and queried using Azure vector databases, which are optimized for handling high-dimensional data, thereby enhancing the performance of AI applications. We also discussed the standard RAG pattern, outlining its step-by-step process flow. Furthermore, we learned that AOAI On Your Data allows organizations to do quick a prototype and leverage these models on their proprietary datasets, ensuring the AI solutions are tailored to specific <span class="No-Break">business needs.</span></p>
			<p>AOAI capabilities extend into multimodal models, which can process and integrate information from multiple data types, such as text and images, broadening the scope of AI applications. We learned that the function calling feature allows seamless integration of AI models with Azure’s robust ecosystem, facilitating the execution of predefined functions based on AI outputs. Developers can leverage the AOAI Assistants API to create sophisticated, context-aware conversational agents, enhancing user interactions. For operations requiring high throughput, the AOAI Batch API provides a scalable solution for processing large volumes of data efficiently, as we learned in this chapter. Finally, we learned that AOAI fine-tuning empowers users to customize pre-trained models to better align with specific tasks or domains, improving performance and accuracy in specialized applications. Together, these tools offer a powerful and flexible platform for developing advanced AI solutions tailored to diverse business requirements, as we learned in <span class="No-Break">this chapter.</span></p>
			<p>In the following chapters, we will focus on practical examples of generative AI applications, accompanied by coding exercises to help you build these applications effortlessly. Through hands-on implementations, you’ll develop a comprehensive understanding of how to apply generative AI technologies to real-world use cases. Topics will include various scenarios such as document-based question answering and contact center analytics, as well as querying structured data, generating code using AOAI, creating recommender systems, generating text-to-video content, and building a multimodal multi-agent system using the Assistant API. Each example will include step-by-step guidance and code snippets to support you in integrating these features into <span class="No-Break">your projects.</span></p>
		</div>
	

		<div id="_idContainer075" class="Content">
			<h1 id="_idParaDest-59" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor058"/>Part 2: Practical Applications of Azure OpenAI: Real-World Use Cases</h1>
			<p>In Part 2, we transition from foundational concepts to hands-on implementations, exploring practical use cases that demonstrate the transformative potential of Azure OpenAI in solving real-world challenges. Each chapter presents a distinct application, offering detailed insights into the problem context, technical architecture, and step-by-step solution development. From creating enterprise-level document question-answering systems to building multimodal, multi-agent frameworks, this section equips readers with the knowledge and tools to harness Azure OpenAI for diverse and <span class="No-Break">impactful applications.</span></p>
			<p>This part has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B21019_04.xhtml#_idTextAnchor059"><em class="italic">Chapter 4</em></a>, Developing an Enterprise Document Question-Answer Solution</li>
				<li><a href="B21019_05.xhtml#_idTextAnchor067"><em class="italic">Chapter 5</em></a>, Building a Contact Center Analytics Solution</li>
				<li><a href="B21019_06.xhtml#_idTextAnchor077"><em class="italic">Chapter 6</em></a>, <em class="italic">Querying From a Structured Database</em></li>
				<li><a href="B21019_07.xhtml#_idTextAnchor088"><em class="italic">Chapter 7</em></a>, <em class="italic">Code Generation and Documentation</em></li>
				<li><a href="B21019_08.xhtml#_idTextAnchor095"><em class="italic">Chapter 8</em></a>, <em class="italic">Creating a Basic Recommender Solution with Azure OpenAI</em></li>
				<li><a href="B21019_09.xhtml#_idTextAnchor101"><em class="italic">Chapter 9</em></a>, <em class="italic">Transforming Text to Video</em></li>
				<li><a href="B21019_10.xhtml#_idTextAnchor109"><em class="italic">Chapter 10</em></a>, <em class="italic">Creating a Multimodal Multi-Agent Framework with the Azure OpenAI Assistant API</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer076">
			</div>
		</div>
	</body></html>