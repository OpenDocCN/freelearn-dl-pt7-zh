["```py\n    !git clone https://github.com/152334H/tortoise-tts-fast\n    %cd tortoise-tts-fast\n    !pip3 install -r requirements.txt --no-deps\n    !pip3 install -e .\n    !pip3 install git+https://github.com/152334H/BigVGAN.git\n    !pip install transformers==4.29.2\n    !pip install voicefixer==0.1.2\n    %cd tortoise-tts-fast\n    from huggingface_hub import notebook_login\n    notebook_login()\n    from huggingface_hub import whoami\n    whoami()\n    ```", "```py\n    from tortoise.api import TextToSpeech\n    from tortoise.utils.audio import load_audio, load_voice, load_voices\n    # This will download all the models Tortoise uses from the HuggingFace hub.\n    tts = TextToSpeech()\n    ```", "```py\n    import os\n    from ipywidgets import Dropdown\n    voices_dir = \"tortoise/voices\"\n    # Get a list of all directories in the voices directory\n    voice_names = os.listdir(voices_dir)\n    voice_folder = Dropdown(\n        options=sorted(voice_names),\n        description='Select a voice:',\n        value='tom',\n        disabled=False,\n        style={'description_width': 'initial'},\n    )\n    voice_folder\n    import os\n    from ipywidgets import Dropdown\n    voices_dir = f\"tortoise/voices/{voice_folder.value}\"\n    # Get a list of all directories in the voices directory\n    voice_files = os.listdir(voices_dir)\n    voice = Dropdown(\n        options=sorted(voice_files),\n        description='Select a voice:',\n        # value='tom',\n        disabled=False,\n        style={'description_width': 'initial'},\n    )\n    Voice\n    #Pick one of the voices from the output above\n    IPython.display.Audio(filename=f'tortoise/voices/{voice_folder.value}/{voice.value}')\n    ```", "```py\n    text = \" Words, once silent, now dance on digital breath, speaking volumes through the magic of text-to-speech.\"\n    preset = \"ultra_fast\"\n    voice = voice_folder.value\n    voice_samples, conditioning_latents = load_voice(voice)\n    gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=preset)\n    torchaudio.save(generated_filename, gen.squeeze(0).cpu(), 24000)\n    IPython.display.Audio(generated_filename)\n    ```", "```py\n    gen = tts.tts_with_preset(text, voice_samples=None, conditioning_latents=None, preset=preset)\n    torchaudio.save(' synthetized_voice_sample.wav', gen.squeeze(0).cpu(), 24000)\n    IPython.display.Audio('synthetized_voice_sample.wav')\n    ```", "```py\n    CUSTOM_VOICE_NAME = \"custom\"\n    import os\n    from google.colab import files\n    custom_voice_folder = f\"tortoise/voices/{CUSTOM_VOICE_NAME}\"\n    os.makedirs(custom_voice_folder)\n    for i, file_data in enumerate(files.upload().values()):\n      with open(os.path.join(custom_voice_folder, f'{i}.wav'), 'wb') as f:\n        f.write(file_data)\n    # Generate speech with the custom voice.\n    voice_samples, conditioning_latents = load_voice(CUSTOM_VOICE_NAME)\n    gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents,\n                              preset=preset)\n    torchaudio.save(f'generated-{CUSTOM_VOICE_NAME}.wav', gen.squeeze(0).cpu(), 24000)\n    IPython.display.Audio(f'generated-{CUSTOM_VOICE_NAME}.wav')\n    ```", "```py\n    voice_samples, conditioning_latents = load_voices(['freeman', 'deniro'])\n    gen = tts.tts_with_preset(\"Words, once silent, now dance on digital breath, speaking volumes through the magic of text-to-speech.\",\n                              voice_samples=voice_samples, conditioning_latents=conditioning_latents,\n                              preset=preset)\n    torchaudio.save('freeman_deniro.wav', gen.squeeze(0).cpu(), 24000)\n    IPython.display.Audio('freeman_deniro.wav')\n    ```", "```py\n/MyDataset\n---├── train.txt\n---├── valid.txt\n---├── /wavs\n---------├── 0.wav\n---------├── 1.wav\n        ...\n```", "```py\nwavs/0.wav|This is Josue Batista.\nwavs/1.wav|I am the author of the book Learn OpenAI Whisper, Transform Your Understanding of Generative AI\nwavs/2.wav|through robust and accurate speech processing solutions.\n...\n```", "```py\n    !git clone https://github.com/devilismyfriend/ozen-toolkit\n    ```", "```py\n    !pip install transformers\n    !pip install huggingface\n    !pip install pydub\n    !pip install yt-dlp\n    !pip install pyannote.audio\n    !pip install colorama\n    !pip install termcolor\n    ozen-toolkit directory:\n\n    ```", "```py\n\n    ```", "```py\n    /content/ozen-toolkit to store the uploaded files and saves them in that directory:\n\n    ```", "```py\n\n    ```", "```py\n    import configparser\n    config = configparser.ConfigParser()\n    config['DEFAULT'] = {\n        'hf_token': '<Your HF API key>',\n        'whisper_model': 'openai/whisper-medium',\n        'device': 'cuda',\n        'diaization_model': 'pyannote/speaker-diarization',\n        'segmentation_model': 'pyannote/segmentation',\n        'valid_ratio': '0.2',\n        'seg_onset': '0.7',\n        'seg_offset': '0.55',\n        'seg_min_duration': '2.0',\n        'seg_min_duration_off': '0.0'\n    }\n    with open('config.ini', 'w') as configfile:\n        config.write(configfile)\n    ```", "```py\n    ozen.py requires Hugging Face’s pyannote/segmentation model. This is a gated model; you MUST request access before attempting to run the next cell. Thankfully, getting access is relatively straightforward and fast. Here are the steps:1.  You must already have a Hugging Face account; if you do not have one, see the instructions in the notebook for [*Chapter 3*](B21020_03.xhtml#_idTextAnchor088): `LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb` ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter03/LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter03/LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb))2.  Visit [https://hf.co/pyannote/segmentation](https://hf.co/pyannote/segmentation) to accept the user conditions:\n    ```", "```py\nozen-toolkit/output/\n---├── Learn_OAI_Whisper_Sample_Audio01.mp3_2024_03_16-16_36/\n------------------├── valid.txt\n------------------├── train.txt\n------------------├── wavs/\n--------------------------├── 0.wav\n--------------------------├── 1.wav\n--------------------------├── 2.wav\n```", "```py\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    ```", "```py\n    %cp -r /content/ozen-toolkit/output/ /content/gdrive/MyDrive/ozen-toolkit/output/\n    ```", "```py\n    gpu_info = !nvidia-smi\n    gpu_info = '\\n'.join(gpu_info)\n    if gpu_info.find('failed') >= 0:\n      print('Not connected to a GPU')\n    else:\n      print(gpu_info)\n    ```", "```py\n    from psutil import virtual_memory\n    ram_gb = virtual_memory().total / 1e9\n    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n    if ram_gb < 20:\n      print('Not using a high-RAM runtime')\n    else:\n      print('You are using a high-RAM runtime!')\n    ```", "```py\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    ```", "```py\n    !git clone https://github.com/josuebatista/DL-Art-School.git\n    %cd DL-Art-School\n    !wget https://huggingface.co/Gatozu35/tortoise-tts/resolve/main/dvae.pth -O experiments/dvae.pth\n    !wget https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth -O experiments/autoregressive.pth\n    Dataset_Training_Path and ValidationDataset_Training_Path, click on Google Colab’s Files option and search Google Drive for the directory where the DJ-format datasets were stored in the previous notebook. *Figure 9**.6* shows an example of where the DJ-format dataset is found. Keep in mind that *Figure 9**.6* is just an example. Do not search for that literal name. Instead, you must search for the directory name you set while creating the DJ-formatted files:\n    ```", "```py\nfrom pathlib import Path\nfrom math import ceil\nDEFAULT_TRAIN_BS = 64\nDEFAULT_VAL_BS = 32\nDataset_Training_Path = \"/content/gdrive/MyDrive/Generative_AI/Deep_Fakes_Voice/output/Learn_OAI_Whisper_Sample_Audio01.mp3_2024_03_16-16_36/train.txt\" #@param {type:\"string\"}\nValidationDataset_Training_Path = \"/content/gdrive/MyDrive/Generative_AI/Deep_Fakes_Voice/output/Learn_OAI_Whisper_Sample_Audio01.mp3_2024_03_16-\nif Dataset_Training_Path == ValidationDataset_Training_Path:\n  print(\"WARNING: training dataset path == validation dataset path!!!\")\n  print(\"\\tThis is technically okay but will make all of the validation metrics useless. \")\n  print(\"it will also SUBSTANTIALLY slow down the rate of training, because validation datasets are supposed to be much smaller than training ones.\")\ndef txt_file_lines(p: str) -> int:\n  return len(Path(p).read_text().strip().split('\\n'))\ntraining_samples = txt_file_lines(Dataset_Training_Path)\nval_samples = txt_file_lines(ValidationDataset_Training_Path)\nif training_samples < 128: print(\"WARNING: very small dataset! the smallest dataset tested thus far had ~200 samples.\")\nif val_samples < 20: print(\"WARNING: very small validation dataset! val batch size will be scaled down to account\")\ndef div_spillover(n: int, bs: int) -> int: # returns new batch size\n  epoch_steps,remain = divmod(n,bs)\n  if epoch_steps*2 > bs: return bs # don't bother optimising this stuff if epoch_steps are high\n  if not remain: return bs # unlikely but still\n  if remain*2 < bs: # \"easier\" to get rid of remainder -- should increase bs\n    target_bs = n//epoch_steps\n  else: # easier to increase epoch_steps by 1 -- decrease bs\n    target_bs = n//(epoch_steps+1)\n  assert n%target_bs < epoch_steps+2 # should be very few extra\n  return target_bs\nif training_samples < DEFAULT_TRAIN_BS:\n  print(\"WARNING: dataset is smaller than a single batch. This will almost certainly perform poorly. Trying anyway\")\n  train_bs = training_samples\nelse:\n  train_bs = div_spillover(training_samples, DEFAULT_TRAIN_BS)\nif val_samples < DEFAULT_VAL_BS:\n  val_bs = val_samples\nelse:\n  val_bs = div_spillover(val_samples, DEFAULT_VAL_BS)\nsteps_per_epoch = training_samples//train_bs\nlr_decay_epochs = [20, 40, 56, 72]\nlr_decay_steps = [steps_per_epoch * e for e in lr_decay_epochs]\nprint_freq = min(100, max(20, steps_per_epoch))\nval_freq = save_checkpoint_freq = print_freq * 3\nprint(\"===CALCULATED SETTINGS===\")\nprint(f'{train_bs=} {val_bs=}')\nprint(f'{val_freq=} {lr_decay_steps=}')\nprint(f'{print_freq=} {save_checkpoint_freq=}')\n```", "```py\n    Experiment_Name = \"Learn_OAI_Whisper_20240316\"\n    Dataset_Training_Name= \"TestDataset\"\n    ValidationDataset_Name = \"TestValidation\"\n    SaveTrainingStates = False\n    Keep_Last_N_Checkpoints = 0\n    Fp16 = False\n    Use8bit = True\n    TrainingRate = \"1e-5\"\n    TortoiseCompat = False\n    TrainBS = \"\"\n    ValBS = \"\"\n    ValFreq = \"\"\n    LRDecaySteps = \"\"\n    PrintFreq = \"\"\n    SaveCheckpointFreq = \"\"\n    def take(orig, override):\n      if override == \"\": return orig\n      return type(orig)(override)\n    train_bs = take(train_bs, TrainBS)\n    val_bs = take(val_bs, ValBS)\n    val_freq = take(val_freq, ValFreq)\n    lr_decay_steps = eval(LRDecaySteps) if LRDecaySteps else lr_decay_steps\n    print_freq = take(print_freq, PrintFreq)\n    save_checkpoint_freq = take(save_checkpoint_freq, SaveCheckpointFreq)\n    assert len(lr_decay_steps) == 4\n    gen_lr_steps = ', '.join(str(v) for v in lr_decay_steps)\n    ```", "```py\n    %cd /content/DL-Art-School\n    # !wget https://raw.githubusercontent.com/152334H/DL-Art-School/master/experiments/EXAMPLE_gpt.yml -O experiments/EXAMPLE_gpt.yml\n    !wget https://raw.githubusercontent.com/josuebatista/DL-Art-School/master/experiments/EXAMPLE_gpt.yml -O experiments/EXAMPLE_gpt.yml\n    import os\n    %cd /content/DL-Art-School\n    !sed -i 's/batch_size: 128/batch_size: '\"$train_bs\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's/batch_size: 64/batch_size: '\"$val_bs\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's/val_freq: 500/val_freq: '\"$val_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's/500, 1000, 1400, 1800/'\"$gen_lr_steps\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's/print_freq: 100/print_freq: '\"$print_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's/save_checkpoint_freq: 500/save_checkpoint_freq: '\"$save_checkpoint_freq\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's+CHANGEME_validation_dataset_name+'\"$ValidationDataset_Name\"'+g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's+CHANGEME_path_to_validation_dataset+'\"$ValidationDataset_Training_Path\"'+g' ./experiments/EXAMPLE_gpt.yml\n    if(Fp16==True):\n      os.system(\"sed -i 's+fp16: false+fp16: true+g' ./experiments/EXAMPLE_gpt.yml\")\n    !sed -i 's/use_8bit: true/use_8bit: '\"$Use8bit\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's/disable_state_saving: true/disable_state_saving: '\"$SaveTrainingStates\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's/tortoise_compat: True/tortoise_compat: '\"$TortoiseCompat\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's/number_of_checkpoints_to_save: 0/number_of_checkpoints_to_save: '\"$Keep_Last_N_Checkpoints\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's/CHANGEME_training_dataset_name/'\"$Dataset_Training_Name\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's/CHANGEME_your_experiment_name/'\"$Experiment_Name\"'/g' ./experiments/EXAMPLE_gpt.yml\n    !sed -i 's+CHANGEME_path_to_training_dataset+'\"$Dataset_Training_Path\"'+g' ./experiments/EXAMPLE_gpt.yml\n    if (not TrainingRate==\"1e-5\"):\n      os.system(\"sed -i 's+!!float 1e-5 # CHANGEME:+!!float '\" + TrainingRate + \"' #+g' ./experiments/EXAMPLE_gpt.yml\")\n    ```", "```py\n    INFO:base:Saving models and training states\n    ```", "```py\n%cd /content/DL-Art-School/codes\n!python3 train.py -opt ../experiments/EXAMPLE_gpt.yml\n```", "```py\n    Experiment_Name variable:\n    ```", "```py\n    gpu_info = !nvidia-smi\n    gpu_info = '\\n'.join(gpu_info)\n    if gpu_info.find('failed') >= 0:\n      print('Not connected to a GPU')\n    else:\n      print(gpu_info)\n    ```", "```py\n    from psutil import virtual_memory\n    ram_gb = virtual_memory().total / 1e9\n    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n    if ram_gb < 20:\n      print('Not using a high-RAM runtime')\n    else:\n      print('You are using a high-RAM runtime!')\n    ```", "```py\n    !git clone **https://github.com/152334H/tortoise-tts-fast**\n    %cd tortoise-tts-fast\n    !pip3 install -r requirements.txt --no-deps\n    transformers, voicefixer, and BigVGAN, using pip3:\n\n    ```", "```py\n\n    ```", "```py\n    from google.colab import drive\n    drive.mount('/content/gdrive')\n    ```", "```py\n    gpt_path = '/content/gdrive/MyDrive/<filepath/ filename_gpt.pth'\n    .pth checkpoint file in Google Colab’s Copy Path:\n    ```", "```py\n    tortoise-tts-fast/scripts/results/ directory. You will find the generated audio from the voice synthesis model in that directory. We use IPython to display and play the synthesized audio file.\n\n    ```", "```py\n\n    *Figure 9**.11* shows an example of the directory structure and files created by TorToiSe-TTS-Fast:\n    ```"]