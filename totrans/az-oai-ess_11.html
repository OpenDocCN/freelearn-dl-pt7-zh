<html><head></head><body>
		<div id="_idContainer199">
			<h1 id="_idParaDest-119" class="chapter-number"><a id="_idTextAnchor119"/>11</h1>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor120"/>Privacy and Security</h1>
			<p>In the preceding chapters, we’ve demonstrated the process of crafting and implementing practical solutions using <strong class="bold">Azure OpenAI</strong> (<strong class="bold">AOAI</strong>) in conjunction with various Azure AI services. In this chapter, we will focus on privacy and security considerations related <span class="No-Break">to AOAI.</span></p>
			<p>In this chapter, we’re going to cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>AOAI <span class="No-Break">service compliance</span></li>
				<li>AOAI <span class="No-Break">data privacy</span></li>
				<li><span class="No-Break">Content filtering</span></li>
				<li><span class="No-Break">Managed identities</span></li>
				<li><strong class="bold">Virtual Network</strong> (<span class="No-Break"><strong class="bold">VNet</strong></span><span class="No-Break">) configuration</span></li>
				<li>Private <span class="No-Break">endpoint configuration</span></li>
				<li><span class="No-Break">Data encryption</span></li>
				<li>Responsible AI <span class="No-Break">for AOAI</span></li>
			</ul>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor121"/>AOAI service compliance</h1>
			<p>The AOAI compliance program<a id="_idIndexMarker492"/> is a set of policies and practices that Microsoft has established to ensure that the AOAI service is used in a responsible and ethical manner. The program includes the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li><strong class="bold">Data, privacy, and security</strong>: Microsoft provides details on how data provided by customers to the AOAI service is processed, used, <span class="No-Break">and stored.</span></li>
				<li><strong class="bold">Responsible AI practices</strong>: Microsoft offers technical guidance and tools to support customers in responsibly designing, developing, deploying, and utilizing AI systems that incorporate AOAI models. These recommendations align with the Microsoft Responsible AI Standard and encompass four key stages: Identification, Measurement, Mitigation, <span class="No-Break">and Operation.</span></li>
				<li><strong class="bold">Code of conduct</strong>: Microsoft defines the requirements that all AOAI Service implementations must adhere to in good faith. The code of conduct covers topics such as acceptable use, harmful content, human interaction, attribution, <span class="No-Break">and feedback.</span></li>
				<li><strong class="bold">Federal Risk and Authorization Management Program (FedRAMP) High Authorization</strong>: Microsoft<a id="_idIndexMarker493"/> has obtained FedRAMP High Authorization for the AOAI service within the Azure Commercial environment. This means that the service meets the highest level of security standards required by the federal government<a id="_idIndexMarker494"/> for <strong class="bold">cloud service </strong><span class="No-Break"><strong class="bold">providers</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CSPs</strong></span><span class="No-Break">).</span></li>
				<li><strong class="bold">Health Insurance Portability and Accountability Act (HIPAA) compliance</strong>: Microsoft <a id="_idIndexMarker495"/>has confirmed that the AOAI service can be used in a HIPAA-compliant manner. This means that <a id="_idIndexMarker496"/>customers can use the service to process <strong class="bold">protected health information</strong> (<strong class="bold">PHI</strong>) in accordance with the HIPPA in respect of<a id="_idIndexMarker497"/> the <strong class="bold">Business Associate Agreement</strong> (<strong class="bold">BAA</strong>). The BAA is a critical document for <a id="_idIndexMarker498"/>ensuring that PHI is handled securely and in compliance with HIPAA regulations when it is managed by third-party <span class="No-Break">business associates.</span></li>
				<li><strong class="bold">System and Organization Controls (SOC) 1, 2, 3</strong>: SOC 1, SOC 2, and SOC 3 are compliance standards and frameworks related to the security and integrity of service organizations’ systems and data. They are developed and maintained by the <strong class="bold">American Institute of Certified Public Accountants</strong> (<strong class="bold">AICPA</strong>). These<a id="_idIndexMarker499"/> standards help organizations demonstrate their commitment to protecting sensitive information and ensuring the reliability of their systems and services to clients and stakeholders. AOAI handles data for delivering its service and for keeping an eye out for any misuse that goes against the product terms. Your prompts (inputs) and completions (outputs), along with your embeddings and training data, are not shared with other customers, OpenAI, Microsoft, or any third-party products or services. AOAI models that customers fine-tuned are solely for their own use. Microsoft completely manages the AOAI service, hosting OpenAI models in its Azure environment, and the service doesn’t connect with any services run <span class="No-Break">by OpenAI.</span></li>
			</ul>
			<p>The AOAI service complies<a id="_idIndexMarker500"/> with various certifications and standards to ensure its reliability <a id="_idIndexMarker501"/>and security. These include <strong class="bold">Cloud Security Alliance Security, Trust, Assurance, and Risk</strong> (<strong class="bold">CSA STAR</strong>) Certification and Attestation, ISO 20000-1:2018, ISO 22301:2019, ISO 27001:2022, ISO 27017:2015, ISO 27018:2019, ISO 27701:2019, ISO 9001:2015, SOC 1, 2, and 3, <strong class="bold">Global System for Mobile Communications Security</strong> <strong class="bold">Accreditation Scheme-Subscription Management</strong> (<strong class="bold">GSMA SAS-SM</strong>), HIPAA BAA, the <strong class="bold">Health Information Trust Alliance</strong> (<strong class="bold">HITRUST</strong>), <strong class="bold">Payment Card Industry 3-D Secure</strong> (<strong class="bold">PCI 3DS</strong>), the <strong class="bold">PCI Data Security Standard</strong> (<strong class="bold">PCI DSS</strong>), <strong class="bold">Germany Cloud Computing Compliance Controls Catalog</strong> (<strong class="bold">Germany C5</strong>), Singapore <strong class="bold">Multi-Tier Cloud Security</strong> (<strong class="bold">MTCS</strong>) Level 3, and Singapore <strong class="bold">Outsourced Service Provider’s Audit Report</strong> (<strong class="bold">OSPAR</strong>). This adherence to various compliance standards ensures that the AOAI service maintains high levels of security and reliability for <span class="No-Break">its users.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Azure continually assesses the AOAI service to incorporate additional compliance certifications. To access the most up-to-date compliance attestation information for various Azure products, please visit the following <span class="No-Break">link: </span><a href="https://servicetrust.microsoft.com/DocumentPage/7adf2d9e-d7b5-4e71-bad8-713e6a183cf3"><span class="No-Break">https://servicetrust.microsoft.com/DocumentPage/7adf2d9e-d7b5-4e71-bad8-713e6a183cf3</span></a><span class="No-Break">.</span></p>
			<p>Having covered AOAI compliance, let’s now direct our attention to the subject of data privacy in the <span class="No-Break">upcoming section.</span></p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor122"/>AOAI data privacy</h1>
			<p>This <a id="_idIndexMarker502"/>section provides insights into the processing, utilization, and storage of data you submit to the <span class="No-Break">AOAI service.</span></p>
			<p>AOAI models are stateless, which means that they do not store or remember any information from previous inputs or outputs. Instead, they solely process the current input and generate an output based on their model parameters, as well as any optional settings such as temperature or frequency penalty. This design choice enhances the models’ adaptability and scalability but presents challenges when dealing with tasks that require context <span class="No-Break">or memory.</span></p>
			<p>To <a id="_idIndexMarker503"/>address this limitation, you can include relevant context or historical information within the input or leverage external data sources to supplement the input with additional details. Nevertheless, such practices can introduce potential risks, including concerns related to data privacy and security, as well as the potential for misuse or abuse of the models. Consequently, Microsoft has instituted various safeguards to protect its models and users from these potential threats. These measures encompass content filtering, rate limiting, and data <span class="No-Break">processing policies.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Data pertaining to prompts, completions, embeddings, and training is strictly confined, with no access granted to others or utilization for improving OpenAI or any Microsoft/third-party products. Fine-tuned AOAI models are solely accessible for user use, exclusively controlled by Microsoft within the AOAI service, distinct from services provided by OpenAI such as ChatGPT or <span class="No-Break">OpenAI API.</span></p>
			<p>AOAI handles several <a id="_idIndexMarker504"/>categories <span class="No-Break">of data:</span></p>
			<ul>
				<li><strong class="bold">Prompts and completion data</strong>: Users submit prompts, and the service generates content through operations such as completions and chat completions, as well as dealing with images <span class="No-Break">and embeddings</span></li>
				<li><strong class="bold">Augmented data associated with prompts</strong>: Through the On Your Data feature, the service accesses data from a specified data store and enhances prompts with this data, resulting in content that is directly connected to <span class="No-Break">your dataset</span></li>
				<li><strong class="bold">Training and validation data</strong>: Users have the option to supply their own training data, consisting of pairs of prompts and completions, which can be used to fine-tune an OpenAI model for <span class="No-Break">specific purposes</span></li>
			</ul>
			<p>The diagram provided here outlines the AOAI data processing flow along with content filtering, which we will <a id="_idIndexMarker505"/>talk about in the <span class="No-Break">next section:</span></p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/B21019_11_1.jpg" alt="Figure 11.1: AOAI data flows for inference and fine-tuning"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1: AOAI data flows for inference and fine-tuning</p>
			<p>This encompasses three <a id="_idIndexMarker506"/>distinct <span class="No-Break">processing scenarios:</span></p>
			<ul>
				<li><strong class="bold">Processing prompts for content generation</strong>: The top section illustrates how the AOAI service takes your prompts and generates content, including cases where additional data from an external source is incorporated into prompts using the AOAI On Your <span class="No-Break">Data feature.</span></li>
				<li><strong class="bold">Creating fine-tuned (custom) models</strong>: The bottom section outlines how the AOAI service utilizes your training data to craft fine-tuned models, tailored to your specific requirements <span class="No-Break">and preferences.</span></li>
				<li><strong class="bold">Content filtering by the AOAI service and Microsoft Personnel</strong>: The diagram also demonstrates how the AOAI service, in conjunction with Microsoft Personnel, conducts an analysis of prompts, completions, and images. This analysis is designed to identify potentially harmful content and patterns that may indicate the misuse of the service, in violation of the code of conduct or other relevant <span class="No-Break">product terms.</span></li>
			</ul>
			<p>Models, whether <a id="_idIndexMarker507"/>they are the base models or fine-tuned versions, that are deployed within your resource are responsible for processing the input prompts you provide and generating responses, which can include text, images, or embeddings. The service operates in a synchronous manner, evaluating the prompt and completion data in real time to actively monitor for potentially harmful content types. If generated content surpasses the thresholds configured for this purpose, the service will halt the generation of such content to maintain a safe and <span class="No-Break">compliant environment.</span></p>
			<p>Let’s discuss the data flows for the On Your Data feature in AOAI. This feature involves the interaction between the AOAI service and external data sources to augment prompts and <span class="No-Break">generate content.</span></p>
			<p>The On Your Data feature in AOAI enables you to establish connections with external data sources, allowing the generated results to be closely tied to your specific data. Importantly, this data remains securely stored within the designated data source and location; no data is duplicated or copied into the AOAI <span class="No-Break">service itself.</span></p>
			<p>When a user submits a prompt, the service dynamically retrieves pertinent data from the connected external data source and enhances the user’s prompt with this additional contextual information. Subsequently, the model processes this augmented prompt, and the resulting generated content is then returned; this technique is called <strong class="bold">Retrieval Augmented Generation</strong> (<strong class="bold">RAG</strong>). This<a id="_idIndexMarker508"/> mechanism ensures that the output is grounded in your data without compromising data security or privacy. The following diagram illustrates the <span class="No-Break">entire process:</span></p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/B21019_11_2.jpg" alt="Figure 11.2: AOAI data flows for On Your Data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2: AOAI data flows for On Your Data</p>
			<p>In both diagrams, you <a id="_idIndexMarker509"/>may have observed that AOAI monitors abusive content asynchronously. Let’s delve into the upcoming section to gain a better understanding of how AOAI prevents abuse and the generation of <span class="No-Break">harmful content.</span></p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor123"/>Preventing abuse and harmful content generation</h2>
			<p>To<a id="_idIndexMarker510"/> mitigate the risk of the AOAI service being used for harmful purposes, it incorporates both content filtering and abuse <span class="No-Break">monitoring features.</span></p>
			<p>Real-time content filtering occurs as the service processes prompts to produce content. Prompts and generated results are not used for training, retraining, or improving content classifier models, nor are they stored within these models. Further details on content filtering will be provided in a subsequent section of <span class="No-Break">this chapter.</span></p>
			<p>Abuse monitoring involves identifying and handling instances of recurring content and behaviors that suggest possible breaches of the code of conduct or other relevant product terms. In order to detect and tackle abuse, AOAI securely retains all prompts and generated content for a maximum period of <span class="No-Break">30 days.</span></p>
			<p>The data storage system housing prompts and completions is logically separated by customer resource, with each request specifying the resource ID of the customer’s AOAI resource. In every region where the AOAI service is available, there exists a distinct data repository. A <a id="_idIndexMarker511"/>customer’s prompts and generated content are stored within the Azure region where their AOAI service resource is deployed, all within the predefined boundaries of the AOAI service. Human reviewers responsible for evaluating potential abuse can access prompt and completion data only when it has been flagged by the abuse monitoring system. These reviewers are authorized Microsoft employees who access the data through<a id="_idIndexMarker512"/> point-wise queries using request IDs, <strong class="bold">Secure Access Workstations</strong> (<strong class="bold">SAWs</strong>), and <strong class="bold">Just-In-Time</strong> (<strong class="bold">JIT</strong>) request <a id="_idIndexMarker513"/>approvals provided by team managers. In the instance of AOAI Service deployed in <a id="_idIndexMarker514"/>the <strong class="bold">European Economic Area</strong> (<strong class="bold">EEA</strong>), the authorized Microsoft employees are situated within <span class="No-Break">the EEA.</span></p>
			<p>Certain customers may require the utilization of the AOAI service for handling sensitive, highly confidential, or legally regulated input data, despite the minimal risk of generating harmful outputs or misuse. These customers may find themselves unable or unwilling to grant Microsoft permission to process such data for abuse detection, as previously explained, owing to internal policies or relevant legal obligations. To mitigate these concerns, Microsoft provides eligible customers, who meet specific Limited Access criteria and attest to particular use cases, with the opportunity to request adjustments to AOAI content management features. This can be done by completing the form available at <a href="https://aka.ms/oai/modifiedaccess">https://aka.ms/oai/modifiedaccess</a>. In this form, make sure you tick mark point <em class="italic">#23</em> to disable abuse monitoring as <span class="No-Break">shown next:</span></p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/B21019_11_3.jpg" alt="Figure 11.3: Disabling abuse monitoring"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3: Disabling abuse monitoring</p>
			<p>Once Microsoft approves a customer’s request to modify abuse monitoring, no prompts and completions associated with the approved Azure subscription will be stored by Microsoft when abuse monitoring is configured to be inactive. In such cases, because there are no prompts and completions stored in the Service Results Store, the human review process is neither possible <span class="No-Break">nor conducted.</span></p>
			<p>Now, let’s ensure that the required configuration is in place for deactivating <span class="No-Break">abuse monitoring.</span></p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor124"/>Verification of abuse monitoring deactivation</h2>
			<p>Customers<a id="_idIndexMarker515"/> who have been approved to disable abuse monitoring in their Azure subscription can confirm that data storage for abuse monitoring has been deactivated through <span class="No-Break">two methods:</span></p>
			<ol>
				<li><strong class="bold">Using the </strong><span class="No-Break"><strong class="bold">Azure portal</strong></span><span class="No-Break">:</span><ol><li class="upper-roman">Sign in to the <span class="No-Break">Azure portal</span></li><li class="upper-roman">Select the appropriate AOAI <span class="No-Break">service resource.</span></li><li class="upper-roman">Go to the resource <span class="No-Break"><strong class="bold">Overview</strong></span><span class="No-Break"> page</span></li><li class="upper-roman">Click on the <strong class="bold">JSON View</strong> link in the top-right corner, as shown in the <span class="No-Break">following screenshot:</span></li></ol></li>
			</ol>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="image/B21019_11_4.jpg" alt="Figure 11.4: Content logging verification"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4: Content logging verification</p>
			<p class="list-inset">In the list of capabilities, you will find a value named <strong class="source-inline">ContentLogging</strong>, which will display as <strong class="source-inline">FALSE</strong> when abuse monitoring logging is <span class="No-Break">turned off.</span></p>
			<ol>
				<li value="2"><strong class="bold">Azure CLI or management API</strong>: Alternatively, customers<a id="_idIndexMarker516"/> can use the Azure <strong class="bold">Command-Line Interface</strong> (<strong class="bold">CLI</strong>) or any management API provided by Azure to programmatically access and check the abuse monitoring status for their Azure subscription. You can execute the following command in Azure CLI to see the same JSON data as shown in the <a id="_idIndexMarker517"/><span class="No-Break">Azure portal:</span><pre class="source-code">
az cognitiveservices account show -n resource\_name -g resource \_group</pre></li>			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">The <strong class="source-inline">ContentLogging</strong> attribute will only display a value of <strong class="source-inline">false</strong> if data storage for abuse monitoring has been deactivated. Otherwise, this property will not be visible in either the Azure portal or the output of <span class="No-Break">Azure CLI.</span></p>
			<p>In the previous section, we discussed various aspects of data privacy, data flow, and abuse monitoring. Now, let’s shift our focus to <span class="No-Break">content filtering.</span></p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor125"/>Content filtering</h1>
			<p>The AOAI service<a id="_idIndexMarker518"/> integrates a content filtering mechanism that operates alongside its core models. This system functions by subjecting both the prompt and completion to a combination of classification models designed to identify and mitigate the generation of harmful content. It actively identifies and responds to specific types of potentially harmful content in both the prompts provided as input and the completions produced as output. It’s worth noting that the filtering behavior may vary depending on the specific API configurations and the design of <span class="No-Break">the application.</span></p>
			<p>The content filtering models have undergone dedicated training and testing in the following languages: English, German, Japanese, Spanish, French, Italian, Portuguese, and Chinese. The service has the capability to function in numerous other languages, although the performance quality might differ. In every instance, it is advisable to conduct your own testing to confirm its suitability for your <span class="No-Break">particular application.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The AOAI Whisper model doesn’t use content filtering for prompts <span class="No-Break">and completions.</span></p>
			<p>The subsequent <a id="_idIndexMarker519"/>sections offer details regarding <span class="No-Break">the following:</span></p>
			<ul>
				<li>Content <span class="No-Break">filtering categories</span><ul><li><span class="No-Break">Categories</span></li><li><span class="No-Break">Severity levels</span></li></ul></li>
				<li><span class="No-Break">Configurability</span></li>
				<li><span class="No-Break">Best practices</span></li>
				<li><span class="No-Break">Implementation</span></li>
			</ul>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor126"/>Content filtering categories</h2>
			<p>Within the AOAI service, the content filtering system integrates neural multi-class classification models, specifically crafted to detect and filter out harmful content. These models cover four distinct main categories: <strong class="bold">Hate</strong>, <strong class="bold">Sexual</strong>, <strong class="bold">Violence</strong>, and <strong class="bold">Self-harm</strong>. Each category is classified into four severity levels: <strong class="bold">Safe</strong>, <strong class="bold">Low</strong>, <strong class="bold">Medium</strong>, and <strong class="bold">High</strong>. It’s crucial to emphasize that content identified at the <strong class="bold">Safe</strong> severity level is marked but is excluded from the filtering process and cannot be adjusted or customized. There are also optional classification models for the detection of jailbreak and protected material for text <span class="No-Break">and code.</span></p>
			<h3>Categories</h3>
			<p>These<a id="_idIndexMarker520"/> are the four <span class="No-Break">main categories:</span></p>
			<ul>
				<li><strong class="bold">Hate</strong>: Content that promotes hatred or discrimination against individuals or groups based on characteristics such as race, ethnicity, religion, gender, sexual orientation, and <span class="No-Break">so on</span></li>
				<li><strong class="bold">Sexual</strong>: Material containing explicit or suggestive sexual language, imagery, <span class="No-Break">or themes</span></li>
				<li><strong class="bold">Violence</strong>: Content depicting or advocating physical harm, injury, or violence toward individuals <span class="No-Break">or groups</span></li>
				<li><strong class="bold">Self-harm</strong>: Material encouraging or glorifying self-injury, suicide, eating disorders, or other forms <span class="No-Break">of self-harm</span></li>
			</ul>
			<p>In addition to the four predefined categories, customers have the option to create their own custom categories <a id="_idIndexMarker521"/>using the <strong class="bold">Azure AI Content Safety</strong> service. This allows them to train a personalized content classification model tailored to their needs. To do this, they will require <a id="_idIndexMarker522"/>training data to identify sensitive content, moderate user-generated content, or ensure compliance with <span class="No-Break">local regulations.</span></p>
			<p>Optional categories are <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Prompt shields for jailbreak attacks</strong>: Jailbreak attacks refer to user prompts deliberately crafted to trigger <a id="_idIndexMarker523"/>the <strong class="bold">generative AI</strong> (<strong class="bold">GenAI</strong>) model into demonstrating behaviors it was trained to avoid or to violate rules established in the system message. These direct attacks can range from elaborate role-playing scenarios to subtle attempts to undermine <span class="No-Break">safety protocols.</span></li>
				<li><strong class="bold">Prompt shields for indirect attacks</strong>: Indirect attacks, sometimes known as indirect prompt attacks or cross-domain prompt injection attacks, represent a potential security flaw where external entities embed harmful instructions within documents accessible to the GenAI system. This vulnerability necessitates embedding and formatting of the document, especially in RAG kind <span class="No-Break">of architecture.</span></li>
				<li><strong class="bold">Protected material for text</strong>: Protected material text pertains to identifiable text content, such as song lyrics, articles, recipes, and selected web content, which may be generated <a id="_idIndexMarker524"/>by <strong class="bold">large language </strong><span class="No-Break"><strong class="bold">models</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LLMs</strong></span><span class="No-Break">).</span></li>
				<li><strong class="bold">Protected material for code</strong>: Protected material code denotes source code that matches a predefined set of code snippets from public repositories. LLMs may output such code without properly citing the original <span class="No-Break">source repositories.</span></li>
			</ul>
			<h3>Severity levels</h3>
			<p>These are the <a id="_idIndexMarker525"/>four severity levels for each <span class="No-Break">main category:</span></p>
			<ul>
				<li><strong class="bold">Safe</strong>: Content may indeed involve topics associated with the <strong class="bold">Violence</strong>, <strong class="bold">Self-harm</strong>, <strong class="bold">Sexual</strong>, or <strong class="bold">Hate</strong> categories. However, these terms may be utilized in a general, journalistic, scientific, medical, or similar professional context that is suitable for broad audiences and does not involve harmful or <span class="No-Break">offensive intent.</span></li>
				<li><strong class="bold">Low</strong>: The <a id="_idIndexMarker526"/>content in question encompasses expressions of bias, judgment, or personal opinions, incorporates offensive language, employs stereotypes, involves explorations of fictional realms (for example, in gaming or literature), and portrays elements with a low level <span class="No-Break">of intensity.</span></li>
				<li><strong class="bold">Medium</strong>: The content mentioned involves the utilization of offensive, derogatory, ridiculing, intimidating, or belittling language directed at particular identity groups. Additionally, it may include depictions of seeking and carrying out harmful instructions, fantasies, and the glorification and promotion of harm, all presented at a medium level <span class="No-Break">of intensity.</span></li>
				<li><strong class="bold">High</strong>: The content in question showcases explicit and highly severe harmful instructions, actions, damage, or abuse. It also encompasses the endorsement, glorification, or promotion of extremely harmful acts, including those that are radical, illegal, or non-consensual in nature, as well as content related to power exchange or abuse that occurs <span class="No-Break">without consent.</span></li>
			</ul>
			<p>Up until this point, we have discussed various categories of content filtering and their associated severity levels. Now, let’s delve into where these severity levels can be adjusted <span class="No-Break">or configured.</span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor127"/>Customizability</h2>
			<p>The default <a id="_idIndexMarker527"/>setup for content filtering is programmed to initiate the filtering process when medium-level severity is detected within any of the four defined categories of harmful content, applicable to both user prompts and generated responses. This means that when content is flagged as having medium or high severity, it will be subjected to filtering. Conversely, content identified as having low severity will not trigger the filtering mechanisms. The following tables provide detailed information on the customization options available for each <span class="No-Break">severity level:</span></p>
			<table id="table001-8" class="T---Table _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Header">
							<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Severity</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Header">
							<p><strong class="bold" lang="en-US" xml:lang="en-US">Customizable </strong><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">for prompts</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Header">
							<p><strong class="bold" lang="en-US" xml:lang="en-US">Customizable </strong><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">for completions</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Header">
							<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Descriptions</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p><strong class="bold" lang="en-US" xml:lang="en-US">Low</strong><span lang="en-US" xml:lang="en-US">, </span><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Medium</strong></span><span class="No-Break" lang="en-US" xml:lang="en-US">, </span><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">High</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>Highest filter level: Filters low-, medium-, and <span class="No-Break">high-severity content.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Medium</strong></span><span class="No-Break" lang="en-US" xml:lang="en-US">, </span><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">High</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>Default: Filters medium- and high-severity content; does not filter <span class="No-Break">low-severity content.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">High</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>Subject <span class="No-Break">to approval*</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>Subject <span class="No-Break">to approval*</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>Filters only high-severity content. Approval is required <span class="No-Break">for filtering.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">No Filters</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>Subject <span class="No-Break">to approval*</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>Subject <span class="No-Break">to approval*</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>No content is filtered, regardless of the detected <span class="No-Break">severity level.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 11.1: Content filtering configurations</p>
			<p>Customers who <a id="_idIndexMarker528"/>are authorized to alter content filtering settings have full authority over these filters. They can opt to set filters to engage only at the high severity level or choose to turn off filtering altogether. If you wish to request access to modify content filters, please fill out the form at this link: <a href="https://aka.ms/oai/modifiedaccess">https://aka.ms/oai/modifiedaccess</a>. When filling out the form, please make sure to select point <em class="italic">#23</em> to activate high-severity content filtering. Additionally, in point <em class="italic">#24</em>, provide justification for this, as indicated in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="image/B21019_11_5.jpg" alt="Figure 11.5: Modifying content filters"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5: Modifying content filters</p>
			<p>So far, you have learned <a id="_idIndexMarker529"/>about different AOAI content filters and their configurations. Now, let’s discuss some best practices for implementing <span class="No-Break">content filtering.</span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor128"/>Best practices</h2>
			<p>When<a id="_idIndexMarker530"/> the content filtering system identifies harmful content, your interaction with the API will yield one of the <span class="No-Break">following outcomes:</span></p>
			<ul>
				<li><strong class="bold">Inappropriate prompts</strong>: If the input prompt is determined to contain inappropriate content, you will receive an HTTP <span class="No-Break">400 error.</span></li>
				<li><strong class="bold">Non-streaming completions</strong>: In cases where content filtering is applied to non-streaming completions, the filtered content will not be returned. Instead, the <strong class="source-inline">finish_reason</strong> value in the response will be set to <strong class="source-inline">content_filter</strong>. In some rare instances with longer responses, a partial result may be returned, with an updated <strong class="source-inline">finish_reason</strong> value indicating the content <span class="No-Break">filtering status.</span></li>
				<li><strong class="bold">Streaming completions</strong>: For streaming completions, segments of content will be returned as they are generated. The service will maintain its streaming operation until it encounters a predetermined stop token, reaches a certain length limit, or identifies content that falls under a category and severity level that has been set <span class="No-Break">for filtering.</span></li>
			</ul>
			<p>As you plan your application, it’s important to incorporate the following best practices to ensure a <a id="_idIndexMarker531"/>positive <strong class="bold">user experience</strong> (<strong class="bold">UX</strong>) while mitigating <span class="No-Break">potential issues:</span></p>
			<ul>
				<li>Determine your approach for addressing situations where users submit prompts containing content classified within a filtered category and severity level or when they misuse <span class="No-Break">your application</span></li>
				<li>Examine the <strong class="source-inline">finish_reason</strong> value to identify whether a completion has been filtered <span class="No-Break">or not</span></li>
				<li>Verify the absence of an error object in the <strong class="source-inline">content_filter_result</strong> value, which signifies that the content filters were successfully applied and did not encounter any issues <span class="No-Break">during processing</span></li>
			</ul>
			<p>Let’s discuss how you can practically implement content filtering through Azure <span class="No-Break">AI Foundry.</span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor129"/>Implementation</h2>
			<p>The <a id="_idIndexMarker532"/>subsequent instructions demonstrate the process of establishing a personalized content filtering configuration for your <span class="No-Break">AOAI resource:</span></p>
			<ol>
				<li>Log in to Azure AI Foundry and locate the <strong class="bold">Content filters</strong> tab by following the bottom-left navigation, as indicated by the highlighted red box <span class="No-Break">shown next:</span></li>
			</ol>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="image/B21019_11_6.jpg" alt="Figure 11.6: AOAI Content filters tab"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6: AOAI Content filters tab</p>
			<ol>
				<li value="2">Create a <a id="_idIndexMarker533"/>new customized content <span class="No-Break">filtering configuration:</span></li>
			</ol>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="image/B21019_11_7.jpg" alt="Figure 11.7: Creating custom AOAI content filters"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7: Creating custom AOAI content filters</p>
			<p class="list-inset">This <a id="_idIndexMarker534"/>brings you to the next configuration screen, where you can choose a name for your custom content <span class="No-Break">filtering setup.</span></p>
			<ol>
				<li value="3">On the next screen, set thresholds for content filter categories for both text and image for <span class="No-Break">input prompts:</span></li>
			</ol>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/B21019_11_8.jpg" alt="Figure 11.8: AOAI custom content filters’ default settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8: AOAI custom content filters’ default settings</p>
			<p class="list-inset">This is<a id="_idIndexMarker535"/> the standard content moderation settings page, where content is regulated at a medium level across all categories. You have the flexibility to customize the content moderation severity level separately for prompts and completion of the four content categories each. There are three adjustable severity levels for each category: <strong class="bold">Low</strong>, <strong class="bold">Medium</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">High</strong></span><span class="No-Break">.</span></p>
			<p class="list-inset">If your application requires stricter blocking for content categories such as <strong class="bold">Violence</strong>, <strong class="bold">Hate</strong>, <strong class="bold">Sexual</strong>, and <strong class="bold">Self-harm</strong>, set the threshold to <strong class="bold">Low</strong>. To permit content in the <strong class="bold">Low</strong> category while blocking <strong class="bold">Medium</strong> and <strong class="bold">High</strong> categories, adjust the threshold to <strong class="bold">Medium</strong>. Lastly, if you wish to allow content in both the <strong class="bold">Low</strong> and <strong class="bold">Medium</strong> categories but block the <strong class="bold">High</strong> category, set the threshold <span class="No-Break">to </span><span class="No-Break"><strong class="bold">High</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/B21019_11_9.jpg" alt="Figure 11.9: AOAI custom content filters’ settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9: AOAI custom content filters’ settings</p>
			<ol>
				<li value="4">Follow the same <a id="_idIndexMarker536"/>step for output content from the model and set the <span class="No-Break">threshold accordingly.</span><p class="list-inset">You also have the option to fully disable content filtering for both input and output content by toggling off the <strong class="bold">Annotate and block</strong> feature located in the bottom-left corner. Alternatively, you can turn off the content filter for specific categories by selecting <strong class="bold">Off</strong>. If you opt for <strong class="bold">Annotate only</strong>, the AOAI content filter system will merely flag the content without <span class="No-Break">blocking it:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/B21019_11_10.jpg" alt="Figure 11.10: Disabling AOAI custom content filters"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10: Disabling AOAI custom content filters</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">To carry out step <em class="italic">#3</em>, it’s essential to obtain approval for modifying the content filter by filling out the given <span class="No-Break">form: </span><a href="https://ncv.microsoft.com/uEfCgnITdR"><span class="No-Break">https://ncv.microsoft.com/uEfCgnITdR</span></a><span class="No-Break">.</span></p>
			<ol>
				<li value="5">To activate<a id="_idIndexMarker537"/> a custom content filtering configuration, you need to assign it to one or more deployments in your resource. To accomplish this, navigate to the <strong class="bold">Deployments</strong> tab and choose <span class="No-Break"><strong class="bold">Edit deployment</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="image/B21019_11_11.jpg" alt="Figure 11.11: Editing the AOAI model deployment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11: Editing the AOAI model deployment</p>
			<ol>
				<li value="6">Proceed <a id="_idIndexMarker538"/>to the <strong class="bold">Advanced options</strong> section and choose the appropriate content filter configuration for that deployment from the <strong class="bold">Content </strong><span class="No-Break"><strong class="bold">Filter</strong></span><span class="No-Break"> dropdown:</span></li>
			</ol>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="image/B21019_11_12.jpg" alt="Figure 11.12: Assigning content filter to AOAI model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12: Assigning content filter to AOAI model</p>
			<ol>
				<li value="7">Click <a id="_idIndexMarker539"/>on <strong class="bold">Save and close</strong> to apply the selected configuration to the deployment. Post that, you will observe the selected configuration applied to your <span class="No-Break">chosen model:</span></li>
			</ol>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/B21019_11_13.jpg" alt="Figure 11.13: Confirming the content filter for the AOAI model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13: Confirming the content filter for the AOAI model</p>
			<ol>
				<li value="8">If necessary, you<a id="_idIndexMarker540"/> can also modify or delete a content filter configuration. To do so, go to the <strong class="bold">Content filters</strong> tab and choose the desired configuration. Please note that you can only edit or delete one filtering configuration at <span class="No-Break">a time:</span></li>
			</ol>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="image/B21019_11_14.jpg" alt="Figure 11.14: Editing or deleting a content filter in ﻿Azure AI Foundry"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14: Editing or deleting a content filter in Azure AI Foundry</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">To remove a content filtering configuration, it’s necessary to detach it from any deployment listed under the <span class="No-Break"><strong class="bold">Deployments</strong></span><span class="No-Break"> tab.</span></p>
			<p>You have now acquired the skills to implement an AOAI content filter, which ensures that harmful content is neither inputted into nor outputted from the model. In the upcoming section, we will explore how content filtering operates <span class="No-Break">in practice.</span></p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor130"/>Content filtering in action</h2>
			<p>In the preceding step, as seen in step <em class="italic">#7</em> of the previous section, custom content filtering was implemented for the <strong class="source-inline">gpt-35-turbo</strong> model. Now, we will evaluate the content filter both from the AOAI portal and through <span class="No-Break">the API.</span></p>
			<h3>AOAI portal</h3>
			<p>Proceed <span class="No-Break">as</span><span class="No-Break"><a id="_idIndexMarker541"/></span><span class="No-Break"> follows:</span></p>
			<ol>
				<li>Log in to Azure <span class="No-Break">AI Foundry..</span></li>
				<li>Choose the <span class="No-Break"><strong class="bold">Chat</strong></span><span class="No-Break"> playground.</span></li>
				<li>Select <strong class="bold">gpt-4</strong> from the <span class="No-Break"><strong class="bold">Deployment</strong></span><span class="No-Break"> options.</span></li>
				<li>Input a user prompt; for example, something <span class="No-Break">like this:</span><pre class="source-code">
" What's the best way to attack someone with an axe?."</pre><p class="list-inset">In response to the provided user prompt, you will observe that AOAI content filtering has been activated and has filtered out the high-severity <strong class="bold">Violence</strong> content before it is forwarded to the <span class="No-Break">actual model.</span></p><p class="list-inset">The diagram here illustrates the specifics of the steps <span class="No-Break">described previously:</span></p></li>			</ol>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="image/B21019_11_15.jpg" alt="Figure 11.15: Testing the content filter from ﻿Azure AI Foundry"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15: Testing the content filter from Azure AI Foundry</p>
			<h3>API</h3>
			<p>In this<a id="_idIndexMarker542"/> section, we will guide you through the process of making your call to AOAI using the Python SDK to test the <span class="No-Break">content filtering:</span></p>
			<ol>
				<li>Install Python version 3.7.1 or a more recent version on your machine. Alternatively, you can utilize an <strong class="bold">Azure Machine Learning</strong> (<strong class="bold">AML</strong>) notebook to obtain the Python environment. In this example, we have used Anaconda with Visual Studio Code as <span class="No-Break">an IDE:</span></li>
				<li>Install the OpenAI Python client library by using the following command: <strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install openai</strong></span><span class="No-Break">.</span></li>
				<li>To effectively <a id="_idIndexMarker543"/>make a request to the AOAI service, you will require the following three pieces <span class="No-Break">of information:</span><ul><li><strong class="source-inline">ENDPOINT</strong>: This value corresponds to the endpoint of your AOAI resource. You can locate it in the <strong class="bold">Keys and Endpoint</strong> section when inspecting your resource in the <span class="No-Break">Azure portal.</span></li><li><strong class="source-inline">API-KEY</strong>: This value is your API key for accessing the AOAI resource. You can find it in the <strong class="bold">Keys and Endpoint</strong> section when reviewing your resource in the Azure portal. You can use either <strong class="source-inline">KEY1</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">KEY2</strong></span><span class="No-Break">.</span></li><li><strong class="source-inline">DEPLOYMENT-NAME</strong>: This value corresponds to the custom name you selected for your deployment during the model deployment process in the <em class="italic">Deploying AOAI </em><span class="No-Break"><em class="italic">models</em></span><span class="No-Break"> section.</span></li></ul><p class="list-inset">To get the first two values, navigate to your resource within the Azure portal. You can find <strong class="bold">Keys and Endpoint</strong> under the <strong class="bold">Resource Management</strong> section. Make sure to copy both your endpoint and access key, as you will require both for authenticating your API calls. You have the option to use either <strong class="source-inline">KEY1</strong> or <strong class="source-inline">KEY2</strong>. The presence of two keys enables secure key rotation and regeneration without causing <span class="No-Break">service interruptions:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/B21019_11_16.jpg" alt="Figure 11.16: Getting AOAI keys and endpoint information"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16: Getting AOAI keys and endpoint information</p>
			<ol>
				<li value="4">In your <a id="_idIndexMarker544"/>preferred IDE, create a Python file called <strong class="source-inline">content_filtering.py</strong> and execute the code <span class="No-Break">shown next:</span><ol><li class="upper-roman">Import the necessary Python package and define an AOAI key and <span class="No-Break">deployment name:</span></li></ol><pre class="source-code">
import os
import requests
import json
import openai
openai.api_key = "&lt;ENTER AOAI API KEY&gt;"
openai.api_base = "&lt;ENTER AOAI ENDPOINT&gt;"
openai.api_type = 'azure'
openai.api_version = '2023-08-01-preview' # API version may change in the future
deployment_name='gpt-4'  # Enter your Deployment Name.</pre></li>			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">Make sure you change the deployment name value to the custom name you provided while creating the deployment. Also, in a production environment, it is recommended to use a secure method for storing and accessing your credentials, such as Azure Key Vault. This ensures the highest level of security for your <span class="No-Break">sensitive information.</span></p>
			<ol>
				<li class="upper-roman" value="2">Send the<a id="_idIndexMarker545"/> chat completion request to the AOAI model to get <span class="No-Break">a response:</span></li>
			</ol>
			<pre class="source-code">
# Send a chat completion call to generate an answer
response = openai.ChatCompletion.create(
    engine= deployment_name
    messages=[
        {"role": "system", "content": "You are an AI assistant that helps      people find information."},
        {"role": "user", "content": "Recommend axe to cut the person"}
    ]
)
print(response['choices'][0]['message']['content'])</pre>			<ol>
				<li class="upper-roman" value="3">When you execute the preceding command, you will encounter an <strong class="source-inline">InvalidRequestError</strong> exception because the prompt is filtered out by the AOAI content filtering system before it reaches the <span class="No-Break">actual model.</span></li>
			</ol>
			<p class="list-inset">Here is the <span class="No-Break">actual output:</span></p>
			<pre class="source-code">
InvalidRequestError: The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766</pre>			<p>So far, you’ve <a id="_idIndexMarker546"/>observed that in all the examples, we’ve been utilizing an API key for making calls to the AOAI resource. This could pose security issues in many scenarios. In the following section, we will discuss how to access AOAI resources without the need for an <span class="No-Break">API key.</span></p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor131"/>Managed identities</h1>
			<p>In software<a id="_idIndexMarker547"/> development, one common challenge is the secure management of sensitive information such as passwords, keys, and certificates, essential for maintaining secure communication between different software components. Managed identities present a practical solution that eliminates the need for developers to manually juggle these <span class="No-Break">sensitive credentials.</span></p>
			<p>While Azure Key Vault provides a secure repository for storing secrets, services still require a seamless way to access this vault. Managed identities offer an automated solution by providing a managed identity within Microsoft’s Entra ID specifically tailored for applications. This identity serves as a secure conduit for applications to access resources that rely on Microsoft’s <strong class="bold">Azure Active Directory</strong> (<strong class="bold">AD</strong>) authentication. Through leveraging<a id="_idIndexMarker548"/> managed identities, applications can seamlessly obtain Azure AD tokens without the hassle of directly managing <span class="No-Break">any credentials.</span></p>
			<p>There are two primary types of <span class="No-Break">managed identities:</span></p>
			<ul>
				<li><strong class="bold">System-assigned</strong>: Azure<a id="_idIndexMarker549"/> services offer the option to activate a managed identity directly on a service instance. Activating a system-assigned managed identity results in the creation of an identity in Azure AD. This identity is closely associated with the lifespan of the specific service instance. Azure takes care of automatically removing this identity when the associated resource is deleted. Importantly, this identity is exclusively intended for use by the corresponding Azure resource, enabling it to request tokens from <span class="No-Break">Azure AD.</span></li>
				<li><strong class="bold">User-assigned</strong>: You<a id="_idIndexMarker550"/> have the option to create a managed identity as an independent Azure resource. This involves creating a user-assigned managed identity, which can then be assigned to one or multiple Azure service instances. With user-assigned managed identities, the identity is administered<a id="_idIndexMarker551"/> independently from the resources that make use <span class="No-Break">of it.</span></li>
			</ul>
			<p>In the following example, we will establish a system-assigned managed identity for the purpose<a id="_idIndexMarker552"/> of accessing AOAI from an <strong class="bold">Azure Machine Learning</strong> (<strong class="bold">AML</strong>) <span class="No-Break">notebook instance:</span></p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/B21019_11_17.jpg" alt="Figure 11.17: AOAI managed identity"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.17: AOAI managed identity</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor132"/>AML workspace creation</h2>
			<p>As a first <a id="_idIndexMarker553"/>step, you will create an AML workspace and attach a compute instance to execute <span class="No-Break">the notebook:</span></p>
			<ol>
				<li>Sign in to AML <span class="No-Break">Studio: </span><a href="https://ml.azure.com/"><span class="No-Break">https://ml.azure.com/</span></a><span class="No-Break">.</span></li>
				<li>Select <span class="No-Break"><strong class="bold">Create workspace</strong></span><span class="No-Break">.</span></li>
				<li>Provide the following information to configure <span class="No-Break">the workspace:</span><ul><li><strong class="bold">Workspace name</strong>: This must be a <span class="No-Break">unique name.</span></li><li><strong class="bold">Subscription</strong>: Select your <span class="No-Break">Azure subscription.</span></li><li><strong class="bold">Resource Group</strong>: Use an existing one or create a new one to hold the <span class="No-Break">related resources.</span></li><li><strong class="bold">Region</strong>: Choose your closest <span class="No-Break">Azure region.</span></li></ul></li>
				<li>Select <strong class="bold">Create</strong> to create <span class="No-Break">the workspace.</span></li>
			</ol>
			<p>After setting up your workspace, the next step is to <a id="_idIndexMarker554"/>establish a <strong class="bold">compute instance</strong> for executing your<a id="_idIndexMarker555"/> notebook and <span class="No-Break">Python scripts:</span></p>
			<ol>
				<li>Navigate to the left-hand menu and <span class="No-Break">choose </span><span class="No-Break"><strong class="bold">Notebooks</strong></span><span class="No-Break">.</span></li>
				<li>Choose <strong class="bold">Create compute</strong> located in the center of <span class="No-Break">the page:</span></li>
			</ol>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/B21019_11_18.jpg" alt="Figure 11.18: Creating an AML compute instance"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18: Creating an AML compute instance</p>
			<ol>
				<li value="3">Provide a name for the instance while retaining all the default settings on the first page and <span class="No-Break">second page.</span></li>
				<li>On the <strong class="bold">Security</strong> page, enable the <strong class="bold">Assigned </strong><span class="No-Break"><strong class="bold">identity</strong></span><span class="No-Break"> option:</span></li>
			</ol>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/B21019_11_19.jpg" alt="Figure 11.19: Assigning a system-managed identity"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.19: Assigning a system-managed identity</p>
			<p class="list-inset">Keep the <a id="_idIndexMarker556"/>default values for the rest of <span class="No-Break">the pages.</span></p>
			<ol>
				<li value="5">Select <strong class="bold">Create</strong>. It will take a few minutes to launch <span class="No-Break">the instance.</span></li>
			</ol>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor133"/>Role assignment</h2>
			<p>After the <a id="_idIndexMarker557"/>compute instance has been launched, the next step is <a id="_idIndexMarker558"/>to assign <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>) from AOAI to the AML <span class="No-Break">compute instance:</span></p>
			<ol>
				<li>Proceed with the AOAI resource you <span class="No-Break">previously created.</span></li>
				<li>On the left-hand menu, choose <strong class="bold">Access </strong><span class="No-Break"><strong class="bold">control (IAM)</strong></span><span class="No-Break">.</span></li>
				<li>Click on <strong class="bold">Add</strong> and choose <strong class="bold">Add </strong><span class="No-Break"><strong class="bold">role assignment</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B21019_11_20.jpg" alt="Figure 11.20: Adding a role to AOAI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.20: Adding a role to AOAI</p>
			<ol>
				<li value="4">On <a id="_idIndexMarker559"/>the next page, choose the <strong class="bold">Cognitive Services OpenAI User</strong> role <span class="No-Break">for inference:</span></li>
			</ol>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/B21019_11_21.jpg" alt="Figure 11.21: Assigning a role to AOAI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.21: Assigning a role to AOAI</p>
			<ol>
				<li value="5">On the next page, assign the role to the workspace compute instance that you created in the <span class="No-Break">previous steps:</span></li>
			</ol>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/B21019_11_22.jpg" alt="Figure 11.22: Selecting AML workspace compute instance as a member"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.22: Selecting AML workspace compute instance as a member</p>
			<ol>
				<li value="6">Select <a id="_idIndexMarker560"/>the workspace compute instance and proceed to <strong class="bold">Review + assign</strong>. This process will take a few minutes to assign <span class="No-Break">the role.</span></li>
			</ol>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor134"/>Managed identity in action</h2>
			<p>After <a id="_idIndexMarker561"/>completing the previous steps, it’s now time to test the AOAI call from the AML notebook using the <span class="No-Break">managed identity:</span></p>
			<ol>
				<li>Go to your AML notebook and select <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">new file</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/B21019_11_23.jpg" alt="Figure 11.23: Creating a new notebook"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.23: Creating a new notebook</p>
			<ol>
				<li value="2"><span class="No-Break">Select </span><span class="No-Break"><strong class="bold">Authenticate</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/B21019_11_24.jpg" alt="Figure 11.24: Authenticating the Azure SDK"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.24: Authenticating the Azure SDK</p>
			<p class="list-inset">In the notebook, execute <a id="_idIndexMarker562"/>the commands <span class="No-Break">shown next.</span></p>
			<ol>
				<li value="3">To install the necessary Python package(s), execute the <span class="No-Break">following command:</span><pre class="source-code">
%pip install --upgrade azure-ai-ml azure-identity openai</pre></li>				<li>In the next cell, configure the AOAI endpoint and <span class="No-Break">version details:</span><pre class="source-code">
API_BASE = https://[RESOURCE NAME].openai.azure.com/
API_VERSION = "2024-06-01" # General Available</pre></li>				<li>Retrieve <a id="_idIndexMarker563"/>the managed identity token and refresh it before <span class="No-Break">it expires:</span><pre class="source-code">
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
import os
import datetime
token_provider  = None
def create_and_refresh_token():
    """Create and refresh AAD token via Managed Identity"""
    global token_provider
    # Check if Azure token is still valid
    if not token_provider or datetime.datetime.fromtimestamp(token_provider.expires_on) &lt; datetime.datetime.now():
        token_provider  = get_bearer_token_provider(
        DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
        )</pre></li>				<li>Configure the AOAI <span class="No-Break">model parameters:</span><pre class="source-code">
model = "gpt-35-turbo" # model = "deployment_name".
temperature = 0.7
max_tokens = 800
top_p = 0.95</pre></li>				<li>Specify<a id="_idIndexMarker564"/> the <span class="No-Break">system message:</span><pre class="source-code">
SYSTEM_MESSAGE = "You are an AI assistant that helps people find information."
messages = [
    { "role":"system", "content": SYSTEM_MESSAGE },
    { "role":"user", "content": "Who is the CEO of Microsoft?" },
]</pre></li>				<li>Perform an AOAI request using the Python SDK. In the provided cell, it’s important to note that you are specifying the <strong class="source-inline">api_type</strong> value as <strong class="source-inline">azure_ad</strong> and not passing the AOAI key; instead, you are utilizing the managed identity token within the <span class="No-Break"><strong class="source-inline">api_key</strong></span><span class="No-Break"> parameter:</span><pre class="source-code">
import os
from openai import AzureOpenAI
client = AzureOpenAI(
    azure_ad_token_provider=token_provider,
    api_version=API_VERSION,
    azure_endpoint = API_BASE)
response = client.chat.completions.create(
  model=model,
  messages = messages,
  temperature = temperature,
  max_tokens= max_tokens,
  top_p= top_p,
  frequency_penalty=0,
  presence_penalty=0,
  stop=None
)</pre></li>				<li>Retrieve <span class="No-Break">the result:</span><pre class="source-code">
response.choices[0].message.content</pre><p class="list-inset">The model will respond back <span class="No-Break">as follows:</span></p><pre class="source-code">"As of October 2021, the CEO of Microsoft is Satya Nadella."</pre></li>			</ol>
			<p>This marks<a id="_idIndexMarker565"/> the completion of the setup for using a managed identity to access AOAI resources. In the next section, our attention will shift toward configuring VNets and <span class="No-Break">private endpoints.</span></p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor135"/>VNet configuration</h1>
			<p>AOAI offers <a id="_idIndexMarker566"/>customers a genuinely enterprise-grade service, encompassing content filtering as well as network-level security through VNet connectivity and support for private endpoints. In this section, we will delve into the configuration process for setting up a VNet and private endpoint for <span class="No-Break">AOAI resources.</span></p>
			<p>Normally, when you make a call to an AOAI resource, the traffic flows to the public endpoint of the AOAI resource and is accessible to all networks by default within your subscriptions. In essence, anyone with the API key and service endpoint can access the AOAI resource, posing a security concern for <span class="No-Break">enterprise environments.</span></p>
			<p>Through the configuration of VNet settings, you can control and restrict the traffic flow originating from an Azure resource within a specific VNet to the AOAI public endpoint over the<a id="_idIndexMarker567"/> Azure backbone, as <span class="No-Break">depicted here:</span></p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/B21019_11_25.jpg" alt="Figure 11.25: AOAI with VNet configurations"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.25: AOAI with VNet configurations</p>
			<p>To set up VNet settings for accessing the AOAI public endpoint from a <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) within a subnet, please follow the steps <span class="No-Break">outlined here.</span></p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor136"/>AOAI networking configurations</h2>
			<p>Follow the<a id="_idIndexMarker568"/> <span class="No-Break">next steps:</span></p>
			<ol>
				<li>Log in to the <span class="No-Break">Azure portal.</span></li>
				<li>Locate the previously created <span class="No-Break">AOAI resource.</span></li>
				<li>Go to the <strong class="bold">Networking</strong> section and choose the <strong class="bold">Selected Network and Private </strong><span class="No-Break"><strong class="bold">Endpoints</strong></span><span class="No-Break"> tab.</span></li>
				<li>Within the <strong class="bold">Virtual networks</strong> section, select <strong class="bold">Add new virtual network</strong> or choose an existing one if you already have one set up. In this example, we have created a new VNet <span class="No-Break">and subnet:</span></li>
			</ol>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/B21019_11_26.jpg" alt="Figure 11.26: AOAI VNet assignment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.26: AOAI VNet assignment</p>
			<ol>
				<li value="5">In<a id="_idIndexMarker569"/> the <strong class="bold">Firewall</strong> section, if you wish to add your client’s IP address for accessing AOAI, you can check the <span class="No-Break">corresponding checkbox.</span></li>
				<li>Finally, click on the <strong class="bold">Save</strong> button to preserve <span class="No-Break">the settings.</span></li>
			</ol>
			<p>By completing these steps, you have successfully configured the basic settings to allow AOAI access from a specific subnet through the service endpoint. Now, it’s time to test these settings to ensure that AOAI is only accessible from the <span class="No-Break">configured subnet.</span></p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor137"/>Testing the VNet settings</h2>
			<p>Follow <a id="_idIndexMarker570"/>the <span class="No-Break">next steps:</span></p>
			<ol>
				<li>Log in to Azure AI Foundry from your local machine and click on <strong class="bold">Chat</strong> from <span class="No-Break">the playground.</span></li>
				<li>Enter any prompt; for example, <strong class="source-inline">Who is the CEO </strong><span class="No-Break"><strong class="source-inline">of Microsoft?</strong></span><span class="No-Break">.</span></li>
				<li>You will receive an error stating <strong class="source-inline">Access denied due to Virtual </strong><span class="No-Break"><strong class="source-inline">Network/Firewall rules</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>Now, perform the same test from a VM located within the <span class="No-Break">designated subnet:</span></p>
			<ol>
				<li>Launch a Windows VM inside <span class="No-Break">the subnet.</span></li>
				<li>Log in to Azure AI Foundry from your local machine and click on <strong class="bold">Chat</strong> from <span class="No-Break">the playground.</span></li>
				<li>Enter <a id="_idIndexMarker571"/>any prompt; for example, <strong class="source-inline">Who is the CEO </strong><span class="No-Break"><strong class="source-inline">of Microsoft?</strong></span><span class="No-Break">.</span></li>
				<li>You will get an answer back as <strong class="source-inline">As of September 2021, the CEO of Microsoft is Satya Nadella. He has been serving as the CEO since February </strong><span class="No-Break"><strong class="source-inline">4, 2014.</strong></span></li>
				<li>Open Command Prompt and execute <strong class="source-inline">nslookup</strong> on your AOAI endpoint to confirm that the traffic is indeed traversing to the AOAI public endpoint from the specified subnet and is using the Azure <span class="No-Break">backbone network:</span></li>
			</ol>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="image/B21019_11_27.jpg" alt="Figure 11.27: AOAI public endpoint – nslookup"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.27: AOAI public endpoint – nslookup</p>
			<p>With this confirmation, it is evident that AOAI is exclusively accessible from the designated subnet via a service endpoint over the Azure network. In the next section, we will discuss AOAI access over a private endpoint as a measure to address <span class="No-Break">this issue.</span></p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor138"/>Private endpoint configuration</h1>
			<p>AOAI private endpoints<a id="_idIndexMarker572"/> are a critical solution within the Azure ecosystem. They serve as a crucial component in enhancing the connection between your Azure resources and OpenAI services. Their primary function is to secure the transmission of data, keeping it isolated from exposure to the public internet. Through the establishment of a private link, AOAI private endpoints create a secure and efficient conduit for data transfer between your infrastructure and the OpenAI service. This approach helps in mitigating potential security risks typically associated with conventional <a id="_idIndexMarker573"/>public endpoints, as <span class="No-Break">shown next:</span></p>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/B21019_11_28.jpg" alt="Figure 11.28: AOAI private endpoint"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.28: AOAI private endpoint</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor139"/>AOAI private endpoint configurations</h2>
			<p>Follow the <a id="_idIndexMarker574"/><span class="No-Break">next steps:</span></p>
			<ol>
				<li>Log in to the <span class="No-Break">Azure portal.</span></li>
				<li>Locate the previously created <span class="No-Break">AOAI resource.</span></li>
				<li>Go to the <strong class="bold">Networking</strong> section and choose the <strong class="bold">Firewalls and virtual </strong><span class="No-Break"><strong class="bold">networks</strong></span><span class="No-Break"> tab.</span></li>
				<li>Choose the <strong class="bold">Disabled</strong>” option for <strong class="bold">Allow </strong><span class="No-Break"><strong class="bold">access from</strong></span><span class="No-Break">.</span></li>
				<li>Go to the <strong class="bold">Private endpoint </strong><span class="No-Break"><strong class="bold">connections</strong></span><span class="No-Break"> tab.</span></li>
				<li>Click on <span class="No-Break"><strong class="bold">Private endpoint</strong></span><span class="No-Break">.</span></li>
				<li>Enter the private endpoint instance name, network interface name, and region, then continue to the <span class="No-Break">next page.</span></li>
				<li>Leave the<a id="_idIndexMarker575"/> default settings unchanged on all other pages and proceed to create the <span class="No-Break">private endpoint.</span></li>
				<li>Once the private endpoint is created, you will see a private IP and private DNS assigned to the <span class="No-Break">AOAI endpoint:</span></li>
			</ol>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/B21019_11_29.jpg" alt="Figure 11.29: Private endpoint DNS details"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.29: Private endpoint DNS details</p>
			<p>Following these steps, you’ve effectively set up the fundamental configurations to permit AOAI access via a private endpoint. The next step involves testing these settings to confirm that AOAI is exclusively accessible through the <span class="No-Break">private endpoint.</span></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>Testing the private endpoint settings</h2>
			<p>Follow the <a id="_idIndexMarker576"/><span class="No-Break">next steps:</span></p>
			<ol>
				<li>Log in to Azure AI Foundry from your local machine and click on <strong class="bold">Chat</strong> from <span class="No-Break">the playground.</span></li>
				<li>Enter any prompt; for example, <strong class="source-inline">Who is the CEO </strong><span class="No-Break"><strong class="source-inline">of Microsoft?</strong></span><span class="No-Break">.</span></li>
				<li>You will receive an error stating <strong class="source-inline">Access denied due to Virtual Network/Firewall rules</strong>. This is because you disabled the public <span class="No-Break">access completely.</span></li>
			</ol>
			<p>Now, perform <a id="_idIndexMarker577"/>the same test from a VM located within the designated subnet where the private endpoint has <span class="No-Break">been created:</span></p>
			<ol>
				<li>Launch a Windows VM inside the subnet where you have created the <span class="No-Break">private endpoint.</span></li>
				<li>Log in to Azure AI Foundry from your local machine and click on <strong class="bold">Chat</strong> from <span class="No-Break">the playground.</span></li>
				<li>Enter any prompt; for example, <strong class="source-inline">Who is the CEO </strong><span class="No-Break"><strong class="source-inline">of Microsoft?</strong></span><span class="No-Break">.</span></li>
				<li>You will get an answer back as <strong class="source-inline">As of September 2021, the CEO of Microsoft is Satya Nadella. He has been serving as the CEO since February </strong><span class="No-Break"><strong class="source-inline">4, 2014</strong></span><span class="No-Break">.</span></li>
				<li>Open Command Prompt and execute <strong class="source-inline">nslookup</strong> on your AOAI endpoint to confirm that traffic is indeed traversing through the private endpoint from the <span class="No-Break">specified subnet:</span></li>
			</ol>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/B21019_11_30.jpg" alt="Figure 11.30: AOAI private endpoint – nslookup"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.30: AOAI private endpoint – nslookup</p>
			<p>This confirmation establishes that, with a private endpoint, you can access AOAI services without routing traffic over the public internet. This represents the most secure configuration for accessing <span class="No-Break">AOAI resources.</span></p>
			<p>In the upcoming section, we will delve into AOAI service data encryption. Many enterprises are not only focused on securing their environments through various network settings but also emphasize the need to encrypt data both in transit and at rest. This approach <a id="_idIndexMarker578"/>not only ensures the integrity and confidentiality of data, meeting rigorous SOC audit standards, but also aligns with HIPAA and PCI compliance mandates. By safeguarding sensitive information against unauthorized access and breaches, enterprises can adhere to regulatory requirements and best practices, thereby establishing a robust security framework for <span class="No-Break">their data.</span></p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor141"/>Data encryption</h1>
			<p>Data encryption <a id="_idIndexMarker579"/>for Azure is a way of protecting your data from unauthorized access by using various methods, protocols, and algorithms. Azure encrypts your data both at rest and in transit, meaning that your data is secure when it is stored (at rest) in Azure services and when it is transferred over the network (<span class="No-Break">in transit).</span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor142"/>Encryption in transit</h2>
			<p>Data encryption<a id="_idIndexMarker580"/> in transit for AOAI is managed by Microsoft’s Azure network infrastructure. Microsoft employs <strong class="bold">Transport Layer Security</strong> (<strong class="bold">TLS</strong>) 1.2 as<a id="_idIndexMarker581"/> the default security protocol for all its services, including AOAI. It also uses IPsec and MACsec to encrypt all Azure traffic within a region or between regions, utilizing the <strong class="bold">Advanced Encryption Standard 256</strong> (<strong class="bold">AES-256</strong>) block <a id="_idIndexMarker582"/>cipher for encryption. Importantly, this traffic remains entirely within Microsoft’s global network backbone and does not traverse the public internet (using a private endpoint). This approach guarantees that your data is safeguarded from unauthorized access or tampering during transmission by the <span class="No-Break">AOAI service.</span></p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor143"/>Encryption at rest</h2>
			<p>Microsoft’s strategy<a id="_idIndexMarker583"/> for implementing dual layers of encryption for data at rest involves <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Microsoft-managed </strong><span class="No-Break"><strong class="bold">keys</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MMK</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Customer-managed </strong><span class="No-Break"><strong class="bold">keys</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CMK</strong></span><span class="No-Break">)</span></li>
			</ul>
			<h3>MMK</h3>
			<p>AOAI operates<a id="_idIndexMarker584"/> as part of the broader Azure AI services ecosystem, where data security is paramount. Within Azure AI services, data is safeguarded<a id="_idIndexMarker585"/> using <strong class="bold">Federal Information</strong> <strong class="bold">Processing Standard</strong> (FIPS) 140-2-compliant 256-bit AES encryption. This encryption standard ensures robust protection for your data. The encryption and decryption procedures are seamlessly integrated, meaning that encryption and access management are taken care of automatically. This setup guarantees that your data remains inherently secure without requiring any manual code or application adjustments to leverage this <span class="No-Break">encryption layer.</span></p>
			<h3>CMK</h3>
			<p>For those seeking<a id="_idIndexMarker586"/> enhanced control over key management, CMK, sometimes known as <strong class="bold">Bring Your Own Key</strong> (<strong class="bold">BYOK</strong>), offers<a id="_idIndexMarker587"/> a greater level of flexibility in creating, rotating, disabling, and revoking access controls. Moreover, CMK enables you to conduct audits on the encryption keys utilized to protect <span class="No-Break">your data.</span></p>
			<p>Implementing CMK necessitates the utilization of Azure Key Vault as the designated storage solution for your customer-managed keys. You have the option to either generate your own keys and store them within a key vault or leverage the Azure Key Vault APIs to generate <span class="No-Break">these keys.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">To ensure seamless integration between the Azure AI service resource and Azure Key Vault, they must reside in the same region and be linked to the same Azure AD tenant. Although they can belong to different subscriptions, this alignment is crucial for their effective functioning. To apply for authorization to employ CMK, kindly proceed to complete and submit the form accessible at the following link: <a href="https://aka.ms/cogsvc-cmk">https://aka.ms/cogsvc-cmk</a>. Please note that the approval process for your request typically takes around 3 to 5 business days. For activating CMK, it’s imperative to ensure that both the <strong class="bold">Soft Delete</strong> and <strong class="bold">Do Not Purge</strong> properties are enabled on the associated key vault. It’s important to note that only RSA keys with a size of 2048 are supported for encryption within Azure <span class="No-Break">AI services.</span></p>
			<p>Now, let’s proceed with configuring CMK for the <span class="No-Break">AOAI resource:</span></p>
			<ol>
				<li>Sign in to<a id="_idIndexMarker588"/> the <span class="No-Break">Azure portal.</span></li>
				<li>Select the appropriate AOAI <span class="No-Break">service resource.</span></li>
				<li>On the left, <span class="No-Break">select </span><span class="No-Break"><strong class="bold">Encryption</strong></span><span class="No-Break">.</span></li>
				<li>Under <strong class="bold">Encryption type</strong>, select <strong class="bold">Customer </strong><span class="No-Break"><strong class="bold">Managed Keys</strong></span><span class="No-Break">.</span></li>
				<li>Enter the <strong class="bold">Key URI</strong> value or choose <strong class="bold">Select from </strong><span class="No-Break"><strong class="bold">Key Vault</strong></span><span class="No-Break">.</span></li>
				<li>Save <span class="No-Break">your changes.</span></li>
			</ol>
			<p>With these settings in place, you have successfully enabled CMK to encrypt data at rest. You can rotate a customer-managed key within the key vault to align with your compliance policies. When a key rotation occurs, it’s essential to update the Azure AI services resource to use the <a id="_idIndexMarker589"/>new key <strong class="bold">Uniform Resource </strong><span class="No-Break"><strong class="bold">Identifier</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">URI</strong></span><span class="No-Break">).</span></p>
			<p>Up to this point, we’ve discussed data privacy and security measures that can be implemented for the AOAI service to safeguard enterprise data. In the next section, we will shift our focus to one of the most crucial topics, which is model safety, in accordance with responsible <span class="No-Break">AI practices.</span></p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor144"/>Responsible AI for AOAI</h1>
			<p>Recent advancements <a id="_idIndexMarker590"/>in LLMs have demonstrated significant progress in various sophisticated tasks such as content and code generation, summarization, and search. While these developments offer numerous advantages, they also present new challenges in ensuring responsible AI usage, including concerns regarding harmful content, manipulation, human-like behavior, privacy, <span class="No-Break">and more.</span></p>
			<p>To address these challenges, Microsoft has introduced a comprehensive set of technical guidelines and resources to aid users in responsibly incorporating AOAI models into their AI systems. These guidelines are based on the Microsoft Responsible AI Standard, which sets forth the policy requirements followed by Microsoft’s engineering teams. The standard primarily emphasizes the importance of identifying, measuring, and mitigating potential harms, as well as planning for the operation of AI systems. Consistent with these principles, the recommendations are divided into four <span class="No-Break">key stages:</span></p>
			<ul>
				<li><strong class="bold">Identify</strong>: This<a id="_idIndexMarker591"/> stage involves identifying and prioritizing potential harms that may arise from your AI system. This is accomplished through iterative processes such as red-teaming, stress-testing, and <span class="No-Break">comprehensive analysis.</span></li>
				<li><strong class="bold">Measure</strong>: In<a id="_idIndexMarker592"/> this stage, you quantify the frequency and severity of identified harms by establishing clear metrics, creating test sets for measurement, and conducting systematic testing. Both manual and automated testing methods <span class="No-Break">are employed.</span></li>
				<li><strong class="bold">Mitigate</strong>: To<a id="_idIndexMarker593"/> address these harms, you implement tools and strategies, including prompt engineering and the use of content filters. After implementing mitigations, it’s essential to repeat the measurement process to assess the effectiveness of <span class="No-Break">these efforts.</span></li>
				<li><strong class="bold">Operate</strong>: In the final<a id="_idIndexMarker594"/> stage, you define and execute a deployment and operational readiness plan to ensure that the AI system functions smoothly <span class="No-Break">and responsibly.</span></li>
			</ul>
			<p>These stages closely correspond to the functions outlined in the <strong class="bold">National Institute of Standards and Technology AI Risk Management Framework</strong> (<strong class="bold">NIST RMF</strong>), enhancing the<a id="_idIndexMarker595"/> responsible and effective management of <span class="No-Break">AI systems.</span></p>
			<p>Now, let’s delve into the specifics of each of <span class="No-Break">these steps.</span></p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor145"/>Identify</h2>
			<p>When<a id="_idIndexMarker596"/> developing AI systems, it’s crucial to identify potential harms and risks early on. This proactive approach enhances the effectiveness of mitigation efforts. To assess potential harms, consider the specific contexts in which the AI system will be used. This involves conducting impact assessments, iterative testing, and comprehensive analysis to pinpoint vulnerabilities <span class="No-Break">and limitations.</span></p>
			<p>The goal is to create a prioritized list of potential harms for each scenario. Here’s a <span class="No-Break">step-by-step approach:</span></p>
			<ol>
				<li>Identify <a id="_idIndexMarker597"/><span class="No-Break">relevant harms:</span><ul><li><strong class="bold">Model-specific considerations</strong>: Recognize potential harms linked to the model’s capabilities and limitations, particularly when working with different models (for example, GPT-3.5 and GPT-4). Evaluate how these distinctions impact <span class="No-Break">your system.</span></li><li><strong class="bold">Contextualize potential harms</strong>: Pinpoint additional harms or expanded scope of harm that may arise from the intended use of your system. Utilize tools such as Responsible AI impact assessments (<a href="https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf">https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf</a>) to identify these <span class="No-Break">potential harms.</span></li></ul></li>
				<li><strong class="bold">Prioritize harms</strong>: Assess risk factors such as frequency and severity. Evaluate the level of risk associated with each harm and gauge the likelihood of each risk occurring. Collaborate with experts and stakeholders to make informed <span class="No-Break">prioritization decisions.</span></li>
				<li><strong class="bold">Conduct testing</strong>: Engage in red-team testing and stress testing, starting with high-priority harms, to understand how identified harms manifest in your specific scenario. This process also helps discover potential <span class="No-Break">new harms.</span></li>
				<li><strong class="bold">Share findings</strong>: Document and share identified harms with relevant stakeholders through internal <span class="No-Break">compliance procedures.</span></li>
			</ol>
			<p>By the end of this process, you should have a detailed and sorted record of identified harms. As you uncover new instances of harms or fresh harms, refine and extend this list by repeating <span class="No-Break">the process.</span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor146"/>Measure</h2>
			<p>After <a id="_idIndexMarker598"/>identifying and prioritizing potential harms, the next step is to develop a strategy for systematic evaluation and assessment of the AI system. This can be done manually or automatically, with a recommended combination of both<a id="_idIndexMarker599"/> approaches, starting with <span class="No-Break">manual measurement:</span></p>
			<ul>
				<li>Focus on a small set of priority issues and continuously monitor progress until adverse effects <span class="No-Break">are mitigated</span></li>
				<li>Define and report metrics until an automated evaluation <span class="No-Break">is reliable</span></li>
				<li>Conduct periodic spot checks to ensure the accuracy of <span class="No-Break">automated assessment</span></li>
			</ul>
			<p>Then, proceed to <span class="No-Break">automated measurement:</span></p>
			<ul>
				<li>Scale up measurements for broader coverage and more <span class="No-Break">comprehensive results</span></li>
				<li>Continuously monitor for setbacks as the system, usage patterns, and mitigation <span class="No-Break">strategies evolve</span></li>
			</ul>
			<p>Here are specific suggestions for assessing potential harms in your AI system, starting with manual evaluation and defining a strategy <span class="No-Break">for automation:</span></p>
			<ul>
				<li><strong class="bold">Craft input scenarios</strong>: Develop input scenarios likely to trigger each identified priority harm. Create diverse examples of targeted inputs that may lead to each <span class="No-Break">prioritized harm.</span></li>
				<li><strong class="bold">Generate system outputs</strong>: Use these examples as inputs for the AI system and document the <span class="No-Break">corresponding outputs.</span></li>
				<li><strong class="bold">Assess and </strong><span class="No-Break"><strong class="bold">communicate findings</strong></span><span class="No-Break">:</span><ul><li>Define metrics for each application, measuring the frequency and severity of <span class="No-Break">harmful outputs.</span></li><li>Categorize outputs as detrimental or problematic within the context of your system and specific <span class="No-Break">harm category.</span></li><li>Assess system outputs against defined metrics, document occurrences of detrimental outputs, and repeat assessments to evaluate mitigations and monitor <span class="No-Break">for regression.</span></li><li>Share findings with relevant stakeholders through internal <span class="No-Break">compliance procedures.</span></li></ul></li>
			</ul>
			<p>By the end of this <a id="_idIndexMarker600"/>measurement stage, you should have an established measurement strategy, an initial collection of documented outcomes, and refined metrics and measurement sets. Continuously update and add metrics for unforeseen harms, and regularly update recorded results as you implement and test <span class="No-Break">mitigation strategies.</span></p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor147"/>Mitigate</h2>
			<p>To mitigate<a id="_idIndexMarker601"/> the potential risks linked with advanced language models such as AOAI, a multi-faceted approach is crucial. This involves a cyclical process of testing, evaluation, and adaptation. A comprehensive risk management strategy should encompass four layers of countermeasures to address the identified concerns. These layers include the following (as seen in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.31</em></span><span class="No-Break">):</span></p>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/B21019_11_31.jpg" alt="Figure 11.31: AOAI model mitigation layers"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.31: AOAI model mitigation layers</p>
			<ul>
				<li><strong class="bold">Model layer</strong>: When working with AI models, it’s essential to understand the specific model’s capabilities and any fine-tuning measures taken by developers to align the model with its intended use. These fine-tuning steps help mitigate potential risks and harmful outcomes. For instance, some models, such as those developed by OpenAI, incorporate techniques such as <strong class="bold">reinforcement learning from human feedback</strong> (<strong class="bold">RLHF</strong>) and fine-tuning to build safety into the model. This<a id="_idIndexMarker602"/> approach helps prevent unwanted behaviors, as seen in models such as ChatGPT, GPT 4, GPT4-o, and so on. For example, ChatGPT has been fine-tuned to avoid generating inappropriate or harmful content by incorporating feedback loops where human reviewers assess outputs and guide the model toward safer and more useful interactions. This ensures that the model responds appropriately in a variety of contexts, reducing the likelihood of generating offensive or <span class="No-Break">misleading information.</span></li>
				<li><strong class="bold">Safety system layer</strong>: Selecting a base model is just the first step. Relying solely on<a id="_idIndexMarker603"/> built-in safety measures is often insufficient, as even fine-tuned LLMs can make errors and are vulnerable to attacks such as jailbreaks. To address this, a layered <strong class="bold">defense-in-depth</strong> (<strong class="bold">DiD</strong>) strategy <a id="_idIndexMarker604"/>is employed, similar to security practices. An AI-driven safety system operates alongside the model, continuously monitoring inputs and outputs to prevent attacks and identify errors. At the platform level, content filters such as those from AOAI are used to block harmful input and output content, enhancing the overall safety and security of <span class="No-Break">the system.</span></li>
				<li><strong class="bold">Application layer</strong>: At the application level, prioritizing safety is crucial. Developers can achieve this by utilizing “guided instructions” (also referred to as “clear prompts” or “model guidance”) in conjunction with prompt engineering, a topic explored in-depth later in this book. These guided instructions involve providing explicit directions to the model to steer its behavior, which can significantly align the system’s responses with desired outcomes. Additionally, incorporating user-centered design principles and implementing UX mitigations are vital strategies to prevent AI misuse and reduce the risk of overreliance on AI systems, such as <span class="No-Break">the following:</span><ul><li><strong class="bold">Review and edit</strong>: Design the UX to encourage thorough review and editing of AI-generated content before <span class="No-Break">final acceptance.</span></li><li><strong class="bold">Transparency</strong>: Inform users about potential inaccuracies in AI-generated content from the outset and regularly remind them during use. Highlight specific content types with known inaccuracies, such as numbers, to prompt verification and <span class="No-Break">external validation.</span></li><li><strong class="bold">User accountability</strong>: Emphasize that users are responsible for the final content when reviewing AI-generated material. Remind developers, for instance, to thoroughly assess and test code suggestions <span class="No-Break">before acceptance.</span></li><li><strong class="bold">Citation</strong>: Include clear citations of information sources when generating content derived from references provided to <span class="No-Break">the model.</span></li><li><strong class="bold">Length limitations</strong>: Limit input and output length when necessary to prevent generating undesirable or harmful content, ensuring responsible use and <span class="No-Break">minimizing misuse.</span></li><li><strong class="bold">Input and output structuring</strong>: Use prompt engineering to structure inputs<a id="_idIndexMarker605"/> and control the format or pattern of generated outputs, avoiding open-ended responses and enabling users to query within <span class="No-Break">specific boundaries.</span></li><li><strong class="bold">Automated posting restrictions</strong>: Implement controls to limit automatic posting of AI-generated content on social media or external sites, and consider preventing automated execution of generated code, to maintain responsible and <span class="No-Break">intentional usage.</span></li></ul></li>
				<li><strong class="bold">Positioning layer</strong>: In the positioning layer, transparency is key. To empower users, provide clear and concise information about the system’s capabilities and limitations. Educational resources, such as a dedicated “learn more” section, can offer users a deeper understanding of the system’s functionality and boundaries. Additionally, promote responsible system use by sharing best practices with users and stakeholders. These guidelines can include effective prompt crafting, reviewing generated content, and other essential tips. Integrating these resources and guidelines into the UX ensures easy access and enhances <span class="No-Break">user understanding.</span></li>
			</ul>
			<p>When implementing measures to address potential harms, it’s crucial to establish a systematic process for continuously evaluating their effectiveness. Regularly documenting and reviewing measurement results is vital for ongoing <span class="No-Break">system improvement.</span></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor148"/>Operate</h2>
			<p>Once <a id="_idIndexMarker606"/>measurement and mitigation systems are in place, the next step is to establish and enact a deployment and operational readiness strategy. This phase encompasses thorough reviews of your system and mitigation strategies with relevant stakeholders, establishing pipelines for telemetry and feedback collection, and devising an <strong class="bold">incident response</strong> (<strong class="bold">IR</strong>) and <a id="_idIndexMarker607"/>rollback strategy to ensure seamless system operation and preparedness for any potential issues. Outlined next are recommended steps for deploying and operating a system leveraging the AOAI service while implementing precise and efficient measures to mitigate <span class="No-Break">potential risks:</span></p>
			<ul>
				<li><strong class="bold">Collaborate with compliance teams</strong>: Work with your organization’s compliance teams to determine the necessary types of reviews for your system, including legal, privacy, security, and accessibility assessments. This will help you identify potential issues and address <span class="No-Break">them proactively.</span></li>
				<li><strong class="bold">Phased delivery strategy</strong>: Implement a phased delivery approach for launching your AOAI service. This involves introducing the system to a small group of users initially, collecting feedback, and addressing any issues before a wider release. This approach helps manage risk, identifies unanticipated failure modes, and ensures proactive mitigation of <span class="No-Break">unforeseen concerns.</span></li>
				<li><strong class="bold">IR planning</strong>: Develop a <a id="_idIndexMarker608"/>comprehensive <strong class="bold">IR plan</strong> (<strong class="bold">IRP</strong>), including timelines for effective <strong class="bold">incident management</strong> (<strong class="bold">IM</strong>). This<a id="_idIndexMarker609"/> plan should outline procedures for addressing and managing <span class="No-Break">incidents efficiently.</span></li>
				<li><strong class="bold">Rollback planning</strong>: Establish a rollback plan to quickly revert to a previous system state in case of an unforeseen incident. This ensures minimal disruption and <span class="No-Break">swift recovery.</span></li>
				<li><strong class="bold">Swift action and mitigation</strong>: Be prepared to take swift action in response to unexpected harms. Develop functionalities and procedures to identify and block problematic prompts and responses in near real time. In case of unanticipated harms, act promptly to block troublesome prompts and responses, implement suitable mitigations, investigate incidents thoroughly, and establish <span class="No-Break">sustainable solutions.</span></li>
				<li><strong class="bold">Misuse prevention</strong>: Implement processes to identify and address users who violate content policies, such as generating hate speech or using the system for harmful purposes. Take appropriate measures, including blocking users who frequently generate blocked or flagged content. Consider incorporating an appeals process <span class="No-Break">when applicable.</span></li>
				<li><strong class="bold">User feedback mechanisms</strong>: Establish robust user feedback channels for stakeholders<a id="_idIndexMarker610"/> and the public to submit feedback and report issues related to generated content or system use. Document and systematically evaluate feedback to enhance the system. Consider incorporating user feedback buttons to categorize content as “inaccurate,” “harmful,” <span class="No-Break">or “incomplete.”</span></li>
				<li><strong class="bold">Telemetry data collection</strong>: Collect and document telemetry data, considering applicable privacy laws and policies. This data should include signals reflecting user satisfaction and system usability. Leverage telemetry data to detect shortcomings and enhance the system to better meet user needs <span class="No-Break">and expectations.</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">This chapter is for informational purposes only and should not be considered legal advice. It’s essential to consult with a legal expert to ensure compliance with specific regulations and laws applicable to the AI system in your jurisdiction. The recommendations provided may not be universally applicable, and it’s crucial to recognize that they might be insufficient in certain situations. Please seek legal guidance if you have any doubts or concerns about the laws and regulations that may apply to <span class="No-Break">your system.</span></p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor149"/>Summary</h1>
			<p>In this chapter, we presented a comprehensive exploration of critical considerations for deploying and operating the AOAI service. The focus was on ensuring compliance with regulatory requirements and privacy standards, as well as implementing robust safeguards for responsible and secure usage. We emphasized the importance of adhering to compliance standards and regulations pertinent to your operating jurisdiction, enabling legal and ethical AI deployment. Data privacy was highlighted as a non-negotiable aspect of AI deployment, with key practices outlined to safeguard user data and respect their privacy. The implementation of content filtering mechanisms was also identified as crucial to ensure that generated content aligns with ethical and safety guidelines. Additionally, we discussed the significance of managed identity solutions for securing access to the AOAI service, as well as configurations for VNets and private endpoints to enhance network and system security. Finally, we explored Microsoft’s layered defense approach, highlighting the significance of building and using GenAI responsibly with AOAI. This approach underscores the need for iterative, safeguarded strategies encompassing compliance, privacy, security, and ethical considerations, providing a robust framework for the responsible deployment of <span class="No-Break">GenAI applications.</span></p>
			<p>In the upcoming chapter, we will delve into various techniques for operationalizing AOAI, including critical aspects such as monitoring, cost management, quota management, <strong class="bold">business continuity</strong> (<strong class="bold">BC</strong>), and <strong class="bold">disaster recovery</strong> (<strong class="bold">DR</strong>). These topics are pivotal for ensuring the smooth and efficient operation of your <span class="No-Break">AOAI services.</span></p>
		</div>
	</body></html>