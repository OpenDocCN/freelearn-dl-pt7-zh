["```py\n## def contained under  __init__(self, actions): ##\n\n## Initialize the base network\nself.inputVal, self.QValue = self.deepQarchitecture()\n\n## Initialize the target network\nself.inputValT, self.QValueT = self.deepQarchitecture()\n```", "```py\nself.starting_ep = 1.0\nself.ending_ep = 0.1\n```", "```py\ndef select(self):\n    ## Select a Q Value from the Base Q Network\n    QValue = self.QValue.eval(feed_dict = {self.iputVal:[self.currentState]})[0]\n    ## Initialize actions as zeros\n    action = np.zeros(self.action)\n    action_index = 0\n    ## If this timestep is the first, start with a random action\n    if self.timeStep % 1 == 0:\n      ##\n      if random.random() <= self.starting_ep:\n        a_index = random.randrange(self.action)\n        action[a_index] = 1\n      else:\n        action_index = np.argmax(QValue)\n        action[action_index] = 1\n    else:\n      action[0] = 1\n```", "```py\n## Anneal the value of epsilon\n    if self.starting_ep > self.ending_ep and self.timeStep > self.observe:\n      self.starting_ep -= (self.starting_ep - self.ending_ep) / self.explore\n```", "```py\ndef train(self):\n    ''' Training procedure for the Q Network'''\n\n    minibatch = random.sample(self.replayBuffer, 32)\n    stateBatch = [data[0] for data in minibatch]\n    actionBatch = [data[1] for data in minibatch]\n    rewardBatch = [data[2] for data in minibatch]\n    nextBatch = [data[3] for data in minibatch]\n```", "```py\nbatch = []\n    qBatch = self.QValueT.eval(feed_dict = {self.inputValT: nextBatch})\n    for i in range(0, 32):\n      terminal = minibatch[i][4]\n      if terminal:\n        batch.append(rewardBatch[i])\n      else:\n        batch.append(rewardBatch[i] + self.gamma * np.max(qBatch[i]))\n```", "```py\nself.trainStep.run(feed_dict={\n      self.yInput : batch,\n      self.actionInput : actionBatch,\n      self.inputVal : stateBatch\n      })\n```", "```py\n## Save the network on specific iterations\n    if self.timeStep % 10000 == 0:\n      self.saver.save(self.session, './savedweights' + '-atari', global_step = self.timeStep)\n```", "```py\n  def er_replay(self, nextObservation, action, reward, terminal):\n    newState = np.append(nextObservation, self.currentState[:,:,1:], axis = 2)\n    self.replayMemory.append((self.currentState, action, reward, newState, terminal))\n    if len(self.replayBuffer) > 40000:\n      self.replayBuffer.popleft()\n    if self.timeStep > self.explore:\n      self.trainQNetwork()\n\n    self.currentState = newState\n    self.timeStep += 1\n```", "```py\nimport cv2import sysfrom deepQ import deepQimport numpy as npimport gym\n```", "```py\nclass Atari:    def __init__(self):    self.env = gym.make('SpaceInvaders-v0')    self.env.reset()    self.actions = self.env.action_space.n    self.deepQ = deepQ(self.actions)    self.action0 = 0\n```", "```py\ndef preprocess(self,observation): ...\n```"]