<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Dynamic Programming for Optimal Policies</h1>
                </header>
            
            <article>
                
<p><strong>Dynamic programming</strong> (<strong>DP</strong>) represents a set of algorithms that can be used to calculate an optimal policy given a perfect model of the environment in the form of a <strong>Markov decision process</strong> (<strong>MDP</strong>). DP methods update the estimates of the state values on the basis of the estimates made in the previous steps.¬†In DP, an optimization problem is decomposed into simpler subproblems and the solution for each subproblem is stored so that each subproblem is solved only once. In this chapter, we will learn how to select the optimal portfolio using DP through an R code implementation.</p>
<p>The following topics are covered in the chapter:</p>
<ul>
<li>Understanding DP</li>
<li>Learning the top-down¬†DP approach</li>
<li>Analyzing the difference between recursion and memoization</li>
<li>Learning the optimization techniques</li>
<li>Implementing DP in reinforcement learning applications</li>
<li>Solving the knapsack problem</li>
<li>Optimization of a robot navigation system</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2RPIFfB">http://bit.ly/2RPIFfB</a></p>
<p>¬†</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding DP</h1>
                </header>
            
            <article>
                
<p>DP is a mathematical methodology developed by Richard Bellman in the 1950s. It is used to solve problems in which a series of interdependent decisions must be addressed in sequence. The underlying principle behind this methodology is that of Bellman's optimality‚Äîwhatever the initial status and the initial decision, subsequent decisions must provide an optimal policy with respect to the status resulting from the previous decision. This is the essential feature of an optimal policy.</p>
<p>Consider an example of finding the best path that joins two positions. The optimality principle states that every sub-path included in it, between any intermediate and final position, must in turn be optimal. Based on this principle, DP solves a problem by making one decision at a time. At each step, the best policy for the future is determined, regardless of past choices (it is a Markov process), if these are also optimal.</p>
<p>DP is therefore effectively applicable whenever the original problem can be broken down into a series of minor subproblems, or when the cost paid or the profit obtained are expressed as the sum of the elementary costs associated with each individual decision. More generally, the cost must be expressed through some operators as a composition of elementary costs¬†<span>that are individually dependent on a single decision</span>.</p>
<p>The following diagram shows the best route between two nodes of a network (the red path) among all those available (and by "best" I mean "shortest"):</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-483 image-border" src="assets/70f9d33c-5edf-4d4c-8b2b-4ca7c71f4682.png" style="width:27.08em;height:14.25em;"/></p>
<p>There are many paths available to reach the same destination: only one is the shortest.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's now examine the concepts underlying this technology. We will begin by comparing two very popular techniques.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning the top-down DP approach</h1>
                </header>
            
            <article>
                
<p>To understand the mechanism behind DP, we can compare it with another very common problem-solving mechanism‚Äî<strong>divide and conquer</strong>. With this mechanism, a problem is divided into two or more subproblems, and the solution of the original problem is constructed starting from the subproblem solutions. This approach is called a top-down technique and works according to the following procedure:</p>
<ol>
<li>Divide the problem instance into two or more sub-instances.</li>
<li>Recursively solve the problem for each sub-instance.</li>
<li>Recombine the subproblem solution in order to obtain the global solution.</li>
</ol>
<p>This mechanism is widely applied to the resolution of multiple problems. The most popular applications are two of the most commonly used sorting algorithms‚Äîquick sort and merge sort.</p>
<p>For example, in the <strong>quick sort</strong> algorithm, the elements of the list to be sorted are divided into two blocks, the smaller ones and the larger ones of a pivot, and the algorithm is called recursively on the two blocks.¬†In merge sort, the algorithm finds the index of the central position and divides the list into two blocks each with n/2 elements. Then the algorithm is called recursively on the two blocks.</p>
<p>There are cases in which divide and conquer is not applicable because we do not know how to get subproblems‚Äîthe problem does not contain enough information to allow us to decide how to break it into several parts.</p>
<p>In this case, DP comes into play: we proceed to calculate the solutions of all possible subproblems, and starting from sub-solutions we obtain new sub-solutions until the original problem is solved. Unlike the divide and conquer mechanism, the subproblems to be solved are not necessarily disjoint, which means one subproblem can be common to several other subproblems. In order to avoid recalculation of a subproblem more than once, the subproblems are resolved with a bottom-up strategy‚Äîfrom the smallest subproblem to the largest subproblem, and the solutions to these subproblems are stored in appropriate tables so that they are available (if necessary) for the solution of other subproblems.</p>
<p>Often in recursive algorithms we find ourselves dealing with procedures that are unnecessarily burdensome from a computational point of view. Let's see how we can tackle this problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the difference between recursion and memoization</h1>
                </header>
            
            <article>
                
<p>On the basis of what has been said here, we can deduce that DP is used in cases where there is a recursive definition of the problem, but the direct transformation of this definition into an algorithm generates a program of exponential complexity due to the repeated calculation on the same subsets of data from the different recursive calls. An example is the calculation of the Fibonacci numbers, which we will analyze in detail later.</p>
<p>We have seen that DP is a technique for solving recursive problems more efficiently. Why does this happen? Many times in the recursive procedures we solve the subproblems repeatedly. In DP this does not happen‚Äîwe memorize the solution of these subproblems so that we do not have to solve them again. This is called <strong>memoization</strong>.</p>
<p>If the value of a variable at a given step depends on the results of previous calculations, and if the same calculations are repeated over and over, then it is convenient to store intermediate results so as to avoid repeating computationally expensive calculations.</p>
<p>To better understand the difference between recursion and memoization, let's analyze a simple example: the calculation of the factorial of a number. It is called factorial of a natural number n, indicated with <em>n!</em>, the product of positive integers less than or equal to that number. The calculation of the factorial of n is given by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6c2795e7-8eef-4276-8325-fccbaf3ad1aa.png" style="width:15.83em;height:3.17em;"/></p>
<p>The calculation of the factorial of a number can also be defined recursively:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/68a097b4-8e80-4694-9e01-3ec25e5e14b2.png" style="width:15.67em;height:2.92em;"/></p>
<p>A function is called <strong>recursive</strong> if it calls itself. The recursive function can directly solve only particular cases of a problem called base cases (such as those present in the previous formula): if it is invoked, passing some data that constitute one of the base cases, then it returns a result. At each call, the data is reduced, so at a certain point we arrive at one of the base cases. When the function calls itself, it suspends its execution to make the new call. The execution resumes when the internal call to itself ends. The sequence of recursive calls ends when the innermost one (nested) encounters one of the base cases. Now let's see how to optimize this technique.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning the¬†optimization techniques</h1>
                </header>
            
            <article>
                
<p>An optimization problem is a problem whose solution can be measured in terms of a cost function, also called an objective. The value to look for is normally the minimum value or the maximum value of this function. Optimization problems can be reduced to a sequence of decision problems.</p>
<p>To solve an optimization problem it is necessary to use an iterative algorithm. That is a calculation program that, given a current approximation of the solution, determines a new approximation<span>¬†with an appropriate sequence of operations</span>. Starting from an initial approximation, a succession of possible solutions to the problem is thus determined.</p>
<p>The search algorithms of the optimal solution can be classified into the following three classes:</p>
<ul>
<li><strong>Enumerative techniques</strong>:¬†Enumerative techniques look for the optimal solution in all the points of the domain of the function. Simplifications can come from reducing the problem to simpler subproblems. <span>DP</span><span>¬†</span>is one of these techniques.</li>
<li><strong>Numerical techniques</strong>:¬†These techniques optimize the problem by exploiting a set of necessary and sufficient conditions.<span>¬†</span>These can be classified as direct and indirect. Indirect methods look for the least function by solving a set of non-linear functions and searching the solution iteratively until the gradient of the cost function is null. Direct methods make the gradient guide the search for the solution.</li>
<li><strong>Probabilistic techniques</strong>:¬†Probabilistic techniques are based on enumerative techniques, but they use additional information to carry out the research and can be seen as evolutionary processes.<span>¬†</span>This category includes the simulated annealing <span>algorithm</span>, which uses a thermodynamic evolutionary process, and the class of genetic algorithms, which exploit biological evolutionary techniques.</li>
</ul>
<p>In the next section, we will discuss these optimization techniques based on DP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating the Fibonacci sequence</h1>
                </header>
            
            <article>
                
<p>Leonardo Pisano, also called Fibonacci, was a famous Italian mathematician (Pisa 1175 - 1240). His name is linked to the Fibonacci succession, which arose from a problem proposed by the Emperor Frederick II of Swabia. In Pisa in 1223 in a tournament of mathematicians, he proposed the following problem: <em>How many pairs of rabbits are obtained in a year, excluding cases of death, supposing that each couple gives birth to another couple every month and that the youngest couples are able to reproduce already at the second month of life?</em></p>
<p>Fibonacci gave such a quick answer to the test that someone thought the tournament was rigged:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c66e948b-867c-4e25-b839-a0f4eacfa772.png" style="width:23.92em;height:1.17em;"/></p>
<p>Look at the series given here:</p>
<ul>
<li>The first 2 elements are 1, 1.</li>
<li>Every other element is given by the sum of the two preceding it.</li>
</ul>
<p>Indicating with <em>F(n)</em>, the number of pairs present in month n, the Fibonacci sequence becomes the following:</p>
<ul>
<li><em>F(1) = 1</em></li>
<li><em>F(2) = 1</em></li>
<li><em>F(n) = F (n-1) + F (n-2)</em> in the <em>n</em><sup>th</sup> <span>month</span>, where <em>n</em>&gt; 2</li>
</ul>
<p>On the basis of this definition we conventionally assume <em>F(0) = 0</em>, so that the recursive relation <em>F(n) = F(n-1) + F(n-2)</em> is valid also for <em>n = 2</em>.</p>
<p>The Fibonacci succession has led us to study many areas of mathematics and natural sciences. However, despite having discovered this important succession, Fibonacci did not grasp many aspects of it. Four centuries later, Kepler observed that the relationship between two successive terms tended to the Golden Ratio.</p>
<p>So let's see a simple function in R that calculates the Fibonacci number through a recursive procedure:</p>
<pre>FibRec &lt;- function(n) {<br/>  if (n&lt;=2)<br/>    return(1)<br/>  return (FibRec(n-1)+FibRec(n-2))<br/>}<br/><br/>StartTime &lt;- Sys.time()<br/>paste("20th Fibonacci number is: ",FibRec(20))<br/>EndTime &lt;- Sys.time()<br/>paste("Computational time using Recursion is: ",EndTime - StartTime)</pre>
<p>Within the function, there is an <kbd>if</kbd> structure with two options: if <kbd>n&gt; 2</kbd> the function calls itself when <kbd>n&lt;=2</kbd> returns <kbd>1</kbd>. The call to <kbd>FibRec(n-1)</kbd> asks the function to solve a problem simpler than the initial one (the value is lower), but it is always the same problem. The function continues to call itself until it reaches the basic case that it can solve immediately. To compare two solution techniques, the <kbd>Sys.time()</kbd> function is used to calculate the computational cost. The results are shown as follows:</p>
<pre><strong>"20th Fibonacci number is:  6765"</strong><br/><strong>Computational time using Recursion is:  0.0400021076202393</strong></pre>
<p>This program, given the nature of the recursive algorithm used, requires n + 1 invocations of the factorial function to arrive at a result, and each of these invocations, in turn, has a cost associated with the time taken by the function to return the calculated value.</p>
<p>This program can be improved through memoization as follows:</p>
<ol>
<li>Create a variable to store temporary results (<kbd>RecTable</kbd>).</li>
<li>Before performing a calculation, find out if the calculation has already been done. If so, use the stored result.</li>
<li>If we are calculating this for the first time, store the results for future use.</li>
</ol>
<p>A memoized version of the program is shown in the following code:</p>
<pre>RecTable &lt;- c(1, 1, rep(NA, 100))<br/>FibMem &lt;- function(x) {<br/>    if(!is.na(RecTable[x])) return(RecTable[x])<br/>    ans &lt;- FibMem(x-2) + FibMem(x-1)<br/>    RecTable[x] &lt;&lt;- ans<br/>    ans<br/>  }<br/><br/>StartTime &lt;- Sys.time()<br/>paste("20th Fibonacci number is: ",FibMem(20))<br/>EndTime &lt;- Sys.time()<br/><br/>paste("Computational time using Memoization is: ",EndTime - StartTime)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In this case, we store the Fibonacci numbers in a table that can then be retrieved for the next calculation. In this way, the entire calculation is avoided at each occurrence. Memoization increases the efficiency of a function in terms of time. The improvement is highlighted as often as this function is called, thus determining an acceleration of the algorithm. Here are the results:</p>
<pre><strong>20th Fibonacci number is:  6765</strong><br/><strong>Computational time using Memoization is:  0.0310020446777344</strong></pre>
<p>From the comparison between the two computational costs, it is possible to notice that the version with memoization is faster. Now let's see how to use the potential offered by DP in the context of reinforcement learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing DP¬†in reinforcement learning applications</h1>
                </header>
            
            <article>
                
<p>DP represents a set of algorithms that can be used to calculate an optimal policy given a perfect model of the environment in the form of an MDP. The fundamental idea of DP, as well as reinforcement learning in general, is using state values and actions to look for good policies.</p>
<p>The DP methods approach the resolution of Markov decision-making processes through the iteration of two processes called <strong>policy evaluation</strong> and <strong>policy improvement</strong>:</p>
<ul>
<li>Policy evaluation algorithm consists of applying an iterative method to the resolution of the Bellman equation. Since convergence is guaranteed to us only for k ‚Üí ‚àû, we must be content to have good approximations by imposing a stopping condition.</li>
<li>Policy improvement algorithm improves the policy based on current values.</li>
</ul>
<p>A disadvantage of the policy iteration algorithm is that in every step we have to evaluate a policy. This involves an iterative process that we do not know¬†<span>the time of convergence of¬†</span><span>a priori, which will depend on how the starting policy was chosen.</span></p>
<p>One way to overcome this drawback is to cut off the evaluation of the policy at a specific step. This operation does not change the guarantee of convergence to the optimal value. A special case in which the assessment of the policy is blocked¬†step by step¬†(also called sweep) defines the value iteration algorithm. In the value iteration algorithm, a single iteration of calculation of the values is performed between each step of the policy improvement.</p>
<p class="mce-root"/>
<p>The DP algorithms are therefore essentially based on <span>policy evaluation and policy improvement</span> that take place in parallel. The repeated execution of these two processes makes the general process converge towards the optimal solution. In the policy iteration algorithm, the two phases alternate and each ends before the other begins.</p>
<p>The DP methods operate through the entire set of states that can be assumed by the environment, performing a complete backup for each state at each iteration. Each update operation performed by the backup updates the value of a status based on the values ‚Äã‚Äãof all possible successor states. These states are weighed for their probability of occurrence, induced by the policy of choice and by the dynamics of the environment. Full backups are closely related to the Bellman equation, they are nothing more than the transformation of the equation into assignment instructions.</p>
<p>When a complete backup iteration does not bring any change to the state values, convergence is obtained and therefore the final state values ‚Äã‚Äãfully satisfy the Bellman equation. The DP methods are applicable only if there is a perfect model of the alternator, which must be equivalent to an MDP.</p>
<p>Precisely for this reason, the DP algorithms are of little use in reinforcement learning, both for their assumption of a perfect model of the environment and for the high and expensive computation. But it is still opportune to mention them because they represent the theoretical basis of reinforcement learning. In fact, all the methods of reinforcement learning try to achieve the same goal of the DP methods, only with lower computational cost and without the assumption of a perfect model of the environment.</p>
<p>The DP methods converge to the optimal solution with a number of polynomial operations with respect to the number of states ùëõ and actions ùëö against the number of exponential operations ùëö*ùëõ required by methods based on direct search.</p>
<p>DP methods update the estimates of the state values on the basis of the estimates made in the previous steps.¬†This represents a special property, which is called bootstrapping. Several methods of RL perform bootstrapping, even methods that do not require a perfect model of the environment, as required by the DP methods. Let's see a practical case of using DP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Solving the knapsack problem</h1>
                </header>
            
            <article>
                
<p><span><span>In this section, we will</span></span> analyze a classic problem that's been studied for more than a century, since 1897‚Äîthe knapsack problem. The first to deal with the knapsack problem was the mathematician Tobias Dantzig who gave it the name deriving it from the common problem of packing the most useful items without overloading the knapsack.</p>
<p class="mce-root"/>
<p>A problem of this type can be associated with different situations arising from real life. To better characterize the problem, we will propose a rather unique one: a thief goes into a house and wants to steal valuables. He puts them in his knapsack, but is bound by weight. Each object has its own value and weight, so he must choose the objects that are of high value but not high weight. It must not exceed the weight limit in the knapsack, but at the same time optimize the value.</p>
<p>Now, we will address the problem from a mathematical point of view. Suppose we have a set X composed of n objects labeled with integers from 1 to <em>n</em>: {1, 2, ..., <em>n</em>}. These objects meet the following conditions:</p>
<ul>
<li>The <em>i</em><sup>th</sup> object has a weight <em>p[i]</em> and value <em>v[i]</em>.</li>
<li>There is only one instance of each object.</li>
</ul>
<p>We have a container able to carry at most a weight equal to P. We want to determine a subset Y ‚äÜ X of objects:</p>
<ul>
<li>The total weight of the objects in Y is ‚â§ P.</li>
<li>The total value of the objects in Y is the maximum possible.</li>
</ul>
<p>These two conditions in mathematical formalism take the following form:</p>
<ul>
<li>We want to determine a subset Y ‚äÜ X of objects so that:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bd3ae9bc-72ff-4ab3-a500-4520fd52432d.png" style="width:6.00em;height:2.50em;"/></p>
<ul>
<li>To maximize the following overall value:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c4604c6c-d216-4a30-93ad-9152c363f5ef.png" style="width:4.50em;height:3.00em;"/></p>
<p>As it has been placed, this is an optimization problem. In general, an optimization problem has two parts:</p>
<ul>
<li>A set of constraints (possibly empty) that must be respected.</li>
<li>An objective function that must be maximized or minimized.</li>
</ul>
<p>The mathematical formalism that we have adopted to define the problem unequivocally clarifies the two parts we've just mentioned. Many real problems can be formulated relatively simply as optimization problems that can then be solved with a calculator. Reducing a new problem to a known problem allows the use of existing solutions.</p>
<p>As with most of the problems, even for optimization problems, there are different approaches to the problem that allow us to reach the solution. Naturally, they differ in the complexity of each algorithm in terms of time and memory requirements and in terms of programming efforts required.</p>
<p>There are two versions of the problem:</p>
<ul>
<li><strong>0-1 knapsack problem</strong>: Each item must be entirely accepted or rejected.</li>
<li><strong>Fractional knapsack problem</strong>: We can take fractions of items.</li>
</ul>
<p>The substantial difference between the two problems lies in the possibility of splitting the items. In the 0-1 knapsack problem, we cannot divide the articles. On the contrary, in the <span>fractional knapsack</span>, we can divide the objects to maximize the total value of the backpack.</p>
<p>The problem of the knapsack that we have introduced can be easily applied to a problem of optimization of a financial portfolio. In fact, it is sufficient to associate the weight of the objects with the weight of the risk of the financial product we are considering and the value of the objects with the expected value of the financial product. Based on these assumptions, it is possible to select financial products that maximize the expected value, keeping the risk below a specific value.</p>
<p>In the following sections, we will address the knapsack problem through three different approaches:</p>
<ul>
<li>Brute force</li>
<li>Greedy algorithms</li>
<li>DP</li>
</ul>
<p>We will try to go into <span>each solution in¬†</span>depth, highlighting their strengths and weaknesses.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Brute force</h1>
                </header>
            
            <article>
                
<p>Brute force lists all the possible values that could represent a solution and checks whether each value satisfies the conditions imposed by the problem.¬†This algorithm is easily implemented and always returns a solution if it exists, but its cost is proportional to the number of possible solutions. Therefore, brute force research is typically used when the size of the problem is limited or when hypotheses are available that allow us to reduce the set of possible solutions. The method is also used when the simplicity of implementation is more important than speed.</p>
<p>To solve the knapsack problem, brute force is the most immediate solution: examine all the possible ways to fill the knapsack, which are <em>2n</em>, and print an optimal solution (there could be more than one). This approach, for <em>n</em> &gt; 15, becomes very slow. This algorithm is usually based directly on the definition of the problem and on the <span><span>understanding¬†</span></span>of the concepts involved.</p>
<p>Here are the essentials of this straightforward algorithm:</p>
<ul>
<li>Enumerates every possible combination.</li>
<li>Choose the best solution (all the combinations are examined, and the one with the maximum value and with a total weight less than or equal to P¬†<span>is returned</span>).</li>
<li>Optimality is ensured.</li>
<li>Extremely costly in time for large <em>n</em>. Running time will be <em>O(2n)</em>.</li>
</ul>
<p>In the following code block is an example code for solving a 0-1 knapsack problem:</p>
<pre>W = 10<br/>WeightArray = c(5,2,4,6)<br/>ValueArray = c(18,9,12,25)<br/>DataKnap&lt;-data.frame(WeightArray,ValueArray)<br/><br/>BestValue = 0    <br/>ItemsSelected = c()<br/>TempWeights&lt;-c()<br/>TempValues&lt;-c()<br/><br/>for(i in 1:4){<br/>  CombWeights&lt;-as.data.frame(combn(DataKnap[,1], i))<br/>  CombValues&lt;-as.data.frame(combn(DataKnap[,2], i))<br/>  SumWeights&lt;-colSums(CombWeights)<br/>  SumValue&lt;-colSums(CombValues)<br/>  TempWeights&lt;-which(SumWeights&lt;=W)<br/>    if(length(TempWeights) != 0){<br/>      TempValues&lt;-SumValue[TempWeights]<br/>      BestValue&lt;-max(TempValues)<br/>      Index&lt;-which((TempValues)==BestValue)<br/>      MaxIndex&lt;-TempWeights[Index]<br/>      MaxVW&lt;-CombWeights[, MaxIndex]<br/>      j=1<br/>      while (j&lt;=i){<br/>        ItemsSelected[j]&lt;-which(DataKnap[,1]==MaxVW[j])<br/>        j=j+1<br/>      }<br/>    }<br/>  }<br/>list(value=round(BestValue),elements=ItemsSelected) </pre>
<p>We will analyze this code line by line. The first lines set the data:</p>
<pre>W = 10<br/>WeightArray = c(5,2,4,6)<br/>ValueArray = c(18,9,12,25)<br/>DataKnap&lt;-data.frame(WeightArray,ValueArray)</pre>
<p>Let's look at each element:</p>
<ul>
<li><kbd>W</kbd> is the maximum weight capacity.</li>
<li><kbd>WeightArray</kbd> is the weights array.</li>
<li><kbd>ValueArray</kbd> is the values array.</li>
<li><kbd>DataKnap</kbd> is a dataframe containing weights and values.</li>
</ul>
<p>Now we will initialize the variables used in the algorithm:</p>
<pre>BestValue = 0    <br/>ItemsSelected = c()<br/>TempWeights&lt;-c()<br/>TempValues&lt;-c()</pre>
<p>As we said, the brute force algorithm systematically lists all the possible values that could represent a solution and check whether each value satisfies the conditions imposed by the problem. Since four objects were passed to the system, a loop with four steps will be set as follows:</p>
<pre>for(i in 1:4){</pre>
<p>For each step of the <kbd>for</kbd> loop, we will calculate all the combinations of the objects taken,¬†<kbd>i</kbd>, at a time without repetition. To do this, the <kbd>combn()</kbd> function was used:</p>
<pre>CombWeights&lt;-as.data.frame(combn(DataKnap[,1], i))<br/>CombValues&lt;-as.data.frame(combn(DataKnap[,2], i))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This function generates all combinations of the elements of the¬†<kbd>DataKnap</kbd> columns taken <kbd>i</kbd> at a time. Now we will sum the¬†<span>returned¬†</span><span>array:</span></p>
<pre>SumWeights&lt;-colSums(CombWeights)<br/>SumValue&lt;-colSums(CombValues)</pre>
<p>Now it is necessary to select only the combinations that return a sum of weights¬†<kbd>&lt;=W</kbd>:</p>
<pre>TempWeights&lt;-which(SumWeights&lt;=W)</pre>
<p>If this operation returns at least one combination, we will calculate the best solution:</p>
<pre>if(length(TempWeights) != 0){<br/>      TempValues&lt;-SumValue[TempWeights]<br/>      BestValue&lt;-max(TempValues)<br/>      Index&lt;-which((TempValues)==BestValue)<br/>      MaxIndex&lt;-TempWeights[Index]<br/>      MaxVW&lt;-CombWeights[, MaxIndex]<br/>      j=1<br/>      while (j&lt;=i){<br/>        ItemsSelected[j]&lt;-which(DataKnap[,1]==MaxVW[j])<br/>        j=j+1<br/>      }<br/>    }<br/>  }</pre>
<p>Finally, the list of the best combinations will be printed:</p>
<pre>list(value=round(BestValue),elements=ItemsSelected) </pre>
<p>Here are the results returned by the 0-1 knapsack problem solution:</p>
<pre>$value<br/><strong>[1] 37</strong><br/><br/>$elements<br/><strong>[1] 3 4</strong></pre>
<p>The results mean that the best solution return values equal to 37 and the objects selected are in 3rd and 4th position. As anticipated, the optimal solution to the problem we have just dealt with is the most immediate but also the most expensive from the computational point of view. In the following sections, we will seek to obtain other solutions, trying save in calculation terms.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Greedy algorithms</h1>
                </header>
            
            <article>
                
<p>Before introducing a greedy algorithm to find an optimal solution to the problem of the knapsack, it is appropriate to recall the main characteristics of any greedy technique. Any greedy technique proceeds iteratively. Starting from an empty solution, element A is added to the partial solution under construction at each iteration. Of all the possible candidates to be added, element A is the most promising one, that is, if chosen, it leads to the greatest improvement of the objective function. It is clear that not all problems can be solved with this strategy; but, only those for which it is possible to show that making the best choice at the moment leads to an optimal solution globally.</p>
<p>Let's look at an algorithm first that simply performs the following operations:</p>
<ol>
<li>Discards all objects weighing more than the maximum capacity (preprocessing).</li>
<li>Sorts the objects for a given criterion.</li>
<li>Selects the objects one at a time until the weight constraint is respected.</li>
<li>Returns the value of the solution and the set of selected objects.</li>
</ol>
<p>In the following code block, we can see the code for executing the algorithm:</p>
<pre>K = 10<br/>w = c(5,2,4,6)<br/>v = c(18,9,12,25)<br/>DataKnap&lt;-data.frame(w,v)<br/><br/>DataKnap$rows_idx &lt;- row(DataKnap)<br/>DataKnap &lt;- DataKnap[DataKnap$w &lt; K,]<br/>DataKnap$VWRatio &lt;- DataKnap$v/DataKnap$w<br/>DescOrder &lt;- order(DataKnap$VWRatio, decreasing = TRUE)<br/>DataKnap &lt;- DataKnap[DescOrder,]<br/><br/>KnapSol &lt;- list(value = 0)<br/>SumWeights &lt;- 0<br/>i &lt;- 1<br/><br/>while (i&lt;=nrow(DataKnap) &amp; SumWeights + DataKnap$w[i]&lt;=K){<br/>    SumWeights &lt;- SumWeights + DataKnap$w[i]<br/>    KnapSol$value &lt;- KnapSol$value + DataKnap$v[i]<br/>    KnapSol$elements[i] &lt;- DataKnap$row[i]<br/>    i &lt;- i + 1<br/>}<br/><br/>print(KnapSol)</pre>
<p>We will analyze this code line by line. The initial lines set the initial data:</p>
<pre>K = 10<br/>w = c(5,2,4,6)<br/>v = c(18,9,12,25)<br/>DataKnap&lt;-data.frame(w,v)</pre>
<p>To find the best solution, we first perform a descending pre-order of the objects based on the density of value, calculated as follows:</p>
<p class="CDPAlignLeft CDPAlign">¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†<img src="assets/72ec34ce-d3bb-4c0e-ad08-d93ac0b700fd.png" style="width:8.17em;height:2.50em;"/></p>
<p>This technique is implemented in the following code:</p>
<pre>DataKnap$rows_idx &lt;- row(DataKnap)<br/>DataKnap &lt;- DataKnap[DataKnap$w &lt; K,]<br/>DataKnap$VWRatio &lt;- DataKnap$v/DataKnap$w<br/>DescOrder &lt;- order(DataKnap$VWRatio, decreasing = TRUE)<br/>DataKnap &lt;- DataKnap[DescOrder,]</pre>
<p>The following lines are used to initialize the variables:</p>
<pre>KnapSol &lt;- list(value = 0)<br/>SumWeights &lt;- 0<br/>i &lt;- 1</pre>
<p>Now, a <kbd>while</kbd> loop will be used to iterate the procedure:</p>
<pre>while (i&lt;=nrow(DataKnap) &amp; SumWeights + DataKnap$w[i]&lt;=K){<br/>    SumWeights &lt;- SumWeights + DataKnap$w[i]<br/>    KnapSol$value &lt;- KnapSol$value + DataKnap$v[i]<br/>    KnapSol$elements[i] &lt;- DataKnap$row[i]<br/>    i &lt;- i + 1<br/>}</pre>
<p>The loop will be repeated until both codes are true. As soon as one of the two is false, the loop will be stopped. The first check is performed on the number of rows of the data matrix, at most there will be as many iterations as there are rows. The second check is carried out on the maximum capacity set. As soon as this is exceeded, the loop will be stopped.</p>
<p>Finally, the results are printed:</p>
<pre>print(KnapSol)</pre>
<p class="mce-root"/>
<p>The results are shown in the following:</p>
<pre>$value<br/><strong>[1] 34</strong><br/><br/>$elements<br/><strong>[1] 2 4</strong></pre>
<p>From the analysis of the previous data, we can note that we have not obtained the optimal solution as in the case of the brute force algorithm, but the procedure is very fast.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing a solution with DP</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we have seen how the knapsack problem can be solved through different approaches. In particular, we have learned to treat this problem with an algorithm called brute force. In this case, we obtained the optimal solution with an extremely heavy computational cost. On the contrary, the greedy solution, seen later, gave us a lighter algorithm from a computational point of view but did not allow us to obtain the optimal solution. A solution that combines both these needs ‚Äì optimal solution and fast algorithm ‚Äì can be provided by DP.</p>
<p>In DP, we subdivide an optimization problem into simpler subproblems and store the solution for each subproblem so that each subproblem is solved only once. The idea behind the method is to calculate solutions to subproblems once and store the solutions in a table so that they can be reused (repeatedly) later.</p>
<p>In the following code block, a knapsack problem solution is implemented using DP:</p>
<pre>v &lt;- c(18,9,12,25)<br/>w &lt;- c(5,2,4,6)<br/>W &lt;- 10<br/><br/>Tabweights&lt;-c(0,w)<br/>TabValues&lt;-c(0,v)<br/>n&lt;-length(w)<br/>TabMatrix&lt;-matrix(NA,nrow =n+1,ncol = W+1)<br/><br/>TabMatrix[,]&lt;-0<br/><br/>for (j in 2:W+1){<br/>  for (i in 2:n+1){<br/>      if (Tabweights[i] &gt; j) {<br/>        TabMatrix[i,j] = TabMatrix[i-1,j]<br/>      } <br/>      else<br/>      {<br/>        TabMatrix[i,j]&lt;-max(TabMatrix[i-1,j], TabValues[i] + TabMatrix[i-1,j-Tabweights[i]])<br/>      }<br/>  }<br/>}<br/><br/>cat("The best value is",TabMatrix[i,j])<br/><br/>i = n+1<br/>w = W+1<br/>ItemSelected = c()<br/>while(i&gt;1 &amp; w&gt;0)<br/>  {<br/>  if(TabMatrix[i,w]!=TabMatrix[i-1,w])<br/>    {<br/>    ItemSelected&lt;-c(ItemSelected,(i)-1)<br/>    w = w - Tabweights[i]<br/>    i = i - 1<br/>   }<br/>  else<br/>  {<br/>    i = i - 1<br/>  }<br/>}<br/><br/>cat("The items selected are",ItemSelected)</pre>
<p>We will analyze this code line by line. This algorithm starts with the definition of the data that will be used in the procedure:</p>
<pre>v &lt;- c(18,9,12,25)<br/>w &lt;- c(5,2,4,6)<br/>W &lt;- 10</pre>
<p>And then define the other variables:</p>
<pre>Tabweights&lt;-c(0,w)<br/>TabValues&lt;-c(0,v)<br/>n&lt;-length(w)<br/>TabMatrix&lt;-matrix(NA,nrow =n+1,ncol = W+1)<br/>TabMatrix[,]&lt;-0</pre>
<p>Let's look at the elements of this code:</p>
<ul>
<li><kbd>Tabweights</kbd>¬†is a vector containing the weights and 0 as the first element.</li>
<li><kbd>TabValues</kbd>¬†is a vector containing the values and 0 as the first element.</li>
<li><kbd>n</kbd>¬†is the number of objects.</li>
<li><kbd>TabMatrix</kbd>¬†is a tabular matrix.</li>
</ul>
<p>We first define and initialize the table that will contain the values. The table is built column by column from the top to the bottom, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-484 image-border" src="assets/ff9520f5-86ed-4b1c-b46e-36f64f08a3cd.png" style="width:18.33em;height:9.58em;"/></p>
<p>Then we set an iterative loop on all objects and on all weight values:¬†¬†</p>
<pre>for (j in 2:W+1){<br/>  for (i in 2:n+1){<br/>      if (Tabweights[i] &gt; j) {<br/>        TabMatrix[i,j] = TabMatrix[i-1,j]<br/>      } <br/>      else<br/>      {<br/>        TabMatrix[i,j]&lt;-max(TabMatrix[i-1,j], TabValues[i] + TabMatrix[i-1,j-Tabweights[i]])<br/>      }<br/>  }<br/>}</pre>
<p>First, we fill the first line <kbd>i=1</kbd> with 0. This means that when no object is considered the weight is 0, so we fill the first column w = 1 with 0. This means that when the weight is 0, the objects considered are 0. Practically, we initialize the first line to 0, which corresponds to the case in which, for different transportable weights, we have no object (T[1, w] = 0). Initialize the first column to 0, which corresponds to the case in which, for several possible objects, I have a backpack of zero capacity (T [i, 1] = 0).</p>
<p>The rule to fill the table is provided by the following algorithm:</p>
<pre>if (Tabweights[i] &gt; j) {<br/>        TabMatrix[i,j] = TabMatrix[i-1,j]<br/>      } <br/>      else<br/>      {<br/>        TabMatrix[i,j]&lt;-max(TabMatrix[i-1,j], TabValues[i] + TabMatrix[i-1,j-Tabweights[i]])</pre>
<p>If the weight of the <span><em>i<sup>th</sup></em>¬†</span>element is greater than that of the column then the <em>i<sup>th</sup></em> element will be equal to the previous one, altitudes will be calculated with the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e0cc1149-7f7d-46fc-a9b1-9202e45e46b4.png" style="width:31.33em;height:1.25em;"/></p>
<p>Once the last cell of the last row of the table has been reached, we can memorize the result obtained, which represents the maximum value of the objects that can be carried in the knapsack:¬†¬†¬†</p>
<pre>cat("The best value is",TabMatrix[i,j])</pre>
<p>The following result is returned:</p>
<pre><strong>The best value is 37</strong></pre>
<p>The procedure followed so far does not indicate which subset provides the optimal solution. We must extract this information by analyzing the last column of the table (w = P); we will run through it from the last value to the top one:</p>
<pre>i = n+1<br/>w = W+1<br/>ItemSelected = c()<br/>while(i&gt;1 &amp; w&gt;0)<br/>  {<br/>  if(TabMatrix[i,w]!=TabMatrix[i-1,w])<br/>    {<br/>    ItemSelected&lt;-c(ItemSelected,(i)-1)<br/>    w = w - Tabweights[i]<br/>    i = i - 1<br/>   }<br/>  else<br/>  {<br/>    i = i - 1<br/>  }<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>If the current element is the same as the previous one, let's move on to the next one; otherwise, the current object is included in the knapsack:¬†¬†¬†</p>
<pre> if(TabMatrix[i,w]!=TabMatrix[i-1,w])<br/>    {<br/>    ItemSelected&lt;-c(ItemSelected,(i)-1)</pre>
<p>If an element is inserted in the backpack, the column is obtained by subtracting the current value of <kbd>w</kbd> from the weight relative to the selected object:¬†¬†¬†¬†¬†¬†¬†</p>
<pre>w = w - Tabweights[i]</pre>
<p>Finally, the items selected will be printed:</p>
<pre>cat("The best value is",TabMatrix[i,j])</pre>
<p>The results are shown in the following:</p>
<pre><strong>The items selected are 4 3</strong></pre>
<p>The DP algorithm allowed us to obtain the optimal solution, saving on computational costs.</p>
<p>In the next section, we will analyze a practical case; we will optimize the navigation system of a robot.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimization of a robot navigation system</h1>
                </header>
            
            <article>
                
<p>A robot is a machine that performs particular actions based on the commands that are provided, either based on direct human supervision, or independently based on general guidelines using the artificial intelligence processes. Robots should be able to replace or assist humans with activities such as, manufacturing, construction, handling heavy and dangerous materials in conditions not suitable for humans, or simply to free a person from commitments.</p>
<p>A robot should be equipped with guided connections by feedback between perception and action, and not by direct human control. The action can take the form of electromagnetic motors, or actuators, that move a limb, open and close a gripper, or move the robot. Step-by-step control and feedback are provided by a program that runs from an external or internal robot computer, or from a microcontroller. Based on this definition, the robot concept can include almost all automated devices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training an agent to move in an environment</h1>
                </header>
            
            <article>
                
<p>To understand how the problems connected with the autonomous navigation of a robot are tackled, we will begin with a broadly-based problem‚Äîthe¬†<kbd>gridworld</kbd> problem. In these problems, the environment is defined as a simple 2D rectangular grid of dimensions (N, M) with an agent starting from a grid square and trying to move to another grid square located elsewhere. This environment is perfect for the application of reinforcement learning algorithms to discover optimal routes and policies for the agents on the grid to reach the desired target grid squares in the fewest number of moves.</p>
<p>The following diagram shows a 5 x 5 grid:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-485 image-border" src="assets/725e3703-65b4-4651-91c5-45826b1d2643.png" style="width:8.50em;height:8.42em;"/></p>
<p>The agent evolves while exploring the states. There were no terminals. The agent can move {right, left, up, and down}. If the action takes the agent off the grid, he remains in the same state, but a negative reward is applied. For all other states (and actions) it is R = -2, except for the actions that move the agent to the finish line. In this case, all four actions get R = +20 and bring the agent to the final state.</p>
<p>To better understand the context, we will only deal with the problem of a 2 x 2 grid with a wall that prohibits the passage from the 1st grid to the 4th, as indicated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-486 image-border" src="assets/53d3833c-7bc8-453c-bd47-49b2a51a1c49.png" style="width:13.25em;height:10.75em;"/></p>
<p>Our goal is to develop the best policy to reach cell C4 (end) starting from cell C1 (start). The following code is an example of solving a gridworld problem:</p>
<pre>library(MDPtoolbox)<br/><br/>UpAct=matrix(c(0.3, 0.7, 0, 0,<br/>            0, 0.9, 0.1, 0,<br/>            0, 0.1, 0.9, 0,<br/>            0, 0, 0.7, 0.3),<br/>          nrow=4,ncol=4,byrow=TRUE)<br/><br/>DownAct=matrix(c( 1, 0, 0, 0,<br/>               0.7, 0.2, 0.1, 0,<br/>               0, 0.1, 0.2, 0.7,<br/>               0, 0, 0, 1),<br/>            nrow=4,ncol=4,byrow=TRUE)<br/><br/>LeftAct=matrix(c( 0.9, 0.1, 0, 0,<br/>               0.1, 0.9, 0, 0,<br/>               0, 0.7, 0.2, 0.1,<br/>               0, 0, 0.1, 0.9),<br/>            nrow=4,ncol=4,byrow=TRUE)<br/><br/>RightAct=matrix(c( 0.9, 0.1, 0, 0,<br/>                0.1, 0.2, 0.7, 0,<br/>                0, 0, 0.9, 0.1,<br/>                0, 0, 0.1, 0.9),<br/>             nrow=4,ncol=4,byrow=TRUE)<br/><br/>AllActions=list(up=UpAct, down=DownAct, left=LeftAct, right=RightAct)<br/><br/>AllRewards=matrix(c( -2, -2, -2, -2,<br/>                  -2, -2, -2, -2,<br/>                  -2, -2, -2, -2,<br/>                  20, 20, 20, 20),<br/>               nrow=4,ncol=4,byrow=TRUE)<br/><br/>mdp_check(AllActions, AllRewards)<br/>GridModel=mdp_policy_iteration(P=AllActions, R=AllRewards, discount = 0.1)<br/>GridModel$policy<br/>names(AllActions)[GridModel$policy]<br/>GridModel$V<br/>GridModel$iter<br/>GridModel$time</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will analyze this code line by line. First, we loaded the library:</p>
<pre>library(MDPtoolbox)</pre>
<p>Next, we set all possible actions:</p>
<pre>UpAct=matrix(c(0.3, 0.7, 0, 0,<br/> 0, 0.9, 0.1, 0,<br/> 0, 0.1, 0.9, 0,<br/> 0, 0, 0.7, 0.3),<br/> nrow=4,ncol=4,byrow=TRUE)<br/><br/>DownAct=matrix(c( 1, 0, 0, 0,<br/> 0.7, 0.2, 0.1, 0,<br/> 0, 0.1, 0.2, 0.7,<br/> 0, 0, 0, 1),<br/> nrow=4,ncol=4,byrow=TRUE)<br/><br/>LeftAct=matrix(c( 0.9, 0.1, 0, 0,<br/> 0.1, 0.9, 0, 0,<br/> 0, 0.7, 0.2, 0.1,<br/> 0, 0, 0.1, 0.9),<br/> nrow=4,ncol=4,byrow=TRUE)<br/><br/>RightAct=matrix(c( 0.9, 0.1, 0, 0,<br/> 0.1, 0.2, 0.7, 0,<br/> 0, 0, 0.9, 0.1,<br/> 0, 0, 0.1, 0.9),<br/> nrow=4,ncol=4,byrow=TRUE)</pre>
<p>Each action matrix is of the 4 x 4 type. In fact, it contains all the probabilities from each state (represented by the lines) when it is possible to transit to another state (represented by the columns). For example, the first row of the MoveUp matrix contains all the probabilities that start from the state C1 it is possible to pass in the other states with the action up. It is clear that in this state, with this action, I can transit in the state C2. In fact, the relative probability is 0.7. The second row of the same matrix contains the probabilities from the state C2 passes to the other states with the action up, in which case the highest probability is that it remains in this state.</p>
<p>In the next step, we will combine all actions defined in a list:</p>
<pre>AllActions=list(up=UpAct, down=DownAct, left=LeftAct, right=RightAct)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Let's define the rewards and penalties allowed for the problem according to the hypothesis:¬†</p>
<pre>AllRewards=matrix(c( -2, -2, -2, -2,<br/>                  -2, -2, -2, -2,<br/>                  -2, -2, -2, -2,<br/>                  20, 20, 20, 20),<br/>               nrow=4,ncol=4,byrow=TRUE)</pre>
<p>Before we move on, we have to check the format of the problem as we have defined. We will use the <kbd>mdp_check()</kbd> function:</p>
<pre>mdp_check(AllActions, AllRewards)</pre>
<p>This function checks whether the MDP defined by the transition probability array (<kbd>AllActions</kbd>) and the reward matrix (<kbd>AllRewards</kbd>) is valid. If <kbd>AllActions</kbd> and <kbd>AllRewards</kbd> are correct, the function returns an empty error message. In the opposite case, the function returns an error message describing the problem. Let's search the optimal policy for the navigation from C1 to C4:</p>
<pre>GridModel=mdp_policy_iteration(P=AllActions, R=AllRewards, discount = 0.1)</pre>
<p>The¬†<kbd>mdp_policy_iteration()</kbd> function was used. This function applies the policy iteration algorithm to solve discounted MDP. The algorithm consists in improving the policy iteratively, using the evaluation of the current policy. Iterating is stopped when two successive policies are identical or when a specified number of iterations have been performed. Three arguments were passed:</p>
<ul>
<li><kbd>AllActions</kbd>: Transition probability array. This array can be a three-dimensional array, or a list, each element containing a sparse matrix.</li>
<li><kbd>AllRewards</kbd>: Reward array. This array can be a three-dimensional array or a list, each element containing a sparse matrix or a two-dimensional matrix possibly sparse.</li>
<li><kbd>discount</kbd>: Discount factor. Discount is a real number between [0; 1].</li>
</ul>
<p>At this point, we can recover the policy:</p>
<pre>GridModel$policy</pre>
<p>The following results are returned:</p>
<pre><strong>1 4 2 2</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To better understand the policy, we can extract the names of the actions defined with the policy:</p>
<pre>names(AllActions)[GridModel$policy]</pre>
<p>The following results are returned:</p>
<pre><strong>"up"    "right" "down"  "down"</strong></pre>
<p>Now we can extract the optimal values at each step. These values can be different in each run:</p>
<pre>GridModel$V</pre>
<p>The following results are returned:</p>
<pre><strong>-2.213209 -2.097323 -0.474916 22.222222</strong></pre>
<p>We talked about iterations. In fact, we can see in how many iterations the algorithm has taken to reach convergence:</p>
<pre>GridModel$iter</pre>
<p>The following results are returned:</p>
<pre><strong>3</strong></pre>
<p>Finally, we print the time of execution:</p>
<pre>GridModel$time</pre>
<p>The following result is returned:</p>
<pre><strong>Time difference of 0.377022 secs</strong></pre>
<p>The problem faced is trivial but allows us to understand how it must be addressed. A larger grid must be treated in the same way unless it defines the matrices of the actions and rewards that will be the greatest.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have addressed the basics concepts of the optimization techniques. To begin with, we learned the essential elements underlying DP. In DP, we subdivide an optimization problem into simpler subproblems: we proceed to calculate the solutions of all possible subproblems, and from these sub-solutions we obtain new sub-solutions, and then solve the original problem.</p>
<p>Then we learned the difference between recursion and memoization.¬†Subsequently, we learned the basis of the knapsack problem. This problem was addressed through three different approaches: brute force, greedy algorithms, and DP. For each approach, a solution algorithm was provided and the results were compared.</p>
<p>Finally, the optimization of a navigation route was discussed. To handle autonomous navigation of a robot, we learned to tackle the problem of searching for a path in a gridworld. In this way, we have seen how to solve the problem of finding the optimal policy to find the route.</p>
<p>In the next chapter, we will learn the basic concepts of forecasting techniques.¬†</p>


            </article>

            
        </section>
    </body></html>