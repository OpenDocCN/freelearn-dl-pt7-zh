<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;String Comparison and Clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. String Comparison and Clustering</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Distance and proximity – simple edit distance</li><li class="listitem" style="list-style-type: disc">Weighted edit distance</li><li class="listitem" style="list-style-type: disc">The Jaccard distance</li><li class="listitem" style="list-style-type: disc">The Tf-Idf distance</li><li class="listitem" style="list-style-type: disc">Using edit distance and language models for spelling correction</li><li class="listitem" style="list-style-type: disc">The case restoring corrector</li><li class="listitem" style="list-style-type: disc">Automatic phrase completion</li><li class="listitem" style="list-style-type: disc">Single-link and complete-link clustering using edit distance</li><li class="listitem" style="list-style-type: disc">Latent Dirichlet allocation (LDA) for multitopic clustering</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec69"/>Introduction</h1></div></div></div><p>This chapter starts off by comparing strings using standard language neutral techniques. Then, we will use these techniques to build some commonly used applications. We will also look at clustering techniques based on distances between strings.</p><p>For a string, we use the canonical definition that a string is a sequence of characters. So, clearly, these techniques apply to words, phrases, sentences, paragraphs, and so on, all of which you have learnt to extract in the previous chapters.</p></div></div>
<div class="section" title="Distance and proximity &#x2013; simple edit distance"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec70"/>Distance and proximity – simple edit distance</h1></div></div></div><p>String comparison<a class="indexterm" id="id519"/> refers to techniques used to measure the similarity between two strings. We will use distance and proximity to specify how similar any two strings are. The more similar any two strings are, the lesser the distance between them, so the distance from a string to itself is 0. An inverse measure is proximity, which means that the more similar any two strings are, the greater their proximity.</p><p>We will take a look at simple edit distance first. Simple edit distance <a class="indexterm" id="id520"/>measures distance in terms of how many edits are required to convert one string to the other. A common distance measure proposed by Levenshtien in 1965 allows deletion, insertion, and substitution as basic operations. Adding in transposition is called <a class="indexterm" id="id521"/>Damerau-Levenshtien distance. For example, the distance between <code class="literal">foo</code> and <code class="literal">boo</code> is 1, as we're looking at a substitution of <code class="literal">f</code> with <code class="literal">b</code>.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>For more on <a class="indexterm" id="id522"/>distance metrics, refer to the Wikipedia article on distance at <a class="ulink" href="http://en.wikipedia.org/wiki/Distance">http://en.wikipedia.org/wiki/Distance</a>.</p></div></div><p>Let's look at some more examples of editable operations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Deletion</strong></span>: <code class="literal">Bart</code> <a class="indexterm" id="id523"/>and <code class="literal">Bar</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Insertion</strong></span>: <code class="literal">Bar</code> <a class="indexterm" id="id524"/>and <code class="literal">Bart</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Substitution</strong></span>: <code class="literal">Bar</code> <a class="indexterm" id="id525"/>and <code class="literal">Car</code></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Transposition</strong></span>: <code class="literal">Bart</code> <a class="indexterm" id="id526"/>and <code class="literal">Brat</code></li></ul></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec159"/>How to do it...</h2></div></div></div><p>Now, we will run a <a class="indexterm" id="id527"/>simple example on edit distance:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Run the <code class="literal">SimpleEditDistance</code> class using the command line or your IDE:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java –cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.SimpleEditDistance</strong></span>
</pre></div></li><li class="listitem">In the command prompt, you will be prompted for two strings:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Enter the first string:</strong></span>
<span class="strong"><strong>ab</strong></span>
<span class="strong"><strong>Enter the second string:</strong></span>
<span class="strong"><strong>ba</strong></span>
<span class="strong"><strong>Allowing Transposition Distance between: ab and ba is 1.0</strong></span>
<span class="strong"><strong>No Transposition Distance between: ab and ba is 2.0</strong></span>
</pre></div></li><li class="listitem">You will see the distance between the two strings with transposition allowed and with transposition not allowed.</li><li class="listitem">Play with some more examples to get a sense of how it works—try them first by hand and then verify that you got the optimal case.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec160"/>How it works...</h2></div></div></div><p>This is a very simple <a class="indexterm" id="id528"/>piece of code, and all it does is create two instances of the <code class="literal">EditDistance</code> class: one that allows transpositions, and the other that does not allow transpositions:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException {
  
  EditDistance dmAllowTrans = new EditDistance(true);
  EditDistance dmNoTrans = new EditDistance(false);</pre></div><p>The remaining code will set up an I/O routing, apply the edit distances, and print them out:</p><div class="informalexample"><pre class="programlisting">BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
while (true) {
  System.out.println("Enter the first string:");
  String text1 = reader.readLine();
  System.out.println("Enter the second string:");
  String text2 = reader.readLine();
  double allowTransDist = dmAllowTrans.distance(text1, text2);
  double noTransDist = dmNoTrans.distance(text1, text2);
  System.out.println("Allowing Transposition Distance " +" between: " + text1 + " and " + text2 + " is " + allowTransDist);
  System.out.println("No Transposition Distance between: " + text1 + " and " + text2 + " is " + noTransDist);
}
}</pre></div><p>If we wanted the proximity instead of distance, we would just use the <code class="literal">proximity</code> method instead of the <code class="literal">distance</code> method.</p><p>In simple <code class="literal">EditDistance</code>, all the editable operations have a fixed cost of 1.0, that is, each editable operation (deletion, substitution, insertion, and, if allowed, transposition) is counted with a cost of 1.0 each. So, in the example where we find the distance between <code class="literal">ab</code> and <code class="literal">ba</code>, there is one deletion and one insertion, both of which have a cost of 1.0. Therefore, this makes the distance between <code class="literal">ab</code> and <code class="literal">ba</code> 2.0 if transposition is not allowed and 1.0 if it is. Note that typically, there will be more than one way to edit one string into the other.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>While <code class="literal">EditDistance</code> is quite simple to use, it is not simple to implement. This is what the Javadoc has to say about this class:</p><p>
<span class="emphasis"><em>Implementation note: This class implements</em></span><a class="indexterm" id="id529"/>
<span class="emphasis"><em> edit distance using dynamic programming in time O(n * m) where n and m are the length of the sequences being compared. Using a sliding window of three lattice slices rather than allocating the entire lattice at once, the space required is that for three arrays of integers as long as the shorter of the two character sequences being compared.</em></span>
</p></div></div><p>In the following sections, we will see how to assign varying costs to each edit operation.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec161"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Refer to the LingPipe Javadoc on <code class="literal">EditDistance</code> <a class="indexterm" id="id530"/>at <a class="ulink" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html">http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/EditDistance.html</a> for more details</li><li class="listitem" style="list-style-type: disc">For more details on<a class="indexterm" id="id531"/> distance, refer to the Javadoc at <a class="ulink" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html">http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Distance.html</a></li><li class="listitem" style="list-style-type: disc">For more details on <a class="indexterm" id="id532"/>proximity, refer to the Javadoc at <a class="ulink" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html">http://alias-i.com/lingpipe/docs/api/com/aliasi/util/Proximity.html</a></li></ul></div></div></div>
<div class="section" title="Weighted edit distance"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec71"/>Weighted edit distance</h1></div></div></div><p>Weighted edit distance<a class="indexterm" id="id533"/> is essentially a simple edit distance, except that the edits allow different costs to be associated with each kind of edit operation. The edit operations we identified in the previous recipe are substitution, insertion, deletion, and transposition. Additionally, there can be a cost associated with the exact matches to increase the weight for matching – this might be used when edits are required, such as a string-variation generator. Edit weights are generally scaled as log probabilities so that you can assign likelihood to an edit operation. The larger the weight, the more likely that edit operation is. As probabilities are between 0 and 1, log probabilities, or weights, will be between negative infinity and zero. For more on this refer to the Javadoc on the <code class="literal">WeightedEditDistance</code> class<a class="indexterm" id="id534"/> at <a class="ulink" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html">http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html</a>.</p><p>On the log scale, weighted edit distance can be generalized to produce exactly the same results as simple edit distance did in the previous recipe by setting the match weight to 0 and substituting, deleting, and inserting weights to -1 and transposition weights to either -1 or negative infinity, if we want to turn transposition off.</p><p>We will look at weighted edit distance for spell checking and Chinese word segmentation in other recipes.</p><p>In this section, we will use the <code class="literal">FixedWeightEditDistance</code> instance and create the <code class="literal">CustomWeightEditDistance</code> class that extends the <code class="literal">WeightedEditDistance</code> abstract class. The <code class="literal">FixedWeightEditDistance</code> class is initialized with weights for each edit operation. The <code class="literal">CustomWeightEditDistance</code> class extends <code class="literal">WeightedEditDistance</code> and has rules for each edit operation weights. The weight for deleting alphanumeric characters is -1, and for all other characters, that is, punctuation and spaces, it is is 0. We will set insertion weights to be the same as deletion weights.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec162"/>How to do it...</h2></div></div></div><p>Let's expand on our <a class="indexterm" id="id535"/>previous example and look at a version that runs the simple edit distance as well as our weighted edit distance:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In your IDE run the <code class="literal">SimpleWeightedEditDistance class, </code>or in the command line, type:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.SimpleWeightedEditDistance</strong></span>
</pre></div></li><li class="listitem">In the command line, you will be prompted for two strings: enter the examples shown here or choose your own:<div class="mediaobject"><img alt="How to do it..." src="graphics/4672OS_06_01.jpg"/></div></li><li class="listitem">As you can see, there<a class="indexterm" id="id536"/> are two other distance measures being shown here: a fixed weight edit distance and a custom weight edit distance.</li><li class="listitem">Play around with other examples, including punctuation and spaces.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec163"/>How it works...</h2></div></div></div><p>We will instantiate a <code class="literal">FixedWeightEditDistance</code> class with some weights that are, arbitrarily chosen:</p><div class="informalexample"><pre class="programlisting">double matchWeight = 0;
double deleteWeight = -2;
double insertWeight = -2;
double substituteWeight = -2;
double transposeWeight = Double.NEGATIVE_INFINITY;
WeightedEditDistance wed = new FixedWeightEditDistance(matchWeight,deleteWeight,insertWeight,substituteWeight,transposeWeight);
System.out.println("Fixed Weight Edit Distance: "+ wed.toString());</pre></div><p>In this example, we set the delete, substitute, and insert weights to be equal. This is very similar to the standard edit distance, except that we modified the weights associated with the edit operations from 1 to 2. Setting the transpose weight to negative infinity effectively turns off transpositions completely. Obviously, it's not necessary that the delete, substitute, and insert weights should be equal.</p><p>We will also create a <code class="literal">CustomWeightEditDistance</code> class, which treats punctuations and whitespaces as matches, that is, zero cost for the insert and delete operations (for letters or digits, the cost remains -1). For substitutions, if the character is different only in case, the cost is zero; for all other cases, the cost is -1. We will also turn off transposition by setting its cost to negative <a class="indexterm" id="id537"/>infinity. This will result in <code class="literal">Abc+</code> matching <code class="literal">abc-</code>:</p><div class="informalexample"><pre class="programlisting">public static class CustomWeightedEditDistance extends WeightedEditDistance{

  @Override
  public double deleteWeight(char arg0) {
    return (Character.isDigit(arg0)||Character.isLetter(arg0)) ? -1 : 0;
    
  }

  @Override
  public double insertWeight(char arg0) {
    return deleteWeight(arg0);
  }

  @Override
  public double matchWeight(char arg0) {
    return 0;
  }

  @Override
  public double substituteWeight(char cDeleted, char cInserted) {
    return Character.toLowerCase(cDeleted) == Character.toLowerCase(cInserted) ? 0 :-1;
    
  }

  @Override
  public double transposeWeight(char arg0, char arg1) {
    return Double.NEGATIVE_INFINITY;
  }

}</pre></div><p>This sort of custom weighted edit distance is particularly useful in comparing strings where minor formatting <a class="indexterm" id="id538"/>changes are encountered, such as gene/protein names that vary from <code class="literal">Serpin A3</code> to <code class="literal">serpina3</code> but refer to the same thing.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec164"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There is a T&amp;T (Tsuruoka and Tsujii) specification<a class="indexterm" id="id539"/> for edit distance to compare protein names, refer to <a class="ulink" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE">http://alias-i.com/lingpipe/docs/api/com/aliasi/dict/ApproxDictionaryChunker.html#TT_DISTANCE</a></li><li class="listitem" style="list-style-type: disc">More details on the <code class="literal">WeightedEditDistance</code> class<a class="indexterm" id="id540"/> can be found on the Javadoc page at <a class="ulink" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html">http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/WeightedEditDistance.html</a></li></ul></div></div></div>
<div class="section" title="The Jaccard distance"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec72"/>The Jaccard distance</h1></div></div></div><p>The Jaccard distance<a class="indexterm" id="id541"/> is a very popular and efficient way of comparing strings. The Jaccard distance operates at a token level and compares two strings by first tokenizing them and then dividing the number of common tokens by the total number of tokens. In the <span class="emphasis"><em>Eliminate near duplicates with the Jaccard distance</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, we applied the distance to eliminate near-duplicate tweets. This recipe will go into a bit more detail and show you how it is computed.</p><p>A distance of 0 is a perfect match, that is, the strings share all their terms, and a distance of 1 is a perfect mismatch, that is, the strings have no terms in common. Remember that proximity and distance are additive inverses, so proximity also ranges from 1 to 0. Proximity of 1 is a perfect match, and proximity of 0 is a perfect mismatch:</p><div class="informalexample"><pre class="programlisting">proximity  = count(common tokens)/count(total tokens)
distance = 1 – proximity</pre></div><p>The tokens are generated by <code class="literal">TokenizerFactory</code>, which is passed in during construction. For example, let's use <code class="literal">IndoEuropeanTokenizerFactory</code> and take a look at a concrete example. If <code class="literal">string1</code> is <code class="literal">fruit flies like a banana</code> and <code class="literal">string2</code> is <code class="literal">time flies like an arrow</code>, then the token set for <code class="literal">string1</code> would be <code class="literal">{'fruit', 'flies', 'like', 'a', 'banana'}</code>, and the token set for <code class="literal">string2</code> would be <code class="literal">{'time', 'flies', 'like', 'an', 'arrow'}</code>. The common terms (or the intersection) between these two token sets are <code class="literal">{'flies', 'like'}</code>, and the union of these terms is <code class="literal">{'fruit',' flies', 'like', 'a', 'banana', 'time', 'an', 'arrow'}</code>. Now, we can calculate the Jaccard proximity by dividing the number of common terms by the total number of terms, that is, 2/8, which equals 0.25. Thus, the distance is 0.75 (1 - 0.25).Obviously, the Jaccard distance is eminently tunable by modifying the tokenizer that the class is initialized with. For example, one could use a case-normalizing tokenizer so that <code class="literal">Abc</code> and <code class="literal">abc</code> would be considered equivalent. Similarly, a stemming tokenizer would consider the words <code class="literal">runs</code> and <code class="literal">run</code> to be equivalent. We will see a similar ability in the next distance metric, the Tf-Idf distance, as well.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec165"/>How to do it...</h2></div></div></div><p>Here's how to run the<a class="indexterm" id="id542"/> <code class="literal">JaccardDistance</code> example:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In Eclipse, run the <code class="literal">JaccardDistanceSample</code> class, or in the command line, type:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.JaccardDistanceSample</strong></span>
</pre></div></li><li class="listitem">As in the previous recipes, you will be prompted for two strings. The first string that we will use is <code class="literal">Mimsey Were the Borogroves</code>, which is an excellent sci-fi short-story title, and the second string <code class="literal">All mimsy were the borogoves,</code> is the actual line from <span class="emphasis"><em>Jabberwocky</em></span> that inspired it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Enter the first string:</strong></span>
<span class="strong"><strong>Mimsey Were the Borogroves</strong></span>
<span class="strong"><strong>Enter the second string:</strong></span>
<span class="strong"><strong>All mimsy were the borogoves,</strong></span>

<span class="strong"><strong>IndoEuropean Tokenizer</strong></span>
<span class="strong"><strong>Text1 Tokens: {'Mimsey''Were''the'}</strong></span>
<span class="strong"><strong>Text2 Tokens: {'All''mimsy''were''the''borogoves'}</strong></span>
<span class="strong"><strong>IndoEuropean Jaccard Distance is 0.8888888888888888</strong></span>

<span class="strong"><strong>Character Tokenizer</strong></span>
<span class="strong"><strong>Text1 Tokens: {'M''i''m''s''e''y''W''e''r''e''t''h''e''B''o''r''o''g''r''o''v''e'}</strong></span>
<span class="strong"><strong>Text2 Tokens: {'A''l''l''m''i''m''s''y''w''e''r''e''t''h''e''b''o''r''o''g''o''v''e''s'}</strong></span>
<span class="strong"><strong>Character Jaccard Distance between is 0.42105263157894735</strong></span>

<span class="strong"><strong>EnglishStopWord Tokenizer</strong></span>
<span class="strong"><strong>Text1 Tokens: {'Mimsey''Were'}</strong></span>
<span class="strong"><strong>Text2 Tokens: {'All''mimsy''borogoves'}</strong></span>
<span class="strong"><strong>English Stopword Jaccard Distance between is 1.0</strong></span>
</pre></div></li><li class="listitem">The output contains the tokens and the distances using three different tokenizers. The <code class="literal">IndoEuropean</code> and <code class="literal">EnglishStopWord</code> tokenizers are pretty close and show that these two lines are far apart. Remember that the closer two strings are, the lesser is the distance between them. The character tokenizer, however, shows that these lines are closer to each other with characters as the basis of comparison. Tokenizers can make a big difference in calculating the distance between strings.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec166"/>How it works...</h2></div></div></div><p>The code is <a class="indexterm" id="id543"/>straightforward, and we will just cover the creation of the <code class="literal">JaccardDistance</code> objects. We will start with three tokenizer factories:</p><div class="informalexample"><pre class="programlisting">TokenizerFactory indoEuropeanTf = IndoEuropeanTokenizerFactory.INSTANCE;

TokenizerFactory characterTf = CharacterTokenizerFactory.INSTANCE;

TokenizerFactory englishStopWordTf = new EnglishStopTokenizerFactory(indoEuropeanTf);</pre></div><p>Note that <code class="literal">englishStopWordTf</code> uses a base tokenizer factory to construct itself. Refer to <a class="link" href="ch02.html" title="Chapter 2. Finding and Working with Words">Chapter 2</a>, <span class="emphasis"><em>Finding and Working with Words</em></span>, if there are questions on what is going on here.</p><p>Next, the Jaccard distance classes are constructed, given a tokenizer factory as an argument:</p><div class="informalexample"><pre class="programlisting">JaccardDistance jaccardIndoEuropean = new JaccardDistance(indoEuropeanTf);
JaccardDistance jaccardCharacter = new JaccardDistance(characterTf);

JaccardDistance jaccardEnglishStopWord = new JaccardDistance(englishStopWordTf);</pre></div><p>The rest of the code is just our standard I/O loop and some print statements. That's it! On to more sophisticated measures of string distance.</p></div></div>
<div class="section" title="The Tf-Idf distance"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec73"/>The Tf-Idf distance</h1></div></div></div><p>A very useful distance metric between strings is provided by the <code class="literal">TfIdfDistance</code> class. It is, in fact, closely related to the distance metric from the popular open source search engine, Lucene/SOLR/Elastic Search, where the strings being compared are the query against documents in the index. Tf-Idf stands for the core formula that is <span class="strong"><strong>term frequency</strong></span> (<span class="strong"><strong>TF</strong></span>)<a class="indexterm" id="id544"/> times <a class="indexterm" id="id545"/>
<span class="strong"><strong>inverse document frequency</strong></span> (<span class="strong"><strong>IDF</strong></span>) for terms shared by the query and the document. A very cool thing about this approach is that common terms (for example, <code class="literal">the</code>) that are very frequent in documents are downweighted, while rare terms are upweighted in the distance comparison. This can help focus the distance on terms that are actually discriminating in the document collection.</p><p>Not only does <code class="literal">TfIdfDistance</code> come in handy for search-engine-like applications, it can be very useful for clustering and for any problem that calls for document similarity without supervised training data. It has a desirable property; scores are normalized to a score between 0 and 1, and for a fixed document <code class="literal">d1</code> and varying length documents <code class="literal">d2</code>, do not overwhelm the assigned score. In our experience, the scores for different pairs of documents are fairly robust if you were trying to rank the quality of match for a pair of documents.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>Note that there are a range of different distances called <a class="indexterm" id="id546"/>Tf-Idf distances. The one in this class is defined to be symmetric, unlike typical Tf-Idf distances that are defined for information-retrieval purposes.</p></div></div><p>There is a lot of information in the Javadoc that is well worth a good look. However, for the purposes of these recipes, all you need to know is that the Tf-Idf distance is useful for finding similar documents on a word-by-word basis.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec167"/>How to do it...</h2></div></div></div><p>In the quest to keep things a little interesting, we will use our <code class="literal">TfIdfDistance</code> class to build a really simple search engine over tweets. We will perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">If you have not done it already, run the <code class="literal">TwitterSearch</code> class from <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, and get some tweets to play with, or go with our provided data. We will use the tweets found by running the <code class="literal">Disney World</code> query, and they are already in the <code class="literal">data</code> directory.</li><li class="listitem">Type the following in the command line—this uses our defaults:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter6.TfIdfSearch</strong></span>
<span class="strong"><strong>Reading search index from data/disney.csv</strong></span>
<span class="strong"><strong>Getting IDF data from data/connecticut_yankee_king_arthur.txt</strong></span>
<span class="strong"><strong>enter a query:</strong></span>
</pre></div></li><li class="listitem">Enter a query that has some likely word matches:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>I want to go to disney world</strong></span>
<span class="strong"><strong>0.86 : I want to go to Disneyworld</strong></span>
<span class="strong"><strong>0.86 : I want to go to disneyworld</strong></span>
<span class="strong"><strong>0.75 : I just want to go to DisneyWorld...</strong></span>
<span class="strong"><strong>0.75 : I just want to go to Disneyworld ???</strong></span>
<span class="strong"><strong>0.65 : Cause I wanna go to Disneyworld.</strong></span>
<span class="strong"><strong>0.56 : I wanna go to disneyworld with Demi</strong></span>
<span class="strong"><strong>0.50 : I wanna go back to disneyworld</strong></span>
<span class="strong"><strong>0.50 : I so want to go to Disneyland I've never been. I've been to Disneyworld in Florida.</strong></span>
<span class="strong"><strong>0.47 : I want to go to #DisneyWorld again... It's so magical!!</strong></span>
<span class="strong"><strong>0.45 : I want to go to DisneyWorld.. Never been there :( #jadedchildhood</strong></span>
</pre></div></li><li class="listitem">That's it. Try different queries and play around with the scores. Then, have a look at the source.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec168"/>How it works...</h2></div></div></div><p>This code is a very <a class="indexterm" id="id547"/>simple way to build a search engine rather than a good way to build one. However, it is a decent way to explore how the concept of string distance works in the search context. Later in the book, we will perform clustering based on the same distance metric. Start with the <code class="literal">main()</code> class in <code class="literal">src/com/lingpipe/cookbook/chapter6/TfIdfSearch.java</code>:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException {
  String searchableDocs = args.length &gt; 0 ? args[0] : "data/disneyWorld.csv";
  System.out.println("Reading search index from " + searchableDocs);

  String idfFile = args.length &gt; 1 ? args[1] : "data/connecticut_yankee_king_arthur.txt";
  System.out.println("Getting IDF data from " + idfFile);</pre></div><p>This program can take command-line-supplied files for the searched data in the <code class="literal">.csv</code> format and a text file for use as the source of training data. Next, we will set up a tokenizer factory and <code class="literal">TfIdfDistance</code>. If you are not familiar with tokenizer factories, then refer to the <span class="emphasis"><em>Modifying tokenizer factories</em></span> recipe in <a class="link" href="ch02.html" title="Chapter 2. Finding and Working with Words">Chapter 2</a>, <span class="emphasis"><em>Modifying Tokenizer Factories</em></span>, for an explanation:</p><div class="informalexample"><pre class="programlisting">TokenizerFactory tokFact = IndoEuropeanTokenizerFactory.INSTANCE;
TfIdfDistance tfIdfDist = new TfIdfDistance(tokFact);</pre></div><p>Then, we will get the <a class="indexterm" id="id548"/>data that will be the IDF component by splitting the training text on ".", which approximates sentence detection—we could have done a proper sentence detection like we did in the <span class="emphasis"><em>Sentence detection</em></span> recipe in <a class="link" href="ch05.html" title="Chapter 5. Finding Spans in Text – Chunking">Chapter 5</a>, <span class="emphasis"><em>Finding Spans in Text – Chunking</em></span>, but we chose to keep the example as simple as possible:</p><div class="informalexample"><pre class="programlisting">String training = Files.readFromFile(new File(idfFile), Strings.UTF8);
for (String line: training.split("\\.")) {
  tfIdfDist.handle(line);
}</pre></div><p>Inside the <code class="literal">for</code> loop, there is <code class="literal">handle()</code>, which trains the class with knowledge of the token distributions in the corpus, with sentences being the document. It often happens that the concept of document is either smaller (sentence, paragraph, and word) or larger than what is typically termed <code class="literal">document</code>. In this case, the document frequency will be the number of sentences the token is in.</p><p>Next, the documents that we are searching are loaded:</p><div class="informalexample"><pre class="programlisting">List&lt;String[]&gt; docsToSearch = Util.readCsvRemoveHeader(new File(searchableDocs));</pre></div><p>The console is set up to read in the query:</p><div class="informalexample"><pre class="programlisting">BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
while (true) {
  System.out.println("enter a query: ");
  String query = reader.readLine();</pre></div><p>Next, each document is scored against the query with <code class="literal">TfIdfDistance</code> and put into <code class="literal">ObjectToDoubleMap</code>, which keeps track of the proximity:</p><div class="informalexample"><pre class="programlisting">ObjectToDoubleMap&lt;String&gt; scoredMatches = new ObjectToDoubleMap&lt;String&gt;();
for (String [] line : docsToSearch) {
  scoredMatches.put(line[Util.TEXT_OFFSET], tfIdfDist.proximity(line[Util.TEXT_OFFSET], query));
}</pre></div><p>Finally, <code class="literal">scoredMatches</code> is retrieved in the proximity order, and the first 10 examples are printed out:</p><div class="informalexample"><pre class="programlisting">List&lt;String&gt; rankedDocs = scoredMatches.keysOrderedByValueList();
for (int i = 0; i &lt; 10; ++i) {
  System.out.printf("%.2f : ", scoredMatches.get(rankedDocs.get(i)));
  System.out.println(rankedDocs.get(i));
}
}</pre></div><p>While this approach is <a class="indexterm" id="id549"/>very inefficient, in that, each query iterates over all the training data, does an explicit <code class="literal">TfIdfDistance</code> comparison, and stores it, it is not a bad way to play around with small datasets and comparison metrics.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec169"/>There's more...</h2></div></div></div><p>There are some subtleties worth highlighting about <code class="literal">TfIdfDistance</code>.</p><div class="section" title="Difference between supervised and unsupervised trainings"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec19"/>Difference between supervised and unsupervised trainings</h3></div></div></div><p>When we train <code class="literal">TfIdfDistance</code>, there are <a class="indexterm" id="id550"/>some important differences<a class="indexterm" id="id551"/> in the use of training from the ones used in the rest of the book. The training done here is unsupervised, which means that no human or other external source has marked up the data for the expected outcome. Most of the recipes in this book that train use human annotated, or supervised, data.</p></div><div class="section" title="Training on test data is OK"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec20"/>Training on test data is OK</h3></div></div></div><p>As this is unsupervised data, there is no requirement that the training data should be be distinct from the evaluation or production data.</p></div></div></div>
<div class="section" title="Using edit distance and language models for spelling correction"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec74"/>Using edit distance and language models for spelling correction</h1></div></div></div><p>Spelling correction<a class="indexterm" id="id552"/> takes a user input text and provides a corrected form. Most of us are familiar with automatic spelling correction via our smart phones or editors such as Microsoft Word. There are obviously quite a few amusing examples of these on the Web where the spelling correction fails. In this example, we'll build our own spelling-correction engine and look at how to tune it.</p><p>LingPipe's spelling correction is based on a noisy-channel model which models user mistakes and expected user input (based on the data). Expected user input is modeled by a character-language model, and mistakes (or noise) is modeled by weighted edit distance. The spelling correction is done using the <code class="literal">CompiledSpellChecker</code> class. This class implements the<a class="indexterm" id="id553"/> noisy-channel model and provides an <a class="indexterm" id="id554"/>estimate of the most likely message, given<a class="indexterm" id="id555"/> that the message actually received. We<a class="indexterm" id="id556"/> can express this through a formula in the following manner:</p><div class="informalexample"><pre class="programlisting">didYouMean(received) = ArgMaxintended P(intended | received) 
= ArgMaxintended P(intended,received) / P(received) 
= ArgMaxintended P(intended,received) 
= ArgMaxintended P(intended) * P(received | intended)</pre></div><p>In other words, we will first create a model of the intended message by creating an n-gram character-language model. The language model stores the statistics of seen phrases, that is, essentially, it stores counts of how many times the n-grams occurred. This gives us <code class="literal">P(intended)</code>. For example, <code class="literal">P(intended)</code> is how likely is the character sequence <code class="literal">the</code>. Next, we will create the channel model, which is a weighted edit distance and gives us the probability that the error was typed for that intended text. Again, for example, how likely is the error <code class="literal">teh</code> when the user intended to type <code class="literal">the</code>. In our case, we will model the likeliness using weighted edit distance where the weights are scaled as log probabilities. Refer to the <span class="emphasis"><em>Weighted edit distance</em></span> recipe earlier in the chapter.</p><p>The usual way of creating a compiled spell checker is through an instance of <code class="literal">TrainSpellChecker</code>. The result of compiling the spell-checker-training class and reading it back in is a compiled spell checker. <code class="literal">TrainSpellChecker</code> creates the basic models, weighted edit distance, and token set through the compilation process. We will then need to set various parameters on the <code class="literal">CompiledSpellChecker</code> object.</p><p>A tokenizer factory can be optionally specified to train token-sensitive spell checkers. With tokenization, input is further normalized to insert a single whitespace between all the tokens that are not already separated by a space in the input. The tokens are then output during compilation and read back into the compiled spell checker. The output of set of tokens may be pruned to remove any below a given count threshold. The thresholding doesn't make sense in the absence of tokens because we only have characters to count in the absence of tokens. Additionally, the set of known tokens can be used to constrain the set of alternative spellings suggested during spelling correction to include only tokens in the observed token set.</p><p>This approach to spell check has several advantages over a pure dictionary-based solution:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The context is usefully modeled. <code class="literal">Frod</code> can be corrected to <code class="literal">Ford</code> if the next word is <code class="literal">dealership</code> and to <code class="literal">Frodo</code> if the next word is <code class="literal">Baggins</code>—a character from <span class="emphasis"><em>The Lord of the Rings</em></span> trilogy.</li><li class="listitem" style="list-style-type: disc">Spell checking <a class="indexterm" id="id557"/>can be sensitive to domains. Another<a class="indexterm" id="id558"/> big advantage of this <a class="indexterm" id="id559"/>approach over dictionary-based<a class="indexterm" id="id560"/> spell checking is that the corrections are motivated by data in the<a class="indexterm" id="id561"/> training corpus. So, <code class="literal">trt</code> will be corrected to <code class="literal">tort</code> in a legal domain, <code class="literal">tart</code> in a cooking domain, and <code class="literal">TRt</code> in a bioinformatics domain.</li></ul></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec170"/>How to do it...</h2></div></div></div><p>Let's look at the steps involved in running<a class="indexterm" id="id562"/> spell checking:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In your IDE, run the <code class="literal">SpellCheck</code> class, or in the command line, type the following—note that we are allocating 1 gigabyte of heap space with the <code class="literal">–Xmx1g</code> flag:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -Xmx1g -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter6.SpellCheck </strong></span>
</pre></div></li><li class="listitem">Be patient; the spell checker takes a minute or two to train.</li><li class="listitem">Now, let's enter some misspelled words such as <code class="literal">beleive</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Enter word, . to quit:</strong></span>
<span class="strong"><strong>&gt;beleive</strong></span>
<span class="strong"><strong>Query Text: beleive</strong></span>
<span class="strong"><strong>Best Alternative: believe</strong></span>
<span class="strong"><strong>Nbest: 0: believe Score:-13.97322991490364</strong></span>
<span class="strong"><strong>Nbest: 1: believed Score:-17.326215342327487</strong></span>
<span class="strong"><strong>Nbest: 2: believes Score:-20.8595682233572</strong></span>
<span class="strong"><strong>Nbest: 3: because Score:-21.468056442099623</strong></span>
</pre></div></li><li class="listitem">As you can see, we got the best alternative to the input text as well as some other alternatives. They are sorted by the likelihood of being the best alternative.</li><li class="listitem">Now, we can play around with different input and see how well this spell checker does. Try multiple words in the input and see how it performs:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>The rain in Spani falls mainly on the plain.</strong></span>
<span class="strong"><strong>Query Text: The rain in Spani falls mainly on the plain.</strong></span>
<span class="strong"><strong>Best Alternative: the rain in spain falls mainly on the plain .</strong></span>
<span class="strong"><strong>Nbest: 0: the rain in spain falls mainly on the plain . Score:-96.30435947472415</strong></span>
<span class="strong"><strong>Nbest: 1: the rain in spain falls mainly on the plan . Score:-100.55447634639404</strong></span>
<span class="strong"><strong>Nbest: 2: the rain in spain falls mainly on the place . Score:-101.32592701496742</strong></span>
<span class="strong"><strong>Nbest: 3: the rain in spain falls mainly on the plain , Score:-101.81294112237359</strong></span>
</pre></div></li><li class="listitem">Also, try inputting some proper names to see how they get evaluated.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec171"/>How it works...</h2></div></div></div><p>Now, let's look at what <a class="indexterm" id="id563"/>makes all this tick. We will start off by <a class="indexterm" id="id564"/>setting up <code class="literal">TrainSpellChecker</code>, which<a class="indexterm" id="id565"/> requires a <code class="literal">NGramProcessLM</code> instance, <code class="literal">TokenizerFactory</code>, and an <code class="literal">EditDistance</code> object that sets up weights for edit<a class="indexterm" id="id566"/> operations such as<a class="indexterm" id="id567"/> deletion, insertion, substitution, and so on:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException, ClassNotFoundException {
  double matchWeight = -0.0;
  double deleteWeight = -4.0;
  double insertWeight = -2.5;
  double substituteWeight = -2.5;
  double transposeWeight = -1.0;
  
  FixedWeightEditDistance fixedEdit = new FixedWeightEditDistance(matchWeight,deleteWeight,insertWeight,substituteWeight,transposeWeight);
  int NGRAM_LENGTH = 6;
  NGramProcessLM lm = new NGramProcessLM(NGRAM_LENGTH);
  
  TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
  tokenizerFactory = new com.aliasi.tokenizer.LowerCaseTokenizerFactory(tokenizerFactory);</pre></div><p>
<code class="literal">NGramProcessLM</code> needs to know the number of characters to sample in its modeling of the data. Reasonable values have been supplied in this example for the weighted edit distance, but they can be played with to help with variations due to particular datasets:</p><div class="informalexample"><pre class="programlisting">TrainSpellChecker sc = new TrainSpellChecker(lm,fixedEdit,tokenizerFactory);</pre></div><p>
<code class="literal">TrainSpellChecker</code> can now<a class="indexterm" id="id568"/> be constructed, and next, we will<a class="indexterm" id="id569"/> load 150,000 lines of books from <a class="indexterm" id="id570"/>Project Gutenberg. In a search-engine <a class="indexterm" id="id571"/>context, this data will be the data in your index:</p><div class="informalexample"><pre class="programlisting">File inFile = new File("data/project_gutenberg_books.txt");
String bigEnglish = Files.readFromFile(inFile,Strings.UTF8);
sc.handle(bigEnglish);</pre></div><p>Next, we will add entries from a <a class="indexterm" id="id572"/>dictionary to help with rare words:</p><div class="informalexample"><pre class="programlisting">File dict = new File("data/websters_words.txt");
String webster = Files.readFromFile(dict, Strings.UTF8);
sc.handle(webster);</pre></div><p>Next, we will compile <code class="literal">TrainSpellChecker</code> so that we can instantiate <code class="literal">CompiledSpellChecker</code>. Typically, the output of the <code class="literal">compileTo()</code> operation is written to disk, and <code class="literal">CompiledSpellChecker</code> is read and instantiated from the disk, but the in-memory option is being used here:</p><div class="informalexample"><pre class="programlisting">CompiledSpellChecker csc = (CompiledSpellChecker) AbstractExternalizable.compile(sc);</pre></div><p>Note that there is also a way to deserialize to <code class="literal">TrainSpellChecker</code> in cases where more data might be added later. <code class="literal">CompiledSpellChecker</code> will not accept further training instances.</p><p>
<code class="literal">CompiledSpellChecker</code> admits many fine-tuning methods that are not relevant during training but are relevant in use. For example, it can take a set of strings that are not to be edited; in this case, the single value is <code class="literal">lingpipe</code>:</p><div class="informalexample"><pre class="programlisting">Set&lt;String&gt; dontEdit = new HashSet&lt;String&gt;();
dontEdit.add("lingpipe");
csc.setDoNotEditTokens(dontEdit);</pre></div><p>If these tokens are seen in the input, they will not be considered for edits. This can have a huge impact on the run time. The larger this set is, the faster the decoder will run. Configure the set of do-not-edit tokens to be as large as possible if execution speed is important. Usually, this is done by taking the object to counter-map from the compiled spell checker and saving tokens with high counts.</p><p>During training, the tokenizer factory was used to normalize data into tokens separated by a single whitespace. It is not serialized in the compile step, so if token sensitivity is needed in do-not-edit tokens, then it must be supplied:</p><div class="informalexample"><pre class="programlisting">csc.setTokenizerFactory(tokenizerFactory);
int nBest = 3;
csc.setNBest(64);</pre></div><p>The nBest parameter is set for the number of hypotheses that will be considered in modifying inputs. Even though the <code class="literal">nBest</code> size in the output is set to 3, it is advisable to allow for a larger hypothesis space in the left-to-right exploration of best performing edits. Also, the class has<a class="indexterm" id="id573"/> methods to control what edits are allowed and <a class="indexterm" id="id574"/>how they are scored. See the<a class="indexterm" id="id575"/> tutorial and Javadoc for more about them.</p><p>Finally, we will do a <a class="indexterm" id="id576"/>console I/O loop to generate spelling <a class="indexterm" id="id577"/>variations:</p><div class="informalexample"><pre class="programlisting">BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
String query = "";
while (true) {
  System.out.println("Enter word, . to quit:");
  query = reader.readLine();
  if (query.equals(".")){
    break;
  }
  String bestAlternative = csc.didYouMean(query);
  System.out.println("Best Alternative: " + bestAlternative);
  int i = 0;
  Iterator&lt;ScoredObject&lt;String&gt;&gt; iterator = csc.didYouMeanNBest(query);
  while (i &lt; nBest) {
    ScoredObject&lt;String&gt; so = iterator.next();
    System.out.println("Nbest: " + i + ": " + so.getObject() + " Score:" + so.score());
    i++;
  }
}</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip04"/>Tip</h3><p>We have included a dictionary in this model, and we will just feed the dictionary entries into the trainer like any other data.</p><p>It might be worthwhile to boost the dictionary by training each word in the dictionary more than once. Depending on the count of the dictionary, it might dominate or be dominated by the source training data.</p></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec172"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The spelling-correction tutorial<a class="indexterm" id="id578"/> is more complete and covers evaluation at <a class="ulink" href="http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/querySpellChecker/read-me.html</a></li><li class="listitem" style="list-style-type: disc">The Javadoc for <code class="literal">CompiledSpellChecker</code> can be <a class="indexterm" id="id579"/>found at <a class="ulink" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html">http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/CompiledSpellChecker.html</a></li><li class="listitem" style="list-style-type: disc">More information on how spell checkers work is given in the textbook, <span class="emphasis"><em> Speech and Language Processing</em></span>, <span class="emphasis"><em>Jurafsky</em></span>, <span class="emphasis"><em>Dan</em></span>, and <span class="emphasis"><em>James H. Martin</em></span>, <span class="emphasis"><em>2000</em></span>, <span class="emphasis"><em>Prentice-Hall</em></span></li></ul></div></div></div>
<div class="section" title="The case restoring corrector"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec75"/>The case restoring corrector</h1></div></div></div><p>A<a class="indexterm" id="id580"/> case-restoring spell corrector, also called a truecasing corrector, only restores the case and does not change anything else, that is, it does not correct spelling errors. This is very useful when dealing with low-quality text from transcriptions, automatic speech-recognition output, chat logs, and so on, which contain a variety of case challenges. We typically want to enhance this text to build better rule-based or machine-learning systems. For example, news and video transcriptions (such as closed captions) typically have errors, and this makes it harder to use this data to train NER. Case restoration can be used as a normalization tool across different data sources to ensure that all the data is consistent.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec173"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In your IDE, run the <code class="literal">CaseRestore</code> class, or in the command line, type the following:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.CaseRestore </strong></span>
</pre></div></li><li class="listitem">Now, let's type in some mangled-case or single-case input:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Enter input, . to quit:</strong></span>
<span class="strong"><strong>george washington was the first president of the u.s.a</strong></span>
<span class="strong"><strong>Best Alternative: George Washington was the first President of the U.S.A</strong></span>
<span class="strong"><strong>Enter input, . to quit:</strong></span>
<span class="strong"><strong>ITS RUDE TO SHOUT ON THE WEB</strong></span>
<span class="strong"><strong>Best Alternative: its rude to shout on the Web</strong></span>
</pre></div></li><li class="listitem">As you can see, the mangled case gets corrected. If we use more modern text, such as current newspaper data or something similar, this would be directly applicable to case-normalizing broadcast news transcripts or closed captions.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec174"/>How it works...</h2></div></div></div><p>The class works in a manner<a class="indexterm" id="id581"/> similar to the spelling correction in which we have a model specified by the language model and a channel model specified by the edit distance metric. The distance metric, however, only allows case changes, that is, case variants are zero cost, and all other edit costs are set to <code class="literal">Double.NEGATIVE_INFINITY</code>:</p><p>We will focus on what is different from the previous recipe rather than going over all the source. We will train the spell checker with some English text from Project Gutenberg and use the <code class="literal">CASE_RESTORING</code> edit distance from the <code class="literal">CompiledSpellChecker</code> class:</p><div class="informalexample"><pre class="programlisting">int NGRAM_LENGTH = 5;
NGramProcessLM lm = new NGramProcessLM(NGRAM_LENGTH);
TrainSpellChecker sc = new TrainSpellChecker(lm,CompiledSpellChecker.CASE_RESTORING);</pre></div><p>Once again, by invoking the <code class="literal">bestAlternative</code> method, we will get the best estimate of case-restored text:</p><div class="informalexample"><pre class="programlisting">String bestAlternative = csc.didYouMean(query);</pre></div><p>That's it. Case restoration is made easy.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec175"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The paper by <a class="indexterm" id="id582"/>Lucian Vlad Lita et al., 2003, at <a class="ulink" href="http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf">http://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf</a> is a good reference on truecasing</li></ul></div></div></div>
<div class="section" title="Automatic phrase completion"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec76"/>Automatic phrase completion</h1></div></div></div><p>Automatic phrase completion<a class="indexterm" id="id583"/> is different from spelling correction, in that, it finds the most likely completion among a set of fixed phrases for the text entered so far by a user.</p><p>Obviously, automatic phrase completion is ubiquitous on the Web, for instance, on<code class="literal"> </code>
<a class="ulink" href="https://google.com">https://google.com</a>. For example, if I type <code class="literal">anaz</code> as a query, Google<a class="indexterm" id="id584"/> pops up the following suggestions:</p><div class="mediaobject"><img alt="Automatic phrase completion" src="graphics/4672OS_06_02.jpg"/></div><p>Note that the application is performing spelling checking at the same time as completion. For instance, the top suggestion is <span class="strong"><strong>amazon</strong></span>, even though the query so far is <span class="strong"><strong>anaz</strong></span>. This is not surprising, given that the number of results reported for the phrases that start with <span class="strong"><strong>anaz</strong></span> is probably very small.</p><p>Next, note that it's not doing word suggestion but phrase suggestion. Some of the results, such as <span class="strong"><strong>amazon prime</strong></span> are two words.</p><p>One important difference between autocompletion and spell checking is that autocompletion typically operates over a fixed set of phrases that must match the beginning to be completed. What this means is that if I type a query <code class="literal">I want to find anaz</code>, there are no suggested completions. The source of phrases for a web search is typically high-frequency queries from the query logs.</p><p>In LingPipe, we use the <code class="literal">AutoCompleter</code> class, which maintains a dictionary of phrases with counts and provides suggested completions based on prefix matching by weighted edit distance and phrase likelihood.</p><p>The autocompleter finds the best scoring phrases for a given prefix. The score of a phrase versus a prefix is the sum of the score of the phrase and the maximum score of the prefix against any prefix of the phrase. The score for a phrase is just its maximum likelihood probability estimate, that is, the log of its count divided by the sum of all counts.</p><p>Google and other search engines most likely use their query counts as the data for the best scoring phrases. As we don't have query logs here, we'll use US census data about cities in the US with populations greater than 100,000. The phrases are the city names, and their counts are their populations.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec176"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In your <a class="indexterm" id="id585"/>IDE, run the <code class="literal">AutoComplete</code> class, or in the command line, type the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.AutoComplete </strong></span>
</pre></div></li><li class="listitem">Enter some US city names and look at the output. For example, typing <code class="literal">new</code> will result in the following output:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Enter word, . to quit:</strong></span>
<span class="strong"><strong>new</strong></span>
<span class="strong"><strong>|new|</strong></span>
<span class="strong"><strong>-13.39 New York,New York</strong></span>
<span class="strong"><strong>-17.89 New Orleans,Louisiana</strong></span>
<span class="strong"><strong>-18.30 Newark,New Jersey</strong></span>
<span class="strong"><strong>-18.92 Newport News,Virginia</strong></span>
<span class="strong"><strong>-19.39 New Haven,Connecticut</strong></span>
<span class="strong"><strong>If we misspell 'new' and type 'mew' instead, </strong></span>
<span class="strong"><strong>Enter word, . to quit:</strong></span>
<span class="strong"><strong>mew </strong></span>

<span class="strong"><strong>|mew |</strong></span>
<span class="strong"><strong>-13.39 New York,New York</strong></span>
<span class="strong"><strong>-17.89 New Orleans,Louisiana</strong></span>
<span class="strong"><strong>-19.39 New Haven,Connecticut</strong></span>
</pre></div></li><li class="listitem">Typing city names that don't exist in our initial list will not return any output:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Enter word, . to quit:</strong></span>
<span class="strong"><strong>Alta,Wyoming</strong></span>
<span class="strong"><strong>|Alta,Wyoming|</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec177"/>How it works...</h2></div></div></div><p>Configuring an autocompleter is very similar to configuring spelling, except that instead of training a language model, we will supply it with a fixed list of phrases and counts, an edit distance metric, and some configuration parameters. The initial portion of this code just reads a file and sets up a map of phrases to counts:</p><div class="informalexample"><pre class="programlisting">File wordsFile = new File("data/city_populations_2012.csv");
String[] lines = FileLineReader.readLineArray(wordsFile,"ISO-8859-1");
ObjectToCounterMap&lt;String&gt; cityPopMap = new ObjectToCounterMap&lt;String&gt;();
int lineCount = 0;
for (String line : lines) {
if(lineCount++ &lt;1) continue;
  int i = line.lastIndexOf(',');
  if (i &lt; 0) continue;
  String phrase = line.substring(0,i);
  String countString = line.substring(i+1);
  Integer count = Integer.valueOf(countString);
  
  cityPopMap.set(phrase,count);
}</pre></div><p>The next step is to configure the edit distance. This will measure how close a prefix of a target phrase is to the query prefix. This class uses a fixed weight edit distance, but any edit distance might be used in general:</p><div class="informalexample"><pre class="programlisting">double matchWeight = 0.0;
double insertWeight = -10.0;
double substituteWeight = -10.0;
double deleteWeight = -10.0;
double transposeWeight = Double.NEGATIVE_INFINITY;
FixedWeightEditDistance editDistance = new FixedWeightEditDistance(matchWeight,deleteWeight,insertWeight,substituteWeight,transposeWeight);</pre></div><p>There are <a class="indexterm" id="id586"/>a few parameters to tune autocompletion: the edit distance and search parameters. The edit distance is tuned in exactly the same way as it is for spelling. The maximum number of results to return is more of an application's decision than a tuning's decision. Having said this, smaller result sets are faster to compute. The maximum queue size indicates how big the set of hypotheses can get inside the autocompleter before being pruned. Set <code class="literal">maxQueueSize</code> as low as possible while still performing adequately to increase speed:</p><div class="informalexample"><pre class="programlisting">int maxResults = 5;
int maxQueueSize = 10000;
double minScore = -25.0;
AutoCompleter completer = new AutoCompleter(cityPopMap, editDistance,maxResults, maxQueueSize, minScore);</pre></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec178"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Review the Javadoc for the <code class="literal">AutoCompleter</code> class<a class="indexterm" id="id587"/> at <a class="ulink" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html">http://alias-i.com/lingpipe/docs/api/com/aliasi/spell/AutoCompleter.html</a></li></ul></div></div></div>
<div class="section" title="Single-link and complete-link clustering using edit distance"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec77"/>Single-link and complete-link clustering using edit distance</h1></div></div></div><p>Clustering<a class="indexterm" id="id588"/> is the process of grouping a collection of objects by their similarities, that is, using some sort of distance measure. The idea behind clustering is that objects within a cluster are located close to each other, but objects in different clusters are farther away from each other. We can divide clustering techniques very broadly into hierarchical (or agglomerative) and divisional techniques. Hierarchical techniques start by assuming that every object is its own cluster and merge clusters together until a stopping criterion has been met.</p><p>For example, a stopping criterion can be a fixed distance between every cluster. Divisional techniques go the other way and start by grouping all the objects into one cluster and split it until a stopping criterion has been met, such as the number of clusters.</p><p>We will review hierarchical <a class="indexterm" id="id589"/>techniques in the next few recipes. The two<a class="indexterm" id="id590"/> clustering implementations we <a class="indexterm" id="id591"/>will provide in LingPipe are<a class="indexterm" id="id592"/> single-link clustering and complete-link clustering; the resulting clusters form what is known as a partition of the input set. A set of sets is a partition of another set if each element of the set is a member of exactly one set of the partition. In mathematical terms, the sets that make up a partition are pair-wise disjoint, and the union is the original set.</p><p>A clusterer<a class="indexterm" id="id593"/> takes a set of objects as input and returns a set of sets of objects as output, that is, in code, <code class="literal">Clusterer&lt;String&gt;</code> has a <code class="literal">cluster</code> method, which operates on <code class="literal">Set&lt;String&gt;</code> and returns <code class="literal">Set&lt;Set&lt;String&gt;&gt;</code>.</p><p>A hierarchical clusterer<a class="indexterm" id="id594"/> extends the <code class="literal">Clusterer</code> interface and also operates on a set of objects, but it returns <code class="literal">Dendrogram</code> instead of a set of sets of objects. A dendrogram is a binary tree over the elements being clustered, with distances attached to each branch, which indicates the distance between the two sub-branches. For the <code class="literal">aa</code>, <code class="literal">aaa</code>, <code class="literal">aaaaa</code>, <code class="literal">bbb</code>, <code class="literal">bbbb</code> strings, the single-link based dendrogram with <code class="literal">EditDistance</code> as the metric looks like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>3.0</strong></span>
<span class="strong"><strong>    2.0</strong></span>
<span class="strong"><strong>        1.0</strong></span>
<span class="strong"><strong>            aaa</strong></span>
<span class="strong"><strong>            aa</strong></span>
<span class="strong"><strong>        aaaaa</strong></span>
<span class="strong"><strong>    1.0</strong></span>
<span class="strong"><strong>        bbbb</strong></span>
<span class="strong"><strong>        bbb</strong></span>
</pre></div><p>The preceding dendrogram is based on single-link clustering, which takes the minimum distance between any two elements as the measure of similarity. So, when <code class="literal">{'aa','aaa'}</code> is merged with <code class="literal">{'aaaa'}</code>, the score is 2.0 by adding two <code class="literal">a</code> to <code class="literal">aaa</code>. Complete-link clustering takes the maximum distance between any two elements, which would be 3.0, with an addition of three <code class="literal">a</code> to <code class="literal">aa</code>. Single-link clustering tends to create highly separated clusters, whereas complete-link clustering tends to create more tightly centered clusters.</p><p>There are two<a class="indexterm" id="id595"/> ways to extract clusterings from <a class="indexterm" id="id596"/>dendrograms. The simplest way is to set a<a class="indexterm" id="id597"/> distance bound and maintain every <a class="indexterm" id="id598"/>cluster formed at less than or equal to this bound. The other way to construct a clustering is to continue cutting the highest distance cluster until a specified number of clusters is obtained.</p><p>In this example, we will look at single-link and complete-link clustering with edit distance as the distance metric. We will try to cluster city names by <code class="literal">EditDistance</code>, where the maximum distance is 4.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec179"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In your IDE, run the <code class="literal">HierarchicalClustering</code><code class="literal"> </code>class, or in the command line, type the following:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter6.HierarchicalClustering</strong></span>
</pre></div></li><li class="listitem">The output is various clustering approaches to the same underlying set of <code class="literal">Strings</code>. In this recipe, we will intersperse the source and output. First, we will create our set of strings:<div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws UnsupportedEncodingException, IOException {
  
  Set&lt;String&gt; inputSet = new HashSet&lt;String&gt;();
  String [] input = { "aa", "aaa", "aaaaa", "bbb", "bbbb" };
  inputSet.addAll(Arrays.asList(input));</pre></div></li><li class="listitem">Next, we will set up a single-link instance with <code class="literal">EditDistance</code> and create the dendrogram for the preceding set and print it out:<div class="informalexample"><pre class="programlisting">boolean allowTranspositions = false;
Distance&lt;CharSequence&gt; editDistance = new EditDistance(allowTranspositions);

AbstractHierarchicalClusterer&lt;String&gt; slClusterer = new SingleLinkClusterer&lt;String&gt;(editDistance);

Dendrogram&lt;String&gt; slDendrogram = slClusterer.hierarchicalCluster(inputSet);

System.out.println("\nSingle Link Dendrogram");
System.out.println(slDendrogram.prettyPrint());</pre></div></li><li class="listitem">The output will be as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Single Link Dendrogram</strong></span>

<span class="strong"><strong>3.0</strong></span>
<span class="strong"><strong>    2.0</strong></span>
<span class="strong"><strong>        1.0</strong></span>
<span class="strong"><strong>            aaa</strong></span>
<span class="strong"><strong>            aa</strong></span>
<span class="strong"><strong>        aaaaa</strong></span>
<span class="strong"><strong>    1.0</strong></span>
<span class="strong"><strong>        bbbb</strong></span>
<span class="strong"><strong>        bbb</strong></span>
</pre></div></li><li class="listitem">Next up, we will <a class="indexterm" id="id599"/>create and print out <a class="indexterm" id="id600"/>the <a class="indexterm" id="id601"/>complete-link<a class="indexterm" id="id602"/> treatment of the same set:<div class="informalexample"><pre class="programlisting">AbstractHierarchicalClusterer&lt;String&gt; clClusterer = new CompleteLinkClusterer&lt;String&gt;(editDistance);

Dendrogram&lt;String&gt; clDendrogram = clClusterer.hierarchicalCluster(inputSet);

System.out.println("\nComplete Link Dendrogram");
System.out.println(clDendrogram.prettyPrint());</pre></div></li><li class="listitem">This will produce the same dendrogram, but with different scores:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Complete Link Dendrogram</strong></span>

<span class="strong"><strong>5.0</strong></span>
<span class="strong"><strong>    3.0</strong></span>
<span class="strong"><strong>        1.0</strong></span>
<span class="strong"><strong>            aaa</strong></span>
<span class="strong"><strong>            aa</strong></span>
<span class="strong"><strong>        aaaaa</strong></span>
<span class="strong"><strong>    1.0</strong></span>
<span class="strong"><strong>        bbbb</strong></span>
<span class="strong"><strong>        bbb</strong></span>
</pre></div></li><li class="listitem">Next, we will produce the clusters where the number of clusters is being controlled for the single-link case:<div class="informalexample"><pre class="programlisting">System.out.println("\nSingle Link Clusterings with k Clusters");
for (int k = 1; k &lt; 6; ++k ) {
  Set&lt;Set&lt;String&gt;&gt; slKClustering = slDendrogram.partitionK(k);
  System.out.println(k + "  " + slKClustering);
}</pre></div></li><li class="listitem">This will produce <a class="indexterm" id="id603"/>the following—it will be the<a class="indexterm" id="id604"/> same for <a class="indexterm" id="id605"/>the complete link, given the <a class="indexterm" id="id606"/>input set:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Single Link Clusterings with k Clusters</strong></span>
<span class="strong"><strong>1  [[bbbb, aaa, aa, aaaaa, bbb]]</strong></span>
<span class="strong"><strong>2  [[aaa, aa, aaaaa], [bbbb, bbb]]</strong></span>
<span class="strong"><strong>3  [[aaaaa], [bbbb, bbb], [aaa, aa]]</strong></span>
<span class="strong"><strong>4  [[bbbb, bbb], [aa], [aaa], [aaaaa]]</strong></span>
<span class="strong"><strong>5  [[bbbb], [aa], [aaa], [aaaaa], [bbb]]</strong></span>
</pre></div></li><li class="listitem">The following code snippet is the complete-link clustering without a max distance:<div class="informalexample"><pre class="programlisting">Set&lt;Set&lt;String&gt;&gt; slClustering = slClusterer.cluster(inputSet);
System.out.println("\nComplete Link Clustering No " + "Max Distance");
System.out.println(slClustering + "\n");</pre></div></li><li class="listitem">The output will be:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Complete Link Clustering No Max Distance</strong></span>
<span class="strong"><strong>[[bbbb, aaa, aa, aaaaa, bbb]]</strong></span>
</pre></div></li><li class="listitem">Next, we will control the max distance:<div class="informalexample"><pre class="programlisting">for(int k = 1; k &lt; 6; ++k ){
  clClusterer.setMaxDistance(k);
  System.out.println("Complete Link Clustering at " + "Max Distance= " + k);
  
  Set&lt;Set&lt;String&gt;&gt; slClusteringMd = clClusterer.cluster(inputSet);
  System.out.println(slClusteringMd);
}</pre></div></li><li class="listitem">The following<a class="indexterm" id="id607"/> is the effects of clustering<a class="indexterm" id="id608"/> limited by maximum <a class="indexterm" id="id609"/>distance for the <a class="indexterm" id="id610"/>complete-link case. Note that the single-link input here will have all elements in the same cluster at 3:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Complete Link Clustering at Max Distance= 1</strong></span>
<span class="strong"><strong>[[bbbb, bbb], [aaa, aa], [aaaaa]]</strong></span>
<span class="strong"><strong>Complete Link Clustering at Max Distance= 2</strong></span>
<span class="strong"><strong>[[bbbb, bbb], [aaa, aa], [aaaaa]]</strong></span>
<span class="strong"><strong>Complete Link Clustering at Max Distance= 3</strong></span>
<span class="strong"><strong>[[bbbb, bbb], [aaa, aa, aaaaa]]</strong></span>
<span class="strong"><strong>Complete Link Clustering at Max Distance= 4</strong></span>
<span class="strong"><strong>[[bbbb, bbb], [aaa, aa, aaaaa]]</strong></span>
<span class="strong"><strong>Complete Link Clustering at Max Distance= 5</strong></span>
<span class="strong"><strong>[[bbbb, aaa, aa, aaaaa, bbb]] </strong></span>
</pre></div></li><li class="listitem">That's it! We have exercised a good portion of LingPipe's clustering API.</li></ol></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec180"/>There's more…</h2></div></div></div><p>Clustering is very sensitive to <code class="literal">Distance</code> that is used for the comparison of clusters. Consult the Javadoc for the 10 implementing classes for possible variations. <code class="literal">TfIdfDistance</code> can come in very handy to cluster language data.</p><p>K-means (++) clustering<a class="indexterm" id="id611"/> is a feature-extractor-based clustering. This is what Javadoc says about it:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>K-means clustering</em></span><a class="indexterm" id="id612"/> <span class="emphasis"><em>may be viewed as an iterative approach to the minimization of the average square distance between items and their cluster centers…</em></span></p></blockquote></div></div><div class="section" title="See also…"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec181"/>See also…</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For a detailed tutorial <a class="indexterm" id="id613"/>including details on evaluations, go over to <a class="ulink" href="http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html</a></li></ul></div></div></div>
<div class="section" title="Latent Dirichlet allocation (LDA) for multitopic clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec78"/>Latent Dirichlet allocation (LDA) for multitopic clustering</h1></div></div></div><p>
<span class="strong"><strong>Latent Dirichlet allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>) <a class="indexterm" id="id614"/>is a statistical technique to document clustering based on the tokens or words that are present in the document. Clustering such as classification generally assumes that categories are mutually exclusive. The neat thing about LDA is that it allows for documents to be in multiple topics at the same time, instead of just one category. This better reflects the fact that a tweet can be about <span class="emphasis"><em>Disney</em></span> and <span class="emphasis"><em>Wally World</em></span>, among other topics.</p><p>The other neat thing about LDA, like many clustering techniques, is that it is unsupervised, which means that no supervised training data is required! The closest thing to training data is that the number of topics must be specified before hand.</p><p>LDA can be a great way to explore a dataset where you don't know what you don't know. It can also be difficult to tune, but generally, it does something interesting. Let's get a system working.</p><p>For each document, LDA <a class="indexterm" id="id615"/>assigns a probability of belonging to a topic based on the words in that document. We will start with documents that are converted to sequences of tokens. LDA uses the count of the tokens and does not care about the context or order in which those words appear. The model that LDA operates on for each document is called "a bag of words" to denote that the order is not important.</p><p>The LDA model consists of a fixed number of topics, each of which is modeled as a distribution over words. A document under LDA is modeled as a distribution over topics. There is a Dirichlet prior on both the topic distributions over words and the document distributions over topics. Check out the Javadoc, referenced tutorial, and research literature if you want to know more about what is going on behind the scenes.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec182"/>Getting ready</h2></div></div></div><p>We will continue to work with the <code class="literal">.csv</code> data from tweets. Refer to <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, to know how to get tweets, or use the example data from the book. The recipe uses <code class="literal">data/gravity_tweets.csv</code>.</p><p>This recipe closely <a class="indexterm" id="id616"/>follows the tutorial at <a class="ulink" href="http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html</a>, which goes into much more detail than we do in the recipe. The LDA portion is at the end of the tutorial.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec183"/>How to do it…</h2></div></div></div><p>This section will be a source code review for <code class="literal">src/com/lingpipe/cookbook/chapter6/Lda.java</code> with some references to the <code class="literal">src/com/lingpipe/cookbook/chapter6/LdaReportingHandler.java</code> helping class that will get discussed as we use parts of it:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The top of the <code class="literal">main()</code> method gets data from a standard <code class="literal">csv reader</code>:<div class="informalexample"><pre class="programlisting">File corpusFile = new File(args[0]);
 List&lt;String[]&gt; tweets = Util.readCsvRemoveHeader(corpusFile);</pre></div></li><li class="listitem">Next up is a pile of configuration that we will address line by line. The <code class="literal">minTokenCount</code> filters all tokens that are seen less than five times in the algorithm. As datasets get bigger, this number can get larger. For 1100 tweets, we are assuming that at least five mentions will help reduce the overall noisiness of Twitter data:<div class="informalexample"><pre class="programlisting">int minTokenCount = 5;</pre></div></li><li class="listitem">The <code class="literal">numTopics</code> parameter is probably the most critical configuration value, because it informs the algorithm about how many topics to find. Changes to this number can produce very different topics. You can experiment with it. By choosing 10, we are saying that the 1100 tweets talk about 10 things overall. This is clearly wrong; maybe, 100 is closer to the mark. It is possible that the 1100 tweets had more than 1100 topics, since a tweet can be in more than one topic. Play around with it:<div class="informalexample"><pre class="programlisting">short numTopics = 10;</pre></div></li><li class="listitem">According to the <a class="indexterm" id="id617"/>Javadoc, a rule of thumb for <code class="literal">documentTopicPrior</code> is to set it to 5 divided by the number of topics (or less if there are very few topics; 0.1 is typically the maximum value used):<div class="informalexample"><pre class="programlisting">double documentTopicPrior = 0.1;</pre></div></li><li class="listitem">A generally useful value for the <code class="literal">topicWordPrior</code> is as follows:<div class="informalexample"><pre class="programlisting">double topicWordPrior = 0.01;</pre></div></li><li class="listitem">The <code class="literal">burninEpochs</code> parameter sets how many epochs to run before sampling. Setting this to greater than 0 has desirable properties, in that, it avoids correlation in the samples. The <code class="literal">sampleLag</code> controls how often the sample is taken after burning is complete, and <code class="literal">numSamples</code> controls how many samples to take. Currently, 2000 samples will be taken. If <code class="literal">burninEpochs</code> were 1000, then 3000 samples would be taken with a sample lag of 1 (every time). If <code class="literal">sampleLag</code> was 2, then there would be 5000 iterations (1000 burnin, 2000 samples taken every 2 epochs for a total of 4000 epochs). Consult the Javadoc and tutorial for more about what is going on here:<div class="informalexample"><pre class="programlisting">int burninEpochs = 0;
int sampleLag = 1;
int numSamples = 2000;</pre></div></li><li class="listitem">Finally, <code class="literal">randomSeed</code> initializes the random process in <code class="literal">GibbsSampler</code>:<div class="informalexample"><pre class="programlisting">long randomSeed = 6474835;</pre></div></li><li class="listitem"><code class="literal">SymbolTable</code> is constructed; this will store the mapping from strings to integers for efficient processing:<div class="informalexample"><pre class="programlisting">SymbolTable symbolTable = new MapSymbolTable();</pre></div></li><li class="listitem">A tokenizer is next with our standard one:<div class="informalexample"><pre class="programlisting">TokenzierFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;</pre></div></li><li class="listitem">Next, the configuration of LDA is printed out:<div class="informalexample"><pre class="programlisting">System.out.println("Input file=" + corpusFile);
System.out.println("Minimum token count=" + minTokenCount);
System.out.println("Number of topics=" + numTopics);
System.out.println("Topic prior in docs=" + documenttopicPrior);
System.out.println("Word prior in topics=" + wordPrior);
System.out.println("Burnin epochs=" + burninEpochs);
System.out.println("Sample lag=" + sampleLag);
System.out.println("Number of samples=" + numSamples);</pre></div></li><li class="listitem">Then, we will <a class="indexterm" id="id618"/>create a matrix of documents and tokens that will be input to LDA and a report on how many tokens are present:<div class="informalexample"><pre class="programlisting">int[][] docTokens = LatentDirichletAllocation.tokenizeDocuments(IdaTexts,tokFactory,symbolTable, minTokenCount);
System.out.println("Number of unique words above count" + " threshold=" + symbolTable.numSymbols());</pre></div></li><li class="listitem">A sanity check will follow by reporting a total token count:<div class="informalexample"><pre class="programlisting">int numTokens = 0;
for (int[] tokens : docTokens){
  numTokens += tokens.length;
}
System.out.println("Tokenized.  #Tokens After Pruning=" + numTokens);</pre></div></li><li class="listitem">In order to get <a class="indexterm" id="id619"/>progress reports on the epochs/samples, a handler is created to deliver the desired news. It takes <code class="literal">symbolTable</code> as an argument to be able to recreate the tokens in reporting:<div class="informalexample"><pre class="programlisting">LdaReportingHandler handler = new LdaReportingHandler(symbolTable);</pre></div></li><li class="listitem">The method that the search accesses in <code class="literal">LdaReportingHandler</code> follows:<div class="informalexample"><pre class="programlisting">public void handle(LatentDirichletAllocation.GibbsSample sample) {
  System.out.printf("Epoch=%3d   elapsed time=%s\n", sample.epoch(), Strings.msToString(System.currentTimeMillis() - mStartTime));

  if ((sample.epoch() % 10) == 0) {
    double corpusLog2Prob = sample.corpusLog2Probability();
    System.out.println("      log2 p(corpus|phi,theta)=" + corpusLog2Prob + "   token cross" + entropy rate=" + (-corpusLog2Prob/sample.numTokens()));
  }
}</pre></div></li><li class="listitem">After all this setup, we will get to run LDA:<div class="informalexample"><pre class="programlisting">LatentDirichletAllocation.GibbsSample sample = LatentDirichletAllocation.gibbsSampler(docTokens, numTopics,documentTopicPrior,wordPrior,burninEpochs,sampleLag,numSamples,new Random(randomSeed),handler);</pre></div></li><li class="listitem">Wait, there's more! However, we are almost done. We just need a final report:<div class="informalexample"><pre class="programlisting">int maxWordsPerTopic = 20;
int maxTopicsPerDoc = 10;
boolean reportTokens = true;
handler.reportTopics(sample,maxWordsPerTopic,maxTopicsPerDoc,reportTokens);</pre></div></li><li class="listitem">Finally, we will get to run this code. Type the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter6.LDA</strong></span>
</pre></div></li><li class="listitem">Have a look at a sample<a class="indexterm" id="id620"/> of the resulting output that confirms the configuration and the early reports from the search epochs:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Input file=data/gravity_tweets.csv</strong></span>
<span class="strong"><strong>Minimum token count=1</strong></span>
<span class="strong"><strong>Number of topics=10</strong></span>
<span class="strong"><strong>Topic prior in docs=0.1</strong></span>
<span class="strong"><strong>Word prior in topics=0.01</strong></span>
<span class="strong"><strong>Burnin epochs=0</strong></span>
<span class="strong"><strong>Sample lag=1</strong></span>
<span class="strong"><strong>Number of samples=2000</strong></span>
<span class="strong"><strong>Number of unique words above count threshold=1652</strong></span>
<span class="strong"><strong>Tokenized.  #Tokens After Pruning=10101</strong></span>
<span class="strong"><strong>Epoch=  0   elapsed time=:00</strong></span>
<span class="strong"><strong>      log2 p(corpus|phi,theta)=-76895.71967475882     </strong></span>
<span class="strong"><strong>  token cross-entropy rate=7.612683860484983</strong></span>
<span class="strong"><strong>Epoch=  1   elapsed time=:00</strong></span>
</pre></div></li><li class="listitem">After the epochs are done, we will get a report on the topics found. The first topic starts with a listing of words ordered by count. Note that the topic does not have a title. The topic <code class="literal">meaning</code> can be gleaned by scanning the words that have high counts and a high <code class="literal">Z</code> score. In this case, there is a word <code class="literal">movie</code> with a Z score of 4.0, <code class="literal">a</code> gets 6.0, and looking down the list, we see <code class="literal">good</code> with a score of 5.6. The Z score reflects how nonindependent the word is from the topic with a higher score; this means that the word is more tightly associated with the topic. Look at the source for <code class="literal">LdaReportingHandler</code> to get the exact definition:<div class="informalexample"><pre class="programlisting">TOPIC 0  (total count=1033)
           WORD    COUNT        Z
--------------------------------------------------
          movie      109       4.0
        Gravity       73       1.9
              a       72       6.0
             is       57       4.9
              !       52       3.2
            was       45       6.0
              .       42      -0.4
              ?       41       5.8
           good       39       5.6</pre></div></li><li class="listitem">The preceding output is pretty awful, and the other topics don't look any better. The next topic shows no more potential, but some obvious problems are arising because of tokenization:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>TOPIC 1  (total count=1334)</strong></span>
<span class="strong"><strong>           WORD    COUNT        Z</strong></span>
<span class="strong"><strong>--------------------------------------------------</strong></span>
<span class="strong"><strong>              /      144       2.2</strong></span>
<span class="strong"><strong>              .      117       2.5</strong></span>
<span class="strong"><strong>              #       91       3.5</strong></span>
<span class="strong"><strong>              @       73       4.2</strong></span>
<span class="strong"><strong>              :       72       1.0</strong></span>
<span class="strong"><strong>              !       50       2.7</strong></span>
<span class="strong"><strong>             co       49       1.3</strong></span>
<span class="strong"><strong>              t       47       0.8</strong></span>
<span class="strong"><strong>           http       47       1.2</strong></span>
</pre></div></li><li class="listitem">Donning our system tuner's hats, we will adjust the tokenizer to be the <code class="literal">new RegExTokenizerFactory("[^\\s]+")</code> tokenizer, which really cleans up the clusters, increases clusters to 25, and applies <code class="literal">Util.filterJaccard(tweets, tokFactory, .5)</code> to remove duplicates (1100 to 301). These steps were not performed one at a time, but this is a recipe, so we present the results of some experimentation. There was no evaluation harness, so this was a process that was made up of making a change, seeing if the output looks better and so on. Clusters are notoriously difficult to evaluate and tune on such an open-ended problem. The output looks a bit better.</li><li class="listitem">On scanning the topics, we <a class="indexterm" id="id621"/>get to know that there are still lots of low-value words that crap up the topics, but <code class="literal">Topic 18</code> looks somewhat promising, with a high Z score for <code class="literal">best</code> and <code class="literal">ever</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>OPIC 18  (total count=115)</strong></span>
<span class="strong"><strong>           WORD    COUNT        Z</strong></span>
<span class="strong"><strong>--------------------------------------------------</strong></span>
<span class="strong"><strong>          movie       24       1.0</strong></span>
<span class="strong"><strong>            the       24       1.3</strong></span>
<span class="strong"><strong>             of       15       1.7</strong></span>
<span class="strong"><strong>           best       10       3.0</strong></span>
<span class="strong"><strong>           ever        9       2.8</strong></span>
<span class="strong"><strong>            one        9       2.8</strong></span>
<span class="strong"><strong>           I've        8       2.7</strong></span>
<span class="strong"><strong>           seen        7       1.8</strong></span>
<span class="strong"><strong>           most        4       1.4</strong></span>
<span class="strong"><strong>           it's        3       0.9</strong></span>
<span class="strong"><strong>            had        1       0.2</strong></span>
<span class="strong"><strong>            can        1       0.2</strong></span>
</pre></div></li><li class="listitem">Looking further into the output, we will see some documents that score high for <code class="literal">Topic 18</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>DOC 34</strong></span>
<span class="strong"><strong>TOPIC    COUNT    PROB</strong></span>
<span class="strong"><strong>----------------------</strong></span>
<span class="strong"><strong>   18        3   0.270</strong></span>
<span class="strong"><strong>    4        2   0.183</strong></span>
<span class="strong"><strong>    3        1   0.096</strong></span>
<span class="strong"><strong>    6        1   0.096</strong></span>
<span class="strong"><strong>    8        1   0.096</strong></span>
<span class="strong"><strong>   19        1   0.096</strong></span>

<span class="strong"><strong>Gravity(4) is(6) the(8) best(18) movie(19) I've(18) seen(18) in(3) a(4)</strong></span>

<span class="strong"><strong>DOC 50</strong></span>
<span class="strong"><strong>TOPIC    COUNT    PROB</strong></span>
<span class="strong"><strong>----------------------</strong></span>
<span class="strong"><strong>   18        6   0.394</strong></span>
<span class="strong"><strong>   17        4   0.265</strong></span>
<span class="strong"><strong>    5        2   0.135</strong></span>
<span class="strong"><strong>    7        1   0.071</strong></span>

<span class="strong"><strong>The(17) movie(18) Gravity(7) has(17) to(17) be(5) one(18) of(18) the(18) best(18) of(18) all(17) time(5)</strong></span>
</pre></div></li><li class="listitem">Both seem reasonable for a <code class="literal">best movie ever</code> topic. However, be warned that the other topics/document assignments are fairly bad.</li></ol></div><p>We can't really claim <a class="indexterm" id="id622"/>victory over this dataset in all honesty, but we have laid out the mechanics of how LDA works and its configuration. LDA has not been a huge commercial success, but it has produced interesting concept-level implementations for National Institutes of Health and other customers. LDA is a tuner's paradise with many ways to play with the resulting clustering. Check out the tutorial and Javadoc, and send us your success stories.</p></div></div></body></html>