- en: TD Learning in Healthcare
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Artificial intelligence and medicine‚Äîtwo worlds that have decidedly distant
    origins, but which have already started a close relationship of collaboration,
    with the fundamental objective of improving the health and life expectancy of
    the world population. This is a relationship that is destined to be further strengthened
    over the next few decades, especially since the challenges facing medicine in
    the 21^(st) century are extremely demanding. In this chapter, we will address
    some of these problems with reinforcement learning.¬†We will learn how to use reinforcement
    learning in healthcare. Then, we will learn how to model healthcare insurance
    as a Markov decision process. Finally, we will understand how to plan sanitation
    in operating rooms using TD learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing reinforcement learning in healthcare
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling healthcare insurance plans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the transition model in health insurance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating room sanitation scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2PE4NHe](http://bit.ly/2PE4NHe)'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing reinforcement¬†learning in healthcare
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, enormous attention¬†to artificial intelligence has been paid
    by the healthcare world‚Äîin fact, we have observed the collaboration between Google's
    DeepMind project and the United Kingdom's National Health Service, as well as
    IBM's continuous investments in the areas of genomics and drug discovery through
    the assistance of articular intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Many ask themselves what concrete improvements can be brought by the innovations
    introduced by machine learning in healthcare to patients' lives. In the span of
    5-10 years, it is believed that healthcare will be totally revolutionized. Will
    executives, doctors, nurses, and others be prepared for this new challenge?
  prefs: []
  type: TYPE_NORMAL
- en: Numerous experts predict that, between 2025 and 2030, the world of health will
    be invaded by new technologies based on artificial intelligence and machine learning.
    This time frame, in reality, coincides with many other forecasts that indicate
    how the use of digital intelligence will influence many other fields of culture
    and human activities.
  prefs: []
  type: TYPE_NORMAL
- en: At this moment, in the western world, there are many companies actively involved
    in the development of machine learning systems aimed specifically at healthcare
    uses. In addition to these established companies, we also have a certain number
    of university start-ups and spin-offs that, almost daily, turn their attention
    to the world of medicine.
  prefs: []
  type: TYPE_NORMAL
- en: We'll take a look at some of the possible uses of machine learning in the field
    of healthcare in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Diagnosis of diseases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Diagnosing a disease is a difficult process and requires a great deal of experience.
    It serves to recognize a disease or psychopathology based on symptoms or phenomena,
    the former being subjective manifestations present in the patient, and the latter
    also being evident to the doctor or psychologist. The set of symptoms and signs
    of which some are specific or¬†pathognomonic and others are more or less generic
    characterize the clinical picture of a disease. The set of diagnostic methods
    is called diagnostics. Diagnostics is known to be instrumental when it uses special
    equipment or instruments, such as in imaging or clinical diagnostics, when it
    is based on the direct examination of the patient by the doctor.
  prefs: []
  type: TYPE_NORMAL
- en: Information technology offers great support for diagnosis. Various centers around
    the world make use of the space and computing power of the most powerful supercomputers
    in the world to obtain fast and accurate diagnoses, which draw and process information
    starting from the enormous knowledge base available in search engines and internet
    databases, including the same anonymous electronic patient records.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, a large contribution has been made by machine-based algorithms.
    Through these algorithms, the identification and classification of a pathology
    are less affected by errors. In this way, the¬†early diagnosis of a disorder can
    make treatment much more effective.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction of epidemic events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term epidemic means the onset of a disease that rapidly spreads by direct
    or indirect infection, to the point of affecting a large number of people in a
    more or less vast territory, and is extinguished after a more or less long duration.
    This definition generally applies to the case of infectious diseases, although
    currently there is a tendency to transfer this terminology to the field of non-infectious
    diseases, especially since their frequency in certain populations has undergone
    a clear and unexpected increase, coinciding with quantitative variations and qualitative
    of causal factors.
  prefs: []
  type: TYPE_NORMAL
- en: The spread of diseases depends on phenomena characterized by territorial influences
    and population displacement. Correlating all of this information becomes a burdensome
    task. For this, we need the aid of artificial intelligence, which can predict
    how an epidemic can expand¬†by taking into account the place in which it developed.
    The results of this analysis can help the authorities to prepare pandemic risk
    containment plans.
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation with new drugs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning has a very important utility with regards to the identification
    of new drugs, starting from their study of the composition up to predicting their
    effects.
  prefs: []
  type: TYPE_NORMAL
- en: The Royal Society, which is based in the United Kingdom, states that machine
    learning is an optimal solution in the production of drugs through biological
    methods. In this way, pharmaceutical companies are important helpers in the drug
    production process, especially, with regards to timing and lowering production
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: In precision medicine, the MIT Clinical Machine Learning Group uses algorithms
    to identify the best ways to produce drugs and cures, mainly for diabetes.
  prefs: []
  type: TYPE_NORMAL
- en: Another example¬†is given by the Hanover project by Microsoft, which uses machine
    learning in several cases, especially in technologies for therapies aimed at treating
    cancer and, more specifically, the identification of personalized treatments for
    acute myeloid leukemia.
  prefs: []
  type: TYPE_NORMAL
- en: DeepMind Health
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DeepMind can process millions of pieces of¬†medical information in just a few
    minutes, greatly speeding up medical processes¬†that are clinical in nature, such
    as folder archiving¬†and diagnostics. The researchers at¬†DeepMind are also working
    on models to emulate the ability to imagine the consequences of an action before
    undertaking it‚Äîpractically speaking, they are trying to understand what intelligence
    and imagination are to convert them into algorithms. Verily, the Google branch
    of life science, is also working on a project called the **Baseline Study** to
    collect genetic data. The aim is to adopt some of Google's algorithms to analyze
    what allows people to be healthy. For this project, researchers also use disease
    monitoring technologies, such as intelligent contact lenses to measure blood sugar
    levels.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the time has come to apply reinforcement learning algorithms to practical
    cases related to health care.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling healthcare insurance plans
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **long term care** (**LTC**) refers to a set of interventions that
    are¬†necessary to guarantee adequate assistance to, mainly elderly,¬†individuals
    in conditions of non-self-sufficiency caused by an accident or a disease, but
    also only by aging.¬†These interventions, provided by public or private institutions,
    may fall within the overall health services and/or in the context of the complex
    of welfare services, both at home and in specific institutions designed to face
    these risks.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of these forms of insurance, both private and public, is essentially
    due to the demographic aging process that has affected all industrialized countries.
    This process is creating strong financial and coverage problems in various sectors
    of the welfare state. Examples include the pension system, the demand for health
    services and, more particularly, the demand for those long-term social and health
    services that the elderly need in the event of total or partial loss of their
    autonomy.
  prefs: []
  type: TYPE_NORMAL
- en: Health insurance basis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Health insurance** means a wide range of insurance coverage that allows¬†an
    insurer to intervene when the health condition of the insured is affected due
    to illness or injury, resulting in high medical expenses and inability to generate
    income, thereby causing economic damage to the insured.'
  prefs: []
  type: TYPE_NORMAL
- en: The objective of these forms of insurance is to protect the insured from those
    risks connected to their health conditions and to their relative working capacity,
    which, due to the onset of diseases or the occurrence of accidents, can be interrupted
    for longer or shorter¬†periods, temporarily or permanently.
  prefs: []
  type: TYPE_NORMAL
- en: Some insurances¬†companies provide financial support for alternative health solutions
    to those offered by public health systems, such as medical expenses reimbursement
    insurance; others provide revenue in the form of income or capital, which compensates,
    at least in part, for the loss of income caused by periods of incapacity for work.
  prefs: []
  type: TYPE_NORMAL
- en: To monetize an insurance contract, the intervention of a specialist is required
    to study the technical organization of life insurance companies and, in general,
    the social security institutions¬†¬†by establishing the bases and checking the results
    from a statistical, financial, and mathematical¬†point of view. This figure is
    called the actuary.
  prefs: []
  type: TYPE_NORMAL
- en: The first actuarial calculation models for life insurance are, in fact, dated
    between the end of the seventeenth century and the first half of the eighteenth
    century. Over the centuries, different models have been formulated. In the next
    section, we will analyze a model based on multiple decrements.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing multiple decrement models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The theory of multiple decrements developed from the second half of the eighteenth¬†century.
    The theory of multiple decrements has posed, in terms of mathematical formulation,
    an issue that is also relevant to any other type of insurance activity. It first
    developed in the field of continuous-time models and only later, toward the end
    of the eighteenth century, in the context of discrete-time models. The latter
    were mainly taken into consideration at the end of the nineteenth century for
    the first applications of actuarial calculation to public pension systems.
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of insurance premiums is based on the rule of large numbers
    that states the proportion of successes in *n* independent realizations of an
    event *E* converges, for what tends to infinity, to the probability that the event
    *E* takes place. This means that, when the reference sample is sufficiently numerous
    and the probabilities of disease are independent of each other, the proportion
    of individuals who contract the disease approaches the probability of contracting
    the disease so that this percentage of the population will contract the disease.
    Thanks to this principle, the insurance company can calculate the expected repayments
    and increase the homogeneity. The number of the insured also increases the accuracy
    of the estimate. Therefore, the insurance company does not know who will contract
    the disease but only the percentage of individuals who will fall ill.
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of premiums must be accepted by both parties, that is, the¬†insured
    and¬†the insurer. In the simplest situation, it is assumed that the interest rate
    is constant throughout the active period of the policy and the benefit due to
    the individual depends on the event of death. In the model based on common collectivity,
    the service is paid in the same way to each group. In such cases, it is necessary
    to define when to pay the benefit. Two possible solutions are payment on the first
    death in the group and payment on the last death in the group. In both cases,
    the only random variable is represented by the future life¬†expectancy.
  prefs: []
  type: TYPE_NORMAL
- en: Life tables are used to calculate life expectancy. A life table is a table that
    shows, for each age, what the probability¬†is that a person of that age will die
    before their next birthday. Therefore, in the life table, there is a single exit
    mode. This table is defined as a single decrement table. When we expect different
    causes of decrement on a group of individuals, we can model the system with a
    more general family of models based on multiple decrements.
  prefs: []
  type: TYPE_NORMAL
- en: In multiple decrement models, there is a simultaneous different causes of decrement.
    A life ends because of one of these decreases¬†functioning. In the following section,
    we will calculate the benefit that will be paid to an insured person by modeling
    the problem as a Markov decision process.
  prefs: []
  type: TYPE_NORMAL
- en: Using a transition model in health insurance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decrement model can be seen as a special case of multi-state models. In fact,
    just like what happens in a multi-state model, we can consider an individual of
    age *x* who, at time *t*, is in one of the *n + 1* potential states. In a multiple
    decrement model, starting from the state 0 (active), all of the other transition
    states are defined as absorbent states, which, once reached, can no longer be
    exited.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, three states are provided: active, unable to work, and dead. The
    starting point is obviously represented by the active state, while the other two
    (unable and dead) are absorbent states, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/607a8e60-ab4b-46cb-92ee-d0894e9ce96a.png)'
  prefs: []
  type: TYPE_IMG
- en: We have previously stated that all the¬†transition states except the active state
    are absorbent. This means that it is no longer possible to return to the active
    state. A transition from the unable to work state is possible‚Äîunfortunately, it's
    not pleasant as it involves death.¬†Now, let's learn how to set up a decrease table.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the decrement table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To deal with this problem, it is essential to have a decrement table. The decrement
    table contains the probabilities that the individual passes from one state to
    another, which sounds like a transition matrix. We analyzed transition matrices
    in detail in [Chapter 3](7ee860fd-cd4c-4034-8dd6-9c803e129418.xhtml), *Markov
    Decision Processes in Action*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we will learn how to construct the table of decrements. You may recall
    that the decrement matrix differs from the life expectancy table in that, in addition
    to containing the probability of death as a function of the subject''s age, it
    also contains the probabilities of transition in the other predicted states:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We¬†will create vectors that contain this information and start from age:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we used the `seq()` function, which generates regular sequences. Three
    parameters are used, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`from`: The starting values of the sequence.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`to`: The end values of the sequence.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`by`: The increment of the sequence. By default, this is 1.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In our case, we could have omitted the `by` parameter, but we have inserted
    it to make the code more readable. We considered only a part of working life,
    that is,the one that starts from 30 to reach 50\. Of course, we could have extended
    this interval to the whole working life but, for our purposes, it is fine. Now,
    we can create vectors that contain the probabilities of transition from one state
    to another. We start from the active state, and we have three transition states:
    from active to unable, from active to dead, and from active to active.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `seq()` function again, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, we used the `seq()` function, but this time, we used a new parameter:
    `length.out`. This parameter was used to set the desired length of the sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's analyze the newly created vectors. The first vector is the probability
    of transition from the active to unable state and presents increasing values with
    age. The second vector represents the probability of transition from the active
    to dead state and presents increasing values with age, once again. Finally, the
    third¬†vector represents the probability of transition from the active¬†state¬†to
    active and was obtained as a difference between 1 and the sum of the previous
    values. This is because the sum of the three transition probabilities must return
    1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s move on to the unable¬†state. Once again, we have three transitions:
    unable to active, unable to dead, and unable to unable. Let''s create the three
    vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the first vector contains only zeros. This is because, as we
    anticipated, the unable state does not allow transit to the active state; in this
    sense, it is an absorbing state. The second vector represents the probability
    of transition from the unable to dead state and presents increasing values with
    age. Finally, the third ¬†vector represents the probability of transition from
    the unable state¬†to unable and was obtained as a difference between 1 and the
    sum of the previous values. This is because the sum of the three transition probabilities
    must return 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s move on to the dead state. Once again, we have three transitions:
    dead to active, dead to unable, and dead to dead. Let''s create the three vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The dead state is definitely absorbent compared to the other two states. To
    confirm this, the first two vectors contain zeros, while the third contains ones.
    This is because the rule that the sum of the probabilities of state transitions
    must return 1 remains valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we just have to use these vectors to create the decrement table,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the table¬†we just created; we can use the `str()` function,
    which shows a compact display of the internal structure of an R object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataframe contains 21 observations of 10 variables. Each observation represents
    ages from 30 to 50\. The variables are age and nine transition probabilities between
    the three states: active, unable, and dead. As we can see, the variables contained
    in the table are identified by their names. To refer to variables with their names
    in a dataframe, we can interpose a dollar sign ($) between the data frame''s name
    and the variable''s name. Furthermore, we can read other information: the type
    attributed to each variable. In this case, we have 10 numerical type variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More information can be obtained using the `summary()` function, which produces
    summaries of the distribution of the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a03c9bb8-04a0-44e4-a33d-1742ae2b8449.png)'
  prefs: []
  type: TYPE_IMG
- en: The `summary()` function invokes particular methods that depend on the class
    of the first argument. For each feature, the following descriptors are returned‚Äîminimum,
    first quartile, median, mean third quartile, and maximum. A quick look at these
    values allows us to understand the statistical distribution of the values assumed
    by the variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how this table appears, we can print the head of the decrement
    table, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62682d37-5036-4d5a-b6c5-22ba98b8656d.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, every observation corresponds to a worker's age, and for each
    age, the probabilities of transition from one state to another are reported. Now,
    we can set the basics of the model in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Markov decision process model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our goal is to treat the problem as a Markov decision process. To do this,
    we must define the states. Previously, we said that there are three states in
    which a worker can be found‚Äîactive, unable to work, and dead. Let''s follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a variable that contains this information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: A Markovian process is represented by a transition matrix that defines the probabilities
    of transition from one state to another. We have said that our system is defined
    by three states, so the transition matrix will be 3x3 in size. The information
    we need is contained in the decrements table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will extract the transition matrix at a specific age of the worker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To do this, we simply extracted the final nine columns of the decrement table
    in correspondence with the observation identified by the value of the age variable
    equal to 35\. We have used the `matrix` () function to arrange the extracted data
    into a 3x3 matrix (`nrow = 3, ncol = 3`) and we have fixed the values for progressive
    lines (`byrow = TRUE`). Let''s see the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'By analyzing the preceding matrix, we can see that it has a particular shape;
    in fact, all of the elements below the main diagonal are equal to zero. This type
    of matrix is called the upper triangular and has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: The rank is equal to the number of non-null elements present in the main diagonal.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: The determinant is equal to the product of the elements present in the main
    diagonal.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The eigenvalues are represented by the elements present in the main diagonal.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, we can define the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `markovchain` class has been designed to handle homogeneous Markov chain
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following arguments are passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`transitionMatrix`: This is the square transition matrix containing the probabilities
    of the transition matrix.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`states`: This is the name of the states and must be the same as the¬†`colnames`
    and `rownames` of the transition matrix. This is a character vector, listing the
    states for which transition probabilities are defined.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`name`: This is an optional character element to name the discrete-time Markov
    chains.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To show a summary of the model we''ve¬†just created, let''s use the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As we saw in [Chapter 3](7ee860fd-cd4c-4034-8dd6-9c803e129418.xhtml), *Markov
    Decision Processes in Action*, to get the states of the `markovchain` object,
    we can use the `states` method, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the dimension of the `markovchain` object, we can use the `dim()` method,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To see which elements are contained in the object we have created, we can use
    the `str()` function, which shows a compact view of the internal structure of
    an object, R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To retrieve the elements contained in each one, it will be enough to use the
    name of the object (`MCModel35`), followed by the name of the slot, separated
    by the `@`¬†symbol. For example, to print the transition matrix, we will write
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can evaluate the absorbent state¬†like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following state is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As we know, from this state, we can''t¬†go back. At this point, we can predict
    the status of the worker. It''s a simulation, so let''s see what happens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `set.seed()` command defines the seed of the random number generator. In
    this way, all of the random numbers that are used in the algorithm will always
    be the same each time the code is executed, making the example reproducible.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will extract the¬†statistics from the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what happens to a 50-year-old worker:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will extract the transition matrix at¬†the worker''s age of 50:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will set the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will simulate the working life:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can extract the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can make predictions about working life, simulating the status of a
    worker over time:'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing¬†we need to do is create a `markovchain` object for each available
    working age. We will refer to the interval between 30 and 50 years since we have
    the data¬†for it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To speed up this procedure, we will create an iterative cycle and insert each
    `markovchain` object in a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: To start, we have initialized the list and a counter that will allow us to update
    the list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we extracted the values contained in the decrements table to obtain the
    transition matrix at each age.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we created the `markovchain` object for that age. At this point, we
    can create a `markovchainList` object, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: A `markovchainlist` object¬†is a list of `markovchain` objects. These objects
    can be used to model non-homogeneous discrete-time Markov chains when transition
    probabilities change by time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can simulate the transition between the states where the worker can
    be found:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `rmarkovchain()` function generates a sequence of states from homogeneous
    or non-homogeneous Markov chains. In our case, we have generated a sequence of
    10,000 simulations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what the newly created object contains:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This is a dataframe containing 210,000 observations of two variables. We are
    interested in the second value variable, which contains the states of the simulations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will extract some simple statistics that allow us to obtain the count
    of the occurrences of each state in all of the simulations that are¬†carried out:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'With the data available, it is really easy to calculate the expected times
    a worker will be in the `Unable` state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have divided the number of unable occurrences by the number of rows
    contained in the `StatesSequence` object. The number of rows contained in the
    `StatesSequence` object is equal to the product of the number of simulations (10,000)
    for the number of years of working life (21, from 30 to 50).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Starting from the simulations we've¬†carried out, the insurance companies¬†can
    forecast the expected premiums.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we learn to optimize the use of an operating room in a
    hospital.
  prefs: []
  type: TYPE_NORMAL
- en: Operating room sanitation planning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every year, around 8,000 deaths are associated with surgical room infections
    in the United States. Infections increase the duration of hospitalization, hence
    increasing costs and initiating legal action by patients. The hospital is a building
    in which the appropriate cleaning conditions contribute both to a better quality
    of life for the patient and those who work there and to decrease the probability
    of spreading microorganisms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environmental sanitation concerns the complexity of practical and sanitary
    procedures, and operations aimed at making a plant or a specific environment healthy
    through cleaning and detergent activities. When a disinfectant is used later,
    it is called sanitation. An adequate cleaning cycle must be performed before disinfection,
    and in any case, combined with it. Each environment has an optimal standard¬†which
    is a consequence of the intended use of the environment itself. Hence, the sanitation
    of the operating rooms is completely different from that of the hospital rooms,
    which in turn is different from those of the common areas. The hospital can be
    divided into three areas of infectious risk: low, medium, and high. Low-risk areas
    include common areas such as corridors, offices, and waiting rooms. Areas of medium
    risk include hospital rooms, clinics, and laboratories. High-risk areas include
    operating rooms, intensive care rooms, resuscitation¬†rooms, and recovery rooms.'
  prefs: []
  type: TYPE_NORMAL
- en: Prevention of infections in a surgical room is a significant and current problem¬†since
    it represents an important and frequent complication of surgery with serious consequences
    for patients' health and increased costs, understood as hospital and extra-hospital.
    For the patient, a hospital infection results in additional illness; for the doctor
    or the nurse, these infections can invalidate the efficacy of the treatment carried
    out, question their professionalism, and make them responsible for increased mortality
    in the treated patients. For these reasons, the need to implement preventive interventions
    aimed at containing infections must represent a common and shared goal.
  prefs: []
  type: TYPE_NORMAL
- en: We will frame the problem by defining the data we have available and what we
    want to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we¬†will deal with the problem of planning the sanitation and
    sterilization activities of an operating room. This is a routine operation that
    has costs in terms of resources used and the time in which the operating room
    remains inactive.¬†In the planning of hygiene maintenance activities in the operating
    room, two main objectives must be pursued: the first objective is to avoid infection
    by patients treated in the room, while the second goal is to save on the costs
    of sanitizing and sterilizing operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two objectives are, in any case, correlated since a possible infection
    in the operating room activates an intervention protocol that requires additional
    operations that impose a stop to the activities for 9 hours. On the other hand,
    standard activities provide a 3-hour stop in activities. Sanitation and sterilization
    operations are carried out every 30 days. Given the latest cases of infection,
    the operating room manager wants to know whether the planning of additional operations
    before the deadline can lead to an improvement. Two actions are available: do
    not perform additional operations (NoSS = No Sanitation-Sterilization) or perform
    additional operations (SS = Sanitation-Sterilization).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The interval between two operations (30 days) was divided into three sub-intervals
    that correspond to three states in which the operating theater is located:'
  prefs: []
  type: TYPE_NORMAL
- en: 'state 1: Interval between 0-10 days from the last intervention'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'state 2: Interval between 11-20 days from the last intervention'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'state 3: Interval between 21-30 days from the last intervention'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Immediately after a Sanitation-Sterilization operation, the room enters state
    1\. If no additional operations are performed at the end of the first 10 days,
    the room goes to state 2, and finally, after 10 more days, it passes to state
    3, at the end of which¬†an intervention¬†is expected in all cases. If, between one
    operation and the other, ¬†patient becomes infected, the ongoing operations in
    the operating room are interrupted and an emergency operation is carried out,
    bringing the system back to state 1\. The problem is how to manage the Sanitation-Sterilization
    operations in addition to those already foreseen in a long-term vision to maximize
    the reward.
  prefs: []
  type: TYPE_NORMAL
- en: Transition probability and rewards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This problem can be treated as a Markov decision-making process.¬†To begin,
    we must define the transition matrix *P* (*s*, *s''*, *a*). Remember that it tells
    us what the probabilities are of passing from one state to another. Since two
    actions are available¬†(NoSS and SS), we will define two matrices of transitions.
    We denote the probability of infection with *ps*, whose probability depends on
    the state of the operating room. The longer the time spent by the last intervention,
    the higher the probability that there is an infection. In this regard, we will
    define three probabilities of infection: p1 = 0.2, p2 = 0.3, and p3 = 0.4\. Recall
    that since there''s two possible actions, we will define two transition matrices.
    The transition matrix relating to the choice of action 1 (NoSS) will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eefb173c-7d26-40af-9b90-bae2102d3a79.png)'
  prefs: []
  type: TYPE_IMG
- en: If we are in state 1, then we will have a probability *p1* that remains in that
    state if an infection occurs. The remaining probability *1-p1* involves moving
    to the next state if no infection occurs. While the probability of passing to
    the state 3 is equal to 0, it is not possible to pass directly from state 1 to
    3\. Recall that, in the matrix of transitions, the sum of all of the probabilities
    in a state that is in a row must be equal to 1\. If we are in state 2, we will
    have a probability *p2* that passes into state 1 if an infection occurs. The remaining
    probability *1-p2* involves moving to the next state, such as 3 if no infection
    occurs. In this case, the probability of remaining in state 2 is equal to 0\.
    Finally, if we are in state 3, we will have a probability of *p3* of going to
    state 1 if an infection occurs.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining probability *1-p3* expects to remain in state 3 if no infection
    occurs. After this state, a Sanitation-Sterilization operation is still performed,
    while the probability of passing to state 2 is equal to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'By substituting the values of the three probabilities associated with the three
    states¬†in the transition matrix, we obtain the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cb7aeac-aee5-45e3-be83-a08cd93c162f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s define the transition matrix related to the choice of action 2
    (SS):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c2551b8-8dcf-42b3-b3c6-59131e2ec38e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the form of the transition matrix is more immediate: it is the
    action that foresees a Sanitation-Sterilization operation, which, as we anticipated,
    reports the room in any case in state 1\. This is the person who died in all three
    states we have, which means that the probability that we move to state 1 is unitary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the transition matrices, we are finished.¬†Now, we must define the rewards
    matrix. It is a 3x2 matrix: three states and two actions. Action 1 (NoSS) plans
    not to carry out additional operations; in this case, we will have a decreasing
    reward as we approach the end of the period in which the routine operation is
    scheduled (30 days). Then, the first column of the rewards matrix will take the
    following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e714634-9520-4bc9-92dc-2f638082b64b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Its meaning is obvious: if the chosen action is not to carry out the Sanitation-Sterilization
    operations, then we will have 3 of the reward for the first state, 2 of the reward
    for the second state, and the minimum reward for state 3\. In the case where the¬†chosen¬†action
    is to carry out Sanitation-Sterilization operations, we will have the following¬†instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56d4446b-14be-426b-a4f4-6df0b5207a96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, things have been reversed with respect to the previous case:
    if the action that''s¬†chosen is to carry out Sanitation-Sterilization operations,
    we will have 1 reward for the first state, 2 for the second state, and the maximum
    reward for the state 3.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under these conditions, the rewards matrix becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c91f8b3e-9a6f-4a42-b4c5-20f9535be667.png)'
  prefs: []
  type: TYPE_IMG
- en: After defining the indispensable tools to be able to treat the problem as a
    Markov process, let's elaborate about the model.
  prefs: []
  type: TYPE_NORMAL
- en: Model setting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we already mentioned, our goal is to calculate a policy that allows us to
    obtain the maximum prize based on the settings¬†we just developed. This means reducing
    the risk of infection by minimizing the costs of Sanitation-Sterilization operations
    in the operating room. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the code that allows us to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We will analyze the code line by line to understand the meaning of each command.
    Let''s start by importing the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The **Markov decision process**¬†(**MDP**) toolbox contains many functions that
    allow us to tackle the resolution of discrete-time Markov decision processes.
    We introduced the package in [Chapter 3](7ee860fd-cd4c-4034-8dd6-9c803e129418.xhtml),
    *Markov Decision Processes in Action*.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to do is define the matrices *P* for the transition matrix and
    *R* for the reward matrix. Both have been adequately introduced in the transition
    probability and rewards section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The transition matrix is a 3x3 matrix. To start, we will¬†create a new matrix
    and initialize it to zero:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to define the transition matrix related to the action 1 (NoSS):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The following matrix is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define the transition matrix related to the action 2 (SS):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The following matrix is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s move on to the definition of the rewards matrix, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The following matrix is introduced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Before proceeding with the development of the model, it is necessary to verify
    that `P` and `R` satisfy the criteria necessary for the problem to be of the MDP
    type.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we''ll use the `mdp_check()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'This function performs a check on the MDP process that we defined by setting
    the transition probability matrix (*P*) and the reward matrix (*R*). If `P` and
    `R` have been set correctly, then the function returns an empty message. If `P`
    and `R` were not set correctly, then the function returns an error message describing
    the problem.¬†The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the problem has been set. Now, we can try to solve the problem by using
    the `mdp_Q_learning()` function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The `mdp_Q_learning()` function solves discounted MDP with the Q-learning algorithm.
    As we mentioned in [Chapter 7](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml), *Temporal
    Difference Learning*, Q-learning is one of the most used reinforcement learning
    algorithms. Thanks to this technique, it is possible to find an optimal action
    for every given state in a finished MDP.
  prefs: []
  type: TYPE_NORMAL
- en: A general solution to the reinforcement learning problem is to estimate, thanks
    to the learning process, an evaluation function. This function must be able to
    evaluate, through the sum of the rewards, the convenience or otherwise of a particular
    policy. In fact, Q-learning tries to maximize the value of the Q function (action-value
    function), which represents the maximum discounted future reward when we perform
    actions *a* in the state *s*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Q-learning estimates the function value ùëû (ùë†, ùëé) incrementally, updating the
    value of the state-action pair at each step of the environment,¬†by following the
    logic of updating the general formula for estimating the values for the TD methods.
    Q-learning has off-policy characteristics, that is, while the policy is improved
    according to the values estimated by ùëû (ùë†, ùëé), the value function updates the
    estimates by following a strictly greedy secondary policy: given a state, the
    chosen action is always the one that maximizes the value maxùëû (ùë†, ùëé). However,
    the œÄ policy has an important role in estimating values because through it, the
    state-action pairs to be visited and updated are determined.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following arguments are passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**P**: Transition probability array'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**R**: Reward array'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**discount**: Discount factor‚Äîdiscount is a real number, which belongs to ]0;
    1['
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `mdp_Q_learning()` function computes the Q matrix, the mean discrepancy,
    and gives the optimal value function and the optimal policy when allocated enough
    iterations. The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q**: Action-value function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**V**: Value function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**policy**: Policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mean_discrepancy`: Discrepancy means over 100 iterations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we will look at the results:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s analyze the results we''ve obtained, starting with the action-value
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The Q function represents the essential element of the procedure; it is a matrix
    of the same dimensions as the rewards matrix, that is,¬†an SxA matrix. The value-action
    function returns the utility we expect to achieve by taking a given action in
    a given state and following an optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s print the value function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The following vector is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: A value function represents how good a state is for an agent. It is equal to
    the total reward expected for an agent from the status *s*. The value function
    depends on the policy that the agent selects the actions to be performed¬†with.
    *V* is an *S* length vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will extract the policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: A policy is an *S* length vector. Each element is an integer corresponding to
    an action that maximizes the value function. A policy defines the behavior of
    the learning agent at a given time. It maps the detected states of the environment
    and the actions to take when they are in those states. This corresponds to what,
    in psychology, would be called a set of rules or associations of stimulus response.
    The policy is the fundamental part of a reinforcing learning agent in the sense
    that it alone is enough to determine behavior.
  prefs: []
  type: TYPE_NORMAL
- en: So, the policy suggested by the method is not to carry out the Sanitation-Sterilization
    operations in the first two states, but to execute it if we are in state 3\. This
    is due to the low probability of developing an infection in the first two states,
    which becomes important in state 3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we print the discrepancy means over 100 iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15318d02-95bf-41bb-a472-89d352bee333.png)'
  prefs: []
  type: TYPE_IMG
- en: The `mean_discrepancy`¬†value is a vector of *V* discrepancy mean over 100 iterations.
    Here, the length of the vector for the default value of *N* is 100.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use reinforcement learning in the field of
    health care. We began by analyzing an overview of the uses of machine learning-based
    algorithms in healthcare. We saw how these algorithms are used for the diagnosis
    of diseases, the prediction of epidemic events, and the testing of new drugs.
  prefs: []
  type: TYPE_NORMAL
- en: We then dealt with two practical cases. First, we saw how to tackle a problem
    related to healthcare insurance by modeling it as a Markov process. In this way,
    it is possible to foresee the probabilities with which a worker can be injured
    and quantify the premium that will have to be paid. In the second example, we
    determined the best policy to be adopted in the planning of operations for sanitizing
    an operating theater. The problem was solved through the use of the Q-learning
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the world of deep reinforcement learning
    and how neural networks can be used to make the best policy research operations
    even more efficient.
  prefs: []
  type: TYPE_NORMAL
