<html><head></head><body>
		<div id="_idContainer110">
			<h1 id="_idParaDest-171" class="chapter-number"><a id="_idTextAnchor198"/>13</h1>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor199"/>Prompt Engineering</h1>
			<p>In this chapter, we’ll dive into a special set of techniques called prompt engineering. You’ll learn about this technique at a high level, including how it is similar to and different from other learning-based topics covered throughout this book. We’ll explore examples across vision and language and dive into key terms and success metrics. In particular, this chapter covers all of the tips and tricks for improving performance <em class="italic">without updating the model weights</em>. This means we’ll be mimicking the learning process, without necessarily changing any of the model parameters. This includes some advanced techniques such as prompt and prefix tuning. We will cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Prompt engineering – the art of getting more <span class="No-Break">with less</span></li>
				<li>From few- to <span class="No-Break">zero-shot learning</span></li>
				<li>Tips and tricks for <span class="No-Break">text-to-image prompting</span></li>
				<li>Best practices for <span class="No-Break">image-to-image prompting</span></li>
				<li>Prompting large <span class="No-Break">language models</span></li>
				<li>Advanced techniques – prompt and <span class="No-Break">prefix tuning</span></li>
			</ul>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor200"/>Prompt engineering – the art of getting more with less</h1>
			<p>At this point in the book, and in your project, you should<a id="_idIndexMarker673"/> have a lot invested in your new foundation model. From compute costs to datasets, custom code, and research papers you’ve read, you might have spent a solid 50-100 hours or more of your own time eking out performance gains. Kudos to you! It’s a great life <span class="No-Break">to live.</span></p>
			<p>After you’ve done this, however, and especially after you’ve learned how to build a complete application around your model, it’s time to maximize your model’s performance on inference. In the last chapter, we learned about multiple ways to optimize your model’s runtime, from compilation to quantization and distillation to distribution, and each of these is helpful in speeding up your inference results. This entire chapter, however, is dedicated to getting the most accurate response you can. Here, I use the word “accurate” heuristically to indicate any type of model <em class="italic">quality</em> or <em class="italic">evaluation</em> metric. As you learned in the previous chapter on evaluation, accuracy itself is a misleading term and frequently not your best pick for an evaluation metric. Please see <a href="B18942_10.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic">Chapter 10</em></span></a> for <span class="No-Break">more details</span></p>
			<p>Prompt engineering includes a set of techniques related to picking the best input to the model for inference. Inference refers to getting a result out of your model without updating the weights; think of it like just the forward pass without any backpropagation. This is interesting because it’s how you can get <em class="italic">predictions</em> out of your model. When you deploy your model, you’re deploying it <span class="No-Break">for inference.</span></p>
			<p>Prompt engineering includes a huge swath of techniques. It includes things such as <em class="italic">zero- and few-shot learning</em>, where we send multiple examples to the model and ask it to complete the logical sequence. It includes picking the right hyperparameters. It includes a lot of guessing and checking, testing your model results and figuring out the best techniques <span class="No-Break">for it.</span></p>
			<p>For those of you who are hosting a generative AI model for end consumers outside of your direct team, you might even consider standing up a client to handle prompt engineering for you. This seems to be somewhat common in model playgrounds, where not all the parameters and model invocation are directly exposed. As an app developer, you can and should modify the prompts your customer is sending to your models to ensure they get the best performance they can. This might include adding extra terms to the invocation, updating<a id="_idIndexMarker674"/> the hyperparameters, and rephrasing <span class="No-Break">the request.</span></p>
			<p>Let’s explore prompt engineering in more detail and its related skill, <span class="No-Break">few-shot learning.</span></p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor201"/>From few- to zero-shot learning</h1>
			<p>As you’ll remember, a key<a id="_idIndexMarker675"/> model we’ve been<a id="_idIndexMarker676"/> referring back to is <strong class="bold">GPT-3</strong>, <strong class="bold">Generative Pretrained Transformers</strong>. The paper that gave<a id="_idIndexMarker677"/> us the third version<a id="_idIndexMarker678"/> of this is called <em class="italic">Language models are few shot learners</em>. <em class="italic">(1)</em> Why? Because the primary goal of the paper was to develop a model capable of performing well without extensive fine-tuning. This is an advantage because it means you can use one model to cover a much broader array of use cases without needing to develop custom code or curate custom datasets. Said another way, the unit economics are much stronger for zero-shot learning than they are for fine-tuning. In a fine-tuning world, you need to work harder for your base model to solve a use case. This is in contrast to a few-shot world, where it’s easier to solve additional use cases from your base model. This makes the few-shot model<a id="_idIndexMarker679"/> more valuable because the fine-tuning model becomes too expensive<a id="_idIndexMarker680"/> at scale. While in practice fine-tuning solves problems more robustly than few-shot learning, it makes the entire practice of prompt engineering very attractive. Let’s look at a few examples in the <span class="No-Break">following screenshot.</span></p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B18942_Figure_13_01.jpg" alt="Figure 13.1 – Few-shot learning examples from the GPT-3 paper"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – Few-shot learning examples from the GPT-3 paper</p>
			<p>On the left-hand side, we see different options for inputs to the model on inference. In the paper, they use the phrase “in-context learning,” referring to the fact that in the dataset, there can<a id="_idIndexMarker681"/> be samples of a task definition and examples. These repeated samples help the model learn both the name and the example of the learning. Here, the name of the task, or the task description, is <strong class="bold">Translate English to French</strong>. Then, we see examples of this, such as <strong class="bold">sea otter -&gt; loutre de mer</strong>. When you provide the name of the task to GPT-3, along with a few samples, it is then able to respond <span class="No-Break">quite well.</span></p>
			<p>We call this <strong class="bold">few-shot learning</strong>. This is because we’re providing a few examples<a id="_idIndexMarker682"/> to the model, notably more than one and less than a full dataset. I struggle with using the word “learn” here, because technically, the model’s weights and parameters aren’t being updated. The model isn’t changing at all, so arguably we shouldn’t even use the word “learn.” On the other hand, providing these examples as input ahead of time clearly improves the performance of the model, so from an output-alone perspective perhaps we could use the word “learn.” In any case, this is the <span class="No-Break">standard terminology.</span></p>
			<p>A similar example would then be <strong class="bold">zero-shot learning</strong>, where we provide no examples to the model of how we expect<a id="_idIndexMarker683"/> it to complete its task, and hope it performs well. This is ideal for open-domain question-answering, such as ChatGPT. However, as many people have discovered, a model that performs well in a zero-shot learning scenario can also be shown to perform well in a few-shot or even single-shot example. All of these are useful techniques to understand <span class="No-Break">large models.</span></p>
			<p>As we saw in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.1</em>, a natural comparison with this type of learning is fine-tuning. In a fine-tuning approach, as we learned in <a href="B18942_10.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we use the pretrained model as a base and train it again using a larger dataset sample. Usually, this dataset sample will be supervised, but it is possible to use unsupervised fine-tuning when necessary. In a language scenario, this supervision might be classification, question answering, or summarization. In vision, you might see new image and text pairs across any number of use cases: fashion, e-commerce, image design, marketing, media and entertainment, manufacturing, product design, and <span class="No-Break">so on.</span></p>
			<p>The most common progression would entail <span class="No-Break">the following:</span></p>
			<ol>
				<li>First, try zero-shot learning with your model. Does it work perfectly out of the box on every use case and edge scenario? Likely, it does in a few very narrow cases but can use some <span class="No-Break">help elsewhere.</span></li>
				<li>Next, try single- and <span class="No-Break">few-shot learning.</span></li>
			</ol>
			<p>If you give it a few examples of what you are looking for, does it figure it out? Can it follow the prompts you provide and get a good response? If all else fails, move on to fine-tuning. Go collect a dataset more specific to the use case you want to enhance your model in and train it there. Interestingly, fine-tuning seems to be much more successful in language-only scenarios. In vision, fine-tuning very easily overfits or falls into <em class="italic">catastrophic forgetting</em>, where the model loses its ability to hold onto the images and objects provided in the base dataset. You may be better<a id="_idIndexMarker684"/> off exploring an image-to-image<a id="_idIndexMarker685"/> approach, which <span class="No-Break">follows later.</span></p>
			<p>Now, let’s learn a few best practices for prompt engineering across vision <span class="No-Break">and language.</span></p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor202"/>Text-to-image prompt engineering tips</h1>
			<p>As we mentioned earlier in the book, Stable Diffusion<a id="_idIndexMarker686"/> is a great model you can use to interact with via natural language and produce new images. The beauty, fun, and simplicity of Stable Diffusion-based models are that you can be endlessly creative in designing your prompt. In this example, I made up a provocative title for a work of art. I asked the model to imagine what an image would look like if it were created by Ansel Adams, a famous American photographer from the mid-twentieth century known for his black-and-white photographs of the natural world. Here was the full prompt: “<em class="italic">Closed is open” by Ansel Adams, high resolution, black and white, award winning. Guidance (20)</em>. Let’s take a <span class="No-Break">closer look.</span></p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B18942_Figure_13_02.jpg" alt="Figure 13.2 – An image generated by Stable Diffusion"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – An image generated by Stable Diffusion</p>
			<p>In the following list, you’ll find a few helpful tips to improve your Stable <span class="No-Break">Diffusion results:</span></p>
			<ul>
				<li><strong class="bold">Add any of the following words to your prompt</strong>: <em class="italic">Award-winning, high resolution, trending on &lt;your favorite site here&gt;, in the style of &lt;your favorite artist here&gt;, 400 high dpi</em>, and so on. There are thousands of examples of great photos and their corresponding prompts online; a great site is lexica.art. Starting from what <a id="_idIndexMarker687"/>works is always a great path. If you’re passionate about vision, you can easily spend hours of time just pouring through these and finding good examples. For a faster route, that same site lets you search for words as a prompt and renders the images. It’s a quick way to get started with prompting <span class="No-Break">your model.</span></li>
				<li><strong class="bold">Add negative prompts</strong>: Stable Diffusion offers a negative prompt option, which lets you provide words to the model that it will explicitly not use. Common examples of this are hands, human, oversaturated, poorly drawn, <span class="No-Break">and disfigured.</span></li>
				<li><strong class="bold">Upscaling</strong>: While most prompting with Stable Diffusion<a id="_idIndexMarker688"/> results in smaller images, such as size 512x512, you can use another technique, called upscaling, to render that same image into a much larger, higher quality image, of size 1,024x1,024 or even more. Upscaling is a great step you can use to get the best quality Stable Diffusion models today, both on SageMaker <em class="italic">(2)</em> and through Hugging Face directly. <em class="italic">(3)</em> We’ll dive into this in a bit more detail in the upcoming section <span class="No-Break">on image-to-image.</span></li>
				<li><strong class="bold">Precision and detail</strong>: When you provide longer prompts to Stable Diffusion, such as including more terms in your prompt and being extremely descriptive about the types and styles of objects you’d like it to generate, you actually increase your odds of the response being good. Be careful about the words you use in the prompt. As we learned earlier in <a href="B18942_11.xhtml#_idTextAnchor167"><span class="No-Break"><em class="italic">Chapter 11</em></span></a> on bias, most large models are trained on the backbone of the internet. With Stable Diffusion, for better or for worse, this means you want to use language that is common online. This means that punctuation and casing actually aren’t as important, and you can be really creative and spontaneous with how you’re describing what you want <span class="No-Break">to see.</span></li>
				<li><strong class="bold">Order</strong>: Interestingly, the order of your words matters in prompting Stable Diffusion. If you want to make some part of your prompt more impactful, such as <em class="italic">dark</em> or <em class="italic">beautiful</em>, move that to the front of your prompt. If it’s too strong, move it to <span class="No-Break">the back.</span></li>
				<li><strong class="bold">Hyperparameters</strong>: These are also relevant in language-only models, but let’s call out a few that are especially relevant<a id="_idIndexMarker689"/> to <span class="No-Break">Stable Diffusion.</span></li>
			</ul>
			<p class="callout-heading">Key hyperparameters for Stable Diffusion prompt engineering</p>
			<p class="callout">1. <strong class="bold">Guidance</strong>: The technical term here is <em class="italic">classifier-free guidance</em>, and it refers to a mode in Stable Diffusion<a id="_idIndexMarker690"/> that lets the model<a id="_idIndexMarker691"/> pay more (higher guidance) or less (lower guidance) attention<a id="_idIndexMarker692"/> to your prompt. This ranges from 0 up to 20. A lower guidance term means the model is optimizing less for your prompt, and a higher term means it’s entirely focused on your prompt. For example, in my image in the style of Ansel Adams above, I just updated the guidance term from 8 to 20. In an earlier version of the same image, I set guidance to 8. This produced rolling and gentle shadows. However, when I updated <strong class="source-inline">guidance=20</strong> on the second image, the model captures the stark contrast and shadow fades that characterized Adams’ work. In addition, we get a new style, almost like M. C. Escher, where the tree seems to turn into <span class="No-Break">the floor.</span></p>
			<p class="callout">2. <strong class="bold">Seed</strong>: This refers to an integer you can set to baseline<a id="_idIndexMarker693"/> your diffusion process. Setting the seed can have a big impact on your model response. Especially if my prompt isn’t very good, I like to start with the seed hyperparameter and try a few random starts. Seed impacts high-level image attributes such as style, size of objects, and coloration. If your prompt is strong, you may not need to experiment heavily here, but it’s a good <span class="No-Break">starting point.</span></p>
			<p class="callout">3. <strong class="bold">Width and height</strong>: These are straightforward; they’re just the pixel <a id="_idIndexMarker694"/>dimensions of your output image! You can use them to change the scope of your result, and hence the type of picture the model generates. If you want a perfectly square image, use 512x512. If you want a portrait orientation, use 512x768. For a landscape orientation, use 768x512. Remember you can use the upscaling process we’ll learn about shortly to increase the resolution on the image, so start with<a id="_idIndexMarker695"/> smaller <span class="No-Break">dimensions first.</span></p>
			<p class="callout">4. <strong class="bold">Steps</strong>: This refers to the number of denoising steps<a id="_idIndexMarker696"/> the model will take as it generates your new image, and most people start with <strong class="source-inline">steps</strong> set to <strong class="source-inline">50</strong>. Increasing<a id="_idIndexMarker697"/> this number will also increase the processing time. To get great results, personally, I like to scale this against guidance. If you plan on using a very high guidance term (~16), such as with a killer prompt, then I wouldn’t set inference steps to anything over 50. This looks like it overfits, and the results are just plain bad. However, if your guidance scale is lower, closer to 8, then increasing the number of steps can get you a <span class="No-Break">better result.</span></p>
			<p>There are many more hyperparameters to explore for Stable Diffusion and other text-to-image diffusion models. For now, let’s explore techniques <span class="No-Break">around image-to-image!</span></p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor203"/>Image-to-image prompt engineering tips</h1>
			<p>A fascinating trend<a id="_idIndexMarker698"/> in generative AI, especially when prompting the model, is <em class="italic">image-to-image</em>. This covers a broad array of techniques that let you bring an image when you invoke the model. The response will then incorporate your source image into the response, letting you more concretely determine what response the model will provide. This is incredibly helpful for increasing the resolution of the images, adding a mask, or even introducing objects to then seamlessly format into the output image in <span class="No-Break">any context.</span></p>
			<p>These core capabilities are possible through a technique introduced in early 2022 <em class="italic">(4)</em>, called <strong class="bold">Stochastic Differential Equations Edit</strong> (<strong class="bold">SDEdit</strong>), which uses stochastic differential equations to make image<a id="_idIndexMarker699"/> synthesis and editing a lot easier. While it sounds a bit intimidating, it’s actually very intuitive. It lets you add a source image to your pretrained diffusion model and use that base image as inspiration. How? Through iteratively adding and removing noise in a variety of ways until the final result meets your preferred criteria. SDEdit improved on its predecessor, GAN-based<a id="_idIndexMarker700"/> methods, by up to 98% on realism and 91% on human <span class="No-Break">satisfaction scores.</span></p>
			<p>Let’s explore the ways we can use this enhanced image-to-image technique while prompting with your <span class="No-Break">diffusion models!</span></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor204"/>Upscaling</h2>
			<p>As mentioned earlier, this is a simple<a id="_idIndexMarker701"/> and fast way to increase the resolution<a id="_idIndexMarker702"/> of your images. When prompting the model, you can enhance a low-resolution image along with other parameters to increase quality. You have a built-in option for this with SageMaker JumpStart <em class="italic">(5)</em>, and you also have a full upscaling pipeline available through Hugging Face directly.<em class="italic">(6)</em> This can take another textual prompt in addition to the <span class="No-Break">source image.</span></p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor205"/>Masking</h2>
			<p>Another interesting technique when prompting<a id="_idIndexMarker703"/> diffusion models is <strong class="bold">masking</strong>. A mask is simply a set of pixels that covers<a id="_idIndexMarker704"/> a given area in a photo: mountains, cars, humans, dogs, and any other type of object present in the images. How do you find a pixel map? These days, honestly, an easy way might be to start with Meta’s new <strong class="bold">Segment Anything Model</strong><em class="italic"> (</em><strong class="bold">SAM</strong><em class="italic">)</em>. <em class="italic">(7)</em> You can upload an image and ask the model to generate a pixel map for anything in <span class="No-Break">that image.</span></p>
			<p>Once you have a mask, you can send it to a Stable Diffusion image to generate a new image inside of the mask. Classic examples of this are changing the styles of clothing that people seem to be wearing. You can extract the area of a photo with clothing using either SAM or open source CV tools, render the mask, and then send the mask to Stable Diffusion. It will generate a new image, combining the original with a newly generated twist to fill in the area of <span class="No-Break">the mask.</span></p>
			<p>For a nice and simple end-to-end example of this, check out one I just found on <span class="No-Break">GitHub! </span><span class="No-Break"><em class="italic">(8)</em></span></p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor206"/>Prompting for object-to-image with DreamBooth</h2>
			<p>Unlike the previous methods<a id="_idIndexMarker705"/> we looked at in the previous<a id="_idIndexMarker706"/> sections, DreamBooth <em class="italic">(9)</em> does not use the underlying SDEdit method. Instead, it uses a handful of input images and runs a type of fine-tuning process, combined with textual guidance, to place the source object from all of the input images into the target scene generated by the model. The technique uses two loss functions, one to preserve the previous class learned by the pretrained model and another to reconstruct the new object into the <span class="No-Break">final image.</span></p>
			<p>This means arguably, it’s not a prompting technique; it’s closer to a fine-tuning technique. However, I’m including it here because I find the intention more similar to masking than to creating a net-new model, but that is actually the outcome. Let’s take a closer look at the DreamBooth <span class="No-Break">loss function.</span></p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B18942_Figure_13_03.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – The DreamBooth loss function</p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 - Dreambooth prior-preserving loss function</p>
			<p>DreamBooth is a great open source solution<a id="_idIndexMarker707"/> you can use to take any object<a id="_idIndexMarker708"/> you like and place it onto any background of your choice! Next, let’s learn about some techniques you can use to improve your prompts for <span class="No-Break">language models.</span></p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor207"/>Prompting large language models</h1>
			<p>I’ve said this before: I am a huge <a id="_idIndexMarker709"/>fan and big advocate of Hugging Face. I’ve learned a lot about <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) from and with them, so I’d be remiss if I didn’t call out their book<a id="_idIndexMarker710"/> as a great source for prompt engineering tips and techniques. <em class="italic">(10)</em> Most of those practices center around picking the right hyperparameters for your model, with each type of model offering slightly <span class="No-Break">different results.</span></p>
			<p>However, I would argue that the rise of ChatGPT has now almost completely thrown that out of consideration. In today’s world, the extremely accurate performance of OpenAI’s model raises the bar for all NLP developers, pushing us to deliver comparable results. For better or worse, there is no going back. Let’s try to understand how to prompt our <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>)! We’ll start with <span class="No-Break">instruction fine-tuning.</span></p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor208"/>Instruction fine-tuning</h2>
			<p>First, it’s helpful to really<a id="_idIndexMarker711"/> understand the difference<a id="_idIndexMarker712"/> between a model that has been <em class="italic">instruction fine-tuned</em> and one that has not. As we learned in <a href="B18942_10.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic">Chapter 10</em></span></a> on fine-tuning, instruction fine-tuning refers to a supervised fine-tuning<a id="_idIndexMarker713"/> step that uses instructions provided to the model, such as “Tell me the difference between a mimosa and a samosa,” and pairs these with an answer, something such as “While a mimosa is an alcoholic drink combining champagne with orange juice, a samosa is an Indian pastry filled with either vegetables or meat, and commonly with a potato filling.” The model then explicitly learns what it means to <span class="No-Break">follow instructions.</span></p>
			<p>This matters for prompting LLMs because it will completely change your prompting style. If you’re working with an LLM that has already been instruction fine-tuned, you can jump right into zero-shot performance and immediately have it execute tasks for you seamlessly. If not, you will probably need to add some examples to your prompt, that is, few-shot learning, to encourage it to respond in the way you want <span class="No-Break">it to.</span></p>
			<p>Once you’ve sorted out this key difference, it’s helpful to also spend some time trying out the model of your choice. They have nuances; some of them look for different tokens and separators, while others respond well to keywords and phrases. You want to get to know and test your LLM, and prompt engineering is a great way to do that. Another style to learn is <span class="No-Break"><em class="italic">chain-of-thought prompting</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor209"/>Chain-of-thought prompting</h2>
			<p>Even if you are working<a id="_idIndexMarker714"/> with a model that performs<a id="_idIndexMarker715"/> well in zero-shot cases, such as one that has received instruction fine-tuning, as we discussed previously, you may still come across use cases where you need to add some examples in the prompt to get the desired output. A great example of this is chain-of-thought prompting. Chain-of-thought prompting refers to prompting the model <em class="italic">to demonstrate how it</em> <em class="italic">arrives at an answer</em>. This is extremely valuable in scenarios where explainability is critical, such as explaining why an LLM makes stylistic updates or classification decisions. Imagine you are using LLMs in legal scenarios, for example, and you’d like the LLM to update the language in a legal document. When you prompt it as follows, instead of simply providing the answer, the model can explain step by step how it came to a given conclusion. This logical clarity helps most users have more trust in the system, helping them understand and trust that the suggestions made by the model <span class="No-Break">are valid.</span></p>
			<p>It also in many cases helps with accuracy! This is because most LLMs are inherently auto-regressive; they’re very good at predicting which word is most likely to come next in a given string. When you prompt them into a chain of thought, you’re pushing them to generate thought by thought, keeping them closer to the truth. Let’s take a closer look at this visually, using the following graphic from the original <span class="No-Break">paper. </span><span class="No-Break"><em class="italic">(11)</em></span></p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B18942_Figure_13_04.jpg" alt="Figure 13.4 – Chain-of-thought prompting"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – Chain-of-thought prompting</p>
			<p>As you can see, on the left-hand side, we’re still doing some few-shot learning, but the answer provided in the prompt is simple. It only answers the question, full-stop. On the right-hand side, however, we prompt the model by providing an answer that <em class="italic">rephrases the question</em>. Now, the answer starts with re-generating a quick summary of the information provided in the question, then taking exactly one logical leap to output the correct answer. You can see that the model on the left-hand side fails to answer correctly, while the one on the right is correct. Actually, the model itself is the same, but the only difference here is <span class="No-Break">the prompt.</span></p>
			<p>For a model that has been instruction<a id="_idIndexMarker716"/> fine-tuned, you can also trigger<a id="_idIndexMarker717"/> a chain-of-thought performance with a statement such as “walk me through step by step <span class="No-Break">how to...”.</span></p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor210"/>Summarization</h2>
			<p>This is possibly the most<a id="_idIndexMarker718"/> common LLM scenario I see today: summarizing call transcripts, documents, and more. Summarization is now very easy with top LLMs. Simply paste as much of your document into the LLM as you can, based on the model’s context length, and add <em class="italic">Summarize</em>: at the bottom of the prompt. Some models will vary; you can also add <em class="italic">TL;DR</em>, <em class="italic">in summary</em>:, or any similar variant. Will they all work perfectly? No way. Will they catch absolutely everything? Absolutely not. Will they occasionally hallucinate? Without a doubt. How do we mitigate that? Fine-tuning, extensive validation, entity recognition, <span class="No-Break">and auditing.</span></p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor211"/>Defending against prompt injections and jailbreaking</h2>
			<p>One technique to consider<a id="_idIndexMarker719"/> in prompting your <a id="_idIndexMarker720"/>model is <em class="italic">how sensitive it is to jailbreaking</em>. As we learned in <a href="B18942_11.xhtml#_idTextAnchor167"><span class="No-Break"><em class="italic">Chapter 11</em></span></a> on detecting<a id="_idIndexMarker721"/> and mitigating bias, jailbreaking<a id="_idIndexMarker722"/> refers to malicious <a id="_idIndexMarker723"/>users prompting your model to engage in harmful behavior. This might be like asking your model to tell a rude joke about certain groups of people, asking it for instructions about theft, or asking its opinion about certain politicians or social groups. Please anticipate that in every LLM application, at least some of your users will try to jailbreak your model to see whether they can trick it into behaving poorly. A parallel method is <strong class="bold">prompt injection</strong>, where users can maliciously trick<a id="_idIndexMarker724"/> your model into responding with IP from your dataset, from your prompt set, or anything else from your <span class="No-Break">instruction list.</span></p>
			<p>How can you defend against this? One way is with supervised fine-tuning. Anthropic maintains a large dataset of red-teamed data, available on Hugging Face here. <em class="italic">(12)</em><span class="superscript"> </span>Please proceed with caution; the words used in this dataset are extremely graphic and may be triggering to some readers. Personally, I find it hard to study even a few lines of this dataset. As a supervised fine-tuning technique, or even, as suggested by Anthropic, as a reinforcement learning with human feedback technique, you can fine-tune your model to reject anything that looks malicious <span class="No-Break">or harmful.</span></p>
			<p>On top of this, you can <em class="italic">add classifiers to your application ingest</em>. This means as your app is taking in new questions from your users, you can easily add extra machine learning models to detect<a id="_idIndexMarker725"/> any malicious<a id="_idIndexMarker726"/> or odd behavior<a id="_idIndexMarker727"/> in these questions and circumvent<a id="_idIndexMarker728"/> the answer. This gives you a lot of control over how your <span class="No-Break">app responds.</span></p>
			<p>Now that we’ve learned about some basic techniques for prompting LLMs, let’s look at a few <span class="No-Break">advanced techniques!</span></p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor212"/>Advanced techniques – prefix and prompt tuning</h1>
			<p>You might be wondering; isn’t there some sophisticated way to use optimization techniques and find the right prompt, without even updating the model parameters? The answer is yes, there are many ways of doing this. First, let’s try to understand <span class="No-Break"><strong class="bold">prefix tuning</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor213"/>Prefix tuning</h2>
			<p>This technique was proposed <em class="italic">(13)</em> by a pair of Stanford<a id="_idIndexMarker729"/> researchers in 2021 specifically for text generation. The core idea, as you can see in the following diagram from their paper, is that instead of producing a net-new model for each downstream task, a less resource-intensive option<a id="_idIndexMarker730"/> is to create a simple vector for each task itself, called <span class="No-Break">the prefix.</span></p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B18942_Figure_13_05.jpg" alt="Figure 13.5 – Prefix tuning"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – Prefix tuning</p>
			<p>The core idea here is that instead<a id="_idIndexMarker731"/> of fine-tuning the entire pretrained transformer for each downstream task, let’s try to update just a single vector for that task. Then, we don’t need to store all of the model weights; we can just store <span class="No-Break">that vector!</span></p>
			<p>Arguably, this technique is similar to one we briefly touched on in <a href="B18942_10.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><em class="italic">, Fine-Tuning and Evaluating.</em> This technique injects trainable weights into an LLM, letting us learn just the new parameters rather than updating the entire model itself. I find prefix tuning interesting because we’re not really touching the model architecture at all; we’re just learning this basic object right at <span class="No-Break">the start.</span></p>
			<p>Why should you learn about this? Because, as the Stanford team shows, this method uses only 0.1% of the parameters of the full model yet gives performance comparable to fine-tuning the <span class="No-Break">entire model.</span></p>
			<p>How can you get started with prefix tuning? Using the new library from our friends at Hugging Face! They’re building an open source library to make all kinds of parameter-efficient fine-tuning available here: <a href="https://github.com/huggingface/peft">https://github.com/huggingface/peft</a>. Prefix tuning is <span class="No-Break">certainly available.</span></p>
			<p>Fortunately, the example for general PEFT, coming from the inimitable Phill Schmid, seems quite accessible here. <em class="italic">(14)</em><span class="superscript"> </span>With some specialized<a id="_idIndexMarker732"/> data preprocessing and custom model configs, you too can add this to <span class="No-Break">your scripts.</span></p>
			<p>Now, let’s look at <span class="No-Break">prompt tuning.</span></p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor214"/>Prompt tuning</h2>
			<p>As we’ve seen, finding the right prompt<a id="_idIndexMarker733"/> is quite challenging. Usually, they are built on discrete words in human natural language and can require a fair amount of manual iteration to trick the model into providing the expected answer. In Google’s 2021 ACL paper <em class="italic">(15)</em> introducing this concept, they proposed ”soft prompts” that are learnable through backpropagation. Thankfully, this incorporates the signal from any number of labeled examples, simplifying the prefix-tuning approach <span class="No-Break">proposed previously.</span></p>
			<p>With prompt tuning, we freeze the entire pretrained model but allow an extra <em class="italic">k</em> tunable tokens per each downstream task to be added to the input text. These are then considered soft tokens, or signals learned by the model to recognize each downstream task. You can see this in the diagram from their paper <span class="No-Break">shown here.</span></p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B18942_Figure_13_06.jpg" alt="Figure 13.6 – Prompt tuning"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – Prompt tuning</p>
			<p>Similar to prefix tuning, using prompt tuning, we still freeze the foundation model weights. We are also still adding some new, learnable items to the input dataset mixed with a variety of downstream task data samples. The key difference is that instead of learning full blocks for the model, we learn new, machine-readable tokens. That means the tokens themselves should change after the gradient updating, signaling something the model recognizes as basically a trigger for that type of downstream task. If you are working on scenarios where parameter-efficient fine-tuning isn’t an option, such as where the model is completely obscured to you, then prefix or prompt tuning maybe be a good option to explore. Both techniques<a id="_idIndexMarker734"/> are available in the relevant Hugging Face <span class="No-Break">library, </span><span class="No-Break"><strong class="source-inline">peft</strong></span><span class="No-Break">.</span></p>
			<p>Now, let’s close out the chapter with a <span class="No-Break">quick summary.</span></p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor215"/>Summary</h1>
			<p>In this chapter, we introduced the concept of prompt engineering. I’d define that as everything that ekes out accuracy gains from your model without updating the weights of the model itself. Said another way, this is the art of getting more with less. We walked through few-shot learning, where you send a few examples of your desired inference results to the model, to zero-shot learning, where you hope to get a response from the model without any prior information. Needless to say, consumers tend to strongly prefer zero-shot learning. We covered a few tips and tricks for prompting text-to-image models, especially how to get good performance out of the open source Stable Diffusion. We learned about image-to-image prompting, where you can pass images to your diffusion-based models to produce a new image using an intersection. We also learned about prompting LLMs, including the implications of instruction fine-tuning, chain-of-thought prompting, summarization, and defending against prompt injections and jailbreaking. Finally, we introduced a few advanced techniques, including prompt and <span class="No-Break">prefix tuning.</span></p>
			<p>Now, let’s get started on <a href="B18942_14.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 14</em></span></a>, which is on MLOps for vision <span class="No-Break">and LLMs!</span></p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor216"/>References</h1>
			<p>Please go through the following content for more information on some topics covered in <span class="No-Break">the chapter:</span></p>
			<ol>
				<li>Language Models are Few-Shot <span class="No-Break">Learners: </span><a href="https://arxiv.org/pdf/2005.14165.pdf"><span class="No-Break">https://arxiv.org/pdf/2005.14165.pdf</span></a></li>
				<li>Upscale images with Stable Diffusion in Amazon SageMaker <span class="No-Break">JumpStart:</span><span class="No-Break"> </span><a href="https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/</span></a></li>
				<li>stabilityai/stable-diffusion-x4-upscaler <span class="No-Break">Copied: </span><a href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler"><span class="No-Break">https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler</span></a></li>
				<li>SDEDIT: GUIDED IMAGE SYNTHESIS AND EDITING WITH STOCHASTIC DIFFERENTIAL <span class="No-Break">EQUATIONS: </span><a href="https://arxiv.org/pdf/2108.01073.pdf"><span class="No-Break">https://arxiv.org/pdf/2108.01073.pdf</span></a></li>
				<li>Upscale images with Stable Diffusion in Amazon SageMaker <span class="No-Break">JumpStart: </span><a href="https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/upscale-images-with-stable-diffusion-in-amazon-sagemaker-jumpstart/</span></a></li>
				<li>Hugging <span class="No-Break">Face: </span><a href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler "><span class="No-Break">https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler</span></a></li>
				<li>Segment <span class="No-Break">Anything: </span><a href="https://arxiv.org/pdf/2304.02643.pdf"><span class="No-Break">https://arxiv.org/pdf/2304.02643.pdf</span></a></li>
				<li><span class="No-Break">amrrs/stable-diffusion-prompt-inpainting: </span><a href="https://github.com/amrrs/stable-diffusion-prompt-inpainting/blob/main/Prompt_based_Image_In_Painting_powered_by_ClipSeg.ipynb"><span class="No-Break">https://github.com/amrrs/stable-diffusion-prompt-inpainting/blob/main/Prompt_based_Image_In_Painting_powered_by_ClipSeg.ipynb</span></a></li>
				<li>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven <span class="No-Break">Generation: </span><a href="https://arxiv.org/pdf/2208.12242.pdf"><span class="No-Break">https://arxiv.org/pdf/2208.12242.pdf</span></a></li>
				<li><span class="No-Break">nlp-with-transformers/website: </span><a href="https://github.com/nlp-with-transformers/website"><span class="No-Break">https://github.com/nlp-with-transformers/website</span></a></li>
				<li>Chain-of-Thought Prompting Elicits Reasoning in Large Language <span class="No-Break">Models: </span><a href="https://arxiv.org/pdf/2201.11903.pdf"><span class="No-Break">https://arxiv.org/pdf/2201.11903.pdf</span></a></li>
				<li>Hugging <span class="No-Break">Face: </span><a href="https://huggingface.co/datasets/Anthropic/hh-rlhf"><span class="No-Break">https://huggingface.co/datasets/Anthropic/hh-rlhf</span></a></li>
				<li>Prefix-Tuning: Optimizing Continuous Prompts for <span class="No-Break">Generation: </span><a href="https://arxiv.org/pdf/2101.00190.pdf"><span class="No-Break">https://arxiv.org/pdf/2101.00190.pdf</span></a></li>
				<li><span class="No-Break">huggingface/notebooks: </span><a href="https://github.com/huggingface/notebooks/blob/main/sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py"><span class="No-Break">https://github.com/huggingface/notebooks/blob/main/sagemaker/24_train_bloom_peft_lora/scripts/run_clm.py</span></a></li>
				<li>The Power of Scale for Parameter-Efficient Prompt <span class="No-Break">Tuning: </span><a href="https://aclanthology.org/2021.emnlp-main.243.pdf"><span class="No-Break">https://aclanthology.org/2021.emnlp-main.243.pdf</span></a></li>
				<li><a href="https://arxiv.org/pdf/1902.00751.pdhttps://arxiv.org/pdf/1902.00751.pd"><span class="No-Break">https://arxiv.org/pdf/1902.00751.pdhttps://arxiv.org/pdf/1902.00751.pd</span></a></li>
			</ol>
		</div>
	</body></html>