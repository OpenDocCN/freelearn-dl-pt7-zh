<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p><strong>Convolutional Neural Networks</strong> (<strong>CNNs</strong>), or <strong>ConvNets</strong>, are a special class of feedforward networks; they are primarily used for computer vision tasks, but have also been adapted to other domains with unstructured data, such as natural language processing. As they are feedforward networks, they are very similar to the simple networks that we just learned about; information passes through them in one direction, and they are made up of layers, weights, and biases. </p>
<p>CNNs are the image recognition methods used by Facebook for image tagging, Amazon for product recommendations, and by self-driving cars for recognizing objects in their field of vision. <span>In this chapter, we'll discuss the functions that make CNNs different from standard feedforward networks, and then jump into some examples of how to apply them to a variety of tasks. </span></p>
<p>In this chapter, we will be covering the following topics:</p>
<ul>
<li>Convolutions</li>
<li>Pooling layers</li>
<li>Fully formed convolutional neural networks</li>
<li>Convolutional neural networks for image tagging</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Overview of CNNs</h1>
                </header>
            
            <article>
                
<p>CNNs are one of the most influential classes of networks in the history of deep learning. Invented by Yann LeCun (now head of <strong>Facebook</strong> <strong>Artificial Intelligence Research</strong>), CNNs really came into their own in 2012, with the introduction of deep Convolutional Neural Networks by Alex Krizhevsky.</p>
<p>Plain old neural networks don't scale well to images; CNNs adapt regular old feedforward neural networks by adding one or more convolutional layers as the input layer to the network. These convolutions are specifically designed to take in two-dimensional input, such as images or even sound, as illustrated in the following diagram: </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-994 image-border" src="Images/1fb7dbc0-ba45-4829-a8ca-222b42a22d13.png" style="width:162.50em;height:55.75em;" width="1950" height="669"/></div>
<p>As you ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolutional layers</h1>
                </header>
            
            <article>
                
<p>Suppose we have an image recognition program to identify objects in an image, such as the example we referred to previously. Now i<span>magine how hard it would be to try and classify an image with a standard feedforward network; each pixel in the image would be a feature that would have to be sent through the network with its own set of parameters. Our parameter space would be quite large, and we could likely run out of computing power! Images, which in technical terms are just high-dimensional vectors, require some special treatment. </span></p>
<p>What would happen if we were to try and accomplish this task with a basic feedforward network? Let's recall that basic feedforward networks operate on top of vector spaces. <span>We start with an image, which is made up of independent pixels. Let's say our image is 32 pixels by 32 pixels; the input to our convolutional layer would be <em>32 x 32 x 3</em>, where <em>3</em> represent the RGB color scale of images. To translate this to vector space, we'd end up with a <em>3072 x 1</em> vector to represent the entire image:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1364 image-border" src="Images/a32d2a13-e0b9-4a6a-aaf0-49bad075facb.png" style="width:32.67em;height:5.33em;" width="962" height="158"/></div>
<p>Let's also say our network has <strong>10</strong> neuron units; using the simple feedforward model, our weight matrix alone would have <strong>30,720</strong> learnable parameters. CNNs mitigate this problem, as well as others, with their convolutional layers.</p>
<p>Convolutional layers have four parameters that we have to define when we create a CNN:</p>
<ul>
<li>The number of filters, <em>K</em></li>
<li>The size of each filter, <em>F</em></li>
<li>The stride, <em>S</em></li>
<li>The amount of zero padding, <em>P</em></li>
</ul>
<p>In the next section, we'll walk through each of these and look at how they play into the structure of the network.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Layer parameters and structure</h1>
                </header>
            
            <article>
                
<p>Convolutional layers allow us to preserve the original image dimensions, thereby improving our ability to learn features and reduce our computational load. They do this by utilizing something called a <strong>filter, </strong>which slides across the image, learning features by computing dot products. For example, a typical filter on the first layer of a CNN might have size <em>5 x 5 x 3</em> (namely, <em>5</em> pixels width and height, and <em>3</em> because images have a depth of three colors, RGB).</p>
<p>Mathematically, it's done as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/0c5e4230-d491-4f74-864f-ed054c9ad84d.png" style="width:5.50em;height:1.75em;" width="660" height="210"/></div>
<p>Here, <em>w</em> represents our filter, but it also represents our learnable weights. We take a transpose of our input filter, ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Pooling layers</h1>
                </header>
            
            <article>
                
<p>Convolutional layers are often intertwined with <strong>pooling layers</strong>, which down sample the output of the previous convolutional layer in order to decrease the amount of parameters we need to compute. A particular form of these layers, <strong>max pooling layers</strong>, has become the most widely used variant. In general terms, max pooling layers tell us if a feature was present in the region, the previous convolutional layer was looking at; it looks for the most significant value in a particular region (the maximum value), and utilizes that value as a representation of the region, as shown as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1369 image-border" src="Images/bafe3b38-c671-4e75-be57-9325252498a3.png" style="width:35.58em;height:16.25em;" width="972" height="443"/></div>
<p>Max pooling layers help subsequent convolutional layers focus on larger sections of the data, providing abstractions of the  that help both reduce overfitting and the amount of hyperparameters that we have to learn, ultimately reducing our computational cost. This form of automatic feature selection also helps prevent overfitting by preventing the network from focusing on too-specific areas of the image. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Fully connected layers</h1>
                </header>
            
            <article>
                
<p>The <strong>fully connected layer</strong> of a CNN works in the same manner as that of a vanilla feedforward network. This layer maps the outputs extracted from the image to the outputs that we desire from the network, such as a label for an image:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-995 image-border" src="Images/85b80730-0f00-4d28-b8c2-df0383730a18.png" style="width:145.17em;height:82.33em;" width="1742" height="988"/></div>
<p>In the preceding diagram, our inputs are represented by the blue nodes, which are fed into the first convolutional layer, A. We then have a max pooling layer, a second convolutional layer, and finally the fully connected layer, which transforms our output into human– readable output. As with vanilla feedforward networks, we typically use a cross-entropy loss function for classification tasks. ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The training process</h1>
                </header>
            
            <article>
                
<p><span>When we connect convolutional layers, a hyperparameter known as the <strong>receptive field</strong> or </span><strong>filter size</strong><span> prevents us from having to connect the unit to the entire input, but rather focuses on learning a particular feature. Our convolutional layers typically learn features from simple to complex. </span><span>The first layer typically learns low-level features, the next layer learns mid-level features, and the last convolutional layer learns high-level features. One of the beautiful features of this is that we do not explicitly tell the network to learn different features at these various levels; it learns to differentiate its task in this manner through the training process:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-large wp-image-193 image-border" src="Images/0da64743-509a-4be8-9549-0a7674c648a7.png" style="width:52.58em;height:25.08em;" width="1024" height="489"/></div>
<p>As we pass through this process, our network will develop a two-dimensional <strong>activation map</strong> to track the response of that particular filter at a given position. The network will learn to keep filters that activate when they reach an edge of a shape, or perhaps the color of an object. Sometimes, these will be lines and smaller pieces of the puzzle, or perhaps a filter will learn entire subsections of the image, maybe the horses' ears, in the preceding diagram. Each filter for a specific convolutional layer leaves us with individual activation maps. If we have six filters for the image, we would have six activation maps, each focusing on something different within the image: </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-996 image-border" src="Images/eb448261-bb5d-4ba1-baf7-0be7192fabde.png" style="width:13.25em;height:20.75em;" width="637" height="1006"/></div>
<p class="mce-root CDPAlignLeft CDPAlign">Our convolutional layers then become a sequence of stacked individual layers, interspersed by ReLU activation functions. We typically use ReLU or Leaky ReLU here to increase the training speed: </p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-997 image-border" src="Images/d584ec46-dcf8-44f0-a354-6530ad4f89b8.png" style="width:23.58em;height:14.67em;" width="1493" height="925"/></div>
<p>Between these convolutional layers, we add in our max pooling layers. The output of these combined convolutional + max pooling layers will be sent to a fully connected layer, which will contain our transformation for classification and other tasks. Once we reach the end of a forward pass, the error is backpropagated through the network in the same manner as vanilla feedforward networks. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CNNs for image tagging</h1>
                </header>
            
            <article>
                
<p>Let's work on putting our new knowledge of CNNs to the test. We're going to work through one of the most popular tasks for CNNs: image classification. </p>
<p>In an image classification task, our horse looks at a given image and determines the probability that a certain object is an image. <span>In the following example, the image is 248 pixels wide, 400 pixels tall, and has three color channels: <strong>red</strong>, <strong>green</strong>, and <strong>blue</strong> (<strong>RGB</strong>). Therefore, the image consists of <em>248 x 400 x 3</em> numbers, or a total of 2,97, 600 numbers. Our job is to turn these numbers into a single classified label;</span> is this horse<em>?</em></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-large wp-image-192 image-border" src="Images/2e8f3d14-79ff-4a59-906d-b3c7be0c51ff.png" style="width:52.17em;height:27.75em;" width="1024" height="544"/></div>
<p>While this might seem a simple task for ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>CNNs have been seminal in solving many computer vision tasks. In this chapter, we learned about how these networks differ from our basic feedforward networks, what their structures are, and how we can utilize them. CNNs are primarily used for computer vision tasks, although they can be adapted for use in other unstructured domains, such as natural language processing and audio signal processing.</p>
<p>CNNs are made up of convolutional layers interspersed with pooling layers,<strong> </strong>all of which output to a fully connected layer<strong>.</strong> CNNs iterate over images using filters. Filters have a size and a stride, which is how quickly they iterate over an input image. Input consistency can be better guaranteed by utilizing the zero padding technique.</p>
<p>In the next chapter, we'll learn about another important class of networks, called <strong>Recurrent Neural Networks</strong>. </p>


            </article>

            
        </section>
    </div>



  </body></html>