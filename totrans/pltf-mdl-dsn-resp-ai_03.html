<html><head></head><body>
		<div id="_idContainer060">
			<h1 id="_idParaDest-67" class="chapter-nu ber"><a id="_idTextAnchor066"/>3</h1>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/>Regulations and Policies Surrounding Trustworthy AI</h1>
			<p>This chapter provides an outline of the regulations and laws that have been passed in various countries that relate to the adoption of ethical AI practices in organizations and companies developing large-scale AI solutions. The primary objective of this chapter is to enable you to understand the main principles of Responsible AI so that you are aware of the legal implications if you fail to abide by the regulations. You will also be made aware of the different levels of risk associated with AI applications and what it means to accept or ban AI systems on the basis of their potential risk. Furthermore, you will learn about the commonly followed and enforced initiatives, actions, and guidelines that remove bias against different minority groups in populations. In addition, this chapter also studies the problems and roadblocks faced by governments when designing fair ML solutions and provides recommended actions for building large-scale trustworthy <span class="No-Break">AI solutions.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Regulations and enforcements passed by <span class="No-Break">individual nations</span></li>
				<li>Special regulations for children and <span class="No-Break">minority groups</span></li>
				<li>Next steps for <span class="No-Break">trustworthy AI</span></li>
			</ul>
			<h1 id="_idParaDest-69">Regulations and enforcements<a id="_idTextAnchor068"/> under different authorities</h1>
			<p>Regulating AI is still a nascent<a id="_idIndexMarker342"/> area. Many countries and<a id="_idIndexMarker343"/> organizations have come up with proposals for how to regulate and enforce laws and rules related to AI adoption in various industries. In this section, we will discuss the regulations put forward by different authorities to enforce the unbiased implementation of <span class="No-Break">AI systems<a id="_idTextAnchor069"/>.</span></p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor070"/>Regulations in the European Union</h2>
			<p>On<a id="_idIndexMarker344"/> October 20, 2020, the European Parliament of the <strong class="bold">European Union</strong> (<strong class="bold">EU</strong>) adopted three resolutions primarily aimed at <span class="No-Break">the following:</span></p>
			<ul>
				<li>Developing<a id="_idIndexMarker345"/> a structured foundational system for the ethical aspects of <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>), robotics, automation, and other transformational changes that impact the everyday lives of <span class="No-Break">ordinary people</span></li>
				<li>Formulating an accountable civil authority that will judge the aforementioned impact and decide on <span class="No-Break">punitive action</span></li>
				<li>Strategizing a technique to respond to the challenges posed by AI systems regarding intellectual <span class="No-Break">property rights</span></li>
			</ul>
			<p>The first resolution gave the EU the opportunity to highlight the essence of a human-centric and human-created AI approach. The second resolution prompted the EU to take the initiative to address the risks created by AI-based systems by introducing proportionate and flexible<a id="_idIndexMarker346"/> rules for different types of risks. These risks were classified with the <a id="_idIndexMarker347"/>following <a id="_idIndexMarker348"/>labels: <strong class="bold">unacceptable risk</strong>, <strong class="bold">high risk</strong>, <strong class="bold">limited risk</strong>, and <strong class="bold">minimal risk</strong>. Furthermore, there were different liability rules according to the <a id="_idIndexMarker349"/>severity of <span class="No-Break">the risk.</span></p>
			<p>However, the last resolution, which <em class="italic">focuses on the intellectual property rights at stake in the development of AI technologies</em> (<a href="https://ai-regulation.com/news-eps-resolutions-on-ethical-framework-civil-liability-and-intellectual-property-rights-for-ai/">https://ai-regulation.com/news-eps-resolutions-on-ethical-framework-civil-liability-and-intellectual-property-rights-for-ai/</a>), still <span class="No-Break">remains unaddressed.</span></p>
			<p>These AI rules set forth in the draft regulation are harmonized (meaning that they apply throughout the EU) and will not have a direct effect on countries outside the EU, but they will have some extra-territorial impact. For instance, compliance with these rules by all countries making AI services available in the EU is mandatory, just as with the <strong class="bold">General Data Protection Regulation</strong> (<strong class="bold">GDPR</strong>). The EU’s AI regulation has an extra-territorial effect on <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Providers</strong> releasing AI-enabled products, solutions, and services to the EU market (irrespective of where the services originate from and who the providers of the <span class="No-Break">services are)</span></li>
				<li><strong class="bold">Customers</strong> of AI-enabled systems who use AI systems within <span class="No-Break">the EU</span></li>
				<li><strong class="bold">Providers and consumers</strong> of AI systems geographically positioned outside the EU who are consuming services that reside within <span class="No-Break">the EU</span></li>
			</ul>
			<p>The<a id="_idIndexMarker350"/> EU has placed special emphasis on the governance of AI systems. It has also stressed the importance of regulating the algorithm-driven systems used by consumers. This can address issues related to privacy, especially where biometric recognition systems are concerned. In essence, the objectives of the EU regulations are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Make sure that AI systems in the EU market are safe and respect existing laws on fundamental rights and wider <span class="No-Break">EU values</span></li>
				<li>Make sure the law is clear to help investment and innovation <span class="No-Break">in AI</span></li>
				<li>Make sure the laws that protect people’s rights and safety are enforced for <span class="No-Break">AI systems</span></li>
				<li>Make sure that there is a single market for safe and trustworthy AI applications so that people can <span class="No-Break">trust them</span></li>
			</ul>
			<p>These <a id="_idIndexMarker351"/>regulations provide a new legal framework for AI in the EU. This framework establishes rules regarding the development, market placement, and use of AI systems that follow a proportionate, risk-based approach with specific restrictions to protect humans from harm. The rules aim for an open, connected community that can benefit from the rewards offered by AI technology while protecting <span class="No-Break">citizens<a id="_idTextAnchor071"/>’ safety.</span></p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor072"/>Propositions/acts passed by other countries</h2>
			<p>Similar to the EU, other attempts to regulate AI have been made. The regulations passed/proposed by the US, Australia, and the <strong class="bold">Institute of Electrical and Electronics Engineers</strong> (<strong class="bold">IEEE</strong>) are <a id="_idIndexMarker352"/>noteworthy in <span class="No-Break">this regard.</span></p>
			<h3>AI regulation acts in the US</h3>
			<p>In the consumer <a id="_idIndexMarker353"/>finance context, to eliminate bias from AI algorithms, two main acts have been passed by the US government that have received attention: the <strong class="bold">Equal Credit Opportunity Act</strong> (<strong class="bold">ECOA</strong>) and <a id="_idIndexMarker354"/>the <strong class="bold">Fair Housing Act</strong> (<a href="https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/">https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/</a>). The ECOA <a id="_idIndexMarker355"/>laid down rules that prohibit creditors from issuing, sanctioning, or approving credit transactions from lenders who discriminate against race, color, religion, national origin, sex, marital status, and age. Even allowing discrimination against individuals who may have received income from public or government authorities or have legal employment rights is treated as a violation of the act. Likewise, the Fair Housing Act has issued prohibitory orders against any discrimination when properties are being sold or rented or when assigning mortgages. Both of these acts aim to<a id="_idIndexMarker356"/> ban <strong class="bold">disparate treatment</strong> and <strong class="bold">disparate impact</strong>, where <a id="_idIndexMarker357"/>bias emerges intentionally <span class="No-Break">or unintentionally.</span></p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B18681_03_001.jpg" alt="Figure 3.1 – Graphs from the Zillow Housing Aspirations Report, showing housing discrimination observed in young adults and black communities"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Graphs from the Zillow Housing Aspirations Report, showing housing discrimination observed in young adults and black communities</p>
			<p>As demonstrated in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em>, this act seeks to eliminate discrimination against black, Hispanic, Asian, and other minority communities when searching for housing, where white people are likely to be favored by housing authorities and owners as potential renters over equally qualified <span class="No-Break">minority groups.</span></p>
			<p>The <strong class="bold">Federal Trade Commission</strong> (<strong class="bold">FTC</strong>) has <a id="_idIndexMarker358"/>put forward a proposition for truth, fairness, and equity in any AI-based service that an organization wants to promote and sell to its customers. The FTC memo made it clear that the FTC’s authority is to be used under Section 5 of the FTC act. This, along with the implementation of the <strong class="bold">Fair Credit Reporting Act</strong> (<strong class="bold">FCRA</strong>) and<a id="_idIndexMarker359"/> the ECOA, should curb the application of <span class="No-Break">biased algorithms.</span></p>
			<p>FTC <a id="_idIndexMarker360"/>chair Rebecca Slaughter has been a prominent voice for economic justice, raising concerns on issues related to algorithm-based bias. The FTC has further warned that companies could suffer severe punitive action, even prosecution, under the ECOA or the FCRA for biased and unfair predictions from AI systems. The most important guideline by the FTC is that organizations must stay transparent and accountable for the algorithms they develop and put <span class="No-Break">into practice.</span></p>
			<p>The need for an unbiased system and a defined risk management framework has been felt by other bodies, such as the<a id="_idIndexMarker361"/> US <strong class="bold">Department of Commerce</strong> (<strong class="bold">DoC</strong>). There was new momentum in the area of trustworthy AI with the passing of the National Defense Authorization Act in 2021. This led US<a id="_idIndexMarker362"/> Congress to have the <strong class="bold">National Institute of Standards and Technology</strong> (<strong class="bold">NIST</strong>) devise “<em class="italic">a voluntary risk management framework for trustworthy AI systems</em>.” This initiative triggered the development of <a id="_idIndexMarker363"/>the <strong class="bold">AI Risk Management Framework</strong> (<strong class="bold">AI RMF</strong>), drafting best practices for organizations to control risks emerging from AI systems. It also helped them to select the right trade-off between fairness and accuracy, privacy and accuracy, and privacy and fairness. The DoC has also established the <strong class="bold">National Artificial Intelligence Advisory Committee</strong> (<strong class="bold">NAIAC</strong>) as <a id="_idIndexMarker364"/>part of the National AI Initiative Act of 2020. The primary objective of this committee is to study the<a id="_idIndexMarker365"/> current state of AI in the US, evaluate the competency and state of the science around AI, and accordingly, offer recommendations to increase opportunities for historically <span class="No-Break">underrepresented populations.</span></p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B18681_03_002.jpg" alt="Figure 3.2 – Healthcare discrimination observed in black adults"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Healthcare discrimination observed in black adults</p>
			<p>Time and <a id="_idIndexMarker366"/>again, limiting societal benefits only to privileged groups has had a negative impact on society. As illustrated in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em>, restricting opportunities for minority populations has not only impeded extending social facilities to black adults but also resulted in denying them adequate healthcare facilities and the right to receive equal treatment facilities. The NAIAC has become more vigilant in curbing such discrimination to promote equality and justice. In the absence of action to promote fairness and equity, our society will continue to create biased real-world datasets that generate biased ML models. The absence of equity and justice in society leads to more discrimination against minority groups, which limits their opportunities for quality education, healthcare, career advancement, and more. The previous figure (taken from <a href="https://www.kff.org/report-section/kff-the-undefeated-survey-on-race-and-health-main-findings/">https://www.kff.org/report-section/kff-the-undefeated-survey-on-race-and-health-main-findings/</a>) shows the percentage by which, in the US, the black community falls short of having proper access to quality services as compared to the <span class="No-Break">white community.</span></p>
			<p>To <a id="_idIndexMarker367"/>foster trade and technology between the EU and the US, the EU-US <strong class="bold">Trade and Technology Council</strong> (<strong class="bold">TTC</strong>) released an <strong class="bold">Inaugural Joint Statement</strong> to <a id="_idIndexMarker368"/>aid in<a id="_idIndexMarker369"/> the development of new transformational systems that use AI and promote universal human rights. Without this kind of action, biased systems will foster a loss of respect and egalitarianism for the <span class="No-Break">common man.</span></p>
			<p>Other trustworthy AI initiatives have been advocated <a id="_idIndexMarker370"/>by the <strong class="bold">United Nations Educational, Scientific and Cultural Organization</strong> (<strong class="bold">UNESCO</strong>), the <strong class="bold">Organization for Economic Co-operation and Development</strong> (<strong class="bold">OECD</strong>), and<a id="_idIndexMarker371"/> the Council of Europe. The OECD has been the greatest supporter of the EU government in analyzing and measuring the socioeconomic impacts of AI technologies and applications. In addition, it actively engages with policy-makers and regulators to increase opportunities for underrepresented sectors, classify AI systems, and evaluate risk, fairness, transparency, safety, and accountability to converge AI practices consistently <span class="No-Break">across borders.</span></p>
			<p>The<a id="_idIndexMarker372"/> White House <strong class="bold">Office of Science and Technology Policy</strong> (<strong class="bold">OSTP</strong>), established by Congress in 1976, promulgated 10 principles for consideration in the regulatory and non-regulatory approaches to the advancement and application <span class="No-Break">of AI:</span></p>
			<ul>
				<li>Use best practices to build systems and frameworks that help to bolster public confidence, faith, <span class="No-Break">and belief.</span></li>
				<li>Ensure that the right education and tools are in place with regard to AI standards and technology so that people are more likely to participate and exchange their views on the real-world experience of large-scale, productionized <span class="No-Break">AI systems.</span></li>
				<li>Incorporate the best levels of scientific honesty, morality, and righteousness with regard to any kind of input, such as data that impacts AI <span class="No-Break">model predictions.</span></li>
				<li>Develop transparent risk assessment and risk management methodologies across business units in a collaborative manner in order to learn from each other’s ethical issues and take <span class="No-Break">proactive action.</span></li>
				<li>Predict the costs associated with the positives and negatives of the practical use of AI by <span class="No-Break">production systems.</span></li>
				<li>Analyze performance metrics and adopt dynamic learning methodologies to learn about and make changes to system behavior <span class="No-Break">and data.</span></li>
				<li>Evaluate and formulate the standards needed to promote fairness by removing any kind of bias and discrimination from <span class="No-Break">AI systems.</span></li>
				<li>Use <a id="_idIndexMarker373"/>transparency tools to gain public trust and guarantee the design of safe and <span class="No-Break">secure systems</span></li>
				<li>Establish regular checkpoints to ensure the confidentiality, integrity, and availability of unbiased AI data that can be used to build safe and <span class="No-Break">fair systems</span></li>
				<li>Support collaboration, partnership, and inter-departmental teamwork to ensure the consistency and predictability of <span class="No-Break">AI policies</span></li>
			</ul>
			<p>The US has been very proactive in investigating loopholes in current AI solutions and has come up with an initial proposal to curb discrimination. The <strong class="bold">Blueprint for an AI Bill of Rights</strong>, published <a id="_idIndexMarker374"/>by the White House in October 2022, is a stepping-stone toward protecting individuals and communities from threats caused by <span class="No-Break">AI-driven technologies.</span></p>
			<p>President Biden wanted to remove inequity from all decision-making processes by ensuring the incorporation of fairness principles that respect the civil rights of Americans and grant equality of opportunity and radical justice in the US. With his support, the OSTP has stated five principles that respect civil rights, civil liberties, and privacy. It will defend freedom of speech and voting and forbid discriminatory practices. By protecting the public’s private data across public and private sectors, the guiding principles seek to provide a structured framework that can be applied to automated systems. In an attempt to provide equality of opportunity in the areas of education, housing, credit, employment, healthcare, financial services, safety, social services, non-deceptive information about goods and services, and government benefits, the five basic <a id="_idIndexMarker375"/>principles have been laid out <span class="No-Break">as follows.</span></p>
			<h4>Safe and effective systems</h4>
			<p>The objective of this principle<a id="_idIndexMarker376"/> is to protect the public from unsafe or ineffective systems. There is to be an audit process to validate the risks assessed by domain experts and stakeholders from different communities. In addition, this principle lays down the best practices for risk mitigation and 24-hour monitoring for AI-based systems. By following rigorous, domain-specific standards, automated systems should be able to prevent unforeseen dangers and ensure <span class="No-Break">public safety.</span></p>
			<h4>Algorithmic discrimination protections</h4>
			<p>The objective of this <a id="_idIndexMarker377"/>principle is to proactively prevent discrimination by algorithms. This will not only prevent unjustified differential treatment relating to differences in race, sex (including pregnancy, medical conditions, intersex status, and sexual orientation), gender identity, religion, age, national origin, disability, veteran status, genetic information, and other demographic and social statuses, but also ensure continuous measures to provide equitable treatment <span class="No-Break">to everyone.</span></p>
			<h4>Data privacy</h4>
			<p>The objective of this<a id="_idIndexMarker378"/> principle is to ensure adequate built-in protection and safety standards during data collection, use, access, transfer, and deletion processes to avoid any violation of data privacy. This principle governs seeking appropriate permission (by designers and developers) from customers to fully respect users’ <span class="No-Break">privacy choices.</span></p>
			<h4>Notice and explanation</h4>
			<p>The objective of this <a id="_idIndexMarker379"/>principle is to provide transparent, authentic, and up-to-date documentation of the overall system functionality, with adequate descriptions of the individual system components. In addition, such systems should notify the public of any change in system functionality after calibrating the level of risk caused by <span class="No-Break">the changes.</span></p>
			<h4>Human alternatives and fallback</h4>
			<p>The objective of this principle is to<a id="_idIndexMarker380"/> ensure quick fallback to human alternatives when automated systems fail. In such cases, public safety is of paramount importance, so a fallback can guarantee equitable, accessible, and effective functionality without any <span class="No-Break">harmful effects.</span></p>
			<h3>AI regulation acts in India</h3>
			<p>The <a id="_idIndexMarker381"/>Responsible AI Proposal given by India’s <strong class="bold">National Association of Software and Service Companies (NASSCOM)</strong> has introduced a concept of licensing (called <strong class="bold">Responsible AI Licenses (RAIL)</strong>) in order to protect developers’ AI source code against malicious use. Furthermore, the licenses provide a way to restrict the misuse of the code for inequity, bias, or societal harm. In addition to licensing, NASSCOM has stressed the communication of risks both internally and externally through dashboards, where different stakeholders (such as senior leadership, legal experts, data scientists, and DevOps) can collaborate on Responsible AI metrics and regulatory compliance. Transparency through visualization tools, dashboards, internal audits, and proactive communication to users about privacy risks is the top priority for NASSCOM to curb the ethical and societal risks <span class="No-Break">of AI.</span></p>
			<p>NASSCOM has also highlighted the following aspects of dataset verification and validation in the creation of <span class="No-Break">unbiased models:</span></p>
			<ul>
				<li>Identify the principal elements, their importance, and the learning parameters in the model that govern the <span class="No-Break">model’s outcomes.</span></li>
				<li>Assess the ability of the model to predict results correctly for all demographics through a fair, representative dataset, without skew in any of <span class="No-Break">the features.</span></li>
				<li>Evaluate the target variable and any links with present data features that can have a positive or <span class="No-Break">negative impact.</span></li>
				<li>Ensure legal compliance. Collect data with permission and organize timely reviews/audits as mentioned in GDPR. This includes having <a id="_idIndexMarker382"/>an agreement for processing data that states the possible uses of the data that <span class="No-Break">is collected.</span></li>
				<li>Ensure bare-minimum data collection to avoid the unauthorized use of data by third parties, thereby minimizing <span class="No-Break">privacy attacks.</span></li>
				<li>Adhere to special guidelines while collecting unique data. With regard to regulations such as the EU’s GDPR, unique or special datasets containing data such as political beliefs, sexual orientation, and religious beliefs should not result in unintended use without <span class="No-Break">prior consent.</span></li>
				<li>Prevent<a id="_idIndexMarker383"/> the unauthorized use of data by tracking the data management life cycle and maintaining <span class="No-Break">up-to-date information.</span></li>
			</ul>
			<p>NASSCOM has come up with best practices for data collection, training research personnel, making evaluation guidelines for error estimation, and the peer review of datasets. It has emphasized the importance of mitigating the harm caused by discrimination during the preprocessing, in-processing, and postprocessing stages of the algorithmic pipeline. Local, global, attribute-based, and causal explanations (discussed in <a href="B18681_09.xhtml#_idTextAnchor198"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Model Explainability</em>) have been recommended to assess fairness and transparency. Recommendations put forward by EU regulations have been adopted by NASSCOM. NASSCOM also provides best practices for deploying production-grade applications, including <span class="No-Break">the following:</span></p>
			<ul>
				<li>Use counterfactual analysis to understand the impact of a feature that may be surpassed or perturbed for a specific data point, due to the dominance of other features. This helps to analyze how features affect prediction results, such as a loan application being rejected by a model that would have accepted the application if the applicant’s income was 10,000 <span class="No-Break">USD higher.</span></li>
				<li>Use a robust and secure deployment workflow to handle peak load and bursts without causing any downtime for production systems. A well-managed security pipeline should sustain an AI system in <span class="No-Break">extreme environments.</span></li>
				<li>Use <strong class="bold">Continuous Integration and Continuous Deployment</strong> (<strong class="bold">CI/CD</strong>) pipelines to monitor <a id="_idIndexMarker384"/>and retrain models based on data drift. The objective is to establish feedback channels and put an escalation chain in place that ensures trustworthy <a id="_idIndexMarker385"/>and scalable deployment <span class="No-Break">in practice.</span></li>
			</ul>
			<h3>AI regulation acts in Australia</h3>
			<p>Australia has also come up <a id="_idIndexMarker386"/>with principles and practices for ethical AI regulation, not only to build public trust in AI-enabled products and organizations but also to drive customer loyalty to AI-enabled services. Under Australian regulations, AI outcomes should be geared toward the <span class="No-Break">following objectives:</span></p>
			<ul>
				<li><strong class="bold">Human, societal, and environmental well-being</strong>: AI-powered decision systems and architectures should act for the benefit of individuals, society, and the environment. They should empower human beings, providing them with enough data to make informed decisions. Furthermore, overall societal and environmental well-being cannot be achieved by solely depending on machines. Hence, there should be support for proper review mechanisms, such as human-in-the-loop, human-on-the-loop, and human-in-command approaches. These defined processes involve the optimization of the entire ML process, with active feedback from human beings. Using human intervention methods, tasks such as annotation, active learning, transfer learning, and step-by-step optimization become easier as machines and humans work together effectively <span class="No-Break">and collaboratively.</span></li>
				<li><strong class="bold">Human-centered values</strong>: AI predictions should respect human rights, morals, diversity, and the autonomy <a id="_idIndexMarker387"/>of individuals. They should benefit all human beings, including future generations, and other living beings. Design choices and parameters should have sustainable and environmentally friendly techniques in place to carefully evaluate and certify that AI systems are considering all potential <span class="No-Break">harmful effects.</span></li>
				<li><strong class="bold">Fairness</strong>: AI-/ML-based systems should be designed to be inclusive and accessible for individuals with disabilities. The accessibility options should not unfairly discriminate against individuals, communities, or groups. This is a key objective to prevent unfair and biased systems that could lead to several negative effects, from disrespecting and disregarding vulnerable groups to aggravating prejudice and discrimination. As is evident from <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.3</em> (sourced from <a href="https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/">https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/</a>), four major face recognition algorithms demonstrated poor performance on darker-skinned women, with error rates shooting higher than<a id="_idIndexMarker388"/> 34% than for lighter-skinned men. The objective of introducing fairness into AI/ML algorithms is not only to remove bias in the recognition of skin tones or gender but also to foster diversity by ensuring <a id="_idIndexMarker389"/>accessibility for all, irrespective <span class="No-Break">of attributes.</span></li>
			</ul>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B18681_03_003.jpg" alt="Figure 3.3 – Lower accuracy for darker female and male populations from face recognition technology applications"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Lower accuracy for darker female and male populations from face recognition technology applications</p>
			<ul>
				<li><strong class="bold">Privacy protection and security</strong>: AI systems should have strong security policies<a id="_idIndexMarker390"/> built in to protect any individual’s privacy rights. There should be a strong emphasis on all kinds of data protection, from data ingestion to building ML models, to ensure the security of data. In addition to privacy and data protection, ample data governance mechanisms must also be ensured. Lineage and data governance have big roles to play as far as quality, integrity, and legitimate access to data <span class="No-Break">are concerned.</span></li>
				<li><strong class="bold">Reliability and safety</strong>: AI systems should not deviate from their stated goals and should reliably operate in accordance with their intended purpose and with sufficient resilience and security. They need to be safe, accurate, reliable, and reproducible. In addition, they should be designed with a fallback plan so that if they break the <strong class="bold">Service-Level Agreement </strong>(<strong class="bold">SLA</strong>) or exhibit a failure in their operational routine, they<a id="_idIndexMarker391"/> can be restored to a previous state (based on defined checkpoints). Such restore activities should also be reliable and safe to reduce and prevent <span class="No-Break">unintentional harm.</span></li>
				<li><strong class="bold">Transparency and explainability</strong>: AI systems should be transparent, responsible, and easily understood by people. It should be easy for people to recognize the cause of the outcomes of any model. Transparent data and AI models should be traceable so it is possible to deduce present and past outcomes, and causes should be explained to concerned stakeholders in simple language. This is one of the major requirements, not only for businesses but also for individuals, to make people aware of the system’s capabilities <span class="No-Break">and limitations.</span></li>
				<li><strong class="bold">Contestability</strong>: When the outcome of a predictive ML model/AI system significantly influences society by impacting an individual, community, group, or environment, there should be a timely auditing process. The auditing process provides a mechanism to analyze model results in terms of fairness. People should even be allowed to challenge the use or outcomes of an <span class="No-Break">AI system.</span></li>
				<li><strong class="bold">Accountability</strong>: The people responsible for the different phases of the ML model should be identifiable and accountable for the outcomes of the AI systems, and human oversight of such AI systems should be provided. The auditability and assessment of algorithms, data, and design processes play a key role therein, especially in <span class="No-Break">critical applications.</span></li>
			</ul>
			<p>The same ethical <a id="_idIndexMarker392"/>principles (including privacy and fairness) were put forward by NITI Aayog in India in 2021. Their proposal focuses on the safety and reliability of AI systems through <span class="No-Break">continuous monitoring.</span></p>
			<h3>IEEE AI regulation guidelines</h3>
			<p>The <a id="_idIndexMarker393"/>IEEE’s <em class="italic">Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems</em> document, first released in 2019, lays out eight key aspects related to ethical issues of AI and mitigation strategies. These areas of focus, as depicted in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.4,</em> include principles for creating a strong foundation for Ethically Aligned Design frameworks <span class="No-Break">and platforms:</span></p>
			<ul>
				<li><span class="No-Break">Sustainable development</span></li>
				<li>Personal data rights and agency over <span class="No-Break">digital identity</span></li>
				<li>Legal frameworks <span class="No-Break">for accountability</span></li>
				<li>Policies for education <span class="No-Break">and awareness</span></li>
			</ul>
			<p>The system<a id="_idIndexMarker394"/> serves as a guideline to increase human trust in data-driven <span class="No-Break">AI systems:</span></p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B18681_03_004.jpg" alt="Figure 3.4 – The  fundamental pillars of Responsible AI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – The  fundamental pillars of Responsible AI</p>
			<p>We have learned about the fundamental building blocks of an ethical system. Now let’s discuss how collaboration among different organizations can enable us to leverage the power of AI/ML to<a id="_idTextAnchor073"/> <a id="_idTextAnchor074"/>address the needs of entire <span class="No-Break">population segments.</span></p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor075"/>Special regulations for children and minority groups</h1>
			<p>AI-based <a id="_idIndexMarker395"/>predictive <a id="_idIndexMarker396"/>analytics and profiling have demonstrated many limitations pertaining to minority groups’ opportunities and development. To promote social welfare facilities (such as improving facilities used by children), there has been extensive statistical analysis of studies related to different cases in the past, where data has been sourced from different databases, including public welfare benefits data, medical records, and judicial information. Studies and detailed investigations have revealed that there have been wide variations/inconsistencies in model input data, the data recorded is not systematic, and validation criteria have been <span class="No-Break">applied inconsistently.</span></p>
			<p>Along with the draft regulation (from the EU Commission) and regulatory proposals set forth by individual countries, regulatory bodies have come forward to translate the proposals for the digital space. The respective bodies have been instrumental in proposing a new regulatory framework for machinery products and transformational digital techniques being applied in the fields of robotics, IoT, IoMT, blockchain, VR, military, autonomous vehicles, and others. The key goal is to enable trust between AI providers and users by addressing the risks posed by <span class="No-Break">such systems.</span></p>
			<p><strong class="bold">Chatbots</strong> are <a id="_idIndexMarker397"/>one such practical example of real-world AI systems. In some cases, they have led to increased risk for children with developmental disabilities because chatbots don’t recognize the children’s disabilities, and hence do not recognize appeals for help. Neither can a chatbot provide adequate advice to children. One instance was recorded in 2018 and was shared with the public by the BBC (<a href="https://www.bbc.com/news/technology-46507900">https://www.bbc.com/news/technology-46507900</a>). The report demonstrates how two mental health chatbots failed to capture children’s reports of sexual abuse. Even though the chatbot had been designed with children in mind and was considered safe for children, its confused response posed additional challenges to <span class="No-Break">young users.</span></p>
			<p>This is<a id="_idIndexMarker398"/> not the only problem<a id="_idIndexMarker399"/> with chatbots. The privacy threats from chatbots include spoofing (impersonating someone else), tampering with data, data theft, and vulnerability to cyberattacks. Security and fairness are primary considerations of AI ethics, and when chatbots are found to enforce bias by responding to a reply based on the best-matching keywords, it has raised concern among ethicists. Ideally, chatbots should not respond with biased responses when the input does not match exactly with the words already in its store, and instead, it should learn and update its dictionary with the input. Hence, it has become the top priority among AI ethicists to educate social organizations as well as producers and companies about the side effects of AI solutions. Furthermore, child rights advocates have raised questions about data retention policies in chatbots and issues related to parental consent, because some chatbots rely on stored voice recordings to continuously learn and respond. Historical data has been found to reinforce systemic bias and introduce discrimination among children. Hence, it is essential for experts to validate individual profiles and stop individuals from being impacted by bias from AI <span class="No-Break">system proxies.</span></p>
			<p>Privacy advocates have become increasingly concerned with data privacy and have cautiously taken steps to warn about government mass surveillance activities, law enforcement, and examination activities, as well as other inquiry-based tools. Private information about individuals cannot be collected without consent, as this information can be utilized to identify, segment, investigate, and suppress communities. </p>
			<p>Lack of knowledge of the collection process of biometric data, the people involved in the collection process, the storage methodology, and the application of techniques risks the trust that people have in AI. Some instances demonstrate the availability of limited data for minority groups, and these instances have raised questions related to fairness. Success rates for facial recognition detection software have been questioned by AI ethicists, especially for groups such as children and women of color. Biased training datasets have <a id="_idIndexMarker400"/>been known to<a id="_idIndexMarker401"/> promote social bias and lead to discrimina<a id="_idTextAnchor076"/>tion or further disadvantages for <span class="No-Break">minority communities.</span></p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor077"/>Promoting equality for minority groups</h2>
			<p>It is <a id="_idIndexMarker402"/>essential to stop bias from flowing into AI systems. Bias prevents a system from supporting alternative developmental trajectories for minority groups (even children). Trustworthy, equal-opportunity-oriented AI-enabled systems should be free of stereotypes and enable possibilities for every child, including girls and LGBT children. Some actions centered around the development and inclusion of children, ethnic minorities, and LGBT people would be <span class="No-Break">the following:</span></p>
			<ul>
				<li>Prioritize fairness and non-discrimination among all minorities who are vulnerable, such as children, the black community, and <span class="No-Break">LGBT people.</span></li>
				<li>Safeguard children’s data and privacy rights to ensure <span class="No-Break">their safety.</span></li>
				<li>Increase transparency, explainability, and accountability for children so that the moral rights and dignity of children <span class="No-Break">are preserved.</span></li>
				<li>Empower governments and businesses with knowledge of AI and <span class="No-Break">children’s rights.</span></li>
				<li>Prepare minority groups by educating them so that they are aware of the harmful impacts of biased AI solutions when deployed <span class="No-Break">at scale.</span></li>
				<li>Enable an AI-driven ecosystem to support and nurture individual talent to fulfill its full potential. This necessitates the elimination of any prejudicial bias against children, or certain groups of children, that directly or indirectly leads to <span class="No-Break">biased treatment.</span></li>
			</ul>
			<p>AI must be for everyone, including all<a id="_idTextAnchor078"/> children; also, privacy must be ensured in an <span class="No-Break">AI world.</span></p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor079"/>Educational initiatives</h2>
			<p>The<a id="_idIndexMarker403"/> initiatives that have been organized to educate institutions, educational agencies, and data professionals are the first step toward making people aware of the existence of AI systems and their risk <span class="No-Break">levels (</span><a href="https://www.orrick.com/en/Insights/2021/07/AI-Tips-10-Steps-to-Future-Proof-Your-Artificial-Intelligence-Regulatory-Strategy"><span class="No-Break">https://www.orrick.com/en/Insights/2021/07/AI-Tips-10-Steps-to-Future-Proof-Your-Artificial-Intelligence-Regulatory-Strategy</span></a><span class="No-Break">).</span></p>
			<p>One of the primary purposes of such educational initiatives is to educate AI practitioners on how to identify high, medium, and low risks and take appropriate action to evaluate an AI model’s <span class="No-Break">predictive <a id="_idTextAnchor080"/>competence.</span></p>
			<h3>Identifying and evaluating the risk levels of systems</h3>
			<p>High-risk AI systems<a id="_idIndexMarker404"/> should be banned for public use, and their usage can be limited <a id="_idIndexMarker405"/>for research and future improvement. AI-enabled systems that should be banned include those that do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Manipulate human behavior, opinions, or decisions through their design (such as their user interfaces), architectural frameworks, and AI models, which can lead to detrimental decisions <span class="No-Break">against individuals.</span></li>
				<li>Make predictions about an individual or a group based on protected attributes (such as gender <span class="No-Break">and ethnicity).</span></li>
				<li>Execute indiscriminate surveillance on all individuals without any differentiation with respect to protected attributes. General surveillance can lead to the monitoring of, spying on, or tracking of individuals, without receiving <span class="No-Break">user consent.</span></li>
			</ul>
			<p>In addition to putting best practices in place, we should be able to identify high-risk systems. Along with general public education, we need to educate experienced data professionals who are building large-scale ML systems. Only then will we be able to prevent the default prediction of, say, a “captain” as a man. To do that, we need to remove probabilistic bias, as illustrated in the following figure for a word embedding <span class="No-Break">model (</span><a href="https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html"><span class="No-Break">https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html</span></a><span class="No-Break">):</span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B18681_03_005.jpg" alt="Figure 3.5 – Higher probability of a captain being predicted as male"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Higher probability of a captain being predicted as male</p>
			<p>As you can see, the model predicts that the pronou<a id="_idTextAnchor081"/>n “he” is more likely to be appropriate for <span class="No-Break">this scenario.</span></p>
			<h3>Understanding high-risk systems</h3>
			<p>Systems classified as high-risk<a id="_idIndexMarker406"/> enable the government and authorities to take measures such as adding security techniques and conducting audits to identify the risks. AI-enabled systems that are identified as high-risk are those that do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Perform remote biometric identification of people in publicly <span class="No-Break">accessible spaces.</span></li>
				<li>Function as safety components in the operation of essential public <span class="No-Break">infrastructure networks.</span></li>
				<li>Prioritize the dispatching of emergency services, such as firefighters and <span class="No-Break">medical aid.</span></li>
				<li>Determine merit access and assign individuals to educational and vocational training institutions through admissions tests and other <span class="No-Break">merit assessments.</span></li>
				<li>Evaluate performance related to recruitment, promotion, rewards, and the termination of any kind of <span class="No-Break">contractual relationship.</span></li>
				<li>Evaluate the creditworthiness of people to certify their eligibility for benefits, grants, and services. This principle aims to avoid discrepancies in the creditworthiness of individuals by reducing, for example, the likelihood of higher mortgage rates being <a id="_idIndexMarker407"/>assigned to black and Hispanic people, as shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></li>
			</ul>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B18681_03_006.jpg" alt="Figure 3.6 – A biased system resulting in some minorities paying higher mortgage amounts"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – A biased system resulting in some minorities paying higher mortgage amounts</p>
			<ul>
				<li>Access individual risks and use predictions to be proactive in governing the risk. The central objective is to determine the authenticity of the information provided by a person. Transparent, reliable systems should have proper validation schemes incorporated to prevent, investigate, detect, or prosecute a <span class="No-Break">criminal offense.</span></li>
				<li>Function as prediction systems for crimes or events of social unrest to evaluate and assign resources for monitoring and surveillance as part of a detailed <span class="No-Break">criminal investigation.</span></li>
				<li>Examine and enable the processing of asylum and visa applications and associated complaints to determine the eligibility of individuals to enter specific countries/territories, such as <span class="No-Break">the EU.</span></li>
				<li>Aid legal systems, such as judges at court, except for <span class="No-Break">ancillary tasks.</span></li>
				<li>Access safety metrics before deploying drones in a warzone. One current example of this, at the time of writing, is in the Ukraine-Russia war, where Russian forces have tested new “swarm” drones. It comes under the umbrella of military ethics to evaluate the accuracy of these automated weapons, which are capable of tracking and shooting down <span class="No-Break">enemy aircraft.</span></li>
			</ul>
			<p>Such high-risk <a id="_idIndexMarker408"/>systems mandate that the training and testing datasets used in building the ML models work using authentic and unbiased representations. This is to allow sufficient transparency to understand their outputs and necessitates the inclusion of proper human-interface tools. In addition, these systems require consistent performance and system monitoring throughout the life cycle, along with ensuring high accuracy, robustness, and security. Non-high-risk systems should also be subjected to national testing and <span class="No-Break">piloting schemes.</span></p>
			<p>EU regulations have stressed the importance of ethical risk assessment procedures for AI systems where misuse can be foreseen and risks arise due to limited knowledge of application, control failures, and hazards associated with robotics applications. Risk assessment frameworks should apply the same acceptable risk standards to AI-based robotic applications as they do to tasks performed by a human. Standards set forth by EU regulations encourage public and stakeholder participation in the development of robots; some of the key design considerations for AI models and robots are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Forbid the design of AI systems and robots that would be responsible for societal harm. This ensures that AI equipment invented and manufactured by humans does not kill humans. Robots should be safe and designed to serve the purpose for which they are intended, without having any <span class="No-Break">hidden motivations.</span></li>
				<li>Hold responsible the designers or manufacturers of <span class="No-Break">AI-enabled robots.</span></li>
				<li>Incorporate privacy in every design that deals with data, AI, ML, and models as long as the system is available to <span class="No-Break">the public.</span></li>
				<li>Allow the safe and ethical use of robots, so as to prohibit their unethical behavior through <span class="No-Break">detailed accounting.</span></li>
				<li>Run educative training programs for AI robot designers, individual and organizational users, and industry and government bodies, as well as civil <span class="No-Break">society groups.</span></li>
				<li>Autonomous <a id="_idIndexMarker409"/>robots should be categorized, based on their behavior, as either <strong class="bold">illegal</strong>, <strong class="bold">immoral</strong>, <strong class="bold">permissible</strong>, or <strong class="bold">supererogatory</strong>, allowing them to <a id="_idTextAnchor082"/>be withdrawn from service according to <span class="No-Break">regulatory guidelines.</span></li>
			</ul>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor083"/>International AI initiatives and cooperative actions</h2>
			<p>International<a id="_idIndexMarker410"/> initiatives are undertaken to provide harmony and synchronization to educate people across different nations. The primary aim is to establish a universal standard to raise awareness of AI ethics and support research and technology collaboration. This enables organizations across different countries to join hands and contribute to establishing synergy in <span class="No-Break">the initiatives.</span></p>
			<p>Some of the international strategies on AI aim to resolve the differences between humans due to protected attributes. In an attempt to provide a consolidated framework for governments across nations, they strongly uphold a “<em class="italic">common vision for the Future of AI. G7 Common Vision is one of those, where the leaders of the G7 (Canada, France, Germany, Italy, Japan, the United Kingdom and the United States) met in 2018 and committed to 12 principles for AI</em>” (<a href="https://www.europarl.europa.eu/RegData/etudes/STUD/2020/634452/EPRS_STU(2020)634452_EN.pdf">https://www.europarl.europa.eu/RegData/etudes/STUD/2020/634452/EPRS_STU(2020)634452_EN.pdf</a>) by the <span class="No-Break">following means:</span></p>
			<ul>
				<li>Support, lead, and educate human-centric AI through commercial adoption in a systematic way that allows technology to be implemented in an ethical and <span class="No-Break">trustworthy manner.</span></li>
				<li>Allocate funds to encourage investment in R&amp;D so that, along with the efforts of researchers, architects, and data professionals, it generates public interest in <span class="No-Break">new technologies.</span></li>
			</ul>
			<p>Educate, train, and reskill individuals for the workforce to help them adapt to emerging <span class="No-Break">innovative technologies.</span></p>
			<p>Support campaigns and research studies to investigate the sources of bias for underrepresented groups, such that sufficient data for women and marginalized individuals is taken into consideration before <span class="No-Break">building models.</span></p>
			<p>The WHO’s ethics <a id="_idIndexMarker411"/>and governance team for AI (<a href="https://news.un.org/en/story/2021/06/1094902">https://news.un.org/en/story/2021/06/1094902</a>) for health has reported that AI in healthcare is already being used among rich nations to aid the speed and accuracy of diagnosis and screening for diseases. The use of AI in clinical care and improved health research and drug development should involve mandatory safety and privacy mechanisms that respect the confidentiality of patient data. Furthermore, the WHO stresses the need to develop systems that comply with regulations, have robust quality control measures, and provide accurate predictions for all individuals irrespective of age, gender, ethnicity, and features protected by human <span class="No-Break">rights codes.</span></p>
			<p>The UN General Assembly resolution UN A/RES/74/299 and the AI for Road Safety initiative have aligned to highlight the role of innovative automotive and digital technologies in reducing the number of global deaths and injuries from road traffic accidents by more than 50% by 2030 (<a href="https://news.un.org/en/story/2021/10/1102522">https://news.un.org/en/story/2021/10/1102522</a>). The objective is to concentrate on road safety data, regulatory frameworks, and the design of safer vehicles and road infrastructure to yield a much easier post-crash response. The International Telecommunication Union has also been working with AI for Road Safety to support continuous monitoring and automated driving safety data protocols that are aligned with ethical and <span class="No-Break">legal bodies.</span></p>
			<p>We even see ads promoted by Facebook (such as one in 2019) that are biased against gender, race, or religion. Facebook has been active “<em class="italic">in promoting job advertisements for roles in nursing or secretarial work [to women], whereas job ads for janitors and taxi drivers had been mostly shown to men</em>” (<a href="https://research.aimultiple.com/ai-bias/">https://research.aimultiple.com/ai-bias/</a>). These examples demonstrate how important it is for organizations to educate strategic, data, and architectural teams to abide by laws and <a id="_idIndexMarker412"/><a id="_idTextAnchor084"/>follow the right practices to build and release <span class="No-Break">ethical systems.</span></p>
			<h3>Implications of law enforcement</h3>
			<p>The <strong class="bold">European</strong><strong class="bold"><a id="_idIndexMarker413"/></strong><strong class="bold"> Commission for the Efficiency of Justice (CEPEJ)</strong> of the Council of Europe has been playing a leading role in setting ethical principles pertaining to the use of AI in judicial systems to guide policy-makers, legislators, and justice professionals (<a href="https://www.coe.int/en/web/cepej/cepej-european-ethical-charter-on-the-use-of-artificial-intelligence-ai-in-judicial-systems-and-their-environment">https://www.coe.int/en/web/cepej/cepej-european-ethical-charter-on-the-use-of-artificial-intelligence-ai-in-judicial-systems-and-their-environment</a>). The aim is to apply AI responsibly to improve efficiency and quality in a manner that complies with the fundamental rights guaranteed in the <strong class="bold">European Convention on Human Rights (ECHR)</strong> and the Council of Europe’s Convention on the Protection of <span class="No-Break">Personal Data.</span></p>
			<p>With regard to privacy standards put forward by the <strong class="bold">European Data Protection Board (EDPB),</strong> a violation <a id="_idIndexMarker414"/>of GDPR can result in a fine of 30 million euros or 6% of the organization's global annual turnover (whichever is higher). The same penalty is expected in the case of a breach of an unacceptable-risk AI system or infringement of the data governance provisions for high-risk AI systems. The maximum fines can increase based on the degree of infringement, such as releasing a prohibited AI application on the market. Other kinds of infringements related to issuing incorrect information would result in fines of up to 10 million euros, or 2% of the global annual turnover. The fines may be as high as 20 million euros, or 4% of the total worldwide annual turnover, for acts <span class="No-Break">of non-compliance.</span></p>
			<p>One of the foremost examples where regulations and ethical AI laws can be applied is in the usage of unethical AI tools such as the biased recruiting tool that was revealed by Amazon’s ML specialists. Amazon realized in 2015 that the AI-based tool they had in place to aid in their hiring process was not rating candidates in a gender-neutral fashion for software developer and other technical positions. The tool eventually turned out to make biased decisions against women. On further investigation, the organization found out that ML models were trained to vet applicants that were solely based on the observed patterns that were submitted in the company database over a 10-year period. As the tool penalized resumes containing words such as “women’s,” the organization decided to scrap <a id="_idIndexMarker415"/>this hiring tool and disband the team working on <span class="No-Break">the project.</span></p>
			<p>In these sections, we have learned about the AI regulations formulated by the EU and the acts and initiatives being undertaken by the US, Australia, and other international bodies. Now, let’s look at how organizations,<a id="_idTextAnchor085"/> companies, and AI researchers should build trustworthy <span class="No-Break">AI systems.</span></p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor086"/>Next steps for trustworthy AI</h1>
			<p>At the <a id="_idIndexMarker416"/>time of writing this book, more than 50 countries have adopted some form of AI regulation policy (<a href="https://oecd.ai/en/dashboards">https://oecd.ai/en/dashboards</a>). Still, like AI itself, forming a robust policy around AI is a difficult task, made more difficult because of the evolving nature of AI itself. However, there are some steps put forward by AI researchers and policy-makers that can help provide trustworthy AI <span class="No-Break">to consumers.</span></p>
			<p>One of the main steps for companies is crafting organization-wide policies and procedures to create a compliance-by-design program. Such programs help to promote the transparency and explainability of systems and support innovation at the same time. Furthermore, they should establish a regular audit-and-review process to analyze usage regularly and document all such processes. A design-and-review process should be established. This would entail having a process built to respond to any questions from regulators seeking additional information. In addition to the principles stated in the preceding sections formulated by the EU or the regulatory bodies of respective countries, companies also need to engage in PR/external communication to raise awareness of their products and connect potential customers and partners to <span class="No-Break">revenue-generating channels.</span></p>
			<p>Organizations should continuously seek to improve the fairness, integrity, privacy, and accuracy of any AI system. Some of the recommended best practices laid down by Google include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Equip all AI-enabled systems with a human-centered design approach so that they focus on user experience and account for the diversity of users and <span class="No-Break">use cases.</span></li>
				<li>Identify and monitor multiple governance criteria to evaluate training and set up proactive measures based on monitoring results. This helps to determine the usefulness of model metrics to the goals of the <span class="No-Break">AI system.</span></li>
				<li>Examine raw data to assess the evaluation metrics, including accuracy, and judge the predictive capabilities of <span class="No-Break">the system.</span></li>
				<li>Ascertain the limits of the dataset and model, propagate these limitations across departments, and follow the recommended practices to remove bias and ensure fairness in terms of the <span class="No-Break">input data.</span></li>
				<li>Undertake<a id="_idIndexMarker417"/> rigorous, diverse, and regular integration and unit testing of <span class="No-Break">AI systems.</span></li>
				<li>Put in place regular monitoring and update techniques for all AI systems after deployment to consider real-world performance and <span class="No-Break">user feedback.</span></li>
			</ul>
			<p>The increasing adaptation of AI and solutions for minorities, including children, has led ethicists and data professionals to realize the need for unbiased AI systems. The goal is to uphold children’s collective right to protection and increase the participation of children from all backgrounds. This holds good for any AI-based systems with which children interact, irrespective of whether or not the system was designed for children. Some of the principles that organizations should infuse in their AI system development value proposal and technical know-how are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Include <a id="_idIndexMarker418"/>specially designed KPIs and metrics that can be integrated into AI-enabled systems to monitor and track children’s well-being. As children are influenced by AI systems during their interactions with those systems, the design of such systems should pay special attention to well-being frameworks and metrics. Furthermore, such systems should be tested on children and success parameters should be closely linked to children’s well-being <span class="No-Break">and development.</span></li>
				<li>Formulate AI policies and strategies by evaluating how AI systems can benefit children. This will help to ascertain the overall benefits received by children, as existing policies and strategies already evaluate the risks associated when children interact with <span class="No-Break">such systems.</span></li>
				<li>Adopt design styles that protect children’s rights by placing the child at the center of ethical AI policy and system design, development, and deployment. Protecting children’s rights helps children’s development and well-being by enforcing privacy by design, safety by design, and inclusion <span class="No-Break">by design.</span></li>
				<li>Design AI systems such that children’s development opportunities and rights to health, education, clean air, water, and safety are ensured. For example, AI-based educational and recommendation systems developed for children should not display advertisements that are not beneficial to children’s education and intellectual growth. AI-based systems should increase environmental sustainability and not impact the environment negatively. Any negative impact, such as an increase in carbon footprint, would seriously impact the well-being of children and their ability to live on a sustainable and healthy planet. The training, deployment, and computational infrastructure of AI systems should be tracked using carbon emission metrics to combat climate change, devise mitigation strategies, and promote better <span class="No-Break">ML modeling.</span></li>
				<li>Promote the explainability and transparency of AI systems by addressing children’s needs, such as the use of age-appropriate language to describe AI. This would add more meaning to AI systems’ explainability and make them transparent. Furthermore, the AI application should empower child users according to legal and policy frameworks, principles, and regulations. There should also be a space <span class="No-Break">for redressal.</span></li>
				<li>Develop systems <a id="_idIndexMarker419"/>to facilitate necessary skills among children and teachers to prepare children for present and future developments in AI. Furthermore, funding and incentives for child-centered AI policies and strategies should aim for infrastructure development and bridge the digital divide in a way that supports the equitable sharing of the benefits of AI. The end goal is to build a solid foundation of child-centered AI that includes <strong class="bold">protection (do no harm)</strong>, <strong class="bold">provision (do good)</strong>, and <strong class="bold">participation (include </strong><span class="No-Break"><strong class="bold">all children)</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>Despite the identification of the next steps in successfully building next-generation ethical AI systems, some direct challenges to government adoption of AI have been cited by the World Economic Forum  (<a href="https://www.weforum.org/agenda/2019/08/artificial-intelligence-government-public-sector/">https://www.weforum.org/agenda/2019/08/artificial-intelligence-government-public-sector/</a>). The main roadblocks that were identified include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Effective use of data</strong>: There’s a lack of understanding of the value of data and it is rarely deployed in a scalable infrastructure. There is also a lack of implementation of data <span class="No-Break">governance processes.</span></li>
				<li><strong class="bold">Data and AI skills</strong>: Governments, and other government-funded organizations, face a shortage of funds to hire expert data professionals that many big private companies can afford. This gap in attracting talent affects the quality of the AI <span class="No-Break">solutions produced.</span></li>
				<li><strong class="bold">AI ecosystem</strong>: Companies operating in the AI market experience frequent shifts in the problem they are solving and how customers see the solutions. Start-ups pioneering AI solutions also experience a dearth of talent to scale ML models ethically for <span class="No-Break">large projects.</span></li>
				<li><strong class="bold">Legacy culture</strong>: Governments and government-funded organizations have difficulty in adopting transformative technology, mostly because employees are not educated on ethical principles and are reluctant to <span class="No-Break">take risks.</span></li>
				<li><strong class="bold">Procurement mechanisms</strong>: ML-based solutions and systems are often treated as intellectual property with terms and conditions, which may make it difficult <a id="_idIndexMarker420"/>for<a id="_idTextAnchor087"/> governments to customize existing solutions within the <span class="No-Break">stipulated time.</span></li>
			</ul>
			<h3>Existing challenges and gaps</h3>
			<p>The frameworks proposed<a id="_idIndexMarker421"/> by different ethical associations and bodies have addressed the moral and ethical dilemmas identified and presented previously, but there are still a few gaps <span class="No-Break">to address.</span></p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B18681_03_007.jpg" alt="Figure 3.7 – How human psychology plays a role in determining profession, exhibiting bias toward﻿ women on the left and bias toward men on the right"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – How human psychology plays a role in determining profession, exhibiting bias toward women on the left and bias toward men on the right</p>
			<p>Along with the<a id="_idIndexMarker422"/> OECD, the EU has stressed the importance of societal and environmental well-being (including sustainability and environmental friendliness) and formulated ethics guidelines on the principle of prevention of harm. However, it does not provide examples of how to <span class="No-Break">achieve this.</span></p>
			<p>Ethics bodies have also looked at the impacts on human psychology of using AI and how people interact with AI. This is demonstrated in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.7</em>, which shows how our existing social structure tends to affect how we perceive different professions. Hence, it is evident that the EU needs to reposition social relationships within the context of ethics so that the data available in our society (demonstrating biased decisions concerning different protected attributes, as shown in the figure) does not end up training ML systems, consequently leading to circular bias. The <em class="italic">x</em> axis here represents how different professions tend to be associated with men, women, <span class="No-Break">or both.</span></p>
			<p>The EU draft on Responsible AI emphasizes the need for constant monitoring to evaluate AI interaction with humans and sets forth that in the event of such interactions, the system should be well tested against all types of social interactions through creative simulations. Such simulations can create different scenarios involving human and robot interactions. Even this specification is not sufficient to provide safety and protection in the long run, as human-robot relationships are complex and have an impact on the human psyche. Such complex interactions need to be considered in the creation of next-generation robotic autonomous <span class="No-Break">AI systems.</span></p>
			<p>As <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.8</em> illustrates, there<a id="_idIndexMarker423"/> is still fear among pedestrians about self-driving autonomous vehicles. One of the primary elements of future autonomous vehicle systems is using real test data to increase the percentage of pedestrians who feel safe with self-driving autonomous cars. In addition, accountability is also an important factor, in case of any harm caused by these vehicles. Hence, quantifying safety and protection remains one of the main tasks when defining trustworthy AI systems in <span class="No-Break">the future:</span><a id="_idTextAnchor088"/></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B18681_03_008.jpg" alt="Figure 3.8 – Safety concerns among pedestrians about self-driving cars"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Safety concerns among pedestrians about self-driving cars</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor089"/>Proposed solutions and improvement areas</h2>
			<p>The art of <a id="_idIndexMarker424"/>making an AI solution public comes with several<a id="_idIndexMarker425"/> challenges and questions. To address the questions, experiments were conducted by the French Council of State and other countries of the EU, as well as the US, to implement four different types of regulations. These regulations, described here, attempt to develop and certify authentic ethical <span class="No-Break">AI solutions:</span></p>
			<ul>
				<li>Algorithmic Accountability <span class="No-Break">Act-driven regulation</span></li>
				<li>Regulations proposed by <a id="_idIndexMarker426"/>government regulators, including the <strong class="bold">Food and Drug Administration</strong> (<strong class="bold">FDA</strong>) (for the healthcare <a id="_idIndexMarker427"/>industry), the <strong class="bold">National Highway Traffic Safety Administration</strong> (<strong class="bold">NHTSA</strong>) (transportation), and<a id="_idIndexMarker428"/> the <strong class="bold">Federal Trade Commission</strong> (<span class="No-Break"><strong class="bold">FTC</strong></span><span class="No-Break">) (retail)</span></li>
				<li>Regulations enabled by considering labor laws and civil <span class="No-Break">rights laws</span></li>
				<li>Regulations driven by the California Consumer <span class="No-Break">Privacy Act</span></li>
			</ul>
			<p>A survey has shown that after applying experimental regulations to AI solutions, the adoption of AI in business processes decreases by 16% due to a significant increase in the cost of implementing the AI strategy. It has been observed that, often, managers reduce the funds allocated for employee training to develop a wider AI strategy that can sustain AI regulations. The need at present is to set standards for certification, testing, auditing, and technology that validate model performance metrics in all <span class="No-Break">use cases.</span></p>
			<p>Organizations have come up with regulatory sandboxes that allow private sectors to run tests to evaluate how systems perform with real individuals and local regulatory constraints. As illustrated in the following figure, these sandboxes involve a few rounds of internal evaluation and stress testing <a id="_idIndexMarker429"/>with <strong class="bold">Explainable AI</strong> (<strong class="bold">XAI</strong>) to audit and correct the systems with <span class="No-Break">appropriate feedback.</span></p>
			<p>The figure illustrates the different components of a regulatory sandbox. This kind of sandbox allows us to determine <span class="No-Break">the following:</span></p>
			<ul>
				<li>The <a id="_idIndexMarker430"/>eligibility criteria for a dataset to be considered, such <a id="_idIndexMarker431"/>as how diverse the dataset is in terms of attributes. The procedure of application is to be included in the regulatory <span class="No-Break">sandbox testing.</span></li>
				<li>Actual participation of the individuals in the regulatory <span class="No-Break">testing process.</span></li>
				<li>The exiting condition for the sandbox, depending on the models being evaluated <span class="No-Break">for risk.</span></li>
				<li>The rights and obligations <span class="No-Break">of participants.</span></li>
			</ul>
			<p>An XAI assessment framework helps us to do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Understand and explain the internal characteristics to <span class="No-Break">validate interpretability.</span></li>
				<li>Evaluate the effect of the external environment on <span class="No-Break">the system.</span></li>
				<li>Assess the effect of the system on the external environment in <span class="No-Break">safety-critical scenarios.</span></li>
				<li>Ensure the application of better governance, policy, and <span class="No-Break">regulatory theories.</span></li>
			</ul>
			<p>If a system <a id="_idIndexMarker432"/>poses life-threatening risks to health and safety and fundamental rights, it should be stopped right away to prevent any further damage during its development and testing efforts. These experimental regulations and <a id="_idIndexMarker433"/>regulatory sandboxes help with evidence-based lawmaking. The core structure comprises pre-established facts, assumptions, and a legislative framework, which are the building blocks of an authentic, mature, and scalable AI system that abides by <span class="No-Break">regulatory compliance.</span></p>
			<p>The following figure shows an AI system in a sandbox to evaluate the effectiveness of the overall AI regulatory framework for economic, social, and <span class="No-Break">political impact.</span></p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B18681_03_009.jpg" alt="Figure 3.9 – AI sandbox for risk assessment and testing for regulations and policies"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – AI sandbox for risk assessment and testing for regulations and policies</p>
			<p>Let’s wrap up <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor090"/>Summary</h1>
			<p>In this chapter, we have learned about the regulations and laws put into practice by different ethical and governing bodies from varying nations. We now understand their basic principles and how they can aid in nurturing the growth of equitable systems, particularly those that fall under the high-risk category, such as models to do with justice, autonomous cars, healthcare, and systems that are expected to not only protect basic human rights but also promote the development of sustainable activities on earth. In addition, we stressed the importance of generating awareness about any possible misuse of AI by concentrating on overall well-being, effectiveness, transparency, accountability, competence, privacy, and fairness. We now understand the potential loss, or the damage done, in the case of a violation or failure and why it is important to abide by existing regulations or laws, as well as how this loss has a negative impact on society. This chapter also provided us with an insight into the gaps in existing regulations and laws and what further refinement is needed. These gaps leave us with a responsibility to be mindful of the design of such systems so that they serve as demonstrative examples for future <span class="No-Break">generations’ systems.</span></p>
			<p>In order to do that, let’s study how to build and evaluate large-scale big data and ML model pipelines in the <span class="No-Break">next chapter.</span></p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor091"/>Further reading</h1>
			<ul>
				<li><em class="italic">Key provisions of the Draft AI </em><span class="No-Break"><em class="italic">Regulation</em></span><span class="No-Break">: </span><a href="https://www.allenovery.com/en-gb/global/news-and-insights/publications/key-provisions-of-the-draft-ai-regulation"><span class="No-Break">https://www.allenovery.com/en-gb/global/news-and-insights/publications/key-provisions-of-the-draft-ai-regulation</span></a></li>
				<li><em class="italic">Policy guidance on AI for </em><span class="No-Break"><em class="italic">children</em></span><span class="No-Break">: </span><a href="https://www.unicef.org/globalinsight/media/2356/file/UNICEF-Global-Insight-policy-guidance-AI-children-2.0-2021.pdf"><span class="No-Break">https://www.unicef.org/globalinsight/media/2356/file/UNICEF-Global-Insight-policy-guidance-AI-children-2.0-2021.pdf</span></a></li>
				<li><em class="italic">AI Policy and National </em><span class="No-Break"><em class="italic">Strategies</em></span><span class="No-Break">: </span><a href="https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-7.pdf"><span class="No-Break">https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-7.pdf</span></a></li>
				<li><em class="italic">European draft Regulation on artificial intelligence: Key questions </em><span class="No-Break"><em class="italic">answered</em></span><span class="No-Break">: </span><a href="https://www.ey.com/en_ch/law/european-draft-regulation-on-artificial-intelligence-key-questions-answered"><span class="No-Break">https://www.ey.com/en_ch/law/european-draft-regulation-on-artificial-intelligence-key-questions-answered</span></a></li>
				<li><em class="italic">Amazon scraps secret AI recruiting tool that showed bias against </em><span class="No-Break"><em class="italic">women</em></span><span class="No-Break">: </span><a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G"><span class="No-Break">https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G</span></a></li>
				<li><em class="italic">AI, Machine Learning &amp; Big Data Laws and Regulations 2022 | </em><span class="No-Break"><em class="italic">Australia</em></span><span class="No-Break">: </span><a href="https://www.globallegalinsights.com/practice-areas/ai-machine-learning-and-big-data-laws-and-regulations/australia"><span class="No-Break">https://www.globallegalinsights.com/practice-areas/ai-machine-learning-and-big-data-laws-and-regulations/australia</span></a></li>
				<li><em class="italic">U.S. Artificial Intelligence Regulation Takes </em><span class="No-Break"><em class="italic">Shape</em></span><span class="No-Break">: </span><a href="https://www.jdsupra.com/legalnews/u-s-artificial-intelligence-regulation-1161759/"><span class="No-Break">https://www.jdsupra.com/legalnews/u-s-artificial-intelligence-regulation-1161759/</span></a></li>
				<li><em class="italic">Draft Eu Regulation On Ai And Its Impact On </em><span class="No-Break"><em class="italic">Healthcare</em></span><span class="No-Break">: </span><a href="https://www.kantify.com/insights/draft-eu-regulation-on-ai-and-its-impact-on-healthcare"><span class="No-Break">https://www.kantify.com/insights/draft-eu-regulation-on-ai-and-its-impact-on-healthcare</span></a></li>
				<li><em class="italic">FTC warns it could crack down on biased </em><span class="No-Break"><em class="italic">AI</em></span><span class="No-Break">: </span><a href="https://www.theverge.com/2021/4/20/22393873/ftc-ai-machine-learning-race-gender-bias-legal-violation"><span class="No-Break">https://www.theverge.com/2021/4/20/22393873/ftc-ai-machine-learning-race-gender-bias-legal-violation</span></a></li>
				<li>Ranchordas, S. (2021). <em class="italic">Experimental Regulations for AI: Sandboxes for Morals and Mores</em>. University of Groningen Faculty of Law Research Paper, (7). <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3839744"><span class="No-Break">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3839744</span></a><span class="No-Break">, </span><a href="https://www.nomos-elibrary.de/10.5771/2747-5174-2021-1-86.pdf"><span class="No-Break">https://www.nomos-elibrary.de/10.5771/2747-5174-2021-1-86.pdf</span></a></li>
				<li><em class="italic">An AI fair lending policy agenda for the federal financial </em><span class="No-Break"><em class="italic">regulators</em></span><span class="No-Break">: </span><a href="https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/"><span class="No-Break">https://www.brookings.edu/research/an-ai-fair-lending-policy-agenda-for-the-federal-financial-regulators/</span></a></li>
				<li><em class="italic">Blueprint For An Ai Bill Of </em><span class="No-Break"><em class="italic">Rights</em></span><span class="No-Break">: </span><a href="https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf"><span class="No-Break">https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf</span></a></li>
				<li><em class="italic">Responsible Ai Architect’s </em><span class="No-Break"><em class="italic">Guide</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://indiaai.gov.in/responsible-ai/pdf/architect-guide.pdf"><span class="No-Break">https://indiaai.gov.in/responsible-ai/pdf/architect-guide.pdf</span></a></li>
			</ul>
		</div>
	

		<div id="_idContainer061">
			<h1 id="_idParaDest-80"><a id="_idTextAnchor092"/>Part 2: Building Blocks and Patterns for a Next-Generation AI Ecosystem</h1>
			<p>This part of the book provides a comprehensive exploration of big data systems and AI/ML workflows, emphasizing privacy management, model design pipelines, and life cycle management. It covers various stages of the machine learning pipeline, model evaluation, and handling uncertainty, and addresses common challenges. Additionally, it discusses advanced topics such as hyperparameter tuning, MLOps practices, and AutoML. By offering theoretical discussions and practical guidance, this part equips you with the knowledge and tools to navigate the complex landscape of deploying AI models on top of big data systems while ensuring robust, efficient, and <span class="No-Break">privacy-preserving solutions.</span></p>
			<p>This part is made up of the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B18681_04.xhtml#_idTextAnchor093"><em class="italic">Chapter 4</em></a>, <em class="italic">Privacy Management in Big Data and Model Design Pipelines</em></li>
				<li><a href="B18681_05.xhtml#_idTextAnchor110"><em class="italic">Chapter 5</em></a>, <em class="italic">ML Pipeline, Model Evaluation, and Handling Uncertainty</em></li>
				<li><a href="B18681_06.xhtml#_idTextAnchor126"><em class="italic">Chapter 6</em></a>, <em class="italic">Hyperparameter Tuning, MLOps, and AutoML</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer062" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer063" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer064">
			</div>
		</div>
	</body></html>