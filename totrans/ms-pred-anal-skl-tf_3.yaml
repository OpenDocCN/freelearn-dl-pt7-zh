- en: Working with Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to take a close look at how features play an
    important role in the feature engineering technique. We''ll learn some techniques
    that will allow us to improve our predictive analytics models in two ways: in
    terms of the performance metrics of our models and to understand the relationship
    between the features and the target variables that we are trying to predict.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction and PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating new features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving models with feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Feature selection methods are used for selecting features that are likely to
    help with predictions. The following are the three methods for feature selection:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing dummy features with low variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying important features statistically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursive feature elimination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When building predictive analytics models, some features won't be related to
    the target and this will prove to be less helpful in prediction. Now, the problem
    is that including irrelevant features in the model can introduce noise and add
    bias to the model. So, feature selection techniques are a set of techniques used
    to select the most relevant and useful features that will help either with prediction
    or with understanding our model.
  prefs: []
  type: TYPE_NORMAL
- en: Removing dummy features with low variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first technique of feature selection that we will learn about is removing
    dummy features with low variance. The only transformation that we have been applying
    so far to our features is to transform the categorical features using the encoding
    technique. If we take one categorical feature and use this encoding technique,
    we get a set of dummy features, which are to be examined to see whether they have
    variability or not. So, features with a very low variance are likely to have little
    impact on prediction. Now, why is that? Imagine that you have a dataset where
    you have a gender feature and that 98% of the observations correspond to just
    the female gender. This feature won''t have any impact on prediction because almost
    all of the cases are just of a single category, so there is not enough variability.
    These cases become candidates lined up for elimination and such features should
    be examined more carefully. Now, take a look at the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fc96b69-e4fa-497c-9c96-25fc7005239d.png)'
  prefs: []
  type: TYPE_IMG
- en: You can remove all dummy features that are either 0 or 1 in more than x% of
    the samples, or what you can do is to establish a minimum threshold for the variance
    of such features. Now, the variance of such features can be obtained with the
    preceding formula, where **p** is the number or the proportion of **1** in your
    dummy features. We will see how this works in a Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying important features statistically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This method will help you make use of some statistical tests for identifying
    and selecting relevant features. So, for example, for classification tasks we
    can use an ANOVA F-statistic to evaluate the relationship between numerical features
    and the target, which will be a categorical feature because this is an example
    of a classic task. Or, to evaluate the statistical relationship between a categorical
    feature and the target, we will use the chi-squared test to evaluate such a relationship.
    In `scikit-learn`, we can use the `SelectKBest` object and we will see how to
    use these objects in a Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive feature elimination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of identifying important features and removing the ones that we
    think are not important for our model is called **recursive feature elimination**
    (**RFE**). RFE can also be applied in `scikit-learn` and we can use this technique
    for calculating coefficients, such as linear, logistic regression, or with models
    to calculate something called **feature importance**. The random forests model
    provides us with those feature importance metrics. So, for models that don't calculate
    either coefficients or feature importance, these methods cannot be used; for example,
    for KNN models, you cannot apply the RFE technique because this begins by predefining
    the required features to use in your model. Using all features, this method fits
    the model and then, based on the coefficients or the feature importance, the least
    important features are eliminated. This procedure is recursively repeated on the
    selected set of features until the desired number of features to select is eventually
    reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are the following few methods to select important features in your models:'
  prefs: []
  type: TYPE_NORMAL
- en: L1 feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selection threshold methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree-based methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s go to our Jupyter Notebook to see how we actually apply these methods
    in `scikit-learn`. The following screenshot depicts the necessary libraries and
    modules to import:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d000765a-0b4c-4f0a-8728-f5099e1253e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following screenshot, we have first used the credit card default dataset
    and we are applying the traditional transformations that we do to the raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f819808-eb10-415a-932d-7e3ea519fb41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the dummy features that we have in our dataset
    and the numerical features, depending on the type of feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4060697e-b56b-42ff-a373-fba66f231307.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we are applying the scaling operation for feature modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89e9ea9d-aabf-4dc1-9df0-c8b36d6e2b98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first method that we talked about in the presentation was removing dummy
    features with low variance to get the variances from our features using the `var()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f9a3dae-0abb-442a-a84e-06eee184e9e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s see the variances only for the dummy features; for example, a threshold
    for the variance will consider only the dummy features with a variance over `0.1`.
    In that case, with such a threshold of 0.1, the two candidates for elimination, `pay_5` and `pay_6`,
    would be the first few unnecessary dummy features with low variance that will
    be removed. Take a look at the following screenshot, which depicts the candidates
    for elimination:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36866649-2516-4eca-b8ec-7d2e8624cf8f.png)'
  prefs: []
  type: TYPE_IMG
- en: The second approach that we talked about is statistically selecting the features
    that are related to the target, and we have two cases, dummy features and numerical
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s perform the statistical tests for the dummy features. We are going to
    import objects in the `chi2` object from the `feature_selection` module in the
    `scikit-learn` library. We will also use the `SelectKBest` object to perform the
    statistical tests in all of the dummy features as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22827f00-aeb2-41ce-8d4f-7fa9e1b74377.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we instantiate an object called `dummy _selector` and pass the required
    statistical test to apply to it. Here, we are passing the `k ="all"` argument because
    this statistical test is to be applied to all of the dummy features. After instantiating
    this object, the `fit()` method is called. Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb58b92b-6578-4731-9691-e36707a4aca6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following screenshot, we have the chi-squared scores. This isn''t a
    statistical test and, the larger the number, the stronger the relationship between
    the feature and the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a4ab20a-c9c9-447b-99b6-e9127829ea6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if you remember your statistics class, this is a hypothesis testing setting.
    So, we can also calculate the p values and we can say that the features where
    `pvalues_` is greater than `0.05` are not related to the target. Now, in this
    case, we get very small p values for all of the features, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a45bc992-e373-4029-a504-a3a1cab481d5.png)'
  prefs: []
  type: TYPE_IMG
- en: There is a relationship between the target and all of the dummy features, so
    under this methodology, we shouldn't eliminate any of these dummy features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can use another statistical test called `f_ classif` to evaluate the
    relationship between numerical features and the target, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a383b1d7-db46-402f-8685-561a59bb9efe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Reusing this `f_classif` object, we will pass the required statistical tests
    and number of features. In this case, we want to apply the test to all numerical
    features and then use the `fit()` method again with the numerical features and
    the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d176673-5b2a-4baf-bcca-c02a1d8d8d1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The p values that we receive from the application of this statistical test
    are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f7a09ea-105f-424e-8e4c-9c4887a7ced9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can pass the `f_classif` statistical test and then select the numerical
    features that have a p value greater than `0.05`, which is the usual threshold
    for statistical tests; the resulting features here are `bill_amt4`, `bill_amt5`,
    and `bill_amt6`, which are likely to be irrelevant, or not related to the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d128d68-8142-47e4-b6d0-95982e12dc43.png)'
  prefs: []
  type: TYPE_IMG
- en: We have three candidates for elimination which can be eliminated or can be applied.
    We have used the second technique in the preceding steps and now we will use the
    third one in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RFE is the third technique in which we will use the `RandomForestClassifier`
    model, and remember that we have 25 features here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccc37681-6a78-4244-ac36-8cef9d4360ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, let''s assume that we want to select only 12 features and we want a model
    that uses only 12 features. So, we are using about half of the features. We can
    use the `RFE` object present in `scikit-learn` from the `feature_selection` module.
    We can use this to actually select these 12 features using the RFE technique.
    So, we instantiate this object by passing the required estimator and the number
    of features to select:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/515cf59d-8bc9-428d-99ee-c0693af054cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, remember that random forest provides us with a metric of feature importance,
    which can be used with the RFE technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c041ba0-ebbf-4f72-acb1-331a690b14fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After using the `fit()` method on the whole dataset, we get `recursive_selector.support_`
    and `True` for the features that are included in our model, the 12 that we wanted,
    and we get `False` for the ones that should be eliminated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/316a9ba5-01e9-4406-b46e-7f3db0970ea8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, according to this object and method, we should include the 12 most important
    features in our random forest model in order to predict targets such as `limit_bal`,
    `age`, `pay`; all of the bill amounts; and `pay_amt1`, `pay_amt2`, and `pay_amt3`,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc1ad6e3-8378-4971-b073-ec6ffe1631a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These are the features that should be eliminated because they are not very
    relevant according to this method and this model for predicting the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a290e198-14e2-4776-aa59-c31137a3b76c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can evaluate the simpler model, the one with the 12 features against
    the full model that we have been using so far, after which we can calculate the
    metrics using cross-validation. So, in this example, we are using 10-fold cross-validation
    to get an estimation of the performance of these two models. Remember, this selector
    model is the full model according to the RFE technique and these are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c19b384c-506d-4e42-85bc-8cfe711c1e3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The full model has a recall of `0.361365`, and the model that includes only
    12 features has a recall of `0.355791`. Since this model has less recall, the
    full model remains the best one. But if we use half of the features, the full
    model will also give us similar performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2f3dcbb-fedd-4e46-b2e9-3a4d52135093.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the following screenshot, the values are really close:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d85e222b-999c-40d8-bf81-05b389a3b7c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now you can decide whether you want to use the full model or you want to use
    the simpler model. This is up to you, but in terms of accuracy we get almost the
    same, although with still a little bit more accuracy for the full model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de3fc718-3686-4136-a8e3-b483cdfa8f62.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, you have a technique to decide whether you want to use a more complicated
    model that uses more features, or a simpler model.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction and PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dimensionality reduction method is the process of reducing the number of
    features under consideration by obtaining a set of principal variables. The **Principal
    Component Analysis **(**PCA**) technique is the most important technique used
    for dimensionality reduction. Here, we will talk about why we need dimensionality
    reduction, and we will also see how to perform the PCA technique in `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the reasons for having a high number of features while working on
    predictive analytics:'
  prefs: []
  type: TYPE_NORMAL
- en: It enables the simplification of models, in order to make them easier to understand
    and to interpret. There might be some computational considerations if you are
    dealing with thousands of features. It might be a good idea to reduce the number
    of features in order to save computational resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another reason is to avoid the "curse of dimensionality." Now, this is a technical
    term and a set of problems that arise when you are working with high-dimensional
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This also helps us to minimize overfitting because if you are including a lot
    of irrelevant features to predict the target, then your model can overfit to that
    noise. So, removing irrelevant features will help you with overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature selection, seen earlier in this chapter, can be considered a form of
    dimensionality reduction. When you have a set of features that are closely related
    or even redundant, PCA will be the preferred technique to encode the same information
    using less features. So, what is PCA? It's a statistical procedure that converts
    a set of observations of possibly correlated variables into a set of linearly
    uncorrelated variables called **principal components**. Let's not go into the
    mathematical details about what's going on with PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume we have a dataset that is two-dimensional. PCA identifies a direction
    where the dataset varies the most and encodes the maximum amount of information
    on these two features into one single feature to reduce the dimensions from two
    to one. This method projects every point onto these axes or new dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following screenshot, the first principal component of
    these two features would be the projections of the points onto the red line, which
    is the main mathematical intuition behind what''s going on in PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/941fd408-2576-441e-b2eb-37e2ea9a84fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s go to the Jupyter Notebook to see how to implement the dimensionality
    reduction method and to apply PCA on the given dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0029c5de-ae04-47d1-b311-e42d0d1bff40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, we will use the credit card default dataset. So, here we are
    doing the transformations that we have covered so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5fd83e2-cab0-468d-af4d-1270ef9b3656.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s take a look at the bill amount features. We have six of these features,
    the history of the bill amounts from one to six months ago, which are closely
    related, as you can see from the visualization generated from the following screenshot
    of code snippets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/360ba160-c0a2-457e-9435-abe501b4c447.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, they represent the same information. If you see a customer with a very
    high bill amount two or three months ago, it is very likely that they also got
    a very high bill amount one month ago. So, these features, as you can see from
    the visualization shown in the following screenshot, are really closely related:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11d8b84c-f9c7-4cb8-a6d2-8bea6e03ad7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We confirm this with the calculation of the correlation coefficient. As you
    can see, they are really highly correlated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8a4f728-6e65-43d6-887f-331d841c3298.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The correlation between the bill amount one month ago and two months ago is
    `0.95`. We have very high correlations, which is a good opportunity to apply a
    dimensionality reduction technique, such as PCA in `scikit-learn`, for which we
    import it from `sklearn.decomposition` , as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14f1e55c-9574-4e3f-a89e-902ba51eb556.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After that, we instantiate this `PCA` object. Then, we pass the columns or
    the features that we want to apply PCA decomposition to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4998794-1b14-4646-83ca-ab9e871a4de8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So after using the `fit()` method derived from this object, we receive one
    of the attributes, the explained variance ratio, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b99b1080-769c-40c4-8c7f-23e63fe9adbd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s plot this quantity to get a feel for what''s going on with these features.
    As you can see here, we get the explained variance of all six components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e63ef069-1047-4913-b7d1-396f817af0df.png)'
  prefs: []
  type: TYPE_IMG
- en: The way to read this plot is that the first component of the PCA that we did
    on these six features encodes more than 90% of the total variance of all six features.
    The second one shows a very small variance, and the third, fourth, fifth, and
    sixth components also have minimal variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can see this in the plot of cumulative explained variance shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef93a131-19d8-4ce0-b10e-698b21ef1cfa.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the first component encodes more than 90% of the variance of
    the six features that we used. So, you are getting more than 90% of the information
    in just one feature. Therefore, instead of using six features, you can use just
    one single feature and still get more than 90% of the variance. Or, you can use
    the first two components and get more than 95% of the total information contained
    in the six features in just two features, the first and second components of this
    PCA. So, this is how this works in practice and we can use this as one technique
    for performing feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering plays a vital role in making machine learning algorithms
    work and, if carried out properly, it enhances the predictive ability of machine
    learning algorithms. In other words, feature engineering is the process of extracting
    existing features or creating new features from the raw data using domain knowledge,
    the context of the problem, or specialized techniques that result in more accurate
    predictive models. This is an activity where domain knowledge and creativity play
    a very important role. This is an important process, which can significantly improve
    the performance of our predictive models. The more context you have about a problem,
    the better your ability to create new and useful features. Basically, the feature
    engineering process converts the features into input values that algorithms can
    understand.
  prefs: []
  type: TYPE_NORMAL
- en: There are various ways of implementing feature engineering. You might not find
    all of the techniques feasible and may end up excluding a few. The motive here
    is not to have an academic discussion about this topic, but to show you some of
    the common things that we do when we work with features and when we try to create
    new features. The first one is scaling features, used to transform their range
    to a more suitable one. The other one is to encode information in a better way,
    and we will see an example of this later in this chapter. Feature engineering
    involves creating new features from existing ones so that you can combine existing
    features by performing some mathematical operations on them.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of creating new features is by using a dimensionality reduction
    technique, such as PCA, which we saw previously. It doesn't matter what technique
    you use, as long as you make it creative. As mentioned previously, the more knowledge
    you have about the problem, the better.
  prefs: []
  type: TYPE_NORMAL
- en: Creating new features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the credit card default and diamond datasets here. Now, let’s
    go to the Jupyter Notebook to create new features and see what these techniques
    are in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f149084-6127-4ec7-be75-0550b89c0895.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s import the credit card default dataset by executing a few commands,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fcfac44-86fd-447e-a414-866ba15d6164.png)'
  prefs: []
  type: TYPE_IMG
- en: The first transformation that we will do is to create another way to encode
    the information that we have in the `education` feature. So far, we have been
    using one encoding technique in the `education` feature, and we will use the context
    of the `x` variable to come up with another encoding. People with graduate-level
    education are more highly educated than people with other levels of education.
    So, we can come up with some sort of points system for these features; for example,
    we can assign two points for people with graduate-level education, maybe one point
    for people with university-level education, and negative points for the other
    levels of education that we have in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following screenshot to see how this is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0580fcd-23d3-41a8-b5e1-9ec4978e0dd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous screenshot reflects the sequence that we have in these education
    levels, so this could be an another way to encode information. This might or might
    not be helpful in predicting defaulters for the next month. However, we can try
    this new technique to encode this information and see the results in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be40f9c7-3a02-4f08-a59f-befb73a3a7c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another technique that we can use in this dataset is to use the bill amount
    and payment amount features in the context of this problem to calculate the difference
    between these two variables/features. So, if we take the bill amount from a particular
    month and subtract the payment amount for that month, we will get an amount or
    quantity. In this example, we are calling the `bill_minus_pay` variable, which
    represents the payment made by the client against the bill for that month. So,
    this newly derived quantity can be used to predict defaulters for the next month.
    We have included them in a potential predictive model for this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1275ad80-b856-4812-ac04-b562ac2fc4a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now take a look at the following output, which depicts the defaulters
    for a particular month:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a5678a6-7c8b-4658-9c06-d20905e46e44.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another method that we can use here, now that we have part of the information
    of these features in a new feature called `bill_minus_pay`, is that we can summarize
    the main information of the six features shown in the preceding screenshot in
    just one feature using the PCA technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbf447a5-af00-402d-8bb9-b5c0d34cb41d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can do the same operation with the pay features. From the previous analysis,
    we know that the `pay_1` feature is very important for predicting who is going
    to pay next. So, in order to reduce the other five `pay_i` features to just two,
    we are reducing the six bill amount features to just one, and the six `pay_i`
    features to two. Apart from this, we again apply the PCA technique on the remaining
    five `pay_i` features to reduce these five to just one. Take a look at the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b49bf31-73fe-48a7-9e42-dc69ef617e7e.png)'
  prefs: []
  type: TYPE_IMG
- en: These are some of the feature engineering techniques, with examples, that you
    can perform on your datasets, but you might want to create other transformations
    or variables from the existing ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see a couple of examples in the diamonds dataset. We need to import
    the diamonds dataset by executing a few commands, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a5b38e2-d2e8-4ed0-b02e-add5805bddac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As seen in the preceding screenshot, we have transformed some of the categorical
    features using the encoding technique. Now, let''s take a look at our imported
    dataset, shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa90eb24-2ac3-4ab2-8541-8e645dc22c8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is what our scatter plot matrix with four features, `x`, `y`, `z`, and
    `price`, looks like. The first three features refer to the measurements of the
    diamond, and `price` represents how those three features are related to the pricing
    of the diamond:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/050dd302-e6ac-4f0e-a5e2-d12c18a1ec60.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, as you can see, there is a very strong linear relationship
    between the first three features, which is one of the interesting things in this
    scatter plot matrix.
  prefs: []
  type: TYPE_NORMAL
- en: As diamonds are three-dimensional objects, we will combine these three features
    into just a single feature, called volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will multiply the measurements of the `x`, `y`, and `z` axes, which
    will derive a number that is close to the volume of that object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d50740ee-536d-4157-bc16-bd22cdc23c3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we know that they are not boxes, and they don't have any fixed shape. However,
    this will be a really good approximation of the volume of the diamonds. So, this
    can be another way in which we can create a new feature volume from the existing
    features in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next screenshot, we have the volume of our object and also the weight
    of the diamond, which is measured as `carat`, and we will use these to create
    a new feature called `density`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3af98542-6143-49f9-97c4-1b739b8fe2f1.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see in the preceding screenshot, we have divided the carat by the
    volume in order to get the density of the diamond object.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is how we created two features from the given context, which justifies
    the statement: "the more the knowledge or context of the problem, the better".
    As you can see, with just the provided knowledge of the problem, we were able
    to come up with new features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s try and see how helpful these features might be in predicting models.
    The example we will use here is, how you can combine existing features to produce
    new features. The following plot shows the close relationship between the volume
    and price:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b91bb49e-1916-444c-9476-2e49dd2d06cb.png)'
  prefs: []
  type: TYPE_IMG
- en: We can assume that the volume will be helpful in predicting the price.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in the following scatterplot of density and price, we see that all
    diamonds have the same expected density:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4374d52-538a-45a8-841e-ef21e716744d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we see the correlation between `price` and `carat`, which we already had,
    it seems that `density` might not relate to `price` much:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/939ece93-e875-418f-b250-fa4e70de9f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: So, this new feature might not help much in prediction. The volume and carat
    features have the same kind of relationship. We might not gain a lot of predictive
    power with this feature, but the main goal behind explaining this example was
    to show how to combine different features that you already have in your dataset
    to create new features.
  prefs: []
  type: TYPE_NORMAL
- en: This is what feature engineering is all about. You might also come up with other
    features for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Improving models with feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen how feature engineering techniques help in building predictive
    models, let's try and improve the performance of these models and evaluate whether
    the newly built model works better than the previous built model. Then, we will
    talk about two very important concepts that you must always keep in mind when
    doing predictive analytics, and these are the reducible and irreducible errors
    in your predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first import the necessary modules, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9b2c9df-4eeb-4402-82d7-a47930ea0880.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, let''s go to the Jupyter Notebook and take a look at the imported credit
    card default dataset that we saw earlier in this chapter, but as you can see,
    some modifications have been made to this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18835e99-b7bb-4d75-bc85-c5ff4582984f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For this model, instead of transforming the `sex` and `marriage` features into
    two dummy features, the ones that we have been using were `male` and `married`;
    therefore, let''s encode the information in a slightly different way to see if
    this works better. So, we will encode the information as `married_male` and `not_married_female`,
    and see if this works better. This is the first transformation that we are doing
    here. This is what the dataset looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9782f93-7ebb-4755-89b9-1a18495aad5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s do a little bit more feature engineering. The first thing that
    we will do is calculate these new features, which are built from subtracting the
    payment amount from the bill amount, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a543d2e0-8351-4c63-b35c-e118875fb5b4.png)'
  prefs: []
  type: TYPE_IMG
- en: For this problem, we will perform one mathematical operation. We will use the
    new features shown in the preceding screenshot to predict the target. Most of
    the information in the bill amount features is now encoded in these features,
    which are not needed anymore, but instead of throwing them away, what we can do
    is reduce the six bill amount features to just one using the PCA technique. So,
    let's apply the PCA technique to reduce the six features to just one component.
    Now there is a new feature called `bill_amt_new_feat`. So, this was the second
    feature engineering step that we performed. Finally, for the `pay_i` features,
    we will preserve the first one as is, and apply the PCA technique to the last
    five features, `pay_2`, `pay_3`, `pay_4`, `pay_5`, and `pay_6`, to reduce these
    five features to just two components. You can use the `fit_transform` method on
    the `PCA` object to get the components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at the following screenshot, showing all of the features
    that have to do with money. As you can see, the variances here are really huge
    because the currency amounts are large:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9405c76-fd54-48c7-bb70-94af065be775.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, rescale these features by dividing them by 1,000 in order to reduce the
    variances, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ef1e002-540e-4c2b-8e7a-858c96a183b0.png)'
  prefs: []
  type: TYPE_IMG
- en: This helps us to make these numbers understandable. So, this is the other transformation
    that we did, and now let's train our model with these new features.
  prefs: []
  type: TYPE_NORMAL
- en: Training your model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following model is a new module, as it has different features compared
    to the other models. Since the features have changed, we need to find the best
    hyperparameters for the `RandomForestClassifier` module using the `GridSearchCV`
    module. So, perhaps the previously found best parameters are not the best for
    these new features; therefore, we will run the `GridSearchCV` algorithm again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e2ac386-5dc1-4970-b04a-44456a49da23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the following screenshot, in this case the best combination of
    parameters for these new features is `max _depth` of `30`, `max_features` in `auto`,
    and `n_estimators` (number of estimators) should be `100`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4892e44-1011-4f54-9605-577ce1933e06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s evaluate this new model that we have built using feature engineering,
    and let''s compare it with the previous metrics that we have from the previously
    built model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af57d7ee-8abb-400f-b988-7d53e42ed206.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the preceding screenshot, we are using a threshold of 0.2\.
    This model generates a recall of `71.39`% and a precision of `37.38`. Here, the
    precisions are similar, but, as mentioned earlier, the recall might be the metric
    that we should care about, as it''s slightly different compared to the previous
    one. We got a little better recall for this model; the change may only be 2% or
    3% , which might not look like much, but remember that in these financial applications,
    an improvement of 1% or 2% could, in practice, mean a lot of money. So, we got
    a slight improvement in the predictive power of our model using this little feature
    engineering technique; let''s take a look at the feature importance in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9225a3eb-041c-46f0-bc51-d6724b30d4f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can assess whether this feature importance make sense in the following
    screenshot of the random forest model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6062dd52-525e-48e5-90ed-caa3e760dbe1.png)'
  prefs: []
  type: TYPE_IMG
- en: You can compare this feature importance with the previous ones. There are a
    lot of things that you can do after you have applied feature engineering. We may
    improve performance and gain insight from the model. It's been observed that we
    improved our model a little bit by using this technique. Now, you can come up
    with different ways to combine the existing features to improve the model even
    more. This was just a small, simple example to show you that you can actually
    play around with the features in a way that actually makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: Reducible and irreducible error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before moving on, there are two really important concepts to be covered for
    predictive analytics. Errors can be divided into the following two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reducible errors**: These errors can be reduced by making certain improvements
    to the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Irreducible errors**: These errors cannot be reduced at all'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s assume that, in machine learning, there is a relationship between features
    and target that is represented with a function, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4dcab463-bc46-4047-9f46-8d6b362ce700.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s assume that the target (**y**) is the underlying supposition of machine
    learning, and the relationship between the features and the target is given by
    a function. Since, in most cases we consider that there is some randomness in
    the relationship between features and target, we add a noise term here, which
    will always be present in reality. This is the underlying supposition in machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In models, we try to approximate the theoretical function by using an actual
    function while performing feature engineering, tuning the parameters, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86502f63-aa51-40f7-8605-e6feaecaf2c3.png)'
  prefs: []
  type: TYPE_IMG
- en: So, our predictions are the results of the application of these approximations
    to the conceptual or theoretical **f**. All that we do in machine learning is
    try to approximate this **f** function by training the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training a model means approximating this function. It is possible to show
    mathematically that the expected error, defined as the difference between the
    real **y** and the predicted **y**, can be decomposed into two terms. One term
    is called **Reducible error** and the other one is called **Irreducible error**,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba4c0671-5d52-4204-806c-85b9c4ecfcd9.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, the **Irreducible error** term is the variance of this random term. You
    don't have any control over this term. There will always be an irreducible error component.
    So, your model will always make mistakes; it doesn't matter how many features
    and data points you have, your model cannot always be 100% correct. What we must
    try to do is to use better and more sophisticated methods to perform feature engineering,
    and try to approximate our estimation to the real function. Just because you are
    working with more sophisticated models or you have more data, your model will
    not be perfect and you will not be able to predict exactly what **y** is, because
    there is some randomness in almost all the processes that you will work with.
    So this is the end of a very interesting section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about feature selection methods, how to distinguish
    between useful features, and features that are not likely to be helpful in prediction.
    We talked about dimensionality reduction and we learned how to perform PCA in
    `scikit-learn`. We also talked about feature engineering, and we tried to come
    up with new features in the datasets that we have been using so far. Finally,
    we tried to improve our credit card model by coming up with new features, and
    by working with all of the techniques that we learned in this chapter. I hope
    you have enjoyed this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about artificial neural networks and how
    the `tensorflow` library is used when working with neural networks and artificial
    intelligence.
  prefs: []
  type: TYPE_NORMAL
