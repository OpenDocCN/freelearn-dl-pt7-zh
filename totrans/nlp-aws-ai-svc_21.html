<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer311">
			<h1 id="_idParaDest-206"><em class="italic"><a id="_idTextAnchor211"/>Chapter 18</em>: Building Secure, Reliable, and Efficient NLP Solutions </h1>
			<p>Thank you, dear reader, for staying with us through this (hopefully informative) journey in building best-in-class <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) solutions for organizations looking to uncover insights from their text data. Our aim in writing this book was to create awareness that <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) is mainstream and that we are at the cusp of a huge tidal wave of AI adoption that many enterprises are moving toward. This exciting technology not only helps you advance your career but also provides opportunities to explore new avenues of innovation that were not possible previously. For example, according to a BBC article (<a href="https://www.bbc.com/future/article/20181129-the-ai-transforming-the-way-aircraft-are-built">https://www.bbc.com/future/article/20181129-the-ai-transforming-the-way-aircraft-are-built</a>), <strong class="bold">Autodesk</strong> (<a href="https://www.autodesk.com/">https://www.autodesk.com/</a>), a global leader in designing and making technology, uses <strong class="bold">Generative AI</strong> (<a href="https://www.amazon.com/Generative-AI-Python-TensorFlow-Transformer/dp/1800200889">https://www.amazon.com/Generative-AI-Python-TensorFlow-Transformer/dp/1800200889</a>) to help aircraft manufacturers design more efficient airframes, a key requirement to reduce fuel consumption. </p>
			<p>Throughout this book, we reviewed several types of use cases that enterprises deal with today to garner useful information from text-based data. This information will either directly help them derive insights or serve as a precursor to driving downstream decision-making, with operational implications in both cases. We read through different business scenarios and discussed different solution design approaches, architecture implementation frameworks', and real-time and batch solutions that met these requirements. </p>
			<p>We now understand how to design, architect, and build NLP solutions with <strong class="bold">AWS</strong> <strong class="bold">AI</strong> <strong class="bold">services</strong>, but we are still missing an important step. How do we ensure our solution is production-ready? What are the operational requirements to ensure our solution works as expected when dealing with real-world data? What are the non-functional requirements that the architecture needs to adhere to? And how do we build this into the solution as we go along? To answer these questions, we will review the best practices, techniques, and guidance on what makes a good NLP solution great.    </p>
			<p>In this chapter, we will discuss the following topics:</p>
			<ul>
				<li>Defining best practices for NLP solutions</li>
				<li>Applying best practices for optimization </li>
			</ul>
			<h1 id="_idParaDest-207"><a id="_idTextAnchor212"/>Technical requirements</h1>
			<p>For this chapter, you will need access to an AWS account (<a href="https://aws.amazon.com/console/">https://aws.amazon.com/console/</a>).</p>
			<p>Please refer to the <em class="italic">Signing up for an AWS account</em> sub-section within the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, for detailed instructions on how you can sign up for an AWS account and sign in to the <strong class="bold">AWS</strong> <strong class="bold">Management Console</strong>.</p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor213"/>Defining best practices for NLP solutions</h1>
			<p>If you have done DIY (do-it-yourself) projects in the past, you know how important your tools are for your work. When<a id="_idIndexMarker965"/> building an NLP solution, or any solution<a id="_idIndexMarker966"/> for that matter, you need to keep the following in mind: </p>
			<ul>
				<li>You need to know your requirements (the "<em class="italic">what</em>"). </li>
				<li>You need to know the problem that you are trying to solve by building the solution (the "<em class="italic">why</em>").</li>
				<li>You need to know the tools and techniques required to build the solution (the "<em class="italic">how</em>"). </li>
				<li>You need to estimate the time you require to build the solution (the "<em class="italic">when</em>"). </li>
				<li>Finally, you need to determine the required skills for the team (the "<em class="italic">who</em>").</li>
			</ul>
			<p>But with this approach, you haven't necessarily addressed the needs that will make your solution reliable, scalable, efficient, secure, or cost-effective. And these are equally important (if not more so) to building long-lasting solutions that will delight your customers. </p>
			<p>When building with AWS, you have access to prescriptive guidance and valuable insights garnered from decades of building and operating highly performant, massive-scale applications such as <strong class="bold">Amazon</strong>, along with the expertise that comes from helping some of the world's largest enterprises with running<a id="_idIndexMarker967"/> their cloud workloads on AWS. All of this experience and knowledge has been curated into a collection of architectural guidelines and best practices called the <strong class="bold">AWS Well-Architected Framework</strong>. </p>
			<p>Think of <em class="italic">well-architected</em> as a comprehensive checklist of questions that is defined by five pillars, as follows:</p>
			<ul>
				<li><strong class="bold">Operational excellence</strong>: The operational excellence pillar recommends automating<a id="_idIndexMarker968"/> infrastructure provisioning and management (if applicable), modularizing the solution architecture into components that can be managed independently, enabling agility, implementing CI/CD-based DevOps practices, simulating operational failures, and preparing for and learning from it.</li>
				<li><strong class="bold">Security</strong>: The security pillar recommends making security the top priority by implementing<a id="_idIndexMarker969"/> least privilege governance measures and associated guardrails from the ground up with a focus on identity and access management, compute and network security, data protection in transit and at rest, automation, simulation, and incident response.</li>
				<li><strong class="bold">Reliability</strong>: The reliability<a id="_idIndexMarker970"/> pillar requires the setting up of highly resilient architectures with the ability to self-heal from failures, with a focus on fail-fast and recovery testing, elastic capacity with auto scale-in/scale-out, and a high degree of automation.</li>
				<li><strong class="bold">Performance efficiency</strong>: The performance efficiency pillar<a id="_idIndexMarker971"/> recommends the use of <strong class="bold">AWS Managed Services</strong> (<strong class="bold">AMS</strong>) to remove the undifferentiated heavy lifting associated<a id="_idIndexMarker972"/> with managing infrastructure, using the global AWS network to reduce latency for your end users and remove the need for repeated experimentation and decoupling resource interactions by means of APIs. </li>
				<li><strong class="bold">Cost optimization</strong>: The cost optimization pillar provides recommendations on measures you can take to track<a id="_idIndexMarker973"/> and minimize usage costs.</li>
			</ul>
			<p>For more details on the <em class="italic">Well-Architected Framework</em>, along with the resources<a id="_idIndexMarker974"/> to get you started, please go to the Amazon documentation: <a href="https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html">https://docs.aws.amazon.com/wellarchitected/latest/framework/welcome.html</a></p>
			<p>Taken together, the Well-Architected questions from the various pillars guide you through your architecture design, build, and implementation. This enables you to include critical design principles, resulting in solutions that are built for secure, reliable, cost-effective, and efficient operations (and hence the term Well-Architected). So, what does Well-Architected mean in our case with regard to NLP solutions and AI services? To help you understand this clearly, we will create a matrix of the Well-Architected pillars, aligned with NLP development<a id="_idIndexMarker975"/> stages such as document preprocessing, prediction, and post-processing, from the perspective of the major AWS AI services we used in this book (<strong class="bold">Amazon</strong> <strong class="bold">Textract</strong> and <strong class="bold">Amazon</strong> <strong class="bold">Comprehend</strong>). We will also look at the application of principles<a id="_idIndexMarker976"/> for NLP solution builds in general. In each cell of this matrix, we will summarize how to apply the Well-Architected principles for design and implementation using best practices. </p>
			<div>
				<div id="_idContainer304" class="IMG---Figure">
					<img src="Images/B17528_18_01.jpg" alt="Figure 18.1 – Well-Architected NLP solutions matrix&#13;&#10;" width="1569" height="1569"/>
				</div>
			</div>
			<p class="figure-caption">Figure 18.1 – Well-Architected NLP solutions matrix</p>
			<p>As you can see from the preceding matrix, there are a number of design principles you can adopt during your solution development to build efficient and secure NLP applications. For the sake of clarity, we separated these principles based on the Well-Architected Framework pillars and the NLP development stages. However, as you might have noticed, some of these principles are repetitive across the cells. This is because an application<a id="_idIndexMarker977"/> of a principle for a particular pillar may automatically also address the needs of a different pillar based on what principle we refer to. For example, when using <strong class="bold">AWS</strong> <strong class="bold">Glue</strong> <strong class="bold">ETL</strong> jobs for document pre-processing and post-processing tasks, our Well-Architected needs for operational excellence, cost optimization, and performance efficiency are addressed without the need to do anything else. </p>
			<p>We will explain the reason for this in more detail in the next section. We introduced the AWS Well-Architected Framework in this section and reviewed a matrix of how the Well-Architected principles can be applied to the AWS AI services we used throughout this book. In the next section, we will delve deeper and discuss how to implement some of the principles from the matrix.</p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor214"/>Applying best practices for optimization </h1>
			<p>In this section, we will dive a bit deeper into what each of the design principles that we documented in the Well-Architected NLP solutions matrix means, and how to implement them<a id="_idIndexMarker978"/> for your requirements. Since the scope of this book is primarily about AWS AI services, we already have the advantage of using serverless<a id="_idIndexMarker979"/> managed services, and this addresses a number of suggestions that the Well-Architected Framework alludes to. Additionally, as mentioned previously, some of the best practices documented in the matrix may appear to be repetitive – this is not a mistake but intentional, as the application of one design pattern may have a cascading benefit across multiple pillars of the Well-Architected Framework. We will highlight these as we come across them. Without further ado, let's dig in.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor215"/>Using an AWS S3 data lake</h2>
			<p>This section addresses principles <em class="italic">1.1a</em> and <em class="italic">1.3a</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p>A <strong class="bold">data lake</strong> is a repository for structured, semi-structured, and unstructured data. Initially, it is a collection<a id="_idIndexMarker980"/> of data from disparate<a id="_idIndexMarker981"/> sources within the enterprise<a id="_idIndexMarker982"/> and it serves as the data source for downstream analytics, business intelligence, <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), and operational needs. However, since the data<a id="_idIndexMarker983"/> hydrated into a data lake<a id="_idIndexMarker984"/> retains its source format (data is not transformed before loading into the data lake), the data needs to undergo transformation<a id="_idIndexMarker985"/> at the time of consumption from the data lake. Building a data lake using <strong class="bold">Amazon S3</strong> (a fully managed object storage solution in the AWS cloud) makes a lot of sense because it scales as much as you want, and data stored in S3 is highly durable (for more information, see the section on the <em class="italic">"11 9s"</em> of durability at (<a href="https://aws.amazon.com/s3/faqs/">https://aws.amazon.com/s3/faqs/</a>). </p>
			<p>Furthermore, AWS provides a number of ways in which you can get your data into S3, and several options to read<a id="_idIndexMarker986"/> data from S3, transform it, and feed it to your consumers for whatever needs you have, and all of these steps are carried out in a highly secure manner. To read in detail<a id="_idIndexMarker987"/> about creating a data lake on S3, hydrating it with your data sources, creating<a id="_idIndexMarker988"/> a data catalog, and securing, managing, and<a id="_idIndexMarker989"/> transforming the data, please refer to the <em class="italic">Building Big Data Storage Solutions</em> white paper (<a href="https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html">https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/building-data-lake-aws.html</a>). For instructions on how to set up your data lake using <strong class="bold">AWS</strong> <strong class="bold">Lake Formation</strong> (a fully managed service to build and manage data lakes), please refer to the getting<a id="_idIndexMarker990"/> started guide (<a href="https://docs.aws.amazon.com/lake-formation/latest/dg/getting-started.html">https://docs.aws.amazon.com/lake-formation/latest/dg/getting-started.html</a>).</p>
			<p>Let's now discuss how to use AWS Glue for data processing.</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor216"/>Using AWS Glue for data processing and transformation tasks</h2>
			<p>This section addresses principles <em class="italic">1.4a</em>, <em class="italic">1.5a</em>, <em class="italic">3.1a</em>, <em class="italic">3.4b</em>, and <em class="italic">3.5a</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>). </p>
			<p>AWS Glue is a fully managed<a id="_idIndexMarker991"/> and serverless data cataloging, processing, and transformation<a id="_idIndexMarker992"/> service that enables you to build end-to-end ETL pipelines providing ready-made connections<a id="_idIndexMarker993"/> to data stores both on-premises and on AWS. The serverless, managed nature of AWS Glue removes the costs associated with infrastructure management<a id="_idIndexMarker994"/> and undifferentiated heavy lifting. AWS Glue enables you to configure connections to your data stores and your S3 data lake to directly pull this data for document pre-processing. You can use Glue ETL jobs to deliver this data after transformation (as required) to an NLP solution pipeline, both for training your custom NLP models and for predictions at inference time. This makes your solution more elegant and efficient, avoiding the need to create multiple solution components to take care of these tasks. </p>
			<p>AWS Glue ETL jobs can be triggered on-demand or scheduled based on your requirements. You can also use it for document post-processing after your NLP solution has completed its recognition<a id="_idIndexMarker995"/> or classification tasks, for persisting to downstream data stores, or for consumption by<a id="_idIndexMarker996"/> operational processes that need this data. For a detailed demonstration of how AWS Glue<a id="_idIndexMarker997"/> can help you, please refer to the following tutorial using <strong class="bold">AWS</strong> <strong class="bold">Glue Studio</strong>, a graphical interface<a id="_idIndexMarker998"/> to make it easy<a id="_idIndexMarker999"/> to interact with Glue when creating<a id="_idIndexMarker1000"/> and running ETL jobs: <a href="https://docs.aws.amazon.com/glue/latest/ug/tutorial-create-job.html">https://docs.aws.amazon.com/glue/latest/ug/tutorial-create-job.html</a></p>
			<p>In the next section, we will review how to use <strong class="bold">Amazon</strong> <strong class="bold">SageMaker</strong> <strong class="bold">Ground Truth</strong> for our text labeling tasks.</p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor217"/>Using Amazon SageMaker Ground Truth for annotations</h2>
			<p>This section addresses principle <em class="italic">1.4b</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p>The accuracy of an NLP model is directly proportional to the quality of the labeled data it is based on. Though we primarily<a id="_idIndexMarker1001"/> use AWS AI services that are pre-trained models, we saw quite a few use cases that needed Amazon Comprehend custom models for entity recognition<a id="_idIndexMarker1002"/> and classification tasks. Amazon Comprehend custom models use <strong class="bold">transfer learning</strong> to train a custom model incrementally from its own pre-trained models with<a id="_idIndexMarker1003"/> data that we provide (that is, data that isunique to our business). And so, for these custom training requirements, we need to provide high-quality labeled data that influences<a id="_idIndexMarker1004"/> the accuracy of our models. As a best practice, we recommend using Amazon SageMaker Ground Truth (<a href="https://aws.amazon.com/sagemaker/groundtruth/">https://aws.amazon.com/sagemaker/groundtruth/</a>) for these labeling tasks. Ground Truth is directly integrated with Amazon Comprehend, and all you need to do is point to the location of the Ground Truth manifest when you set up your Amazon Comprehend job. </p>
			<p>Ground Truth is a fully managed<a id="_idIndexMarker1005"/> service, providing easy-to-use capabilities for data labeling with options<a id="_idIndexMarker1006"/> to either use your own private workforce, third-party data labelers that you can source from <strong class="bold">AWS</strong> <strong class="bold">Marketplace</strong>, or with crowdsourced public data labelers using <strong class="bold">Amazon</strong> <strong class="bold">Mechanical Turk</strong>. </p>
			<p>Ground Truth provides data encryption by default, and it automatically learns from the labeling activities conducted by the human labelers by training an ML model behind the scenes. This ML model will be applied<a id="_idIndexMarker1007"/> to automate the labeling tasks once a confidence threshold has been reached. Ground Truth provides pre-built task templates for various data formats, such as image, text, and video files. You can also create custom templates for your own requirements<a id="_idIndexMarker1008"/> by selecting the <strong class="bold">Custom</strong> task type when creating a labeling job. Please see the following screenshot for different types of text-based labeling tasks supported by Ground Truth:</p>
			<div>
				<div id="_idContainer305" class="IMG---Figure">
					<img src="Images/B17528_18_02.jpg" alt="Figure 18.2 – Types of text labeling tasks in Amazon SageMaker Ground Truth&#13;&#10;" width="1238" height="968"/>
				</div>
			</div>
			<p class="figure-caption">Figure 18.2 – Types of text labeling tasks in Amazon SageMaker Ground Truth</p>
			<p>To get started, you<a id="_idIndexMarker1009"/> create a labeling job, select an S3 location for your dataset, specify an <strong class="bold">IAM</strong> role (or ask Ground Truth to create one for you), select the task category from a list of pre-built templates (or you can select <strong class="bold">Custom</strong> for your own template), and choose<a id="_idIndexMarker1010"/> the labeling workforce who will work on your request. Please refer to the following documentation for more details on how to get started: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-text.html">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-text.html</a></p>
			<p>Now that we know<a id="_idIndexMarker1011"/> how to use Ground Truth for annotations, let's review a new feature that was launched recently to enable <strong class="bold">custom entity recognizer training</strong> directly from <strong class="bold">PDF</strong> or <strong class="bold">Microsoft</strong> <strong class="bold">Word</strong> documents.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor218"/>Using Amazon Comprehend with PDF and Word formats directly</h2>
			<p>This section addresses principle <em class="italic">1.4c</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p class="callout-heading">Note:</p>
			<p class="callout">Amazon Comprehend updated the custom entity recognition feature in September 2021 to support training and inference with PDF and Word documents directly. </p>
			<p>To improve<a id="_idIndexMarker1012"/> the performance efficiency of your NLP solution pipeline, Amazon Comprehend now supports training<a id="_idIndexMarker1013"/> custom entity recognizers directly from PDF and Word document<a id="_idIndexMarker1014"/> formats, without having to run pre-processing<a id="_idIndexMarker1015"/> steps to flatten the document into a machine-readable format. To use this feature, you follow the same steps we specified in <a href="B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162"><em class="italic">Chapter 14</em></a>, <em class="italic">Auditing Named Entity Recognition Workflows</em>, to train an Amazon Comprehend custom entity recognizer, but with a small difference. </p>
			<p class="callout-heading">Note:</p>
			<p class="callout">You still need to annotate your entities in the training document and create an augmented manifest using Ground Truth. For more details, please refer to the instructions in this blog: <a href="https://aws.amazon.com/blogs/machine-learning/custom-document-annotation-for-extracting-named-entities-in-documents-using-amazon-comprehend/">https://aws.amazon.com/blogs/machine-learning/custom-document-annotation-for-extracting-named-entities-in-documents-using-amazon-comprehend/</a> </p>
			<p>Please use the following<a id="_idIndexMarker1016"/> steps to train and infer from PDF or Word<a id="_idIndexMarker1017"/> documents directly with Amazon Comprehend:</p>
			<ol>
				<li>Log in to your AWS Management Console (please refer to the <em class="italic">Technical requirements</em> section for more details) and navigate to the <strong class="bold">Amazon Comprehend</strong> Console by typing <strong class="source-inline">comprehend</strong> in the <strong class="bold">Services</strong> search bar.</li>
				<li>Click on <strong class="bold">Custom entity recognition</strong> on the left pane and then click on <strong class="bold">Create new model</strong>.</li>
				<li>In the <strong class="bold">Model settings</strong> section, provide the name for your model and scroll down to the <strong class="bold">Data specifications</strong> section to select the <strong class="bold">Augmented manifest</strong> and <strong class="bold">PDF, Word documents</strong> formats for training. Provide the S3 location for your augmented manifest. Scroll down to select or create an IAM role and click on <strong class="bold">Create</strong> to start<a id="_idIndexMarker1018"/> the training. Once the model is trained, you can run<a id="_idIndexMarker1019"/> an inference using the same steps we discussed in <a href="B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162"><em class="italic">Chapter 14</em></a>, <em class="italic">Auditing Named Entity Recognition Workflows</em>. But, provide a PDF or Word document as an input instead of a CSV file.</li>
			</ol>
			<div>
				<div id="_idContainer306" class="IMG---Figure">
					<img src="Images/B17528_18_03.jpg" alt="Figure 18.3 – Amazon Comprehend custom entity recognizer training using PDF, Word documents&#13;&#10;" width="781" height="612"/>
				</div>
			</div>
			<p class="figure-caption">Figure 18.3 – Amazon Comprehend custom entity recognizer training using PDF, Word documents</p>
			<p>This feature update<a id="_idIndexMarker1020"/> improves pre-processing efficiency and reduces upfront time investments in setting<a id="_idIndexMarker1021"/> up our NLP solution<a id="_idIndexMarker1022"/> pipelines. In the next<a id="_idIndexMarker1023"/> section, we will review how to enforce access control when building NLP solutions.</p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor219"/>Enforcing least privilege access</h2>
			<p>This section<a id="_idIndexMarker1024"/> addresses principles <em class="italic">1.2c</em>, <em class="italic">3.2b</em>, and <em class="italic">4.2a</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p>One of the core tenets of a highly secure architecture is enforcing what is called the <em class="italic">least privilege for access to resources</em>. <strong class="bold">AWS</strong> <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) is a security service that enables defining and implementing your authentication and authorization<a id="_idIndexMarker1025"/> strategy for secured access to your AWS infrastructure<a id="_idIndexMarker1026"/> for your users. With IAM, you can create permissions policies that are attached to roles or users (<em class="italic">identities</em>) that define what (<em class="italic">actions</em>) the identity can or cannot do with AWS services (<em class="italic">resources</em>). Least privilege, as the name indicates, is all about defining highly restrictive permissions policies for your users and roles. The default permission in AWS is a <em class="italic">deny</em>. </p>
			<p>If no policies are specified for a user, the user does not have permission to do anything in AWS. So, you add policy statements that allow a user or a role to perform specific tasks using AWS services or resources. In our examples in this book, due to the nature of our use cases and for the sake<a id="_idIndexMarker1027"/> of simplicity and ease of configuration, we suggest you add managed permissions policies such as <em class="italic">TextractFullAccess</em> or <em class="italic">ComprehendFullAccess</em> to your <strong class="bold">SageMaker execution IAM role</strong> for your notebook. When you build your NLP solution and promote it to production, as a best practice, you should enforce least privilege access. Let's discuss what this means through an example. The <em class="italic">ComprehendFullAccess</em> permissions policy is defined by the following <strong class="bold">JSON</strong> statement:</p>
			<p class="source-code">{</p>
			<p class="source-code">    "Version": "2012-10-17",</p>
			<p class="source-code">    "Statement": [</p>
			<p class="source-code">        {</p>
			<p class="source-code">            "Action": [</p>
			<p class="source-code">                <strong class="bold">"comprehend:*"</strong>,</p>
			<p class="source-code">                "s3:ListAllMyBuckets",</p>
			<p class="source-code">                "s3:ListBucket",</p>
			<p class="source-code">                "s3:GetBucketLocation",</p>
			<p class="source-code">                "iam:ListRoles",</p>
			<p class="source-code">                "iam:GetRole"</p>
			<p class="source-code">            ],</p>
			<p class="source-code">            "Effect": "Allow",</p>
			<p class="source-code">            <strong class="bold">"Resource": "*"</strong></p>
			<p class="source-code">        }</p>
			<p class="source-code">    ]</p>
			<p class="source-code">}    </p>
			<p>If you refer to the highlighted section in the preceding JSON code, the wildcard (<strong class="source-inline">*</strong>) attached to <strong class="source-inline">"comprehend"</strong> indicates that all Amazon Comprehend API actions are allowed for the role or the user that wields this policy. This is not a restrictive policy but rather provides a broad set of permissions. </p>
			<p>To enforce least privilege<a id="_idIndexMarker1028"/> access, a new policy should be created that should<a id="_idIndexMarker1029"/> be changed as shown in the following JSON statement: </p>
			<p class="source-code">{</p>
			<p class="source-code">    "Version": "2012-10-17",</p>
			<p class="source-code">    "Statement": [</p>
			<p class="source-code">        {</p>
			<p class="source-code">            "Action": [</p>
			<p class="source-code">                <strong class="bold">"comprehend:DetectEntities"</strong>,</p>
			<p class="source-code">                <strong class="bold">"comprehend:BatchDetectEntities"</strong>,</p>
			<p class="source-code">                <strong class="bold">"comprehend:DetectPiiEntities"</strong>,</p>
			<p class="source-code">                <strong class="bold">"comprehend:ContainsPiiEntities"</strong>,</p>
			<p class="source-code">                "s3:ListAllMyBuckets",</p>
			<p class="source-code">                "s3:ListBucket",</p>
			<p class="source-code">                "s3:GetBucketLocation",</p>
			<p class="source-code">                "iam:ListRoles",</p>
			<p class="source-code">                "iam:GetRole"</p>
			<p class="source-code">            ],</p>
			<p class="source-code">            "Effect": "Allow",</p>
			<p class="source-code">            "Resource": "*"</p>
			<p class="source-code">        }</p>
			<p class="source-code">    ]</p>
			<p class="source-code">}</p>
			<p>In this changed JSON statement, we provide restrictive permissions that allow a user or a role to only use the <strong class="bold">entities detection</strong> feature in Amazon Comprehend. A good approach would be to only provide those permissions<a id="_idIndexMarker1030"/> that are absolutely needed for a user or role to perform a task. Also, you will have to ensure that you monitor IAM roles and policy assignments to ensure that<a id="_idIndexMarker1031"/> you clean up the permissions once the user or role has completed the task. This way, you avoid the situation of an old permission granting a user more access than they need. AWS provides a feature called <strong class="bold">IAM Access Analyzer</strong> to proactively monitor permissions and take actions<a id="_idIndexMarker1032"/> as required. For a detailed introduction<a id="_idIndexMarker1033"/> to Access Analyzer, please refer to the following documentation: <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html">https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html</a>.</p>
			<p>In the next section, we will review how to protect sensitive data during our NLP solution building task.</p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor220"/>Obfuscating sensitive data</h2>
			<p>This section addresses principles <em class="italic">1.2a</em> and <em class="italic">4.2b</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p>Protecting the<a id="_idIndexMarker1034"/> confidentiality of your data is highly important. Enterprises typically classify their data into categories such as <em class="italic">public</em>, <em class="italic">confidential</em>, <em class="italic">secret</em>, and <em class="italic">top-secret</em>, and apply controls and guardrails based on these classifications. If you are unsure of how to classify your data, please refer to an existing data classification model such as the <strong class="bold">US National Classification Scheme</strong>. More details on this model, as well as best practices for data classification as recommended by AWS, can be found in the following documentation: <a href="https://docs.aws.amazon.com/whitepapers/latest/data-classification/welcome.html">https://docs.aws.amazon.com/whitepapers/latest/data-classification/welcome.html</a>. </p>
			<p>Once you have classified<a id="_idIndexMarker1035"/> your data, the next step is to determine the type of confidentiality your data contains. Data can be <strong class="bold">personally Identifiable Information</strong> (<strong class="bold">PII</strong>), for example, it may contain social security numbers, credit<a id="_idIndexMarker1036"/> card numbers, bank account numbers, and so on. Or, if your data contains your customers' private health records, it is called <strong class="bold">protected health information</strong> (<strong class="bold">PHI</strong>). If you are in the legal industry, the <em class="italic">Attorney-Client Privileged Information</em> is protected data and must be kept confidential. </p>
			<p>So, as we can see, data protection is vitally important and must be a key consideration in our NLP solution development. When building on AWS, there are multiple ways in which you can protect<a id="_idIndexMarker1037"/> your confidential data, including data encryption at rest and in transit, which we cover in subsequent sections in this chapter. For details on how AWS supports the highest privacy<a id="_idIndexMarker1038"/> standards, and information about the resources to help you protect your customers' data, please refer to the following link: <a href="https://aws.amazon.com/compliance/data-privacy/">https://aws.amazon.com/compliance/data-privacy/</a></p>
			<p>The following screenshot shows the results of an Amazon Comprehend PII detection in real time.</p>
			<div>
				<div id="_idContainer307" class="IMG---Figure">
					<img src="Images/B17528_18_04.jpg" alt="Figure 18.4 – Amazon Comprehend PII detection&#13;&#10;" width="1319" height="614"/>
				</div>
			</div>
			<p class="figure-caption">Figure 18.4 – Amazon Comprehend PII detection</p>
			<p>In <a href="B17528_04_Final_SB_ePub.xhtml#_idTextAnchor063"><em class="italic">Chapter 4</em></a>, <em class="italic">Automating Document Processing Workflows</em>, we reviewed how Amazon Comprehend provides support for detecting PII entities from your data. You can use this capability as part of your document pre-processing stage by means of the <strong class="bold">AWS</strong> <strong class="bold">S3</strong> <strong class="bold">Object Access Lambda</strong>. This service can be used to detect and redact PII data from your S3 bucket before using them in your NLP pipelines, if that is your requirement. The S3 Object Access Lambda<a id="_idIndexMarker1039"/> allows you to execute an AWS <strong class="source-inline">Lambda</strong> function automatically to either process or transform your data when you fetch it from Amazon S3. For PII detection, two <strong class="source-inline">Lambda</strong> functions are made available to be used with S3 Object Access<a id="_idIndexMarker1040"/> Lambda. The Amazon Comprehend <strong class="source-inline">ContainsPiiEntites</strong> API is used to classify documents that contain PII data and the <strong class="source-inline">DetectPiiEntities</strong> API is used to identify the actual PII data within the document for the purposes of redaction. For a tutorial on how to detect and redact PII data from your documents using the S3 Object Access Lambda, please refer to this GitHub repository: <a href="https://github.com/aws-samples/amazon-comprehend-s3-object-lambda-functions">https://github.com/aws-samples/amazon-comprehend-s3-object-lambda-functions</a></p>
			<p>In the next section, we will review how to implement data protection at rest and in transit.</p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor221"/>Protecting data at rest and in transit</h2>
			<p>This section addresses principles <em class="italic">1.2b</em>, <em class="italic">2.2a</em>, <em class="italic">3.2a</em>, and <em class="italic">4.3b</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p>Now that we have discussed least privilege and data confidentiality, let's review the best practices for protecting your data <em class="italic">at rest</em> and <em class="italic">in transit</em> (that is, when it resides in a data store and during transit, for example, as a result of service API calls). When we talk about<a id="_idIndexMarker1041"/> data protection at rest, we refer<a id="_idIndexMarker1042"/> to encrypting your data<a id="_idIndexMarker1043"/> during storage in AWS. Your data can reside in an<a id="_idIndexMarker1044"/> Amazon S3 data lake, a relational database in <strong class="bold">Amazon</strong> <strong class="bold">RDS</strong> (a managed AWS service<a id="_idIndexMarker1045"/> for relational databases), in <strong class="bold">Amazon</strong> <strong class="bold">Redshift</strong> (an exabyte-scale, cloud-based data warehouse), in <strong class="bold">Amazon</strong> <strong class="bold">DynamoDB</strong>, or in one of the other<a id="_idIndexMarker1046"/> purpose-built<a id="_idIndexMarker1047"/> databases such as <strong class="bold">Amazon</strong> <strong class="bold">DocumentDB</strong> (a managed<a id="_idIndexMarker1048"/> AWS service for <strong class="bold">MongoDB</strong>), or <strong class="bold">Amazon</strong> <strong class="bold">Neptune</strong> (a managed AWS<a id="_idIndexMarker1049"/> service for <strong class="bold">Graph</strong> databases), and many more. </p>
			<p>With AWS, the advantage is that you can enable encryption to protect your data at rest easily using <strong class="bold">AWS</strong> <strong class="bold">Key Management Service</strong> (<strong class="bold">KMS</strong>) (a reliable and secure service to create, manage, and protect<a id="_idIndexMarker1050"/> your encryption keys, and for applying<a id="_idIndexMarker1051"/> the encryption of data across many services in AWS). Encryption is supported using the <strong class="bold">AES-256</strong> standard (<a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard">https://en.wikipedia.org/wiki/Advanced_Encryption_Standard</a>). </p>
			<p>For example, when you store<a id="_idIndexMarker1052"/> objects in Amazon S3, you can request <strong class="bold">Server-side encryption</strong> (encrypting data at the destination as it is stored in S3) by selecting to either use S3-managed encryption keys (that is, keys you create in KMS at the bucket level) or your own encryption keys (which you provide when you upload the objects to your S3 bucket). </p>
			<div>
				<div id="_idContainer308" class="IMG---Figure">
					<img src="Images/B17528_18_05.jpg" alt="Figure 18.5 – Enabling encryption for your S3 bucket" width="710" height="401"/>
				</div>
			</div>
			<p class="figure-caption">Figure 18.5 – Enabling encryption for your S3 bucket</p>
			<p>Amazon Redshift provides<a id="_idIndexMarker1053"/> similar options at cluster creation time to encrypt your data (<a href="https://docs.aws.amazon.com/redshift/latest/mgmt/security-server-side-encryption.html">https://docs.aws.amazon.com/redshift/latest/mgmt/security-server-side-encryption.html</a>).</p>
			<p>Amazon DynamoDB encrypts all your data<a id="_idIndexMarker1054"/> by default using AWS KMS (<a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html">https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html</a>).</p>
			<p>You can enable<a id="_idIndexMarker1055"/> encryption for your Amazon RDS databases (<a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html">https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Overview.Encryption.html</a>).</p>
			<p>You can also enable encryption<a id="_idIndexMarker1056"/> for any purpose-built AWS databases, such as Amazon DocumentDB (<a href="https://docs.aws.amazon.com/documentdb/latest/developerguide/encryption-at-rest.html">https://docs.aws.amazon.com/documentdb/latest/developerguide/encryption-at-rest.html</a>). </p>
			<p>All AWS services<a id="_idIndexMarker1057"/> that handle customer data support encryption. Please refer to this blog for more details: <a href="https://aws.amazon.com/blogs/security/importance-of-encryption-and-how-aws-can-help/">https://aws.amazon.com/blogs/security/importance-of-encryption-and-how-aws-can-help/</a><span class="hidden"> </span></p>
			<p>To protect data in transit, you secure your API endpoints using a protocol such as <strong class="bold">Transport Layer Security</strong> (<strong class="bold">TLS</strong>): <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security">https://en.wikipedia.org/wiki/Transport_Layer_Security</a><span class="hidden"> </span></p>
			<p>Similar to protecting<a id="_idIndexMarker1058"/> data at rest, AWS provides the means to secure<a id="_idIndexMarker1059"/> your data in transit using <strong class="bold">AWS</strong> <strong class="bold">Certificate Manager</strong> (a managed service to provision and manage TLS certificates) to secure<a id="_idIndexMarker1060"/> communications, verify<a id="_idIndexMarker1061"/> identities, and implement <strong class="bold">HTTPS</strong> endpoints for application interactions. All AWS services that handle customer data are secured using TLS with HTTPS. </p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor222"/>Using Amazon API Gateway for request throttling</h2>
			<p>This section addresses principle <em class="italic">2.1a</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p>When we build Amazon<a id="_idIndexMarker1062"/> Comprehend custom entity recognizers or classifiers, we host these models<a id="_idIndexMarker1063"/> by creating Comprehend real-time endpoints, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer309" class="IMG---Figure">
					<img src="Images/B17528_18_06.jpg" alt="Figure 18.6 – Creating Amazon Comprehend real-time endpoints&#13;&#10;" width="808" height="535"/>
				</div>
			</div>
			<p class="figure-caption">Figure 18.6 – Creating Amazon Comprehend real-time endpoints</p>
			<p>You can call these endpoints<a id="_idIndexMarker1064"/> directly from your code to detect entities or for text classification needs, as shown in the following<a id="_idIndexMarker1065"/> code snippet: </p>
			<p class="source-code">response = comprehend.detect_entities(Text=entry,</p>
			<p class="source-code">                    LanguageCode='en',</p>
			<p class="source-code">                    <strong class="bold">EndpointArn='endpoint-arn'</strong></p>
			<p class="source-code">            )</p>
			<p>We will talk about how to set up auto scaling for your Amazon Comprehend real-time endpoints in a subsequent section, but you are billed by the second for your inference endpoints (<a href="https://aws.amazon.com/comprehend/pricing/">https://aws.amazon.com/comprehend/pricing/</a>), and the capacity is measured in inference<a id="_idIndexMarker1066"/> units that represent a throughput of 100 characters per second. Throttling requests to the endpoint will allow<a id="_idIndexMarker1067"/> for a more managed use of capacity. <strong class="bold">Amazon</strong> <strong class="bold">API Gateway</strong> (<a href="https://aws.amazon.com/api-gateway/">https://aws.amazon.com/api-gateway/</a>) is a fully managed, secure, and scalable service for API management that can be used to create an API to abstract the calls<a id="_idIndexMarker1068"/> to the Amazon Comprehend endpoint by using an AWS Lambda function, as demonstrated in the tutorial in the following link: <a href="https://github.com/aws-samples/amazon-comprehend-custom-entity-recognizer-api-example">https://github.com/aws-samples/amazon-comprehend-custom-entity-recognizer-api-example</a></p>
			<p>Apart from throttling, API Gateway also supports traffic management, access control, monitoring, and version<a id="_idIndexMarker1069"/> management, which can help implement a robust approach for handling requests for our solution. For more details, please refer to the following documentation: <a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-api-as-simple-proxy-for-lambda.html</a></p>
			<p>In the next section, we will cover how to set up auto scaling for your Amazon Comprehend real-time endpoints.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor223"/>Setting up auto scaling for Amazon Comprehend endpoints</h2>
			<p>This section addresses principle <em class="italic">2.3a</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p>In the previous section, we discussed<a id="_idIndexMarker1070"/> that you need endpoints to enable real-time predictions from your Amazon Comprehend<a id="_idIndexMarker1071"/> custom models. The endpoint inference capacity is denoted as an <strong class="bold">Inference Unit</strong> (<strong class="bold">IU</strong>), which represents a throughput of 100 characters per second. When you create an endpoint, you specify the number of IUs you need, which<a id="_idIndexMarker1072"/> helps Amazon Comprehend determine the resources to allocate to your endpoint. You calculate IUs based on the output throughput you need from the endpoint in terms of characters per second, and you are charged for the duration of when the endpoint is active, irrespective of whether it is receiving requests or not. So, you need to manage the IUs carefully to ensure you receive the required capacity when needed (for performance) but can also discard the capacity when not<a id="_idIndexMarker1073"/> needed (to save costs). You can do this using Amazon Comprehend auto scaling: <a href="https://docs.aws.amazon.com/comprehend/latest/dg/comprehend-autoscaling.html">https://docs.aws.amazon.com/comprehend/latest/dg/comprehend-autoscaling.html</a></p>
			<p>You can set up auto scaling only by using the <strong class="bold">AWS</strong> <strong class="bold">Command Line Interface</strong> (<strong class="bold">AWS CLI</strong>). The following example shows<a id="_idIndexMarker1074"/> how to enable auto scaling for<a id="_idIndexMarker1075"/> custom entity recognition:</p>
			<ol>
				<li value="1">Register a scalable target<a id="_idIndexMarker1076"/> by running the following code snippet in the AWS CLI. Here <strong class="source-inline">scalable-dimension</strong> refers to the Amazon Comprehend resource type along with the unit of measurement for capacity (IUs):<p class="source-code">aws application-autoscaling register-scalable-target \</p><p class="source-code">--service-namespace comprehend \</p><p class="source-code">--region &lt;region&gt; \</p><p class="source-code">--resource-id &lt;your-comprehend-custom-endpoint&gt; \</p><p class="source-code">--scalable-dimension comprehend:entity-recognizer-endpoint:DesiredInferenceUnits \</p><p class="source-code">--min-capacity 1 \</p><p class="source-code">--max-capacity 2</p></li>
				<li>You then create a JSON configuration for what target you would like to track, as shown in the following code snippet:<p class="source-code">{</p><p class="source-code">"TargetValue": 90,</p><p class="source-code">"PredefinedMetricSpecification": </p><p class="source-code">{</p><p class="source-code">"PredefinedMetricType": "ComprehendInferenceUtilization"</p><p class="source-code">}</p><p class="source-code">}</p></li>
				<li>Finally, you put this scaling policy into action, as shown in the following code snippet:<p class="source-code">aws application-autoscaling put-scaling-policy \</p><p class="source-code">--service-namespace comprehend \</p><p class="source-code">--region &lt;region&gt; \</p><p class="source-code">--scalable-dimension comprehend:entity-recognizer-endpoint:DesiredInferenceUnits \</p><p class="source-code">--resource-id &lt;your-comprehend-custom-endpoint&gt; \</p><p class="source-code">--policy-name CERPolicy \</p><p class="source-code">--policy-type TargetTrackingScaling \</p><p class="source-code">--target-tracking-scaling-policy-configuration file://config.json </p></li>
			</ol>
			<p>In the next section, we will<a id="_idIndexMarker1077"/> review how to monitor training metrics for our custom models to enable<a id="_idIndexMarker1078"/> proactive actions.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor224"/>Automating monitoring of custom training metrics</h2>
			<p>This section addresses principle <em class="italic">4.3a</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p>When training your custom classification<a id="_idIndexMarker1079"/> or entity recognition models, Amazon<a id="_idIndexMarker1080"/> Comprehend generates <strong class="bold">metrics</strong> (<a href="https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html">https://docs.aws.amazon.com/comprehend/latest/dg/cer-metrics.html</a>), such as F1 score, precision, recall, and more, based on<a id="_idIndexMarker1081"/> the evaluation it performed with the trained<a id="_idIndexMarker1082"/> model. You can use the <strong class="source-inline">DescribeEntityRecognizer</strong> API (for entity recognition) or <strong class="source-inline">DescribeDocumentClassifier</strong> API (for classification) to get the evaluation metrics for your custom model. </p>
			<p>The following is a code snippet of how to use the <strong class="source-inline">DescribeEntityRecognizer</strong> API:</p>
			<p class="source-code">comprehend = boto3.client('comprehend')</p>
			<p class="source-code">response = comprehend.describe_entity_recognizer(</p>
			<p class="source-code">    EntityRecognizerArn='&lt;arn-of-your-entity-recognizer&gt;'</p>
			<p class="source-code">) </p>
			<p>To monitor for the completion<a id="_idIndexMarker1083"/> of the Amazon Comprehend custom training job, you can use <strong class="bold">Amazon</strong> <strong class="bold">EventBridge</strong> (a managed serverless event bus) to create an event that is enabled when a training job<a id="_idIndexMarker1084"/> is submitted, and which runs an AWS Lambda function to monitor the status of the<a id="_idIndexMarker1085"/> training job in periodic intervals. When training completes, this AWS Lambda function will use the <strong class="source-inline">DescribeEntityRecognizer</strong> or <strong class="source-inline">DescribeDocumentClassifier</strong> APIs to retrieve the evaluation metrics. If the metrics are below a threshold, this function<a id="_idIndexMarker1086"/> can send alerts or notifications using <strong class="bold">Amazon</strong> <strong class="bold">Simple Notification Service</strong> (<strong class="bold">SNS</strong>). For details on how to schedule<a id="_idIndexMarker1087"/> an event using Amazon EventBridge, please refer to the documentation: <a href="https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html">https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html</a>.<span class="hidden"> </span></p>
			<p>In the next section, we will look at using Amazon A2I to set up human loops to review predictions.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor225"/>Using Amazon A2I to review predictions</h2>
			<p>This section addresses principle <em class="italic">2.3b</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p>We covered using <strong class="bold">Amazon</strong> <strong class="bold">Augmented AI</strong> (<strong class="bold">Amazon</strong> <strong class="bold">A2I</strong>) (a managed service to set up human<a id="_idIndexMarker1088"/> reviews of ML predictions) in great<a id="_idIndexMarker1089"/> detail in many of the previous chapters in this book, starting with <a href="B17528_13_Final_SB_ePub.xhtml#_idTextAnchor151"><em class="italic">Chapter 13</em></a>, <em class="italic">Improving the Accuracy of Document Processing Workflows</em>. When your solution is newly developed, it is a best practice to set up a human loop for prediction<a id="_idIndexMarker1090"/> reviews, auditing, and making corrections as needed. Your solution should also include a feedback loop with model re-training based<a id="_idIndexMarker1091"/> on human-reviewed data. We recommend having a human loop with Amazon A2I for the first three to six months to allow your solution to evolve based on direct feedback. Subsequently, you can disable the human loop. </p>
			<p>In the next section, we will cover how to build modular, loosely coupled solutions using <strong class="bold">Async</strong> <strong class="bold">APIs</strong>.</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor226"/>Using Async APIs for loose coupling</h2>
			<p>This section addresses principles <em class="italic">2.4a</em> and <em class="italic">4.4a</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p>When we set up an NLP solution pipeline that is required to scale to processing millions of documents, it is a good idea to use the <strong class="bold">Asynchronous Batch APIs</strong> to implement this architecture. Synchronous APIs<a id="_idIndexMarker1092"/> follow the request-response paradigm, meaning the requesting application will wait for a response and will be held up until a response is received. This approach works<a id="_idIndexMarker1093"/> well when the need is to process a few documents quickly for a real-time or near-real-time, mission-critical requirement. However, when the<a id="_idIndexMarker1094"/> document volume increases, a synchronous approach will hold compute resources, and slow down the process. Typically, organizations implement two separate NLP solution pipelines: one for real-time processing, and a second for batch processing. For batch processing, depending on the number of documents to be processed, the inference results are available after a few minutes to a few hours, depending on how the architecture is set up.</p>
			<p>With Amazon Comprehend, once the entity recognizer or classifier training is completed, use the <strong class="source-inline">Batch</strong> API when you need to run <strong class="source-inline">inference</strong> for large document volumes, as shown in the following code snippets for entity recognition.</p>
			<ol>
				<li value="1">First, we submit an <strong class="source-inline">entities detection</strong> job (if we provide the endpoint <strong class="source-inline">ARN</strong>, it will use our custom entity recognizer). The response returns a <strong class="source-inline">JobId</strong>, a <strong class="source-inline">Job ARN</strong>, and a <strong class="source-inline">job status</strong>. Once the job is completed, the results are sent to the S3 location you specify in the <strong class="source-inline">OutputDataConfig</strong>:<p class="source-code">response = comprehend.start_entities_detection_job(</p><p class="source-code">    InputDataConfig={</p><p class="source-code">        'S3Uri': '&lt;s3-location-input-documents&gt;',</p><p class="source-code">        'InputFormat': 'ONE_DOC_PER_FILE'|'ONE_DOC_PER_LINE'</p><p class="source-code">    },</p><p class="source-code">    OutputDataConfig={</p><p class="source-code">        'S3Uri': '&lt;s3-location-output-results&gt;'</p><p class="source-code">    }, </p><p class="source-code">    DataAccessRoleArn='&lt;IAM-role&gt;',</p><p class="source-code">    JobName='&lt;provide a job name&gt;',</p><p class="source-code">    EntityRecognizerArn='&lt;ARN of your custom entity recognizer&gt;')</p><p>When using Amazon Textract for processing large document volumes, you can use the <strong class="source-inline">Batch</strong> APIs to first submit a text analysis or detection job, and then subsequently get the extraction results once the analysis job is completed. The following steps show how you can use the Amazon Textract Batch APIs.</p><p>Let's assume our use<a id="_idIndexMarker1095"/> case is to process documents<a id="_idIndexMarker1096"/> that contain tables and form data along with text. In this case, we will use the <strong class="source-inline">StartDocumentAnalysis</strong> API (<a href="https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html">https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html</a>) as a first step, and ask it to look for table and form contents. Text in paragraphs is extracted by default. We also pass an Amazon SNS <strong class="source-inline">topic</strong> and an <strong class="source-inline">IAM role</strong> that provides permissions for Amazon Textract to publish a message to the SNS <strong class="source-inline">topic</strong>. This API returns a <strong class="source-inline">JobId</strong> that we will use in the next step:</p><p class="source-code">textract = boto3.client('textract') </p><p class="source-code">job = textract.start_document_analysis(</p><p class="source-code">    DocumentLocation={</p><p class="source-code">        'S3Object': {</p><p class="source-code">            'Bucket': '&lt;s3-bucket-name&gt;',</p><p class="source-code">            'Name': '&lt;file-name&gt;'</p><p class="source-code">        }</p><p class="source-code">    },</p><p class="source-code">    FeatureTypes=[</p><p class="source-code">        'TABLES'|'FORMS',</p><p class="source-code">    ],</p><p class="source-code">    NotificationChannel={</p><p class="source-code">        'SNSTopicArn': '&lt;SNS-topic-arn&gt;',</p><p class="source-code">        'RoleArn': '&lt;IAM-role-arn&gt;'</p><p class="source-code">    }</p><p class="source-code">)</p></li>
				<li>When the job completes, Amazon Textract sends a message to the SNS topic indicating the job<a id="_idIndexMarker1097"/> status. You can attach an AWS Lambda<a id="_idIndexMarker1098"/> function to this SNS topic as an event trigger. This Lambda function will call the <strong class="source-inline">GetDocumentAnalysis</strong> API (<a href="https://docs.aws.amazon.com/textract/latest/dg/API_GetDocumentAnalysis.html">https://docs.aws.amazon.com/textract/latest/dg/API_GetDocumentAnalysis.html</a>) to retrieve the results from<a id="_idIndexMarker1099"/> the Amazon Textract job, as shown in the following code snippet:<p class="source-code">textract_results = textract.get_document_analysis(</p><p class="source-code">    JobId='&lt;JobID that was returned in the previous step&gt;',</p><p class="source-code">    NextToken='&lt;pagination-token&gt;'</p><p class="source-code">)</p></li>
			</ol>
			<p>The response is a JSON object of blocks of text data that include both tabular and form content. In the next section, we will discuss how we can simplify the parsing of the JSON response object using Amazon Textract Response Parser.</p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor227"/>Using Amazon Textract Response Parser</h2>
			<p>This section addresses principle <em class="italic">3.4a</em> from the Well-Architected NLP solutions matrix (<em class="italic">Figure 18.1</em>).</p>
			<p>The JSON documents returned by the Amazon Textract APIs are comprehensive, with document contents categorized<a id="_idIndexMarker1100"/> as blocks that encapsulate information for pages, lines, words, tables, forms, and the relationships between them. When using Amazon Textract to process complex<a id="_idIndexMarker1101"/> or descriptive documents, it can seem time-consuming to understand the JSON results and parse them to obtain the data we need from the various ways text is contained in the document. The following code snippet shows the JSON response for a line that Amazon Textract extracted from the document we used in <a href="B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162"><em class="italic">Chapter 14</em></a>, <em class="italic">Auditing Named Entity Recognition Workflows</em>: </p>
			<p class="source-code">{'BlockType': 'LINE',</p>
			<p class="source-code">   'Confidence': 98.13241577148438,</p>
			<p class="source-code">   'Text': 'Lender Loan No./Universal Loan Identifier',</p>
			<p class="source-code">   'Geometry': {'BoundingBox': {'Width': 0.1989699900150299,</p>
			<p class="source-code">     'Height': 0.008062363602221012,</p>
			<p class="source-code">     'Left': 0.06528056412935257,</p>
			<p class="source-code">     'Top': 0.06330667436122894},</p>
			<p class="source-code">    'Polygon': [{'X': 0.06528056412935257, 'Y': 0.06330667436122894},</p>
			<p class="source-code">     {'X': 0.2642505466938019, 'Y': 0.06330667436122894},</p>
			<p class="source-code">     {'X': 0.2642505466938019, 'Y': 0.07136903703212738},</p>
			<p class="source-code">     {'X': 0.06528056412935257, 'Y': 0.07136903703212738}]},</p>
			<p class="source-code">   'Id': '678695ec-6c9c-4943-9dad-2d64fc5acc44',</p>
			<p class="source-code">   'Relationships': [{'Type': 'CHILD',</p>
			<p class="source-code">     'Ids': ['2600b0dc-ee1b-421b-a7f6-49de293c7b20',</p>
			<p class="source-code">      '70e20616-32b6-45f6-970d-e1a268ee97ec',</p>
			<p class="source-code">      '69792a6d-5df6-4729-8d25-b1f3b05a8cd5',</p>
			<p class="source-code">      'dfc16ed6-a526-46ac-98f3-f50453354c03',</p>
			<p class="source-code">      '71a1f5a2-3ff3-40de-9e58-3288f2ac83ee']}]},   </p>
			<p>So, in order to make the process of retrieving the content we need from the JSON output simpler, the <strong class="bold">Amazon</strong> <strong class="bold">Textract Response Parser</strong> library (or <strong class="bold">TRP</strong>) (<a href="https://github.com/aws-samples/amazon-textract-response-parser">https://github.com/aws-samples/amazon-textract-response-parser</a>) was created. TRP makes it easy to get<a id="_idIndexMarker1102"/> all the data we need with very few lines of code and improves the efficiency of our overall solution. We have already used TRP in this book, for example, in <a href="B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162"><em class="italic">Chapter 14</em></a>, <em class="italic">Auditing Named Entity Recognition Workflows</em>, and <a href="B17528_17_Final_SB_ePub.xhtml#_idTextAnchor202"><em class="italic">Chapter 17</em></a>, <em class="italic">Visualizing Insights from Handwritten Content</em>. </p>
			<p>The following code snippets<a id="_idIndexMarker1103"/> show how to install and use the TRP library:  </p>
			<ol>
				<li value="1">To install the TRP library, use the following code snippet:<p class="source-code">!python -m pip install amazon-textract-response-parser</p></li>
				<li>Import the library, call the <strong class="source-inline">Textract</strong> API to analyze a document, and use <strong class="source-inline">TRP</strong> to parse the results:<p class="source-code">from trp import Document</p><p class="source-code">textract = boto3.client('textract')</p><p class="source-code">response = textract.analyze_document(Document={'Bytes': bytes_test}, FeatureTypes=['TABLES','FORMS'])</p><p class="source-code">text = Document(response)</p></li>
				<li>Now, we can loop through<a id="_idIndexMarker1104"/> the results to extract data from pages, tables, and more. For a <strong class="bold">Python</strong> example of how to use TRP, please refer to the code sample: <a href="https://github.com/aws-samples/amazon-textract-response-parser/tree/master/src-python#parse-json-response-from-textract">https://github.com/aws-samples/amazon-textract-response-parser/tree/master/src-python#parse-json-response-from-textract</a><p class="source-code">for page in text.pages:</p><p class="source-code">            for table in page.tables:</p><p class="source-code">for r, row in enumerate(table.rows):</p><p class="source-code">for c, cell in enumerate(row.cells):</p></li>
			</ol>
			<p>In the next section, we will review why it is important to persist the prediction results from our NLP solution, and how we can make use of this data.</p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor228"/>Persisting prediction results</h2>
			<p>During the course of this book, we have seen examples where the results of an entity recognition or classification task are sent to an <strong class="bold">Amazon</strong> <strong class="bold">Elasticsearch</strong> instance (for metadata extraction) or to Amazon DynamoDB (for persistence). We also saw examples where these results<a id="_idIndexMarker1105"/> are used to inform decisions that impact<a id="_idIndexMarker1106"/> downstream systems. The reason for this is because we often see with organizations that document processing provides important inputs to their mainstream operations. So, when you design and build NLP solutions, you have to keep in mind how your prediction results are going to be consumed, by whom, and for what purpose. Depending on the consumption use case, there are different options available for us to use. Let's review some of these options:</p>
			<ul>
				<li>If the need is for real-time access to inference results, set up an API Gateway and AWS Lambda function to abstract the Amazon Comprehend real-time endpoint. Persist the inference request and response in an Amazon S3 bucket or Amazon DynamoDB for future reference. Please refer to the <em class="italic">Using API Gateway for request throttling</em> section for more details. </li>
				<li>If the results are to be sent to downstream applications that need these inputs for decision-making or functional requirements, you can persist the results to Amazon S3, or an Amazon RDS database, or any of the purpose-built data stores in AWS. To notify the applications that new results are available, you can publish a message to an Amazon SNS topic or use an event trigger in the data stores. For more information, please refer to the following: <a href="https://docs.aws.amazon.com/lambda/latest/dg/services-rds-tutorial.html">https://docs.aws.amazon.com/lambda/latest/dg/services-rds-tutorial.html</a></li>
				<li>If you need the results to populate<a id="_idIndexMarker1107"/> a knowledge repository or make it available for user search, send<a id="_idIndexMarker1108"/> it to an Amazon Elasticsearch (now called <strong class="bold">OpenSearch</strong>) index. For more information, please refer to the following: <a href="https://docs.aws.amazon.com/opensearch-service/latest/developerguide/search-example.html">https://docs.aws.amazon.com/opensearch-service/latest/developerguide/search-example.html</a></li>
				<li>If you would like to use the results for business intelligence or visualization, you can send the results<a id="_idIndexMarker1109"/> to an Amazon S3 bucket and use <strong class="bold">Amazon</strong> <strong class="bold">QuickSight</strong> with the data in Amazon S3. For<a id="_idIndexMarker1110"/> more information, please refer to the following: <a href="https://docs.aws.amazon.com/quicksight/latest/user/getting-started-create-analysis-s3.html">https://docs.aws.amazon.com/quicksight/latest/user/getting-started-create-analysis-s3.html</a></li>
				<li>If you would like to transform the results before sending them to the data stores, use AWS Glue ETL jobs. For more details, please refer to the <em class="italic">Using AWS Glue for data processing and transformation</em> section.</li>
			</ul>
			<p>Let's now review<a id="_idIndexMarker1111"/> how to automate the NLP solution development using <strong class="bold">AWS</strong> <strong class="bold">Step Function</strong>.</p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor229"/>Using AWS Step Function for orchestration</h2>
			<p>In a previous section, we read<a id="_idIndexMarker1112"/> how using Batch APIs can help scale the architecture to handle large volumes<a id="_idIndexMarker1113"/> of documents. We reviewed Amazon Comprehend and Textract APIs that can help us implement a batch-processing pipeline. When we start designing a batch solution, it may take the shape of an<a id="_idIndexMarker1114"/> Amazon S3 bucket, to which an AWS Lambda event trigger is attached that will call the Amazon Textract API to start document analysis. To this, an Amazon SNS topic would be provided, a message will be sent by Amazon Textract to this topic, to which an AWS Lambda is attached, and so on. You get the point. It can get really difficult to manage all of these moving parts<a id="_idIndexMarker1115"/> in our solution. To design an elegant and efficient NLP solution, you can use AWS Step Function to manage the orchestration of your entire pipeline (<a href="https://aws.amazon.com/step-functions/">https://aws.amazon.com/step-functions/</a>). </p>
			<p>AWS Step Function is a serverless, event-driven orchestration service that can help tie together several steps in a process and manage it end to end. Error handling is built in, so you can configure retries, branching, and compensation logic into the orchestration. An example of a Step Function orchestration from the samples available in the AWS Console is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer310" class="IMG---Figure">
					<img src="Images/B17528_18_07.jpg" alt="Figure 18.7 – A sample Step Function orchestration&#13;&#10;" width="848" height="594"/>
				</div>
			</div>
			<p class="figure-caption">Figure 18.7 – A sample Step Function orchestration</p>
			<p>To get started, you can run a sample orchestration in your Step Function Console in the AWS Management Console by selecting <strong class="bold">Run a sample project</strong> in the <strong class="bold">State Machines</strong> option<a id="_idIndexMarker1116"/> on the left of the console. You can also<a id="_idIndexMarker1117"/> try the Step Function tutorials available here: <a href="https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/">https://aws.amazon.com/getting-started/hands-on/create-a-serverless-workflow-step-functions-lambda/</a></p>
			<p>In the next section, let's discuss how to automate our deployment process using AWS CloudFormation.</p>
			<h2 id="_idParaDest-225"><a id="_idTextAnchor230"/>Using AWS CloudFormation templates</h2>
			<p><strong class="bold">AWS</strong> <strong class="bold">CloudFormation</strong> (<a href="https://aws.amazon.com/cloudformation/">https://aws.amazon.com/cloudformation/</a>) is an infrastructure-as-code service<a id="_idIndexMarker1118"/> that helps you automate and manage your resource provisioning tasks in AWS. When building NLP solutions using AWS AI services, we primarily deal with managed<a id="_idIndexMarker1119"/> services, but, depending on how our operational architecture looks, it makes a lot of sense to use AWS CloudFormation to automate our deployment process. This is mainly because it removes a lot of overhead related to the setup, makes change<a id="_idIndexMarker1120"/> management easier, and helps us achieve operational excellence. Every solution topology is different, but if your NLP architecture includes Amazon S3 buckets and other types of AWS data stores, AWS Step Function, AWS Lambda functions, Amazon SNS topics, and so on, you will benefit from using AWS CloudFormation. Templates can be written in JSON or <strong class="bold">YAML</strong>, and there are lots of resources available to help you get started. </p>
			<p>For an example CloudFormation template with<a id="_idIndexMarker1121"/> AWS Step Function and AWS Lambda functions, please refer to the following: <a href="https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-lambda-state-machine-cloudformation.html">https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-lambda-state-machine-cloudformation.html</a></p>
			<p>For template snippet code examples<a id="_idIndexMarker1122"/> for a variety of AWS services, please refer to the following: <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/CHAP_TemplateQuickRef.html">https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/CHAP_TemplateQuickRef.html</a></p>
			<p>As we saw in detail in the preceding sections, these are some of the principles and best practices you can adopt to design and build long-lasting NLP solutions that are cost-effective, resilient, scalable, secure, and performance-efficient. These are the characteristics that make great solutions.</p>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor231"/>Summary</h1>
			<p>After having learned how to build NLP solutions for a number of real-world use cases in the previous chapters, we spent this chapter reading about how to build secure, reliable, and efficient architectures using the AWS Well-Architected Framework. We first introduced what the Well-Architected Framework is, and reviewed the five pillars it is comprised of: operational excellence, security, reliability, performance efficiency, and cost optimization. We read about each of the pillars in brief, and then discussed how the Well-Architected Framework can help us build better and more efficient NLP solutions by using a matrix of best practices aligned with the Well-Architected principles and the different stages of NLP solution development. </p>
			<p>We followed this summary of the best practices by diving deep into each one, learning how to implement them using the AWS Management Console, AWS documentation references, and some code snippets.</p>
			<p>That brings us to the end of this book. It is with a heavy heart that we bid adieu to you, our wonderful readers. We hope that you had as much fun reading this book as we had writing it. Please don't forget to check out the <em class="italic">Further reading</em> section for a few references we have included to continue your learning journey in the exciting space that is NLP. </p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor232"/>Further reading</h1>
			<ul>
				<li>AWS Well-Architected Labs (<a href="https://www.wellarchitectedlabs.com/">https://www.wellarchitectedlabs.com/</a>)</li>
				<li>Amazon Textract blogs (<a href="https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-textract/">https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-textract/</a>)</li>
				<li>Amazon Comprehend blogs (<a href="https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-comprehend/">https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-comprehend/</a>)</li>
				<li>Amazon Comprehend workshops (<a href="https://comprehend-immersionday.workshop.aws/">https://comprehend-immersionday.workshop.aws/</a>)</li>
				<li>AWS Automating data processing from documents<em class="italic"> </em>(<a href="https://aws.amazon.com/machine-learning/ml-use-cases/document-processing/">https://aws.amazon.com/machine-learning/ml-use-cases/document-processing/</a>)</li>
			</ul>
		</div>
	</div></body></html>