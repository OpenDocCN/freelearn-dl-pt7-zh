<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer272">
			<h1 id="_idParaDest-158"><em class="italic"><a id="_idTextAnchor162"/>Chapter 14</em>: Auditing Named Entity Recognition Workflows</h1>
			<p>In the previous chapter, we were introduced to an approach for improving the accuracy of the results we wanted to extract from documents using <strong class="bold">Amazon</strong> <strong class="bold">Augmented AI </strong>(<strong class="bold">Amazon</strong> <strong class="bold">A2I</strong>). We saw that Amazon A2I can be added to a document processing workflow to review model prediction accuracy. This enabled us to include human reviews in LiveRight's check processing system.</p>
			<p>In this chapter, we will walk through an extension of the previous approach by including <strong class="bold">Amazon</strong> <strong class="bold">Comprehend</strong> for text-based insights thereby demonstrating an end-to-end process for setting up an auditing workflow for your custom named entity recognition use cases. We put together this solution based on our collective experience and the usage trends we have observed in our careers. We expect to be hands-on throughout the course of this chapter, but we have all the code samples we need to get going. </p>
			<p>With <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), companies can set up automated document processing solutions that can be trained to recognize and extract custom entities from your documents. This helps you derive unique insights from your text corpus. These insights can help drive strategic decisions. However, there are certain challenges that need to be navigated first. Typically, companies receive large volumes of incoming documents of different templates, with varying contents, in multiple languages. Also, as businesses grow, the type and volume of documents evolve, and very soon you get into a maintenance overhead situation trying to keep the various templates, formats, and rules synchronized with how you are trying to use these documents for your operational needs. Furthermore, you will have to ensure your infrastructure is able to scale to support your processing needs. </p>
			<p>To solve these challenges, we will show you how you can use the ready-made ML capabilities of <strong class="bold">Amazon</strong> <strong class="bold">Textract</strong>, leveraging transfer learning to create a custom entity recognition model with <strong class="bold">Amazon</strong> <strong class="bold">Comprehend</strong>, and auditing the predictions with a human reviewer loop using A2I. We introduced Amazon A2I in detail in <a href="B17528_13_Final_SB_ePub.xhtml#_idTextAnchor151"><em class="italic">Chapter 13</em></a>, <em class="italic">Improving the Accuracy of Document Processing Workflows</em>. In this chapter, we will navigate through the following sections:</p>
			<ul>
				<li>Authenticating loan applications</li>
				<li>Building the loan authentication solution </li>
			</ul>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor163"/>Technical requirements</h1>
			<p>For this chapter, you will need access to an <strong class="bold">AWS</strong> <strong class="bold">account</strong> at <a href="https://aws.amazon.com/console/">https://aws.amazon.com/console/</a>. Please refer to the <em class="italic">Signing up for an AWS account</em> subsection within the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract,</em> for detailed instructions on how you can sign up for an AWS account and sign in to the <strong class="bold">AWS</strong> <strong class="bold">Management Console</strong>.</p>
			<p>The <strong class="bold">Python</strong> code and sample datasets for the solution discussed in this chapter can be found at the following link: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2014">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2014</a>.</p>
			<p>Check out the following video to see the Code in Action at <a href="https://bit.ly/3GoBh1B">https://bit.ly/3GoBh1B</a>.</p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor164"/>Authenticating loan applications </h1>
			<p>Financial <a id="_idIndexMarker804"/>organizations receive significant volumes of loan applications every day. While the major organizations have switched to fully digital processing, there are still many banks and institutions across the world that rely on paper documents. To illustrate our example, let's go back to our fictitious banking corporation, <em class="italic">LiveRight Holdings Private Limited</em>, and review the requirements for this use case:</p>
			<ul>
				<li>LiveRight offers a number of lending products to its customers, which are primarily small-to-medium businesses and individual consumers. To apply for a loan, consumers fill out a paper-based loan/mortgage application form that is validated by a team of experts to determine the authenticity of the application (called the <em class="italic">authenticity-check</em> process). If found to be a valid applicant, LiveRight's loan processors will request supporting documentation from the consumers for pre-approval qualification.  </li>
				<li>LiveRight receives anywhere from 8,000 to 10,000 loan applications a day from potential customers. These applications are forwarded nightly from its various branches to the document inlet center at the company's <strong class="bold">headquarters (HQ)</strong>. Today, their authenticity-check process takes approximately 2 to 4 weeks for the team to scan all the applications and determine whether they are good enough to be forwarded to the loan processors, causing significant delays even at the pre-approval stage. This has irked many customers who are taking their business elsewhere. LiveRight has hired you to automate the authenticity-check process with a target to reduce the processing time to 24 hours within the first 3 months of the solution being implemented.    </li>
			</ul>
			<p>As the enterprise<a id="_idIndexMarker805"/> architect for the project, you decide to use Amazon Textract to leverage its pre-trained ML model for text extraction, the <strong class="bold">Custom Entity Recognizer</strong> feature<a id="_idIndexMarker806"/> of Amazon Comprehend to incrementally create your own entity recognizer for loan application checks without the need to build complex <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) algorithms, and <a id="_idIndexMarker807"/>A2I to set up a human review workflow to monitor predictions from your entity recognizer and send feedback to the recognizer to improve its detection capabilities for entities unique to the use case.</p>
			<p>You plan to have the private human workflow available for the first 2 to 3 months and subsequently disable it, at which point the document processing workflow will become fully automated. As the human team checks and updates the entity labels, you need to determine the authenticity check decision to be either <em class="italic">APPROVE</em>, <em class="italic">SUMMARY APPROVE</em>, or <em class="italic">REJECT</em>. This decision, along with the relevant content from the loan application, should be stored in <a id="_idIndexMarker808"/>an <strong class="bold">Amazon</strong> <strong class="bold">DynamoDB</strong> (a fully managed, low-latency <strong class="bold">NoSQL</strong> database service) table for loan processors to access the content and enable pre-approval qualification. The components of the solution we will build are shown in the following figure:</p>
			<div>
				<div id="_idContainer262" class="IMG---Figure">
					<img src="Images/B17528_14_01.jpg" alt="Figure 14.1 – Loan approval document processing solution architecture&#13;&#10;" width="1287" height="524"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – Loan approval document processing solution architecture</p>
			<p>We will be walking<a id="_idIndexMarker809"/> through our solution using an <strong class="bold">Amazon</strong> <strong class="bold">SageMaker</strong> <strong class="bold">Jupyter</strong> <strong class="bold">notebook</strong> that will allow us to review the code and results as we execute it step by step. The<a id="_idIndexMarker810"/> solution build includes the following tasks:</p>
			<ol>
				<li value="1">As a first step, we will create an Amazon Comprehend custom entity recognizer based on the training dataset provided in our GitHub repository. </li>
				<li>We will then create a private labeling workforce and add a team member who will be responsible for reviewing predictions from the Amazon Comprehend custom entity recognizer, using the Amazon A2I service. We will create the <a id="_idIndexMarker811"/>private workforce using the <em class="italic">labeling</em> workforces feature available in the Amazon SageMaker console. For more details, please refer to this link: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-private.html">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-private.html</a>.</li>
				<li>We start the solution workflow by inspecting our sample input loan application available in our GitHub repository. We display the image of the loan application in our notebook and look at the contents. </li>
				<li>Next, we use Amazon Textract to extract the key-value pairs from our input document.</li>
				<li>We then create an inference request string from the key-value pairs and prepare it to send to Amazon Comprehend custom entity detection. </li>
				<li>Next, we set up an Amazon Comprehend real-time endpoint and invoke it to detect entities from our inference request string.</li>
				<li>We will set up an Amazon A2I human review loop using the entity recognition task UI template and send the results of the custom entity detection to an Amazon A2I human loop.</li>
				<li>Logging in as a private worker, we will review the detected entities and modify the labels as required. </li>
				<li>We will then check whether a new entity detection event occurred or whether an existing entity detection was modified, update the entity list, and send it back to Amazon <a id="_idIndexMarker812"/>Comprehend for retraining our entity detection model.</li>
				<li>Based on the output from the human loop review, we will also determine a decision for the loan application and upload this to a <strong class="bold">DynamoDB</strong> table for downstream processing.</li>
			</ol>
			<p>Now that we've got the context for the exercise and gone over our intended process, let's start building the solution.</p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor165"/>Building the loan authentication solution</h1>
			<p>In the previous section, we <a id="_idIndexMarker813"/>introduced the loan application approval use case, covered the architecture of the solution we will be building, and briefly walked through the solution components and workflow steps. In this section, we will get right down to action and start executing the tasks to build our solution. But first, there are pre-requisites we will have to take care of. </p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor166"/>Setting up to solve the use case</h2>
			<p>If you<a id="_idIndexMarker814"/> have not done so in the previous chapters, you will first have to create a Jupyter notebook and set up <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) permissions for that notebook role to access the AWS services we will use in this notebook. After that, you will need to clone the GitHub repository (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services</a>), create an <strong class="bold">Amazon</strong> <strong class="bold">S3</strong> bucket (<a href="https://aws.amazon.com/s3/">https://aws.amazon.com/s3/</a>), and provide the bucket name in the notebook to start execution. Please follow the next steps to complete these tasks before we can execute the cells from our notebook:</p>
			<p class="callout-heading">Note:  </p>
			<p class="callout">Please ensure you have completed the tasks mentioned in the <em class="italic">Technical requirements</em> section.</p>
			<ol>
				<li value="1">To create your Jupyter Notebook instance, follow the instructions in the <em class="italic">Create an Amazon SageMaker Jupyter Notebook instance</em> section in the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>.<p class="callout-heading">IAM role permissions while creating Amazon SageMaker Jupyter notebooks</p><p class="callout">Accept the default for the IAM role at the notebook creation time to allow access to any S3 bucket. </p></li>
				<li>Once you create the notebook instance and its status is <strong class="bold">InService</strong>, click on <strong class="bold">Open Jupyter</strong> in the <strong class="bold">Actions</strong> menu, heading for the notebook instance. </li>
				<li>This will take you to the <strong class="bold">home</strong> folder of your notebook instance. </li>
				<li>Click on <strong class="bold">New</strong> and select <strong class="bold">Terminal</strong>.</li>
				<li>In the <strong class="bold">Terminal window</strong>, first, type <strong class="source-inline">cd SageMaker</strong>, and then, type <strong class="source-inline">git clone </strong><a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services</a>.</li>
				<li>Now, exit the Terminal window and go back to the <strong class="bold">home</strong> folder, and you will see a folder called <strong class="source-inline">Natural-Language-Processing-with-AWS-AI-Services</strong>. Click this folder to bring up the chapter folders and click <strong class="source-inline">Chapter 14</strong>. Open this folder by clicking. You should see a notebook called <strong class="source-inline">chapter14-auditing-workflows-named-entity-detection-forGitHub.ipynb</strong>.</li>
				<li>Open <a id="_idIndexMarker815"/>this notebook by clicking it. </li>
			</ol>
			<p>Follow through the steps in this notebook that correspond to the next few subheadings in this section by executing one cell at a time. Please do read the descriptions provided preceding each notebook cell.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor167"/>Additional IAM pre-requisites</h2>
			<p>To train the<a id="_idIndexMarker816"/> Comprehend custom entity recognizer, to set up real-time endpoints, we have to enable additional policies and also update the trust relationships for our SageMaker notebook role. Please refer to <em class="italic">Changing IAM permissions and trust relationships for the Amazon SageMaker Notebook execution role</em> in the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract,</em> for more detailed instructions on how to execute the following steps:</p>
			<ol>
				<li value="1">Please attach <strong class="source-inline">TextractFullAccess</strong>, <strong class="source-inline">ComprehendFullAccess</strong>, and <strong class="source-inline">AmazonAugmentedAIFullAccess</strong> policies to your Amazon SageMaker Notebook IAM role. </li>
				<li>Add an <strong class="source-inline">IAM:PassRole</strong> permission as an inline policy to your SageMaker Notebook execution role:<p class="source-code">{ "Version": "2012-10-17", "Statement": [ {</p><p class="source-code">  "Action": [</p><p class="source-code">      "iam:PassRole"</p><p class="source-code">  ],</p><p class="source-code">  "Effect": "Allow",</p><p class="source-code">  "Resource": "&lt;your sagemaker notebook execution role ARN"&gt;</p><p class="source-code">  }</p><p class="source-code"> ]</p><p class="source-code">}</p></li>
				<li>Finally, update<a id="_idIndexMarker817"/> the trust relationships: <p class="source-code">{ "Version": "2012-10-17", "Statement": [</p><p class="source-code">  { "Effect": "Allow", </p><p class="source-code">    "Principal": </p><p class="source-code">      { "Service": </p><p class="source-code">          [ "sagemaker.amazonaws.com", </p><p class="source-code">            "s3.amazonaws.com", </p><p class="source-code">            "comprehend.amazonaws.com" ] </p><p class="source-code">          }, </p><p class="source-code">          "Action": "sts:AssumeRole" } </p><p class="source-code">      ] </p><p class="source-code">  }</p></li>
			</ol>
			<p>Now that we have set up our notebook and set up the IAM role to run the walkthrough notebook, in the next section, we will train an Amazon Comprehend entity recognizer.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor168"/>Training an Amazon Comprehend custom entity recognizer</h2>
			<p>Let's begin by <a id="_idIndexMarker818"/>training a custom entity recognizer to detect entities unique to this solution. Amazon Comprehend offers pre-trained entity recognition features that we learned about in the previous chapter. For this solution, we will use the <strong class="bold">Custom Entity Recognition</strong> feature<a id="_idIndexMarker819"/> of Amazon Comprehend that allows you to train a recognizer for custom needs using incremental training. All we have to do is provide a list of entities we want it to recognize, and a raw dataset containing the lines of text comprising the context that will be detected as entities. Open the notebook and execute the steps as follows:</p>
			<ol>
				<li value="1">Execute the cell under <strong class="bold">Step 0 – Import Libraries</strong> to ensure we have the libraries we need for the notebook. Note that in this cell you are getting the Amazon SageMaker execution role for the notebook, along with the SageMaker session. Please ensure you create an Amazon S3 bucket (<a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html</a>) and provide the bucket name in the following line: <p class="source-code">bucket = '&lt;bucket-name&gt;'</p></li>
				<li>Execute the cells under <strong class="bold">Step 1 – Train an Amazon Comprehend Custom Entity Recognizer</strong>:<p>a) First, initialize the <strong class="source-inline">boto3</strong> handle for Amazon Comprehend:</p><p class="source-code">comprehend = boto3.client('comprehend')</p><p>b) Then, define the variables for the S3 prefixes and upload the training dataset and the entity list to the S3 bucket:</p><p class="source-code">s3_raw_key = prefix + "/train/raw_txt.csv" </p><p class="source-code">s3_entity_key = prefix + "/train/entitylist.csv"</p><p class="source-code">s3.upload_file('train/raw_txt.csv',bucket,s3_raw_key)</p><p class="source-code">s3.upload_file('train/entitylist.csv',bucket,s3_entity_key)</p><p>c) Continue executing the rest of the cells in the notebook to declare the variables with the full S3 <strong class="bold">URIs</strong> for our input documents, define the input object for the entity recognizer, and finally, call the Comprehend API to create the custom entity <a id="_idIndexMarker820"/>recognizer. This will start the training job:</p><p class="source-code">import datetime</p><p class="source-code">cer_name = "loan-app-recognizer"+str(datetime.datetime.now().strftime("%s"))</p><p class="source-code">cer_response = comprehend.create_entity_recognizer(</p><p class="source-code">        RecognizerName = cer_name, </p><p class="source-code">        DataAccessRoleArn = role,</p><p class="source-code">        InputDataConfig = cer_input_object,</p><p class="source-code">        LanguageCode = "en"</p><p class="source-code">)</p><p>d) Print the results of the custom entity recognizer training job:</p><p class="source-code">import pprint</p><p class="source-code">pp = pprint.PrettyPrinter(indent=4)</p><p class="source-code">response = comprehend.describe_entity_recognizer(</p><p class="source-code">    EntityRecognizerArn=cer_response['EntityRecognizerArn']</p><p class="source-code">)</p><p class="source-code">pp.pprint(response)</p></li>
				<li>Check the status of the training job periodically by visiting the Amazon Comprehend AWS console (<a href="https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#entity-recognition">https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#entity-recognition</a>). The training should take approximately 15 to 30 minutes. Time for a coffee/snack break.</li>
			</ol>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor169"/>Creating a private team for the human loop</h2>
			<p>Refer <a id="_idIndexMarker821"/>to <em class="italic">Step 2</em> in the notebook (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2014/chapter14-auditing-workflows-named-entity-detection-forGitHub.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2014/chapter14-auditing-workflows-named-entity-detection-forGitHub.ipynb</a>) for the instructions we will execute now. </p>
			<p>In this step, we will create a private team using the Amazon SageMaker labeling workforce console, and we will add ourselves to the private team as a worker. This is required so we can log in to the labeling task UI when we reach the Amazon A2I step in this solution. Please execute the following tasks:</p>
			<ol>
				<li value="1">Log in to the AWS Management Console if not already done (please refer to the <em class="italic">Technical requirements</em> section at the start of this chapter for more details), type <strong class="source-inline">amazon sagemaker</strong> in the <strong class="bold">Services</strong> search bar, and go to the Amazon SageMaker console. Once there, on the left of the UI, click on <strong class="bold">Ground Truth</strong> and then <strong class="bold">Labeling workforces</strong>. On this screen, select <strong class="bold">Private</strong> from the tab at the top and click on <strong class="bold">Create private team</strong>.<div id="_idContainer263" class="IMG---Figure"><img src="Images/B17528_14_02.jpg" alt="Figure 14.2 – SageMaker labeling workforces&#13;&#10;" width="1295" height="479"/></div><p class="figure-caption">Figure 14.2 – SageMaker labeling workforces</p></li>
				<li>Enter a name for your private team in the <strong class="bold">Team name</strong> field and leave the default selection of <strong class="bold">Create a new Amazon Cognito user group</strong> in the <strong class="bold">Add workers</strong> section. Scroll down and click <strong class="bold">Create private team</strong>.</li>
				<li>You will now be returned to the <strong class="bold">Labeling workforces</strong> screen. The private team, <strong class="source-inline">nlp-doc-team</strong>, should be visible under <strong class="bold">Private teams</strong>. Next to that, you will see an ARN, which is a long string that looks like <strong class="source-inline">arn:aws:sagemaker:region-name-123456:workteam/private-crowd/team-name</strong>. Please copy the ARN from the screen and provide this in the notebook cell:<p class="source-code">WORKTEAM_ARN= '&lt;workteam-arn&gt;'</p></li>
				<li>Next, scroll<a id="_idIndexMarker822"/> down in the previous screen, go to the <strong class="bold">Workers</strong> section, and click on <strong class="bold">Invite new workers</strong>. Provide your email address and click <strong class="bold">Invite new workers</strong>. You will receive an email from <strong class="source-inline">no-reply@verificationemail.com</strong>. Follow the instructions to complete the sign-up process.<div id="_idContainer264" class="IMG---Figure"><img src="Images/B17528_14_03.jpg" alt="Figure 14.3 – Inviting new workers&#13;&#10;" width="1007" height="168"/></div><p class="figure-caption">Figure 14.3 – Inviting new workers</p></li>
				<li>Now, add yourself to the private team by clicking on <strong class="bold">nlp-doc-team</strong> and then clicking on <strong class="bold">Add workers to team</strong>. Select your email address from the list and click on <strong class="bold">Add workers to team</strong>.</li>
			</ol>
			<div>
				<div id="_idContainer265" class="IMG---Figure">
					<img src="Images/B17528_14_04.jpg" alt="Figure 14.4 – Adding workers to team&#13;&#10;" width="1254" height="618"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.4 – Adding workers to team</p>
			<p>Now that we <a id="_idIndexMarker823"/>have added the private team, let's review our loan application by extracting the contents using Amazon Textract.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor170"/>Extracting sample document contents using Amazon Textract</h2>
			<p>This <a id="_idIndexMarker824"/>section corresponds to <em class="italic">Step 3</em> in the notebook: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2014/chapter14-auditing-workflows-named-entity-detection-forGitHub.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2014/chapter14-auditing-workflows-named-entity-detection-forGitHub.ipynb</a>. </p>
			<p>In this step, we will review the sample loan application, and then use Amazon Textract to extract the key-value pairs or form data that is of interest to our solution, creating an inference request CSV file to pass as an input to our Comprehend custom entity recognizer for detecting entities. Please follow through using the notebook and execute the cells to perform the tasks required for this step:</p>
			<ol>
				<li value="1">Review the input document by executing the code in the notebook cell, as shown here:<p class="source-code">documentName = "input/sample-loan-application.png"</p><p class="source-code">display(Image(filename=documentName))</p></li>
				<li>Let's now load this image into our S3 bucket:<p class="source-code">s3.upload_file(documentName,bucket,prefix+'/'+documentName)</p></li>
				<li>We will <a id="_idIndexMarker825"/>extract the key-value pair data from this document to transform and create a request string for inference using the Amazon Textract <strong class="bold">AnalyzeDocument</strong> API. This accepts image files (PNG or JPEG) as an input. To use this example with a PDF file, or for processing multiple documents together, you can use the <strong class="bold">StartDocumentAnalysis</strong> API: <a href="https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html">https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentAnalysis.html</a>.</li>
				<li>We will use the <strong class="source-inline">amazon-textract-response-parser</strong> library to help with the JSON response from Textract. Install it by typing the following:<p class="source-code"> !pip install amazon-textract-response-parser        </p></li>
				<li>Now, let's use the Textract <strong class="source-inline">boto3</strong> Python SDK to retrieve the contents of the document, as shown here:<p class="source-code">textract = boto3.client('textract')</p><p class="source-code">response = textract.analyze_document(Document={'S3Object': {</p><p class="source-code">            'Bucket': bucket,</p><p class="source-code">            'Name': prefix+'/'+documentName</p><p class="source-code">        }}, FeatureTypes=['FORMS'])</p></li>
				<li>We will now extract the key-value pairs we need for our solution. We will not use the checkbox fields but only those fields with values in them. Also, we will filter out the<a id="_idIndexMarker826"/> fields that we actually need in the next few steps:<p class="source-code">from trp import Document</p><p class="source-code">doc = Document(response)</p><p class="source-code">df = pd.DataFrame()</p><p class="source-code"># Iterate over elements in the document</p><p class="source-code">x = 0</p><p class="source-code">for page in doc.pages:</p><p class="source-code">    for field in page.form.fields:   </p><p class="source-code">        if field.key is not None and field.value is not None:</p><p class="source-code">            if field.value.text not in ('SELECTED','NOT_SELECTED'):</p><p class="source-code">                df.at[x,'key'] = field.key.text</p><p class="source-code">                df.at[x,'value'] = field.value.text</p><p class="source-code">                x+=1</p><p class="source-code">df</p></li>
				<li>Now that we have loaded the results from Textract into a <strong class="bold">p</strong><strong class="bold">andas</strong> <strong class="bold">DataFrame</strong> (<a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html</a>), we will run a series of operations to filter the columns we are interested in from the loan application. Execute all the cells under the <em class="italic">Extract contents for sending to Comprehend CER</em> section in the notebook. We should see the final filtered list of fields as follows:</li>
			</ol>
			<div>
				<div id="_idContainer266" class="IMG---Figure">
					<img src="Images/B17528_14_05.jpg" alt="Figure 14.5 – Finalized list of fields we will use for Comprehend entity recognition&#13;&#10;" width="651" height="56"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.5 – Finalized list of fields we will use for Comprehend entity recognition</p>
			<p>Now, let's cover detecting entities using the Amazon Comprehend custom entity recognizer.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor171"/>Detecting entities using the Amazon Comprehend custom entity recognizer</h2>
			<p>Now that we <a id="_idIndexMarker827"/>have what we need from the loan application, let's construct a string that will become our inference request to the Comprehend custom entity recognizer we trained at the beginning of this walkthrough (<em class="italic">Step 1</em> in the notebook). Before we can detect the entities, we need to create a real-time endpoint and associate that with our entity recognizer. When you deploy this solution in batch mode or use it for<a id="_idIndexMarker828"/> processing multiple documents, you will use the Amazon Comprehend <strong class="bold">StartEntitiesDetection</strong> API: <a href="https://docs.aws.amazon.com/comprehend/latest/dg/API_StartEntitiesDetectionJob.html">https://docs.aws.amazon.com/comprehend/latest/dg/API_StartEntitiesDetectionJob.html</a>.</p>
			<p>Please follow the instructions in this section by executing the cells in <em class="italic">Step 4</em> in the notebook: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2014/chapter14-auditing-workflows-named-entity-detection-forGitHub.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2014/chapter14-auditing-workflows-named-entity-detection-forGitHub.ipynb</a>:</p>
			<ol>
				<li value="1">We will now create a request string that will be sent to the Amazon Comprehend custom entity recognizer model to detect the entities we trained it on. This string comprises data that we extracted from our loan application document using Amazon Textract in the previous step. We will transpose our pandas DataFrame, add a document number column, and use it to prepare the inference request string:<p class="source-code">df_T.columns = df_T.columns.str.rstrip()</p><p class="source-code">df_T['doc'] = 1</p><p class="source-code">df_T</p><p class="source-code">for idx, row in df_T.iterrows():</p><p class="source-code">        entry = 'Country'+':'+str(row['Country']).strip()+" "+'Years'+':'+str(row['Years']).strip()+" "+'Cell Phone'+':'+str(row['Cell Phone']).strip()+" "+'Name'+':'+str(row['Name']).strip()+" "+'Social Security Number'+':'+str(row['Social Security Number']).strip()+" "+'TOTAL $'+':'+str(row['TOTAL $']).strip()+" "+'Date of Birth'+':'+str(row['Date of Birth']).strip()</p></li>
				<li>Next, let's<a id="_idIndexMarker829"/> create a real-time endpoint for Comprehend:<p class="source-code">custom_recognizer_arn=cer_response['EntityRecognizerArn']</p><p class="source-code">endpoint_response = comprehend.create_endpoint(</p><p class="source-code">    EndpointName='nlp-chapter4-cer-endpoint',</p><p class="source-code">    ModelArn=custom_recognizer_arn,</p><p class="source-code">    DesiredInferenceUnits=2,</p><p class="source-code">    DataAccessRoleArn=role</p><p class="source-code">)</p><p class="source-code">endpoint_response['EndpointArn']</p></li>
				<li>We see the endpoint Arn printed as follows:<p class="source-code">arn:aws:comprehend:us-east-1:&lt;aws-account-nr&gt;:entity-recognizer-endpoint/nlp-chapter4-cer-endpoint</p></li>
				<li>Check the status of the endpoint by navigating to the <strong class="bold">Amazon Comprehend</strong> console, go to <strong class="bold">custom entity recognition</strong> in the left menu, click on your recognizer, and scroll down to verify your real-time endpoint has been created successfully. If the endpoint is not active, the code in the next cell in the notebook will fail. It may take about <em class="italic">15 minutes</em> for the endpoint to be ready:<div id="_idContainer267" class="IMG---Figure"><img src="Images/B17528_14_06.jpg" alt="Figure 14.6 – Waiting for an endpoint to be ready &#13;&#10;" width="993" height="206"/></div><p class="figure-caption">Figure 14.6 – Waiting for an endpoint to be ready </p></li>
				<li>When the <a id="_idIndexMarker830"/>endpoint is <strong class="bold">Ready</strong>, execute the code in the notebook cell to send the inference request to the custom entity recognizer, as shown here:<p class="source-code">response = comprehend.detect_entities(Text=entry,</p><p class="source-code">                    LanguageCode='en',</p><p class="source-code">EndpointArn=endpoint_response['EndpointArn']</p><p class="source-code">            )</p><p class="source-code">print(response)</p></li>
				<li>We see the output as shown in the following code block. This display shows that our Comprehend entity recognition has identified all these attributes that represent a valid person:<p class="source-code">{'Entities': [{'Score': 0.9999999403953552, 'Type': 'PERSON', 'Text': 'Years:18', 'BeginOffset': 11, 'EndOffset': 19}, {'Score': 0.9999998211860657, 'Type': 'PERSON', 'Text': 'Cell Phone:(555 ) 0200 1234', 'BeginOffset': 20, 'EndOffset': 47}, {'Score': 1.0, 'Type': 'PERSON', 'Text': 'Name:Kwaku Mensah', 'BeginOffset': 48, 'EndOffset': 65}, {'Score': 1.0, 'Type': 'PERSON', 'Text': 'Social Security Number:123 - 45 - 6789', 'BeginOffset': 66, 'EndOffset': 104}, {'Score': 1.0, 'Type': 'PERSON', 'Text': 'TOTAL $:8000.00/month', 'BeginOffset': 105, 'EndOffset': 126}, {'Score': 1.0, 'Type': 'PERSON', 'Text': 'Date of Birth:01 / 01 / 1953', 'BeginOffset': 127, 'EndOffset': 155}], 'ResponseMetadata': {'RequestId': 'ecbd75fd-22bc-4dca-9aa0-73f58f6784e4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ecbd75fd-22bc-4dca-9aa0-73f58f6784e4', 'content-type': 'application/x-amz-json-1.1', 'content-length': '620', 'date': 'Tue, 06 Jul 2021 22:26:11 GMT'}, 'RetryAttempts': 0}}</p></li>
				<li>The last<a id="_idIndexMarker831"/> task in <em class="italic">Step 4</em> is to prepare a <strong class="source-inline">human_loop_input</strong> list to be used with the Amazon A2I human workflow that we will create in the next step:<p class="source-code">import json</p><p class="source-code">human_loop_input = []</p><p class="source-code">data = {}</p><p class="source-code">ent = response['Entities']</p><p class="source-code">existing_entities = []</p><p class="source-code">if ent != None and len(ent) &gt; 0:</p><p class="source-code">    for entity in ent:       </p><p class="source-code">        current_entity = {}</p><p class="source-code">        current_entity['label'] = entity['Type']</p><p class="source-code">        current_entity['text'] = entity['Text']</p><p class="source-code">        current_entity['startOffset'] = entity['BeginOffset']</p><p class="source-code">        current_entity['endOffset'] = entity['EndOffset']</p><p class="source-code">        existing_entities.append(current_entity)</p><p class="source-code">    data['ORIGINAL_TEXT'] = entry</p><p class="source-code">    data['ENTITIES'] = existing_entities   </p><p class="source-code">    human_loop_input.append(data)</p><p class="source-code">print(human_loop_input)        </p><p class="source-code">126}, {'label': 'PERSON', 'text': 'Date of Birth:01 / 01 / 1953', 'startOffset': 127, 'endOffset': 155}]}]</p></li>
			</ol>
			<p>In this section, we were able to detect entities with the Amazon Comprehend entity recognizer. In the next section, we will walk through how you can use Amazon A2I to review the predictions and make changes to the predicted versus actual entity.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor172"/>Setting up an Amazon A2I human workflow loop</h2>
			<p>For the <a id="_idIndexMarker832"/>code blocks discussed here, refer to <em class="italic">Step 5</em> in the notebook: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2014/chapter14-auditing-workflows-named-entity-detection-forGitHub.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2014/chapter14-auditing-workflows-named-entity-detection-forGitHub.ipynb</a>. </p>
			<p>Now that we have the detected entities from our Comprehend custom entity recognizer, it's time to set up a human workflow using the <em class="italic">private team</em> we created in <em class="italic">Step 2</em> and send the results to the Amazon A2I human loop for review, and any modifications/augmentation as required. Subsequently, we will update the <strong class="source-inline">entitylist.csv</strong> file that we originally used to train our Comprehend custom entity recognizer so we can prepare it for retraining based on the human feedback:</p>
			<ol>
				<li value="1">Let's start by initializing some variables we will need for the next few tasks:<p class="source-code">timestamp = time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</p><p class="source-code"># Amazon SageMaker client</p><p class="source-code">sagemaker = boto3.client('sagemaker')</p><p class="source-code"># Amazon Augment AI (A2I) client</p><p class="source-code">a2i = boto3.client('sagemaker-a2i-runtime')</p><p class="source-code"># Flow definition name</p><p class="source-code">flowDefinition = 'fd-nlp-chapter14-' + timestamp</p><p class="source-code"># Task UI name - this value is unique per account and region. You can also provide your own value here.</p><p class="source-code">taskUIName = 'ui-nlp-chapter14-' + timestamp</p><p class="source-code"># Flow definition outputs</p><p class="source-code">OUTPUT_PATH = f's3://' + bucket + '/' + prefix + '/a2i-results'</p></li>
				<li>Now, we<a id="_idIndexMarker833"/> will create the human task UI by executing the next cell in the notebook (refer to <em class="italic">Step 5</em> in the notebook). We selected the task template for named entity recognition from the Amazon A2I Sample Task UI GitHub repository (<a href="https://github.com/aws-samples/amazon-a2i-sample-task-uis">https://github.com/aws-samples/amazon-a2i-sample-task-uis</a>) and customized it for our needs.</li>
				<li>Create the task UI based on the template:<p class="source-code">def create_task_ui():</p><p class="source-code">    '''</p><p class="source-code">    Creates a Human Task UI resource.</p><p class="source-code">    Returns:</p><p class="source-code">    struct: HumanTaskUiArn</p><p class="source-code">    '''</p><p class="source-code">    response = sagemaker.create_human_task_ui(</p><p class="source-code">        HumanTaskUiName=taskUIName,</p><p class="source-code">        UiTemplate={'Content': template})</p><p class="source-code">    return response</p><p class="source-code"># Create task UI</p><p class="source-code">humanTaskUiResponse = create_task_ui()</p><p class="source-code">humanTaskUiArn = humanTaskUiResponse['HumanTaskUiArn']</p><p class="source-code">print(humanTaskUiArn)</p></li>
				<li>We get the output as shown:<p class="source-code">arn:aws:sagemaker:us-east-1:&lt;aws-account-nr&gt;:human-task-ui/ui-nlp-chapter14-&lt;timestamp&gt;</p></li>
				<li>Execute the<a id="_idIndexMarker834"/> next couple of cells in the notebook to create the <strong class="bold">Amazon A2I flow definition</strong> that manages the orchestration of tasks to workforces and the collection of the output data. We are now ready to start the human workflow loop. Execute the next code block in the notebook to start the human loop.</li>
				<li>Check the status of your human loop by executing the code block in the next cell in the notebook – it should be <strong class="source-inline">InProgress</strong>:<p class="source-code">completed_human_loops = []</p><p class="source-code">a2i_resp = a2i.describe_human_loop(HumanLoopName=humanLoopName)</p><p class="source-code">print(f'HumanLoop Name: {humanLoopName}')</p><p class="source-code">print(f'HumanLoop Status: {a2i_resp["HumanLoopStatus"]}')</p><p class="source-code">print(f'HumanLoop Output Destination: {a2i_resp["HumanLoopOutput"]}')</p><p class="source-code">print('\n')</p><p class="source-code">if a2i_resp["HumanLoopStatus"] == "Completed":</p><p class="source-code">    completed_human_loops.append(resp)</p></li>
				<li>We get the output as shown:<p class="source-code">HumanLoop Name: 0fe076a4-b6eb-49ea-83bf-78f953a71c89</p><p class="source-code">HumanLoop Status: InProgress</p><p class="source-code">HumanLoop Output Destination: {'OutputS3Uri': 's3://&lt;your-bucket-name&gt;/chapter4/a2i-results/fd-nlp-chapter4-2021-07-06-22-32-21/2021/07/06/22/33/08/&lt;hashnr&gt;/output.json'</p></li>
			</ol>
			<p>In the next section, we will walk through how your private reviewers can log in to the console and review the entities detected by Amazon Comprehend.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor173"/>Reviewing and modifying detected entities</h2>
			<p>Now, we <a id="_idIndexMarker835"/>will log in to the <strong class="bold">Amazon</strong> <strong class="bold">A2I</strong> <strong class="bold">Task UI</strong> to review, change, and re-label the detected entities from our Comprehend custom entity <a id="_idIndexMarker836"/>recognizer. Execute the cells in the notebook based on the instructions discussed in this sectio:.</p>
			<ol>
				<li value="1">Let's log in to the worker portal to review the predictions and modify them as required. Execute the following code to get the URL to our Task UI:<p class="source-code">workteamName = WORKTEAM_ARN[WORKTEAM_ARN.rfind('/') + 1:]</p><p class="source-code">print("Navigate to the private worker portal and do the tasks. Make sure you've invited yourself to your workteam!")</p><p class="source-code">print('https://' + sagemaker.describe_workteam(WorkteamName=workteamName)['Workteam']['SubDomain'])</p></li>
				<li>Once you log in you will see a <strong class="bold">LOAN APPLICATION REVIEW</strong> task. Select it and click on <strong class="bold">Start working</strong>:<div id="_idContainer268" class="IMG---Figure"><img src="Images/B17528_14_07.jpg" alt="Figure 14.7 – Amazon A2I task list&#13;&#10;" width="743" height="151"/></div><p class="figure-caption">Figure 14.7 – Amazon A2I task list</p></li>
				<li>You should see the Amazon A2I labeling UI with the list of entities detected by Comprehend custom entity recognition highlighted along with the labels, as shown in the following screenshot:<div id="_idContainer269" class="IMG---Figure"><img src="Images/B17528_14_08.jpg" alt="Figure 14.8 – Amazon A2I labeling UI ready for human review" width="1278" height="331"/></div><p class="figure-caption">Figure 14.8 – Amazon A2I labeling UI ready for human review</p></li>
				<li>Now, select <a id="_idIndexMarker837"/>the <strong class="bold">GHOST</strong> label from the labels on the right then assign this to the unlabeled <strong class="bold">Country:US</strong> entry in the UI and click <strong class="bold">Submit</strong>.   <div id="_idContainer270" class="IMG---Figure"><img src="Images/B17528_14_09.jpg" alt="Figure 14.9 – Adding/modifying labels to the detected entities and clicking Submit" width="1278" height="363"/></div><p class="figure-caption">Figure 14.9 – Adding/modifying labels to the detected entities and clicking Submit</p></li>
				<li>Continue <a id="_idIndexMarker838"/>executing the cells in the notebook to check the status of the human loop again (this should show a status of <strong class="bold">Completed</strong>) and print the Amazon A2I output JSON object. If there is a difference in entities, we will update the <strong class="source-inline">entitylist.csv</strong> file and trigger a retraining of our Comprehend custom entity recognizer. Let's verify whether new entities are present:<p class="source-code">retrain='N'</p><p class="source-code">el = open('train/entitylist.csv','r').read()</p><p class="source-code">for annotated_entity in a2i_entities:</p><p class="source-code">    if original_text[annotated_entity['startOffset']:annotated_entity['endOffset']] not in el:</p><p class="source-code">        retrain='Y'</p><p class="source-code">        word = '\n'+original_text[annotated_entity['startOffset']:annotated_entity['endOffset']]+','+annotated_entity['label'].upper()</p><p class="source-code">        print("Updating Entity List with: " + word)</p><p class="source-code">        open('train/entitylist.csv','a').write(word)</p><p class="source-code">if retrain == 'Y':</p><p class="source-code">    print("Entity list updated, model to be retrained")</p></li>
				<li>We see the<a id="_idIndexMarker839"/> output as shown in the following code block. Though Comprehend detected <strong class="source-inline">Years</strong> and <strong class="source-inline">Cell Phone</strong> to be a <strong class="source-inline">PERSON</strong> entity, it was not present in the original <strong class="source-inline">entitylist.csv</strong> file, and so it will be updated with these values and the Comprehend entity recognition will be re-trained:<p class="source-code">Updating Entity List with: </p><p class="source-code">Country:US,GHOST</p><p class="source-code">Updating Entity List with: </p><p class="source-code">Years:18,PERSON</p><p class="source-code">Updating Entity List with: </p><p class="source-code">Cell Phone:(555 ) 0200 1234,PERSON</p><p class="source-code">Entity list updated, model to be retrained</p></li>
			</ol>
			<p>This response is saved automatically in the Amazon S3 bucket JSON file in the form of labels. In the next section, we will use these modified or reviewed labels to retrain our custom entity recognizer model.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor174"/>Retraining Comprehend custom entity recognizer</h2>
			<p>We will now retrain our <a id="_idIndexMarker840"/>Comprehend custom entity recognizer. The cells to be executed are similar to what we did when we originally trained our recognizer:</p>
			<ol>
				<li value="1">Execute the cells in <em class="italic">Step 7</em> of the notebook: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2014/chapter14-auditing-workflows-named-entity-detection-forGitHub.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2014/chapter14-auditing-workflows-named-entity-detection-forGitHub.ipynb</a>.<p>After declaring<a id="_idIndexMarker841"/> variables, we execute the following code block to start the training:</p><p class="source-code">Import datetime</p><p class="source-code">cer_name = "retrain-loan-recognizer"+str(datetime.datetime.now().strftime("%s"))</p><p class="source-code">cer_response = comprehend.create_entity_recognizer(</p><p class="source-code">        RecognizerName = cer_name, </p><p class="source-code">        DataAccessRoleArn = role,</p><p class="source-code">        InputDataConfig = cer_input_object,</p><p class="source-code">        LanguageCode = "en"</p><p class="source-code">)</p></li>
				<li>We see the output shown that indicates the retraining job has been submitted. The metadata has been removed from the response for clarity:<p class="source-code">{   'EntityRecognizerProperties': {   'DataAccessRoleArn': 'arn:aws:iam::&lt;aws-account-nr&gt;:role/service-role/&lt;execution-role&gt;',</p><p class="source-code">                                      'EntityRecognizerArn': 'arn:aws:comprehend:us-east-1:&lt;aws-account-nr&gt;:entity-recognizer/retrain-loan-recognizer1625612436',</p><p class="source-code">                                      'InputDataConfig': {   'DataFormat': 'COMPREHEND_CSV',</p><p class="source-code">                                                             'Documents': {   'S3Uri': 's3://&lt;s3-bucket&gt;/chapter4/train/raw_txt.csv'},</p><p class="source-code">                                                             'EntityList': {   'S3Uri': 's3://&lt;s3-bucket&gt;/chapter4/train/entitylist.csv'},</p><p class="source-code">                                                             'EntityTypes': [   {   'Type': 'PERSON'},</p><p class="source-code">                                                                                {   'Type': 'GHOST'}]},</p><p class="source-code">                                      'LanguageCode': 'en',</p><p class="source-code">                                      'Status': 'SUBMITTED',</p><p class="source-code">                                      'SubmitTime': datetime.datetime(2021, 7, 6, 23, 0, 36, 759000, tzinfo=tzlocal())}}</p></li>
				<li>As before, go<a id="_idIndexMarker842"/> to the Amazon Comprehend console to check the status of the entity recognizer, and verify that the status has changed to <strong class="bold">Trained</strong>.</li>
				<li>Please repeat <em class="italic">Steps 3</em> to <em class="italic">5</em> from the notebook to test the newly retrained recognizer.</li>
			</ol>
			<p>Let's now execute the steps to store the results of the authentication check for access by applications downstream.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor175"/>Storing decisions for downstream processing</h2>
			<p>Now we understand <a id="_idIndexMarker843"/>how to set up an auditing workflow, let's execute the steps needed to persist the results from our entity detection so we can send them to a downstream application. If the majority or all of the entities are of the <strong class="source-inline">GHOST</strong> type, we will send a <em class="italic">rejection</em> decision, if the majority is of the <strong class="source-inline">PERSON</strong> type, we will send a <em class="italic">summary approval</em>, if all of them are <strong class="source-inline">PERSON</strong>, we will send <em class="italic">approval</em>, and if they are evenly distributed, we will send a <em class="italic">rejection</em> decisio:.</p>
			<ol>
				<li value="1">First, let's<a id="_idIndexMarker844"/> check how many entities were detected to be of the <strong class="source-inline">PERSON</strong> or <strong class="source-inline">GHOST</strong> type from A2I. Execute the first cell in <em class="italic">Step 8</em> from the notebook. We get the output as shown:<p class="source-code">[{'endOffset': 10, 'label': 'GHOST', 'startOffset': 0},</p><p class="source-code"> {'endOffset': 19, 'label': 'PERSON', 'startOffset': 11},</p><p class="source-code"> {'endOffset': 47, 'label': 'PERSON', 'startOffset': 20},</p><p class="source-code"> {'endOffset': 65, 'label': 'PERSON', 'startOffset': 48},</p><p class="source-code"> {'endOffset': 104, 'label': 'PERSON', 'startOffset': 66},</p><p class="source-code"> {'endOffset': 126, 'label': 'PERSON', 'startOffset': 105},</p><p class="source-code"> {'endOffset': 155, 'label': 'PERSON', 'startOffset': 127}]</p></li>
				<li>Let's apply the <a id="_idIndexMarker845"/>preceding rules to determine the decision for this loan application:<p class="source-code">from collections import Counter</p><p class="source-code">docstatus = ''</p><p class="source-code">ghost = float(Counter(labellist)['GHOST'])</p><p class="source-code">person = float(Counter(labellist)['PERSON'])</p><p class="source-code">if ghost &gt;= len(labellist)*.5:</p><p class="source-code">    docstatus = 'REJECT'</p><p class="source-code">elif min(len(labellist)*.5, len(labellist)*.8) &lt; person &lt; max(len(labellist)*.5, len(labellist)*.8):</p><p class="source-code">    docstatus = 'SUMMARY APPROVE'</p><p class="source-code">elif person &gt; len(labellist)*.8:</p><p class="source-code">    docstatus = 'APPROVE'</p><p class="source-code">print(docstatus)    </p></li>
				<li>We get the output <strong class="source-inline">APPROVE</strong>.</li>
				<li>Store the decision in an Amazon DynamoDB table (reminder: a managed database service for storing and accessing key-value pairs with very low latency). Loan processors can use this data to start the pre-qualification process. Execute the next cell in the notebook to create the DynamoDB table.</li>
				<li>Now, execute<a id="_idIndexMarker846"/> the next cell in the notebook to insert the contents of the loan application and the decision into the table. We see the values inserted into the DynamoDB table as follows:</li>
			</ol>
			<div>
				<div id="_idContainer271" class="IMG---Figure">
					<img src="Images/B17528_14_10.jpg" alt="Figure 14.10 – Loan authenticity check status in DynamoDB&#13;&#10;" width="1376" height="488"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.10 – Loan authenticity check status in DynamoDB</p>
			<p>That concludes the solution build. Please refer to the <em class="italic">Further reading</em> section for more examples of approaches for this use case, as well as the code sample for building a similar solution using <strong class="bold">AWS</strong> <strong class="bold">Lambda</strong> and <strong class="bold">CloudFormation</strong>.</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor176"/>Summary</h1>
			<p>In this chapter, we learned how to build an auditing workflow for named entity recognition to solve real-world challenges that many organizations face today with document processing, using Amazon Textract, Amazon Comprehend, and Amazon A2I. We reviewed the loan authentication use case to validate the documents before they can be passed to a loan processor. We considered an architecture based on conditions such as reducing the validation time from 2 to 4 weeks to 24 hours within the first 3 months of solution implementation. We assumed that you, the reader, are the solution architect assigned to this project, and we reviewed an overview of the solution components along with an architectural illustration in <em class="italic">Figure 4.1</em>.</p>
			<p>We then went through the pre-requisites for the solution build, set up an Amazon SageMaker Notebook instance, cloned our GitHub repository, and started executing the code in the notebook based on instructions from this chapter. We covered training an Amazon Comprehend custom entity recognizer, setting up our private work team using Amazon SageMaker labeling workforces, extracting the relevant content from the loan application using Amazon Textract, sending it to the Comprehend custom entity recognizer for detecting entities, forwarding the detection results to an Amazon A2I human review loop, completing the human task steps using the UI, reviewing the results of the review, updating the entities list to retrain the custom entity recognizer, and finally, storing the document contents and the loan validation decision to an Amazon DynamoDB table for downstream processing.</p>
			<p>In the next chapter, we will be building a classical use case that's tailor-made for NLP – namely, the active learning workflow for text classification. We will be training a text classification model using Amazon Comprehend custom for labeling documents into classes, review predictions using Amazon A2I, and retrain the classifier based on feedback from the Amazon A2I human review loop. We will demonstrate how the solution evolves in intelligence in being able to improve classification accuracy because of the feedback loop.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor177"/>Further reading</h1>
			<ul>
				<li><em class="italic">Building an end-to-end intelligent document processing solution using AWS</em> by Purnesh Tripathi: <a href="https://aws.amazon.com/blogs/machine-learning/building-an-end-to-end-intelligent-document-processing-solution-using-aws/">https://aws.amazon.com/blogs/machine-learning/building-an-end-to-end-intelligent-document-processing-solution-using-aws/</a></li>
				<li><em class="italic">Setting up human review of your NLP-based entity recognition models with Amazon SageMaker Ground Truth, Amazon Comprehend, and Amazon A2I</em> by Mona Mona and Prem Ranga: <a href="https://aws.amazon.com/blogs/machine-learning/setting-up-human-review-of-your-nlp-based-entity-recognition-models-with-amazon-sagemaker-ground-truth-amazon-comprehend-and-amazon-a2i/">https://aws.amazon.com/blogs/machine-learning/setting-up-human-review-of-your-nlp-based-entity-recognition-models-with-amazon-sagemaker-ground-truth-amazon-comprehend-and-amazon-a2i/</a></li>
				<li><em class="italic">Announcing model improvements and lower annotation limits for Amazon Comprehend custom entity recognition</em> by Prem Ranga, Chethan Krishna, and Mona Mona: <a href="https://aws.amazon.com/blogs/machine-learning/announcing-model-improvements-and-lower-annotation-limits-for-amazon-comprehend-custom-entity-recognition/">https://aws.amazon.com/blogs/machine-learning/announcing-model-improvements-and-lower-annotation-limits-for-amazon-comprehend-custom-entity-recognition/</a></li>
			</ul>
		</div>
	</div></body></html>