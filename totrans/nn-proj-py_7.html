<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Implementing a Facial Recognition System with Neural Networks</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we will implement a facial recognition system using a <strong class="calibre4">Siamese neural network</strong>. Such facial recognition systems are prevalent in smartphones and other smart security systems in modern buildings and facilities. We will go through the theory behind Siamese neural networks, and why facial recognition problems are a special class of problems in image recognition, making it difficult for a conventional <strong class="calibre4">convolutional neural networks</strong> (<strong class="calibre4">CNNs</strong>) to solve them. We will train and implement a robust model that can recognize faces, even when the subject has different expressions and when the photo is taken from different angles. Finally, we will write our own program that uses the pre-trained neural network and a webcam, to authenticate the user sitting in front of the computer.</p>
<p class="calibre2">Specifically, these are the topics that we will cover in this chapter:</p>
<ul class="calibre11">
<li class="calibre12">The facial recognition problem</li>
<li class="calibre12">Face detection and face recognition</li>
<li class="calibre12">One-shot learning</li>
<li class="calibre12">Siamese neural networks</li>
<li class="calibre12">Contrastive loss</li>
<li class="calibre12">Faces dataset</li>
<li class="calibre12">Training a Siamese neural network in Keras</li>
<li class="calibre12">Creating your own facial recognition system</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="calibre2">The following Python libraries are required for this chapter:</p>
<ul class="calibre11">
<li class="calibre12">Numpy 1.15.2</li>
<li class="calibre12">Keras 2.2.4</li>
<li class="calibre12">OpenCV 3.4.2</li>
<li class="calibre12">PIL 5.4.1</li>
</ul>
<p class="calibre2">The code for this chapter can be found in the GitHub repository for this book at <a href="https://github.com/PacktPublishing/Neural-Network-Projects-with-Python/tree/master/chapter7" target="_blank" class="calibre10">https://github.com/PacktPublishing/Neural-Network-Projects-with-Python/tree/master/Chapter07</a>.</p>
<p class="calibre2">To download the code onto your computer, you may run the following <kbd class="calibre13">git clone</kbd> command:</p>
<pre class="calibre17"><strong class="calibre1">$ git clone https://github.com/PacktPublishing/Neural-Network-Projects-with-Python.git</strong></pre>
<p class="calibre2">After the process is complete, there will be a folder entitled <kbd class="calibre13">Neural-Network-Projects-with-Python</kbd>. Enter the folder by running the following:</p>
<pre class="calibre17"><strong class="calibre1">$ cd Neural-Network-Projects-with-Python</strong></pre>
<p class="calibre2">To install the required Python libraries in a virtual environment, run the following command:</p>
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre17"><strong class="calibre1"><span>$ conda</span> <span>env</span> <span>create</span> <span>-</span><span>f</span> <span>environment</span><span>.</span><span>yml</span></strong></pre></div>
</div>
<p class="calibre2">Note that you should have installed Anaconda on your computer first before running this command. To enter the virtual environment, run the following command:</p>
<pre class="calibre17"><strong class="calibre1">$ conda activate neural-network-projects-python</strong></pre>
<p class="calibre2">Navigate to the<span class="calibre5"> </span><kbd class="calibre13">Chapter07</kbd><span class="calibre5"> folder </span>by running the following command:</p>
<pre class="calibre17"><strong class="calibre1">$ cd Chapter07</strong></pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">The following files are located in the folder:</p>
<ul class="calibre11">
<li class="calibre12"><kbd class="calibre13">face_detection.py</kbd> contains the Python code for face detection using OpenCV</li>
<li class="calibre12"><kbd class="calibre13">siamese_nn.py</kbd> contains the Python code to create and train a Siamese neural network</li>
<li class="calibre12"><kbd class="calibre13">onboarding.py</kbd> contains the Python code for the onboarding process of the face recognition system</li>
<li class="calibre12"><kbd class="calibre13">face_recognition_system.py</kbd> contains the complete face recognition system program</li>
</ul>
<p class="calibre2">Please run the Python files in this order:</p>
<ol class="calibre14">
<li class="calibre12"><span><kbd class="calibre13">siamese_nn.py</kbd>: To train a Siamese neural network for face recognition</span></li>
<li class="calibre12"><kbd class="calibre13">onboarding.py</kbd>: To start the onboarding process for the face recognition system</li>
<li class="calibre12"><span><kbd class="calibre13">face_recognition_system.py</kbd>: The actual face recognition program that uses your webcam</span></li>
</ol>
<p class="calibre2">To run each Python file, simply execute the files as follows:</p>
<pre class="calibre17"><strong class="calibre1">$ python siamese_nn.py</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Facial recognition systems</h1>
                </header>
            
            <article>
                
<p class="calibre2">Facial recognition systems have become ubiquitous in our every lives. When the iPhone X was first unveiled in 2017, Apple boasted that their new state-of-the-art face ID system was able to instantaneously recognize and authenticate users with just a single glance. Driving this was the Apple A11 Bionic chip, which includes dedicated neural network hardware, allowing the iPhone to perform blazingly fast<span class="calibre5"> facial recognition and </span>machine learning operations. Today, almost all smartphones have a facial recognition security system. </p>
<p class="calibre2">In 2016, Amazon started its first supermarket with advanced facial recognition capabilities, known as <strong class="calibre4">Amazon Go</strong>. Unlike traditional supermarkets, Amazon Go uses facial recognition to know when you first arrive at the supermarket and when you removed an item from the shelf. When you've finished shopping, you can simply walk out of the store, without waiting in line at the cashier, as all your purchases are captured by Amazon's AI systems. This allows busy shoppers to do their grocery shopping in person at the supermarket, without wasting time waiting in line for the cashier. No longer belonging to a dystopian future, facial recognition systems have already become an important part of our everyday lives.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Breaking down the face recognition problem</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's break down the face recognition problem into smaller steps and subproblems. That way, we can better understand what's going on under the hood of a facial recognition system. A face recognition problem can be broken down into the following smaller subproblems:</p>
<ul class="calibre11">
<li class="calibre12"><strong class="calibre1">Face</strong> <strong class="calibre1">detection</strong>: Detect and isolate faces in the image. In an image with multiple faces, we need to detect each of them separately. In this step, we should also crop the detected faces from the original input image, to identify them separately.</li>
<li class="calibre12"><strong class="calibre1">Face recognition</strong>: For each detected face in the image, we run it through a neural network to classify the subject. Note that we need to repeat this step for each detected face.</li>
</ul>
<p class="calibre2">Intuitively, this process makes a lot of sense. If we think of how humans recognize faces, we see that it is very similar to the process that we described. Given an image, our eyes immediately zoom into each face (face detection), and we recognize the faces individually (face recognition).</p>
<p class="calibre2">The following diagram illustrates the subprocesses in face recognition:</p>
<p class="mce-root"><img class="alignnone109" src="assets/3d23ef1b-ff8d-41bd-bae8-09ae292d5ab1.png"/></p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Face detection</h1>
                </header>
            
            <article>
                
<p class="calibre2">First of all, let's take a look at face detection. The face detection problem is actually a rather interesting problem in computer vision that researchers have worked on for many years. In 2001, Viola and Jones demonstrated how real-time, large-scale face detection can be done with minimal computational resources. This was a significant discovery at the time, as researchers seek to do real-time, large-scale face detection (for example, to monitor a large crowd in real-time). Today, face detection algorithms can be run on simple hardware such as our personal computers with just a few lines of code. In fact, as we shall see shortly, we will use OpenCV in Python to construct a face detector, using your own webcam.</p>
<p class="calibre2">There are several approaches to face detection, including the following:</p>
<ul class="calibre11">
<li class="calibre12">Haar Cascades</li>
<li class="calibre12">Eigenfaces</li>
<li class="calibre12"><strong class="calibre1">Histogram of Oriented Gradients</strong> (<strong class="calibre1">HOG</strong>)</li>
</ul>
<p class="calibre2">We'll explain how to do face detection using Haar Cascades (as presented by Viola and Jones in 2001), and we'll see the beautiful simplicity in this algorithm.</p>
<p class="calibre2">The key idea behind the Viola-Jones algorithm is that all human faces share certain properties, such as the following:</p>
<ul class="calibre11">
<li class="calibre12">The area of the eye is darker than the forehead and the cheeks</li>
<li class="calibre12">The area of the nose is brighter than the eyes</li>
</ul>
<p class="calibre2">In a frontal, non-occluded image of a human face, we can see features such as the eyes, the nose, and the lips. If we look closely at the area around the eyes, we see that there is a repeating pattern of dark and light pixels, as shown in the following diagram:</p>
<p class="mce-root"><img class="alignnone110" src="assets/8a57d1f7-b0b9-472f-b431-4c0ef7dfa433.png"/></p>
<p class="calibre2">Of course, the preceding example is just one possible feature. We can also construct other features that capture other regions of the face, such as the nose, lips, chin, and so on. Some examples of other features are shown in the following diagram:</p>
<p class="mce-root"><img class="alignnone111" src="assets/e1edeb53-95bc-4219-b485-cedd8bd3b55a.png"/></p>
<p class="calibre2">These features with alternating regions of dark and light pixels are known as Haar features. Depending on your imagination, you can construct an almost infinite number of features. In fact, in the final algorithm presented by Viola and Jones, there were more than 6,000 Haar features used! </p>
<div class="packttip">Do you see the similarities between Haar features and convolutional filters? They both detect identifying geometric representations in images! The difference is that Haar features are handcrafted features that detect eyes, noses, lips, and so on, in human faces, based on what we know. On the other hand, convolutional filters are created during training, using a labeled dataset and are not handcrafted. However, they perform the same function: identifying geometric representation in images. The similarities between Haar features and convolutional filters show that many ideas in machine learning and AI are shared and improved iteratively over the years. </div>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">To use the Haar features, we slide them over every region in the image and compute the similarity of the pixels with the Haar features. However, since most areas in an image do not contain a face (think about the photos we take—faces are usually limited to a small area in the photo), it is computationally wasteful to test all the features. To overcome this, Viola and Jones introduced a cascade classifier<strong class="calibre4">.</strong> The idea is to start with the most simple Haar feature. If the candidate region fails, this simple Haar feature (that is, the prediction from this feature is that the region does not contain a face), we immediately move on to the next candidate region. This way, we do not waste computational resources on regions that do not contain a face. We progressively move on to more complex Haar features, and we repeat the process. Eventually, the regions in the image with a face are the regions that pass all the Haar features. This classifier is known as a <strong class="calibre4">cascade classifier</strong>.</p>
<p class="calibre2">The Viola-Jones algorithm using Haar features demonstrated remarkable accuracy and false positive rates in face detection, while being computationally efficient. In fact, when the algorithm was first presented in 2001, it was running on a 700 Mhz Pentium III processor!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Face detection in Python</h1>
                </header>
            
            <article>
                
<p class="calibre2">Face detection can be implemented by the OpenCV library in Python. OpenCV is an open source computer vision library for computer vision tasks. Let's see how we can use OpenCV for face detection.</p>
<p class="calibre2">First, we import OpenCV:</p>
<pre class="calibre17">import cv2</pre>
<p class="calibre2">Next, let's load a pre-trained cascade classifier for face detection. This cascade classifier can be found in the accompanying GitHub repository and should have been downloaded to your computer (refer to the <em class="calibre8">Technical requirements </em>section):</p>
<pre class="calibre17">face_cascades = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')</pre>
<p class="calibre2">Next, we define a function that takes in an image, performs face detection on the image, and draws a bounding box around the image:</p>
<pre class="calibre17">def detect_faces(img, draw_box=True):<br class="title-page-name"/>    # convert image to grayscale<br class="title-page-name"/>    grayscale_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br class="title-page-name"/><br class="title-page-name"/>    # detect faces<br class="title-page-name"/>    faces = face_cascades.detectMultiScale(grayscale_img, scaleFactor=1.6)<br class="title-page-name"/><br class="title-page-name"/>    # draw bounding box around detected faces<br class="title-page-name"/>    for (x, y, width, height) in faces:<br class="title-page-name"/>        if draw_box:<br class="title-page-name"/>            cv2.rectangle(img, (x, y), (x+width, y+height), (0, 255, 0), 5)<br class="title-page-name"/>    face_box = img[y:y+height, x:x+width]<br class="title-page-name"/>    face_coords = [x,y,width,height]<br class="title-page-name"/>    return img, face_box, face_coords</pre>
<p class="calibre2">Let's test our face detector on some sample images. The images can be found in the<span class="calibre5"> </span><kbd class="calibre13">'sample_faces'</kbd><span class="calibre5"> </span>folder, and they look like this:</p>
<p class="mce-root"><img class="alignnone112" src="assets/6f9b5a39-b84c-4350-9e3a-4bfe7cee3bbc.png"/></p>
<p class="calibre2">As we can see, there is a fair amount of noise (that is, non-face structures) in each image, which can potentially trip up our face detector. In the bottom-right image, we can also see that there are multiple faces.</p>
<p class="calibre2">We apply the<span class="calibre5"> </span><kbd class="calibre13">detect_faces</kbd><span class="calibre5"> </span>function that we defined earlier on these images:</p>
<pre class="calibre17">import os<br class="title-page-name"/>files = os.listdir('sample_faces')<br class="title-page-name"/>images = [file for file in files if 'jpg' in file]<br class="title-page-name"/>for image in images:<br class="title-page-name"/>    img = cv2.imread('sample_faces/' + image)<br class="title-page-name"/>    detected_faces, _, _ = detect_faces(img)<br class="title-page-name"/>    cv2.imwrite('sample_faces/detected_faces/' + image, detected_faces)</pre>
<p class="calibre2">We see the following output images saved in the<span class="calibre5"> </span><kbd class="calibre13">'sample_faces/detected_faces'</kbd><span class="calibre5"> </span>folder:</p>
<p class="mce-root"><img class="alignnone113" src="assets/75737513-8a3e-4437-8c8a-d6441259c7d9.png"/></p>
<p class="calibre2">Fantastic! Our face detector passed with flying colors. The speed of the detection was really impressive as well. We can see that face detection using OpenCV in Python is simple and takes no time at all.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Face recognition</h1>
                </header>
            
            <article>
                
<p class="calibre2">With face detection complete, let's turn our attention to the next step: face recognition. You might have noticed that face detection had nothing to do with neural networks! Face detection using Haar features is an old but reliable algorithm that is still widely used today. However, face detection only extracts the region that contains a face. Our next step would be to perform face recognition using the extracted faces.</p>
<p class="calibre2">Face recognition using neural networks is the main topic in this chapter. For the rest of the chapter, we'll focus on training a neural network for face recognition.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Requirements of face recognition systems</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">At this point, you should be fairly familiar with using neural networks for image recognition tasks. In <a href="48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml" target="_blank" class="calibre10">Chapter 4</a>, <em class="calibre8">Cats Versus Dogs – Image Classification Using CNNs,</em> we built a CNN for classifying images of cats versus dogs. Can the same techniques be used in facial recognition? Sadly, CNNs fall short for this task. To understand why, we need to look at the requirements of facial recognition systems.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speed</h1>
                </header>
            
            <article>
                
<p class="calibre2">The first requirement of a facial recognition system is that they need to be fast. If we look at the onboarding process of the facial recognition systems in our smartphones, we usually need to use the front-facing camera in the phone to scan our face at various angles for a few seconds. During this short process, our phone captures images of our face, and uses an image to train a neural network to recognize us. This process needs to be fast.</p>
<p class="calibre2">The following picture shows the typical onboarding process for a facial recognition system in smartphones:</p>
<p class="mce-root"><img class="alignnone114" src="assets/33ca966e-e49a-4837-9f0d-a833047c7a71.png"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Can a CNN satisfy this speed requirement? From the project that we built in <a href="48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml" target="_blank" class="calibre10">Chapter 4</a>, <em class="calibre8">Cats Versus Dogs – Image Classification Using CNNs,</em> we saw how slow it is to train a CNN to identify images of cats and dogs. Even with powerful GPUs, training a CNN can sometimes take hours (or even days!). From a user experience point of view, it is not practical for the onboarding process of facial recognition systems to take this long. Therefore, CNNs do not satisfy the speed requirement of facial recognition systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Scalability</h1>
                </header>
            
            <article>
                
<p class="calibre2">The second requirement of facial recognition systems is that it needs to be scalable. The model that we train must ultimately be able to scale to millions of different users, each with a unique face. Again, this is where CNNs fall short. Recall that in <a href="48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml" target="_blank" class="calibre10">Chapter 4</a>, <em class="calibre8">Cats Versus Dogs – Image Classification Using CNNs,</em> we trained a CNN to differentiate cats from dogs. This neural network is only able to identify and classify images of cats and dogs, and not of other animals, which it was not trained on. This means that if we were to use CNNs for facial recognition, we would have to train a separate neural network for each individual user. This would simply be unworkable from a scalability point of view! This would mean that Amazon would need to train an individual neural network for each of its millions of users, and to run through millions of different neural networks whenever a user walks through the doors of Amazon Go.</p>
<p class="calibre2">The following diagram illustrates the constraints faced by CNNs on facial recognition:</p>
<p class="mce-root"><img class="alignnone115" src="assets/cf156d05-e93d-4747-9587-30b7f6e81ca4.png"/></p>
<p class="calibre2">Given the constraints in memory, it is impractical to train a neural network for every user. Such a system would get bogged down very quickly as the number of users grew. Therefore, CNNs fail to provide a scalable solution for facial recognition.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High accuracy with small data</h1>
                </header>
            
            <article>
                
<p class="calibre2">The third requirement of a facial recognition system is that it needs to be sufficiently accurate (hence secure) while working with a small amount of training data. In <a href="48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml" target="_blank" class="calibre10">Chapter 4</a>, <em class="calibre8">Cats Versus Dogs – Image Classification Using CNNs,</em> we used a huge dataset containing thousands of images of cats and dogs for training our CNN. By contrast, we almost never get this luxury when it comes to the dataset size for facial recognition. Going back to the example of the onboarding process for facial recognition in smartphones, we can see that only a handful of photos are taken, and we need to be able to train our model, using this limited dataset. </p>
<p class="calibre2">Once again, CNNs do not satisfy this requirement, because we need lots of images to train a CNN. While CNNs are fairly accurate at image classification tasks, this comes at the expense of requiring a huge training set. Imagine having to take thousands of selfies with our smartphones before we can start using the facial recognition systems in our phones! <span class="calibre5">This would simply not work for most facial recognition systems.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">One-shot learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">Given the unique requirements and constraints faced by facial recognition systems, it is clear that the paradigm of training a CNN for classification using a huge dataset (known as batch learning classification) is unsuitable for the facial recognition problem. Instead, our objective is to create a neural network that can learn to recognize any face using just a single training sample. This form of neural network training is known as <strong class="calibre4">one-shot learning</strong>.</p>
<p class="calibre2">One-shot learning brings about a new and interesting paradigm in machine learning problems. Thus far, we have thought of machine learning problems as mostly classification problems. In <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre10">Chapter 2</a>, <em class="calibre8"><span class="calibre5">Predicting Diabetes, with Multilayer Perceptrons</span></em>, we used an MLP to classify patients at risk of diabetes. In <a href="48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml" target="_blank" class="calibre10">Chapter 4</a>, <em class="calibre8"><span class="calibre5">Cats Versus Dogs – Image Classification Using CNNs</span></em>,<em class="calibre8"> </em>we used a CNN to classify images of cats and dogs. In <a href="21ef7df7-5976-4e0d-bec5-d736ec571d94.xhtml" target="_blank" class="calibre10">Chapter 6</a>, <em class="calibre8">Sentiment Analysis of Movie Reviews Using LSTM</em>, we used an LSTM network to classify the sentiment of movie reviews. In this chapter, we need to approach facial recognition not simply as a classification problem, but also as an estimation of the similarity between two input images.</p>
<p class="calibre2">As an example, a one-shot learning facial recognition model should perform the following tasks when determining whether the presented face belongs to an arbitrary person (say, person A):</p>
<ol class="calibre14">
<li class="calibre12">Retrieve the stored image of person A (obtained during the onboarding process). This is the <em class="calibre18">true</em> image of person A.</li>
<li class="calibre12">At testing time (for example, when someone is is trying to unlock the phone of person A), capture the image of the person. This is the <em class="calibre18">test</em> image.</li>
<li class="calibre12">Using the <em class="calibre18">true</em> photo and the <em class="calibre18">test</em> photo, the neural network should output a similarity score of the faces in the two photos. </li>
<li class="calibre12">If the similarity score output by the neural network is below a certain threshold (that is, the people in the two photos look dissimilar), we deny access, and if they are above the threshold, we grant access.</li>
</ol>
<p class="calibre2">The following diagram illustrates this process:</p>
<p class="mce-root"><img class="alignnone116" src="assets/2dae64c6-aa7e-41f7-90dd-08d55ef122e8.png"/></p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive one-shot prediction – Euclidean distance between two vectors</h1>
                </header>
            
            <article>
                
<p class="calibre2">Before we dive into how neural networks can be used for one-shot learning, let's look at one naive approach.</p>
<p class="calibre2">Given the true image and a test image, one naive approach for a one-shot prediction is to simply measure the difference between the two images. As we have already seen, all images are simply three-dimensional vectors. We know that the Euclidean distance provides a mathematical formulation of the difference between two vectors. To refresh your memory, the Euclidean distance between two vectors is shown in the following diagram:</p>
<p class="mce-root"><img class="alignnone117" src="assets/0635d454-58c0-4267-b65c-2eca8083c55d.png"/></p>
<p class="calibre2">Measuring the Euclidean distance between two images provides us with a naive approach for a one-shot prediction. However, does it provide us with a satisfactory similar score for facial recognition? The answer is no. Although the Euclidean distance for facial recognition makes sense on paper, it has a poor practical value. In reality, photos can be different due to variations in angles and lighting, and also changes in the appearance of the subject, which can arise due to the wearing of accessories such as glasses. As you can imagine, a facial recognition system that uses the Euclidean distance alone would perform terribly in reality.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Siamese neural networks</h1>
                </header>
            
            <article>
                
<p class="calibre2">So far, we have seen that a pure CNN and a pure Euclidean distance approach would not work well for facial recognition. However, we don't have to discard them entirely. Each of them provides something useful for us. Can we combine them together to form something better?</p>
<p class="calibre2"/>
<p class="calibre2">Intuitively, humans recognize faces by comparing their key features. For example, humans use features such as the shape of the eyes, the thickness of the eyebrows, the size of the nose, the overall shape of the face, and so on to recognize a person. This ability comes naturally to us, and we are seldom affected by variations in angles and lighting. Could we somehow teach a neural network to identify these features from images of faces, before using the Euclidean distance to measure the similarity between the identified features? This should sound familiar to you! As we have seen in the previous chapters, convolutional layers excel in finding such identifying features automatically. For facial recognition, researchers have found that when convolutional layers are applied to human faces, they extract spatial features, such as eyes and noses.</p>
<p class="calibre2">This insight forms the core of our algorithm for one-shot learning:</p>
<ul class="calibre11">
<li class="calibre12">Use convolutional layers to extract identifying features from faces. The output from the convolutional layers should be a mapping of the image to a lower-dimension feature space (for example, a 128 x 1 vector). The convolutional layers should map faces from the same subject close to one another in this <span>lower-dimension feature </span>space and vice versa, faces from different subjects should be as far away as possible in this lower-dimension feature space.</li>
<li class="calibre12">Using the Euclidean distance, measure the difference of the two lower-dimension vectors output from the convolutional layers. Note that there are two vectors, because we are comparing two images (the true image and the test image). The Euclidean distance is inversely proportional to the similarity between the two images.</li>
</ul>
<p class="calibre2">This works better than the naive Euclidean distance approach from the previous section (applied to raw-image pixels), because the output from the convolutional layers in the first step represents identifying features in faces (such as eyes and noses), which are invariant to angles and lighting.</p>
<p class="calibre2">One last thing to note is that, since we are feeding two images into our neural network simultaneously, we need two separate sets of convolutional layers. However, we require the two separate sets of convolutional layers to share the same weights, because we want similar faces to be mapped to the same point in the lower-dimension feature space. If the weights from the two sets of convolutional layers are different, similar faces would be mapped to different points, and the Euclidean distance would not be a useful metric at all!</p>
<p class="calibre2"/>
<p class="calibre2">We can thus think of these two sets of convolutional layers as twins, as they share the same weights. The following diagram provides an illustration of the neural network that we have just described:</p>
<p class="mce-root"><img class="alignnone118" src="assets/9450953e-02e9-42d5-a113-5b48fc61ed6e.png"/></p>
<p class="calibre2">This neural network is known as a Siamese neural network, because just like a Siamese twin, it has a conjoined component at the convolutional layers. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Contrastive loss</h1>
                </header>
            
            <article>
                
<p class="calibre2">This new paradigm of training a neural network for distance-based predictions instead of classification-based predictions requires a new loss function. Recall that in previous chapters, we used simple loss functions such as categorical cross-entropy to measure the accuracy of our predictions in classification problems.</p>
<p class="calibre2">In distance-based predictions, loss functions based on accuracy would not work. Therefore, we require a new distance-based loss function to train our Siamese neural network for facial recognition. The distance-based loss function that we will be using is called the <strong class="calibre4">contrastive loss function</strong>.</p>
<p class="calibre2">Take a look at the following variables:</p>
<ul class="calibre11">
<li class="CDPAlignLeft2"><em class="calibre18">Y<sub class="calibre21">true</sub></em>: Let <em class="calibre18">Y<sub class="calibre21">true</sub></em> be <em class="calibre18">1</em> if the two input images are from the same subject (same face) and 0 if the two input images are from different subjects (different faces)</li>
<li class="calibre12"><em class="calibre18">D</em>: The predicted distance output from the neural network</li>
</ul>
<p class="calibre2"/>
<p class="calibre2">So, the <em class="calibre8">Contrastive Loss</em> is defined as follows:</p>
<p class="mce-root"><img class="fm-editor-equation28" src="assets/6be69b84-79e2-48b6-85df-b47688f46533.png"/></p>
<p class="calibre2">Here, the margin is simply a constant regularizing term. Don't worry if the preceding equation looks scary! All it does is simply produce a high loss (that is, a penalty) when the predicted distance is large when the faces are similar, and a low loss when the predicted distance is small, and vice versa for the case when the faces are different. </p>
<p class="calibre2">The following graph shows the loss for the increasing predicted distance, when the faces are similar (left) and when the faces are different (right):</p>
<p class="mce-root"><img class="alignnone119" src="assets/d12eef3c-5aa0-4c2b-abe1-b440e44c05f6.png"/></p>
<p class="calibre2">Simply put, the contrastive loss function ensures that our Siamese neural network learns to predict a small distance when the faces in the true and test images are the same, and a large distance when the faces in the true and test images are different.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The faces dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's now look at the faces dataset that we will be using for this project. There are numerous publicly available faces dataset for use, as consolidated at <a href="http://www.face-rec.org/databases/" target="_blank" class="calibre10">http://www.face-rec.org/databases/</a>.</p>
<p class="calibre2">While there are many face datasets that we can use, the most appropriate dataset for training a facial recognition system should contain photos of different subjects, with each subject having multiple photos taken from different angles. It should also ideally contain photos of the subject wearing different expressions (eyes closed and so on), as such photos are commonly encountered by facial recognition systems.</p>
<p class="calibre2">With these considerations in mind, the dataset that we have chosen is the Database of Faces, created by AT&amp;T Laboratories, Cambridge. The database contains photos of 40 subjects, with 10 photos of each subject. The photos of each subject were taken under different lighting and angles, and they have different facial expressions. For certain subjects, multiple photos were taken of people with and without glasses. You may visit the website at <a href="https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html" target="_blank" class="calibre10">https://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html</a> to learn more about the AT&amp;T faces dataset.</p>
<p class="calibre2">The faces dataset is provided together with the code for this chapter. To download the dataset and the code from the GitHub repository, please follow the instructions in the <em class="calibre8">Technical requirements</em> section earlier in the chapter.</p>
<p class="calibre2">After downloading the GitHub repository, the dataset is located in the following path:</p>
<pre class="calibre17">'Chapter07/att_faces/'</pre>
<p class="calibre2">The images are stored in subfolders, with one subfolder per subject. Let's import the raw-image files as NumPy arrays in Python. We start by declaring a variable with the file path:</p>
<pre class="calibre17">faces_dir = 'att_faces/'</pre>
<p class="calibre2">Next, we want to iterate through each subfolder in the directory, and load each image in the subfolder as a NumPy array. To do that, we can import and use the <kbd class="calibre13">load_img</kbd> and <kbd class="calibre13">img_to_array</kbd> functions provided in <kbd class="calibre13">keras.preprocessing.image</kbd>:</p>
<pre class="calibre17">from keras.preprocessing.image import load_img, img_to_array</pre>
<p class="calibre2">Since there are 40 subjects, let's use images from the first 35 subjects as training samples and the remaining five subjects as testing samples. The following code iterates through each subfolder and loads the images into an <kbd class="calibre13">X_train</kbd> and an <kbd class="calibre13">X_test</kbd> array accordingly:</p>
<pre class="calibre17">import numpy as np<br class="title-page-name"/><br class="title-page-name"/>X_train, Y_train = [], []<br class="title-page-name"/>X_test, Y_test = [], []<br class="title-page-name"/><br class="title-page-name"/># Get list of subfolders from faces_dir<br class="title-page-name"/># Each subfolder contains images from one subject<br class="title-page-name"/>subfolders = sorted([f.path for f in os.scandir(faces_dir) if f.is_dir()])<br class="title-page-name"/><br class="title-page-name"/># Iterate through the list of subfolders (subjects)<br class="title-page-name"/># Idx is the subject ID<br class="title-page-name"/>for idx, folder in enumerate(subfolders):<br class="title-page-name"/>    for file in sorted(os.listdir(folder)):<br class="title-page-name"/>        img = load_img(folder+"/"+file, color_mode='grayscale')<br class="title-page-name"/>        img = img_to_array(img).astype('float32')/255<br class="title-page-name"/>        if idx &lt; 35:<br class="title-page-name"/>            X_train.append(img)<br class="title-page-name"/>            Y_train.append(idx)<br class="title-page-name"/>        else:<br class="title-page-name"/>            X_test.append(img)<br class="title-page-name"/>            Y_test.append(idx-35)</pre>
<p class="calibre2">Note that the label in <kbd class="calibre13">Y_train</kbd> and <kbd class="calibre13">Y_test</kbd> is simply the index of the subfolders as we iterate through each of them (that is, the subject in the first subfolder is assigned label <kbd class="calibre13">1</kbd>, the subject in the second subfolder is assigned label <kbd class="calibre13">2</kbd>, and so on).</p>
<p class="calibre2">Finally, we convert <kbd class="calibre13">X_train</kbd>, <kbd class="calibre13">Y_train</kbd>, <kbd class="calibre13">X_test</kbd>, and <kbd class="calibre13">X_test</kbd> into NumPy arrays:</p>
<pre class="calibre17">X_train = np.array(X_train)<br class="title-page-name"/>X_test = np.array(X_test)<br class="title-page-name"/>Y_train = np.array(Y_train)<br class="title-page-name"/>Y_test = np.array(Y_test)</pre>
<p class="calibre2">Good! We now have our training-and-testing dataset. We'll train our Siamese neural network using the training set and test it using the photos in the testing dataset.</p>
<p class="calibre2">Now, let's plot out some images from a subject to better understand the kind of data we are working with. The following code plots nine of the images from a particular subject (as entered in the <kbd class="calibre13">subject_idx</kbd> <span class="calibre5">variable</span>):</p>
<pre class="calibre17">from matplotlib import pyplot as plt<br class="title-page-name"/><br class="title-page-name"/>subject_idx = 4<br class="title-page-name"/>fig, ((ax1,ax2,ax3),(ax4,ax5,ax6),<br class="title-page-name"/>      (ax7,ax8,ax9)) = plt.subplots(3,3,figsize=(10,10))<br class="title-page-name"/>subject_img_idx = np.where(Y_train==subject_idx)[0].tolist()<br class="title-page-name"/><br class="title-page-name"/>for i, ax in enumerate([ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9]):<br class="title-page-name"/>    img = X_train[subject_img_idx[i]]<br class="title-page-name"/>    img = np.squeeze(img)<br class="title-page-name"/>    ax.imshow(img, cmap='gray')<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/>plt.tight_layout()<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">We see the following output:</p>
<p class="mce-root"><img class="alignnone120" src="assets/4d27829b-58e4-441a-81f1-c5f82e171b74.png"/></p>
<p class="calibre2">As we can see, each photo of the subject was taken at a different angle, and the subject had different facial expressions. In some photos, we can also see that the subject removed his glasses. There's certainly a lot of variation from image to image.</p>
<p class="calibre2">We can also plot a single image from the first nine subjects, using the following code:</p>
<pre class="calibre17"># Plot the first 9 subjects<br class="title-page-name"/>subjects = range(10)<br class="title-page-name"/><br class="title-page-name"/>fig, ((ax1,ax2,ax3),(ax4,ax5,ax6),<br class="title-page-name"/>      (ax7,ax8,ax9)) = plt.subplots(3,3,figsize=(10,12))<br class="title-page-name"/>subject_img_idx = [np.where(Y_train==i)[0].tolist()[0] for i in subjects]<br class="title-page-name"/><br class="title-page-name"/>for i, ax in enumerate([ax1,ax2,ax3,ax4,ax5,ax6,ax7,ax8,ax9]):<br class="title-page-name"/>    img = X_train[subject_img_idx[i]]<br class="title-page-name"/>    img = np.squeeze(img)<br class="title-page-name"/>    ax.imshow(img, cmap='gray')<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/>    ax.set_title("Subject {}".format(i))<br class="title-page-name"/>plt.show()<br class="title-page-name"/>plt.tight_layout()</pre>
<p class="calibre2">We'll get the following output:</p>
<p class="mce-root"><img class="alignnone121" src="assets/6b3d9602-0cab-459b-8cc0-ff40b29a1eee.png"/></p>
<p class="calibre2">Cool! It looks as though we have a diverse bunch of subjects to work with.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Siamese neural network in Keras</h1>
                </header>
            
            <article>
                
<p class="calibre2">We are finally ready to start creating a Siamese neural network in Keras. In the previous sections, we looked at the theory and the high-level structure of a Siamese neural network. Let's now look at the architecture of a Siamese neural network in greater detail. </p>
<p class="calibre2"/>
<p class="calibre2">The following diagram shows the detailed architecture of the Siamese neural network we'll build in this chapter:</p>
<p class="mce-root"><img class="alignnone122" src="assets/891ea05d-fa4d-468d-b4ce-dd340ae12b13.png"/></p>
<p class="calibre2">Let's start by creating the shared convolutional network (boxed in the preceding diagram) in Keras. By now, you should be familiar with the <strong class="calibre4">Conv</strong> layer, <strong class="calibre4">Pooling</strong> layer, and <strong class="calibre4">Dense</strong> layer. If you need a refresher, feel free to refer to <a href="48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml" target="_blank" class="calibre10">Chapter 4</a>, <em class="calibre8"><span class="calibre5">Cats Versus Dogs – Image Classification</span></em><em class="calibre8"> Using CNNs,</em> for their definitions. </p>
<p class="calibre2">Let's define a function that builds this shared convolutional network using the <kbd class="calibre13">Sequential</kbd> class in Keras:</p>
<pre class="calibre17">from keras.models import Sequential, Input<br class="title-page-name"/>from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense<br class="title-page-name"/><br class="title-page-name"/>def create_shared_network(input_shape):<br class="title-page-name"/>    model = Sequential()<br class="title-page-name"/>    model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', <br class="title-page-name"/>                     input_shape=input_shape))<br class="title-page-name"/>    model.add(MaxPooling2D())<br class="title-page-name"/>    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))<br class="title-page-name"/>    model.add(Flatten())<br class="title-page-name"/>    model.add(Dense(units=128, activation='sigmoid'))<br class="title-page-name"/>    return model</pre>
<p class="calibre2">We can see that this function creates a convolutional network according to the architecture in the preceding diagram. At this point, you might be wondering, <em class="calibre8">how do we actually share weights across two twin networks in Keras?</em> Well, the short answer is that we don't actually need to create two different networks. We only need a single instance of the shared network to be declared in Keras. We can create the top and bottom convolutional network using this single instance. Because we are reusing this single instance, Keras will automatically understand that the weights are to be shared.</p>
<p class="calibre2">This is how we can do it. First, let's create a single instance of the shared network, using the function that we defined previously:</p>
<pre class="calibre17">input_shape = X_train.shape[1:]<br class="title-page-name"/>shared_network = create_shared_network(input_shape)</pre>
<p class="calibre2">We specify the input for the top and bottom layers using the <kbd class="calibre13">Input</kbd> class:</p>
<pre class="calibre17">input_top = Input(shape=input_shape)<br class="title-page-name"/>input_bottom = Input(shape=input_shape)</pre>
<p class="calibre2">Next, we stack the shared network to the right of the input layers, using the <kbd class="calibre13">functional</kbd> method in Keras. The syntax to do this is as follows:</p>
<pre class="calibre17">output_top = shared_network(input_top)<br class="title-page-name"/>output_bottom = shared_network(input_bottom)</pre>
<p class="calibre2">Now, this syntax may not be familiar to you, because we have been using the more user-friendly <kbd class="calibre13">Sequential</kbd> method for building models so far. Although it is simpler, it tends to lose a bit of flexibility, and there are certain things that we cannot do using the <kbd class="calibre13">Sequential</kbd> method alone, including building such a network, as shown. Therefore, we use the <kbd class="calibre13">functional</kbd> method for building such a model.</p>
<p class="calibre2">At this point, this is what our model looks like:</p>
<p class="mce-root"><img class="alignnone123" src="assets/bdf6cbf4-30c2-4627-b039-9c1d59da7fed.png"/></p>
<p class="calibre2">Great! All that's left is to combine the output from the top and bottom, and to measure the Euclidean distance between the two outputs. Remember, the outputs from the top and bottom at this point are 128 x 1-dimensional vectors, representing the lower-dimensional feature space.</p>
<p class="calibre2">Since there is no layer in Keras that can readily compute the Euclidean distance between two arrays, we would have to define our own layer. The <kbd class="calibre13">Lambda</kbd> layer in Keras allows us to do exactly that by wrapping an arbitrary function as a <kbd class="calibre13">Layer</kbd> object. </p>
<p class="calibre2">Let's create a <kbd class="calibre13">euclidean_distance</kbd> function to compute the Euclidean distance between two vectors:</p>
<pre class="calibre17">from keras import backend as K<br class="title-page-name"/>def euclidean_distance(vectors):<br class="title-page-name"/>    vector1, vector2 = vectors<br class="title-page-name"/>    sum_square = K.sum(K.square(vector1 - vector2), axis=1, keepdims=True)<br class="title-page-name"/>    return K.sqrt(K.maximum(sum_square, K.epsilon()))</pre>
<p class="calibre2">We can then wrap this <kbd class="calibre13">euclidean_distance</kbd> function inside a <kbd class="calibre13">Lambda</kbd> layer:</p>
<pre class="calibre17">from keras.layers import Lambda<br class="title-page-name"/>distance = Lambda(euclidean_distance, output_shape=(1,))([output_top, <br class="title-page-name"/>                  output_bottom])</pre>
<p class="calibre2">Finally, we combine the <kbd class="calibre13">distance</kbd> layer defined in the previous line with our inputs to complete our model:</p>
<pre class="calibre17">from keras.models import Model<br class="title-page-name"/>model = Model(inputs=[input_top, input_bottom], outputs=distance)</pre>
<p class="calibre2">We can verify the structure of our model by calling the <kbd class="calibre13">summary()</kbd> function:</p>
<pre class="calibre17">print(model.summary())</pre>
<p class="calibre2">We'll see the following output:</p>
<p class="mce-root"><img class="alignnone124" src="assets/429b03fb-4924-4bdf-be2a-67bf9469cf3a.png"/></p>
<p class="calibre2">If we take a look at the summary in the previous screenshot, we can see that there are two input layers in our model, each of 112 x 92 x 1 in shape (because our images are 112 x 92 x 1). The two input layers are connected to a single shared convolutional network. The two outputs (each a 128-dimensional array) from the shared convolutional network are then combined into a <kbd class="calibre13">Lambda</kbd> layer, which calculates the Euclidean distance of the two 128-dimensional arrays. Finally, this Euclidean distance is output from our model.</p>
<p class="calibre2">That's it! We have successfully created our Siamese neural network. We can see that most of the complexity in the network comes from the shared convolutional network. With this basic framework in place, we can easily tune and increase the complexity of the shared convolutional network as required. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training in Keras</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we have created our Siamese neural network, we can start to train our model. Training a Siamese neural network is slightly different than training a regular CNN. Recall that when training a CNN, the training samples are arrays of images, along with the corresponding class label for each image. In contrast, to train a Siamese neural network we need to use pairs of arrays of images, along with the corresponding class label for the pairs of images (that is, 1 if the pairs of images are from the same subject, and 0 if the pairs of images are from different subjects).</p>
<p class="calibre2">The following diagram illustrates the differences between training a CNN and a Siamese neural network:</p>
<p class="mce-root"><img class="alignnone125" src="assets/f9afedd7-4853-4767-8daa-4016628d46fd.png"/></p>
<p class="calibre2">So far, we have loaded the raw image into an <kbd class="calibre13">X_train</kbd> NumPy array, along with an array with the <kbd class="calibre13"><span>Y_train</span></kbd> class labels. We need to write a function that creates these pairs of arrays of images from <kbd class="calibre13">X_train</kbd> and <kbd class="calibre13">Y_train</kbd>. An important point we need to note is that in the pair of arrays of images, the number of classes should be equal (that is, an equal number of positive and negative pairs, where <em class="calibre8">positive</em> refers to images from the same subject and <em class="calibre8">negative</em> refers to images from different subjects), and that we should alternate between positive and negative pairs. This prevents our model from being biased, and ensures that it learns both positive and negative pairs of images equally well.</p>
<p class="calibre2">The following function creates pairs of arrays of images and their labels from <kbd class="calibre13">X_train</kbd> and <kbd class="calibre13">Y_train</kbd>:</p>
<pre class="calibre17">import random <br class="title-page-name"/>def create_pairs(X,Y, num_classes):<br class="title-page-name"/>    pairs, labels = [], []<br class="title-page-name"/>    # index of images in X and Y for each class<br class="title-page-name"/>    class_idx = [np.where(Y==i)[0] for i in range(num_classes)]<br class="title-page-name"/>    # The minimum number of images across all classes<br class="title-page-name"/>    min_images = min(len(class_idx[i]) for i in range(num_classes)) - 1<br class="title-page-name"/>    for c in range(num_classes):<br class="title-page-name"/>        for n in range(min_images):<br class="title-page-name"/>            # create positive pair<br class="title-page-name"/>            img1 = X[class_idx[c][n]]<br class="title-page-name"/>            img2 = X[class_idx[c][n+1]]<br class="title-page-name"/>            pairs.append((img1, img2))<br class="title-page-name"/>            labels.append(1)<br class="title-page-name"/>            # create negative pair<br class="title-page-name"/>            # list of classes that are different from the current class<br class="title-page-name"/>            neg_list = list(range(num_classes))<br class="title-page-name"/>            neg_list.remove(c)<br class="title-page-name"/>            # select a random class from the negative list. <br class="title-page-name"/>            # This class will be used to form the negative pair.<br class="title-page-name"/>            neg_c = random.sample(neg_list,1)[0]<br class="title-page-name"/>            img1 = X[class_idx[c][n]]<br class="title-page-name"/>            img2 = X[class_idx[neg_c][n]]<br class="title-page-name"/>            pairs.append((img1,img2))<br class="title-page-name"/>            labels.append(0)<br class="title-page-name"/>    return np.array(pairs), np.array(labels)<br class="title-page-name"/><br class="title-page-name"/>num_classes = len(np.unique(Y_train))<br class="title-page-name"/>training_pairs, training_labels = create_pairs(X_train, Y_train,<br class="title-page-name"/>                                              len(np.unique(Y_train)))<br class="title-page-name"/>test_pairs, test_labels = create_pairs(X_test, Y_test,<br class="title-page-name"/>                                       len(np.unique(Y_test)))</pre>
<p class="calibre2">There is one more thing to do before we can start training our model. We need to define a function for the contrastive loss, since contrastive loss is not a default loss function in Keras. </p>
<p class="calibre2">To recap, this is the formula for <em class="calibre8">Contrastive Loss</em>:</p>
<p class="mce-root"><img class="fm-editor-equation29" src="assets/49ca5015-0d84-4574-8003-9bf048113545.png"/></p>
<p class="calibre2">Where <em class="calibre8">Y<sub class="calibre21">true</sub></em> is the true label of the training pairs and <em class="calibre8">D</em> is the predicted distance output from the neural network.</p>
<p class="calibre2">We define the following function for calculating the contrastive loss:</p>
<pre class="calibre17">def contrastive_loss(Y_true, D):<br class="title-page-name"/>    margin = 1<br class="title-page-name"/>    return K.mean(Y_true*K.square(D)+(1 - Y_true)*K.maximum((margin-D),0))</pre>
<p class="calibre2">Notice that the function includes <kbd class="calibre13">K.mean</kbd>, <kbd class="calibre13">K.square</kbd>, and <kbd class="calibre13">K.maximum</kbd>. These are simply Keras's backend functions to simplify array calculations such as the mean, max, and square.</p>
<p class="calibre2">Alright, we have all the necessary functions to train our Siamese neural network. As usual, we define the parameters of the training using the <kbd class="calibre13">compile</kbd> function:</p>
<pre class="calibre17">model.compile(loss=contrastive_loss, optimizer='adam')</pre>
<p class="calibre2">And we train our model for <kbd class="calibre13">10</kbd> epochs by calling the <kbd class="calibre13">fit</kbd> function:</p>
<pre class="calibre17">model.fit([training_pairs[:, 0], training_pairs[:, 1]], training_labels,<br class="title-page-name"/>          batch_size=64, epochs=10)</pre>
<p class="calibre2">Once the training is complete, we'll see the following output:</p>
<p class="mce-root"><img class="alignnone126" src="assets/99ea353c-a20b-4b61-b5d5-cbd018a506cd.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the results</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's apply our model on the withheld testing set to see how well it does. Remember, our model has never seen the images and subjects from the testing set, so this is a good measurement of its real-world performance. </p>
<p class="calibre2">First, we pick two images from the same subject, plot them out side by side, and apply the model to this pair of images:</p>
<pre class="calibre17">idx1, idx2 = 21, 29<br class="title-page-name"/>img1 = np.expand_dims(X_test[idx1], axis=0)<br class="title-page-name"/>img2 = np.expand_dims(X_test[idx2], axis=0)<br class="title-page-name"/><br class="title-page-name"/>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))<br class="title-page-name"/>ax1.imshow(np.squeeze(img1), cmap='gray')<br class="title-page-name"/>ax2.imshow(np.squeeze(img2), cmap='gray')<br class="title-page-name"/><br class="title-page-name"/>for ax in [ax1, ax2]:<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>dissimilarity = model.predict([img1, img2])[0][0]<br class="title-page-name"/>fig.suptitle("Dissimilarity Score = {:.3f}".format(dissimilarity), size=30)<br class="title-page-name"/>plt.tight_layout()<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We'll see the following output:</p>
<p class="mce-root"><img class="alignnone127" src="assets/9aa4fab9-05c7-45de-8a3b-aa8d161f195a.png"/> </p>
<p class="calibre2">Note that the <span class="calibre5">Dissimilarity Score</span> is just the distance output by the model. The greater the distance, the greater the dissimilarity between the two faces. </p>
<p class="calibre2">Our model works well! We can clearly see that the subjects in the photos are the same. In the first image, the subject is wearing glasses, looking into the camera, and smiling. In the second image, the same subject is not wearing glasses, not looking into the camera, and not smiling. Our face recognition model is still able to recognize that the two faces in this pair of photos belong to the same person, as we can see from the low dissimilarity score.</p>
<p class="calibre2">Next, we pick a pair of faces from different subjects and see how well our model performs:</p>
<pre class="calibre17">idx1, idx2 = 1, 39<br class="title-page-name"/>img1 = np.expand_dims(X_test[idx1], axis=0)<br class="title-page-name"/>img2 = np.expand_dims(X_test[idx2], axis=0)<br class="title-page-name"/><br class="title-page-name"/>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))<br class="title-page-name"/>ax1.imshow(np.squeeze(img1), cmap='gray')<br class="title-page-name"/>ax2.imshow(np.squeeze(img2), cmap='gray')<br class="title-page-name"/><br class="title-page-name"/>for ax in [ax1, ax2]:<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>dissimilarity = model.predict([img1, img2])[0][0]<br class="title-page-name"/>fig.suptitle("Dissimilarity Score = {:.3f}".format(dissimilarity), size=30)<br class="title-page-name"/>plt.tight_layout()<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We'll see the following output:</p>
<p class="mce-root"><img class="alignnone128" src="assets/48f696f6-7131-4918-be02-aada15c3b4c8.png"/></p>
<p class="calibre2">Our model performs well for negative pairs (pairs of images where the subjects are different) as well. In this case, the <span class="calibre5">Dissimilarity Score</span> is <span class="calibre5">1.28</span>. We know that positive pairs have a low <span class="calibre5">dissimilarity score and that negative pairs have a high dissimilarity score. But what is the threshold score that separates them? Let's do more tests on positive and negative pairs to find out:</span></p>
<pre class="calibre17">for i in range(5):<br class="title-page-name"/>    for n in range(0,2):<br class="title-page-name"/>        fig, (ax1, ax2) = plt.subplots(1,2, figsize=(7,5))<br class="title-page-name"/>        img1 = np.expand_dims(test_pairs[i*20+n, 0], axis=0)<br class="title-page-name"/>        img2 = np.expand_dims(test_pairs[i*20+n, 1], axis=0)<br class="title-page-name"/>        dissimilarity = model.predict([img1, img2])[0][0]<br class="title-page-name"/>        img1, img2 = np.squeeze(img1), np.squeeze(img2)<br class="title-page-name"/>        ax1.imshow(img1, cmap='gray')<br class="title-page-name"/>        ax2.imshow(img2, cmap='gray')<br class="title-page-name"/><br class="title-page-name"/>        for ax in [ax1, ax2]:<br class="title-page-name"/>            ax.grid(False)<br class="title-page-name"/>            ax.set_xticks([])<br class="title-page-name"/>            ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>        plt.tight_layout()<br class="title-page-name"/>        fig.suptitle("Dissimilarity Score = {:.3f}".format(dissimilarity), <br class="title-page-name"/>                      size=20)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">The following screenshot shows the results for certain pairs of subjects. Note that positive pairs are on the left, while negative pairs are on the right:</p>
<p class="mce-root"><img class="alignnone129" src="assets/d09ea078-ce2e-4a4c-b9fc-43762e329705.png"/></p>
<p class="calibre2"/>
<p class="calibre2">Did you spot anything interesting?  Judging from the preceding results, the threshold score seems to be around 0.5. Anything below 0.5 should be classified as a positive pair (that is, faces match), and anything above 0.5 should be classified as a negative pair. Note that the negative pair on the second-row-to-the-right column is really near the threshold, with a score of 0.501. Interestingly, the two subjects do look alike, with similar glasses and hairstyles!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Consolidating our code</h1>
                </header>
            
            <article>
                
<p class="calibre2">At this point, it would be useful to consolidate our code. We have written a lot of code so far, including helper functions. Let's consolidate the helper functions into a <kbd class="calibre13">utils.py</kbd> file as follows.</p>
<p class="calibre2">First, we import the necessary libraries:</p>
<pre class="calibre17">import numpy as np<br class="title-page-name"/>import random<br class="title-page-name"/>import os<br class="title-page-name"/>import cv2<br class="title-page-name"/>from keras.models import Sequential<br class="title-page-name"/>from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D<br class="title-page-name"/>from keras import backend as K<br class="title-page-name"/>from keras.preprocessing.image import load_img, img_to_array</pre>
<p class="calibre2">We include the <kbd class="calibre13">euclidean_distance</kbd>, <kbd class="calibre13">contrastive_loss</kbd>, and <kbd class="calibre13">accuracy</kbd> functions needed to train a Siamese neural network in the <kbd class="calibre13">utils.py</kbd> file:</p>
<pre class="calibre17">def euclidean_distance(vectors):<br class="title-page-name"/>    vector1, vector2 = vectors<br class="title-page-name"/>    sum_square = K.sum(K.square(vector1 - vector2), axis=1, keepdims=True)<br class="title-page-name"/>    return K.sqrt(K.maximum(sum_square, K.epsilon()))<br class="title-page-name"/><br class="title-page-name"/>def contrastive_loss(Y_true, D):<br class="title-page-name"/>    margin = 1<br class="title-page-name"/>    return K.mean(Y_true*K.square(D)+(1 - Y_true)*K.maximum((margin-D),0))<br class="title-page-name"/><br class="title-page-name"/>def accuracy(y_true, y_pred):<br class="title-page-name"/>    return K.mean(K.equal(y_true, K.cast(y_pred &lt; 0.5, y_true.dtype)))</pre>
<p class="calibre2"/>
<p class="calibre2">We include the <kbd class="calibre13">create_pairs</kbd> function in the <kbd class="calibre13">utils.py</kbd> file. Recall that this helper function is used to generate negative and positive pairs of images for training a Siamese neural network:</p>
<pre class="calibre17">def create_pairs(X,Y, num_classes):<br class="title-page-name"/>    pairs, labels = [], []<br class="title-page-name"/>    # index of images in X and Y for each class<br class="title-page-name"/>    class_idx = [np.where(Y==i)[0] for i in range(num_classes)]<br class="title-page-name"/>    # The minimum number of images across all classes<br class="title-page-name"/>    min_images = min(len(class_idx[i]) for i in range(num_classes)) - 1<br class="title-page-name"/>  <br class="title-page-name"/>    for c in range(num_classes):<br class="title-page-name"/>        for n in range(min_images):<br class="title-page-name"/>            # create positive pair<br class="title-page-name"/>            img1 = X[class_idx[c][n]]<br class="title-page-name"/>            img2 = X[class_idx[c][n+1]]<br class="title-page-name"/>            pairs.append((img1, img2))<br class="title-page-name"/>            labels.append(1)<br class="title-page-name"/>      <br class="title-page-name"/>            # create negative pair<br class="title-page-name"/>            neg_list = list(range(num_classes))<br class="title-page-name"/>            neg_list.remove(c)<br class="title-page-name"/>            # select a random class from the negative list. <br class="title-page-name"/>            # this class will be used to form the negative pair<br class="title-page-name"/>            neg_c = random.sample(neg_list,1)[0]<br class="title-page-name"/>            img1 = X[class_idx[c][n]]<br class="title-page-name"/>            img2 = X[class_idx[neg_c][n]]<br class="title-page-name"/>            pairs.append((img1,img2))<br class="title-page-name"/>            labels.append(0)<br class="title-page-name"/><br class="title-page-name"/>    return np.array(pairs), np.array(labels)</pre>
<p class="calibre2">We also include the <kbd class="calibre13">create_shared_network</kbd> helper function in our <kbd class="calibre13">utils.py</kbd> file, which is used to create a Siamese neural network in Keras:</p>
<pre class="calibre17">def create_shared_network(input_shape):<br class="title-page-name"/>    model = Sequential(name='Shared_Conv_Network')<br class="title-page-name"/>    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', <br class="title-page-name"/>                     input_shape=input_shape))<br class="title-page-name"/>    model.add(MaxPooling2D())<br class="title-page-name"/>    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))<br class="title-page-name"/>    model.add(Flatten())<br class="title-page-name"/>    model.add(Dense(units=128, activation='sigmoid'))<br class="title-page-name"/>    return model</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">The last helper function in our <kbd class="calibre13">utils.py</kbd> file is the <kbd class="calibre13">get_data</kbd> function. This function helps us to load the respective raw images into NumPy arrays:</p>
<pre class="calibre17">def get_data(dir):<br class="title-page-name"/>    X_train, Y_train = [], []<br class="title-page-name"/>    X_test, Y_test = [], []<br class="title-page-name"/>    subfolders = sorted([file.path for file in os.scandir(dir) if <br class="title-page-name"/>                         file.is_dir()])<br class="title-page-name"/>    for idx, folder in enumerate(subfolders):<br class="title-page-name"/>        for file in sorted(os.listdir(folder)):<br class="title-page-name"/>            img = load_img(folder+"/"+file, color_mode='grayscale')<br class="title-page-name"/>            img = img_to_array(img).astype('float32')/255<br class="title-page-name"/>            img = img.reshape(img.shape[0], img.shape[1],1)<br class="title-page-name"/>            if idx &lt; 35:<br class="title-page-name"/>                X_train.append(img)<br class="title-page-name"/>                Y_train.append(idx)<br class="title-page-name"/>            else:<br class="title-page-name"/>                X_test.append(img)<br class="title-page-name"/>                Y_test.append(idx-35)<br class="title-page-name"/><br class="title-page-name"/>    X_train = np.array(X_train)<br class="title-page-name"/>    X_test = np.array(X_test)<br class="title-page-name"/>    Y_train = np.array(Y_train)<br class="title-page-name"/>    Y_test = np.array(Y_test)<br class="title-page-name"/>    return (X_train, Y_train), (X_test, Y_test)</pre>
<p class="calibre2">You can see the <kbd class="calibre13">utils.py</kbd> file in the code we provided.</p>
<p class="calibre2">Similarly, we can create a <kbd class="calibre13">siamese_nn.py</kbd> file. This Python file will hold the main code to create and train our Siamese neural network:</p>
<pre class="calibre17">'''<br class="title-page-name"/>Main code for training a Siamese neural network for face recognition<br class="title-page-name"/>'''<br class="title-page-name"/>import utils<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>from keras.layers import Input, Lambda<br class="title-page-name"/>from keras.models import Model<br class="title-page-name"/><br class="title-page-name"/>faces_dir = 'att_faces/'<br class="title-page-name"/><br class="title-page-name"/># Import Training and Testing Data<br class="title-page-name"/>(X_train, Y_train), (X_test, Y_test) = utils.get_data(faces_dir)<br class="title-page-name"/>num_classes = len(np.unique(Y_train))<br class="title-page-name"/><br class="title-page-name"/># Create Siamese Neural Network<br class="title-page-name"/>input_shape = X_train.shape[1:]<br class="title-page-name"/>shared_network = utils.create_shared_network(input_shape)<br class="title-page-name"/>input_top = Input(shape=input_shape)<br class="title-page-name"/>input_bottom = Input(shape=input_shape)<br class="title-page-name"/>output_top = shared_network(input_top)<br class="title-page-name"/>output_bottom = shared_network(input_bottom)<br class="title-page-name"/>distance = Lambda(utils.euclidean_distance, output_shape=(1,))([output_top, output_bottom])<br class="title-page-name"/>model = Model(inputs=[input_top, input_bottom], outputs=distance)<br class="title-page-name"/><br class="title-page-name"/># Train the model<br class="title-page-name"/>training_pairs, training_labels = utils.create_pairs(X_train, Y_train, <br class="title-page-name"/>                                num_classes=num_classes)<br class="title-page-name"/>model.compile(loss=utils.contrastive_loss, optimizer='adam',<br class="title-page-name"/>              metrics=[utils.accuracy])<br class="title-page-name"/>model.fit([training_pairs[:, 0], training_pairs[:, 1]], training_labels,<br class="title-page-name"/>          batch_size=128,<br class="title-page-name"/>          epochs=10)<br class="title-page-name"/><br class="title-page-name"/># Save the model<br class="title-page-name"/>model.save('siamese_nn.h5')</pre>
<p class="calibre2">This Python file is saved as<span class="calibre5"> </span><kbd class="calibre13">'Chapter07/siamese_nn.py'</kbd><span class="calibre5"> </span>in the code we provided. Notice how the code is a lot shorter than before, as we have refactored our code to call the helper functions in the <kbd class="calibre13">utils.py</kbd>.</p>
<p class="calibre2">Note that the last line in the preceding code saves the trained model at the <kbd class="calibre13"><span>Chapter07/siamese_nn.h5</span></kbd> location. This allows us to easily import the trained model for face recognition, without retraining a model from scratch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a real-time face recognition program</h1>
                </header>
            
            <article>
                
<p class="calibre2">We have finally come to the most important part of the project. We are going to put together the code that we have written so far to create a real-time face recognition program. This program will use the webcam in our computer for facial recognition, and to authenticate whether the person sitting in front of the webcam is indeed you.</p>
<p class="calibre2"/>
<p class="calibre2">To do so, the program needs to do the following:</p>
<ol class="calibre14">
<li class="calibre12">Train a Siamese neural network for facial recognition (this has already been done in the previous section).</li>
<li class="calibre12">Use the webcam to capture a true image of the authorized user. This is the onboarding process of the facial recognition system.</li>
<li class="calibre12">Subsequently, when a user wants to unlock the program, use the pre-trained Siamese neural network from <em class="calibre18">Step 1</em> and the true image from <em class="calibre18">Step 2</em> to authenticate the user.</li>
</ol>
<div class="packttip">This part of the project requires a webcam (either the one in your laptop or an external webcam attached to your computer). If you do not have a webcam in your computer, you may skip this part of the project.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The onboarding process</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's write the code for the onboarding process. During the onboarding process, we need to activate the webcam to capture a true image of the authorized user. OpenCV has a function called <kbd class="calibre13">VideoCapture</kbd> that allows us to activate and capture the image from the computer's webcam:</p>
<pre class="calibre17">import cv2<br class="title-page-name"/>video_capture = cv2.VideoCapture(0)</pre>
<p class="calibre2">Let's give the user five seconds to prepare before taking a photo using the webcam. We start a <kbd class="calibre13">counter</kbd> variable with an initial value of <kbd class="calibre13">5</kbd> and snap a photo using the webcam once the counter reaches <kbd class="calibre13">0</kbd>. Note that we use the code in the <kbd class="calibre13">face_detection.py</kbd> file that we have written earlier in the chapter to detect faces in front of the webcam. The photo will be saved as <kbd class="calibre13">'true_img.png'</kbd> in the same folder as the code:</p>
<pre class="calibre17">import math<br class="title-page-name"/>import utils<br class="title-page-name"/>import face_detection<br class="title-page-name"/><br class="title-page-name"/>counter = 5<br class="title-page-name"/><br class="title-page-name"/>while True:<br class="title-page-name"/>    _, frame = video_capture.read()<br class="title-page-name"/>    frame, face_box, face_coords = face_detection.detect_faces(frame)<br class="title-page-name"/>    text = 'Image will be taken in {}..'.format(math.ceil(counter))<br class="title-page-name"/>    if face_box is not None:<br class="title-page-name"/>        frame = utils.write_on_frame(frame, text, face_coords[0], <br class="title-page-name"/>                                     face_coords[1]-10)<br class="title-page-name"/>    cv2.imshow('Video', frame)<br class="title-page-name"/>    cv2.waitKey(1)<br class="title-page-name"/>    counter -= 0.1<br class="title-page-name"/>    if counter &lt;= 0:<br class="title-page-name"/>        cv2.imwrite('true_img.png', face_box)<br class="title-page-name"/>        break<br class="title-page-name"/><br class="title-page-name"/># When everything is done, release the capture<br class="title-page-name"/>video_capture.release()<br class="title-page-name"/>cv2.destroyAllWindows()<br class="title-page-name"/>print("Onboarding Image Captured")</pre>
<p class="calibre2">The onboarding process looks like this:</p>
<p class="mce-root"><img class="alignnone130" src="assets/c10f4692-85c4-40b6-b97a-4f8b6aa1bfe5.png"/></p>
<p class="calibre2">This code is <span class="calibre5">saved as</span><span class="calibre5"> </span><kbd class="calibre13">Chapter07/onboarding.py</kbd><span class="calibre5"> </span><span class="calibre5">in the files we provided. To run the onboarding process for yourself, simply execute the Python file from a command prompt (in Windows) or a Terminal (macOS/Linux) by calling the following:</span></p>
<pre class="calibre17"><strong class="calibre1">$ python onboarding.py</strong></pre>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Face recognition process</h1>
                </header>
            
            <article>
                
<p class="calibre2">With the onboarding process complete, we can now move on to the actual face recognition process. We start by asking the user for their name. The name will be displayed above the detected face, as we shall see later. The <kbd class="calibre13">input</kbd> function in Python allows the user to enter their name:</p>
<pre class="calibre17">name = input("What is your name?")</pre>
<p class="calibre2">The user will then enter a name on the command line when prompted.</p>
<p class="calibre2">Next, let's import our pre-trained Siamese neural network from earlier in the chapter:</p>
<pre class="calibre17">from keras.models import load_model<br class="title-page-name"/>model = load_model('siamese_nn.h5', <br class="title-page-name"/>                    custom_objects={'contrastive_loss': <br class="title-page-name"/>                    utils.contrastive_loss, <br class="title-page-name"/>                    'euclidean_distance':utils.euclidean_distance})</pre>
<p class="calibre2">Next, we load the true image of the user captured during the onboarding process and preprocess it by normalizing, resizing, and reshaping the image for our Siamese neural network:</p>
<pre class="calibre17">true_img = cv2.imread('true_img.png', 0)<br class="title-page-name"/>true_img = true_img.astype('float32')/255<br class="title-page-name"/>true_img = cv2.resize(true_img, (92, 112))<br class="title-page-name"/>true_img = true_img.reshape(1, true_img.shape[0], true_img.shape[1], 1)</pre>
<p class="calibre2">The rest of the code uses the <kbd class="calibre13">VideoCapture</kbd> function in OpenCV to capture a video from the user's webcam, and passes each frame from the video to our <kbd class="calibre13">face_detection</kbd> instance. We use a fixed-length list (implemented by Python's <kbd class="calibre13">collections.deque</kbd> class) of 15 to collect the 15 most recent predictions (one prediction per frame). We average the scores from the 15 most recent predictions, and we authenticate the user if the average similarity scores is over a certain threshold. The rest of the code is shown as follows:</p>
<pre class="calibre17">video_capture = cv2.VideoCapture(0)<br class="title-page-name"/>preds = collections.deque(maxlen=15)<br class="title-page-name"/><br class="title-page-name"/>while True:<br class="title-page-name"/>    # Capture frame-by-frame<br class="title-page-name"/>    _, frame = video_capture.read()<br class="title-page-name"/><br class="title-page-name"/>    # Detect Faces<br class="title-page-name"/>    frame, face_img, face_coords = face_detection.detect_faces(frame, <br class="title-page-name"/>                                 draw_box=False)<br class="title-page-name"/><br class="title-page-name"/>    if face_img is not None:<br class="title-page-name"/>        face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)<br class="title-page-name"/>        face_img = face_img.astype('float32')/255<br class="title-page-name"/>        face_img = cv2.resize(face_img, (92, 112))<br class="title-page-name"/>        face_img = face_img.reshape(1, face_img.shape[0], <br class="title-page-name"/>                   face_img.shape[1], 1)<br class="title-page-name"/>        preds.append(1-model.predict([true_img, face_img])[0][0])<br class="title-page-name"/>        x,y,w,h = face_coords<br class="title-page-name"/>        if len(preds) == 15 and sum(preds)/15 &gt;= 0.3:<br class="title-page-name"/>            text = "Identity: {}".format(name)<br class="title-page-name"/>            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 5) <br class="title-page-name"/>        elif len(preds) &lt; 15:<br class="title-page-name"/>            text = "Identifying ..."<br class="title-page-name"/>            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 165, 255), 5) <br class="title-page-name"/>        else:<br class="title-page-name"/>            text = "Identity Unknown!"<br class="title-page-name"/>            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255), 5)<br class="title-page-name"/>        frame = utils.write_on_frame(frame, text, face_coords[0], <br class="title-page-name"/>                                     face_coords[1]-10)<br class="title-page-name"/>    else:<br class="title-page-name"/>        # clear existing predictions if no face detected <br class="title-page-name"/>        preds = collections.deque(maxlen=15) <br class="title-page-name"/><br class="title-page-name"/>    # Display the resulting frame<br class="title-page-name"/>    cv2.imshow('Video', frame)<br class="title-page-name"/><br class="title-page-name"/>    if cv2.waitKey(1) &amp; 0xFF == ord('q'):<br class="title-page-name"/>        break<br class="title-page-name"/><br class="title-page-name"/># When everything is done, release the capture<br class="title-page-name"/>video_capture.release()<br class="title-page-name"/>cv2.destroyAllWindows()</pre>
<p class="calibre2">This code is <span class="calibre5">saved as</span><span class="calibre5"> </span><kbd class="calibre13">'Chapter07/face_recognition_system.py'</kbd><span class="calibre5"> </span><span class="calibre5">in the files we provided. To run the program for yourself, simply execute the Python file from a command prompt (in Windows) or a Terminal (macOS/Linux) by calling the following:</span></p>
<pre class="calibre17"><strong class="calibre1">$ python face_recognition_system.py</strong></pre>
<p class="calibre2">Make sure that you run the onboarding program first (to capture a true image) before running the face recognition program.</p>
<p class="calibre2">This is what it looks like when the program is trying to identify your face initially:</p>
<p class="mce-root"><img class="alignnone131" src="assets/8b346220-2425-4bf5-8eff-8466625b0a7e.png"/></p>
<p class="calibre2">After a few seconds, the program should recognize you:</p>
<p class="mce-root"><img class="alignnone132" src="assets/1d60c868-b71e-417f-b3e0-cff18f1a667e.png"/></p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Future work</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we saw, our face recognition system certainly works well under simple conditions. However, it is definitely not fool-proof, and certainly not secure enough to be implemented in important applications. For one, the face detection system can be fooled by a static photo (try it yourself!). Theoretically, that means we can bypass the authentication by placing a static photo of an authorized user in front of the webcam. </p>
<p class="calibre2">Techniques to solve this problem are known as <strong class="calibre4">anti-spoofing techniques</strong>. Anti-spoofing techniques are a keenly studied area in face recognition. In general, there are two main anti-spoofing techniques used today:</p>
<ul class="calibre11">
<li class="calibre12"><strong class="calibre1">Liveness detection</strong>: Since a photo is a static two-dimensional image and a real face is dynamic and three-dimensional, we can check for the <em class="calibre18">liveness</em> of the detected face. Ways to perform liveness detection include checking the optic flow of the detected face, and checking the lighting and texture of the detected face in contrast to the surroundings.</li>
<li class="calibre12"><strong class="calibre1">Machine learning</strong>: We can also differentiate a real face from an image by using machine learning! We can train a CNN to classify whether the detected face belongs to a real face or a static image. However, you would need plenty of labeled data (face versus non-face) to accomplish this.</li>
</ul>
<p class="calibre2">Here's a video from Andrew Ng, showing how face recognition (with liveness detection) is implemented in Baidu's headquarters in China:</p>
<p class="calibre2"><a href="https://www.youtube.com/watch?v=wr4rx0Spihs" target="_blank" class="calibre10">https://www.youtube.com/watch?v=wr4rx0Spihs</a></p>
<p class="calibre2">If you would like to understand how Apple implements its face ID system in iPhones, you can refer to the paper at <a href="https://www.apple.com/business/site/docs/FaceID_Security_Guide.pdf" target="_blank" class="calibre10">https://www.apple.com/business/site/docs/FaceID_Security_Guide.pdf</a> published by Apple. </p>
<p class="calibre2">Apple's implementation of face ID is more secure than the system that we used in this chapter. Apple uses a TrueDepth camera to project infrared dots on your face, creating a depth map, which is then used for facial recognition.</p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we created a face recognition system based on a Siamese neural network. The face recognition system uses a webcam to stream frames from a live video to a pre-trained Siamese neural network, and using a true image of the user, the system is able to authenticate the user in front of the webcam. </p>
<p class="calibre2">We first dissected the face recognition problem into smaller subproblems, and we saw how a face recognition system first performs a face detection step to isolate the face from the rest of the image, before the actual face recognition step. We saw how face detection is commonly done by the Viola-Jones algorithm, which uses Haar features to detect faces in real time. Face detection using Haar filters is implemented in Python via the OpenCV library, which allows us to perform face detection in just a few lines of code.</p>
<p class="calibre2">We then focused on face recognition, and we discussed how the requirements of face recognition systems (speed, scalability, high accuracy with small data) makes CNNs unsuitable for this problem. We introduced the architecture of Siamese neural networks, and how distance-based predictions in Siamese neural networks can be used for face recognition. We trained a Siamese neural network from scratch in Keras, using the AT&amp;T faces dataset.</p>
<p class="calibre2">Lastly, using the pre-trained Siamese neural network, we created our own face recognition system in Python. The face recognition system consists of two steps. In the first step (the onboarding process), we used OpenCV's face detection API to capture an image of the user using a webcam, as the true image for the Siamese neural network. In the second step, the system uses the true image to recognize and authenticate users of the program.</p>
<p class="calibre2">In the next and final chapter, <a href="cf13b5e9-5a3d-4cd7-ba65-aeee25e0e6bb.xhtml" target="_blank" class="calibre10">Chapter 8</a>, <em class="calibre8">What's Next?</em>,<em class="calibre8"> </em>we'll consolidate and recap the different projects that we've completed so far in this book. We'll also peer into the future, and see what neural networks and AI will look like in the next few years.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol class="calibre14">
<li class="calibre12">How is face detection different than face recognition?</li>
</ol>
<p class="calibre26">The objective of face detection is to locate human faces in an image. The output from the face detection process is a bounding box around detected faces. On the other hand, the objective of face recognition is to classify faces (that is, identify subjects). Face detection and face recognition are the two key steps in every facial recognition system, and the output from the face detection step is passed as input to the face recognition step.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<ol start="2" class="calibre14">
<li class="calibre12">What is the Viola-Jones algorithm for face detection?</li>
</ol>
<p class="calibre26">The Viola-Jones algorithm uses Haar features for face detection. Haar features are filters with alternating dark and bright areas that represents the contrast in pixel intensity in human faces. For example, the eye area in an image of a human face has a darker pixel value than the forehead and the cheeks areas. These Haar filters are used to localize areas in an image that may contain faces.</p>
<ol start="3" class="calibre14">
<li class="calibre12"> What is one-shot learning, and how is it different than batch learning?</li>
</ol>
<p class="calibre26">In one-shot learning, the objective is to train a machine learning model with very little data. In contrast, batch learning uses a big dataset to train a machine learning model. One-shot learning is often used in image recognition tasks, as the quantity of training samples can be very sparse.</p>
<ol start="4" class="calibre14">
<li class="calibre12">Describe the architecture of a Siamese neural network.</li>
</ol>
<p class="calibre26">Siamese neural networks consist of two conjoined convolutional layers with shared weights, accepting a pair of input images. The conjoined convolutional layers project the two input images to a lower-dimension feature space. Using a Euclidean distance layer, we compute and output the distance of the two lower-dimension vectors, which is inversely proportional to the similarity of the two images.</p>
<ol start="5" class="calibre14">
<li class="calibre12">When training a Siamese neural network for face recognition, what is the loss function used?</li>
</ol>
<p class="calibre26">We use a contrastive loss function to train a Siamese neural network for face recognition. The contrastive loss function encourages a neural network to output a small distance when the pair of input images are similar, and vice versa, it encourages a large output distance when the pair of input images are different. </p>


            </article>

            
        </section>
    </body></html>