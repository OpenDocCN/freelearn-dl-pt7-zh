<html><head></head><body>
		<div>
			<div id="_idContainer046" class="Content">
			</div>
		</div>
		<div id="_idContainer047" class="Content">
			<h1 id="_idParaDest-65">2. <a id="_idTextAnchor066"/>An Introduction to Regression</h1>
		</div>
		<div id="_idContainer110" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, you will be introduced to regression. Regression comes in handy when you are trying to predict future variables using historical data. You will learn various regression techniques such as linear regression with single and multiple variables, along with polynomial and Support Vector Regression (SVR). You will use these techniques to predict future stock prices from a stock price data. By the end of this chapter, you will be comfortable using regression techniques to solve practical problems in a variety of fields.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor067"/>Introduction</h1>
			<p>In the previous chapter, you were introduced to the fundamentals of <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>), which helped you create the game Tic-Tac-Toe. In this chapter, we will be looking at regression, which is a machine learning algorithm that can be used to measure how closely related independent variable(s), called <strong class="bold">features</strong>, relate to a dependent variable called a <strong class="bold">label</strong>.</p>
			<p>Linear regression is a concept with many applications a variety of fields, ranging from finance (predicting the price of an asset) to business (predicting the sales of a product) and even the economy (predicting economy growth).</p>
			<p>Most of this chapter will deal with different forms of linear regression, including linear regression with one variable, linear regression with multiple variables, polynomial regression with one variable, and polynomial regression with multiple variables. Python provides lots of forms of support for performing regression operations and we will also be looking at these later on in this chapter.</p>
			<p>We will also use an alternative regression model, called <strong class="bold">Support Vector Regression</strong> (<strong class="bold">SVR</strong>), with different forms of linear regression. Throughout this chapter, we will be using a few sample datasets along with the stock price data loaded from the <strong class="bold">Quandl</strong> Python library to predict future prices using different types of regression.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Although it is not recommended that you use the models in this chapter to provide trading or investment advice, this is a very exciting and interesting journey that explains the fundamentals of regression.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor068"/>Linear Regression with One Variable</h1>
			<p>A general regression problem can be defined with the following example. Suppose we have a set of data points and we need to figure out the best fit curve to approximately fit the given data points. This curve will describe the relationship between our input variable, <strong class="source-inline">x</strong>, which is the data point, and the output variable, <strong class="source-inline">y</strong>, which is the curve. </p>
			<p>Remember, in real life, we often have more than one input variable determining the output variable. However, linear regression with one variable will help us to understand how the input variable impacts the output variable.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor069"/>Types of Regression</h2>
			<p>In this chapter, we will work with regression on the two-dimensional plane. This means that our data points are two-dimensional, and we are looking for a curve to approximate how to calculate one variable from another.</p>
			<p>We will come across the following types of regression in this chapter:</p>
			<ul>
				<li><strong class="bold">Linear regression with one variable using a polynomial of degree 1</strong>: This is the most basic form of regression, where a straight line approximates the trajectory of future data.</li>
				<li><strong class="bold">Linear regression with multiple variables using a polynomial of degree 1</strong>: We will be using equations of degree 1, but we will also allow multiple input variables, called features.</li>
				<li><strong class="bold">Polynomial regression with one variable</strong>: This is a generic form of the linear regression of one variable. As the polynomial used to approximate the relationship between the input and the output is of an arbitrary degree, we can create curves that fit the data points better than a straight line. The regression is still linear – not because the polynomial is linear, but because the regression problem can be modeled using linear algebra.</li>
				<li><strong class="bold">Polynomial regression with multiple variables</strong>: This is the most generic regression problem, using higher degree polynomials and multiple features to predict the future.</li>
				<li><strong class="bold">SVR</strong>: This form of regression uses <strong class="bold">Support Vector Machines</strong> (<strong class="bold">SVMs</strong>) to predict data points. This type of regression is included to explain SVR's usage compared to the other four regression types.</li>
			</ul>
			<p>Now we will deal with the first type of linear regression: we will use one variable, and the polynomial of the regression will describe a straight line.</p>
			<p>On the two-dimensional plane, we will use the Déscartes coordinate system, more commonly known as the <em class="italic">Cartesian coordinate system</em>. We have an <em class="italic">x</em> and a <em class="italic">y</em>-axis, and the intersection of these two axes is the origin. We denote points by their <em class="italic">x</em> and <em class="italic">y</em> coordinates.</p>
			<p>For instance, point <em class="italic">(2, 1)</em> corresponds to the black point on the following coordinate system:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B16060_02_01.jpg" alt="Figure 2.1: Representation of point (2,1) on the coordinate system&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1: Representation of point (2,1) on the coordinate system</p>
			<p>A straight line can be described with the equation <em class="italic">y = a*x + b</em>, where <em class="italic">a</em> is the slope of the equation, determining how steeply the equation climbs up, and <em class="italic">b</em> is a constant determining where the line intersects the <em class="italic">y</em>-axis.</p>
			<p>In <em class="italic">Figure 2.2</em>, you can see three equations:</p>
			<ul>
				<li>The straight line is described with the equation <em class="italic">y = 2*x + 1</em>.</li>
				<li>The dashed line is described with the equation <em class="italic">y = x + 1</em>.</li>
				<li>The dotted line is described with the equation <em class="italic">y = 0.5*x + 1</em>.</li>
			</ul>
			<p>You can see that all three equations intersect the <em class="italic">y</em>-axis at <em class="italic">1</em>, and their slope is determined by the factor by which we multiply <em class="italic">x</em>.</p>
			<p>If you know <em class="italic">x</em>, you can solve <em class="italic">y</em>. Similarly, if you know <em class="italic">y</em>, you can solve <em class="italic">x</em>. This equation is a polynomial equation of degree <em class="italic">1</em>, which is the base of linear regression with one variable:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B16060_02_02.jpg" alt="Figure 2.2: Representation of the equations y = 2*x + 1, y = x + 1, and y = 0.5*x + 1 on the coordinate system&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2: Representation of the equations y = 2*x + 1, y = x + 1, and y = 0.5*x + 1 on the coordinate system</p>
			<p>We can describe curves instead of straight lines using polynomial equations; for example, the polynomial equation <em class="italic">4x</em><span class="superscript">4</span><em class="italic">-3x</em><span class="superscript">3</span><em class="italic">-x</em><span class="superscript">2</span><em class="italic">-3x+3</em> will result in <em class="italic">Figure 2.3</em>. This type of equation is the base of polynomial regression with one variable:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B16060_02_03.jpg" alt="Figure 2.3: Representation of the polynomial equation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3: Representation of the polynomial equation</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you would like to experiment further with the Cartesian coordinate system, you can use the following plotter: <a href="https://s3-us-west-2.amazonaws.com/oerfiles/College+Algebra/calculator.html">https://s3-us-west-2.amazonaws.com/oerfiles/College+Algebra/calculator.html</a>.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor070"/>Features and Labels</h2>
			<p>In machine learning, we differentiate between features and labels. Features are considered our <strong class="bold">input</strong> variables, and labels are our <strong class="bold">output</strong> variables.</p>
			<p>When talking about regression, the possible value of the labels is a continuous set of rational numbers. Think of features as the values on the <em class="italic">x</em>-axis and labels as the values on the <em class="italic">y</em>-axis.</p>
			<p>The task of regression is to predict label values based on feature values. </p>
			<p>We often create a label by projecting the values of a feature in the future. </p>
			<p>For instance, if we would like to predict the price of a stock for next month using historical monthly data, we would create the label by shifting the stock price feature one month into the future:</p>
			<ul>
				<li>For each stock price feature, the label would be the stock price feature of the next month.</li>
				<li>For the last month, prediction data would not be available, so these values are all <strong class="source-inline">NaN</strong> (Not a Number).</li>
			</ul>
			<p>Let's say we have data for the months of <strong class="source-inline">January</strong>, <strong class="source-inline">February</strong>, and <strong class="source-inline">March</strong>, and we want to predict the price for <strong class="source-inline">April</strong>. Our feature for each month will be the current monthly price and the label will be the price of the next month.</p>
			<p>For instance, take a look at the following table:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B16060_02_04.jpg" alt="Figure 2.4: Example of a feature and a label&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4: Example of a feature and a label</p>
			<p>This means that the label for <strong class="source-inline">January</strong> is the price of <strong class="source-inline">February</strong> and that the label for <strong class="source-inline">February</strong> is actually the price of <strong class="source-inline">March</strong>. The label for <strong class="source-inline">March</strong> is unknown (<strong class="source-inline">NaN</strong>) as this is the value we are trying to predict.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor071"/>Feature Scaling</h2>
			<p>At times, we have multiple features (inputs) that may have values within completely different ranges. Imagine comparing micrometers on a map to kilometers in the real world. They won't be easy to handle because of the difference in magnitude of nine zeros.</p>
			<p>A less dramatic difference is the difference between imperial and metric data. For instance, pounds and kilograms, and centimeters and inches, do not compare that well.</p>
			<p>Therefore, we often scale our features to normalized values that are easier to handle, as we can compare the values of these ranges more easily.</p>
			<p>We will demonstrate two types of scaling:</p>
			<ul>
				<li>Min-max normalization</li>
				<li>Mean normalization</li>
			</ul>
			<p>Min-max normalization is calculated as follows:</p>
			<p><img src="image/B16060_02_4a.png" alt="1"/></p>
			<p>Here, <em class="italic">X</em><span class="subscript">MIN</span> is the minimum value of the feature and <em class="italic">X</em><span class="subscript">MAX </span>is<span class="subscript"> </span>the maximum value. </p>
			<p>The feature-scaled values will be within the range of <strong class="source-inline">[0;1]</strong>.</p>
			<p>Mean normalization is calculated as follows:</p>
			<p><img src="image/B16060_02_4b.png" alt="2"/></p>
			<p>Here, <strong class="source-inline">AVG</strong> is the average.</p>
			<p>The feature-scaled values will be within the range of <strong class="source-inline">[-1;1]</strong>.</p>
			<p>Here's an example of both normalizations applied on the first 13 numbers of the Fibonacci sequence.</p>
			<p>We begin with finding the min-max normalization:</p>
			<p class="source-code">fibonacci = [0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144]</p>
			<p class="source-code"># Min-Max normalization:</p>
			<p class="source-code">[(float(i)-min(fibonacci))/(max(fibonacci)-min(fibonacci)) \</p>
			<p class="source-code">for i in fibonacci]</p>
			<p>The expected output is this:</p>
			<p class="source-code">[0.0,</p>
			<p class="source-code"> 0.006944444444444444,</p>
			<p class="source-code"> 0.006944444444444444,</p>
			<p class="source-code"> 0.013888888888888888,</p>
			<p class="source-code"> 0.020833333333333332,</p>
			<p class="source-code"> 0.034722222222222224,</p>
			<p class="source-code"> 0.05555555555555555,</p>
			<p class="source-code"> 0.09027777777777778,</p>
			<p class="source-code"> 0.14583333333333334,</p>
			<p class="source-code"> 0.2361111111111111,</p>
			<p class="source-code"> 0.3819444444444444,</p>
			<p class="source-code"> 0.6180555555555556,</p>
			<p class="source-code"> 1.0]</p>
			<p>Now, take a look at the following code snippet to find the mean normalization:</p>
			<p class="source-code"># Mean normalization:</p>
			<p class="source-code">avg = sum(fibonacci) / len(fibonacci)</p>
			<p class="source-code"># 28.923076923076923</p>
			<p class="source-code">[(float(i)-avg)/(max(fibonacci)-min(fibonacci)) \</p>
			<p class="source-code">for i in fibonacci]</p>
			<p>The expected output is this:</p>
			<p class="source-code">[-0.20085470085470086,</p>
			<p class="source-code"> -0.19391025641025642,</p>
			<p class="source-code"> -0.19391025641025642,</p>
			<p class="source-code"> -0.18696581196581197,</p>
			<p class="source-code"> -0.18002136752136752,</p>
			<p class="source-code"> -0.16613247863247863,</p>
			<p class="source-code"> -0.1452991452991453,</p>
			<p class="source-code"> -0.11057692307692307,</p>
			<p class="source-code"> -0.05502136752136752,</p>
			<p class="source-code"> 0.035256410256410256,</p>
			<p class="source-code"> 0.18108974358974358,</p>
			<p class="source-code"> 0.4172008547008547,</p>
			<p class="source-code"> 0.7991452991452992]</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Scaling could add to the processing time, but, often, it is an important step to add.</p>
			<p>In the scikit-learn library, we have access to the <strong class="source-inline">preprocessing.scale</strong> function, which scales NumPy arrays:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from sklearn import preprocessing</p>
			<p class="source-code">preprocessing.scale(fibonacci)</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([-0.6925069 , -0.66856384, -0.66856384, -0.64462079,</p>
			<p class="source-code">       -0.62067773-0.57279161, -0.50096244, -0.38124715,</p>
			<p class="source-code">       -0.18970269,  0.12155706, 0.62436127,  1.43842524,</p>
			<p class="source-code">       2.75529341]</p>
			<p>The <strong class="source-inline">scale</strong> method performs a standardization, which is another type of normalization. Notice that the result is a NumPy array.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor072"/>Splitting Data into Training and Testing</h2>
			<p>Now that we have learned how to normalize our dataset, we need to learn about the training-testing split. In order to measure how well our model can generalize its predictive performance, we need to split our dataset into a training set and a testing set. The training set is used by the model to learn from so that it can build predictions. Then, the model will use the testing set to evaluate the performance of its prediction. </p>
			<p>When we split the dataset, we first need to shuffle it to ensure that our testing set will be a generic representation of our dataset. The split is usually 90% for the training set and 10% for the testing set.</p>
			<p>With training and testing, we can measure whether our model is overfitting or underfitting.</p>
			<p><strong class="bold">Overfitting</strong> occurs when the trained model fits the training dataset too well. The model will be very accurate on the training data, but it will not be usable in real life, as its accuracy will decrease when used on any other data. The model adjusts to the random noise in the training data and assumes patterns on this noise that yield false predictions. </p>
			<p><strong class="bold">Underfitting</strong> occurs when the trained model does not fit the training data well enough to recognize important patterns in the data. As a result, it cannot make accurate predictions on new data. One example of this is when we attempt to do linear regression on a dataset that is not linear. For example, the Fibonacci sequence is not linear; therefore, a model on a Fibonacci-like sequence cannot be linear either.</p>
			<p>We can do the training-testing split using the <strong class="source-inline">model_selection</strong> library of scikit- learn.</p>
			<p>Suppose, in our example, that we have scaled the Fibonacci data and defined its indices as labels:</p>
			<p class="source-code">features = preprocessing.scale(fibonacci)</p>
			<p class="source-code">label = np.array(range(13))</p>
			<p>Now, let's use 10% of the data as test data, <strong class="source-inline">test_size=0.1</strong>, and specify <strong class="source-inline">random_state</strong> parameter in order to get the exact same split every time we run the code:</p>
			<p class="source-code">from sklearn import model_selection</p>
			<p class="source-code">(x_train, x_test, y_train, y_test) = \</p>
			<p class="source-code">model_selection.train_test_split(features, \</p>
			<p class="source-code">                                 label, test_size=0.1, \</p>
			<p class="source-code">                                 random_state=8)</p>
			<p>Our dataset has been split into test and training sets for our features (<strong class="source-inline">x_train</strong> and <strong class="source-inline">x_test</strong>) and for our labels (<strong class="source-inline">y_train</strong> and <strong class="source-inline">y_test</strong>).</p>
			<p>Finally, let's check each set, beginning with the <strong class="source-inline">x_train</strong> feature:</p>
			<p class="source-code">x_train</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([ 1.43842524, -0.18970269, -0.50096244,  2.75529341,</p>
			<p class="source-code">       -0.6925069 , -0.66856384, -0.57279161,  0.12155706,</p>
			<p class="source-code">       -0.66856384, -0.62067773, -0.64462079])</p>
			<p>Next, we check for <strong class="source-inline">x_test</strong>:</p>
			<p class="source-code">x_test</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([-0.38124715, 0.62436127])</p>
			<p>Then, we check for <strong class="source-inline">y_train</strong>:</p>
			<p class="source-code">y_train</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([11, 8, 6, 12, 0, 2, 5, 9, 1, 4, 3])</p>
			<p>Next, we check for <strong class="source-inline">y_test</strong>:</p>
			<p class="source-code">y_test</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([7, 10])</p>
			<p>In the preceding output, we can see that our split has been properly executed; for instance, our label has been split into <strong class="source-inline">y_test</strong>, which contains the <strong class="source-inline">7</strong> and <strong class="source-inline">10</strong> indexes, and <strong class="source-inline">y_train</strong> which contains the remaining <strong class="source-inline">11</strong> indexes. The same logic has been applied to our features and we have <strong class="source-inline">2</strong> values in <strong class="source-inline">x_test</strong> and <strong class="source-inline">11</strong> values in <strong class="source-inline">x_train</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you remember the Cartesian coordinate system, you know that the horizontal axis is the <em class="italic">x</em>-axis and that the vertical axis is the <em class="italic">y</em>-axis. Our features are on the <em class="italic">x</em>-axis, while our labels are on the <em class="italic">y</em>-axis. Therefore, we use features and <em class="italic">x</em> as synonyms, while labels are often denoted by <em class="italic">y</em>. Therefore, <strong class="source-inline">x_test</strong> denotes feature test data, <strong class="source-inline">x_train</strong> denotes feature training data, <strong class="source-inline">y_test</strong> denotes label test data, and <strong class="source-inline">y_train</strong> denotes label training data.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor073"/>Fitting a Model on Data with scikit-learn </h2>
			<p>We are now going to illustrate the process of regression on an example where we only have one feature and minimal data.</p>
			<p>As we only have one feature, we have to format <strong class="source-inline">x_train</strong> by reshaping it with <strong class="source-inline">x_train.reshape (-1,1)</strong> to a NumPy array containing one feature.</p>
			<p>Therefore, before executing the code on fitting the best line, execute the following code:</p>
			<p class="source-code">x_train = x_train.reshape(-1, 1)</p>
			<p class="source-code">x_test = x_test.reshape(-1, 1)</p>
			<p>We can fit a linear regression model on our data with the following code:</p>
			<p class="source-code">from sklearn import linear_model</p>
			<p class="source-code">linear_regression = linear_model.LinearRegression()</p>
			<p class="source-code">model = linear_regression.fit(x_train, y_train)</p>
			<p class="source-code">model.predict(x_test)</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([4.46396931, 7.49212796])</p>
			<p>We can also calculate the score associated with the model:</p>
			<p class="source-code">model.score(x_test, y_test)</p>
			<p>The expected output is this:</p>
			<p class="source-code">-1.8268608450379087</p>
			<p>This score represents the accuracy of the model and is defined as the R<span class="superscript">2 </span>or <strong class="bold">coefficient of determination</strong>. It represents how well we can predict the features from the labels.</p>
			<p>In our example, an R<span class="superscript">2</span> of <strong class="source-inline">-1.8268</strong> indicates a very bad model as the best possible score is <strong class="bold">1</strong>. A score of <strong class="bold">0</strong> can be achieved if we constantly predict the labels by using the average value of the features. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">We will omit the mathematical background of this score in this book.</p>
			<p>Our model does not perform well for two reasons:</p>
			<ul>
				<li>If we check our previous Fibonacci sequence, 11 training data points and 2 testing data points are simply not enough to perform a proper predictive analysis.</li>
				<li>Even if we ignore the number of points, the Fibonacci sequence does not describe a linear relationship between <em class="italic">x</em> and <em class="italic">y</em>. Approximating a nonlinear function with a line is only useful if we are looking at two very close data points.</li>
			</ul>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor074"/>Linear Regression Using NumPy Arrays</h2>
			<p>One reason why NumPy arrays are handier than Python lists is that they can be treated as vectors. There are a few operations defined on vectors that can simplify our calculations. We can perform operations on vectors of similar lengths. </p>
			<p>Let's take, for example, two vectors, V<span class="subscript">1</span> and V<span class="subscript">2</span>,<span class="subscript"> </span>with three coordinates each:</p>
			<p>V<span class="subscript">1 </span>= (a, b, c) with a=1, b=2, and c=3</p>
			<p>V<span class="subscript">2 </span>= (d, e, f) with d=2, e=0, and f=2</p>
			<p>The addition of these two vectors will be this:</p>
			<p>V<span class="subscript">1</span> + V<span class="subscript">2</span> = (a+d, b+e, c+f) = (1+2, 2+0, 3+2) = (3,2,5)</p>
			<p>The product of these two vectors will be this:</p>
			<p>V<span class="subscript">1</span> + V<span class="subscript">2</span> = (a*d, b*e, c*f) = (1*2, 2*0, 3*2) = (2,0,6)</p>
			<p>You can think of each vector as our datasets with, for example, the first vector as our <strong class="bold">features set</strong> and the second vector as our <strong class="bold">labels set</strong>. With Python being able to do vector calculations, this will greatly simplify the calculations required for our linear regression models.</p>
			<p>Now, let's build a linear regression using NumPy in the following example.</p>
			<p>Suppose we have two sets of data with 13 data points each; we want to build a linear regression that best fits all the data points for each set.</p>
			<p>Our first set is defined as follows:</p>
			<p class="source-code">[2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62]</p>
			<p>If we plot this dataset with the values (<strong class="source-inline">2,8,8,18,25,21,32,44,32,48,61,45,62</strong>) as the <em class="italic">y</em>-axis, and the index of each value (<strong class="source-inline">1,2,3,4,5,6,7,8,9,10,11,12,13</strong>) as the <em class="italic">x</em>-axis, we will get the following plot:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B16060_02_05.jpg" alt="Figure 2.5: Plotted graph of the first dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5: Plotted graph of the first dataset</p>
			<p>We can see that this dataset's distribution seems linear in nature, and if we wanted to draw a line that was as close as possible to each dot, it wouldn't be too hard. A simple linear regression appears appropriate in this case.</p>
			<p>Our second set is the first 13 values scaled in the Fibonacci sequence that we saw earlier in the <em class="italic">Feature Scaling</em> section:</p>
			<p class="source-code">[-0.6925069, -0.66856384, -0.66856384, -0.64462079, -0.62067773, -0.57279161, -0.50096244, -0.38124715, -0.18970269, 0.12155706, 0.62436127, 1.43842524, 2.75529341]</p>
			<p>If we plot this dataset with the values as the <em class="italic">y</em>-axis and the index of each value as the <em class="italic">x</em>-axis, we will get the following plot:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B16060_02_06.jpg" alt="Figure 2.6: Plotted graph of the second dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6: Plotted graph of the second dataset</p>
			<p>We can see that this dataset's distribution doesn't appear to be linear, and if we wanted to draw a line that was as close as possible to each dot, our line would miss quite a lot of dots. A simple linear regression will probably struggle in this situation.</p>
			<p>We know that the equation of a straight line is <img src="image/B16060_02_6a.png" alt="3"/>.</p>
			<p>In this equation, <img src="image/B16060_02_6b.png" alt="4"/> is the slope, and <img src="image/B16060_02_6c.png" alt="5"/> is the <em class="italic">y</em> intercept. To find the line of best fit, we must find the coefficients of <img src="image/B16060_02_6b.png" alt="6"/> and <img src="image/B16060_02_6c.png" alt="7"/>.</p>
			<p>In order to do this, we will use the least-squares method, which can be achieved by completing the following steps:</p>
			<ol>
				<li>For each data point, calculate <em class="italic">x</em><span class="superscript">2</span> and <em class="italic">xy</em>.<p>Sum all of <em class="italic">x</em>, <em class="italic">y</em>, <em class="italic">x</em><span class="superscript">2</span>, and <em class="italic">x * y</em>, which gives us <img src="image/B16060_02_6f.png" alt="8"/></p></li>
				<li>Calculate the slope, <img src="image/B16060_02_6b.png" alt="9"/>, as <img src="image/B16060_02_6h.png" alt="10"/> with <em class="italic">N</em> as the total number of data points.</li>
				<li>Calculate the <em class="italic">y</em> intercept, <img src="image/B16060_02_6c.png" alt="11"/>, as <img src="image/B16060_02_6j.png" alt="12"/>.</li>
			</ol>
			<p>Now, let's apply these steps using NumPy as an example for the first dataset in the following code.</p>
			<p>Let's take a look at the first step:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">x = np.array(range(1, 14))</p>
			<p class="source-code">y = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
			<p class="source-code">x_2 = x**2</p>
			<p class="source-code">xy = x*y</p>
			<p>For <strong class="source-inline">x_2</strong>, the output will be this:</p>
			<p class="source-code">array([  1,   4,   9,  16,  25,  36,  49,  64,  81, </p>
			<p class="source-code">       100, 121, 144, 169],  dtype=int32)</p>
			<p>For <strong class="source-inline">xy</strong>, the output will be this:</p>
			<p class="source-code">array([2, 16, 24, 72, 125, 126, 224, </p>
			<p class="source-code">       352, 288, 480, 671, 540, 806])</p>
			<p>Now, let's move on to the next step:</p>
			<p class="source-code">sum_x = sum(x)</p>
			<p class="source-code">sum_y = sum(y)</p>
			<p class="source-code">sum_x_2 = sum(x_2)</p>
			<p class="source-code">sum_xy = sum(xy)</p>
			<p>For <strong class="source-inline">sum_x</strong>, the output will be this: </p>
			<p class="source-code">91</p>
			<p>For <strong class="source-inline">sum_y</strong>, the output will be this:</p>
			<p class="source-code">406</p>
			<p>For <strong class="source-inline">sum_x_2</strong>, the output will be this:</p>
			<p class="source-code">819</p>
			<p>For <strong class="source-inline">sum_xy</strong>, the output will be this:</p>
			<p class="source-code">3726</p>
			<p>Now, let's move on to the next step:</p>
			<p class="source-code">N = len(x)</p>
			<p class="source-code">a = (N*sum_xy - (sum_x*sum_y))/(N*sum_x_2-(sum_x)**2)</p>
			<p>For <strong class="source-inline">N</strong>, the output will be this:</p>
			<p class="source-code">13</p>
			<p>For <strong class="source-inline">a</strong>, the output will be this:</p>
			<p class="source-code">4.857142857142857</p>
			<p>Now, let's move on to the final step:</p>
			<p class="source-code">b = (sum_y - a*sum_x)/N</p>
			<p>For <strong class="source-inline">b</strong>, the output will be this:</p>
			<p class="source-code">-2.7692307692307647</p>
			<p>Once we plot the line <img src="image/B16060_02_6l.png" alt="13"/> with the preceding coefficients, we get the following graph:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B16060_02_07.jpg" alt="Figure 2.7: Plotted graph of the linear regression for the first dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7: Plotted graph of the linear regression for the first dataset</p>
			<p>As you can see, our linear regression model works quite well on this dataset, which has a linear distribution.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find a linear regression calculator at <a href="http://www.endmemo.com/statistics/lr.php">http://www.endmemo.com/statistics/lr.php</a>. You can also check the calculator to get an idea of what lines of best fit look like on a given dataset.</p>
			<p>We will now repeat the exact same steps for the second dataset:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">x = np.array(range(1, 14))</p>
			<p class="source-code">y = np.array([-0.6925069, -0.66856384, -0.66856384, \</p>
			<p class="source-code">              -0.64462079, -0.62067773, -0.57279161, \</p>
			<p class="source-code">              -0.50096244, -0.38124715, -0.18970269, \</p>
			<p class="source-code">              0.12155706, 0.62436127, 1.43842524, 2.75529341])</p>
			<p class="source-code">x_2 = x**2</p>
			<p class="source-code">xy = x*y</p>
			<p class="source-code">sum_x = sum(x)</p>
			<p class="source-code">sum_y = sum(y)</p>
			<p class="source-code">sum_x_2 = sum(x_2)</p>
			<p class="source-code">sum_xy = sum(xy)</p>
			<p class="source-code">N = len(x)</p>
			<p class="source-code">a = (N*sum_xy - (sum_x*sum_y))/(N*sum_x_2-(sum_x)**2)</p>
			<p class="source-code">b = (sum_y - a*sum_x)/N</p>
			<p>For <strong class="source-inline">a</strong>, the output will be this:</p>
			<p class="source-code">0.21838173510989017</p>
			<p>For <strong class="source-inline">b</strong>, the output will be this:</p>
			<p class="source-code">-1.528672146538462</p>
			<p>Once we plot the line <img src="image/B16060_02_6l1.png" alt="14"/> with the preceding coefficients, we get the following graph:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B16060_02_08.jpg" alt="Figure 2.8: Plotted graph of the linear regression for the second dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8: Plotted graph of the linear regression for the second dataset</p>
			<p>Clearly, with a nonlinear distribution, our linear regression model struggles to fit the data.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We don't have to use this method to perform linear regression. Many libraries, including scikit-learn, will help us to automate this process. Once we perform linear regression with multiple variables, we are better off using a library to perform the regression for us.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor075"/>Fitting a Model Using NumPy Polyfit</h2>
			<p>NumPy Polyfit can also be used to create a line of best fit for linear regression with one variable.</p>
			<p>Recall the calculation for the line of best fit:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">x = np.array(range(1, 14))</p>
			<p class="source-code">y = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
			<p class="source-code">x_2 = x**2</p>
			<p class="source-code">xy = x*y</p>
			<p class="source-code">sum_x = sum(x)</p>
			<p class="source-code">sum_y = sum(y)</p>
			<p class="source-code">sum_x_2 = sum(x_2)</p>
			<p class="source-code">sum_xy = sum(xy)</p>
			<p class="source-code">N = len(x)</p>
			<p class="source-code">a = (N*sum_xy - (sum_x*sum_y))/(N*sum_x_2-(sum_x)**2)</p>
			<p class="source-code">b = (sum_y - a*sum_x)/N</p>
			<p>The equation for finding the coefficients <img src="image/B16060_02_6b.png" alt="15"/> and <img src="image/B16060_02_6c.png" alt="16"/> is quite long. Fortunately, <strong class="source-inline">numpy.polyfit</strong> in Python performs these calculations to find the coefficients of the line of best fit. The <strong class="source-inline">polyfit</strong> function accepts three arguments: the array of <strong class="source-inline">x</strong> values, the array of <strong class="source-inline">y</strong> values, and the degree of polynomial to look for. As we are looking for a straight line, the highest power of <strong class="source-inline">x</strong> is <strong class="source-inline">1</strong> in the polynomial:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">x = np.array(range(1, 14))</p>
			<p class="source-code">y = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
			<p class="source-code">[a,b] = np.polyfit(x, y, 1)</p>
			<p>For <strong class="source-inline">[a,b]</strong>, the output will be this:</p>
			<p class="source-code">[4.857142857142858, -2.769230769230769]</p>
			<h3 id="_idParaDest-75"><a id="_idTextAnchor076"/>Plotting the Results in Python</h3>
			<p>Suppose you have a set of data points and a regression line; our task is to plot the points and the line together so that we can see the results with our eyes.</p>
			<p>We will use the <strong class="source-inline">matplotlib.pyplot</strong> library for this. This library has two important functions:</p>
			<ul>
				<li><strong class="source-inline">scatter</strong>: This displays scattered points on the plane, defined by a list of <em class="italic">x</em> coordinates and a list of <em class="italic">y</em> coordinates. </li>
				<li><strong class="source-inline">plot</strong>: Along with two arguments, this function plots a segment defined by two points or a sequence of segments defined by multiple points. A plot is like a scatter, except that instead of displaying the points, they are connected by lines. </li>
			</ul>
			<p>A plot with three arguments plots a segment and/or two points formatted according to the third argument.</p>
			<p>A segment is defined by two points. As <em class="italic">x</em> ranges between 1 and 13 (remember the dataset contains 13 data points), it makes sense to display a segment between 0 and 15. We must substitute the value of <em class="italic">x</em> in the equation <img src="image/B16060_02_8c.png" alt="17"/> to get the corresponding <em class="italic">y</em> values:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import matplotlib.pyplot as plot</p>
			<p class="source-code">x = np.array(range(1, 14))</p>
			<p class="source-code">y = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
			<p class="source-code">x_2 = x**2</p>
			<p class="source-code">xy = x*y</p>
			<p class="source-code">sum_x = sum(x)</p>
			<p class="source-code">sum_y = sum(y)</p>
			<p class="source-code">sum_x_2 = sum(x_2)</p>
			<p class="source-code">sum_xy = sum(xy)</p>
			<p class="source-code">N = len(x)</p>
			<p class="source-code">a = (N*sum_xy - (sum_x*sum_y))/(N*sum_x_2-(sum_x)**2)</p>
			<p class="source-code">b = (sum_y - a*sum_x)/N</p>
			<p class="source-code"># Plotting the points</p>
			<p class="source-code">plot.scatter(x, y)</p>
			<p class="source-code"># Plotting the line</p>
			<p class="source-code">plot.plot([0, 15], [b, 15*a+b])</p>
			<p class="source-code">plot.show()</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B16060_02_09.jpg" alt="Figure 2.9: Plotted graph of the linear regression for the first dataset using matplotlib&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9: Plotted graph of the linear regression for the first dataset using matplotlib</p>
			<p>The regression line and the scattered data points are displayed as expected. </p>
			<p>However, the plot has an advanced signature. You can use <strong class="source-inline">plot</strong> to draw scattered dots, lines, and any curves on this figure. These variables are interpreted in groups of three:</p>
			<ul>
				<li><strong class="source-inline">x</strong> values</li>
				<li><strong class="source-inline">y</strong> values</li>
				<li>Formatting options in the form of a string</li>
			</ul>
			<p>Let's create a function for deriving an array of approximated <strong class="source-inline">y</strong> values from an array of approximated <strong class="source-inline">x</strong> values:</p>
			<p class="source-code">def fitY( arr ):</p>
			<p class="source-code">    return [4.857142857142859 * x - 2.7692307692307843 for x in arr]</p>
			<p>We will use the <strong class="source-inline">fit</strong> function to plot the values:</p>
			<p class="source-code">plot.plot(x, y, 'go',x, fitY(x), 'r--o')</p>
			<p>Every third argument handles formatting. The letter <strong class="source-inline">g</strong> stands for green, while the letter <strong class="source-inline">r</strong> stands for red. You could have used <strong class="source-inline">b</strong> for blue and <strong class="source-inline">y</strong> for yellow, among other examples. In the absence of a color, each triple value will be displayed using a different color. The <strong class="source-inline">o</strong> character symbolizes that we want to display a dot where each data point lies. Therefore, <strong class="source-inline">go</strong> has nothing to do with movement – it requests the plotter to plot green dots. The <strong class="source-inline">-</strong> characters are responsible for displaying a dashed line. If you just use -1, a straight line appears instead of the dashed line.</p>
			<p>The output is as follows:</p>
			<p> </p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B16060_02_10.jpg" alt="Figure 2.10: Graph for the plot function using the fit function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.10: Graph for the plot function using the fit function</p>
			<p>The Python plotter library offers a simple solution for most of your graphing problems. You can draw as many lines, dots, and curves as you want on this graph.</p>
			<p>When displaying curves, the plotter connects the dots with segments. Also, bear in mind that even a complex sequence of curves is an approximation that connects the dots. For instance, if you execute the code from <a href="https://gist.github.com/traeblain/1487795">https://gist.github.com/traeblain/1487795</a>, you will recognize the segments of the <strong class="source-inline">batman</strong> function as connected lines:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B16060_02_11.jpg" alt="Figure 2.11: Graph for the batman function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11: Graph for the batman function</p>
			<p>There is a large variety of ways to plot curves. We have seen that the <strong class="source-inline">polyfit</strong> method of the NumPy library returns an array of coefficients to describe a linear equation:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">x = np.array(range(1, 14))</p>
			<p class="source-code">y = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
			<p class="source-code">np.polyfit(x, y, 1)</p>
			<p>Here the output is as follows:</p>
			<p class="source-code">[4.857142857142857, -2.769230769230768]</p>
			<p>This array describes the equation <em class="italic">4.85714286 * x - 2.76923077</em>.</p>
			<p>Suppose we now want to plot a curve, <img src="image/B16060_02_11a.png" alt="18"/>. This quadratic equation is described by the coefficient array <strong class="source-inline">[-1, 3, -2]</strong> as <img src="image/B16060_02_11b.png" alt="19"/>. We could write our own function to calculate the <strong class="source-inline">y</strong> values belonging to <strong class="source-inline">x</strong> values. However, the NumPy library already has a feature that can do this work for us – <strong class="source-inline">np.poly1d</strong>:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">x = np.array(range( -10, 10, 1 ))</p>
			<p class="source-code">f = np.poly1d([-1,3,-2])</p>
			<p>The <strong class="source-inline">f</strong> function that's created by the <strong class="source-inline">poly1d</strong> call not only works with single values but also with lists or NumPy arrays:</p>
			<p class="source-code">f(5)</p>
			<p>The expected output is this:</p>
			<p class="source-code">-12</p>
			<p>Similarly, for <strong class="source-inline">f(x)</strong>:</p>
			<p class="source-code">f(x)</p>
			<p>The output will be:</p>
			<p class="source-code">array ([-132. -110, -90, -72, -56, -42, -30, -20, -12, -6, -2,</p>
			<p class="source-code">        0, 0, -2, -6, -12, -20, -30, -42, -56])</p>
			<p>We can now use these values to plot a nonlinear curve:</p>
			<p class="source-code">import matplotlib.pyplot as plot</p>
			<p class="source-code">plot.plot(x, f(x))</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B16060_02_12.jpg" alt="Figure 2.12: Graph for a nonlinear curve&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.12: Graph for a nonlinear curve</p>
			<p>As you can see, we can use the <strong class="source-inline">pyplot</strong> library to easily create the plot of a nonlinear curve.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor077"/>Predicting Values with Linear Regression</h2>
			<p>Suppose we are interested in the <strong class="source-inline">y</strong> value belonging to the <strong class="source-inline">x</strong> coordinate <strong class="source-inline">20</strong>. Based on the linear regression model, all we need to do is substitute the value of <strong class="source-inline">20</strong> in the place of <strong class="source-inline">x</strong> on the previously used code:</p>
			<p class="source-code">x = np.array(range(1, 14))</p>
			<p class="source-code">y = np.array([2, 8, 8, 18, 25, 21, 32, 44, 32, 48, 61, 45, 62])</p>
			<p class="source-code"># Plotting the points</p>
			<p class="source-code">plot.scatter(x, y)</p>
			<p class="source-code"># Plotting the prediction belonging to x = 20</p>
			<p class="source-code">plot.scatter(20, a * 20 + b, color='red')</p>
			<p class="source-code"># Plotting the line</p>
			<p class="source-code">plot.plot([0, 25], [b, 25*a+b])</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B16060_02_13.jpg" alt="Figure 2.13: Graph showing the predicted value using linear regression&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13: Graph showing the predicted value using linear regression</p>
			<p>Here, we denoted the predicted value with red. This red point is on the best line of fit.</p>
			<p>Let's look at next exercise where we will be predicting populations based on linear regression.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor078"/>Exercise 2.01: Predicting the Student Capacity of an Elementary School</h2>
			<p>In this exercise, you will be trying to forecast the need for elementary school capacity. Your task is to figure out 2025 and 2030 predictions for the number of children starting elementary school.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The data is contained inside the <strong class="source-inline">population.csv</strong> file, which you can find on our GitHub repository: <a href="https://packt.live/2YYlPoj">https://packt.live/2YYlPoj</a>.</p>
			<p>The following steps will help you to complete this exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Import <strong class="source-inline">pandas</strong> and <strong class="source-inline">numpy</strong>:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plot</p></li>
				<li>Next, load the CSV file as a DataFrame on the Notebook and read the CSV file:<p class="callout-heading">Note</p><p class="callout">Watch out for the slashes in the string below. Remember that the backslashes (<em class="italic"> \ </em>) are used to split the code across multiple lines, while the forward slashes (<em class="italic"> / </em>) are part of the URL.</p><p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/The-Applied-Artificial-'\</p><p class="source-code">           'Intelligence-Workshop/master/Datasets/'\</p><p class="source-code">           'population.csv'</p><p class="source-code">df = pd.read_csv(file_url)</p><p class="source-code">df</p><p>The expected output is this:</p><div id="_idContainer080" class="IMG---Figure"><img src="image/B16060_02_14.jpg" alt="Figure 2.14: Reading the CSV file&#13;&#10;"/></div><p class="figure-caption">Figure 2.14: Reading the CSV file</p></li>
				<li>Now, convert the DataFrame into two NumPy arrays. For simplicity, we can indicate that the <strong class="source-inline">year</strong> feature, which is from <strong class="source-inline">2001</strong> to <strong class="source-inline">2018</strong>, is the same as <strong class="source-inline">1</strong> to <strong class="source-inline">18</strong>:<p class="source-code">x = np.array(range(1, 19))</p><p class="source-code">y = np.array(df['population'])</p><p>The <strong class="source-inline">x</strong> output will be:</p><p class="source-code">array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18])</p><p>The <strong class="source-inline">y</strong> output will be:</p><p class="source-code">array([147026, 144272, 140020, 143801, 146233,</p><p class="source-code">       144539, 141273, 135389, 142500, 139452,</p><p class="source-code">       139722, 135300, 137289, 136511, 132884,</p><p class="source-code">       125683, 127255, 124275], dtype=int64)</p></li>
				<li>Now, with the two NumPy arrays, use the <strong class="source-inline">polyfit</strong> method (with a degree of <strong class="source-inline">1</strong> as we only have one feature) to determine the coefficients of the regression line:<p class="source-code">[a, b] = np.polyfit(x, y, 1)</p><p>The output for <strong class="source-inline">[a, b]</strong> will be:</p><p class="source-code">[-1142.0557275541803, 148817.5294117647]</p></li>
				<li>Now, plot the results using <strong class="source-inline">matplotlib.pyplot</strong> and predict the future until <strong class="source-inline">2030</strong>:<p class="source-code">plot.scatter( x, y ) </p><p class="source-code">plot.plot( [0, 30], [b, 30*a+b] )</p><p class="source-code">plot.show()</p><p>The expected output is this:</p><div id="_idContainer081" class="IMG---Figure"><img src="image/B16060_02_15.jpg" alt="Figure 2.15: Plot showing the future for 2030&#13;&#10;"/></div><p class="figure-caption">Figure 2.15: Plot showing th<a id="_idTextAnchor079"/>e future for 2030</p><p>As you can see, the data appears linear and our model seems to be a good fit.</p></li>
				<li>Finally, predict the population for <strong class="source-inline">2025</strong> and <strong class="source-inline">2030</strong>:<p class="source-code">population_2025 = 25*a+b</p><p class="source-code">population_2030 = 30*a+b</p><p>The output for <strong class="source-inline">population_2025</strong> will be:</p><p class="source-code">120266.1362229102</p><p>The output for <strong class="source-inline">population_2030</strong> will be:</p><p class="source-code">114555.85758513928</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31dvuKt">https://packt.live/31dvuKt</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/317qeIc">https://packt.live/317qeIc</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>By completing this exercise, we can now conclude that the population of children starting elementary school is going to decrease in the future and that there is no need to increase the elementary school capacity if we are currently meeting the needs.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor080"/>Linear Regression with Multiple Variables</h1>
			<p>In the previous section, we dealt with linear regression with one variable. Now we will learn an extended version of linear regression, where we will use multiple input variables to predict the output. </p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor081"/>Multiple Linear Regression</h2>
			<p>If you recall the formula for the line of best fit in linear regression, it was defined as <img src="image/B16060_02_15a.png" alt="20"/>, where <img src="image/B16060_02_6b.png" alt="21"/> is the slope of the line, <img src="image/B16060_02_6c.png" alt="22"/> is the <em class="italic">y</em> intercept of the line, <em class="italic">x</em> is the feature value, and <em class="italic">y</em> is the calculated label value.</p>
			<p>In multiple regression, we have multiple features and one label. If we have three features, <em class="italic">x</em><span class="subscript">1</span>, <em class="italic">x</em><span class="subscript">2</span>, and <em class="italic">x</em><span class="subscript">3</span>, our model changes to <img src="image/B16060_02_15d.png" alt="23"/>.</p>
			<p>In NumPy array format, we can write this equation as follows:</p>
			<p class="source-code">y = np.dot(np.array([a1, a2, a3]), np.array([x1, x2, x3])) + b</p>
			<p>For convenience, it makes sense to define the whole equation in a vector multiplication format. The coefficient of <img src="image/B16060_02_6c.png" alt="24"/> is going to be <strong class="source-inline">1</strong>:</p>
			<p class="source-code">y = np.dot(np.array([b, a1, a2, a3]) * np.array([1, x1, x2, x3]))</p>
			<p>Multiple linear regression is a simple scalar product of two vectors, where the coefficients <img src="image/B16060_02_6c.png" alt="25"/>, <img src="image/B16060_02_15g.png" alt="26"/>, <img src="image/B16060_02_15h.png" alt="27"/>, and <img src="image/B16060_02_15i.png" alt="28"/> determine the best fit equation in a four-dimensional space.</p>
			<p>To understand the formula of multiple linear regression, you will need the scalar product of two vectors. As the other name for a scalar product is a dot product, the NumPy function performing this operation is called <strong class="source-inline">dot</strong>:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">v1 = [1, 2, 3]</p>
			<p class="source-code">v2 = [4, 5, 6]</p>
			<p class="source-code">np.dot(v1, v2)</p>
			<p>The output will be <strong class="source-inline">32</strong> as <strong class="source-inline">np.dot(v1, v2)= 1 * 4 + 2 * 5 + 3 * 6 = 32</strong>.</p>
			<p>We simply sum the product of each respective coordinate.</p>
			<p>We can determine these coefficients by minimizing the error between the data points and the nearest points described by the equation. For simplicity, we will omit the mathematical solution of the best-fit equation and use scikit-learn instead.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In <em class="italic">n</em>-dimensional spaces, where <em class="italic">n</em> is greater than 3, the number of dimensions determines the different variables that are in our model. In the preceding example, we have three features (<em class="italic">x</em><span class="subscript">1</span>, <em class="italic">x</em><span class="subscript">2</span>, and <em class="italic">x</em><span class="subscript">3</span>) and one label, <em class="italic">y</em>. This yields four dimensions. If you want to imagine a four-dimensional space, you can imagine a three-dimensional space with a fourth dimension of time. A five-dimensional space can be imagined as a four-dimensional space, where each point in time has a temperature. Dimensions are just features (and labels); they do not necessarily correlate with our concept of three-dimensional space.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor082"/>The Process of Linear Regression</h2>
			<p>We will follow the following simple steps to solve linear regression problems: </p>
			<ol>
				<li value="1">Load data from the data sources.</li>
				<li>Prepare data for prediction. Data is prepared in this (<strong class="source-inline">normalize</strong>, <strong class="source-inline">format</strong>, and <strong class="source-inline">filter</strong>) format.</li>
				<li>Compute the parameters of the regression line. Regardless of whether we use linear regression with one variable or with multiple variables, we will follow these steps. </li>
			</ol>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor083"/>Importing Data from Data Sources</h2>
			<p>There are multiple libraries that can provide us with access to data sources. As we will be working with stock data, let's cover two examples that are geared toward retrieving financial data: Quandl and Yahoo Finance. Take a look at these important points before moving ahead:</p>
			<ul>
				<li>Scikit-learn comes with a few datasets that can be used for practicing your skills.</li>
				<li><a href="https://www.quandl.com">https://www.quandl.com</a> provides you with free and paid financial datasets.</li>
				<li><a href="https://pandas.pydata.org/">https://pandas.pydata.org/</a> helps you load any CSV, Excel, JSON, or SQL data.</li>
				<li>Yahoo Finance provides you with financial datasets.</li>
			</ul>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor084"/>Loading Stock Prices with Yahoo Finance</h2>
			<p>The process of loading stock data with Yahoo Finance is straightforward. All you need to do is install the <strong class="source-inline">yfinance</strong> package using the following command in Jupyter Notebook:</p>
			<p class="source-code">!pip install yfinance</p>
			<p>We will download a dataset that has an open price, high price, low price, close price, adjusted close price, and volume values of the S&amp;P 500 index starting from 2015 to January 1, 2020. The S&amp;P 500 index is the stock market index that measures the stock performance of 500 large companies listed in the United States:</p>
			<p class="source-code">import yfinance as yahoo</p>
			<p class="source-code">spx_data_frame = yahoo.download(“^GSPC”, “2015-01-01”, “2020-01-01”)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset file can also be found in our GitHub repository: <a href="https://packt.live/3fRI5Hk">https://packt.live/3fRI5Hk</a>.</p>
			<p class="callout">The original dataset can be found here: <a href="https://github.com/ranaroussi/yfinance">https://github.com/ranaroussi/yfinance</a>.</p>
			<p>That's all you need to do. The DataFrame containing the S&amp;P 500 index is ready.</p>
			<p>You can plot the index closing prices using the <strong class="source-inline">plot</strong> method:</p>
			<p class="source-code">spx_data_frame.Close.plot()</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B16060_02_16.jpg" alt="Figure 2.16: Graph showing the S&amp;P 500 index closing price since 2015&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.16: Graph showing the S&amp;P 500<a id="_idTextAnchor085"/> index closing price since 2015</p>
			<p>The data does not appear to be linear; a polynomial regression might be a better model for this dataset.</p>
			<p>It is also possible to save data to a CSV file using the following code:</p>
			<p class="source-code">spx_data_frame.to_csv(“yahoo_spx.csv”)</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><a href="https://www.quandl.com">https://www.quandl.com</a> is a reliable source of financial and economic datasets that we will be using in this chapter.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor086"/>Exercise 2.02: Using Quandl to Load Stock Prices</h2>
			<p>The goal of this exercise is to download data from the Quandl package and load it into a DataFrame like we previously did with Yahoo Finance.</p>
			<p>The following steps will help you to complete the exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Install <strong class="source-inline">Quandl</strong> using the following command:<p class="source-code">!pip install quandl</p></li>
				<li>Download the data into a DataFrame using Quandl for the S&amp;P 500. Its ticker is <strong class="source-inline">“YALE/SPCOMP”</strong>:<p class="source-code">import quandl</p><p class="source-code">data_frame = quandl.get(“YALE/SPCOMP”)</p></li>
				<li>Use the DataFrame <strong class="source-inline">head()</strong> method to inspect the first five rows of data in your DataFrame:<p class="source-code">data_frame.head()</p><p>The output is as follows:</p><div id="_idContainer092" class="IMG---Figure"><img src="image/B16060_02_17.jpg" alt="Figure 2.17: Dataset displayed as the output&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 2.17: Dataset displayed as the o<a id="_idTextAnchor087"/>utput</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3dwDUz6">https://packt.live/3dwDUz6</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/31812B6">https://packt.live/31812B6</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>By completing this exercise, we have learned how to download an external dataset in <strong class="source-inline">CSV</strong> format and import it as a DataFrame. We also learned about the <strong class="source-inline">.head()</strong> method, which provides a quick view of the first five rows of your DataFrame. </p>
			<p>In the next section, we will be moving on to prepare the dataset to perform multiple linear regression.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor088"/>Preparing Data for Prediction</h2>
			<p>Before we perform multiple linear regression on our dataset, we must choose the relevant features and the data range on which we will perform the regression.</p>
			<p>Preparing the data for prediction is the second step in the regression process. This step also has several sub-steps. We will go through these sub-steps in the following exercise.</p>
			<h2 id="_idParaDest-85">Exercise 2.03: Preparing the Quandl Dat<a id="_idTextAnchor089"/>a for Prediction</h2>
			<p>The goal of this exercise is to download an external dataset from the Quandl library and then prepare it so that it is ready for use in our linear regression models.</p>
			<p>The following steps will help you to complete this exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.<p class="callout-heading">Note</p><p class="callout">If the Qaundl library is not installed on your system, remember to run the command <strong class="source-inline">!pip install quandl</strong>.</p></li>
				<li>Next, download the data into a DataFrame using Quandl for the S&amp;P 500 between 1950 and 2019. Its ticker is <strong class="source-inline">“YALE/SPCOMP”</strong>:<p class="source-code">import quandl</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn import preprocessing</p><p class="source-code">from sklearn import model_selection</p><p class="source-code">data_frame = quandl.get(“YALE/SPCOMP”, \</p><p class="source-code">                        start_date=”1950-01-01”, \</p><p class="source-code">                        end_date=”2019-12-31”)</p></li>
				<li>Use the <strong class="source-inline">head()</strong> method to visualize the columns inside the <strong class="source-inline">data_frame.head()</strong> DataFrame:<p class="source-code">data_frame.head()</p><p>The output is as follows:</p><div id="_idContainer093" class="IMG---Figure"><img src="image/B16060_02_18.jpg" alt="Figure 2.18: Dataset displayed as the output&#13;&#10;"/></div><p class="figure-caption">Figure 2.18: Dataset displayed as the output</p><p>A few features seem to highly correlate with each other. For instance, the <strong class="source-inline">Real Dividend</strong> column grows proportionally with <strong class="source-inline">Real Price</strong>. The ratio between them is not always similar, but they do correlate. </p><p>As regression is not about detecting the correlation between features, we would rather get rid of the features that we know are correlated and perform regression on the features that are non-correlated. In this case, we will keep the <strong class="source-inline">Long Interest Rate</strong>, <strong class="source-inline">Real Price</strong>, and <strong class="source-inline">Real Dividend</strong> columns.</p></li>
				<li>Keep only the relevant columns in the <strong class="source-inline">Long Interest Rate</strong>, <strong class="source-inline">Real Price</strong>, and <strong class="source-inline">Real Dividend</strong> DataFrames:<p class="source-code">data_frame = data_frame[['Long Interest Rate', \</p><p class="source-code">                         'Real Price', 'Real Dividend']]</p><p class="source-code">data_frame</p><p>The output is as follows:</p><div id="_idContainer094" class="IMG---Figure"><img src="image/B16060_02_19.jpg" alt="Figure 2.19: Dataset showing only the relevant columns&#13;&#10;"/></div><p>     </p><p class="figure-caption">Figure 2.19: Dataset showing only th<a id="_idTextAnchor090"/>e relevant columns</p><p>You can see that the DataFrame contains a few missing values <strong class="source-inline">NaN</strong>. As regression doesn't work with missing values, we need to either replace them or delete them. In the real world, we will usually choose to replace them. In this case, we will replace the missing values by the preceding values using a method called <strong class="bold">forward filling</strong>.</p></li>
				<li>We can replace the missing values with a forward filling as shown in the following code snippet:<p class="source-code">data_frame.fillna(method='ffill', inplace=True)</p><p class="source-code">data_frame</p><p>The output is as follows:</p><div id="_idContainer095" class="IMG---Figure"><img src="image/B16060_02_20.jpg" alt="Figure 2.20: Missing values have been replaced&#13;&#10;"/></div><p class="figure-caption">Figure 2.20: Missing values have been repl<a id="_idTextAnchor091"/>aced</p><p>Now that we have cleaned the missing data, we need to create our label. We want to predict the <strong class="source-inline">Real Price</strong> column 3 months in advance using the current <strong class="source-inline">Real Price</strong>, <strong class="source-inline">Long Interest Rate</strong>, and <strong class="source-inline">Real Dividend</strong> columns. In order to create our label, we need to shift the <strong class="source-inline">Real Price</strong> values up by three units and call it <strong class="source-inline">Real Price Label</strong>.</p></li>
				<li>Create the <strong class="source-inline">Real Price Label</strong> label by shifting <strong class="source-inline">Real Price</strong> by 3 months as shown in the following code:<p class="source-code">data_frame['Real Price Label'] = data_frame['Real Price'].shift(-3)</p><p class="source-code">data_frame</p><p>The output is as follows:</p><div id="_idContainer096" class="IMG---Figure"><img src="image/B16060_02_21.jpg" alt="Figure 2.21: New labels have been created&#13;&#10;"/></div><p class="figure-caption">Figure 2.21: New labels have been created</p><p>The side effect of shifting these values is that missing values will appear in the last three rows for <strong class="source-inline">Real Price Label</strong>, so we need to remove the last three rows of data. However, before that, we need to convert the features into a NumPy array and scale it. We can use the <strong class="source-inline">drop</strong> method of the DataFrame to remove the label column and the preprocessing function from <strong class="source-inline">sklearn</strong> to scale the features.</p></li>
				<li>Create a NumPy array for the features and scale it in the following code:<p class="source-code">features = np.array(data_frame.drop('Real Price Label', 1))</p><p class="source-code">scaled_features = preprocessing.scale(features)</p><p class="source-code">scaled_features</p><p>The output is as follows:</p><p class="source-code">array([[-1.14839975, -1.13009904, -1.19222544],</p><p class="source-code">       [-1.14114523, -1.12483455, -1.18037146],</p><p class="source-code">       [-1.13389072, -1.12377394, -1.17439424],</p><p class="source-code">       ...,</p><p class="source-code">       [-1.360812  ,  2.9384288 ,  3.65260385],</p><p class="source-code">       [-1.32599032,  3.12619329,  3.65260385],</p><p class="source-code">       [-1.29116864,  3.30013894,  3.65260385]])</p><p>The <strong class="source-inline">1</strong> in the second argument specifies that we are dropping columns. As the original DataFrame was not modified, the label can be directly extracted from it. Now that the features are scaled, we need to remove the last three values of the features as they are the features of the missing values in the label column. We will save them for later in the prediction part.</p></li>
				<li>Remove the last three values of the <strong class="source-inline">features</strong> array and save them into another array using the following code:<p class="source-code">scaled_features_latest_3 = scaled_features[-3:]</p><p class="source-code">scaled_features = scaled_features[:-3]</p><p class="source-code">scaled_features</p><p>The output for <strong class="source-inline">scaled_features</strong> is as follows:</p><p class="source-code">array([[-1.14839975, -1.13009904, -1.19222544],</p><p class="source-code">       [-1.14114523, -1.12483455, -1.18037146],</p><p class="source-code">       [-1.13389072, -1.12377394, -1.17439424],</p><p class="source-code">       ...,</p><p class="source-code">       [-1.38866935,  2.97846643,  3.57443947],</p><p class="source-code">       [-1.38866935,  2.83458633,  3.6161088 ],</p><p class="source-code">       [-1.36429417,  2.95488131,  3.65260385]])</p><p>The <strong class="source-inline">scaled_features</strong> variable doesn't contain the three data points anymore as they are now in <strong class="source-inline">scaled_features_latest_3</strong>. Now we can remove the last three rows with missing data from the DataFrame, then convert the label into a NumPy array using <strong class="source-inline">sklearn</strong>.</p></li>
				<li>Remove the rows with missing data in the following code:<p class="source-code">data_frame.dropna(inplace=True)</p><p class="source-code">data_frame</p><p>The output for <strong class="source-inline">data_frame</strong> is as follows:</p><div id="_idContainer097" class="IMG---Figure"><img src="image/B16060_02_22.jpg" alt="Figure 2.22: Dataset updated with the removal of missing values&#13;&#10;"/></div><p>     </p><p class="figure-caption">Figure 2.22: Dataset updated with the r<a id="_idTextAnchor092"/>emoval of missing values</p><p>As you can see, the last three rows were also removed from the DataFrame.</p></li>
				<li>Now let's see if we have accurately created our label. Go ahead and run the following code:<p class="source-code">label = np.array(data_frame['Real Price Label'])</p><p class="source-code">label</p><p>The output for the <strong class="source-inline">label</strong> is as follows:</p><div id="_idContainer098" class="IMG---Figure"><img src="image/B16060_02_23.jpg" alt="Figure 2.23: Output showing the expected labels&#13;&#10;"/></div><p class="figure-caption">Figure 2.23: Output showing the expected labe<a id="_idTextAnchor093"/>ls</p><p>Our variable contains all the labels and is exactly the same as the <strong class="source-inline">Real Price Label</strong> column in the DataFrame.</p><p>Our next task is to separate the training and testing data from each other. As we saw in the <em class="italic">Splitting Data into Training and Testing</em> section, we will use 90% of the data as the training data and the remaining 10% as the test data.</p></li>
				<li>Split the <strong class="source-inline">features</strong> data into training and test sets using <strong class="source-inline">sklearn</strong> with the following code:<p class="source-code">from sklearn import model_selection</p><p class="source-code">(features_train, features_test, \</p><p class="source-code">label_train, label_test) = model_selection\</p><p class="source-code">                           .train_test_split(scaled_features, \</p><p class="source-code">                                             label, test_size=0.1, \</p><p class="source-code">                                             random_state=8)</p><p>The <strong class="source-inline">train_test_split</strong> function shuffles the lines of our data, keeps the correspondence, and puts approximately 10% of all data in the test variables, keeping 90% for the training variables. We also use <strong class="source-inline">random_state=8</strong> in order to reproduce the results. Our data is now ready to be used for the multiple linear regression model.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2zZssOG">https://packt.live/2zZssOG</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2zW8WCH">https://packt.live/2zW8WCH</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>By completing this exercise, we have learned all the required steps for data preparation before performing a regression.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor094"/>Performing and Validating Linear Regression</h2>
			<p>Now that our data has been prepared, we can perform our linear regression. After that, we will measure our model performance and see how well it performs.</p>
			<p>We can now create the linear regression model based on the training data:</p>
			<p class="source-code">from sklearn import linear_model</p>
			<p class="source-code">model = linear_model.LinearRegression()</p>
			<p class="source-code">model.fit(features_train, label_train)</p>
			<p>Once the model is ready, we can use it to predict the labels belonging to the test feature values and use the <strong class="source-inline">score</strong> method from the model to see how accurate it is:</p>
			<p class="source-code">label_predicted = model.predict(features_test)</p>
			<p class="source-code">model.score(features_test, label_test)</p>
			<p>The output is as follows:</p>
			<p class="source-code">0.9847223874806746</p>
			<p>With a score or R<span class="superscript">2 </span>of <strong class="source-inline">0.985</strong>, we can conclude that the model is very accurate. This is not a surprise since the financial market grows at around 6-7% a year. This is linear growth, and the model essentially predicts that the markets will continue growing at a linear rate. Concluding that markets tend to increase in the long run is not rocket science.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor095"/>Predicting the Future</h2>
			<p>Now that our model has been trained, we can use it to predict future values. We will use the <strong class="source-inline">scaled_features_latest_3</strong> variable that we created by taking the last three values of the features NumPy array and using it to predict the index price of the next three months in the following code:</p>
			<p class="source-code">label_predicted = model.predict(scaled_features_latest_3) </p>
			<p>The output is as follows:</p>
			<p class="source-code">array ([3046.2347327, 3171.47495182, 3287.48258298])</p>
			<p>By looking at the output, you might think it se<a id="_idTextAnchor096"/>ems easy to forecast the value of the S&amp;P 500 and use it to earn money by investing in it. Unfortunately, in practice, using this model for making money by betting on the forecast is by no means better than gambling in a casino. This is just an example to illustrate prediction; it is not enough to be used for short-term or long-term speculation on market prices. In addition to this, stock prices are sensitive to many external factors, such as economic recession and government policy. This means that past patterns do not necessarily reflect any patterns in the future.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor097"/>Polynomial and Support Vector Regression</h1>
			<p>When performing a polynomial regression, the relationship between <em class="italic">x</em> and <em class="italic">y</em>, or using their other names, features, and labels, is not a linear equation, but a polynomial equation. This means that instead of the <img src="image/B16060_02_6l.png" alt="29"/> equation, we can have multiple coefficients and multiple powers of <em class="italic">x</em> in the equation.</p>
			<p>To make matters even more complicated, we can perform polynomial regression using multiple variables, where each feature may have coefficients multiplying different powers of the feature.  </p>
			<p>Our task is to find a curve that best fits our dataset. Once polynomial regression is extended to multiple variables, we will learn the SVM model to perform polynomial regression.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor098"/>Polynomial Regression with One Variable</h2>
			<p>As a recap, we have performed two types of regression so far:</p>
			<ul>
				<li>Simple linear regression: <img src="image/B16060_02_6l.png" alt="30"/></li>
				<li>Multiple linear regression: <img src="image/B16060_02_23c_New.png" alt="31"/></li>
			</ul>
			<p>We will now learn how to do polynomial linear regression with one variable. The equation for polynomial linear regression is <img src="image/B16060_02_23d.png" alt="33"/>.</p>
			<p>Polynomial linear regression has a vector of coefficients, <img src="image/B16060_02_23e.png" alt="34"/>, multiplying a vector of degrees of <em class="italic">x</em> in the polynomial, <img src="image/B16060_02_23f.png" alt="35"/>.</p>
			<p>At times, polynomial regression works better than linear regression. If the relationship between labels and features can be described using a linear equation, then using a linear equation makes perfect sense. If we have a nonlinear growth, polynomial regression tends to approximate the relationship between features and labels better.</p>
			<p>The simplest implementation of linear regression with one variable was the <strong class="source-inline">polyfit</strong> method of the NumPy library. In the next exercise, we will perform multiple polynomial linear regression with degrees of 2 and 3.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Even though our polynomial regression has an equation containing coefficients of <em class="italic">x</em><span class="superscript">n</span>, this equation is still referred to as polynomial linear regression in literature. Regression is made linear not because we restrict the usage of higher powers of <em class="italic">x</em> in the equation, but because the coefficients <em class="italic">a</em><span class="subscript">1</span>,<em class="italic">a</em><span class="subscript">2</span> … and so on are linear in the equation. This means that we use the toolset of linear algebra and work with matrices and vectors to find the missing coefficients that minimize the error of the approximation.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor099"/>Exercise 2.04: First-, Second-, and Third-Degree Polynomial Regression</h2>
			<p>The goal of this exercise is to perform first-, second-, and third-degree polynomial regression on the two sample datasets that we used earlier in this chapter. The first dataset has a linear distribution and the second one is the Fibonacci sequence and has a nonlinear distribution.</p>
			<p>The following steps will help you to complete the exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Import the <strong class="source-inline">numpy</strong> and <strong class="source-inline">matplotlib</strong> packages:<p class="source-code">import numpy as np</p><p class="source-code">from matplotlib import pyplot as plot</p></li>
				<li>Define the first dataset:<p class="source-code">x1 = np.array(range(1, 14))</p><p class="source-code">y1 = np.array([2, 8, 8, 18, 25, 21, 32, \</p><p class="source-code">               44, 32, 48, 61, 45, 62])</p></li>
				<li>Define the second dataset:<p class="source-code">x2 = np.array(range(1, 14))</p><p class="source-code">y2 = np.array([0, 1, 1, 2, 3, 5, 8, 13, \</p><p class="source-code">               21, 34, 55, 89, 144])</p></li>
				<li>Perform a polynomial regression of degrees <strong class="source-inline">1</strong>, <strong class="source-inline">2</strong>, and <strong class="source-inline">3</strong> on the first dataset using the <strong class="source-inline">polyfit</strong> method from <strong class="source-inline">numpy</strong> in the following code:<p class="source-code">f1 = np.poly1d(np.polyfit(x1, y1, 1))</p><p class="source-code">f2 = np.poly1d(np.polyfit(x1, y1, 2))</p><p class="source-code">f3 = np.poly1d(np.polyfit(x1, y1, 3))</p><p>The output for <strong class="source-inline">f1</strong> is as follows:</p><p class="source-code">poly1d([ 4.85714286, -2.76923077])</p><p>As you can see, a polynomial regression of degree <strong class="source-inline">1</strong> has two coefficients.</p><p>The output for <strong class="source-inline">f2</strong> is as follows:</p><p class="source-code">poly1d([-0.03196803, 5.3046953, -3.88811189])</p><p>As you can see, a polynomial regression of degree <strong class="source-inline">2</strong> has three coefficients.</p><p>The output for <strong class="source-inline">f3</strong> is as follows:</p><p class="source-code">poly1d([-0.01136364, 0.20666833, -3.91833167, -1.97902098])</p><p>As you can see, a polynomial regression of degree <strong class="source-inline">3</strong> has four coefficients.</p><p>Now that we have calculated the three polynomial regressions, we can plot them together with the data on a graph to see how they behave.</p></li>
				<li>Plot the three polynomial regressions and the data on a graph in the following code:<p class="source-code">import matplotlib.pyplot as plot</p><p class="source-code">plot.plot(x1, y1, 'ko', # black dots \</p><p class="source-code">          x1, f1(x1),'k-',  # straight line \</p><p class="source-code">          x1, f2(x1),'k--',  # black dashed line \</p><p class="source-code">          x1, f3(x1),'k-.' # dot line</p><p class="source-code">)</p><p class="source-code">plot.show()</p><p>The output is as follows:</p><div id="_idContainer105" class="IMG---Figure"><img src="image/B16060_02_24.jpg" alt="Figure 2.24: Graph showing the polynomial regressions for the first dataset&#13;&#10;"/></div><p class="figure-caption">Figure 2.24: Graph showing the polynomial regression<a id="_idTextAnchor100"/>s for the first dataset</p><p>As the coefficients are enumerated from left to right in order of decreasing degree, we can see that the higher-degree coefficients stay close to negligible. In other words, the three curves are almost on top of each other, and we can only detect a divergence near the right edge. This is because we are working on a dataset that can be very well approximated with a linear model.</p><p>In fact, the first dataset was created out of a linear function. Any non-zero coefficients for <em class="italic">x</em><span class="superscript">2</span> and <em class="italic">x</em><span class="superscript">3</span> are the result of overfitting the model based on the available data. The linear model is better for predicting values outside the range of the training data than any higher-degree polynomial.</p><p>Let's contrast this behavior with the second example. We know that the Fibonacci sequence is nonlinear. So, using a linear equation to approximate it is a clear case for underfitting. Here, we expect a higher polynomial degree to perform better. </p></li>
				<li>Perform a polynomial regression of degrees <strong class="source-inline">1</strong>, <strong class="source-inline">2</strong>, and <strong class="source-inline">3</strong> on the second dataset using the <strong class="source-inline">polyfit</strong> method from <strong class="source-inline">numpy</strong> with the following code:<p class="source-code">g1 = np.poly1d(np.polyfit(x2, y2, 1))</p><p class="source-code">g2 = np.poly1d(np.polyfit(x2, y2, 2))</p><p class="source-code">g3 = np.poly1d(np.polyfit(x2, y2, 3))</p><p>The output for <strong class="source-inline">g1</strong> is as follows:</p><p class="source-code">poly1d([ 9.12087912, -34.92307692])</p><p>As you can see, a polynomial regression of degree <strong class="source-inline">1</strong> has <strong class="source-inline">2</strong> coefficients.</p><p>The output for <strong class="source-inline">g2</strong> is as follows:</p><p class="source-code">poly1d([ 1.75024975, -15.38261738, 26.33566434])</p><p>As you can see, a polynomial regression of degree <strong class="source-inline">2</strong> has <strong class="source-inline">3</strong> coefficients.</p><p>The output for <strong class="source-inline">g3</strong> is as follows:</p><p class="source-code">poly1d([ 0.2465035, -3.42632368, 14.69080919, -15.07692308])</p><p>As you can see, a polynomial regression of degree <strong class="source-inline">3</strong> has <strong class="source-inline">4</strong> coefficients.</p></li>
				<li>Plot the three polynomial regressions and the data on a graph in the following code:<p class="source-code">plot.plot(x2, y2, 'ko', # black dots \</p><p class="source-code">          x2, g1(x2),'k-',  # straight line \</p><p class="source-code">          x2, g2(x2),'k--',  # black dashed line \</p><p class="source-code">          x2, g3(x2),'k-.' # dot line</p><p class="source-code">)</p><p class="source-code">plot.show()</p><p>The output is as follows:</p><div id="_idContainer106" class="IMG---Figure"><img src="image/B16060_02_25.jpg" alt="Figure 2.25: Graph showing the second dataset points and three polynomial curves &#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 2.25: Graph showing the second dataset points <a id="_idTextAnchor101"/>and three polynomial curves </p>
			<p>The difference is clear. The quadratic curve fits the points a lot better than the linear one. The cubic curve is even better.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3dpCgyY">https://packt.live/3dpCgyY</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2B09xDN">https://packt.live/2B09xDN</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>If you research Binet's formula, you will find out that the Fibonacci function is an exponential function, as the <em class="italic">n</em><span class="superscript">th</span> Fibonacci number is calculated as the <em class="italic">n</em><span class="superscript">th</span> power of a constant. Therefore, the higher the polynomial degree we use, the more accurate our approximation will be. </p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor102"/>Polynomial Regression with Multiple Variables</h2>
			<p>When we have one variable of degree <em class="italic">n</em>, we have <em class="italic">n+1</em> coefficients in the equation as <img src="image/B16060_02_23d.png" alt="36"/>.</p>
			<p>Once we deal with multiple features, <em class="italic">x</em><span class="superscript">1</span>, <em class="italic">x</em><span class="superscript">2</span>, …, <em class="italic">x</em><span class="superscript">m</span>, and their powers of up to the <em class="italic">n</em><span class="superscript">th</span> degree, we get an <em class="italic">m * (n+1)</em> matrix of coefficients. The math will become quite lengthy when we start exploring the details and prove how a polynomial model works. We will also lose the nice visualizations of two-dimensional curves. </p>
			<p>Therefore, we will apply the concepts learned in the previous section on polynomial regression with one variable and omit the math. When training and testing a linear regression model, we can calculate the mean square error to see how good an approximation a model is.</p>
			<p>In scikit-learn, the degree of the polynomials used in the approximation is a simple parameter in the model.  </p>
			<p>As polynomial regression is a form of linear regression, we can perform polynomial regression without changing the regression model. All we need to do is to transform the input and keep the linear regression model. The transformation of the input is performed by the <strong class="source-inline">fit_transform</strong> method of the <strong class="source-inline">PolynomialFeatures</strong> package. </p>
			<p>First, we can reuse the code from <em class="italic">Exercise 2.03</em>, <em class="italic">Preparing the Quandl Data for Prediction</em>, up to <em class="italic">Step 9</em> and import <strong class="source-inline">PolynomialFeatures</strong> from the <strong class="source-inline">preprocessing</strong> module of <strong class="source-inline">sklearn</strong>:</p>
			<p class="source-code">!pip install quandl</p>
			<p class="source-code">import quandl</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from sklearn import preprocessing</p>
			<p class="source-code">from sklearn import model_selection</p>
			<p class="source-code">from sklearn import linear_model</p>
			<p class="source-code">from matplotlib import pyplot as plot</p>
			<p class="source-code">from sklearn.preprocessing import PolynomialFeatures</p>
			<p class="source-code">data_frame = quandl.get(“YALE/SPCOMP”, \</p>
			<p class="source-code">                        start_date=”1950-01-01”, \</p>
			<p class="source-code">                        end_date=”2019-12-31”)</p>
			<p class="source-code">data_frame = data_frame[['Long Interest Rate', \</p>
			<p class="source-code">                         'Real Price', 'Real Dividend']]</p>
			<p class="source-code">data_frame.fillna(method='ffill', inplace=True)</p>
			<p class="source-code">data_frame['Real Price Label'] = data_frame['Real Price'].shift(-3)</p>
			<p class="source-code">features = np.array(data_frame.drop('Real Price Label', 1))</p>
			<p class="source-code">scaled_features = preprocessing.scale(features)</p>
			<p class="source-code">scaled_features_latest_3 = scaled_features[-3:]</p>
			<p class="source-code">scaled_features = scaled_features[:-3]</p>
			<p class="source-code">data_frame.dropna(inplace=True)</p>
			<p class="source-code">label = np.array(data_frame['Real Price Label'])</p>
			<p>Now, we can create a polynomial regression of degree <strong class="source-inline">3</strong> using the <strong class="source-inline">fit_transform</strong> method of <strong class="source-inline">PolynomialFeatures</strong>:</p>
			<p class="source-code">poly_regressor = PolynomialFeatures(degree=3)</p>
			<p class="source-code">poly_scaled_features = poly_regressor.fit_transform(scaled_features)</p>
			<p class="source-code">poly_scaled_features</p>
			<p>The output of <strong class="source-inline">poly_scaled_features</strong> is as follows:</p>
			<p class="source-code">array([[ 1.        , -1.14839975, -1.13009904, ..., -1.52261953,</p>
			<p class="source-code">        -1.60632446, -1.69463102],</p>
			<p class="source-code">       [ 1.        , -1.14114523, -1.12483455, ..., -1.49346824,</p>
			<p class="source-code">        -1.56720585, -1.64458414],</p>
			<p class="source-code">       [ 1.        , -1.13389072, -1.12377394, ..., -1.48310475,</p>
			<p class="source-code">        -1.54991107, -1.61972667],</p>
			<p class="source-code">       ...,</p>
			<p class="source-code">       [ 1.        , -1.38866935,  2.97846643, ..., 31.70979016,</p>
			<p class="source-code">        38.05472653, 45.66924612],</p>
			<p class="source-code">       [ 1.        , -1.38866935,  2.83458633, ..., 29.05499915,</p>
			<p class="source-code">        37.06573938, 47.28511704],</p>
			<p class="source-code">       [ 1.        , -1.36429417,  2.95488131, ..., 31.89206605,</p>
			<p class="source-code">        39.42259303, 48.73126873]])</p>
			<p>Then, we need to split the data into testing and training sets:</p>
			<p class="source-code">(poly_features_train, poly_features_test, \</p>
			<p class="source-code">poly_label_train, poly_label_test) = \</p>
			<p class="source-code">model_selection.train_test_split(poly_scaled_features, \</p>
			<p class="source-code">                                 label, test_size=0.1, \</p>
			<p class="source-code">                                 random_state=8)</p>
			<p>The <strong class="source-inline">train_test_split</strong> function shuffles the lines of our data, keeps the correspondence, and puts approximately 10% of all data in the test variables, keeping 90% for the training variables. We also use <strong class="source-inline">random_state=8</strong> in order to reproduce the results. </p>
			<p>Our data is now ready to be used for the multiple polynomial regression model; we will also measure its performance with the <strong class="source-inline">score</strong> function:</p>
			<p class="source-code">model = linear_model.LinearRegression()</p>
			<p class="source-code">model.fit(poly_features_train, poly_label_train)</p>
			<p class="source-code">model.score(poly_features_test, poly_label_test)</p>
			<p>The output is as follows:</p>
			<p class="source-code">0.988000620369118</p>
			<p>With a score or R<span class="superscript">2 </span>of <strong class="source-inline">0.988</strong>, our multiple polynomial regression model is slightly better than our multiple linear regression model (<strong class="source-inline">0.985</strong>), which we built in <em class="italic">Exercise 2.03</em>, <em class="italic">Preparing the Quandl Data for Prediction</em>. It might be possible that both models are overfitting the dataset.</p>
			<p>There is another model in scikit-learn that performs polynomial regression, called the SVM model.</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor103"/>Support Vector Regression</h1>
			<p>SVMs are binary classifiers and are usually used in classification problems (you will learn more about this in <em class="italic">Chapter 3</em>, <em class="italic">An Introduction to Classification</em>). An SVM classifier takes data and tries to predict which class it belongs to. Once the classification of a data point is determined, it gets labeled. But SVMs can also be used for regression; that is, instead of labeling data, it can predict future values in a series. </p>
			<p>The SVR model uses the space between our data as a margin of error. Based on the margin of error, it makes predictions regarding future values.</p>
			<p>If the margin of error is too small, we risk overfitting the existing dataset. If the margin of error is too big, we risk underfitting the existing dataset.</p>
			<p>In the case of a classifier, the kernel describes the surface dividing the state space, whereas, in a regression, the kernel measures the margin of error. This kernel can use a linear model, a polynomial model, or many other possible models. The default kernel is <strong class="bold">RBF</strong>, which stands for <strong class="bold">Radial Basis Function</strong>.</p>
			<p>SVR is an advanced topic that is outside the scope of this book. Therefore, we will only stick to an easy walk-through as an opportunity to try out another regression model on our data. </p>
			<p>We can reuse the code from <em class="italic">Exercise 2.03</em>, <em class="italic">Preparing the Quandl Data for Prediction</em>, up to <em class="italic">Step 11</em>:</p>
			<p class="source-code">import quandl</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from sklearn import preprocessing</p>
			<p class="source-code">from sklearn import model_selection</p>
			<p class="source-code">from sklearn import linear_model</p>
			<p class="source-code">from matplotlib import pyplot as plot</p>
			<p class="source-code"> </p>
			<p class="source-code">data_frame = quandl.get(“YALE/SPCOMP”, \</p>
			<p class="source-code">                        start_date=”1950-01-01”, \</p>
			<p class="source-code">                        end_date=”2019-12-31”)</p>
			<p class="source-code">data_frame = data_frame[['Long Interest Rate', \</p>
			<p class="source-code">                         'Real Price', 'Real Dividend']]</p>
			<p class="source-code">data_frame.fillna(method='ffill', inplace=True)</p>
			<p class="source-code">data_frame['Real Price Label'] = data_frame['Real Price'].shift(-3)</p>
			<p class="source-code">features = np.array(data_frame.drop('Real Price Label', 1))</p>
			<p class="source-code">scaled_features = preprocessing.scale(features)</p>
			<p class="source-code">scaled_features_latest_3 = scaled_features[-3:]</p>
			<p class="source-code">scaled_features = scaled_features[:-3]</p>
			<p class="source-code">data_frame.dropna(inplace=True)</p>
			<p class="source-code">label = np.array(data_frame['Real Price Label'])</p>
			<p class="source-code">(features_train, features_test, label_train, label_test) = \</p>
			<p class="source-code">model_selection.train_test_split(scaled_features, label, \</p>
			<p class="source-code">                                 test_size=0.1, \</p>
			<p class="source-code">                                 random_state=8)</p>
			<p>Then, we can perform a regression with <strong class="source-inline">svm</strong> by simply changing the linear model to a support vector model by using the <strong class="source-inline">svm</strong> method from <strong class="source-inline">sklearn</strong>:</p>
			<p class="source-code">from sklearn import svm</p>
			<p class="source-code">model = svm.SVR()</p>
			<p class="source-code">model.fit(features_train, label_train)</p>
			<p>As you can see, performing an SVR is exactly the same as performing a linear regression, with the exception of defining the model as <strong class="source-inline">svm.SVR()</strong>.</p>
			<p>Finally, we can predict and measure the performance of our model:</p>
			<p class="source-code">label_predicted = model.predict(features_test)</p>
			<p class="source-code">model.score(features_test, label_test)</p>
			<p>The output is as follows:</p>
			<p class="source-code">0.03262153550014424</p>
			<p>As you can see, the score or R<span class="superscript">2</span> is quite low, our SVR's parameters need to be optimized in order to increase the accuracy of the model.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor104"/>Support Vector Machines with a 3-Degree Polynomial Kernel</h2>
			<p>Let's switch the kernel of the SVM to a polynomial function (the default degree is <strong class="source-inline">3</strong>) and measure the performance of the new model:</p>
			<p class="source-code">model = svm.SVR(kernel='poly') </p>
			<p class="source-code">model.fit(features_train, label_train) </p>
			<p class="source-code">label_predicted = model.predict(features_test) </p>
			<p class="source-code">model.score(features_test, label_test)</p>
			<p>The output is as follows:</p>
			<p class="source-code">0.44465054598560627</p>
			<p>We managed to increase the performance of the SVM by simply changing the kernel function to a polynomial function; however, the model still needs a lot of tuning to reach the same performance as the linear regression models.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor105"/>Activity 2.01: Boston House Price Prediction with Polynomial Regression of Degrees 1, 2, and 3 on Multiple Variables</h2>
			<p>In this activity, you will need to perform linear polynomial regression of degrees 1, 2, and 3 with scikit-learn and find the best model. You will work on the Boston House Prices dataset. The Boston House Price dataset is very famous and has been used as an example for research on regression models.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">More details about the Boston House Prices dataset can be found at <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/">https://archive.ics.uci.edu/ml/machine-learning-databases/housing/</a>.</p>
			<p class="callout">The dataset file can also be found in our GitHub repository: <a href="https://packt.live/2V9kRUU">https://packt.live/2V9kRUU</a>.</p>
			<p>You will need to predict the prices of houses in Boston (label) based on their characteristics (features). Your main goal will be to build 3 linear models using polynomial regressions of degrees <strong class="source-inline">1</strong>, <strong class="source-inline">2</strong>, and <strong class="source-inline">3</strong> with all the features of the dataset. You can find the following dataset description:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B16060_02_26.jpg" alt="Figure 2.26: Boston housing dataset description&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.26: Boston housing dataset description</p>
			<p>We will<a id="_idTextAnchor106"/> define our label as the <strong class="source-inline">MEDV</strong> field, which is the median value of the house in $1,000s. All of the other fields will be used as our features for our models. As this dataset does not contain any missing values, we won't have to replace missing values as we did in the previous exercises.</p>
			<p>The following steps will help you to complete the activity:</p>
			<ol>
				<li value="1">Open a Jupyter Notebook.</li>
				<li>Import the required packages and load the Boston House Prices data into a DataFrame.</li>
				<li>Prepare the dataset for prediction by converting the label and features into NumPy arrays and scaling the features.</li>
				<li>Create three different sets of features by transforming the scaled features into suitable formats for each of the polynomial regressions.</li>
				<li>Split the data into training and testing sets with <strong class="source-inline">random state = 8</strong>.</li>
				<li>Perform a polynomial regression of degree <strong class="source-inline">1</strong> and evaluate whether the model is overfitting.</li>
				<li>Perform a polynomial regression of degree <strong class="source-inline">2</strong> and evaluate whether the model is overfitting.</li>
				<li>Perform a polynomial regression of degree <strong class="source-inline">3</strong> and evaluate whether the model is overfitting.</li>
				<li>Compare the predictions of the three models against the label on the testing set.</li>
			</ol>
			<p>The expected output is this:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B16060_02_27.jpg" alt="Figure 2.27: Expected output based on the predictions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.27: Expected output based on the predictions</p>
			<p class="callout-heading"><a id="_idTextAnchor107"/>Note</p>
			<p class="callout">The solution to this activity is available on page 334.</p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor108"/>Summary</h1>
			<p>In this chapter, we have learned the fundamentals of linear regression. After going through some basic mathematics, we looked at the mathematics of linear regression using one variable and multiple variables.</p>
			<p>Then, we learned how to load external data from sources such as a CSV file, Yahoo Finance, and Quandl. After loading the data, we learned how to identify features and labels, how to scale data, and how to format data to perform regression.</p>
			<p>We learned how to train and test a linear regression model, and how to predict the future. Our results were visualized by an easy-to-use Python graph plotting library called <strong class="source-inline">pyplot</strong>.</p>
			<p>We also learned about a more complex form of linear regression: linear polynomial regression using arbitrary degrees. We learned how to define these regression problems on multiple variables and compare their performance on the Boston House Price dataset. As an alternative to polynomial regression, we also introduced SVMs as a regression model and experimented with two kernels.</p>
			<p>In the next chapter, you will learn about classification and its models.</p>
		</div>
		<div>
			<div id="_idContainer111" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer112" class="Content">
			</div>
		</div>
	</body></html>