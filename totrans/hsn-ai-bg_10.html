<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep Learning for Game Playing</h1>
                </header>
            
            <article>
                
<p>Over the past several years, one of the most notable applications of <strong>Artificial</strong> <strong>Intelligence </strong>(<strong>AI</strong>) has been in the game-playing space. Especially with the recent success of AlphaGo and AlphaGo Zero, game-playing AIs have been a topic of great public interest. In this chapter, we'll implement two basic versions of game-playing AIs; one for a video game and one for a board game. We'll primarily be utilizing reinforcement learning methods as our workhorse. We'll also touch upon the methods behind some of the most advanced game-playing AIs in existence at the moment.</p>
<p>In this chapter, the following topics will be covered: </p>
<ul>
<li>Game Trees and Fundamental Game Theory</li>
<li>Constructing an AI agent to play tic-tac-toe</li>
<li>Constructing ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements </h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be utilizing the following: </p>
<ul>
<li>TensorFlow</li>
<li>OpenAI gym </li>
</ul>
<p>It is recommend that you have access to a GPU for training the models in this chapter, whether on your machine itself or via a cloud service platform. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>AI has been making a splash lately in the game-playing arena. DeepMind, the Google research group that created AlphaGo, have been proponents of utilizing reinforcement learning methods for game-playing applications. More and more video game companies are using deep learning methods for their AI players. So, where are we coming from, and where are we going with AI in the gaming space? </p>
<p>Traditionally, game-playing systems have been made up of a combination of hardcoded rules that have covered the range of behaviors that the AI is supposed to cover. Have you ever played an older adventure, first-person shooter, or strategy game where the AI players were clearly operating off a hardcoded strategy? More often than not, these AIs used ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Networks for board games</h1>
                </header>
            
            <article>
                
<p>When we talk about creating algorithms for game-playing, we are really talking about creating them for a specific type of game, known as a <strong>finite two person zero</strong>-<strong>sum sequential game</strong>. This is really just a fancy way of saying the following: </p>
<ul>
<li>An interactive situation between two independent actors (a game)</li>
<li>There are a finite amount of ways in which the two actors can interact with each other at any point</li>
</ul>
<ul>
<li>The game is zero-sum, meaning that the end state of the game results in a complete win for one of the actors</li>
<li>The game is sequential, meaning that the actors make their moves in sequence, one after another</li>
</ul>
<p class="mce-root">Classic examples of these types of games that we'll cover in this section are Tic Tac Toe, Chess, and the game of Go. Since creating and training an algorithm for a board game such as Go would be an immense task, for time and computation constraint, we'll be creating an agent to compete in a much more reasonable game with a finite amount of steps: chess. In this section, we'll introduce some of the fundamental concepts behind game-playing AIs, and walk through examples in Python for the different strategies. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding game trees</h1>
                </header>
            
            <article>
                
<p>If you're familiar with formal <strong>game theory</strong>, then you know that there is an entire branch of mathematics devoted to understanding and analyzing games. In the computer science world, we can analyze a game by simplifying it into a decision tree structure called a <strong>game tree</strong>. Game trees are a way of mapping out possible moves and states within a board game. Let's take the simple example of a game tree for Tic Tac Toe, as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1331 image-border" src="Images/f66fda2e-7c5a-4229-bf6b-eb1e35775fb6.png" style="width:33.92em;height:19.08em;" width="1010" height="568"/></p>
<p>This tree gives us all of the possible combinations and outcomes for the game based on a certain starting point. Moving from one node to another represents a <strong>move</strong> in the game, and moves continue ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">AlphaGo and intelligent game-playing AIs</h1>
                </header>
            
            <article>
                
<p><span>While MCTS has been a cornerstone of game-playing AI for a while, it was DeepMind's AlphaGo program that really took game-playing AIs into the modern age. AlphaGo and its derivatives (AlphaGo Zero and AlphaGo Master) are game-playing AI systems that utilize MCTS to play the notoriously difficult ancient Chinese game of Go. With 10<sup>761</sup> possible game combinations, creating a system to play the game became something of a milestone in the AI world. It's even the subject of a much talked about documentary by the same name.</span></p>
<p><span>AlphaGo uses a combination of MCTS with deep learning methods that made the Alpha Go programs truly extraordinary. DeepMind trained deep neural networks, such as the ones that we have been learning about throughout this book, to learn the state of the game and effectively guide the MCTS in a more intelligent manner. This network looks at the current state of the board, along with the previous moves that have been made, and decides which move to play next. </span><span>DeepMind's major innovation with AlphaGo was to use deep neural networks to understand the state of the game, and then use this understanding to intelligently guide the search of the MCTS. </span>The system was architected in a way that AlphaGo would teach itself to learn the game, first by watching humans play the game, and secondly by playing the game against itself and correcting itself for its prior mistakes. </p>
<p>The architecture actually uses two different neural networks; a policy network and a value network:</p>
<ul>
<li><strong>Value network</strong>: Reduces the depth of the MCTS search by approximating a value function.</li>
<li><strong>Policy network</strong>: Reduces the breadth of the MCTS search by simulating future actions. The policy network learns from actual human play of the game Go, and develops a policy accordingly:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1339 image-border" src="Images/2827cca8-d649-45e4-8cf4-20107b4b8e06.png" style="width:44.33em;height:16.50em;" width="1619" height="602"/></p>
<p>Let's dive into each of these to understand how the system works. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">AlphaGo policy network</h1>
                </header>
            
            <article>
                
<p>The goal of the policy network is to capture and understand the general actions of players on the board in order to aid the MCTS by guiding the algorithm toward promising actions during the search process; this reduces the <strong>breadth of the search</strong>. <span>Architecturally, the policy network comes in two parts: a supervised learning policy network and a reinforcement learning policy network. </span></p>
<p>The first network, the supervised network, <span>is a 13-layer <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>).</span><span> </span>It was trained by observing the moves that humans make while playing the game <span>–</span> 30 million, to be exact <span>– </span>and outputs a probability distribution for each action given a certain state. We call this type of supervised learning<span> </span><strong>behavior cloning</strong>. </p>
<p>The ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">AlphaGo value network</h1>
                </header>
            
            <article>
                
<p>The value network was used to reduce the error in the system's play by guiding MCTS toward certain nodes. It helped reduce the <strong>depth of the search</strong>. The AlphaGo value network was trained by playing further games against itself in order to optimize the policy that it learned from the policy networks by estimating the value function, specifically the action value function. Recall from <a href="91114074-444f-4201-98ef-e510210380f2.xhtml" target="_blank">Chapter 8</a>, <em>Reinforcement Learning</em>, that <span>action value functions describe the value of taking a certain action while in a specific state. It measures the cumulative reward from a pair of states and actions; for a given state and action we take, how much will this increase our reward? It lets us postulate what would happen by taking a different action in the first time step than what they may want the agent to do, and then afterward following the policy. The action value function is also often called the <strong>Q</strong>-<strong>function</strong>,<strong> </strong>because of the Q that we use to represent it:</span></p>
<p class="CDPAlignCenter CDPAlign"><img style="font-size: 1em;width:22.17em;height:3.75em;" class="fm-editor-equation" src="Images/31ddcfc7-7f63-4340-a66e-c9434f7e38c9.png" width="3230" height="540"/></p>
<p><span>The network approximated the value function by utilizing a noisy version of the policy from the policy network and regressing the state of the board against the result of the game. </span></p>
<p>The network was trained using the reinforce algorithm that we learned about in <a href="91114074-444f-4201-98ef-e510210380f2.xhtml" target="_blank">Chapter 8</a>, <em>Reinforcement Learning</em>. If you recall, Reinforce is a Monte Carlo policy gradient method that uses likelihood ratios to estimate the value of a policy at a given point. The Reinforce algorithm attempts to maximize the expected reward, so that the entire system has the dual goal of playing like a professional human player while attempting to win the game.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">AlphaGo in action</h1>
                </header>
            
            <article>
                
<p>We've gone over how AlphaGo helped select actions, so now let's get back to the core of any game-playing system: the game tree. While AlphaGo utilized game trees and MCTS, the authors created a variation of it called <em>asynchronous policy</em> and <em>value MCTS</em> (APV-MCTS). Unlike standard MCTS, which we discussed previously, APV-MCTS decides which node to expand and evaluate by two different metrics:</p>
<ul>
<li>The outcome of the value network </li>
<li>The outcome of the Monte Carlo simulations</li>
</ul>
<p>The results of these methods are combined with mixing parameters, λ. The algorithm then chooses an action according to the probabilities that were obtained during the initial supervised learning phase. While it may seem counter intuitive to use the probabilities ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Networks for video games</h1>
                </header>
            
            <article>
                
<p>Thus far, we've learned how we can use reinforcement learning methods to play board games utilizing UCT and MCTS; now, let's see what we can do with video games. In <a href="91114074-444f-4201-98ef-e510210380f2.xhtml" target="_blank">Chapter 8</a>, <em>Reinforcement Learning</em>, we saw how we could use reinforcement learning methods to complete basic tasks such as the OpenAI cartpole challenge. In this section, we'll be focusing on a more difficult set of games: classic Atari video games, which have become standard benchmarks for deep learning tasks.</p>
<p>You might be thinking <span>–</span> <em>can't we extend the methods that we used in the cartpole environment to Atari games?</em> While we can, there's a lot more input that we have to handle. In Atari environments, and really any video game environment, the inputs to the network are individual pixels. Instead of the simple four control variables for cartpole, we are now dealing with <span>100,800 variables (210 * 160 * 3). As such, complexity and training times for these networks can be much higher. In this section, we'll try to make the network as simple as possible in order to make it easier to learn from and train. </span></p>
<p>We'll be using the OpenAI gym environment to simulate the Atari game Space Invaders:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1340 image-border" src="Images/2798b969-f161-453a-a10a-e9d05fee3ab6.png" style="width:16.50em;height:22.58em;" width="736" height="1010"/></div>
<p>For those of you who aren't familiar with Space Invaders, the concept is simple <span>–</span> you (the green rocket at the bottom) must destroy a grouping of alien spaceships before they destroy you. The spaceships attempt to hit you with missiles, and vice versa. Google's DeepMind originally introduced Space Invaders as a benchmark task in their paper <em>Playing Atari with Deep Reinforcement Learning</em>, which really set off the concept of Atari games as benchmarks to beat with these intelligent agents. </p>
<p>We'll be constructing something called a <strong>Deep Q-network</strong>, which we touched upon in <a href="91114074-444f-4201-98ef-e510210380f2.xhtml" target="_blank">Chapter 8</a>, <em>Reinforcement Learning. </em>In the next section, we expand upon many of the fundamental Q-learning subjects that we covered in that chapter. With that <span>–</span> let's dive in!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Constructing a Deep Q–network</h1>
                </header>
            
            <article>
                
<p>Deep Q-networks were first introduced by DeepMind in their paper <em>Human-level control through deep reinforcement</em> <em>learning,</em> published in the British scientific journal <em>Nature</em>, and now commonly referred to as the <em>Nature Paper</em>. The goal of Deep Q-learning was to create an AI agent that could learn a policy from high-dimensional inputs such as video games. In our case, w<span>e'll want to construct a Deep Q-network that can advance through basic tasks and then towards harder tasks. </span></p>
<p>Deep Q-networks approximate Q-values instead of calculating them individually with Q tables, and they do this by using artificial neural networks as a value approximator. The input of the network will be a stack of preprocessed frames, and the ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Utilizing a target network</h1>
                </header>
            
            <article>
                
<p>Let's look back at the Q- function optimization process:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/3f120be8-ba2f-4aa8-b4df-31fe58195ff7.png" style="width:17.08em;height:1.75em;" width="2330" height="230"/></p>
<p>You might notice that this function has a unique property in that it's recursive; one set of Q-values depend on the value of the other set of Q-values. This becomes a problem in training; if we change one set of values, we'll end up changing the other set of values. To put it simply, as we get closer to the targeted Q-value, that Q-value moves even further away. It is continually moving the finish line when you are about to finish a race!</p>
<p class="mce-root">To remedy this, the Q-network creates a copy of itself every 10,000 iterations, called a <strong>target network</strong>, which will represent the targeted Q-value. We can do this in TensorFlow by creating a target network variable that we'll first initialize with the class, and later run in a TensorFlow session:</p>
<pre>## def contained under  __init__(self, actions): ##<br/><br/>## Initialize the base network<br/>self.inputVal, self.QValue = self.deepQarchitecture()<br/><br/>## Initialize the target network<br/>self.inputValT, self.QValueT = self.deepQarchitecture()</pre>
<p>As for the 10,000 iterations, we've already defined that as <kbd>self.update_time = 10000</kbd> when we started building out our DQN class. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Experience replay buffer</h1>
                </header>
            
            <article>
                
<p>While we touched upon it briefly in <a href="91114074-444f-4201-98ef-e510210380f2.xhtml" target="_blank">Chapter 8</a>, <em>Reinforcement Learning</em>, let's dive into the technical details of experience replay buffers. Experience replay is a biologically inspired tool that stores an agent's experience at each time step process. Specifically, it stores the [state, action, reward, next_state] pairs <span>at each time step:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/8358a2c5-8cb6-4bea-a46a-f3dea5f8c237.png" style="width:13.17em;height:1.83em;" width="1580" height="220"/></p>
<p>Instead of running Q-learning on state-action pairs as they occur, experience replay stores these pairs as they are discovered. Experience replay buffers help with two things: </p>
<ul>
<li>Remember past experiences by storing and randomly sampling them </li>
<li>Reduce the chance that experiences will ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Choosing action</h1>
                </header>
            
            <article>
                
<p>Thus far, we've told our network to follow random weights that we've initialized for it, without giving it direction on how to decide what actions to take to update those weights. In the case of policy methods such as the ones we used in <a href="91114074-444f-4201-98ef-e510210380f2.xhtml" target="_blank">Chapter 8</a>, <em>Reinforcement Learning</em>, and preceding with Tic Tac Toe, Q-learning methods work toward approximating the value of the Q-function instead of learning a policy directly. So, what do we do?</p>
<p>Deep Q-networks use a tactic called <strong>exploration</strong> to determine what actions to take. If we didn't use a tactic, our network would probably be limited to the most basic levels of the game because it wouldn't have any idea what moves would allow it to improve!</p>
<p>To remedy this, we will utilize a strategy called<span><strong> ∈-greedy</strong>. This strategy works by choosing actions to learn from based on two methods; first, choosing methods that give our model the highest reward (maximum Q-value), and second, choosing methods at random with the probability ∈. </span>In this strategy, we use a basic inequality:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/eba46b04-720b-4280-9119-5947fa5b3557.png" style="width:3.58em;height:1.08em;" width="430" height="130"/></div>
<p>We set the epsilon variable as 1, and draw random numbers from a distribution. If the number doesn't satisfy the inequality and is less than epsilon, our agent<span> </span><strong>explores</strong><span> </span>the space, meaning it seeks to figure out more information about where it is in order to start choosing state/action pairs so that we can calculate the Q-value. If the number is in fact larger than epsilon, we<span> </span><strong>exploit</strong> the information that we already know to choose a Q-value to calculate.<span> </span>The algorithm starts with a high ∈ <span>and reduces its value by a fixed amount as training continues. We call this process <strong>annealing</strong>. </span>In the original DeepMind paper, the researchers used this strategy with an annealing from 1 to 0.1 over the first million frames, and then held at 0.1 afterward. Let's look back at the parameters we initialized in the beginning of the section:</p>
<pre>self.starting_ep = 1.0<br/>self.ending_ep = 0.1</pre>
<p>You'll notice that we used these exact specifications. During the testing phase of the network, epsilon will be considerably lower, and hence be biased toward an exploitation strategy.</p>
<p>Let's implement this strategy in Python and take a closer look at the mechanics. We'll define a function, <kbd>getAction</kbd>, which sits within our <kbd>deepQ</kbd> class:</p>
<pre>def select(self):<br/>    ## Select a Q Value from the Base Q Network<br/>    QValue = self.QValue.eval(feed_dict = {self.iputVal:[self.currentState]})[0]<br/>    ## Initialize actions as zeros<br/>    action = np.zeros(self.action)<br/>    action_index = 0<br/>    ## If this timestep is the first, start with a random action<br/>    if self.timeStep % 1 == 0:<br/>      ##<br/>      if random.random() &lt;= self.starting_ep:<br/>        a_index = random.randrange(self.action)<br/>        action[a_index] = 1<br/>      else:<br/>        action_index = np.argmax(QValue)<br/>        action[action_index] = 1<br/>    else:<br/>      action[0] = 1</pre>
<p>We'll also adjust our epsilon value:</p>
<pre>## Anneal the value of epsilon<br/>    if self.starting_ep &gt; self.ending_ep and self.timeStep &gt; self.observe:<br/>      self.starting_ep -= (self.starting_ep - self.ending_ep) / self.explore</pre>
<p>Now that we've defined the bulk of our network, let's move on to training. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training methods</h1>
                </header>
            
            <article>
                
<p>First, let's define our training method. We'll call this function <kbd>trainingPipeline</kbd>; it will take in an action input as well as a <em>y</em> input which represents the targeting Q-value, which we'll define here as placeholders, and calculate a Q-value for an action based on those action/state pairs: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/4e5acb70-ef78-414f-a93e-e7c04e723f4c.png" style="width:5.92em;height:1.50em;" width="910" height="220"/></p>
<p>We'll use a <strong>Mean Squared Error</strong> (<strong>MSE</strong>) loss function and calculate it, utilizing the predicted Q-value minus the actual Q-value. Lastly, you might notice that we are using an optimizer here that you might not be familiar with, RMSProp. It's an adaptive learning rate optimizer similar to Adam that was proposed by Geoffrey Hinton. We won't ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training the network</h1>
                </header>
            
            <article>
                
<p>We'll give our training function a simple name: <kbd>train</kbd>. First, we'll feed it mini-batches of data from the replay memory:</p>
<pre>def train(self):<br/>    ''' Training procedure for the Q Network'''<br/><br/>    minibatch = random.sample(self.replayBuffer, 32)<br/>    stateBatch = [data[0] for data in minibatch]<br/>    actionBatch = [data[1] for data in minibatch]<br/>    rewardBatch = [data[2] for data in minibatch]<br/>    nextBatch = [data[3] for data in minibatch]</pre>
<p>Next, we'll calculate the Q-value for each batch:</p>
<pre>batch = []<br/>    qBatch = self.QValueT.eval(feed_dict = {self.inputValT: nextBatch})<br/>    for i in range(0, 32):<br/>      terminal = minibatch[i][4]<br/>      if terminal:<br/>        batch.append(rewardBatch[i])<br/>      else:<br/>        batch.append(rewardBatch[i] + self.gamma * np.max(qBatch[i]))</pre>
<p>Now, let's bind this all together with out training method. We'll take the <kbd>trainStep</kbd> variable that we defined and run the training cycle. We'll feed in three variables as input; the targeted Q-value, an action, and a state:</p>
<pre>self.trainStep.run(feed_dict={<br/>      self.yInput : batch,<br/>      self.actionInput : actionBatch,<br/>      self.inputVal : stateBatch<br/>      })</pre>
<p>We'll define a handler function to save network weights and states for us. While we didn't go over the definition of the saver explicitly in this chapter, you can find it in the fully assembled code in the GitHub repository:</p>
<pre>## Save the network on specific iterations<br/>    if self.timeStep % 10000 == 0:<br/>      self.saver.save(self.session, './savedweights' + '-atari', global_step = self.timeStep)</pre>
<p>Lastly, let's define the cycle the appends to experience replay:</p>
<pre>  def er_replay(self, nextObservation, action, reward, terminal):<br/>    newState = np.append(nextObservation, self.currentState[:,:,1:], axis = 2)<br/>    self.replayMemory.append((self.currentState, action, reward, newState, terminal))<br/>    if len(self.replayBuffer) &gt; 40000:<br/>      self.replayBuffer.popleft()<br/>    if self.timeStep &gt; self.explore:<br/>      self.trainQNetwork()<br/><br/>    self.currentState = newState<br/>    self.timeStep += 1</pre>
<p>We've assembled our network, so now let's move on to running it!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Running the network </h1>
                </header>
            
            <article>
                
<p>Now, let's get to the moment we've been waiting for! Let's import <kbd>gym</kbd>, NumPy, our deep-q network file, as well as a few handler functions: </p>
<pre>import cv2import sysfrom deepQ import deepQimport numpy as npimport gym</pre>
<p>We'll define our agent class as <kbd>Atari</kbd>, and initialize the environment, network, and actions with the class:</p>
<pre>class Atari:    def __init__(self):    self.env = gym.make('SpaceInvaders-v0')    self.env.reset()    self.actions = self.env.action_space.n    self.deepQ = deepQ(self.actions)    self.action0 = 0</pre>
<p>Our Deep Q-network can't innately ingest the Atari games, so we have to write a bit of preprocessing code to handle the video input. We'll call this function <kbd>preprocess</kbd> and it will take in a single game observation: </p>
<pre>def preprocess(self,observation): ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We've learned a great deal in this chapter, from how to implement MCTS methods to play board games, to creating an advanced network to play an Atari game, and even the technology behind the famous AlphaGo system. Let's recap what we have learned. </p>
<p>Reinforcement learning methods have become the main tools to create AIs for playing games. Whether we are creating systems for real-life board games, or systems for video games, the fundamental concepts of policies, Q-learning, and more that we learned about in <a href="91114074-444f-4201-98ef-e510210380f2.xhtml" target="_blank">Chapter 8</a>, <em>Reinforcement Learning</em>, form the basis for these complex AI systems. When we create AIs for board games, we rely on the building block of the game tree, and use MCTS to simulate various game outcomes from that game tree. For more advanced systems such as AlphaGo and chess-playing AIs, we utilize neural networks to help guide MCTS and make its simulations more effective.</p>
<p>When it comes to video game-playing AIs, we can utilize either policy gradient methods or Q-learning methods. In this chapter, we learned about utilizing a variant of the latter, deep Q-learning, to play the Atari game Space Invaders. Deep Q-learning makes advances on basic Q-learning by using techniques such as target networks and experience replay buffers to improve performance.</p>
<p>We'll look more into how reinforcement learning methods can create intelligent systems in one of our upcoming chapters on deep learning for robotics. </p>


            </article>

            
        </section>
    </div>



  </body></html>