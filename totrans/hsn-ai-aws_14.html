<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Sales Forecasting with Deep Learning and Auto Regression</h1>
                </header>
            
            <article>
                
<p>Demand forecasting is key to many industries such as airlines, retail, telecommunications, and healthcare. Inaccurate and imprecise demand forecasting leads to missed sales and customers, significantly impacting an organization's bottom line. One of the key challenges facing retailers is effectively managing inventory based on multiple internal and external factors. Inventory management is a complex business problem to solve—the demand for a product changes by location, weather, promotions, holidays, day of the week, special events, and other external factors, such as store demographics, consumer confidence, and unemployment. </p>
<p>In this chapter, we will look at <span>how traditional techniques of time series forecasting</span> <span>such as ARIMA and exponential smoothing are different from neural network-based techniques. We will also look at how DeepAR works, discussing its model architecture. </span></p>
<p>Following are the topics that will be covered in this chapter:</p>
<ul>
<li>Understanding traditional time series forecasting</li>
<li>Understanding how the DeepAR model works</li>
<li>Understanding model sales through DeepAR</li>
<li>Predicting and evaluating sales</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>For the following sections, we will employ the <a href="https://www.kaggle.com/manjeetsingh/retaildataset">retail</a> dataset containing sales of around 45 stores t<span>o illustrate how DeepAR predicts future sales given multiple factors such as holidays, promotions, and macro-economic indicators (unemployment).</span></p>
<p>In<span> </span>the<span> </span><a href="https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services">folder</a><span> </span>associated with this chapter, you will find three CSV files:</p>
<ul>
<li><strong>Features dataset</strong>: <span>This contains the data of r</span><span>egional activity related to the store.</span></li>
<li><strong>Sales <span>dataset</span></strong><span>: This contains historical sales data covering three years, from 2010 to 2012. It covers sales for 143 weeks.</span></li>
<li><strong>Store <span>dataset</span>:<em> </em></strong><span>This contains anonymized information about the 45 stores, including the type and size of the store.</span></li>
</ul>
<p>Please refer to the following link of GitHub for the source code of this chapter:</p>
<p><a href="https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services">https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services</a></p>
<p><span>It is now time to understand traditional time series forecasting techniques.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding traditional time series forecasting</h1>
                </header>
            
            <article>
                
<p>Let's begin by looking at traditional time series forecasting techniques, specifically ARIMA and exponential smoothing to model demand in simple use cases. We will look at how ARIMA estimates sales using historical sales and forecast errors. Also, we'll review how exponential smoothing accounts for irregularities in historical sales and captures trends and seasonality to forecast sales.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Auto-Regressive Integrated Moving Average (ARIMA )</h1>
                </header>
            
            <article>
                
<p>ARIMA is a time series analytical technique used to capture different temporal structures in univariate data. To model the time series data, differencing is applied across the series to make the data stationary. Differencing is the technique of <span>subtracting the previous data point from the current one for every data point excluding the first one. The technique makes the </span>mean and variance of the probability distribution of the time series constant over time, making future values of the series much more predictable. A specific number of lagged forecasts and forecast errors are used to model time series. This number is iteratively adjusted until the residuals are uncorrelated with the target (sales forecast) or all of the signals in the data have been picked up.</p>
<p>Let's unpack ARIMA to look at the underlying components—autoregressive, integrated, and moving average:</p>
<ul>
<li><strong>The number of autoregressive terms</strong>: <span>These establish a relationship between a specific number of historical data points and the current one that is, it uses historical demand to estimate the current demand.</span></li>
<li><strong>The number of non-seasonal differences</strong>: <span>These make temporal or time series data stationary by differencing. We're assuming that </span>future demand will look like historical demand if the difference in demand during the last few time steps is very small.</li>
<li><strong>The number of moving-average terms (lagged forecast errors</strong>): These a<span>ccount for forecast error—actual versus forecasted demand—or a specific number of historical data points.</span></li>
</ul>
<p>Let's look at the ARIMA equation, both in words and mathematical form:</p>
<p class="CDPAlignCenter CDPAlign"><em>Demand forecast = constant term + autoregressive terms + moving average<span> </span>terms</em></p>
<div class="CenterAlign"><img class="fm-editor-equation" src="assets/f5713d77-c78b-494d-b4f8-d86d38c16ad1.png" style="width:33.17em;height:1.50em;"/></div>
<p class="CDPAlignLeft CDPAlign">Here is a visual representation of how ARIMA works:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/33aca5fa-63ba-47e5-92e2-2233610d3166.png" style=""/></div>
<p>In the ARIMA model, the AR terms are positive, while the MA terms are negative; in other words, the autoregressive terms have a positive impact on demand while the moving-average of lagged errors has a negative impact.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exponential smoothing</h1>
                </header>
            
            <article>
                
<p>The other alternative to ARIMA is the exponential smoothing technique, which is also a time series forecasting method for univariate data, where random noise is neglected, revealing the underlying time structure. Although it is like ARIMA in that demand forecast is a weighted sum of past observations, the method of applying weights to lagged observations is different—instead of providing equal weights to past observations, the model employs exponentially decreasing weights for lags. In other words, the most recent observations are more relevant than historical ones. Exponential smoothing is used to make short-term forecasts, where we assume that future patterns and trends will look like current patterns and trends.</p>
<p>Following are the three types of exponential smoothing methods:</p>
<ul>
<li><strong>Single exponential smoothing</strong>: As the name indicates, the technique does not account for seasonality or trend. It requires a single parameter, alpha (<img class="fm-editor-equation" src="assets/9b3dc95b-fb71-42e8-a1be-ecafa0677063.png" style="width:1.00em;height:0.92em;"/>), to control the level of smoothing. Low alpha means there are no irregularities in the data, implying that the latest observations are given lower weight.</li>
<li><strong>Double exponential smoothing</strong>: This technique, on the other hand, supports trends in univariate series. In addition to controlling how important recent observations are relative to historical ones, an additional factor is used to control the influence of trend on demand forecasts. The trend can be either multiplicative or additive and is controlled using a smoothing factor, <img class="fm-editor-equation" src="assets/4d2ce8ab-d8fd-4828-9883-1e50e0960d31.png" style="width:0.58em;height:1.08em;"/>.</li>
<li><strong>Triple exponential smoothing</strong>: This one adds support for seasonality. Another new parameter, gamma (<img class="fm-editor-equation" src="assets/1d5a8cac-5805-4fd7-b1ef-bad2f80d0901.png" style="width:0.58em;height:0.92em;"/>), is used to control the influence of the seasonal component on demand forecasts.</li>
</ul>
<p>The following diagram illustrates the difference between the different<span> types of exponential smoothing:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/92261167-415b-47bb-a882-29ff7b30e1a6.png" style=""/></div>
<p>In the preceding diagram, we can see the following:</p>
<ul>
<li>Single exponential smoothing that forecasts demand at time, <em>t</em>, is based on estimated demand and forecast error (actual—estimated demand) at time, <em>t-1</em>.</li>
<li>In the case of double exponential smoothing, demand is forecasted by capturing both trend and historical data. We use two smoothing factors here, data and trend (here's a visual on how double exponential smoothing captures trends):</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e96e3a23-7a46-4c74-bf70-9ecd3f451aed.png" style=""/></div>
<ul>
<li class="mce-root">For triple exponential smoothing, we also account for seasonality through a third smoothing factor called a seasonal smoothing factor. See the following diagram, which captures seasonal peaks and troughs, along with trend:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img src="assets/30cb92f2-f89e-4b0f-ac64-25e3b97dd9a1.png" style=""/></div>
<p>The problem with this approach is that it views past sales as indicative of future sales. Besides, they are all forecasting techniques for univariate time series. As detailed earlier, there could be other factors that impact current and future sales, such as weather, promotions, day of the week, holidays, and special events.</p>
<p>Let's look at how the DeepAR model from SageMaker can be leveraged to model multi-variate time series, defining a non-linear relationship between an output variable (demand) and input variables (includes historical sales, promotions, weather, and time of the day.)</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How the DeepAR model works</h1>
                </header>
            
            <article>
                
<p>The DeepAR algorithm offered by Sagemaker is a generalized deep learning model that learns about demand across several related time series. Unlike traditional forecasting methods, in which an individual time series is modeled, DeepAR models thousands or millions of related time series.</p>
<p>Examples include forecasting load for servers in a data center, or forecasting demand for all products that a retailer offers, and energy consumption of individual households. The unique thing about this approach is that a substantial amount of data on past behavior of similar or related time series can be leveraged for forecasting an individual time series. This approach addresses over-fitting issues and time—and labor-intensive manual feature engineering and model selection steps required by traditional techniques.</p>
<p>DeepAR is a forecasting method based on autoregressive neural networks and it learns about a global model from historical data of all-time series in the data set. DeepAR employs <strong><span>Long Short-Term Memory</span></strong> (<strong><span>LSTM</span></strong>), a type of <strong>Recurrent Neural Network</strong> (<strong>RNN</strong>), to model time series. The main idea of RNNs is to capture sequential information. Unlike normal neural networks, the inputs (and outputs) are dependent on each other. RNNs hence have a memory that captures information about what has been estimated so far. The following is a diagram of an unfolded RNN—to remember what has been learned so far, at each step, the hidden state is computed, not only based on the <span>current input, but also the </span>previous hidden state:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/42eb4d1b-9929-46e3-8d68-79f721545547.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><em>A recurrent neural network and illustration of sequential learning as the time steps are unfolded. Source: Nature; Image Credits </em><em><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">WildML</a></em><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">.</a></div>
<p>Let's explain in more detail:</p>
<ul>
<li><img class="fm-editor-equation" src="assets/a87f6d79-2416-4f6f-97f2-29c700f669cb.png" style="width:0.67em;height:0.67em;"/> is input at a time, <em>t.</em></li>
<li><strong><sub><img class="fm-editor-equation" src="assets/993cee63-e3bb-4921-b07c-84c9ca961f8b.png" style="width:1.17em;height:1.25em;"/></sub></strong> is the hidden state at time, <em>t</em>. This state is computed based on previous hidden state and current input, and in <sub><img class="fm-editor-equation" src="assets/de230eb3-e03d-4922-a6c0-93e7b113f3fc.png" style="width:10.00em;height:1.33em;"/></sub>, function, <em>f</em>, is an activation function.</li>
<li><em><sub><img class="fm-editor-equation" src="assets/b5a3338a-e13a-4540-90a4-dbab88ba7dcb.png" style="width:1.42em;height:1.25em;"/></sub></em> is output at time, <em>t</em>, and <sub><img class="fm-editor-equation" src="assets/e8a8ae94-70b9-4b1c-9bf2-d62459b40b1e.png" style="width:5.83em;height:1.33em;"/></sub>. The activation function, <em>f</em>, can vary depending on the use case. For example, the softmax activation function is used when we need to predict which of the classes the input belongs to—in other words, whether the image being detected is a cat or a dog or a giraffe.</li>
<li>The network weights, <em>U</em>, <em>V,</em> and <em>W</em>, remain the same across all of the time steps.</li>
</ul>
<p>RNNs have interesting applications in different fields, such as the following:</p>
<ul>
<li><strong>Natural language processing</strong>: From generating image captions to generating text to machine translations, RNNs can act as generative models.</li>
<li><strong>Autonomous cars</strong>: They are used to conduct dynamic facial analysis.</li>
<li><strong>Time series</strong>: RNNs are used in econometrics (finance and trend monitoring) and for demand forecasting.</li>
</ul>
<p>However, general RNNs fail to learn long-term dependencies due to the gap between recent and older data. LSTMs, on the other hand, can solve this challenge: the inner cells of LSTMs can carry information unchanged through special structures called <strong>gates</strong>—input, forget, and output. Through these cells, LSTMs can control the information to be retained or erased.</p>
<p>Let's now look at the model architecture of DeepAR, an LSTM network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model architecture</h1>
                </header>
            
            <article>
                
<p>The DeepAR algorithm employs the LSTM network and probability models to identify non-linear structures in time series data and provide probabilistic estimates of forecasts.</p>
<p>The model is autoregressive in that it consumes observations from the last time step as input. It is also recurrent since it uses the previous output of the network as input at the next time step. During the training phase, the hidden or the encoded state of the network, at each time step, is computed based on current covariates, previous observation, and previous network output. The hidden state is then used to compute parameters for a probability model that characterizes the behavior of time series (product demand, for example).</p>
<p>In other words, we assume the demand to be a random variable following a specific probability distribution. Once we have the probability model that can be defined through a set of parameters (say, mean and variance), it can be used to estimate forecasts. DeepAR uses the Adam optimizer, a stochastic gradient descent algorithm, to optimize the maximum log likelihood of training data, given Gaussian model parameters. Using this approach, we can derive (optimize) both probability model parameters and LSTM parameters to accurately estimate forecasts.</p>
<p>The following diagram demonstrates how the DeepAR algorithm works:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/99f778cb-1c7a-435d-bdb3-51667927ba9c.png" style=""/></div>
<p><span>As shown in the preceding diagram, </span><strong>Maximum Likelihood Estimation</strong> (<strong>MLE</strong>) is used to estimate two sets of parameters, given all of the time series in the input dataset:</p>
<ul>
<li><strong>Parameters of RNN</strong>: These parameters or the hidden state of the RNN network are used to compute Gaussian parameters.</li>
<li><strong>Parameters of the Gaussian model</strong>: The Gaussian parameters are used to provide probabilistic estimates of forecasts.</li>
</ul>
<p><span>MLE is computed by leveraging data across all time series, <em>i</em>, where <em>i</em> goes from 1 to <em>N</em>—that is, there could be <em>N</em> different products the demand of which you're trying to estimate. <em>T</em> represents the length of the time series.</span></p>
<div class="packt_infobox"><span>For more information on MLE, refer to this </span><a href="https://www.analyticsvidhya.com/blog/2018/07/introductory-guide-maximum-likelihood-estimation-case-study-r/">article</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Arriving at optimal network weights </h1>
                </header>
            
            <article>
                
<p>The time series or observations are fed to DeepAR as part of the training. At each time step, current covariates, previous observations, and previous network output are used. The model uses <strong>Back Propagation Through Time</strong> (<strong>BPTT</strong>) to compute gradient descent after each iteration. In particular, the Adam optimizer is used to conduct BPTT. Through the stochastic gradient descent algorithm, Adam, we arrive at optimal network weights via back propagation.</p>
<p>At each time step, <em>t</em>, the inputs to the network are covariates, <img class="fm-editor-equation" src="assets/f1640cc4-07d1-4a1c-b5e6-2ef157a2a133.png" style="width:1.92em;height:1.25em;"/>; the target at the previous time step, <img class="fm-editor-equation" src="assets/f0d0aca4-9ef6-4a61-89f3-941b9a9a899a.png" style="width:2.67em;height:1.17em;"/>; as well as the previous network output, <img class="fm-editor-equation" src="assets/598bd999-812c-48d8-b09d-5132c687bf0d.png" style="width:2.58em;height:1.33em;"/>. The network output, <img class="fm-editor-equation" src="assets/b09d5fc7-6d40-4b8b-b705-97d4249f181e.png" style="width:11.33em;height:1.50em;"/>, is then used to compute Gaussian parameters that maximize the probability of observing the input dataset.</p>
<p>The following visual illustrates sequence-to-sequence learning, where the encoder encapsulates demand patterns in the historical time series and sends the same (<img class="fm-editor-equation" src="assets/4b65cef4-67fb-4548-a977-78c387d12d11.png" style="width:3.50em;height:1.50em;"/>) as input to the decoder. The function of the decoder is to predict demand, taking into consideration the input from encoder:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0321fea6-ad86-42b5-af68-a1a7bd55d6b9.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><em>Source: Probabilistic Forecasting with Autoregressive Recurrent Networks (</em><em><a href="https://arxiv.org/abs/1704.04110">link</a></em><em>)</em></div>
<p>For prediction, the history of the time series, <img class="fm-editor-equation" src="assets/5bab8f64-cf7a-49a7-be0f-4b67c2502ad5.png" style="width:1.92em;height:1.33em;"/>, is fed in for <img class="fm-editor-equation" src="assets/a1e275cf-a586-4b6b-8e2f-2645f4bc3acb.png" style="width:2.83em;height:1.00em;"/>, and, then, in the prediction range for <img class="fm-editor-equation" src="assets/29f766f6-f2a3-4eab-8f83-85d79dd9ca75.png" style="width:3.50em;height:1.00em;"/>, a sample <img class="fm-editor-equation" src="assets/f081231c-3bf5-41f7-b212-0265496b27ef.png" style="width:2.17em;height:1.92em;"/> is drawn and fed back for the next point until the end of the prediction range, <img class="fm-editor-equation" src="assets/cda36150-c0e7-4c79-a21d-d309947330f5.png" style="width:4.25em;height:1.00em;"/>.</p>
<p>DeepAR produces accurate forecast distributions learned from historical behavior of all of the time series jointly. Also, probabilistic forecasts provide optimal decisions under uncertainty versus point estimates.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding model sales through DeepAR</h1>
                </header>
            
            <article>
                
<p>As noted in the introduction to this chapter, managing inventory <span>for retailers is</span> a complex activity to handle. Holidays, special events, and markdowns can have a significant impact on how a store performs and, in turn, how a department within a store performs.</p>
<p>The Kaggle <span class="MsoHyperlink"><a href="https://www.kaggle.com/manjeetsingh/retaildataset">dataset</a></span> contains historical sales for 45 stores, with each store belonging to a specific type (location and performance) and size. The retailer runs several promotional markdowns throughout the year. These markdowns precede holidays, such as SuperBowl, Labor Day, Thanksgiving, and Christmas.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Brief description of the dataset</h1>
                </header>
            
            <article>
                
<p>Let's briefly consider the dataset that we are about to model:</p>
<ul>
<li><span><strong>Features data:</strong><em> </em>This is data of r</span>egional activity related to the store:
<ul>
<li><strong>Store</strong>: Numeric store ID for each store.</li>
<li><strong>Date</strong>: Important dates for store.</li>
<li><strong>Fuel price</strong>: Current fuel prices.</li>
<li><strong>Markdowns</strong>: The discount you take on merchandise in your retail store from the original marked <span>sale price.</span></li>
<li><strong>CPI</strong> (<strong>Consumer Price Index</strong>): A measure that examines the weighted average of prices of a basket of consumer goods and services, such as transportation, food, and medical care.</li>
<li><strong>Unemployment</strong>: Current unemployment rate.</li>
<li><strong>IsHoliday</strong>: Whether it's a holiday or not, on a particular date.</li>
</ul>
</li>
<li><strong>Sales data</strong>: This is historical sales data covering three years, from 2010 to 2012. It covers sales for 143 weeks:
<ul>
<li><strong>Store</strong>: Numeric store ID for each store.</li>
<li><strong>Dept</strong>: Numeric department ID for each department of the store.</li>
<li><strong>Date</strong>: Important dates for the store.</li>
<li><strong>Weekly sales</strong>: Weekly sales to measure the sales performance of each store.</li>
<li><strong>IsHoliday</strong>: Is it a holiday or not on a particular date.</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Store data:<em> </em></strong>This is anonymized information about the 45 stores, including the type and size of the store:
<ul>
<li><strong>Store</strong>: Numeric store ID for each store.</li>
<li><strong>Type</strong>: The type of store.</li>
<li><strong>Size</strong>: The size of the store.</li>
</ul>
</li>
<li><strong>Model Input and Output:</strong> Now let's look at the input and output formats including the hyperparameters of the SageMaker DeepAR algorithm.</li>
</ul>
<p>The algorithm has two input channels and it takes training and test JSONs as input through two channels. The training JSON contains only 134 weeks of sales, while the test JSON contains sales from all 143 weeks.</p>
<p>The following is the structure of the training JSON:</p>
<pre style="padding-left: 60px"><strong>Training JSON<br/></strong>{<br/>Start: The starting date of weekly sales<br/>Target: Weekly sales<br/>Cat: Category or Department used to group sales<br/>Dynamic_feat: Dynamic features used to explain variation in sales. Beyond holidays, these features can include price, promotion and other covariates.<br/>}<br/>{"<strong>start</strong>":"2010-01-01 00:00:00","<strong>target</strong>":[19145.49, 17743.27, 14700.85, 20092.86, 17884.43, 19269.09, 22988.12, 17679.72, 16876.61, 14539.77, 16026.23, 14249.85, 15474.07, 22464.57, 19075.56, 20999.38, 18139.89, 13496.23, 15361.65, 16164.48, 15039.44, 14077.75, 16733.58, 16552.23, 17393.2, 16608.36, 21183.71, 16089.01, 18076.54, 19378.51, 15001.62, 14691.15, 19127.39, 17968.37, 20380.96, 29874.28, 19240.27, 17462.27, 17327.15, 16313.51, 20978.94, 28561.95, 19232.34, 20396.46, 21052.61, 30278.47, 47913.44, 17054.1, 15355.95, 15704.19, 15193.36, 14040.86, 13720.49, 17758.99, 24013.25, 24157.54, 22574.19, 12911.72, 20266.06, 18102.13, 21749.04, 22252.73, 21672.82, 15231.31, 16781.35, 14919.64, 15948.11, 17263.32, 16859.26, 13326.75, 17929.47, 15888.17, 13827.35, 16180.46, 22720.76, 15347.18, 15089.43, 14016.56, 17147.61, 14301.9, 16951.62, 16623.8, 19349.35, 24535.59, 18402.46, 19320.64, 20048.28, 14622.65, 19402.27, 19657.79, 18587.11, 20878.24, 19686.7, 23664.29, 20825.85, 27059.08, 15693.12, 29177.6, 45362.67, 20011.27, 13499.62, 15187.32, 16988.52, 14707.59, 20127.86, 23249.25, 20804.15, 19921.62, 16096.04, 18055.34, 17727.24, 16478.45, 16117.33, 15082.89, 15050.07, 17302.59, 20399.83, 17484.31, 14056.35, 16979.18, 17279.4, 14494.48, 14661.37, 13979.33, 13476.7, 18898.57, 13740.2, 15684.97, 15266.29, 16321.69, 15728.07, 17429.51, 17514.05, 20629.24], <br/>"<strong>cat</strong>":[15], "<strong>dynamic_feat</strong>":[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]}</pre>
<p>In the preceding structure, we can see the following:</p>
<ul>
<li><kbd>start</kbd>: Is the start date of weekly sales.</li>
<li><kbd>target</kbd>: Is for sorted weekly sales.</li>
<li><kbd>cat</kbd>: Is the category to group time series.</li>
<li><kbd>Dynamic_feat</kbd>: Includes the dynamic features to account for factors impacting sales such as holidays.</li>
</ul>
<p>The test JSON also has the same format as that of the training JSON. Let's have a look at the following code:</p>
<pre style="padding-left: 60px"><strong>Test JSON<br/></strong>{"<strong>start</strong>":"2010-01-01 00:00:00","<strong>target</strong>":[19145.49, 17743.27, 14700.85, 20092.86, 17884.43, 19269.09, 22988.12, 17679.72, 16876.61, 14539.77, 16026.23, 14249.85, 15474.07, 22464.57, 19075.56, 20999.38, 18139.89, 13496.23, 15361.65, 16164.48, 15039.44, 14077.75, 16733.58, 16552.23, 17393.2, 16608.36, 21183.71, 16089.01, 18076.54, 19378.51, 15001.62, 14691.15, 19127.39, 17968.37, 20380.96, 29874.28, 19240.27, 17462.27, 17327.15, 16313.51, 20978.94, 28561.95, 19232.34, 20396.46, 21052.61, 30278.47, 47913.44, 17054.1, 15355.95, 15704.19, 15193.36, 14040.86, 13720.49, 17758.99, 24013.25, 24157.54, 22574.19, 12911.72, 20266.06, 18102.13, 21749.04, 22252.73, 21672.82, 15231.31, 16781.35, 14919.64, 15948.11, 17263.32, 16859.26, 13326.75, 17929.47, 15888.17, 13827.35, 16180.46, 22720.76, 15347.18, 15089.43, 14016.56, 17147.61, 14301.9, 16951.62, 16623.8, 19349.35, 24535.59, 18402.46, 19320.64, 20048.28, 14622.65, 19402.27, 19657.79, 18587.11, 20878.24, 19686.7, 23664.29, 20825.85, 27059.08, 15693.12, 29177.6, 45362.67, 20011.27, 13499.62, 15187.32, 16988.52, 14707.59, 20127.86, 23249.25, 20804.15, 19921.62, 16096.04, 18055.34, 17727.24, 16478.45, 16117.33, 15082.89, 15050.07, 17302.59, 20399.83, 17484.31, 14056.35, 16979.18, 17279.4, 14494.48, 14661.37, 13979.33, 13476.7, 18898.57, 13740.2, 15684.97, 15266.29, 16321.69, 15728.07, 17429.51, 17514.05, 20629.24, 17730.73, 18966.48, 20781.46, 22979.73, 16402.34, 20037.44, 18535.65, 16809.01, 19275.43], "<strong>cat</strong>":[15], "<strong>dynamic_feat</strong>":[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]</pre>
<p>DeepAR supports a range of hyperparameters. Following, is a list of some of the key hyperparameters. For a detailed list, check out Amazon documentation <span class="MsoHyperlink"><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html">here</a></span>:</p>
<ul>
<li><strong>Time frequency</strong>: Indicates whether the time series is hourly, weekly, monthly, or yearly.</li>
<li><strong>Context length</strong>: How many time steps in the past the algorithm should look at for training.</li>
<li><strong>Prediction length</strong>: The number of data points to predict.</li>
<li><strong>Number of cells</strong>: The number of neurons to use in each of the hidden layers.</li>
<li><strong>Number of layers</strong>: The number of hidden layers.</li>
<li><strong>Likelihood function</strong>: We will choose the Gaussian model since weekly sales are real values.</li>
<li><strong>epochs</strong>: The maximum number of passes over the training data.</li>
<li><strong>Mini batch size</strong>: The size of the mini-batches used during training.</li>
<li><strong>Learning rate</strong>: The pace at which the loss is optimized.</li>
<li><strong>Dropout rate</strong>: For each epoch, the percentage of hidden neurons that are not updated.</li>
<li><strong>Early stopping patience</strong>: The training stops after a designated number of unsuccessful epochs, those in which the loss doesn't improve.</li>
<li><strong>Inference</strong>:<strong> </strong><span>For a given department, we sent 134 weeks of historical sales, along with the department category and holiday flag across all of the weeks.</span></li>
</ul>
<p><span>Following, is a sample JSON output from the model endpoint. Because DeepAR produces probabilistic forecasts, the output contains several sales samples from the Gaussian distribution. Mean and quantiles (50% and 90%) of these samples are also reported, as shown in the following:</span></p>
<pre>{<br/>   "predictions": [<br/>       {<br/>           "quantiles": { <br/>               "0.9": [...],<br/>               "0.5": [...]<br/>           },<br/>           "samples": [...],<br/>           "mean": [...]<br/>       }<br/>   ]<br/>}</pre>
<p>We have just reviewed sample input and output of the DeepAR algorithm for items with historical weekly sales. </p>
<p>DeepAR also offers unique capabilities that account for complexities in real-world time series problems. For new items or products, the length of time series is going to be shorter than that of regular items that have full sales history. DeepAR captures the distance to the first observation for new items or products. Because the algorithm learns item demand across multiple time series, it can estimate demand even for newly introduced items—the length of weekly sales across all time series need not remain the same. Additionally, the algorithm can also handle missing values, with missing values replaced with "Nan".</p>
<p><span>The following screenshot is a visual representation of the variety of inputs and output of DeepAR:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5279d84f-a5f5-47a3-b9b3-ce0fa1a0b37c.png" style=""/></div>
<p><span>As shown in the preceding, the probabilistic forecasts of weekly sales can be produced by modeling historical weekly sales (<span class="packt_screen">Sales Time Series</span>) across all new (<span class="packt_screen">Age</span>) and regular items, along with taking into input item category <span class="packt_screen">(Item Embedding</span>) and other features (<span class="packt_screen">Price</span> and <span class="packt_screen">Promotion</span>). </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploratory data analysis</h1>
                </header>
            
            <article>
                
<p>Although there are 45 stores, we will select one store, store number 20, to analyze performance across different departments across three years. The main idea here is that, using DeepAR, we can learn the sales of items across different departments.</p>
<p>In SageMaker, through Lifecycle <span class="packt_screen">Configurations</span>, we can custom install Python packages before notebook instances are started. This eliminates the need to manually track packages required before the notebooks are executed.</p>
<p>For exploring the retail sales data, we will need the latest version, 0.9.0, of <kbd>seaborn</kbd> installed.</p>
<p>In SageMaker, under <span class="packt_screen">Notebook</span>, click on <span class="packt_screen">Lifecycle Configurations</span>:</p>
<ol>
<li>Under <span class="packt_screen">Start notebook</span>, enter the command to upgrade the <kbd>seaborn</kbd> Python package, as shown:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/18891f22-8ceb-4aba-bc86-973064a16de3.png" style=""/></div>
<ol start="2">
<li>Edit the notebook settings by clicking on the notebook instance, selecting <span class="packt_screen">Actions</span>, and picking <span class="packt_screen">Update Settings</span>.</li>
<li>Under the <span class="packt_screen">Update Settings a Lifecycle configuration</span> section, select the name of the newly created Lifecycle <span class="packt_screen">configuration</span>.</li>
</ol>
<p>This option enables SageMaker to manage all Python pre-requisites before the notebook instances are made available, as shown:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0fe80c03-6ce2-40b4-b56e-073781fb8a1a.png" style=""/></div>
<p>Let's merge the data across the sales, store, and features CSV files:</p>
<ol>
<li>We will import the key Python libraries, a<span>s shown in the following</span>:</li>
</ol>
<pre style="padding-left: 60px">import numpy #library to compute linear algebraic equations<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</pre>
<ol start="2">
<li>Let's read the <kbd>.csv</kbd> files into Python DataFrames, as shown:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">features = pd.read_csv('Features data set.csv')<br/>sales = pd.read_csv('sales data-set.csv')<br/>stores = pd.read_csv('stores data-set.csv')</pre>
<ol start="3">
<li>Let's look at the shape of each of the <span>DataFrames </span>created, as in the <span>following:</span></li>
</ol>
<pre style="padding-left: 60px">features.shape #There are 8,190 store, date and holiday combinations<br/>sales.shape #There are 421,570 sales transactions<br/>stores.shape #There are 45 stores in question</pre>
<ol start="4">
<li>Now, merge the <kbd>features</kbd> DataFrame with <kbd>sales</kbd> and <kbd>stores</kbd> to create one <span>DataFrame </span>containing all of the required information, as shown:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">merged_df = features.merge(sales, on=['Store', 'Date', 'IsHoliday']).merge(stores, on=['Store'])<br/>merged_df.head()</pre>
<ol start="5">
<li>Convert <kbd>IsHoliday</kbd> into numerical form and convert the <kbd>Date</kbd> field into the <kbd>pandas</kbd> date format, as in the <span>following:</span></li>
</ol>
<pre style="padding-left: 60px">merged_df = features.merge(sales, on=['Store', 'Date', 'IsHoliday']).merge(stores, on=['Store'])<br/>merged_df.head()</pre>
<ol start="6">
<li>Write merged dataset to <kbd>.csv</kbd> with the help of the <span>following </span>code:</li>
</ol>
<pre style="padding-left: 60px">merged_df.to_csv('retailsales.csv')</pre>
<ol start="7">
<li>Now, let's look at the distribution of each of the key factors (<kbd>Temperature</kbd>, <kbd>Fuel_Price</kbd>, <kbd>Unemployment</kbd>, and <kbd>CPI</kbd>) that may impact sales, as shown<span>:</span></li>
</ol>
<pre style="padding-left: 60px">#Create a figure and a set of subplots<br/>f, ax = plt.subplots(4, figsize=(15, 15)) #f=figure; ax=axes<br/>sns.distplot(merged_df.Temperature, ax=ax[0])<br/>sns.distplot(merged_df.Fuel_Price, ax=ax[1])<br/>sns.distplot(merged_df.Unemployment, ax=ax[2])<br/>sns.distplot(merged_df.CPI, ax=ax[3])</pre>
<p>We use the <kbd>seaborn</kbd> Python library to plot the distribution of <kbd><span>Temperature</span></kbd>, <kbd><span>Fuel_Price</span></kbd>, <kbd><span>Unemployment</span></kbd>, and <kbd><span>CPI </span></kbd>in the dataset. Let's have a look at the following output:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2e79d837-9b0b-4a1a-a47e-450670f67576.png" style=""/></div>
<p>As can be seen from the preceding distributions, the temperature is mostly between 60 to 80 degrees when the sales happened. Also, fuel prices were around $2.75 and $3.75 during the majority of the sales activity:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/50c88a7c-8426-42e7-b808-58eeb3ca6090.png" style=""/></div>
<p>From the preceding visual, unemployment rate was between 6% and 9% during the majority of the sales activity. As for CPI, sales activity occurred during both low and high CPI levels.</p>
<p>Now that we have looked at the distribution of each of the key features, let's see how they are correlated to weekly sales:</p>
<ol>
<li>First, let's look at the scatter plot between sales (target) and each of the explanatory variables— <kbd>Holidays</kbd>, <kbd>Temperature</kbd>, <kbd>CPI</kbd>, <kbd>Unemployment</kbd>, and <kbd>Store Type</kbd>:</li>
</ol>
<pre style="padding-left: 60px">f, ax = plt.subplots(6, figsize=(20,20))<br/>sns.scatterplot(x="Fuel_Price", y="Weekly_Sales", data=merged_df, ax=ax[0])<br/>sns.scatterplot(x="Temperature", y="Weekly_Sales", data=merged_df, ax=ax[1])<br/></pre>
<p style="padding-left: 60px">In the preceding code, we've plotted a scatterplot between sales and fuel price and sales and temperature. Let's analyze how fuel price and temperature are related to sales:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a3a863f8-ee4a-4592-a7e3-70b6a3fe6a58.png" style=""/></div>
<ol start="2">
<li class="mce-root">It is evident from the preceding visual that the fuel price between $3.25 and $3.75 is generating higher weekly sales. Also, a temperature between 50 and 65 degrees is generating higher weekly sales.</li>
</ol>
<p style="padding-left: 60px">Let's now plot holiday or not and CPI against sales, as shown in the following code:</p>
<pre style="padding-left: 60px">sns.scatterplot(x="IsHoliday", y="Weekly_Sales", data=merged_df, ax=ax[2])<br/>sns.scatterplot(x="CPI", y="Weekly_Sales", data=merged_df, ax=ax[3])</pre>
<p style="padding-left: 60px">Let's look at how sales vary with a <span>holiday or not and CPI in the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cba77cff-6337-4dbd-a0ac-5e2fff453593.png" style=""/></div>
<ol start="3">
<li class="mce-root">It seems that holiday sales are higher than non-holiday sales. Also, there appears to be no material impact of CPI on weekly sales.</li>
</ol>
<p style="padding-left: 60px"><span>Let's now plot <kbd>Unemployment</kbd> and <kbd>Store Type</kbd> against sales, as shown in the following code:</span></p>
<pre style="padding-left: 60px">sns.scatterplot(x="Unemployment", y="Weekly_Sales", data=merged_df, ax=ax[4])<br/>sns.scatterplot(x="Type", y="Weekly_Sales", data=merged_df, ax=ax[5])</pre>
<p style="padding-left: 60px">Let's see how sales vary with unemployment and store type in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3f4b38ce-74b3-403d-b16a-108a48bf2fb4.png" style=""/></div>
<p class="mce-root" style="padding-left: 60px">From the preceding visual, weekly sales appear to be higher when unemployment rate is lower (7 to 8.5) and B type stores seem to have higher weekly sales.</p>
<ol start="4">
<li>Second, let's look at a heatmap across all of the features to identify what features impact sales. Let's draw a heatmap to see correlations between sales and several sales predictors all in one go.</li>
</ol>
<p style="padding-left: 60px">The following screenshot is a heatmap of numerical attributes in the dataset—we drop store and department from the dataset since they are categorical variables:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f3d3ab63-2650-4b62-a1fb-12341db2f89b.png" style=""/></div>
<p>From the scatter plot and heat map, the following is apparent:</p>
<ul>
<li>Markdowns are happening during holidays.</li>
<li>Sales are higher during holidays.</li>
<li>Type B stores generate higher sales.</li>
<li>Lower fuel prices (between $3 and $3.75) generate higher sales.</li>
<li>The ideal temperature (between 50 and 65 degrees) generates higher sales.</li>
</ul>
<p>For our further modeling, we will pick the best performing store, store 20, to model sales across different departments and years. For each of the time steps in the time series, we will also pass whether a particular day was observed as a holiday or not.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data pre-processing</h1>
                </header>
            
            <article>
                
<p>Let's begin with preparing the dataset for modeling:</p>
<ul>
<li>Create a module named <kbd>retailsales.py</kbd> to create JSON files that DeepAR can consume for training and validation.</li>
<li>Create a module named <kbd>salesinference.py</kbd> to build inference data and retrieve and plot predictions.</li>
</ul>
<p>For details on the modules, please refer to the source code associated with this chapter.</p>
<p>To modularize the code to test DeepAR, we will package the two modules, <kbd>retailsales</kbd> and <kbd>salesinference</kbd>. To package the modules, we will create the <kbd>__init__.py</kbd> file to import the modules. We will then create <kbd>setup.py</kbd>, detailing the pre-requisite packages to be installed.</p>
<p>Following, is the folder structure of the DeepAR project:</p>
<pre>DeepAR project structure.<br/>Project Organization<br/>------------<br/>    ├── notebooks/            &lt;- All notebooks are residing here.<br/>    ├── data/                 &lt;- Input data is residing here<br/>    ├── deepar/               &lt;- Python package with source code of this project.<br/>      ├──retailsales.py       &lt;- Creating training and testing datasets for DeepAR.<br/>      ├──salesinference.py    &lt;- Preparing data for predictions, obtaining and plotting predictions from DeepAR<br/>    ├── README.md             &lt;- The top-level README for developers using this project.<br/>    ├── setup.py              &lt;- Defines pre-requisite packages to install and distribute package.</pre>
<p>Let's take a look at the following steps:</p>
<ol>
<li>In<span> </span><kbd>setup.py</kbd>, we will define pre-requisite packages to be installed:</li>
</ol>
<pre style="padding-left: 60px">import os<br/>from setuptools import setup, find_packages<br/><br/>def read(fname):<br/>    return open(os.path.join(os.path.dirname(__file__), fname)).read()<br/><br/>setup(<br/>    name="deepar",<br/>    description="DeepAR project structure.",<br/>    author="&lt;your-name&gt;",<br/>    packages=find_packages(exclude=['data', 'figures', 'output', 'notebooks']),\<br/>    long_description=read('README.md'),<br/>)<strong> <br/></strong></pre>
<ol start="2">
<li>In <kbd>_init_.py</kbd>, we will import the modules, <kbd>retailsales</kbd> and <kbd>salesinference</kbd>, defined earlier:</li>
</ol>
<pre style="padding-left: 60px">from . import retailsales<br/>from . import salesinference</pre>
<ol start="3">
<li>We will now install the package for the modules to be available while training DeepAR:</li>
</ol>
<pre style="padding-left: 60px">#Navidate to deep-ar directory to install the deepar package containing commonly used functions<br/>path = ".."<br/>os.chdir(path)<br/><br/>#install predefined functions<br/>!pip install .<br/><br/>#Navigate to the parent directory to train the DeepAR model<br/># org_path = ".."<br/># os.chdir(org_path)</pre>
<p class="mce-root">We are now ready with all of the packages required to pre-process weekly sales data. Pre-processing included not only converting categorical data into numerical, but also creating training and testing data in JSON formats required by the DeepAR algorithm. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training DeepAR</h1>
                </header>
            
            <article>
                
<p>In this section, we will fit DeepAR to the weekly sales. Let's start by preparing<em> </em>training and test datasets in JSON format<em>.</em></p>
<p>Let's have a look at the following code, which demonstrates the creation of <kbd>json</kbd> lines:</p>
<pre>import deepar as da<br/><br/>train_key      = 'deepar_sales_training.json'<br/>test_key       = 'deepar_sales_test.json'<br/>#Prediction and context length for training the DeepAR model<br/>prediction_length = 9<br/><br/>salesfn = 'data/store20_sales.csv'<br/>salesdf = da.retailsales.prepareSalesData(salesfn)<br/>testSet = da.retailsales.getTestSales(salesdf, test_key)<br/>trainingSet = da.retailsales.getTrainSales(salesdf, train_key, prediction_length)</pre>
<p><span>In the preceding code block, we have created JSON lines for training and testing the datasets:</span></p>
<ul>
<li>The <kbd>prepareSalesData()</kbd> function is used to select departments with sales across all of the 143 weeks. This step is to ensure that there were no missing values in the data. Although DeepAR can handle missing values, we've tried to make the problem less complex by only considering departments that have sales in almost all weeks.</li>
<li>We use department numbers to group or categorize the time series for the DeepAR algorithm. This grouping will be used by DeepAR to make demand predictions by department.</li>
<li><span>The</span> <kbd>getTestSales()</kbd> <span>function is used to create JSON lines for the </span>testing<span> dataset. </span></li>
<li>The <kbd>getTrainSales()</kbd> function, on the other hand, is used to create JSON lines for the training dataset, which is a subset of<span> the </span>testing<span> </span>dataset. For each of the departments, we will chop the last nine weekly sales determined by<span> </span>prediction length.</li>
</ul>
<p>Now, we will look at uploading the <kbd>json</kbd> files to the S3 bucket, as shown in the following code:</p>
<pre>bucket         = 'ai-in-aws'<br/>prefix         = 'sagemaker/deepar-weekly-sales'<br/><br/>train_prefix   = '{}/{}'.format(prefix, 'train')<br/>test_prefix    = '{}/{}'.format(prefix, 'test')<br/>output_prefix  = '{}/{}'.format(prefix, 'output')<br/><br/>sagemaker_session = sagemaker.Session()<br/><br/>train_path = sagemaker_session.upload_data(train_key, bucket=bucket, key_prefix=train_prefix)<br/>test_path = sagemaker_session.upload_data(test_key, bucket=bucket, key_prefix=test_prefix)</pre>
<p><span>In the preceding code, the newly created <kbd>json</kbd> files are uploaded to the designated S3 bucket via the <kbd>upload_data()</kbd> function from the Sagemaker session object (Sagemaker Python SDK).</span></p>
<p><span>We will be obtaining the URI of the DeepAR Docker image with the help of the following code:</span></p>
<pre>role = get_execution_role()<br/>output_path = r's3://{0}/{1}'.format(bucket, output_prefix)<br/><br/>container = get_image_uri(boto3.Session().region_name, 'forecasting-deepar')<br/><br/>deepAR = sagemaker.estimator.Estimator(container,<br/>                                   role,<br/>                                   train_instance_count=1,<br/>                                   train_instance_type='ml.c4.xlarge',<br/>                                   output_path=output_path,<br/>                                   sagemaker_session=sagemaker_session)</pre>
<p>In the preceding code block, we can see the following:</p>
<ul>
<li>The <kbd>get_image_uri()</kbd> function from the SageMaker estimator object is used to obtain <kbd>uri</kbd> of the DeepAR Docker image.</li>
<li>Once <kbd>uri</kbd> is obtained, the DeepAR estimator is created.</li>
<li>The constructor parameters include the Docker image <kbd>uri</kbd>, execution role, training instance type and count, and <kbd>outpath</kbd> path to save the trained algorithm and SageMaker session.</li>
</ul>
<p><span>Hyperparameters are used to configure the learning or training process. Let's have a look at <kbd>hyperparameters</kbd> used in the following code:</span></p>
<pre>hyperparameters = {<br/>    "time_freq": 'W',<br/>    "context_length": prediction_length, <br/>    "prediction_length": prediction_length,<br/>    "num_cells": "40", <br/>    "num_layers": "2", <br/>    "likelihood": "gaussian",<br/>    "epochs": "300", <br/>    "mini_batch_size": "32", <br/>    "learning_rate": "0.00001",<br/>    "dropout_rate": "0.05", <br/>    "early_stopping_patience": "10" <br/>}<br/>deepAR.set_hyperparameters(**hyperparameters) </pre>
<p>In the preceding code, we came across the following hyperparameters:</p>
<ul>
<li><kbd>learning_rate</kbd>: Defines how fast the weights are updated during training.</li>
<li><span><kbd>dropout_rate</kbd>: To avoid overfitting, for each iteration, a random subset of hidden neurons are not updated.</span></li>
<li><span><kbd>num_cells</kbd>: Defines the number of cells to use in each of the hidden layers.</span></li>
<li><span><kbd>num_layers</kbd>: Defines the number of hidden layers in the RNN.</span></li>
<li><kbd>time_freq</kbd>: Defines the frequency of time series.</li>
<li><kbd>epochs</kbd>: Defines the maximum  number of passes over the training data.</li>
<li><kbd>context_length</kbd>: Defines look back period—how many data points are we going to look at before predicting.</li>
<li><kbd>prediction_length</kbd>: Defines the number of data points to predict. </li>
<li><kbd>mini_batch_size</kbd>: Defines how often weights are updated—that is, weights are updated after processing the designated number of data points.</li>
</ul>
<p><span>In the following </span>code<span>, we fit <kbd>deepAR</kbd> to the training dataset:</span></p>
<pre>data_channels = {"train": train_path, "test": test_path}\<br/>deepAR.fit(inputs=data_channels)</pre>
<p><span>In the preceding code, we can see the following:</span></p>
<ul>
<li><span>We passed the location of the training and testing JSONs on the S3 bucket.</span></li>
<li><span>The testing dataset is used to evaluate the performance of the model.</span></li>
<li><span>For training, we called the <kbd>fit()</kbd> function on the DeepAR estimator.</span></li>
</ul>
<p>Following is the output from training DeepAR:</p>
<pre class="mce-root">#test_score (algo-1, RMSE): 7307.12501604<br/>#test_score (algo-1, mean_wQuantileLoss): 0.198078<br/>#test_score (algo-1, wQuantileLoss[0.1]): 0.172473<br/>#test_score (algo-1, wQuantileLoss[0.2]): 0.236177<br/>#test_score (algo-1, wQuantileLoss[0.3]): 0.236742<br/>#test_score (algo-1, wQuantileLoss[0.4]): 0.190065<br/>#test_score (algo-1, wQuantileLoss[0.5]): 0.1485<br/>#test_score (algo-1, wQuantileLoss[0.6]): 0.178847<br/>#test_score (algo-1, wQuantileLoss[0.7]): 0.223082<br/>#test_score (algo-1, wQuantileLoss[0.8]): 0.226312<br/>#test_score (algo-1, wQuantileLoss[0.9]): 0.170508</pre>
<p><span>As it is seen in the preceding output, the <strong>Root Mean Squared Error</strong> (<strong>RMSE </strong>) is used as a metric to pick the best performing model.</span></p>
<p class="mce-root">We have successfully trained the DeepAR model on our training dataset, which had 134 weekly sales. To fit training data to the model, we have defined the location of training and testing JSONs on the S3 bucket. Also, we've defined hyperparameters to control the learning or fitting process. The best performing model (based on the lowest RMSE—in that predicted sales are close as possible to actual sales) is then persisted.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predicting and evaluating sales</h1>
                </header>
            
            <article>
                
<p>In this section, <span>the trained model will be deployed, so that we can predict weekly sales for the next nine weeks for a given department.  </span></p>
<p>Let's have a look at the following code :</p>
<pre>deepAR_predictor = deepAR.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')</pre>
<p><span>In the preceding code, the <kbd>deploy</kbd> function of the <kbd>deepAR</kbd> estimator is used to host the model as an endpoint. The number and type of hosting instances should be specified through the following parameters :</span></p>
<ul>
<li><kbd><span>initial_instance_count</span></kbd></li>
<li><kbd><span>instance_type</span></kbd></li>
</ul>
<p>To assess the model performance, we use department number 90, as shown in the following code:</p>
<pre>#Predict last 9 weeks of a department and compare to ground truth<br/><br/>deepAR_predictor.content_type = 'application/json'<br/>dept = 90<br/><br/>prediction_data = da.salesinference.buildInferenceData(dept, trainingSet, testSet)<br/>#print(prediction_data)<br/>result = deepAR_predictor.predict(prediction_data)<br/><br/>y_mean, y_q1, y_q2, y_sample = da.salesinference.getInferenceSeries(result)<br/>print("Predicted Sales: ", y_mean)<br/>print("Actual Sales: ", list(testSet[dept]['Weekly_Sales'][134:]))<br/><br/>da.salesinference.plotResults(prediction_length, result, truth=True, truth_data=testSet[dept]['Weekly_Sales'][134:], truth_label='truth')</pre>
<p>In the preceding code, we can see the following:</p>
<ul>
<li>The <kbd>buildInferencedata()</kbd> function is used to prepare the time series data in JSON format<span>. We build inference data, by a given department,  listing holidays across the entire 143 weeks, weekly sales for 134 weeks, and corresponding item category. The goal here is to estimate sales in the last nine weeks, where <kbd>9</kbd> is the prediction length.</span></li>
</ul>
<p style="padding-left: 60px"><span>Following is a JSON sample produced by the <kbd>buildInferenceData</kbd> function:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dd96649e-fd17-4eb9-8edb-d8b3385901f1.png" style=""/></div>
<ul>
<li>The SageMaker predictor object is used for inference.</li>
<li>The <kbd>getInferenceSeries()</kbd> function is <span>used to parse the JSON results from the DeepAR algorithm to identify mean sales, sales in the 10 percentile, and sales in the 90 percentile.</span><span> </span><span>Note that, using Gaussian distribution, DeepAR generates 100 samples of weekly sales for the next nine weeks. Therefore, sales in 10 percentile and 90 percentile indicate the lower and upper bounds of weekly sales during the prediction period.</span></li>
<li>The results returned from the endpoint are then plotted against actual sales via the <kbd>plotResults()</kbd> function<span>. For each of the nine weeks, we will look at mean sales, ground truth sales, sample sales, 10 percentile sales, and 90 percentile sales.</span></li>
</ul>
<p><span>As shown in the following, the mean estimated sales are close to the actual sales, indicating that the DeepAR algorithm has adequately picked up sales demand across different departments. Change the department number to evaluate model performance across all departments. The probabilistic sales estimates hence enable us to estimate demand more accurately than point estimates</span>. Here is the output of the preceding code:</p>
<pre>Predicted Sales:  [92707.65625, 101316.90625, 86202.3984375, 87715.5625, 95967.359375, 101363.71875, 106354.90625, 94017.921875, 103476.71875]<br/><br/>Actual Sales:  [100422.86, 94987.08, 90889.75, 115695.71, 100372.02, 96616.19, 93460.57, 99398.64, 105059.88]</pre>
<p><span>In the following graph, we can see the following:</span></p>
<ul>
<li><span>The blue line indicates mean sales from the prediction of nine weeks in the future.</span></li>
<li><span>The purple line, on the other hand, reflects ground truth.</span></li>
<li><span>The two lines are close enough, indicating that the model has done a decent job capturing patterns in sales, given holidays and historical weekly sales:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/22eef7f3-f8c1-45b0-b12e-e1bbbe498629.png" style="width:24.92em;height:17.67em;"/></span></p>
<p>We have only looked at store 20 sales. However, you can train on all store sales by including the store number in the category list—for each time series in the train and test sets, include the following code:</p>
<pre>"cat": [department number, store number]<span> </span></pre>
<p><span>With a large number of time series across different products and stores, we would have been able to achieve better performance.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we briefly looked at univariate time series forecasting techniques, such as ARIMA and exponential smoothing. However, as demand varies by multiple variables, it becomes important to model multi-variate series. DeepAR enables modeling of multi-variate series, along with providing probabilistic forecasting. While point estimates may work in some situations, probabilistic estimates provide better data for improved decision making. The algorithm works by generating a global model that is trained across a large number of time series. Each item or product across several stores and departments will have its own weekly sales. The trained model accounts for newly introduced items, missing sales per item, and multiple predictors that explain sales. With the LSTM network and Gaussian likelihood, DeepAR in SageMaker provides a flexible approach to demand forecasting. Additionally, we walked through model training, selection, hosting, and inference in SageMaker through the SageMaker Python SDK.</p>
<p>Now, that we've experienced SageMaker's capabilities to solve demand forecasting at scale, in the next chapter, we will walk through model monitoring and governance and will learn about why models degrade in production.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>Overview of a variety of univariate time series forecasting methods:</p>
<ul>
<li><span class="MsoHyperlink"><a href="https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-python/">https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-Python/</a><br/></span></li>
<li><span class="MsoHyperlink"><a href="https://towardsdatascience.com/unboxing-arima-models-1dc09d2746f8">https://towardsdatascience.com/unboxing-arima-models-1dc09d2746f8</a><br/></span></li>
</ul>
<p><span class="MsoHyperlink">Details on how the DeepAR algorithm works:</span></p>
<ul>
<li><span class="MsoHyperlink"><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_how-it-works.html">https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_how-it-works.html</a><br/></span></li>
</ul>
<p>Details on DeepAR inference formats:</p>
<ul>
<li><span class="MsoHyperlink"><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html">https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html</a></span></li>
</ul>


            </article>

            
        </section>
    </body></html>