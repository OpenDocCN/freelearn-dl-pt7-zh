<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer146">
			<h1 id="_idParaDest-94"><em class="italic"><a id="_idTextAnchor098"/>Chapter 7</em>: Understanding the Voice of Your Customer Analytics </h1>
			<p>In the previous chapters, to see improving customer service in action, we built an AI solution that uses the <strong class="bold">AWS NLP</strong> service <strong class="bold">Amazon Comprehend</strong> to first analyze historical customer service records to derive key topics using Amazon Comprehend Topic Modeling and train a custom classification model that will predict routing topics for call routing using <strong class="bold">Amazon Comprehend Custom Classification</strong>. Finally, we used <strong class="bold">Amazon Comprehend detect sentiment</strong> to understand the emotional aspect of the customer feedback. </p>
			<p>In this chapter, we are going to focus more on the emotional aspect of the customer feedback, which could be an Instagrammer, Yelp reviewer, or your aunt posting comments about your business on Facebook, and so on and so forth.</p>
			<p>Twenty years back, it was extremely tough to find out as soon as possible what people felt about your products and get meaningful feedback to improve them. With globalization and the invention of social media, nowadays, everyone has social media apps and people express their emotions more freely than ever before. We all love tweeting about Twitter posts and expressing our opinions in happy, sad, angry, and neutral modes. If you are a very popular person such as a movie star or a business getting millions of such comments, the challenges become going through large volumes of tweets posted by your fans and then quickly finding out whether your movie did well or it was a flop. In the case of a business, a company, or a start-up, people quickly know by reading comments whether your customer service and product support are good or not.</p>
			<p>Social media analytics is a really popular and challenging use case. In this chapter, we will focus on text analytics use cases. We will talk about some of the very common use cases where the power of NLP will be combined with analytics to analyze unstructured data from chats, social media comments, emails, or PDFs. We will show you how you can quickly set up powerful analytics for social media reviews.</p>
			<p>We will navigate through the following sections:</p>
			<ul>
				<li>Challenges of setting up a text analytics solution</li>
				<li>Setting up a Yelp review text analytics workflow</li>
			</ul>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor099"/>Technical requirements</h1>
			<p>For this chapter, you will need access to an AWS account. Before getting started, we recommend that you create an AWS account, and if you have not created one, please refer to the previous chapter for sign-up details. You can skip signup if you already have an existing AWS account and are following instructions from past chapters for creating the AWS account. The Python code and sample datasets for Amazon Textract examples are at the repository link here: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2007">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2007</a>.</p>
			<p>We will walk you through the setup steps on how you can set up the preceding code repository on a Jupyter notebook with the correct IAM permissions in the <em class="italic">Setting up to solve the use case</em> section.</p>
			<p>Check out the following video to see the Code in Action at <a href="https://bit.ly/3mfioWX">https://bit.ly/3mfioWX</a>.</p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor100"/>Challenges of setting up a text analytics solution</h1>
			<p>One of the challenges most organizations face is getting business insights from unstructured data.</p>
			<p>This data <a id="_idIndexMarker412"/>can be in various formats, such as chats, PDF documents, emails, tweets, and so on. Since this data does not have a structure, it's really challenging for the traditional data analytics tool to perform analytics on it. This is where Amazon Textract and Amazon Comprehend can help.</p>
			<p>Amazon Textract can make this unstructured data structured by extracting text and then Amazon Comprehend can extract insights. Once we have the data in text, you can perform <a id="_idIndexMarker413"/>serverless <strong class="bold">Extract, Transform, Load (ETL)</strong> by using <strong class="bold">Amazon Glue</strong> and convert it into a structured format. Moreover, you can use <strong class="bold">Amazon Athena</strong> to perform serverless ad hoc SQL analytics <a id="_idIndexMarker414"/>on the unstructured text you just extracted and transformed using Glue ETL. Amazon Glue can also crawl your unstructured data or text extracted from Amazon Textract in Amazon S3 and store it in a Hive metadata store.</p>
			<p>Lastly, you can also analyze and visualize this data using <strong class="bold">Amazon QuickSight</strong> to gain business insight.</p>
			<p>Amazon QuickSight <a id="_idIndexMarker415"/>helps you create quick visualizations and dashboards to be shared with your business or integrated into an application for your end users. For example, you are using Instagram for business and want to analyze the comments posted on your product pictures and create a real-time dashboard to know whether people are posting positive or negative comments about them. You can use the components mentioned to create a real-time social media analytics dashboard.</p>
			<p>We all love <a id="_idIndexMarker416"/>eating out in a restaurant. The challenge is picking the best restaurant based just on reviews.</p>
			<p>We are going to show you how you can analyze Yelp reviews by using the succeeding architecture:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="Images/B17528_07_01.jpg" alt="Figure 7.1 – Social media analytics serverless architecture&#13;&#10;" width="1110" height="534"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Social media analytics serverless architecture</p>
			<p>The architecture in <em class="italic">Figure 7.1</em> walks you through how you take raw data and transform it to provide new insights, optimize datasets in your data lake, and visualize the serverless results. We will start by doing the following:</p>
			<ol>
				<li>Getting the Yelp review dataset and uploading it to an Amazon S3 bucket using the Jupyter notebook.</li>
				<li> Registering the raw data in the AWS Glue Data Catalog by crawling it.</li>
				<li>Performing AWS Glue ETL on this raw data cataloged in the AWS Glue Data Catalog. Once registered in the AWS Glue Data Catalog, after crawling the Yelp reviews data in AWS S3, this ETL will transform the raw data into Parquet format and enrich it with Amazon Comprehend insights.</li>
				<li>Now we can crawl and catalog this data again in the AWS Glue Data Catalog. Cataloging adds the tables and metadata for Amazon Athena for ad hoc SQL analytics. </li>
				<li>Lastly, you can quickly create visualizations on this transformed Parquet data crawled in Amazon Athena using Amazon QuickSight. Amazon QuickSight integrates directly with Amazon Athena as well as Amazon S3 as a data source for visualization.</li>
			</ol>
			<p>You can also <a id="_idIndexMarker417"/>convert this solution into a real-time streaming solution by using Amazon Kinesis Data Firehose, which will call these Yelp reviews and Twitter APIs, and directly store near real-time streaming data in Amazon S3, and from there you can use AWS Glue ETL and cataloging with Amazon Comprehend to transform and enrich with NLP. Then this transformed data can be directly visualized in QuickSight and Athena. Moreover, you can use AWS Lambda functions and step functions to set up a completely serverless architecture and automate these steps.</p>
			<p>In this section, we covered the challenges of setting up a text analytics workflow with unstructured data and proposed architecture. In the next section, we will walk you through how you can build this out for a Yelp reviews dataset or any social media analytics dataset using a few lines of code through a Jupyter notebook.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor101"/>Setting up a Yelp review text analytics workflow</h1>
			<p>In this <a id="_idIndexMarker418"/>section, we will walk you through how you can build this out for a Yelp reviews dataset or any social media analytics dataset by following the steps using a Jupyter notebook and Python APIs:</p>
			<ul>
				<li>Setting up to solve the use case</li>
				<li>Walking through the solution using a Jupyter notebook</li>
			</ul>
			<p>The setup <a id="_idIndexMarker419"/>steps will involve the steps to configure <strong class="bold">Identity and Access Management (IAM)</strong> roles and the walkthrough notebook will walk you through the architecture. So, let's get started.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor102"/>Setting up to solve the use case</h2>
			<p>If you <a id="_idIndexMarker420"/>have not done so in the previous chapters, you will first have to create an Amazon SageMaker Jupyter notebook and set up <strong class="bold">IAM</strong> permissions for that <a id="_idIndexMarker421"/>notebook role to access the AWS services we will use in this notebook. After that, you will need to clone the GitHub repository (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services</a>), go to the <strong class="source-inline">Chapter 07</strong> folder, and open the <strong class="source-inline">chapter07 social media text analytics.ipynb</strong> notebook:</p>
			<ol>
				<li value="1">You can refer to the Amazon SageMaker documentation to create a notebook instance: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html">https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html</a>. To follow these steps, please sign in to the <strong class="bold">AWS Management Console</strong>, type <strong class="source-inline">Amazon SageMaker</strong> in the search window, select it, and navigate to the <strong class="bold">Amazon SageMaker</strong> console.</li>
				<li>Select <strong class="bold">Notebook instances</strong> and create a notebook instance by specifying an instance type, storage, and an IAM role.</li>
				<li>When creating a SageMaker notebook for this setup, you will need IAM access to the services that follow:<ol><li>AWS Glue to run ETL jobs and crawl the data</li><li>AWS Athena to call Athena APIs through the notebook</li><li>AWS Comprehend to perform sentiment analysis</li><li>AWS QuickSight for visualization</li><li>AWS S3 access</li></ol><p>Make sure your IAM for the notebook has the following roles:</p></li>
			</ol>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="Images/B17528_07_02.jpg" alt="Figure 7.2 – Important IAM roles to run the notebook&#13;&#10;" width="1034" height="574"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Important IAM roles to run the notebook</p>
			<p>You <a id="_idIndexMarker422"/>can modify an existing notebook role you are using to add these permissions. In <em class="italic">Figure 7.2</em> is the SageMaker IAM role that you used to create your notebook instance. You can navigate to that role and click <strong class="bold">Attach policies</strong> to make sure you have the necessary roles for your notebook to execute the service APIs we are going to use.</p>
			<p>Follow through the steps in this notebook that correspond to the next few subheadings in this section, by executing one cell at a time. Please do read the descriptions provided in each notebook cell.</p>
			<h3>Additional IAM pre-requisites for invoking AWS Glue from this notebook</h3>
			<p>In the <a id="_idIndexMarker423"/>previous section, we walked you through how to set up the notebook and important IAM roles to run this notebook. In this section, we assume that you are already in the notebook and we have added instructions in the notebook on how you can get the execution role of the notebook and enable it to invoke AWS Glue jobs from this notebook.</p>
			<p>Go to the notebook and follow the <em class="italic">Finding out the current execution role of the notebook</em> section by running the following code:</p>
			<p class="source-code">import sagemaker</p>
			<p class="source-code">from sagemaker import get_execution_role</p>
			<p class="source-code">sess = sagemaker.Session()</p>
			<p class="source-code">role = get_execution_role()</p>
			<p class="source-code">role_name = role[role.rfind('/') + 1:]</p>
			<p class="source-code">print(role_name)</p>
			<p>You <a id="_idIndexMarker424"/>will get the role associated with this notebook. Now, in the next section, we will walk you through how you can add AWS Glue as an additional trusted entity to this role.</p>
			<h3>Adding AWS Glue and Amazon Comprehend as an additional trusted entity to this role</h3>
			<p>This step <a id="_idIndexMarker425"/>is needed if you <a id="_idIndexMarker426"/>want to pass the execution role of this notebook while calling Glue APIs as well, without creating an additional role. If you have not used AWS Glue before, then this step is mandatory. If you have used AWS Glue previously, then you should have an already existing role that can be used to invoke Glue APIs. In that case, you can pass that role while calling Glue (later in this notebook) and skip this next step:</p>
			<ol>
				<li value="1">On the IAM dashboard, click on <strong class="bold">Roles</strong> on the left-side navigation and search for the <strong class="bold">r</strong><strong class="bold">ole</strong>. Once the role appears, click on <strong class="bold">Role</strong> to go to its <strong class="bold">Summary</strong> page.</li>
				<li>Click on the <strong class="bold">Trust relationships</strong> tab on the <strong class="bold">Summary</strong> page to add AWS Glue as an additional trusted entity.</li>
				<li>Click on <strong class="bold">Edit trust relationship</strong> and replace the JSON with this JSON:<p class="source-code">{</p><p class="source-code">  "Version": "2012-10-17",</p><p class="source-code">  "Statement": [</p><p class="source-code">    {</p><p class="source-code">      "Effect": "Allow",</p><p class="source-code">      "Principal": {</p><p class="source-code">        "Service": [</p><p class="source-code">          "sagemaker.amazonaws.com",</p><p class="source-code">          "glue.amazonaws.com"</p><p class="source-code">        ]</p><p class="source-code">      },</p><p class="source-code">      "Action": "sts:AssumeRole"</p><p class="source-code">    }</p><p class="source-code">  ]</p><p class="source-code">}</p></li>
				<li>Once <a id="_idIndexMarker427"/>this is <a id="_idIndexMarker428"/>complete, click on <strong class="bold">Update Trust Policy</strong> and you are done.</li>
			</ol>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="Images/B17528_07_03.jpg" alt="Figure 7.3 – Setting up trust with Glue in an IAM role&#13;&#10;" width="1427" height="370"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Setting up trust with Glue in an IAM role</p>
			<p>In this section, we covered how you can set up the Jupyter notebook and the necessary IAM permissions to run the Jupyter notebook. In the next section, we will walk you through the solution by the notebook setup you did.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor103"/>Walking through the solution using Jupyter Notebook</h2>
			<p>In this section, we will walk you through how you can build this out for a Yelp review dataset <a id="_idIndexMarker429"/>or any social media analytics dataset by following the given steps using a Jupyter notebook and Python APIs:</p>
			<ol>
				<li value="1">Download the review dataset from Yelp Reviews NLP Fast.ai, which we have already done for you in the notebook.</li>
				<li>Register the raw dataset as a table with the AWS Glue Data Catalog.</li>
				<li>Run PySpark (AWS Glue job) to convert the dataset into Parquet and get the review sentiment with Amazon Comprehend.</li>
				<li>Store the transformed results in a newly curated dataset.</li>
				<li>Run a serverless query for the optimized dataset with Amazon Athena.</li>
				<li>Provide visual insights of the results with Amazon QuickSight or Bokeh AWS Glue Data Catalog.</li>
				<li>Go to this notebook and run the following steps in the code block to set up the libraries:<p class="source-code">import boto3</p><p class="source-code">import botocore</p><p class="source-code">import json</p><p class="source-code">import time</p><p class="source-code">import os</p><p class="source-code">              import project_path</p><p class="source-code">from lib import workshop</p><p class="source-code">glue = boto3.client('glue')</p><p class="source-code">s3 = boto3.resource('s3')</p><p class="source-code">s3_client = boto3.client('s3')</p><p class="source-code">session = boto3.session.Session()</p><p class="source-code">region = session.region_name</p><p class="source-code">account_id = boto3.client('sts').get_caller_identity().get('Account')</p><p class="source-code">database_name = 'yelp' # AWS Glue Data Catalog Database Name</p><p class="source-code">raw_table_name = 'raw_reviews' # AWS Glue Data Catalog raw table name</p><p class="source-code">parquet_table_name = 'parq_reviews' # AWS Glue Data Catalog parquet table name</p><p class="source-code">open_data_bucket = 'fast-ai-nlp'</p></li>
				<li>We <a id="_idIndexMarker430"/>have imported the necessary libraries for notebook setup and defined the database name and table names for the AWS Glue Data Catalog. In the next step, we will download the Yelp reviews dataset by running the following code:<p class="source-code">try:</p><p class="source-code">    s3.Bucket(open_data_bucket).download_file('yelp_review_full_csv.tgz', 'yelp_review_full_csv.tgz')</p><p class="source-code">except botocore.exceptions.ClientError as e:</p><p class="source-code">    if e.response['Error']['Code'] == "404":</p><p class="source-code">        print("The object does not exist.")</p><p class="source-code">    else:</p><p class="source-code">        raise</p></li>
				<li>Now, we will run the following code to untar or unzip this review dataset:<p class="source-code">!tar -xvzf yelp_review_full_csv.tgz</p></li>
				<li>There are two CSV files in the tarball. One is called <strong class="source-inline">train.csv</strong>, the other is <strong class="source-inline">test.csv</strong>. For those interested, the <strong class="source-inline">readme.txt</strong> file describes the dataset in more detail.</li>
				<li>We will use Python pandas to read the CSV files and view the dataset. You will notice the data contains two unnamed columns for the rating and review. The rating is between 1 and 5 and the review is a free-form text field:<p class="source-code">import pandas as pd</p><p class="source-code">pd.set_option('display.max_colwidth', -1)</p><p class="source-code">df = pd.read_csv('yelp_review_full_csv/train.csv', header=None)</p><p class="source-code">df.head(5)</p></li>
				<li>You <a id="_idIndexMarker431"/>get the following output:<div id="_idContainer135" class="IMG---Figure"><img src="Images/B17528_07_04.jpg" alt="Figure 7.4 – Raw Yelp review data&#13;&#10;" width="1478" height="529"/></div><p class="figure-caption">Figure 7.4 – Raw Yelp review data</p></li>
				<li>Now, we will upload the file created previously to S3 to be used later by executing the following notebook code:<p class="source-code">file_name = 'train.csv'</p><p class="source-code">session.resource('s3').Bucket(bucket).Object(os.path.join('yelp', 'raw', file_name)).upload_file('yelp_review_full_csv/'+file_name)</p></li>
			</ol>
			<p>We downloaded the Yelp dataset and stored it in a raw S3 bucket. In the next section, we will create the AWS Glue Data Catalog database.</p>
			<h3>Creating the AWS Glue Catalog database</h3>
			<p>In this section, we will walk you through how you can define a table and add it to the Glue <a id="_idIndexMarker432"/>Data Catalog database. Glue crawlers automatically crawl your data from Amazon S3 or from any on-premises database as it supports multiple data stores. You can also bring your own Hive metadata store and get started with crawlers. Once you have created a crawler, you can perform Glue ETL jobs as these jobs use these Data Catalog tables as source data and target data. Moreover, the Glue ETL job will read and write to the data stores that are specified in the source and target Data Catalog tables in Glue crawlers. There is a central Glue Data Catalog for each AWS account:</p>
			<ol>
				<li value="1">We are going to use the <strong class="source-inline">glue.create_database</strong> API (<a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.create_database">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.create_database</a>) to create a Glue database:<p class="source-code">workshop.create_db(glue, account_id, database_name, 'Database for Yelp Reviews')</p></li>
				<li>Now we will create the raw table in Glue (<a href="https://docs.aws.amazon.com/glue/latest/dg/tables-described.html">https://docs.aws.amazon.com/glue/latest/dg/tables-described.html</a>). There is more than one way to create a table in Glue:<ol><li><em class="italic">Using an AWS Glue crawler</em>: We have classifiers that automatically determine the schema of the dataset while crawling using a built-in classifier. You can also use a custom classifier if your data schema has a complicated nested JSON structure.</li><li><em class="italic">Creating a table manually or using the APIs</em>: You create a table manually in the console or by using an API. You specify the schema to be classified when you define the table.<p class="callout-heading">Note:</p><p class="callout">For more information about creating a table using the AWS Glue console, see <em class="italic">Working with Tables on the AWS Glue Console</em>: <a href="https://docs.aws.amazon.com/glue/latest/dg/console-tables.html">https://docs.aws.amazon.com/glue/latest/dg/console-tables.html</a>.</p></li></ol></li>
				<li>We <a id="_idIndexMarker433"/>are using the <strong class="source-inline">glue.create_table</strong> API to create tables:<p class="source-code">location = 's3://{0}/yelp/raw'.format(bucket)</p><p class="source-code">response = glue.create_table(</p><p class="source-code">    CatalogId=account_id,</p><p class="source-code">    DatabaseName=database_name,</p><p class="source-code">    TableInput={</p><p class="source-code">        'Name': raw_table_name,</p><p class="source-code">        'Description': 'Raw Yelp reviews dataset',</p><p class="source-code">        'StorageDescriptor': {</p><p class="source-code">            'Columns': [</p><p class="source-code">                {</p><p class="source-code">                    'Name': 'rating',</p><p class="source-code">                    'Type': 'tinyint',</p><p class="source-code">                    'Comment': 'Rating of from the Yelp review'</p><p class="source-code">                },</p><p class="source-code">                {</p><p class="source-code">                    'Name': 'review',</p><p class="source-code">                    'Type': 'string',</p><p class="source-code">                    'Comment': 'Review text of from the Yelp review'</p><p class="source-code">                }</p><p class="source-code">            ],</p><p class="source-code">            'Location': location,</p><p class="source-code">            'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',</p><p class="source-code">            'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',</p><p class="source-code">            'SerdeInfo': {</p><p class="source-code">                'SerializationLibrary': 'org.apache.hadoop.hive.serde2.OpenCSVSerde',</p><p class="source-code">                'Parameters': {</p><p class="source-code">                    'escapeChar': '\\',</p><p class="source-code">                    'separatorChar': ',',</p><p class="source-code">                    'serialization.format': '1'</p><p class="source-code">                }</p><p class="source-code">            },</p><p class="source-code">        },</p><p class="source-code">        'TableType': 'EXTERNAL_TABLE',</p><p class="source-code">        'Parameters': {</p><p class="source-code">            'classification': 'csv'</p><p class="source-code">        }</p><p class="source-code">    }</p><p class="source-code">)</p><p>The <a id="_idIndexMarker434"/>preceding code will create tables in the AWS Glue Data Catalog.</p></li>
				<li>Now we will visualize this data using the Amazon Athena <strong class="source-inline">pyAthena</strong> API. To see the raw Yelp reviews, we will be installing this Python library for querying the data in the Glue Data Catalog with Athena:<p class="source-code">!pip install PyAthena</p></li>
				<li>The following code will help us visualize the database and tables in Glue for the raw dataset:<p class="source-code">from pyathena import connect</p><p class="source-code">from pyathena.pandas.util import as_pandas</p><p class="source-code">cursor = connect(region_name=region, s3_staging_dir='s3://'+bucket+'/yelp/temp').cursor()</p><p class="source-code">cursor.execute('select * from ' + database_name + '.' + raw_table_name + ' limit 10')</p><p class="source-code">df = as_pandas(cursor)</p><p class="source-code">df.head(5)</p></li>
				<li>You <a id="_idIndexMarker435"/>will see the following table in the output:</li>
			</ol>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="Images/B17528_07_05.jpg" alt="Figure 7.5 – Athena output for the raw dataset in Glue Data Catalog" width="518" height="235"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Athena output for the raw dataset in Glue Data Catalog</p>
			<p>So, we covered how you can create a Glue Data Catalog and crawl the raw Yelp data we downloaded in Amazon S3. Then we showed you how you can visualize this data in Amazon Athena using <strong class="source-inline">pyAthena</strong> libraries. In the next section, we will walk you through how you can transform this data.</p>
			<h3>Transforming raw data to provide insights and visualization</h3>
			<p>Now <a id="_idIndexMarker436"/>we will walk you through how you can transform the raw data using PySpark in an AWS Glue job to call Amazon Comprehend APIs to get sentiment analysis on the review, convert the data into Parquet, and partition by sentiment. This will allow us to optimize analytics queries when viewing data by sentiment and return just the values we need, leveraging the columnar format of Parquet.</p>
			<p>We covered Comprehend's detect sentiment real-time API in <a href="B17528_03_Final_SB_ePub.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>, <em class="italic">Introducing Amazon Comprehend</em>. In this job, we will use the real-time batch detect sentiment APIs.</p>
			<p>We <a id="_idIndexMarker437"/>will create a PySpark job to add a primary key and run a batch of reviews through Amazon Comprehend to get a sentiment analysis of the reviews. The job will limit the number of rows it converts, but this code could be modified to run the entire dataset:</p>
			<ol>
				<li value="1">In order to run your code in AWS Glue, we will upload the code and dependencies directly to S3 and pass those locations while invoking the Glue job. We will write the ETL job using Jupyter Notebook's cell magic <strong class="source-inline">%%writefile</strong>. We are using the AWS Glue script next to transform or perform ETL on the Yelp reviews dataset by using Glue transform and adding a sentiment column to the DataFrame by analyzing sentiment using Amazon Comprehend:<p class="source-code">%%writefile yelp_etl.py</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">import boto3</p><p class="source-code">from awsglue.transforms import *</p><p class="source-code">from awsglue.utils import getResolvedOptions</p><p class="source-code">from pyspark.context import SparkContext</p><p class="source-code">from awsglue.context import GlueContext</p><p class="source-code">from awsglue.job import Job</p><p class="source-code">from awsglue.dynamicframe import DynamicFrame</p><p class="source-code">import pyspark.sql.functions as F</p><p class="source-code">from pyspark.sql import Row, Window, SparkSession</p><p class="source-code">from pyspark.sql.types import *</p><p class="source-code">from pyspark.conf import SparkConf</p><p class="source-code">from pyspark.context import SparkContext</p><p class="source-code">from pyspark.sql.functions import *</p><p class="source-code">args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_OUTPUT_BUCKET', 'S3_OUTPUT_KEY_PREFIX', 'DATABASE_NAME', 'TABLE_NAME', 'REGION'])</p><p class="source-code">sc = SparkContext()</p><p class="source-code">glueContext = GlueContext(sc)</p><p class="source-code">spark = glueContext.spark_session</p><p class="source-code">job = Job(glueContext)</p><p class="source-code">job.init(args['JOB_NAME'], args)</p></li>
				<li>We <a id="_idIndexMarker438"/>have imported the necessary APIs and now we will convert the Glue DynamicFrame to a Spark DataFrame to read the data from our Glue Data Catalog. We will select the review and rating columns from the database and the table we created in the Glue Data Catalog:<p class="source-code">yelp = glueContext.create_dynamic_frame.from_catalog(database=args['DATABASE_NAME'], table_name=args['TABLE_NAME'], transformation_ctx = "datasource0")</p><p class="source-code">yelpDF = yelp.toDF().select('rating', 'review')</p><p>We are defining some limits such as how many characters are to be sent for batch detect sentiment – <strong class="source-inline">MAX_SENTENCE_LENGTH_IN_CHARS</strong> – <strong class="source-inline">w</strong>hat the batch size of reviews sent for getting sentiment should be – <strong class="source-inline">COMPREHEND_BATCH_SIZE</strong> – and how many batches are to be sent:</p><p class="source-code">MIN_SENTENCE_LENGTH_IN_CHARS = 10 </p><p class="source-code">MAX_SENTENCE_LENGTH_IN_CHARS = 4500</p><p class="source-code">COMPREHEND_BATCH_SIZE = 5  </p><p class="source-code">NUMBER_OF_BATCHES = 10</p><p class="source-code">ROW_LIMIT = 1000 #Number of reviews we will process for this workshop</p></li>
				<li>Each <a id="_idIndexMarker439"/>task handles 5*10 records and we are calling the Comprehend batch detect sentiment API to get the sentiment and add that sentiment after the transformation with AWS Glue. Creating a function where we are passing the text list and calling the batch detect sentiment API was covered in <a href="B17528_03_Final_SB_ePub.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a>, <em class="italic">Introducing Amazon Comprehend</em>:<p class="source-code">ComprehendRow = Row("review", "rating", "sentiment")</p><p class="source-code">def getBatchComprehend(input_list):</p><p class="source-code">    arr = []</p><p class="source-code">    bodies = [i[0] for i in input_list]</p><p class="source-code">    client = boto3.client('comprehend',region_name=args['REGION'])</p><p class="source-code">    def callApi(text_list):</p><p class="source-code">        response = client.batch_detect_sentiment(TextList = text_list, LanguageCode = 'en')</p><p class="source-code">        return response</p><p class="source-code">  </p><p class="source-code">    for i in range(NUMBER_OF_BATCHES):</p><p class="source-code">        text_list = bodies[COMPREHEND_BATCH_SIZE * i : COMPREHEND_BATCH_SIZE * (i+1)]</p><p class="source-code">        #response = client.batch_detect_sentiment(TextList = text_list, LanguageCode = 'en')</p><p class="source-code">        response = callApi(text_list)</p><p class="source-code">        for r in response['ResultList']:</p><p class="source-code">            idx = COMPREHEND_BATCH_SIZE * i + r['Index']</p><p class="source-code">            arr.append(ComprehendRow(input_list[idx][0], input_list[idx][1], r['Sentiment']))</p><p class="source-code">  </p><p class="source-code">    return arr</p></li>
				<li>The <a id="_idIndexMarker440"/>following code will grab a sample set of records with a review size under the Comprehend limits:<p class="source-code">yelpDF = yelpDF \</p><p class="source-code">  .withColumn('review_len', F.length('review')) \</p><p class="source-code">  .filter(F.col('review_len') &gt; MIN_SENTENCE_LENGTH_IN_CHARS) \</p><p class="source-code">  .filter(F.col('review_len') &lt; MAX_SENTENCE_LENGTH_IN_CHARS) \</p><p class="source-code">  .limit(ROW_LIMIT)</p><p class="source-code">record_count = yelpDF.count()</p><p class="source-code">print('record count=' + str(record_count))</p><p class="source-code">yelpDF = yelpDF.repartition(record_count/(NUMBER_OF_BATCHES*COMPREHEND_BATCH_SIZE))</p></li>
				<li>We are using the Glue DataFrame to concatenate the submission ID and body tuples into arrays of a similar size and transforming the results:<p class="source-code">group_rdd = yelpDF.rdd.map(lambda l: (l.review.encode("utf-8"), l.rating)).glom()</p><p class="source-code">  </p><p class="source-code">transformed = group_rdd \</p><p class="source-code">  .map(lambda l: getBatchComprehend(l)) \</p><p class="source-code">  .flatMap(lambda x: x) \</p><p class="source-code">  .toDF()</p><p class="source-code">print("transformed count=" + str(transformed.count()))</p><p>We are converting the transformed DataFrame with sentiments into Parquet format and saving it in our transformed Amazon S3 bucket:</p><p class="source-code">transformedsink = DynamicFrame.fromDF(transformed, glueContext, "joined")</p><p class="source-code">parquet_output_path = 's3://' + os.path.join(args['S3_OUTPUT_BUCKET'], args['S3_OUTPUT_KEY_PREFIX'])</p><p class="source-code">print(parquet_output_path)</p><p class="source-code">datasink5 = glueContext.write_dynamic_frame.from_options(frame = transformedsink, connection_type = "s3", connection_options = {"path": parquet_output_path, "partitionKeys": ["sentiment"]}, format="parquet", transformation_ctx="datasink5")</p><p class="source-code">                                                </p><p class="source-code">job.commit()</p></li>
				<li>The <a id="_idIndexMarker441"/>key point in this code is how easy it is to get access to the AWS Glue Data Catalog leveraging the Glue libraries:<p class="source-code">glueContext.create_dynamic_frame.from_catalog- Read table metadata from the Glue Data Catalog using Glue libs to load tables into the job.</p><p class="source-code">yelpDF = yelp.toDF() - Easy conversion from Glue DynamicFrame to Spark DataFrame and vice-versa joinedsink= DynamicFrame.fromDF(joinedDF, glueContext, "joined").</p></li>
				<li>Write S3 using <strong class="source-inline">glueContext.write_dynamic_frame. from_options</strong> with the following options:<ul><li>Partition the data based on columns – <strong class="source-inline">connection_options = {"path": parquet_output_path, "partitionKeys": ["sentiment"]}</strong>.<p>Convert data to a columnar format – <strong class="source-inline">format="parquet"</strong>.</p></li></ul></li>
				<li>We will be uploading the <strong class="source-inline">github_etl.py</strong> script to S3 now so that Glue can use it to run the PySpark job. You can replace it with your own script if needed. If your code has multiple files, you need to zip those files and upload them to S3 instead of uploading a single file like is being done here:<p class="source-code">script_location = sess.upload_data(path='yelp_etl.py', bucket=bucket, key_prefix='yelp/codes')</p><p class="source-code">s3_output_key_prefix = 'yelp/parquet/'</p></li>
				<li>Next, we'll be creating a Glue client via Boto3 so that we can invoke the <strong class="source-inline">create_job</strong> API of Glue. The <strong class="source-inline">create_job</strong> API will create a job definition that can be used to execute your jobs in Glue. The job definition created here is mutable. While creating the job, we are also passing the code location <a id="_idIndexMarker442"/>as well as the dependencies' locations to Glue. The <strong class="source-inline">AllocatedCapacity</strong> parameter controls the hardware resources that Glue will use to execute this job. It is measured in units of <strong class="bold">DPU</strong>. For more information on <strong class="bold">DPU</strong>, please see <a href="https://docs.aws.amazon.com/glue/latest/dg/add-job.html">https://docs.aws.amazon.com/glue/latest/dg/add-job.html</a>:<p class="source-code">from time import gmtime, strftime</p><p class="source-code">import time</p><p class="source-code">timestamp_prefix = strftime("%Y-%m-%d-%H-%M-%S", gmtime())</p><p class="source-code">job_name = 'yelp-etl-' + timestamp_prefix</p><p class="source-code">response = glue.create_job(</p><p class="source-code">    Name=job_name,</p><p class="source-code">    Description='PySpark job to extract Yelp review sentiment analysis',</p><p class="source-code">    Role=role, # you can pass your existing AWS Glue role here if you have used Glue before</p><p class="source-code">    ExecutionProperty={</p><p class="source-code">        'MaxConcurrentRuns': 1</p><p class="source-code">    },</p><p class="source-code">    Command={</p><p class="source-code">        'Name': 'glueetl',</p><p class="source-code">        'ScriptLocation': script_location</p><p class="source-code">    },</p><p class="source-code">    DefaultArguments={</p><p class="source-code">        '--job-language': 'python',</p><p class="source-code">        '--job-bookmark-option': 'job-bookmark-disable'</p><p class="source-code">    },</p><p class="source-code">    AllocatedCapacity=5,</p><p class="source-code">    Timeout=60,</p><p class="source-code">)</p><p class="source-code">glue_job_name = response['Name']</p><p class="source-code">print(glue_job_name)</p></li>
				<li>The <a id="_idIndexMarker443"/>aforementioned job will be executed now by calling the <strong class="source-inline">start_job_run</strong> API. This API creates an immutable run/execution corresponding to the job definition created previously. We will require the <strong class="source-inline">job_run_id</strong>  value for the particular job execution to check the status. We'll pass the data and model locations as part of the job execution parameters:<p class="source-code">job_run_id = glue.start_job_run(JobName=job_name,</p><p class="source-code">                                       Arguments = {</p><p class="source-code">                                        '--S3_OUTPUT_BUCKET': bucket,</p><p class="source-code">                                        '--S3_OUTPUT_KEY_PREFIX': s3_output_key_prefix,</p><p class="source-code">                                        '--DATABASE_NAME': database_name,</p><p class="source-code">                                        '--TABLE_NAME': raw_table_name,</p><p class="source-code">                                        '--REGION': region</p><p class="source-code">                                       })['JobRunId']</p><p class="source-code">print(job_run_id)</p><p class="callout-heading">Note: </p><p class="callout">This job will take approximately 2 minutes to run.</p></li>
				<li>Now we will check the job status to see whether it is <strong class="source-inline">SUCCEEDED</strong>, <strong class="source-inline">FAILED</strong>, or <strong class="source-inline">STOPPED</strong>. Once the job has succeeded, we have <a id="_idIndexMarker444"/>the transformed data in S3 in Parquet format, which we will use to query with Athena and visualize with QuickSight. If the job fails, you can go to the AWS Glue console, click on the <strong class="bold">Jobs</strong> tab on the left, and from the page, click on this particular job and you will be able to find the CloudWatch Logs link (the link under <strong class="bold">Logs</strong>) for these jobs, which can help you to see what exactly went wrong in the job execution:<p class="source-code">job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']</p><p class="source-code">while job_run_status not in ('FAILED', 'SUCCEEDED', 'STOPPED'):</p><p class="source-code">    job_run_status = glue.get_job_run(JobName=job_name,RunId=job_run_id)['JobRun']['JobRunState']</p><p class="source-code">    print (job_run_status)</p><p class="source-code">    time.sleep(60)</p><p class="source-code">print(job_run_status)</p></li>
			</ol>
			<p>In the next section, we will walk you through how to use a Glue crawler to discover the transformed data.</p>
			<h3>Using a Glue crawler to discover the transformed data</h3>
			<p>Most AWS Glue users use a crawler to populate the AWS Glue Data Catalog with tables as <a id="_idIndexMarker445"/>a primary method. To do this, what you have to do is add a crawler within your Data Catalog to traverse your data stores. The output of the crawler consists of one or more <a id="_idIndexMarker446"/>metadata tables that are defined in your Data Catalog. <strong class="bold">ETL</strong> jobs that you define in AWS Glue use these metadata tables as sources and targets:</p>
			<ul>
				<li>A crawler can crawl both file-based and table-based data stores. Crawlers can crawl the data from various types of data stores, such as Amazon S3, RDS, Redshift, DynamoDB, or on-premises databases:<p class="source-code">parq_crawler_name = 'YelpCuratedCrawler'</p><p class="source-code">parq_crawler_path = 's3://{0}/yelp/parquet/'.format(bucket)</p><p class="source-code">                 response = glue.create_crawler(</p><p class="source-code">    Name=parq_crawler_name,</p><p class="source-code">    Role=role,</p><p class="source-code">    DatabaseName=database_name,</p><p class="source-code">    Description='Crawler for the Parquet Yelp Reviews with Sentiment',</p><p class="source-code">    Targets={</p><p class="source-code">        'S3Targets': [</p><p class="source-code">            {</p><p class="source-code">                'Path': parq_crawler_path</p><p class="source-code">            }</p><p class="source-code">        ]</p><p class="source-code">    },</p><p class="source-code">    SchemaChangePolicy={</p><p class="source-code">        'UpdateBehavior': 'UPDATE_IN_DATABASE',</p><p class="source-code">        'DeleteBehavior': 'DEPRECATE_IN_DATABASE'</p><p class="source-code">    },</p><p class="source-code">    TablePrefix='reviews_'</p><p class="source-code">)</p></li>
				<li><strong class="bold">Start the Glue Crawler</strong>: You can use the Glue crawler to populate the AWS Glue Data Catalog with tables. The crawler will automatically crawl your data source, which can be an on-premises database or raw CSV files in Amazon S3, and <a id="_idIndexMarker447"/>create a metadata table in the Glue Data Catalog, as well as inferring the schema. Glue ETL jobs use these metadata tables in the Glue Data Catalog as sources and targets. You can also bring your existing Hive data catalog to run Glue ETL:<p class="source-code">response = glue.start_crawler(</p><p class="source-code">    Name=parq_crawler_name</p><p class="source-code">)</p><p class="source-code">print ("Parquet Crawler: https://{0}.console.aws.amazon.com/glue/home?region={0}#crawler:name={1}".format(region, parq_crawler_name))</p></li>
				<li>Go to the link in the output to visualize your crawler in Amazon Athena:</li>
			</ul>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="Images/B17528_07_06.jpg" alt="Figure 7.6 – Yelp curated crawler&#13;&#10;" width="1045" height="702"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Yelp curated crawler</p>
			<ul>
				<li><strong class="bold">Checking the Glue crawler status</strong>: We will now monitor the crawler status, waiting <a id="_idIndexMarker448"/>for it to get back into the <strong class="source-inline">READY</strong> state, meaning the crawler completed its crawl. You can also look at the CloudWatch logs for the crawler for more details:<p class="source-code">crawler_status = glue.get_crawler(Name=parq_crawler_name)['Crawler']['State']</p><p class="source-code">while crawler_status not in ('READY'):</p><p class="source-code">    crawler_status = glue.get_crawler(Name=parq_crawler_name)['Crawler']['State']</p><p class="source-code">    print(crawler_status)</p><p class="source-code">    time.sleep(30)</p></li>
			</ul>
			<p>Once you get an output of <strong class="source-inline">READY</strong>, move on to the next step. In this section, we showed you how you can crawl the transformed Parquet data for Yelp reviews, which has sentiment scores from Amazon Comprehend in the AWS Glue Data Catalog. In the next section, we will cover how you can visualize this data to gain meaningful insights into the voice of customer analytics.</p>
			<h3>Viewing the transformed results</h3>
			<p>We will again use the PyAthena library to run queries against the newly created dataset with <a id="_idIndexMarker449"/>sentiment results and in the Parquet format. In the interest of time, we will be using the Bokeh AWS Glue Data Catalog within the notebook to visualize the results instead of Amazon QuickSight:</p>
			<ul>
				<li>QuickSight is able to use the same Athena queries to visualize the results as well as numerous built-in connectors to many data sources:<p class="source-code">cursor.execute('select rating, review, sentiment from yelp.reviews_parquet')</p><p class="source-code">              df = as_pandas(cursor)</p><p class="source-code">df.head(10)</p></li>
				<li>Here's the output of querying the Glue Data Catalog via Amazon Athena to get the rating, review, and sentiment from the Yelp reviews table:</li>
			</ul>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="Images/B17528_07_07.jpg" alt="Figure 7.7 – Athena query to select rating and sentiment from the Yelp transformed table in the Glue Data Catalog&#13;&#10;" width="737" height="454"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Athena query to select rating and sentiment from the Yelp transformed table in the Glue Data Catalog</p>
			<ul>
				<li><strong class="bold">Group the data in the DataFrame by sentiment</strong>: Using the pandas DataFrame functionality, we will do <strong class="source-inline">groupby</strong> locally. Alternatively, we could have <a id="_idIndexMarker450"/>used the built-in SQL and aggregate functions in Athena to achieve the same result:<p class="source-code">group = df.groupby(('sentiment'))</p><p class="source-code">group.describe()</p><p>Next, you will find the output of the <strong class="source-inline">groupby</strong> sentiment query:</p></li>
			</ul>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="Images/B17528_07_08.jpg" alt="Figure 7.8 – Output of the groupby sentiment query&#13;&#10;" width="1102" height="298"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – Output of the groupby sentiment query</p>
			<p>We can see from the preceding output that we have more positive reviews than negative ones for the Yelp reviews dataset.</p>
			<ul>
				<li>The Bokeh framework has a number of built-in visualizations. We will use Bokeh to visualize reviews by sentiment and rating in subsequent code.</li>
				<li><em class="italic">Visualize by rating</em>: We will now compare what the Comprehend API came up with <a id="_idIndexMarker451"/>compared to the user rating in the dataset. We are changing <strong class="source-inline">groupby</strong> in the DataFrame to change the dataset:<p class="source-code">group = df.groupby(('rating'))</p><p class="source-code">group.describe()</p><p>You will find the output of the query result grouped by rating in the following screenshot:</p></li>
			</ul>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="Images/B17528_07_09.jpg" alt="Figure 7.9 – Output of the query result grouped by rating" width="1108" height="339"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – Output of the query result grouped by rating</p>
			<ul>
				<li>Now, we <a id="_idIndexMarker452"/>will show you how you can plot this in Bokeh:<p class="source-code">source = ColumnDataSource(group)</p><p class="source-code">",".join(source.column_names)</p><p class="source-code">rating_cmap = factor_cmap('rating', palette=Spectral5, factors=sorted(df.rating.unique()))</p><p class="source-code">p = figure(plot_height=350, x_range=group)</p><p class="source-code">p.vbar(x='rating', top='review_count', width=1, line_color="white", </p><p class="source-code">       fill_color=rating_cmap, source=source)</p><p class="source-code">p.xgrid.grid_line_color = None</p><p class="source-code">p.xaxis.axis_label = "Rating"</p><p class="source-code">p.yaxis.axis_label = "Count"</p><p class="source-code">p.y_range.start = 0</p><p>You will find the bar graph showing the count of users with their ratings.</p></li>
			</ul>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="Images/B17528_07_10.jpg" alt="Figure 7.10 – User rating by count&#13;&#10;" width="910" height="537"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10 – User rating by count</p>
			<p>We can see that the most popular rating given is 1 and the least popular is 2. We covered in this <a id="_idIndexMarker453"/>section how you can use simple queries in Amazon Athena and visualization tools such as Bokeh to perform quick visualization and SQL analytics on the Yelp reviews dataset enriched with the sentiment from Amazon Comprehend. We got some cool insights, such as most of the reviews are positive, and also the most popular rating given by users is 1. You can further drill down and get very specific insights into particular comments by using simple SQL. This helps drive business outcomes very quickly. In the next section, we will walk you through how you can easily create a QuickSight dashboard to gain some cool insights for your business users.</p>
			<h3>Amazon QuickSight visualization</h3>
			<p>In this section, we will walk you through how you can set up or get started with Amazon QuickSight and visualize the tables in Athena, which was transformed with sentiments using AWS Glue and Comprehend:</p>
			<ol>
				<li value="1">You can follow along with the <strong class="source-inline">Getting Started</strong> guide for QuickSight at <a href="https://docs.aws.amazon.com/quicksight/latest/user/getting-started.html">https://docs.aws.amazon.com/quicksight/latest/user/getting-started.html</a> to set up your account and then follow the code block to navigate to the QuickSight console:<p class="source-code">print('https://{0}.quicksight.aws.amazon.com/sn/start?#'.format(region))</p></li>
				<li><strong class="bold">Manage S3 Access in QuickSight</strong>: We need to do this mandatory step <a id="_idIndexMarker454"/>to make sure we do not get an access denied exception while accessing Amazon Athena with Amazon QuickSight.</li>
				<li>Go to <strong class="bold">Manage QuickSight | Security and permission | Add or remove | In S3. C</strong>lick on <strong class="bold">details</strong> | select the bucket you want to query | <strong class="bold">update</strong>.<div id="_idContainer142" class="IMG---Figure"><img src="Images/B17528_07_11.jpg" alt="Figure 7.11 – Manage QuickSight access for the S3 data lake&#13;&#10;" width="1050" height="629"/></div><p class="figure-caption">Figure 7.11 – Manage QuickSight access for the S3 data lake</p></li>
				<li>Click on the <strong class="bold">Create Dataset</strong> option and you will see the following options:<div id="_idContainer143" class="IMG---Figure"><img src="Images/B17528_07_12.jpg" alt="Figure 7.12 – Amazon QuickSight setup with the Create a Dataset option&#13;&#10;" width="1617" height="577"/></div><p class="figure-caption">Figure 7.12 – Amazon QuickSight setup with the Create a Dataset option</p></li>
				<li>Choose <strong class="bold">Athena</strong> from the preceding options. Click the <strong class="bold">New data set </strong>button and <a id="_idIndexMarker455"/>select <strong class="bold">Athena data source</strong>. Name the data source and choose the Yelp <strong class="bold">Glue database</strong> and <strong class="bold">reviews_parquet</strong> table. Finish the creation by clicking the <strong class="bold">Create data source</strong> button. QuickSight supports a number of data connectors.</li>
				<li>In the <strong class="bold">Data source name</strong> textbox, enter the name <strong class="source-inline">yelp_reviews</strong> and click <strong class="bold">Create data source</strong>. Also, click on <strong class="bold">validated</strong>.<div id="_idContainer144" class="IMG---Figure"><img src="Images/B17528_07_13.jpg" alt="Figure 7.13 – Creating an Athena data source in QuickSight&#13;&#10;" width="683" height="302"/></div><p class="figure-caption">Figure 7.13 – Creating an Athena data source in QuickSight</p></li>
				<li>Next, you will be selecting the <strong class="bold">Yelp</strong> database <a id="_idIndexMarker456"/>we created in the Glue Data Catalog and the <strong class="bold">reviews_parquet</strong> table.<div id="_idContainer145" class="IMG---Figure"><img src="Images/B17528_07_14.jpg" alt="Figure 7.14 – Select the table to visualize in QuickSight&#13;&#10;" width="685" height="553"/></div><p class="figure-caption">Figure 7.14 – Select the table to visualize in QuickSight</p></li>
				<li>Click on <strong class="bold">Save and Visualize</strong>.</li>
			</ol>
			<p>In this <a id="_idIndexMarker457"/>section, we covered how you can create a QuickSight dashboard to analyze Yelp reviews. In the next section, we will talk about deleting the resources you created during this exercise to avoid incurring charges.</p>
			<h3>Cleaning up</h3>
			<p>In this section, we will walk you through code samples in the notebook to clean up AWS resources <a id="_idIndexMarker458"/>created while going through the Yelp sentiment analysis solution to avoid incurring a cost.</p>
			<p>Run the cleanup steps to delete the resources you have created for this notebook by running the following code:</p>
			<p class="source-code">response = glue.delete_crawler(Name=parq_crawler_name)</p>
			<p class="source-code">response = glue.delete_job(JobName=glue_job_name)</p>
			<p class="source-code">response = glue.delete_database(</p>
			<p class="source-code">    CatalogId = account_id,</p>
			<p class="source-code">    Name = database_name</p>
			<p class="source-code">)</p>
			<p class="source-code">workshop.delete_bucket_completely(bucket)</p>
			<p>We deleted the Glue crawlers, Glue ETL jobs, and the database we created using the notebook here. Let's move on to the next section to wrap it up.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor104"/>Summary</h1>
			<p>In this chapter, we covered how you can set up a text analytics solution with your existing social media analytics workflow. We gave a specific example of using the Yelp reviews dataset and using serverless ETL with NLP using Amazon Comprehend to set up a quick visual dashboard using Amazon QuickSight. We also covered ad hoc SQL analytics using Amazon Athena to understand the voice or sentiment of the majority of your users using some easy SQL queries. This solution can be implemented with any social media integration, such as Twitter, Reddit, and Facebook, in batch or real-time mode.</p>
			<p>In the case of a real-time setup, you would integrate Kinesis Data Firehose to have near real-time streaming tweets or social media feeds in this proposed workflow or architecture. Check out the <em class="italic">Further reading</em> section for a really cool <strong class="bold">AI-driven social media dashboard</strong> to implement this architecture at scale.</p>
			<p>Another approach you can take in terms of document automation is to have Amazon Textract extract data from your PDFs in the case of RFPs or agreements, and this pipeline can be used to gather sentiment quickly paragraph-wise after performing Glue ETL on the extracted text.</p>
			<p>In the next chapter, we will talk about how you can use AI to automate media workflows to reduce costs and monetize content.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor105"/>Further reading</h1>
			<ul>
				<li><em class="italic">How to scale sentiment analysis using Amazon Comprehend, AWS Glue and Amazon Athena</em> by Roy Hasson (<a href="https://aws.amazon.com/blogs/machine-learning/how-to-scale-sentiment-analysis-using-amazon-comprehend-aws-glue-and-amazon-athena/">https://aws.amazon.com/blogs/machine-learning/how-to-scale-sentiment-analysis-using-amazon-comprehend-aws-glue-and-amazon-athena/</a>)</li>
				<li><em class="italic">AI-Driven Social Media Dashboard </em>(<a href="https://aws.amazon.com/solutions/implementations/ai-driven-social-media-dashboard/">https://aws.amazon.com/solutions/implementations/ai-driven-social-media-dashboard/</a>)</li>
				<li><em class="italic">Harmonize, Query, and Visualize Data from Various Providers using AWS Glue, Amazon Athena, and Amazon QuickSight</em> by Ben Snively (<a href="https://aws.amazon.com/blogs/big-data/harmonize-query-and-visualize-data-from-various-providers-using-aws-glue-amazon-athena-and-amazon-quicksight/">https://aws.amazon.com/blogs/big-data/harmonize-query-and-visualize-data-from-various-providers-using-aws-glue-amazon-athena-and-amazon-quicksight/</a>)</li>
			</ul>
		</div>
	</div></body></html>