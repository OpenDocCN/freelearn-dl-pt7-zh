<html><head></head><body>
		<div>
			<div id="_idContainer172" class="Content">
			</div>
		</div>
		<div id="_idContainer173" class="Content">
			<h1 id="_idParaDest-169">6. <a id="_idTextAnchor187"/>Neural Networks and Deep Learning</h1>
		</div>
		<div id="_idContainer207" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, you will be introduced to the final topic on neural networks and deep learning. You will be learning about TensorFlow, Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs). You will use key deep learning concepts to determine creditworthiness of individuals and predict housing prices in a neighborhood. Later on, you will also implement an image classification program using the skills you learned. By the end of this chapter, you will have a firm grasp on the concepts of neural networks and deep learning.</p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor188"/>Introduction</h1>
			<p>In the previous chapter, we learned <a id="_idTextAnchor189"/>about what clustering problems are and saw several algorithms, such as k-means, that can automatically group data points on their own. In this chapter, we will learn about neural networks and deep learning networks. </p>
			<p>The difference between neural networks and deep learning networks is the complexity and depth of the networks. Traditionally, neural networks have only one hidden layer, while deep learning networks have more than that. </p>
			<p>Although we will use neural networks and deep learning for supervised learning, note that neural networks can also model unsupervised learning techniques. This kind of model was actually quite popular in the 1980s, but because the computation power required was limited at the time, it's only recently that this model has been widely adopted. With the democratization of Graphics Processing Units (GPUs) and cloud computing, we now have access to a tremendous amount of computation power. This is the main reason why neural networks and especially deep learning are hot topics again. </p>
			<p>Deep learning can model more complex patterns than traditional neural networks, and so deep learning is more widely used nowadays in computer vision (in applications such as face detection and image recognition) and natural language processing (in applications such as chatbots and text generation). </p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor190"/>Artificial Neurons</h1>
			<p><strong class="bold">Artificial Neural Networks</strong> (<strong class="bold">ANNs</strong>), as the name implies, try to replicate how a human brain works, and more specifically how neurons work.</p>
			<p>A neuron is a cell in the brain that communicates with other cells via electrical signals. Neurons can respond to stimuli such as sound, light, and touch. They can also trigger actions such as muscle contractions. On average, a human brain contains 10 to 20 billion neurons. That's a pretty huge network, right? This is the reason why humans can achieve so many amazing things. This is also why researchers have tried to emulate how the brain operates and in doing so created ANNs.</p>
			<p>ANNs are composed of multiple artificial neurons that connect to each other and form a network. An artificial neuron is simply a processing unit that performs mathematical operations on some inputs (<strong class="source-inline">x1</strong>, <strong class="source-inline">x2</strong>, …, <strong class="source-inline">xn</strong>) and returns the final results (<strong class="source-inline">y</strong>) to the next unit, as shown here:</p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="image/B16060_06_01.jpg" alt="Figure 6.1: Representation of an artificial neuron&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1: Representation of an artificial neuron</p>
			<p>We will see how an artificial neuron works more in detail in the coming sections.</p>
			<h1 id="_idParaDest-172">N<a id="_idTextAnchor191"/>eurons in TensorFlow</h1>
			<p>TensorFlow is currently the most popular neural network and deep learning framework. It was created and is maintained by Google. TensorFlow is used for voice recognition and voice search, and it is also the brain behind <a href="http://translate.google.com">translate.google.com</a>. Later in this chapter, we will use TensorFlow to recognize written characters.</p>
			<p>The TensorFlow API is available in many languages, including Python, JavaScript, Java, and C. TensorFlow works with <strong class="bold">tensors</strong>. You can think of a tensor as a container composed of a matrix (usually with high dimensions) and additional information related to the operations it will perform (such as weights and biases, which you will be looking at later in this chapter). A tensor with no dimensions (with no rank) is a scalar. A tensor of rank 1 is a vector, rank 2 tensors are matrices, and a rank 3 tensor is a three-dimensional matrix. The rank indicates the dimensions of a tensor. In this chapter, we will be looking at tensors of ranks 2 and 3.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Mathematicians use the terms matrix and dimension, whereas deep learning programmers use tensor and rank instead.</p>
			<p>TensorFlow also comes with mathematical functions to transform tensors, such as the following:</p>
			<ul>
				<li><strong class="bold">Arithmetic operations</strong>: <strong class="source-inline">add</strong> and <strong class="source-inline">multiply</strong></li>
				<li><strong class="bold">Exponential operations</strong>: <strong class="source-inline">exp</strong> and <strong class="source-inline">log</strong></li>
				<li><strong class="bold">Relational operations</strong>: <strong class="source-inline">greater</strong>, <strong class="source-inline">less</strong>, and <strong class="source-inline">equal</strong></li>
				<li><strong class="bold">Array operations</strong>: <strong class="source-inline">concat</strong>, <strong class="source-inline">slice</strong>, and <strong class="source-inline">split</strong></li>
				<li><strong class="bold">Matrix operations</strong>: <strong class="source-inline">matrix_inverse</strong>, <strong class="source-inline">matrix_determinant</strong>, and <strong class="source-inline">matmul</strong></li>
				<li><strong class="bold">Non-linear operations</strong>: <strong class="source-inline">sigmoid</strong>, <strong class="source-inline">relu</strong>, and <strong class="source-inline">softmax</strong></li>
			</ul>
			<p>We will go through them in more detail later in this chapter.</p>
			<p>In the next exercise, we will be using TensorFlow to compute an artificial neuron.</p>
			<h2 id="_idParaDest-173">E<a id="_idTextAnchor192"/>xercise 6.01: Using Basic Operations and TensorFlow Constants</h2>
			<p>In this exercise, we will be using arithmetic operations in TensorFlow to emulate an artificial neuron by performing a matrix multiplication and addition, and applying a non-linear function, <strong class="source-inline">sigmoid</strong>.</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li>Open a new Jupyter Notebook file.</li>
				<li>Import the <strong class="source-inline">tensorflow</strong> package as <strong class="source-inline">tf</strong>:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create a tensor called <strong class="source-inline">W</strong> of shape <strong class="source-inline">[1,6]</strong> (that is, with 1 row and 6 columns), using <strong class="source-inline">tf.constant()</strong>, that contains the matrix <strong class="source-inline">[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]</strong>. Print its value:<p class="source-code">W = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[1, 6]) </p><p class="source-code">W</p><p>The expected output is this:</p><p class="source-code">&lt;tf.Tensor: shape=(1, 6), dtype=float32, numpy=array([[1., 2., 3., 4., 5., 6.]], dtype=float32)&gt;</p></li>
				<li>Create a tensor called <strong class="source-inline">X</strong> of shape <strong class="source-inline">[6,1]</strong> (that is, with 6 rows and 1 column), using <strong class="source-inline">tf.constant()</strong>, that contains <strong class="source-inline">[7.0, 8.0, 9.0, 10.0, 11.0, 12.0]</strong>. Print its value:<p class="source-code">X = tf.constant([7.0, 8.0, 9.0, 10.0, 11.0, 12.0], \</p><p class="source-code">                shape=[6, 1]) </p><p class="source-code">X</p><p>The expected output is this:</p><p class="source-code">&lt;tf.Tensor: shape=(6, 1), dtype=float32, numpy= </p><p class="source-code">array([[ 7.],</p><p class="source-code">       [ 8.],</p><p class="source-code">       [ 9.],</p><p class="source-code">       [10.],</p><p class="source-code">       [11.],</p><p class="source-code">       [12.]], dtype=float32)&gt;</p></li>
				<li>Now, create a tensor called <strong class="source-inline">b</strong>, using <strong class="source-inline">tf.constant()</strong>, that contains <strong class="source-inline">-88</strong>. Print its value:<p class="source-code">b = tf.constant(-88.0)</p><p class="source-code">b</p><p>The expected output is this:</p><p class="source-code">&lt;tf.Tensor: shape=(), dtype=float32, numpy=-88.0&gt;</p></li>
				<li>Perform a matrix multiplication between <strong class="source-inline">W</strong> and <strong class="source-inline">X</strong> using <strong class="source-inline">tf.matmul</strong>, save its results in the <strong class="source-inline">mult</strong> variable, and print its value:<p class="source-code">mult = tf.matmul(W, X)</p><p class="source-code">mult</p><p>The expected output is this:</p><p class="source-code">&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[217.]], dtype=float32)&gt;</p></li>
				<li>Perform a matrix addition between <strong class="source-inline">mult</strong> and <strong class="source-inline">b</strong>, save its results in a variable called <strong class="source-inline">Z</strong>, and print its value:<p class="source-code">Z = mult + b</p><p class="source-code">Z</p><p>The expected output is this:</p><p class="source-code">&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[129.]], dtype=float32)&gt;</p></li>
				<li>Apply the <strong class="source-inline">sigmoid</strong> function to <strong class="source-inline">Z</strong> using <strong class="source-inline">tf.math.sigmoid</strong>, save its results in a variable called <strong class="source-inline">a</strong>, and print its value. The <strong class="source-inline">sigmoid </strong>function transforms any numerical value within the range <strong class="bold">0</strong> to <strong class="bold">1</strong> (we will learn more about this in the following sections):<p class="source-code">a = tf.math.sigmoid(Z)</p><p class="source-code">a</p><p>The expected output is this:</p><p class="source-code">&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)&gt;</p></li>
			</ol>
			<p>The <strong class="source-inline">sigmoid</strong> function has transformed the original value of <strong class="source-inline">Z</strong>, which was <strong class="source-inline">129</strong>, to <strong class="source-inline">1</strong>. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31ekGLM">https://packt.live/31ekGLM</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3evuKnC">https://packt.live/3evuKnC</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>In this exercise, you successfully implemented an artificial neuron using TensorFlow. This is the base of any neural network model.</p>
			<p>In the next section, we will be looking at the architecture of neural networks.</p>
			<h1 id="_idParaDest-174">N<a id="_idTextAnchor193"/>eural Network Architecture</h1>
			<p>Neural networks are the newest branch of <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>). Neural networks are inspired by how the human brain works. They were invented in the 1940s by Warren McCulloch and Walter Pitts. The neural network was a mathematical model that was used to describe how the human brain can solve problems.</p>
			<p>We will use ANN to refer to both the mathematical model, and the biological neural network when talking about the human brain.</p>
			<p>The way a neural network learns is more complex compared to other classification or regression models. The neural network model has a lot of internal variables, and the relationship between the input and output variables may involve multiple internal layers. Neural networks have higher accuracy than other supervised learning algorithms.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Mastering neural networks with TensorFlow is a complex process. The purpose of this section is to provide you with an introductory resource to get started.</p>
			<p>In this chapter, the main example we are going to use is the recognition of digits from an image. We are considering this format since each image is small, and we have around 70,000 images available. The processing power required to process these images is similar to that of a regular computer.</p>
			<p>ANNs work similarly to how the human brain works. A dendroid in a human brain is connected to a nucleus, and the nucleus is connected to an axon. In an ANN, the input is the dendroid, where the calculations occur is the nucleus, and the output is the axon.</p>
			<p>An artificial neuron is designed to replicate how a nucleus works. It will transform an input signal by calculating a matrix multiplication followed by an activation function. If this function determines that a neuron has to fire, a signal appears in the output. This signal can be the input of other neurons in the network:</p>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/B16060_06_02.jpg" alt="Figure 6.2: Figure showing how an ANN works&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2: Figure showing how an ANN works</p>
			<p>Let's understand the preceding figure further by taking the example of <strong class="source-inline">n=4</strong>. In this case, the following applies:</p>
			<ul>
				<li><strong class="source-inline">X</strong> is the input matrix, which is composed of <strong class="source-inline">x1</strong>, <strong class="source-inline">x2</strong>, <strong class="source-inline">x3</strong>, and <strong class="source-inline">x4</strong>. </li>
				<li><strong class="source-inline">W</strong>, the weight matrix, will be composed of <strong class="source-inline">w1</strong>, <strong class="source-inline">w2</strong>, <strong class="source-inline">w3</strong>, and <strong class="source-inline">w4</strong>.</li>
				<li><strong class="source-inline">b</strong> is the bias.</li>
				<li><strong class="source-inline">f</strong> is the activation function.</li>
			</ul>
			<p>We will first calculate <strong class="source-inline">Z</strong> (the left-hand side of the neuron) with matrix multiplication and bias:</p>
			<p class="source-code">Z = W * X + b = x1*w1 + x2*w2 + x3*w3 + x4*w4 + b</p>
			<p>Then the output, <strong class="source-inline">y</strong>, will be calculated by applying a function, <strong class="source-inline">f</strong>:</p>
			<p class="source-code">y = f(Z) = f(x1*w1 + x2*w2 + x3*w3 + x4*w4 + b)</p>
			<p>Great – this is how an artificial neuron works under the hood. It is two matrix operations, a product followed by a sum, and a function transformation.</p>
			<p>We now move on to the next section – weights.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor194"/>Weights</h2>
			<p><strong class="bold">W</strong> (<em class="italic">also called the weight matrix</em>) refers to weights, which are parameters that are automatically learned by neural networks in order to predict accurately the output, <strong class="source-inline">y</strong>.</p>
			<p>A single neuron is the combination of the weighted sum and the activation function and can be referred to as a hidden layer. A neural network with one hidden layer is called a <strong class="bold">regular neural network</strong>:</p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/B16060_06_03.jpg" alt="Figure 6.3: Neurons 1, 2, and 3 form the hidden layer of this sample network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3: Neurons 1, 2, and 3 form the hidden layer of this sample network</p>
			<p>When connecting inputs and outputs, we may have multiple hidden layers. A neural network with multiple layers is called a <strong class="bold">deep neural network</strong>.</p>
			<p>The term deep learning comes from the presence of multiple layers. When creating an <strong class="bold">Artificial Neural Network</strong> (<strong class="bold">ANN</strong>), we can specify the number of hidden layers.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor195"/>Biases</h2>
			<p>Previously, we saw that the equation for a neuron is as follows:</p>
			<p class="source-code">y = f(x1*w1 + x2*w2 + x3*w3 + x4*w4)</p>
			<p>The problem with this equation is that there is no constant factor that depends on the inputs <strong class="source-inline">x1</strong>, <strong class="source-inline">x2</strong>, <strong class="source-inline">x3</strong>, and <strong class="source-inline">x4</strong>. The preceding equation can model any linear function that will go through the point 0: if all <strong class="source-inline">w</strong> values are equal to 0 then <strong class="source-inline">y</strong> will also equal to 0. But what about other functions that don't go through the point 0? For example, imagine that we are predicting the probability of churn for an employee by their month of tenure. Even if they haven't worked for the full month yet, the probability of churn is not zero. </p>
			<p>To accommodate this situation, we need to introduce a new parameter called <strong class="bold">bias</strong>. It is a constant that is also referred to as the <strong class="bold">intercept</strong>. Using the churn example, the bias <strong class="source-inline">b</strong> can equal to 0.5 and therefore the churn probability for a new employer during the first month will be 50%.</p>
			<p>Therefore, we add bias to the equation:</p>
			<p class="source-code">y = f(x1*w1 + x2*w2 + x3*w3 + x4*w4 + b)</p>
			<p class="source-code">y = f(x  w + b)</p>
			<p>The first equation is the verbose form, describing the role of each coordinate, weight coefficient, and bias. The second equation is the vector form, where <strong class="source-inline">x = (x1, x2, x3, x4)</strong> and <strong class="source-inline">w = (w1, w2, w3, w4)</strong>. The dot operator between the vectors symbolizes the dot or scalar product of the two vectors. The two equations are equivalent. We will use the second form in practice because it is easier to define a vector of variables using TensorFlow than to define each variable one by one.</p>
			<p>Similarly, for <strong class="source-inline">w1</strong>, <strong class="source-inline">w2</strong>, <strong class="source-inline">w3</strong>, and <strong class="source-inline">w4</strong>, the bias, <strong class="source-inline">b</strong>, is a variable, meaning that its value can change during the learning process.</p>
			<p>With this constant factor built into each neuron, a neural network model becomes more flexible in terms of fitting a specific training dataset better.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">It may happen that the product <strong class="source-inline">p = x1*w1 + x2*w2 + x3*w3 + x4*w4</strong> is negative due to the presence of a few negative weights. We may still want to give the model the flexibility to execute (<em class="italic">or fire</em>) a neuron with values above a given negative number. Therefore, adding a constant bias, <strong class="source-inline">b = 5</strong>, for instance, can ensure that the neuron fires for values between <strong class="source-inline">-5</strong> and <strong class="source-inline">0</strong> as well.</p>
			<p>TensorFlow provides the <strong class="source-inline">Dense()</strong> class to model the hidden layer of a neural network (<em class="italic">also called the fully connected layer</em>):</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">layer1 = layers.Dense(units=128, input_shape=[200])</p>
			<p>In this example, we have created a fully connected layer of <strong class="source-inline">128</strong> neurons that takes as input a tensor of shape <strong class="source-inline">200</strong>. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find more information on this TensorFlow class at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense</a>.</p>
			<p>The <strong class="source-inline">Dense()</strong> class is expected to have a flattened input (only one row). For instance, if your input is of shape <strong class="source-inline">28</strong> by <strong class="source-inline">28</strong>, you will have to flatten it beforehand with the <strong class="source-inline">Flatten()</strong> class in order to get a single row with 784 neurons (<strong class="source-inline">28 * 28</strong>):</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">input_layer = layers.Flatten(input_shape=(28, 28))</p>
			<p class="source-code">layer1 = layers.Dense(units=128)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find more information on this TensorFlow class at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten</a>.</p>
			<p>In the following sections, we will learn about how we can extend this layer of neurons with additional parameters.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor196"/>Use Cases for ANNs</h2>
			<p>ANNs have their place among supervised learning techniques. They can model both classification and regression problems. A classifier neural network seeks a relationship between features and labels. The features are the input variables, while each class the classifier can choose as a return value is a separate output. In the case of regression, the input variables are the features, while there is one single output: the predicted value. While traditional classification and regression techniques have their use cases in AI, ANNs are generally better at finding complex relationships between inputs and outputs.</p>
			<p>In the next section, we will be looking at activation functions and their different types.</p>
			<h1 id="_idParaDest-178">Act<a id="_idTextAnchor197"/>ivation Functions</h1>
			<p>As seen previously, a single neuron needs to perform a transformation by applying an activation function. Different activation functions can be used in neural networks. Without these functions, a neural network would simply be a linear model that could easily be described using matrix multiplication.</p>
			<p>The activation function of a neural network provides non-linearity and therefore can model more complex patterns. Two very common activation functions are <strong class="source-inline">sigmoid</strong> and <strong class="source-inline">tanh</strong> (the hyperbolic tangent function).</p>
			<h2 id="_idParaDest-179">Sig<a id="_idTextAnchor198"/>moid</h2>
			<p>The formula of <strong class="source-inline">sigmoid</strong> is as follows:</p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/B16060_06_04.jpg" alt="Figure 6.4: The sigmoid formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4: The sigmoid formula</p>
			<p>The output values of a <strong class="source-inline">sigmoid</strong> function range from <strong class="bold">0</strong> to <strong class="bold">1</strong>. This activation function is usually used at the last layer of a neural network for a binary classification problem.</p>
			<h2 id="_idParaDest-180">Tanh<a id="_idTextAnchor199"/></h2>
			<p>The formula of the hyperbolic tangent is as follows:</p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="image/B16060_06_05.jpg" alt="Figure 6.5: The tanh formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5: The tanh formula</p>
			<p>The <strong class="source-inline">tanh</strong> activation function is very similar to the <strong class="source-inline">sigmoid</strong> function and was quite popular until recently. It is usually used in the hidden layers of a neural network. Its values range between <strong class="bold">-1</strong> and <strong class="bold">1</strong>.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor200"/>ReLU</h2>
			<p><a id="_idTextAnchor201"/>Another important activation function is <strong class="source-inline">relu</strong>. <strong class="bold">ReLU</strong> stands for <strong class="bold">Rectified Linear Unit</strong>. It is currently the most widely used activation function for hidden layers. Its formula is as follows:</p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="image/B16060_06_06.jpg" alt="Figure 6.6: The ReLU formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6: The ReLU formula</p>
			<p>There are now different variants of <strong class="source-inline">relu</strong> functions, such as <strong class="source-inline">leaky ReLU</strong> and <strong class="source-inline">PReLU</strong>.</p>
			<h2 id="_idParaDest-182">Softma<a id="_idTextAnchor202"/>x</h2>
			<p>The function shrinks the values of a list to be between <strong class="bold">0</strong> and <strong class="bold">1</strong> so that the sum of the elements of the list becomes <strong class="bold">1</strong>. The definition of the <strong class="source-inline">softmax</strong> function is as follows:</p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/B16060_06_07.jpg" alt="Figure 6.7: The softmax formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7: The softmax formula</p>
			<p>The <strong class="source-inline">softmax</strong> function is usually used as the last layer of a neural network for multi-class classification problems as it can generate probabilities for each of the different output classes.</p>
			<p>Remember, in TensorFlow, we can extend a <strong class="source-inline">Dense()</strong> layer with an activation function; we just need to set the <strong class="source-inline">activation</strong> parameter. In the following example, we will add the <strong class="source-inline">relu</strong> activation function:</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">layer1 = layers.Dense(units=128, input_shape=[200], \</p>
			<p class="source-code">                      activation='relu')</p>
			<p>Let's use these different activation functions and observe how these functions dampen the weighted inputs by solving the following exercise. </p>
			<h2 id="_idParaDest-183">Exercis<a id="_idTextAnchor203"/>e 6.02: Activation Functions</h2>
			<p>In this exercise, we will be implementing the following activation functions using the <strong class="source-inline">numpy</strong> package: <strong class="source-inline">sigmoid</strong>, <strong class="source-inline">tanh</strong>, <strong class="source-inline">relu</strong>, and <strong class="source-inline">softmax</strong>.</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Import the <strong class="source-inline">numpy</strong> package as <strong class="source-inline">np</strong>:<p class="source-code">import numpy as np</p></li>
				<li>Create a <strong class="source-inline">sigmoid</strong> function, as shown in the following code snippet, that implements the sigmoid formula (shown in the previous section) using the <strong class="source-inline">np.exp()</strong> method:<p class="source-code">def sigmoid(x): </p><p class="source-code">    return 1 / (1 + np.exp(-x))</p></li>
				<li>Calculate the result of <strong class="source-inline">sigmoid</strong> function on the value <strong class="source-inline">-1</strong>:<p class="source-code">sigmoid(-1)</p><p>The expected output is this:</p><p class="source-code">0.2689414213699951</p><p>This is the result of performing a sigmoid transformation on the value <strong class="source-inline">-1</strong>.</p></li>
				<li>Import the <strong class="source-inline">matplotlib.pyplot</strong> package as <strong class="source-inline">plt</strong>:<p class="source-code">import matplotlib.pyplot as plt</p></li>
				<li>Create a <strong class="source-inline">numpy</strong> array called <strong class="source-inline">x</strong> that contains values from <strong class="source-inline">-10</strong> to <strong class="source-inline">10</strong> evenly spaced by an increment of <strong class="source-inline">0.1</strong>, using the <strong class="source-inline">np.arange()</strong> method. Print its value:<p class="source-code">x = np.arange(-10, 10, 0.1)</p><p class="source-code">x</p><p>The expected output is this:</p><p class="source-code">array([-1.00000000e+01, -9.90000000e+00, -9.80000000e+00,</p><p class="source-code">       -9.70000000e+00, -9.60000000e+00, -9.50000000e+00,</p><p class="source-code">       -9.40000000e+00, -9.30000000e+00, -9.20000000e+00,</p><p class="source-code">       -9.10000000e+00, -9.00000000e+00, -8.90000000e+00,</p><p class="source-code">       -8.80000000e+00, -8.70000000e+00, -8.60000000e+00,</p><p class="source-code">       -8.50000000e+00, -8.40000000e+00, -8.30000000e+00,</p><p class="source-code">       -8.20000000e+00, -8.10000000e+00, -8.00000000e+00,</p><p class="source-code">       -7.90000000e+00, -7.80000000e+00, -7.70000000e+00,</p><p class="source-code">       -7.60000000e+00, -7.50000000e+00, -7.40000000e+00,</p><p class="source-code">       -7.30000000e+00, -7.20000000e+00, -7.10000000e+00,</p><p class="source-code">       -7.00000000e+00, -6.90000000e+00,</p><p>Great – we generated a <strong class="source-inline">numpy</strong> array containing values between <strong class="source-inline">-10</strong> and <strong class="source-inline">10</strong>.</p><p class="callout-heading">Note</p><p class="callout">The preceding output is truncated.</p></li>
				<li>Plot a line chart with <strong class="source-inline">x</strong> and <strong class="source-inline">sigmoid(x)</strong> using <strong class="source-inline">plt.plot()</strong> and <strong class="source-inline">plt.show()</strong>:<p class="source-code">plt.plot(x, sigmoid(x))</p><p class="source-code">plt.show()</p><p>The expected output is this:</p><div id="_idContainer181" class="IMG---Figure"><img src="image/B16060_06_08.jpg" alt="Figure 6.8: Line chart using the sigmoid function&#13;&#10;"/></div><p class="figure-caption">Figure 6.8: Line chart using the sigmoid function</p><p>We can see here that the output of the <strong class="source-inline">sigmoid</strong> function ranges between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. The slope is quite steep for values around <strong class="source-inline">0</strong>.</p></li>
				<li>Create a <strong class="source-inline">tanh()</strong> function that implements the Tanh formula (shown in the previous section) using the <strong class="source-inline">np.exp()</strong> method:<p class="source-code">def tanh(x): </p><p class="source-code">    return 2 / (1 + np.exp(-2*x)) - 1</p></li>
				<li>Plot a line chart with <strong class="source-inline">x</strong> and <strong class="source-inline">tanh(x)</strong> using <strong class="source-inline">plt.plot()</strong> and <strong class="source-inline">plt.show()</strong>:<p class="source-code">plt.plot(x, tanh(x))</p><p class="source-code">plt.show()</p><p>The expected output is this:</p><div id="_idContainer182" class="IMG---Figure"><img src="image/B16060_06_09.jpg" alt="Figure 6.9: Line chart using the tanh function&#13;&#10;"/></div><p class="figure-caption">Figure 6.9: Line chart using the tanh function</p><p>The shape of the <strong class="source-inline">tanh</strong> function is very similar to <strong class="source-inline">sigmoid</strong> but its slope is steeper for values close to <strong class="source-inline">0</strong>. Remember, its range is between <strong class="bold">-1</strong> and <strong class="bold">1</strong>.</p></li>
				<li>Create a <strong class="source-inline">relu</strong> function that implements the ReLU formula (shown in the previous section) using the <strong class="source-inline">np.maximum()</strong> method:<p class="source-code">def relu(x):</p><p class="source-code">    return np.maximum(0, x)</p></li>
				<li>Plot a line chart with <strong class="source-inline">x</strong> and <strong class="source-inline">relu(x)</strong> using <strong class="source-inline">plt.plot()</strong> and <strong class="source-inline">plt.show()</strong>:<p class="source-code">plt.plot(x, relu(x))</p><p class="source-code">plt.show()</p><p>The expected output is this:</p><div id="_idContainer183" class="IMG---Figure"><img src="image/B16060_06_10.jpg" alt="Figure 6.10: Line chart using the relu function&#13;&#10;"/></div><p class="figure-caption">Figure 6.10: Line chart using the relu function</p><p>The ReLU function equals <strong class="source-inline">0</strong> when values are negative, and equals the identity function, <strong class="source-inline">f(x)=x</strong>, for positive values.</p></li>
				<li>Create a <strong class="source-inline">softmax</strong> function that implements the softmax formula (shown in the previous section) using the <strong class="source-inline">np.exp()</strong> method:<p class="source-code">def softmax(list): </p><p class="source-code">    return np.exp(list) / np.sum(np.exp(list))</p></li>
				<li>Calculate the output of <strong class="source-inline">softmax</strong> on the list of values, <strong class="source-inline">[0, 1, 168, 8, 2]</strong>:<p class="source-code">result = softmax( [0, 1, 168, 8, 2]) </p><p class="source-code">result</p><p>The expected output is this:</p><p class="source-code">array([1.09276566e-73, 2.97044505e-73, 1.00000000e+00, </p><p class="source-code">       3.25748853e-70, 8.07450679e-73])</p></li>
			</ol>
			<p>As expected, the item at the third position has the highest softmax probabilities as its original value was the highest.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fJzoOU">https://packt.live/3fJzoOU</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3188pZi">https://packt.live/3188pZi</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>By completing this exercise, we have implemented some of the most important activation functions for neural networks.</p>
			<h1 id="_idParaDest-184">Forward Pr<a id="_idTextAnchor204"/>opagation and the Loss Function</h1>
			<p>So far, we have seen how a neuron can take an input and perform some mathematical operations on it and get an output. We learned that a neural network is a combination of multiple layers of neurons. </p>
			<p>The process of transforming the inputs of a neural network into a result is called <strong class="bold">forward propagation</strong> (or the forward pass). What we are asking the neural network to do is to make a prediction (the final output of the neural network) by applying multiple neurons to the input data:</p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/B16060_06_11.jpg" alt="Figure 6.11: Figure showing forward propagation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11: Figure showing forward propagation</p>
			<p>The neural network relies on the weights matrices, biases, and activation function of each neuron to calculate the predicted output value, <img src="image/B16060_06_11a.png" alt="b"/>. For now, let's assume the values of the weight matrices and biases are set in advance. The activation functions are defined when you design the architecture of the neural networks.</p>
			<p>As for any supervised machine learning algorithm, the goal is to make accurate predictions. This implies that we need to assess how accurate the predictions are compared to the true values. For traditional machine learning algorithms, we used scoring metrics such as mean squared error, accuracy, or the F<span class="subscript">1</span> score. This can also be applied to neural networks, but the only difference is that such scores are used in two different ways:</p>
			<ul>
				<li>They are used by data scientists to assess the performance of a model on training and testing sets and then tune hyperparameters if needed. This also applies to neural networks, so nothing new here.</li>
				<li>They are used by neural networks to automatically learn from mistakes and update weight matrices and biases. This will be explained in more detail in the next section, which is about backpropagation. So, the neural network will use a metric (also called a <strong class="bold">loss function</strong>) to compare its predicted values, <img src="image/B16060_06_11b.png" alt="38"/> to the true label, (y), and then learn how to make better predictions automatically.</li>
			</ul>
			<p>The loss function is critical to a neural network learning to make good predictions. This is a hyperparameter that needs to be defined by data scientists while designing the architecture of a neural network. The choice of which loss function to use is totally arbitrary and depending on the dataset or the problem you want to solve, you will pick one or another. Luckily for us, though, there are some basic rules of thumb that work in most cases:</p>
			<ul>
				<li>If you are working on a regression problem, you can use mean squared error.</li>
				<li>If it is a binary classification, the loss function should be binary cross-entropy.</li>
				<li>If it is a multi-class classification, then categorical cross-entropy should be your go-to choice.</li>
			</ul>
			<p>As a final note, the choice of loss function will also define which activation function you will have to use on the last layer of the neural network. Each loss function expects a certain type of data in order to properly assess prediction performance. </p>
			<p>Here is the list of activation functions according to the loss function and type of project/problem:</p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/B16060_06_12.jpg" alt="Figure 6.12: Overview of the different activation functions and their applications&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12: Overview of the different activation functions and their applications</p>
			<p>With TensorFlow, in order to build your custom architecture, you can instantiate the <strong class="source-inline">Sequential()</strong> class and add your layers of fully connected neurons as shown in the following code snippet:</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">model = tf.keras.Sequential()</p>
			<p class="source-code">input_layer = layers.Flatten(input_shape=(28,28))</p>
			<p class="source-code">layer1 = layers.Dense(128, activation='relu')</p>
			<p class="source-code">model.add(input_layer)</p>
			<p class="source-code">model.add(layer1)</p>
			<p>Now it is time to have a look at how a neural network improves its predictions with backpropagation.</p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor205"/>Backpropagation</h1>
			<p>Previously, we learned how a neural network makes predictions by using weight matrices and biases (we can combine them into a single matrix) from its neurons. Using the loss function, a network determines how good or bad the predictions are. It would be great if it could use this information and update the parameters accordingly. This is exactly what backpropagation is about: optimizing a neural network's parameters. </p>
			<p>Training a neural network involves executing forward propagation and backpropagation multiple times in order to make predictions and update the parameters from the errors. During the first pass (or propagation), we start by initializing all the weights of the neural network. Then, we apply forward propagation, followed by backpropagation, which updates the weights.</p>
			<p>We apply this process several times and the neural network will optimize its parameters iteratively. You can decide to stop this learning process by setting the maximum number of times the neural networks will go through the entire dataset (also called epochs) or define an early stop threshold if the neural network's score is not improving anymore after few epochs.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor206"/>Optimizers and the Learning Rate</h1>
			<p>In the previous section, we saw that a neural network follows an iterative process to find the best solution for any input dataset. Its learning process is an optimization process. You can use different optimization algorithms (also called <strong class="bold">optimizers</strong>) for a neural network. The most popular ones are <strong class="source-inline">Adam</strong>, <strong class="source-inline">SGD</strong>, and <strong class="source-inline">RMSprop</strong>.</p>
			<p>One important parameter for the neural networks optimizer is the learning rate. This value defines how quickly the neural network will update its weights. Defining a too-low learning rate will slow down the learning process and the neural network will take a long time before finding the right parameters. On the other hand, having too-high a learning rate can make the neural network not learn a solution as it is making bigger weight changes than required. A good practice is to start with a not-too-small learning rate (such as <strong class="bold">0.01</strong> or <strong class="bold">0.001</strong>), then stop the neural network training once its score starts to plateau or get worse, and lower the learning rate (by an order of magnitude, for instance) and keep training the network.</p>
			<p>With TensorFlow, you can instantiate an optimizer from <strong class="source-inline">tf.keras.optimizers</strong>. For instance, the following code snippet shows us how to create an <strong class="source-inline">Adam</strong> optimizer with <strong class="source-inline">0.001</strong> as the learning rate and then compile our neural network by specifying the loss function (<strong class="source-inline">'sparse_categorical_crossentropy'</strong>) and metrics to be displayed (<strong class="source-inline">'accuracy'</strong>):</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p>
			<p class="source-code">model.compile(loss='sparse_categorical_crossentropy', \</p>
			<p class="source-code">              optimizer=optimizer, metrics=['accuracy'])</p>
			<p>Once the model is compiled, we can then train the neural network with the <strong class="source-inline">.fit()</strong> method like this:</p>
			<p class="source-code">model.fit(features_train, label_train, epochs=5)</p>
			<p>Here we trained the neural network on the training set for <strong class="source-inline">5</strong> epochs. Once trained, we can use the model on the testing set and assess its performance with the <strong class="source-inline">.evaluate()</strong> method:</p>
			<p class="source-code">model.evaluate(features_test, label_test)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find more information on this TensorFlow optimizers at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers</a>.</p>
			<p><a id="_idTextAnchor207"/>In the next exercise, we will be training a neural network on a dataset. </p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor208"/>Exercise 6.03: Classifying Credit Approval</h2>
			<p>In this exercise, we will be using the German credit approval dataset, and train a neural network to classify whether an individual is creditworthy or not.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset file can also be found in our GitHub repository:</p>
			<p class="callout"><a href="https://packt.live/2V7uiV5">https://packt.live/2V7uiV5</a>.</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Import the <strong class="source-inline">loadtxt</strong> method from <strong class="source-inline">numpy</strong>:<p class="source-code">from numpy import loadtxt</p></li>
				<li>Create a variable called <strong class="source-inline">file_url</strong> containing the link to the raw dataset:<p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/'\</p><p class="source-code">           'The-Applied-Artificial-Intelligence-Workshop'\</p><p class="source-code">           '/master/Datasets/german_scaled.csv'</p></li>
				<li>Load the data into a variable called <strong class="source-inline">data</strong> using <strong class="source-inline">loadtxt()</strong> and specify the <strong class="source-inline">delimiter=','</strong> parameter. Print its content:<p class="source-code">data = loadtxt(file_url, delimiter=',')</p><p class="source-code">data</p><p>The expected output is this:</p><p class="source-code">array([[0.        , 0.33333333, 0.02941176, ..., 0.      , 1.      ,</p><p class="source-code">        1.        ],</p><p class="source-code">       [1.        , 0.        , 0.64705882, ..., 0.      , 0.      ,</p><p class="source-code">        1.        ],</p><p class="source-code">       [0.        , 1.        , 0.11764706, ..., 1.      , 0.      ,</p><p class="source-code">        1.        ],</p><p class="source-code">       ...,</p><p class="source-code">       [0.        , 1.        , 0.11764706, ..., 0.      , 0.      ,</p><p class="source-code">        1.        ],</p><p class="source-code">       [1.        , 0.33333333, 0.60294118, ..., 0.      , 1.      ,</p><p class="source-code">        1.        ],</p><p class="source-code">       [0.        , 0.        , 0.60294118, ..., 0.      , 0.      ,</p><p class="source-code">        1.        ]])</p></li>
				<li>Create a variable called <strong class="source-inline">label</strong> that contains the data only from the first column (this will be our response variable):<p class="source-code">label = data[:, 0]</p></li>
				<li>Create a variable called <strong class="source-inline">features</strong> that contains all the data except for the first column (which corresponds to the response variable):<p class="source-code">features = data[:, 1:]</p></li>
				<li>Import the <strong class="source-inline">train_test_split</strong> method from <strong class="source-inline">sklearn.model_selection</strong>:<p class="source-code">from sklearn.model_selection import train_test_split</p></li>
				<li>Split the data into training and testing sets and save the results into four variables called <strong class="source-inline">features_train</strong>, <strong class="source-inline">features_test</strong>, <strong class="source-inline">label_train</strong>, and <strong class="source-inline">label_test</strong>. Use 20% of the data for testing and specify <strong class="source-inline">random_state=7</strong>:<p class="source-code">features_train, features_test, \</p><p class="source-code">label_train, label_test = train_test_split(features, \</p><p class="source-code">                                           label, \</p><p class="source-code">                                           test_size=0.2, \</p><p class="source-code">                                           random_state=7)</p></li>
				<li>Import <strong class="source-inline">numpy</strong> as <strong class="source-inline">np</strong>, <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">tf</strong>, and <strong class="source-inline">layers</strong> from <strong class="source-inline">tensorflow.keras</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import layers</p></li>
				<li>Set <strong class="source-inline">1</strong> as the seed for <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong> using <strong class="source-inline">np.random_seed()</strong> and <strong class="source-inline">tf.random.set_seed()</strong>: <p class="source-code">np.random.seed(1)</p><p class="source-code">tf.random.set_seed(1)</p></li>
				<li>Instantantiate a <strong class="source-inline">tf.keras.Sequential()</strong> class and save it into a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Instantantiate a <strong class="source-inline">layers.Dense()</strong> class with <strong class="source-inline">16</strong> neurons, <strong class="source-inline">activation='relu'</strong>, and <strong class="source-inline">input_shape=[19]</strong>, then save it into a variable called <strong class="source-inline">layer1</strong>:<p class="source-code">layer1 = layers.Dense(16, activation='relu', \</p><p class="source-code">                      input_shape=[19])</p></li>
				<li>Instantantiate a second <strong class="source-inline">layers.Dense()</strong> class with <strong class="source-inline">1</strong> neuron and <strong class="source-inline">activation='sigmoid'</strong>, then save it into a variable called <strong class="source-inline">final_layer</strong>:<p class="source-code">final_layer = layers.Dense(1, activation='sigmoid')</p></li>
				<li>Add the two layers you just defined to the model using <strong class="source-inline">.add()</strong>:<p class="source-code">model.add(layer1)</p><p class="source-code">model.add(final_layer)</p></li>
				<li>Instantantiate a <strong class="source-inline">tf.keras.optimizers.Adam()</strong> class with <strong class="source-inline">0.001</strong> as the learning rate and save it into a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the neural network using <strong class="source-inline">.compile()</strong> with <strong class="source-inline">loss='binary_crossentropy'</strong>, <strong class="source-inline">optimizer=optimizer, metrics=['accuracy']</strong> as shown in the following code snippet:<p class="source-code">model.compile(loss='binary_crossentropy', \</p><p class="source-code">              optimizer=optimizer, metrics=['accuracy'])</p></li>
				<li>Print a summary of the model using <strong class="source-inline">.summary()</strong>:<p class="source-code">model.summary()</p><p>The expected output is this:</p><div id="_idContainer188" class="IMG---Figure"><img src="image/B16060_06_13.jpg" alt="Figure 6.13: Summary of the sequential model&#13;&#10;"/></div><p class="figure-caption">Figure 6.13: Summary of the sequential model</p><p>This output summarizes the architecture of our neural networks. We can see it is composed of three layers, as expected, and we know each layer's output size and number of parameters, which corresponds to the weights and biases. For instance, the first layer has <strong class="source-inline">16</strong> neurons and <strong class="source-inline">320</strong> parameters to be learned (weights and biases).</p></li>
				<li>Next, fit the neural networks with the training set and specify <strong class="source-inline">epochs=10</strong>:<p class="source-code">model.fit(features_train, label_train, epochs=10)</p><p>The expected output is this:</p><div id="_idContainer189" class="IMG---Figure"><img src="image/B16060_06_14.jpg" alt="Figure 6.14: Fitting the neural network with the training set&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.14: Fitting the neural network with the training set</p>
			<p>The output provides a lot of information about the training of the neural network. The first line tells us the training set was composed of <strong class="source-inline">800</strong> observations. Then we can see the results of each epoch:</p>
			<p>Total processing time in seconds</p>
			<p>Processing time by data sample in us/sample</p>
			<p>Loss value and accuracy score</p>
			<p>The final result of this neural network is the last epoch (<strong class="source-inline">epoch=10</strong>), where we achieved an accuracy score of <strong class="source-inline">0.6888</strong>. But we can see that the trend was improving: the accuracy score was still increasing after each epoch. So, we may get better results if we train the neural network for longer by increasing the number of epochs or lowering the learning rate.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fMhyLk">https://packt.live/3fMhyLk</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Njghza">https://packt.live/2Njghza</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>By completing this exercise, you just trained your first classifier. In traditional machine learning algorithms, you would need to use more lines of code to achieve this, as you would have to define the entire architecture of the neural network. Here the neural network got <strong class="source-inline">0.6888</strong> after <strong class="source-inline">10</strong> epochs, but it could still improve if we let it train for longer. You can try this on your own.</p>
			<p>Next, we will be looking at regularization.</p>
			<h1 id="_idParaDest-188">Re<a id="_idTextAnchor209"/>gularization</h1>
			<p>As with any machine learning algorithm, neural networks can face the problem of overfitting when they learn patterns that are only relevant to the training set. In such a case, the model will not be able to generalize the unseen data.</p>
			<p>Luckily, there are multiple techniques that can help reduce the risk of overfitting:</p>
			<ul>
				<li>L1 regularization, which adds a penalty parameter (absolute value of the weights) to the loss function</li>
				<li>L2 regularization, which adds a penalty parameter (squared value of the weights) to the loss function</li>
				<li>Early stopping, which stops the training if the error for the validation set increases while the error decreases for the training set</li>
				<li>Dropout, which will randomly remove some neurons during training</li>
			</ul>
			<p>All these techniques can be added at each layer of a neural network we create. We will be looking at this in the next exercise.</p>
			<h2 id="_idParaDest-189">Ex<a id="_idTextAnchor210"/>ercise 6.04: Predicting Boston House Prices with Regularization</h2>
			<p>In this exercise, you will build a neural network that will predict the median house price for a suburb in Boston and see how to add regularizers to a network.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset file can also be found in our GitHub repository: <a href="https://packt.live/2V9kRUU">https://packt.live/2V9kRUU</a>.</p>
			<p class="callout">Citation: The data was originally published by <em class="italic">Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the demand for clean air', J. Environ. Economics &amp; Management, vol.5, 81-102, 1978</em>.</p>
			<p>The dataset is composed of <strong class="source-inline">12</strong> different features that provide information about the suburb and a target variable (<strong class="source-inline">MEDV</strong>). The target variable is numeric and represents the median value of owner-occupied homes in units of $1,000.</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Import the <strong class="source-inline">pandas</strong> package as <strong class="source-inline">pd</strong>:<p class="source-code">import pandas as pd</p></li>
				<li>Create a <strong class="source-inline">file_url</strong> variable containing a link to the raw dataset:<p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/'\</p><p class="source-code">           'The-Applied-Artificial-Intelligence-Workshop'\</p><p class="source-code">           '/master/Datasets/boston_house_price.csv'</p></li>
				<li>Load the dataset into a variable called <strong class="source-inline">df</strong> using <strong class="source-inline">pd.read_csv()</strong>:<p class="source-code">df = pd.read_csv(file_url)</p></li>
				<li>Display the first five rows using <strong class="source-inline">.head()</strong>:<p class="source-code">df.head()</p><p>The expected output is this:</p><div id="_idContainer190" class="IMG---Figure"><img src="image/B16060_06_15.jpg" alt="Figure 6.15: Output showing the first five rows of the dataset&#13;&#10;"/></div><p class="figure-caption">Figure 6.15: Output showing the first five rows of the dataset</p></li>
				<li>Extract the target variable using <strong class="source-inline">.pop()</strong> and save it into a variable called <strong class="source-inline">label</strong>:<p class="source-code">label = df.pop('MEDV')</p></li>
				<li>Import the <strong class="source-inline">scale</strong> function from <strong class="source-inline">sklearn.preprocessing</strong>:<p class="source-code">from sklearn.preprocessing import scale</p></li>
				<li>Scale the DataFrame, <strong class="source-inline">df</strong>, and save the results into a variable called <strong class="source-inline">scaled_features</strong>. Print its content:<p class="source-code">scaled_features = scale(df)</p><p class="source-code">scaled_features</p><p>The expected output is this:</p><p class="source-code">array([[-0.41978194,  0.28482986, -1.2879095 , ..., -0.66660821,</p><p class="source-code">        -1.45900038, -1.0755623 ],</p><p class="source-code">       [-0.41733926, -0.48772236, -0.59338101, ..., -0.98732948,</p><p class="source-code">        -0.30309415, -0.49243937],</p><p class="source-code">       [-0.41734159, -0.48772236, -0.59338101, ..., -0.98732948,</p><p class="source-code">        -0.30309415, -1.2087274 ],</p><p class="source-code">       ...,</p><p class="source-code">       [-0.41344658, -0.48772236,  0.11573841, ..., -0.80321172,</p><p class="source-code">         1.17646583, -0.98304761],</p><p class="source-code">       [-0.40776407, -0.48772236,  0.11573841, ..., -0.80321172,</p><p class="source-code">         1.17646583, -0.86530163],</p><p class="source-code">       [-0.41500016, -0.48772236,  0.11573841, ..., -0.80321172,</p><p>In the output, you can see that all our features are now standardized.</p></li>
				<li>Import <strong class="source-inline">train_test_split</strong> from <strong class="source-inline">sklearn.model_selection</strong>:<p class="source-code">from sklearn.model_selection import train_test_split</p></li>
				<li>Split the data into training and testing sets and save the results into four variables called <strong class="source-inline">features_train</strong>, <strong class="source-inline">features_test</strong>, <strong class="source-inline">label_train</strong>, and <strong class="source-inline">label_test</strong>. Use 10% of the data for testing and specify <strong class="source-inline">random_state=8</strong>:<p class="source-code">features_train, features_test, \</p><p class="source-code">label_train, label_test = train_test_split(scaled_features, \</p><p class="source-code">                                           label, \</p><p class="source-code">                                           test_size=0.1, \</p><p class="source-code">                                           random_state=8)</p></li>
				<li>Import <strong class="source-inline">numpy</strong> as <strong class="source-inline">np</strong>, <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">tf</strong>, and <strong class="source-inline">layers</strong> from <strong class="source-inline">tensorflow.keras</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import layers</p></li>
				<li>Set <strong class="source-inline">8</strong> as the seed for NumPy and TensorFlow using <strong class="source-inline">np.random_seed()</strong> and <strong class="source-inline">tf.random.set_seed()</strong>:<p class="source-code">np.random.seed(8)</p><p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Instantantiate a <strong class="source-inline">tf.keras.Sequential()</strong> class and save it into a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Next, create a combined <strong class="source-inline">l1</strong> and <strong class="source-inline">l2</strong> regularizer using <strong class="source-inline">tf.keras.regularizers.l1_l2</strong> with <strong class="source-inline">l1=0.01 </strong>and <strong class="source-inline">l2=0.01</strong>. Save it into a variable called <strong class="source-inline">regularizer</strong>:<p class="source-code">regularizer = tf.keras.regularizers.l1_l2(l1=0.1, l2=0.01)</p></li>
				<li>Instantantiate a <strong class="source-inline">layers.Dense()</strong> class with <strong class="source-inline">10</strong> neurons, <strong class="source-inline">activation='relu'</strong>,<strong class="source-inline"> input_shape=[12]</strong>, and <strong class="source-inline">kernel_regularizer=regularizer</strong>, and save it into a variable called <strong class="source-inline">layer1</strong>:<p class="source-code">layer1 = layers.Dense(10, activation='relu', \</p><p class="source-code">         input_shape=[12], kernel_regularizer=regularizer)</p></li>
				<li>Instantantiate a second <strong class="source-inline">layers.Dense()</strong> class with <strong class="source-inline">1</strong> neuron and save it into a variable called <strong class="source-inline">final_layer</strong>:<p class="source-code">final_layer = layers.Dense(1)</p></li>
				<li>Add the two layers you just defined to the model using <strong class="source-inline">.add()</strong> and add a layer in between each of them with <strong class="source-inline">layers.Dropout(0.25)</strong>:<p class="source-code">model.add(layer1)</p><p class="source-code">model.add(layers.Dropout(0.25))</p><p class="source-code">model.add(final_layer)</p><p>We added a dropout layer in between each dense layer that will randomly remove 25% of the neurons.</p></li>
				<li>Instantantiate a <strong class="source-inline">tf.keras.optimizers.SGD()</strong> class with <strong class="source-inline">0.001</strong> as the learning rate and save it into a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.SGD(0.001)</p></li>
				<li>Compile the neural network using <strong class="source-inline">.compile()</strong> with <strong class="source-inline">loss='mse', optimizer=optimizer, metrics=['mse']</strong>:<p class="source-code">model.compile(loss='mse', optimizer=optimizer, \</p><p class="source-code">              metrics=['mse'])</p></li>
				<li>Print a summary of the model using <strong class="source-inline">.summary()</strong>:<p class="source-code">model.summary()</p><p>The expected output is this:</p><div id="_idContainer191" class="IMG---Figure"><img src="image/B16060_06_16.jpg" alt="Figure 6.16: Summary of the model&#13;&#10;"/></div><p class="figure-caption">Figure 6.16: Summary of the model</p><p>This output summarizes the architecture of our neural networks. We can see it is composed of three layers with two dense layers and one dropout layer.</p></li>
				<li>Instantiate a <strong class="source-inline">tf.keras.callbacks.EarlyStopping()</strong> class with <strong class="source-inline">monitor='val_loss'</strong> and <strong class="source-inline">patience=2</strong> as the learning rate and save it into a variable called <strong class="source-inline">callback</strong>:<p class="source-code">callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \</p><p class="source-code">                                            patience=2)</p><p>We just defined a callback stating the neural network will stop its training if the validation loss (<strong class="source-inline">monitor='val_loss'</strong>) does not improve after <strong class="source-inline">2</strong> epochs (<strong class="source-inline">patience=2</strong>).</p></li>
				<li>Fit the neural networks with the training set and specify <strong class="source-inline">epochs=50</strong>, <strong class="source-inline">validation_split=0.2</strong>, <strong class="source-inline">callbacks=[callback]</strong>, and <strong class="source-inline">verbose=2</strong>:<p class="source-code">model.fit(features_train, label_train, \</p><p class="source-code">          epochs=50, validation_split = 0.2, \</p><p class="source-code">          callbacks=[callback], verbose=2)</p><p>The expected output is this:</p><div id="_idContainer192" class="IMG---Figure"><img src="image/B16060_06_17.jpg" alt="Figure 6.17: Fitting the neural network with the training set&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.17: Fitting the neural network with the training set</p>
			<p>In the output, we see that the neural network stopped its training after the 22<span class="superscript">nd</span> epoch. It stopped well before the maximum number of epochs, <strong class="source-inline">50</strong>. This is due to the callback we set earlier: if the validation loss does not improve after two epochs, the training should stop.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Yobbba">https://packt.live/2Yobbba</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/37SVSu6">https://packt.live/37SVSu6</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>You just applied multiple regularization techniques and trained a neural network to predict the median value of housing in Boston suburbs.</p>
			<h2 id="_idParaDest-190">Activ<a id="_idTextAnchor211"/>ity 6.01: Finding the Best Accuracy Score for the Digits Dataset</h2>
			<p>In this activity, you will be training and evaluating a neural network that will be recognizing handwritten digits from the images provided by the MNIST dataset. You will be focusing on achieving an optimal accuracy score.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can read more about this dataset on TensorFlow's website at <a href="https://www.tensorflow.org/datasets/catalog/mnist">https://www.tensorflow.org/datasets/catalog/mnist</a>.</p>
			<p class="callout">Citation: This dataset was originally shared by <em class="italic">Yann Lecun</em>.</p>
			<p>The following steps will help you complete the activity:</p>
			<ol>
				<li value="1">Import the MNIST dataset.</li>
				<li>Standardize the data by applying a division by <strong class="source-inline">255</strong>.</li>
				<li>Create a neural network architecture with the following layers:<p>A flatten input layer using <strong class="source-inline">layers.Flatten(input_shape=(28,28))</strong></p><p>A fully connected layer with <strong class="source-inline">layers.Dense(128, activation='relu')</strong></p><p>A dropout layer with <strong class="source-inline">layers.Dropout(0.25)</strong></p><p>A fully connected layer with <strong class="source-inline">layers.Dense(10, activation='softmax')</strong></p></li>
				<li>Specify an <strong class="source-inline">Adam</strong> optimizer with a learning rate of <strong class="source-inline">0.001</strong>.</li>
				<li>Define an early stopping on the validation loss and patience of <strong class="source-inline">5</strong>.</li>
				<li>Train the model.</li>
				<li>Evaluate the model and find the accuracy score.</li>
			</ol>
			<p>The expected output is this:</p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/B16060_06_18.jpg" alt="Figure 6.18: Expected accuracy score&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.18: Expected accuracy score</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 378</p>
			<p>In the next part, we will dive into deep learning topics.</p>
			<h1 id="_idParaDest-191">Deep L<a id="_idTextAnchor212"/>earning</h1>
			<p>Now that we are comfortable in building and training a neural network with one hidden layer, we can look at more complex architecture with deep learning. </p>
			<p>Deep learning is just an extension of traditional neural networks but with deeper and more complex architecture. Deep learning can model very complex patterns, be applied in tasks such as detecting objects in images and translating text into a different language.</p>
			<h2 id="_idParaDest-192">Shallo<a id="_idTextAnchor213"/>w versus Deep Networks</h2>
			<p>Now that we are comfortable in building and training a neural network with one hidden layer, we can look at more complex architecture with deep learning.</p>
			<p>As mentioned earlier, we can add more hidden layers to a neural network. This will increase the number of parameters to be learned but can potentially help to model more complex patterns. This is what deep learning is about: increasing the depth of a neural network to tackle more complex problems.</p>
			<p>For instance, we can add a second layer to the neural network we presented earlier in the section on forward propagation and loss functions:</p>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="image/B16060_06_19.jpg" alt="Figure 6.19: Figure showing two hidden layers in a neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.19: Figure showing two hidden layers in a neural network</p>
			<p>In theory, we can add an infinite number of hidden layers. But there is a drawback with deeper networks. Increasing the depth will also increase the number of parameters to be optimized. So, the neural network will have to train for longer. So, as good practice, it is better to start with a simpler architecture and then steadily increase its depth.</p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor214"/>Computer Vision and Image Classification</h1>
			<p>Deep learning has achieved amazing results in computer vision and natural language processing. Computer vision is a field that involves analyzing digital images. A digital image is a matrix composed of <strong class="bold">pixels</strong>. Each pixel has a value between <strong class="bold">0</strong> and <strong class="bold">255</strong> and this value represents the intensity of the pixel. An image can be black and white and have only one channel. But it can also have colors, and in that case, it will have three channels for the colors red, green, and blue. This digital version of an image that can be fed to a deep learning model.</p>
			<p>There are multiple applications of computer vision, such as image classification (recognizing the main object in an image), object detection (localizing different objects in an image), and image segmentation (finding the edges of objects in an image). In this book, we will only look at image classification.</p>
			<p>In the next section, we will look at a specific type of architecture: CNNs.</p>
			<h2 id="_idParaDest-194">Convolu<a id="_idTextAnchor215"/>tional Neural Networks (CNNs)</h2>
			<p><strong class="bold">CNNs</strong> are ANNs that are optimized for image-related pattern recognition. CNNs are based on convolutional layers instead of fully connected layers.</p>
			<p>A convolutional layer is used to detect patterns in an image with a filter. A filter is just a matrix that is applied to a portion of an input image through a convolutional operation and the output will be another image (also called a feature map) with the highlighted patterns found by the filter. For instance, a simple filter can be one that recognizes vertical lines on a flower, such as for the following image:</p>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/B16060_06_20.jpg" alt="Figure 6.20: Convolution detecting patterns in an image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.20: Convolution detecting patterns in an image</p>
			<p>These filters are not set in advance but learned by CNNs automatically. After the training is over, a CNN can recognize different shapes in an image. These shapes can be anywhere on the image, and the convolutional operator recognizes similar image information regardless of its exact position and orientation.</p>
			<h2 id="_idParaDest-195">Convolut<a id="_idTextAnchor216"/>ional Operations</h2>
			<p>A convolution is a specific type of matrix operation. For an input image, a filter of size <strong class="source-inline">n*n</strong> will go through a specific area of an image and apply an element-wise product and a sum and return the calculated value:</p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/B16060_06_21.jpg" alt="Figure 6.21: Convolutional operations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.21: Convolutional operations</p>
			<p>In the preceding example, we applied a filter to the top-left part of the image. Then we applied an element-wise product that just multiplied an element from the input image to the corresponding value on the filter. In the example, we calculated the following:</p>
			<ul>
				<li>1st row, 1st column: <strong class="source-inline">5</strong> * <strong class="source-inline">2</strong> = <strong class="source-inline">10</strong></li>
				<li>1st row, 2nd column: <strong class="source-inline">10</strong> * <strong class="source-inline">0</strong> = <strong class="source-inline">0</strong></li>
				<li>1st row, 3rd column: <strong class="source-inline">15</strong> * <strong class="source-inline">(-1)</strong> = <strong class="source-inline">-15</strong></li>
				<li>2nd row, 1st column: <strong class="source-inline">10</strong> * <strong class="source-inline">2</strong> = <strong class="source-inline">20</strong></li>
				<li>2nd row, 2nd column: <strong class="source-inline">20</strong> * <strong class="source-inline">0</strong> = <strong class="source-inline">0</strong></li>
				<li>2nd row, 3rd column: <strong class="source-inline">30</strong> * <strong class="source-inline">(-1)</strong> = <strong class="source-inline">-30</strong></li>
				<li>3rd row, 1st column: <strong class="source-inline">100</strong> * <strong class="source-inline">2</strong> = <strong class="source-inline">200</strong></li>
				<li>3rd row, 2nd column: <strong class="source-inline">150</strong> * <strong class="source-inline">0</strong> = <strong class="source-inline">0</strong></li>
				<li>3rd row, 3rd column: <strong class="source-inline">200</strong> * <strong class="source-inline">(-1)</strong> = <strong class="source-inline">-200</strong></li>
			</ul>
			<p>Finally, we perform the sum of these values: <strong class="source-inline">10</strong> + <strong class="source-inline">0</strong> -<strong class="source-inline">15</strong> + <strong class="source-inline">20</strong> + <strong class="source-inline">0</strong> - <strong class="source-inline">30</strong> + <strong class="source-inline">200</strong> + <strong class="source-inline">0</strong> - <strong class="source-inline">200</strong> = <strong class="source-inline">-15</strong>.</p>
			<p>Then we will perform the same operation by sliding the filter to the right by one column from the input image. We keep sliding the filter until we have covered the entire image:</p>
			<div>
				<div id="_idContainer197" class="IMG---Figure">
					<img src="image/B16060_06_22.jpg" alt="Figure 6.22: Convolutional operations on different rows and columns&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.22: Convolutional operations on different rows and columns</p>
			<p>Rather than sliding column by column, we can also slide by two, three, or more columns. The parameter defining the length of this sliding operation is called the <strong class="bold">stride</strong>.</p>
			<p>You may have noticed that the result of the convolutional operation is an image (or feature map) with smaller dimensions than the input image. If you want to keep the exact same dimensions, you can add additional rows and columns with the value 0 around the border of the input image. This operation is called <strong class="bold">padding</strong>.</p>
			<p>This is what is behind a convolutional operation. A convolutional layer is just the application of this operation with multiple filters.</p>
			<p>We can declare a convolutional layer in TensorFlow with the following code snippet:</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">layers.Conv2D(32, kernel_size=(3, 3), strides=(1,1), \</p>
			<p class="source-code">              padding="valid", activation="relu")</p>
			<p>In the preceding example, we have instantiated a convolutional layer with <strong class="source-inline">32</strong> filters (also called <strong class="bold">kernels</strong>) of size <strong class="source-inline">(3, 3)</strong> with stride of <strong class="source-inline">1</strong> (sliding window by 1 column or row at a time) and no padding (<strong class="source-inline">padding="valid"</strong>).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can read more about this Conv2D class on TensorFlow's website, at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D</a>.</p>
			<p>In TensorFlow, convolutional layers expect the input to be tensors with the following format: (<strong class="bold">rows</strong>, <strong class="bold">height</strong>, <strong class="bold">width</strong>, <strong class="bold">channel</strong>). Depending on the dataset, you may have to reshape the images to conform to this requirement. TensorFlow provides a function for this, shown in the following code snippet:</p>
			<p class="source-code">features_train.reshape(60000, 28, 28, 1)</p>
			<h2 id="_idParaDest-196">Pooling La<a id="_idTextAnchor217"/>yer</h2>
			<p>Another frequent layer in a CNN's architecture is the pooling layer. We have seen previously that the convolutional layer reduces the size of the image if no padding is added. Is this behavior expected? Why don't we keep the exact same size as for the input image? In general, with CNNs, we tend to reduce the size of the feature maps as we progress through different layers. The main reason for this is that we want to have more and more specific pattern detectors closer to the end of the network. </p>
			<p>Closer to the beginning of the network, a CNN will tend to have more generic filters, such as vertical or horizontal line detectors, but as it goes deeper, we would, for example, have filters that can detect a dog's tail or a cat's whiskers if we were training a CNN to recognize cats versus dogs, or the texture of objects if we were classifying images of fruits. Also, having smaller feature maps reduces the risk of false patterns being detected.</p>
			<p>By increasing the stride, we can further reduce the size of the output feature map. But there is another way to do this: adding a pooling layer after a convolutional layer. A pooling layer is a matrix of a given size and will apply an aggregation function to each area of the feature map. The most frequent aggregation method is finding the maximum value of a group of pixels:</p>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/B16060_06_23.jpg" alt="Figure 6.23: Workings of the pooling layer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.23: Workings of the pooling layer</p>
			<p>In the preceding example, we use a max pooling of size (<strong class="source-inline">2, 2</strong>) and <strong class="source-inline">stride=2</strong>. We look at the top-left corner of the feature map and find the maximum value among the pixels <strong class="source-inline">6</strong>, <strong class="source-inline">8</strong>, <strong class="source-inline">1</strong>, and <strong class="source-inline">2</strong> and get the result, <strong class="source-inline">8</strong>. Then we slide the max pooling by a stride of <strong class="source-inline">2</strong> and perform the same operation on the pixels <strong class="source-inline">6</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">7</strong>, and <strong class="source-inline">4</strong>. We repeat the same operation on the bottom groups and get a new feature map of size (<strong class="source-inline">2,2</strong>).</p>
			<p>In TensorFlow, we can use the <strong class="source-inline">MaxPool2D()</strong> class to declare a max-pooling layer:</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">layers.MaxPool2D(pool_size=(2, 2), strides=2)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can read more about this Conv2D class on TensorFlow's website at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D">https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D</a>.</p>
			<h2 id="_idParaDest-197">CNN Archite<a id="_idTextAnchor218"/>cture</h2>
			<p>As you saw earlier, you can define your own custom CNN architecture by specifying the type and number of hidden layers, the activation functions to be used, and so on. But this may be a bit daunting for beginners. How do we know how many filters need to be added at each layer or what the right stride will be? We will have to try multiple combinations and see which ones work.</p>
			<p>Luckily, a lot of researchers in deep learning have already done such exploratory work and have published the architecture they designed. Currently, the most famous ones are these:</p>
			<ul>
				<li>AlexNet</li>
				<li>VGG</li>
				<li>ResNet</li>
				<li>Inception<p class="callout-heading">Note</p><p class="callout">We will not go through the details of each architecture as it is not in the scope of this book, but you can read more about the different CNN architectures implemented on TensorFlow at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications">https://www.tensorflow.org/api_docs/python/tf/keras/applications</a>.</p></li>
			</ul>
			<h2 id="_idParaDest-198">Activity 6.<a id="_idTextAnchor219"/>02: Evaluating a Fashion Image Recognition Model Using CNNs</h2>
			<p>In this activity, we will be training a CNN to recognize clothing images that belong to 10 different classes from the Fashion MNIST dataset. We will be finding the accuracy of this CNN model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can read more about this dataset on TensorFlow's website at <a href="https://www.tensorflow.org/datasets/catalog/fashion_mnist">https://www.tensorflow.org/datasets/catalog/fashion_mnist</a>.</p>
			<p class="callout">The original dataset was shared by <em class="italic">Han Xiao</em>.</p>
			<p>The following steps will help you complete the activity:</p>
			<ol>
				<li value="1">Import the Fashion MNIST dataset.</li>
				<li>Reshape the training and testing set.</li>
				<li>Standardize the data by applying a division by <strong class="source-inline">255</strong>.</li>
				<li>Create a neural network architecture with the following layers:<p>Three convolutional layers with <strong class="source-inline">Conv2D(64, (3,3), activation='relu')</strong> followed by <strong class="source-inline">MaxPooling2D(2,2)</strong></p><p>A flatten layer</p><p>A fully connected layer with <strong class="source-inline">Dense(128, activation=relu)</strong></p><p>A fully connected layer with <strong class="source-inline">Dense(10, activation='softmax')</strong></p></li>
				<li>Specify an <strong class="source-inline">Adam</strong> optimizer with a learning rate of <strong class="source-inline">0.001</strong>.</li>
				<li>Train the model.</li>
				<li>Evaluate the model on the testing set.</li>
			</ol>
			<p>The expected output is this:</p>
			<p class="source-code">10000/10000 [==============================] - 1s 108us/sample - loss: 0.2746 - accuracy: 0.8976</p>
			<p class="source-code">[0.27461639745235444, 0.8976]</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 382.</p>
			<p>In the following section, we will learn about a different type of deep learning architecture: the RNN.</p>
			<h1 id="_idParaDest-199">Recurrent N<a id="_idTextAnchor220"/>eural Networks (RNNs)</h1>
			<p>In the last section, we learned how we can use CNNs for computer vision tasks such as classifying images. With deep learning, computers are now capable of achieving and sometimes surpassing human performance. Another field that is attracting a lot of interest from researchers is natural language processing. This is a field where RNNs excel.</p>
			<p>In the last few years, we have seen a lot of different applications of RNN technology, such as speech recognition, chatbots, and text translation applications. But RNNs are also quite performant in predicting time series patterns, something that's used for forecasting stock markets. </p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor221"/>RNN Layers</h2>
			<p><a id="_idTextAnchor222"/>The common point with all the applications mentioned earlier is that the inputs are sequential. There is a time component with the input. For instance, a sentence is a sequence of words, and the order of words matters; stock market data consists of a sequence of dates with corresponding stock prices.</p>
			<p>To accommodate such input, we need neural networks to be able to handle sequences of inputs and be able to maintain an understanding of the relationships between them. One way to do this is to create memory where the network can take into account previous inputs. This is exactly how a basic RNN works:</p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B16060_06_24.jpg" alt="Figure 6.24: Overview of a single RNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.24: Overview of a single RNN</p>
			<p>In the preceding figure, we can see a neural network that takes an input called <strong class="source-inline">X</strong><span class="subscript">t</span> and performs some transformations and gives the output results, <img src="image/B16060_06_24a.png" alt="a"/>. Nothing new so far. </p>
			<p>But you may have noticed that there is an additional output called H<span class="subscript">t-1</span> that is an output but also an input to the neural network. This is how RNN simulates memory – by considering its previous results and taking them in as an additional input. Therefore, the result <img src="image/B16060_06_24b.png" alt="b"/> will depend on the input x<span class="subscript">t</span> but also H<span class="subscript">t-1</span>. Now, we can represent a sequence of four inputs that get fed into the same neural network:</p>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/B16060_06_25.jpg" alt="Figure 6.25: Overview of an RNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.25: Overview of an RNN</p>
			<p>We can see the neural network is taking an input (<strong class="source-inline">x</strong>) and generating an output (<strong class="source-inline">y</strong>) at each time step (<strong class="source-inline">t</strong>, <strong class="source-inline">t+1</strong>, …, <strong class="source-inline">t+3</strong>) but also another output (<strong class="source-inline">h</strong>), which is feeding the next iteration.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The preceding figure may be a bit misleading – there is actually only one RNN here (all the RNN boxes in the middle form one neural network), but it is easier to see how the sequencing works in this format.</p>
			<p>An RNN cell looks like this on the inside:</p>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/B16060_06_26.jpg" alt="Figure 6.26: Internal workings of an RNN using tanh&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.26: Internal workings of an RNN using tanh</p>
			<p>It is very similar to a simple neuron, but it takes more inputs and uses <strong class="source-inline">tanh</strong> as the activation function.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can use any activation function in an RNN cell. The default value in TensorFlow is <strong class="source-inline">tanh</strong>.</p>
			<p>This is the basic logic of RNNs. In TensorFlow, we can instantiate an RNN layer with <strong class="source-inline">layers.SimpleRNN</strong>:</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">layers.SimpleRNN(4, activation='tanh')</p>
			<p>In the code snippet, we created an RNN layer with <strong class="source-inline">4</strong> outputs and the <strong class="source-inline">tanh</strong> activation function (which is the most widely used activation function for RNNs). </p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor223"/>The GRU Layer</h2>
			<p>One drawback with the previous type of layer is that the final output takes into consideration all the previous outputs. If you have a sequence of 1,000 input units, the final output, <strong class="source-inline">y</strong>, is influenced by every single previous result. If this sequence was composed of 1,000 words and we were trying to predict the next word, it would really be overkill to have to memorize all of the 1,000 words before making a prediction. Probably, you only need to look at the previous 100 words from the final output.</p>
			<p>This is exactly what <strong class="bold">Gated Recurrent Unit</strong> (<strong class="bold">GRU</strong>) cells are for. Let's look at what is inside them:</p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/B16060_06_27.jpg" alt="Figure 6.27: Internal workings of an RNN using tanh and sigmoid&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.27: Internal workings of an RNN using tanh and sigmoid</p>
			<p>Compared to a simple RNN cell, a GRU cell has a few more elements:</p>
			<ul>
				<li>A second activation function, which is <strong class="source-inline">sigmoid</strong></li>
				<li>A multiplier operation performed before generating the outputs <img src="image/B16060_06_27a.png" alt="39"/> and H<span class="subscript">t</span></li>
			</ul>
			<p>The usual path with <strong class="source-inline">tanh</strong> is still responsible for making a prediction, but this time we will call it the "candidate." The sigmoid path acts as an "update" gate. This will tell the GRU cell whether it needs to discard the use of this candidate or not. Remember that the output ranges between <strong class="bold">0</strong> and <strong class="bold">1</strong>. If close to 0, the update gate (that is, the sigmoid path) will say we should not consider this candidate. </p>
			<p>On the other hand, if it is closer to 1, we should definitely use the result of this candidate.</p>
			<p>Remember that the output H<span class="subscript">t </span>is related to H<span class="subscript">t-1</span>, which is related to H<span class="subscript">t-2</span>, and so on. So, this update gate will also define how much "memory" we should keep. It tends to prioritize previous outputs closer to the current one.</p>
			<p>This is the basic logic of GRU (note that the GRU cell has one more component, the reset gate, but for the purpose of simplicity, we will not look at it). In TensorFlow, we can instantiate such a layer with <strong class="source-inline">layers.GRU</strong>:</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">layers.GRU(4, activation='tanh', \</p>
			<p class="source-code">           recurrent_activation='sigmoid')</p>
			<p>In the code snippet, we have created a GRU layer with <strong class="source-inline">4</strong> output units and the <strong class="source-inline">tanh</strong> activation function for the candidate prediction and sigmoid for the update gate. </p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor224"/>The LSTM Layer</h2>
			<p>The<a id="_idTextAnchor225"/>re is another very popular type of cell for RNN architecture called the LSTM cell. <strong class="bold">LSTM</strong> stands for <strong class="bold">Long Short-Term Memory</strong>. LSTM came before GRU, but the latter is much simpler, and this is the reason why we presented it first. Here is what is under the hood of LSTM:</p>
			<div>
				<div id="_idContainer206" class="IMG---Figure">
					<img src="image/B16060_06_28.jpg" alt="Figure 6.28: Overview of LSTM&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.28: Overview of LSTM</p>
			<p>At first, this looks very complicated. It is composed of several elements:</p>
			<ul>
				<li><strong class="source-inline">Cell state</strong>: This is the concatenation of all the previous outputs. It is the "memory" of the LSTM cell.</li>
				<li><strong class="source-inline">Forget gate</strong>: This is responsible for defining whether we should keep or forget a given memory. </li>
				<li><strong class="source-inline">Input gate</strong>: This is responsible for defining whether the new memory candidate needs to be updated or not. This new memory candidate is then added to the previous memory.</li>
				<li><strong class="source-inline">Output gate</strong>: This is responsible for making the prediction based on the previous output (H<span class="subscript">t-1</span>), the current input (x<span class="subscript">t</span>), and the memory.</li>
			</ul>
			<p>An LSTM cell can consider previous results but also past memory, and this is the reason why it is so powerful. </p>
			<p>In TensorFlow, we can instantiate such a layer with <strong class="source-inline">layers.SimpleRNN</strong>:</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">layers.LSTM(4, activation='tanh', \</p>
			<p class="source-code">            recurrent_activation='sigmoid')</p>
			<p>In the code snippet, we have created an LSTM layer with <strong class="source-inline">4</strong> output units and the <strong class="source-inline">tanh</strong> activation function for the candidate prediction and sigmoid for the update gate. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can read more about SimpleRNN implementation in TensorFlow here: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN">https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN</a>.</p>
			<h2 id="_idParaDest-203">Activity 6.03: Eval<a id="_idTextAnchor226"/>uating a Yahoo Stock Model with an RNN</h2>
			<p>In this activity, we will be training an RNN model with LSTM to predict the stock price of Yahoo! based on the data of the past <strong class="source-inline">30</strong> days. We will be finding the optimal mean squared error value and checking whether the model overfits. We will be using the same Yahoo Stock dataset that we saw in <em class="italic">Chapter 2</em>, <em class="italic">An Introduction to Regression</em>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset file can also be found in our GitHub repository: <a href="https://packt.live/3fRI5Hk">https://packt.live/3fRI5Hk</a>.</p>
			<p>The following steps will help you to complete this activity:</p>
			<ol>
				<li value="1">Import the Yahoo Stock dataset.</li>
				<li>Extract the <strong class="source-inline">close price</strong> column.</li>
				<li>Standardize the dataset.</li>
				<li>Create the previous <strong class="source-inline">30</strong> days' stock price features.</li>
				<li>Reshape the training and testing sets.</li>
				<li>Create the neural network architecture with the following layers:<p>Five LSTM layers with <strong class="source-inline">LSTM(50, (3,3), activation='relu') followed by Dropout(0.2)</strong></p><p>A fully connected layer with <strong class="source-inline">Dense(1)</strong></p></li>
				<li>Specify an <strong class="source-inline">Adam</strong> optimizer with a learning rate of <strong class="source-inline">0.001</strong>.</li>
				<li>Train the model.</li>
				<li>Evaluate the model on the testing set.</li>
			</ol>
			<p>The expected output is this:</p>
			<p class="source-code">1000/1000 [==============================] - 0s 279us/sample - loss: 0.0016 - mse: 0.0016</p>
			<p class="source-code">[0.00158528157370165, 0.0015852816]</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 387.</p>
			<p>In the next section, we will be looking at the hardware needed for deep learning.</p>
			<h2 id="_idParaDest-204">Hardware for Deep L<a id="_idTextAnchor227"/>earning</h2>
			<p>As you may have noticed, training deep learning models takes longer than traditional machine learning algorithms. This is due to the number of calculations required for the forward pass and backpropagation. In this book, we trained very simple models with just a few layers. But there are architectures with hundreds of layers, and some with even more than that. That kind of network can take days or even weeks to train.</p>
			<p>To speed up the training process, it is recommended to use a specific piece of hardware called a GPU. GPUs specialize in performing mathematical operations and therefore are perfect for deep learning. Compared to a <strong class="bold">Central Processing Unit</strong> (<strong class="bold">CPU</strong>), a GPU can be up to 10X faster at training a deep learning model. You can personally buy a GPU and set up your own deep learning computer. You just need to get one that is CUDA-compliant (currently only NVIDIA GPUs are).</p>
			<p>Another possibility is to use cloud providers such as AWS or Google Cloud Platform and train your models in the cloud. You will pay only for what you use and can switch them off as soon as you are done. The benefit is that you can scale the configuration up or down depending on the needs of your projects – but be mindful of the cost. You will be charged for the time your instance is up even if you are not training a model. So, don't forget to switch things off if you're not using them.</p>
			<p>Finally, Google recently released some new hardware dedicated to deep learning: <strong class="bold">Tensor Processing Unit</strong> (<strong class="bold">TPUs</strong>). They are much faster than GPUs, but they are quite costly. Currently, only Google Cloud Platform provides such hardware in their cloud instances.</p>
			<h2 id="_idParaDest-205">Challenges and Futu<a id="_idTextAnchor228"/>re Trends</h2>
			<p>As with any new technology, deep learning comes with challenges. One of them is the big barrier to entry. To become a deep learning practitioner, you used to have to know all the mathematical theory behind deep learning very well and be a confirmed programmer. On top of this, you had to learn the specifics of the deep learning framework you chose to use (be it TensorFlow, PyTorch, Caffe, or anything else). For a while, deep learning couldn't reach a broad audience and was mainly limited to researchers. This situation has changed, though it is not perfect. For instance, TensorFlow now comes with a higher-level API called Keras (this is the one you saw in this chapter) that is much easier to use than the core API. Hopefully, this trend will keep going and make deep learning frameworks more accessible to anyone interested in this field.</p>
			<p>The second challeng<a id="_idTextAnchor229"/>e was that deep learning models require a lot of computation power, as mentioned in the previous section. This was again a major blocker for anyone who wanted to have a go at it. Even though the cost of GPUs has gone down, deep learning still requires some upfront investment. Luckily for us, there is now a free option to train deep learning models with GPUs: Google Colab. It is an initiative from Google to promote research by providing temporary cloud computing for free. The only thing you need is a Google account. Once signed up, you can create Notebooks (similar to Jupyter Notebooks) and choose a kernel to be run on a CPU, GPU (limited to 10 hours per day), or even a TPU (limited to ½ hour per day). So, before investing in purchasing or renting out GPU, you can first practice with Google Colab.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find more information about Google Colab at <a href="https://colab.research.google.com/">https://colab.research.google.com/</a>.</p>
			<p>More advanced deep learning models can be very deep and require weeks of training. So, it is hard for basic practitioners to use such architecture. But thankfully, a lot of researchers have embraced the open source movement and have shared not only the architectures they have designed but also the weights of the networks. This means you can now access state-of-the-art pre-trained models and fine-tune them to fit your own projects. This is called transfer learning (which is out of the scope of this book). It is very popular in computer vision, where you can find pre-trained models on ImageNet or MS-Coco, for instance, which are large datasets of pictures. Transfer learning is also happening in natural language processing, but it is not as developed as it is for computer vision.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find more information about these datasets at <a href="http://www.image-net.org/">http://www.image-net.org/</a> and <a href="http://cocodataset.org/">http://cocodataset.org/</a>.</p>
			<p>Another very important topic related to deep learning is the increasing need to be able to interpret model results. Soon, these kinds of algorithms may be regulated, and deep learning practitioners will have to be able to explain why a model is making a given decision. Currently, deep learning models are more like black boxes due to the complexity of the networks. There are already some initiatives from researchers to find ways to interpret and understand deep neural networks, such as <em class="italic">Zeiler and Fergus</em>, "<em class="italic">Visualizing and Understanding Convolutional Networks</em>", <em class="italic">ECCV 2014</em>. However, more work needs to be done in this field with the democratization of such technologies in our day-to-day lives. For instance, we will need to make sure that these algorithms are not biased and are not making unfair decisions affecting specific groups of people.</p>
			<h1 id="_idParaDest-206"><a id="_idTextAnchor230"/>Summary</h1>
			<p>We have ju<a id="_idTextAnchor231"/>st completed the entire book of <em class="italic">The Applied Artificial Intelligence Workshop, Second Edition</em>. In this workshop, we have learned about the fundamentals of AI and its applications. We wrote a Python program to play tic-tac-toe. We learned about search techniques such as breadth-first search and depth-first search and how they can help us solve the tic-tac-toe game.</p>
			<p>In the next couple of chapters after that, we learned about supervised learning using regression and classification. These chapters included data preprocessing, train-test splitting, and models that were used in several real-life scenarios. Linear regression, polynomial regression, and support vector machines all came in handy when it came to predicting stock data. Classification was performed using k-nearest neighbor and support vector classifiers. Several activities helped you to apply the basics of classification in an interesting real-life use case: credit scoring.</p>
			<p>In <em class="italic">Chapter 4</em>, <em class="italic">An Introduction to Decision Trees</em>, you were introduced to decision trees, random forests, and extremely randomized trees. This chapter introduced different means of evaluating the utility of models. We learned how to calculate the accuracy, precision, recall, and F<span class="subscript">1</span> score of models. We also learned how to create the confusion matrix of a model. The models of this chapter were put into practice through the evaluation of car data.</p>
			<p>Unsupervised learni<a id="_idTextAnchor232"/>ng was introduced in <em class="italic">Chapter 5</em>, <em class="italic">Artificial Intelligence: Clustering</em>, along with the k-means and hierarchical clustering algorithms. One interesting aspect of these algorithms is that the labels are not given in advance, but they are detected during the clustering process.</p>
			<p>This workshop concluded with <em class="italic">Chapter 6</em>, <em class="italic">Neural Networks and Deep Learning</em>, where neural networks and deep learning using TensorFlow was presented. We used these techniques to achieve the best accuracy in real-life applications, such as the detection of written digits, image classification, and time series forecasting.</p>
		</div>
		<div>
			<div id="_idContainer208" class="Content">
			</div>
		</div>
	</body></html>