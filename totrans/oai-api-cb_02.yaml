- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI API Endpoints Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenAI API is not just one endpoint, but instead a collection of different
    endpoints that can be used to generate text completions, images, and even transcribe
    audio. In this chapter, we dive into all these use cases covering the main endpoints
    that are used by developers when integrating the OpenAI API into their applications.
    By the end of this chapter, you will know how to use the diverse capabilities
    of the OpenAI API, the first step in building intelligent applications. You’ll
    understand the nuances of each endpoint, ensuring that you can tailor your integration
    to your specific needs. Whether you’re aiming to create dynamic text content,
    generate captivating visuals, or transcribe audio, you will learn how to accomplish
    these tasks all using the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating customized responses using the Chat Completions endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating pictures using the Images endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating transcripts using the Audio endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires you to have access to the OpenAI API (via a generated
    API key) and have an API client installed, such as Postman. You can refer to the
    *Making OpenAI API requests with Postman* recipe in [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021)for
    more information on how to obtain your API key and set up Postman.
  prefs: []
  type: TYPE_NORMAL
- en: Generating customized responses using the Chat Completions endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We previously explored the Chat Completions endpoint at the end of [*Chapter
    1*](B21007_01.xhtml#_idTextAnchor021), but our request body was fairly simple
    and did not make use of the important parameters that we also discussed in the
    *Running a completion request in the OpenAI Playground* recipe. For example, we
    learned how the Chat Log could be used to *fine-tune* the responses that are generated.
    Additionally, the Chat Log feature can be used to deploy a chat bot inside your
    application.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will cover how to generate responses using the Chat Completions
    endpoint while using the Chat Log parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure you have an OpenAI Platform account with available usage credits. If
    you don’t, please follow the *Setting up your OpenAI Playground environment* recipe
    in [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, ensure you have Postman installed, you have created a new workspace,
    you have created a new HTTP request, and that the `Headers` for that request are
    correctly configured. This is important because without the `Authorization` configured,
    you will not be able to use the API. If you don’t have Postman installed and configured
    as mentioned, please follow the *Making OpenAI API requests with Postman* recipe
    in [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021).
  prefs: []
  type: TYPE_NORMAL
- en: All the recipes in this chapter will have this same requirement.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Postman, create a new request by selecting the **New** button on the top-left
    menu bar, and then select **HTTP**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the HTTP request type from **GET** to **POST** in the **Method** drop-down
    menu (by default, it will be set to **GET**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the following URL as the endpoint for Chat Completions: [https://api.openai.com/v1/chat/completions](https://api.openai.com/v1/chat/completions).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select **Headers** from the sub-menu and add the following key-value pairs
    into the table below it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| *Key* | *Value* |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Content-Type` | `application/json` |'
  prefs: []
  type: TYPE_TB
- en: '| `Authorization` | `Bearer <your API` `key here>` |'
  prefs: []
  type: TYPE_TB
- en: 'Select **Body** from the sub-menu and then select **raw** for the request type.
    Enter the following in **Body**. After that, select **Send**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '5. After sending the HTTP request, you should see the following response from
    the OpenAI API. Note that your response may be different. The section of the HTTP
    response that we particularly want to take note of is the `content` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '6. Let’s now add messages into the Chat Log (or `messages`, as it’s referred
    to in the API) to modify and tune the responses that are generated. Modify `messages`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '7. Click **Send** to execute the HTTP request. You should see a response similar
    to the following from the OpenAI API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The OpenAI API works very similar to the **Playground**, where developers can
    add messages to the *Chat Log*. This, in turn, fine-tunes the responses that the
    API generates – it learns from the assistant responses that we have created.
  prefs: []
  type: TYPE_NORMAL
- en: We did this by first executing a simple chat completion request, and only specified
    a `System message` and a `User message` (i.e., the prompt), as shown in *Figure
    2**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Excerpt of messages object within the request body](img/B21007_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Excerpt of messages object within the request body
  prefs: []
  type: TYPE_NORMAL
- en: This was done by adding message objects within the messages object in **Body**.
    Each message object must contain a *role* and some *content*, as we learned in
    [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021).
  prefs: []
  type: TYPE_NORMAL
- en: We can then add additional `Assistant` and `User` messages to teach the model
    how it should formulate its responses. Importantly, the model will attempt to
    match any pattern it sees within the messages tagged as `Assistant`. It is almost
    as if we are *teaching* the model how it should generate its responses. In this
    case, all of the `Assistant` responses contained the words **Hello Ice Cream Fan**
    and **SCREAM FOR** **ICE CREAM!**
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result of us adding messages to the *Chat Log*, when we provided the API
    with an additional prompt, it returned a response that matched the pattern described
    in the preceding paragraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hello Ice Cream Fan, thank you for the kind words. We love our location and
    our staff too. SCREAM FOR** **ICE CREAM!**'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we learned how to use the Chat Completions endpoint in the OpenAI
    API to generate text, and how to use the System Message, User prompts, and Chat
    Log to modify the generated text. This is important because the Chat Completions
    endpoint is the most common OpenAI API endpoint to use when integrating with other
    systems to create intelligent applications. Furthermore, as we begin to load the
    API for practical purposes, it’s important to know how to adjust the generated
    text to fit our desired use case using levers such as the Chat Log.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Adding messages to the *Chat Log* has many more purposes than just fine-tuning
    the assistant responses. Another important use case is creating a chat bot, where
    the conversation (the preceding `User` and `Assistant` messages) must be taken
    into context before a response can be generated. For instance, consider the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the preceding block is pseudocode for the purposes of illustration
    and because it’s easier to read than the proper JSON structure that would be required
    for the OpenAI API. In order to use this example in the OpenAI API, the request
    **Body** would need to be what is displayed in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, the user prompt is `How big is it?`. However, if we were to call the API
    using just this prompt, it would not generate the correct answer – in fact, it
    wouldn’t know at all what we were talking about because the previous `User` and
    `Assistant` messages are not in the request **Body**.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, we would need to construct the request body with all of the messages
    listed in the preceding code block for the model to provide an accurate response.
  prefs: []
  type: TYPE_NORMAL
- en: Creating pictures using the Images endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenAI API can do more than generate text (although that is its main purpose);
    it can also create images. It does this with a similar methodology to text generation,
    but instead of predicting characters, it predicts pixels. The inner workings of
    the model are complex (it involves the use of encoders, decoders, and embeddings),
    but that doesn’t stop us from actually using the model.
  prefs: []
  type: TYPE_NORMAL
- en: This significantly opens up the types of applications you can create with the
    OpenAI API. For example, you can create an application that produces stock images
    based on a user prompt. In this recipe, we will use the OpenAI API to generate
    several types of images.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Postman, create a new request by selecting the **New** button on the top-left
    menu bar, and then select **HTTP**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the HTTP request type from **GET** to **POST** in the **Method** drop-down
    menu (by default, it will be set to **GET**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the following URL as the endpoint for images: [https://api.openai.com/v1/images/generations](https://api.openai.com/v1/images/generations)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select **Headers** in the sub-menu, and add the following key-value pairs into
    the table below it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| *Key* | *Value* |'
  prefs: []
  type: TYPE_TB
- en: '| `Content-Type` | `application/json` |'
  prefs: []
  type: TYPE_TB
- en: '| `Authorization` | `Bearer <your API` `key here>` |'
  prefs: []
  type: TYPE_TB
- en: 'Select **Body** in the sub-menu and then select **raw** for the request type.
    Enter the following and after that, select **Send**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '5. After sending the HTTP request, you should see the following response from
    the OpenAI API. Note that your response will be different – it will produce a
    completely different URL. The following URL has been artificially condensed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 6. The image is contained within the URL that is generated in the response.
    Copy the URL from the `url` object and paste it into your internet browser. You
    should see an image of a dog. Similar to text generation, the image that you see
    will be different than the following one.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Output of DALLE-2 OpenAI image generation of a dog](img/B21007_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Output of DALLE-2 OpenAI image generation of a dog
  prefs: []
  type: TYPE_NORMAL
- en: 7. Return to Postman and modify the request `A brown furry medium-sized corgi
    dog on a green grass field,` `profile view`.
  prefs: []
  type: TYPE_NORMAL
- en: 8. Copy and paste the URL from the **Response** field into your browser, and
    should see a clear photo of a Corgi dog.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Output of DALL-E 2 OpenAI image generation with a more detailed
    prompt](img/B21007_02_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Output of DALL-E 2 OpenAI image generation with a more detailed
    prompt
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Images endpoint works using an AI model called **DALL-E**, which is a variation
    of the GPT model, but used to produce visualizations instead of text output. The
    model was trained on several billion image-text prompts to associate certain textual
    characteristics with visual representations. The power of this model is available
    within OpenAI, but a particular endpoint needs to be used, as we described in
    the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: DALL-E was introduced by OpenAI in early 2021 as its first text-to-image generation
    model. DALL-E 2, its next iteration, was released one year later ([https://openai.com/dall-e-2](https://openai.com/dall-e-2)).
  prefs: []
  type: TYPE_NORMAL
- en: Both DALL-E and DALL-E 2 serve the same purpose (generating images from text)
    and are even called using the same API. The key difference is that DALL-E 2 employs
    a more advanced and more popular technique called **diffusion**, a particular
    image generation method that leads to more realistic and high-resolution images.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, DALL-E and DALL-E 2 will be used interchangeably to refer to the
    OpenAI’s text-to-image generation model.
  prefs: []
  type: TYPE_NORMAL
- en: Request body and response
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The request **Body** for the Images endpoint is actually far simpler than the
    Chat Completions endpoint. Here are all its components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt**: This is the textual instruction provided to the model for image
    generation. Usually, the more detailed this statement is, the more accurate your
    desired image will be.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**n**: This expresses to the API the number of images that the model should
    generate. Each image will have slightly different variations, as the model attempts
    to provide different interpretations or angles based on the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**size**: This specifies the dimensions of the generated image. Note that,
    at this point, only the following image sizes can be created: 256x256, 512x512,
    and 1024x1024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Response** that we receive from the Images endpoint is also very easy
    to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '**created**: This represents the Unix timestamp of when the image was generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**data**: The value of this object is actually an array of objects, each containing
    the **url** parameter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**url**: This contains the direct link to the generated image'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Importance of being detailed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike text generation, where you can afford to be a bit general and any ambiguity
    is usually addressed by the context of the Chat Log and System Message, image
    generation requires detailed, descriptive, and specific language to produced the
    desired output. We clearly saw this in this recipe. The simle prompt, `A dog`,
    resulted in a standard image of a dog, but observe in *Figure 2**.4* the variation
    of images that are produced when we run image generation again with the same prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Outputs generated for the same image prompt](img/B21007_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Outputs generated for the same image prompt
  prefs: []
  type: TYPE_NORMAL
- en: The images contain different types of dogs, different environments, different
    angles, and different lighting. Compare this to the more detailed prompt we used
    – the following figure shows the additional images that were produced by the more
    detailed prompt about a Corgi in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Different outputs generated from a specific prompt](img/B21007_02_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Different outputs generated from a specific prompt
  prefs: []
  type: TYPE_NORMAL
- en: A more descriptive prompt significantly narrows down the possibilities, guiding
    the model towards producing an image that matches the user’s wants and intentions.
    It’s important to specify colors, positioning, objects, emotions, lighting, and
    other details so that the user can ensure that the generated image is not only
    relevant but also intricately tailored to their requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The Images endpoint in the OpenAI API unlocks a variety of different use cases.
    For example, the endpoint can be used to build personalized pictures in stories,
    visualize different food recipe ideas, and even create funny pictures for greeting
    cards. The possibilities are truly endless!
  prefs: []
  type: TYPE_NORMAL
- en: Generating transcripts using the Audio endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use the OpenAI API’s Audio endpoint, which
    converts audio into text. This enables developers to create voice applications,
    such as voice agents and speech conversational bots.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will also use Postman, but the typical set of Headers that we use
    will need to be modified so that the HTTP client uses form data instead of typical
    JSON. In addition, we must have a sample audio file that we can use as an example
    to convert speech to text. **Form data** is a way to encode and send data as key-value
    pairs in HTTP requests instead of JSON-formatted strings. Form data is often used
    for uploading files.
  prefs: []
  type: TYPE_NORMAL
- en: After opening a new request in Postman, navigate to the `Content-Type application/json`
    entry. This will force Postman to default to the `Content-Type` of the request
    based on what is passed in the request **Body**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need an audio file. Any short (i.e., less than a minute) file will
    do, but it must contain words that can be transcribed and must be in one of the
    following formats: `.mp3`, `.mp4`, `.mpeg`, `.mpga`, `.m4a`, `.wav`, or `.webm`.
    You can also download a 10-second audio snip that I cr[eated here: https://github.com/hasygithub/ChatGPT-API-Book/raw/main/aud](https://github.com/hasygithub/ChatGPT-API-Book/raw/main/audiosample.mp3)iosample.mp3.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can add examples of prompts and responses to the Chat Log to modify the
    model’s behavior. Let’s observe this with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Postman, change the **Endpoint** value to the following URL and change the
    request type to **Post**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Select **Body**, and then select the **form-data** radio button. This will open
    the form data fields. Each field contains a **Key** and a **Value**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We have the option to set each field as **Text** or **File**. We can do this
    by hovering over **Key** and selecting our chosen option from the drop-down menu,
    as demonstrated in *Figure 2**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following fields in the form data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| *Key* | *Instruction/Value* |'
  prefs: []
  type: TYPE_TB
- en: '| `file` | Click **Select Files** and upload the audio file you created or
    downloaded earlier |'
  prefs: []
  type: TYPE_TB
- en: '| `model` | `whisper-1` |'
  prefs: []
  type: TYPE_TB
- en: '![Figure 2.6 – Selecting field types in Postman](img/B21007_02_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Selecting field types in Postman
  prefs: []
  type: TYPE_NORMAL
- en: 4. Select the **Send** button to submit the HTTP request.
  prefs: []
  type: TYPE_NORMAL
- en: '5. You should see the following response from OpenAI, specifying the text that
    was transcribed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe uses the *Audio* endpoint, but note that the way we interacted with
    this endpoint’s API was completely different than the other endpoints. In particular,
    this endpoint expects the `Content-Type` of the request to be `form-data`, instead
    of the typical JSON structure that we saw previously. The reason for this is because
    `form-data` is capable of handling file streaming, which we needed here as we
    are uploading audio to OpenAI. In the other endpoints, raw JSON is sufficient
    because only text is being sent to OpenAI, not files.
  prefs: []
  type: TYPE_NORMAL
- en: '**HTTP requests** are the bedrock of data communication on the web and are
    frequently used to interact with APIs. Two options for sending data within HTTP
    requests are **JSON** and **forms**. JSON has now become the ubiquitous data interchange
    format due to its flexibility in structure – it can be used to communicate key-value
    pairs, lists, and hierarchy concepts. However, the reason we used forms is that
    they enable developers to encode binary data. In short, forms enable you to transfer
    files (such as a 10-second MP3 file, in this case).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the form, there were two fields: **file** and **model**. The **file** field
    represents the audio file object that we want to transcribe, and the **model**
    field represents the particular transcription model we want to use – which, in
    this case, was actually limited to only one option: **whisper-1**.'
  prefs: []
  type: TYPE_NORMAL
- en: The **Response** is simple – a JSON with one object (*text*), representing the
    transcribed text.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to use this endpoint, we can integrate it to create powerful
    and smart business applications. For example, typical workflows such as transcribing
    recorded meetings and lectures can be done with a single API request. Several
    endpoints can be chained together for even more advanced use cases. For example,
    we can create voice assistants similar to Siri that take a recorded voice, convert
    it to text, and then call the Chat Completions API to get a response.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to know that the OpenAI API is more than just text generation;
    it can perform image generation, voice-to-text, and even create embeddings for
    semantic search. The OpenAI API is a collection of several different endpoints
    that, when combined, offer developers an invaluable set of tools to build intelligent
    applications.
  prefs: []
  type: TYPE_NORMAL
