["```py\n    %matplotlib inline\n    from IPython.core.interactiveshell import InteractiveShell\n    InteractiveShell.ast_node_interactivity = \"all\"\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    from sklearn.metrics import confusion_matrix\n    ```", "```py\n    url = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'\n    df = pd.read_csv(url)\n    ```", "```py\n    df['is_med_or_high_risk'] = (df['decile_score']>=5).astype(int)\n    ```", "```py\n    cm = pd.crosstab(df[‘is_med_or_high_risk’], df[‘two_year_recid’], rownames=[‘Predicted’], colnames=[‘Actual’], normalize=’index’)\n    p = plt.figure(figsize=(5,5));\n    p = sns.heatmap(cm, annot=True, fmt=\".2f\", cbar=False\n    ```", "```py\n    [[tn , fp],[fn , tp]]  = confusion_matrix(df['two_year_recid'], df['is_med_or_high_risk'])\n    print(\"True negatives:  \", tn)\n    print(\"False positives: \", fp)\n    print(\"False negatives: \", fn)\n    print(\"True positives:  \", tp)\n    ```", "```py\n    df = df[df.race.isin([\"African-American\",\"Caucasian\"])]\n    (df['two_year_recid']==df['is_med_or_high_risk']).astype(int).groupby(df['race']).mean()\n    ```", "```py\nimport pandas as pd\nimport fairlens as fl\ndf = pd.read_csv\"\"../datasets/compas.cs\"\")\n```", "```py\n    fscorer = fl.FairnessScorer(df,\"\"RawScor\"\", \"\"Ethnicit\"\",\"\"Se\"\"])\n    fscorer.plot_distributions()\n    print\"\"Demo Re\"\", fscorer.demographic_report())\n    ```", "```py\n    group1 = \"\"rac\"\": \"\"African-America\"\"]}\n    group2 = \"\"rac\"\": \"\"Caucasia\"\"]}\n    fl.plot.distr_plot(df,\"\"decile_scor\"\", [group1, group2])\n    plt.legend(\"\"African-America\"\",\"\"Caucasia\"\"])\n    plt.show()\n    ```", "```py\ngroup1 = \"\"Ethnicity\"\": \"\"African-America\"\"]}\ngroup2 = df\"\"Ethnicity\"\"] ==\"\"Caucasia\"\"\nprint (fl. metrics. stat_distance(df, target_attr, group1, group2, mode\"\"aut\"\", p_value=True))\n```", "```py\ncol_names = \"\"gende\"\",\"\"nationalit\"\",\"\"rando\"\",\"\"corr\"\",\"\"corr\"\"]\ndata = [\n    \"\"woma\"\",\"\"spanis\"\", 715, 10, 20],\n    \"\"ma\"\",\"\"spanis\"\", 1008, 20, 20],\n    \"\"ma\"\",\"\"frenc\"\", 932, 20, 10],\n    \"\"woma\"\",\"\"frenc\"\", 1300, 10, 10],\n]\ndf = pd.DataFrame(data, columns=col_names)\nprint(fl.sensitive.find_sensitive_correlations(df))\n```", "```py\n    covar_sex_dur = data.Sex.cov(data.Duration)\n    ```", "```py\n    variance_sex = data.Sex.var()\n    variance_dur= data.Duration.var()\n    ```", "```py\n    association = covar_sex_dur/(variance_sex*variance_dur)\n    print \"\"Association between Sex and Duratio\"\", association)\n    ```", "```py\nAssociation between Sex and Duration -0.014593816319825929\n```", "```py\n    import pandas as pd\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    df = pd.read_csv\"\"../datasets/german_credit_data.cs\"\")\n    data = df[''Ag'',''Se'',''Jo'',''Duratio'',''Credit amoun'']]\n    data = data.dropna()\n    ```", "```py\n    data''Se''] = data''Se''].map(''mal'': 0,''femal'': 1})\n    X = data[''Ag'',''Se'',''Jo'',''Credit amoun'',''Duratio'']]\n    ```", "```py\n    vif_data = pd.DataFrame()\n    vif_data\"\"featur\"\"] = X.columns\n    vif_data\"\"VI\"\"] = [variance_inflation_factor(X.values, i)\n                       for i in range(len(X.columns))]\n    print(vif_data)\n    ```", "```py\n               feature    VIF\n                Age  5.704637\n                Sex  1.365161\n                Job  7.180779\n       Credit amount 3.970147\n            Duration 6.022894\n    ```", "```py\n    mi_1 = mutual_info_score(data''Ag''], data''Se''])\n    mi_2 = mutual_info_score(data''Jo''], data''Se''])\n    mi_3 = mutual_info_score(data''Duratio''], data''Se''])\n    mi_4 = mutual_info_score(data''Credit amoun''], data''Se''])\n    print\"\"Mutual Inf\"\", mi_1, mi_2, mi_3, mi_4, mi_5)\n    ```", "```py\n    Mutual Info 0.06543499129250782 0.003960052834578523 0.019041038432321293 0.5717832060773372\n    ```", "```py\n    group1 = df[df\"\"Se\"\"] ==\"\"Male\"] \"\"RawScor\"\"]\n    group2 = df[df\"\"Se\"\"] ==\"\"Female\"] \"\"RawScor\"\"]\n    ```", "```py\n    test_statistic = lambda x, y: x.mean()–- y.mean()\n    t_distribution = fl.metrics.permutation_statistic(group1, group2, test_statistic, n_perm=100)\n    ```", "```py\nt_distribution = fl.metrics.bootstrap_statistic(group1, group2, test_statistic, n_samples=100)\n```", "```py\n    t_observed = test_statistic(group1, group2)\n    print(\"Resampling Interval\", fl.metrics.resampling_interval(t_observed, t_distribution, cl=0.95))\n    print(\"Resampling Pval\", fl.metrics.resampling_p_value(t_observed, t_distribution, alternative=\"two-sided\"))\n    ```", "```py\n    Resampling Interval (0.24478083502138195, 0.31558333333333327)\n    Resampling Pval 0.37\n    ```", "```py\n    X, y, grouplabels, group_names, group_types, is_categorical = \\\n        setup_matrices(path, label, groups, usable_features=usable_features,\n                       drop_group_as_feature=drop_group_as_feature,\n                       categorical_columns=categorical_columns, groups_to_drop=groups_to_drop,\n                       verbose=verbose,\n                       save_data=save_data, file_dir=file_dir, file_name=file_name)\n    ```", "```py\n    generate_synthetic_data(numdims, noise, numsamples=1000, num_group_types=1, min_subgroups=2, max_subgroups=10, min_subgroup_size=20, mean_range=0, variability=1, num_uniform_features=0, intercept_scale=2, binary=False, drop_group_as_feature=False, save_data=False, file_dir='', file_name='', random_seed=0)\n    ```", "```py\n    minimax_err, max_err, initial_pop_err, agg_grouperrs, agg_poperrs, _, pop_err_type, total_steps, _, _, _, \\\n    _, _, _ = \\\n        do_learning(X, y, numsteps, grouplabels, a, b, equal_error=False,\n                    scale_eta_by_label_range=scale_eta_by_label_range, model_type=model_type,\n                    gamma=0.0, relaxed=False, random_split_seed=random_split_seed,\n                    group_names=group_names, group_types=group_types, data_name=data_name,\n                    verbose=verbose, use_input_commands=use_input_commands,\n                    error_type=error_type, extra_error_types=extra_error_types, pop_error_type=pop_error_type,\n                    convergence_threshold=convergence_threshold,\n                    show_legend=show_legend, save_models=False,\n                    display_plots=display_intermediate_plots,\n                    test_size=test_size, fit_intercept=fit_intercept, logistic_solver=logistic_solver,\n                    max_logi_iters=max_logi_iters, tol=tol, penalty=penalty, C=C,\n                    n_epochs=n_epochs, lr=lr, momentum=momentum, weight_decay=weight_decay,\n                    hidden_sizes=hidden_sizes,\n                    save_plots=save_intermediate_plots, dirname=dirname)\n    ```", "```py\n    import numpy as np\n    import fatf.utils.data.datasets as fatf_datasets\n    import fatf.utils.models as fatf_models\n    import fatf.fairness.predictions.measures as fatf_pfm\n    import fatf.transparency.predictions.counterfactuals as fatf_cf\n    ```", "```py\n    hr_data_dict = fatf_datasets.load_health_records()\n    hr_X = hr_data_dict['data']\n    hr_y = hr_data_dict['target']\n    hr_feature_names = hr_data_dict['feature_names']\n    hr_class_names = hr_data_dict['target_names']\n    ```", "```py\n    hr_y = np.array([hr_class_names[i] for i in hr_y])\n    unique_identifiers = ['name', 'email', 'zipcode', 'dob']\n    columns_to_keep = [i for i in hr_X.dtype.names if i not in unique_identifiers]\n    hr_X = hr_X[columns_to_keep]\n    hr_feature_names = [i for i in hr_feature_names if i not in unique_identifiers]\n    ```", "```py\n    clf = fatf_models.KNN()\n    clf.fit(hr_X, hr_y)\n    ```", "```py\n    data_point_index = 4 + 2\n    data_point = hr_X[data_point_index]\n    data_point_y = hr_y[data_point_index]\n    protected_features = ['gender', 'age']\n    ```", "```py\n    assert protected_features, 'The protected features list cannot be empty.'\n    person = ' is' if len(protected_features) == 1 else 's are'\n    print('The following fautre{} considered protected:'.format(person))\n    for feature_name in protected_features:\n        print('     \"{}\".'.format(feature_name))\n    print('\\nEvaluating counterfactual fairness of a data point (index {}) of '\n          'class *{}* with the following features:'.format(data_point_index,\n    data_point_y))\n    for feature_name in data_point.dtype.names:\n        print('     The feature *{}* has value: {}.'.format(\n            feature_name, data_point[feature_name]))\n    ```", "```py\nThe following features are considered protected:\n \"gender\".\n     \"age\".\nEvaluating counterfactual fairness of a data point (index 6) of class *fail* with the following features:\n     The feature *age* has value: 41.\n     The feature *weight* has value: 73.\n     The feature *gender* has value: female.\n     The feature *diagnosis* has value: heart.\n```", "```py\n    cfs, cfs_distances, cfs_classes = fatf_pfm.counterfactual_fairness(\n        instance=data_point,\n        protected_feature_indices=protected_features,\n        model=clf,\n        default_numerical_step_size=1,\n        dataset=hr_X)\n    ```", "```py\n    cfs_text = fatf_cf.textualise_counterfactuals(\n        data_point,\n        cfs,\n        instance_class=data_point_y,\n        counterfactuals_distances=cfs_distances,\n        counterfactuals_predictions=cfs_classes)\n    print('\\n{}'.format(cfs_text))\n    ```", "```py\n    Counterfactual instance (of class *success*):\n    Distance: 19\n        feature *age*: *41* -> *22*\n    Counterfactual instance (of class *success*):\n    Distance: 20\n    feature *age*: *41* -> *22*\n        feature *gender*: *female* -> *male*\n    ```", "```py\n    equal_accuracy_matrix = fatf_mfm.equal_accuracy(confusion_matrix_per_bin)\n    print_fairness('Equal Accuracy', equal_accuracy_matrix)\n    ```", "```py\n    equal_opportunity_matrix = fatf_mfm.equal_opportunity(confusion_matrix_per_bin)\n    print_fairness('Equal Opportunity', equal_opportunity_matrix)\n    ```", "```py\n    demographic_parity_matrix = fatf_mfm.demographic_parity(\n        confusion_matrix_per_bin)\n    print_fairness('Demographic Parity', demographic_parity_matrix)\n    ```", "```py\n    The *Equal Accuracy* group-based fairness metric for *gender* feature split is:\n         The fairness metric is satisfied for \"('female',)\" and \"('male',)\" sub-populations.\n    The *Equal Opportunity* group-based fairness metric for *gender* feature split are:\n         The fairness metric is satisfied for \"('female',)\" and \"('male',)\" sub-populations.\n    The *Demographic Parity* group-based fairness metric for *gender* feature split is:\n         The fairness metric is >not< satisfied for \"('female',)\" and \"('male',)\" sub-populations.\n    ```", "```py\n    inputsize = train_dataset[\"data\"]. shape[1]\n    layersizes = [100]\n    classifier_type = \"paritynn\"\n    hparams = {\n        \"classifier_type\": classifier_type,\n        \"layersizes\": layersizes,\n        \"inputsize\": inputsize,\n    }\n    ```", "```py\n    classifier = construct_classifier(hparams, loaddir=loaddir)\n    classifier.train(train_dataset, logdir, epochs=args.epochs,]\n    validation_dataset=validation_dataset)\n    savepath = classifier.save_model(logdir)\n    n = validation_dataset[\"label\"].shape[0]\n    ```", "```py\n    classifier_types = [SimpleNN, ParityNN, AdversariallyCensoredNN]\n    def construct_classifier(hparams, sess=None, loaddir=None):\n        for c_type in classifier_types:\n            if c_type.name == hparams[\"classifier_type\"]:\n                classifier = c_type(sess=sess)\n                classifier.build(hparams=hparams)\n                if loaddir is not None:\n                    classifier.load_model(loaddir)\n                return classifier\n    ```", "```py\n    n_males = sum(validation_dataset[\"label\"])\n    limiting_gender = n_males > n - n_males\n    n_limiting_gender = sum(validation_dataset[\"label\"] == limiting_gender)\n    max_points_per_gender = 500\n    n_per_gender = min(max_points_per_gender, n_limiting_gender)\n    inds = np.concatenate([\n        np.where(validation_dataset[\"label\"] == limiting_gender)[0][:n_per_gender],\n        np.where(validation_dataset[\"label\"] != limiting_gender)[0][:n_per_gender]],\n        axis=0)\n    vis_dataset = {k:v[inds, ...] for k, v in validation_dataset.items()}\n    val_embeddings = classifier.compute_embedding(vis_dataset[\"data\"]\n    ```", "```py\n    overall_loss = crossentropy +\\\n            self.hparams[\"dpe_scalar\"]*dpe +\\\n            self.hparams[\"fnpe_scalar\"]*fnpe +\\\n            self.hparams[\"fppe_scalar\"]*fppe +\\\n            self.hparams[\"cpe_scalar\"]*cpe +\\\n            self.hparams[\"l2_weight_penalty\"]*l2_penalty\n    ```", "```py\n    plot_embeddings(val_embeddings,\n                    vis_dataset[\"label\"],\n                    vis_dataset[\"protected\"],\n                    plot3d=True,\n                    subsample=False,\n                    label_names=[\"income<=50k\", \"income>50k\"],protected_names=[\"female\", \"male\"])\n    ```", "```py\n    import networkx as nx\n    import pytorch_lightning as pl\n    from utils import gen_data_nonlinear\n    import sys\n    from decaf import DECAF\n    from decaf.data import DataModule\n    ```", "```py\n    dag_seed = [[1, 2], [1, 3], [1, 4], [2, 5], [2, 0], [3, 0], [3, 6], [3, 7], [6, 9], [0, 8], [0, 9], ]\n    ```", "```py\n    bias_dict = {6: [3]}\n    G = nx.DiGraph(dag_seed)\n    data = gen_data_nonlinear(G, SIZE=2000)\n    dm = DataModule(data.values)\n    data_tensor = dm.dataset.x\n    ```", "```py\n    x_dim = dm.dims[0]\n    z_dim = x_dim\n    lambda_privacy = 0\n    lambda_gp = 10\n    l1_g = 0\n    ```", "```py\n    weight_decay = 1e-2\n    ```", "```py\n    p_gen = (-1)\n    use_mask = True\n    grad_dag_loss = False\n    number_of_gpus = 0\n    ```", "```py\n    model = DECAF(\n        dm.dims[0],\n        dag_seed=dag_seed,\n        h_dim=args.h_dim,\n        lr=args.lr,\n        batch_size=args.batch_size,\n        lambda_privacy=lambda_privacy,\n        lambda_gp=lambda_gp,\n        d_updates=args.d_updates,\n        alpha=args.alpha,\n        rho=args.rho,\n        weight_decay=weight_decay,\n        grad_dag_loss=grad_dag_loss,\n        l1_g=l1_g,\n        l1_W=args.l1_W,\n        p_gen=p_gen,\n        use_mask=use_mask,\n    )\n    trainer = pl.Trainer(\n        gpus=number_of_gpus,\n        max_epochs=args.epochs,\n        progress_bar_refresh_rate=1,\n        profiler=False,\n        callbacks=[],\n    )\n    trainer.fit(model, dm)\n    synth_data = (\n        model.gen_synthetic(\n            data_tensor, gen_order=model.get_gen_order(), biased_edges=bias_dict\n        ).detach()\n        .numpy()\n    )\n    ```"]