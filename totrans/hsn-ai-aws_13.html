<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Classifying Images Using Amazon SageMaker</h1>
                </header>
            
            <article>
                
<p>Image classification has been one of the leading research fields in the last five years. This is not surprising because being able to successfully classify images solves many business problems across a variety of industries. For example, the entire autonomous vehicle industry is dependent on the accuracy of these image classification and object detection models.</p>
<p>In this chapter, we will look at how Amazon SageMaker drastically simplifies the image classification problem. Aside from gathering a rich set of images for training, we will look at how to specify hyperparameters (parameters internal to the algorithm), train Docker images, and use infrastructure specifications for training.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Walking through convolutional neural and residual networks</li>
<li>Classifying images through transfer learning</li>
<li>Performing inferences on images through Batch Transform </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Please use the following link to refer to the source code for this chapter :<a href="https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services">https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Walking through convolutional neural and residual networks</h1>
                </header>
            
            <article>
                
<p><span>The SageMaker image classification algorithm is an implementation of <strong>residual networks</strong> (<strong>ResNets</strong>). Before we delve into the details of the algorithm, let's briefly understand <strong>convolutional neural networks</strong> (<strong>CNN</strong>) and ResNet and how they </span>learn patterns from images.</p>
<p>Like any other neural network, CNNs are made up of input, hidden, and output layers. These networks have learnable parameters called weights and biases. These weights and biases can be adjusted through an appropriate optimizer, such as <strong>Stochastic Gradient Descent</strong> (<strong>SGD</strong>), with backpropagation. However, the difference between any feedforward artificial neural network and CNNs is that the hidden layers in CNNs are convolutional layers. Each convolutional layer consists of one or more filters. The job of these filters is to recognize patterns in input images.</p>
<p>These filters can have varying shapes, ranging from 1 x 1 to 3 x 3 and so on, and are initialized with random weights. As the input image passes through the convolutional layer, each filter will slide over every 3 x 3 block of pixels (in the case of a 3 x 3 filter) until the entire image is covered. This sliding is referred to as convolving. During the process of convolving, a dot product is applied to the filter weights and pixel values in the 3 x 3 block, thus learning about the image's features. The initial layers of the CNN learn basic geometric shapes, such as edges and circles, while later layers learn about more sophisticated objects, such as eyes, ears, feathers, beaks, cats, and dogs.</p>
<p><span>With deeper convolutional neural networks, as we stack more layers to learn complex features, vanishing gradient problems arise. In other words, during the training process, some neurons die (do not activate), causing a vanishing gradient. This happens when an activation function receives input with varying distributions (for example, if you're passing black and white images of cats as opposed to colored ones through the network, the input raw pixels belong to a different distribution, causing a vanishing gradient problem). If we restrict neuron output to the area to around zero, we can ensure that each layer will pass a substantive gradient back to the previous layers.</span></p>
<p>To solve the challenges that CNNs bring with them, deep residual learning combines what has been learned from previous layers with what has been learned from shallower models:</p>
<p class="CDPAlignCenter CDPAlign"><em><img class="fm-editor-equation" src="assets/81c15e7f-52f1-4cef-9ae2-3ce15e463a64.png" style="width:13.58em;height:1.25em;"/></em></p>
<p>Here,<em><span> <img class="fm-editor-equation" src="assets/59f4c0f0-897d-4e35-b86e-0d8299b3f806.png" style="width:1.75em;height:1.00em;"/></span></em><span> is a convolutional layer or shallower model and </span><span><img class="fm-editor-equation" src="assets/2bcc9ac6-3a27-4ef9-8ae4-02b8105d0834.png" style="width:0.67em;height:0.67em;"/> is the previous layer.</span></p>
<p>Residual networks, while addressing the challenges of CNNs, are an optimal approach to use when classifying images. In the next section, we will look at transfer learning as an approach to incrementally training an already-trained image classification model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying images through transfer learning in Amazon SageMaker</h1>
                </header>
            
            <article>
                
<p>One of the key challenges in classifying images<span> is the availability of large training datasets. For example, to create Amazon Go-type experiences, the e-commerce retailer may have trained their machine learning algorithms on large volumes of images. When we don't have images covering all types of real-world scenarios – scenarios ranging from time of the day (brightness), ambience around the target item, and item angle – we're unable to train image classification algorithms that are able to perform well in real-life environments. Furthermore, it takes a lot of effort to build a convolutional neural network architecture that is optimal for the dataset at hand. These considerations range from the number of convolutional layers to the batch size, to the optimizer, and to dropout rates. It takes multiple trial-and-error experiments to arrive at an optimal model iteration.</span></p>
<p>Because image classification requires a large number of images for training convolutional networks, an alternative approach can be used to classify images when the size of the training dataset is small. Transfer learning allows you to apply the knowledge of an already trained model to a different but related problem. We can reuse the weights of a pre-trained deep learning model that's been trained on millions of images and fine-tune the network with a new/custom dataset that's unique to our business case. Through transfer learning, low-level geometric features, such as edges, can already be recognized by a pre-trained ResNet-18 (18 layer network). However, for mid- to high-level feature learning, the top <strong>fully connected</strong> (<strong>FC</strong>) layer<span> </span>is reinitialized with random weights. Then, the whole network is fine-tuned with the new data—the random weights are adjusted by passing training data through the network and using an optimization technique, for example, stochastic gradient descent with backpropagation.</p>
<p>In this chapter, <span>we'll employ SageMaker's image classification algorithm in transfer learning mode to classify some bakery and fast food items. </span>We will use the pre-trained ResNet-18 that's provided by Amazon SageMaker. The image classification algorithm implements ResNets to categorize images. We can either train ResNets from scratch or use pre-trained networks. Since we have a small image dataset to train, we'll use an 18-layer pre-trained ResNet provided by Amazon SageMaker. We can also experiment with ResNet50, a 50-layer residual network, to determine which network yields a higher performance. Usually, deeper networks perform better than shallow networks since they are able to represent images better. However, given the type and complexity of the input images, the results can vary.</p>
<p>Our new dataset contains around 302 images with five categories (Hot Dog, Berry Donut, Glazed Twist, Muffin, and Peanut Butter Cookie). Each of these items contains 40 to 90 images, covering the item from varying angles, as well as brightness, contrast, and size.</p>
<p>The image classifier learns about the image's low-level features from a pre-trained ResNet and the high-level features by training the same ResNet-18 with a new dataset. The following is an illustration of how the features—low, mid, and high-level—of a Berry Donut are learned by SageMaker's image classification algorithm:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7a6fc71d-830a-4765-b9b6-d132bb3e8394.png" style=""/></div>
<p>So far, we've reviewed what transfer learning is and when it is appropriate. We've also briefly described the image dataset that we're going to feed to the <span>image classification</span> algorithm in SageMaker. Let's get the images dataset prepared for training. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating input for image classification</h1>
                </header>
            
            <article>
                
<p>Amazon SageMaker's <span>image classification</span> algorithm accepts images in file mode via two content types, namely:</p>
<ul>
<li>RecordIO (application/<kbd>x-recordio</kbd>)</li>
<li>Image (image/<kbd>.png</kbd>, image/<kbd>.jpeg</kbd>, and application/<kbd>x-image</kbd>)</li>
</ul>
<p>In this chapter, we will use the RecordIO format. <strong>RecordIO</strong> is a binary format for representing images efficiently and s<span>toring them in a compact format</span>. Training and validation images are available in a zipped format as part of the source code associated with this chapter.</p>
<p>In order to create RecordIO files for our training and validation datasets, we will do the following:</p>
<ul>
<li>Extract <kbd>.zip</kbd> files, both training and validation (via the <kbd>extract_zipfile</kbd> function)</li>
<li>Create list files for training and validation (via the <kbd>create_listfile</kbd> function)</li>
<li>Create Record IO files for training and validation (via the <kbd>create_recordio</kbd> function)</li>
</ul>
<p>For definitions of these functions, please refer to the accompanying source code folder:</p>
<pre># Extract training and validation zipped folders to merch_data/&lt;train/val&gt;<br/><br/>extract_zipfile(bucket, train_key, rel_train_path)<br/>extract_zipfile(bucket, val_key, rel_val_path)<br/><br/># Create List files (./merch_data)<br/>create_listfile(rel_train_path, listfile_train_prefix) #data path, prefix path<br/>create_listfile(rel_val_path, listfile_val_prefix)<br/><br/># # Create RecordIO file<br/># data path --&gt; prefix path (location of list file)<br/># mxnet's im2rec.py uses ./merch_data folder to locate .lst files for train and val<br/># mxnet's im2rec.py uses ./merch_data/&lt;train/val&gt; as data path<br/># list files are used to create recordio files<br/><br/>create_recordio(rel_train_path, listfile_train_prefix)<br/>create_recordio(rel_val_path, listfile_val_prefix)</pre>
<p>To create a RecordIO format for training and validation datasets, we need to create a list file that outlines the image index, followed by image classification (note that we have five categories of images) and the location of the image itself. We need to define these attributes for each of the images in the training and validation datasets. To create a list file for images, we will use the <strong>im2rec</strong> (<strong>image to Recordio</strong>) module of MXNet, an open-source deep-learning library for training and deploying deep learning models.</p>
<p>The following code snippet illustrates how to create a list file using the <kbd>im2rec</kbd> module. In order to create a list file, <kbd>im2rec</kbd> requires the location of the images:</p>
<pre># Create List file for all images present in a directory<br/><br/>def create_listfile(data_path, prefix_path):<br/>    """<br/>    input: location of data -- path and prefix<br/>    """<br/><br/>    # Obtain the path of im2rec.py on the current ec2 instance<br/>    im2rec_path = mx.test_utils.get_im2rec_path()<br/><br/>    with open(os.devnull, 'wb') as devnull:<br/>        subprocess.check_call(['python', im2rec_path, '--list', '--recursive', prefix_path, data_path], stdout=devnull) </pre>
<p>The <kbd>create_listfile()</kbd> function produces the following output. The following is an excerpt of a sample list file:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6442d6ce-5baf-450d-82ac-68680a82a7a1.png" style=""/></div>
<p>From the list file we created, we produce a compressed representation of images via the RecordIO format—again, using the im2rec module from MXNet.</p>
<p>We will now upload the aforementioned training and validation datasets (<kbd>.rec</kbd> files) to an S3 bucket. Additionally, we will upload test images, separately from the training and validation images, to a test folder. Please refer to the <span>accompanying source code folder. The following screenshot shows the S3 bucket, along with the relevant datasets:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c3ec101d-0706-494b-8877-bbb71867f51f.png" style=""/></div>
<p>Now that we have all the datasets for training and inference, we are ready to define the parameters of the <span>image classification</span> algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining hyperparameters for image classification</h1>
                </header>
            
            <article>
                
<p>There are two kinds of parameter that we need to specify before fitting the model to the training and validation datasets:</p>
<ul>
<li><span>Parameters for the training job</span></li>
<li><span>Hyperparameters that are specific to the algorithm</span></li>
</ul>
<p>The parameters for the training job deal with the input and output configuration, including the type of infrastructure to provision.</p>
<p><span>To train the job configuration, we need to do the following:</span></p>
<ol>
<li><span>First, we need to define the image classification Docker image and training input mode (file versus pipe mode. Pipe mode is a recent addition to the SageMaker toolkit, where input data is fed on the fly to the algorithm's container with no need to download it before training).</span></li>
<li><span>Next, we </span>define the location of the training output (<kbd>S3OutputPath</kbd>), along with the number and type of EC2 instances, to provision and the hyperparameters.</li>
<li>After that, we specify the <em>train</em> and <em>validation</em> channels<span>, which are going to be the </span>locations of the training and validation data. As for distribution training, the algorithm currently only supports <kbd>fullyreplicated</kbd> mode, where data is copied onto each machine.</li>
</ol>
<p>The following hyperparameters are specific to the algorithm:</p>
<ul>
<li><kbd>num_layers</kbd>: The number of layers for the network. In this example, we will use the default 18 layers.</li>
<li><kbd>image_shape</kbd>: Image dimensions (<em>width x height</em>).</li>
<li><kbd>num_training_samples</kbd>: This is the total number of training data points. In our case, this is set to <kbd>302</kbd>.</li>
<li><kbd>num_classes</kbd>: This is the number of categories. For our dataset, this is 5. We will classify five pieces of merchandise.</li>
<li><kbd>mini_batch_size</kbd>: The number of training samples that are used for each mini-batch. In a single machine multi-GPU setting, each GPU handles <kbd>mini_batch_size</kbd>/num of GPU samples. In the case of distributed training, where multiple machines are involved, the actual batch size is the number of <kbd>machines</kbd> * <kbd>mini_batch_size</kbd>.</li>
<li><kbd>epochs</kbd>: The number of iterations to go through to train the classification algorithm.</li>
<li><kbd>learning_rate</kbd>: This defines how big the steps should be when back-propagating to reduce loss. In the case of transfer learning, we will take smaller steps so that we can incrementally train the pre-trained network.</li>
</ul>
<p>In the following code, we've defined the values of each of the hyperparameters:</p>
<pre># The algorithm supports multiple network depth (number of layers). They are 18, 34, 50, 101, 152 and 200<br/># For this training, we will use 18 layers<br/><br/>num_layers = 18<br/>image_shape = "3,224,224" # Number of channels for color image, Number of rows, and columns (blue, green and red)<br/>num_training_samples = 302 # number of training samples in the training set<br/>num_classes = 5 # specify the number of output classes<br/>mini_batch_size = 60 # batch size for training<br/>epochs = 4  # number of epochs<br/>learning_rate = 0.01 #learning rate<br/>top_k=2<br/># Since we are using transfer learning, we set use_pretrained_model to 1 so that weights can be initialized with pre-trained weights<br/>use_pretrained_model = 1</pre>
<p>It is now time for training: we will provide the <em>training parameters</em> that we defined as input to the <kbd>create_training_job</kbd> method of SageMaker<span>. The </span>SageMaker service is invoked using <kbd>boto3</kbd>, an Amazon Web Services SDK for Python. Once the training job has been created, we can check its status.</p>
<p>Use the following code to create a training job in SageMaker:</p>
<pre># create the Amazon SageMaker training job<br/>sagemaker = boto3.client(service_name='sagemaker')<br/>sagemaker.create_training_job(**training_params)<br/><br/># confirm that the training job has started<br/>status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']<br/>print('Training job current status: {}'.format(status))<br/><br/>Output:<br/>Training job current status: InProgress<br/>Training job ended with status: Completed</pre>
<p>We will now plot the results to evaluate the training and validation accuracy of ResNet-18. We want to ensure that we've not overfitted the network—<span>a </span>scenario where validation accuracy decreases as training accuracy increases. Let's have a look at the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a248ee06-cc86-4884-8388-0b8fee7d2d96.png" style=""/></div>
<p>The results from training are available in the CloudWatch logs. The preceding representation is a visual of how the accuracy of the training and validation sets varies during the training period. The following code explains the blue and orange lines in the preceding graph:</p>
<pre>Training: Blue Line -- trn_acc[0.366667, 0.86, 0.966667, 0.986667]<br/><br/>Validation: Orange Line -- val_acc[0.45, 0.583333, 0.583333, 0.716667] </pre>
<p>As we can see, the trained ResNet model has picked up enough patterns from the fast-food and bakery images. We deployed the trained model for inference.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing inference through Batch Transform</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this section, we will classify (<span>in batch mode)</span> a few images that form part of the test dataset. Since we want to classify more than one image at a time, we will create a Batch Transform job. Please refer to <a href="16e50aca-401b-47b0-87c3-34cc0346e66e.xhtml"/><a href="16e50aca-401b-47b0-87c3-34cc0346e66e.xhtml">Chapter 8</a>, <em>Creating Machine Learning Inference Pipelines</em>, to learn about when and where Batch Transform jobs are used and how they work.</p>
<p>Before we create a Batch Transform job, we need to provision the trained model.</p>
<p>In the following code snippet, we are going to do the following:</p>
<ol>
<li>We will create a trained model by calling the <kbd>create_model()</kbd> function of the SageMaker service (<kbd>boto3</kbd>, the AWS SDK for Python, is used to provision a low-level interface to the SageMaker service).</li>
<li><span>We will pass a Docker image of the image classification algorithm and the path to the trained model to this function:</span></li>
</ol>
<pre style="padding-left: 60px">info = sage.describe_training_job(TrainingJobName=job_name)<br/># Get S3 location of the model artifacts<br/>model_data = info['ModelArtifacts']['S3ModelArtifacts']<br/>print(model_data)<br/># Get the docker image of image classification algorithm<br/>hosting_image = get_image_uri(boto3.Session().region_name, 'image-classification')<br/>primary_container = {<br/>    'Image': hosting_image,<br/>    'ModelDataUrl': model_data,<br/>}<br/># Create model <br/>create_model_response = sage.create_model(<br/>    ModelName = model_name,<br/>    ExecutionRoleArn = role,<br/>    PrimaryContainer = primary_container)<br/>print(create_model_response['ModelArn'])</pre>
<ol start="3">
<li>Now that the trained model has been provisioned, we will need to create a Batch Transform job.</li>
</ol>
<p style="padding-left: 60px">We will specify the transform input, output, and resources to configure a Batch Transform job. The following are the definitions:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>Transform input defines the location and format of images.</li>
<li>Transform output defines the location of the results of the inference.</li>
<li>Transform resources define the number and type of instances to provision.</li>
</ul>
</li>
</ul>
<p style="padding-left: 60px"><span>In the following code snippet, </span>we call the <kbd>create_transform_job</kbd><em> </em>function<em> </em>of the SageMaker service by passing job specifications as part of the <kbd>request</kbd><em> </em>JSON file:</p>
<pre style="padding-left: 30px">sagemaker = boto3.client('sagemaker')<br/>sagemaker.create_transform_job(**request)<br/><br/>print("Created Transform job with name: ", batch_job_name)<br/><br/>while(True):<br/>    response = sagemaker.describe_transform_job(TransformJobName=batch_job_name)<br/>    status = response['TransformJobStatus']<br/>    if status == 'Completed':<br/>        print("Transform job ended with status: " + status)<br/>        break<br/>    if status == 'Failed':<br/>        message = response['FailureReason']<br/>        print('Transform failed with the following error: {}'.format(message))<br/>        raise Exception('Transform job failed') <br/>    time.sleep(30) </pre>
<ol start="4">
<li>In the previous code, we used the <kbd>describe_transform_job()</kbd> function of the SageMaker service to obtain the status of the Batch Transform job. The preceding code will return the following message:</li>
</ol>
<pre style="padding-left: 60px">Created Transform job with name: merch-classification-model-2019-03-13-11-59-13<br/>Transform job ended with status: Completed</pre>
<p><span>It is now time to review the results. </span>Let's navigate to the Batch Transform output and test dataset folders on the S3 bucket to review the results. For each of the images in the test dataset, we will print their highest classification probability, that is, what the trained model classifies an input image as:</p>
<ol>
<li class="mce-root">The first image in the test dataset is a Hot Dog, as shown in the following screenshot. The trained model identifies the Hot Dog with 92% probability.</li>
</ol>
<p style="padding-left: 60px"><span><span>The following is the result of the prediction, that is, l</span></span>abel: <kbd>Hot_Dog_1</kbd>, probability: <kbd>0.92</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ab14c2e6-4f2d-4fd6-ba9d-8dfcb376585f.png" style=""/></div>
<ol start="2">
<li>The second image is of a Berry Donut, as shown in the <span>following screenshot</span>. <span>The trained model identifies the following screenshot as a Berry Donut with 99% probability:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/990ed830-f963-4d25-9ff5-74c3a3f97108.png" style=""/></div>
<ol start="3">
<li>The third image is a Muffin, as shown <span>in the </span><span>following screenshot</span>. <span>The trained model identifies the following screenshot as a Muffin with 66% probability:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e139f0b1-4d1a-4cfd-89b1-05a79efce4a0.png" style=""/></div>
<ol start="4">
<li>In the case of the fourth image, however, the trained model does not correctly identify the image. While the real image is a Peanut Butter Cookie, the model misidentifies it as a Muffin. One interesting thing to note here is that the Cookie looks like a Muffin:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6cc98739-63fd-4fa4-b439-cb7b9fff3326.png" style=""/></div>
<p>As we can see, out of the four images three were classified correctly. To improve the accuracy of the model, we can consider hyperparameter tuning and collecting large volumes of fast-food and bakery images. Transfer learning, therefore, is employed to incrementally train pre-trained image classification models with use-case-specific images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we've gone through an overview of convolutional neural and residual networks. In addition, we've illustrated how SageMaker's image classification algorithm can be used to identify fast-food and bakery images. Specifically, we've reviewed training an image classification algorithm<span>, including </span>provisioning its infrastructure; creating a compressed image format (RecordIO) for training and validation datasets; and supplying formatted datasets for model fitting. For inference, we've employed the Batch Transform feature of SageMaker to classify multiple images in one go. </p>
<p>Most importantly, we've learned how to apply transfer learning to image classification. This technique becomes very powerful in instances where you do not have large amounts of training data.</p>
<p>In the next chapter, you'll learn how to forecast retail sales using the DeepAR algorithm from SageMaker—another use case where deep learning can be used to solve real business challenges.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>MXNet estimator in SageMaker</strong>: <span class="MsoHyperlink"><a href="https://medium.com/devseed/use-label-maker-and-amazon-sagemaker-to-automatically-map-buildings-in-vietnam-a63090fb399f">https://medium.com/devseed/use-label-maker-and-amazon-sagemaker-to-automatically-map-buildings-in-vietnam-a63090fb399f</a><br/></span></li>
<li><strong>Vanishing Gradient</strong>: <span class="MsoHyperlink"><a href="https://towardsdatascience.com/intuit-and-implement-batch-normalization-c05480333c5b">https://towardsdatascience.com/intuit-and-implement-batch-normalization-c05480333c5b</a><br/></span></li>
<li><strong>AWS SageMaker Labs</strong>: <span class="MsoHyperlink"><a href="https://github.com/awslabs/amazon-sagemaker-examples">https://github.com/awslabs/amazon-sagemaker-examples</a></span></li>
</ul>
<p><span class="MsoHyperlink"> </span></p>


            </article>

            
        </section>
    </body></html>