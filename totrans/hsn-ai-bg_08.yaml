- en: Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Along with generative networks, reinforcement learning algorithms have provided
    the most visible advances in **Artificial Intelligence** (**AI**) today. For many
    years, computer scientists have worked toward creating algorithms and machines
    that can perceive and react to their environment like a human would. Reinforcement
    learning is a manifestation of that, giving us the wildly popular AlphaGo and
    self-driving cars. In this chapter, we'll cover the foundations of reinforcement
    learning that will allow us to create advanced artificial agents later in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning plays off the human notion of learning from experience. Like
    generative models, it learns based on **evaluative feedback**. Unlike instructive
    feedback ...
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be utilizing TensorFlow in Python. We will also be
    using the OpenAI gym to test our algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI gym is an open source toolkit for developing and testing reinforcement
    learning algorithms. Written in Python, there are environments from Atari games
    to robot simulations. As we develop reinforcement learning algorithms in this
    chapter and later chapters, gym will give us access to test environments that
    would otherwise be very complicated to construct on our own.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need either a macOS or Linux environment to run gym. You can install
    gym by running a simple `pip install` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You should now have gym installed! If you've run into an error, you may have
    dependency issues. Check the official gym GitHub repo for the latest dependencies
    ([https://github.com/openai/gym](https://github.com/openai/gym)).
  prefs: []
  type: TYPE_NORMAL
- en: Principles of reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reinforcement learning is based on the concept of learning from interaction
    with a surrounding environment and consequently rewarding positive actions taken
    in that environment. In reinforcement learning, we refer to our algorithm as the **agent** because
    it takes action on the world around it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb5e4352-6aa7-4258-adf7-dade399596c8.png)'
  prefs: []
  type: TYPE_IMG
- en: When an agent takes an action, it receives a reward or penalty depending on
    whether it took the *correct *action or not. Our goal in reinforcement learning
    is to let the agent learn to take actions that maximize the rewards it receives
    from its environment. These concepts are not at all new; in fact, they've been
    around ...
  prefs: []
  type: TYPE_NORMAL
- en: Markov processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the crux of reinforcement learning is the **Markov Decision process** (**MDP**).
    Markov processes are random strings of events where the future probabilities of
    events happening are determined by the probability of the most recent event. They
    extend the basic Markov Chain by adding rewards and decisions to the process.
    The fundamental problem of reinforcement learning can be modeled as an MDP. **Markov
    models** are a general class of models that are utilized to solve MDPs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Markov models** rely on a very important property, called the **Markov property**,
    where the current state in a Markov process completely characterizes and explains
    the state of the world at that time; everything we need to know about predicting
    future events is dependent on where we are in the process. For instance, the following
    Markov process models the state of the stock market at any given time. There are
    three states– a **Bull market**, a **Bear market**, or a **Stagnant market **–
    and the respective probabilities for staying in each **state** or transitioning
    to another state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5df44dcc-f24f-45d3-94cd-e6f3b9cc3cd1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The entity that navigates an MDP is called an **agent**. In this case, the
    agent would be the stock market itself. We can remember the parameters of the
    Markov process by SAP:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Set of possible states** (**S**):The possible states of being that an agent
    can be in at any given time. When we talk about states in reinforcement learning,
    this is what we are referring to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Set of possible actions** (**A**): All of the possible actions that an agent
    can take in its environment. These are the lines between the states; what actions
    can happen between two states?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transition probability** (**P**): The probability of moving to any of the
    new given states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of any reinforcement learning agent is to solve a given MDP by maximizing
    the **reward** it receives from taking specific actions.
  prefs: []
  type: TYPE_NORMAL
- en: Rewards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we mentioned previously, reinforcement learning algorithms seek to maximize
    their potential future reward. In deep learning languages, we call this the expected **reward**.
    At each time step, *t*, in the training process of a reinforcement learning algorithm,
    we want to maximize the return, *R*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dddc4ed9-78e8-410a-8c21-4484f8c0a281.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our final reward is the summation of all of the expected rewards at each time
    step – we call this the **cumulative reward**. Mathematically, we can write the
    preceding equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8b23352-c032-4d95-a205-32d3e3c353a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Theoretically, this process could go on forever; the termination ...
  prefs: []
  type: TYPE_NORMAL
- en: Policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A policy, simply stated, is a way of acting; your place of employment or education
    has policies about how, what, and when you can do things. This term is no different
    when used in the context of reinforcement learning. We use policies to map states
    to potential actions that a reinforcement learning agent can take. Mathematically
    speaking, policies in reinforcement learning are represented by the Greek letter
    π, and they tell an agent what action to take at any given state in an MDP. Let''s
    look at a simple MDP to examine this; imagine that you are up late at night, you
    are sleepy, but maybe you are stuck into a good movie. Do you stay awake or go
    to bed? In this scenario, we would have three states:'
  prefs: []
  type: TYPE_NORMAL
- en: Your initial state of sleepiness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being well rested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Being sleep deprived
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these states has a transition probability and reward associated with
    taking an action based on them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/838e883c-5989-4ca8-b55f-ff3db83f0249.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, Let's say you decide to stay up. In this scenario, you'll find
    yourself in the **Don't Sleep** transition state. From here, there is only one
    place to go - a 100% chance of ending up in the **Sleep Deprived** State.
  prefs: []
  type: TYPE_NORMAL
- en: If you slept, there is a 90% chance of you being well rested and a 10% chance
    of you still being tired. You could return to your tired state by not sleeping
    as well. In this scenario, we'd want to choose the actions (sleeping) that maximize
    our rewards. That voice in your head that's telling you to go to sleep is the
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: Our objective is to learn a policy (![](img/f069ed33-7df1-4b2e-8f86-580c65a7db09.png))
    that maximizes the network's reward; we call this the **optimal policy**. In this
    case, the optimal policy is **deterministic**, meaning that there is a clear optimal
    action to take at each state. Policies can also be **stochastic**, meaning that
    there is a distribution of possible actions that can be drawn from.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning agents can learn **on-policy** or **off-policy**; when
    an algorithm is on-policy, it learns the policy from all of the agent's actions,
    including exploratory actions that it may take. It improves the *existing policy*.
    Off-policy learning is *off from the previous policy*, or in other words, evaluating
    or learning a policy that was different from the original policy. Off-policy learning
    happens independent of an agent's previous. Later on in this chapter, we'll discuss
    two different approaches to reinforcement learning – one on-policy (policy gradients)
    and one off-policy (Q-learning).
  prefs: []
  type: TYPE_NORMAL
- en: To help our algorithm learn an optimal policy, we utilize **value functions**.
  prefs: []
  type: TYPE_NORMAL
- en: Value functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A value function helps us measure the expected reward at certain states; it
    represents the expected cumulative reward from following a certain policy at any
    given state. There are two types of value functions used in the field of reinforcement
    learning; **state value functions ***V*(*s*) and **action value functions ![](img/c08de824-20ac-4874-a897-77581021565b.png)**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The state value function describes the value of a state when following a policy.
    It is the expected return that an agent will achieve when starting at state *s* under
    a policy π. This function will give us the expected reward for an agent given
    that it starts following a policy at state*s*:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's break down what this function ...
  prefs: []
  type: TYPE_NORMAL
- en: The Bellman equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As one of the most important equations in the entire field of reinforcement
    learning, the Bellman equation is the cornerstone of solving reinforcement learning
    problems. Developed by applied mathematician Richard Bellman, it''s less of a
    equation and more of a condition of optimization that models the reward of an
    agent''s decision at a point in time based on the expected choices and rewards
    that could come from said decision. The Bellman equation can be derived for either
    the state value function or the action value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b28e04b-517e-4e36-a3b8-a3dac6d176e5.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/43056f1b-558f-4419-9e41-5644a86bc786.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As usual, let''s break down these equations. We''re going to focus on the state
    value function. First, we have the summation of all policies for every state/action
    pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd04bdaf-5aa1-4575-9faa-20dd81d56f38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we have the transition probability; it''s the probability of being in
    state *s*, taking action *a*, and ending up in state ![](img/825e8573-ef10-4033-8474-2e48bda6425a.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6d0db5d-ba0c-4f29-9cef-64e094924511.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next is the cumulative reward that we discussed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29d7d849-d342-487c-b4b3-32ff1fbec440.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, we have the discounted value of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/568aee06-6340-4d42-90c5-515f1b87864e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Altogether, we''re describing the entire reinforcement learning process; we
    want to find a state value function or an action value function that satisfies
    the Bellman equation. We''re missing one key part here; how do we solve this in
    practice? One option is to use a paradigm called **dynamic programming**, which
    is an optimization method that was also developed by Bellman himself. One means
    of solving for the optimal policy with dynamic programming is to use the **value
    iteration method**. In this manner, we use the Bellman equation as an iterative
    update function. We want to converge from Q to Q* by enforcing the Bellman equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ae2e7b8-896c-4e3d-b9b1-1921a1ea5b86.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's see how this would work in Python by trying to solve the cartpole problem. With
    value iteration, we start with a random value function and then find an improved
    value function in an iterative process until we reach an optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can attempt this on the cartpole problem. Let''s import `gym` and generate
    a random value function to begin with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s turn that policy into an action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we can run the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: While value iterations work in this simplistic environment, we can quickly run
    into problems when utilizing it in larger, more complex environments. As we have
    to individually compute the value for every state/value pair, many unstructured
    inputs such as images become impossibly large. Imagine how expensive it would
    be to compute this function for every single pixel in an advanced video game,
    every time our reinforcement learning algorithm tried to make a move!
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, we utilize deep learning methods to do these computations for
    us. Deep Neural Networks can act as function approximators. There are two primary
    methods, called **Deep Q**-**learning** and **policy gradients**.
  prefs: []
  type: TYPE_NORMAL
- en: Q–learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning is a reinforcement learning method that utilizes the action value
    function, or Q function, to solve tasks. In this section, we'll talk about both
    traditional Q-learning as well as Deep Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard Q-learning works off the core concept of the Q-table. You can think
    of the Q-table as a reference table; every row represents a state and every column
    represents an action. The values of the table are the expected future rewards
    that are received for a specific combination of actions and states. Procedurally,
    we do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the Q-table
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose an action
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform that action
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the reward that was received
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the Q- value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's walk through each of these steps to better understand the algorithm. ...
  prefs: []
  type: TYPE_NORMAL
- en: Policy optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Policy optimization methods** are an alternative to Q-learning and value
    function approximation. Instead of learning the Q-values for state/action pairs,
    these methods directly learn a policy π that maps state to an action by calculating
    a gradient. Fundamentally, for a search such as for an optimization problem, policy
    methods are a means of learning the correct policy from a stochastic distribution
    of potential policy actions. Therefore, our network architecture changes a bit
    to learn a policy directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e760c7dd-ebb7-4f86-80a1-99b79ca3f002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Because every state has a distribution of possible actions, the optimization
    problem becomes easier. We no longer have to compute exact rewards for specific
    actions. Recall that deep learning methods rely on the concept of an episode.
    In the case of deep reinforcement learning, each episode represents a game or
    task, while **trajectories** represent plays or directions within that game or
    task. We can define a trajectory as a path of state/action/reward combinations,
    represented by the Greek letter tau (*Τ*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2ccc04a-71d3-4348-9738-0e06229375e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Think about a robot learning to walk; if we used Q-learning or another dynamic
    programming method for this task, our algorithms would need to learn exactly how
    much reward to assign to every single joint movement for every possible trajectory.
    The algorithm would need to learn timings, exact angles to bend the robotic limbs,
    and so on. By learning a policy directly, the algorithm can simply focus on the
    overall task of moving the robot's feet when it walks.
  prefs: []
  type: TYPE_NORMAL
- en: 'When utilizing policy gradient methods, we can define an individual policy
    in the same simple manner as we did with Q-learning; our policy is the expected
    sum of future discounted rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/679aca0a-2d0d-4591-baf9-47de01f4025f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the goal of our network becomes to *maximize* some policy, *J*,
    in order to maximize our expected future reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/effaee75-f1d0-4f0e-a1db-7efaede13cdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One way to learn a policy is to use gradient methods, hence the name *policy*
    gradients. Since we want to find a policy that maximizes reward, we perform the
    opposite of gradient descent, *gradient ascent*, on a distribution of potential
    policies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f9110be-7efa-4c04-8d9d-ad8bbee171e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The parameters of the new neural network become the parameters of our policy,
    so by performing gradient ascent and updating the network parameters, ![](img/1fe06433-f3bb-4efd-a882-95e32b284d69.png)becomes ![](img/449d7209-ba25-4190-8b0b-f499112d9a48.png). Fully
    deriving policy gradients is outside the scope of this book, however, let''s touch
    upon some high-level concepts of how policy gradient methods work in practice.
    Take a look at the following landscape; there are three potential paths with three
    potential end rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f98503e9-e293-4df2-a122-80db07f2d5ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Policy gradient methods seek to increase the probability of taking the path
    that leads to the highest reward. Mathematically, the standard policy gradient
    method looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8bf327fa-4f88-42ab-89aa-1a183c69e620.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As usual, let''s break this down a bit. We''ll start by saying that our gradient,
    *g*, is the expected value of the gradient *times* the log of our policy for a
    given action/state pair, times the **advantage**. Advantage is simply the action
    value function minus the state value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef3592e7-994c-43bd-9d6b-cd714f9512e0.png)'
  prefs: []
  type: TYPE_IMG
- en: In implementation, vanilla policy gradient methods still run into problems.
    One particularly notable issue is called the credit assignment problem, in which
    a reward signal received from a long trajectory of interaction happens at the
    end of the trajectory. Policy gradient methods have a hard time ascertaining which
    action caused this reward and finds difficulty *assigning credit* for that action.
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradients are an outline for an entire class of algorithms that can be
    built around this idea. The key to optimizing these methods is in the details
    of individual optimization procedures. In the next section, we'll look at several
    means to improve on vanilla policy gradient methods.
  prefs: []
  type: TYPE_NORMAL
- en: Extensions on policy optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One common way to compute policy gradients is with the **Reinforce** **algorithm**. Reinforce
    is a Monte-Carlo policy gradient method that uses likelihood ratios to estimate
    the value of a policy at a given point. The algorithm can lead to high variance.
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla policy gradient methods can be challenging as they are extremely sensitive
    to what you choose for your step size parameter. Choose a step size too big and
    the correct policy is overwhelmed by noise – too small and the training becomes
    incredibly slow. Our next class of reinforcement learning algorithms, **proximal
    policy optimization** (**PPO**), seeks to remedy this shortcoming of policy gradients. PPO is
    a new class of reinforcement learning algorithms that was ...
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned the important foundations of reinforcement learning,
    one of the most visible practices in the AI field.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is based on the concepts of agents acting in an environment
    and taking action based on what it sees in its surrounding environment. An agent's
    actions are guided by either policy optimization methods or dynamic programming
    methods that help it learn how to interact with its environment. We use dynamic
    programming methods when we care more about exploration and off-policy learning.
    On the other hand, we use policy optimization methods when we have dense, continuous
    problem spaces and we only want to optimize for what we care about.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at several different real-world applications of reinforcement learning
    in the upcoming chapter.
  prefs: []
  type: TYPE_NORMAL
