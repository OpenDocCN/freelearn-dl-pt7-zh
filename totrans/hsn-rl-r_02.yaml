- en: Overview of Reinforcement Learning with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reinforcement learning** (**RL**) is a very exciting area of machine learning,
    used in a variety of applications ranging from autonomous cars to games. Learning
    is a process that manifests itself in the form of lasting adaptive changes in
    behavior induced by individual experience. The ability to learn—to establish causal
    relationships between events, to modify one''s behavior based on those experiences,
    and to memorize those relationships—is made possible by the functional organization
    of our nervous system. Animal studies have shown that the brain has one or more
    neural mechanisms through which stimuli and actions can be associated with each
    other. Based on these considerations, a new paradigm has been formulated that
    takes concepts from cognitive learning and transfers them to machine learning.
    In this paradigm, the concepts of environment, agent, and reward become essential
    in the search for a policy that guides us in making the correct decision.'
  prefs: []
  type: TYPE_NORMAL
- en: In RL, algorithms are created that can learn and adapt to environmental changes.
    Interaction with the outside world occurs through external feedback signals (reward
    signals) generated by the environment based on the choices made by the algorithm.
    A correct choice involves a reward while an incorrect choice leads to a penalty.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to the R environment, and you will learn
    how it can be used to solve problems with the use of RL. We will take a first
    look at some packages available in R to solve Markov decision problems, and we
    will see many examples of applications from the real world.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, we will have learned about the concepts underlying RL and
    the different approaches to this technique. We will also have started to learn
    about the packages available in the R environment to deal with this technology,
    and we will have learned about some different practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding RL algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing R for RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with the MDPtoolbox package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RL applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RL is the main behavioral approach that an individual adopts in the environment
    in which they live. The child who learns the correct behavior in the classroom
    thanks to the verbal rewards received from the teacher, the basketball player
    who learns to make a good shot in the basket by making numerous attempts, the
    strategist who plans their moves by analyzing the possible opposing countermeasures:
    all are examples of adaptive behavior. In these contexts, everything depends on
    the environmental conditions in which the agent is immersed. RL is the term used
    to indicate both the problem arising from the examples just described and the
    set of computational methods suitable for maximizing a certain reward function.
    It is the discipline that studies these problems and the possible methods of resolution. In RL,
    the system does not provide the correct answer to the problem, but returns a criticism
    to a response.'
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, we mentioned that RL represents a new machine learning paradigm, so
    before proceeding, it is useful to distinguish the different approaches available
    in artificial intelligence. Machine learning refers to the ability to learn from
    experience without any outside help, which is what we humans do in most cases.
    Why should it not be the same for machines?
  prefs: []
  type: TYPE_NORMAL
- en: The success of machine learning in solving multiple problems of everyday life
    is because of the quality of its algorithms. These algorithms have been improved
    and updated over time. It is possible to diversify by grouping them into large
    categories that are dependent on the nature of the signal that is used for learning
    or the type of feedback adopted by the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'These categories are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: This algorithm is based on the observation of a series
    of examples in which each data input has been previously labeled. Through this
    preliminary analysis, it generates a function that links the input values to the
    desired output. It is used to build predictive models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: This algorithm family tries to derive knowledge
    from a nonlabeled generic input. These algorithms are used to build descriptive
    models. A typical example of the application of these algorithms is search engines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RL**: This kind of algorithm can learn depending on the changes that occur
    in the environment in which it is performed. In fact, since every action has some
    effect on the environment concerned, the algorithm is driven by the same feedback
    environment. Some of these algorithms are used in speech or text recognition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The subdivisions that we have just proposed do not prohibit the use of hybrid
    approaches between some or all of these different areas, which have often recorded
    good results.
  prefs: []
  type: TYPE_NORMAL
- en: RL means to make calculations that can learn and adjust to natural changes.
    This programming strategy depends on the idea of accepting outside upgrades relying
    upon the calculation decisions. A correct decision will include a reward, while
    an off-base decision will prompt a penalty. The objective of the framework is
    to accomplish the most ideal outcome, obviously. In this section, we learned about
    the rudiments of RL.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try to better characterize the different paradigms we introduced, starting
    with supervised algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In supervised learning, we try to build a model, starting from the labeled training
    data, with which we will try to make predictions about data that is not available
    or which is in the future. **Supervision**, therefore, means that in our set of
    samples (the dataset), the desired output signals are already known as previously
    labeled. In this type of learning, based on labels of discrete classes, we will
    therefore have a task based on classification techniques. Another type of technique
    used in supervised learning is **regression**, where output signals are continuous
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, the samples belonging to two sets have been labeled
    with a different symbol, thereby making it easy to identify the line that separates
    the two sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44f0845a-7453-4423-a4bd-c73750941c1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Supervised learning algorithms, starting from an adequate number of examples,
    allow us to create a derived function that is able to approximate the searched-for
    function. If the algorithm returns an adequate degree of approximation, providing
    input data to the derived function, we should be able to obtain output responses
    like those provided by the searched-for function. These algorithms are based on
    the notion that similar inputs correspond to similar outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the real world, this is not always true; however, there are some situations
    in which this approximation can be considered acceptable. The success of these
    algorithms depends significantly on the input data. If there are only a few training
    inputs, the algorithm may not have enough experience to provide the correct output.
    Conversely, too many input values ​​can make it too slow because the derivative
    function generated by many inputs could be very complicated. Furthermore, erroneous
    data can make the whole system unreliable and lead to the agent making the wrong
    decisions, which tells us that the supervised algorithms are very sensitive to
    noise. Supervised algorithms are divided into two large families:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**:If the output value is categorical—for example, belonging
    or not belonging to a class—this is a classification problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: If the output is a continuous real value in a given range,
    then it is a regression problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In real life, we don't always have labeled data available to learn from. When
    this happens, we need to address the problem through a different approach, as
    we will see in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Unlike supervised learning, with unsupervised learning we have data without
    a label, or unstructured data. With these techniques, we can observe the data
    structure and extract meaningful information. In these techniques, however, one
    cannot rely on a known variable relating to the result, or a reward function.
    In the following diagram, we see an example of **clustering**, which is an exploratory
    technique that allows us to aggregate data within groups (called **clusters**)
    that we had no previous knowledge of belonging to the groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92cdbcc9-99cd-4b4e-96b4-016adb27a561.png)'
  prefs: []
  type: TYPE_IMG
- en: The success of these algorithms depends on the importance of the information
    they can extract from databases. In principle, these algorithms are based on comparing
    data and searching for similarities or differences. The input data must contain
    only the set of functionalities necessary for the description of each example.
    Unsupervised algorithms achieve excellent results if the input data consists of
    numerical elements, but are much less precise with nonnumerical data. Obviously,
    they work correctly in the presence of data that contains a clearly identifiable
    order or grouping. Learning from data is the approach on which supervised and
    unsupervised algorithms are based. In both cases, there is no interaction in real
    time with the environment, and this limits the use of these technologies for the
    solution of numerous problems. In the next section, we will see how to deal with
    these problems.
  prefs: []
  type: TYPE_NORMAL
- en: RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The third type of machine learning paradigm is RL. The objective of this type
    of learning is to build a system (agent) that improves its performance through
    interactions with the environment. In order to improve the functionality of the
    system, reinforcements are introduced—that is, reward signals. This reinforcement
    is not given by the labels or the correct values of truth, but it is a measurement
    of the quality of the actions taken by the system. For this reason, it cannot
    be used in supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see the agent–environment interaction scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f96ca6b-ac91-4494-ac63-344bc1fc6064.png)'
  prefs: []
  type: TYPE_IMG
- en: In the real world, we do not always have an explicit indication of what the
    correct output is; we often only have qualitative information (reinforcement signals).
    The data available often does not provide any information on how to update the
    agent's behavior, and so there is no policy that indicates how to update weights.
    It is not possible to define a cost function or a gradient. In these cases, we
    can define a system with the aim of creating intelligent agents able to learn
    from their own experience.
  prefs: []
  type: TYPE_NORMAL
- en: Interaction with the environment is the main concept on which this technology
    is based. Let's try to understand in depth how this all happens.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding RL algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in the previous sections, RL is a programming technique that
    aims to develop algorithms that can learn and adapt to changes in the environment.
    This programming technique is based on the assumption that an agent is able to
    receive stimuli from the outside and change its actions according to them. So
    a correct choice will result in a reward while an incorrect choice will lead to
    the penalization of the system. The goal of the system is to achieve the highest
    possible reward and, consequently, the best possible result.
  prefs: []
  type: TYPE_NORMAL
- en: 'This result can be obtained through two approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: The first approach involves evaluating the choices of the algorithm and then
    rewarding or punishing the algorithm based on the result. These techniques can
    also adapt to substantial changes in the environment. An example is an image recognition
    program that improves its performance with use. In this case, we can say that
    learning takes place continuously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second approach, a first phase is applied in which the algorithm is previously
    trained, and when the system is considered reliable, it becomes crystallized and
    no longer modifiable. This derives from the observation that constantly evaluating
    the actions of the algorithm can be a process that cannot be automated or that
    is very expensive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are only implementation choices, so it may be the case that an algorithm
    includes the newly analyzed approaches.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have introduced the basic concepts of RL. Now we can analyze the
    various ways in which these concepts have been transformed into algorithms. In
    this section, we will list them, providing an overview and looking in depth at
    practical cases that we will address in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Dynamic programming** (**DP**) represents a set of algorithms that can be
    used to calculate an optimal policy given a perfect model of the environment in
    the form of a **Markov decision process** (**MDP**). The fundamental idea of dynamic
    programming, as well as RL in general, is the use of state values and actions
    to look for good policies.'
  prefs: []
  type: TYPE_NORMAL
- en: The Monte Carlo methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Monte Carlo methods for estimating the value function and discovering excellent
    policies do not require the presence of a model of the environment. They are able
    to learn through the use of the agent's experience alone or from samples of state
    sequences, actions, and rewards obtained from the interactions between the agent
    and environment. The experience can be acquired by the agent in line with the
    learning process or emulated by a previously populated dataset. The possibility
    of gaining experience during learning (online learning) is interesting because
    it allows the agent to obtain excellent behavior even in the absence of *a priori*
    knowledge of the dynamics of the environment. Even learning through an already-populated
    experience dataset can be interesting, because when combined with online learning,
    it makes automatic policy improvement induced by others' experiences possible.
  prefs: []
  type: TYPE_NORMAL
- en: Temporal difference learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Temporal difference** (**TD**) learning algorithms are based on reducing
    the differences between estimates made by the agent at different times. The TD
    algorithm tries to predict a quantity that depends on the future values of a given
    signal. Its name is derived from the differences used in the predictions of successive
    time steps to guide the learning process. The prediction at any time is updated
    to bring it closer to the prediction of the same quantity in the next time step.
    In RL, these predictions are used to predict a measure of the total amount of
    reward expected in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will introduce three families of algorithms that deal
    with TD learning, each with a different approach.
  prefs: []
  type: TYPE_NORMAL
- en: SARSA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SARSA algorithm implements an on-policy time-difference method, in which
    the update of the action–value function is performed based on the outcome of the
    transition from state *s* to state *t* through action *a*, based on a selected
    policy, ![](img/984bfd99-18ed-4698-8e76-f3edd9202840.png)(*s*, *a*).
  prefs: []
  type: TYPE_NORMAL
- en: Q learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q learning is one of the most-used RL algorithms. This is because of its ability
    to compare the expected utility of the available actions without requiring an
    environment model. Thanks to this technique, it is possible to find an optimal
    action for every given state in a finished MDP.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **deep Q learning** identifies an RL method of approximation of function.
    It therefore represents an evolution of the basic Q learning method since the
    state–action table is replaced by a neural network, with the aim of approximating
    the optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the previous approaches, where it was used to structure the network
    in order to request both input and action and provide its expected return, deep
    Q learning revolutionizes the structure in order to request only the state of
    the environment and supply as many status–action values as there are actions that
    can be performed in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: RL therefore represents an advanced technology that solves different real-life
    problems. Now that the basics of this technology have been introduced in detail,
    it is time to explore the programming platform that we will be using for the rest
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing R for RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R represents an interpreted scripting language, where the interpreted term tells
    us that an application will be executed without it needing to be compiled beforehand.
    R adopts the object-oriented programming paradigm, through which it will be possible
    to create modern and flexible applications; in the R environment, everything represents
    an object that can be reused for specific needs. R is also an environment that
    was originally developed for statistical calculation and producing quality graphs.
    It consists of a language and a runtime environment with a graphical interface,
    a debugger, and access to some system functions, and offers the ability to execute
    programs stored in script files.
  prefs: []
  type: TYPE_NORMAL
- en: Its predisposition toward statistics does not derive from the nature of the
    language, but from the availability of large collections of statistical functions
    and from the interests of the researchers who invented it and developed it over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: The core of R is an interpreted programming language that allows the use of
    common structures for the control of information flow, and the use of modular
    programming using functions. Most of the functions visible to the user in the
    R environment are written in R itself. R is also an open source program, and its
    popularity reflects a change in the type of software used within the company.
    In this regard, we should remember that open source software is free from any
    constraint not only in its use, but even more importantly, in its development.
    The strong point of R is its flexibility in analyzing and representing data. The
    language is specialized in this branch, and it has innumerable functions to make
    life easier for the statistician or data scientist on duty.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can summarize the features of R as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity in data management and manipulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The availability of a suite of tools for calculating vectors, matrices, and
    other complex operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a vast set of integrated tools for statistical analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The production of numerous particularly flexible graphic potentials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The possibility of using a real object-oriented programming language that allows
    the use of conditional and cyclical structures, as well as user-created functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What makes R so useful and helps to explain its rapid acceptance by the user?
    The reason lies in the fact that statisticians, engineers, and scientists who
    over time have used the software to improve code or write variants for specific
    tasks have developed a large collection of scripts that have been grouped together
    in the form of packages. Packages written in R can add advanced algorithms, colored
    graphics with textures, and data mining techniques to analyze the information
    contained in a database in more detail. Now let's see how to install the R environment
    on our machine so we can replicate the examples in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Installing R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let's see where we can get the software to install R on our machine to
    start programming with it. The packages we need to install are available on the
    official website of the language at [https://www.r-project.org/](https://www.r-project.org/).
  prefs: []
  type: TYPE_NORMAL
- en: The **Comprehensive R Archive Network** (CRAN) is a network of servers located
    all over the world (updated in real time) that memorize identical versions of
    the source code and documentation relating to R. CRAN is directly accessible from
    the R site, and on this site, it is also possible to find information on R, some
    technical manuals, the R magazine, and details on the packages that have been
    developed for R and are stored on CRAN repositories.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see the official website of the R Project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/460460d9-b5a2-420e-9421-9825b6bf6c31.png)'
  prefs: []
  type: TYPE_IMG
- en: Naturally, before downloading the software versions, we will have to learn about
    the types of machine available to us and the operating system installed on them;
    however, bear in mind that R is available in practice for all operating systems
    in circulation. Programming in the R environment is facilitated by the availability
    of numerous packages. Let's see what they are.
  prefs: []
  type: TYPE_NORMAL
- en: R packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The packages use the functionalities of R to decompose a complex algorithm into
    simple units that perform a specific task and facilitate data sharing. In programming,
    we often use repetitive pieces of code, either because the same operation must
    be performed on data deriving from different sources or, more simply, because
    two different programs perform similar procedures. In such cases, it is patently
    unproductive to rewrite the same code unit each time to perform similar operations.
  prefs: []
  type: TYPE_NORMAL
- en: In this regard, R, like all high-level programming languages, allows the realization
    of subprograms that represent parts of programs written in separate files, or
    in the same file, that act as independent units with the possibility of being
    recalled by the main program. Starting from the first version of the R scripting
    language, it is possible to exploit the packages that represent a modern and extremely
    effective way of exchanging information between different program units and implementing
    additional functions that enhance the programming environment. The R environment
    is formed of a collection of functions aggregated in the packages—that is, in
    groups of functions that are generally specialized to achieve certain goals and
    objectives. A package is, therefore, a related set of functions, help files, and
    data files that have been grouped into a single file. The packages available in
    R are like the Perl modules that are used with the libraries that are present
    in C or C ++ and the Java classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The R distribution installed on our machine already has a series of packages
    installed by default. These packages are joined by many others that need to be
    activated when the need arises. Furthermore, there are a very large number of
    highly specialized packages (contributed packages) that provide useful functions
    for performing types of calculations and analysis. These packages must first be
    installed. In general, the installation is carried out automatically: R connects
    via the internet to a repository (a package archive), allows you to choose the
    package you want, downloads the required program, and installs it. The installed
    program must then be activated by loading it in the work area. R offers good tools
    for installing packages within the GUI, but does not provide an equally effective
    way to find a specific package. Fortunately, it is quite easy to find a package
    on the net, thanks to the use of a simple web browser. For example, we will be
    able to search our package on the CRAN website at [https://cran.r-project.org/web/packages/](https://cran.r-project.org/web/packages/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, the CRAN package repository features 14,345 available packages.
    The following screenshot shows the web page of the CRAN repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b20d7cd-321d-4f3c-a54f-c429e0bcf8fe.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, it will be enough to identify the packages needed to implement
    an algorithm based on RL—let's start with the `MDPtoolbox` package.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the MDPtoolbox package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In RL, it is usually assumed that the environment can be described by an MDP.
    This topic will be discussed further in [Chapter 3](7ee860fd-cd4c-4034-8dd6-9c803e129418.xhtml),
    *Markov Decision Processes in Action*. For now, we will discuss the `MDPtoolbox`
    package, which is a specific package in R that was created to address MDP-based
    problems. This package proposes functions related to the resolution of discrete-time
    Markov decision processes—such as finite horizon, value iteration, policy iteration,
    and linear programming algorithms (with some variants)—and also proposes some
    functions related to RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table gives some information about this package:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Package | `MDPtoolbox` |'
  prefs: []
  type: TYPE_TB
- en: '| Date | 2017-03-02 |'
  prefs: []
  type: TYPE_TB
- en: '| Version | 4.0.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Title | Markov Decision Processes Toolbox |'
  prefs: []
  type: TYPE_TB
- en: '| Authors | Iadine Chades, Guillaume Chapron, Marie-Josee Cros, Frederick Garcia,
    Regis Sabbadin |'
  prefs: []
  type: TYPE_TB
- en: 'The following list shows the most useful functions contained in this package,
    with a short description from the official documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mdp_Q_learning`: Solves discounted MDP using the Q learning algorithm (RL)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mdp_computePR`: Computes a reward matrix for any form of transition and reward
    function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mdp_eval_policy_iterative`: Evaluates a policy using an iterative method'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mdp_LP`: Solves discounted MDP using a linear-programming algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mdp_policy_iteration`: Solves discounted MDP using a policy-iteration algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mdp_relative_value_iteration`: Solves MDP with an average reward using a relative-value-iteration
    algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mdp_value_iteration`: Solves discounted MDP using a value-iteration algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `MDPtoolbox` solves a Markov decision process by finding the optimal policy
    after setting an optimization criterion. Based on this criterion, policies are
    identified that will provide the highest number of accumulated rewards. The package
    uses four of the most commonly used optimization criteria. RL is particularly
    useful in different applications in real life: let''s see some of these.'
  prefs: []
  type: TYPE_NORMAL
- en: RL applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From mobile phones to driverless cars, the consumer society has begun to consider
    the power of RL. In recent years, in fact, RL has emerged as a fundamental technology
    in various fields: from voice, textual, and facial recognition to multilingual
    translation, from traffic control systems to internet traffic control systems.
    But the latest examples of the application of this technology in the real world
    involve medical diagnostics, internet security, and the construction of forecasting
    models designed to make important business decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, RL teaches robots and machines to do what humans do naturally: interact
    with the environment and learn from experience. New, low-cost hardware has shaped
    the use of deep and multilayer neural networks that simulate the neural networks
    of the human brain. Production technologies have thus acquired new, extraordinary
    abilities to recognize images and trends, to make predictions, and to make intelligent
    decisions. Starting from a basic logic developed during the initial training,
    the algorithms based on RL can continually refine the performance through the
    feedback provided by the agent that monitors the state of the environment. In
    the following sections, we will examine some examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Software fault prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern software systems are constantly growing in complexity, and this increase
    in complexity leads to an increase in software failures that play a significant
    role in system failures. The aim we have is to achieve the highest possible probability
    that the system will not fail for a certain period, known as the mission time.
    Treating software failures in general is a complex operation. One of the main
    problems is the reproducibility of a failure, which is the ability to identify
    the fault-activation pattern. Testing activities have proven to be inadequate
    at dealing with this type of failure. Since it is practically impossible to identify
    all possible failures, critical systems adopt fault tolerance mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: A fault tolerance mechanism consists of a set of procedures that allow the system
    to continue operating even in the presence of faults. One of the main techniques
    for validating fault tolerance mechanisms is software fault injection, or the
    introduction of software failures (bugs) within a system component to analyze
    the impact on the other components and in the overall system. This technique is
    of great importance as software failures are a significant cause of system failure.
    Modern approaches to the problem involve the use of methodologies based on RL
    for failure injection into complex software systems. This system architecture
    allows us to perform an analysis of the algorithms in a simple and effective way
    that is easily integrated into a complex system, and that is easily extensible.
    Moreover, it allows us to carry out an exploratory analysis of the algorithms
    in order to evaluate the applicability of the approach in significant cases with
    a view to future integration.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive traffic flow control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Road traffic should ideally be studied in the same way as the methods used to
    study fluid dynamics. In the same way, vehicular traffic has a natural inertia
    that leads levels to settle over time, in the same way that fluid creates a situation
    of unstable imbalance where it accumulates because of its nature. However, the
    onset of any bottleneck hinders the flow, and a traffic jam is always around the
    corner. In the adaptive adjustment of a signalized intersection, the objective
    is to adjust an isolated intersection to optimize its capacity and minimize vehicle
    delays. There are numerous approaches to adaptive control that generally consider
    the available detection capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The most widespread are based on flow rate and density measurements at the different
    entrances to the intersection, at distances varying from the intersection. Other
    more recent approaches make use of measures derived from cameras, such as tail
    lengths and turning maneuvers. The data that can be useful is therefore basically
    the basic variables flow rate and density (or employment rate), the queue lengths,
    and the turning maneuvers. In these systems, technologies based on machine learning
    have been repeatedly used to solve the problem. RL is particularly suitable given
    its ability to interact with the environment through measurements detected in
    real time by sensors placed near signalized intersections.
  prefs: []
  type: TYPE_NORMAL
- en: Display advertising
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Display advertising uses the commercial spaces on a page of content to promote
    a product or service. This advertising technique differs from a pay per click
    model because it also uses a graphic element. The company acquires the space of
    one or more pages belonging to a **circuit** of sites, and in the spaces acquired,
    it can show the user its own advertisement. Furthermore, the search engine does
    not show ads randomly in the spaces purchased by the advertiser, but only shows
    ads relevant to the searches made by the user and in line with the history of
    the pages that they have viewed. To place an ad automatically and optimally, it
    is essential that advertisers develop a learning algorithm to make an intelligent
    offer of a real-time ad impression.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the algorithms used so far address the problem with an approach based
    on the static optimization of the treatment of the value of each impression. Optimization
    occurs either independently or by setting an offer price for each ad volume segment;
    however, offers for an advertising campaign are repeated several times during
    its duration before the budget expires. Bid decision-making can be treated as
    an RL problem in which the status space is represented by auction information
    and real-time campaign parameters, while an action is the bid price to be set.
  prefs: []
  type: TYPE_NORMAL
- en: Robot autonomy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Self-governance implies the capacity to work in unique and unstructured conditions
    without the requirement for consistent human involvement. In such conditions,
    a considerable number of circumstances are not known from previous experience,
    obliging the robot (or any self-governing framework) to have the option to identify
    the notable qualities of the present circumstance and to carry on as needed, choosing
    what moves to make.
  prefs: []
  type: TYPE_NORMAL
- en: The need to stay away from human mediation over extensive stretches of time
    suggests that the full capacity of the robot to self-oversee and get by (for instance,
    by evading obstacles to remain physically free or avoid totally draining its energy
    supply) must be a priority. For the most part, these machines perform the tasks
    given within work cells that facilitate such operations, where access to any foreign
    element, including humans, is forbidden. Robots of this type can be created by
    estimating numerous points of view related to the workplace, and most of the time,
    large programmed control calculations are used to give the robot the ability to
    work by exploiting the data collected from the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computer vision is a set of processes that aim to create an approximate model
    of the real world (3D) starting from two-dimensional (2D) images. The main purpose
    of machine vision is to reproduce human vision. Seeing is understood not only
    as the acquisition of a two-dimensional photograph of an area, but above all as
    the interpretation of the content of that area. Information, in this case, is
    understood as something that implies an automatic decision.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning has become a standard in solving various activities related
    to computer vision, such as feature detection, image segmentation, object recognition,
    and tracking. In modern control systems, robots are equipped with visual sensors
    that they can use to learn the status of a surrounding environment by solving
    corresponding computer vision tasks. These systems are used to make decisions
    about possible future actions. RL is used both to solve computer vision problems—such
    as object detection, visual detection, and the recognition of actions—and robot
    navigation.
  prefs: []
  type: TYPE_NORMAL
- en: Games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Games are a privileged research field for **artificial intelligence** (**AI**),
    as they provide convenient models of real problems. In fact, games, while presenting
    problems of complexity comparable to problems in the real world, have well-defined
    and formalizable rules. Furthermore, for each game, there are experts who are
    able to judge the quality of the results elaborated by a machine. It is therefore
    convenient to work and experiment first in a well-defined environment, such as
    the game world, to then generalize and adapt the results obtained to a more variable
    environment, such as the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Among the various types of existing games, those that have had the most success
    at the scientific research level are perfect information games, which are deterministic
    with two players. This category includes chess, checkers, and Go. Researchers'
    attention has therefore mainly focused on the game of chess. In past years, thanks
    to the commitment and dedication of several scientists, it has been possible to
    create an AI player capable of playing at world-champion level.
  prefs: []
  type: TYPE_NORMAL
- en: The most important success in this project was achieved when Deep Blue defeated
    the chess world champion Garry Kasparov. Another example concerns AI that learned
    how to play Go, a game of Chinese origin that is similar to chess. In early 2016,
    a historic game of Go was played between South Korean Lee Sedol and Google's artificial
    intelligence AlphaGo.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting in financial markets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent times, the analysis of financial markets has undergone an important
    development, both in terms of basic research and in terms of direct applications
    on the market. In particular, the computerization of data has made detailed information
    on price trends and volumes traded easily available, thereby creating new fields
    of investigation. At the same time, the introduction of electronic trading systems
    has meant that large financial institutions are interested in automating trading
    processes through algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: These models, however, cannot be applied blindly. Although there are general
    rules, it is the researcher's task to determine the characteristic parameters
    that best fit the description of the problem presented. In this context, RL fits
    in naturally, which, given its ability to interact with the environment, can verify
    in real time the market reaction to the forecasts that have been made. Based on
    the feedback received, it can correct a forecast by diverting the data in the
    right direction.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored the fantastic world of machine learning and
    have analyzed the three available paradigms according to the nature of the signal
    used for learning and the type of feedback adopted by the system. We took a tour
    of the most popular RL algorithms to choose the right one for our needs and to
    understand what is best suited to our needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we introduced the R scripting language and the features that make it
    particularly suitable for dealing with RL problems. We then explored the `MDPtoolbox`
    package—this package proposes functions related to the resolution of discrete-time
    Markov decision processes: finite horizon, value iteration, policy iteration,
    and linear programming algorithms. Finally, we analyzed a series of RL applications,
    the most modern ones that are spreading in the real world, showing surprising
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about the agent–environment interface. We
    will learn how to work with Markov's decision-making process. We will also learn
    about the gradient methods of norms and we will look at the most widely used RL
    package, R.
  prefs: []
  type: TYPE_NORMAL
