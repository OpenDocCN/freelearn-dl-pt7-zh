<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building a Voice Chatbot with Amazon Lex</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we will build a chatbot that allows the user to search for information and perform actions using voice or text conversations. This chatbot offers a more intuitive interface for humans to interact with computers. We will use Amazon Lex to build a custom AI capability to understand requests in natural language, to ask for missing inputs, and to fulfill tasks. We will provide guidance on the Amazon Lex <span>development paradigm, including its conventions and norms.</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Building conversational interfaces with Amazon Lex</li>
<li>Implementing task fulfillment logics with AWS Lambda</li>
<li>Adding a RESTful API in front of Amazon Lex custom AI capability</li>
<li>Discussing design concerns for conversational interfaces</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the friendly human-computer interface</h1>
                </header>
            
            <article>
                
<p>The intelligent personal assistant, sometimes called the <strong>chatbot</strong>, is rapidly appearing in more and more products with which we interact. The most prominent of these products are smart speakers, such as Amazon Echo and Google Home. Interacting with machines <span>using your voice </span>used to be the stuff of science fiction. Nowadays, fun facts and jokes are just an <em>Alexa</em> or <em>Hey Google</em> away. The tasks we can ask these intelligent assistants to perform include media control, information search, home automation, and administrative tasks, such as emails, to-dos, and reminders.</p>
<p>The capability of the intelligent personal assistant can be integrated into many more types of devices and platforms than just smart speakers. These include mobile operating systems such as Android and iOS, instant messaging apps such as Facebook Messenger, and company websites such as restaurants (to take orders) and banks (to check account balances). There are two main methods of interaction: through text or voice. This <span>intelligent assistant</span> capability is a combination of several AI technologies. For both interaction methods, <strong>Natural Language Processing</strong> (<strong>NLP</strong>) is needed to interpret and match the text to supported questions or commands. For voice interaction, speech-to-text and text-to-speech are needed to enable voice communication, which is something we have had hands-on experience with in Amazon Transcribe and Amazon Polly, respectively.</p>
<p>It may appear that these intelligent assistants are performing tasks such as getting answers to questions, placing orders, and automating our homes. But behind the scenes, these tasks are almost always fulfilled by traditional APIs and services. What we actually get with the intelligent assistant capability is a more flexible human-computer interface. Leveraging this new capability is not simply slapping a fancy voice interface on top of an existing application. When designing intelligent assistants, it's important to understand the use cases and operating environments where the intelligent assistant can provide a better user experience. Not all applications should have such interfaces; for example, use cases requiring precise inputs or dense outputs, noisy operating environments, and workflows that are long and complex.</p>
<p>In this chapter, we will be implementing an intelligent assistant, called Contact Assistant, for searching contact information. The contact assistant will work with the same contact data store we created for the contact organizer project in <a href="cffd245d-bee7-40bc-a64f-e108c039a8ec.xhtml">Chapter 5</a>, <em>Extracting Information from Text with Amazon Comprehend</em>. We will also add a RESTful API in front of the contact assistant, giving us multiple applications to leverage its capabilities. The way this intelligent assistant is designed makes it most useful out in the field, for example, when a traveling salesman is driving to a client and needs to get the client's contact information verbally. Instead of performing the searches through a web application running in a browser, this use case is better suited to be a mobile application with a driver-friendly user interface. This mobile application and its user interface are beyond the scope of this book, but may turn out to be interesting hands-on projects for some of you.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Contact assistant architecture</h1>
                </header>
            
            <article>
                
<p>The architecture for the contact assistant project includes the following:</p>
<ul>
<li>An orchestration layer </li>
<li>A service implementation layer</li>
</ul>
<p>The following architecture does not include the user interface layer, since we will not be implementing the mobile or web application that connects to the contact assistant. Instead, we will be focusing our efforts on developing a custom AI capability, an intelligent assistant bot, using the Amazon Lex platform. Let's have a look at a screenshot of the following architecture:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/591fce9b-cdb7-4636-b130-7bd852ac3ff9.png" style=""/></div>
<p>The contact assistant architecture includes the following:</p>
<ul>
<li>In the orchestration layer, we will build a <strong>Contact Assistant Endpoint</strong> that provides a RESTful interface to access our contact assistant's capabilities.</li>
<li>In the service implementation layer, we will build a service, called the <strong>intelligent assistant service</strong>, that shields implementation details of our custom AI capability, including its Amazon Lex implementation details. This way, when we want to reimpleme<span>nt the contact assistant bot with a different chatbot technology, only the intelligent assistant service needs to be modified.</span></li>
</ul>
<div class="packt_infobox">In previous chapters, we built our own services, such as Recognition Service and Speech Service, that connect to AWS AI capabilities such as Rekognition and Polly, respectively. Just like these services are shielding implementation details of the AWS AI services, the intelligent assistant service is shielding implementation details of our custom AI capability built on top of Amazon Lex.</div>
<ul>
<li>The contact assistant bot will be able to perform two tasks, <kbd>LookupPhoneNumberByName</kbd> and <kbd>MakePhoneCallByName</kbd>. This bot leverages Amazon Lex's underlying AI capabilities to interpret the user's verbal commands, and then performs the tasks using AWS Lambda functions. These Lambda functions implement the fulfillment of the tasks, looking up phone numbers and making phone calls.</li>
<li>
<p>Contact Assistant will be looking up contact information stored in the same DynamoDB table that we used in <a href="https://cdp.packtpub.com/hands_on_artificial_intelligence_on_amazon_web_services/wp-admin/post.php?post=527&amp;action=edit#post_301">Chapter 5</a><span>, </span><em>Extracting Information from Text with Amazon Comprehend</em>, in the contact organizer application. I<span>n the spirit of reuse, we will be reusing the contact store implementation that connects to the DynamoDB table. More specifically, the Lambda functions will delegate the contact searches to the contact store. The fact that the contact information is stored in a DynamoDB table is transparent to the contact assistant.</span></p>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the Amazon Lex development paradigm</h1>
                </header>
            
            <article>
                
<p>Amazon Lex is a development platform for building intelligent assistants or chatbots. With Amazon Lex, we are building our own custom intelligent assistant capabilities. Lex itself provides many AI capabilities, including <strong>Automatic Speech Recognition</strong> (<strong>ASR</strong>) and <strong>Natural Language Understanding</strong> (<strong>NLU</strong>), that are useful for building conversational interfaces. However, developers must follow Lex's development constructs, conventions, and norms to leverage these underlying AI capabilities.</p>
<p>These Amazon Lex conversational interfaces are built from Lex's specific building blocks:</p>
<ul>
<li><strong>Bot</strong>: A Lex bot can perform a set of related tasks through the custom conversational interfaces. A bot organizes the related tasks into a unit for development, deployment, and execution.
<ul>
<li>For example, to make the tasks available to the applications, they are deployed or published as a bot and the application must specify the bot name in order to access the available tasks.</li>
</ul>
</li>
<li><strong>Intent</strong>: An intent represents an automated task the users want to perform. An intent belongs to a specific AWS account rather than a specific bot and can be used by different bots in the same AWS account. This design decision makes them more reusable.</li>
<li><strong>Sample utterance</strong>: An utterance is a typed or spoken phrase in natural language that the user might say to invoke an automated task. Amazon Lex encourages developers to provide multiple utterances to make the conversational interface more flexible for the users.
<ul>
<li>For example, a user might <span>either </span><span>say <em>What's the weather like today?</em>, or <em>Tell me about the weather today?</em> to check the weather report. Amazon Lex uses advanced NLU to understand the intent of the user.</span></li>
<li><span>Given the previous two sample utterances, Amazon Lex also uses the NLU capability to handle variations of the utterances. </span>Lex can understand <em>Tell me what the weather's like today?</em> even if the exact phrase is not provided.</li>
</ul>
</li>
<li><strong>Slot</strong>: An automated task may require zero or more slots (parameters) to complete. For example, the date and the location are parameters used to fetch the weather report the user is interested in. In the conversational interface, Lex asks the user to provide all of the required slots.
<ul>
<li>For example, the location can be defaulted to the user's home address if not specified.</li>
</ul>
</li>
<li><strong>Slot type</strong>: Each slot has a type. Similar to a parameter type in programming languages, a slot type restricts input space and simplifies verification to make the conversational interface more user friendly. In verbal communication in particular, knowing the types of slots can help the AI technologies more accurately determine the typed or spoken texts.
<ul>
<li>There are numerous built-in slot types, such as Number, City, Airport, Language, and Musician, to name but a few. Developers can also create custom slot types specific to their applications.</li>
</ul>
</li>
<li><strong>Prompt and response</strong>: A prompt is a question in which Lex asks the users to either provide input to a slot, or to confirm the <span>input </span>provided. A response is a message to inform the user about the <span>result</span> of the task, such as the weather report.
<ul>
<li>The design of prompts and responses for conversational interfaces should take into account the use case, communication modality (text or speech), and the operating environment. The design should get user confirmation while not overburdening users with unnecessary communication.</li>
</ul>
</li>
<li><strong>Session attributes</strong>: Amazon Lex provides mechanisms to keep contextual data that can be shared across intents in the same session.
<ul>
<li>For example, if a user just asked for the weather report for a city and then follows up with a question, <em>How's the traffic there?</em>, the session context should be able to infer that <em>there</em> means the city reference in the previous intent. This type of contextual information can be stored in Lex's session attributes for developers to build smarter bots.</li>
</ul>
</li>
</ul>
<p>The Amazon Lex platform focuses on building conversational interfaces; fulfillment of the automated tasks is delegated to AWS Lambda. There are two built-in hook types for developers to integrate lambda functions:</p>
<ul>
<li><strong>Lambda initialization and validation</strong>: This hook allows developers to write AWS Lambda functions to validate the user inputs. For example, the lambda function can verify a user's inputs from a data source and with more complex business logic.</li>
<li><strong>The fulfillment lambda function</strong>: This hook allows developers to write AWS Lambda code that performs the task. With this lambda hook, developers can tap into AWS services, API endpoints, and much more to write the business logic for tasks such as checking the weather, ordering a pizza, and sending messages.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the contact assistant bot</h1>
                </header>
            
            <article>
                
<p>Now that we understand the Amazon Lex's development paradigm and terminology, let's put them to use by building a bot with both the conversational interface and the fulfillment business logic. We will be building the contact assistant using the AWS Console. Observe the following steps:</p>
<ol>
<li>Navigate to the <span class="packt_screen">Amazon Lex</span> page and click on the <span class="packt_screen">Create</span> button.</li>
<li><span>On the </span><span class="packt_screen">Create your bot</span><span> page, s</span>elect <span class="packt_screen">Custom bot</span> to create our own bot instead of building from a sample bot.</li>
<li>For the <span class="packt_screen">B</span><span class="packt_screen">ot name</span> field, enter <kbd>ContactAssistant</kbd>.</li>
<li>For the <span class="packt_screen">Output voice</span>, select <span class="packt_screen">Joanna</span>. Currently, Lex only supports US English.</li>
<li>For the <span class="packt_screen">Session timeout</span>, enter <span class="packt_screen">5 min</span>. This is the maximum idle time before the contact assistant closes a session.</li>
<li>For the <span class="packt_screen">IAM role</span>, leave it as the default <span class="packt_screen">AWSServiceRoleForLexBots</span>.</li>
<li>Select <span class="packt_screen">No</span> for COPPA; the contact assistant is designed for a traveling salesman, not children.</li>
</ol>
<ol start="8">
<li>Click on the <span class="packt_screen">Create</span> button.</li>
</ol>
<p style="padding-left: 60px">The <span class="packt_screen">Create your bot</span> page should have the following settings after the preceding steps have been carried out:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/21e9c9e0-a17f-45da-997a-94c55c901931.png" style=""/></div>
<ol start="9">
<li>Once the contact assistant has been created, you will be taken to the development console for Lex, similar to the one shown in the following screenshot: </li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1741 image-border" src="assets/acd2c4db-1656-478e-8eed-b95f5befab22.png" style=""/></div>
<p>Let's familiarize ourselves with the Lex development console:</p>
<ol>
<li>The bot's name can be found in the top-left corner, <span class="packt_screen">Contact_Assistant</span>.</li>
<li>There are a pair of disabled buttons for <span class="packt_screen">Build</span> and <span class="packt_screen">Publish</span> in the top-right corner.</li>
<li>Below the bot name and buttons are the tabs for <span class="packt_screen">Editor</span>, <span class="packt_screen">Settings</span>, <span class="packt_screen">Channels</span>, and <span class="packt_screen">Monitoring</span> screens. We will perform most of our bot development in the <span class="packt_screen">Editor</span> tab.</li>
<li>With the <span class="packt_screen">Editor</span> tab selected, we see that no <span class="packt_screen">Intents</span> or <span class="packt_screen">Slot types</span> have been added to the contact assistant.</li>
<li>In the top-right corner of the screen, there is a <span class="packt_screen">Test bot</span> sidebar that can be expanded (shown in the diagram) to reveal a chat interface. This chat interface is used to issue verbal commands to the bot under development. The chat interface is currently disabled. The bot needs to be built, with at least one intent created.</li>
<li>Finally, Click on the <span class="packt_screen">Create Intent</span> button to build our first intent.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The LookupPhoneNumberByName intent</h1>
                </header>
            
            <article>
                
<p>Our first intent allows the user to look up a contact's phone number by stating the contact's first and last names. This intent is essentially a search feature built on top of the contact store, but with a conversational interface.</p>
<div class="packt_tip"><span>We recommend designing each intent to focus on a narrow use case, and building up multiple intents to expand the bot's use cases.</span></div>
<p><span>The <kbd>LookupPhoneNumberByName</kbd> intent has very focused inputs and outputs, but we can build many related intents, such as <kbd>LookupAddressByName</kbd> and <kbd>LookupContactNamesByState</kbd>. </span>Even though we can consider the <kbd>LookupPhoneNumberByName</kbd> intent as a search feature to a data source, it requires a different design thinking.</p>
<p>Let's highlight a few design differences when comparing this intent to a more conventional search feature on a web application:</p>
<ul>
<li>In a web interface, we would provide the user with several search parameters, such as name, organization, and location. In a conversational interface, we would want to limit the number of search parameters, or inputs, for each intent. In a voice chatbot in particular, prompting and confirming all of the inputs might be cumbersome.</li>
<li>In a web interface, we would return many pieces of information about the contact and display them on the screen. In a conversational interface, we need to consider the modality. If this is a text chatbot, we might be able to get away with displaying multiple pieces of information. But, if this is a voice chatbot, then reading a long list of information to the user might create a cognitive burden.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sample utterances and slots for LookupPhoneNumberByName</h1>
                </header>
            
            <article>
                
<p>When designing a new intent, all stakeholders, not just developers, must carefully think through the conversational flow between the user and the bot. Let's start with the sample utterances.</p>
<div class="packt_infobox">The intelligent assistant is a likely replacement for existing communication channels to users, such as phone calls to customer representatives, inquiry emails for product issues, and text chats with technical agents. It's common practice to use recordings of user conversations from these existing channels to design the conversational flow of the intelligent assistant. These recordings provide the most accurate reflection of your users' interactions with the products; they are a good starting point for designing the utterances and the prompts.</div>
<p>Sample utterances are phrases that invoke the intent to perform an automated task. Here are a few sample utterances for our <kbd>LookupPhoneNumberByName</kbd> intent:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/083c2cef-cd8d-4b05-b86c-e29d1b7ff0de.png" style=""/></div>
<p>As we can see in the preceding screenshot, two of the sample utterances naturally included slots or input parameters, <span class="packt_screen">{FirstName}</span> and <span class="packt_screen">{LastName}</span>, in the conversation flow. This way users can provide some or all of the inputs needed to fulfill the task when it is invoked.</p>
<p>For <kbd>LookupPhoneNumberByName</kbd>, we need both the <span class="packt_screen">{FirstName}</span> and <span class="packt_screen">{LastName}</span> to look up a phone number as they are both required. Let's have a look at the following screenshot of slots:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/76967ad7-b5f4-4041-b538-14cd4945d233.png" style=""/></div>
<p>As shown in the preceding screenshot, for the slot types, there are built-in <span class="packt_screen">AMAZON.US_FIRST_NAME</span> and <span class="packt_screen">AMAZON.US_LAST_NAME</span> types. As we noted previously, specifying the most relevant and specific types for the inputs makes natural language understanding and value validation much easier for the underlying AI technologies.</p>
<p>What if the user did not provide the inputs to the slots? For example, what if the user spoke the first sample utterance, <em>I would like to look up a phone number</em>. Each slot must have one or more prompts to ask the user for the input value if it was not provided in the invoking utterance. For <kbd>{FirstName}</kbd> and <kbd>{LastName}</kbd>, we used <kbd>What's the contact's first name?</kbd> and <kbd>What's the {FirstName}'s last name?</kbd>, respectively. Notice that the prompt for <kbd>{LastName}</kbd> included the slot value for <kbd>{FirstName}</kbd>. This can make the conversation flow more natural and human-like.</p>
<p>To add more than one prompt for a slot, click on the gear icon to edit the slot's settings. Here, you can add additional prompts, set the <span class="packt_screen">Maximum number of retries</span> to elicit this input, and corresponding utterances, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a022eb63-88f9-415f-8a07-9f18b0f1c149.png" style=""/></div>
<p><span><span>The bot will select from this list of prompts to ask the user for the slot. The bot will attempt the prompts up to two times before giving up.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confirmation prompt and response for LookupPhoneNumberByName</h1>
                </header>
            
            <article>
                
<p>To complete the conversational flow design, let's move on to the confirmation prompt and response. Both of these are optional, but they can greatly improve the behavior and user experience of the intelligent assistant.</p>
<p class="mce-root">The following is a screenshot of a confirmation prompt. A confirmation prompt is an opportunity to inform the user about the action about to be taken. At this point, values for all of the required slots and potentially optional slots have been elicited:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/490f4b0f-bc09-4245-ae2c-316db9a0005a.png" style=""/></div>
<p>We are able to use <kbd>{FirstName}</kbd> and <kbd>{LastName}</kbd> in the confirmation message. It's a good design to read or display back the values for the <span><kbd>{FirstName}</kbd> and <kbd>{LastName}</kbd> </span>slots; this confirms with the user that the bot understood the inputs correctly. Natural language conversations can be ambiguous at times. Let's take a look at this example conversation:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/34de4f55-b6d6-4fbc-9193-a053f19ec030.png" style=""/></div>
<p>Do you spot the issue? One of our sample utterances is <span class="packt_screen">What's {FirstName} {LastName} phone number</span>. However, the user invoked the intent without providing a <kbd>{LastName}</kbd>. Our bot interpreted <em>what's</em> as the <kbd>{FirstName}</kbd>, and <kbd>John</kbd> as the <kbd>{LastName}</kbd>. By reading back the input values with the confirmation prompt, the user can notice and correct the input error before the action is taken.</p>
<p>We will skip fulfillment of the task for now and move on to the response. In the following screenshot, the response for the <kbd>LookupPhoneNumberByName</kbd> intent closes out the task by displaying or reading the phone number for the contact:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4109580b-2e25-4014-abb8-7b17302d4131.png" style=""/></div>
<div class="packt_infobox"><span><kbd>[Phone]</kbd> is a session attribute that is holding the phone number for the contact. It will be set in the fulfillment lambda function. We will cover how that's implemented later in the chapter.</span></div>
<p>This intent is used to query for information. Providing the information in the response will feel natural to users. There are also intents that will perform a task without the need to provide information back to the users. In such cases, it is still a good idea to respond to the users of the outcome of the task.</p>
<p>Now, we have completed the conversational interface for our first intent. Next, we will implement the AWS Lambda function that will perform the task asked of our intelligent assistant.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fulfillment for LookupPhoneNumberByName using AWS Lambda</h1>
                </header>
            
            <article>
                
<p>To perform any fulfillment action with the intelligent assistant, developers need to invoke AWS Lambda functions. The <em>Fulfillment</em> section provides a hook to an existing lambda function. Let's implement a lambda function called <strong>LookupPhoneNumberByName</strong> to search for the phone number of a contact by his or her first and last names.</p>
<p>In contrast to previous projects, where we used AWS Chalice to develop and deploy the lambda code and AWS permissions, we will be using the AWS Lambda console page to create the <kbd>LookupPhoneNumberByName</kbd> function. Here are the steps:</p>
<ol start="1">
<li>Navigate to the AWS Lambda service from AWS Console, and then click on the <span class="packt_screen">Create function</span> button.</li>
<li><span>Select <span class="packt_screen">Author from scratch</span>. We will implement the lambda function without any blueprints or sample applications.</span></li>
<li>Name the function <kbd>LookupPhoneNumberByName</kbd>.</li>
<li>Select the <span class="packt_screen">Python 3.7</span> runtime to match the language version for our other hands-on projects.</li>
<li>Choose <span class="packt_screen">Create a new role with basic Lambda permissions to create a role</span>. We will need to add additional policies later to connect to additional AWS services.</li>
<li>Click the <span class="packt_screen">Create function</span> button.</li>
</ol>
<p>The settings on the <span class="packt_screen">Create function</span> page should look similar to the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ef9490a5-90a4-40a7-ba82-b3e1870a3135.png" style=""/></div>
<p class="mce-root"/>
<p>After the lambda function and its execution role have been created, you will see a development console similar to this one: </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1e628f19-1f6a-4d78-bb33-78311efbafed.png" style=""/></div>
<p>The preceding screenshot demonstrates the following:</p>
<ul>
<li>In the <span class="packt_screen">Designer</span> section, we can add triggers to invoke this lambda function. For Lex bots, we do not need to select a trigger.</li>
<li>We also see that the <kbd>LookupPhoneNumberByName</kbd> function has access to CloudWatch logs. Any outputs or error messages from the execution of this function will be written to CloudWatch, and we can view those logs from the <span class="packt_screen">CloudWatch</span> console page. This will be useful while developing and debugging the function.</li>
<li>In the <span class="packt_screen">Function code</span> section, we can choose <span class="packt_screen">Edit code inline</span>, modify the function runtime, and change the <span class="packt_screen">Handler</span> function name. The handler function specifies the Python file and function name that constitute the entry point to our lambda function.</li>
<li>Beneath the three lambda configuration fields, we have the inline code editor. Here, we can create additional source files and edit the code for each source file.</li>
</ul>
<p>Our lambda function will need to interact with the same DynamoDB that stores the contact information from the contact organizer application. We can leverage the existing contact store and then add a new function to query for contact information with the help of the following steps:</p>
<ol>
<li>Right-click within the left panel of the inline editor and then select <span class="packt_screen">New File.</span></li>
<li>Name the file <kbd>contact_store.py</kbd>.</li>
<li>Replace the content of <kbd>contact_store.py</kbd> with the contact store implementation from <a href="https://cdp.packtpub.com/hands_on_artificial_intelligence_on_amazon_web_services/wp-admin/post.php?post=527&amp;action=edit#post_301">Chapter 5</a><span>, </span><em>Extracting Information from Text with Amazon Comprehend</em>.</li>
<li>Add <kbd>get_contact_by_name()</kbd> after the existing function implementation:</li>
</ol>
<pre style="padding-left: 60px">import boto3<br/><br/>class ContactStore:<br/>    def __init__(self, store_location):<br/>        self.table = boto3.resource('dynamodb').Table(store_location)<br/><br/>    ...<br/><br/>    def get_contact_by_name(self, name):<br/>        response = self.table.get_item(<br/>            Key = {'name': name}<br/>        )<br/><br/>        if 'Item' in response:<br/>            contact_info = response['Item']<br/>        else:<br/>            contact_info = {}<br/><br/>        return contact_info<span><br/></span></pre>
<p>The preceding code includes the following elements:</p>
<ul>
<li>The <kbd>get_contact_by_name()</kbd> method retrieves a single contact by its unique identifier, which is the name. In this method, we are calling DynamoDB's <kbd>get_item()</kbd> function. The response from <kbd>get_item()</kbd> contains a dictionary. If the item key exists, then we get a return value containing the contact information<span>.</span></li>
<li>Here, we are getting an item from the DynamoDB table by key. The key is the name of the contact, first name, and last name, separated by a space. This code will be executed in the Python 3.7 lambda runtime environment. In this environment, the <kbd>boto3</kbd> package is already installed.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DynamoDB IAM role for LookupPhoneNumberByName</h1>
                </header>
            
            <article>
                
<p>Since this code will need to connect to DynamoDB, we need to add a policy to our lambda function's execution role:</p>
<ol>
<li>Navigate to the IAM page from AWS Console.</li>
<li>Click on <span class="packt_screen">Roles</span> on the left-hand panel.</li>
<li>From the list of roles, find and click on the <span class="packt_screen">LookupPhoneNumberByName-role-&lt;unique id&gt;</span> role for our lambda function.</li>
<li>Click on the <span class="packt_screen">Attach policies</span> button.</li>
<li>Find and select the <span class="packt_screen">AmazonDynamoDBFullAccess</span> policy, and then click on the <span class="packt_screen">Attach policy</span> button.</li>
</ol>
<p>Now, let's have a look at the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/17d6d4d5-449a-4a18-a3b6-eaebb7749df6.png" style=""/></div>
<p>Now, our <kbd>LookupPhoneNumberByName</kbd> lambda function can access DynamoDB. The <kbd>AmazonDynamoDBFullAccess</kbd> policy is fine for our hands-on projects, but, for real production application, you should fine-tune the policy to limit the number of permissions granted.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fulfillment lambda function for LookupPhoneNumberByName</h1>
                </header>
            
            <article>
                
<p>In the lambda editor window, open the existing <kbd>lambda_function.py</kbd> file and replace its content with the following implementation:</p>
<pre>import contact_store<br/><br/>store_location = 'Contacts'<br/>contact_store = contact_store.ContactStore(store_location)<br/><br/>def lex_lambda_handler(event, context):<br/>    intent_name = event['currentIntent']['name']<br/>    parameters = event['currentIntent']['slots']<br/>    attributes = event['sessionAttributes'] if event['sessionAttributes'] is not None else {}<br/><br/>    response = lookup_phone(intent_name, parameters, attributes)<br/><br/>    return response<br/><br/>def lookup_phone(intent_name, parameters, attributes):<br/>    first_name = parameters['FirstName']<br/>    last_name = parameters['LastName']<br/><br/>    # get phone number from dynamodb<br/>    name = (first_name + ' ' + last_name).title()<br/>    contact_info = contact_store.get_contact_by_name(name)<br/><br/>    if 'phone' in contact_info:<br/>        attributes['Phone'] = contact_info['phone']<br/>        attributes['FirstName'] = first_name<br/>        attributes['LastName'] = last_name<br/>        response = intent_success(intent_name, parameters, attributes)<br/>    else:<br/>        response = intent_failure(intent_name, parameters, attributes, 'Could not find contact information.')<br/><br/>    return response<br/><br/># Amazon lex helper functions<br/>...</pre>
<p>In the preceding code, the following takes place:</p>
<ul>
<li>We first initialize the contact store with the DynamoDB table contacts.</li>
<li>In the <kbd>lambda_handler()</kbd> function, we are extracting the intent name, the slots, and the attributes from the event object passed in. The event object is passed in to our Amazon Lex bot when the fulfillment hook is triggered. All of the slot input values, as well as the session attributes, will be included in this event object.</li>
<li><kbd>lambda_handler()</kbd> then calls the <kbd>lookup_phone()</kbd> function that uses the contact store to retrieve the contact information.</li>
<li>In the <kbd>lookup_phone()</kbd> function, we are constructing the item key from the <kbd>FirstName</kbd> and <kbd>LastName</kbd> slot values. The item key must be <kbd>FirstName</kbd> and <kbd>LastName</kbd> separated by a space, with the correct capitalization.
<ul>
<li>For example, the first name <kbd>john</kbd> and the last name <kbd>smith</kbd> will result in the item key <kbd>John Smith</kbd>; the first letter of each part of the name is capitalized.</li>
<li>We are using the <kbd>title()</kbd> function to ensure correct capitalization, irrespective of how the user inputs the names.</li>
</ul>
</li>
</ul>
<p>If we are able to retrieve a contact with those names, we will save the contact's phone number, first name, and last name in the session attributes. This is how the phone number is passed back to be displayed or spoken in this intent's response. We will cover why the first name and last name are saved in the session attributes in a later section of this chapter.</p>
<p>If we are successful at fulfilling the lookup, we respond with <kbd>intent_success()</kbd>, otherwise, we respond with <kbd>intent_failure()</kbd> with an explanation message. These are helper functions that encapsulate some of Amazon Lex's specific response formats.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Amazon Lex helper functions</h1>
                </header>
            
            <article>
                
<p>The Amazon Lex helper functions format the responses to what Lex is expecting. We have four helper functions here:</p>
<ul>
<li><kbd>intent_success()</kbd> indicates that the intent has been successfully fulfilled, and any session attributes are passed back to Lex as <kbd>sessionAttributes</kbd>.</li>
<li><kbd>intent_failure()</kbd> indicates that the intent was not fulfilled successfully. This response also includes an explanation message.</li>
<li><kbd>intent_elicitation()</kbd> asks the Lex bot to elicit a value for the specified parameter name. This elicitation might be due to missing slot values or invalid slot values. This helper function is useful when we create custom <kbd>Lambda initialization and validation</kbd> logic.</li>
<li><kbd>intent_delegation()</kbd> indicates that the lambda function has completed its obligation and directs Lex to choose the next course of action based on the bot's configuration.</li>
</ul>
<p>We only used the first two helper functions for this <kbd>LookupPhoneNumberByName</kbd> intent. Here is the code implementation:</p>
<pre># Amazon lex helper functions<br/>def intent_success(intent_name, parameters, attributes):<br/>    return {<br/>        'sessionAttributes': attributes,<br/>        'dialogAction': {<br/>            'type': 'Close',<br/>            'fulfillmentState': 'Fulfilled'<br/>        }<br/>    }<br/><br/>def intent_failure(intent_name, parameters, attributes, message):<br/>    return {<br/>        'dialogAction': {<br/>            'type': 'Close',<br/>            'fulfillmentState': 'Failed',<br/>            'message': {<br/>                'contentType': 'PlainText',<br/>                'content': message<br/>            }<br/>        }<br/>    }<br/><br/>def intent_delegation(intent_name, parameters, attributes):<br/>    return {<br/>        'sessionAttributes': attributes,<br/>        'dialogAction': {<br/>            'type': 'Delegate',<br/>            'slots': parameters,<br/><br/>        }<br/>    }<br/><br/>def intent_elicitation(intent_name, parameters, attributes, parameter_name):<br/>    return {<br/>        'sessionAttributes': attributes,<br/>        'dialogAction': {<br/>            'type': 'ElicitSlot',<br/>            'intentName': intent_name,<br/>            'slots': parameters,<br/>            'slotToElicit': parameter_name<br/>        }<br/>    }</pre>
<div class="packt_infobox">Even though the <kbd>lambda_function.py</kbd> file is relatively short, we still applied a few clean code practices. We organized all of the AWS Lambda- and Amazon Lex-specific implementation details into the <kbd>lambda_handler()</kbd> function and the Amazon Lex helper functions.<br/>
<br/>
For example, how are the slots from the Lambda event object and the response format to Amazon Lex <span>to be retrieved</span><span>?</span> This way, the <kbd>lookup_phone()</kbd> function is free from those platform specific details and, hence, is more likely to be reusable on other platforms. The <kbd>lookup_phone()</kbd> function only requires <kbd>intent_name</kbd> to be a string, and the parameters and attributes to be dictionaries.</div>
<p><span>Save the lambda function implementation by clicking on the <span class="packt_screen">Save</span> button in the top-right corner of the lambda development console.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The intent fulfillment for LookupPhoneNumberByName</h1>
                </header>
            
            <article>
                
<p><span>Now, let's add this lambda function to the fulfillment hook:</span></p>
<ol start="1">
<li> Go to the Amazon Lex development console and, under <span class="packt_screen">Fulfillment</span>, select <span class="packt_screen">LookupPhoneNumberByName</span> from the <span class="packt_screen">Lambda function</span> list, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4001a0dc-1c8a-42e0-80cd-056493ab3b12.png" style=""/></div>
<ol start="2">
<li>As shown in the following screenshot, Amazon Lex will ask for permission to invoke this lambda function. Click <span class="packt_screen">OK</span> to grant permission:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/79ab4701-6483-4a7d-9e22-b18056a1db37.png" style=""/></div>
<ol start="3">
<li>In the Lex development console, click on the <span class="packt_screen">Save Intent</span> button at the bottom of the page, and then click on the <span class="packt_screen">Build</span> button in the top-right corner of the page. It will take a few seconds for our first Lex bot to build.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test conversations for LookupPhoneNumberByName</h1>
                </header>
            
            <article>
                
<p><span>Now, we are ready to build and test our first intent. </span>In the <span class="packt_screen">Test bot</span> panel on the right of the page, issue a few variations of the sample utterances and follow the conversation with the contact assistant. Here is a sample conversation:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8512b6bf-4b0b-4d9a-98a8-8a061da526e4.png" style=""/></div>
<p>In the preceding conversation, the following happened:</p>
<ul>
<li>The utterance did not include the slots, and our contact assistant prompted for the first and last names</li>
<li>The assistant confirmed the lookup for <span class="packt_screen">John Smith</span> before proceeding with fulfillment</li>
<li>The response included the contact's first name and the phone number</li>
</ul>
<p>Now, think through how this conversation plays out, both as a text chat and as a voice conversation.</p>
<p>Here is another sample conversation:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/aecd439d-51e2-4a4f-a1a1-61271779447c.png" style=""/></div>
<p><span>In the preceding conversation, the following happened</span>:</p>
<ul>
<li>The utterance included both of the required slots</li>
<li>This time, our contact assistant only had to confirm the lookup before proceeding with the fulfillment and response</li>
<li>The user can also respond <strong>no</strong> to the confirmation prompt to cancel the fulfillment</li>
</ul>
<p>Congratulations! You just completed your first intelligent assistant with a conversation interface and AWS Lambda fulfillment implementation.</p>
<div class="packt_infobox">The test bot panel's chat interface also supports voice inputs. You can use the microphone icon to issue the utterances and responses via voice. <br/>
<img src="assets/7da95da6-551a-4e10-a08d-30da7c48b114.png"/><br/>
In the test bot chat interface, the response from Lex will always be in text.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The MakePhoneCallByName intent</h1>
                </header>
            
            <article>
                
<p>Next, we will create a second intent for our contact assistant, called <strong>MakePhoneCallByName</strong>. The task performed by this intent should be obvious from its name; it can phone the contacts. However, we will not be implementing the phone call functionality in this project.</p>
<p>The goal as regards implementing this second intent is to demonstrate how multiple intents of an intelligent assistant can interact and collaborate. We want to design the conversational interface of <kbd>MakePhoneCallByName</kbd> to be able to function independently, but also to be able to function in conjunction with the <kbd>LookupPhoneNumberByName</kbd> intent.</p>
<p>To make this intent collaboration concrete, imagine that the user just looked up the phone number of a contact and then decided to make a call to this contact. Should the second intent start over with the first name and last name slot elicitations? Or would it be more fluid and natural to know that the user wants to call the same contact that the assistant just looked up? Of course, the latter. After <kbd>LookupPhoneNumberByName</kbd> was fulfilled successfully, and then the user utters <kbd>Call him</kbd> or <kbd>Call her</kbd>, <kbd>MakePhoneCallByName</kbd> should just know who <kbd>him</kbd> or <kbd>her</kbd> is referring to based on the context of prior conversations. This is where session attributes can help to maintain the context.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sample utterances and lambda initialization/validation for MakePhoneCallByName</h1>
                </header>
            
            <article>
                
<p><span>We'll start by adding a new intent from the Lex development console by clicking on the blue plus button next to <span class="packt_screen">Intents</span> on the left-hand pan</span><span>el, as shown in the following screenshot:</span></p>
<ol>
<li><span>Select <span class="packt_screen">Create intent</span>, name it <kbd>MakePhoneCallByName</kbd>, and then click on <span class="packt_screen">Add</span>.</span></li>
</ol>
<ol start="2">
<li>Let's create a few sample utterances for this intent. The first utterance <strong>Call</strong> <span class="packt_screen">{FirstName} {LastName}</span> provides the values for the two required slots. For the other utterances, the intent should try its best to get the slot values from the conversation context if possible:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/53a0464f-1811-4fb1-9092-0ed84692cb74.png" style=""/></div>
<p>To do this, we will use the second type of AWS Lambda hook from Amazon Lex, lambda initialization and validation. The following steps will create the hook:</p>
<ol>
<li>In the <span class="packt_screen">Lambda initialization and validation</span> section, check the box for <span class="packt_screen">Initialization and validation code hook.</span></li>
<li>Go to the AWS Lambda page from AWS Console and create a lambda function named <kbd>InitContact</kbd> from scratch for Python 3.7.</li>
<li>Create a new default lambda execution role. We do not need the <span class="packt_screen">AmazonDynamoDBFullAccess</span> policy for this lambda function.</li>
<li>In the inline function code editor, replace the <kbd>lambda_function.py</kbd> file contact with the following implementation:</li>
</ol>
<pre style="padding-left: 60px">def lex_lambda_handler(event, context):<br/>    intent_name = event['currentIntent']['name']<br/>    parameters = event['currentIntent']['slots']<br/>    attributes = event['sessionAttributes'] if event['sessionAttributes'] is not None else {}<br/><br/>    response = init_contact(intent_name, parameters, attributes)<br/><br/>    return response<br/><br/>def init_contact(intent_name, parameters, attributes):<br/>    first_name = parameters.get('FirstName')<br/>    last_name = parameters.get('LastName')<br/><br/>    prev_first_name = attributes.get('FirstName')<br/>    prev_last_name = attributes.get('LastName')<br/><br/>    if first_name is None and prev_first_name is not None:<br/>        parameters['FirstName'] = prev_first_name<br/><br/>    if last_name is None and prev_last_name is not None:<br/>        parameters['LastName'] = prev_last_name<br/><br/>    if parameters['FirstName'] is not None and parameters['LastName'] is not None:<br/>        response = intent_delegation(intent_name, parameters, attributes)<br/>    elif parameters['FirstName'] is None:<br/>        response = intent_elicitation(intent_name, parameters, attributes, 'FirstName')<br/>    elif parameters['LastName'] is None:<br/>        response = intent_elicitation(intent_name, parameters, attributes, 'LastName')<br/><br/>    return response<br/><br/># lex response helper functions<br/>...</pre>
<p>In the preceding code<span>, the following takes place</span>:</p>
<ul>
<li>In the <kbd>init_contact()</kbd> function, we check whether <kbd>FirstName</kbd> and <kbd>LastName</kbd> are missing from the slots coming from the utterance. If so, we then check whether <kbd>FistName</kbd> and <kbd>LastName</kbd> exist in the session attributes.
<ul>
<li>Do you recall that we saved <kbd><span>FirstName</span></kbd> and <kbd>LastName</kbd> to the session attributes in the fulfillment implementation for the <kbd>LookupPhoneNumberByName</kbd> intent? We are retrieving those saved values here.</li>
</ul>
</li>
<li>If both <kbd>FirstName</kbd> and <kbd>LastName</kbd> are set, then we respond back to Lex with a delegation response.
<ul>
<li>The delegation response tells Lex that initialization and validation are complete, and that the bot should continue with its execution based on its configuration, including fulfillment.</li>
</ul>
</li>
<li>If either <kbd>FirstName</kbd> or <kbd>LastName</kbd> is still missing its value, then we respond back with an elicitation response.
<ul>
<li>The elicitation response will trigger the prompt for the missing slot that was configured for the bot.</li>
</ul>
</li>
</ul>
<p>Save the lambda function, and then go back to the Amazon Lex development console:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/add41d5e-43f0-4d83-8926-fb1400099d1b.png" style=""/></div>
<p><span>Select</span> <span class="packt_screen">InitContact</span><span> for the <span class="packt_screen">Lambda initialization and validation</span> function.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Slots and confirmation prompt for MakePhoneCallByName</h1>
                </header>
            
            <article>
                
<p>The slots configuration for the <kbd>MakePhoneCallByName</kbd> intent can be exactly the same as the configuration for <kbd>LookupPhoneNumberByName</kbd>. See the details in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1c949a72-d569-4606-97c4-41def12adabd.png" style=""/></div>
<p>Both slots are required, and are set to <span>the built-in <kbd>AMAZON.US_FIRST_NAME</kbd> and <kbd>AMAZON.US_LAST_NAME</kbd> types.</span></p>
<p>The <span class="packt_screen">Confirmation prompt</span> can be tailored for making phone calls, as shown <span>in the following screenshot</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bcb3bbfd-52df-4527-aa68-ae348b7d5e3c.png" style=""/></div>
<p>Both the <span class="packt_screen">Confirm</span> and <span class="packt_screen">Cancel</span> messages are tailored to the <kbd>MakePhoneCallByName</kbd> intent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fulfillment and response for MakePhoneCallByName</h1>
                </header>
            
            <article>
                
<p>We could implement a new lambda function to fulfill the contact lookup and phone call functionalities. But, since we are not actually making phone calls in this project, the business logic of the fulfillment lambda function will be the same as the contact lookup function we already implemented.</p>
<p>In fact, for this project, <span class="packt_screen">Fulfillment</span> can be handled by the <span class="packt_screen">LookupPhoneNumberByName</span> lambda function, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/924d3c1f-1518-4ffb-bd79-da9ed740fe6b.png" style=""/></div>
<p>Finally, the <span class="packt_screen">Response</span> configuration can also be tailored to make phone calls, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8ab7aef8-01b9-4d5a-8eab-fe0c94983ecb.png" style=""/></div>
<p>Now, click on the <span class="packt_screen">Save Intent</span> button on the bottom of the Lex development console, and then click on the <span class="packt_screen">Build</span> button in the top-right corner of the development console.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test conversations for MakePhoneCallByName</h1>
                </header>
            
            <article>
                
<p>In the <span class="packt_screen">Test bot</span> panel on the right-hand side of the page, issue a few variations of the sample utterances and follow the conversation with the contact assistant. Here is a sample conversation:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1a69bccd-6e5f-4bb3-ac49-4ccdaf7e5d92.png" style=""/></div>
<p>The preceding conversation demonstrates that the <kbd>MakePhoneCallByName</kbd> intent can function independently without running the <span><kbd>LookupPhoneNumberByName</kbd> intent first</span>.</p>
<p>Here is another sample conversation:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8516bcf7-f1e5-4af9-88be-eed12ee104ad.png" style=""/></div>
<p>The <span>preceding</span> conversation shows the power of context:</p>
<ul>
<li>The user first asked for John Smith's phone number with the <kbd>LookupPhoneNumberByName</kbd> intent.</li>
<li>Then, the user requested to <kbd>call him</kbd>.</li>
<li>At this point, our <kbd>InitContact</kbd> lambda function grabbed the <kbd>FirstName</kbd> and <kbd>LastName</kbd> from the session attributes and confirmed whether John Smith is the contact to call.
<ul>
<li>The confirmation prompt is important here, since the contact assistant is inferring the contact. We do not want to automatically make awkward calls to the wrong contact; it is better to confirm with the user first before taking action.</li>
</ul>
</li>
</ul>
<p>Click on <span class="packt_screen">Clear chat history</span> before issuing the next utterance. This will clear the session and its stored attributes. Continue the <span>sample conversation with the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/33f380a8-2482-48fe-941e-f7c2e588521e.png" style=""/></div>
<p>In this conversation, the following took place:</p>
<ul>
<li>The user started with an utterance without any slots. On this occasion, however, there was no previous conversational context saved in the session attributes.</li>
<li>The <span><kbd>InitContact</kbd> lambda function was not able to retrieve a first and last name; therefore, it responds with </span>intent elicitations.</li>
<li> It's important to test our intelligent assistant to handle all possible orders and combinations of intents and utterances. This quality assurance becomes more difficult as more intents share the session attributes.</li>
</ul>
<p>Congratulations! Our contact assistant just became more intelligent with context awareness.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying the contact assistant bot</h1>
                </header>
            
            <article>
                
<p>We can now publish the contact assistant as a custom intelligent assistant capability.</p>
<p>Click on the <span class="packt_screen">Publish</span> button on the top right of the Lex development console and set <span class="packt_screen">Alias</span> to <span class="packt_screen">Production</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5ef89a4d-2cbb-4659-9f03-9468b4fff2c4.png" style=""/></div>
<p>The <span>preceding</span> screenshot shows that the c<span>ontact assistant </span>is now published. Once <span>the contact assistant is published, the applications can start to leverage it through various integration methods, including the boto3 SDK.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Integrating the contact assistant into applications</h1>
                </header>
            
            <article>
                
<p>Next, we will create the layers to integrate the contact assistant capability into applications. As mentioned at the beginning of this chapter, we will not implement any application; we will only implement the service and RESTful endpoint layers.</p>
<p>As with previous hands-on projects, we will be using Python, Pipenv, Chalice, and boto3 as part of the technology stack. Let's create a project structure first.</p>
<ol>
<li>In the terminal, we will create the <kbd>root</kbd> project directory and enter it with the following commands:</li>
</ol>
<pre class="p1" style="padding-left: 60px"><strong><span class="s1">$ mkdir </span>ContactAssistant<span class="s1"><br/></span><span class="s1">$ cd </span>ContactAssistant</strong></pre>
<ol start="2">
<li>We will create a Python 3 virtual environment with<span> </span><kbd>Pipenv</kbd><span> </span>in the project's <kbd>root</kbd> directory. Our Python portion of the project requires two packages,<span> </span><kbd>boto3</kbd><span> </span>and<span> </span><kbd>chalice</kbd>. We can install them with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ pipenv --three</strong><br/><strong>$ pipenv install boto3</strong><br/><strong>$ pipenv install chalice</strong></pre>
<ol start="3">
<li>Remember that the Python packages installed via<span> </span><kbd>pipenv</kbd><span> </span>are only available if we activate the virtual environment. One way to do this is by means of the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ pipenv shell</strong></pre>
<ol start="4">
<li>Next, while still in the virtual environment, we will create the orchestration layer as an AWS Chalice project named <kbd>Capabilities</kbd> with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ chalice new-project Capabilities</strong></pre>
<ol start="5">
<li>To create the <kbd>chalicelib</kbd> Python package, issue the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd Capabilities</strong><br/><strong>mkdir chalicelib</strong><br/><strong>touch chalicelib/__init__.py</strong><br/><strong>cd ..</strong></pre>
<p style="padding-left: 60px">The initial project structure should look like the following:</p>
<pre style="padding-left: 60px"><strong>Project Structure</strong><br/><strong>------------</strong><br/><strong>├── ContactAssistant/</strong><br/><strong>    ├── Capabilities/</strong><br/><strong>        ├── .chalice/</strong><br/><strong>            ├── config.json</strong><br/><strong>        ├── chalicelib/</strong><br/><strong>            ├── __init__.py</strong><br/><strong>        ├── app.py</strong><br/><strong>        ├── requirements.txt</strong><br/><strong>    ├── Pipfile</strong><br/><strong>    ├── Pipfile.lock</strong></pre>
<p><span>The project structure is slightly different to those structures created in previous chapters. This project structure contains the orchestration and service implementations layers, but does not include a web user interface.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Intelligent assistant service implementation</h1>
                </header>
            
            <article>
                
<p>The contact assistant is backed by a Lex bot in the current implementation, but good architecture design should have the flexibility to change implementations easily. The service implementation serves to shield the Lex implementation details from the client applications.</p>
<p>Create a Python file in <kbd>chalicelib</kbd> with the name <kbd>intelligent_assistant_service.py</kbd>, as shown <span>in the following screenshot:</span></p>
<pre>import boto3<br/><br/>class IntelligentAssistantService:<br/>    def __init__(self, assistant_name):<br/>        self.client = boto3.client('lex-runtime')<br/>        self.assistant_name = assistant_name<br/><br/>    def send_user_text(self, user_id, input_text):<br/>        response = self.client.post_text(<br/>            botName = self.assistant_name,<br/>            botAlias = 'Production',<br/>            userId = user_id,<br/>            inputText = input_text<br/>        )<br/><br/> return response['message']</pre>
<p>In the <span>preceding</span> code<span>, the following takes place</span>:</p>
<ul>
<li><kbd>IntelligentAssistantService</kbd> is a generic implementation that can be configured to work with different intelligent assistants, and not just the contact assistant.</li>
<li>The <kbd>__init__()</kbd> constructor takes in the assistant name to configure itself for a particular intelligent assistant at creation time. The constructor creates a <kbd>boto3</kbd> client for <kbd>lex-runtime</kbd>, which can communicate with published Lex bots.</li>
<li><kbd>IntelligentAssistantService</kbd> implements the <kbd>send_user_text()</kbd> method to send text chat messages to the assistants. This method takes in a <kbd>user_id</kbd> and the <kbd>input_text</kbd> from the application, and uses the <kbd>post_text()</kbd> function from <kbd>lex-runtime</kbd> to send the input text.
<ul>
<li><kbd>user_id</kbd> is an ID created by the client application. A Lex bot can have multiple conversations with different users at once. This <kbd>user_id</kbd> identifies a user; in other words, it identifies a chat session.</li>
</ul>
</li>
</ul>
<div class="packt_infobox">There is also a <kbd>post_content()</kbd> function from <kbd>lex-runtime</kbd> for sending both text and speech inputs. In addition to <kbd>botName</kbd>, <kbd>botAlias</kbd>, and <kbd>userId</kbd>, the <kbd>post_content()</kbd> function also requires the <kbd>contentType </kbd>and <kbd>inputStream</kbd> parameters to be set. <kbd>contentType</kbd> can be either audio or text, with a few supported audio formats. <kbd>inputStream</kbd> contains the byte stream for the audio or text contents.<br/>
If the application would like to receive an audio response from the Lex bot, the <kbd>accept</kbd> parameter should be set to one of the audio output formats <span>supported</span>. The supported audio input and output formats are implementation details of Lex. Any format conversions for the audio inputs and outputs should be performed in this service implementation to hide those details from the client applications.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Contact assistant RESTful endpoint</h1>
                </header>
            
            <article>
                
<p>Let's have a look at the following steps:</p>
<ol>
<li>Now, let's build a quick RESTful endpoint to the contact assistant in <kbd>app.py</kbd>. This way, we can test our <kbd>IntelligentAssistantService</kbd> with <kbd>curl</kbd> commands:</li>
</ol>
<pre style="padding-left: 60px">from chalice import Chalice<br/>from chalicelib import intelligent_assistant_service<br/><br/>import json<br/><br/>#####<br/># chalice app configuration<br/>#####<br/>app = Chalice(app_name='Capabilities')<br/>app.debug = True<br/><br/>#####<br/># services initialization<br/>#####<br/>assistant_name = 'ContactAssistant'<br/>assistant_service = intelligent_assistant_service.IntelligentAssistantService(assistant_name)<br/><br/>#####<br/># RESTful endpoints<br/>#####<br/>@app.route('/contact-assistant/user-id/{user_id}/send-text', methods = ['POST'], cors = True)<br/>def send_user_text(user_id):<br/>    request_data = json.loads(app.current_request.raw_body)<br/><br/>    message = assistant_service.send_user_text(user_id, request_data['text'])<br/><br/>    return message</pre>
<p style="padding-left: 60px">The RESTful endpoint implementation is short and simple:</p>
<ul>
<li style="padding-left: 30px">The initialization code binds our generic <kbd>IntelligentAssistantService</kbd> implementation to the contact assistant</li>
<li style="padding-left: 30px">The RESTful endpoint itself takes in the <kbd>user_id</kbd> through the URL and the input text as JSON in the request body</li>
</ul>
<ol start="2">
<li>Start the <kbd>chalice local</kbd> environment with the following command in the terminal:</li>
</ol>
<pre style="padding-left: 60px">$ chalice local<br/>Restarting local dev server.<br/>Found credentials in shared credentials file: ~/.aws/credentials<br/>Serving on http://127.0.0.1:8000</pre>
<ol start="3">
<li>Now, we can have a conversation with our contact assistant using the <kbd>curl</kbd> commands:</li>
</ol>
<pre style="padding-left: 60px">$ curl --header "Content-Type: application/json" --request POST --data '{"text": "Call John Smith"}' http://127.0.0.1:8000/contact-assistant/user-id/me/send-text<br/>&gt; Would you like me to call John Smith?<br/><br/>$ curl --header "Content-Type: application/json" --request POST --data '{"text": "Yes"}' http://127.0.0.1:8000/contact-assistant/user-id/me/send-text<br/>&gt; Calling John Smith at (202) 123-4567</pre>
<p><span><span>In the preceding conversation, the following takes place:</span></span></p>
<ul>
<li><span><span>The first <kbd>curl</kbd> command issues the intent <kbd>Call John Smith</kbd>, which includes both slots required for the first and last names of the contact.</span></span></li>
<li><span><span>The response is a confirmation from the contact assistant, <em>Would you like me to call John Smith?</em></span></span></li>
<li>The second <kbd>curl</kbd> command continues the conversation by replying, <em>Yes</em>.</li>
<li>The contact assistant then responds with, <em>Calling John Smith at (202) 123-4567</em>.</li>
</ul>
<p>The applications that will leverage the capabilities of the contact assistant will provide the appropriate user interface to best facilitate the conversation, for example, a mobile app for a traveling salesman. The application will pass the verbal communication between the users and the contact assistant using the RESTful endpoint.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we built the contact assistant, a chatbot that allows the user to search for contact information using a voice or text conversational interface. We built the contact assistant's conversational interface using Amazon Lex. We learned the development paradigm of Amazon Lex to build a custom AI capability, including concepts such as intents, utterances, prompts, and confirmations. The contact assistant supports two intents, <kbd>LookupPhoneNumberByName</kbd> and <kbd>MakePhoneCallByName</kbd>. The task fulfillment of these intents is implemented using AWS Lambda. We also designed these two intents to be context aware by using Amazon Lex's session attributes; being context aware reduces the cognitive burden of the user and makes the chatbot smarter.</p>
<p>Amazon Lex is the last of the AWS AI services that we will cover in this book. In the next part of the book, we will cover AWS ML services to train customer AI capabilities using machine learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>For more information on building a voice chatbot with Amazon Lex, you can refer to the following links:</p>
<p><a href="https://restechtoday.com/smart-speaker-industry/">https://restechtoday.com/smart-speaker-industry/</a></p>
<p><a href="https://www.lifewire.com/amazon-alexa-voice-assistant-4152107">https://www.lifewire.com/amazon-alexa-voice-assistant-4152107</a></p>
<p><a href="https://www.nngroup.com/articles/intelligent-assistants-poor-usability-high-adoption/">https://www.nngroup.com/articles/intelligent-assistants-poor-usability-high-adoption/</a></p>


            </article>

            
        </section>
    </body></html>