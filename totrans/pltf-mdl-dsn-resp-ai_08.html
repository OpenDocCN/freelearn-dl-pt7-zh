<html><head></head><body>
		<div id="_idContainer182">
			<h1 id="_idParaDest-156" class="chapter-nu ber"><a id="_idTextAnchor176"/>8</h1>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor177"/>Fairness in Model Optimization</h1>
			<p><strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) has made massive strides in recent years, with applications in everything from finance to healthcare. However, these systems can often be opaque and biased against <a id="_idIndexMarker1007"/>certain groups of people. In order for ML to live up to its potential, we must ensure that it is fair <span class="No-Break">and unbiased.</span></p>
			<p>In <a href="B18681_07.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, we discussed the concept of fairness in the context of data generation. In this chapter, we will cover different optimization constraints and techniques that are essential to optimizing and obtaining fair ML models. The focus of this chapter is to enlighten you about new custom optimizers, unveiled by research that can serve to build fair supervised, unsupervised, and semi-supervised ML models. In a broader sense, you will learn the foundational steps to create and define model constraints that can be used by different optimizers during the training process. You will also gain an understanding of how to evaluate such constraint-based models with proper metrics, and the extra training overheads incurred when employing the optimization techniques, which will enable you to design your <span class="No-Break">own algorithms.</span></p>
			<p>In this chapter, the following topics will <span class="No-Break">be covered:</span></p>
			<ul>
				<li>The concept of fairness as applied <span class="No-Break">to ML</span></li>
				<li>Explicit and implicit mitigation of <span class="No-Break">fairness issues</span></li>
				<li>Fairness constraints for a <span class="No-Break">classification task</span></li>
				<li>Fairness constraints for a <span class="No-Break">regression task</span></li>
				<li>Fairness constraints for a <span class="No-Break">clustering task</span></li>
				<li>Fairness constraints in <span class="No-Break">reinforcement learning</span></li>
				<li>Fairness constraints for a <span class="No-Break">recommendation task</span></li>
				<li>Challenges <span class="No-Break">of fairness</span></li>
			</ul>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor178"/>Technical requirements</h1>
			<p>This chapter requires you to have Python 3.8 along with some <span class="No-Break">Python packages:</span></p>
			<ul>
				<li><span class="No-Break">TensorFlow 2.7.0</span></li>
				<li><span class="No-Break">NumPy</span></li>
				<li><span class="No-Break">Matplotlib</span></li>
			</ul>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor179"/>The notion of fairness in ML</h1>
			<p>Fairness is a subjective term. It has different meanings depending on culture, time, race, and so on. We could <a id="_idIndexMarker1008"/>write endlessly on the topic of fairness in society; <a id="_idIndexMarker1009"/>however, in this chapter, our focus is fairness with respect to ML. In <a href="B18681_07.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, you became familiar with the concept of fairness and how data plays an important role in the fairness of your ML model. In this chapter, we will expand on the concept <span class="No-Break">of fairness.</span></p>
			<p>As you might have already realized, fairness does not have a universal definition. For example, seen from the point of view of an organization, fairness can have three different dimensions (Cropanzano et al., 2001. See <span class="No-Break"><em class="italic">Further reading</em></span><span class="No-Break">):</span></p>
			<ul>
				<li><strong class="bold">Distributive fairness</strong>: The <a id="_idIndexMarker1010"/>algorithm should be fair in allocating <a id="_idIndexMarker1011"/>important resources, for example, <span class="No-Break">in hiring.</span></li>
				<li><strong class="bold">Interactional fairness</strong>: The algorithm should be able to explain the rationale behind it, and <a id="_idIndexMarker1012"/>the explanation provided <a id="_idIndexMarker1013"/>by the algorithm should be perceived as fair by the people concerned, for example, why <em class="italic">A</em> should get a promotion <span class="No-Break">over </span><span class="No-Break"><em class="italic">B</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Procedural fairness</strong>: The <a id="_idIndexMarker1014"/>algorithm should not <a id="_idIndexMarker1015"/>generate different results when used for different subgroups within an organization, for example, different recommendations for women than for men. If it does, it should be because of explainable societal or <span class="No-Break">biological reasons.</span></li>
			</ul>
			<p>These are demonstrated in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/Figure_8.01_B18681.jpg" alt="Figure 8.1 – Different dimensions of fairness in an organization"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Different dimensions of fairness in an organization</p>
			<p>Bias and fairness are two terms that are often used interchangeably in the ML community. However, we feel that they should be differentiated. Taking a leaf out of Booth’s book (<a href="https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897">https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897</a>), we define bias as a systematic <a id="_idIndexMarker1016"/>error that alters the model’s decision based on specific <a id="_idIndexMarker1017"/>input information (such as gender and geographical location). Often called measurement bias, this bias arises either because <a id="_idIndexMarker1018"/>unnecessary information is added to an input feature space (<strong class="bold">construct contamination</strong>) or because <a id="_idIndexMarker1019"/>the model is not <a id="_idIndexMarker1020"/>capturing all the aspects of what you are measuring (<strong class="bold">construct deficiency</strong>). Here, <strong class="bold">construct</strong> refers to features that cannot be directly measured, and instead <a id="_idIndexMarker1021"/>need to be inferred from measurements. <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.2</em> shows <a id="_idIndexMarker1022"/>different sources <span class="No-Break">of bias.</span></p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/Figure_8.02_B18681.jpg" alt="Figure 8.2 – Sources of measurement (adapted from Booth et al., 2021)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Sources of measurement (adapted from Booth et al., 2021)</p>
			<p>A biased model may not always be a bad thing – it is possible for a model to be biased but fair (<span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.3</em>). As illustrated in the paper <em class="italic">Algorithmic Fairness</em>, by Pessach et al., it is possible for a <a id="_idIndexMarker1023"/>model to consider both SAT scores and demographics for <a id="_idIndexMarker1024"/>the admission selection process and allocate seats unequally to students based on whether they come from a privileged background or an unprivileged background, favoring those from underprivileged backgrounds. While such a model is <em class="italic">biased</em>, it is <em class="italic">fair</em> because a student from an underprivileged background will have had to overcome many more challenges to achieve that score and probably has <span class="No-Break">higher potential.</span></p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/Figure_8.03_B18681.jpg" alt="Figure 8.3 – Biased resource allocation to candidates based on their demographics and so on can result in a fair algorithm by giving tools to the underrepresented classes of society"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Biased resource allocation to candidates based on their demographics and so on can result in a fair algorithm by giving tools to the underrepresented classes of society</p>
			<p>Now that we have <a id="_idIndexMarker1025"/>set the boundaries between fairness and bias, we will <a id="_idIndexMarker1026"/>proceed to look at unfairness and methods to <span class="No-Break">mitigate unfairness.</span></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor180"/>Unfairness mitigation methods</h1>
			<p>In recent years, researchers <a id="_idIndexMarker1027"/>have developed various techniques to mitigate unfairness in AI models. These methods can be applied at different stages of <span class="No-Break">model development:</span></p>
			<ul>
				<li><strong class="bold">Preprocessing</strong>: Preprocessing <a id="_idIndexMarker1028"/>methods work by adjusting the training data distribution so that the sensitive groups <span class="No-Break">are balanced.</span></li>
				<li><strong class="bold">Postprocessing</strong>: Postprocessing <a id="_idIndexMarker1029"/>methods work by calibrating the predictions after the model has <span class="No-Break">been trained.</span></li>
				<li><strong class="bold">In-processing</strong>: These methods <a id="_idIndexMarker1030"/>incorporate fairness directly into the <span class="No-Break">model design.</span></li>
			</ul>
			<p>In <a href="B18681_07.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, we discussed preprocessing methods and techniques. In this chapter, we will discuss in-processing unfairness mitigation methods first, and later discuss different fairness constraints as applied to different <span class="No-Break">ML tasks.</span></p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor181"/>In-processing methods</h2>
			<p>Using in-processing methods <a id="_idIndexMarker1031"/>offers <span class="No-Break">certain benefits:</span></p>
			<ul>
				<li>It has been observed that ML models often amplify the bias present in data. Using in-processing techniques for unfairness mitigation can help in addressing this problem. This is because the problem of amplification is caused by the algorithm, and since in-processing methods take fairness into consideration alongside model optimization it can help reduce or eliminate <span class="No-Break">the issue.</span></li>
				<li>Fine-tuning pre-trained models has become a popular approach to customize modeling for different tasks with limited training data. Although transfer learning from pre-trained models has alleviated the need for large amounts of training data, the issue of bias in pre-trained models has become more critical as the pre-trained models are usually trained on biased data. By using techniques such as contrastive learning, it is possible to use in-processing methods to mitigate unfairness in <span class="No-Break">pre-trained models.</span></li>
			</ul>
			<p>Developing efficient in-processing methods is still a challenge. Existing algorithms are not sufficient, and <a id="_idIndexMarker1032"/>we need to focus on developing new methods that can address fairness issues when sensitive attributes are not disclosed <span class="No-Break">or available.</span></p>
			<p>In-processing unfairness mitigation methods can be further subdivided into two: <strong class="bold">implicit methods</strong> and <strong class="bold">explicit methods</strong>. In explicit methods, unfairness mitigation is achieved by incorporating <a id="_idIndexMarker1033"/>the fairness metrics within the <a id="_idIndexMarker1034"/>objective function using either <a id="_idIndexMarker1035"/>fairness constraints or regularization parameters. Implicit methods, on the <a id="_idIndexMarker1036"/>other hand, rely on modifying learning strategies of the deep learning model – for example, by including adversarial examples while training. Explicit methods are easy to implement and are often flexible. Let us now explore the ways to mitigate unfairness with the help of <span class="No-Break">explicit methods.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">In <a href="B18681_07.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, we covered many fairness measurement metrics. In this chapter, we will call an algorithm fair if it is able to achieve good results for the fairness metrics it is <span class="No-Break">designed for.</span></p>
			<p>In the rest of the <a id="_idIndexMarker1037"/>chapter, we will discuss various techniques for unfairness mitigation. Specifically, we will tackle two specific <span class="No-Break">fairness issues:</span></p>
			<ul>
				<li><strong class="bold">Disparate treatment</strong>: Disparate treatment is when there is a difference in the ways individuals <a id="_idIndexMarker1038"/>are treated based on their race, gender, or other protected characteristics. For example, if an algorithm is trained on data that is mostly about males, it may predict a higher salary rise for a male candidate and a lower salary rise for a female candidate despite them having the same number of years of experience. Most often, disparate treatment arises from biased data, or using sensitive information while training <span class="No-Break">the model.</span></li>
			</ul>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/Figure_8.04_B18681.jpg" alt="Figure 8.4 – Example of disparate treatment by Google’s translate algorithm (adopted from Fitria, 2021)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Example of disparate treatment by Google’s translate algorithm (adopted from Fitria, 2021)</p>
			<ul>
				<li><strong class="bold">Disparate impact</strong>: Disparate impact is when the model treats certain groups differently, even when <a id="_idIndexMarker1039"/>the model is not explicitly trained on corresponding sensitive attributes. For example, if an algorithm is trained on data that is mostly male, it may learn to associate male names with high-status jobs and female names with low-status jobs. This can happen when unknowingly, the model creates some proxy attributes that correlate with <span class="No-Break">sensitive information.</span></li>
			</ul>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor182"/>Explicit unfairness mitigation</h1>
			<p>As discussed in the previous section, explicit unfairness mitigation is achieved by adding regularizers or by <a id="_idIndexMarker1040"/>including constraints within the loss function. In this section, we will expand on the explicit unfairness mitigation techniques and explore some of the recently proposed strategies <span class="No-Break">for this.</span></p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor183"/>Fairness constraints for a classification task</h2>
			<p>Let us first <a id="_idIndexMarker1041"/>build an <a id="_idIndexMarker1042"/>understanding of classification tasks in ML. Consider <em class="italic">Y</em>, <em class="italic">X</em>, and <em class="italic">S</em> to be random variables, corresponding to the class/label, non-sensitive input features, and sensitive input features, respectively. Our training dataset, D, will then consist of instances of these <span class="No-Break">random variables:</span></p>
			<p>D = { (y, x, <span class="No-Break">s</span><span class="No-Break">) }</span></p>
			<p>The aim of the classification task is to find a model, M, defined by parameters, Θ, such that it is able to correctly predict the conditional probability of a class when sensitive and non-sensitive <a id="_idIndexMarker1043"/>features are given, M[Y|X,S; Θ]. The model parameters are estimated by using the <strong class="bold">Maximum Likelihood </strong><span class="No-Break"><strong class="bold">Estimator</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MLE</strong></span><span class="No-Break">):</span></p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/Formula_08_006.jpg" alt=""/>
				</div>
			</div>
			<p>Let us see <a id="_idIndexMarker1044"/>how we can <a id="_idIndexMarker1045"/>modify this loss function to <span class="No-Break">mitigate unfairness.</span></p>
			<h3>Adding a prejudice removal regularizer</h3>
			<p>The paper <em class="italic">Fairness-Aware Classifier with Prejudice Remover Regularizer</em>, by Kamishima et al., introduced <a id="_idIndexMarker1046"/>a prejudice removal regularizer term to the loss function, as shown in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/Figure_8.05_B18681.jpg" alt="Figure 8.5 – Adding a prejudice removal regularizer term to the loss function"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Adding a prejudice removal regularizer term to the loss function</p>
			<p>To measure the model fairness, <strong class="bold">Calder’s and Verwer’s 2 Naïve Bayes</strong> (<strong class="bold">CV2NB</strong>) score is <a id="_idIndexMarker1047"/>used. In the <strong class="bold">NNabla</strong> Examples library, the <strong class="source-inline">util</strong> module contains the <strong class="source-inline">CVS</strong> function for <a id="_idIndexMarker1048"/>calculating the score. To have <a id="_idIndexMarker1049"/>a point of comparison, it is good to first build a <span class="No-Break">simple classifier:</span></p>
			<ol>
				<li>To begin, we create a function to create a simple four-layered neural network classifier. Between each layer, we have added dropout layers for regularization to <span class="No-Break">avoid overfitting:</span><pre class="console">
def Classifier(n_features, n_hidden=32, p_dropout=0.2,train=True):
    with nn.parameter_scope('classifier'):
        layer1 = PF.affine(n_features, n_hidden, name='layer1')
        layer1 = F.relu(layer1)
        if (train):
            layer1 = F.dropout(layer1,p_dropout)
        layer2 = PF.affine(layer1, n_hidden, name='layer2')
        layer2 = F.relu(layer2)
        if (train):
            layer2 = F.dropout(layer2,p_dropout)
        layer3 = PF.affine(layer2, n_hidden, name='layer3')
        layer3 = F.relu(layer3)
        if (train):
            layer3 = F.dropout(layer3,p_dropout)
        layer4 = PF.affine(layer3, 1, name='layer4')
    return layer4</pre></li>
			</ol>
			<p>The preceding code makes use of the NNabla (imported as <strong class="source-inline">nn</strong>) library to build the network.</p>
			<ol>
				<li value="2">Now, if we create a simple classifier using the previous function and train it on the <strong class="source-inline">Adult</strong> dataset (<a href="https://archive.ics.uci.edu/ml/datasets/Adult">https://archive.ics.uci.edu/ml/datasets/Adult</a>), the result shows <a id="_idIndexMarker1050"/>the accuracy and <span class="No-Break">CV2NB score:</span><pre class="console">
<strong class="bold">Classifier accuracy :  84.85797101449275 Classifier CV2NB Score : 0.1644</strong></pre></li>
				<li>But <a id="_idIndexMarker1051"/>now, if you include the prejudice removal regularizer term and retrain the same classifier with the modified loss function, you can see that the CV2NB <span class="No-Break">score improves:</span></li>
			</ol>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/Figure_8.06_B18681.jpg" alt="Figure 8.6 – Accuracy versus CV2NB score on adult dataset after adding prejudice removal regularizer term to the loss function"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Accuracy versus CV2NB score on adult dataset after adding prejudice removal regularizer term to the loss function</p>
			<p>In the preceding figure, we can see that as η (the fairness parameter) increases, the CV2NB score also improves. The larger value of the fairness parameter enforces fairness. However, we can see that the cost of increased fairness is accuracy; a balance must be <a id="_idIndexMarker1052"/>reached between the two. Also, you can see that a classifier with the prejudice removal regularizer had a CV2NB score of 0.16, which is higher compared to a classifier trained without using the prejudice removal regularizer (<span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.6</em>), thus proving that adding the regularizer creates a more <span class="No-Break">fair model.</span></p>
			<h3>Modifying the objective</h3>
			<p>A great deal <a id="_idIndexMarker1053"/>of work in this area has been done by M. B. Zafar. Those interested in his complete work can refer to <a id="_idIndexMarker1054"/>his GitHub repo: <a href="https://github.com/mbilalzafar/fair-classification">https://github.com/mbilalzafar/fair-classification</a>. We will consider two different classifiers: <strong class="bold">support vector machines</strong> and <strong class="bold">logistic regressors</strong>. The common thing between these <a id="_idIndexMarker1055"/>classifiers is that both are based on finding a decision boundary that separates the classes. In support vector machines, the decision boundary is found by maximizing the distance between minimum support vectors, and in logistic regressors, the decision boundary is found by minimizing the <span class="No-Break">log-loss function.</span></p>
			<p>For simplicity, we consider the case of a binary classifier. For a linear binary classifier, the decision boundary is defined <span class="No-Break">as follows:</span></p>
			<p>Θ<span class="superscript">T</span>X = 0</p>
			<p>Here, Θ refers to the coefficients of the hyperplane defining the <span class="No-Break">decision boundary.</span></p>
			<p>For this to work, fairness against disparate treatment is ensured by making sure that sensitive attributes (<em class="italic">S</em>) are not used in decision-making/prediction. Continuing with our approach, this <span class="No-Break">means that</span>
S ∩ X = ∅; that is, they <span class="No-Break">are disjointed.</span></p>
			<p>For disparate impact, the 80% rule (more generally called the p-rule) is used as the fairness metric. Translating the 80% rule<a id="_idIndexMarker1056"/> to the distance boundary, it means that if d<span class="subscript">θ</span>(<em class="italic">x</em>) is the signed (or oriented) distance of feature vector <em class="italic">x</em> from the decision boundary, then <a id="_idIndexMarker1057"/>the ratio of users with a specific sensitive feature having a positive signed distance and users without the sensitive feature having a positive signed distance is no less than 80:100. Mathematically, for a binary sensitive attribute, s ∈ {0,1}, we can express the p-rule as follows: </p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/Formula_08_013.jpg" alt=""/>
				</div>
			</div>
			<p>To mitigate the unfairness, Zafar et al. introduced the concept of decision boundary covariance. They defined it as the covariance between the user’s sensitive features and the signed distance of the user’s input features to the decision boundary. Mathematically, it would be <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/Formula_08_014.jpg" alt=""/>
				</div>
			</div>
			<p>Simplified, this expression is reduced to <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/Formula_08_015.jpg" alt=""/>
				</div>
			</div>
			<p>It is easy to include this in the classifier objective since this is a convex function. Let us put it into action. We created a synthetic dataset with two sensitive features and one non-sensitive feature. The following graph shows the generated <span class="No-Break">data points.</span></p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/Figure_8.07_B18681.jpg" alt="Figure 8.7 – Synthetic data generated with two sensitive features and one non-sensitive feature (showing only 200 sample points)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Synthetic data generated with two sensitive features and one non-sensitive feature (showing only 200 sample points)</p>
			<p>The details <a id="_idIndexMarker1058"/>of the data generated by us are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
Total data points: 4000
# non-protected examples: 1873
# protected examples: 2127
Non-protected in positive class: 1323 (71%)
Protected in positive class: 677 (32%)</pre>
			<p>If we do not use any fairness constraints and train a simple classifier, the accuracy that we get is 87%, but the classifier follows only a 41% p-rule, as opposed to the desired 80%. Now, there are two strategies we <span class="No-Break">can adopt:</span></p>
			<ul>
				<li><strong class="bold">Maximize accuracy under fairness constraints</strong>: In this case, the constraints are put so that the p-rule is satisfied while maximizing the accuracy of the classifier. This is achieved by <span class="No-Break">the following:</span></li>
			</ul>
			<p>Minimize L(D,Θ) subject to <img src="image/Formula_08_017.png" alt=""/> and <img src="image/Formula_08_018.png" alt=""/></p>
			<p>Here, <em class="italic">c</em> refers to the covariance threshold. It specifies an upper bound on the covariance <a id="_idIndexMarker1059"/>between sensitive features and the signed distance of the input features from the decision boundary. The covariance threshold provides a trade-off between accuracy and fairness. </p>
			<p>As <em class="italic">c</em> decreases (c → 0), the classifier becomes fairer at the cost of accuracy. The following is the result of the classifier trained using the fairness constraints.</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/Figure_8.08_B18681.jpg" alt="Figure 8.8 – Decision boundary of an unconstrained and constrained logistic classifier when trained on the synthetic data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Decision boundary of an unconstrained and constrained logistic classifier when trained on the synthetic data</p>
			<p>You can see that the accuracy of the classifier reduced slightly (71%), but the p-rule value is 104% – this means the classifier is fair. The protected versus non-protected ratio in the positive class is 53:51. The graph also shows the decision boundary for the original and constrained classifiers.</p>
			<ul>
				<li><strong class="bold">Maximize fairness under accuracy constraints</strong>: In this case, we minimize the <a id="_idIndexMarker1060"/>covariance decision boundary, subject to the constraint that the loss of the constrained classifier (L(D,Θ)) is less than the loss of the unconstrained classifier (L(D,Θ*)) – in other terms, <span class="No-Break">the following:</span></li>
			</ul>
			<p>Minimize <img src="image/Formula_08_022.png" alt=""/> subject to <img src="image/Formula_08_023.png" alt=""/><img src="image/Formula_08_024.png" alt=""/>.</p>
			<p>Here, L<span class="subscript">i</span> represents <a id="_idIndexMarker1061"/>loss associated with the <em class="italic">i</em>-th user in the training set. The following graph shows the results obtained using these constraints.</p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/Figure_8.09_B18681.jpg" alt="Figure 8.9 – Decision boundary of an unconstrained and constrained logistic classifier when trained on the synthetic data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – Decision boundary of an unconstrained and constrained logistic classifier when trained on the synthetic data</p>
			<p>We can see that now, while the p-rule has improved from 41% to 61%, the decrease in accuracy is not very significant. For businesses, this is important. You cannot launch a product that is fair but not accurate.</p>
			<p>The preceding <a id="_idIndexMarker1062"/>graphs were generated using the code provided by the Kamishima group with the <span class="No-Break">following parameters:</span></p>
			<pre class="source-code">
n_samples = 2000 # Number of data points
disc_factor = math.pi / 4.0 # represents <strong class="source-inline">f</strong> in the paper-- decrease it to generate more discrimination
gamma = 2000.0</pre>
			<p>This approach helps mitigate the disparate impact on classification tasks. The approach can be extended to many <span class="No-Break">classification tasks.</span></p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor184"/>Fairness constraints for a regression task</h2>
			<p>In regression, the task changes from finding a decision boundary to finding a hyperplane that contains <a id="_idIndexMarker1063"/>most of the <a id="_idIndexMarker1064"/>data points. A classifier is good when the decision space is small and discrete, but more often, decision spaces are continuous. For example, instead of telling whether a person will default on a loan or not (a binary world), it is more useful for decision-makers to know the probability of a person defaulting on a loan (a continuous value between 0 and 1). In these scenarios, regression is a better option. The major difference is that while in the case of classification the logit loss function is used, in the case of regression, the loss function is conventionally the <strong class="bold">Mean Square Error</strong> (<strong class="bold">MSE</strong>). This means <a id="_idIndexMarker1065"/>that we can add a regularizer term, just as we did with the classifier, to get a fair regression. Indeed, this was attempted by Berk et al., and they proposed a general framework for fairness in both regression and <span class="No-Break">classification tasks:</span></p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/Formula_08_026.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <em class="italic">L</em> is the conventional loss function (for the classifier, the conventional loss is logit loss, and for regression, we conventionally use the MSE loss). They added a λ -weighted fairness loss term, f<span class="subscript">p</span>. To take care of overfitting, there is the standard L2 regularization term. This method can be applied to any model with a convex <span class="No-Break">optimization function.</span></p>
			<p>Using this <a id="_idIndexMarker1066"/>approach, they <a id="_idIndexMarker1067"/>were able to achieve both individual and group fairness. Let us <span class="No-Break">see how.</span></p>
			<h3>Individual fairness using a fairness penalty term</h3>
			<p>Continuing <a id="_idIndexMarker1068"/>with our previous terminology, let <em class="italic">s</em> be a binary sensitive feature, s ∈ {0,1}. It tells us whether a user belongs to the sensitive group or not. Then, for individual fairness, the penalty term is defined <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer167" class="IMG---Figure">
					<img src="image/Formula_08_030.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <em class="italic">d</em> is a non-negative function, which is a measure of the absolute difference between the outputs for sensitive and non-sensitive groups (|y<span class="subscript">i</span> – y<span class="subscript">j</span>|), <em class="italic">n</em><span class="subscript">1</span> is the number of individuals belonging to the sensitive group, and <em class="italic">n</em><span class="subscript">2</span> is the number of individuals not belonging to the <span class="No-Break">sensitive group.</span></p>
			<p>We can <a id="_idIndexMarker1069"/>see that every time the model treats the inputs corresponding to sensitive and non-sensitive differently, it <span class="No-Break">is penalized.</span></p>
			<h3>Group fairness using a fairness penalty term</h3>
			<p>Now, for <a id="_idIndexMarker1070"/>group fairness, we would like, on average, for the two group samples to have similar predictions. This is achieved by defining the fairness penalty term <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/Formula_08_034.jpg" alt=""/>
				</div>
			</div>
			<p>Now, to verify whether it really works for the regression case, we make use of the <strong class="bold">communities and crime</strong> dataset, available from the UCI ML repository (<a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>). The dataset contains features related to per-capita violent crime <a id="_idIndexMarker1071"/>rates in different communities across the US. Here, the sensitive feature is the <em class="italic">race</em> of the individual and the task is to predict the <span class="No-Break">crime rate.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.10</em> shows fairness loss versus MSE loss for the dataset. Here, <strong class="bold">single</strong> refers to the case where <a id="_idIndexMarker1072"/>only one model is built for the two sensitive <a id="_idIndexMarker1073"/>groups and <strong class="bold">separate</strong> refers to where we have <span class="No-Break">separate models.</span></p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/Figure_8.10_B18681.jpg" alt="Figure 8.10 – MSE loss versus fairness for the communities and crime dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – MSE loss versus fairness for the communities and crime dataset</p>
			<p>We can see <a id="_idIndexMarker1074"/>once again that there is a price to be paid for fairness in terms of accuracy. Thus, care must be taken by model builders when defining the type of fairness they care about, making a decision specific to their application, and they should decide on a proper balance between prediction accuracy <span class="No-Break">and fairness.</span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor185"/>Fairness constraints for a clustering task</h2>
			<p>A clustering task <a id="_idIndexMarker1075"/>involves <a id="_idIndexMarker1076"/>grouping the given data points into clusters. Although there are supervised clustering algorithms, unsupervised clustering is more common. Here, the data points X = {x<span class="subscript">p</span> ∈ R<span class="superscript">M</span>,p = 1, … , N} are provided and the task is to assign them to <em class="italic">K</em> clusters. Let <em class="italic">M</em> be the cluster assignment vector: M = [m<span class="subscript">1</span>, … , m<span class="subscript">N</span> ] ∈ [0,1]<span class="superscript">NK</span>. Now, say the data contains sensitive information, for example, <em class="italic">J</em> for different demographic groups. We can represent the sensitive information as a vector, S<span class="subscript">j</span> = [s<span class="subscript">j, p</span>] ∈ {0,1}<span class="superscript">N</span> , so if point <em class="italic">p</em> is assigned to group <em class="italic">j</em>, then s<span class="subscript">j, p</span> = 1; otherwise, it <span class="No-Break">is 0.</span></p>
			<p>One of the <a id="_idIndexMarker1077"/>most widely used <a id="_idIndexMarker1078"/>clustering algorithms, K-means, clusters the given data points in <em class="italic">K</em> clusters by minimizing the sum of the distance between individual data points and the representative cluster centroids. We can add a fairness penalty. In the paper <em class="italic">Variational Fair Clustering</em>, the authors added the Kullback-Leibler divergence term between the required demographic proportions and the marginal probability (P<span class="subscript">m</span>=[P(j|m)]) of the demographics within cluster <em class="italic">m</em>. In terms of the cluster assignment vector and demographic information vector, the marginal probability can be expressed <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer170" class="IMG---Figure">
					<img src="image/Formula_08_040.jpg" alt=""/>
				</div>
			</div>
			<p>The <em class="italic">T</em> here denotes the mathematical transpose operator. The modified objective function is shown in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer171" class="IMG---Figure">
					<img src="image/Figure_8.11_B18681.jpg" alt="Figure 8.11 – Loss function with a fairness penalty for the clustering task"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Loss function with a fairness penalty for the clustering task</p>
			<p>The interesting thing to note is that the fairness penalty is represented by a cross-entropy term <a id="_idIndexMarker1079"/>between the <a id="_idIndexMarker1080"/>required demographic proportion, <em class="italic">U</em>, and the respective marginal probability, <em class="italic">Pk</em>. This penalty can further be decomposed into two parts, one convex and the other concave. You are encouraged to read the original paper for complete mathematical proofs of bounds, convergence, and monotonicity <span class="No-Break">guarantees (</span><a href="https://arxiv.org/abs/1906.08207"><span class="No-Break">https://arxiv.org/abs/1906.08207</span></a><span class="No-Break">).</span></p>
			<p><em class="italic">Table 8.1</em> lists the results of applying these fairness constraints on different datasets. The synthetic datasets are created according to the demographic breakdown; both are created with 450 data points and two demographic groups. In the synthetic dataset, both groups have an equal number of required proportions generated; however, in synthetic unequal, we created 310 and 140 data points for the two <span class="No-Break">demographic groups:</span></p>
			<table id="table001-6" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Dataset</strong></p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Clustering Energy</strong></p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Fairness Error</strong></p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Average Balance</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Synthetic Unequal (number of samples = 450, J = 2, l = 10)</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">159.75</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">0.00</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">0.33</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Synthetic (number of samples = 450, J = 2, l = 10)</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">207.80</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">3.69</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">0.01</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Adult (number of samples = 32,561, J = 2, l = 10)</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">9,507</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">0.27</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">0.48</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Bank (number of samples = 41,108, J = 3, l = 10)</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">8,443.88</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">0.69</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">0.15</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Census II (number of samples = 2,458,285, J = 2, l = 10)</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">922,558.03</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">26.22</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">0.44</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Table 8.1 – Table listing clustering objective, fairness error, and average balance on different datasets using K-means clustering with a fairness penalty</p>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.12</em> shows the clusters obtained on the synthetic and synthetic unequal datasets by applying K-means with a <span class="No-Break">fairness penalty.</span></p>
			<div>
				<div id="_idContainer172" class="IMG---Figure">
					<img src="image/Figure_8.12_B18681.jpg" alt="Figure ﻿8.12 – Clusters on the two datasets using K-means with a fairness penalty"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – Clusters on the two datasets using K-means with a fairness penalty</p>
			<p>The same <a id="_idIndexMarker1081"/>approach can <a id="_idIndexMarker1082"/>also be applied to the K-median clustering algorithm and the Ncut <span class="No-Break">clustering algorithm.</span></p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor186"/>Fairness constraints for a reinforcement learning task</h2>
			<p>Reinforcement learning is <a id="_idIndexMarker1083"/>quite different from the supervised learning used in regression and<a id="_idIndexMarker1084"/> classification tasks and <a id="_idIndexMarker1085"/>the unsupervised learning used in the task of clustering. What makes it different is that here, the algorithm has no idea of the desired output. Every choice it makes affects not only its present learning but also future outcomes. In reinforcement learning, the algorithm gets feedback from the environment; it is this feedback that helps <span class="No-Break">it learn.</span></p>
			<p>To define the reinforcement learning task, we can say that the system consists of the agent (reinforcement learning agent or algorithm) and an environment. The agent can perceive the environment through state vectors, <strong class="source-inline">s</strong>, it can bring changes in the environment through actions, <strong class="source-inline">a</strong>, and the environment may give the agent feedback in terms of reward, <strong class="source-inline">r</strong> (<span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.13</em>). The goal of the agent is to find the optimal policy, π(s,a), that will maximize <span class="No-Break">the rewards.</span></p>
			<div>
				<div id="_idContainer173" class="IMG---Figure">
					<img src="image/Figure_8.13_B18681.jpg" alt="Figure ﻿8.13 – Reinforcement learning framework"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – Reinforcement learning framework</p>
			<p>We can <a id="_idIndexMarker1086"/>represent a simple reinforcement <a id="_idIndexMarker1087"/>learning problem as a multi-armed bandit problem. The problem imagines a gambler playing at a row of slot machines. They have to decide the order in which the slot arms will be played so as to maximize their rewards. We can define the problem as having <em class="italic">N</em> slots, with the task of the gambler to choose one arm at each time <span class="No-Break">step, </span><span class="No-Break"><em class="italic">t</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer174" class="IMG---Figure">
					<img src="image/Formula_08_042.jpg" alt=""/>
				</div>
			</div>
			<p>At each time step, <em class="italic">t</em>, the gambler receives a reward, r<span class="subscript">t</span> ∈ [0, 1]<span class="superscript">N</span>, decided by a fixed distribution, E[r<span class="subscript">t</span>(i<span class="subscript">t</span>)] = μ(i<span class="subscript">t</span>). However, the gambler is not aware of this distribution. The only <a id="_idIndexMarker1088"/>thing they can observe <a id="_idIndexMarker1089"/>is the reward, r<span class="subscript">t</span>(i<span class="subscript">t</span>). The goal of the gambler is to choose the arms in a sequence that maximizes the <span class="No-Break">total reward:</span></p>
			<p class="IMG---Figure"><img src="image/Formula_08_046.png" alt=""/></p>
			<p>The optimal reward probability of best the optimal slot arm is <span class="No-Break">as follows:</span></p>
			<p><img src="image/Formula_08_047.png" alt=""/> <span class="No-Break">where, <img src="image/Formula_08_048.png" alt=""/></span></p>
			<p>To train our agent to play the role of the gambler, we can define our loss function as a regret function – representing the regret that the agent will have by not selecting the optimal slot arm at time <span class="No-Break">step </span><span class="No-Break"><em class="italic">t</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="image/Formula_08_049.jpg" alt=""/>
				</div>
			</div>
			<p>We can think of the multi-armed problem as a resource allocation problem – there are limited resources and they should be allocated for maximum performance. When the reinforcement learning agent starts, it has no idea what action would result in a good reward, and therefore most reinforcement learning agents use the technique called exploration and exploitation. The idea is that the agent initially explores all the possible actions by choosing random actions, learns from them (<strong class="bold">exploration</strong>), and later, when sufficient experience is <a id="_idIndexMarker1090"/>gained, uses the action as guided by the learned policy (<strong class="bold">exploitation</strong>). One of the <a id="_idIndexMarker1091"/>algorithms <a id="_idIndexMarker1092"/>used to implement exploration/exploitation is the <strong class="bold">ε greedy algorithm</strong> (<span class="No-Break">Mnih, 2015).</span></p>
			<p>There is always a trade-off between exploration and exploitation. In the ɛ greedy algorithm, the balance between exploration and exploitation is achieved by decreasing the exploration parameter, ɛ. Another way to achieve the balance between exploration and exploitation is using the <strong class="bold">Upper Confidence Bound</strong> (<strong class="bold">UCB</strong>). The UCB algorithm works by <a id="_idIndexMarker1093"/>maintaining an estimate of the expected reward for each action. At each step, the algorithm <a id="_idIndexMarker1094"/>selects the action with <a id="_idIndexMarker1095"/>the highest estimated reward. However, the UCB algorithm also adds a term to each estimate that encourages exploration. This term is based on the uncertainty of the estimate, and it increases as the number of times an action has been taken decreases. As a result, the UCB algorithm strikes a balance between exploration and exploitation, and it often outperforms other algorithms in <span class="No-Break">online settings.</span></p>
			<p>In this section, we will apply fairness constraints to the multi-armed bandit-based resource allocation problem. The intention is to introduce the notion of fairness to resource distribution. Today, AI-based programs are making decisions in allocating resources such as medical care, loans, and subsidies. Can we ensure fairness in this distribution? Also, does fairness in distribution have an effect <span class="No-Break">on collaboration?</span></p>
			<p>One approach to deal with this issue was given by Claure et al. in the paper <em class="italic">Multi-Armed Bandits with Fairness Constraints for Distributing Resources to Human Teammates</em>. They put fair-use constraint limits on the number of times an individual may be assigned a resource, ensuring that all users get what they require. They modified the unconstrained UCB algorithm so as to estimate the expected reward of each arm by using the mean of its empirical rewards in the past and how often it has <span class="No-Break">been pulled.</span></p>
			<p>To evaluate their method, they considered the environment of a collaborative Tetris game, pairing teams of two people with the task of completing a game of Tetris together using their algorithm. The algorithm would decide at each time step which of the two players has control over the falling piece. They later asked the participants to describe whether they felt that the algorithm gave them turns fairly or not: that is their perception of the game’s fairness. </p>
			<p>The following table shows the participants’ perception for different <span class="No-Break">constraint limits:</span></p>
			<table id="table002-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Constraint Limits</strong></p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Participants’ Perception</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">25%, 33%</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><em class="italic">“I felt the more competent player was given more turns, which makes sense but was why it felt unfair.”</em></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">50%</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><em class="italic">“I think it was even, it made us take turns one after the other, enough that it made me feel I was making an equal contribution to the game.”</em></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – Participants’ perception in the experiment for different constraint limits</p>
			<p>Their work <a id="_idIndexMarker1096"/>shows that using their variation <a id="_idIndexMarker1097"/>of the algorithm, it is possible to allocate resources fairly. This leads to higher trust among individuals without any decrease <span class="No-Break">in performance.</span></p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor187"/>Fairness constraints for recommendation systems</h2>
			<p>Recommender systems are <a id="_idIndexMarker1098"/>used to make suggestions for products, services, potential friends, or content. They rely on feedback data and typically run using an ML algorithm. For example, an online bookstore may use a recommender system to classify books by genre <a id="_idIndexMarker1099"/>and suggest similar books to customers who have purchased a book. Facebook <a id="_idIndexMarker1100"/>uses a recommender system to suggest potential friends to users. YouTube uses a recommender system to suggest related videos to users who are watching a video. Generally, recommender systems are used to solve the <strong class="bold">cold-start</strong> problem, where it is difficult to make suggestions for new users or items with <span class="No-Break">little data.</span></p>
			<p>Recommender systems are classified into three types based on the information on which they are based: collaborative, <a id="_idIndexMarker1101"/>content-based, and hybrid filtering. Collaborative filtering recommender <a id="_idIndexMarker1102"/>systems rely on the collective intelligence of the community to make recommendations. Content-based recommender systems use information about the items themselves to make recommendations. Hybrid recommender systems combine both collaborative <a id="_idIndexMarker1103"/>and content-based approaches to make recommendations. Each of these approaches has its own strengths and weaknesses, and the best recommender system for a particular application will depend on the <span class="No-Break">specific requirements.</span></p>
			<p>In recent years, recommender systems have become increasingly prevalent in many aspects of our lives. From online shopping to social media, these systems play a key role in helping us find the products, services, and content that we are looking for. However, these systems are not perfect. In particular, they can often suffer from a variety of biases that <a id="_idIndexMarker1104"/>can lead to unfair recommendations. For example, a recommender system may <a id="_idIndexMarker1105"/>inadvertently favor certain groups of users over others. This can result in unusable recommendations for some users and can even lead to discriminatory behavior. Therefore, it is essential to address the potential for unfairness in recommender systems. Fortunately, there has been recent progress in this area, and there are now a number of methods that can help to mitigate the problem. With continued research, there is the potential for recommender systems to be made more fair and inclusive for <span class="No-Break">all users.</span></p>
			<p>Recommender systems are multi-stake platforms, with three stakeholders: consumers, the supplier, and <span class="No-Break">the platform:</span></p>
			<ul>
				<li><strong class="bold">Consumers</strong> are the <a id="_idIndexMarker1106"/>users of the platform. They come for suggestions. They use the platform because they might be searching for something or having difficulty in deciding, say, in what to buy. They anticipate that the platform will provide a fair and <span class="No-Break">objective suggestion.</span></li>
				<li><strong class="bold">Suppliers</strong> or providers <a id="_idIndexMarker1107"/>are on the other side of the recommender system. They provide the service and, in return, gain some utility from <span class="No-Break">the consumers.</span></li>
				<li>Finally, there is the <strong class="bold">platform</strong> itself. It brings consumers and providers together and makes <a id="_idIndexMarker1108"/>some benefit <span class="No-Break">from it.</span></li>
			</ul>
			<p>Therefore, when we talk of fairness in recommender systems, we also need to clarify which type of fairness we are talking about. In general, we talk of two types of fairness, that is, provider fairness and <span class="No-Break">consumer fairness:</span></p>
			<ul>
				<li><strong class="bold">Provider fairness</strong> is concerned with the items that are being ranked or recommended. Here, the notion of fairness is that similar items or groups of items should be <a id="_idIndexMarker1109"/>ranked or recommended in a similar way. As <a id="_idIndexMarker1110"/>an example, consider two articles about the same subject. If one is ranked higher because it has more views, this would be unfair because it is giving more weight to popularity instead of quality. To fix this, the algorithm could look at other factors, such as recency, number of shares, and number of likes, to get a more accurate idea of quality. By taking these factors into account, the algorithm would be fairer to <span class="No-Break">both articles.</span></li>
				<li><strong class="bold">Consumer fairness</strong> has a focus on the users who receive or use the data or service. A fair <a id="_idIndexMarker1111"/>recommendation algorithm, in this <a id="_idIndexMarker1112"/>case, would ensure that users or groups of users receive the same recommendations or rankings. Continuing with the example of the recommender system suggesting articles, if two readers have the same educational background and interest, the algorithm should recommend the same/similar articles to them, without considering sensitive attributes such as gender <span class="No-Break">or age.</span></li>
			</ul>
			<p>One approach to achieving fairness in recommendation systems is to treat them as classification <a id="_idIndexMarker1113"/>algorithms, with <em class="italic">recommendation</em> being <a id="_idIndexMarker1114"/>a class. Another way is to treat recommendation systems as a ranking problem; here, the recommendation is a ranked list, <em class="italic">L</em>. We have already covered how fairness constraints can be added to classification tasks. In this section, we will discuss fairness constraints in ranking, based on the approach developed in the paper <em class="italic">Ranking with Fairness Constraints</em> by Celis et al. The basic idea is to put constraints on the number of items that can appear in the top <em class="italic">k</em> places from <span class="No-Break">different groups.</span></p>
			<p>Let us start with defining the ranking problem. We have <em class="italic">m</em> items, and the goal is to output a list of <em class="italic">n</em> items (n ≪ m) in an order that is most valuable to a consumer or provider. We define a number, W<span class="subscript">ij</span>, that captures the value that an item, <em class="italic">i</em>, <em class="italic">i</em> ∈ [m], contributes if placed at rank <em class="italic">j</em>. There is more than one way to get the value of W<span class="subscript">ij</span>. We can define it as a <strong class="bold">ranking maximization problem</strong>. Here, the task is to assign each item a position <a id="_idIndexMarker1115"/>such that the total value is maximized. </p>
			<p>To ensure fairness, Celis et al. added constraints in the form of upper (<em class="italic">U</em><span class="subscript">l</span><span class="subscript">,</span><span class="subscript">k</span>) and lower (<em class="italic">L</em><span class="subscript">l</span><span class="subscript">,</span><span class="subscript">k</span>) bounds on the number of objects with property <em class="italic">l</em> that are allowed to appear in the top <em class="italic">k</em> position of the ranking. We can represent the position of items via a binary matrix, <em class="italic">x</em>, which would be an <em class="italic">n</em>x<em class="italic">m</em> matrix. The <em class="italic">x</em><span class="subscript">ij</span> element <a id="_idIndexMarker1116"/>of this binary matrix tells <a id="_idIndexMarker1117"/>us whether the <em class="italic">i</em>-th item is ranked at the <em class="italic">j</em>-th position. The constraints are put on <span class="No-Break">the matrix:</span></p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="image/Formula_08_055.jpg" alt=""/>
				</div>
			</div>
			<p>Here, {1, 2, ... , p} is the set of properties, and P<span class="subscript">l </span>⊑ [m] represents the set of items with property <em class="italic">l</em>. With this change, the ranking maximization will become <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/Formula_08_058.jpg" alt=""/>
				</div>
			</div>
			<p>This is illustrated in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="image/Figure_8.14_B18681.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – A sample matrix of W. The values represent the optimal unconstrained ranking (gray) and constrained ranking (orange). There is an upper constraint of 2 on the number of males in the top six positions (image adapted from the paper Fairness in rankings and recommendations: an overview)</p>
			<p>Fairness in decision-making <a id="_idIndexMarker1118"/>is an important issue that <a id="_idIndexMarker1119"/>we have only begun to understand. For example, when there are many applicants for a job or loan, it can be difficult, if not impossible, to decide who should get what (job, loan or whatever decision is to be made) because all seem equally qualified. However, some candidates might deserve more consideration than others based on factors such as race, which could cause a societal disadvantage, even without those involved <span class="No-Break">realizing it.</span></p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor188"/>Challenges of fairness</h1>
			<p>The development of various methods for ML fairness has attracted increasing attention within the research <a id="_idIndexMarker1120"/>community and we have made significant progress. However, there are still several challenges that need to be looked into. In this section, we briefly touch on different challenges that exist in building <span class="No-Break">fair models.</span></p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor189"/>Missing sensitive attributes</h2>
			<p>Fairness in ML models continues to be a challenge even if very few or even no sensitive attribute is known. Achieving fairness generally means ensuring that the resulting model is not biased against any particular group. This can be difficult to do when training data does not include information about individuals’ sensitive attributes. Most of the existing methods assume that sensitive attributes are explicitly known. However, with growing concern about privacy, and regulations such as GDPR, businesses are required to protect <span class="No-Break">sensitive data.</span></p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor190"/>Multiple sensitive attributes</h2>
			<p>The techniques that we’ve covered in this chapter work with only one sensitive attribute. However, there is the possibility of there being more than one sensitive attribute, for example, gender and race. When there are multiple sensitive attributes in the data, a model that gives fair predictions for one sensitive attribute could give unfair predictions for other sensitive attributes. A model that is trained to be fair for gender can still be unfair for race. Multiple-attribute fairness is at present a relatively less-explored problem and should be explored in <span class="No-Break">the future.</span></p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor191"/>Choice of fairness measurements</h2>
			<p>We have seen, in this chapter, that the design of different techniques for mitigating unfairness in an algorithm depends on the desired fairness measurements. Selecting the right metrics to measure fairness is of utmost importance, and it depends on the specific circumstances <span class="No-Break">being considered.</span></p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor192"/>Individual versus group fairness trade-off</h2>
			<p>Group fairness is concerned with ensuring that the protected group (such as women) is represented fairly <a id="_idIndexMarker1121"/>in the results of the ML model. Individual fairness is <a id="_idIndexMarker1122"/>concerned with making sure that individuals with similar sensitive attributes (for example, race or gender) are treated similarly by the model. Group fairness and individual fairness have different goals. The existing unfairness mitigation algorithms often focus on only one of these goals. It is possible, therefore, that a model that is optimized for individual fairness might not be fair for a group and vice versa. However, there can be situations where both types of fairness <span class="No-Break">are desirable.</span></p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor193"/>Interpretation and fairness</h2>
			<p>By utilizing interpretable ML techniques, we can gain a better understanding of our models and <a id="_idIndexMarker1123"/>debug the model. This can serve as a tool to identify and remove bias and achieve fairness. For instance, in a sentiment analysis task, we can leverage model interpretation techniques to detect racial bias by examining the most significant features of each <span class="No-Break">demographic group.</span></p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor194"/>Fairness versus model performance</h2>
			<p>Fairness constraints <a id="_idIndexMarker1124"/>typically limit the decision space <a id="_idIndexMarker1125"/>of the ML model, resulting in a trade-off between fairness and model performance. There is a need for systematic and theoretical investigation of the relationship between fairness constraints and <span class="No-Break">model performance.</span></p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor195"/>Limited datasets</h2>
			<p>There are very limited public datasets that can be used to study fairness. The following table lists some of the datasets available for benchmarking the fairness <span class="No-Break">mitigating algorithm:</span></p>
			<table id="table003-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Dataset</strong></p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Description</strong></p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Size</strong></p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Field</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">German credit card dataset</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Contains attributes such as personal status, gender, credit score, and housing status. It can be used to study fairness on gender- and credit-related issues.</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">1,000</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Financial</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">UCI adult dataset</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">It has attributes such as age, occupation, education, race, gender, marital status, and whether a person’s annual income is above $50K or not.</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">48,842</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Social</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Diversity in faces dataset</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">This is a large dataset of annotated facial images. Besides images, it also contains information about skin color, gender, and facial symmetry.</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">1 million</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Facial images</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">COMPAS dataset</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">This contains records for defendants from Broward County. It includes their jail and prison time and their demographic information.</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">18,610</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Social</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Communities and crime dataset</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">This data from various communities in the US is from the US LEMAS survey and the FBI Unified Crime Report.</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">1,994</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Social</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Pilot parliaments benchmark dataset</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">This contains the data of individuals in the national parliaments of three European countries (Iceland, Sweden, and Finland) and three African countries (South Africa, Senegal, and Rwanda).</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">1,270</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Facial images</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">WinoBias dataset</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">A collection of texts designed to evaluate coreference resolution systems, containing sentences regarding 40 occupations, each described multiple times with different human pronouns, to uncover and address gender bias.</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">3,160</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Coreference resolution</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Recidivism in juvenile justice dataset</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">This contains data about juvenile offenders who committed a crime between the years 2002 and 2010 in the Catalonia juvenile justice system.</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">4,753</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Social</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.2 – A table of useful datasets for benchmarking fairness</p>
			<p>These are some of the <a id="_idIndexMarker1126"/>widely used datasets when studying fairness <span class="No-Break">in algorithms.</span></p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor196"/>Summary</h1>
			<p>In this chapter, we covered fairness constraints as applied to different ML tasks. We started with the classification task and saw how we can add a regularizer to the loss function to mitigate unfairness. The chapter also covered how we can modify the loss function (objective) and mitigate unfairness. After that, we worked on regression tasks. There, again, we saw how adding a regularizer term can ensure fair algorithms. We covered the penalty terms for both individual and group fairness. Then, we explored the term that can be added to cluster a task to make it fair. We also discussed reinforcement learning and saw how fairness constraints can be added to the regret function. The recommendation task was considered next, where we showed how adding fairness constraints in the form of upper and lower bounds can help in mitigating unfairness. We also discussed how the recommendation task is similar and different compared to the other tasks. Finally, we covered the challenges in fairness. We saw that there is still a lot of work needed in this area. Most fairness algorithms are currently in the nascent stage. There is a need to adopt fairness strategies in existing deep learning and ML frameworks so that they can be adopted widely. In the next chapter, we will talk about explainability <span class="No-Break">in AI.</span></p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor197"/>Further reading</h1>
			<ul>
				<li><em class="italic">Three roads to organizational justice, </em>Cropanzano, R., Rupp, D. E., Mohler, C. J., and Schminke, M. (2001). <em class="italic"> </em>(<a href="https://www.emerald.com/insight/content/doi/10.1016/S0742-7301(01)20001-2/full/html"><span class="No-Break">https://www.emerald.com/insight/content/doi/10.1016/S0742-7301(01)20001-2/full/html</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Bias and Fairness in Multimodal Machine Learning: A Case Study of Automated Video Interviews,  </em>Booth, B. M., Hickman, L., Subburaj, S. K., Tay, L., Woo, S. E., and D’Mello, S. K. (2021, October). In <em class="italic">Proceedings of the 2021 International Conference on Multimodal Interaction</em> (pp. <span class="No-Break">268-277) (</span><a href="https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897"><span class="No-Break">https://dl.acm.org/doi/fullHtml/10.1145/3462244.3479897</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Algorithmic Fairness</em>. arXiv preprint arXiv:2001.09784, Pessach, D. and Shmueli, E. (2020).  (<a href="https://arxiv.org/pdf/2001.09784.pdf"><span class="No-Break">https://arxiv.org/pdf/2001.09784.pdf</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Gender Bias in Translation Using Google Translate: Problems and Solution</em>. <em class="italic">Language Circle: Journal of Language and Literature</em>, 15(2), Fitria, T. N. (<span class="No-Break">2021). </span><span class="No-Break">(</span><a href="https://journal.unnes.ac.id/nju/index.php/LC/article/download/28641/11534"><span class="No-Break">https://journal.unnes.ac.id/nju/index.php/LC/article/download/28641/11534</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Machine Learning: A Probabilistic Perspective</em>. MIT Press, Murphy, K. P. (2012).  (<a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/38136.pdf"><span class="No-Break">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/38136.pdf</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Fairness-Aware Classifier with Prejudice Remover Regularizer</em>. In <em class="italic">Joint European conference on machine learning and knowledge discovery in databases</em> (pp. 35-50), Springer, Berlin, Heidelberg. Kamishima, T., Akaho, S., Asoh, H., and Sakuma, J. (2012, September).  (<a href="https://link.springer.com/content/pdf/10.1007/978-3-642-33486-3_3.pdf"><span class="No-Break">https://link.springer.com/content/pdf/10.1007/978-3-642-33486-3_3.pdf</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Three naive Bayes approaches for discrimination-free classification</em>. In <em class="italic">Data mining and knowledge discovery</em>, 21(2), pp 277-292, Calders, T. and Verwer, S. (<span class="No-Break">2010). (</span><a href="https://link.springer.com/content/pdf/10.1007/s10618-010-0190-x.pdf"><span class="No-Break">https://link.springer.com/content/pdf/10.1007/s10618-010-0190-x.pdf</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Fairness Constraints: Mechanisms for Fair Classification</em>. In <em class="italic">Artificial intelligence and statistics</em> (pp. 962-970). PMLR, Zafar, M. B., Valera, I., Rogriguez, M. G., and Gummadi, K. P. (2017, <span class="No-Break">April). </span><span class="No-Break">(</span><a href="https://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf"><span class="No-Break">https://proceedings.mlr.press/v54/zafar17a/zafar17a.pdf</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">A Convex Framework for Fair Regression, </em>Berk, R., Heidari, H., Jabbari, S., Joseph, M., Kearns, M., Morgenstern, J., ... and Roth, A. (2017). arXiv preprint <span class="No-Break">arXiv:1706.02409. (</span><a href="https://arxiv.org/pdf/1706.02409"><span class="No-Break">https://arxiv.org/pdf/1706.02409</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">UCI machine learning repository</em>, 2013, Lichman, M. <span class="No-Break">URL: </span><a href="http://archive.ics.uci.edu/ml"><span class="No-Break">http://archive.ics.uci.edu/ml</span></a></li>
				<li><em class="italic">Human-level control through deep reinforcement learning. Nature</em>, 518(7540), pp 529-533, Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... and Hassabis, D. (<span class="No-Break">2015). (</span><a href="https://www.nature.com/articles/nature14236?wm=book_wap_0005"><span class="No-Break">https://www.nature.com/articles/nature14236?wm=book_wap_0005</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Fairness in rankings and recommendations: an overview</em>. <em class="italic">The VLDB Journal</em>, pp 1-28, Pitoura, E., Stefanidis, K., and Koutrika, G. (<span class="No-Break">2021). (</span><a href="https://link.springer.com/article/10.1007/s00778-021-00697-y"><span class="No-Break">https://link.springer.com/article/10.1007/s00778-021-00697-y</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Ranking with Fairness Constraints</em>. arXiv preprint arXiv:1704.06840, Celis, L. E., Straszak, D., and Vishnoi, N. K. (2017).  (<a href="https://arxiv.org/pdf/1704.06840.pdf"><span class="No-Break">https://arxiv.org/pdf/1704.06840.pdf</span></a><span class="No-Break">)</span></li>
			</ul>
		</div>
	</body></html>