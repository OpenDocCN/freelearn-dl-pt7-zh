["```py\ncorpus = \"Hamlet was the only son of the King of Denmark. He loved his father and mother dearly--and was happy in the love of a sweet lady named Ophelia. Her father, Polonius, was the King's Chamberlain. While Hamlet was away studying at Wittenberg, his father died. Young Hamlet hastened home in great grief to hear that a serpent had stung the King, and that he was dead. The young Prince had loved ...\n```", "```py\nimport math\nimport os\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow.python.platform\nfrom keras.preprocessing import sequence\n```", "```py\ncaptions = 'image_captions.token'\nfeatures = 'features.npy'\n```", "```py\nannotations = pd.read_table(captions, sep='\\t', header=None, names=['image', 'caption'])\nimages = np.load(features,'r'), \ncaptions = annotations['caption'].values\n```", "```py\noccuranceDict = {} ## This Dictionary will store the occurance count of our words \nwordCount = 0 ## wordCount is a value that we will use to keep track of the number of occurances of a word\n```", "```py\ndef ConstructVocab(captions):\n    '''Function to Construct the Vocab that we will be generating from'''\n\n    occuranceDict = {} ## This Dictionary will store the occurance count of our words \n    wordCount = 0 ## wordCount is a valuee that we will use to keep track of the number of occurances of a word\n\n    ## Iterate over the captions to split them into individuals words to construct the vocab\n    for item in captions: \n        wordCount += 1\n\n        for word in item.lower().split(' '):\n            occuranceDict[word] = occuranceDict.get(word, 0) + 1\n\n    vocab = [word for word in occuranceDict if occuranceDict[word] >= 20]\n\n    ## Set a dictionary to set a word for each index\n    IndexesToWords = {} ## \n    ixtoword[0] = '.' \n\n    ## Set a dictionary to the indexes of each word at each steps\n    WordsToIndexes = {}\n    WordsToIndexes['#START#'] = 0 \n    index = 1\n\n    ## Iterate over the words in the vocab to store them and index their position. \n    for word in vocab:\n      WordstoIndexes[word] = index \n      IndexestoWords[index] = word\n      index += 1\n\n    ## Set the wordcount for the occurance dictionary\n    occuranceDict['.'] = wordCount\n\n    ## Initiative the word bias vectors\n    biasVector = np.array([1.0*occuranceDict[IndexestoWords[i]] for i in IndexestoWords])\n    biasVector = biasVector / np.sum(biasVector) \n    biasVector = np.log(biasVector)\n    biasVector = biasVector - np.max(biasVector) \n\n    ## Return the dictionarties, as well as the bias vector\n    return WordstoIndexes, IndexestoWords,         biasVector.astype(np.float32)\n```", "```py\ndef captionRNN():\n    ''' RNN for Image Captioning '''\n\n    ## Define our Networks Parameters\n    dim_embed = 256\n    dim_hidden = 256\n    dim_in = 4096\n    batch_size = 128\n    momentum = 0.9\n    n_epochs = 150\n\n    ## Initialize the embedding distribution and bias factor as a random uniform distribution. \n    captionEmbedding = tf.Variable(tf.random_uniform([n_words, dim_embed], -0.1, 0.1))\n    captionBias = tf.Variable(tf.zeros([dim_embed]))\n\n    ## Initialize the embedding distribution and bias for the images \n    imgEmbedding = tf.Variable(tf.random_uniform([dim_in, dim_hidden], -0.1, 0.1))\n    imgBias = tf.Variable(tf.zeros([dim_hidden]))\n\n    ## Initialize the encodings for the words\n    wordEncoding = tf.Variable(tf.random_uniform([dim_hidden, n_words], -0.1, 0.1))\n    wordBias = tf.Variable(init_b)\n\n    ## Initialize the variables for our images \n    img = tf.placeholder(tf.float32, [batch_size, dim_in]) ## Placeholder for our image variables\n    capHolder = tf.placeholder(tf.int32, [batch_size, n_lstm_steps]) ## Placeholder for our image captions\n    mask = tf.placeholder(tf.float32, [batch_size, n_lstm_steps]) \n\n    ## Compute an initial embedding for the LSTM\n    imgEmbedding = tf.matmul(img, imgEmbedding) + imgBias\n\n    ## Initialize the LSTM and its starting state\n    lstm = tf.contrib.rnn.BasicLSTMCell(dim_hidden)\n    state = self.lstm.zero_state(batch_size, dtype=tf.float32)\n\n    ## Define a starting loss\n    totalLoss = 0.0\n```", "```py\n ## Training Cycle for the Model\n    with tf.variable_scope(\"RNN\"):\n        for i in range(n_lstm_steps): \n            ## Tell the model to utilizing the embedding corresponding to the appropriate caption, \n            ## if not, utilize the image at the first embedding\n            if i > 0:\n                current_embedding = tf.nn.embedding_lookup(captionEmbedding, capHolder[:,i-1]) + captionBias\n                tf.get_variable_scope().reuse_variables()\n            else:\n                current_embedding = imgEmbedding\n\n            out, state = lstm(current_embedding, state) ## Output the current embedding and state from the LSTM\n\n            if i > 0:\n\n                labels = tf.expand_dims(capHolder[:, i], 1)\n                ix_range = tf.range(0, batch_size, 1) ## get the index range\n                indexes = tf.expand_dims(ix_range, 1) ## get the indexes\n                concat = tf.concat([indexes, labels],1) ## Concatonate the indexes with their labels\n\n                ## Utilizng a \"One Hot\" encoding scheme for the labels\n                oneHot = tf.sparse_to_dense(concat, tf.stack([batch_size, n_words]), 1.0, 0.0) \n\n                ## Run the results through a softmax function to generate the next word \n                logit = tf.matmul(out, wordEncoding) + wordBias\n\n                ## Utilizing Cross Entropy as our Loss Function\n                crossEntropyLoss = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=oneHot)\n                crossEntropyLoss = crossEntropyLoss * mask[:,i]\n\n                ## Tell Tensorflow to reduce our loss\n                loss = tf.reduce_sum(crossEntropyLoss)\n\n                ## Add the loss at each iteration to our total loss \n                totalLoss = totalLoss + loss\n\n        totalLoss = totalLoss / tf.reduce_sum(mask[:,1:])\n\n        return totalLoss, img, capHolder, mask\n```", "```py\ndef trainNeuralCaption(learning_rate=0.0001):\n '''Function to train the Neural Machine Translation Model '''\n\n ## Initialize a Tensorflow Session\n sess = tf.InteractiveSession()\n\n ## Load the images and construct the vocab using the functions we described above\n images, captions = load_images('path/to/captions', 'path/to/features')\n WordstoIndexes, IndexestoWords, init_b = constructVocab(captions)\n\n ## Store the indexes\n index = (np.arange(len(images)).astype(int))\n np.random.shuffle(index) \n n_words = len(WordstoIndexes)\n maxlen = np.max( [x for x in map(lambda x: len(x.split(' ')), captions) ] )\n```", "```py\n\n ## Initialize the Caption RNN model function and build the model \n nc = neuralCaption(dim_in, dim_hidden, dim_embed, batch_size, maxlen+2, n_words, init_b)\n loss, image, sentence, mask = nc.build_model()\n\n ## Define our timestep and the overall learning rate\n global_step = tf.Variable(0,trainable=False)\n learning_rate = tf.train.exponential_decay(learning_rate, global_step, int(len(index)/batch_size), 0.95)\n\n ## Utilize Adam as our optimization function \n train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\n ## Initialize all our variables\n tf.global_variables_initializer().run()\n\n ## Run the training cucle\n for epoch in range(n_epochs):\n for start, end in zip( range(0, len(index), batch_size), range(batch_size, len(index), batch_size)):\n\n ## Current Images and Captions\n currentImages = images[index[start:end]]\n currentCaptions = captions[index[start:end]]\n current_caption_ind = [x for x in map(lambda cap: [WordstoIndexes[word] for word in cap.lower().split(' ')[:-1] if word in WordstoIndezes], current_captions)]\n\n ## Pad the incoming sequences\n current_caption_matrix = sequence.pad_sequences(current_caption_ind, padding='post', maxlen=maxlen+1)\n current_caption_matrix = np.hstack( [np.full( (len(current_caption_matrix),1), 0), current_caption_matrix] )\n\n current_mask_matrix = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n nonzeros = np.array([x for x in map(lambda x: (x != 0).sum()+2, current_caption_matrix )])\n\n for ind, row in enumerate(current_mask_matrix):\n row[:nonzeros[ind]] = 1\n\n ## Run the operations in a TensorFlow session \n _, currentLoss = sess.run([train_op, loss], feed_dict={\n image: current_feats.astype(np.float32),\n sentence : current_caption_matrix.astype(np.int32),\n mask : current_mask_matrix.astype(np.float32)\n })\n\n print(\"Loss: \", currentLoss)\n```", "```py\n trainNeuralCaption()\n```"]