<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Combining It All Together</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">Now that we have understood and implemented different </span><strong><span class="koboSpan" id="kobo.3.1">Artificial Intelligence</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">AI</span></strong><span class="koboSpan" id="kobo.6.1">)/</span><strong><span class="koboSpan" id="kobo.7.1">machine learning</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong><span class="koboSpan" id="kobo.9.1">ML</span></strong><span class="koboSpan" id="kobo.10.1">) algorithms, it is time to combine it all together, understand which type of data is best suited for each, and, at the same time, understand the basic preprocessing required for each type of data. </span><span class="koboSpan" id="kobo.10.2">By the end of this chapter, you will know the following:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">The different types of data that can be fed to your model</span></li>
<li><span class="koboSpan" id="kobo.12.1">How to process time series data</span></li>
<li><span class="koboSpan" id="kobo.13.1">Preprocessing of textual data</span></li>
<li><span class="koboSpan" id="kobo.14.1">Different transforms that can be done on image data</span></li>
<li><span class="koboSpan" id="kobo.15.1">How to handle video files</span></li>
<li><span class="koboSpan" id="kobo.16.1">How to handle speech data</span></li>
<li><span class="koboSpan" id="kobo.17.1">Cloud computing options</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Processing different types of data</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Data is available in all shapes, sizes, and forms: tweets, daily stock prices, per minute heartbeat signals, photos from cameras, video obtained from CCTV, audio recordings, and so on. </span><span class="koboSpan" id="kobo.2.2">Each of them contain information and when properly processed and used with the right model, we can analyze the data and, obtain advanced information about the underlying patterns. </span><span class="koboSpan" id="kobo.2.3">In this section, we will cover the basic preprocessing required for each type of data before it can be fed to a model and the models that can be used for it.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Time series modeling</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Time underlies many interesting human behaviors, and hence, it is important that AI-powered IoT systems know how to deal with time-dependent data. </span><span class="koboSpan" id="kobo.2.2">Time can be represented either explicitly, for example, capturing data at regular intervals where the time-stamp is also part of data, or implicitly, for example, in speech or written text. </span><span class="koboSpan" id="kobo.2.3">The methods that allow us to capture inherent patterns in time-dependent data is called </span><strong><span class="koboSpan" id="kobo.3.1">time series modeling</span></strong><span class="koboSpan" id="kobo.4.1">.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The data that is captured at regular intervals is a time series data, for example, stock price data is a time series data. </span><span class="koboSpan" id="kobo.5.2">Let's take a look at Apple stock price data; this data can be downloaded from the NASDAQ site (</span><a href="https://www.nasdaq.com/symbol/aapl/historical"><span class="koboSpan" id="kobo.6.1">https://www.nasdaq.com/symbol/aapl/historical</span></a><span class="koboSpan" id="kobo.7.1">). </span><span class="koboSpan" id="kobo.7.2">Alternatively, you can use the </span><kbd><span class="koboSpan" id="kobo.8.1">pandas_datareader</span></kbd><span class="koboSpan" id="kobo.9.1"> module to directly download the data by specifying the data source. </span><span class="koboSpan" id="kobo.9.2">To install </span><kbd><span class="koboSpan" id="kobo.10.1">pandas_datareader</span></kbd><span class="koboSpan" id="kobo.11.1"> in your working environment, use the following:</span></p>
<pre><span class="koboSpan" id="kobo.12.1">pip install pandas_datareader</span></pre>
<ol>
<li><span class="koboSpan" id="kobo.13.1">The following code downloads the Apple Inc stock price from Yahoo Finance from 1</span><sup><span class="koboSpan" id="kobo.14.1">st</span></sup><span class="koboSpan" id="kobo.15.1"> January 2010 to 31</span><sup><span class="koboSpan" id="kobo.16.1">st</span></sup><span class="koboSpan" id="kobo.17.1"> December 2015:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.18.1">import datetime</span><br/><span class="koboSpan" id="kobo.19.1">from pandas_datareader import DataReader</span><br/><span class="koboSpan" id="kobo.20.1">%matplotlib inline</span><br/><br/><br/><span class="koboSpan" id="kobo.21.1">Apple = DataReader("AAPL", "yahoo", </span><br/><span class="koboSpan" id="kobo.22.1">        start=datetime.datetime(2010, 1, 1), </span><br/><span class="koboSpan" id="kobo.23.1">        end=datetime.datetime(2015,12,31)) </span><br/><span class="koboSpan" id="kobo.24.1">Apple.head()</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.25.1">The downloaded DataFrame provides </span><kbd><span class="koboSpan" id="kobo.26.1">High</span></kbd><span class="koboSpan" id="kobo.27.1">, </span><kbd><span class="koboSpan" id="kobo.28.1">Low</span></kbd><span class="koboSpan" id="kobo.29.1">, </span><kbd><span class="koboSpan" id="kobo.30.1">Open</span></kbd><span class="koboSpan" id="kobo.31.1">, </span><kbd><span class="koboSpan" id="kobo.32.1">Close</span></kbd><span class="koboSpan" id="kobo.33.1">, </span><kbd><span class="koboSpan" id="kobo.34.1">Volume</span></kbd><span class="koboSpan" id="kobo.35.1">, and </span><kbd><span class="koboSpan" id="kobo.36.1">Adj Close</span></kbd><span class="koboSpan" id="kobo.37.1"> values for each working day:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.38.1"><img src="assets/1fb0ab65-15be-41bb-b84b-0aca162443a4.png" style="width:36.83em;height:12.83em;"/></span></p>
<p class="mce-root"/>
<ol start="3">
<li><span class="koboSpan" id="kobo.39.1">Let's now plot it, shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.40.1">close = Apple['Adj Close']</span><br/><span class="koboSpan" id="kobo.41.1">plt.figure(figsize= (10,10))</span><br/><span class="koboSpan" id="kobo.42.1">close.plot()</span><br/><span class="koboSpan" id="kobo.43.1">plt.ylabel("Apple stocj close price")</span><br/><span class="koboSpan" id="kobo.44.1">plt.show()</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.45.1"><img class="aligncenter size-full wp-image-1125 image-border" src="assets/fa7e0db3-3e92-49f9-a9ef-bf5e5a396a44.png" style="width:40.58em;height:36.17em;"/></span></p>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.46.1">To be able to model time series data, we need to identify a few things: trend, seasonality, and stationarity.</span></p>
<ol start="4">
<li><strong><span class="koboSpan" id="kobo.47.1">Trend</span></strong><span class="koboSpan" id="kobo.48.1"> means to find whether, on average, the measurements tend to decrease (or increase) over time. </span><span class="koboSpan" id="kobo.48.2">The most common way to find a trend is by plotting a moving average, shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.49.1">moving_average = close.rolling(window=20).mean()</span><br/><br/><span class="koboSpan" id="kobo.50.1">plt.figure(figsize= (10,10))</span><br/><span class="koboSpan" id="kobo.51.1">close.plot(label='Adj Close')</span><br/><span class="koboSpan" id="kobo.52.1">moving_average.plot(label='Moving Average Window 20')</span><br/><span class="koboSpan" id="kobo.53.1">plt.legend(loc='best')</span><br/><span class="koboSpan" id="kobo.54.1">plt.show()</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.55.1"><img class="aligncenter size-full wp-image-1126 image-border" src="assets/860eb6d8-e51e-418e-a446-c6d9a88a5876.png" style="width:41.75em;height:38.08em;"/></span></p>
<ol start="5">
<li><span class="koboSpan" id="kobo.56.1">We can see, with a window of 20, the upward and downward trend. </span><span class="koboSpan" id="kobo.56.2">For time series modeling, we should detrend the data. </span><span class="koboSpan" id="kobo.56.3">Detrending can be done by subtracting the trend (moving average) from the original signal. </span><span class="koboSpan" id="kobo.56.4">Another popular way is using the first order difference method, where you take the difference between successive data points:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.57.1">fod = close.diff()</span><br/><span class="koboSpan" id="kobo.58.1">plt.figure(figsize= (10,10))</span><br/><span class="koboSpan" id="kobo.59.1">fod.plot(label='First order difference')</span><br/><span class="koboSpan" id="kobo.60.1">fod.rolling(window=40).mean().\</span><br/><span class="koboSpan" id="kobo.61.1">        plot(label='Rolling Average')</span><br/><span class="koboSpan" id="kobo.62.1">plt.legend(loc='best')</span><br/><span class="koboSpan" id="kobo.63.1">plt.show()</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.64.1"><img class="aligncenter size-full wp-image-1127 image-border" src="assets/5730da71-8b00-4cf1-8b44-a59920e3cab6.png" style="width:39.33em;height:36.17em;"/></span></p>
<ol start="6">
<li><strong><span class="koboSpan" id="kobo.65.1">Seasonality</span></strong><span class="koboSpan" id="kobo.66.1"> is the presence of a regularly repeating pattern of highs and lows related to time (for example, sine series). </span><span class="koboSpan" id="kobo.66.2">The easiest way is to find autocorrelation in the data. </span><span class="koboSpan" id="kobo.66.3">Once you find the seasonality, you can remove it by differencing the data by a time lag corresponding to the season length:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.67.1"># Autocorrelation</span><br/><span class="koboSpan" id="kobo.68.1">plt.figure(figsize= (10,10))</span><br/><span class="koboSpan" id="kobo.69.1">fod.plot(label='First order difference')</span><br/><span class="koboSpan" id="kobo.70.1">fod.rolling(window=40).mean().\</span><br/><span class="koboSpan" id="kobo.71.1">        plot(label='Rolling Average')</span><br/><span class="koboSpan" id="kobo.72.1">fod.rolling(window=40).corr(fod.shift(5)).\</span><br/><span class="koboSpan" id="kobo.73.1">        plot(label='Auto correlation')</span><br/><span class="koboSpan" id="kobo.74.1">plt.legend(loc='best')</span><br/><span class="koboSpan" id="kobo.75.1">plt.show()</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.76.1"><img class="aligncenter size-full wp-image-1128 image-border" src="assets/d024cf46-d51b-45f2-a1d4-307cf981e712.png" style="width:38.67em;height:35.50em;"/></span></p>
<ol start="7">
<li><span class="koboSpan" id="kobo.77.1">The last thing is to ensure whether the series is </span><strong><span class="koboSpan" id="kobo.78.1">stationary</span></strong><span class="koboSpan" id="kobo.79.1">, that is, the mean of the series is no longer a function of time. </span><span class="koboSpan" id="kobo.79.2">Stationarity of data is essential for time series modeling. </span><span class="koboSpan" id="kobo.79.3">We achieve stationarity by removing any trends or seasonality present within the data. </span><span class="koboSpan" id="kobo.79.4">Once the data is stationary, we can use regression models to model it.</span></li>
</ol>
<div class="packt_tip"><span class="koboSpan" id="kobo.80.1">Traditionally, time series data was modeled using auto-regressive and moving average based models like ARMA and ARIMA. </span><span class="koboSpan" id="kobo.80.2">To learn more about time series modeling, the interested reader can refer to these books: </span><br/>
<ul>
<li><span><span class="koboSpan" id="kobo.81.1">Pandit, S. </span><span class="koboSpan" id="kobo.81.2">M., and Wu, S. </span><span class="koboSpan" id="kobo.81.3">M. </span><span class="koboSpan" id="kobo.81.4">(1983).</span></span> <em><span class="koboSpan" id="kobo.82.1">Time Series and System Analysis with Applications</span></em><span><span class="koboSpan" id="kobo.83.1">(Vol. </span><span class="koboSpan" id="kobo.83.2">3). </span><span class="koboSpan" id="kobo.83.3">New York: Wiley.</span><br/></span></li>
<li><span><span class="koboSpan" id="kobo.84.1">Brockw</span></span><span><span class="koboSpan" id="kobo.85.1">ell, P. </span><span class="koboSpan" id="kobo.85.2">J., Davis, R. </span><span class="koboSpan" id="kobo.85.3">A., and Calder, M. </span><span class="koboSpan" id="kobo.85.4">V. </span><span class="koboSpan" id="kobo.85.5">(2002).</span></span> <em><span class="koboSpan" id="kobo.86.1">Introduction to Time Series and Forecasting</span></em><span><span class="koboSpan" id="kobo.87.1">(Vol. </span><span class="koboSpan" id="kobo.87.2">2). </span><span class="koboSpan" id="kobo.87.3">New York</span></span><span><span class="koboSpan" id="kobo.88.1">: Springer.</span></span></li>
</ul>
</div>
<p><span class="koboSpan" id="kobo.89.1">The stationarity is an important property for any time series data, whether you are using traditional time series modeling or deep learning models. </span><span class="koboSpan" id="kobo.89.2">This is so because, if a series has stationarity (even if it is weak stationarity), then it means the data has same distribution across time, and hence, can be estimated in time. If you are planning to use deep learning models such as RNN or LSTM, then after confirming stationarity of the time series, additionally, you need to normalize the data and use a sliding window transform to convert the series in to input-output pairs on which regression can be done. </span><span class="koboSpan" id="kobo.89.3"> This can be very easily done using the scikit-learn library and NumPy:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.90.1">Let's normalize the </span><kbd><span class="koboSpan" id="kobo.91.1">close</span></kbd><span class="koboSpan" id="kobo.92.1"> DataFrame. </span><span class="koboSpan" id="kobo.92.2">Normalization ensures that data lies between </span><kbd><span class="koboSpan" id="kobo.93.1">0</span></kbd><span class="koboSpan" id="kobo.94.1"> and </span><kbd><span class="koboSpan" id="kobo.95.1">1</span></kbd><span class="koboSpan" id="kobo.96.1">. </span><span class="koboSpan" id="kobo.96.2">Observe that the following plot is the same as the plot of the </span><kbd><span class="koboSpan" id="kobo.97.1">close</span></kbd><span class="koboSpan" id="kobo.98.1"> DataFrame in the preceding </span><em><span class="koboSpan" id="kobo.99.1">step 3</span></em><span class="koboSpan" id="kobo.100.1"> , however, the </span><em><span class="koboSpan" id="kobo.101.1">y</span></em><span class="koboSpan" id="kobo.102.1">-axis scale is now different:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.103.1"># Normalization</span><br/><span class="koboSpan" id="kobo.104.1">from sklearn.preprocessing import MinMaxScaler</span><br/><span class="koboSpan" id="kobo.105.1">def normalize(data):</span><br/><span class="koboSpan" id="kobo.106.1">    x = data.values.reshape(-1,1)</span><br/><span class="koboSpan" id="kobo.107.1">    pre_process = MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.108.1">    x_normalized = pre_process.fit_transform(x)</span><br/><span class="koboSpan" id="kobo.109.1">    return x_normalized</span><br/><br/><span class="koboSpan" id="kobo.110.1">x_norm = normalize(close)</span><br/><br/><span class="koboSpan" id="kobo.111.1">plt.figure(figsize= (10,10))</span><br/><span class="koboSpan" id="kobo.112.1">pd.DataFrame(x_norm, index = close.index).plot(label="Normalized Stock prices")</span><br/><span class="koboSpan" id="kobo.113.1">plt.legend(loc='best')</span><br/><span class="koboSpan" id="kobo.114.1">plt.show()</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.115.1"><img src="assets/ea95f634-5716-4007-9e07-689e5f4b1095.png" style="width:27.25em;height:19.50em;"/></span></p>
<ol start="2">
<li><span class="koboSpan" id="kobo.116.1">We define a </span><kbd><span class="koboSpan" id="kobo.117.1">window_transform()</span></kbd><span class="koboSpan" id="kobo.118.1"> function, which will convert the data series into a sequence of input-output pairs. </span><span class="koboSpan" id="kobo.118.2">For example, you want to construct an RNN that takes the previous five values as output and predicts the sixth value. </span><span class="koboSpan" id="kobo.118.3">Then, you choose </span><kbd><span class="koboSpan" id="kobo.119.1">window_size = 5</span></kbd><span class="koboSpan" id="kobo.120.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.121.1"># Create window from the normalized data</span><br/><span class="koboSpan" id="kobo.122.1">def window_transform(series, window_size):</span><br/><span class="koboSpan" id="kobo.123.1">    X = []</span><br/><span class="koboSpan" id="kobo.124.1">    y = []</span><br/><br/><span class="koboSpan" id="kobo.125.1">    # Generate a sequence input/output pairs from series</span><br/><span class="koboSpan" id="kobo.126.1">    # x= &lt;s1,s2,s3,s4,s5,... </span><span class="koboSpan" id="kobo.126.2">s_n&gt; y = s_n+1 and so on</span><br/><span class="koboSpan" id="kobo.127.1">    for i in range(len(series) - window_size):</span><br/><span class="koboSpan" id="kobo.128.1">    X.append(series[i:i+window_size])</span><br/><span class="koboSpan" id="kobo.129.1">    y.append(series[i+window_size])</span><br/><br/><span class="koboSpan" id="kobo.130.1">    # reshape each </span><br/><span class="koboSpan" id="kobo.131.1">    X = np.asarray(X)</span><br/><span class="koboSpan" id="kobo.132.1">    X.shape = (np.shape(X)[0:2])</span><br/><span class="koboSpan" id="kobo.133.1">    y = np.asarray(y)</span><br/><span class="koboSpan" id="kobo.134.1">    y.shape = (len(y),1)</span><br/><br/><span class="koboSpan" id="kobo.135.1">    return X,y</span><br/><br/><span class="koboSpan" id="kobo.136.1">window_size = 7</span><br/><span class="koboSpan" id="kobo.137.1">X,y = window_transform(x_norm,window_size = window_size)</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.138.1">Please refer to the GitHub repository, </span><kbd><span class="koboSpan" id="kobo.139.1">Chapter-12/time_series_data_preprocessing.ipynb</span></kbd><span class="koboSpan" id="kobo.140.1">, for the complete code of this section. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Preprocessing textual data</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Language plays a very important role in our daily life. </span><span class="koboSpan" id="kobo.2.2">For us, reading a written text is very natural, but what about computers? Can they read it? </span><span class="koboSpan" id="kobo.2.3">Can we make our deep learning models generate new text based on the old pattern? </span><span class="koboSpan" id="kobo.2.4">For example, if I say, "Yesterday, I had ____ at Starbucks," most of us will be able to guess that the blank space is coffee, but can our deep learning models do it? </span><span class="koboSpan" id="kobo.2.5">The answer is yes; we can train our deep learning models to guess the next word (or character). </span><span class="koboSpan" id="kobo.2.6">However, deep learning models run on computers, and computers understand only binary, only 0s and 1s. </span><span class="koboSpan" id="kobo.2.7">Hence, we need a way to process out textual data so that it can be converted in to a form that is easy for the computer to handle. </span><span class="koboSpan" id="kobo.2.8">Moreover, while cat or CAT or Cat have different ASCII representation, they mean the same; it is easy for us to see, but for models to take them as the same, we need to preprocess the textual data. </span><span class="koboSpan" id="kobo.2.9">This section will list the necessary preprocessing steps for the textual data, and you will learn how to do it in Python:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.3.1">For this section, we will consider a small text from my favorite science fiction novel, </span><em><span class="koboSpan" id="kobo.4.1">Foundation,</span></em><span class="koboSpan" id="kobo.5.1"> by Isaac Asimov. </span><span class="koboSpan" id="kobo.5.2">The text is in the </span><kbd><span class="koboSpan" id="kobo.6.1">foundation.txt</span></kbd><span class="koboSpan" id="kobo.7.1"> </span><span><span class="koboSpan" id="kobo.8.1">file</span></span><span class="koboSpan" id="kobo.9.1">. </span><span class="koboSpan" id="kobo.9.2">The first step is, we read in the text:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.10.1">f = open('foundation.txt')</span><br/><span class="koboSpan" id="kobo.11.1">text = f.read()</span><br/><span class="koboSpan" id="kobo.12.1">print(text)</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.13.1">The next step in text processing is cleaning the data. </span><span class="koboSpan" id="kobo.13.2">We retain only that part of the text that is relevant. </span><span class="koboSpan" id="kobo.13.3">In most cases, punctuation does not add any additional meaning to the text, so we can safely remove it:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.14.1"># clean data</span><br/><span class="koboSpan" id="kobo.15.1">import re</span><br/><span class="koboSpan" id="kobo.16.1"># remove Punctuation</span><br/><span class="koboSpan" id="kobo.17.1">text = re.sub(r"[^a-zA-Z0-9]", " ", text) </span><br/><span class="koboSpan" id="kobo.18.1">print(text)</span></pre>
<p class="mce-root"/>
<ol start="3">
<li><span class="koboSpan" id="kobo.19.1">After cleaning the data, we need to normalize the text. </span><span class="koboSpan" id="kobo.19.2">In text processing, normalizing the text means converting all text in to the same case, lowercase or uppercase. </span><span class="koboSpan" id="kobo.19.3">Conventionally, lowercase is preferred, so we convert the text in to lowercase:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.20.1"># Normalize text</span><br/><span class="koboSpan" id="kobo.21.1"># Convert to lowercase</span><br/><span class="koboSpan" id="kobo.22.1">text = text.lower() </span><br/><span class="koboSpan" id="kobo.23.1">print(text)</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.24.1">Once the text is normalized, the next step is tokenizing the text. </span><span class="koboSpan" id="kobo.24.2">We can tokenize a text in word tokens or sentence tokens. </span><span class="koboSpan" id="kobo.24.3">To do this, you can use either the split function or use the powerful NLTK module. </span><span class="koboSpan" id="kobo.24.4">If you do not have NLTK installed in your system, you can do it using </span><kbd><span class="koboSpan" id="kobo.25.1">pip install nltk</span></kbd><span class="koboSpan" id="kobo.26.1">. </span><span class="koboSpan" id="kobo.26.2">In the following, we use NLTK's word tokenizer to do the task:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.27.1">import os</span><br/><span class="koboSpan" id="kobo.28.1">import nltk</span><br/><span class="koboSpan" id="kobo.29.1">nltk.download('punkt') </span><br/><span class="koboSpan" id="kobo.30.1">from nltk.tokenize import word_tokenize</span><br/><br/><span class="koboSpan" id="kobo.31.1"># Split text into words using NLTK</span><br/><span class="koboSpan" id="kobo.32.1">words_nltk = word_tokenize(text)</span><br/><span class="koboSpan" id="kobo.33.1">print(words_nltk)</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.34.1">Depending on the type of text you have and the work you are doing, you will need to remove stop words. </span><span class="koboSpan" id="kobo.34.2">Stop words are words that are present in most text samples, and hence, do not add any information to the context or meaning of the text. </span><span class="koboSpan" id="kobo.34.3">For example, the, a, and an. </span><span class="koboSpan" id="kobo.34.4">You can declare your own stop words or use the stop words provided by NLTK. </span><span class="koboSpan" id="kobo.34.5">Here, we remove </span><kbd><span class="koboSpan" id="kobo.35.1">stopwords</span></kbd><span class="koboSpan" id="kobo.36.1"> of the </span><kbd><span class="koboSpan" id="kobo.37.1">english</span></kbd><span class="koboSpan" id="kobo.38.1"> language from our text:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.39.1">from nltk.corpus import stopwords</span><br/><span class="koboSpan" id="kobo.40.1">nltk.download('stopwords')</span><br/><span class="koboSpan" id="kobo.41.1">#Remove stop words</span><br/><span class="koboSpan" id="kobo.42.1">words = [w for w in words \</span><br/><span class="koboSpan" id="kobo.43.1">        if w not in stopwords.words("english")]</span><br/><br/></pre>
<p class="mce-root"/>
<ol start="6">
<li><span class="koboSpan" id="kobo.44.1">Another thing that you can perform on the textual data is stemming and lemmatization. </span><span class="koboSpan" id="kobo.44.2">These are used to convert the words into canonical form:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.45.1">from nltk.stem.porter import PorterStemmer</span><br/><br/><span class="koboSpan" id="kobo.46.1"># Reduce words to their stems</span><br/><span class="koboSpan" id="kobo.47.1">stemmed = [PorterStemmer().stem(w) for w in words]</span><br/><span class="koboSpan" id="kobo.48.1">print(stemmed)</span><br/><br/><span class="koboSpan" id="kobo.49.1">from nltk.stem.wordnet import WordNetLemmatizer</span><br/><br/><span class="koboSpan" id="kobo.50.1"># Reduce words to their root form</span><br/><span class="koboSpan" id="kobo.51.1">lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]</span><br/><span class="koboSpan" id="kobo.52.1">print(lemmed)</span></pre>
<p><span class="koboSpan" id="kobo.53.1">You can access the notebook with this code at GitHub: </span><kbd><span class="koboSpan" id="kobo.54.1">Chapter12/text_processing.ipynb</span></kbd><span class="koboSpan" id="kobo.55.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Data augmentation for images</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Python has OpenCV, which provides very good support for images. </span><span class="koboSpan" id="kobo.2.2">OpenCV can be downloaded from both Conda channels and PyPi for installation. </span><span class="koboSpan" id="kobo.2.3">Once the image is read using the OpenCV </span><kbd><span class="koboSpan" id="kobo.3.1">imread()</span></kbd><span class="koboSpan" id="kobo.4.1"> function, the image is represented as an array. </span><span class="koboSpan" id="kobo.4.2">In case the image is coloured, the channels are stored in BGR order. </span><span class="koboSpan" id="kobo.4.3">Each element of the array represents the intensity of the corresponding pixel value (the values lie in the range 0 to 255). </span></p>
<p><span class="koboSpan" id="kobo.5.1">Let's say you have trained a model to recognize a ball: you present it with a tennis ball, and it recognizes it as a ball. </span><span class="koboSpan" id="kobo.5.2">The next image of the ball that we present is taken after zooming: will our model still recognize it? </span><span class="koboSpan" id="kobo.5.3">A model is just as good as the dataset it has been trained on, and so, if the model while training had seen rescaled images, it will be easy for it to identify the zoomed ball as a ball. </span><span class="koboSpan" id="kobo.5.4">One way to ensure that such images are available in your dataset is to implicitly include such variable images, however, since images are represented as an array, we can perform mathematical transformations to rescale, flip, rotate, and even change intensities. </span><span class="koboSpan" id="kobo.5.5">The process of performing these transformations on existing training images to generate new images is called </span><strong><span class="koboSpan" id="kobo.6.1">data augmentation</span></strong><span class="koboSpan" id="kobo.7.1">. </span><span class="koboSpan" id="kobo.7.2">Another advantage of using data augmentation  is that you are able to increase the size of your training dataset (when used with data generators, we can get infinite images). </span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.8.1">Most deep learning libraries have standard APIs to perform data augmentation. </span><span class="koboSpan" id="kobo.8.2">In Keras (</span><a href="https://keras.io/preprocessing/image/"><span class="koboSpan" id="kobo.9.1">https://keras.io/preprocessing/image/</span></a><span class="koboSpan" id="kobo.10.1"> ), there is </span><kbd><span class="koboSpan" id="kobo.11.1">ImageDataGenerator</span></kbd><span class="koboSpan" id="kobo.12.1">, and in TensorFlow-TfLearn, we have </span><kbd><span class="koboSpan" id="kobo.13.1">ImageAugmentation</span></kbd><span class="koboSpan" id="kobo.14.1">. </span><span class="koboSpan" id="kobo.14.2">TensorFlow also has Ops to perform image conversions and transformations (</span><a href="https://www.tensorflow.org/api_guides/python/image"><span class="koboSpan" id="kobo.15.1">https://www.tensorflow.org/api_guides/python/image</span></a><span class="koboSpan" id="kobo.16.1">). </span><span class="koboSpan" id="kobo.16.2">Here we will see how we can use OpenCV's powerful library for data augmentation and create our own data generator:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.17.1">We import the necessary modules: OpenCV to read and process images, </span><kbd><span class="koboSpan" id="kobo.18.1">numpy</span></kbd><span class="koboSpan" id="kobo.19.1"> for matrix manipulations, Matplotlib to visualize images, </span><kbd><span class="koboSpan" id="kobo.20.1">shuffle</span></kbd><span class="koboSpan" id="kobo.21.1"> from scikit-learn for randomly shuffling the data, and Glob to find files within directories:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.22.1">import cv2 # for image reading and processsing</span><br/><span class="koboSpan" id="kobo.23.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.24.1">from glob import glob</span><br/><span class="koboSpan" id="kobo.25.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.26.1">from sklearn.utils import shuffle</span><br/><span class="koboSpan" id="kobo.27.1">%matplotlib inline</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.28.1">We read the necessary files. </span><span class="koboSpan" id="kobo.28.2">For this example, we downloaded some images of the previous President of the United States, Barack Obama, from Google image search:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.29.1">img_files = np.array(glob("Obama/*"))</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.30.1">We create a function that can randomly introduce any of the following distortions in the image: random rotation in the range 0–50 degrees, randomly change the intensity, randomly shift the image horizontally and vertically by up to 50 pixels, or randomly flip the image:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.31.1">def distort_image(img, rot = 50, shift_px = 40):</span><br/><span class="koboSpan" id="kobo.32.1">    """</span><br/><span class="koboSpan" id="kobo.33.1">    Function to introduce random distortion: brightness, flip,</span><br/><span class="koboSpan" id="kobo.34.1">    rotation, and shift </span><br/><span class="koboSpan" id="kobo.35.1">    """</span><br/><span class="koboSpan" id="kobo.36.1">    rows, cols,_ = img.shape</span><br/><span class="koboSpan" id="kobo.37.1">    choice = np.random.randint(5)</span><br/><span class="koboSpan" id="kobo.38.1">    #print(choice)</span><br/><span class="koboSpan" id="kobo.39.1">    if choice == 0: # Randomly rotate 0-50 degreee</span><br/><span class="koboSpan" id="kobo.40.1">        rot *= np.random.random() </span><br/><span class="koboSpan" id="kobo.41.1">        M = cv2.getRotationMatrix2D((cols/2,rows/2), rot, 1)</span><br/><span class="koboSpan" id="kobo.42.1">        dst = cv2.warpAffine(img,M,(cols,rows))</span><br/><span class="koboSpan" id="kobo.43.1">    elif choice == 1: # Randomly change the intensity</span><br/><span class="koboSpan" id="kobo.44.1">        hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)</span><br/><span class="koboSpan" id="kobo.45.1">        ratio = 1.0 + 0.4 * (np.random.rand() - 0.5)</span><br/><span class="koboSpan" id="kobo.46.1">        hsv[:, :, 2] = hsv[:, :, 2] * ratio</span><br/><span class="koboSpan" id="kobo.47.1">        dst = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)</span><br/><span class="koboSpan" id="kobo.48.1">    elif choice == 2: # Randomly shift the image in horizontal and vertical direction</span><br/><span class="koboSpan" id="kobo.49.1">        x_shift,y_shift = np.random.randint(-shift_px,shift_px,2)</span><br/><span class="koboSpan" id="kobo.50.1">        M = np.float32([[1,0,x_shift],[0,1,y_shift]])</span><br/><span class="koboSpan" id="kobo.51.1">        dst = cv2.warpAffine(img,M,(cols,rows))</span><br/><span class="koboSpan" id="kobo.52.1">    elif choice == 3: # Randomly flip the image</span><br/><span class="koboSpan" id="kobo.53.1">        dst = np.fliplr(img)</span><br/><span class="koboSpan" id="kobo.54.1">    else:</span><br/><span class="koboSpan" id="kobo.55.1">        dst = img</span><br/> <br/><span class="koboSpan" id="kobo.56.1">    return dst</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.57.1">In the following image, you can see the result of the preceding function on randomly chosen images from our dataset:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.58.1"><img class="aligncenter size-full wp-image-1129 image-border" src="assets/e35d972d-8a55-4be5-88d9-647b3aaee380.png" style="width:29.75em;height:37.08em;"/></span></p>
<p class="mce-root"/>
<ol start="5">
<li><span class="koboSpan" id="kobo.59.1">And finally, you can create a data generator using Python </span><kbd><span class="koboSpan" id="kobo.60.1">yield</span></kbd><span class="koboSpan" id="kobo.61.1"> to generate as many images as you want:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.62.1"># data generator</span><br/><span class="koboSpan" id="kobo.63.1">def data_generator(samples, batch_size=32, validation_flag = False):</span><br/><span class="koboSpan" id="kobo.64.1">    """</span><br/><span class="koboSpan" id="kobo.65.1">    Function to generate data after, it reads the image files, </span><br/><span class="koboSpan" id="kobo.66.1">    performs random distortions and finally </span><br/><span class="koboSpan" id="kobo.67.1">    returns a batch of training or validation data</span><br/><span class="koboSpan" id="kobo.68.1">    """</span><br/><span class="koboSpan" id="kobo.69.1">    num_samples = len(samples)</span><br/><span class="koboSpan" id="kobo.70.1">    while True: # Loop forever so the generator never terminates</span><br/><span class="koboSpan" id="kobo.71.1"> shuffle(samples)</span><br/><span class="koboSpan" id="kobo.72.1">        for offset in range(0, num_samples, batch_size):</span><br/><span class="koboSpan" id="kobo.73.1">            batch_samples = samples[offset:offset+batch_size]</span><br/><span class="koboSpan" id="kobo.74.1">            images = []</span><br/> <br/><span class="koboSpan" id="kobo.75.1">            for batch_sample in batch_samples:</span><br/><span class="koboSpan" id="kobo.76.1">                if validation_flag: # The validation data consists only of center image and without distortions</span><br/><span class="koboSpan" id="kobo.77.1">                    image = cv2.imread(batch_sample)</span><br/><span class="koboSpan" id="kobo.78.1">                    images.append(image)</span><br/><span class="koboSpan" id="kobo.79.1">                    continue</span><br/><span class="koboSpan" id="kobo.80.1">                else: # In training dataset we introduce distortions to augment it and improve performance</span><br/><span class="koboSpan" id="kobo.81.1">                    image = cv2.imread(batch_sample)</span><br/><span class="koboSpan" id="kobo.82.1">                    # Randomly augment the training dataset to reduce overfitting</span><br/><span class="koboSpan" id="kobo.83.1">                    image = distort_image(image)</span><br/><span class="koboSpan" id="kobo.84.1">                    images.append(image)</span><br/><br/><span class="koboSpan" id="kobo.85.1">        # Convert the data into numpy arrays</span><br/><span class="koboSpan" id="kobo.86.1">        X_train = np.array(images)</span><br/> <br/><span class="koboSpan" id="kobo.87.1">        yield X_train </span><br/><br/><span class="koboSpan" id="kobo.88.1">train_generator = data_generator(img_files,  batch_size=32)</span></pre>
<p><span class="koboSpan" id="kobo.89.1">The </span><kbd><span class="koboSpan" id="kobo.90.1">Chapter12/data_augmentation.ipynb</span></kbd><span class="koboSpan" id="kobo.91.1"> file contains the code for this section. </span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Handling videos files</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Videos are nothing but a collection of still images (frames), therefore, if we can extract images from the videos, we can apply our trusted CNN networks on the same. </span><span class="koboSpan" id="kobo.2.2">The only necessary thing to do is convert the video in to a list of frames:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.3.1">The first thing we import are the requisite modules. </span><span class="koboSpan" id="kobo.3.2">We will need OpenCV to read the video and convert it in to frames. </span><span class="koboSpan" id="kobo.3.3">We will also need the </span><kbd><span class="koboSpan" id="kobo.4.1">math</span></kbd><span class="koboSpan" id="kobo.5.1"> module for basic mathematical operations and Matplotlib for visualizing the frames:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.6.1">import cv2 # for capturing videos</span><br/><span class="koboSpan" id="kobo.7.1">import math # for mathematical operations</span><br/><span class="koboSpan" id="kobo.8.1">import matplotlib.pyplot as plt # for plotting the images</span><br/><span class="koboSpan" id="kobo.9.1">%matplotlib inline</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.10.1">We read the video file using the OpenCV function and get its frame rate by using the property identifier, </span><kbd><span class="koboSpan" id="kobo.11.1">5</span></kbd><span class="koboSpan" id="kobo.12.1"> (</span><a href="https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get"><span class="koboSpan" id="kobo.13.1">https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get</span></a><span class="koboSpan" id="kobo.14.1">):</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.15.1">videoFile = "video.avi" # Video file with complete path</span><br/><span class="koboSpan" id="kobo.16.1">cap = cv2.VideoCapture(videoFile) # capturing the video from the given path</span><br/><span class="koboSpan" id="kobo.17.1">frameRate = cap.get(5) #frame rate</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.18.1">We loop through all of the frames of the video one by one using the </span><kbd><span class="koboSpan" id="kobo.19.1">read()</span></kbd><span class="koboSpan" id="kobo.20.1"> function. </span><span class="koboSpan" id="kobo.20.2">Although we read only one frame at a time, we save only the first frame in each second. </span><span class="koboSpan" id="kobo.20.3">This way, we can cover the whole video, and yet reduce the data size:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.21.1">count = 0</span><br/><span class="koboSpan" id="kobo.22.1">while(cap.isOpened()):</span><br/><span class="koboSpan" id="kobo.23.1">    frameId = cap.get(1) #current frame number</span><br/><span class="koboSpan" id="kobo.24.1">    ret, frame = cap.read()</span><br/><span class="koboSpan" id="kobo.25.1">    if (ret != True):</span><br/><span class="koboSpan" id="kobo.26.1">        break</span><br/><span class="koboSpan" id="kobo.27.1">    if (frameId % math.floor(frameRate) == 0):</span><br/><span class="koboSpan" id="kobo.28.1">        filename ="frame%d.jpg" % count</span><br/><span class="koboSpan" id="kobo.29.1">        count += 1</span><br/><span class="koboSpan" id="kobo.30.1">        cv2.imwrite(filename, frame)</span><br/><br/><span class="koboSpan" id="kobo.31.1">cap.release()</span><br/><span class="koboSpan" id="kobo.32.1">print ("Finished!")</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.33.1">Let's visualize the fifth frame that we saved:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.34.1">img = plt.imread('frame5.jpg') # reading image using its name</span><br/><span class="koboSpan" id="kobo.35.1">plt.imshow(img)</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.36.1"><img class="aligncenter size-full wp-image-1130 image-border" src="assets/7aa195c7-3649-451f-ad8c-5a1145c049e1.png" style="width:31.25em;height:23.58em;"/></span></p>
<p><span class="koboSpan" id="kobo.37.1">The video file for this code was taken from the site maintained by Ivan Laptev and Barbara Caputo (</span><a href="http://www.nada.kth.se/cvap/actions/"><span class="koboSpan" id="kobo.38.1">http://www.nada.kth.se/cvap/actions/</span></a><span class="koboSpan" id="kobo.39.1">). </span><span class="koboSpan" id="kobo.39.2">The code is available at GitHub: </span><kbd><span class="koboSpan" id="kobo.40.1">Chapter12/Video_to_frames.ipynb</span></kbd><span class="koboSpan" id="kobo.41.1">. </span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.42.1">One of the best papers that uses CNN for classifying videos is </span><span><em><span class="koboSpan" id="kobo.43.1">Large-scale Video Classification with Convolutional Neural Networks</span></em><span class="koboSpan" id="kobo.44.1"> by Andrej Karpathy et al.. </span><span class="koboSpan" id="kobo.44.2">You can access here: </span><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.html"><span class="koboSpan" id="kobo.45.1">https://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.html</span></a><span class="koboSpan" id="kobo.46.1">.</span></span></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Audio files as input data</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Another interesting data type is audio files. </span><span class="koboSpan" id="kobo.2.2">Models that convert speech in to text or classify audio sounds take as input audio files. </span><span class="koboSpan" id="kobo.2.3">If you want to work with audio files, then you will need the </span><kbd><span class="koboSpan" id="kobo.3.1">librosa</span></kbd><span class="koboSpan" id="kobo.4.1"> module. </span><span class="koboSpan" id="kobo.4.2">There are many ways to treat an audio file; we can convert it into a time series and use a recurrent network. </span><span class="koboSpan" id="kobo.4.3">Another way that has given good results is to use them as one-dimensional or two-dimensional patterns, and train a CNN to classify them. </span><span class="koboSpan" id="kobo.4.4">Some good papers that adopt this approach are as follows:</span></p>
<ul>
<li class="gs_citr">
<p><span class="koboSpan" id="kobo.5.1">Hershey, S., Chaudhuri, S., Ellis, D. </span><span class="koboSpan" id="kobo.5.2">P., Gemmeke, J. </span><span class="koboSpan" id="kobo.5.3">F., Jansen, A., Moore, R. </span><span class="koboSpan" id="kobo.5.4">C., and Slaney, M. </span><span class="koboSpan" id="kobo.5.5">(2017, March). </span><em><span class="koboSpan" id="kobo.6.1">CNN architectures for large-scale audio classification.</span></em><span class="koboSpan" id="kobo.7.1"> In Acoustics, Speech, and Signal Processing (ICASSP), 2017 IEEE International Conference on (pp. </span><span class="koboSpan" id="kobo.7.2">131-135). </span><span class="koboSpan" id="kobo.7.3">IEEE.</span></p>
</li>
<li class="gs_citr">
<p><span class="koboSpan" id="kobo.8.1">Palaz, D., Magimai-Doss, M., and Collobert, R. </span><span class="koboSpan" id="kobo.8.2">(2015). </span><em><span class="koboSpan" id="kobo.9.1">Analysis of CNN-based speech recognition system using raw speech as input</span></em><span class="koboSpan" id="kobo.10.1">. </span><span class="koboSpan" id="kobo.10.2">In Sixteenth Annual Conference of the International Speech Communication Association.</span></p>
</li>
<li>
<p class="gs_citr"><span class="koboSpan" id="kobo.11.1">Zhang, H., McLoughlin, I., and Song, Y. </span><span class="koboSpan" id="kobo.11.2">(2015, April). </span><em><span class="koboSpan" id="kobo.12.1">Robust sound event recognition using convolutional neural networks</span></em><span class="koboSpan" id="kobo.13.1">. </span><span class="koboSpan" id="kobo.13.2">In Acoustics, Speech, and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. </span><span class="koboSpan" id="kobo.13.3">559-563). </span><span class="koboSpan" id="kobo.13.4">IEEE.</span></p>
</li>
<li>
<p class="gs_citr"><span class="koboSpan" id="kobo.14.1">Costa, Y. </span><span class="koboSpan" id="kobo.14.2">M., Oliveira, L. </span><span class="koboSpan" id="kobo.14.3">S., and Silla Jr, C. </span><span class="koboSpan" id="kobo.14.4">N. </span><span class="koboSpan" id="kobo.14.5">(2017). </span><em><span class="koboSpan" id="kobo.15.1">An evaluation of convolutional neural networks for music classification using spectrograms</span></em><span class="koboSpan" id="kobo.16.1">. </span><span class="koboSpan" id="kobo.16.2">Applied soft computing, 52, 28–38.</span></p>
</li>
</ul>
<p><span class="koboSpan" id="kobo.17.1">We will use the </span><kbd><span class="koboSpan" id="kobo.18.1">librosa</span></kbd><span class="koboSpan" id="kobo.19.1"> module to read an audio file and convert it in to a one-dimensional sound pattern and two-dimensional spectrogram. </span><span class="koboSpan" id="kobo.19.2">You can install </span><kbd><span class="koboSpan" id="kobo.20.1">librosa</span></kbd><span class="koboSpan" id="kobo.21.1"> in your Anaconda environment using the following:</span></p>
<pre><span class="koboSpan" id="kobo.22.1">pip install librosa</span></pre>
<ol>
<li><span class="koboSpan" id="kobo.23.1">Here, we will import </span><kbd><span class="koboSpan" id="kobo.24.1">numpy</span></kbd><span class="koboSpan" id="kobo.25.1">, </span><kbd><span class="koboSpan" id="kobo.26.1">matplotlib</span></kbd><span class="koboSpan" id="kobo.27.1">, and </span><kbd><span class="koboSpan" id="kobo.28.1">librosa</span></kbd><span class="koboSpan" id="kobo.29.1">. </span><span class="koboSpan" id="kobo.29.2">We will take the example audio file from the </span><kbd><span class="koboSpan" id="kobo.30.1">librosa</span></kbd><span class="koboSpan" id="kobo.31.1"> datasets:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.32.1">import librosa</span><br/><span class="koboSpan" id="kobo.33.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.34.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.35.1">%matplotlib inline</span><br/><span class="koboSpan" id="kobo.36.1"># Get the file path to the included audio example</span><br/><span class="koboSpan" id="kobo.37.1">filename = librosa.util.example_audio_file()</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li><span class="koboSpan" id="kobo.38.1">The </span><kbd><span class="koboSpan" id="kobo.39.1">librosa</span></kbd><span class="koboSpan" id="kobo.40.1"> load function returns the audio data as time series represented as a one-dimensional NumPy floating-point array. </span><span class="koboSpan" id="kobo.40.2">We can use them as time series or even as a one-dimensional pattern for a CNN:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.41.1">input_length=16000*4</span><br/><span class="koboSpan" id="kobo.42.1">def audio_norm(data):</span><br/><span class="koboSpan" id="kobo.43.1">    # Function to Normalize</span><br/><span class="koboSpan" id="kobo.44.1">    max_data = np.max(data)</span><br/><span class="koboSpan" id="kobo.45.1">    min_data = np.min(data)</span><br/><span class="koboSpan" id="kobo.46.1">    data = (data-min_data)/(max_data-min_data) </span><br/><span class="koboSpan" id="kobo.47.1">    return data</span><br/><br/><span class="koboSpan" id="kobo.48.1">def load_audio_file(file_path, </span><br/><span class="koboSpan" id="kobo.49.1">            input_length=input_length):</span><br/><span class="koboSpan" id="kobo.50.1">    # Function to load an audio file and </span><br/><span class="koboSpan" id="kobo.51.1">    # return a 1D numpy array </span><br/><span class="koboSpan" id="kobo.52.1">    data, sr = librosa.load(file_path, sr=None)</span><br/> <br/><span class="koboSpan" id="kobo.53.1">    max_offset = abs(len(data)-input_length)</span><br/><span class="koboSpan" id="kobo.54.1">    offset = np.random.randint(max_offset)</span><br/><span class="koboSpan" id="kobo.55.1">    if len(data)&gt;input_length:</span><br/><span class="koboSpan" id="kobo.56.1">        data = data[offset:(input_length+offset)]</span><br/><span class="koboSpan" id="kobo.57.1">    else:</span><br/><span class="koboSpan" id="kobo.58.1">        data = np.pad(data, (offset, </span><br/><span class="koboSpan" id="kobo.59.1">            input_size - len(data) - offset), </span><br/><span class="koboSpan" id="kobo.60.1">            "constant")    </span><br/> <br/><span class="koboSpan" id="kobo.61.1">    data = audio_norm(data)</span><br/><span class="koboSpan" id="kobo.62.1">    return data</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.63.1">In the following, you can see the one-dimensional audio wave pattern after normalization:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.64.1">data_base = load_audio_file(filename)</span><br/><span class="koboSpan" id="kobo.65.1">fig = plt.figure(figsize=(14, 8))</span><br/><span class="koboSpan" id="kobo.66.1">plt.title('Raw wave ')</span><br/><span class="koboSpan" id="kobo.67.1">plt.ylabel('Amplitude')</span><br/><span class="koboSpan" id="kobo.68.1">plt.plot(np.linspace(0, 1, input_length), data_base)</span><br/><span class="koboSpan" id="kobo.69.1">plt.show()</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.70.1"><img class="aligncenter size-full wp-image-1131 image-border" src="assets/058703e1-2a99-474e-b3d6-6aa9d4e0d830.png" style="width:69.58em;height:40.17em;"/></span></p>
<ol start="4">
<li><kbd><span class="koboSpan" id="kobo.71.1">librosa</span></kbd><span class="koboSpan" id="kobo.72.1"> also has a </span><kbd><span class="koboSpan" id="kobo.73.1">melspectrogram</span></kbd><span class="koboSpan" id="kobo.74.1"> function that we can use to form a mel spectrogram, which can be used as a two-dimensional image for a CNN:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.75.1">def preprocess_audio_mel_T(audio, sample_rate=16000, </span><br/><span class="koboSpan" id="kobo.76.1">        window_size=20, #log_specgram</span><br/><span class="koboSpan" id="kobo.77.1">        step_size=10, eps=1e-10):</span><br/><br/><span class="koboSpan" id="kobo.78.1">    mel_spec = librosa.feature.melspectrogram(y=audio,</span><br/><span class="koboSpan" id="kobo.79.1">             sr=sample_rate, n_mels= 256)</span><br/><span class="koboSpan" id="kobo.80.1">    mel_db = (librosa.power_to_db(mel_spec,</span><br/><span class="koboSpan" id="kobo.81.1">         ref=np.max) + 40)/40</span><br/><span class="koboSpan" id="kobo.82.1">    return mel_db.T</span><br/><br/><br/><span class="koboSpan" id="kobo.83.1">def load_audio_file2(file_path,</span><br/><span class="koboSpan" id="kobo.84.1">             input_length=input_length):</span><br/><span class="koboSpan" id="kobo.85.1">    #Function to load the audio file  </span><br/><span class="koboSpan" id="kobo.86.1">    data, sr = librosa.load(file_path, sr=None)</span><br/> <br/><span class="koboSpan" id="kobo.87.1">    max_offset = abs(len(data)-input_length)</span><br/><span class="koboSpan" id="kobo.88.1">    offset = np.random.randint(max_offset)</span><br/><span class="koboSpan" id="kobo.89.1">    if len(data)&gt;input_length:</span><br/><span class="koboSpan" id="kobo.90.1">        data = data[offset:(input_length+offset)]</span><br/><span class="koboSpan" id="kobo.91.1">    else:</span><br/><span class="koboSpan" id="kobo.92.1">        data = np.pad(data, (offset, </span><br/><span class="koboSpan" id="kobo.93.1">            input_size - len(data) - offset),</span><br/><span class="koboSpan" id="kobo.94.1">            "constant")</span><br/> <br/><span class="koboSpan" id="kobo.95.1">    data = preprocess_audio_mel_T(data, sr)</span><br/><span class="koboSpan" id="kobo.96.1">    return data</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.97.1">Here is a mel spectrogram of the same audio signal:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.98.1">data_base = load_audio_file2(filename)</span><br/><span class="koboSpan" id="kobo.99.1">print(data_base.shape)</span><br/><span class="koboSpan" id="kobo.100.1">fig = plt.figure(figsize=(14, 8))</span><br/><span class="koboSpan" id="kobo.101.1">plt.imshow(data_base)</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.102.1"><img class="aligncenter size-full wp-image-1132 image-border" src="assets/d3217b50-224b-4b88-8a53-957f64dd20fb.png" style="width:68.75em;height:34.92em;"/></span></p>
<p><span class="koboSpan" id="kobo.103.1">You can find the code file for the example in the GitHub repository under the </span><kbd><span class="koboSpan" id="kobo.104.1">Chapter12/audio_processing.ipynb</span></kbd><span class="koboSpan" id="kobo.105.1"> file.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Computing in the cloud</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Applying AI algorithms to IoT-generated data requires computing resources. </span><span class="koboSpan" id="kobo.2.2">With the availability of a large number of cloud platforms offering service at competitive prices, cloud computing offers a cost-effective solution. </span><span class="koboSpan" id="kobo.2.3">Out of the many cloud platforms available today, we will talk about three main cloud platform providers that occupy the majority of the market share: </span><strong><span class="koboSpan" id="kobo.3.1">Amazon Web Service</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">AWS</span></strong><span class="koboSpan" id="kobo.6.1">), </span><strong><span class="koboSpan" id="kobo.7.1">Google Cloud Platform</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong><span class="koboSpan" id="kobo.9.1">GCP</span></strong><span class="koboSpan" id="kobo.10.1">), and Microsoft Azure.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">AWS</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Amazon offers almost every feature under the cloud, from a cloud database, to cloud computing resources, to even cloud analytics. </span><span class="koboSpan" id="kobo.2.2">It even provides space to build a secure data lake. </span><span class="koboSpan" id="kobo.2.3">Its IoT core allows users to connect devices to the cloud. </span><span class="koboSpan" id="kobo.2.4">It provides a single dashboard that can be used to control the services you sign for. </span><span class="koboSpan" id="kobo.2.5">It charges per hour for its services. </span><span class="koboSpan" id="kobo.2.6">It has been offering these services for almost 15 years. </span><span class="koboSpan" id="kobo.2.7">Amazon continuously upgrades the service providing a better user experience. </span><span class="koboSpan" id="kobo.2.8">You can learn more about AWS from its site: </span><a href="https://aws.amazon.com/"><span class="koboSpan" id="kobo.3.1">https://aws.amazon.com/</span></a><span class="koboSpan" id="kobo.4.1">. </span></p>
<p><span class="koboSpan" id="kobo.5.1">It allows new users to make use of many of its services for free for one whole year. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Google Cloud Platform</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Google Cloud Platform (</span><a href="https://cloud.google.com/"><span class="koboSpan" id="kobo.3.1">https://cloud.google.com/</span></a><span class="koboSpan" id="kobo.4.1">) also offers a myriad of services. </span><span class="koboSpan" id="kobo.4.2">It offers cloud computing, data analytics, data storage, and even cloud AI products that provide users with pre-trained models and service to generate their own tailored models. </span><span class="koboSpan" id="kobo.4.3">The platform allows you to pay per minute. </span><span class="koboSpan" id="kobo.4.4">It offers enterprise-level secure services. </span><span class="koboSpan" id="kobo.4.5">The Google Cloud console is the one place stop to access and control all of your GCP services. </span><span class="koboSpan" id="kobo.4.6">GCP offers $300 credit for the first year, which allows you to access all of its services for free.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Microsoft Azure</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Microsoft Azure offers a wide variety of cloud services too. </span><span class="koboSpan" id="kobo.2.2">The best part of Microsoft Cloud services (</span><a href="https://azure.microsoft.com/en-in/"><span class="koboSpan" id="kobo.3.1">https://azure.microsoft.com/en-in/</span></a><span class="koboSpan" id="kobo.4.1">) is its ease of use; you can integrate it easily with available Microsoft tools. </span><span class="koboSpan" id="kobo.4.2">It claims to be five times less expensive compared to AWS. </span><span class="koboSpan" id="kobo.4.3">Like AWS and GCP, Azure also offers a one-year free trial worth $200 credits.</span></p>
<p><span class="koboSpan" id="kobo.5.1">You can use these cloud services to develop, test, and deploy your applications. </span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">This chapter focused on providing the reader with tools to handle different types of data and how to prepare them for the deep learning models. </span><span class="koboSpan" id="kobo.2.2">We started with time series data. </span><span class="koboSpan" id="kobo.2.3">This chapter next detailed how textual data needs to be preprocessed. </span><span class="koboSpan" id="kobo.2.4">This chapter showed how to perform data augmentation, an important technique for image classification and object detection. </span><span class="koboSpan" id="kobo.2.5">We next moved on to handling video; we show how to form image frames from a video. </span><span class="koboSpan" id="kobo.2.6">Next, this chapter covered audio files; we formed a time series and mel spectrogram from an audio file. </span><span class="koboSpan" id="kobo.2.7">Finally, we moved on to cloud platforms and discussed the features and services provided by three major cloud service providers.</span></p>


            </article>

            
        </section>
    </body></html>