- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure OpenAI Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we talked about **large language models** (**LLMs**),
    LLM concepts, and different enterprise-ready LLM examples. We also talked about
    foundation model concepts and discussed different use cases for LLMs. In this
    chapter, we’re going to dive into **Azure OpenAI** (**AOAI**) Service, different
    model types, how to deploy models, and various pricing aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will navigate the following sections in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is AOAI Service?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOAI model types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing AOAI Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating AOAI resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying AOAI models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing AOAI models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pricing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But before we get into all that, let’s understand the Microsoft and OpenAI partnership
    a bit better.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft has made a multi-billion-dollar investment into OpenAI to make sure
    they can develop advanced AI technology and share its benefits with everyone.
    This partnership builds upon Microsoft’s previous investments in 2019 and 2021\.
    This allows both Microsoft and OpenAI to use the advanced AI technology they create
    for their businesses. Microsoft is also invested in powerful supercomputers to
    help OpenAI with their important AI research.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft has formed a strategic partnership with OpenAI, integrating their
    advanced LLMs while doing so. They use OpenAI’s models in various products and
    are creating innovative digital experiences. Microsoft has a service called AOAI
    where developers can harness the power of cutting-edge models, combined with Microsoft’s
    robust tools. Microsoft is the only company providing cloud services for OpenAI,
    which means they will handle all the computing work for OpenAI’s research, products,
    and services to build sophisticated AI applications. This collaboration enables
    the creation of transformative solutions, unlocking new possibilities in AI-driven
    development. In the next section, we’ll dive deeper into AOAI Service and how
    to use it.
  prefs: []
  type: TYPE_NORMAL
- en: What is AOAI Service?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Microsoft offers a wide range of AI tools and solutions to help customers at
    every stage of their AI journey, regardless of their team’s expertise. Whether
    you’re new to AI or have specific use cases in mind, Microsoft has you covered.
    They provide easy-to-use options for those starting while also supporting data
    scientists with more advanced needs. When you explore the Azure AI offerings,
    you have the freedom to start with high-level AI services and dig deeper into
    the Azure Machine Learning platform to build, train, tune, and deploy deep learning
    models at scale. The overall Azure AI stack is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Azure AI stack](img/B21019_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Azure AI stack
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top layer, you have AI services for specific applications, such as **Cognitive
    Search**, **Bot Service**, and **Document Intelligence**. There are also domain-specific
    pretrained models such as **Vision**, **Speech**, **Language**, **Decision**,
    and **Azure OpenAI Service**. These are built on the foundation of Azure Machine
    Learning as a managed endpoint. If you’re using AI services, you don’t need to
    worry about the foundation layer. However, if you want to access the foundation
    layer so that you have control, you can use the bottom layer: **Azure Machine
    Learning**. It’s a managed end-to-end machine learning platform for building,
    training, deploying, and operating machine learning models responsibly and securely
    at scale.'
  prefs: []
  type: TYPE_NORMAL
- en: AOAI Service (marked in a red rectangular box in *Figure 2**.1*) is a new Azure
    AI Service that provides REST API access to OpenAI’s powerful language models,
    including the GPT-4 Turbo, GPT-4o, GPT4-o mini, GPT-3.5 Turbo, Whisper, DALL-E
    3, and Embeddings model series. They have enterprise capabilities such as security,
    private networking, compliance, regional availability, and responsible AI content
    filtering that are available only on Microsoft Azure. These models can be easily
    adapted for a variety of tasks, including but not limited to content generation,
    summarization, semantic search, and natural language to code translation. Users
    can access the various services through REST APIs, the Python SDK, or a web-based
    interface in Azure AI Foundry.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s talk about the different model types in AOAI.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI model types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AOAI Service offers various models that can do different things and have different
    costs. Let’s dive into the different model types and their usability:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT base**: GPT base models can comprehend and produce both natural language
    and code but lack specific training in following instructions. They’re designed
    to serve as alternatives to original GPT-3 base models and rely on the legacy
    Completions API. For most users, we recommend utilizing GPT-3.5 or GPT-4 for their
    tasks. These models come in two different variations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Babbage-002**: Serves as a replacement for the GPT-3 ada and babbage base
    models. It can support up to 16,384 tokens and can be fine-tuned.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Davinci-002**: Serves as a replacement for the GPT-3 curie and davinci base
    models. It can support up to 16,384 tokens and can be fine-tuned.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-4**: This is the latest model and is used for solving complex problems.
    It’s even more accurate than any of OpenAI’s earlier models. GPT4 models are capable
    of understanding and producing both natural language and coden along with advanced
    reasoning capabilities. It’s an optimized chat completion model, which means it’s
    best suited for interactive chat applications and performs exceptionally well
    with regular completion tasks. Being a large, highly optimized model, GPT-4 is
    capable of excelling at both interactive chats as well as completion activities.
    The latest GPT-4 class has three types of flagship models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-4 Turbo**: This model supports a maximum of 128,000 input tokens/context
    window and 4,096 output tokens'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-4o**: This model supports a maximum of 128,000 input tokens/context window
    and 4,096 output tokens'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-4o mini**: This model supports a maximum of 128,000 input tokens/context
    window and 16,384 output tokens'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-3.5**: GPT-3.5 represents a series of models that build upon the capabilities
    of GPT-3\. These models excel at comprehending and generating both human language
    and computer code. Among the GPT-3.5 models, the most capable and cost-efficient
    one is GPT-3.5 Turbo. It’s specifically fine-tuned for interactive conversations
    and performs well when it comes to regular completion tasks. The latest version
    of GPT-3.5 also comes in two different flavors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gpt-35-turbo-1106**: This model supports a maximum of 16,385 input tokens/context
    window and 4,096 output tokens.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**gpt-35-turbo-0125**: This model supports a maximum of 16,385 input tokens/context
    window and 4,096 output tokens.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**isgpt-35-turbo-instruct**: This model supports a maximum of 4,097 input tokens/context
    window. This model cannot be fine-tuned.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3.5 Turbo Instruct offers comparable capabilities to text-davinci-003 but
    utilizes the Completions API, not the Chat Completions API. We strongly advise
    utilizing GPT-3.5 Turbo and GPT-3.5 Turbo Instruct rather than the older GPT-3.5
    and GPT-3 models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an interactive chat example that can employ either GPT-3.5 or GPT-4\.
    Typically, you input a prompt as the user, and the model responds to the completion
    (model output). This can be a continuous exchange in conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: Basic prompt and completion example](img/B21019_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Basic prompt and completion example'
  prefs: []
  type: TYPE_NORMAL
- en: Along with prompt completion, there’s one more concept you need to understand
    about tokens. When you send a prompt to GPT-3.5 or GPT-4, it undergoes tokenization
    through the embedding process, where words or – more commonly – parts of words
    are converted into numeric vector representations. Numeric tokens are used instead
    of full words or sentences to process the information. This design allows the
    GPT model to handle relatively large volumes of text. However, there is a constraint
    regarding tokens in GPT-3.5 and GPT-4 (depending on which model type you choose,
    as mentioned previously) for both the input prompt and the generated completion
    combined.
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure you stay within the token limit, you can estimate the number of tokens
    required for your prompt and the resulting completion. As a rough guideline, in
    English, every four characters typically correspond to one token. Therefore, you
    can calculate the tokens needed by adding the character count of your prompt to
    the desired response length and dividing the total by four. This calculation provides
    you with a rough estimate of the token count required, which is valuable for planning
    tasks where token constraints must be kept in mind. *Figure 2**.3* shows an example
    of the tokens for a given sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: Token counter](img/B21019_02_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Token counter'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, there are a total of seven tokens. Notably, the word “generative”
    is represented by two distinct tokens, while the remaining words are each represented
    by a single token.
  prefs: []
  type: TYPE_NORMAL
- en: '**Embedding models**: An embedding is a list of vectors of floating-point numbers.
    When we measure how far apart two of these vectors are, it tells us how similar
    or different they are. If the distance is small, it means they’re very similar,
    but if it’s large, it means they’re quite different. When you input raw text into
    an embedding model, it produces a list of vector representations for the provided
    text:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.4: Basic embedding process](img/B21019_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: Basic embedding process'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, you provide a collection of document words to an embedding model
    as input, which then uses them to create an embedding vector. This vector is usually
    stored in a vector database, such as Azure Cognitive Search or Azure Cosmos DB.
    Subsequently, when a user submits a query, it passes through the same embedding
    model to generate a query vector, which is used to search for similar vectors
    in the vector database. This pattern is called **retrieval-augmented** **generation**
    (**RAG**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: Document embedding process](img/B21019_02_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Document embedding process'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, AOAI service offers four different types of embedding
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**text-embedding-ada-002 (version 1)**: This version uses the GPT-2/GPT-3 tokenizer.
    It can handle a maximum of **2,046** input tokens and provides an output with
    **1,024** dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text-embedding-ada-002 (version 2)**: This version uses the cl100k_base tokenizer.
    It supports a four times larger input with a maximum of **8,191** tokens and returns
    an output with **1,536** dimensions. This is the second-generation embedding model.
    We highly recommend using text-embedding-ada-002 version 2 as it provides parity
    with OpenAI’s text-embedding-ada-002 model in terms of its capabilities and performance.
    This model is not only more cost-effective but also simpler and more efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text-embedding-3-small**: The new text-embedding-3-small model significantly
    outperforms its predecessor, text-embedding-ada-002, with an increase in benchmark
    scores from 31.4% to 44.0% for multi-language retrieval and from 61.0% to 62.3%
    for English tasks. Additionally, it is five times more cost-effective, reducing
    the price from $0.0001 to $0.00002 per 1k tokens. While text-embedding-ada-002
    will remain available, text-embedding-3-small is recommended for its improved
    efficiency and performance. A new model, text-embedding-3-large, offers even greater
    capacity with embeddings for up to 3,072 dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text-embedding-3-large**: The new text-embedding-3-large model is the top-performing
    embedding model, showing substantial improvements over text-embedding-ada-002\.
    It achieves an average score of 54.9% on the MIRACL benchmark and 64.6% on the
    MTEB benchmark, up from 31.4% and 61.0%, respectively. Priced at $0.00013 per
    1k tokens, text-embedding-3-large offers the highest performance among other embedding
    models, surpassing both text-embedding-3-small and text-embedding-ada-002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To create a generative AI application, the key models you’ll primarily need
    are the ones mentioned here. It’s important to note that these models have only
    been trained on an extensive dataset that goes up to September 2021\. Therefore,
    they don’t possess any knowledge or information beyond that date. If you want
    to create a generative AI application that incorporates the most up-to-date information,
    you can explore a technique known as **RAG**. We’ll explore this technique in
    more detail later in this book, providing hands-on tutorials and in-depth discussions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are additional models available as part of AOAI Service, including the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DALL-E 3**: This model can generate realistic images and artwork based on
    a description provided in natural language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Whisper**: This is a general-purpose speech recognition (ASR) model. It has
    been trained on 680,000 hours of a vast and varied dataset containing both audio
    and text, and it can handle various tasks, such as recognizing speech in multiple
    languages, translating spoken words, and identifying languages. Whisper utilizes
    an encoder-decoder architecture based on Transformers, allowing it to convert
    spoken words into written text, and it can also handle special tokens to indicate
    the task or language involved. This model can be accessed through AOAI Service
    or Azure Speech.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve discussed various types of models, let’s discuss how to get access
    to AOAI Service.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing AOAI Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To access AOAI, you need to have an Azure account with an active subscription
    and AOAI access enabled. This section we will walk you through how to get AOAI
    Service access.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1: Create an* *Azure account.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing, AOAI access can only be approved for enterprise customers
    and partners. So, technically, you can create an Azure account using one of four
    options ([https://learn.microsoft.com/en-us/dotnet/azure/create-azure-account](https://learn.microsoft.com/en-us/dotnet/azure/create-azure-account)).
    However, when creating an account, you must use your company email address; personal
    email accounts will not be approved for AOAI access:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Option 1**: Visual Studio subscribers can use monthly Azure credits. This
    option allows you to activate your credits and use them for AOAI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Option 2**: Sign up for a free Azure account. This option gives you 12 months
    of free services and $200 credit to explore Azure for 30 days. You can use this
    credit for AOAI if you’ve been approved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Option 3**: Sign up for a pay-as-you-go account. This option charges you
    for what you use beyond the free limits. You can pay for AOAI with this option
    if you’ve been approved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Option 4**: Use a corporate account. This option requires your company to
    have a cloud administrator who can grant you access to AOAI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you create an Azure account, you will get a default Azure subscription.
    You can use that or create a new one to access AOAI:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find your subscription information under the **Subscriptions** resource:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.6: Locating Subscriptions](img/B21019_02_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Locating Subscriptions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note down the **Subscription ID** value; you’ll need it to apply for AOAI access:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.7: Retrieving subscriptions details](img/B21019_02_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Retrieving subscriptions details'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2: Apply* *AOAI access.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply AOAI access, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: AOAI is a restricted general availability service. Typically, you will have
    access to create AOAI resources in your subscriptions. If you don’t have access,
    you must complete the form provided at [https://aka.ms/oaiapply](https://aka.ms/oaiapply).
    Please ensure that you use your company email address when filling out this form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upon completing the form, it will take approximately 3-4 days for it to be approved.
    Once your access is approved, you will receive an email from the Azure Cognitive
    Service Team.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This approval will allow you to access and use AOAI models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With that, we’ve covered how to obtain access to AOAI Service. Now, let’s turn
    our attention to how to utilize these models effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Creating AOAI resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After gaining access to AOAI models, you can create an AOAI resource to deploy
    the models. This section will guide you through the process of creating AOAI resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the Azure subscription that you created previously within the Azure
    portal ([https://portal.azure.com/](https://portal.azure.com/)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to `Azure OpenAI`. Once you find the service, click **Create**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF8: Creating an AOAI resource](img/B21019_02_8.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Creating an AOAI resource'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the AOAI **Creation** page, provide the subsequent details within the **Basics**
    tab’s fields:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **Field** | **Description** |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **Subscriptions** | The Azure subscription mentioned in your application
    for onboarding AOAI Service. |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **Resource Group** | The Azure resource group that will contain your AOAI
    resource. You can create a new group or use a pre-existing one. |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **Region** | The geographical location of your instance. Varying locations
    can potentially introduce latency but won’t impact the operational availability
    of your resource. |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **Name** | A descriptive name for your AOAI Service resource. |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| **Pricing Tier** | The pricing tier for the resource. At the time of writing,
    AOAI Service only offers the Standard tier. |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Table 2.1: AOAI resource creation details'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create the AOAI resource by providing these details in the **Basics**
    tab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF9: Basic information to create an AOAI resource](img/B21019_02_9.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: Basic information to create an AOAI resource'
  prefs: []
  type: TYPE_NORMAL
- en: Click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **Network** tab offers three options for security. You can select option
    2 or 3 from within the **Network** tab if you require enhanced security during
    deployment. This choice allows you to configure AOAI for increased security, making
    it accessible through Azure Virtual Network integration and a private endpoint
    connection. Later in this book, we will focus on exploring options 2 and 3\. For
    now, choose option 1 (**All networks, including the internet, can access this
    resource.**) to access the resource:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF0: Public access network settings to create an AOAI resource](img/B21019_02_10.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Public access network settings to create an AOAI resource'
  prefs: []
  type: TYPE_NORMAL
- en: Proceed by clicking **Next** and configure any tags for your resource as needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Next** to advance to the final stage of the process: **Review +** **submit**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify your configuration settings, then click **Create** to initiate the process.
    Please be patient; this process may take 2-3 minutes to complete.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Azure portal will notify you once the new resource becomes available. Click
    **Go to resource** to access the newly created resource.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So far, we’ve set up AOAI Service. In the next section, we’ll deploy the model.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying AOAI models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, you created an AOAI resource. With this in hand, you
    can proceed to deploy and utilize your models through various means, such as Azure
    AI Foundry, REST APIs, or an SDK. This section will guide you through the process
    of deploying these models from Azure AI Foundry and accessing them via Studio
    and the Python SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to your Azure subscription within the Azure portal ([https://portal.azure.com/](https://portal.azure.com/)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the resource search bar, search for `Azure OpenAI`. Locate the AOAI resource
    that you created in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF1: Locating the AOAI resource created previously](img/B21019_02_11.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11: Locating the AOAI resource created previously'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the correct service name (highlighted by a red rectangle in the preceding
    figure).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to Azure AI Foundry by clicking either **Explore Azure AI Foundry
    portal** or **Go to Azure AI** **Foundry portal**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF2: Accessing \uFEFFAzure AI Foundry](img/B21019_02_12.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.12: Accessing Azure AI Foundry'
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the **Management** section, choose **Deployments**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF3: Creating a new AOAI model deployment](img/B21019_02_13.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.13: Creating a new AOAI model deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose **Deploy model** and proceed to configure the following fields:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **Fi****el****d** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Select** **a model** | Choose a model from the drop-down list. Model availability
    is subject to regional variations. To view a list of available models per region,
    please refer to the model summary table and region-specific availability provided
    at [https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability).
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Deployment name** | Be thoughtful in selecting a deployment name. The deployment
    name is crucial as it will be used in your code to invoke the model using client
    libraries and REST APIs. |'
  prefs: []
  type: TYPE_TB
- en: '| **Advance Options**(optional) | You have the option to configure advanced
    settings as required for your resource, such as content filter, **tokens per minute**
    (**TPM**), and more. |'
  prefs: []
  type: TYPE_TB
- en: 'For your initial deployment, keep **Advanced options** as-is. Further details
    on content filtering and TPM will be provided in upcoming chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF4: Deploying the gpt-35-turbo model](img/B21019_02_14.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.14: Deploying the gpt-35-turbo model'
  prefs: []
  type: TYPE_NORMAL
- en: Click **Create** to proceed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **Deployments** table will display a new entry corresponding to the model
    you just created with a status of **Succeeded**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF5: The gpt-35-turbo model’s deployment status](img/B21019_02_15.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.15: The gpt-35-turbo model’s deployment status'
  prefs: []
  type: TYPE_NORMAL
- en: '(**Optional**): If you have access to GPT-4, you can repeat all the previous
    steps to deploy the GPT-4 or GPT base model as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve successfully deployed the model using Azure AI Foundry, let’s
    explore how to utilize these models both within Azure AI Foundry and via the Python
    SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing AOAI models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make effective use of the model, it’s important to understand some basic
    elements associated with chat models. These concepts are essential for utilizing
    the model properly.
  prefs: []
  type: TYPE_NORMAL
- en: The GPT-3.5 Turbo and GPT-4 models are LLMs that have been optimized for conversational
    interfaces. They differ from the older GPT-3 models in how they operate. GPT-3
    models are text-in, text-out, meaning they accept an input (prompt) string and
    return an output (completion); the GPT-3.5 Turbo and GPT-4 models are designed
    for conversation-style interactions. These models expect input to be formatted
    in a chat-like transcript structure and return a completion that represents a
    message generated by the model in the chat. While this format was primarily designed
    for multi-turn conversations, it can also be effectively used in non-chat scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'In AOAI, there are two distinct options for interacting with models such as
    GPT-3.5 Turbo and GPT-4:'
  prefs: []
  type: TYPE_NORMAL
- en: '`messages` parameter, which expects an array of message objects organized by
    role. When interacting with these models using the Python API, you typically provide
    a list of dictionaries to represent the conversation format effectively. Each
    dictionary in the list should include the role and content of a message, enabling
    you to have meaningful back-and-forth exchanges with the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The format of a basic Chat Completion API is shown in the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The primary input for interacting with these models is the `messages` parameter,
    which expects an array of message objects. Each message object consists of a `role`
    type (either `system`, `user`, or `assistant`) and `content`. Conversations can
    be as brief as a single message or involve multiple back-and-forth exchanges.
    Let’s take a closer look at these different roles:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`system`: The `system` role, also referred to as the system message, is typically
    placed at the beginning of the message array. It serves as the initial instructions
    to the model to guide the model’s responses and interactions. These roles are
    set by the developers and can shape how the model handles various tasks and communicates
    with users. In the `system` role, you have the flexibility to provide various
    types of information:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`system` role defines how the model should interact with users. For instance,
    a model might have a role that prioritizes being helpful and informative or one
    that ensures it maintains a neutral and unbiased stance.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Behavioral constraints**: The role establishes boundaries for what the model
    should and shouldn’t do. This can include avoiding certain topics, adhering to
    privacy guidelines, or refraining from giving medical or legal advice:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`system` role can influence the tone and style of responses. For example, the
    model might be set to respond formally to professional contexts or adopt a casual
    tone for more relaxed interactions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`system` role is designed for customer support, the model might be optimized
    for troubleshooting and providing assistance related to specific products or services.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`system` roles often include parameters to ensure that responses adhere to
    ethical guidelines and safety protocols, helping to prevent the generation of
    harmful or inappropriate content.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overall, the `system` role helps tailor the behavior of the LLM to better meet
    the needs of users and align with the intended use case, whether it’s for specific
    and detailed instructions or just basic guidance. While the `system` role is optional,
    it’s generally recommended to include at least a basic message to ensure you get
    the best results and guide the assistant’s behavior effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '`user`: The `user` role refers to the function or context that the user assumes
    during the interaction. It influences how the user frames questions and what they
    seek from the model, such as information, advice, or creative assistance. The
    `user` role affects the model’s responses by providing context for the interaction
    and shaping the style and detail of answers based on the user’s needs and expectations.
    It also impacts how the model adapts its responses to fit the user’s goals, whether
    it’s for educational purposes, technical support, or casual conversation. Overall,
    the `user` role helps tailor the interaction so that it’s more effective and relevant
    to the user’s specific needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assistant`: The `assistant` role represents the model that’s chatting with
    the user. It’s used to distinguish the messages that are written by the model
    from those that are written by the user. The `assistant` role is also used to
    indicate who the model is addressing in the chat. For example, if the last message
    in the chat transcript has the `assistant` role, then the model will generate
    a response as if it’s continuing the conversation with the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Typically, a conversation is structured with a system message at the beginning,
    followed by alternating user and assistant messages. The system message plays
    a crucial role in setting the behavior of the assistant. It can be used to customize
    the assistant’s personality or provide specific instructions on how it should
    respond throughout the conversation. User messages are used to convey requests
    or comments for the assistant to address. Assistant messages not only store prior
    assistant responses but can also be authored by you to provide examples of the
    desired behavior you expect from the assistant during the conversation. This structured
    format allows for effective communication with the model, guiding its responses
    in a controlled manner. Figure 2.16 provides a clearer understanding of this structure:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF6: Skeleton of the system, user, and assistant roles/messages](img/B21019_02_16.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.16: Skeleton of the system, user, and assistant roles/messages'
  prefs: []
  type: TYPE_NORMAL
- en: The first square box represents the system message, where you set the meta prompt
    to guide the LLM’s responses. The second box is the user message, where users
    ask their questions. The last box shows the assistant message, which provides
    responses to the user’s queries.
  prefs: []
  type: TYPE_NORMAL
- en: '**Completion API with Chat Markup Language (ChatML)**: ChatML employs the same
    Completion API that you use for other GPT-3 models, such as davinci-002 or babbage-002,
    but it utilizes a distinct token-based prompt format. While ChatML offers lower-level
    access compared to the dedicated Chat Completion API, it comes with certain limitations
    and considerations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input validation**: You must perform additional input validation when using
    ChatML to ensure the correct format and structure of your messages.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model compatibility**: ChatML is only compatible with the GPT-3.5 Turbo models
    and doesn’t work with the new GPT-4 models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Potential format changes**: The underlying format for ChatML may change over
    time'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, while ChatML provides flexibility, it’s essential to be aware of
    these constraints and the potential for evolving formats when using it with GPT-3.5
    Turbo models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that you have a fundamental understanding of the Chat Completion API and
    its structure, let’s explore how to use the model from both Azure AI Foundry and
    the Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: The Azure AI Foundry experience
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To utilize the model in Azure AI Foundry, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to your Azure subscription within the Azure portal ([https://portal.azure.com/](https://portal.azure.com/)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the resource search bar, search for `Azure OpenAI`. Locate the AOAI resource
    that you created in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF7: Locating the AOAI instance created previously](img/B21019_02_17.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.17: Locating the AOAI instance created previously'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the service name highlighted within the red rectangle in the preceding
    figure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to Azure AI Foundry by clicking either **Explore Azure AI Foundry
    portal** or **Go to Azure AI** **Foundry portal**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF8: Launching \uFEFFAzure AI Foundry](img/B21019_02_18.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.18: Launching Azure AI Foundry'
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the **Playground** section, choose **Chat**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF19: Testing a prompt with \uFEFFAzure AI Foundry's Chat playground](img/B21019_02_19.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.19: Testing a prompt with Azure AI Foundry''s Chat playground'
  prefs: []
  type: TYPE_NORMAL
- en: In **Chat playground**, you have the flexibility to provide a custom **System
    message** tailored to your specific business requirements. We’ll delve into various
    prompting techniques in later chapters. However, for now, feel free to use the
    default system message provided.
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the **Deployment** tab within the **Configuration** panel and select
    **Deployment name** (which you created in the previous section) from the **Deployment**
    dropdown. Here, you have the option to choose either the gpt-3.5 or gpt-4 model
    to interact with.
  prefs: []
  type: TYPE_NORMAL
- en: Opt for the default **Session settings**. This setting determines how many ongoing
    conversations you can retain in the model’s context to facilitate responses to
    user prompts. It serves to manage token limit restrictions, which we covered at
    the beginning of this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, select the **Parameters** tab within the **Configuration** panel. Select
    the default settings for the parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF20: Setting up parameters](img/B21019_02_20.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.20: Setting up parameters'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several parameters to consider. Let’s look at each one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Max response**: The maximum number of tokens to generate in the chat completion
    reply. The total length of input tokens and generated tokens is limited by the
    model’s context length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temperature**: Which sampling temperature to employ, ranging from 0 to 1,
    is a crucial decision. Opting for higher values, such as 0.9, will introduce greater
    randomness into the output, whereas selecting lower values, such as 0.1, will
    enhance its focus and determinism. It’s important to note that a setting of 0
    will *not* make the model deterministic; instead, it will reduce the overall variability
    in replies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0.1` would imply that only tokens within the top 10% probability mass are
    taken into consideration. It is generally advisable to adjust either the **Top
    P** parameter or the **Temperature** parameter, but not both simultaneously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop sequence**: This helps you conclude the model’s response at a specific
    point and ensure it doesn’t include the provided stop sequence text. By doing
    so, you can prevent the model from generating a follow-up user query. You have
    the option to include up to four different stop sequences for this purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frequency penalty**: This number falls within the range of 0 to 2.0\. It
    lowers the likelihood of token repetition in proportion to its prior occurrence
    in the text, thus reducing the chances of reiterating identical text in response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Presence penalty**: This number falls within the range of 0 to 2.0\. It minimizes
    the probability of reusing any token that has already been used in the text, thereby
    enhancing the chances of introducing fresh topics in response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among these parameters, the one you’re most likely to adjust according to your
    specific use cases is the **Temperature** parameter. When you’re working on use
    cases that demand creativity, such as content generation, it’s common to set this
    value closer to 1\. In contrast, for use cases that require factual and precise
    answers, you’d typically opt for a value closer to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s use the GPT-3.5 model for chat interaction, as specified in *Figure
    2**.21*. In this instance, we’ll begin by posing the prompt, “Who emerged as the
    victor in the ICC World Cup series of 2011?” The model’s response is, “India emerged
    as the victor in the ICC World Cup series of 2011.” Subsequently, we pose a follow-up
    question, stating “Where was the final match held?” The model’s response is, “The
    final match of the ICC World Cup series in 2011 took place at the Wankhede Stadium
    in Mumbai, India.” This showcases the conversational-style interaction that can
    be sustained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF21: Describing agent and user chat interactions](img/B21019_02_21.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.21: Describing agent and user chat interactions'
  prefs: []
  type: TYPE_NORMAL
- en: With that, you’ve effectively utilized AOAI models to engage in a ChatGPT-style
    conversation. In the next section, we’ll discuss how to utilize the same GPT-3.5
    Turbo or GPT-4 model using the Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: Programmatic experience
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll guide you through the process of making your first call
    to AOAI using the Python SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: Install Python version 3.7.1 or a more recent version on your machine. Alternatively,
    you can utilize an Azure Machine Learning notebook to obtain the Python environment.
    In this example, we’ve used Anaconda with Visual Studio Code as an IDE.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the OpenAI Python client library by running `pip` `install openai`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To make a request to AOAI Service, you’ll require the following inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| `ENDPOINT` | You can locate this value in the `API-KEY` | You can find this
    value in the `DEPLOYMENT-NAME` | This value will correspond to the custom name
    you selected for your deployment during the model deployment process in the *Deploying
    AOAI* *models* section. |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'To get the first two values, navigate to your resource within the Azure portal.
    You can find the **Keys and Endpoint** under the **Resource Management** section.
    Make sure you copy both your endpoint and access key; you’ll require both to authenticate
    your API calls. You have the option to use either **KEY 1** or **KEY 2**. The
    presence of two keys enables secure key rotation and regeneration without service
    interruptions being made:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF22: Retrieving AOAI keys and endpoint information](img/B21019_02_22.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.22: Retrieving AOAI keys and endpoint information'
  prefs: []
  type: TYPE_NORMAL
- en: 'In your preferred IDE, create a Python file called `quickstart.py` and execute
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary Python packages and set up the AOAI keys and endpoint
    information. Make sure you change the deployment name’s value to the custom name
    you provided while creating the deployment:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the AOAI Chat Completion API and provide the AOAI deployment name, along
    with system and user message details:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the final response from the model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'The output should be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In a production environment, it’s recommended to use a secure method for storing
    and accessing your credentials, such as Azure Key Vault. This ensures the highest
    level of security for your sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: With that, you understand how to utilize the AOAI GPT-3.5 and GPT-4 models.
    Something we haven’t discussed yet is how to use the embedding model; we’ll cover
    that in the next chapter. In the next section, we’ll talk about AOAI pricing.
  prefs: []
  type: TYPE_NORMAL
- en: Pricing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll discuss the various AOAI pricing options and help you
    select the most suitable plan for your needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'AOAI has two distinct pricing plans:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pay-As-You-Go**: Under this pricing model, you’re billed based on the consumption
    of 1,000 tokens for both prompts and completions. Note that each model has its
    distinct pricing for both prompts and completions. This plan is better suited
    for development and testing environments, as well as specific production workloads
    where the volume of API calls and processed tokens isn’t substantial. For the
    complete pricing table, please refer to [https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/).
    It’s important to note that these costs may change in the future, so it’s advisable
    to check the preceding link provided for the most up-to-date pricing information:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Models** | **Context Window** | **Prompt****(per** **1,000 Tokens)** |
    **Completion****(er** **1,000 Tokens)** |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-35-turbo | 4K | $0.0015 | $0.002 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-35-turbo | 16K | $0.003 | $0.004 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-4 | 8K | $0.03 | $0.06 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-4 | 32K | $0.06 | $0.12 |'
  prefs: []
  type: TYPE_TB
- en: '| text-embedding-ada-002 | 8K | $0.0001 |  |'
  prefs: []
  type: TYPE_TB
- en: '**PAUG**: In this pricing plan, you’re initially provided with a default level
    of throughput, also known as a quota. This implies that the models will process
    a set number of tokens per minute based on your chosen region. You have the option
    to request an additional quota up to a certain threshold. If your needs exceed
    the predefined limit, you should consider the **Provisioned Throughput Unit**
    (**PTU**) pricing plan, which we’ll discuss shortly. We’ll delve into quota management
    in later chapters. For the most up-to-date quotas and limits, please refer to
    the following link:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table will help you determine the amount of throughput each model
    can deliver:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| **Models** | **Context Window** | **Default** **Throughput (Tokens/Minute)**
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| gpt-35-turbo | 4K | 240K |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| gpt-35-turbo | 16K | 240K |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| gpt-4 | 8K | 20K |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| gpt-4 | 32K | 40K |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| text-embedding-ada-002 | 8K | 240K |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Please note that this information may change over time, so always consult the
    latest documentation for the most up-to-date information: [https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits](https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Provisioned Throughput Unit - Managed (PTU-M)**: PTU-M is a new feature in
    AOAI Service that allows you to reserve processing capacity specifically tailored
    for high-volume or latency-critical workloads. PTU processing capacity delivers
    consistent levels of latency and throughput, which is ideal for workloads with
    stable characteristics, such as uniform prompt size, completion size, and concurrent
    API request numbers. PTUs can be procured hourly (no commitment) or monthly or
    yearly (commitment). In this pricing plan, you pay a fixed, flat rate, and you
    have the freedom to send an unlimited number of tokens throughout the commitment
    period. Additionally, this plan offers superior throughput compared to the standard
    pay-as-you-go option. PTU throughput can fluctuate based on factors such as input
    tokens, output tokens, and the number of API calls per minute. To accurately assess
    throughput for a specific PTU unit, you can utilize the benchmark script provided
    by Microsoft. You can access the benchmarking tool here: [https://github.com/Azure/azure-openai-benchmark](https://github.com/Azure/azure-openai-benchmark).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s break down the pricing structure for PTU. For hourly PTU usage without
    any commitment, the cost is $2 per PTU per hour. For a monthly commitment, the
    rate is $260 per PTU per month, while for a yearly commitment, it’s $221 per PTU
    per year.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each model has a minimum PTU requirement and minimum scaling increment. For
    example, GPT-4o requires a minimum of 50 PTUs to operate and 50 units for incremental
    scaling. Therefore, if you opt for a monthly commitment, GPT-4o will cost $260
    multiplied by 50 PTUs, totaling $13,000 per month. If you need to scale up your
    usage of GPT-4o, you will still need to add 50 PTUs as an increment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For detailed pricing information on various models, please refer to the table
    provided table. Now, let’s delve into how the pricing structure operates. Each
    PTU costs $312 per month. Different model types require varying amounts of PTUs
    to operate optimally. For instance, the GPT-35 Turbo with a 4K context window
    necessitates a minimum of 300 PTUs, providing a throughput ranging from 900K to
    2.7 million TPM. If you require additional throughput, you can incrementally add
    100 PTUs, resulting in an extra throughput of 300K to 900K TPM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'On the other hand, the GPT-35 Turbo with a 16K context window requires a minimum
    of 600 PTUs, offering a throughput between 1.8 million and 5.4 million TPM. To
    augment the throughput further, you can add 200 PTUs as an increment, resulting
    in an additional throughput of 600K to 1.8 million tokens per minute. For comprehensive
    details on the minimum PTU requirements, associated costs, and throughput capabilities
    for each model type, please refer to the following table:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF23: PTU pricing information](img/B21019_02_23.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.23: PTU pricing information'
  prefs: []
  type: TYPE_NORMAL
- en: Although AOAI offers a diverse set of LLMs, customers often choose different
    models based on their specific use cases to manage cost and accuracy. For complex
    tasks such as clinical protocol writing, drug discovery, and clinical trial matching,
    customers typically use GPT-4o due to its superior accuracy. For less complex
    tasks, such as customer support chatbots or entity extraction, customers might
    opt for GPT-3.5 or GPT-4o mini, which provide better price performance and are
    more economically feasible.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that you cannot directly purchase PTUs from the Azure
    portal as you would with other services. To acquire PTUs for your subscription,
    you must reach out to a Microsoft account representative, who will assist you
    in gaining approval for PTUs within your subscription. Once approved, you can
    proceed with the purchase of PTUs. The price for PTUs is subject to change in
    the future. Therefore, it’s advisable to always consult with a Microsoft account
    representative for the most up-to-date pricing information.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided an in-depth exploration of AOAI Service. We began by defining
    what AOAI Service is and went on to discuss the various model types it offers.
    We also guided you through the steps to access this service, from creating an
    AOAI resource to deploying the models and utilizing them for practical applications.
    Lastly, we shed light on the pricing structure, ensuring you have a clear understanding
    of how to leverage this powerful AI service within the Azure ecosystem. With this
    knowledge, you’re well-equipped to harness the capabilities of AOAI Service for
    your AI and machine learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: Looking ahead to the next chapter, our attention will shift toward delving into
    advanced topics within AOAI. We’ll explore embedding models and discover how to
    store these embeddings within a vector database, empowered by the Azure Cognitive
    Search service. Additionally, we’ll delve into the concept of model grounding
    and the intricacies of fine-tuning models to meet specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we’ll engage in discussions about some of the latest features,
    such as function calling, assistant API, fine-tuning, and the Batch API. These
    advanced topics will provide a deeper understanding of AOAI’s capabilities and
    empower you to leverage its full potential. We’ll conclude by highlighting the
    significance of LLM application development frameworks such as LangChain and Semantic
    Kernel. These frameworks play a pivotal role in simplifying the creation of applications.
    By leveraging the capabilities of these frameworks, developers can streamline
    the development process, harnessing the power of LLMs to build innovative and
    intelligent applications with ease. As we move forward in the field of AI and
    language processing, these frameworks serve as essential tools in harnessing the
    full potential of LLMs for diverse applications.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Introduction to AOAI* *Service* ([https://learn.microsoft.com/en-us/training/modules/explore-azure-openai/](https://learn.microsoft.com/en-us/training/modules/explore-azure-openai/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AOAI: Generative AI Models and How to Use* *Them* ([https://www.linkedin.com/learning/azure-openai-generative-ai-models-and-how-to-use-them](https://www.linkedin.com/learning/azure-openai-generative-ai-models-and-how-to-use-them))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
