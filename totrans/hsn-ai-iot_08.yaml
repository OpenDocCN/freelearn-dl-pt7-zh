- en: Distributed AI for IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The advances in distributed computing environments and an easy availability
    of internet worldwide has resulted in the emergence of **Distributed Artificial
    Intelligence** (**DAI**). In this chapter, we will learn about two frameworks,
    one by Apache the **machine learning library **(**MLlib**), and another H2O.ai,
    both provide distributed and scalable **machine learning** (**ML**) for large,
    streaming data. The chapter will start with an introduction to Apache''s Spark,
    the de facto distributed data processing system. This chapter will cover the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark and its importance in distributed data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Spark architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using MLlib in your deep learning pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delving deep into the H2O.ai platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IoT systems generate a lot of data; while in many cases it is possible to analyze
    the data at leisure, for certain tasks such as security, fraud detection, and
    so on, this latency is not acceptable. What we need in such a situation is a way
    to handle large data within a specified time—the solution—DAI, many machines in
    the cluster processing the big data (data parallelism) and/or training the deep
    learning models (model parallelism) in a distributed manner. There are many ways
    to perform DAI, and most of the approaches are built upon or around Apache Spark. Released
    in the year 2010 under the BSD licence, Apache Spark today is the largest open
    source project in big data. It helps the user to create a fast and general purpose
    cluster computing system.
  prefs: []
  type: TYPE_NORMAL
- en: Spark runs on a Java virtual machine, making it possible to run it on any machine
    with Java installed, be it a laptop or a cluster. It supports a variety of programming
    languages including Python, Scala, and R. A large number of deep learning frameworks
    and APIs are built around Spark and TensorFlow to make the task of DAI easier,
    for example, **TensorFlowOnSpark** (**TFoS**), Spark MLlib, SparkDl, and Hydrogen
    Sparkling (a combination of H2O.ai and Spark).
  prefs: []
  type: TYPE_NORMAL
- en: Spark components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark uses master-slave architecture, with one central coordinator (called
    the **Spark driver**) and many distributed workers (called **Spark executors**).
    The driver process creates a `SparkContext` object and divides the user application
    into smaller execution units (tasks). These tasks are executed by the workers.
    The resources among the workers are managed by a **Cluster** **Manager**. The
    following diagram shows the workings of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b18990e-1d8c-4cb0-a01b-7e2c24272ba8.png)'
  prefs: []
  type: TYPE_IMG
- en: Working of Spark
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now go through the different components of Spark. The following diagram
    shows the basic components that constitute Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab3d925a-6e1b-426e-a3ba-9dbabe167138.png)'
  prefs: []
  type: TYPE_IMG
- en: Components that constitute Spark
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see, in brief, some of the components that we will be using in this
    chapter, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resilient Distributed Datasets**: **Resilient Distributed Datasets** (**RDDs**)
    are the primary API in Spark. They represent an immutable, partitioned collection
    of data that can be operated in parallel. The higher APIs DataFrames and DataSets
    are built on top of RDDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed Variables**: Spark has two types of distributed variables: broadcast
    variables and accumulators. They are used by user-defined functions. Accumulators
    are used for aggregating the information from all the executors into a shared
    result. The broadcast variables, alternatively, are the variables that are shared
    throughout the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataFrames**: It is a distributed collection of data, very much like the
    pandas DataFrame. They can read from various file formats and perform the operation on
    the entire DataFrame using a single command. They are distributed across the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Libraries**: Spark has built-in libraries for MLlib, and for working with
    graphs (GraphX). In this chapter, we will use MLlib and SparkDl that uses Spark
    framework. We will learn how to apply them to make ML predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark is a big topic, and it is beyond the scope of this book to give further
    details on Spark. We recommend the interested reader refer to the Spark documentation: [http://spark.apache.org/docs/latest/index.html](http://spark.apache.org/docs/latest/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Apache MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Spark MLlib provides a powerful computational environment for ML. It
    provides a distributed architecture on a large-scale basis, allowing one to run
    ML models more quickly and efficiently. That''s not all; it is open source with
    a growing and active community continuously working to improve and provide the
    latest features. It provides a scalable implementation of the popular ML algorithms.
    It includes algorithms for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: Logistic regression, linear support vector machine, Naive
    Bayes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: Generalized linear regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaborative filtering**: Alternating least square'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: K-means'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decomposition**: Singular value decomposition and principal component analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has proved to be faster than Hadoop MapReduce. We can write applications in
    Java, Scala, R, or Python. It can also be easily integrated with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Regression in MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark MLlib has built-in methods for regression. To be able to use the built-in
    methods of Spark, you will have to install `pyspark` on your cluster (standalone
    or distributed cluster). The installation can be done using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The MLlib library has the following regression methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear regression**: We already learned about linear regression in earlier
    chapters; we can use this method using the `LinearRegression`  class defined at
    `pyspark.ml.regression`. By default, it uses minimized squared error with regularization.
    It supports L1 and L2 regularization, and a combination of them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalized linear regression**: The Spark MLlib has a subset of exponential
    family distributions like Gaussian, Poissons, and so on. The regression is instantiated
    using the class `GeneralizedLinearRegression`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision tree regression**: The `DecisionTreeRegressor` class can be used
    to make a prediction using decision tree regression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random forest regression**: One of the popular ML methods, they are defined
    in the `RandomForestRegressor` class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient boosted tree regression**: We can use an ensemble of decision trees
    using the `GBTRegressor` class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides, the MLlib also has support for survival regression and isotonic regression
    using the `AFTSurvivalRegression` and `IsotonicRegression` classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the help of these classes, we can build a ML model for regression (or
    classification as you will see in next section) in as little as 10 lines of code.
    The basic steps are outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a Spark session
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement the data-loading pipeline: load the data file, specify the format,
    and read it into Spark DataFrames'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify the features to be used as input and as the target (optionally split
    dataset in train/test)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate the desired class object
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `fit()` method with training dataset as an argument
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depending upon the regressor chosen, you can see the learned parameters and
    evaluate the fitted model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s use linear regression for the Boston house price prediction dataset
    ([https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)),
    where we have the dataset in `csv` format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules. We will be using `LinearRegressor` for defining
    the linear regression class, `RegressionEvaluator` to evaluate the model after
    training, `VectorAssembler` to combine features as one input vector, and `SparkSession` to
    start the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, start a Spark session using `SparkSession` class as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now read the data; we first load the data from the given path, define
    the format we want to use, and finally, read it into Spark DataFrames, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the DataFrame now loaded in the memory, and its structure, shown
    in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6ec3ba28-bb23-4336-a1fc-d38953ce1723.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Like pandas DataFrames, Spark DataFrames can also be processed with a single
    command. Let''s gain a little more insight into our dataset as seen in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/316109bd-4504-4877-b9b8-09d4c053b456.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we define the features we want to use for training; to do this, we make
    use of the `VectorAssembler` class. We define the columns from the `house_df` DataFrame
    to be combined together as an input feature vector and corresponding output prediction
    (similar to defining `X_train`, `Y_train`), and then perform the corresponding
    transformation, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0c1de199-e1b3-4e5f-9f54-4a3b79e57c5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The dataset is then split into train/test datasets, shown in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our dataset ready, we instantiate the `LinearRegression` class
    and fit it for the training dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can obtain the result coefficients of linear regression, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1446b664-709e-4413-883e-4337b7bf15f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The model provides an RMSE value of `4.73` and an `r2` value of `0.71` on the
    training dataset in `21` iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we evaluate our model on the test dataset; we obtain an RMSE of `5.55`
    and R2 value of `0.68`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Once the work is done, you should stop the Spark session using the `stop()`
    method. The complete code is available in `Chapter08/Boston_Price_MLlib.ipynb`.
    The reason for a low `r2` value and high RMSE is that we have considered all the
    features in the training dataset as an input feature vector, and many of them
    play no significant role in determining the house price. Try reducing the features,
    keeping the ones that have a high correlation with the price.
  prefs: []
  type: TYPE_NORMAL
- en: Classification in MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib also offers a wide range of classifiers; it provides both binomial and
    multinomial logistic regressor. The decision tree classifier, random forest classifier,
    gradient-boosted tree classifier, multilayered perceptron classifier, linear support
    vector machine classifier, and Naive Bayes classifier are supported. Each of them
    is defined in its class; for details, refer to [https://spark.apache.org/docs/2.2.0/ml-classification-regression.html](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#naive-bayes).
    The basic steps remain the same as we learned in the case of regression; the only
    difference is now, instead of RMSE or r2 metrics, the models are evaluated on
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section will treat you to the wine quality classification problem implemented
    using Spark MLlib logistic regression classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: For this classification problem, we will use logistic regression available through
    the `LogisticRegressor` class. The `VectorAssembler`, like in the previous example,
    will be used to combine the input features as one vector. In the wine quality
    dataset we have seen ([Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml), *Principles
    and Foundations of IoT and AI*), the quality was an integer number given between
    0–10, and we needed to process it. Here, we will process using `StringIndexer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One of the great features of Spark is that we can define all the preprocessing
    steps as a pipeline. This becomes very useful when there are a large number of
    preprocessing steps. Here, we have only two preprocessing steps, but just to showcase
    how pipelines are formed, we will make use of the `Pipeline` class. We import
    all these modules as our first step and create a Spark session, shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will load and read the `winequality-red.csv` data file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We process the `quality` label in the given dataset, and split it into three
    different classes, and add it to the existing Spark DataFrame as a new `quality_new` column,
    shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Though the modified quality, `quality_new` is an integer already, and we can
    use it directly as our label. In this example, we have added `StringIndexer` to
    convert it into numeric indices for the purpose of illustration. One can use `StringIndexer`
    to convert string labels to numeric indices. We also use `VectorAssembler` to
    combine the columns into one feature vector. The two stages are combined together
    using `Pipeline`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The data obtained after the pipeline is then split into training and testing
    datasets, shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we instantiate the `LogisticRegressor` class and train it on the training
    dataset using the `fit` method, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot, we can see the model parameters learned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7e2b0f57-da14-409a-ac9a-6eca299087e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The accuracy of the model is 94.75%. We can also see other evaluation metrics
    like `precision` and `recall`, F measure, true positive rate, and false positive
    rate in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the performance of the wine quality classifier using MLlib is
    comparable to our earlier approaches. The complete code is available in the GitHub
    repository under `Chapter08/Wine_Classification_MLlib.pynb`.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning using SparkDL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous sections elaborated how you can use the Spark framework with its
    MLlib for ML problems. In most complex tasks, however, deep learning models provide
    better performance. Spark supports SparkDL, a higher-level API working over MLlib.
    It uses TensorFlow at its backend, and it also requires TensorFrames, Keras, and
    TFoS modules.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will make use of SparkDL for classifying images. This will
    allow you to get acquainted with the Spark support for the images. For images,
    as we learned in [Chapter 4](cb9d27c5-e98d-44b6-a947-691b0bc64766.xhtml), *Deep
    Learning for IoT*, **Convolutional Neural Networks** (**CNNs**) are the de facto
    choice. In [Chapter 4](cb9d27c5-e98d-44b6-a947-691b0bc64766.xhtml), *Deep Learning
    for IoT*,we built CNNs from scratch, and also learned about some popular CNN architectures.
    A very interesting property of CNNs is that each convolutional layer learns to
    identify different features from the image, which is they act as feature extractors.
    The lower convolutional layers filter out basic shapes like lines and circles,
    while higher layers filter more abstract shapes. This property can be used to
    employ a CNN trained on one set of images to classify another set of similar domain
    images by just changing the top fully connected layers. This technique is called
    **transfer learning**. Depending upon the availability of new dataset images and
    similarity between the two domains, transfer learning can significantly help in
    reducing the training time and need for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In the NIPS 2016 tutorial, Andrew Ng, one of the key figures in the AI field,
    said that *transfer learning will be the next driver for commercial success*.
    In the image domain, great success in transfer learning has been achieved using
    CNNs trained in ImageNet data for classifying images on other domains. A lot of
    research is being carried out in applying transfer learning to other data domains.
    You can get a primer on *Transfer Learning* from this blog post by Sebastian Ruder: [http://ruder.io/transfer-learning/](http://ruder.io/transfer-learning/).
  prefs: []
  type: TYPE_NORMAL
- en: We will employ InceptionV3, a CNN architecture proposed by Google ([https://arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf)),
    trained on the ImageNet dataset ([http://www.image-net.org](http://www.image-net.org))
    to identify vehicles on roads (at present we restrict ourselves to only buses
    and cars).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can start, ensure that the following modules are installed in your
    working environment:'
  prefs: []
  type: TYPE_NORMAL
- en: PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TFoS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrapt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pillow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Py4J
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparkDL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jieba
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These can be installed using the `pip install` command on your standalone machine
    or machines in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next you will learn how to use Spark and SparkDL for image classification.
    We have taken screenshots of two different flowers, daisies and tulips, using
    Google image search; there are 42 images of daisies and 65 images of tulips. In
    the following screenshot, you can see the sample screenshots of the daisies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47e5e4e6-3443-4225-a3e7-823cc396503e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the sample images of tulips:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99141d8d-f0cd-46b7-9bf0-f20d83a22267.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our dataset is too small, and hence if we make a CNN from scratch, it will
    not be able to give any useful performance. In cases like these, we can make use
    of transfer learning. The SparkDL module provides an easy and convenient way to
    use pre-trained models with the help of the class `DeepImageFeaturizer`. It supports
    the following CNN models (pre-trained on the ImageNet dataset ([http://www.image-net.org](http://www.image-net.org)):'
  prefs: []
  type: TYPE_NORMAL
- en: InceptionV3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VGG16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VGG19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use Google''s InceptionV3 as our base model. The complete code can
    be accessed from the GitHub repository under `Chapter08/Transfer_Learning_Sparkdl.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first step, we will need to specify the environment for the SparkDL
    library. It is an important step; without it, the kernel will not know from where
    the SparkDL packages are to be loaded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Even when you install SparkDL using `pip` on some OSes, it is required that
    you specify the OS environment or SparkDL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s initiate a `SparkSession`, shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We import the necessary modules and read the data images. Along with reading
    the image paths, we also assign the labels to each image in the Spark DataFrame,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you can see the top five rows of the two DataFrames. The first column
    contains the path of each image, and the column shows its label (whether it belongs
    to daisy (label 1) or it belongs to tulips (label 0)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/99b1dae8-6d59-4f48-92e9-e94b553da9a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We split the two image dataset into training and testing set (it is always
    a good practice), using the `randomSplit` function. Conventionally, people choose
    a test-train split of 60%—40%, 70%—30%, or 80%—20%. We have chosen a 70%—30% split
    here. For the purpose of training, we then combine the training images of both
    flowers in the `trainDF` DataFrame and test dataset images in the `testDF` DataFrame,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we build the pipeline with `InceptionV3` as the feature extractor followed
    by a logistic regressor classifier. We use the `trainDF` DataFrame to train the
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now evaluate our trained model on the test dataset. We can see that,
    on the test dataset, we get an accuracy of 90.32% using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the confusion matrix for the two classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/884fed21-8f17-44bc-8c0d-a1c3b39a3bcd.png)'
  prefs: []
  type: TYPE_IMG
- en: In fewer than 20 lines of code, we were able to train the model and obtain a
    good 90.32% accuracy. Remember, here the dataset used is raw; by increasing the
    dataset images, and filtering out low-quality images, you can improve the performance
    of your model. You can learn more about the deep learning library SparkDL from
    the official GitHub repository: [https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing H2O.ai
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: H2O is a fast, scalable ML and deep learning framework developed by H2O.ai,
    released under the open source Apache license. According to the company-provided
    details, more than 9,000 organizations and 80,000+ data scientists use H2O for
    their ML/deep learning needs. It uses in-memory compression, which allows it to
    handle a large amount of data in memory, even with a small cluster of machines.
    It has an interface for R, Python, Java, Scala, and JavaScript, and even has a
    built-in web interface. H2O can run in standalone mode, and on Hadoop or Spark
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: H2O includes a large number of ML algorithms like generalized linear modeling,
    Naive Bayes, random forest, gradient boosting, and deep learning algorithms. The
    best part of H2O is that one can build thousands of models, compare the results,
    and even do hyperparameter tuning with a few lines of codes. H2O also has better
    data preprocessing tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O requires Java, so, ensure that Java is installed on your system. You can
    install H2O to work in Python using `PyPi`, shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: H2O AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most exciting features of H2O is **AutoML**, the automatic ML. It
    is an attempt to develop a user-friendly ML interface that can be used by non-experts.
    H2O AutoML automates the process of training and tuning a large selection of candidate
    models. Its interface is designed so that users just need to specify their dataset,
    input and output features, and any constraints they want on the number of total
    models trained, or time constraint. The rest of the work is done by AutoML itself;
    in the specified time constraint, it identifies the best performing models, and
    provides a leaderboard. It has been observed that, usually, the Stacked Ensemble
    model, the ensemble of all the previously trained models, occupies the top position
    on the leaderboard. There is a large number of options that advanced users can
    use; details of these options and their various features are available at[ http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'To know more about H2O you can visit their website: [http://h2o.ai](http://h2o.ai).'
  prefs: []
  type: TYPE_NORMAL
- en: Regression in H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will first show how regression can be done in H2O. We will use the same
    dataset as we used earlier with MLlib, the Boston house prices, and predict the
    cost of the houses. The complete code can be found at GitHub: `Chapter08/boston_price_h2o.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The necessary modules for the task are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'After importing the necessary modules, the first step is starting an `h2o`
    server. We do this using the `h2o.init()` command. It checks for any existing
    `h20` instances first, and if none are available, it will start one. There is
    also the possibility of connecting to an existing cluster by specifying the IP
    address and the port number as arguments to the `init()` function. In the following
    screenshot, you can see the result of `init()` on the standalone system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/53f4cd2a-ca1f-4c61-b9f7-3b6252b263fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we read the data file using the `h20` `import_file` function. It loads
    it into an H2O DataFrame, which can be processed just as easily as the panda''s
    DataFrame. We can find the correlation among the different input features in the `h20`
    DataFrame very easily using the `cor()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of correlation map among different features of
    the Boston house price dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4034fa0e-2e34-41ca-abd7-67bd6550ba9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, as usual, we split the dataset into training, validation, and test datasets.
    Define the features to be used as input features (`x`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this work is done, the process is very simple. We just instantiate the
    regression model class available from the H2O library, and use `train()` with
    the training and validation datasets as arguments. In the `train` function, we
    also specify what are the input features (`x`) and the output features (`y`).
    In the present case, we are taking all the features available to us as input features
    and the house price `medv` as the output feature. We can see the features of the
    trained model by just using a print statement. Next, you can see the model declaration
    for a generalized linear regression model, and its result after training on both
    training and validation datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/05ca2730-9046-4226-9f2a-0a3fde5fc9c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After training, the next step is checking the performance on the test dataset,
    which can be easily done using the `model_performance()` function. We can also
    pass it to any of the datasets: the train, validation, test, or some new similar
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/37608bc5-7c4d-45e8-891e-00fbcf07a17d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we want to use gradient boost estimator regression, or random forest regression,
    we will instantiate the respective class object; the following steps will remain
    the same. What will vary is the output parameters; in the case of gradient boost
    estimator and random forest, we will also learn the relative importance of the
    different input features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The most difficult part of machine and deep learning is choosing the right
    hyperparameters. In H2O, the task becomes quite easy with the help of its `H2OGridSearch`
    class. The following code snippet performs the grid search on the hyperparameter
    depth for the gradient boost estimator defined previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The best part of H2O is using AutoML to find the best models automatically.
     Let''s ask it to search for us among the 10 models, with the constraint on time
    being 100 seconds. AutoML will, with these parameters, build 10 different models,
    excluding the Stacked Ensembles. It will run, at the most, for 100 seconds before
    training the final Stacked Ensemble models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The leaderboard for our regression task is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/63da4e07-dad8-4b77-b3a0-a6935ec13e70.png)'
  prefs: []
  type: TYPE_IMG
- en: Different models in the leaderboard can be accessed using their respective `model_id`.
    The best model is accessed with the leader parameter. In our case, `aml.leader`
    represents the best model, the Stacked Ensemble of all the models. We can save
    the best model using the `h2o.save_model` function in either binary or MOJO format.
  prefs: []
  type: TYPE_NORMAL
- en: Classification in H20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The same models can be used for classification in H2O, with only one change;
    we will need to change the output features from numeric values to categorical
    values using the `asfactor()` function. We will perform the classification on
    the quality of red wine, and use our old red wine database ([Chapter 3](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml),
    *Machine Learning for IoT*). We will need to import the same modules and initiate
    the H2O server. The full code is available at in the `Chapter08/wine_classification_h2o.ipynb` file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to import the necessary modules and initiate the H2O server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to read the data file. We modify the output feature first
    to account for two classes (good wine and bad wine) and then convert it to a categorical
    variable using the `asfactor()` function. This is an important step in H2O; since
    we are using the same class objects for both regression and classification, they
    require the output label to be numeric in the case of regression, and categorical
    in the case of classification, as seen in the code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, split the data into training, validation, and testing datasets. We feed
    the training and validation datasets to the generalized linear estimator, with
    one change; we specify the `family=binomial` argument because here, we have only
    two categorical classes, good wine or bad wine. If you have more than two classes
    use `family=multinomial`. Remember, specifying the argument is optional; H2O automatically
    detects the output feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After being trained, you can see the model performance on all the performance
    metrics: accuracy, precision, recall, F1 measure, and AUC, even the confusion
    metrics. You can get them for all the three datasets (training, validation, and
    testing). The following are the metrics obtained for the test dataset from the
    generalized linear estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/485b7663-f7da-4308-aacc-2c9eef924c7f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9c6bc796-6352-41ab-bf64-9259a084c731.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Without changing anything else in the previous code, we can perform hyper tuning
    and use H2O''s AutoML to get the better model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/62356947-6833-41e5-9c11-5a6a2947e263.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that, for wine quality classification, the best model is XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the ubiquitous status of IoT, the data being generated is growing at an
    exponential rate. This data, mostly unstructured and available in vast quantities,
    is often referred to as big data. A large number of frameworks and solutions have
    been proposed to deal with the large set of data. One of the promising solutions
    is DAI, distributing the model or data among the cluster of machines. We can use
    distributed TensorFlow, or TFoS frameworks to perform distributed model training.
    In recent years, some easy-to-use open source solutions have been proposed. Two
    of the most popular and successful solutions are Apache Spark's MLlib and H2O.ai's
    H2O. In this chapter, we showed how to train ML models for both regression and
    classification in MLlib and H2O. The Apache Spark MLlib supports SparkDL, which
    provides excellent support for image classification and detection tasks. The chapter
    used SparkDL to classify flower images using the pre-trained InceptionV3\. The
    H2O.ai's H2O, on the other hand, works well with numeric and tabular data. It
    provides an interesting and useful AutoML feature, which allows even non-experts
    to tune and search through a large number of ML/deep learning models, with very
    little details from the user. The chapter covered an example of how to use AutoML
    for both regression and classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: One can take the best advantage of these distributed platforms when working
    on a cluster of machines. With computing and data shifting to the cloud at affordable
    rates, it makes sense to shift the task of ML to the cloud. Thus follows the next
    chapter, where you will learn about different cloud platforms, and how you can
    use them to analyze the data generated by your IoT devices.
  prefs: []
  type: TYPE_NORMAL
