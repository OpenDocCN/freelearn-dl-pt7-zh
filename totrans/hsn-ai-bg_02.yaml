- en: Machine Learning Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial Int****elligence **(**AI**) is rooted in mathematics and statistics.
    When creating an **Artificial Neural Network** (**ANN**), we''re conducting mathematical
    operations on data represented in linear space; it is, by nature, applied mathematics
    and statistics. Machine learning algorithms are nothing but function approximations;
    they try and find a mapping between an input and a correct corresponding output.
    We use algebraic methods to create algorithms that learn these mappings.'
  prefs: []
  type: TYPE_NORMAL
- en: Almost all machine learning can be expressed in a fairly straight-forward formula;
    bringing together a dataset and model, along with a loss function and optimization
    technique that are applicable to the dataset and model. This section is intended
    as a review of the basic mathematical tools and techniques that are essential
    to understanding *what's under the hood* in AI.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll review linear algebra and probability, and then move
    on to the construction of basic and fundamental machine learning algorithms and
    systems, before touching upon optimization techniques that can be used for all
    of your methods going forward. While we will utilize mathematical notation and
    expressions in this chapter and the following chapters, we will focus on translating
    each of these concepts into Python code. In general, Python is easier to read
    and comprehend than mathematical expressions, and allows readers to get off the
    ground quicker.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Applied math basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing basic machine learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be working in Python 3 with the scikit-learn scientific
    computing package. You can install the package, you can run `pip install sklearn` in
    your terminal or command line.
  prefs: []
  type: TYPE_NORMAL
- en: Applied math basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk about mathematics as related to deep learning and AI, we're often
    talking about linear algebra. Linear algebra is a branch of continuous mathematics
    that involves the study of vector space and operations performed in vector space.
    If you remember back to grade-school algebra, algebra in general deals with unknown
    variables. With linear algebra, we're extending this study into linear systems
    that have an arbitrary number of dimensions, which is what makes this a form of
    continuous mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: AI relies on the basic building block of the tensor. Within AI, these mathematical
    objects store information throughout ANNs that allow them to operate; they are
    data structures that are utilized throughout AI. As we will see, a tensor has
    a **rank**,which essentially tells us about the **indices** of the data (how many
    rows and columns the data has).
  prefs: []
  type: TYPE_NORMAL
- en: While many problems in deep learning are not formally linear problems*, *the
    basic building blocks of matrices and tensors are the primary data structures
    for solving, optimizing, and approximating within an ANN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Want to see how linear algebra can help us from a programmatic standpoint?
    Take a look at the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We can eliminate strenuous loops by simply utilizing NumPy's built-in linear
    algebra functions. When you think of AI, and the thousands upon thousands of operations
    that have to be computed at the runtime of an application, the building blocks
    of linear algebra can also help us out programmatically. In the following sections,
    we'll be reviewing these fundamental concepts in both mathematical notation and
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the following examples will use the Python package NumPy; `import numpy
    as np`
  prefs: []
  type: TYPE_NORMAL
- en: The building blocks – scalars, vectors, matrices, and tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following section, we'll introduce the fundamental types of linear algebra
    objects that are used throughout AI applications; **scalars**, **vectors**, **matrices**,
    and **tensors**.
  prefs: []
  type: TYPE_NORMAL
- en: Scalars
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scalars** are nothing but singular, **real numbers** that can take the form
    of an integer or floating point. In Python, we create a scalar by simply assigning
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Vectors** are one-dimensional arrays of integers. Geometrically, they store
    the direction and magnitude of change from a point. We''ll see how this works
    in machine learning algorithms when we discuss **principal component analysis**
    (**PCA**) in the next few pages. Vectors in Python are created as `numpy array`
    objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Vectors can be written in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f77e9e8-5267-45e7-9950-b6cb37c453ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Matrices** are two-dimensional lists of numbers that contain rows and columns. Typically,
    rows in a matrix are denoted by *i*, while columns are denoted by *j*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrices are represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73f016a2-d87e-481b-9229-394b7661d3b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can easily create matrices in Python as NumPy arrays, much like we can with
    vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The only different is that we are adding an additional vector to the array to
    create the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While you may have heard of vectors and matrices before, the name **t****ensor**may
    be new. A tensor is a generalized matrix, and they have different sizes, or ranks*,* which
    measure their dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tensors are three (or more)-dimensional lists; you can think of them as a sort
    of multi-dimensional object of numbers, such as a cube. Tensors have a unique
    transitive property and form; if a tensor transforms another entity, it too must
    transform. Any rank 2 tensor can be represented as a matrix, but not all matrices
    are automatically rank 2 tensors. A tensor must have this transitive property.
    As we''ll see, this will come into play with neural networks in the next chapter.
    We can create tensors in Python such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Matrix math
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic operations of an ANN are based on matrix math. In this section, we'll
    be reviewing the basic operations that you need to know to understand the mechanics
    of ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: Scalar operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scalar operations involve a vector (or matrix) and a scalar. To perform an
    operation with a scalar on a matrix, simply apply to the scalar to every item
    in the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/034243e4-25c0-4081-b381-7a6c87ad45f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In Python, we would simply do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Element–wise operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In element-wise operations, position matters. Values that correspond positionally are
    combined to create a new value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add to and/or subtract matrices or vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e07114f4-e679-4e3e-a997-747773c84017.png)![](img/5f919d62-e0c8-4c0a-9ffc-c19a2b915b6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two forms of multiplication that we may perform with vectors: the** Dot product**, and
    the **Hadamard product**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dot product is a special case of multiplication, and is rooted in larger
    theories of geometry that are used across the physical and computational sciences.
    It is a special case of a more general mathematical principle known as an **inner
    product**. When utilizing the dot product of two vectors, the output is a scalar:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d2dd8e6-671e-4644-93b3-1e8c056760ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dot products are a workhorse in machine learning. Think about a basic operation:
    let''s say we''re doing a simple classification problem where we want to know
    if an image contains a cat or a dog. If we did this with a neural network, it
    would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cc5fe55-f9ae-4e86-becb-f86437c3d3b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* is our classification cat or dog. We determine *y* by utilizing a
    network represented by *f*, where the input is *x*, while *w* and *b* represent
    a weight and bias factor (don't worry, we'll explain this in more detail in the
    coming chapter!). Our *x* and *w* are both matrices, and we need to output a scalar
    ![](img/77024446-eb78-43c0-8166-b29644cd0086.png), which represents either cat
    or dog. We can only do this by taking the dot product of *w* and ![](img/ea06997c-67a5-4c3f-96e1-ec7f15945c3a.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Relating back to our example, if this function were presented with an unknown
    image, taking the dot product will tell us how similar in direction the new vector
    is to the cat vector (*a*) or dog vector (*b*) by the measure of the angle (![](img/fe9988d5-fb5b-406c-bac6-3f9b490e57af.png))
    between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2ac2c9f-cbe9-4b2e-a3f1-58daae581e1e.png)'
  prefs: []
  type: TYPE_IMG
- en: If the vector is closer to the direction of the cat vector (a), we'll classify
    the image as containing a cat. If it's closer to the dog vector (b), we'll classify
    it as containing a dog. In deep learning, a more complex version of this scenario
    is performed over and over; it's the core of how ANNs work.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, we can take the dot product of two vectors by using a built-in function
    from `numpy`, `np.dot()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The Hadamard product, on the other hand, outputs a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae8c61da-3bff-49b9-bd6d-0853315777c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Hadamard product is element-wise, meaning that the individual numbers in
    the new matrix are the scalar multiples of the numbers from the previous matrices.
    Looking back to Python, we can easily perform this operation in Python with a
    simple `*` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we've scratched the surface of basic matrix operations, let's take
    a look at how probability theory can aid us in the artificial intelligence field.
  prefs: []
  type: TYPE_NORMAL
- en: Basic statistics and probability theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Probability**, the mathematical method for modeling uncertain scenarios,
    underpins the algorithms that make AI intelligent, helping to tell us how our
    systems should reason. So, what is probability? We''ll define it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Probability is a frequency expressed as a fraction of the sample size, n*
    [1].'
  prefs: []
  type: TYPE_NORMAL
- en: Simply said, probability is the mathematical study of uncertainty. In this section,
    we'll cover the basics of probability space and probability distributions, as
    well as helpful tools for solving simple problems.
  prefs: []
  type: TYPE_NORMAL
- en: The probability space and general theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When probability is discussed, it''s often referred to in terms of the probability
    of a certain **event** happening. Is it going to rain? Will the price of apples
    go up or down? In the context of machine learning, probabilities tell us the likelihood
    of events such as a comment being classified as positive vs. negative, or whether
    a fraudulent transaction will happen on a credit card. We measure probability
    by defining what we refer to as the **probability space**. A probability space
    is a measure of *how* and *why* of the probabilities of certain events. Probability
    spaces are defined by three characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The sample space, which tells us the possible outcomes or a situation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A defined set of events; such as two fraudulent credit card transactions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The measure of probability of each of these events
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While probability spaces are a subject worthy of studying in their own right,
    for our own understanding, we'll stick to this basic definition.
  prefs: []
  type: TYPE_NORMAL
- en: In probability theory, the idea of **independence** is essential. Independence is
    a state where a random variable does not change based on the value of another
    random variable. This is an important assumption in deep learning, as non–independent
    features can often intertwine and affect the predictive power of our models.
  prefs: []
  type: TYPE_NORMAL
- en: In statistical terms, a collection of data about an event is a **sample, **which
    is drawn from a theoretical superset of data called a **population** that represents
    everything that is known about a grouping or event. For instance, if we were poll
    people on the street about whether they believe in Political View A or Political
    View B, we would be generating a **random sample** from the population, which
    would be entire population of the city, state, or country where we are polling.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's say we wanted to use this sample to predict the likelihood of a person
    having one of the two political views, but we mostly polled people who were at
    an event supporting Political View A. In this case, we may have a **biased sample**.
    When sampling, it is important to take a random sample to decrease bias, otherwise
    any statistical analysis or modeling that we do with sample will be biased as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Probability distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You''ve probably seen a chart such as the following one; it''s showing us the
    values that appear in a dataset, and how many times those values appear. This
    is called a **distribution** of a variable. In this particular case, we''re displaying
    the distribution with the help of a **histogram**, which shows the **frequency**
    of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4d6dc2a-a29b-4cbf-ad33-b1c538ea3618.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we're interested in a particular type of distribution, called
    a **probability** **distribution**. When we talk about probability distributions,
    we're talking about the likelihood of a random variable taking on a certain value,
    and we create one by dividing the frequencies in the preceding ...
  prefs: []
  type: TYPE_NORMAL
- en: Probability mass functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Probability mass functions** (**PMFs**)are discrete distributions. The random
    variables of the distribution can take on a finite number of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f44d2684-9c3f-40cb-b887-ef093f87eaba.png)'
  prefs: []
  type: TYPE_IMG
- en: PMFs look a bit different from our typical view of a distribution, and that
    is because of their finite nature.
  prefs: []
  type: TYPE_NORMAL
- en: Probability density functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Probability density functions** (**PDFs**) are continuous distributions;
    values from the distribution can take on infinitely many values. For example,
    take the image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c89482b-887c-40d8-b880-a7f612bca80a.png)'
  prefs: []
  type: TYPE_IMG
- en: You've probably seen something like this before; it's a probability density
    function of a **standard normal**, or **Gaussian distribution**.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional and joint probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Conditional probability** is the probability that *x* happens, given that
    *y* happens. It''s one of the key tools for reasoning about uncertainty in probability
    theory. Let''s say we are talking about your winning the lottery, given that it''s
    a sunny day. Maybe you''re feeling lucky! How would we write that in a probability
    statement? It would be the probability of your lottery win, *A*, given the probability
    of it being sunny, *B*, so *P(A|B)*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Joint probability** is the probability of two things happening simultaneously:
    what is the probability of you winning the lottery *and* it being a sunny day?'
  prefs: []
  type: TYPE_NORMAL
- en: Chain rule for joint probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Joint probability is important in the AI space; it''s what underlies the mechanics
    of **generative models**, which are able to replicate voice, pictures, and other
    unstructured information. These models learn the joint probability distribution
    of a phenomenon. They generate all possible values for a given object or event.
    A chain rule is a technique by which to evaluate the join probability of two variables.
    Formally, it is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a223b2fa-6759-4f86-b295-482060293233.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayes' rule for conditional probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bayes**'' **rule** is another essential tenet of probability theory in the
    machine learning sphere. It allows us to calculate the conditional probability
    of an event happening by inverting the conditions of the events. Bayes'' rule
    is formally written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dcfd8314-ce15-4407-a1b0-468f855c74be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s use Bayes'' rule to look at a simple conditional probability problem.
    In the following table, we see the likelihood of a patient contacting a disease:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Disease (1%)** | **No Disease (99%)** |'
  prefs: []
  type: TYPE_TB
- en: '| Test Positive | 80% | 9.6% |'
  prefs: []
  type: TYPE_TB
- en: '| Test Negative | 20% | 90.4% |'
  prefs: []
  type: TYPE_TB
- en: How do we interpret this table? The *x *axis tells us the percentage of the
    population who have the disease; if you have it, you are firmly in the Disease
    column. Based on that condition, the *y *axis is the likelihood of you testing
    positive or negative, based on whether you actually have the disease or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s say that we have a positive test result; what is the chance that
    we actually have the disease? We can use Bayes'' formula to figure solve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4469294-7f8e-4add-8f71-f3c50db0bfc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our answer comes out to 7.8%, the actual probability of having the disease
    given a positive test:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we can see how Bayes'' formula can model these conditional
    events based on likelihood. In machine learning and AI in general, this comes
    in handy when modeling situations or perhaps classifying objects. Conditional
    probability problems also play into discriminative models, which we will discuss
    in our section on **Generative adversarial networks**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember: when conducting multiplication, the type of operation matters. We
    can use the Hadamard product to multiply two equally-sized vectors or matrices
    where the output will be another equally-sized vector or matrix. We use the dot
    product in situations where we need a single number as an output. The dot product
    is essential in machine learning and deep learning; with neural networks, inputs
    are passed to each layer as a matrix or vector, and these are then multiplied
    with another matrix of weights, which forms the core of basic network operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Probability distributions and the computations based on them rely on Bayesian
    thinking in the machine learning realm. As we'll see in later chapters, some of
    the most innovative networks in AI directly rely on these distributions and the
    core concepts of Baye's theorem. Recall that there are two primary forms of probability
    distribution: PMFsfor discrete variables, and probability density functions PDFs
    for continuous variables; CDF, which apply to any random variables, also exist.
  prefs: []
  type: TYPE_NORMAL
- en: Baye's rule, in fact, has inspired an entire branch of statistics known as **Bayesian
    statistics**.Thus far, we have discussed frequent statistics**,** which measure
    probability based on an observed space of repeatable events**. **Bayesian probability,
    on the other hand, measures degrees of belief; how likely is an event to happen
    based on the information that is currently available? This will become important
    as we delve into ANNs in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing basic machine learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the last chapter, machine learning is a term that was developed
    as a reaction to the first AI winter. Today, we generally consider machine learning
    to be the overarching subject area for deep learning and ANNs in general.
  prefs: []
  type: TYPE_NORMAL
- en: Most machine learning solutions can be broken down into either a **classification**
    problem or a **regression** problem. A classification problem is when the output
    variables are categorical, such as fraud or not fraud. A regression problem is
    when the output is continuous, such as dollars or site visits. Problems with numerical
    output can be categorical, but are typically transformed to have a categorical
    output such as first class and second class.
  prefs: []
  type: TYPE_NORMAL
- en: Within machine ...
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supervised algorithms rely on human knowledge to complete their tasks. Let''s
    say we have a dataset related to loan repayment that contains several demographic
    indicators, as well as whether a loan was paid back or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Income** | **Age** | **Marital Status** | **Location** | **Savings** |
    **Paid** |'
  prefs: []
  type: TYPE_TB
- en: '| $90,000 | 28 | Married | Austin, Texas | $50,000 | y |'
  prefs: []
  type: TYPE_TB
- en: 'The Paid column, which tells us if a loan was paid back or not, is called the
    **target** - it''s what we would like to predict. The data that contains information
    about the applicants background is known as the **features** of the datasets.
    In supervised learning, algorithms learn to predict the target based on the features,
    or in other words, what indicators give a high probability that an applicant will
    pay back a loan or not? Mathematically, this process looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99f325bf-11d3-4441-b8ae-b87042a3fc7a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we are saying that our label ![](img/c855e39b-a2d0-41eb-b053-60907ec03d63.png)
    *is a function of* the input features ![](img/390b3c7c-cf71-47df-8996-42f6d8b2bc99.png),
    plus some amount of error ![](img/9b202d7a-42f7-4dfe-933e-2a62c9b4deb6.png) that
    it caused naturally by the dataset. We know that a certain set of features will
    likely produce a certain outcome. In supervised learning, we set up an algorithm
    to *learn* what function will produce the correct mapping of a set of features
    to an outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate how supervised learning works, we are going to utilize a famous
    example toy dataset in the machine learning field, the Iris Dataset. It shows
    four features: Sepal Length, Sepal Width, Petal Length, and Petal Width. In this
    dataset, our target variable (sometimes called a **label**) is *Name. *The dataset
    is available in the GitHub repository that corresponds with this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/313467ac-81ee-4e6d-8953-97e7abc893ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have our data ready to go, let's jump into some supervised learning!
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Random forests** are one of the most commonly utilized supervised learning
    algorithms. While they can be used for both classification and regression tasks,
    we''re going to focus on the former. Random forests are an example of an **ensemble
    method**, which works by aggregating the outputs of multiple models in order to
    construct a stronger performing model. Sometimes, you''ll hear this being referred
    to as a grouping of **weak learners** to create a **strong learner**.'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a random forest classifier in Python is quite simple with the help
    of scikit-learn. First, we import the modules and set up our data. We do not have
    to perform any data cleaning here, as the Iris dataset comes pre-cleaned.
  prefs: []
  type: TYPE_NORMAL
- en: Before training machine learning algorithms, ...
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms learn the properties of data on their own without
    explicit human intervention or labeling. Typically within the AI field, unsupervised
    learning technique learn the probability distribution that generated a dataset. These
    algorithms, such as **autoencoders** (we will visit these later in the book),
    are useful for a variety of tasks where we simply don't know important information
    about our data that would allow us to use traditional supervised techniques.
  prefs: []
  type: TYPE_NORMAL
- en: PCA is an unsupervised method for feature extraction. It combines input variables
    in such a way that we can drop those variables that provide the least amount of
    information to us. Afterwards, we are left with new variables that are independent
    of each other, making them easy to utilize in a basic linear model.
  prefs: []
  type: TYPE_NORMAL
- en: AI applications are fundamentally hampered by the **curse of dimensionality**.
    This phenomenon, which occurs when the number of **dimensions** in your data is
    high, makes it incredibly difficult for learning algorithms to perform well. PCA
    can help alleviate this problem for us. PCA is one of the primary examples of
    what we call **dimensionality reduction**, which helps us take high-feature spaces
    (lots of data attributes) and transform them into lower-feature spaces (only the
    important features).
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimensionality reduction can be conducted in two primary ways: **feature elimination**
    and **feature extraction**. Whereas feature elimination may involve the arbitrary
    cutting of features from the dataset, feature extraction (PCA is a form of) this
    gives us a more intuitive way to reduce our dimensionality. So, how does it work?
    In a nutshell:'
  prefs: []
  type: TYPE_NORMAL
- en: We create a matrix (correlation or covariance) that describes how all of our
    data relates to each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We decompose this matrix into separate components, called the **eigenvalues**
    and the **eigenvectors**, which describe the direction and magnitude of our data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then transform or project our original data onto these components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s break this down in Python manually to illustrate the process. We''ll
    use the same Iris dataset that we used for the supervised learning illustration.
    First, we''ll create the correlation matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b9f2573-871b-4295-abe0-f9e104b9e3d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our correlation matrix contains information on how every element of the matrix
    relates to each other element. This record of association is essential in providing
    the algorithm with information. Lots of variability typically indicates a signal,
    whereas a lack of variability indicates noise. The more variability that is present
    in a particular direction, the more there is to detect. Next, we''ll create our
    `eigen_values` and `eigen_vectors`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/faf4a54f-4dc7-48a6-aef8-12465565c874.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Eigenvectors and Eigenvalues come in pairs ; each eigenvectors are directions
    in of data, and eigenvalues tell us how much variance exists within that direction.
    In PCA, we want to understand which inputs account for the most variance in the
    data (that is: how much do they explain the data). By calculating eigenvectors
    and their corresponding eigenvalues, we can begin to understand what is most important
    in our dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: We now want to sort the pairs of eigenvectors/eigenvalues from highest to lowest.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we need to project these pairs into a *lower dimensional spac*e. This
    is dimensionality reduction aspect of PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll then conduct this transformation on the original data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then plot the components against each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0223443c-4129-4038-9661-00e32a31e296.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So when should we use PCA? Use PCA when the following are true:'
  prefs: []
  type: TYPE_NORMAL
- en: Do you have high dimensionality (too many variables) and want a logical way
    to reduce them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you need to ensure that your variables are independent of each other?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the downsides of PCA, however, is that it makes the underlying data
    more opaque, thus hurting it''s interpretability. Besides PCA and the k–means
    clustering model that we precedingly described, other commonly seen non-deep learning
    unsupervised learning algorithms are:'
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixture models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So you've built a model, now what? Can you call it a day? Chances are, you'll
    have some optimization to do on your model. A key part of the machine learning
    process is the optimization of our algorithms and methods. In this section, we'll
    be covering the basic concepts of optimization, and will be continuing our learning
    of tuning methods throughout the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, when our models do not perform well with new data it can be related
    to them **overfitting** or **underfitting**. Let's cover some methods that we
    can use to prevent this from happening. First off, let's look at the random forest
    classifier that we trained earlier. In your notebook, call the `predict` method
    on it and pass the `x_test` data in to receive some ...
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting and underfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Overfitting** is a phenomenon that happens when an algorithm learns it''s
    training data *too well* to the point where it cannot accurately predict on new
    data. Models that overfit learn the small, intricate details of their training
    set and don''t generalize well. For analogy, think about it as if you were learning
    a new language. Instead of learning the general form of the language, say Spanish,
    you''ve learned to perfect a local version of it from a remote part of South America,
    including all of the local slang. If you went to Spain and tried to speak that
    version of Spanish, you would probably get some puzzled looks from the locals! Underfitting
    would be exact opposite of this; you didn''t study enough Spanish, and so you
    do not have enough knowledge to communicate effectively. From a modeling standpoint,
    an underfit model is not complex enough to generalize to new data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overfitting and underfitting are tried to a machine learning phenomenon known
    as the **bias**/**variance** tradeoff:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias** is the error that your model learns as it tries to approximately predict
    things. Understanding that models are simplified versions of reality, bias in
    a model is the error that develops from trying to create this simplified version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variance** is the degree to which your error changes based on variations
    in the input data. It measures your model''s sensitivity to the intricacies of
    the input data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The way to mitigate bias is to increase the complexity of a model, although
    this will increase variance and will lead to overfitting. To mitigate variance
    on the other hand, we could make our model to generalize well by reducing complexing,
    although this would lead to higher bias. As you can see, we cannot have a both
    low bias and low variance at the same time! A good model will be balanced between
    it's bias and variance. There are two ways to combat overfitting; cross-validation
    and regularization. We will touch upon cross-validation methods now, and come
    back to regularization in [Chapter 4](8c724645-08d4-4a08-af9e-45bf607f8a88.xhtml), *Your
    First Artificial Neural Network* when we begin to build our first ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: K-fold cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You've already seen a form of cross-validation before; holding out a portion
    of our data is the simplest form of cross- validation that we can have. While
    this is generally a good practice, it can sometimes leave important features out
    of the training set that can create poor performance when it comes time to test.
    To remedy this, we can take standard cross validation a step further with a technique
    called **k**-**fold cross validation**.
  prefs: []
  type: TYPE_NORMAL
- en: In k-fold cross validation, our dataset is evenly divided in *k* event parts,
    chosen by the user. As a rule of thumb, generally you should stick to k = 5 or
    k = 10 for best performance. The model is then trained and tested *k* times over.
    During each training episode, one *k* segment of the data ...
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aside from protecting against overfitting, we can optimize models by searching
    for the best combination of **model hyperparameters**. Hyperparameters are configuration
    variables that tell the model what methods to use, as opposed to **model parameters**
    which are learned during training - we'll learn more about these in upcoming chapter.
    They are programmatically added to a model, and are present in all modeling packages
    in Python. In the random forest model that we built precedingly, for instance,
    `n_estimators` is a hyperparameter that tells the model how many trees to build.
    The process of searching for the combination of hyperparameters that leads to
    the best model performance is called **hyperparameter tuning**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, we can tune hyperparameter with an exhaustive search over their
    potential values, called a **Grid Search**. Let''s use our random forest model
    to see how we can do this in Python by import `GrisSearchCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we are going to pass the Grid Search a few different hyperparameters
    to check; you can read about what they do in the documentation for the classifier
    ([http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the search, we simply have to initialize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then apply it to the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If we then want to check the performance of the best combination of parameters,
    we can easily do that in sklearn by evaluating it on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Hyperparameter tuning searches can be applied to the neural network models that
    we'll be utilizing in the coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning, and by extension, deep learning, relies on the building blocks
    of linear algebra and statistics at its core. Vectors, matrices, and tensors provide
    the means by which we represent input data and parameters in machine learning
    algorithms, and the computations between these are the core operations of these
    algorithms. Likewise, distributions and probabilities help us model data and events
    in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also covered two classes of algorithms that will inform how we think about
    ANNs in further chapters: supervised learning methods and unsupervised learning
    methods. With supervised learning, we provide the algorithm with a set of features
    and labels, and it learns how to appropriately map certain feature combinations
    ...'
  prefs: []
  type: TYPE_NORMAL
