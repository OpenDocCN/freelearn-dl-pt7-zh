- en: Deep Learning for Game Playing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the past several years, one of the most notable applications of **Artificial** **Intelligence **(**AI**)
    has been in the game-playing space. Especially with the recent success of AlphaGo
    and AlphaGo Zero, game-playing AIs have been a topic of great public interest. In
    this chapter, we'll implement two basic versions of game-playing AIs; one for
    a video game and one for a board game. We'll primarily be utilizing reinforcement
    learning methods as our workhorse. We'll also touch upon the methods behind some
    of the most advanced game-playing AIs in existence at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Game Trees and Fundamental Game Theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing an AI agent to play tic-tac-toe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be utilizing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI gym
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is recommend that you have access to a GPU for training the models in this
    chapter, whether on your machine itself or via a cloud service platform.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI has been making a splash lately in the game-playing arena. DeepMind, the
    Google research group that created AlphaGo, have been proponents of utilizing
    reinforcement learning methods for game-playing applications. More and more video
    game companies are using deep learning methods for their AI players. So, where
    are we coming from, and where are we going with AI in the gaming space?
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, game-playing systems have been made up of a combination of hardcoded
    rules that have covered the range of behaviors that the AI is supposed to cover.
    Have you ever played an older adventure, first-person shooter, or strategy game
    where the AI players were clearly operating off a hardcoded strategy? More often
    than not, these AIs used ...
  prefs: []
  type: TYPE_NORMAL
- en: Networks for board games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we talk about creating algorithms for game-playing, we are really talking
    about creating them for a specific type of game, known as a **finite two person
    zero**-**sum sequential game**. This is really just a fancy way of saying the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: An interactive situation between two independent actors (a game)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a finite amount of ways in which the two actors can interact with
    each other at any point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The game is zero-sum, meaning that the end state of the game results in a complete
    win for one of the actors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The game is sequential, meaning that the actors make their moves in sequence,
    one after another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Classic examples of these types of games that we''ll cover in this section
    are Tic Tac Toe, Chess, and the game of Go. Since creating and training an algorithm
    for a board game such as Go would be an immense task, for time and computation
    constraint, we''ll be creating an agent to compete in a much more reasonable game
    with a finite amount of steps: chess. In this section, we''ll introduce some of
    the fundamental concepts behind game-playing AIs, and walk through examples in
    Python for the different strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding game trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you''re familiar with formal **game theory**, then you know that there is
    an entire branch of mathematics devoted to understanding and analyzing games.
    In the computer science world, we can analyze a game by simplifying it into a
    decision tree structure called a **game tree**. Game trees are a way of mapping
    out possible moves and states within a board game. Let''s take the simple example
    of a game tree for Tic Tac Toe, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f66fda2e-7c5a-4229-bf6b-eb1e35775fb6.png)'
  prefs: []
  type: TYPE_IMG
- en: This tree gives us all of the possible combinations and outcomes for the game
    based on a certain starting point. Moving from one node to another represents
    a **move** in the game, and moves continue ...
  prefs: []
  type: TYPE_NORMAL
- en: AlphaGo and intelligent game-playing AIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While MCTS has been a cornerstone of game-playing AI for a while, it was DeepMind's
    AlphaGo program that really took game-playing AIs into the modern age. AlphaGo
    and its derivatives (AlphaGo Zero and AlphaGo Master) are game-playing AI systems
    that utilize MCTS to play the notoriously difficult ancient Chinese game of Go.
    With 10^(761) possible game combinations, creating a system to play the game became
    something of a milestone in the AI world. It's even the subject of a much talked
    about documentary by the same name.
  prefs: []
  type: TYPE_NORMAL
- en: AlphaGo uses a combination of MCTS with deep learning methods that made the
    Alpha Go programs truly extraordinary. DeepMind trained deep neural networks,
    such as the ones that we have been learning about throughout this book, to learn
    the state of the game and effectively guide the MCTS in a more intelligent manner.
    This network looks at the current state of the board, along with the previous
    moves that have been made, and decides which move to play next. DeepMind's major
    innovation with AlphaGo was to use deep neural networks to understand the state
    of the game, and then use this understanding to intelligently guide the search
    of the MCTS. The system was architected in a way that AlphaGo would teach itself
    to learn the game, first by watching humans play the game, and secondly by playing
    the game against itself and correcting itself for its prior mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture actually uses two different neural networks; a policy network
    and a value network:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Value network**: Reduces the depth of the MCTS search by approximating a
    value function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy network**: Reduces the breadth of the MCTS search by simulating future
    actions. The policy network learns from actual human play of the game Go, and
    develops a policy accordingly:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2827cca8-d649-45e4-8cf4-20107b4b8e06.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's dive into each of these to understand how the system works.
  prefs: []
  type: TYPE_NORMAL
- en: AlphaGo policy network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of the policy network is to capture and understand the general actions
    of players on the board in order to aid the MCTS by guiding the algorithm toward
    promising actions during the search process; this reduces the **breadth of the
    search**. Architecturally, the policy network comes in two parts: a supervised
    learning policy network and a reinforcement learning policy network.'
  prefs: []
  type: TYPE_NORMAL
- en: The first network, the supervised network, is a 13-layer **Convolutional Neural
    Network** (**CNN**). It was trained by observing the moves that humans make while
    playing the game – 30 million, to be exact – and outputs a probability distribution
    for each action given a certain state. We call this type of supervised learning **behavior
    cloning**.
  prefs: []
  type: TYPE_NORMAL
- en: The ...
  prefs: []
  type: TYPE_NORMAL
- en: AlphaGo value network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The value network was used to reduce the error in the system''s play by guiding
    MCTS toward certain nodes. It helped reduce the **depth of the search**. The AlphaGo
    value network was trained by playing further games against itself in order to
    optimize the policy that it learned from the policy networks by estimating the
    value function, specifically the action value function. Recall from [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning*, that action value functions describe the value of taking a certain
    action while in a specific state. It measures the cumulative reward from a pair
    of states and actions; for a given state and action we take, how much will this
    increase our reward? It lets us postulate what would happen by taking a different
    action in the first time step than what they may want the agent to do, and then
    afterward following the policy. The action value function is also often called
    the **Q**-**function**,because of the Q that we use to represent it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31ddcfc7-7f63-4340-a66e-c9434f7e38c9.png)'
  prefs: []
  type: TYPE_IMG
- en: The network approximated the value function by utilizing a noisy version of
    the policy from the policy network and regressing the state of the board against
    the result of the game.
  prefs: []
  type: TYPE_NORMAL
- en: The network was trained using the reinforce algorithm that we learned about
    in [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement Learning*.
    If you recall, Reinforce is a Monte Carlo policy gradient method that uses likelihood
    ratios to estimate the value of a policy at a given point. The Reinforce algorithm
    attempts to maximize the expected reward, so that the entire system has the dual
    goal of playing like a professional human player while attempting to win the game.
  prefs: []
  type: TYPE_NORMAL
- en: AlphaGo in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ve gone over how AlphaGo helped select actions, so now let''s get back
    to the core of any game-playing system: the game tree. While AlphaGo utilized
    game trees and MCTS, the authors created a variation of it called *asynchronous
    policy* and *value MCTS* (APV-MCTS). Unlike standard MCTS, which we discussed
    previously, APV-MCTS decides which node to expand and evaluate by two different
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: The outcome of the value network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outcome of the Monte Carlo simulations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results of these methods are combined with mixing parameters, λ. The algorithm
    then chooses an action according to the probabilities that were obtained during
    the initial supervised learning phase. While it may seem counter intuitive to
    use the probabilities ...
  prefs: []
  type: TYPE_NORMAL
- en: Networks for video games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thus far, we''ve learned how we can use reinforcement learning methods to play
    board games utilizing UCT and MCTS; now, let''s see what we can do with video
    games. In [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning*, we saw how we could use reinforcement learning methods to complete
    basic tasks such as the OpenAI cartpole challenge. In this section, we''ll be
    focusing on a more difficult set of games: classic Atari video games, which have
    become standard benchmarks for deep learning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: You might be thinking – *can't we extend the methods that we used in the cartpole
    environment to Atari games?* While we can, there's a lot more input that we have
    to handle. In Atari environments, and really any video game environment, the inputs
    to the network are individual pixels. Instead of the simple four control variables
    for cartpole, we are now dealing with 100,800 variables (210 * 160 * 3). As such,
    complexity and training times for these networks can be much higher. In this section,
    we'll try to make the network as simple as possible in order to make it easier
    to learn from and train.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be using the OpenAI gym environment to simulate the Atari game Space
    Invaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2798b969-f161-453a-a10a-e9d05fee3ab6.png)'
  prefs: []
  type: TYPE_IMG
- en: For those of you who aren't familiar with Space Invaders, the concept is simple –
    you (the green rocket at the bottom) must destroy a grouping of alien spaceships
    before they destroy you. The spaceships attempt to hit you with missiles, and
    vice versa. Google's DeepMind originally introduced Space Invaders as a benchmark
    task in their paper *Playing Atari with Deep Reinforcement Learning*, which really
    set off the concept of Atari games as benchmarks to beat with these intelligent
    agents.
  prefs: []
  type: TYPE_NORMAL
- en: We'll be constructing something called a **Deep Q-network**, which we touched
    upon in [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning. *In the next section, we expand upon many of the fundamental Q-learning
    subjects that we covered in that chapter. With that – let's dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Constructing a Deep Q–network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Q-networks were first introduced by DeepMind in their paper *Human-level
    control through deep reinforcement* *learning,* published in the British scientific
    journal *Nature*, and now commonly referred to as the *Nature Paper*. The goal
    of Deep Q-learning was to create an AI agent that could learn a policy from high-dimensional
    inputs such as video games. In our case, we'll want to construct a Deep Q-network
    that can advance through basic tasks and then towards harder tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-networks approximate Q-values instead of calculating them individually
    with Q tables, and they do this by using artificial neural networks as a value
    approximator. The input of the network will be a stack of preprocessed frames,
    and the ...
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing a target network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look back at the Q- function optimization process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f120be8-ba2f-4aa8-b4df-31fe58195ff7.png)'
  prefs: []
  type: TYPE_IMG
- en: You might notice that this function has a unique property in that it's recursive;
    one set of Q-values depend on the value of the other set of Q-values. This becomes
    a problem in training; if we change one set of values, we'll end up changing the
    other set of values. To put it simply, as we get closer to the targeted Q-value,
    that Q-value moves even further away. It is continually moving the finish line
    when you are about to finish a race!
  prefs: []
  type: TYPE_NORMAL
- en: 'To remedy this, the Q-network creates a copy of itself every 10,000 iterations,
    called a **target network**, which will represent the targeted Q-value. We can
    do this in TensorFlow by creating a target network variable that we''ll first
    initialize with the class, and later run in a TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As for the 10,000 iterations, we've already defined that as `self.update_time
    = 10000` when we started building out our DQN class.
  prefs: []
  type: TYPE_NORMAL
- en: Experience replay buffer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While we touched upon it briefly in [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning*, let''s dive into the technical details of experience replay buffers.
    Experience replay is a biologically inspired tool that stores an agent''s experience
    at each time step process. Specifically, it stores the [state, action, reward,
    next_state] pairs at each time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8358a2c5-8cb6-4bea-a46a-f3dea5f8c237.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of running Q-learning on state-action pairs as they occur, experience
    replay stores these pairs as they are discovered. Experience replay buffers help
    with two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Remember past experiences by storing and randomly sampling them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the chance that experiences will ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far, we've told our network to follow random weights that we've initialized
    for it, without giving it direction on how to decide what actions to take to update
    those weights. In the case of policy methods such as the ones we used in [Chapter
    8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement Learning*, and
    preceding with Tic Tac Toe, Q-learning methods work toward approximating the value
    of the Q-function instead of learning a policy directly. So, what do we do?
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-networks use a tactic called **exploration** to determine what actions
    to take. If we didn't use a tactic, our network would probably be limited to the
    most basic levels of the game because it wouldn't have any idea what moves would
    allow it to improve!
  prefs: []
  type: TYPE_NORMAL
- en: 'To remedy this, we will utilize a strategy called** ∈-greedy**. This strategy
    works by choosing actions to learn from based on two methods; first, choosing
    methods that give our model the highest reward (maximum Q-value), and second,
    choosing methods at random with the probability ∈. In this strategy, we use a
    basic inequality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eba46b04-720b-4280-9119-5947fa5b3557.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We set the epsilon variable as 1, and draw random numbers from a distribution.
    If the number doesn''t satisfy the inequality and is less than epsilon, our agent **explores** the
    space, meaning it seeks to figure out more information about where it is in order
    to start choosing state/action pairs so that we can calculate the Q-value. If
    the number is in fact larger than epsilon, we **exploit** the information that
    we already know to choose a Q-value to calculate. The algorithm starts with a
    high ∈ and reduces its value by a fixed amount as training continues. We call
    this process **annealing**. In the original DeepMind paper, the researchers used
    this strategy with an annealing from 1 to 0.1 over the first million frames, and
    then held at 0.1 afterward. Let''s look back at the parameters we initialized
    in the beginning of the section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You'll notice that we used these exact specifications. During the testing phase
    of the network, epsilon will be considerably lower, and hence be biased toward
    an exploitation strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement this strategy in Python and take a closer look at the mechanics.
    We''ll define a function, `getAction`, which sits within our `deepQ` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll also adjust our epsilon value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we've defined the bulk of our network, let's move on to training.
  prefs: []
  type: TYPE_NORMAL
- en: Training methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s define our training method. We''ll call this function `trainingPipeline`;
    it will take in an action input as well as a *y* input which represents the targeting
    Q-value, which we''ll define here as placeholders, and calculate a Q-value for
    an action based on those action/state pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e5acb70-ef78-414f-a93e-e7c04e723f4c.png)'
  prefs: []
  type: TYPE_IMG
- en: We'll use a **Mean Squared Error** (**MSE**) loss function and calculate it,
    utilizing the predicted Q-value minus the actual Q-value. Lastly, you might notice
    that we are using an optimizer here that you might not be familiar with, RMSProp.
    It's an adaptive learning rate optimizer similar to Adam that was proposed by
    Geoffrey Hinton. We won't ...
  prefs: []
  type: TYPE_NORMAL
- en: Training the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll give our training function a simple name: `train`. First, we''ll feed
    it mini-batches of data from the replay memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll calculate the Q-value for each batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s bind this all together with out training method. We''ll take the
    `trainStep` variable that we defined and run the training cycle. We''ll feed in
    three variables as input; the targeted Q-value, an action, and a state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll define a handler function to save network weights and states for us.
    While we didn''t go over the definition of the saver explicitly in this chapter,
    you can find it in the fully assembled code in the GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, let''s define the cycle the appends to experience replay:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We've assembled our network, so now let's move on to running it!
  prefs: []
  type: TYPE_NORMAL
- en: Running the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s get to the moment we''ve been waiting for! Let''s import `gym`,
    NumPy, our deep-q network file, as well as a few handler functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll define our agent class as `Atari`, and initialize the environment, network,
    and actions with the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Our Deep Q-network can''t innately ingest the Atari games, so we have to write
    a bit of preprocessing code to handle the video input. We''ll call this function
    `preprocess` and it will take in a single game observation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've learned a great deal in this chapter, from how to implement MCTS methods
    to play board games, to creating an advanced network to play an Atari game, and
    even the technology behind the famous AlphaGo system. Let's recap what we have
    learned.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning methods have become the main tools to create AIs for
    playing games. Whether we are creating systems for real-life board games, or systems
    for video games, the fundamental concepts of policies, Q-learning, and more that
    we learned about in [Chapter 8](91114074-444f-4201-98ef-e510210380f2.xhtml), *Reinforcement
    Learning*, form the basis for these complex AI systems. When we create AIs for
    board games, we rely on the building block of the game tree, and use MCTS to simulate
    various game outcomes from that game tree. For more advanced systems such as AlphaGo
    and chess-playing AIs, we utilize neural networks to help guide MCTS and make
    its simulations more effective.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to video game-playing AIs, we can utilize either policy gradient
    methods or Q-learning methods. In this chapter, we learned about utilizing a variant
    of the latter, deep Q-learning, to play the Atari game Space Invaders. Deep Q-learning
    makes advances on basic Q-learning by using techniques such as target networks
    and experience replay buffers to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look more into how reinforcement learning methods can create intelligent
    systems in one of our upcoming chapters on deep learning for robotics.
  prefs: []
  type: TYPE_NORMAL
