["```py\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n% matplotlib inline\n```", "```py\nclass ArtificialNeuron:\n    def __init__(self,N=2, act_func=tf.nn.sigmoid, learning_rate= 0.001):\n        self.N = N # Number of inputs to the neuron\n        self.act_fn = act_func\n\n        # Build the graph for a single neuron\n        self.X = tf.placeholder(tf.float32, name='X', shape=[None,N])\n        self.y = tf.placeholder(tf.float32, name='Y')\n```", "```py\nself.W = tf.Variable(tf.random_normal([N,1], stddev=2, seed = 0), name = \"weights\")\n        self.bias = tf.Variable(0.0, dtype=tf.float32, name=\"bias\")\n        tf.summary.histogram(\"Weights\",self.W)\n        tf.summary.histogram(\"Bias\", self.bias)\n```", "```py\nactivity = tf.matmul(self.X, self.W) + self.bias\nself.y_hat = self.act_fn(activity)\n```", "```py\nerror = self.y - self.y_hat\n\nself.loss = tf.reduce_mean(tf.square(error))\nself.opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)\n\n```", "```py\ntf.summary.scalar(\"loss\",self.loss)\ninit = tf.global_variables_initializer()\n\nself.sess = tf.Session()\nself.sess.run(init)\n\nself.merge = tf.summary.merge_all()\nself.writer = tf.summary.FileWriter(\"logs/\",graph=tf.get_default_graph())\n```", "```py\ndef train(self, X, Y, X_val, Y_val, epochs=100):\nepoch = 0\nX, Y = shuffle(X,Y)\nloss = []\nloss_val = []\nwhile epoch &amp;lt; epochs:\n            # Run the optimizer for the whole training set batch wise (Stochastic Gradient Descent)     \n            merge, _, l = self.sess.run([self.merge,self.opt,self.loss], feed_dict={self.X: X, self.y: Y})    \n            l_val = self.sess.run(self.loss, feed_dict={self.X: X_val, self.y: Y_val})    \n\n            loss.append(l)\n            loss_val.append(l_val)\n            self.writer.add_summary(merge, epoch)    \n\n            if epoch % 10 == 0:\n                print(\"Epoch {}/{} training loss: {} Validation loss {}\".\\    \n                    format(epoch,epochs,l, l_val ))    \n\n            epoch += 1\n        return loss, loss_val\n```", "```py\n    def predict(self, X):\n        return self.sess.run(self.y_hat, feed_dict={self.X: X})\n```", "```py\nfilename = 'Folds5x2_pp.xlsx'\ndf = pd.read_excel(filename, sheet_name='Sheet1')\nX, Y = df[['AT', 'V','AP','RH']], df['PE']\nscaler = MinMaxScaler()\nX_new = scaler.fit_transform(X)\ntarget_scaler = MinMaxScaler()\nY_new = target_scaler.fit_transform(Y.values.reshape(-1,1))\nX_train, X_val, Y_train, y_val = \\\n        train_test_split(X_new, Y_new, test_size=0.4, random_state=333)\n```", "```py\n_, d = X_train.shape\nmodel = ArtificialNeuron(N=d)\n\nloss, loss_val = model.train(X_train, Y_train, X_val, y_val, 30000)\n\nplt.plot(loss, label=\"Taining Loss\")\nplt.plot(loss_val, label=\"Validation Loss\")\nplt.legend()\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Mean Square Error\")\n```", "```py\nclass MLP:\n    def __init__(self,n_input=2,n_hidden=4, n_output=1, act_func=[tf.nn.elu, tf.sigmoid], learning_rate= 0.001):\n        self.n_input = n_input # Number of inputs to the neuron\n        self.act_fn = act_func\n        seed = 123\n\n        self.X = tf.placeholder(tf.float32, name='X', shape=[None,n_input])\n        self.y = tf.placeholder(tf.float32, name='Y')\n\n        # Build the graph for a single neuron\n        # Hidden layer\n        self.W1 = tf.Variable(tf.random_normal([n_input,n_hidden],\\\n                 stddev=2, seed = seed), name = \"weights\")    \n        self.b1 = tf.Variable(tf.random_normal([1, n_hidden], seed = seed),\\\n                    name=\"bias\")    \n        tf.summary.histogram(\"Weights_Layer_1\",self.W1)\n        tf.summary.histogram(\"Bias_Layer_1\", self.b1)\n\n        # Output Layer\n        self.W2 = tf.Variable(tf.random_normal([n_hidden,n_output],\\\n                stddev=2, seed = 0), name = \"weights\")\n        self.b2 = tf.Variable(tf.random_normal([1, n_output], seed = seed),\\\n                name=\"bias\")\n        tf.summary.histogram(\"Weights_Layer_2\",self.W2)\n        tf.summary.histogram(\"Bias_Layer_2\", self.b2)\n\n        activity = tf.matmul(self.X, self.W1) + self.b1\n        h1 = self.act_fn[0](activity)\n\n        activity = tf.matmul(h1, self.W2) + self.b2\n        self.y_hat = self.act_fn[1](activity)\n\n        error = self.y - self.y_hat\n\n        self.loss = tf.reduce_mean(tf.square(error))\\\n                 + 0.6*tf.nn.l2_loss(self.W1) \n        self.opt = tf.train.GradientDescentOptimizer(learning_rate\\\n                    =learning_rate).minimize(self.loss)        \n\n        tf.summary.scalar(\"loss\",self.loss)\n        init = tf.global_variables_initializer()\n\n        self.sess = tf.Session()\n        self.sess.run(init)\n\n        self.merge = tf.summary.merge_all()\n        self.writer = tf.summary.FileWriter(\"logs/\",\\\n                graph=tf.get_default_graph())\n\n     def train(self, X, Y, X_val, Y_val, epochs=100):\n        epoch = 0\n        X, Y = shuffle(X,Y)\n        loss = []\n        loss_val = []\n        while epoch &amp;lt; epochs:\n            # Run the optimizer for the training set \n            merge, _, l = self.sess.run([self.merge,self.opt,self.loss],\\\n                     feed_dict={self.X: X, self.y: Y})\n            l_val = self.sess.run(self.loss, feed_dict=\\\n                    {self.X: X_val, self.y: Y_val})\n\n            loss.append(l)\n            loss_val.append(l_val)\n            self.writer.add_summary(merge, epoch)\n\n            if epoch % 10 == 0:\n                print(\"Epoch {}/{} training loss: {} Validation loss {}\".\\\n                    format(epoch,epochs,l, l_val ))\n\n            epoch += 1\n        return loss, loss_val\n\n    def predict(self, X):\n        return self.sess.run(self.y_hat, feed_dict={self.X: X})\n```", "```py\nself.loss = tf.reduce_mean(tf.square(error)) + 0.6*tf.nn.l2_loss(self.W1) \n```", "```py\n_, d = X_train.shape\n_, n = Y_train.shape\nmodel = MLP(n_input=d, n_hidden=15, n_output=n)\n```", "```py\nloss, loss_val = model.train(X_train, Y_train, X_val, y_val, 6000)\n```", "```py\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n% matplotlib inline\n```", "```py\nclass MLP:\n    def __init__(self,n_input=2,n_hidden=4, n_output=1, act_func=[tf.nn.relu, tf.nn.sigmoid], learning_rate= 0.001):\n        self.n_input = n_input # Number of inputs to the neuron\n        self.act_fn = act_func\n        seed = 456\n\n        self.X = tf.placeholder(tf.float32, name='X', shape=[None,n_input])\n        self.y = tf.placeholder(tf.float32, name='Y')\n\n        # Build the graph for a single neuron\n        # Hidden layer\n        self.W1 = tf.Variable(tf.random_normal([n_input,n_hidden],\\\n             stddev=2, seed = seed), name = \"weights\")\n        self.b1 = tf.Variable(tf.random_normal([1, n_hidden],\\\n             seed = seed), name=\"bias\")\n        tf.summary.histogram(\"Weights_Layer_1\",self.W1)\n        tf.summary.histogram(\"Bias_Layer_1\", self.b1)\n\n        # Output Layer\n        self.W2 = tf.Variable(tf.random_normal([n_hidden,n_output],\\\n            stddev=2, seed = seed), name = \"weights\")\n        self.b2 = tf.Variable(tf.random_normal([1, n_output],\\\n             seed = seed), name=\"bias\")    \n        tf.summary.histogram(\"Weights_Layer_2\",self.W2)\n        tf.summary.histogram(\"Bias_Layer_2\", self.b2)\n\n        activity1 = tf.matmul(self.X, self.W1) + self.b1\n        h1 = self.act_fn[0](activity1)\n\n        activity2 = tf.matmul(h1, self.W2) + self.b2\n        self.y_hat = self.act_fn[1](activity2)\n\n        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\\\n                logits=self.y_hat, labels=self.y))\n        self.opt = tf.train.AdamOptimizer(learning_rate=\\\n                learning_rate).minimize(self.loss)\n\n        tf.summary.scalar(\"loss\",self.loss)\n        init = tf.global_variables_initializer()\n\n        self.sess = tf.Session()\n        self.sess.run(init)\n\n        self.merge = tf.summary.merge_all()\n        self.writer = tf.summary.FileWriter(\"logs/\",\\\n             graph=tf.get_default_graph())\n\n    def train(self, X, Y, X_val, Y_val, epochs=100):\n        epoch = 0\n        X, Y = shuffle(X,Y)\n        loss = []\n        loss_val = []\n        while epoch &amp;lt; epochs:\n            # Run the optimizer for the training set \n            merge, _, l = self.sess.run([self.merge,self.opt,self.loss],\\\n                 feed_dict={self.X: X, self.y: Y})        \n            l_val = self.sess.run(self.loss, feed_dict={self.X: X_val, self.y: Y_val})\n\n            loss.append(l)\n            loss_val.append(l_val)\n            self.writer.add_summary(merge, epoch)\n\n            if epoch % 10 == 0:\n                print(\"Epoch {}/{} training loss: {} Validation loss {}\".\\\n                    format(epoch,epochs,l, l_val ))\n\n            epoch += 1\n        return loss, loss_val\n\n    def predict(self, X):\n        return self.sess.run(self.y_hat, feed_dict={self.X: X})\n```", "```py\nfilename = 'winequality-red.csv' \n#Download the file from https://archive.ics.uci.edu/ml/datasets/wine+quality\ndf = pd.read_csv(filename, sep=';')\ncolumns = df.columns.values\n# Preprocessing and Categorizing wine into two categories\nX, Y = df[columns[0:-1]], df[columns[-1]]\nscaler = MinMaxScaler()\nX_new = scaler.fit_transform(X)\n#Y.loc[(Y&amp;lt;3.5)]=3\nY.loc[(Y&amp;lt;5.5) ] = 2\nY.loc[(Y&amp;gt;=5.5)] = 1\nY_new = pd.get_dummies(Y) # One hot encode\nX_train, X_val, Y_train, y_val = \\\n train_test_split(X_new, Y_new, test_size=0.2, random_state=333)\n```", "```py\n_, d = X_train.shape\n_, n = Y_train.shape\nmodel = MLP(n_input=d, n_hidden=5, n_output=n)\nloss, loss_val = model.train(X_train, Y_train, X_val, y_val, 10000)\n```", "```py\nplt.plot(loss, label=\"Taining Loss\")\nplt.plot(loss_val, label=\"Validation Loss\")\nplt.legend()\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Cross Entropy Loss\")\n```", "```py\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport seaborn as sns\ncm = confusion_matrix(np.argmax(np.array(y_val),1), np.argmax(Y_pred,1))\nsns.heatmap(cm,annot=True,fmt='2.0f')\n```", "```py\n# Import Modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```", "```py\n# Define your Architecture here\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import flatten\nclass my_LeNet:\n    def __init__(self, d, n, mu = 0, sigma = 0.1, lr = 0.001):\n        self.mu = mu\n        self.sigma = sigma\n        self.n = n\n        # place holder for input image dimension 28 x 28\n        self.x = tf.placeholder(tf.float32, (None, d, d, 1)) \n        self.y = tf.placeholder(tf.int32, (None,n))\n        self.keep_prob = tf.placeholder(tf.float32) # probability to keep units\n\n        self.logits = self.model(self.x)\n        # Define the loss function\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.y,\\\n                        logits=self.logits)\n        self.loss = tf.reduce_mean(cross_entropy)\n        optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n        self.train = optimizer.minimize(self.loss)\n        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n        init = tf.global_variables_initializer()\n        self.sess = tf.Session()\n        self.sess.run(init)\n        self.saver = tf.train.Saver()\n```", "```py\ndef model(self,x):\n    # Build Architecture\n    keep_prob = 0.7\n    # Layer 1: Convolutional. Filter 5x5 num_filters = 6 Input_depth =1\n    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean \\\n                    = self.mu, stddev = self.sigma))\n    conv1_b = tf.Variable(tf.zeros(6))\n    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n    conv1 = tf.nn.relu(conv1)\n\n    # Max Pool 1\n    self.conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1],\\\n                     strides=[1, 2, 2, 1], padding='VALID')\n\n    # Layer 2: Convolutional. Filter 5x5 num_filters = 16 Input_depth =6\n    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), \\\n                    mean = self.mu, stddev = self.sigma))\n    conv2_b = tf.Variable(tf.zeros(16))\n    conv2 = tf.nn.conv2d(self.conv1, conv2_W, strides=[1, 1, 1, 1],\\\n                     padding='VALID') + conv2_b\n    conv2 = tf.nn.relu(conv2)\n\n    # Max Pool 2.\n    self.conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], \\\n                    strides=[1, 2, 2, 1], padding='VALID')\n\n    # Flatten.\n    fc0 = flatten(self.conv2)\n    print(\"x shape:\",fc0.get_shape())\n\n    # Layer 3: Fully Connected. Input = fc0.get_shape[-1]. Output = 120.\n    fc1_W = tf.Variable(tf.truncated_normal(shape=(256, 120), \\\n                mean = self.mu, stddev = self.sigma))\n    fc1_b = tf.Variable(tf.zeros(120))\n    fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n    fc1 = tf.nn.relu(fc1)\n\n    # Dropout\n    x = tf.nn.dropout(fc1, keep_prob)\n\n    # Layer 4: Fully Connected. Input = 120\\. Output = 84.\n    fc2_W = tf.Variable(tf.truncated_normal(shape=(120, 84), \\\n                    mean = self.mu, stddev = self.sigma))\n    fc2_b = tf.Variable(tf.zeros(84))\n    fc2 = tf.matmul(x, fc2_W) + fc2_b\n    fc2 = tf.nn.relu(fc2)\n\n    # Dropout\n    x = tf.nn.dropout(fc2, keep_prob)\n\n    # Layer 6: Fully Connected. Input = 120\\. Output = n_classes.\n    fc3_W = tf.Variable(tf.truncated_normal(shape=(84, self.n), \\\n                    mean = self.mu, stddev = self.sigma))\n    fc3_b = tf.Variable(tf.zeros(self.n))\n    logits = tf.matmul(x, fc3_W) + fc3_b\n    #logits = tf.nn.softmax(logits)\n    return logits\n```", "```py\ndef fit(self,X,Y,X_val,Y_val,epochs=10, batch_size=100):\n    X_train, y_train = X, Y\n    num_examples = len(X_train)\n    l = []\n    val_l = []\n    max_val = 0\n    for i in range(epochs):\n        total = 0\n        for offset in range(0, num_examples, batch_size): # Learn Batch wise\n            end = offset + batch_size\n            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n            _, loss = self.sess.run([self.train,self.loss], \\\n                        feed_dict={self.x: batch_x, self.y: batch_y})\n            total += loss\n            l.append(total/num_examples)\n            accuracy_val = self.sess.run(self.accuracy, \\\n                                feed_dict={self.x: X_val, self.y: Y_val})\n            accuracy = self.sess.run(self.accuracy, feed_dict={self.x: X, self.y: Y})\n            loss_val = self.sess.run(self.loss, feed_dict={self.x:X_val,self.y:Y_val})\n            val_l.append(loss_val)\n            print(\"EPOCH {}/{} loss is {:.3f} training_accuracy {:.3f} and \\\n                        validation accuracy is {:.3f}\".\\\n                        format(i+1,epochs,total/num_examples, accuracy, accuracy_val))\n            # Saving the model with best validation accuracy\n            if accuracy_val &amp;gt; max_val:\n                save_path = self.saver.save(self.sess, \"/tmp/lenet1.ckpt\")\n                print(\"Model saved in path: %s\" % save_path)\n                max_val = accuracy_val\n\n    #Restore the best model\n    self.saver.restore(self.sess, \"/tmp/lenet1.ckpt\")\n    print(\"Restored model with highest validation accuracy\")\n    accuracy_val = self.sess.run(self.accuracy, feed_dict={self.x: X_val, self.y: Y_val})\n    accuracy = self.sess.run(self.accuracy, feed_dict={self.x: X, self.y: Y})\n    return l,val_l, accuracy, accuracy_val\n\ndef predict(self, X):\n    return self.sess.run(self.logits,feed_dict={self.x:X})\n```", "```py\ndef load_data():\n    # Read the data and create train, validation and test dataset\n    data = pd.read_csv('train.csv')\n    # This ensures always 80% of data is training and \n    # rest Validation unlike using np.random\n    train = data.sample(frac=0.8, random_state=255) \n    val = data.drop(train.index)\n    test = pd.read_csv('test.csv')\n    return train, val, test\n\ndef create_data(df):\n    labels = df.loc[:]['label']\n    y_one_hot = pd.get_dummies(labels).astype(np.uint8)\n    y = y_one_hot.values # One Hot encode the labels\n    x = df.iloc[:,1:].values\n    x = x.astype(np.float)\n    # Normalize data\n    x = np.multiply(x, 1.0 / 255.0)\n    x = x.reshape(-1, 28, 28, 1) # return each images as 96 x 96 x 1\n    return x,y\n\ntrain, val, test = load_data()\nX_train, y_train = create_data(train)\nX_val, y_val = create_data(val)\nX_test = (test.iloc[:,:].values).astype(np.float)\nX_test = np.multiply(X_test, 1.0 / 255.0)\nX_test = X_test.reshape(-1, 28, 28, 1) # return each images as 96 x 96 x 1\n\n# Plot a subset of training data\nx_train_subset = X_train[:12]\n\n# visualize subset of training data\nfig = plt.figure(figsize=(20,2))\nfor i in range(0, len(x_train_subset)):\n    ax = fig.add_subplot(1, 12, i+1)\n    ax.imshow(x_train_subset[i].reshape(28,28), cmap='gray')\nfig.suptitle('Subset of Original Training Images', fontsize=20)\nplt.show()\n```", "```py\nn_train = len(X_train)\n# Number of validation examples\nn_validation = len(X_val)\n\n# Number of testing examples.\nn_test = len(X_test)\n\n# What's the shape of an handwritten digits?\nimage_shape = X_train.shape[1:-1]\n\n# How many unique classes/labels there are in the dataset.\nn_classes = y_train.shape[-1]\nprint(\"Number of training examples =\", n_train)\nprint(\"Number of Validation examples =\", n_validation)\nprint(\"Number of testing examples =\", n_test)\nprint(\"Image data shape =\", image_shape)\nprint(\"Number of classes =\", n_classes)\n\n# The result\n## &amp;gt;&amp;gt;&amp;gt; Number of training examples = 33600\n## &amp;gt;&amp;gt;&amp;gt; Number of Validation examples = 8400 \n## &amp;gt;&amp;gt;&amp;gt; Number of testing examples = 28000 \n## &amp;gt;&amp;gt;&amp;gt; Image data shape = (28, 28) \n## &amp;gt;&amp;gt;&amp;gt; Number of classes = 10\n\n# Define the data values\nd = image_shape[0]\nn = n_classes\nfrom sklearn.utils import shuffle\nX_train, y_train = shuffle(X_train,y_train)\n```", "```py\n# Create the Model\nmy_model = my_LeNet(d, n)\n\n### Train model  here.\nloss, val_loss, train_acc, val_acc = my_model.fit(X_train, y_train, \\\n    X_val, y_val, epochs=50) \n```", "```py\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport numpy as np\n```", "```py\n class LSTM:\n    def __init__(self, num_units, n_classes, n_input,\\\n             time_steps, learning_rate=0.001,):    \n        tf.reset_default_graph()\n        self.steps = time_steps\n        self.n = n_input\n        # weights and biases of appropriate shape\n        out_weights = tf.Variable(tf.random_normal([num_units, n_classes]))\n        out_bias = tf.Variable(tf.random_normal([n_classes]))\n        # defining placeholders\n        # input placeholder\n        self.x = tf.placeholder(\"float\", [None, self.steps, self.n])\n        # label placeholder\n        self.y = tf.placeholder(\"float\", [None, n_classes])\n        # processing the input tensor from [batch_size,steps,self.n] to \n        # \"steps\" number of [batch_size,self.n] tensors\n        input = tf.unstack(self.x, self.steps, 1)\n\n        # defining the network\n        lstm_layer = rnn.BasicLSTMCell(num_units, forget_bias=1)\n        outputs, _ = rnn.static_rnn(lstm_layer, input, dtype=\"float32\")\n        # converting last output of dimension [batch_size,num_units] to \n        # [batch_size,n_classes] by out_weight multiplication\n        self.prediction = tf.matmul(outputs[-1], out_weights) + out_bias\n\n        # loss_function\n        self.loss = tf.reduce_mean(tf.squared_difference(self.prediction, self.y))\n        # optimization\n        self.opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)\n\n        # model evaluation\n        correct_prediction = tf.equal(tf.argmax(self.prediction, 1), tf.argmax(self.y, 1))\n        self._accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        init = tf.global_variables_initializer()\n        gpu_options = tf.GPUOptions(allow_growth=True)\n\n        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n        self.sess.run(init)\n```", "```py\ndef train(self, X, Y, epochs=100,batch_size=128):\n    iter = 1\n    #print(X.shape)\n    X = X.reshape((len(X),self.steps,self.n))\n    while iter &amp;lt; epochs:\n        for i in range(int(len(X)/batch_size)):\n            batch_x, batch_y = X[i:i+batch_size,:], Y[i:i+batch_size,:]\n            #print(batch_x.shape)\n            #batch_x = batch_x.reshape((batch_size, self.steps, self.n))    \n            #print(batch_x.shape)    \n            self.sess.run(self.opt, feed_dict={self.x: batch_x, self.y: batch_y})\n            if iter % 10 == 0:\n                acc = self.sess.run(self._accuracy, feed_dict={self.x: X, self.y: Y})\n                los = self.sess.run(self.loss, feed_dict={self.x: X, self.y: Y})\n                print(\"For iter \", iter)\n                print(\"Accuracy \", acc)\n                print(\"Loss \", los)    \n                print(\"__________________\")\n            iter = iter + 1\n\ndef predict(self,X):\n    # predicting the output\n    test_data = X.reshape((-1, self.steps, self.n))\n    out = self.sess.run(self.prediction, feed_dict={self.x:test_data})\n    return out\n```"]