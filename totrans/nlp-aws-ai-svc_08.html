<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer131">
			<h1 id="_idParaDest-85"><em class="italic"><a id="_idTextAnchor089"/>Chapter 6</em>: Using NLP to Improve Customer Service Efficiency</h1>
			<p>So far, we have seen a couple of interesting real-world NLP use cases with intelligent document processing solutions for loan applications in <a href="B17528_04_Final_SB_ePub.xhtml#_idTextAnchor063"><em class="italic">Chapter 4</em></a><em class="italic">,</em> <em class="italic">Automating Document Processing Workflows</em>, and built smart search indexes in <a href="B17528_05_Final_SB_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 5</em></a><em class="italic">, Creating NLP Search</em>. NLP-based indexing for content search is becoming very popular because it bridges the gap between traditional keyword-based searches, which can be frustrating unless you know exactly what keyword to use, and natural language, to quickly search for what you are interested in. We also saw how we can use Amazon Textract and Amazon Comprehend with services such as Amazon Elasticsearch (<a href="https://aws.amazon.com/elasticsearch-service/">https://aws.amazon.com/elasticsearch-service/</a>), a service that's fully managed by AWS and provides search and analytics capabilities offered by the open source Elasticsearch, but without the need for infrastructure heavy lifting, installation, or maintenance associated with setting up an Elasticsearch cluster, and Amazon Kendra (<a href="https://aws.amazon.com/kendra/">https://aws.amazon.com/kendra/</a>), a fully managed enterprise search engine powered by ML that provides NLP-based search capabilities, to create an end-to-end smart search solution. In this chapter, we will address a ubiquitous use case that has been around for decades, if not centuries, and yet remains highly important for any business; that is, customer service improvement.</p>
			<p>Businesses cannot thrive without customers, and customer satisfaction is a key metric that has a direct correlation to the profitability of an organization. While the touchpoints that organizations have with customers during the sales cycle are important, what is even more important is the effectiveness of their customer service process. Organizations need to respond quickly to customer feedback, understand the emotional undercurrent of a customer conversation, and resolve their issues in the shortest possible time. Happy customers are loyal customers and, of course, this means that the customer churn will be low, which will help keep costs low and improve profitability.</p>
			<p>To see improving customer service in action, we will build an AI solution that uses the AWS NLP service known as Amazon Comprehend to analyze historical customer service records to derive key topics using Amazon Comprehend Topic Modeling, train a custom classification model that will predict routing topics for call routing using Amazon Comprehend Custom Classification, and use Amazon Comprehend Detect Sentiments to understand the emotional sentiment of the customer feedback. We will be hands-on throughout this chapter, but we have all the code samples we need to get going.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Introducing the customer service use case</li>
				<li>Building an NLP solution to improve customer service</li>
			</ul>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor090"/>Technical requirements</h1>
			<p>For this chapter, you will need access to an AWS account. Please make sure that you follow the instructions specified in the <em class="italic">Technical requirements</em> section of <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, to create your AWS account. You will also need to log into the AWS Management Console before trying the steps in the <em class="italic">Building an NLP solution to improve customer service</em> section.</p>
			<p>The Python code and sample datasets for our solution can be found at <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2006">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2006</a>. Please use the instructions in the following sections, along with the code in this repository, to build the solution.</p>
			<p>Check out the following video to see the Code in Action at <a href="https://bit.ly/2ZpWveN">https://bit.ly/2ZpWveN</a>.</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor091"/>Introducing the customer service use case</h1>
			<p>So, how can NLP help us improve customer service? To illustrate our example, let's go back to our <a id="_idIndexMarker377"/>fictitious banking corporation, <strong class="bold">LiveRight Holdings private limited</strong>. <strong class="bold">LiveRight</strong> has contact centers in many states of the US, and they receive more than 100,000 calls every day from customers with queries and issues on <a id="_idIndexMarker378"/>various topics, such as credit, accounts, debt, and more. While they have a competent team of agents who are highly experienced in handling customer requests, their first-tier triage teams often struggle with interpreting the nature of the customer's request within the first minute of conversation, which is an important SLA for them. This is required to determine which agents to route the request to. They have a team of specialized agents based on product type and experience levels. Junior agents handle customers who are happy with the products, while the challenge of dealing with irate customers is often the task of more experienced agents.</p>
			<p><strong class="bold">LiveRight's</strong> senior management is unhappy with the first-tier team's performance as they are constantly failing to meet the 1-minute SLA. This is further exacerbated by the fact that in the last 3 months, the first-tier team has been incorrectly routing unhappy customers to junior agents, resulting in an increased customer churn. Therefore, senior management wants to automate the first-tier triage process, which will enable their teams to address these issues. <strong class="bold">LiveRight</strong> has hired you to design a solution architecture that can automatically determine the routing option and the sentiment of the customer conversation. As the enterprise architect for the project, you have decided to use Amazon <a id="_idIndexMarker379"/>Comprehend to leverage its pre-trained ML model for sentiment detection, Comprehend's built-in Topic Modeling feature to determine common themes in a training dataset to determine routing option labels, and the Custom Classifier feature of Amazon Comprehend to incrementally create your own classifier for customer request routing, without the need to build complex NLP algorithms. The components of the solution we will build are shown in the following diagram:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="Images/B17528_06_01.jpg" alt="Figure 6.1 – NLP solution build for customer service" width="1180" height="585"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – NLP solution build for customer service</p>
			<p>We will be walking through this solution using an Amazon SageMaker Jupyter notebook, which will allow us to review the code and results as we execute it step by step. For code samples on how to build this solution as a real-time workflow using AWS Lambda (a serverless, event-driven compute service for running code), please refer to the <em class="italic">Further reading</em> section:</p>
			<ol>
				<li>As a first step, we will preprocess our input dataset, which contains consumer complaints available in this book's GitHub repository, load this into an S3 bucket, and run an Amazon Comprehend Topic Modeling job to determine routing option labels.</li>
				<li>We will then create the training dataset with the routing option labels that have been assigned to the consumer complaints from our input dataset, and then upload this into an S3 bucket.</li>
				<li>We will <a id="_idIndexMarker380"/>use Amazon Comprehend Custom Classification to train a classifier model using the training dataset we created previously.</li>
				<li>Finally, we will create an Amazon Comprehend real-time endpoint to deploy the trained model and show you how to predict the routing option. We will then show you how to use the Amazon Comprehend Detect Sentiment API to determine the sentiment of the customer conversation in real time.</li>
			</ol>
			<p>In this section, we introduced the customer service problem we are trying to solve with our NLP solution, reviewed the challenges faced by <strong class="bold">LiveRight</strong>, and looked at an overview of the solution we will build. In the next section, we will walk through the build of the solution step by step.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor092"/>Building an NLP solution to improve customer service</h1>
			<p>In the <a id="_idIndexMarker381"/>previous section, we introduced the contact <a id="_idIndexMarker382"/>center use case for customer service, covered the architecture of the solution we will be building, and briefly walked through the solution components and workflow steps. In this section, we will start executing the tasks to build our solution. But first, there are some prerequisites that we must take care of.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor093"/>Setting up to solve the use case</h2>
			<p>If you have <a id="_idIndexMarker383"/>not done so already in the previous chapters, you will have to create an Amazon SageMaker Jupyter notebook, and then set up <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) permissions for that notebook role to access the <a id="_idIndexMarker384"/>AWS services we will use in this notebook. After that, you will need to clone this book's GitHub repository (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services</a>), create an Amazon S3 (<a href="https://aws.amazon.com/s3/">https://aws.amazon.com/s3/</a>) bucket, go to the <strong class="source-inline">Chapter 06</strong> folder, open the <strong class="source-inline">chapter6-nlp-in-customer-service-github.ipynb</strong> notebook, and provide the bucket name in the notebook to start execution. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Please ensure you have completed the tasks mentioned in the <em class="italic">Technical requirements</em> section.</p>
			<p>If you have already completed the following steps in one of the previous chapters, please go to the <em class="italic">Preprocessing the customer service history data</em> section:</p>
			<ol>
				<li value="1">Please refer to the Amazon SageMaker documentation to create a notebook instance: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html">https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html</a>. To follow these steps, please sign into <strong class="bold">AWS Management Console</strong> and type in and select <strong class="bold">Amazon SageMaker</strong> from the search window. Then, navigate to the <strong class="bold">Amazon SageMaker</strong> console.</li>
				<li>Select <strong class="bold">Notebook instances</strong> and create a Notebook instance by specifying an instance type, storage, and an IAM role.<p class="callout-heading">IAM role permissions while creating Amazon SageMaker Jupyter notebooks</p><p class="callout">Accept the default for the IAM role at notebook creation time to allow access to any S3 bucket. Select <strong class="bold">ComprehendFullAccess</strong> as a permission policy by clicking on the IAM role and navigating to the Identity and Access Management console for the role being created. You can always go back to the IAM role for your notebook instances and attach other permissions policies as required.</p></li>
				<li>Once <a id="_idIndexMarker385"/>you've created the notebook instance and its status is <strong class="bold">InService</strong>, click on <strong class="bold">Open Jupyter</strong> in the <strong class="bold">Actions</strong> menu heading for the notebook instance:<div id="_idContainer115" class="IMG---Figure"><img src="Images/B17528_06_02.jpg" alt="Figure 6.2 – Opening the Jupyter notebook&#13;&#10;" width="1142" height="362"/></div><p class="figure-caption">Figure 6.2 – Opening the Jupyter notebook</p><p>This will take you to the home folder of your notebook instance.</p></li>
				<li>Click on <strong class="bold">New</strong> and select <strong class="bold">Terminal</strong>, as shown in the following screenshot:<div id="_idContainer116" class="IMG---Figure"><img src="Images/B17528_06_03.jpg" alt="Figure 6.3 – Opening a Terminal in a Jupyter notebook&#13;&#10;" width="1123" height="547"/></div><p class="figure-caption">Figure 6.3 – Opening a Terminal in a Jupyter notebook</p></li>
				<li>In the <a id="_idIndexMarker386"/>Terminal window, type <strong class="source-inline">cd SageMaker</strong> and then <strong class="source-inline">git clone </strong><a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services</a>, as shown in the following screenshot:<div id="_idContainer117" class="IMG---Figure"><img src="Images/B17528_06_04.jpg" alt="Figure 6.4 – git clone command&#13;&#10;" width="946" height="154"/></div><p class="figure-caption">Figure 6.4 – git clone command</p></li>
				<li>Now, exit the Terminal window and go back to the home folder. You will see a folder called <strong class="source-inline">Natural-Language-Processing-with-AWS-AI-Services</strong>. Click it; you will see a folder called <strong class="source-inline">Chapter 06</strong>. Click this folder; you should see a notebook called <strong class="source-inline">chapter6-nlp-in-customer-service-github</strong>.</li>
				<li>Open this notebook by clicking it.</li>
				<li>Follow the steps in this notebook that correspond to the next few subheadings in this section by executing one cell at a time. Please read the descriptions provided above each notebook cell.</li>
			</ol>
			<p>Now that <a id="_idIndexMarker387"/>we have set up our notebook and cloned the repository, let's add the permissions policies we need to successfully run our code sample.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor094"/>Additional IAM prerequisites</h2>
			<p>To train the Comprehend custom entity recognizer and set up real-time endpoints, we have to <a id="_idIndexMarker388"/>enable additional policies and also update the Trust Relationships for our SageMaker notebook role. Please complete the following steps to do this:</p>
			<ol>
				<li value="1">Please attach the <strong class="source-inline">ComprehendFullAccess</strong> policies to your Amazon SageMaker Notebook IAM role. To execute this step, please refer to the <em class="italic">Changing IAM permissions and Trust relationships for the Amazon SageMaker notebook execution role</em> subsection in the <em class="italic">Setting up your AWS environment</em> section of <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a><em class="italic">,</em> <em class="italic">Introducing Amazon Textract</em>.</li>
				<li>Your SageMaker Execution Role should have access to S3 already. If not, add the following JSON statement as an inline policy. For instructions, please refer to the <em class="italic">Changing IAM permissions and Trust relationships for the Amazon SageMaker notebook execution role</em> subsection in the <em class="italic">Setting up your AWS environment</em> section of <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a><em class="italic">,</em> <em class="italic">Introducing Amazon Textract</em>:<p class="source-code">{ "Version": "2012-10-17", "Statement": [ {</p><p class="source-code">  "Action": [</p><p class="source-code">      "s3:GetObject",</p><p class="source-code">      "s3:ListBucket",</p><p class="source-code">      "s3:PutObject"</p><p class="source-code">  ],</p><p class="source-code">  "Resource": ["*"],</p><p class="source-code">  "Effect": "Allow"</p><p class="source-code">      }</p><p class="source-code">  ]</p><p class="source-code">}</p></li>
				<li>Finally, update <a id="_idIndexMarker389"/>the Trust relationships for your SageMaker Notebook execution role. For instructions, please refer to the <em class="italic">Changing IAM permissions and Trust relationships for the Amazon SageMaker notebook execution role</em> subsection in the <em class="italic">Setting up your AWS environment</em> section of <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a><em class="italic">, Introducing Amazon Textract</em>:<p class="source-code">{ "Version": "2012-10-17", "Statement": [</p><p class="source-code">  { "Effect": "Allow", </p><p class="source-code">    "Principal": </p><p class="source-code">      { "Service": </p><p class="source-code">          [ "sagemaker.amazonaws.com", </p><p class="source-code">            "s3.amazonaws.com", </p><p class="source-code">            "comprehend.amazonaws.com" ] </p><p class="source-code">          }, </p><p class="source-code">          "Action": "sts:AssumeRole" } </p><p class="source-code">      ] </p><p class="source-code">  }</p></li>
			</ol>
			<p>Now that we have set up our Notebook and set up an IAM role to run the walkthrough notebook, in the next section, we will start processing the data for topic modeling.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor095"/>Preprocessing the customer service history data</h2>
			<p>Let's begin by downloading and reviewing the customer service records we will use for this chapter. We will use the Consumer Complaints data for the State of Ohio from the Consumer <a id="_idIndexMarker390"/>Financial Protection Bureau for our solution: <a href="https://www.consumerfinance.gov/data-research/consumer-complaints/search/?dataNormalization=None&amp;dateRange=1y&amp;date_received_max=2021-05-17&amp;date_received_min=2020-05-17&amp;searchField=all&amp;state=OH&amp;tab=Map">https://www.consumerfinance.gov/data-research/consumer-complaints/search/?dataNormalization=None&amp;dateRange=1y&amp;date_received_max=2021-05-17&amp;date_received_min=2020-05-17&amp;searchField=all&amp;state=OH&amp;tab=Map</a>. You can try other datasets from this site, or your own unique customer service data. For your convenience, the complaints data is included as a CSV file in the GitHub repository: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv</a>. This should be available to you when you clone the repository. You can click on the CSV file by going to the folder it is present in inside the notebook to review its contents. Alternatively, you can view it using the code provided in the <strong class="source-inline">chapter6-nlp-in-customer-service-github.ipynb</strong> notebook.</p>
			<p>Open the notebook and perform the following steps:</p>
			<ol>
				<li value="1">Execute the cells under <strong class="bold">Prerequisites</strong> to ensure we have the libraries we need for the notebook. Note that in this cell, you are getting the Amazon SageMaker Execution Role for the notebook. Please ensure that you create an Amazon S3 bucket (<a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html</a>) and provide the bucket name in the line. Type in a prefix of your choice or accept what is already provided in the notebook:<p class="source-code">bucket = '&lt;bucket-name&gt;'</p><p class="source-code">prefix = 'chapter6'</p></li>
				<li>Execute the cells under <strong class="bold">Preprocess the Text data</strong>.<p>First, we will load the CSV file containing the consumer complaints data (this is already provided to you in this book's GitHub repository at (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2006/topic-modeling/initial/complaints_data_initial.csv</a>) into a pandas DataFrame object for easy manipulation:</p><p class="source-code">raw_df = pd.read_csv('topic-modeling/initial/complaints_data_initial.csv')</p><p class="source-code">raw_df.shape</p><p>When we execute the preceding cell, we will see that the notebook returns a shape of (11485, 18), which means there are 11,485 rows and 18 columns. We are only interested in the <strong class="bold">Consumer complaint narrative</strong> field, so we <a id="_idIndexMarker391"/>will drop the rest of the fields from the dataset. After we execute this cell, the shape should change to (5152, 1):</p><p class="source-code">raw_df = raw_df.dropna(subset=['Consumer complaint narrative'])</p><p class="source-code">raw_df = pd.DataFrame(raw_df['Consumer complaint narrative'].copy())</p><p class="source-code">raw_df.shape</p><p>Now, let's convert this back into an updated CSV file:</p><p class="source-code">raw_df.to_csv('topic-modeling/raw/complaints_data_subset.csv', header=False, index=False)</p><p>Execute the cells in the notebook to clean up the textual content in our CSV file, including restructuring the text into individual sentences so that each consumer complaint is a separate line. For the source of this code block and a very good discussion on how to use the Python regex function with sentences, please refer to <a href="https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences">https://stackoverflow.com/questions/4576077/how-can-i-split-a-text-into-sentences</a>. Continue executing the cells to remove unnecessary spaces or punctuation, create a new CSV file with these changes, and upload it to an S3 bucket. We will also create a new pandas DataFrame object with the formatted content so that we can use it in the subsequent steps. Please execute all the remaining cells in the notebook from <em class="italic">Preprocess the Text data</em>:</p><p class="source-code"># Write the formatted sentences into a CSV file</p><p class="source-code">import csv</p><p class="source-code">fnfull = "topic-modeling/input/complaints_data_formatted.csv"</p><p class="source-code">with open(fnfull, "w", encoding='utf-8') as ff:</p><p class="source-code">    csv_writer = csv.writer(ff, delimiter=',', quotechar = '"')</p><p class="source-code">    for infile in all_files:</p><p class="source-code">        for num, sentence in enumerate(infile):</p><p class="source-code">            csv_writer.writerow([sentence])</p><p class="source-code"># Let's store the formatted CSV into a Pandas DataFrame </p><p class="source-code"># as we will use this to create the training dataset for our custom classifier</p><p class="source-code">columns = ['Text']</p><p class="source-code">form_df = pd.read_csv('topic-modeling/input/complaints_data_formatted.csv', header=None, names = columns)</p><p class="source-code">form_df.shape</p><p class="source-code"># Upload the CSV file to the input prefix in S3 to be used in the topic modeling job</p><p class="source-code">s3 = boto3.client('s3')</p><p class="source-code">s3.upload_file('topic-modeling/input/complaints_data_formatted.csv', bucket, prefix+'/topic_modeling/input/topic_input.csv')</p></li>
				<li>Next, we <a id="_idIndexMarker392"/>will run an Amazon Comprehend Topic Modeling job on this formatted CSV file to extract a set of topics that can be applied to our list of consumer complaints. These topics represent and help us identify the subject area or the theme for the related text, as well as represent the common set of words with the same contextual reference throughout the document. For more details, please refer to Amazon Comprehend Topic Modeling at <a href="https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html">https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html</a>.<p>To get started, go to the AWS Console (please refer to the <em class="italic">Technical requirements</em> section if you don't have access to the AWS Console) and type Amazon Comprehend in the services search window at the top of the console. Then, navigate to the Amazon Comprehend Console.</p><p>Click the <strong class="bold">Launch Amazon Comprehend</strong> button.</p><p>Click on <strong class="bold">Analysis jobs</strong> in the left pane and click on <strong class="bold">Create job</strong> on the right, as shown in the following screenshot:</p><div id="_idContainer118" class="IMG---Figure"><img src="Images/B17528_06_05.jpg" alt="Figure 6.5 – Creating an analysis job&#13;&#10;" width="1650" height="294"/></div><p class="figure-caption">Figure 6.5 – Creating an analysis job</p><p>Type in <a id="_idIndexMarker393"/>a name for your analysis job and select <strong class="bold">Topic modeling</strong> as the analysis type from the built-in jobs list. Provide the location of the CSV file in your S3 bucket in the <strong class="bold">Input data</strong> section, with <strong class="bold">Data source</strong> set to <strong class="bold">My documents</strong> and <strong class="bold">Number of topics</strong> set to <strong class="source-inline">8</strong>, as shown in the following screenshot:</p><div id="_idContainer119" class="IMG---Figure"><img src="Images/B17528_06_06.jpg" alt="Figure 6.6 – Creating topic modeling job inputs – part1&#13;&#10;" width="1029" height="944"/></div><p class="figure-caption">Figure 6.6 – Creating topic modeling job inputs – part1</p><p>Provide <a id="_idIndexMarker394"/>the details for the rest of the fields and click on <strong class="bold">Create job</strong>, as shown in the following screenshot:</p><div id="_idContainer120" class="IMG---Figure"><img src="Images/B17528_06_07.jpg" alt="Figure 6.7 – Creating topic modeling job inputs – part 2&#13;&#10;" width="812" height="659"/></div><p class="figure-caption">Figure 6.7 – Creating topic modeling job inputs – part 2</p><p>You <a id="_idIndexMarker395"/>should see a job submitted status after the IAM role propagation is completed, as shown in the following screenshot. The job should take about 30 minutes to complete, which gives you time to have a quick snack or a coffee/tea. Now, click on the job's name, copy the S3 link provided in the <strong class="bold">Output data location</strong> field, and go back to your notebook. We will continue the steps in the notebook:</p><div id="_idContainer121" class="IMG---Figure"><img src="Images/B17528_06_08.jpg" alt="Figure 6.8 – Topic modeling job submitted&#13;&#10;" width="1560" height="413"/></div><p class="figure-caption">Figure 6.8 – Topic modeling job submitted</p></li>
				<li>We will <a id="_idIndexMarker396"/>now execute the cells in the Process Topic Modeling Results section.<p>To download the results of the Topic Modeling job, we need the <strong class="bold">Output data location</strong> S3 URI that you copied in the previous step. In the first cell in this section of the notebook, replace the contents of the <strong class="source-inline">tpprefix</strong> variable – specifically <strong class="bold">&lt;name-of-your-output-data-s3-prefix&gt;</strong> – with the results prefix from the S3 URI, as shown in the following code block. This is the string after the results prefix and before the output prefix in your S3 URI:</p><p class="source-code"># Output data location S3 URI</p><p class="source-code">https://s3.console.aws.amazon.com/s3/object/&lt;bucket&gt;/chapter6/topic_modeling/results/123456789-TOPICS-long-hash-code/output/output.tar.gz?region=us-east-1</p><p class="source-code">tpprefix = prefix+'/topic_modeling/results/&lt;name-of-your-comprehend-topic-modeling-job&gt;/output/output.tar.gz'</p><p>The revised code should look as follows. When executed, it will download the <strong class="source-inline">output.tar.gz</strong> file locally and extract it:</p><p class="source-code"># Let's first download the results of the topic modeling job. </p><p class="source-code"># Please copy the output data location from your topic modeling job for this step and use it below</p><p class="source-code">directory = "results"</p><p class="source-code">parent_dir = os.getcwd()+'/topic-modeling'</p><p class="source-code"># Path</p><p class="source-code">path = os.path.join(parent_dir, directory)</p><p class="source-code">os.makedirs(path, exist_ok = True)</p><p class="source-code">print("Directory '%s' created successfully" %directory)</p><p class="source-code">tpprefix = prefix+'/topic_modeling/results/<strong class="bold">123456789-TOPICS-long-hash-code</strong>/output/output.tar.gz'</p><p class="source-code">s3.download_file(bucket, tpprefix, 'topic-modeling/results/output.tar.gz')</p><p class="source-code">!tar -xzvf topic-modeling/results/output.tar.gz</p><p>Now, load <a id="_idIndexMarker397"/>each of the resulting CSV files into their own pandas DataFrames:</p><p class="source-code">tt_df = pd.read_csv('topic-terms.csv')</p><p class="source-code">dt_df = pd.read_csv('doc-topics.csv')</p><p>The topic terms DataFrame contains the topic number, what term corresponds to the topic, and how much weight this term contributes to the topic. Execute the code shown in the following code block to review the contents of the topic terms DataFrame:</p><p class="source-code">for i,x in tt_df.iterrows():</p><p class="source-code">    print(str(x['topic'])+":"+x['term']+":"+str (x['weight']))</p><p>We may have multiple topics in the same line, but for this solution, we are not interested in these duplicates, so we will drop them:</p><p class="source-code">dt_df = dt_df.drop_duplicates(subset=['docname'])</p><p>Now, let's filter the topics so that we select the topic with the maximum weight distribution for text it refers to:</p><p class="source-code">ttdf_max = tt_df.groupby(['topic'], sort=False)['weight'].max()</p><p>Load these into their own DataFrame and display them:</p><p class="source-code">newtt_df = pd.DataFrame()</p><p class="source-code">for x in ttdf_max:</p><p class="source-code">    newtt_df = newtt_df.append(tt_df.query('weight == @x'))</p><p class="source-code">newtt_df = newtt_df.reset_index(drop=True)    </p><p class="source-code">newtt_df</p><p>Having reviewed the consumer complaints input text data, the masked characters <a id="_idIndexMarker398"/>that are displayed mainly correspond to debt-related complaints from customers, so we will replace the masked terms with <strong class="bold">debt</strong> and replace the word <strong class="bold">Husband</strong> with <strong class="bold">family</strong>. These terms will become the training labels for our Amazon Comprehend Custom Classification model, which we will then use to automate request routing in the next section. Please execute the following code in the notebook:</p><p class="source-code">form_df.assign(Label='')</p><p class="source-code">for i, r in dt_df.iterrows():</p><p class="source-code">    line = int(r['docname'].split(':')[1])</p><p class="source-code">    top = r['topic']</p><p class="source-code">    tdf = newtt_df.query('topic == @top')</p><p class="source-code">    term = tdf['term'].values[0]</p><p class="source-code">    if term == 'xxxx':</p><p class="source-code">        term = 'debt'</p><p class="source-code">    if term == 'husband':</p><p class="source-code">        term = 'family'</p><p class="source-code">    form_df.at[line, 'Label'] = term</p><p>Create the <strong class="source-inline">custom-classification</strong> and <strong class="source-inline">train</strong> folders, which we need in the notebook to execute the next step, as shown in the following code block:</p><p class="source-code">directory = "custom-classification"</p><p class="source-code">parent_dir = os.getcwd()</p><p class="source-code"> </p><p class="source-code">path = os.path.join(parent_dir, directory)</p><p class="source-code">os.makedirs(path, exist_ok = True)</p><p class="source-code">print("Directory '%s' created successfully" %directory)</p><p class="source-code">directory = "train"</p><p class="source-code">parent_dir = os.getcwd()+'/custom-classification'</p><p class="source-code"> </p><p class="source-code">path = os.path.join(parent_dir, directory)</p><p class="source-code">os.makedirs(path, exist_ok = True)</p><p class="source-code">print("Directory '%s' created successfully" %directory)</p><p>Now, let's <a id="_idIndexMarker399"/>rearrange the columns so that we have the label as the first column. We will convert this into a CSV file and upload it into our S3 bucket. This CSV file will be the training dataset for our Amazon Comprehend Custom Classification model:</p><p class="source-code">form_df = form_df[['Label','Text']]</p><p class="source-code">form_df.to_csv('custom-classification/train/train.csv', header=None, index=False)</p><p class="source-code">s3.upload_file('custom-classification/train/train.csv', bucket, prefix+'/custom_classification/train/train.csv')</p></li>
				<li>Now, we will go back to the Amazon Comprehend AWS Console to train our Custom Classification model, which can predict a label for a given text. These labels are the topics we modeled in the previous section. With Amazon Comprehend Custom, you can train models that are unique to your business incrementally on top of the pre-trained, highly powerful Comprehend models. So, these <a id="_idIndexMarker400"/>custom models leverage what the default Comprehend model already knows, thereby training quickly, They are also more accurate than if you were to build a custom classification model from the ground up. You can run this training process without any ML skills with just a few clicks in the Amazon Comprehend console. For more details, please refer to <a href="https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html">https://docs.aws.amazon.com/comprehend/latest/dg/how-document-classification.html</a>.<p>To get started, go to the AWS Console (please refer to the <em class="italic">Technical requirements</em> section at the beginning of this chapter if you don't have access to the AWS Console) and type Amazon Comprehend in the services search window at the top of the console. Then, navigate to the Amazon Comprehend Console.</p><p>Click the <strong class="bold">Launch Amazon Comprehend</strong> button.</p><p>Click on <strong class="bold">Custom classification</strong> under the <strong class="bold">Customization</strong> title in the left pane.</p><p>Click on <strong class="bold">Train classifier</strong>, as shown in the following screenshot:</p><div id="_idContainer122" class="IMG---Figure"><img src="Images/B17528_06_09.jpg" alt="Figure 6.9 – Train classifier button&#13;&#10;" width="1231" height="679"/></div><p class="figure-caption">Figure 6.9 – Train classifier button</p><p>Enter a <a id="_idIndexMarker401"/>name for your classifier, leave the language as English, and set <strong class="bold">Classifier mode</strong> to <strong class="bold">Multi-class</strong>. (For our solution, we predict one label per document. If you need to predict multiple labels per document, you can use the <strong class="bold">Multi-label</strong> mode.) Select <strong class="bold">CSV file</strong> under <strong class="bold">Training data format</strong>, as shown in the following screenshot:</p><div id="_idContainer123" class="IMG---Figure"><img src="Images/B17528_06_10.jpg" alt="Figure 6.10 – Custom classifier inputs – part 1" width="825" height="739"/></div><p class="figure-caption">Figure 6.10 – Custom classifier inputs – part 1</p><p>Provide <a id="_idIndexMarker402"/>our training dataset's <strong class="bold">S3 location</strong>; that is, the one we created in the previous section. For <strong class="bold">IAM role</strong>, if you created an <strong class="bold">AmazonComprehendServiceRole</strong> in the previous chapters, use that, or select <strong class="bold">Create an IAM role</strong> and choose <strong class="bold">Any S3 Bucket</strong> from the list. Click the <strong class="bold">Train classifier</strong> button, as shown in the following screenshot:</p><div id="_idContainer124" class="IMG---Figure"><img src="Images/B17528_06_11.jpg" alt="Figure 6.11 – Custom classifier inputs – part 2" width="820" height="659"/></div><p class="figure-caption">Figure 6.11 – Custom classifier inputs – part 2</p><p>The training job will be submitted. Shortly after, the training job's status will change to <strong class="bold">Training</strong>, as shown in the following screenshot:</p><div id="_idContainer125" class="IMG---Figure"><img src="Images/B17528_06_12.jpg" alt="Figure 6.12 – Custom classifier training" width="1015" height="477"/></div><p class="figure-caption">Figure 6.12 – Custom classifier training</p><p>Training <a id="_idIndexMarker403"/>will take approximately 1 hour to complete. The status will change to <strong class="bold">Trained</strong> when the job completes, as shown in the following screenshot:</p><div id="_idContainer126" class="IMG---Figure"><img src="Images/B17528_06_13.jpg" alt="Figure 6.13 – Custom classifier training complete" width="1014" height="577"/></div><p class="figure-caption">Figure 6.13 – Custom classifier training complete</p></li>
				<li>Now <a id="_idIndexMarker404"/>that we have finished training our classifier, we will create a real-time endpoint to deploy the model. We will use this endpoint in our solution to run predictions for routing requests.<p>Click on the name of your classifier in the Amazon Comprehend console. Then, scroll down to the <strong class="bold">Endpoints</strong> section and click <strong class="bold">Create endpoint</strong>, as shown in the following screenshot:</p><div id="_idContainer127" class="IMG---Figure"><img src="Images/B17528_06_14.jpg" alt="Figure 6.14 – Creating a Comprehend endpoint&#13;&#10;" width="1014" height="683"/></div><p class="figure-caption">Figure 6.14 – Creating a Comprehend endpoint</p><p>Type in <a id="_idIndexMarker405"/>a name for your endpoint, provide an inference unit value of 1, and click on <strong class="bold">Create endpoint</strong>, as shown in the following screenshot. Inference units determine the price and capacity of the provisioned endpoint. An inference unit provides a prediction throughput of 100 characters every second. For more details, please refer to Amazon Comprehend's pricing guide at <a href="https://aws.amazon.com/comprehend/pricing/">https://aws.amazon.com/comprehend/pricing/</a>:</p><div id="_idContainer128" class="IMG---Figure"><img src="Images/B17528_06_15.jpg" alt="Figure 6.15 – Creating Comprehend endpoint inputs&#13;&#10;" width="814" height="448"/></div><p class="figure-caption">Figure 6.15 – Creating Comprehend endpoint inputs</p><p>Once <a id="_idIndexMarker406"/>the endpoint has been created, please make a note of the endpoint's ARN by clicking on the name of the endpoint, as shown in the following screenshot. This will be required for running inference in the notebook:</p><div id="_idContainer129" class="IMG---Figure"><img src="Images/B17528_06_16.jpg" alt="Figure 6.16 – Comprehend endpoint ARN" width="964" height="300"/></div><p class="figure-caption">Figure 6.16 – Comprehend endpoint ARN</p></li>
				<li>As a next step, we will navigate back to our notebook and execute the steps in the <em class="italic">Automate Request Routing</em> section.<p>Provide the endpoint ARN you took note of in the previous step in the notebook cell:</p><p class="source-code">endpoint_arn = '&lt;comprehend-custom-classifier-endpoint-arn&gt;'</p><p>Now, let's execute the next cell, which shows us how to run the real-time analysis <a id="_idIndexMarker407"/>with our endpoint. For input, we will use a sample text message that's been assigned to the <strong class="source-inline">test_text</strong> variable, as shown in the following code:</p><p class="source-code">test_text = 'because of your inability to accept my payments on time I now have a really bad credit score, you need to fix this now'</p><p class="source-code">comprehend = boto3.client('comprehend')</p><p class="source-code">response = comprehend.classify_document(Text=test_text, EndpointArn=endpoint_arn)</p><p class="source-code">print(response)</p><p>Our custom classifier returns a response, as shown in the following code block:</p><p class="source-code">{'Classes': [{'Name': 'account', 'Score': 0.9856781363487244}, {'Name': 'credit', 'Score': 0.013113172724843025}, {'Name': 'debt', 'Score': 0.0005924980505369604}], 'ResponseMetadata': {'RequestId': 'c26c226c-3878-447e-95f5-60b4d91bb536', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c26c226c-3878-447e-95f5-60b4d91bb536', 'content-type': 'application/x-amz-json-1.1', 'content-length': '151', 'date': 'Wed, 19 May 2021 17:35:38 GMT'}, 'RetryAttempts': 0}}</p><p>Run the code given in the following code block to select the <strong class="source-inline">Name</strong> property with the highest confidence score from the response. This will be the department or the option that the customer request will be routed to in the contact center:</p><p class="source-code"> cls_df = pd.DataFrame(response['Classes'])</p><p class="source-code">max_score = cls_df['Score'].max()</p><p class="source-code">routing_type = cls_df.query('Score == @max_score')['Name'].values[0]</p><p class="source-code">print("This request should be routed to: " + routing_type)</p><p>This code will return the following response:</p><p class="source-code">This request should be routed to: account</p></li>
				<li>As a <a id="_idIndexMarker408"/>next step, we will execute the steps in the <em class="italic">Automate Feedback Analysis</em> section.<p>To <a id="_idIndexMarker409"/>analyze the sentiment of the customer conversation, we will use the <strong class="bold">Amazon Comprehend Detect Sentiment API</strong>. This is a built-in feature in Amazon Comprehend and does not require us to train any models. We can directly call the API with input. It will return the sentiment of the text, as follows:</p><p class="source-code">sent_response = comprehend.detect_sentiment(</p><p class="source-code">    Text=test_text,</p><p class="source-code">    LanguageCode='en'</p><p class="source-code">)</p><p class="source-code">print("The customer's feedback sentiment is: " + sent_response['Sentiment'])</p><p class="source-code">The customer's feedback sentiment is: NEGATIVE</p></li>
			</ol>
			<p>That concludes the solution build for this chapter. Please refer to the <em class="italic">Further reading</em> section for examples that are similar to this use case. In the case of <strong class="bold">LiveRight</strong>, you can integrate this build into the existing contact center workflow and scale the solution using <strong class="bold">Amazon Transcribe</strong>, <strong class="bold">AWS StepFunctions</strong>, and <strong class="bold">AWS Lambda</strong>. An example of how to do this is shown in the following diagram:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="Images/B17528_06_17.jpg" alt="Figure 6.17 – NLP in customer service with a real-time transcription" width="1009" height="538"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17 – NLP in customer service with a real-time transcription</p>
			<p>Amazon <a id="_idIndexMarker410"/>Transcribe provides real-time streaming transcription capabilities to convert a customer call from speech into text. Please refer to <a href="https://aws.amazon.com/transcribe/">https://aws.amazon.com/transcribe/</a> for more details. An AWS Step Functions (<a href="https://aws.amazon.com/step-functions/">https://aws.amazon.com/step-functions/</a>) workflow that enables orchestration of a complete process flow with AWS Lambda (a fully managed serverless compute service that can run code without the need to provision servers (<a href="https://aws.amazon.com/lambda/">https://aws.amazon.com/lambda/</a>)) and multiple AWS services can be set up to be triggered on receipt of a transcription of a specified length. The Step Functions workflow will call an AWS Lambda function to detect the routing option for the customer request, and the call can be automatically routed to that option, or/and the customer request/feedback sentiment can be analyzed by calling the Detect Sentiment API, as we saw in the <em class="italic">Automate feedback analysis</em> section. The outcome is that while the call is in progress, the contact center agent will have an automated response with a predicted routing <a id="_idIndexMarker411"/>option and sentiment, which, in turn, helps resolve the customer's request quickly and efficiently.</p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor096"/>Summary</h1>
			<p>In this chapter, we learned how to build an NLP solution to accelerate customer service efficiencies using Amazon Comprehend's Topic Modeling feature, Detect Sentiment feature, and by training our own custom classifier to predict routing options using Comprehend Custom Classification before hosting it using Comprehend real-time endpoints. We also saw how we can leverage the flexibility of powerful and accurate NLP models without the need for any ML skills. For your enterprise needs, Amazon Comprehend scales seamlessly to process millions of documents, provides usage-based pricing, supports batch inference, and with autoscaling support for real-time endpoints, you can manage your inference request volumes and control your inference costs effectively.</p>
			<p>For our solution, we started by introducing the customer service use case, the inherent challenges with the way things are set up currently, and the need to perform automated routing and sentiment detection to control the high customer churn caused by current inefficient processes. We then designed an architecture to use Amazon Comprehend to identify common themes or topics, create a training dataset, train a custom classifier to predict routing options, and to run sentiment analysis on the customer request. We assumed that you were the solution architect that had been assigned to this project, and we provided an overview of the solution components, along with a diagram of the architecture in <em class="italic">Figure 6.1</em>.</p>
			<p>We then went through the prerequisites for the solution build, set up an Amazon SageMaker notebook instance, cloned our GitHub repository, and started executing the code in the notebook based on the instructions provided in this chapter.</p>
			<p>In the next chapter, we will look at a slightly related use case on using NLP to run the voice of the customer analytics process. We will introduce the use case, discuss how to design the architecture, establish the prerequisites, and walk through the various steps required to build the solution.</p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor097"/>Further reading</h1>
			<ul>
				<li><em class="italic">Announcing the launch of Amazon Comprehend custom entity recognition real-time endpoints</em>, by Mona Mona and Prem Ranga (<a href="https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-amazon-comprehend-custom-entity-recognition-real-time-endpoints/">https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-amazon-comprehend-custom-entity-recognition-real-time-endpoints/</a>).</li>
				<li><em class="italic">Active learning workflow for Amazon Comprehend custom classification models –Part 2</em>, by Shanthan Kesharaju, Joyson Neville Lewis, and Mona Mona (<a href="https://aws.amazon.com/blogs/machine-learning/active-learning-workflow-for-amazon-comprehend-custom-classification-part-2/">https://aws.amazon.com/blogs/machine-learning/active-learning-workflow-for-amazon-comprehend-custom-classification-part-2/</a>).</li>
			</ul>
		</div>
	</div></body></html>