- en: Introduction to Artificial Neural Networks and TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will give an introduction to **artificial neural networks**
    (**ANNs**), which are basically computational models inspired by living brains,
    and perceptrons, which are the building blocks for ANNs. We will also talk about
    all of the elements to consider when building a deep neural network model. Then,
    we will talk about TensorFlow, which is the library that we will use to create
    these deep neural network models. Finally, we will talk about the core concepts
    that we need to understand about TensorFlow in order to use these library concepts,
    such as variables, placeholders, sessions, graphs, and others that are essential
    for using this library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the topics that will be covered as we progress:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to ANNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elements of a deep neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation of and introduction to TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core concepts in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ANNs are biologically inspired computational models that can be used to train
    a computer to perform a task using data. These models are part of the broad category
    of machine learning models. The distinction between these models and others is
    that these models are based on a collection of connected units called **artificial
    neurons**.
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of ANNs and, in this book, we will use one specific type,
    which is called the **multilayer perceptron **(**MLP**). Please note that there
    are a lot more variations of ANNs. These are machine learning models and we can
    use them for classification and regression tasks, but we can actually extend these
    models and apply them to other very specific tasks such as computer vision, speech
    recognition, and machine translation. These models are the basis of the exciting
    and growing field of deep learning, which has been really successful in the last
    few years in many areas.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perceptrons are the simplest type of artificial neuron, invented as a simple
    model for binary classification. Let''s use the context of the dataset that we
    have been using in this book, the credit card dataset. Let''s say that we have
    only two features for classifying defaulters and nondefaulters: age and bill amount.
    So the idea of the perceptron is to create some kind of a score. To do so, you
    take one constant, `w1` ,and multiply it by the value of `age`, and then you add
    another constant, `w2`, which is multiplied by the value of the `bill` amount
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As a rule, we classify this person as a defaulter if `score` > `b`.
  prefs: []
  type: TYPE_NORMAL
- en: So, from this simple operation, we create a score. Then, we follow the rule
    to classify people as defaulters or as nondefaulters. So, if this `score` is greater
    than some number, then we classify this person as a defaulter.
  prefs: []
  type: TYPE_NORMAL
- en: 'An equivalent way to state this rule is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee8e1fff-8952-4989-b6ff-f3a0dda854dd.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the prediction of this model will be `1`, or defaulter, if the quantity
    is greater than `0`, and the prediction will be `0`, or nondefaulter, if this
    quantity is less than or equal to `0`. The `b` value is also known as the threshold
    or bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, if we have *n* features, then our perceptron would look similar
    to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7938be7f-8c64-4cdd-9d93-b6ba65e4d5a6.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have the same form. We predict **1** if the sum of the weights
    times the values of our features **-b** is actually greater than **0**, otherwise,
    we predict **0**. Assuming that all features are on the same scale, the weights
    would represent the importance of each feature in making the decision. So, we
    know that for this particular problem we have, all features are in very different
    scales. For example, ages are in different scales than bill amount, but let's
    say that you set all of  the features to a similar scale. You can think about
    the **w** variables as the weights, and they are the most important part of each
    feature while making the decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows another way to visualize this perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd6b7f5e-ef5e-4f6e-aac0-a65a57a01b6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, you have the values of the threshold or the bias, **b**, and you have the
    value of **Age**, **x1** ,and the value of **Bill amount**, **x2**. So the three
    values go into an operation, and then you get an output. Now, there is a little
    modification that we can do to the perceptron, and this is to add what is known
    as an **activation function**. An activation function is any function that takes
    the result of the operation and performs some transformation to the input values
    using the **f** function. So the input for the activation function is the resulting
    quantity from the operation, and then, after applying activation function **f**,
    we will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2e13ce5-8226-4536-a9c6-c425619c50d0.png)'
  prefs: []
  type: TYPE_IMG
- en: So, this is the perceptron. We can add an activation function to the perceptron
    and then we get the rule or the classification `1` or `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, maybe you are wondering how do we decide which are the best weights and
    threshold for our perceptron? What activation function can we use? The answers
    to these questions are provided by the perceptron learning algorithm. So, there
    is a learning algorithm that we can use to actually train perceptrons. The good
    thing about perceptrons is that they are very simple to understand. However, they
    are very weak in performance when compared to more sophisticated methods, such
    as the methods that we used in previous chapters. So, it is not worth actually
    learning about this perceptron learning algorithm. However, these very simple
    models are the building blocks for ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ANNs are models based on perceptrons or other similar basic building blocks,
    and the ones that we will learn about in this book are based on perceptrons. One
    of the most popular ANN models is the MLP, which we will use in this book. The
    motivation for using perceptrons in an ANN is that, instead of using one single
    perceptron for classification, what if we used many of them? Take a look at the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f03b677-a86e-4f32-b98d-7818a77e2a6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have three perceptrons and we notice that we have a different bias
    for each perceptron. But the values for our features will be the same in all cases.
    If we use three perceptrons, we will get three output values, but we know that
    this is a binary classification problem so we need only one output. So, now that
    we have three output values, we can combine them or we can view these output values
    as input values for another perceptron. Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a52b180-965c-47c4-993d-f34100c6e829.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see in the following screenshot, we can take the output values from
    the preceding perceptrons and fit them as input values to another perceptron,
    and this perceptron will give us the output. So, this is the intuition on how
    to build neural networks or MLPs, and this is an example of an ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d89276a-5c73-444d-a881-a2659c7699bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding screenshot, we have the following three layers of an MLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Layer**: In this layer, you have the original data or the training
    data that you will use to train this model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden Layer**: This middle layer is the output from the preceding perceptron,
    which is used as the input for the next perceptron'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output Layer**:In this layer, you have the output that you get from the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot is another way to visualize the same ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0c4a68a-1a43-4bc2-96a2-719ca85c0389.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a more compact way to visualize it, but it's actually the same network.
    So, instead of having three biases, we add one constant feature, **1**, for every
    observation. This value of **1** gets multiplied by the different biases and goes
    as input to the neurons in our hidden layer. The value of **x1** gets multiplied
    by some weight and goes as input for the next neurons, and the same happens with
    the value of **x2**. Then, the result of the neurons in the hidden layer is used
    as input for the last perceptron in our network, which is the overall output.
  prefs: []
  type: TYPE_NORMAL
- en: Elements of a deep neural network model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The motivation for **deep neural networks** (**DNNs**) is similar, and the
    question here is, instead of using one single hidden layer, what if we use many
    hidden layers? So in that case, our model will look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d594d3c-f5ca-4433-af2c-1299e12ae028.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we have the same input layer. However, in this case, we will have many
    hidden layers and the output layer will stay the same. The key thing here is the
    hidden part of the network, the hidden layers; instead of having just one, we
    have many hidden layers and this is called a **DNN**.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Deep learning is a set of machine learning models based on neural networks
    and the associated techniques to train such models using data. There are many
    deep learning models. They are a class of machine learning algorithm with the
    following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: These models use a set of many layers of nonlinear processing units, which can
    perform abstract feature extraction and transformation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These models use some form of gradient descent for training through backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They usually need a lot of data and a lot of computational power for these models
    to perform very well
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These models are now considered state-of the-art for many applications such
    as computer vision, speech recognition, and game playing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elements of an MLP model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a lot of things to consider when building a deep learning model in
    an multilayer perceptron. You have to consider the architecture, the activation
    function, the optimization algorithm, the `loss` function, the weight initialization
    strategy, the regularization strategy, and the training strategy. We will discuss
    more about them in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture**: The first element that you need to consider when building
    deep learning models is the architecture of your MLP. When we say architecture,
    we are talking about the number of layers and the number of neurons per layer.
    The number of neurons in the input layer is determined by the number of features
    that you have in your dataset. The same thing is true for the number of output
    values. So, they are basically determined by your problem in a classification
    setting. The number of output values is usually the number of classes in your
    classification problem, and in a regression problem you will have only one output
    in your output layer. The choice that you have to make is how many hidden layers
    you are going to use and the number of neurons per hidden layer. There are not
    easy rules to set these numbers; in practice, what we do is we use a few layers
    at first. If a few layers don''t work, maybe we add more layers, and the number
    of neurons for each layer is a number between the number of input values and the
    number of outputs, `[n_inputs, n_outputs]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is just a rule of thumb. However, there are more formal methods to choose
    the number of hidden layers and the number of neurons, and researchers are constantly
    trying to come up with better methods for choosing these values.
  prefs: []
  type: TYPE_NORMAL
- en: '**Activation function**: The activation function is the function that is used
    in every neuron in the hidden layers. There are many choices; **sigmoid** was
    the first function used when these models were developed, but then researchers
    found that there are many problems with using this function, so they came up with
    other activation functions such as the **rectified Linear Unit** (**ReLU**), the **hyperbolic
    tangent**, the **leaky ReLU**, and some other choices that we will use in the
    examples as we progress.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization algorithm**: This is the algorithm that will be used to learn
    the weights of the networks. Each algorithm that you choose has different hyperparameters
    that need to be chosen by you, the modeler. The most basic algorithm to train
    these networks is **gradient descent**. However, gradient descent can be slow
    and also has some problems, so researchers have come up with other algorithms
    such as **momentum optimizers**, **AdaGrad**, **RMSProp**, and the **Adam** moment
    algorithm. In TensorFlow, we have a lot of algorithms that we can choose from,
    including the Adam moment algorithm, and this is actually the one that we are
    going to use in the examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**: This is the function that will produce the quantity that
    will be minimized by the optimizer. The choice of loss function depends on the
    problem. If we are doing a regression problem, you can choose the mean squared
    error or the mean pairwise squared error. For classification problems, there are
    more choices such as cross entropy, square loss, and hinge loss. This is similar
    to trial and error; sometimes, one loss function will work for your problem and
    sometimes it will not. So, this is why you have to consider a lot of different
    loss functions. However, keep in mind that the loss function will produce the
    quantity that will be used for the optimization algorithm to adjust the different
    weights for the different perceptrons that will be part of your network. Hence,
    this is the function that will produce the quantity, and the goal of the optimizer
    is to make this quantity as small as possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weight initialization strategy**: The weights for each perceptron in your
    network must be initialized with some values, and these values will be progressively
    changed by the optimization algorithm to minimize the loss. There are many ways
    in which you can initialize these values. You can initialize with all zeros. For
    many years, researchers used to initialize using a random normal distribution
    but, in recent years, researchers have come up with better choices, including
    Xavier initialization and He initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization strategy**: This is an optional but highly recommended function
    because deep learning models tend to overfit data due to the quantity of parameters
    that they calculate. You can use many choices, including the L1 regularization,
    L2 regularization, and dropout regularization strategies. In this book, we are
    not going to use regularization in our examples, but keep in mind that, if you
    want to build really effective deep learning models, you will very likely need
    a regularization strategy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training strategy**: The training strategy refers to the way the data will
    be presented to the training algorithm. This is not part of the model itself,
    but it will have an influence on the results and the performance of the model.
    When talking about training deep learning models, you will hear the word epoch.
    One epoch is one pass of all training examples through the network. In these deep
    learning models, you will have to present the data to the network many times so
    the network can learn the best parameters for the model. There is another concept
    here: batch size. This is the number of elements presented simultaneously to the
    training algorithm. So in the case of deep learning models, we don''t present
    the whole training dataset to the model. What we do is we present batches of the
    dataset and, in each batch, we send just a few examples, maybe 100 or 50, and
    this is the way we train deep learning models. Now, you can use epoch and batch
    size to calculate the number of iterations that you will have in your model, and
    this is the number of training steps, which is the number of adjustments that
    the optimization algorithm makes to the weight in your model. So, for example,
    if you have 1,000 training examples and the batch size that you will use is 100,
    it will take 10 iterations to complete one epoch. You can get the total number
    of iterations with the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c5dca712-14af-4483-8080-112349aedbb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, there are a lot of decisions that you have to make as a modeler. These
    are very complex models and they can be very tricky to train. So, here is some
    guidance to consider before you start using these models:'
  prefs: []
  type: TYPE_NORMAL
- en: Because of the number of choices that we have in these models, they can be very
    tricky to build. So, they shouldn't be your first choice when trying to do predictions.
    Always begin with simpler and more understandable models, and then, if those models
    don't work, move to more complex models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are best practices for all of the choices that we have seen, but you need
    more knowledge about these elements if you want to build effective deep learning
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For these models to perform really well, you need a lot of data. So, you cannot
    use these models with very small datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn more about the theory of these models to understand how to use them better.
    So if you really want to use these models for solving real-world problems, learning
    more about the theory behind these models is a must.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is an open source software library for numerical computation using
    data flow graphs. The concept of a computational graph is very important in TensorFlow
    and was specially designed for creating deep learning models. This library allows
    developers to deploy computations to one or more CPUs or GPUs in a desktop, a
    server, or even in mobile devices. This library was originally developed by researchers
    and engineers working at Google. It was open sourced in 2015 and, since then,
    it has become one of the major libraries in the machine learning world.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow provides multiple APIs, and they can be categorized into the following
    two broad types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low level**: Also known as TensorFlow Core, this is the lowest-level API.
    This API gives us complete programming control and is aimed at researchers and
    users who need a high degree of flexibility when building their deep learning
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High level**: High-level APIs such as `tf.contrib.learn`, `keras`, and TF-Slim are
    typically easier to use. They take care of repetitive tasks and low-level details
    that, as a high-level user, you don''t need to worry about. They are designed
    for the fast implementation of commonly used models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, in preparation for our installation, we will create a new virtual environment
    in Anaconda. We can do so by using the following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: We open the Anaconda prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We type the following command line for creating a new virtual environment and
    pass the name of the environment with `anaconda`, which will install all of the
    packages that come with Anaconda:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here `apa` stands for advanced predictive analytics. Installation can take some
    time depending on your internet speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the installation has been completed, type `activate apa` to activate the
    new virtual environment. Here is a screenshot of the Anaconda prompt, showing
    the installation of Anaconda packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/165875cf-3aff-4480-9425-a511a15e8cb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, the new virtual environment has been activated and we are ready to install
    TensorFlow inside this new virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before installing TensorFlow, you must know that there are basically following
    two installations of TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow with CPU support only
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow with GPU support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second option is usually faster because it uses the GPUs in your computer
    or your devices, but this installation needs **Nvidia** support. You also need
    additional software in order to run this installation and it is a little bit more
    complicated to install.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, for easiness, we will install and use the CPU version as there is no
    difference in writing a program and running it in the CPU or the GPU versions,
    apart from the speed. We use the following line of code to install TensorFlow
    in our system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the code, the installation of TensorFlow will be initiated and,
    once the installation is completed, you will see the following output on your
    screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/095eddc0-a439-417a-b2c0-da57bc00ed32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will start a Python shell to test the installation by performing the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We type `python` to start the Python shell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use `import tensorflow as tf` to import TensorFlow into our Python shell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We run `hello = tf.constant("Hello")`; this will create a constant named `hello`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a session using `sess = tf.Session()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you see similar warning messages to the ones in the following screenshot,
    you can ignore them, as they are just telling you that you could install with
    different options so TensorFlow may run faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s print the result of `hello` by running the constant within the session
    using `print(sess.run(hello))`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/06b5fd15-2128-45b1-a376-648c9ec656a7.png)'
  prefs: []
  type: TYPE_IMG
- en: If you get a result of `Hello`, similar to this screenshot, it means that our
    installation is correct. So, now we are ready to use TensorFlow to build some
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Core concepts in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are some major concepts that we need to understand before actually using
    the `tensorflow` library. The following are the concepts that we will cover in
    this book:'
  prefs: []
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computational graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sessions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Placeholders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constants
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **tensor** is the central unit of data in TensorFlow. A tensor consists of
    a set of primitive values shaped into an array of any number of dimensions. It
    is basically a multidimensional array similar to a NumPy array. The number of
    dimensions defines the rank of a tensor. Let''s see some of the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '`3`: If we have a single number, the tensor will be considered a rank `0` tensor.
    This can be a scalar with `shape[]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[2., 2., 1.]`: If we have a vector, it will be considered a rank `1` tensor,
    so this is what we call a vector of shape `3` because it has three elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[[9., 5., 3.], [4., 5., 7]]`: A matrix with shape `[2, 3]` would be a rank
    `2` tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[[[8., 3.]], [[7., 9.,]]]`: A matrix with shape `[2, 1, 2]` would be a rank
    `3` tensor, as you can see in the outermost level we have two elements, then in
    the next level we have only one element, and in the last dimension, we have two
    elements. That''s why we have `2`, `1`, and `2` as the values and these are all
    tensors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computational graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A computational graph is a series of TensorFlow operations, also known as **ops**,
    arranged into a graph of nodes. The following two principle steps are used by
    TensorFlow Core:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a computational graph
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the computational graph
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s try to understand this concept with a very simple example. Let''s say
    that you have a function with two variables, **x** and **y** as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5212c92e-d91d-4024-ac39-d753f31f3926.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will use the preceding formula to calculate or to build a computational
    graph for the actual value of this function when you pass the values **3** and
    2 for **x** and **y** respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4127c2a0-3c52-4a52-85a3-f1f9e63d87cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s build a computational graph for actually getting the result from
    this computation model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef618ce1-282e-41fd-9b3f-55b6317130f4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, we see the values that flow through the computational
    graph to different nodes in the graph. So, in the first node, the value **3**
    gets assigned to **x** and, in the other node, the value **2** gets assigned to
    **y**. Now, the value of **x** flows to an operation node where it gets squared,
    and the result of that node flows to another operation where it gets multiplied
    to the value of **y**. We also have another node, where the value of **y** gets
    multiplied by **4**. The result of the **x** and **y** multiplication node and
    the result of the **y** multiplication node flow to the final node, which is the
    addition node, which gives us the final output **26**. So this is essentially
    how TensorFlow works. What flows between nodes are tensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other following objects that we use in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Session**: A session is an object that encapsulates the environment in which
    operation objects are executed. So, sessions are objects that place operations
    onto devices such as CPUs or GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Placeholders**: A placeholder is a promise to provide a value later. These
    objects are usually used to provide training and testing values in machine learning
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variables**: These are objects that are initialized with a value, and that
    value can change during the execution of the graph. Typically, they are used as
    trainable variables in machine learning models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Constants**: Constants are objects whose values never change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To have a better understanding of these object concepts, let''s see an example.
    First, we will import the required libraries by executing the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define some TensorFlow objects, placeholders, and a constant by executing
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, we define a placeholder called `x` and another placeholder called `y`.
    You have to explicitly give the type of object that you will use in TensorFlow,
    which we have in our example as `float32`. We then define a constant, `c` ,whose
    value is `5`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you create these objects, if you try to print them, you will not see
    the value of the object, but it will show the type of the object as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0760ad7d-7c77-46e1-b4e7-fd8c6607998f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s implement the following function with our placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39bb9a13-593d-4fd6-a16f-23b7d66715fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will use the placeholders that we just created to define the different nodes
    for our graph by executing the following code lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, if you try to print the values of these objects, you will get the object
    type and not the values as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a9eb5f4e-f40b-4fa1-8adf-bc449ca8d24d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, to perform the calculations for these objects, you must create a session
    object and then run all of the objects inside a session:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ebba675-9c71-4e31-b536-c84c7098babd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you are doing some computations, you don''t need to define the computational
    graph, as TensorFlow will do this behind the scenes. So, let''s say that you want
    to calculate `f` and we print the value, it will still give the object type. But
    to actually see the value of `f` when you perform the computation, we will run
    the function in a session object again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e94fb92f-b91a-44bd-9843-10f8d1c6acb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two ways in which you can run objects in TensorFlow. There are other
    ways, but these are the basic and most common ways you can run objects. You can
    use the `run()` method from a session or you can use the `eval()` method from
    the tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bd8b8d8-0c08-4461-a214-2376e38ce378.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, we created a session using the `with` statement and ran those
    two methods inside this statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will build a basic linear model. We will have TensorFlow guess the
    best values for the `b` and `w` parameters shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/270794dc-53bd-4673-8207-ca8fe98c51ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous equation, the value of `w` is `5` and `b` is `1`. We will use
    these values for training and plot the values on a scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ed0ddf5-88be-4207-9482-3e5d43cf1f3b.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we have the linear relationship between the two values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now initiate the variable objects, `w` and `b` ,with the value `0`,
    and they will be our trainable parameters. The placeholders are usually the objects
    that we use to pass the data, so we will create two placeholders, `x` and `y`,
    and now the linear model will be one of the nodes in our computational graph.
    Then, we will define a `loss` function, which will be used by the optimizer to
    actually change the values of our variable. Every time we run the training operation,
    the optimizer will adjust the values of `w` and `b` in order to minimize the loss.
    We will then initialize the variables and create a session to run the `init` initializer
    node as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba6befb8-56cb-4e28-b2eb-f0a824fec4ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can start training our machine learning model. We will run the training
    operation 20 times, which will make corrections to our values of `w` and `b` to
    minimize the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb974a24-7293-41d3-a968-b0adf65204e8.png)'
  prefs: []
  type: TYPE_IMG
- en: As we see, after the first iteration, the optimizer corrected the values of
    `w` and `b`, which is also carried out in every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: We can also do this using some linear algebra, but remember that the goal of
    machine learning is to actually learn the parameters from the data, and in this
    case, we have run our first machine learning model using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about ANNs, deep learning, and the elements of a
    deep learning model. We then installed TensorFlow and learned about the core concepts
    that we use in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will perform predictive analytics with TensorFlow and
    deep learning.
  prefs: []
  type: TYPE_NORMAL
