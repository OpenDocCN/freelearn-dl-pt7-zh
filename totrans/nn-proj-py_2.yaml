- en: Predicting Diabetes with Multilayer Perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first chapter, we went through the inner workings of a neural network,
    how to build our own neural network using Python libraries such as Keras, as well
    as the end-to-end machine learning workflow. In this chapter, we will apply what
    we have learned to build a **multilayer perceptron** (**MLP**) that can predict
    whether a patient is at risk of diabetes. This marks the first neural network
    project that we will build from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the problem that we're trying to tackle—diabetes mellitus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How AI is being used in healthcare today, and how AI will continue to transform
    healthcare
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An in-depth analysis of the diabetes mellitus dataset, including data visualization
    using Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding MLPs, and the model architecture that we will use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A step-by-step guide to implement and train an MLP with Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of our results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The key Python libraries required for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: matplotlib 3.0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas 0.23.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras 2.2.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy 1.15.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: seaborn 0.9.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn 0.20.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To download the dataset required for this project, please refer to the instructions
    at [https://raw.githubusercontent.com/PacktPublishing/Neural-Network-Projects-with-Python/master/Chapter02/how_to_download_the_dataset.txt](https://raw.githubusercontent.com/PacktPublishing/Neural-Network-Projects-with-Python/master/chapter2/how_to_download_the_dataset.txt).
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found in the GitHub repository for the book
    at [https://github.com/PacktPublishing/Neural-Network-Projects-with-Python](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the code into your computer, you may run the following `git clone`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After the process is complete, there will be a folder titled `Neural-Network-Projects-with-Python` .
    Enter the folder by running this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To install the required Python libraries in a virtual environment, run the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that you should have installed Anaconda on your computer first before
    running this command. To enter the virtual environment, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Navigate to the `Chapter02` folder by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following files are located in the folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '`main.py`: This is the main code for the neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`utils.py`: This file contains auxiliary utility code that will help us in
    the implementation of our neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`visualize.py`: This file contains code for exploratory data analysis and data
    visualization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To run the code for the neural network, simply execute the `main.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To recreate the data visualizations covered in this chapter, execute the `visualize.py`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Diabetes – understanding the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Diabetes is a chronic medical condition that is associated with elevated blood
    sugar levels in the body. Diabetes often leads to cardiovascular disease, stroke,
    kidney damage, and long-term damage to the extremities (that is, limbs and eyes).
  prefs: []
  type: TYPE_NORMAL
- en: It is estimated that there are 415 million people in the world suffering from
    diabetes, with up to 5 million deaths every year attributed to diabetes-related
    complications. In the United States, diabetes is estimated to be the seventh highest
    cause of death. Clearly, diabetes is a cause of concern to the wellbeing of modern
    society.
  prefs: []
  type: TYPE_NORMAL
- en: 'Diabetes can be divided into two subtypes: type 1 and type 2\. Type 1 diabetes
    results from the body''s inability to produce sufficient insulin. Type 1 diabetes
    is relatively rare compared to type 2 diabetes, and it only accounts for approximately
    5% of diabetes. Unfortunately, the exact cause of type 1 diabetes is unknown and
    therefore, it is difficult to prevent the onset of type 1 diabetes.'
  prefs: []
  type: TYPE_NORMAL
- en: Type 2 diabetes results from the body's gradual resistance to insulin. Type
    2 diabetes is the prevalent form of diabetes in the world, and it is caused by
    excessive body weight, irregular exercise, and a poor diet. Fortunately, the onset
    of type 2 diabetes can be prevented and reversed if diagnosed early.
  prefs: []
  type: TYPE_NORMAL
- en: One of the barriers for early detection and diagnosis of diabetes is that the
    early stages of diabetes are often non-symptomatic. People who are on the path
    to diabetes (also known as prediabetes) often do not know that they have diabetes
    until it is too late.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we use machine learning to address this problem? If we have a labeled
    dataset that contains some vital measurements of patients (for example, age and
    blood insulin level), as well as a true label indicating the onset of diabetes
    in the patient sometime after the measurements were taken, then we can train a
    neural network (machine learning classifier) on this data and use it to make predictions
    on new patients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e97e9572-4f07-4f7b-85e8-a5659671a799.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we'll briefly explore how AI is transforming healthcare.
  prefs: []
  type: TYPE_NORMAL
- en: AI in healthcare
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beyond predicting diabetes using machine learning, the field of healthcare,
    in general, is ripe for disruption by AI. According to a study by Accenture, the
    market for AI in healthcare is set for explosive growth, with an estimated compound
    annual growth rate of 40% by 2021\. This significant growth is driven by a proliferation
    of AI and tech companies in healthcare.
  prefs: []
  type: TYPE_NORMAL
- en: Apple's chief executive officer, Tim Cook, believes that Apple can make significant
    contributions in healthcare. Apple's vision for disrupting healthcare can be exemplified
    by its developments in wearable technology. In 2018, Apple announced a new generation
    of smartwatches with active monitoring of cardiovascular health. Apple's smartwatches
    can now conduct electrocardiography in real time, and even warn you when your
    heart rate becomes abnormal, which is an early sign of cardiovascular failure.
    Apple's smartwatches also collect accelerometer and gyroscope measurements to
    predict in real time if a significant fall has occurred. Clearly, the impact of
    AI on healthcare will be far-reaching.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of AI in healthcare is not in replacing physicians and other healthcare
    workers, but rather to augment their activities. AI has the potential to support
    healthcare workers throughout a patient''s journey and to assist healthcare workers
    in discovering insights into a patient''s wellbeing using data. According to experts,
    AI in healthcare will see the most growth in the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90889624-341c-4fa6-80dd-ded51086ff81.png)'
  prefs: []
  type: TYPE_IMG
- en: Automated diagnosis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's zoom in on automated diagnosis as that is the area of concern for this
    project. Experts believe that AI will greatly augment the way medical diagnosis
    is conducted. At the moment, most medical diagnosis is performed by skilled medical
    experts. In the case of medical diagnosis through images (such as X-rays and MRI
    scans), skilled radiologists are required to provide their expertise in the diagnostic
    process. These skilled medical professionals go through years of rigorous training
    before being certified, and there is a shortage of these medical experts in certain
    countries, which contributes to poor outcomes. The role of AI is to augment these
    experts and to offload low-level routine diagnosis, which can be done by an AI
    agent with a high degree of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: This ties back to our original problem statement; using AI to predict which
    patients are at risk of diabetes. As we shall see, we can use machine learning
    and neural networks to make this prediction. In this chapter, we will design and
    implement an MLP that can predict the onset of diabetes using machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: The diabetes mellitus dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset that we will be using for this project comes from the Pima Indians Diabetes
    dataset, as provided by the National Institute of Diabetes and Digestive and Kidney
    Diseases (and hosted by Kaggle).
  prefs: []
  type: TYPE_NORMAL
- en: The Pima Indians are a group of native Americans living in Arizona, and they
    are a highly studied group of people due to their genetic predisposition to diabetes.
    It is believed that the Pima Indians carry a gene that allows them to survive
    long periods of starvation. This thrifty gene allowed the Pima Indians to store
    in their bodies whatever glucose and carbohydrates they may eat, which is genetically advantageous
    in an environment where famines were common.
  prefs: []
  type: TYPE_NORMAL
- en: However, as society modernized and the Pima Indians began to change their diet
    to one of processed food, the rate of type 2 diabetes among them began to increase
    as well. Today, the incidence of type 2 diabetes among the Pima Indians is the
    highest in the world. This makes them a highly studied group of people, as researchers
    attempt to find the genetic link of diabetes among the Pima Indians.
  prefs: []
  type: TYPE_NORMAL
- en: The Pima Indians diabetes dataset consists of diagnostic measurements collected
    from a sample of female Pima Indians, along with a label indicating whether the
    patient developed diabetes within five years of the initial measurement. In the
    next section, we'll perform exploratory data analysis on the Pima Indians diabetes
    dataset to uncover important insights about the data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s dive into the dataset to understand the kind of data we are working
    with. We import the dataset into pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a quick look at the first five rows of the dataset by calling the
    `df.head()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f948292-21c1-42bc-9e9f-f44b34b89f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It looks like there are nine columns in the dataset, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Pregnancies`: Number of previous pregnancies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Glucose`: Plasma glucose concentration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BloodPressure`: Diastolic blood pressure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SkinThickness`: Skin fold thickness measured from the triceps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Insulin` : Blood serum insulin concentration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BMI`: Body mass index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DiabetesPedigreeFunction`: A summarized score that indicates the genetic predisposition
    of the patient for diabetes, as extrapolated from the patient''s family record
    for diabetes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Age`: Age in years'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Outcome`: The target variable we are trying to predict, `1` for patients that
    developed diabetes within five years of the initial measurement, and `0` otherwise'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start by visualizing the distribution of the nine variables in the dataset.
    We can do this by plotting a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a73eaea-dfa6-4131-954b-2163e03e0476.png)'
  prefs: []
  type: TYPE_IMG
- en: The histogram provides some interesting insights into the data. From the histogram
    for `Age`, we can see that most of the data was collected from young people, with
    the most common age group between 20-30 years old. We can also see that the distribution
    for `BMI`, `BloodPressure`, and `Glucose` concentration is normally distributed
    (that is, a bell curve shape), which is what we we expect when we collect such
    statistics from a population. However, note that the tail of the `Glucose` concentration
    distribution shows some rather extreme values. It appears that there are people
    with plasma `Glucose` concentration that is almost 200\. On the opposite end of
    the distribution, we can see that there are people with 0 values for `BMI`, `BloodPressure`,
    and `Glucose`. Logically, we know that it is not possible to have a 0 value for
    these measurements. Are these missing values? We shall explore more in the next
    section on data preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the distribution for the number of previous `Pregnancies`, we
    can see some outliers as well. We can see that some patients had more than 15
    previous pregnancies. While that may not be entirely surprising, we should keep
    such outliers in mind when we do our analysis, as it can skew our results.
  prefs: []
  type: TYPE_NORMAL
- en: The distribution of outcome shows that approximately 65% of the population belongs
    to class 0 (no diabetes), while the remaining 35% belongs to class 1 (diabetes).
    When building a machine learning classifier, we should always keep in mind the
    distribution of classes in our training data. In order to ensure that our machine
    learning classifier works well in the real world, we should ensure that the distribution
    of classes in our training data mirrors that of the real world. In this case,
    the distribution of the classes does not match those in the real world, as it
    is estimated by the **World Health Organization** (**WHO**) that only 8.5% of
    the world population suffers from diabetes.
  prefs: []
  type: TYPE_NORMAL
- en: We do not need to worry about the distribution of classes in our training data
    for this project, as we are not going to deploy our classifier in the real world.
    Nevertheless, it is a good practice for data scientists and machine learning engineers
    to check the distribution of classes in the training data, in order to ensure
    strong model performance in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, it is important to note that the variables are on different scales.
    For example, the `DiabetesPedigreeFunction` variable ranges from 0 to ~2.5, while
    the `Insulin` variable ranges from 0 to ~800\. This difference in scale can cause
    problems in training our neural network, as variables with larger scales tend
    to dominate variables with smaller scales. In the next section on data preprocessing,
    we will look at how we can standardize the variables.
  prefs: []
  type: TYPE_NORMAL
- en: We can also plot a density plot to investigate the relationship between each
    variable and the target variable. To do so, we will use seaborn. seaborn is a
    Python data visualization library based on matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to plot a density plot for each variable.
    To visualize the difference in distribution between diabetics and non-diabetics,
    we will also plot them separately on each plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll get the output shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2b68949-1ee2-47e3-ab61-844dd4106287.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the output in continuation to the preceding
    one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a7258813-1cf1-476d-99c4-133d3bd8112b.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding density plots look complicated, but let's focus on each individual
    plot and see what insights can we gain. If we look at the plot for the `Glucose` variable,
    we can see that among the non-diabetics (solid line), the curve has a normal distribution
    centered around the value 100\. This tells us that among non-diabetics, most people
    have a blood glucose value of 100 mg/dL. On the other hand, if we look at the
    `Diabetics` (dashed line), the curve is wider and is centered around a value of
    150\. This tells us that diabetics tends to have a wider range of blood glucose
    value, and the average blood glucose value is around 150 mg/dL. Therefore, there
    is a significant difference in blood glucose values for diabetes vs non-diabetics. 
    A similar analysis can also be made for the variable `BMI` and `Age`. In other
    words, the `Glucose`, `BMI`, and `Age` variables are strong predictors for diabetes.
    People with diabetes tend to have higher blood glucose level, higher BMI, and
    are older.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, we can see that for variables such as `BloodPressure` and
    `SkinThickness`, there is no significant difference in the distribution between
    diabetics and non-diabetics. The two groups of people tend to have similar blood
    pressure and skin thickness values. Therefore, `BloodPressure` and `SkinThickness` are
    poorer predictors for diabetes.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, *Exploratory data analysis*, we have discovered that
    there are 0 values in certain columns, which indicates missing values. We have
    also seen that the variables have different scales, which can negatively impact
    model performance. In this section, we will perform data preprocessing to handle
    these issues.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s call the `isnull()` function to check whether there are any missing
    values in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb03262d-f2ea-4a91-95e9-040c8af8ad6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It seems like there are no missing values in the dataset, but are we sure?
    Let''s get a statistical summary of the dataset to investigate further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3802ada8-f2b4-427d-b5c6-da3f14ce1010.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that there are `768` rows of data, and the `Pregnancies`, `Glucose`,
    `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI` columns have a minimum
    value of `0`. This doesn't quite make sense. The measurements for `Glucose`, `BloodPressure`,
    `SkinThickness`, `Insulin`, and `BMI` should never be `0`. This is an indication
    that there are missing values in our dataset. The values were probably recorded
    as `0` due to certain issues during data collection. Perhaps the equipment was
    faulty, or the patient was unwilling to have their measurements taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case, we need to handle these `0` values. Let''s take a look at how
    many `0` values are there in each column to understand the extent of the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd47ea3c-fd73-48bc-a01d-aa3a51c52f9a.png)'
  prefs: []
  type: TYPE_IMG
- en: In the `Insulin` column, there are `374` rows with `0` values. That is almost
    half of the data that we have! Clearly, we cannot discard these rows with `0`
    values as that will cause a significant drop in model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several techniques to handle these missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: Remove (discard) any rows with missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace the missing values with the mean/median/mode of the non-missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict the actual values using a separate machine learning model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the missing values comes from continuous variables such as `Glucose`,
    `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI`, we will replace the missing
    values with the mean of the non-missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s replace the `0` values in the `Glucose`, `BloodPressure`, `SkinThickness`,
    `Insulin`, and `BMI` columns with `NaN`. This way, pandas will understand that
    these values are invalid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s confirm that the `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`,
    and `BMI` columns no longer contain `0` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/028c2dfd-94bf-4def-a37e-d2949e70e9cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that we did not modify the `Pregnancies` column as `0` values in that column
    (that is, `0` previous pregnancies) are perfectly valid.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s replace the `NaN` values with the mean of the non-missing values.
    We can do this using the handy `fillna()` function in pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Data standardization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data standardization is another important technique in data preprocessing. The
    goal of data standardization is to transform the numeric variables so that each
    variable has zero mean and unit variance.
  prefs: []
  type: TYPE_NORMAL
- en: Standardization of variables as a preprocessing step is a requirement for many
    machine learning algorithms. In neural networks, it is important to standardize
    the data in order to ensure that the backpropagation algorithm works as intended.
    Another positive effect of data standardization is that it shrinks the magnitude
    of the variables, transforming them to a scale that is more proportional.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen earlier, variables such as `Insulin` and `DiabetesPedigreeeFunction` have
    vastly different scales; the maximum value for `Insulin` is `846` while the maximum
    value for `DiabetesPedigreeeFunction` is only `2.42`. With such different scales,
    the variable with the greater scale tends to dominate when training the neural
    network, causing the neural network to inadvertently place more emphasis on the
    variable with a greater scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'To standardize our data, we can use the `preprocessing` class from scikit-learn.
    Let''s import the `preprocessing` class from scikit-learn and use it to scale
    our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the object returned by the `preprocessing.scale()` function is no longer
    a pandas DataFrame, let''s convert it back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, since we do not want to scale the `Outcome` column (which is the target
    variable that we are trying to predict) let''s use the original `Outcome` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the mean, standard deviation and the max of each of the
    transformed variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09b7c567-5add-4a81-a9b0-69fcec4b1cb7.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the scale of each variable is now a lot closer to one another.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the data into training, testing, and validation sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last step in data preprocessing is to split the data into training, testing,
    and validation sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training set**: The neural network will be trained on this subset of the
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation set**: This set of data allows us to perform hyperparameter tuning
    (that is, tuning the number of hidden layers) using an unbiased source of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing set**: The final evaluation of the neural network will be based on
    this subset of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The purpose of splitting the data into training, testing, and validation sets
    is to avoid overfitting and to provide an unbiased source of data for evaluating
    model performance. Typically, we will use the training and validation set to tune
    and improve our model. The validation set can be used for early stopping of training,
    that is, we continue to train our neural network only to the point where model
    performance on the validation set stops improving. This allows us to avoid overfitting
    the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The testing set is also known as the holdout dataset, as the neural network
    will never be trained using it. Instead, we will use the testing set to evaluate
    the model at the end. This provides us with an accurate reflection of the real-world
    performance of our model.
  prefs: []
  type: TYPE_NORMAL
- en: How do we decide the proportion of each split? The competing concerns, in this
    case, is that if we allocate most of the data for training purposes, model performance
    will increase at the detriment of our ability to avoid overfitting. Similarly,
    if we allocate most of the data for validation and testing purposes, model performance
    will decrease as there might be insufficient data for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a general rule of thumb, we should split the original data into 80% training
    and 20% testing, and then to split the training data into 80% training and 20%
    validation again. The following diagram illustrates this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/537a32d0-a0c4-45ff-bb2f-ad5c7a2b8c8d.png)'
  prefs: []
  type: TYPE_IMG
- en: One important point to note is that the splitting of data must be done at random.
    If we were to use a non-random method of splitting the data (for example, the
    first 80% of rows go to the **Training Set** and the last 20% of rows go to the
    **Testing** **Set**), we could potentially be introducing bias into our training
    and testing set. For example, the original data could be sorted in chronological
    order, so a non-random method of splitting the data could mean that our model
    is only trained on data from a certain date, which is highly biased and would
    not work as well in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: The `train_test_split` function from scikit-learn allows us to randomly split
    a dataset easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s separate the dataset into `X` (input features) and `y` (target
    variable):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, make the first split to split the data into the training set (80%) and
    the testing set (20%) according to the preceding diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, make the second split to create the final training set and the validation
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: MLPs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have completed exploratory data analysis and data preprocessing,
    let's turn our attention towards designing the neural network architecture. In
    this project, we will be using MLPs.
  prefs: []
  type: TYPE_NORMAL
- en: An MLP is a class of feedforward neural network, and it distinguishes itself
    from the single-layer perceptron that we've discussed in [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101*, by having at least one hidden layer, with each
    layer activated by a non-linear activation function. This multilayer neural network
    architecture and non-linear activation allows MLPs to produce non-linear decision
    boundaries, which is crucial in multi-dimensional real-world datasets such as
    the Pima Indians Diabetes dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The model architecture of the MLP can be represented graphically as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a44666b-3e19-42d5-ad40-06a4efa666d0.png)'
  prefs: []
  type: TYPE_IMG
- en: As discussed in [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101*, we can use an arbitrary number of hidden layers
    in our MLP. For this project, we will use two hidden layers in our MLP.
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each node in the **input layer** (illustrated by the circles in the pink rectangle)
    refers to each feature (that is, column) in the dataset. Since there are eight
    features in the Pima Indians dataset, there should be eight nodes in the input
    layer of our MLP.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next layer after the input layer is known as a **hidden ****layer. **As
    we have seen in [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101*, the hidden layer takes the input layer and
    applies a **non-linear activation function** to it. Mathematically, we can represent
    the function of the hidden layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25203d3f-47b4-486b-ad64-302c7facf89d.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/62638abb-569a-4dbd-bf4e-9e1fa0344ad9.png) refers to the input passed
    from the previous layer, ![](img/9b29f759-2464-42fc-9031-6b3eeb8df7f3.png) refers
    to the non-linear activation function, ![](img/0a1c0ce4-c44a-4d72-88c5-7c474955aea7.png) are
    the weights, and ![](img/862211f8-85fc-4504-8046-f965be7fcc17.png) refers to the
    biases.'
  prefs: []
  type: TYPE_NORMAL
- en: To keep things simple, we will only use two hidden layers in our model for this
    project. Increasing the number of hidden layers tends to increase the model complexity
    and training time. For this project, two hidden layers will suffice, as we shall
    see later when we look at the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When designing the neural network model architecture, we also need to decide
    what activation functions to use for each layer. Activation functions have an
    important role to play in neural networks. You can think of activation functions
    as *transformers* in neural networks; they take an input value, transform the
    input value, and pass the transformed value to the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: In this project, we will use the **rectified linear unit** (**ReLU**) and the
    **sigmoid** as our activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a general rule of thumb, ReLU is always used as the activation function for
    our intermediate hidden layers (that is, non-output layer). In 2011, it was proved
    by researchers that ReLU is superior to all previously used activation functions
    for training **deep neural networks** (**DNNs**). Today, ReLU is the most popular
    choice of activation function for DNNs, and it has become a default choice for
    activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we can represent ReLU as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8264954-9c83-4950-847d-0305e48abd66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What the ReLU function does is to simply consider only the non-negative portion
    of the original *![](img/ffe50dd3-9cfb-411f-a82c-ca05ef032f78.png)*, and to treat
    the negative portion as *0*. The following graph illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e635eb09-90bd-4f41-a000-6eb6326e29a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the final output layer, we need an activation function that makes a prediction
    on the class of the label. For this project, we are making a simple binary prediction
    on the class: 1 for patients with onset of diabetes and 0 for patients without
    the onset of diabetes. The sigmoid activation function is ideal for binary classification
    problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, we can represent the sigmoid activation function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f9e96de-1353-4b51-b79c-39283c052647.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Although this looks complicated, the underlying function is actually pretty
    simple. The **Sigmoid Activation Function** simply takes a value and squashes
    it between **0** and **1**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/849c7c78-1e4f-4d31-8fc0-b56464c5b3b4.png)'
  prefs: []
  type: TYPE_IMG
- en: If the transformed value ![](img/a4e7a8d0-50e1-41cf-a819-9f40fc9c3e42.png) is
    greater than **0.5**, then we classify it as class **1**. Similarly, if the transformed
    value is less than **0.5**, we classify it as class **0**. The **Sigmoid Activation
    Function** allows us to take an input value and outputs a binary class (**1**
    or **0**), which is exactly what we require for this project (that is, to predict
    whether a person has diabetes or not).
  prefs: []
  type: TYPE_NORMAL
- en: Model building in Python using Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're finally ready to build and train our MLP in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Model building
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned in [Chapter 1](1068b86b-d786-48ba-b91c-35d0ff569460.xhtml), *Machine
    Learning and Neural Networks 101*, the `Sequential()` class in Keras allows us
    to construct a neural network like Lego, stacking layers on top of one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a new `Sequential()` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's stack our first hidden layer. The first hidden will have 32 nodes,
    and the input dimensions will be 8 (because there are 8 columns in `X_train`).
    Notice that for the very first hidden layer, we need to indicate the input dimensions.
    Subsequently, Keras will take care of the size compatibility of other hidden layers
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Another point to note is that we have arbitrarily decided on the number of nodes
    for the first hidden layer. This variable is a hyperparameter that should be carefully
    selected through trial and error. In this project, we will skip hyperparameter
    tuning and just use 32 as the number of nodes since it does not necessarily make
    much of a difference for this simple dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add the first hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `activation` function used is `relu`, as discussed in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's stack on the second hidden layer. Adding more hidden layers increases
    the complexity of our model, but can sometimes cause the model to overfit. For
    this project, we will use two hidden layers only, as that is sufficient to produce
    a satisfactory model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add our second hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Finally, finish off the MLP by adding the output layer. This layer has only
    one single node, as we're dealing with binary classification here. The `activation`
    function used is the `sigmoid` function, and it *squashes* the output between
    0 and 1 (binary output).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we add the output layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Model compilation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start training our model, we need to define the parameters of the
    training process, which is done via the `compile` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three different parameters we need to define for the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizer**: Let''s use the `adam` optimizer, which is a popular optimizer
    in Keras. For most datasets, the `adam` optimizer will work well without much
    tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**: We''ll use `binary_crossentropy` as our `loss` function
    since the problem at hand is a binary classification problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics**: We''ll use `accuracy` (that is, the percentage of correctly classified
    samples) as our evaluation metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, we can run the `compile()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train our MLP model defined in earlier steps, let''s call the `fit` function.
    Let''s train our model for `200` iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e09e058e-ecec-4ed8-a802-082d4a211bb3.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the loss decreases and the accuracy increases over each epoch,
    as the learning algorithm continuously updates the weights and biases in the MLP
    according to the training data. Note that the accuracy shown in the preceding
    screenshot refers to the accuracy based on the training data. In the next section,
    we will take a look at the performance of the MLP based on the held out testing
    data, as well as some other important metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Results analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having successfully trained our MLP, let's evaluate our model based on the testing
    accuracy, confusion matrix, and **receiver operating characteristic** (**ROC**)
    curve.
  prefs: []
  type: TYPE_NORMAL
- en: Testing accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can evaluate our model on the training set and testing set using the `evaluate()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1751887a-189c-42ad-9d5e-b18dbab8bed2.png)'
  prefs: []
  type: TYPE_IMG
- en: The accuracy is 91.85% and 78.57% on the training set and testing set respectively.
    The difference in accuracy between the training and testing set isn't surprising
    since the model was trained on the training set. In fact, by training the model
    over more iterations, we can achieve 100% accuracy on the training set, but that
    would not be desirable as it just means that we are overfitting our model. The
    testing accuracy should always be used to evaluate the real-world performance
    of our model, as the testing set represents real-world data that the model has
    never seen before.
  prefs: []
  type: TYPE_NORMAL
- en: The testing accuracy of 78.57% is pretty impressive for our simple MLP with
    just two hidden layers. What this means is that given the eight measurements from
    a new patient (glucose, blood pressure, insulin, and so on), our MLP is able to
    predict with ~80% accuracy whether that patient will develop diabetes within the
    next five years. In essence, we have developed our first AI agent!
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The confusion matrix is a useful visualization tool that provides analysis on
    the true negative, false positive, false negative, and true positives made by
    our model. Beyond a simple accuracy metric, we should also look at the confusion
    matrix to understand the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of true negative, false positive, false negative, and true positives
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True negative**: Actual class is negative (no diabetes), and the model predicted
    negative (no diabetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive**: Actual class is negative (no diabetes), but the model predicted
    positive (diabetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negative**: Actual class is positive (diabetes), but the model predicted
    negative (no diabetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True positive**: Actual class is positive (diabetes), and the model predicted
    positive (diabetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly, we want our false positive and false negative numbers to be as low
    as possible, and for the true negative and true positive numbers to be as high
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can construct a confusion matrix using the `confusion``_matrix` class from
    `sklearn`, using `seaborn` for the visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And the result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/059c6fc4-a341-4a29-9f3d-2b7f7a574261.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding confusion matrix, we can see that most predictions are true
    negatives and true positives (as indicated by the 78.57% test accuracy in the
    previous section). The remaining 19 predictions are false negatives and 14 other
    predictions are false positives, which are undesirable.
  prefs: []
  type: TYPE_NORMAL
- en: For diabetes prediction, a false negative is perhaps more damaging than a false
    positive. A false negative means telling the patient that they will not develop
    diabetes within the next five years, when in fact they would. Therefore, when
    we evaluate the performance of different models for predicting the onset of diabetes,
    a model with a lower false negative is more desirable.
  prefs: []
  type: TYPE_NORMAL
- en: ROC curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For classification tasks, we should also look at the ROC curve to evaluate
    our model. The ROC curve is a plot with the **True Positive Rate** (**TPR**) on
    the *y* axis and the **False Positive Rate** (**FPR**) on the *x *axis. TPR and
    FPR are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a6b9f5f-8686-4ac2-bda5-1f4914f1429d.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/78951486-57b2-4acb-b924-600aae8dc499.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we analyze the ROC curve, we look at the **area under the curve** (**AUC**)
    to evaluate the performance of the model that produced the curve. A large AUC
    indicates that the model is able to differentiate the respective classes with
    high accuracy, while a low AUC indicates that the model makes poor, often wrong
    predictions. A ROC curve that lies on the diagonal indicates that the model does
    no better than random. The following diagram illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5b37e12b-9118-4dec-9a60-0a6391164a21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s plot the ROC curve for our model and analyze its performance. As always,
    scikit-learn provides a useful `roc_curve` class to help us do this. But first,
    let''s get the predicted probabilities of each class using the `predict()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the `roc_curve` function in order to get the corresponding false
    positive rate and true positive rate for the ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now plot the values on a plot using matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5ccd4e6-20e1-47b5-8ae1-3448b2931bd7.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding **ROC Curve**, we can see that the model performs rather
    well, close to the model **ROC Curve** shown in the preceding diagram. This shows
    that our model is able to differentiate samples of different classes, making good
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Further improvements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, it is worth wondering if it is possible to further improve the
    performance of our model. How can we further improve the accuracy of our model
    and/or improve the false negative and false positive rate?
  prefs: []
  type: TYPE_NORMAL
- en: In general, any limitation in performance is usually due to the lack of strong
    features in the dataset, rather than the complexity of the neural network used.
    The Pima Indians Diabetes dataset only consists of eight features, and it can
    be argued that these features alone are insufficient to really predict the onset
    of diabetes.
  prefs: []
  type: TYPE_NORMAL
- en: One way to increase the number of features we provide to the model is via **feature
    engineering**. Feature engineering is the process of using one's domain knowledge
    of the problem to create new features for the machine learning algorithm. Feature
    engineering is one of the most important aspects of data science. In fact, many
    past winners of Kaggle competitions have credited their success to feature engineering,
    and not just tuning of the machine learning model. However, feature engineering
    is a double-edged sword and must be done carefully. Adding inappropriate features
    may create noise for our machine learning model, affecting the performance of
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: On the opposite spectrum, we may also consider removing features in order to
    improve model performance. This is known as **feature selection**. Feature selection
    is used when we believe that the original dataset contains too much noise, and
    removing the noisy features (features that are not strong predictors) may improve
    model performance. One popular way to do feature selection is to use decision
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are a separate class of machine learning models with a tree-like
    data structure. Decision trees are useful as they calculate and rank the most
    important features according to certain statistical criteria. We can first fit
    the data using the decision tree, and then use the output from the decision tree
    to remove features that are deemed unimportant, before providing the reduced dataset
    to our neural network. Again, feature selection is a double-edged sword that can
    potentially affect model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Although feature engineering and feature selection were not done in this project,
    we will see it being used in other projects in later chapters, as we gradually
    take on more challenging problems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have designed and implemented an MLP that is capable of
    predicting the onset of diabetes with ~80% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: We first performed exploratory data analysis where we looked at the distribution
    of each variable, as well as the relationship between each variable and the target
    variable. We then performed data preprocessing to remove missing data and we also
    standardized our data such that each variable has a mean of 0 with unit standard
    deviation. Finally, we split our original data randomly into a training set, a
    validation set, and a testing set.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at the architecture of the MLP that we used, which consists of
    2 hidden layers, with 32 nodes in the first hidden layer and 16 nodes in the second
    hidden layer. We then implemented this MLP in Keras using the sequential model,
    which allows us to stack layers on one another. We then trained our MLP using
    the training set, where Keras used the Adam optimizer algorithm to modify the
    weights and biases in the neural network over 200 iterations, gradually improving
    model's accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we evaluated our model using metrics such as the testing accuracy,
    confusion matrix, and ROC curve. We saw the importance of looking at metrics such
    as false negatives and false positives when evaluating our model, and how false
    negatives and false positives are important metrics, especially for a classifier
    that predicts the onset of diabetes.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the chapter on using a simple MLP to predict the onset of diabetes.
    In the next chapter, [Chapter 3](bf157365-e4d3-42ae-89f4-58c9047e6500.xhtml),
    *Predicting Taxi Fares with Deep Feedforward Networks*, we will use a more complicated
    dataset that utilizes temporal and geolocation information to make predictions
    of taxi fares.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do we plot a histogram of each variable in a pandas DataFrame, and why are
    histograms useful?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can plot a histogram by calling the `df.hist()` function built into a pandas
    DataFrame class. A histogram provides an accurate representation of the distribution
    of our numerical data.
  prefs: []
  type: TYPE_NORMAL
- en: How do we check for missing values (NaN values) in a pandas DataFrame?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can call the `df.isnull().any()` function to easily check whether there are
    any null values in each column of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Besides NaN values, what other kinds of missing values could appear in a dataset?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Missing values can also appear in the form of 0 values. Missing values are often
    recorded as 0 in a dataset due to certain issues during data collection—perhaps
    the equipment was faulty, or there are other issues hindering data collection.
  prefs: []
  type: TYPE_NORMAL
- en: Why is it crucial to remove missing values in a dataset before training a neural
    network with it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural networks are unable to handle NaN values. Neural networks require all
    of their inputs to be numerical due to the kind of mathematical operations they
    perform during forward and back propagation.
  prefs: []
  type: TYPE_NORMAL
- en: What does data standardization do, and why is it important to perform data standardization
    before training a neural network with the data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The goal of data standardization is to transform the numeric variables so that
    each variable has zero mean and unit variance. When training neural networks,
    it is important to ensure that the data has been standardized. This ensures that
    features with a larger scale does not dominate features with a smaller scale when
    training a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: How do we split our dataset to ensure unbiased evaluation of model performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before training a neural network, we should split our dataset into a training
    set, validation set, and testing set. The neural network will be trained on the
    training set, while the validation set allows us to perform hyperparameter tuning
    using an unbiased source of data. Finally, the testing set provides an unbiased
    source of data to evaluate the performance of the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: What are the characteristic features of the model architecture of a MLP?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MLPs are feedforward neural networks, and they have at least one hidden layer,
    with each layer activated by a non-linear activation function. This multilayer
    neural network architecture and non-linear activation allows MLPs to produce non-linear
    decision boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: What is the purpose of activation functions in neural networks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Activation functions performs a non-linear transformation on the weights and
    biases before passing it to the next layer. The most popular and effective activation
    function between hidden layers is the ReLU activation function.
  prefs: []
  type: TYPE_NORMAL
- en: What is a suitable loss function to use when training our neural network for
    a binary classification problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The binary cross entropy is the most appropriate loss function to use when training
    our neural network for a binary classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: What does a confusion matrix represent, and how can we use it to evaluate the
    performance of our neural network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A confusion matrix provides values on the true negative, false positive, false
    negative, and true positives made by our neural network. Beyond a simple accuracy
    metric, the confusion matrix allows us to drill down into the kind of mistakes
    made by our neural network (false positives and false negatives).
  prefs: []
  type: TYPE_NORMAL
