- en: Mass Customization of Client Lifetime Wealth
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to manage the digital data of customers.
    We also covered the Open Bank Project and the Open Bank API. In addition, we learned
    about document layout analysis and looked at an example of projecting the cash
    flow for a typical household. Then, we looked at another example of how to track
    daily expenses using invoice entity recognition.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to combine data from a survey for personal
    data analysis. We will learn techniques such as Neo4j, which is a graph database.
    We will build a chatbot to serve customers 24/7\. We will also learn how to predict
    customer responses using NLP and Neo4j with the help of an example. After this,
    we will learn how to use cypher languages to manipulate data from the Neo4j database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Financial concepts of wealth instruments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting customer responses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a chatbot to serve customers 24/7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge management using NLP and graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Financial concepts of wealth instruments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be answering a few questions asked by a consumer bank's
    marketers. Then, we will look at another important model development technique—ensemble
    learning—which will be useful in combining predictions from different models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sources of wealth: asset, income, and gifted'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most common tasks in retail banking customer analytics is to retrieve
    additional data that helps us to explain the customers' investment behavior and
    patterns. No doubt we will know the response of the customers, but the work of
    a model is to find out why they respond as they do. Surprisingly, there is a lot
    of aggregated information concerning the behaviors of individuals, such as census
    data. We can also find data from social media, where users use social media for
    authentication. The relevant social media information can then be chained together
    with individual-level transactional data that we observed internally in the organization.
    To explain individual banking behaviors, the most relevant supplementary data
    that we want is the information regarding their wealth.
  prefs: []
  type: TYPE_NORMAL
- en: Customer life cycle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A typical life cycle involves three major phases—acquisition, cross-selling/upselling,
    and retention. The following diagram illustrates these three phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5254c6a-e008-4981-86d9-da80671bf861.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Acquisition** is when we start a commercial relationship with customers.
    Then, we move on to **cross-selling** and **upselling**. Cross-selling is about
    improving the number of products/services that are sold to the customer. Up-selling
    is about deepening the wallet share of the same product with the products/services.
    **Retention** is about keeping the relationship and is a defensive act by the
    bank to protect the relationship. Our first example (described in the following
    section) concerns cross-selling (if the customers do not have the product) and
    up-selling (if the customers own the product).'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble learning is the boosting technique that helps us in improving the accuracy
    of the prediction. We will also learn how to use the graph database for knowledge
    storage. Knowledge storage is the current challenge in knowledge representation
    that can be used to empower AI for professional-grade financial services.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning is an approach that is used to summarize several models in
    order to give a more stable prediction. It was a very common approach before deep
    neural networks became popular. For completeness, we do not want to ignore this
    modeling technique in this very short book. In particular, we have used random
    forest, which means that we build lots of decision trees as a forest and we apply
    logic to cut down trees that have lower performance. Another approach would be
    combining the weaker model to generate a strong result, which is called the **boosting
    method**. We won't cover it here, but readers are encouraged to dig deeper in
    the scikit-learn documentation ([https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)).
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge retrieval via graph databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make a machine talk like a human in customer services, one of the key elements
    is the conversational component. When we engage in conversation, it is normal
    that human customers may not be able to provide the full amount of information
    required for processing. Humans can work with fuzziness. Humans can understand
    the context, and so can extrapolate meaning without the concepts being explicitly
    mentioned. Knowing that a machine can only solve definite problems while humans
    can work on fuzziness, it is the job of the machine to infer meaning from the
    knowledge map that it has for the customers. A graph database is used to serve
    this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Predict customer responses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have not talked about the day-to-day marketing activity of the bank.
    Now, we have finally come to look at how marketing prospects are determined. Even
    though each customer is unique, they are still handled by algorithms in the same
    way.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, you will assume the role of a data scientist tasked with the
    marketing of a term deposit product. We are going to train the model to predict
    the marketing campaign for the term deposit. Data pertaining to the bank's internal
    data regarding customers and their previous responses to the campaign is obtained
    from the Center for Machine Learning and Intelligent Systems ([https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing)),
    the Bren School of Information and Computer Science, and the University of California,
    Irvine. Survey information about personal wealth is obtained from the US Census
    Bureau ([https://www.census.gov/data/tables/time-series/demo/income-poverty/historical-income-households.html](https://www.census.gov/data/tables/time-series/demo/income-poverty/historical-income-households.html)),
    which serves as an augmentation to the bank's internal data.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are four steps to complete this example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We introduce random forest, which is a type of machine learning algorithm that
    utilizes ensemble learning, allowing predictions to be made by multiple models.
    The resulting model is a combination of the results from the multiple models.
    The following is the code snippet to import the required libraries and define
    the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Census data provides information about the deposit and wealth of the age group
    placed in the bank. The following is the code snippet to handle census data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to illustrate the mapping of one column''s data, using age to introduce
    wealth data. The following is the code snippet to combine census data with the
    bank''s data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the code snippet to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You have merged an external dataset with the internal dataset
    to augment our understanding of the customers.
  prefs: []
  type: TYPE_NORMAL
- en: Building a chatbot to service customers 24/7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we interact with a robot, we expect it to understand and speak to us.
    The beauty of having a robot work for us is that it could serve us 24 hours a
    day throughout the week. Realistically, chatbots nowadays interact poorly with
    customers, and so we should try to break down the components of these chatbots
    to raise the bar to a higher standard. For an application-type development, you
    could use Google Assistant, Amazon''s Alexa, or IBM Watson. But for learning purposes,
    let''s break down the components and focus on the key challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9437b12-bdce-4813-9521-471012ec8b4c.png)'
  prefs: []
  type: TYPE_IMG
- en: The chatbot performs two operations at a high level. One is to convert an input
    from voice to text, and another one is to translate an output from text to voice.
    Both of these operations involve extracting the entity and understanding the intent.
    In this example, the resulting text is an entity, whereas the meaning of the text
    is an intent. It represents a conversation between the service requester and the
    service provider. When faced with an incoming service request, the chatbot converts
    the voice instructions into text and adds context to the information received.
    Once the context building is done, the chatbot processes the information to generate
    the output in text format. The chatbot has to convert it into an audible voice
    output to be presented to the service requester. The whole scenario is explained
    in the preceding diagram.
  prefs: []
  type: TYPE_NORMAL
- en: Right now, let's focus on chat only, without worrying about voice recognition
    and utterance—that is, let's ignore voice to text and text to voice. In my opinion,
    since this task is machine- and memory-intensive, and the data is available in
    so many places, it is not for a start-up to work on this task; instead, we should
    leave it to a mainstream cloud provider with a strong infrastructure to deliver
    the service.
  prefs: []
  type: TYPE_NORMAL
- en: For text-only chat, the key focus should be on intent classification and entity
    extraction. While we have touched on entity extraction in the previous chapter,
    the input still needs to be classified before it is extracted. Intent classification
    works similarly to entity extraction, but treats the whole sentence as an entity
    for classification.
  prefs: []
  type: TYPE_NORMAL
- en: While it is very common to run a chatbot using ChatterBot or RasaNLU, we can
    break down the components to run from the bottom up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we are a simple bank that offers deposits and loans. We are
    building a simple chatbot that can serve existing customers only, and at the moment,
    we only have two customers, one called **abc** with a deposit account, and another
    called **bcd** with a loan account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa9f65e9-f9a6-407c-ade7-46a8dbb18e8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Abc's deposit has an outstanding balance of 100 units and a pricing of 1, and
    bcd has an outstanding loan of 100 units and a pricing of 2.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge management using NLP and graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Essentially, there are two ways for us to retrieve and update knowledge about
    our real world. One is to store the knowledge in vector space and read the file
    to our memory during runtime using programs such as Word2Vector and BERT. Another
    approach is to load the knowledge into a graph database, such as Neo4j, and retrieve
    and query the data. The strength and weakness of both approaches lies in speed
    and transparency. For high-speed subject classification, in-memory models fare
    better, but for tasks that require transparency, such as banking decisions, the
    updating of data requires full transparency and permanent record keeping. In these
    cases, we will use a graph database. However, like the example we briefly covered
    in [Chapter 7](d29ff3a8-3879-4d50-8795-a39bae5cc793.xhtml), *Sensing Market Sentiment
    for Algorithmic Marketing at Sell Side*, NLP is required to extract information
    from the document before we can store the information in graph format.
  prefs: []
  type: TYPE_NORMAL
- en: Practical implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the steps to complete this example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the Cypher languages to import `csv` files into the database. We assume
    that the CSV file is dumped from the traditional SQL database. The following are
    the commands to be executed from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the browser and navigate to `http://localhost:7474/browser/`. Then, create
    a `username` and set a `password`. This will be executed only once:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Delete all nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create customer data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Create product data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Match and return the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Cypher is a language in itself; what we do is essentially create the product
    and customers. Then, we load another file that connects customers to products.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will connect to the `Neo4j` database that we just populated with data. The
    parameters follow the default setting. Please note the unique syntax of Cypher.
    In addition, the NLP model is loaded to be used for similarity analysis of the
    inputted instruction. The Cypher queries are stored in a dictionary. After the
    intent is read, the query string is retrieved. Then, we build the knowledge using
    the graph database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Users should be authenticated and identified properly using the SQL database.
    For ease of illustration, we will use `GraphDatabase`, but it is quite clear that
    using `GraphDatabase` for authentication is not right because we want to store
    a huge amount of data with usernames and passwords in a dedicated table whose
    access rights we can set to fewer individuals than the total number of people
    on the database. The following is the code snippet to authenticate the user:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Sentence intent and entity extraction utilizes spaCy on similarity analysis.
    Based on a pretrained word-to-vector model, the reserved words on intents and
    entities are compared with the inputted sentence to extract the relevant intent
    and entities. The model is overly simplified as readers are left with a lot of
    creative space to enhance the extraction works by using a better language model,
    such as BERT, on the assumption that we have made the relevant model to perform
    the relevant classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code snippet to extract entities and add intent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Cross-checking and further requesting missing information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The program will continuously ask for intents, products, and attributes until
    all three pieces of information are clear to the program. Underneath the classification
    of each of these parameters, we deploy Word2vec for simplified classification.
    In fact, we can run a best-in-class topic classification model, such as BERT,
    to understand the languages and topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code snippet to request missing information from the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Extracting the answer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When all information is filled in, the Cypher query will be executed and the
    information will be presented to the user. The following is the code snippet to
    extract the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Sample script of interactions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following snippet shows the users'' output and input. It is meant to show
    that the NLU can indeed extract intent and entities using closely associated words,
    thanks to the spaCy dictionary that allows us to find similar words. The whole
    point of the example is to show that for decisions requiring complete information
    before they are made, the graph database allows us to manage the dialogue and
    follow up with the missing information before any instructions are executed to
    serve the user. This is a very important feature when it comes to making professional
    decisions where we need its rationale to be transparent to a high degree of accuracy,
    as far as the machine can understand the language. The following is a snippet
    of the sample conversation of the chatbot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You have built a very simple chatbot that can show you the
    core functionality of chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: The example we are using is a very good echo of what we start with in commercial
    banking in terms of using borrowers' and depositors' data using reinforcement
    learning. Back then, the data was stored in variables at runtime. Right now, we
    have demonstrated another possibility for storing the data in graph data. Indeed,
    compared to the example in [Chapter 3](61949743-f7c3-4295-aaee-dab1d169d25c.xhtml), *Using
    Features and Reinforcement Learning to Automate Bank Financing,* the speed of
    reinforcement learning will be slower if we were to store data in a graph database
    rather than variables in a Python program. Therefore, we will use a graph database,
    but only for production and application levels when individual dialogues can tolerate
    some delay compared with a computation-intensive training phase.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about NLP and graph databases and we learned about
    the financial concepts that are required to analyze customer data. We also learned
    about an artificial intelligence technique called ensemble learning. We looked
    at an example where we predicted customer responses using natural language processing.
    Lastly, we built a chatbot to serve requests from customers 24/7\. These concepts
    are very powerful. NLP is capable of enabling programs to interpret languages
    that humans speak naturally. The graph database, on the other hand, is helpful
    in designing highly efficient algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about practical considerations to bear in
    mind when you want to build a model to solve your day-to-day challenges. In addition,
    we also want to look at the practical IT considerations when equipping data scientists
    with languages to interact with IT developers who put the algorithm to use in
    real life.
  prefs: []
  type: TYPE_NORMAL
