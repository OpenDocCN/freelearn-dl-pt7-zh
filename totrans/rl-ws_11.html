<html><head></head><body>
		<div>
			<div id="_idContainer731" class="Content">
			</div>
		</div>
		<div id="_idContainer732" class="Content">
			<h1 id="_idParaDest-296"><a id="_idTextAnchor340"/>11. Policy-Based Methods for Reinforcement Learning</h1>
		</div>
		<div id="_idContainer774" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will implement different policy-based methods of <strong class="bold">Reinforcement Learning</strong> (<strong class="bold">RL</strong>), such as policy gradients, <strong class="bold">Deep Deterministic Policy Gradients</strong> (<strong class="bold">DDPGs</strong>), <strong class="bold">Trust Region Policy Optimization</strong> (<strong class="bold">TRPO</strong>), and <strong class="bold">Proximal Policy</strong> <strong class="bold">Optimization</strong> (<strong class="bold">PPO</strong>). You will be introduced to the math behind some of the algorithms and you'll also learn how to code policies for RL agents within the OpenAI Gym environment. By the end of this chapter, you will not only have a base-level understanding of policy-based RL methods but you'll also be able to create complete working prototypes using the previously mentioned policy-based RL methods.</p>
			<h1 id="_idParaDest-297"><a id="_idTextAnchor341"/>Introduction</h1>
			<p>The focus of this chapter is policy-based methods for RL. However, before diving into a formal introduction to policy-based methods for RL, let's spend some time understanding the motivation behind them. Let's go back a few hundred years when the globe was still mostly undiscovered and maps were incomplete. Brave sailors at that time sailed the great oceans with only indomitable courage and unyielding curiosity on their side. But they weren't completely blind in the vastness of the oceans. They looked up to the night sky for direction. The stars and planets in the night sky guided them to their destination. The night sky is viewed differently at different times of the year from different parts of the globe. This information, along with highly accurate maps of the night sky, guided these brave explorers to their destinations and sometimes to unknown, uncharted lands.</p>
			<p>Now, you might question what this story has to do with RL at all. A map of the night sky wasn't always available to those sailors. They were created by globetrotters, sailors, skywatchers, and astronomers over centuries. Sailors actually voyaged blindly at one time. They looked at the stars during night time, and every time they took a turn, they marked their position relative to the position of the stars in the night sky. Upon reaching their destination, they evaluated each turn they took and worked out which was more effective during their voyage. Every other ship that sailed to the same destination could do the same. With time, they had a good assessment of the turns that are the most effective for reaching a certain destination with respect to a ship's position in the sea, as assessed by looking at the position of the stars in the night sky. You can think of this as computing the value function where you know the immediate best move. But once sailors had a complete map of the night sky, they could simply derive a policy that would lead them to their destination.</p>
			<p>You can consider the sea and the night sky as the environment and the sailors as agents within it. Over the span of a few centuries, our agents (sailors) built a model of their environment and so were able to come up with a value function (calculating a ship's relative position) that would lead them to the immediate best possible step (immediate navigational step) and also helped them build the optimal policy (a complete navigation route).</p>
			<p>In the last chapter, you learned about <strong class="bold">Deep Recurrent Q Networks</strong> (<strong class="bold">DRQNs</strong>) and their advantage over simple deep Q networks. You also modeled a DRQN network for playing the very popular Atari video game <em class="italic">Breakout</em>. In this chapter, you'll learn about policy-based approaches to RL.</p>
			<p>We will also learn about the policy gradient, which will help you learn about the model in real time. We will then learn about a policy gradient technique called DDPG to understand the continuous action space. Here, we will also learn how to code the Lunar Lander simulation to understand DDPGs using classes such as the <strong class="source-inline">OUActionNoise</strong> class, the <strong class="source-inline">ReplayBuffer</strong> class, the <strong class="source-inline">ActorNetwork</strong> class, and the <strong class="source-inline">CriticNetwork</strong> class. We will learn about these classes in detail later in this chapter. Finally, we will learn how we can improve the policy gradient technique by using the TRPO, PPO, and <strong class="bold">Advantage Actor-Critic</strong> (<strong class="bold">A2C</strong>) techniques. These techniques will help us reduce the operating cost of training the model and so will improve the policy-gradient technique.</p>
			<p>Let's begin by learning about some basic concepts, such as value-based RL, model-based RL, actor-critic, and action space, in the following sub-sections.</p>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor342"/>Introduction to Value-Based and Model-Based RL</h2>
			<p>While it is useful to have a good model of the environment to be able to predict whether a particular move is better with regard to other possible moves, you still need to assess all the possible moves from every possible state in order to come up with an optimal policy. This is a non-trivial problem and is also computationally expensive if, say, our environment is a simulation and our agent is <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>). This approach of model-based learning, when applied within a simulation, can look like the following scenario.</p>
			<p>Consider the game of <em class="italic">Pong</em> (<em class="italic">Figure 11.1</em>). (<em class="italic">Pong</em>—released in 1972—was one of the first arcade video games manufactured by Atari.) Now, let's see how the model-based learning approach could be beneficial for an optimal policy playing <em class="italic">Pong</em> and what could be its drawbacks. So, suppose our agent has learned how to play <em class="italic">Pong</em> by looking at the game environment—that is, by looking at the black and white pixels of each frame. We can then ask our agent to predict the next possible state given a certain frame of black and white pixels from the game environment. But if there is any background noise in the environment (for example, a random, unrelated video playing in the background), our agent would also take that into consideration. </p>
			<p>Now, in most cases, those background noises would not help us in our planning—that is, determining an optimal policy—but would still eat up our computational resources. Following is a screenshot of <em class="italic">Pong</em> game:</p>
			<div>
				<div id="_idContainer733" class="IMG---Figure">
					<img src="image/B16182_11_01.jpg" alt="Figure 11.1: The Atari Pong game&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1: The Atari Pong game</p>
			<p>A value-based approach is better than the model-based approach because while performing the transition from one state to another, a value-based approach would only care about the value of the action in terms of the cumulative reward that we are predicting for each action. It would deem any background noise as mostly irrelevant. A value-based approach is well-suited for deriving an optimal policy. Imagine you have learned an action-value function—a Q function. Then, you can simply look at the highest values in each state and that gives you the optimal policy. However, value-based functions could still be inefficient. Let me try to explain why with an example. In order to travel from Europe to North America, or from South Africa to the southern coasts of India, the optimal policy for our explorer ship might just be to go straight. However, the ship might encounter icebergs, small islands, or ocean currents that might set it off course temporarily. It might still be the optimal policy for the ship to head straight, but the value function might change arbitrarily. So, a value-based method, in this case, would try to approximate all the arbitrary values, while a policy can be blind and, therefore, be more efficient in terms of computational cost. So, in many cases, it might be less efficient to compute an optimal policy based on the value function.</p>
			<h2 id="_idParaDest-299"><a id="_idTextAnchor343"/>Introduction to Actor-Critic Model</h2>
			<p>So, we have briefly explained the trade-offs between the value-based and model-based approaches. Now, can we somehow take the best of both the worlds and create a hybrid of them? The actor-critic model will help us to do that. If we draw a Venn diagram (<em class="italic">Figure 11.2</em>), we will see that the actor-critic model lies at the intersection of the value-based and policy-based RL approaches. They can basically learn a value function as well as a policy. We will discuss actor-critic model further in the following sections.</p>
			<div>
				<div id="_idContainer734" class="IMG---Figure">
					<img src="image/B16182_11_02.jpg" alt="Figure 11.2: The relation between different RL approaches&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2: The relation between different RL approaches</p>
			<p>In practice, most of the time, we try to learn a policy based on the values yielded by the value function, but we actually learn the policy and the values simultaneously. To end this introduction to actor-critic, let me share a quote by Bertrand Russell. Russell, in his book <em class="italic">The Problems of Philosophy</em>, said: "<em class="italic">We can know the general proposition without inferring it from instances, although some instances are usually necessary to make clear to us what the general proposition means.</em>" Treat that as food for thought. The code on how to implement the actor-critic model is shown later in this chapter. Next, we will learn about action spaces, the basics of which we already covered in <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Reinforcement Learning</em>.</p>
			<p>In the previous chapters, we already covered the basic definition and the types of action spaces. Here, we will quickly revise the concept of action spaces. Action spaces define the properties of the game environment. Let's look at the following diagram to understand the types:</p>
			<div>
				<div id="_idContainer735" class="IMG---Figure">
					<img src="image/B16182_11_03.jpg" alt="Figure 11.3: Action spaces&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3: Action spaces</p>
			<p>There are two types of action spaces—discrete and continuous. Discrete action spaces allow discrete inputs—for example, the buttons on a gamepad. These discrete actions can move in either the left or right direction, going either up or down, moving forward or backward, and so on.</p>
			<p>On the other hand, continuous action spaces allow continuous inputs—for example, inputs from a steering wheel or a joystick. In the following section, we will learn how to apply a policy gradient to a continuous action space.</p>
			<h1 id="_idParaDest-300"><a id="_idTextAnchor344"/>Policy Gradients</h1>
			<p>Now that we have established the motivation behind favoring policy-based methods over value-based ones with the navigation example in the previous section, let's begin our formal introduction to policy gradients. Unlike Q-learning, which uses a storage buffer to store past experiences, policy-gradient methods learn in real time (that is, they learn from the most recent experience or action). A policy gradient's learning is driven by whatever the agent encounters in the environment. After each gradient update, the experience is discarded and the policy moves on. Let's look at a pictorial representation of what we have just learned:</p>
			<div>
				<div id="_idContainer736" class="IMG---Figure">
					<img src="image/B16182_11_04.jpg" alt="Figure 11.4: The policy gradient method explained pictorially&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4: The policy gradient method explained pictorially</p>
			<p>One thing that should immediately catch our attention is that the policy gradient method is, in general, less sample-efficient than Q-learning because the experiences are discarded after each gradient update. The mathematical representation of the gradient estimator is given as follows:</p>
			<div>
				<div id="_idContainer737" class="IMG---Figure">
					<img src="image/B16182_11_05.jpg" alt="Figure 11.5: Mathematical representation of policy gradient estimator&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5: Mathematical representation of policy gradient estimator</p>
			<p>In this equation, <img src="image/B16182_11_05a.png" alt="a"/> is the stochastic policy and <img src="image/B16182_11_05b.png" alt="b"/> is our advantage estimation function at time <img src="image/B16182_11_05c.png" alt="c"/> —the estimate of the relative value of the selected action. The expectation,<img src="image/B16182_11_05d.png" alt="d"/> , indicates the average over a finite batch of samples in our algorithm, where we perform sampling and optimization, alternatively. Here, <img src="image/B16182_11_05e.png" alt="e"/> is the gradient estimator. The <img src="image/B16182_11_05f.png" alt="f"/> and <img src="image/B16182_11_05g.png" alt="g"/> variables define the action and state at the time interval, <img src="image/B16182_11_05h.png" alt="h"/>.</p>
			<p>Finally, the policy gradient loss is defined as follows:</p>
			<div>
				<div id="_idContainer746" class="IMG---Figure">
					<img src="image/B16182_11_06.jpg" alt="Figure 11.6: Policy gradient loss defined&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6: Policy gradient loss defined</p>
			<p>In order to calculate the advantage function, <img src="image/B16182_11_06a.png" alt="a"/> , we need the <strong class="bold">discounted reward</strong> and the <strong class="bold">baseline estimate</strong>. The discounted reward is also known as the <strong class="bold">return</strong>, which is the weighted sum of all the rewards our agent got during the current episode. It is called the discounted reward as there is a discount factor associated with it that prioritizes the immediate rewards over the long-term ones. <img src="image/B16182_11_06b.png" alt="b"/> is basically the difference between the discounted reward and the baseline estimate.</p>
			<p>Note that if you still have any problems with wrapping your head around the concept, then it's not a big problem. Just try to grasp the overall idea and you'll be able to grasp the full concept of it eventually. Having said that, let me also introduce you to a stripped-down version of the vanilla policy gradient algorithm.</p>
			<p>We start by initializing the policy parameter, <img src="image/B16182_11_06c.png" alt="c"/>, and the baseline, <img src="image/B16182_11_06d.png" alt="d"/>:</p>
			<p class="source-code">for iteration=1, 2, 3, … do</p>
			<p class="source-code">    Execute the current policy and collect a set of trajectories</p>
			<p class="source-code">    At each timestep in each trajectory, compute</p>
			<p class="source-code">        the return R<span class="subscript">t</span> and the advantage estimate <img src="image/B16182_11_06f.png" alt="e"/>.</p>
			<p class="source-code">    Refit the baseline minimizing <img src="image/B16182_11_06g.png" alt="f"/>,</p>
			<p class="source-code">        summed over all trajectories and timesteps.</p>
			<p class="source-code">    Update the policy using the policy gradient estimate <img src="image/B16182_11_06h.png" alt="g"/></p>
			<p class="source-code">end for</p>
			<p>One suggestion would be to go through the algorithm multiple times, along with the initial explanation, in order to properly understand the concept of policy gradients. But again, an overall understanding of things should be your first priority.</p>
			<p>Before implementing the practical elements, please install OpenAI Gym and the Box2D environment (which includes environments such as Lunar Lander) using PyPI. To carry out the installation, type the following commands into Terminal/Command Prompt:</p>
			<p class="source-code">pip install torch==0.4.1</p>
			<p class="source-code">pip install pillow</p>
			<p class="source-code">pip install gym "gym[box2d]"</p>
			<p>Now, let's implement an exercise using the policy gradient method.</p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor345"/>Exercise 11.01: Landing a Spacecraft on the Lunar Surface Using Policy Gradients and the Actor-Critic Method</h2>
			<p>In this exercise, we will work on a toy problem (OpenAI Lunar Lander) and help land the Lunar Lander inside the OpenAI Gym Lunar Lander environment using vanilla policy gradients and actor-critic. The following are the steps to implement this exercise:</p>
			<ol>
				<li>Open a new Jupyter Notebook, import all the necessary libraries (<strong class="source-inline">gym</strong>, <strong class="source-inline">torch</strong>, and <strong class="source-inline">numpy</strong>):<p class="source-code">import gym</p><p class="source-code">import torch as T</p><p class="source-code">import numpy as np</p></li>
				<li>Define the <strong class="source-inline">ActorCritic</strong> class:<p class="source-code">class ActorCritic(T.nn.Module):</p><p class="source-code">    def __init__(self):</p><p class="source-code">        super(ActorCritic, self).__init__()</p><p class="source-code">        self.transform = T.nn.Linear(8, 128)</p><p class="source-code">        self.act_layer = T.nn.Linear(128, 4) # Action layer</p><p class="source-code">        self.val_layer = T.nn.Linear(128, 1) # Value layer</p><p class="source-code">        self.log_probs = []</p><p class="source-code">        self.state_vals = []</p><p class="source-code">        self.rewards = []</p><p>So, during the initialization of the <strong class="source-inline">ActorCritic</strong> class in the preceding code, we are creating our action and value networks. We are also creating blank arrays for storing the log probabilities, state values, and rewards.</p></li>
				<li>Next, create a function to pass our state through the layers and name it <strong class="source-inline">forward</strong>:<p class="source-code">    def forward(self, state):</p><p class="source-code">        state = T.from_numpy(state).float()</p><p class="source-code">        state = T.nn.functional.relu(self.transform(state))</p><p class="source-code">        state_value = self.val_layer(state)</p><p class="source-code">        act_probs = T.nn.functional.softmax\</p><p class="source-code">                    (self.act_layer(state))</p><p class="source-code">        act_dist = T.distributions.Categorical(act_probs)</p><p class="source-code">        action = act_dist.sample()</p><p class="source-code">        self.log_probs.append(act_dist.log_prob(action))</p><p class="source-code">        self.state_vals.append(state_value)</p><p class="source-code">        return action.item()</p><p>Here, we are taking the state and passing it through the value layer after a ReLU transformation to get the state value. Similarly, we are passing the state through the action layer, followed by a softmax function, to get the action probabilities. Then, we are transforming the probabilities to discrete values for the purpose of sampling. Finally, we are adding our log probabilities and state values to their respective arrays and returning an action item.</p></li>
				<li>Create the <strong class="source-inline">computeLoss</strong> function to calculate a discounted reward first. This will help give greater priority to the immediate reward. Then, we will calculate the loss as described in the policy gradient loss equation:<p class="source-code">    def computeLoss(self, gamma=0.99):</p><p class="source-code">        rewards = []</p><p class="source-code">        discounted_reward = 0</p><p class="source-code">        for reward in self.rewards[::-1]:</p><p class="source-code">            discounted_reward = reward + gamma \</p><p class="source-code">                                * discounted_reward</p><p class="source-code">            rewards.insert(0, discounted_reward)</p><p class="source-code">        rewards = T.tensor(rewards)</p><p class="source-code">        rewards = (rewards – rewards.mean()) / (rewards.std())</p><p class="source-code">        loss = 0</p><p class="source-code">        for log_probability, value, reward in zip\</p><p class="source-code">        (self.log_probs, self.state_vals, rewards):</p><p class="source-code">            advantage = reward – value.item()</p><p class="source-code">            act_loss = -log_probability * advantage</p><p class="source-code">            val_loss = T.nn.functional.smooth_l1_loss\</p><p class="source-code">                       (value, reward)</p><p class="source-code">            loss += (act_loss + val_loss)</p><p class="source-code">        return loss</p></li>
				<li>Next, create a <strong class="source-inline">clear</strong> method to clear the arrays that store the log probabilities, state values, and rewards after each episode:<p class="source-code">    def clear(self):</p><p class="source-code">        del self.log_probs[:]</p><p class="source-code">        del self.state_vals[:]</p><p class="source-code">        del self.rewards[:]</p></li>
				<li>Now, let's start with the main code, which will help us to call the classes that we defined previously in the exercise. We start by assigning a random seed:<p class="source-code">np.random.seed(0)</p></li>
				<li>Then, we need to set up our environment and initialize our policy:<p class="source-code">env = gym.make(""LunarLander-v2"")</p><p class="source-code">policy = ActorCritic()</p><p class="source-code">optimizer = T.optim.Adam(policy.parameters(), \</p><p class="source-code">                         lr=0.02, betas=(0.9, 0.999))</p></li>
				<li>Finally, we iterate for at least <strong class="source-inline">10000</strong> iterations for proper convergence. In each iteration, we sample an action and get the state and reward for that action. Then, we update our policy based on that action and clear our observations:<p class="source-code">render = True</p><p class="source-code">np.random.seed(0)</p><p class="source-code">running_reward = 0</p><p class="source-code">for i in np.arange(0, 10000):</p><p class="source-code">    state = env.reset()</p><p class="source-code">    for t in range(10000):</p><p class="source-code">        action = policy(state)</p><p class="source-code">        state, reward, done, _ = env.step(action)</p><p class="source-code">        policy.rewards.append(reward)</p><p class="source-code">        running_reward += reward</p><p class="source-code">        if render and i &gt; 1000:</p><p class="source-code">            env.render()</p><p class="source-code">        if done:</p><p class="source-code">            break</p><p class="source-code">    print("Episode {}\tReward: {}".format(i, running_reward))</p><p class="source-code">    # Updating the policy</p><p class="source-code">    optimizer.zero_grad()</p><p class="source-code">    loss = policy.computeLoss(0.99)</p><p class="source-code">    loss.backward()</p><p class="source-code">    optimizer.step()</p><p class="source-code">    policy.clear()</p><p class="source-code">    if i % 20 == 0:</p><p class="source-code">        running_reward = running_reward / 20</p><p class="source-code">        running_reward = 0</p><p>Now, when you run the code, you'll see the running reward for each episode. The following is the reward for the first 20 episodes out of the total 10,000 episodes:</p><p class="source-code">Episode 0	Reward: -320.65657506841114</p><p class="source-code">Episode 1	Reward: -425.64874914703705</p><p class="source-code">Episode 2	Reward: -671.2867424162646</p><p class="source-code">Episode 3	Reward: -1032.281198268248</p><p class="source-code">Episode 4	Reward: -1224.3354097571892</p><p class="source-code">Episode 5	Reward: -1543.1792365484055</p><p class="source-code">Episode 6	Reward: -1927.4910808775028</p><p class="source-code">Episode 7	Reward: -2023.4599189797761</p><p class="source-code">Episode 8	Reward: -2361.9002491621986</p><p class="source-code">Episode 9	Reward: -2677.470775357419</p><p class="source-code">Episode 10	Reward: -2932.068423127369</p><p class="source-code">Episode 11	Reward: -3204.4024449864355</p><p class="source-code">Episode 12	Reward: -3449.3136628102934</p><p class="source-code">Episode 13	Reward: -3465.3763860613317</p><p class="source-code">Episode 14	Reward: -3617.162199366013</p><p class="source-code">Episode 15	Reward: -3736.83983321837</p><p class="source-code">Episode 16	Reward: -3883.140249551331</p><p class="source-code">Episode 17	Reward: -4100.137703945375</p><p class="source-code">Episode 18	Reward: -4303.308164747067</p><p class="source-code">Episode 19	Reward: -4569.71587308837</p><p class="source-code">Episode 20	Reward: -4716.304224574078</p><p class="callout-heading">Note</p><p class="callout">The output for only the first 20 episodes is shown here for ease of presentation.</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hDibst">https://packt.live/3hDibst</a>.</p><p class="callout">This section does not currently have an online interactive example and will need to be run locally.</p></li>
			</ol>
			<p>This output indicates that our agent, the Lunar Lander, has started taking actions. The negative reward indicates that in the beginning, the agent is not smart enough to take the right actions and so it takes random actions, for which it is rewarded negatively. A negative reward is a penalty. With time, the agent will start getting positive rewards as it starts learning. Soon, you'll see the game window popping up on your screen showing the real-time progress of your Lunar Lander, as in the following screenshot:</p>
			<div>
				<div id="_idContainer754" class="IMG---Figure">
					<img src="image/B16182_11_07.jpg" alt="Figure 11.7: The real-time progress of the Lunar Lander&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7: The real-time progress of the Lunar Lander</p>
			<p>In the next section, we will look into DDPGs, which extend the idea of policy gradients.</p>
			<h1 id="_idParaDest-302"><a id="_idTextAnchor346"/>Deep Deterministic Policy Gradients</h1>
			<p>In this section, we will apply the DDPG technique to understand the continuous action space. Moreover, we will learn how to code a moon lander simulation to understand DDPGs.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We suggest that you type all the code given in this section into your Jupyter notebook as we will be using it later, in <em class="italic">Exercise 11.02</em>, <em class="italic">Creating a Learning Agent</em>.</p>
			<p>We are going to use the OpenAI Gym Lunar Lander environment for continuous action spaces here. Let's start by importing the essentials:</p>
			<p class="source-code">import os</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import torch as T</p>
			<p class="source-code">import numpy as np</p>
			<p>Now, we will learn how to define some classes, such as the <strong class="source-inline">OUActionNoise</strong> class, the <strong class="source-inline">ReplayBuffer</strong> class, the <strong class="source-inline">ActorNetwork</strong> class, and the <strong class="source-inline">CriticNetwork</strong> class, which will help us to implement the DDGP technique. At the end of this section, you'll have the complete code base that applies the DDPG within our OpenAI Gym game environment.</p>
			<h2 id="_idParaDest-303"><a id="_idTextAnchor347"/>Ornstein-Uhlenbeck Noise</h2>
			<p>First, we will define a class that will provide us with something known as Ornstein-Uhlenbeck noise. This Ornstein–Uhlenbeck process, in physics, is used to model the velocity of a Brownian particle under the influence of friction. Brownian motion, as you may already know, is the random motion of particles when suspended in a fluid (liquid or gas) resulting from their collisions with other particles in the same fluid. Ornstein–Uhlenbeck noise gives you a type of noise that is temporally correlated and is centered on a mean of 0. Since the agent has zero knowledge of the model, it becomes difficult to train it. Here, Ornstein–Uhlenbeck noise can be used as a sample to generate that knowledge. Let's look at the code implementation of this class:</p>
			<p class="source-code">class OUActionNoise(object):</p>
			<p class="source-code">    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):</p>
			<p class="source-code">        self.theta = theta</p>
			<p class="source-code">        self.mu = mu</p>
			<p class="source-code">        self.sigma = sigma</p>
			<p class="source-code">        self.dt = dt</p>
			<p class="source-code">        self.x0 = x0</p>
			<p class="source-code">        self.reset()</p>
			<p class="source-code">    def __call__(self):</p>
			<p class="source-code">        x = self.x_previous</p>
			<p class="source-code">        dx = self.theta * (self.mu –- x) * self.dt + self.sigma \</p>
			<p class="source-code">             * np.sqrt(self.dt) * np.random.normal\</p>
			<p class="source-code">             (size=self.mu.shape)</p>
			<p class="source-code">        self.x_previous = x + dx</p>
			<p class="source-code">        return x</p>
			<p class="source-code">    def reset(self):</p>
			<p class="source-code">        self.x_previous = self.x0 if self.x0 is not None \</p>
			<p class="source-code">                          else np.zeros_like(self.mu)</p>
			<p>In the preceding code, we defined three different functions—that is, <strong class="source-inline">_init_()</strong>, <strong class="source-inline">_call()_</strong>, and <strong class="source-inline">reset()</strong>. In the next section, we will learn how to implement the <strong class="source-inline">ReplayBuffer</strong> class to store the agent's past learnings.</p>
			<h2 id="_idParaDest-304"><a id="_idTextAnchor348"/>The ReplayBuffer Class</h2>
			<p>Replay buffer is a concept we have borrowed from Q-learning. This buffer is basically a space to store all of our agent's past learnings, which will help us to train the model better. We will initialize the class by defining the memory size for our state, action, and rewards, respectively. So, the initialization would look something like this:</p>
			<p class="source-code">class ReplayBuffer(object):</p>
			<p class="source-code">    def __init__(self, max_size, inp_shape, nb_actions):</p>
			<p class="source-code">        self.memory_size = max_size</p>
			<p class="source-code">        self.memory_counter = 0</p>
			<p class="source-code">        self.memory_state = np.zeros\</p>
			<p class="source-code">                            ((self.memory_size, *inp_shape))</p>
			<p class="source-code">        self.new_memory_state = np.zeros\</p>
			<p class="source-code">                                ((self.memory_size, *inp_shape))</p>
			<p class="source-code">        self.memory_action = np.zeros\</p>
			<p class="source-code">                             ((self.memory_size, nb_actions))</p>
			<p class="source-code">        self.memory_reward = np.zeros(self.memory_size)</p>
			<p class="source-code">    self.memory_terminal = np.zeros(self.memory_size, \</p>
			<p class="source-code">                                    dtype=np.float32)</p>
			<p>Next, we need to define the <strong class="source-inline">store_transition</strong> method. This method takes the state, action, reward, and new state as arguments and stores the transitions from one state to another. There's also a <strong class="source-inline">done</strong> flag to indicate the terminal state of our agent. Note that the index here is just a counter that we initialized previously, and it starts from <strong class="source-inline">0</strong> when its value is equal to the maximum memory size:</p>
			<p class="source-code">    def store_transition(self, state, action, \</p>
			<p class="source-code">                         reward, state_new, done):</p>
			<p class="source-code">        index = self.memory_counter % self.memory_size</p>
			<p class="source-code">        self.memory_state[index] = state</p>
			<p class="source-code">        self.new_memory_state[index] = state_new</p>
			<p class="source-code">        self.memory_action[index] = action</p>
			<p class="source-code">        self.memory_reward[index] = reward</p>
			<p class="source-code">        self.memory_terminal[index] = 1  - done</p>
			<p class="source-code">        self.memory_counter += 1</p>
			<p>Finally, we need the <strong class="source-inline">sample_buffer</strong> method, which will be used to randomly sample the buffer:</p>
			<p class="source-code">    def sample_buffer(self, bs):</p>
			<p class="source-code">        max_memory = min(self.memory_counter, self.memory_size)</p>
			<p class="source-code">        batch = np.random.choice(max_memory, bs)</p>
			<p class="source-code">        states = self.memory_state[batch]</p>
			<p class="source-code">        actions = self.memory_action[batch]</p>
			<p class="source-code">        rewards = self.memory_reward[batch]</p>
			<p class="source-code">        states_ = self.new_memory_state[batch]</p>
			<p class="source-code">        terminal = self.memory_terminal[batch]</p>
			<p class="source-code">        return states, actions, rewards, states_, terminal</p>
			<p>So, the entire class, at a glance, looks like this:</p>
			<p class="source-code-heading">DDPG_Example.ipynb</p>
			<p class="source-code">class ReplayBuffer(object):</p>
			<p class="source-code">    def __init__(self, max_size, inp_shape, nb_actions):</p>
			<p class="source-code">        self.memory_size = max_size</p>
			<p class="source-code">        self.memory_counter = 0</p>
			<p class="source-code">        self.memory_state = np.zeros((self.memory_size, *inp_shape))</p>
			<p class="source-code">        self.new_memory_state = np.zeros\</p>
			<p class="source-code">                                ((self.memory_size, *inp_shape))</p>
			<p class="source-code">        self.memory_action = np.zeros\</p>
			<p class="source-code">                             ((self.memory_size, nb_actions))</p>
			<p class="source-code">        self.memory_reward = np.zeros(self.memory_size)</p>
			<p class="source-code">        self.memory_terminal = np.zeros(self.memory_size, \</p>
			<p class="source-code">                                        dtype=np.float32)</p>
			<p class="source-code-link">The complete code for this example can be found at <a href="https://packt.live/2YNL2BO">https://packt.live/2YNL2BO</a>.</p>
			<p>In this section, we learned how to store the agent's past learnings to train the model better. Next, we will learn in more detail about the actor-critic model, which we briefly explained in this chapter's introduction.</p>
			<h2 id="_idParaDest-305"><a id="_idTextAnchor349"/>The Actor-Critic Model</h2>
			<p>Next, in the DDPG technique, we will define the actor and critic networks. Now, we have already introduced actor-critic, but we haven't talked much about it. Take the actor as the current policy and the critic as the value. You may conceptualize the actor-critic model as a guided policy. We will define our actor-critic model using fully connected neural networks.</p>
			<p>The <strong class="source-inline">CriticNetwork</strong> class starts with an initialization. First, we will explain the parameters. The <strong class="bold">beta</strong> is our learning rate. Then, we have our input dimensions, followed by the dimensions for the two fully connected layers we will be using. Finally, we have the number of actions. We haven't written any mechanisms for saving the models yet. After initializing the input dimension, the dimensions for the fully connected layers, and the number of actions, we will initialize our first layer. It will just be a <strong class="source-inline">Linear</strong> layer and we will initialize it using our input and output dimensions. Next, is the initialization of the weights and biases of our fully connected layer. This initialization restricts the values of the weights and biases to a very narrow band of the parameter space when we sample between the <strong class="source-inline">-f1</strong> to <strong class="source-inline">f1</strong> range, as seen in the following code. This helps our network to better converge. Our initial layer is followed by a batch normalization, which again helps to better converge our network. We will repeat the same process with our second fully connected layer. The <strong class="source-inline">CriticNetwork</strong> class will also get an action value. Finally, the output is a single scalar value, which we will initialize next with a constant initialization. </p>
			<p>We will optimize our <strong class="source-inline">CriticNetwork</strong> class using the <strong class="source-inline">Adam</strong> optimizer with a learning rate beta:</p>
			<p class="source-code">class CriticNetwork(T.nn.Module):</p>
			<p class="source-code">    def __init__(self, beta, inp_dimensions,\</p>
			<p class="source-code">                 fc1_dimensions, fc2_dimensions,\</p>
			<p class="source-code">                 nb_actions):</p>
			<p class="source-code">        super(CriticNetwork, self).__init__()</p>
			<p class="source-code">        self.inp_dimensions = inp_dimensions</p>
			<p class="source-code">        self.fc1_dimensions = fc1_dimensions</p>
			<p class="source-code">        self.fc2_dimensions = fc2_dimensions</p>
			<p class="source-code">        self.nb_actions = nb_actions</p>
			<p class="source-code">        self.fc1 = T.nn.Linear(*self.inp_dimensions, \</p>
			<p class="source-code">                               self.fc1_dimensions)</p>
			<p class="source-code">        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])</p>
			<p class="source-code">        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)</p>
			<p class="source-code">        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)</p>
			<p class="source-code">        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)</p>
			<p class="source-code">        self.fc2 = T.nn.Linear(self.fc1_dimensions, \</p>
			<p class="source-code">                               self.fc2_dimensions)</p>
			<p class="source-code">        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])</p>
			<p class="source-code">        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)</p>
			<p class="source-code">        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)</p>
			<p class="source-code">        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)</p>
			<p class="source-code">        self.action_value = T.nn.Linear(self.nb_actions, \</p>
			<p class="source-code">                                        self.fc2_dimensions)</p>
			<p class="source-code">        f3 = 0.003</p>
			<p class="source-code">        self.q = T.nn.Linear(self.fc2_dimensions, 1)</p>
			<p class="source-code">        T.nn.init.uniform_(self.q.weight.data, -f3, f3)</p>
			<p class="source-code">        T.nn.init.uniform_(self.q.bias.data, -f3, f3)</p>
			<p class="source-code">        self.optimizer = T.optim.Adam(self.parameters(), lr=beta)</p>
			<p class="source-code">        self.device = T.device(""gpu"" if T.cuda.is_available() \</p>
			<p class="source-code">                               else ""cpu"")</p>
			<p class="source-code">        self.to(self.device)</p>
			<p>Now, we have to write the <strong class="source-inline">forward</strong> function for our network. This takes a state and an action as input. We get the state-action value from this method. So, our state goes through the first fully connected layer, followed by the batch normalization and the ReLU activation. The activation is passed through the second fully connected layer, followed by another batch normalization, and before the final activation, we take into account the action value. Notice that we are adding the state and action values together to form the state-action value. The state-action value is then passed through the final layer and there we have our output:</p>
			<p class="source-code">def forward(self, state, action):</p>
			<p class="source-code">    state_value = self.fc1(state)</p>
			<p class="source-code">    state_value = self.bn1(state_value)</p>
			<p class="source-code">    state_value = T.nn.functional.relu(state_value)</p>
			<p class="source-code">    state_value = self.fc2(state_value)</p>
			<p class="source-code">    state_value = self.bn2(state_value)</p>
			<p class="source-code">    action_value = T.nn.functional.relu(self.action_value(action))</p>
			<p class="source-code">    state_action_value = T.nn.functional.relu\</p>
			<p class="source-code">                         (T.add(state_value, action_value))</p>
			<p class="source-code">    state_action_value = self.q(state_action_value)</p>
			<p class="source-code">    return state_action_value</p>
			<p>So, finally, the <strong class="source-inline">CriticNetwork</strong> class would look like this:</p>
			<p class="source-code-heading">DDPG_Example.ipynb</p>
			<p class="source-code">class CriticNetwork(T.nn.Module):</p>
			<p class="source-code">    def __init__(self, beta, inp_dimensions,\</p>
			<p class="source-code">                 fc1_dimensions, fc2_dimensions,\</p>
			<p class="source-code">                 nb_actions):</p>
			<p class="source-code">        super(CriticNetwork, self).__init__()</p>
			<p class="source-code">        self.inp_dimensions = inp_dimensions</p>
			<p class="source-code">        self.fc1_dimensions = fc1_dimensions</p>
			<p class="source-code">        self.fc2_dimensions = fc2_dimensions</p>
			<p class="source-code">        self.nb_actions = nb_actions</p>
			<p class="source-code-link">The complete code for this example can be found at <a href="https://packt.live/2YNL2BO">https://packt.live/2YNL2BO</a>.</p>
			<p>Next, we will define <strong class="source-inline">ActorNetwork</strong>. This would be mostly similar to the <strong class="source-inline">CriticNetwork</strong> class but with some minor yet important changes. Let's code it first and then we will explain it:</p>
			<p class="source-code">class ActorNetwork(T.nn.Module):</p>
			<p class="source-code">    def __init__(self, alpha, inp_dimensions,\</p>
			<p class="source-code">                 fc1_dimensions, fc2_dimensions, nb_actions):</p>
			<p class="source-code">        super(ActorNetwork, self).__init__()</p>
			<p class="source-code">        self.inp_dimensions = inp_dimensions</p>
			<p class="source-code">        self.fc1_dimensions = fc1_dimensions</p>
			<p class="source-code">        self.fc2_dimensions = fc2_dimensions</p>
			<p class="source-code">        self.nb_actions = nb_actions</p>
			<p class="source-code">        self.fc1 = T.nn.Linear(*self.inp_dimensions, \</p>
			<p class="source-code">                               self.fc1_dimensions)</p>
			<p class="source-code">        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])</p>
			<p class="source-code">        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)</p>
			<p class="source-code">        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)</p>
			<p class="source-code">        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)</p>
			<p class="source-code">        self.fc2 = T.nn.Linear(self.fc1_dimensions, \</p>
			<p class="source-code">                               self.fc2_dimensions)</p>
			<p class="source-code">        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])</p>
			<p class="source-code">        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)</p>
			<p class="source-code">        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)</p>
			<p class="source-code">        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)</p>
			<p class="source-code">        f3 = 0.003</p>
			<p class="source-code">        self.mu = T.nn.Linear(self.fc2_dimensions, \</p>
			<p class="source-code">                              self.nb_actions)</p>
			<p class="source-code">        T.nn.init.uniform_(self.mu.weight.data, -f3, f3)</p>
			<p class="source-code">        T.nn.init.uniform_(self.mu.bias.data, -f3, f3)</p>
			<p class="source-code">        self.optimizer = T.optim.Adam(self.parameters(), lr=alpha)</p>
			<p class="source-code">        self.device = T.device("gpu" if T.cuda.is_available() \</p>
			<p class="source-code">                               else "cpu")</p>
			<p class="source-code">        self.to(self.device)</p>
			<p class="source-code">    def forward(self, state):</p>
			<p class="source-code">        x = self.fc1(state)</p>
			<p class="source-code">        x = self.bn1(x)</p>
			<p class="source-code">        x = T.nn.functional.relu(x)</p>
			<p class="source-code">        x = self.fc2(x)</p>
			<p class="source-code">        x = self.bn2(x)</p>
			<p class="source-code">        x = T.nn.functional.relu(x)</p>
			<p class="source-code">        x = T.tanh(self.mu(x))</p>
			<p class="source-code">        return x</p>
			<p>As you can see, this is similar to our <strong class="source-inline">CriticNetwork</strong> class. The main difference here is that we don't have an action value here and that we have written the <strong class="source-inline">forward</strong> function in a slightly different way. Notice that the final output from the <strong class="source-inline">forward</strong> function is a <strong class="source-inline">tanh</strong> function, which will bind our output between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. This is necessary for the environment we are going to play around with. Let's implement an exercise that will help us to create a learning agent.</p>
			<h2 id="_idParaDest-306"><a id="_idTextAnchor350"/>Exercise 11.02: Creating a Learning Agent</h2>
			<p>In this exercise, we will write our <strong class="source-inline">Agent</strong> class. We are already familiar with the concept of a learning agent, so let's see how we can implement one. This exercise will conclude the DDPG example that we have been building. Please make sure that you have run all the example code in this section before starting the exercise.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We have assumed you have typed the code presented in the preceding section into a new notebook. Specifically, we have assumed you already have the code for importing the necessary libraries and creating the <strong class="source-inline">OUActionNoise</strong>, <strong class="source-inline">ReplayBuffer</strong>, <strong class="source-inline">CriticNetwork</strong>, and <strong class="source-inline">ActorNetwork</strong> classes in your notebook. This exercise begins by creating the <strong class="source-inline">Agent</strong> class.</p>
			<p class="callout">For convenience, the complete code for this exercise, including the code in the example, can be found at <a href="https://packt.live/37Jwhnq">https://packt.live/37Jwhnq</a>.</p>
			<p>The following are the steps to implement this exercise:</p>
			<ol>
				<li value="1">Let's start by using the <strong class="source-inline">__init__</strong> method and passing the alpha and the beta, which are the learning rates for our actor and critic networks, respectively. Then, pass the input dimensions and a parameter called <strong class="source-inline">tau</strong>, which we will explain in a bit. Then, we want to pass the environment, which is our continuous action space, gamma, which is the agent's discount factor, which we talked about earlier. Then, the number of actions, the maximum size of the memory, the size of the two layers, and the batch size are passed. Then, initialize our actor and critic. Finally, we will introduce our noise and the <strong class="source-inline">update_params</strong> function:<p class="source-code">class Agent(object):</p><p class="source-code">    def __init__(self, alpha, beta, inp_dimensions, \</p><p class="source-code">                 tau, env, gamma=0.99, nb_actions=2, \</p><p class="source-code">                 max_size=1000000, l1_size=400, \</p><p class="source-code">                 l2_size=300, bs=64):</p><p class="source-code">        self.gamma = gamma</p><p class="source-code">        self.tau = tau</p><p class="source-code">        self.memory = ReplayBuffer(max_size, inp_dimensions, \</p><p class="source-code">                                   nb_actions)</p><p class="source-code">        self.bs = bs</p><p class="source-code">        self.actor = ActorNetwork(alpha, inp_dimensions, \</p><p class="source-code">                                  l1_size, l2_size, \</p><p class="source-code">                                  nb_actions=nb_actions)</p><p class="source-code">        self.critic = CriticNetwork(beta, inp_dimensions, \</p><p class="source-code">                                    l1_size, l2_size, \</p><p class="source-code">                                    nb_actions=nb_actions)</p><p class="source-code">        self.target_actor = ActorNetwork(alpha, inp_dimensions, \</p><p class="source-code">                                         l1_size, l2_size, \</p><p class="source-code">                                         nb_actions=nb_actions)</p><p class="source-code">        self.target_critic = CriticNetwork(beta, inp_dimensions, \</p><p class="source-code">                                           l1_size, l2_size, \</p><p class="source-code">                                           nb_actions=nb_actions)</p><p class="source-code">        self.noise = OUActionNoise(mu=np.zeros(nb_actions))</p><p class="source-code">        self.update_params(tau=1)</p><p>The <strong class="source-inline">update_params</strong> function updates our parameters, but there's a catch. We basically have a moving target. This means we are using the same network to calculate the action and the value of the action simultaneously as we are updating the estimate in every episode. Because we are using the same parameters for both, it may lead to divergence. To tackle that, we use the target network, which learns the value and the action combinations, and the other network is used to learn the policy. We will periodically update the target network's parameters with the parameters of the evaluation network.</p></li>
				<li>Next, we have the <strong class="source-inline">select_action</strong> method. Here, we take the observation from our actor and pass it through the feed-forward network. <strong class="source-inline">mu_prime</strong> here is basically the noise we add to the network. It is also called exploration noise. Finally, we call <strong class="source-inline">actor.train()</strong> and return the <strong class="source-inline">numpy</strong> value for <strong class="source-inline">mu_prime</strong>:<p class="source-code">def select_action(self, observation):</p><p class="source-code">    self.actor.eval()</p><p class="source-code">    observation = T.tensor(observation, dtype=T.float)\</p><p class="source-code">                  .to(self.actor.device)</p><p class="source-code">    mu = self.actor.forward(observation).to(self.actor.device)</p><p class="source-code">    mu_prime = mu + T.tensor(self.noise(),\</p><p class="source-code">                             dtype=T.float).to(self.actor.device)</p><p class="source-code">    self.actor.train()</p><p class="source-code">    return mu_prime.cpu().detach().numpy()</p></li>
				<li>Next comes our <strong class="source-inline">remember</strong> function, which is self-explanatory. This takes the <strong class="source-inline">state</strong>, <strong class="source-inline">action</strong>, <strong class="source-inline">reward</strong>, <strong class="source-inline">new_state</strong>, and <strong class="source-inline">done</strong> flags in order to store them in memory:<p class="source-code">def remember(self, state, action, reward, new_state, done):</p><p class="source-code">    self.memory.store_transition(state, action, reward, \</p><p class="source-code">                                 new_state, done)</p></li>
				<li>Next, we will define the <strong class="source-inline">learn</strong> function:<p class="source-code">def learn(self):</p><p class="source-code">    if self.memory.memory_counter &lt; self.bs:</p><p class="source-code">        return</p><p class="source-code">    state, action, reward, new_state, done = self.memory\</p><p class="source-code">                                             .sample_buffer\</p><p class="source-code">                                             (self.bs)</p><p class="source-code">    reward = T.tensor(reward, dtype=T.float)\</p><p class="source-code">             .to(self.critic.device)</p><p class="source-code">    done = T.tensor(done).to(self.critic.device)</p><p class="source-code">    new_state = T.tensor(new_state, dtype=T.float)\</p><p class="source-code">                .to(self.critic.device)</p><p class="source-code">    action = T.tensor(action, dtype=T.float).to(self.critic.device)</p><p class="source-code">    state = T.tensor(state, dtype=T.float).to(self.critic.device)</p><p class="source-code">    self.target_actor.eval()</p><p class="source-code">    self.target_critic.eval()</p><p class="source-code">    self.critic.eval()</p><p class="source-code">    target_actions = self.target_actor.forward(new_state)</p><p class="source-code">    critic_value_new = self.target_critic.forward\</p><p class="source-code">                       (new_state, target_actions)</p><p class="source-code">    critic_value = self.critic.forward(state, action)</p><p class="source-code">    target = []</p><p class="source-code">    for j in range(self.bs):</p><p class="source-code">        target.append(reward[j] + self.gamma\</p><p class="source-code">                      *critic_value_new[j]*done[j])</p><p class="source-code">    target = T.tensor(target).to(self.critic.device)</p><p class="source-code">    target = target.view(self.bs, 1)</p><p class="source-code">    self.critic.train()</p><p class="source-code">    self.critic.optimizer.zero_grad()</p><p class="source-code">    critic_loss = T.nn.functional.mse_loss(target, critic_value)</p><p class="source-code">    critic_loss.backward()</p><p class="source-code">    self.critic.optimizer.step()</p><p class="source-code">    self.critic.eval()</p><p class="source-code">    self.actor.optimizer.zero_grad()</p><p class="source-code">    mu = self.actor.forward(state)</p><p class="source-code">    self.actor.train()</p><p class="source-code">    actor_loss = -self.critic.forward(state, mu)</p><p class="source-code">    actor_loss = T.mean(actor_loss)</p><p class="source-code">    actor_loss.backward()</p><p class="source-code">    self.actor.optimizer.step()</p><p class="source-code">    self.update_params()</p><p>Here, we first check whether we have enough samples in our memory buffer for learning. So, if our memory counter is less than the batch size—meaning we do not have the batch size number of samples in our memory buffer—we simply return the value. Otherwise, we sample from our memory buffer the <strong class="source-inline">state</strong>, <strong class="source-inline">action</strong>, <strong class="source-inline">reward</strong>, <strong class="source-inline">new_state</strong>, and <strong class="source-inline">done</strong> flags. Once sampled, we must convert all of these flags into tensors for implementation. Then, we need to calculate the target actions, followed by the calculation of the new critic value using the target action states and the new state. Next, we calculate the critic value, which is the value we met for the states and actions in the current replay buffer. After that, we calculate the targets. Note that the part where we multiply <strong class="source-inline">gamma</strong> with the new critic value becomes <strong class="source-inline">0</strong> when the <strong class="source-inline">done</strong> flag is <strong class="source-inline">0</strong>. This basically means that when the episode is over, we only take into account the reward from the current state. The target is then converted into a tensor and reshaped for implementation purposes. Now, we can calculate and backpropagate our loss for the critic. Then, we do the same for our actor network. Finally, we update the parameters for our target actor and target critic network.</p></li>
				<li>Next, define the <strong class="source-inline">update_params</strong> function:<p class="source-code">def update_params(self, tau=None):</p><p class="source-code">    if tau is None:</p><p class="source-code">        tau = self.tau # tau is 1</p><p class="source-code">    actor_params = self.actor.named_parameters()</p><p class="source-code">    critic_params = self.critic.named_parameters()</p><p class="source-code">    target_actor_params = self.target_actor.named_parameters()</p><p class="source-code">    target_critic_params = self.target_critic.named_parameters()</p><p class="source-code">    critic_state_dict = dict(critic_params)</p><p class="source-code">    actor_state_dict = dict(actor_params)</p><p class="source-code">    target_critic_dict = dict(target_critic_params)</p><p class="source-code">    target_actor_dict = dict(target_actor_params)</p><p class="source-code">    for name in critic_state_dict:</p><p class="source-code">        critic_state_dict[name] = tau*critic_state_dict[name]\</p><p class="source-code">                                  .clone() + (1-tau)\</p><p class="source-code">                                  *target_critic_dict[name]\</p><p class="source-code">                                  .clone()</p><p class="source-code">    self.target_critic.load_state_dict(critic_state_dict)</p><p class="source-code">    for name in actor_state_dict:</p><p class="source-code">        actor_state_dict[name] = tau*actor_state_dict[name]\</p><p class="source-code">                                 .clone() + (1-tau)\</p><p class="source-code">                                 *target_actor_dict[name]\</p><p class="source-code">                                 .clone()</p><p class="source-code">    self.target_actor.load_state_dict(actor_state_dict)</p><p>Here, the <strong class="source-inline">update_params</strong> function takes a <strong class="source-inline">tau</strong> value, which basically allows us to update the target network in very small steps. The value for <strong class="source-inline">tau</strong> is typically very small, much smaller than <strong class="source-inline">1</strong>. One thing to note is that we start with <strong class="source-inline">tau</strong> equal to <strong class="source-inline">1</strong> but later, the value is reduced to a much smaller number. What the function does is that it first gets all the names of the parameters for the critic, actor, target critic, and target actor. It then updates those parameters with the target critic and target actor. Now, we can create the main part of our Python code.</p></li>
				<li>If you have created the <strong class="source-inline">Agent</strong> class properly, then, along with the preceding example code, you'll be able to initialize our learning agent with the following bit of code:<p class="source-code">env = gym.make("LunarLanderContinuous-v2")</p><p class="source-code">agent = Agent(alpha=0.000025, beta=0.00025, \</p><p class="source-code">              inp_dimensions=[8], tau=0.001, \</p><p class="source-code">              env=env, bs=64, l1_size=400, \</p><p class="source-code">              l2_size=300, nb_actions=2)</p><p class="source-code">for i in np.arange(100):</p><p class="source-code">    observation = env.reset()</p><p class="source-code">    action = agent.select_action(observation)</p><p class="source-code">    state_new, reward, _, _ = env.step(action)</p><p class="source-code">    observation = state_new</p><p class="source-code">    env.render()</p><p class="source-code">    print("Episode {}\tReward: {}".format(i, reward))</p><p>For the output, you'll see the reward for each episode. Here is the output for the first 10 episodes:</p><p class="source-code">Episode 0	Reward: -0.2911892911560017</p><p class="source-code">Episode 1	Reward: -0.4945150137594737</p><p class="source-code">Episode 2	Reward: 0.5150667951556557</p><p class="source-code">Episode 3	Reward: -1.33324749569461</p><p class="source-code">Episode 4	Reward: -0.9969126433110092</p><p class="source-code">Episode 5	Reward: -1.8466220765944854</p><p class="source-code">Episode 6	Reward: -1.6207456680346013</p><p class="source-code">Episode 7	Reward: -0.4027838988393455</p><p class="source-code">Episode 8	Reward: 0.42631743995534066</p><p class="source-code">Episode 9	Reward: -1.1961709218053898</p><p class="source-code">Episode 10	Reward: -1.0679394471159185</p></li>
			</ol>
			<p>What you see in the preceding output is that the reward oscillates between negative and positive. That is because until now, our agent was sampling random actions from all the actions it could take.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/37Jwhnq">https://packt.live/37Jwhnq</a>.</p>
			<p class="callout">This section does not currently have an online interactive example and will need to be run locally.</p>
			<p>In the next activity, we will make the agent remember its past learnings and learn from them. Here's how the game environment will look:</p>
			<div>
				<div id="_idContainer755" class="IMG---Figure">
					<img src="image/B16182_11_08.jpg" alt="Figure 11.8: The output window showing the Lunar Lander hovering &#13;&#10;in the game environment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8: The output window showing the Lunar Lander hovering in the game environment</p>
			<p>However, you'll find that the Lander doesn't attempt to land, and rather, it hovers over the lunar surface in our game environment. That's because we haven't enabled the agent to learn yet. We will do that in the following activity.</p>
			<p>In the next activity, we will create an agent that will help to learn a model using DDPG.</p>
			<h2 id="_idParaDest-307"><a id="_idTextAnchor351"/>Activity 11.01: Creating an Agent That Learns a Model Using DDPG</h2>
			<p>In this activity, we will implement what we have learned in this section and create an agent that learns through DDPG.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We have created a Python file for the actual DDPG implementation to be imported as a module using <strong class="source-inline">from ddpg import *</strong>. The module and the code of the activity can be downloaded from GitHub at <a href="https://packt.live/2YksdXX">https://packt.live/2YksdXX</a>.</p>
			<p>The following are the steps to perform for this activity:</p>
			<ol>
				<li value="1">Import the necessary libraries (<strong class="source-inline">os</strong>, <strong class="source-inline">gym</strong>, and <strong class="source-inline">ddpg</strong>).</li>
				<li>First, we create our Gym environment (<strong class="source-inline">LunarLanderContinuous-v2</strong>), as we did previously.</li>
				<li>Initialize the agent with some sensible hyperparameters, as in <em class="italic">Exercise 11.02</em>, <em class="italic">Creating a Learning Agent</em>.</li>
				<li>Set up a random seed so that our experiments are reproducible.</li>
				<li>Create a blank array to story the scores; you can name it <strong class="source-inline">history</strong>. Iterate for at least <strong class="source-inline">1000</strong> episodes and in each episode, set a running score variable to <strong class="source-inline">0</strong> and the <strong class="source-inline">done</strong> flag to <strong class="source-inline">False</strong>, then reset the environment. Then, when the <strong class="source-inline">done</strong> flag is not <strong class="source-inline">True</strong>, carry out the following step.</li>
				<li>Select an action from the observations and get the new <strong class="source-inline">state</strong>, <strong class="source-inline">reward</strong>, and <strong class="source-inline">done</strong> flags. Save the <strong class="source-inline">observation</strong>, <strong class="source-inline">action</strong>, <strong class="source-inline">reward</strong>, <strong class="source-inline">state_new</strong>, and <strong class="source-inline">done</strong> flags. Call the <strong class="source-inline">learn</strong> function of the agent and add the current reward to the running score. Set the new state as the observation and finally, when the <strong class="source-inline">done</strong> flag is <strong class="source-inline">True</strong>, append <strong class="source-inline">score</strong> to <strong class="source-inline">history</strong>.<p class="callout-heading">Note</p><p class="callout">To observe the rewards, we can simply add a <strong class="source-inline">print</strong> statement. The rewards will be similar to those in the previous exercise.</p><p>The following is the expected simulation output:</p><div id="_idContainer756" class="IMG---Figure"><img src="image/B16182_11_09.jpg" alt="Figure 11.9: Screenshots from the environment after 1,000 rounds of training&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.9: Screenshots from the environment after 1,000 rounds of training</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 766.</p>
			<p>In the next section, we will see how we can improve the policy gradient approach that we just implemented.</p>
			<h1 id="_idParaDest-308"><a id="_idTextAnchor352"/>Improving Policy Gradients</h1>
			<p>In this section, we will learn the various approaches that will help us improve the policy gradient approach that we learned about in the previous section. We will learn about techniques such as TRPO and PPO.</p>
			<p>We will also learn about the A2C technique in brief. Let's understand the TRPO optimization technique in the next section.</p>
			<h2 id="_idParaDest-309"><a id="_idTextAnchor353"/>Trust Region Policy Optimization</h2>
			<p>In most cases, RL is very sensitive to the initialization of weights. Take, for instance, the learning rate. If our learning rate is too high, then it may so happen that our policy update takes our policy network to a region of the parameter space where the next batch of data it collects is gathered against a very poor policy. This might cause our network to never recover again. Now, we will talk about newer methods that try to get rid of this problem. But before we do that, let's have a quick recap of what we have already covered.</p>
			<p>In the <em class="italic">Policy Gradients</em> section, we defined the estimator of the advantage function, <img src="image/B16182_11_9a.png" alt="b"/> , as the difference between the discounted reward and the baseline estimate. Intuitively, the advantage estimator quantifies how good the action taken by our agent in a certain state was compared to what would typically happen in that state. One problem with the advantage function is that if we simply keep on updating our weights based on one batch of samples using gradient descent, then our parameter updates might stray far from the range where the data was sampled from. That could lead to an inaccurate estimate of the advantage function. In short, if we keep running gradient descent on a single batch of experiences, we might corrupt our policy.</p>
			<p>One way to make sure this problem doesn't occur is to ensure that the updated policy doesn't differ too much from the old policy. This is basically the main crux of TRPO.</p>
			<p>We already understand how the gradient estimator works for vanilla policy gradients:</p>
			<div>
				<div id="_idContainer758" class="IMG---Figure">
					<img src="image/B16182_11_10.jpg" alt="Figure 11.10: The vanilla policy gradient method&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.10: The vanilla policy gradient method</p>
			<p>Here's how it looks for TRPO:</p>
			<div>
				<div id="_idContainer759" class="IMG---Figure">
					<img src="image/B16182_11_11.jpg" alt="Figure 11.11: Mathematical representation of TRPO&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.11: Mathematical representation of TRPO</p>
			<p>The only change here is that the log operator in the preceding equation has been replaced by a division by <img src="image/B16182_11_11a.png" alt="b"/>. This is known as the TRPO objective and optimizing it yields the same result as the vanilla policy gradients. In order to ensure that the new and updated policy doesn't differ much from the old policy, TRPO introduces a constraint known as the KL constraint. </p>
			<p>This constraint, in simple words, makes sure that our new policy doesn't stray too far from the old one. Note that the actual TRPO strategy, however, proposes a penalty instead of a constraint.</p>
			<h2 id="_idParaDest-310"><a id="_idTextAnchor354"/>Proximal Policy Optimization</h2>
			<p>It might seem like everything is fine and good for TRPO, but the introduction of the KL constraint introduces an additional operating cost to our policy. To address that problem, and to basically solve the problems with vanilla policy gradients once and for all, the researchers at OpenAI have introduced PPO, which we will look into now.</p>
			<p>The main motivations behind PPO are as follows:</p>
			<ul>
				<li>Ease of implementation</li>
				<li>Ease of parameter tuning</li>
				<li>Efficient sampling</li>
			</ul>
			<p>One thing to note is that the PPO method doesn't use a replay buffer to store past experiences and learns straight from whatever the agent encounters in the environment. This is also known as an <strong class="source-inline">online</strong> method of learning while the former—using a replay buffer to store past experiences—is known as an <strong class="source-inline">offline</strong> method of learning.</p>
			<p>The authors of PPO define a probability ratio, <img src="image/B16182_11_11b.png" alt="a"/>, which is basically the probability ratio between the new and the old policy. So, we have the following:</p>
			<div>
				<div id="_idContainer762" class="IMG---Figure">
					<img src="image/B16182_11_12.jpg" alt="Figure 11.12: The probability ratio between the old and new policy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.12: The probability ratio between the old and new policy</p>
			<p>When provided with a sampled batch of actions and states, this ratio would be greater than <strong class="source-inline">1</strong> if the action is more likely now than in the old policy. Otherwise, it would remain between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. Now, the final objective of PPO when written down looks like this:</p>
			<div>
				<div id="_idContainer763" class="IMG---Figure">
					<img src="image/B16182_11_13.jpg" alt="Figure 11.13: The final objective of PPO&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.13: The final objective of PPO</p>
			<p>Let's explain. Like the vanilla policy gradient, PPO tries to optimize the expectation and so we compute this expectation operator over batches of trajectories. Now, this is a minimum value of the modified policy gradient objective that we saw in TRPO and the second part is a clipped version of it. The clipping operation keeps the policy gradient objective between <img src="image/B16182_11_13a.png" alt="a"/> and <img src="image/B16182_11_13b.png" alt="b"/>. <img src="image/B16182_11_13c.png" alt="c"/> here is a hyperparameter, often equal to <strong class="source-inline">0.2</strong>.</p>
			<p>Although the function looks simple at a glance, its ingenuity is remarkable. <img src="image/B16182_11_13d.png" alt="d"/> can be both negative and positive, suggesting a negative advantage estimation and a positive advantage estimation, respectively. This behavior of the advantage estimator determines how the <strong class="source-inline">min</strong> operator works. Here's an illustration of the <strong class="source-inline">clip</strong> parameter from the actual PPO paper:</p>
			<div>
				<div id="_idContainer768" class="IMG---Figure">
					<img src="image/B16182_11_14.jpg" alt="Figure 11.14: The clip parameter illustration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.14: The clip parameter illustration</p>
			<p>The plot on the left is where the advantage is positive. This means our actions yielded results that were better than expected. On the other plot on the right are the cases where our actions yielded less than the expected return.</p>
			<p>Notice how on the plot on the left the loss flattens out when <strong class="source-inline">r</strong> is too high. This might occur when the current action is much more plausible under the current policy than the old one. In this case, the objective function is clipped here, thereby ensuring the gradient update doesn't go beyond a certain limit.</p>
			<p>On the other hand, when the objective function is negative, the loss flattens when <strong class="source-inline">r</strong> is approaching <strong class="source-inline">0</strong>. This relates to actions that are more unlikely under the current policy than the old one. It may be clear by now how the clipping operation keeps the updates to the network parameters within a desirable range. It would be a better approach to learn about the PPO technique while implementing it. So, let's start with an exercise to reduce the operating cost of our policy using PPO.</p>
			<h2 id="_idParaDest-311"><a id="_idTextAnchor355"/>Exercise 11.03: Improving the Lunar Lander Example Using PPO</h2>
			<p>In this exercise, we'll implement the Lunar Lander example using PPO. We will follow almost the same structure as before and you'll be able to follow through this exercise easily if you have gone through the previous exercises and examples:</p>
			<ol>
				<li value="1"> Open a new Jupyter Notebook and import the necessary libraries (<strong class="source-inline">gym</strong>, <strong class="source-inline">torch</strong>, and <strong class="source-inline">numpy</strong>):<p class="source-code">import gym</p><p class="source-code">import torch as T</p><p class="source-code">import numpy as np</p></li>
				<li>Set our device as we did in the DDPG example:<p class="source-code">device = T.device("cuda:0" if T.cuda.is_available() else "cpu")</p></li>
				<li>Next, we will create the <strong class="source-inline">ReplayBuffer</strong> class. Here, we will create arrays to store the actions, states, log probabilities, reward, and terminal states:<p class="source-code">class ReplayBuffer:</p><p class="source-code">    def __init__(self):</p><p class="source-code">        self.memory_actions = []</p><p class="source-code">        self.memory_states = []</p><p class="source-code">        self.memory_log_probs = []</p><p class="source-code">        self.memory_rewards = []</p><p class="source-code">        self.is_terminals = []</p><p class="source-code">    def clear_memory(self):</p><p class="source-code">        del self.memory_actions[:]</p><p class="source-code">        del self.memory_states[:]</p><p class="source-code">        del self.memory_log_probs[:]</p><p class="source-code">        del self.memory_rewards[:]</p><p class="source-code">        del self.is_terminals[:]</p></li>
				<li>Now, we will define our <strong class="source-inline">ActorCritic</strong> class. We will define our <strong class="source-inline">action</strong> and <strong class="source-inline">value</strong> layers first:<p class="source-code">class ActorCritic(T.nn.Module):</p><p class="source-code">def __init__(self, state_dimension, action_dimension, \</p><p class="source-code">             nb_latent_variables):</p><p class="source-code">    super(ActorCritic, self).__init__()</p><p class="source-code">    self.action_layer = T.nn.Sequential\</p><p class="source-code">                        (T.nn.Linear(state_dimension, \</p><p class="source-code">                                     nb_latent_variables),\</p><p class="source-code">                        T.nn.Tanh(),\</p><p class="source-code">                        T.nn.Linear(nb_latent_variables, \</p><p class="source-code">                                    nb_latent_variables),\</p><p class="source-code">                        T.nn.Tanh(),\</p><p class="source-code">                        T.nn.Linear(nb_latent_variables, \</p><p class="source-code">                                    action_dimension),\</p><p class="source-code">                        T.nn.Softmax(dim=-1))</p><p class="source-code">    self.value_layer = T.nn.Sequential\</p><p class="source-code">                       (T.nn.Linear(state_dimension, \</p><p class="source-code">                                    nb_latent_variables),\</p><p class="source-code">                       T.nn.Tanh(), \</p><p class="source-code">                       T.nn.Linear(nb_latent_variables, \</p><p class="source-code">                                   nb_latent_variables),\</p><p class="source-code">                       T.nn.Tanh(),\</p><p class="source-code">                       T.nn.Linear(nb_latent_variables, 1))</p></li>
				<li>Now, we will define methods to sample from the action space and evaluate the log probabilities of the actions, state value, and entropy of the distribution:<p class="source-code"># Sample from the action space</p><p class="source-code">def act(self, state, memory):</p><p class="source-code">    state = T.from_numpy(state).float().to(device)</p><p class="source-code">    action_probs = self.action_layer(state)</p><p class="source-code">    dist = T.distributions.Categorical(action_probs)</p><p class="source-code">    action = dist.sample()</p><p class="source-code">    memory.memory_states.append(state)</p><p class="source-code">    memory.memory_actions.append(action)</p><p class="source-code">    memory.memory_log_probs.append(dist.log_prob(action))</p><p class="source-code">    return action.item()</p><p class="source-code"># Evaluate log probabilities</p><p class="source-code">def evaluate(self, state, action):</p><p class="source-code">    action_probs = self.action_layer(state)</p><p class="source-code">    dist = T.distributions.Categorical(action_probs)</p><p class="source-code">    action_log_probs = dist.log_prob(action)</p><p class="source-code">    dist_entropy = dist.entropy()</p><p class="source-code">    state_value = self.value_layer(state)</p><p class="source-code">    return action_log_probs, \</p><p class="source-code">           T.squeeze(state_value), dist_entropy</p><p>Finally, the <strong class="source-inline">ActorCritic</strong> class looks like this:</p><p class="source-code-heading">Exercise11_03.ipynb</p><p class="source-code">class ActorCritic(T.nn.Module):</p><p class="source-code">    def __init__(self, state_dimension, \</p><p class="source-code">                 action_dimension, nb_latent_variables):</p><p class="source-code">        super(ActorCritic, self).__init__()</p><p class="source-code">        self.action_layer = T.nn.Sequential(T.nn.Linear\</p><p class="source-code">                                           (state_dimension, \</p><p class="source-code">                                            nb_latent_variables),\</p><p class="source-code">                            T.nn.Tanh(), \</p><p class="source-code">                            T.nn.Linear(nb_latent_variables, \</p><p class="source-code">                                        nb_latent_variables),\</p><p class="source-code">                            T.nn.Tanh(),\</p><p class="source-code">                            T.nn.Linear(nb_latent_variables, \</p><p class="source-code">                                        action_dimension),\</p><p class="source-code">                            T.nn.Softmax(dim=-1))</p><p class="source-code-link">The complete code for this example can be found at <a href="https://packt.live/2zM1Z6Z">https://packt.live/2zM1Z6Z</a>.</p></li>
				<li>Now, we will define our <strong class="source-inline">Agent</strong> class using the <strong class="source-inline">__init__()</strong> and <strong class="source-inline">update()</strong> functions. First let's define <strong class="source-inline">__init__()</strong> function:<p class="source-code">class Agent:</p><p class="source-code">    def __init__(self, state_dimension, action_dimension, \</p><p class="source-code">                 nb_latent_variables, lr, betas, gamma, \</p><p class="source-code">                 K_epochs, eps_clip):</p><p class="source-code">        self.lr = lr</p><p class="source-code">        self.betas = betas</p><p class="source-code">        self.gamma = gamma</p><p class="source-code">        self.eps_clip = eps_clip</p><p class="source-code">        self.K_epochs = K_epochs</p><p class="source-code">        self.policy = ActorCritic(state_dimension,\</p><p class="source-code">                                  action_dimension,\</p><p class="source-code">                                  nb_latent_variables)\</p><p class="source-code">                                  .to(device)</p><p class="source-code">        self.optimizer = T.optim.Adam\</p><p class="source-code">                         (self.policy.parameters(), \</p><p class="source-code">                          lr=lr, betas=betas)</p><p class="source-code">        self.policy_old = ActorCritic(state_dimension,\</p><p class="source-code">                                      action_dimension,\</p><p class="source-code">                                      nb_latent_variables)\</p><p class="source-code">                                      .to(device)</p><p class="source-code">        self.policy_old.load_state_dict(self.policy.state_dict())</p><p class="source-code">        self.MseLoss = T.nn.MSELoss()</p></li>
				<li>Now let's define the <strong class="source-inline">update</strong> function:<p class="source-code">    def update(self, memory):</p><p class="source-code">        # Monte Carlo estimate</p><p class="source-code">        rewards = []</p><p class="source-code">        discounted_reward = 0</p><p class="source-code">        for reward, is_terminal in \</p><p class="source-code">            zip(reversed(memory.memory_rewards), \</p><p class="source-code">                         reversed(memory.is_terminals)):</p><p class="source-code">            if is_terminal:</p><p class="source-code">                discounted_reward = 0</p><p class="source-code">            discounted_reward = reward + \</p><p class="source-code">                                (self.gamma * discounted_reward)</p><p class="source-code">            rewards.insert(0, discounted_reward)</p></li>
				<li>Next, normalize the rewards and convert them to tensors:<p class="source-code">        rewards = T.tensor(rewards).to(device)</p><p class="source-code">        rewards = (rewards - rewards.mean()) \</p><p class="source-code">                   / (rewards.std() + 1e-5)</p><p class="source-code">        # Convert to Tensor</p><p class="source-code">        old_states = T.stack(memory.memory_states)\</p><p class="source-code">                     .to(device).detach()</p><p class="source-code">        old_actions = T.stack(memory.memory_actions)\</p><p class="source-code">                      .to(device).detach()</p><p class="source-code">        old_log_probs = T.stack(memory.memory_log_probs)\</p><p class="source-code">                        .to(device).detach()</p><p class="source-code">        # Policy Optimization</p><p class="source-code">        for _ in range(self.K_epochs):</p><p class="source-code">            log_probs, state_values, dist_entropy = \</p><p class="source-code">            self.policy.evaluate(old_states, old_actions)</p></li>
				<li>Next, find the probability ratio, find the loss and propagate our loss backwards:<p class="source-code">            # Finding ratio: pi_theta / pi_theta__old</p><p class="source-code">            ratios = T.exp(log_probs - old_log_probs.detach())</p><p class="source-code">            # Surrogate Loss</p><p class="source-code">            advantages = rewards - state_values.detach()</p><p class="source-code">            surr1 = ratios * advantages</p><p class="source-code">            surr2 = T.clamp(ratios, 1-self.eps_clip, \</p><p class="source-code">                            1+self.eps_clip) * advantages</p><p class="source-code">            loss = -T.min(surr1, surr2) \</p><p class="source-code">                   + 0.5*self.MseLoss(state_values, rewards) \</p><p class="source-code">                   - 0.01*dist_entropy</p><p class="source-code">            # Backpropagation</p><p class="source-code">            self.optimizer.zero_grad()</p><p class="source-code">            loss.mean().backward()</p><p class="source-code">            self.optimizer.step()</p></li>
				<li>Update the old policy with the new weights:<p class="source-code">        # New weights to old policy</p><p class="source-code">        self.policy_old.load_state_dict(self.policy.state_dict())</p><p>So, here in <em class="italic">steps 6-10</em> of this exercise we are defining an agent by starting with the initialization of our policy, the optimizer, and the old policy. Then, in the <strong class="source-inline">update</strong> function, we are at first taking the Monte Carlo estimate of the state rewards. After normalizing the rewards, we are converting them into tensors.</p><p>Then, we are carrying out the policy optimization for <strong class="source-inline">K_epochs</strong>. Here, we have to find the probability ratio, <img src="image/B16182_11_14a.png" alt="a"/>, which is the probability ratio between the new and the old policy, as described previously.</p><p>After that, we are finding the loss, <img src="image/B16182_11_14b.png" alt="b"/>, and propagating our loss backward. Finally, we are updating our old policy with the new weights.</p></li>
				<li>Now, we can run the simulation as we did in the previous exercise and save the policy for future use:</li>
			</ol>
			<p class="source-code-heading">Exercise11_03.ipynb</p>
			<p class="source-code">env = gym.make("LunarLander-v2")</p>
			<p class="source-code">render = False</p>
			<p class="source-code">solved_reward = 230</p>
			<p class="source-code">logging_interval = 20</p>
			<p class="source-code">update_timestep = 2000</p>
			<p class="source-code">np.random.seed(0)</p>
			<p class="source-code">memory = ReplayBuffer()</p>
			<p class="source-code">agent = Agent(state_dimension=env.observation_space.shape[0],\</p>
			<p class="source-code">              action_dimension=4, nb_latent_variables=64, \</p>
			<p class="source-code">              lr=0.002, betas=(0.9, 0.999), gamma=0.99,\</p>
			<p class="source-code">              K_epochs=4, eps_clip=0.2)</p>
			<p class="source-code">current_reward = 0</p>
			<p class="source-code">avg_length = 0</p>
			<p class="source-code">timestep = 0</p>
			<p class="source-code">for i_ep in range(50000):</p>
			<p class="source-code">    state = env.reset()</p>
			<p class="source-code">    for t in range(300):</p>
			<p class="source-code">        timestep += 1</p>
			<p class="source-code-link">The complete code for this example can be found at <a href="https://packt.live/2zM1Z6Z">https://packt.live/2zM1Z6Z</a>.</p>
			<p>The following is the first 10 lines of the output:</p>
			<p class="source-code">Episode 0, reward: -8</p>
			<p class="source-code">Episode 20, reward: -182</p>
			<p class="source-code">Episode 40, reward: -154</p>
			<p class="source-code">Episode 60, reward: -175</p>
			<p class="source-code">Episode 80, reward: -136</p>
			<p class="source-code">Episode 100, reward: -178</p>
			<p class="source-code">Episode 120, reward: -128</p>
			<p class="source-code">Episode 140, reward: -137</p>
			<p class="source-code">Episode 160, reward: -140</p>
			<p class="source-code">Episode 180, reward: -150</p>
			<p>Note that we are saving our policy at certain intervals. This is useful if you want to load the policy at a later stage and simply run the simulation from there. The simulation output of this exercise would be the same as in <em class="italic">Figure 11.9</em>, only the operating cost is reduced here.</p>
			<p>Here, if you look at the difference between the rewards, the points given in each consequent episode are much less as we have used the PPO technique. That means the learning is not going haywire as it was in <em class="italic">Exercise 11.01</em>, <em class="italic">Landing a Spacecraft on the Lunar Surface Using Policy Gradients and the Actor-Critic Method</em>, where the difference between the rewards was higher.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2zM1Z6Z">https://packt.live/2zM1Z6Z</a>.</p>
			<p class="callout">This section does not currently have an online interactive example and will need to be run locally.</p>
			<p>We have almost covered all the important topics relating to policy-based RL. So, now we will talk about the last topic, which is the A2C method.</p>
			<h2 id="_idParaDest-312"><a id="_idTextAnchor356"/>The Advantage Actor-Critic Method</h2>
			<p>We have already learned about the actor-critic method and the reason for using it in the introduction, and we have also seen it used in our coding examples. But, a quick recap—actor-critic methods lie at the intersection of the value-based and policy-based methods, where we simultaneously update our policy and our value, which acts as a judge quantifying how good our policy actually is.</p>
			<p>Next, we will learn how A2C works:</p>
			<ol>
				<li value="1">We start by initializing the policy parameter, <img src="image/B16182_11_14c.png" alt="b"/>, with random weights.</li>
				<li>Next, we play <em class="italic">N</em> number of steps with the current policy, <img src="image/B16182_11_14d.png" alt="a"/> , and store the state, action, reward, and transitions.</li>
				<li>We set our reward to <strong class="source-inline">0</strong> if we reach the final episode of the state; otherwise, we set the reward to the value of the current state.</li>
				<li>Then, we calculate the discounted reward, policy loss, and value loss by looping backward from the final episode.</li>
				<li>Finally, we apply <strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>) using the mean policy and value loss for each batch.</li>
				<li>Repeat the steps from <em class="italic">step 2</em> onward until it reaches convergence.<p class="callout-heading">Note</p><p class="callout">We have only briefly introduced A2C here. A detailed description and implementation of this method is beyond the scope of this book.</p></li>
			</ol>
			<p>The first coding example (<em class="italic">Exercise 11.01</em>, <em class="italic">Landing a Spacecraft on the Lunar Surface Using Policy Gradients and the Actor-Critic Method</em>) that we covered follows the basic A2C method. However, there's another technique, called the <strong class="bold">Asynchronous Advantage Actor-Critic</strong> (<strong class="bold">A3C</strong>) method. Remember, our policy gradient methods work online. That is, we only train on the data obtained using the current policy and we do not keep track of past experiences. However, to keep our data independent and identically distributed, we need a large buffer of transitions. The solution provided by A3C is to run multiple training environments in parallel to acquire large amounts of training data. With multiprocessing in Python, this actually becomes very fast in practice.</p>
			<p>In the next activity, we will write code to run the Lunar Lander simulation that we learned about in <em class="italic">Exercise 11.03</em>, <em class="italic">Improving the Lunar Lander Example Using PPO</em>. We will also render the environment to see the Lunar Lander. To do that, we will have to import the PIL library. The code to render the image is as follows:</p>
			<p class="source-code">if render:</p>
			<p class="source-code">    env.render()</p>
			<p class="source-code">          </p>
			<p class="source-code">    img = env.render(mode = "rgb_array")</p>
			<p class="source-code">    img = Image.fromarray(img)</p>
			<p class="source-code">    image_dir = "./gif"</p>
			<p class="source-code">    if not os.path.exists(image_dir):</p>
			<p class="source-code">        os.makedirs(image_dir)</p>
			<p class="source-code">    img.save(os.path.join(image_dir, "{}.jpg".format(t)))</p>
			<p>Let's begin with the implementation of our final activity.</p>
			<h2 id="_idParaDest-313"><a id="_idTextAnchor357"/>Activity 11.02: Loading the Saved Policy to Run the Lunar Lander Simulation</h2>
			<p>In this activity, we will combine multiple aspects of RL that we have explained in previous sections. We will use what we learned in <em class="italic">Exercise 11.03</em>, <em class="italic">Improving the Lunar Lander Example Using PPO</em>, to write simple code to load the saved policy. This activity combines all the essential components of building a working RL prototype—in our case, the Lunar Lander simulation.</p>
			<p>The steps to take are as follows:</p>
			<ol>
				<li value="1">Open Jupyter and in a new notebook, import the essential  Python libraries, including the PIL library to save the image.</li>
				<li>Set your device using the device parameter.</li>
				<li>Define the <strong class="source-inline">ReplayBuffer</strong>, <strong class="source-inline">ActorCritic</strong>, and <strong class="source-inline">Agent</strong> classes. We already defined these in the previous exercise.</li>
				<li>Create the Lunar Lander environment. Initialize the random seed.</li>
				<li>Create the memory buffer and initialize the agent with hyperparameters, as in the previous exercise.</li>
				<li>Load the saved policy as an old policy.</li>
				<li>Finally, loop through your desired number of episodes. In every iteration, start by initializing the episode reward as <strong class="source-inline">0</strong>. Do not forget to reset the state. Run another loop, specifying the <strong class="source-inline">max</strong> timestamp. Get the <strong class="source-inline">state</strong>, <strong class="source-inline">reward</strong>, and <strong class="source-inline">done</strong> flags for each action taken and add the reward to the episode reward.</li>
				<li>Render the environment to see how your Lunar Lander is doing.<p>The following is the expected output:</p><p class="source-code">Episode: 0, Reward: 272</p><p class="source-code">Episode: 1, Reward: 148</p><p class="source-code">Episode: 2, Reward: 249</p><p class="source-code">Episode: 3, Reward: 169</p><p class="source-code">Episode: 4, Reward: 35</p><p>The following screenshot shows the simulation output of some of the stages:</p><div id="_idContainer773" class="IMG---Figure"><img src="image/B16182_11_15.jpg" alt="Figure 11.15: The environment showing the simulation of the Lunar Lander&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.15: The environment showing the simulation of the Lunar Lander</p>
			<p>The complete simulation output can be found in the form of images at <a href="https://packt.live/3ehPaAj">https://packt.live/3ehPaAj</a>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 769. </p>
			<h1 id="_idParaDest-314"><a id="_idTextAnchor358"/>Summary</h1>
			<p>In this chapter, we learned about policy-based methods, principally the drawbacks to value-based methods such as Q-learning, which motivate the use of policy gradients. We discussed the purposes of policy-based methods of RL, along with the trade-offs of other RL approaches.</p>
			<p>You learned about the policy gradients that help a model to learn in a real-time environment. Next, we learned how to implement the DDPG using the actor-critic model, the <strong class="source-inline">ReplayBuffer</strong> class, and Ornstein–Uhlenbeck noise to understand the continuous action space. We also learned how you can improve policy gradients by using techniques such as TRPO and PPO. Finally, we talked in brief about the A2C method, which is an advanced version of the actor-critic model.</p>
			<p>Also, in this chapter, we played around with the Lunar Lander environment in OpenAI Gym—for both continuous and discrete action spaces—and coded the multiple policy-based RL approaches that we discussed.</p>
			<p>In the next chapter, we will learn about a gradient-free method to optimize neural networks and RL-based algorithms. We will then discuss the limitations of gradient-based methods. The chapter presents an alternative optimization solution to gradient methods through genetic algorithms as they ensure global optimum convergence. We will also learn about the hybrid neural networks that use genetic algorithms to solve complex problems.</p>
		</div>
	</body></html>