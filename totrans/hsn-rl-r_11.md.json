["```py\nlibrary(tictactoe)\n```", "```py\nttt(ttt_human(name = \"GIUSEPPE\"), ttt_random())\n```", "```py\nA B C\n ------\n 1| . . .\n 2| . . .\n 3| . . .\n\n Player 1 (GIUSEPPE) to play\nchoose move (e.g. A1) >\n```", "```py\naction = A1\n\n A B C\n ------\n 1| X . . \n 2| . . .\n 3| . . .\n\nPlayer 2 (random AI) to play\n\naction = B1\n\n A B C\n ------\n 1| X O .\n 2| . . .\n 3| . . .\n\nPlayer 1 (GIUSEPPE) to play\nchoose move (e.g. A1) >\n```", "```py\naction = B2\ngame over\n A B C\n ------\n 1| X . X\n 2| O X O\n 3| X . O\n\nwon by Player 1 (GIUSEPPE)!\n```", "```py\nplayer1<- ttt_ai()\nplayer2<- ttt_ai()\nSimulatedGame <- ttt_simulate(player1, player2, N = 100)\n```", "```py\nstr(SimulatedGame)\n```", "```py\nint [1:100] 1 2 2 0 1 1 2 1 2 2 ...\n```", "```py\nprop.table(table(SimulatedGame))\n```", "```py\nSimulatedGame\n 0   1    2\n0.12 0.51 0.37\n```", "```py\nplayer3<- ttt_ai()\nplayer4<- ttt_ai()\n```", "```py\nTrainPlayer4 <- ttt_qlearn(player4, N = 500, verbose = FALSE)\n```", "```py\nSimulatedGameQLearn <- ttt_simulate(player3, player4, N = 100)\n```", "```py\nprop.table(table(SimulatedGameQLearn))\n```", "```py\nSimulatedGameQLearn\n 0    1    2\n0.31 0.21 0.48\n```", "```py\ngit clone https://github.com/openai/gym-http-api\ncd gym-http-api\npip install -r requirements.txt\n```", "```py\npython gym_http_server.py\n```", "```py\nServer starting at: http://127.0.0.1:5000\n```", "```py\ninstall.packages(\"gym\")\n```", "```py\nlibrary(gym)\n```", "```py\nRemoteBase <- \"http://127.0.0.1:5000\"\nClient <- create_GymClient(RemoteBase)\nprint(Client)\n```", "```py\n<GymClient: http://127.0.0.1:5000>\n```", "```py\nEnvId <- \"CartPole-v0\"\nInstanceId <- env_create(Client, EnvId)\nprint(InstanceId)\n```", "```py\n[1] \"376e0df2\"\n```", "```py\nAllEnvsAvailable <- env_list_all(Client)\nprint(AllEnvsAvailable)\n```", "```py\n$`376e0df2`\n[1] \"CartPole-v0\"\n```", "```py\nActionSpaceInfo <- env_action_space_info(Client, InstanceId)\nprint(ActionSpaceInfo)\n```", "```py\n$n\n[1] 2\n\n$name\n[1] \"Discrete\"\n```", "```py\nagent <- random_discrete_agent(ActionSpaceInfo[[\"n\"]])\n```", "```py\nOutDir <- \"/TempFolder/results\"\nenv_monitor_start(Client, InstanceId, OutDir, force = TRUE, resume = FALSE)\n```", "```py\nEpisodeCount <- 100\nMaxSteps <- 200\nReward <- 0\ndone <- FALSE\n```", "```py\nfor (i in 1: EpisodeCount) {\n  Object <- env_reset(Client, InstanceId)\n  for (i in 1: MaxSteps) {\n    action <- env_action_space_sample(Client, InstanceId)\n    results <- env_step(Client, InstanceId, action, render = TRUE)\n    if (results[[\"done\"]]) break\n  }\n}\n```", "```py\nenv_monitor_close(Client, InstanceId)\n```", "```py\nlibrary(gym)\n```", "```py\nremote_base <- \"http://127.0.0.1:5000\"\nclient <- create_GymClient(remote_base)\nprint(client)\n```", "```py\n<GymClient: http://127.0.0.1:5000>\n```", "```py\nenv_id <- \"FrozenLake-v0\"\ninstance_id <- env_create(client, env_id)\nprint(instance_id)\n```", "```py\n[1] \"af775b0a\"\n```", "```py\nAllEnvsAvailable <- env_list_all(Client)\nprint(AllEnvsAvailable)\n```", "```py\n$af775b0a\n[1] \"FrozenLake-v0\"\n```", "```py\nActionSpaceInfo <- env_action_space_info(client, instance_id)\nprint(ActionSpaceInfo)\n```", "```py\n$n\n[1] 4\n\n$name\n[1] \"Discrete\"\n```", "```py\nObservationSpaceInfo <- env_observation_space_info(client, instance_id)\nprint(ObservationSpaceInfo)\n```", "```py\n$n\n[1] 16\n\n$name\n[1] \"Discrete\"\n```", "```py\nActionSize = action_space_info$n\nStateSize = observation_space_info$n\n```", "```py\nQtable <- matrix(data = 0, nrow = StateSize, ncol = ActionSize)\n```", "```py\nprint(Qtable)\n```", "```py\n [,1] [,2] [,3] [,4]\n [1,]    0    0    0    0\n [2,]    0    0    0    0\n [3,]    0    0    0    0\n [4,]    0    0    0    0\n [5,]    0    0    0    0\n [6,]    0    0    0    0\n [7,]    0    0    0    0\n [8,]    0    0    0    0\n [9,]    0    0    0    0\n[10,]    0    0    0    0\n[11,]    0    0    0    0\n[12,]    0    0    0    0\n[13,]    0    0    0    0\n[14,]    0    0    0    0\n[15,]    0    0    0    0\n[16,]    0    0    0    0\n```", "```py\nalpha = 0.80\ngamma = 0.95\n```", "```py\nNumEpisodes = 200\n```", "```py\nRewardsList = list()\n```", "```py\nIndxList = 1\nNumGoal=0\n```", "```py\nfor (i in 1:NumEpisodes) {\n  cat(\"######Episode \", i, \" ######\", \"\\n\")\n```", "```py\n  state = env_reset(client, instance_id)\n```", "```py\n  SumReward = 0\n  j = 0\n```", "```py\n  while (j < 99){\n```", "```py\n    j=j+1\n```", "```py\n    action = which.max(Qtable[state+1,] + runif(4,0, 1)*(1./(i+1)))\n    action = action - 1\n```", "```py\nResults<- env_step(client, instance_id, action, render = TRUE)\n```", "```py\nNewState<-Results$observation\nActualReward<-Results$reward\n```", "```py\n    Qtable[state+1, action+1] = (1 - alpha) * Qtable[state+1, action+1] + alpha * (ActualReward + gamma * max(Qtable[NewState+1,]))\n```", "```py\n    SumReward = SumReward + ActualReward\n    state = NewState\n```", "```py\n    if (Results[[\"done\"]]) {\n      print(\"#####STEP######\")\n      print(j)\n      cat(\"State achieved\", NewState+1, \"\\n\")\n      cat(\"Reward\", ActualReward, \"\\n\")\n\n      if (state==15) {\n        NumGoal=NumGoal+1\n\n      }\n      break\n    }\n  }\n```", "```py\n  RewardsList[IndxList]<-SumReward\n  IndxList = IndxList +1\n}\n```", "```py\ncat(\"Numbers of goal achieved \", NumGoal, \"\\n\")\nprint (\"Score: \")\nprint(do.call(sum,RewardsList)/NumEpisodes)\n```", "```py\nprint (\"Final Q-Table Values\")\nprint (Qtable)\n```", "```py\n [,1]         [,2]         [,3]         [,4]\n [1,] 8.269208e-02 2.915901e-03 0.0026752394 2.927068e-03\n [2,] 3.346197e-05 4.689209e-05 0.0001546271 1.524209e-01\n [3,] 6.866991e-04 9.183121e-04 0.0032547141 2.434100e-01\n [4,] 1.191637e-03 5.130816e-04 0.0005775460 7.667945e-02\n [5,] 8.717573e-02 2.206559e-04 0.0002355794 3.511920e-04\n [6,] 0.000000e+00 0.000000e+00 0.0000000000 0.000000e+00\n [7,] 6.317772e-05 4.168548e-05 0.1003525858 6.572051e-05\n [8,] 0.000000e+00 0.000000e+00 0.0000000000 0.000000e+00\n [9,] 2.501462e-03 9.431424e-04 0.0022643051 7.393004e-02\n[10,] 4.658178e-05 5.656864e-01 0.0000000000 1.469386e-04\n[11,] 1.261091e-02 4.726838e-04 0.0005489759 2.321079e-03\n[12,] 0.000000e+00 0.000000e+00 0.0000000000 0.000000e+00\n[13,] 0.000000e+00 0.000000e+00 0.0000000000 0.000000e+00\n[14,] 8.009153e-04 0.000000e+00 0.8782760471 2.432799e-04\n[15,] 0.000000e+00 9.929596e-01 0.0000000000 0.000000e+00\n[16,] 0.000000e+00 0.000000e+00 0.0000000000 0.000000e+00\n```", "```py\nfor (episode in 1:5) {\n  print(\"****************EPISODE**********************\")\n  print(episode)\n```", "```py\n  state = env_reset(client, instance_id)\n```", "```py\nfor (step in 1:100) {\n```", "```py\nTotRew = 0\n```", "```py\naction = which.max(Qtable[state+1,])\naction = action - 1\n```", "```py\nResults<- env_step(client, instance_id, action, render = TRUE)\n```", "```py\nNewstate<-Results$observation\nreward<-Results$reward\n```", "```py\nTotRew <- TotRew + reward\n```", "```py\n    if (Results[[\"done\"]]) {\n      cat(\"Number of steps\", step, \"\\n\")\n      print(TotRew)\n      cat(\"STATO\", Newstate, \"\\n\")\n      break\n\n    }\n    state = Newstate\n  }\n}\n```"]