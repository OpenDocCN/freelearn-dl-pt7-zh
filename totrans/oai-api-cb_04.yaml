- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Incorporating Additional Features from the OpenAI API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenAI API offers additional features beyond the standard endpoints and
    parameters that we learned about in the previous chapter. These provide additional
    customizability to the existing model and enable far more use cases by linking
    the model to other methods.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the OpenAI API contains a robust embedding model, enabling users
    to vectorize text to perform typical NLP functions such as text clustering, text
    classification, text comparison, and more. This is the same technology that search
    engines such as Google use, for example, to return relevant search results. Now,
    with the OpenAI API, it is available at your fingertips.
  prefs: []
  type: TYPE_NORMAL
- en: The API also contains a method to *fine-tune* or customize a model for a particular
    use case. Instead of the fine-tuning we did earlier, which required *priming*
    the model with several examples, this is a better and typically cheaper alternative.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the API also possesses the ability to create **function calls**. This
    enables you to provide the API with a set of functions and their descriptions,
    and the model in turn intelligently creates a JSON object containing arguments
    to call that function, enabling you to link the OpenAI API to any user-defined
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: However, in order to use these features, we need to call the API through a programmatic
    language such as *Python* instead of through one-time HTTP requests such as Postman.
    As a result, we will first cover how to use the OpenAI API with Python instead
    of Postman, and then learn about the benefits that this change in methodology
    enables.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know how to use these features in your
    applications. This is important because understanding these features will open
    up a plethora of other use cases that would otherwise not be possible to execute.
    Additionally, we will cover applications of each feature beyond what is covered
    within each recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the Python library to call the OpenAI API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the embedding model for text comparison and other use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning a completion model and relevant applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the recipes in this chapter require you to have access to the OpenAI API
    (via a generated API key) and have an API client installed. In case you don’t
    recall how to do this, you can refer to the [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021)
    recipe *Making OpenAI API requests* *with Postman*.
  prefs: []
  type: TYPE_NORMAL
- en: In previous chapters, we have used Postman as our API client. In this case,
    we will use the programmatic language Python instead. Specifically, the recipes
    will use the OpenAI Python library to make calls to the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: We will run Python in a service called **Google Colab**. Colab is an online
    hosted **Jupyter Notebook** service by Google, that requires no setup to use and
    can run Python code within the browser. The Jupyter Notebook is an open sourced
    web application that allows you to create and share documents and contains live
    code that can be run step by step. This is the environment we will use to run
    our Python code.
  prefs: []
  type: TYPE_NORMAL
- en: To use Google Colab, you need to create and be signed in to a valid Google account,
    which is completely free. Follow the steps to create a new Google account at [https://accounts.google.com/](https://accounts.google.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Using the Python library to call the OpenAI API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we used HTTP requests and Postman to call the OpenAI API. Now, we
    are transferring to another method of calling the API, through Python with the
    dedicated OpenAI Python library. Why does this matter and why is this important?
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the Python library for OpenAI API calls offers a significant advantage
    over manual HTTP requests in tools such as Postman, especially for developers
    looking to integrate ChatGPT functionality into their applications seamlessly.
  prefs: []
  type: TYPE_NORMAL
- en: Python’s library simplifies the intricacies involved in making direct HTTP requests
    by offering a more user-friendly and intuitive interface. This facilitates quick
    prototyping, streamlined error management, and efficient parsing of responses.
    The library wraps the fundamental details of the protocol, allowing developers
    to concentrate on their application’s essential functionality without being bogged
    down by the specifics of request headers, query strings, and HTTP methods.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, Python’s extensive package ecosystem readily supports the integration
    of the OpenAI API with other services and systems, allowing for a scalable and
    maintainable code base.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, if you are serious about building intelligent applications with the
    OpenAI API, you need to call the API with a programmatic language that enables
    complex logic and tie-ins to other systems. Python, through the OpenAI library,
    is one way to accomplish that.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will create some simple API calls using Python and the OpenAI
    library. More information on the library can be found here: [https://github.com/openai/openai-python](https://github.com/openai/openai-python).'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure you have an OpenAI platform account with available usage credits. If
    you don’t, please follow the *Setting up your OpenAI Playground environment* recipe
    in [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, ensure you are logged in to a Google account and have access to
    a notebook. You can verify this by going to [https://colab.google/](https://colab.google/)
    and selecting **New Notebook** at the top right. After that, you should have a
    blank screen with an empty notebook open.
  prefs: []
  type: TYPE_NORMAL
- en: All the recipes in this chapter have the same requirements.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In your Google Colab notebook, click the first empty cell, and type in the
    following code to download and install the OpenAI Python library. After you have
    typed the code in, press *Shift* + *Enter* to run the code inside the cell. Alternatively,
    you can run the code inside the cell by clicking the **Play** button to the left
    of the cell. This code will attempt to install the OpenAI Python library and all
    its dependencies. You may see output such as **Requirements already satisfied**
    or **Installing httpcore**. This is Google attempting to install the libraries
    that OpenAI depends on to run its own library, and is perfectly normal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Ensure that the words **Successfully installed openai-X.XX.X** are visible,
    as seen in *Figure 4**.1*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Output of Jupyter notebook after installing the OpenAI library](img/B21007_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Output of Jupyter notebook after installing the OpenAI library
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to perform authentication. This is similar to the previous chapters
    where we had to authenticate our Postman requests by putting our API key in a
    **Header** parameter called *Authorization*. In Python, it’s much simpler. In
    the cell below the one you used in *step 1*, write the following code and press
    *Shift* + *Enter*. Note, replace **<api-key>** with the API key that you generated
    in the last recipe in [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now make a chat completion request to the OpenAI API. Similar to Postman,
    we can use different endpoints and define a variety of different parameters within
    the request in Python. Type the following code into a new cell below and press
    *Shift* + *Enter*, which runs the code and saves the output in a variable called
    **completion**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Output the **completion** variable, which is a **ChatCompletion** object. We
    can convert this into the more familiar JSON format (exactly as in Postman) by
    typing the following in the cell below and running the code by pressing *Shift*
    + *Enter*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Figure 4**.2* shows the output that you will see after running this code.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2 – JSON output of the Python OpenAI completion request](img/B21007_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – JSON output of the Python OpenAI completion request
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Python, we can parse through the JSON and only output the part of the
    JSON that contains the company slogan. We can do this by typing the following
    code into the cell below and pressing *Shift* + *Enter* to run the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.3 – Input and output of step 6](img/B21007_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Input and output of step 6
  prefs: []
  type: TYPE_NORMAL
- en: You now have a working Python Jupyter notebook that calls the OpenAI API, makes
    a chat completion request, and outputs the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we performed the same actions as we have done in previous recipes,
    the difference being that we used the OpenAI Python library instead of invoking
    HTTP requests through Postman. We authenticated using our API key, made a chat
    completion request, and adjusted several parameters (such as *Model*, *Messages*,
    *N*, and *Temperature*), and printed the output result.
  prefs: []
  type: TYPE_NORMAL
- en: Code walk-through
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code that was run within the recipe can be explained in four parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Library installation*: The first line – **!pip install openai; import openai**
    – is a command that installs the OpenAI library as a package in Python. The second
    line imports it into the current Python namespace, enabling the use of the library’s
    functions and classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Authentication*: The **openai.api_key = "sk-..."** line sets the API key for
    authenticating requests to the OpenAI API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*API call*: The **openai.ChatCompletion.create()** line calls the API and makes
    a chat completion request. As you can see, it contains the typical parameters
    that we have discussed in previous chapters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Output*: The **print(completion); print(completion[''choices''][0][''message''][''content''])**
    line prints out the raw response from the API call. The response includes not
    only the content of the completion but also some metadata, similar to when we
    make HTTP requests with Postman. This second line digs into the response object
    to extract and print only the content of the message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most API calls in Python follow these steps. It should be noted that *steps
    1 and 2* (i.e., library installation and authentication) only need to be performed
    once. This is because once a library is installed, it becomes a part of your Python
    environment, ready to be used in any program without needing to be reinstalled
    each time. Similarly, authentication, which is often a process of verifying credentials
    to gain access to the API, is typically required only once per session or configuration,
    as your credentials are then stored and reused for subsequent API calls.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, we delved into using the OpenAI Python library for interacting with
    the OpenAI API, transitioning from the HTTP requests method in Postman. We will
    continue following this process in future recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Components of the Python library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The endpoints and parameters that we have discussed in previous chapters are
    all available within the OpenAI Python library. The syntax is slightly different,
    as we are now using Python code rather than JSON (through Postman) to make API
    requests, but the fundamental idea is the same. Here is a table that compares
    endpoint calls between Postman and Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Endpoint** | **HTTP request in Postman through JSON (the** **Body component)**
    | **Python** **OpenAI Library** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Chat completions** |'
  prefs: []
  type: TYPE_TB
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Images** |'
  prefs: []
  type: TYPE_TB
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Audio** |'
  prefs: []
  type: TYPE_TB
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4.1 – Comparing endpoint calls between Postman and Python libraries
  prefs: []
  type: TYPE_NORMAL
- en: Benefits and drawbacks of using the Python library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several benefits to doing this, aside from it just being a pre-requisite
    to future recipes. It provides abstraction over the API request itself, leading
    to the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Simplified authentication*: The library handles API key and token management,
    abstracting away the details of the authentication process from the user. For
    example, in this case, we did not need to create a new parameter for *Bearer*,
    unlike within HTTP. Furthermore, unlike HTTP requests, we do not need to declare
    our API key for every single request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ease of use*: It provides a high-level interface with methods and classes
    that represent API endpoints, making it easier to understand and implement; the
    library takes care of constructing the correct HTTP requests, encoding parameters,
    and parsing the responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Do more*: The library often includes convenience features that are not available
    with simple HTTP requests, such as pagination helpers, streaming, session management,
    embeddings, function calls, and more (which is why we switched over to the Python
    library in this chapter – the subsequent recipes cover these features).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Programmability*: The Python OpenAI library leverages the full programming
    capabilities of Python, enabling variables, logical conditioning, and functions
    (i.e., all the benefits of a programming language that you don’t get with Postman).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are, however, some specific downsides to using the Python library as
    well:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Limited customization*: High-level abstraction may limit direct access to
    certain API functionalities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Maintenance and compatibility*: There is a dependency on library updates and
    potential conflicts with different Python versions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Performance overheads*: Additional abstraction layers can lead to slower performance
    in resource-critical applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Reduced control*: It offers less flexibility for users needing detailed control
    over API interactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the embedding model for text comparisons and other use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI has a model and endpoint that enables users to create **embeddings**.
    It’s a lesser-known feature of the API but has vast applications in enabling plenty
    of use cases (searching through text, text classification, and much more).
  prefs: []
  type: TYPE_NORMAL
- en: What are embeddings? **Text embedding** is a sophisticated technique employed
    in NLP that transforms text into a numerical format that machines can understand.
    Essentially, embeddings are high-dimensional vectors that capture the essence
    of words, sentences, or even entire documents, encapsulating not just their individual
    meanings but also the nuances and relationships between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, a vector is a point in an n-dimensional vector space, but for
    our purposes, you can think of a vector as just a list of numbers. However, the
    recipes discussed in this chapter do not require you to work with the process
    and science behind converting words to numbers. For more information on the science
    behind embeddings, you can find a great introductory article here: [https://stackoverflow.blog/2023/11/09/an-intuitive-introduction-to-text-embeddings/](https://stackoverflow.blog/2023/11/09/an-intuitive-introduction-to-text-embeddings/).'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will use the OpenAI API to convert various texts into embeddings
    and use those embeddings for the use case of text comparison.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open up a new notebook by navigating to [https://colab.google/](https://colab.google/)
    and selecting **New Notebook** at the top right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the first cell, type in the following code and press *Shift* + *Enter* to
    run the code. This will install the OpenAI library and import the required modules
    for this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similar to the previous recipe, type the following code into the cell below,
    replacing **<api-key>** with your OpenAI API Key. Hit *Shift* + *Enter* to run
    the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create two functions in Python. The first function will create
    an embedding given a text string. To do this, we will use the **Embeddings** endpoint
    from the OpenAI API. The next function takes two embeddings and calculates the
    difference between them using **cosine similarity**, a concept that we will discuss
    in the next section. To do this, type the following code in the cell below and
    press *Shift* + *Enter*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we have everything we need to start comparing texts by creating embeddings
    and calculating the difference between them. Let’s start with two pieces of text
    that are semantically very similar: *I like apples* and *I like bananas*. Type
    in the following code, hit *Shift* + *Enter*, and note the output result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.4 – Output of cosine similarity for similar texts](img/B21007_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Output of cosine similarity for similar texts
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s compare two pieces of text that are not similar: *I like apples*
    and the first section of Article 1 of the US Constitution: *All legislative Powers
    herein granted shall be vested in a Congress of the United States, which shall
    consist of a Senate and House of Representatives* ([https://www.archives.gov/founding-docs/constitution-transcript](https://www.archives.gov/founding-docs/constitution-transcript)).
    Type in the following code, hit *Shift* + *Enter*, and note the output result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.5 – Output of cosine similarity for similar texts](img/B21007_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Output of cosine similarity for similar texts
  prefs: []
  type: TYPE_NORMAL
- en: Note the similarity between the first set of texts (**0.90**) was higher than
    for the next set of texts (**0.70**). This means that the first set of texts is
    more semantically similar than the next two texts, which makes sense given the
    language.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s take this one step further. Repeat *steps 5-7* with the following texts.
    I’ve also noted the output similarities I got:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we converted texts into embeddings and then compared the embeddings.
    The results showed us that, when comparing it to *I like applies*, the text *I
    like bananas* is more semantically similar to the first section of the US Constitution.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, it demonstrated that the text *Birds like to fly* is more semantically
    similar to *Airplanes can soar above the ground* than *A fly can irritate me*.
    This makes sense as in the first two pieces of text, the sentences were about
    objects flying.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding 101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned before, the process of embedding turns text into a list of numbers.
    This is imperative in NLP, as now machines can work with these lists of numbers
    instead of text.
  prefs: []
  type: TYPE_NORMAL
- en: The key feature of OpenAI’s embedding model is that it captures linguistic properties
    and semantic meaning. This means that two pieces of text that are semantically
    similar will have similar vectors (i.e., a similar list of numbers). **Semantically
    similar** means that two pieces of text convey the same or related meanings, concepts,
    or ideas, even if they use different words or structures.
  prefs: []
  type: TYPE_NORMAL
- en: Code structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We used OpenAI’s embedding model to create these vectors, using the Embeddings
    endpoint. The endpoint can be called with the `openai.Embedding.create()` function,
    and takes in two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**input**: This argument represents the text that you want to create embeddings
    for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**model**: This is the ID of the model you want to use for the embeddings.
    This is similar to the **model** parameter in other endpoints. In this example,
    we used the standard **ada** model, which was **text-embedding-ada-002**. OpenAI
    recommends using this as the starting embedding model as it’s quite affordable
    and still has excellent performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function call returns an embedding object in JSON format, which we then
    parse through to get the embedding itself (which again is a list of numbers in
    Python). The parsing is done via the `["``data"][0]["embedding"]` code.
  prefs: []
  type: TYPE_NORMAL
- en: After we have the embeddings from two sets of text, we then need to compare
    them. How do you compare two vectors (i.e., how do you compare two lists of numbers?)?
    The most common method used is called **cosine similarity**. Cosine similarity
    measures the cosine of the angle between two vectors, resulting in a number between
    0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity is often chosen over other similarity measuring techniques
    because it is particularly effective in high-dimensional spaces, such as text
    data, where it emphasizes the orientation rather than the magnitude of vectors.
    This approach allows it to focus on the directional alignment of the vectors,
    making it more robust in assessing the semantic similarity between texts, even
    when they vary in length or word frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The math does not matter here – the implication is that the higher the cosine
    similarity, the more closely the two texts are semantically related:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Cosine similarity close to 1*: The texts are very similar or have similar
    context or meaning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cosine similarity close to 0*: The texts are unrelated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cosine similarity close to -1*: The texts are semantically opposite, which
    is rare in NLP because most text embeddings are designed to have non-negative
    components'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Python, this is achieved through the following code, which takes the dot
    product of the two vectors and divides it by the product of the two vectors’ Euclidean
    norm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: After we set up a function to return the embeddings for each text, and the function
    to compute the cosine similarity between two embeddings, we had all the tools
    we needed to compare two pieces of text.
  prefs: []
  type: TYPE_NORMAL
- en: The normalization in this formula (`norm`) ensures that we’re comparing the
    direction, rather than the magnitude, of the two vectors. This means we are focusing
    on how similar the two vectors are in terms of orientation, regardless of their
    length, which is essential for measuring similarity in many applications such
    as comparing sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Applications in text comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Embeddings are an efficient way to compare two pieces of text, opening lots
    of different real-world applications. Recall the similarity scores that were computed
    in the recipe, described in the table that follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Test** | **Base text** | **Comparison text** | **Cosine similarity** **of
    embeddings** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | I like apples | I like bananas | 0.90 |'
  prefs: []
  type: TYPE_TB
- en: '| All legislative Powers herein granted shall be vested in a Congress of the
    United States, which shall consist of a Senate and House of Representatives |
    0.71 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | Birds like to *fly* | Airplanes can soar above the ground | 0.88
    |'
  prefs: []
  type: TYPE_TB
- en: '| A *fly* can irritate me | 0.84 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – Cosine similarities between OpenAI embeddings for various sets of
    texts
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI API enables you to compute and rank the semantic similarity between
    different sets of text. Note that semantic similarity understands the nuances
    of the meaning of the text. In *Test 2*, the semantic similarity to *Airplanes
    can soar above the ground* was greater than *A fly can* *irritate me*.
  prefs: []
  type: TYPE_NORMAL
- en: This is counterintuitive because you would assume that the text that shares
    the word *fly* would be more similar. However, the embeddings recognize that the
    word *fly* is used in a different context in *Birds like to fly* versus *A fly
    can irritate me*. In this case, embeddings are powerful mechanisms to compare
    the meanings of texts.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other applications of embeddings that, thanks to OpenAI API, you
    can explore when building apps. This is not an exhaustive list but should be a
    good start for you to get some idea about the API’s potential:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Information retrieval with search engines*: Enhancing search algorithms to
    return results that are semantically related to the query, not just textually
    ([https://www.mage.ai/blog/building-semantic-search-engine-with-dual-space-word-embeddings](https://www.mage.ai/blog/building-semantic-search-engine-with-dual-space-word-embeddings))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Document retrieval*: Finding documents that cover similar topics even if they
    don’t share the same keywords ([https://arxiv.org/pdf/1810.10176v2.pdf](https://arxiv.org/pdf/1810.10176v2.pdf))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Content recommendation systems*: Recommending articles, products, or media
    to users based on semantic similarity to items they have liked before ([https://towardsdatascience.com/introduction-to-embedding-based-recommender-systems-956faceb1919](https://towardsdatascience.com/introduction-to-embedding-based-recommender-systems-956faceb1919))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Text classification*: Automatically classifying documents into predefined
    categories based on their semantic content ([https://realpython.com/python-keras-text-classification/](https://realpython.com/python-keras-text-classification/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, the embeddings feature of the OpenAI API opens a plethora of other
    use cases, from text comparison to information retrieval. Another key benefit
    is that these endpoints are far cheaper than the Completions or Images endpoint,
    making it a powerful and efficient tool in your arsenal.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a completion model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Fine-tuning** is the process of taking a pre-trained model and further adapting
    it to a specific task or dataset. The goal is typically to take an original model
    that has been trained on a large, general dataset and apply it to a more specialized
    domain or to improve its performance on a specific type of data.'
  prefs: []
  type: TYPE_NORMAL
- en: We previously saw a version of fine-tuning in the first recipe within [*Chapter
    1*](B21007_01.xhtml#_idTextAnchor021), where we added examples of outputs in the
    `messages` parameter to *fine-tune* the output response. In this case, the model
    had not technically been fine-tuned – we instead performed **few-shot learning**,
    where we gave examples of the output within the prompt itself to the Chat Completion
    model. Fine-tuning, however, is a process where a whole new subset Chat Completion
    model is created with training data (inputs and outputs).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will explore how to fine-tune a model and execute that fine-tuned
    model. Then, we will discuss the benefits and drawbacks of fine-tuning a model
    with the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open up a new notebook by navigating to [https://colab.google/](https://colab.google/)
    and selecting **New Notebook** at the top right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the first cell, type in the following code and press *Shift* + *Enter* to
    run the code. This will install the OpenAI library and import the required modules
    for this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similar to the previous recipe, type the following code into the cell below,
    replacing **<api-key>** with your OpenAI API key. Hit *Shift* + *Enter* to run
    the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the training data into Google Colab. The training data file can be found
    here: [https://drive.google.com/file/d/1x0ciWtW3phjPHAosiCL90qsQY--ZoxsV/view?usp=sharing](https://drive.google.com/file/d/1x0ciWtW3phjPHAosiCL90qsQY--ZoxsV/view?usp=sharing).
    To upload the file into Google Colab, select the *Files* icon on the left and
    select the **Upload File** button at the top of that menu. Both these icons have
    been highlighted in *Figure 4**.6*. Note that the training data includes several
    examples of where the prompt is a scenario (such as *A student in a library*)
    and the completion is a one-liner joke followed by *Haha* (such as *Why did the
    student bring a ladder to the library? Because they heard the knowledge was on
    the top* *shelf! Haha*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.6 – How to add a file to Google Colab](img/B21007_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – How to add a file to Google Colab
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, upload the training dataset to the OpenAI API, by typing in the following
    code and hitting *Shift* + *Enter*. We will also retrieve **file_id** from the
    upload:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After that, we will begin fine-tuning the model, by typing in the following
    code and hitting *Shift* + *Enter*. This will begin the fine-tuning process, by
    instructing OpenAI to use the file that we had previously uploaded through the
    **file_id** variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fine-tuning can take several minutes to finish. We can check the status of
    the job by typing in the following code and hitting *Shift* + *Enter*. If the
    output is *running*, that means the fine-tuning is still in process. Wait until
    the following code returns **succeeded**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the fine-tuning job has been completed, we need the name of the fine-tuned
    model, which we can get by typing in the following code and hitting *Shift* +
    *Enter*. We will save this to the **fine_tuned_model** variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s use our fine-tuned model. Let’s create a simple chat completion
    request, but we will modify the **model** parameter to use the **fine_tuned_model**
    object that we had just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.7 – Chat completion request and output when using a fine-tuned model](img/B21007_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Chat completion request and output when using a fine-tuned model
  prefs: []
  type: TYPE_NORMAL
- en: Note that without providing any examples, the completion output is a one-liner
    joke followed by the word **Haha**. We successfully fine-tuned a model and then
    used the fine-tuned model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we created a fine-tuned model by providing the OpenAI API with
    training data that taught the model how it should respond to prompts. In this
    case, we trained it so that its output should be a one-liner joke followed by
    the word `Haha`. We then changed the `model` parameter to the ID of the model
    we just created, and made a *chat completion* request. After that, we noted that
    the output we received on a prompt it had never seen before also resulted in a
    one-liner joke followed by the word `Haha`. In essence, we had successfully fine-tuned
    the *gpt-3.5-turbo* model to tell one-liner jokes given any prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are five steps that need to be followed when fine-tuning a model and
    then using that fine-tuned model:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prepare a training data file*: The training data consists of examples or prompts
    and desired completions. You need at least 10 examples to successfully train a
    model. Each example looks very similar (with purposeful intent) to the **messages**
    parameter when making a chat completion request. The difference, however, is that
    it also includes the completion (also known as the output from the assistant).
    Here is an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These examples can be added to a JSON file, with each line representing one
    example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Image of JSON file containing training data](img/B21007_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Image of JSON file containing training data
  prefs: []
  type: TYPE_NORMAL
- en: '*Import to OpenAI*: After the training file has been made, it needs to be uploaded
    to OpenAI’s servers, which is what we did with the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Assign an ID*: After uploading the file, the API assigns it an ID. This ID
    can be determined by looking at the response JSON from the preceding code, and
    parsing for the **id** parameter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Fine-tune the model*: After that, we need to instruct the API to fine-tune
    the model using the uploaded training data. We will put the response that we get
    from the API in a variable:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the fine-tuning job is complete, the API will assign a `fine_tuned_model`
    parameter, giving the fine-tuned model a particular identifier, which we can store
    in a variable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Use the fine-tuned model*: The last step is fairly easy – call the Chat Completions
    API as normal but modify the **model** parameter to the newly fine-tuned model
    that was just created:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Benefits of fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuning improves few-shot learning by allowing you to train on many more
    examples than what would fit in a typical prompt context window. Once a model
    has been tuned, these examples are not needed every single time when making a
    completions request, thereby saving tokens (and costs) and resulting in lower
    latency (i.e., faster speed). Recall that tokens are the smallest units of meaning
    in a piece of text (typically words, punctuation marks, or other elements) used
    in NLP and often form the basis of how OpenAI charges chat completion requests.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s go through the number of tokens for two models (i) one that
    uses the *gpt-3.5* base model without any fine-tuning, but we need to include
    examples in the prompt every time, and (ii) a fine-tuned model.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Gpt-3.5** | **Gpt-3.5 that has** **been fine-tuned** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Prompt** | You are an assistant who creates funny one-line jokes based
    on a given scenario. Here are 10 examples:A knight getting ready for a battle
     Why was the knight always calm before battle? Because he was good at keeping
    his “armor” cool! Haha… [9 more examples] …Scenario: A penguin in the Arctic |
    You are an assistant who creates funny one-line jokes based on a given scenario.Scenario:
    A penguin in the Arctic |'
  prefs: []
  type: TYPE_TB
- en: '| **Number of tokens in** **prompt (estimated)** | 400 | 36 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.3 – Comparison of prompt examples and number of tokens between a non-fine-tuned
    and a fine-tuned GPT-3.5 model
  prefs: []
  type: TYPE_NORMAL
- en: A fine-tuned model uses about one-tenth of the number of tokens as the few-shot
    base model. This means that using a fine-tuned model can result in 90% cost savings,
    which can be very high if you deploy these models to heavily used applications.
  prefs: []
  type: TYPE_NORMAL
- en: Other benefits of fine-tuning include higher-quality results by being able to
    train on thousands of examples, which is not possible using few-shot learning
    as there is a maximum length for the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a model to get better-quality results should only be done after
    sufficient attempts at prompt engineering and prompt chaining have been made.
    Fine-tuning a model requires significant resources and effort, so it’s more efficient
    to first exhaust the potential of prompt engineering and prompt chaining, which
    can often achieve desired results without additional training. **Prompt engineering**
    refers to creating more detailed and structured prompts to yield better completions.
    **Prompt chaining** is the idea of breaking down more complex prompts into simpler
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When would you resort to fine-tuning a model rather than (i) using the base
    *gpt-3.5* or *gpt4* model, or (ii) using few-shot learning to prime the model
    instead? In general, here are some of the common use cases where you would need
    to fine-tune the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Enhancement of desired outputs*: Fine-tuning is crucial when there’s a need
    for more reliability in generating specific types of responses. By training the
    model on a specialized dataset, you can increase the chances that it will produce
    the desired output consistently. This is common in content creation for a particular
    brand voice, creating educational resources that must follow a specific language,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Complex prompt compliance*: In instances where the model consistently fails
    to adhere to complex prompts or instructions, fine-tuning can help correct these
    shortcomings. This ensures that the model better understands and follows detailed
    or multifaceted instructions. This is very common when creating programming assistants,
    for example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Specialized style and tone adjustments*: When a certain style, tone, or format
    is required – for example, legal language, a comedic tone, or a journalistic style
    – fine-tuning adjusts the model to capture these qualitative aspects more accurately.
    This is common when developing *customer service bots* – where the bots need to
    maintain a kind but firm tone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Custom task performance*: For teaching the model a new skill or task that
    is difficult to convey through a prompt alone, fine-tuning allows the model to
    learn from examples. This is particularly useful for niche applications or innovative
    tasks that the base model may not have been exposed to during its initial training,
    or more complex tasks such as *dictating a* *medical diagnosis*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, fine-tuning a model is a great, cost-efficient way to get higher-quality,
    consistent results. This is especially useful if you intend to build applications
    where similar prompts and responses are expected, and where a particular tone
    and style is required.
  prefs: []
  type: TYPE_NORMAL
