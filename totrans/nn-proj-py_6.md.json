["```py\n$ git clone https://github.com/PacktPublishing/Neural-Network-Projects-with-Python.git\n```", "```py\n$ cd Neural-Network-Projects-with-Python\n```", "```py\n$ conda env create -f environment.yml\n```", "```py\n$ conda activate neural-network-projects-python\n```", "```py\n$ cd Chapter06\n```", "```py\n$ python lstm.py\n```", "```py\nfrom keras.datasets import imdb\ntraining_set, testing_set = imdb.load_data(index_from = 3)\nX_train, y_train = training_set\nX_test, y_test = testing_set\n```", "```py\nprint(X_train[0])\n```", "```py\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n```", "```py\nword_to_id = imdb.get_word_index()\nword_to_id = {key:(value+3) for key,value in word_to_id.items()}\nword_to_id[\"<PAD>\"] = 0\nword_to_id[\"<START>\"] = 1\nid_to_word = {value:key for key,value in word_to_id.items()}\n```", "```py\nprint(' '.join(id_to_word[id] for id in X_train[159] ))\n```", "```py\n<START> a rating of 1 does not begin to express how dull depressing and relentlessly bad this movie is\n```", "```py\nprint(y_train[159])\n```", "```py\n0\n```", "```py\nprint(' '.join(id_to_word[id] for id in X_train[6]))\n```", "```py\n<START> lavish production values and solid performances in this straightforward adaption of jane austen's satirical classic about the marriage game within and between the classes in provincial 18th century england northam and paltrow are a salutory mixture as friends who must pass through jealousies and lies to discover that they love each other good humor is a sustaining virtue which goes a long way towards explaining the accessability of the aged source material which has been toned down a bit in its harsh scepticism i liked the look of the film and how shots were set up and i thought it didn't rely too much on successions of head shots like most other films of the 80s and 90s do very good results\n```", "```py\nprint(y_train[6])\n```", "```py\n1\n```", "```py\nfrom keras.datasets import imdb\n```", "```py\ntraining_set, testing_set = imdb.load_data(num_words = 10000)\nX_train, y_train = training_set\nX_test, y_test = testing_set\n```", "```py\nprint(\"Number of training samples = {}\".format(X_train.shape[0]))\nprint(\"Number of testing samples = {}\".format(X_test.shape[0]))\n```", "```py\nfrom keras.preprocessing import sequence\n```", "```py\nX_train_padded = sequence.pad_sequences(X_train, maxlen= 100)\nX_test_padded = sequence.pad_sequences(X_test, maxlen= 100)\n```", "```py\nprint(\"X_train vector shape = {}\".format(X_train_padded.shape))\nprint(\"X_test vector shape = {}\".format(X_test_padded.shape))\n```", "```py\nfrom keras.models import Sequential\nmodel = Sequential()\n```", "```py\nfrom keras.layers import Embedding\n```", "```py\nmodel.add(Embedding(input_dim = 10000, output_dim = 128))\n```", "```py\nfrom keras.layers import LSTM\n```", "```py\nmodel.add(LSTM(units=128))\n```", "```py\nfrom keras.layers import Dense\nmodel.add(Dense(units=1, activation='sigmoid'))\n```", "```py\nmodel.summary()\n```", "```py\n# try the SGD optimizer first\nOptimizer = 'SGD'\n\nmodel.compile(loss='binary_crossentropy', optimizer = Optimizer)\n```", "```py\nscores = model.fit(x=X_train_padded, y=y_train,\n                   batch_size = 128, epochs=10, \n                   validation_data=(X_test_padded, y_test))\n```", "```py\ndef train_model(Optimizer, X_train, y_train, X_val, y_val):\n    model = Sequential()\n    model.add(Embedding(input_dim = 10000, output_dim = 128))\n    model.add(LSTM(units=128))\n    model.add(Dense(units=1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer = Optimizer, \n                  metrics=['accuracy'])\n    scores = model.fit(X_train, y_train, batch_size=128, \n                       epochs=10, \n                       validation_data=(X_val, y_val), \n                       verbose=0)\n    return scores, model\n```", "```py\nSGD_score, SGD_model = train_model(Optimizer = 'sgd',\n                                   X_train=X_train_padded, \n                                   y_train=y_train, \n                                   X_val=X_test_padded,\n                                   y_val=y_test)\n\nRMSprop_score, RMSprop_model = train_model(Optimizer = 'RMSprop',\n                                           X_train=X_train_padded,\n                                           y_train=y_train,\n                                           X_val=X_test_padded,\n                                           y_val=y_test)\n\nAdam_score, Adam_model = train_model(Optimizer = 'adam',\n                                     X_train=X_train_padded,\n                                     y_train=y_train,\n                                     X_val=X_test_padded,\n                                     y_val=y_test)\n```", "```py\nfrom matplotlib import pyplot as plt\n\nplt.plot(range(1,11), SGD_score.history['acc'], label='Training Accuracy')\nplt.plot(range(1,11), SGD_score.history['val_acc'], \n         label='Validation Accuracy')\nplt.axis([1, 10, 0, 1])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Train and Validation Accuracy using SGD Optimizer')\nplt.legend()\nplt.show()\n```", "```py\nplt.plot(range(1,11), RMSprop_score.history['acc'], \n         label='Training Accuracy')\nplt.plot(range(1,11), RMSprop_score.history['val_acc'], \n         label='Validation Accuracy')\nplt.axis([1, 10, 0, 1])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Train and Validation Accuracy using RMSprop Optimizer')\nplt.legend()\nplt.show()\n```", "```py\nplt.plot(range(1,11), Adam_score.history['acc'], label='Training Accuracy')\nplt.plot(range(1,11), Adam_score.history['val_acc'], \n         label='Validation Accuracy')\nplt.axis([1, 10, 0, 1])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Train and Validation Accuracy using Adam Optimizer')\nplt.legend()\nplt.show()\n```", "```py\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nplt.figure(figsize=(10,7))\nsns.set(font_scale=2)\ny_test_pred = RMSprop_model.predict_classes(X_test_padded)\nc_matrix = confusion_matrix(y_test, y_test_pred)\nax = sns.heatmap(c_matrix, annot=True, xticklabels=['Negative Sentiment', \n                 'Positive Sentiment'], yticklabels=['Negative Sentiment', \n                 'Positive Sentiment'], cbar=False, cmap='Blues', fmt='g')\nax.set_xlabel(\"Prediction\")\nax.set_ylabel(\"Actual\")\n```", "```py\nfalse_negatives = []\nfalse_positives = []\n\nfor i in range(len(y_test_pred)):\n    if y_test_pred[i][0] != y_test[i]:\n        if y_test[i] == 0: # False Positive\n            false_positives.append(i)\n        else:\n            false_negatives.append(i)\n```", "```py\n\"The sweet is never as sweet without the sour\". This quote was essentially the theme for the movie in my opinion ..... It is a movie that really makes you step back and look at your life and how you live it. You cannot really appreciate the better things in life (the sweet) like love until you have experienced the bad (the sour). ..... Only complaint is that the movie gets very twisted at points and is hard to really understand...... I recommend you watch it and see for yourself.\n```", "```py\nI hate reading reviews that say something like 'don't waste your time this film stinks on ice'. It does to that reviewer yet for me it may have some sort of na√Øve charm ..... This film is not as good in my opinion as any of the earlier series entries ... But the acting is good and so is the lighting and the dialog. It's just lacking in energy and you'll likely figure out exactly what's going on and how it's all going to come out in the end not more than a quarter of the way through ..... But still I'll recommend this one for at least a single viewing. I've watched it at least twice myself and got a reasonable amount of enjoyment out of it both times\n```", "```py\nI just don't understand why this movie is getting beat up in here jeez. It is mindless, it isn't polished ..... I just don't get it. The jokes work on more then one level. If you didn't get it, I know what level you're at.\n```", "```py\nfrom keras.datasets import imdb\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# Import IMDB dataset\ntraining_set, testing_set = imdb.load_data(num_words = 10000)\nX_train, y_train = training_set\nX_test, y_test = testing_set\n\nprint(\"Number of training samples = {}\".format(X_train.shape[0]))\nprint(\"Number of testing samples = {}\".format(X_test.shape[0]))\n\n# Zero-Padding\nX_train_padded = sequence.pad_sequences(X_train, maxlen= 100)\nX_test_padded = sequence.pad_sequences(X_test, maxlen= 100)\n\nprint(\"X_train vector shape = {}\".format(X_train_padded.shape))\nprint(\"X_test vector shape = {}\".format(X_test_padded.shape))\n\n# Model Building\ndef train_model(Optimizer, X_train, y_train, X_val, y_val):\n    model = Sequential()\n    model.add(Embedding(input_dim = 10000, output_dim = 128))\n    model.add(LSTM(units=128))\n    model.add(Dense(units=1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer = Optimizer, \n                  metrics=['accuracy'])\n    scores = model.fit(X_train, y_train, batch_size=128, epochs=10, \n                       validation_data=(X_val, y_val))\n    return scores, model\n\n# Train Model\nRMSprop_score, RMSprop_model = train_model(Optimizer = 'RMSprop', X_train=X_train_padded, y_train=y_train, X_val=X_test_padded, y_val=y_test)\n\n# Plot accuracy per epoch\nplt.plot(range(1,11), RMSprop_score.history['acc'], \n         label='Training Accuracy') \nplt.plot(range(1,11), RMSprop_score.history['val_acc'], \n         label='Validation Accuracy')\nplt.axis([1, 10, 0, 1])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Train and Validation Accuracy using RMSprop Optimizer')\nplt.legend()\nplt.show()\n\n# Plot confusion matrix\ny_test_pred = RMSprop_model.predict_classes(X_test_padded) \nc_matrix = confusion_matrix(y_test, y_test_pred)\nax = sns.heatmap(c_matrix, annot=True, xticklabels=['Negative Sentiment', \n                'Positive Sentiment'], yticklabels=['Negative Sentiment', \n                'Positive Sentiment'], cbar=False, cmap='Blues', fmt='g')\nax.set_xlabel(\"Prediction\")\nax.set_ylabel(\"Actual\")\nplt.show()\n```"]