["```py\nbrew install cmake openmpi\n```", "```py\ncd ~mkdir .mujococd .mujococurl https://www.roboti.us/download/mjpro150_osx.zip\n```", "```py\ncurl https://www.roboti.us/getid/getid_osx\nsudo ./getid_osx\n```", "```py\nwget https://www.roboti.us/getid/getid_linux\nsudo ./getid_linux\n```", "```py\n~/.mujoco/mjkey.txt\n```", "```py\npip install mujoco-py\n```", "```py\nclass ActorNetwork:\n\n    def __init__(self, input_state, output_dim, hidden_layers, activation=tf.nn.relu):\n\n        self.x = input_state\n        self.output_dim = output_dim\n        self.hidden_layers = hidden_layers\n        self.activation = activation\n\n        with tf.variable_scope('actor_network'):\n            self.output = self._build()\n            self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                                          tf.get_variable_scope().name)\n\n    def _build(self):\n\n        layer = self.x\n        init_b = tf.constant_initializer(0.01)\n\n        for i, num_unit in enumerate(self.hidden_layers):\n            layer = dense(layer, num_unit, init_b=init_b, name='hidden_layer_{}'.format(i))\n\n        output = dense(layer, self.output_dim, activation=self.activation, init_b=init_b, name='output')\n        return output\n```", "```py\nclass CriticNetwork:\n\n    def __init__(self, input_state, input_action, hidden_layers):\n\n        assert len(hidden_layers) >= 2\n        self.input_state = input_state\n        self.input_action = input_action\n        self.hidden_layers = hidden_layers\n\n        with tf.variable_scope('critic_network'):\n            self.output = self._build()\n            self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                                          tf.get_variable_scope().name)\n\n    def _build(self):\n\n        layer = self.input_state\n        init_b = tf.constant_initializer(0.01)\n\n        for i, num_unit in enumerate(self.hidden_layers):\n            if i != 1:\n                layer = dense(layer, num_unit, init_b=init_b, name='hidden_layer_{}'.format(i))\n            else:\n                layer = tf.concat([layer, self.input_action], axis=1, name='concat_action')\n                layer = dense(layer, num_unit, init_b=init_b, name='hidden_layer_{}'.format(i))\n\n        output = dense(layer, 1, activation=None, init_b=init_b, name='output')\n        return tf.reshape(output, shape=(-1,))\n```", "```py\nclass ActorCriticNet:\n\n    def __init__(self, input_dim, action_dim, \n                 critic_layers, actor_layers, actor_activation, \n                 scope='ac_network'):\n\n        self.input_dim = input_dim\n        self.action_dim = action_dim\n        self.scope = scope\n\n        self.x = tf.placeholder(shape=(None, input_dim), dtype=tf.float32, name='x')\n        self.y = tf.placeholder(shape=(None,), dtype=tf.float32, name='y')\n\n        with tf.variable_scope(scope):\n            self.actor_network = ActorNetwork(self.x, action_dim, \n                                              hidden_layers=actor_layers, \n                                              activation=actor_activation)\n\n            self.critic_network = CriticNetwork(self.x, \n                                                self.actor_network.get_output_layer(),\n                                                hidden_layers=critic_layers)\n\n            self.vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \n                                          tf.get_variable_scope().name)\n            self._build()\n\n    def _build(self):\n\n        value = self.critic_network.get_output_layer()\n\n        actor_loss = -tf.reduce_mean(value)\n        self.actor_vars = self.actor_network.get_params()\n        self.actor_grad = tf.gradients(actor_loss, self.actor_vars)\n        tf.summary.scalar(\"actor_loss\", actor_loss, collections=['actor'])\n        self.actor_summary = tf.summary.merge_all('actor')\n\n        critic_loss = 0.5 * tf.reduce_mean(tf.square((value - self.y)))\n        self.critic_vars = self.critic_network.get_params()\n        self.critic_grad = tf.gradients(critic_loss, self.critic_vars)\n        tf.summary.scalar(\"critic_loss\", critic_loss, collections=['critic'])\n        self.critic_summary = tf.summary.merge_all('critic')\n```", "```py\nclass ActorCriticNet:\n\n    def get_action(self, sess, state):\n        return self.actor_network.get_action(sess, state)\n\n    def get_value(self, sess, state):\n        return self.critic_network.get_value(sess, state)\n\n    def get_action_value(self, sess, state, action):\n        return self.critic_network.get_action_value(sess, state, action)\n\n    def get_actor_feed_dict(self, state):\n        return {self.x: state}\n\n    def get_critic_feed_dict(self, state, action, target):\n        return {self.x: state, self.y: target, \n                self.critic_network.input_action: action}\n\n    def get_clone_op(self, network, tau=0.9):\n        update_ops = []\n        new_vars = {v.name.replace(network.scope, ''): v for v in network.vars}\n        for v in self.vars:\n            u = (1 - tau) * v + tau * new_vars[v.name.replace(self.scope, '')]\n            update_ops.append(tf.assign(v, u))\n        return update_ops\n```", "```py\nclass DPG:\n\n    def __init__(self, config, task, directory, callback=None, summary_writer=None):\n\n        self.task = task\n        self.directory = directory\n        self.callback = callback\n        self.summary_writer = summary_writer\n\n        self.config = config\n        self.batch_size = config['batch_size']\n        self.n_episode = config['num_episode']\n        self.capacity = config['capacity']\n        self.history_len = config['history_len']\n        self.epsilon_decay = config['epsilon_decay']\n        self.epsilon_min = config['epsilon_min']\n        self.time_between_two_copies = config['time_between_two_copies']\n        self.update_interval = config['update_interval']\n        self.tau = config['tau']\n\n        self.action_dim = task.get_action_dim()\n        self.state_dim = task.get_state_dim() * self.history_len\n        self.critic_layers = [50, 50]\n        self.actor_layers = [50, 50]\n        self.actor_activation = task.get_activation_fn()\n\n        self._init_modules()\n```", "```py\n    def _init_modules(self):\n        # Replay memory\n        self.replay_memory = ReplayMemory(history_len=self.history_len, \n                                          capacity=self.capacity)\n        # Actor critic network\n        self.ac_network = ActorCriticNet(input_dim=self.state_dim, \n                                         action_dim=self.action_dim, \n                                         critic_layers=self.critic_layers, \n                                         actor_layers=self.actor_layers, \n                                         actor_activation=self.actor_activation,\n                                         scope='ac_network')\n        # Target network\n        self.target_network = ActorCriticNet(input_dim=self.state_dim, \n                                             action_dim=self.action_dim, \n                                             critic_layers=self.critic_layers, \n                                             actor_layers=self.actor_layers, \n                                             actor_activation=self.actor_activation,\n                                             scope='target_network')\n        # Optimizer\n        self.optimizer = Optimizer(config=self.config, \n                                   ac_network=self.ac_network, \n                                   target_network=self.target_network, \n                                   replay_memory=self.replay_memory)\n        # Ops for updating target network\n        self.clone_op = self.target_network.get_clone_op(self.ac_network, tau=self.tau)\n        # For tensorboard\n        self.t_score = tf.placeholder(dtype=tf.float32, shape=[], name='new_score')\n        tf.summary.scalar(\"score\", self.t_score, collections=['dpg'])\n        self.summary_op = tf.summary.merge_all('dpg')\n\n    def choose_action(self, sess, state, epsilon=0.1):\n        x = numpy.asarray(numpy.expand_dims(state, axis=0), dtype=numpy.float32)\n        action = self.ac_network.get_action(sess, x)[0]\n        return action + epsilon * numpy.random.randn(len(action))\n\n    def play(self, action):\n        r, new_state, termination = self.task.play_action(action)\n        return r, new_state, termination\n\n    def update_target_network(self, sess):\n        sess.run(self.clone_op)\n```", "```py\n    def train(self, sess, saver=None):\n\n        num_of_trials = -1\n        for episode in range(self.n_episode):\n            frame = self.task.reset()\n            for _ in range(self.history_len+1):\n                self.replay_memory.add(frame, 0, 0, 0)\n\n            for _ in range(self.config['T']):\n                num_of_trials += 1\n                epsilon = self.epsilon_min + \\\n                          max(self.epsilon_decay - num_of_trials, 0) / \\\n                          self.epsilon_decay * (1 - self.epsilon_min)\n                if num_of_trials % self.update_interval == 0:\n                    self.optimizer.train_one_step(sess, num_of_trials, self.batch_size)\n\n                state = self.replay_memory.phi(frame)\n                action = self.choose_action(sess, state, epsilon) \n                r, new_frame, termination = self.play(action)\n                self.replay_memory.add(frame, action, r, termination)\n                frame = new_frame\n\n                if num_of_trials % self.time_between_two_copies == 0:\n                    self.update_target_network(sess)\n                    self.save(sess, saver)\n\n                if self.callback:\n                    self.callback()\n                if termination:\n                    score = self.task.get_total_reward()\n                    summary_str = sess.run(self.summary_op, feed_dict={self.t_score: score})\n                    self.summary_writer.add_summary(summary_str, num_of_trials)\n                    self.summary_writer.flush()\n                    break\n```", "```py\n    def evaluate(self, sess):\n\n        for episode in range(self.n_episode):\n            frame = self.task.reset()\n            for _ in range(self.history_len+1):\n                self.replay_memory.add(frame, 0, 0, 0)\n\n            for _ in range(self.config['T']):\n                print(\"episode {}, total reward {}\".format(episode, \n                                                           self.task.get_total_reward()))\n\n                state = self.replay_memory.phi(frame)\n                action = self.choose_action(sess, state, self.epsilon_min) \n                r, new_frame, termination = self.play(action)\n                self.replay_memory.add(frame, action, r, termination)\n                frame = new_frame\n\n                if self.callback:\n                    self.callback()\n                    if termination:\n                        break\n```"]