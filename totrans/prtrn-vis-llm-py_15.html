<html><head></head><body>
		<div id="_idContainer122">
			<h1 id="_idParaDest-202" class="chapter-number"><a id="_idTextAnchor229"/>15</h1>
			<h1 id="_idParaDest-203"><a id="_idTextAnchor230"/>Future Trends in Pretraining Foundation Models</h1>
			<p>In this chapter, we’ll close out the book by pointing to where trends are headed for all relevant topics presented in this book. We’ll explore trends in foundation model application development, like using LangChain to build interactive dialogue applications, along with techniques like retrieval augmented generation to reduce LLM hallucination. We’ll explore ways to use generative models to solve classification tasks, human-centered design, and other generative modalities like code, music, product documentation, powerpoints, and more! We’ll talk through AWS offerings like SageMaker JumpStart Foundation Models, Amazon Bedrock, Amazon Titan, and Amazon Code Whisperer, and top trends in the future of foundation models and <span class="No-Break">pretraining itself.</span></p>
			<p>In particular, we’ll dive into the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Techniques for building applications <span class="No-Break">for LLMs</span></li>
				<li>Generative modalities outside of vision <span class="No-Break">and language</span></li>
				<li>AWS offerings in <span class="No-Break">foundation models</span></li>
				<li>The future of <span class="No-Break">foundation models</span></li>
				<li>The future <span class="No-Break">of pretraining</span></li>
			</ul>
			<h1 id="_idParaDest-204"><a id="_idTextAnchor231"/>Techniques for building applications for LLMs</h1>
			<p>Now that you’ve learned<a id="_idIndexMarker795"/> about foundation models, and especially large language models, let’s talk through a few key ways you can use them to build applications. One of the most significant takeaways of the ChatGPT moment in December 2022 is that customers clearly love for their chat to be knowledgeable about every moment in the conversation, remember topics mentioned earlier and encompassing all the twists and turns of dialogue. Said another way, beyond generic question answering, there’s a clear consumer preference for chat to be <em class="italic">chained</em>. Let’s take a look at an example in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B18942_Figure_15_01.jpg" alt="Figure 15.1 – Chaining questions for chat applications"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.1 – Chaining questions for chat applications</p>
			<p>The key difference between the left- and the right-hand side of <span class="No-Break"><em class="italic">Figure 15</em></span><em class="italic">.1</em> is that on the left-hand side, the answers are <strong class="bold">discontinuous</strong>. That means the model simply sees each question as a single entity before providing its response. On the right-hand side, however, the answers are <strong class="bold">continuous</strong>. That means the entire dialogue is provided to the model, with the newest question at the bottom. This helps to ensure the continuity of responses, with the model more capable of maintaining <span class="No-Break">the context.</span></p>
			<p>How can you set this up yourself? Well, on the one hand, what I’ve just described isn’t terribly difficult. Imagine just reading from your HTML page, packing in all of that call and response data into the prompt, and siphoning out the response to return it to your end user. If you don’t want to build<a id="_idIndexMarker796"/> it yourself, however, you can just use a few great open <span class="No-Break">source options!</span></p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor232"/>Building interactive dialogue apps with open-source stacks</h2>
			<p>If you haven’t seen<a id="_idIndexMarker797"/> it before, let me quickly introduce<a id="_idIndexMarker798"/> you to LangChain. Available for free on GitHub here: <a href="https://github.com/hwchase17/langchain">https://github.com/hwchase17/langchain</a>, LangChain is an open source toolkit built by Harrison Chase and more than 600 other contributors. It provides functionality similar to the famous ChatGPT by pointing to OpenAI’s API, or any other foundation model, but letting you as the developer and data scientist create your own frontend and <span class="No-Break">customer experience.</span></p>
			<p>Decoupling the application from the model is a smart move; in the last few months alone the world has seen nothing short of hundreds of new large language models come online, with teams around<a id="_idIndexMarker799"/> the world actively developing<a id="_idIndexMarker800"/> more. When your application interacts with the model via a single API call, then you can more easily move from one model to the next as the licensing, pricing, and capabilities upgrade over time. This is a big plus <span class="No-Break">for you!</span></p>
			<p>Another interesting open-source technology here is Haystack (<em class="italic">26</em>). Developed by the German start-up, Deepset, Haystack is a useful tool for, well, finding a needle in a haystack. Specifically they operate like a interface for you to bring your own LLMs into expansive question/answering scenarios. This was their original area of expertise, and since then have expanded quite <span class="No-Break">a bit!</span></p>
			<p>At AWS, we have an open source template for building applications with LangChain on AWS. It’s available on GitHub here: <a href="https://github.com/3coins/langchain-aws-template">https://github.com/3coins/langchain-aws-template</a>. In the following diagram, you can see a quick representation of <span class="No-Break">the architecture:</span></p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B18942_Figure_15_02.jpg" alt="Figure 15.2 – ﻿Hosting LangChain on AWS"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.2 – Hosting LangChain on AWS</p>
			<p>While this can point to any frontend, we provide an example template you can use to get off the ground for your app. You can also easily point to <em class="italic">any</em> custom model, whether it’s on a SageMaker endpoint or in the new AWS service, Bedrock! More on that a bit later in this chapter. As you can see in the previous image, in this template you can easily run a UI anywhere that interacts with the cloud. Let’s take a look at all of <span class="No-Break">the steps.:</span></p>
			<ol>
				<li>First, the UI hits the <span class="No-Break">API gateway.</span></li>
				<li>Second, credentials are retrieved <span class="No-Break">via IAM.</span></li>
				<li>Third, the service is invoked <span class="No-Break">via Lambda.</span></li>
				<li>Fourth, the model credentials are retrieved via <span class="No-Break">Secrets Manager.</span></li>
				<li>Fifth, your model is invoked through either an API call to a serverless model SDK, or a custom model you’ve trained that is hosted on a SageMaker endpoint <span class="No-Break">is invoked.</span></li>
				<li>Sixth, look up the relevant conversation history in DynamoDB to ensure your answer <span class="No-Break">is accurate.</span></li>
			</ol>
			<p>How does this chat interface<a id="_idIndexMarker801"/> ensure it’s not hallucinating<a id="_idIndexMarker802"/> answers? How does it point to a set of data stored in a database? Through <strong class="bold">retrieval augmented generation</strong> (<strong class="bold">RAG</strong>), which we will <span class="No-Break">cover next.</span></p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor233"/>Using RAG to ensure high accuracy in LLM applications</h2>
			<p>As explained<a id="_idIndexMarker803"/> in the original 2020 <em class="italic">(1)</em> paper, RAG<a id="_idIndexMarker804"/> is a way to retrieve documents relevant to a given query. Imagine your chat application takes in a question about a specific item in your database, such as one of your products. Rather than having the model make up the answer, you’d be better off retrieving the right document from your database and simply using the LLM to <em class="italic">stylize</em> the response. That’s where RAG is so powerful; you can use it to ensure the accuracy of your generated answers stays high, while keeping the customer experience consistent in both style and tone. Let’s take a <span class="No-Break">closer look:</span></p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B18942_Figure_15_03.jpg" alt="Figure 15.3 – RAG"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.3 – RAG</p>
			<p>First, a question comes in from the left-hand side. In the top left, you can see a simple question, <strong class="bold">Define “middle ear”</strong>. This is processed by a query encoder, which is simply a language model<a id="_idIndexMarker805"/> producing an embedding<a id="_idIndexMarker806"/> of the query. This embedding is then applied to the index of a database, with many candidate algorithms in use here: <strong class="bold">K</strong> <strong class="bold">Nearest Neighbors</strong>, <strong class="bold">Maximum Inner Product Search</strong> (<strong class="bold">MIPS</strong>), and others. Once you’ve retrieved a set of similar documents, you can feed the best ones into the generator, the final model on the right-hand side. This takes the input documents and returns a simple answer to the question. Here, the answer is <strong class="bold">The middle ear includes the tympanic cavity and the </strong><span class="No-Break"><strong class="bold">three ossicles.</strong></span></p>
			<p>Interestingly, however, the LLM here doesn’t really define what the middle ear is. It’s actually answering the question, “what objects are contained within the middle ear?” Arguably, any definition of the middle ear would include its purpose, notably serving as a buffer between your ear canal<a id="_idIndexMarker807"/> and your inner ear, which helps you keep your balance and lets you hear. So, this would be a good candidate for expert <strong class="bold">reinforcement learning with human feedback</strong>, or <span class="No-Break"><strong class="bold">RLHF</strong></span><span class="No-Break">, optimization.</span></p>
			<p>As shown in <span class="No-Break"><em class="italic">Figure 15</em></span><em class="italic">.3</em>, this entire RAG<a id="_idIndexMarker808"/> system is tunable. That means you can and should fine-tune the encoder<a id="_idIndexMarker809"/> and decoder aspects of the architecture to dial in model performance based on your datasets and query types. Another way to classify documents, as we’ll see, <span class="No-Break">is generation!</span></p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor234"/>Is generation the new classification?</h2>
			<p>As we learned in <a href="B18942_13.xhtml#_idTextAnchor198"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, <em class="italic">Prompt Engineering</em>, there are many<a id="_idIndexMarker810"/> ways you can push your language<a id="_idIndexMarker811"/> model to output the type of response you are looking for. One of these ways is actually to have it classify what it sees in the text! Here is a simple diagram to illustrate <span class="No-Break">this concept:</span></p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B18942_Figure_15_04.jpg" alt="Figure 15.4 – Using generation in place of classification"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 15.4 – Using generation in place of classification</p>
			<p>As you can see in the diagram, with traditional classification<a id="_idIndexMarker812"/> you train the model ahead of time to perform one task: <strong class="bold">classification</strong>. This model may do well on classification, but it won’t be able to handle new tasks at all. This key drawback is one of the main reasons why foundation models, and especially large language models, are now so popular: they are extremely flexible and can handle many different tasks without needing to <span class="No-Break">be retrained.</span></p>
			<p>On the right-hand side of <span class="No-Break"><em class="italic">Figure 15</em></span><em class="italic">.4</em>, you can see we’re using the same text as the starting point, but instead of passing it to an encoder-based text model, we’re passing it to a decoder-based model and simply adding the instruction <strong class="bold">classify this sentence into positive or negative sentiment</strong>. You could just as easily say, “tell me more about how this customer really feels,” or “how optimistic is this home buyer?” or “help this homebuyer find a different house that meets their needs.” Arguably each of those three instructions is slightly different, veering away from pure classification and into more general application development or customer experience. Expect to see more of this over time! Let’s look at one more key technique<a id="_idIndexMarker813"/> for building applications<a id="_idIndexMarker814"/> with LLMs: keeping humans in <span class="No-Break">the loop.</span></p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor235"/>Human-centered design for building applications with LLMs</h2>
			<p>We touched<a id="_idIndexMarker815"/> on this topic<a id="_idIndexMarker816"/> previously, in <a href="B18942_02.xhtml#_idTextAnchor034"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><em class="italic">,</em> <em class="italic">Dataset Preparation: Part One</em>, <a href="B18942_10.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><em class="italic">,</em> <em class="italic">Fine-Tuning and Evaluating</em>, <a href="B18942_11.xhtml#_idTextAnchor167"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><em class="italic">,</em> <em class="italic">Detecting, Mitigating, and Monitoring Bias</em>, and <a href="B18942_14.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 14</em></span></a><em class="italic">,</em> <em class="italic">MLOps for Vision and Language</em>. Let me say this yet again; I believe that human labeling will become even more of a competitive advantage that companies can provide. Why? Building LLMs is now incredibly competitive; you have both the open source and proprietary sides actively competing for your business. Open source options are from the likes of Hugging Face and Stability, while proprietary offerings are from AI21, Anthropic, and OpenAI. The differences between these options are questionable; you can look up the latest models at the top of the leaderboard from <em class="italic">Stanford’s HELM</em> <em class="italic">(2)</em>, which incidentally falls under their human-centered AI initiative. With enough fine-tuning and customization, you should generally be able to <span class="No-Break">meet performance.</span></p>
			<p>What then determines the best LLM applications, if it’s not the foundation model? Obviously, the end-to-end customer experience is critical, and will always remain so. Consumer preferences wax and wane over time, but a few tenets remain for general technology: speed, simplicity, flexibility, and low cost. With foundation models we can clearly see that customers prefer explainability and models they can trust. This means that application designers and developers should grapple with these long-term consumer preferences, picking solutions and systems that maximize them. As you may have guessed, that alone is no <span class="No-Break">small task.</span></p>
			<p>Beyond the core skill of designing and building successful applications, what else can we do to stay competitive in this brave new world of LLMs? I would argue that amounts to customizing your data. Focus on making your data and your datasets unique: singular in purpose, breadth, depth, and completeness. Lean into labeling your data with the best resources you can, and keep that a core<a id="_idIndexMarker817"/> part of your entire application workflow. This brings you to <strong class="bold">continuous learning</strong>, or the ability of the model to constantly get better and better based on signals<a id="_idIndexMarker818"/> from your<a id="_idIndexMarker819"/> <span class="No-Break">end users.</span></p>
			<p>Next, let’s take a look at upcoming <span class="No-Break">generative modalities.</span></p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor236"/>Other generative modalities</h1>
			<p>Since the 2022 ChatGPT moment, most of the technical<a id="_idIndexMarker820"/> world has been fascinated by the proposition of <em class="italic">generating novel content</em>. While this was always somewhat interesting, the meeting of high-performance foundation models with an abundance of media euphoria over the capabilities, combined with a post-pandemic community with an extremely intense fear of missing out, has led us to the perfect storm of a global fixation on <span class="No-Break">generative AI.</span></p>
			<p>s this a good thing? Honestly, I’m happy to finally see the shift; I’ve been working on generating content with AI/ML models in some fashion since at least 2019, and as a writer and creative person myself, I’ve always thought this was the most interesting part of machine learning. I was very impressed by David Foster’s book <em class="italic">(3)</em> on the topic. He’s just published an updated version of this to include the latest foundation models and methods! Let’s quickly recap some other types of modalities that are common in generative AI <span class="No-Break">applications today.</span></p>
			<p class="callout-heading">Key modalities for generation outside of vision and language</p>
			<p class="callout">Here is my shortlist for the most interesting type of content generation outside of what we’ve seen throughout <span class="No-Break">the book:</span></p>
			<ul>
				<li><span class="No-Break">Generating code</span></li>
				<li><span class="No-Break">Generating music</span></li>
				<li>Generating PowerPoint slides, ads, and visuals </li>
				<li>Generating <span class="No-Break">product documentation</span></li>
				<li>Generating architectural designs, and then building <span class="No-Break">the application</span></li>
				<li>Generating movies, TV shows, <span class="No-Break">and entertainment</span></li>
				<li>Generating websites, games, and <span class="No-Break">mobile apps</span></li>
			</ul>
			<p>Generating code<a id="_idIndexMarker821"/> should be no surprise to most of you; its core similarities to language generation make it a perfect candidate! Fine-tuning an LLM to spit out code in your language of choice is pretty easy; here’s my 2019 project <em class="italic">(4)</em> doing exactly that with the SageMaker example notebooks! Is the code great? Absolutely not, but fortunately, LLMs have come a long way since then. Many modern code-generating models are excellent, and thanks to a collaboration between Hugging Face and ServiceNow we have an open source model<a id="_idIndexMarker822"/> to use! This is called <em class="italic">StarCoder</em> and is available<a id="_idIndexMarker823"/> for free on Hugging Face right <span class="No-Break">here: </span><a href="https://huggingface.co/bigcode/starcoder"><span class="No-Break">https://huggingface.co/bigcode/starcoder</span></a><span class="No-Break">.</span></p>
			<p>What I love about using an open source LLM for code generation is that you can <em class="italic">customize it</em>! This means you can point to your own private code repositories, tokenize the data, update the model, and immediately train this LLM to generate code in the style of your organization! At the organizational level you might even do some continued pretraining on a open-source LLM for code generation on your own repositories to speed up all of your developers. We’ll take a look at more ways you can use LLMs to write your own code faster in the next section when we focus on AWS offerings, especially <strong class="bold">Amazon Code </strong><span class="No-Break"><strong class="bold">Whisperer</strong></span><span class="No-Break">. (</span><span class="No-Break"><em class="italic">27</em></span><span class="No-Break">)</span></p>
			<p>The rest of the preceding content can all be great candidates for your own generative AI projects. Truly, just as we saw general machine learning moving from the science lab into the foundation of most businesses and projects, it’s likely that generative capabilities in some fashion will do <span class="No-Break">the same.</span></p>
			<p>Does that mean engineering roles will be eliminated? Honestly, I doubt it. Just as the rise of great search engines didn’t eliminate software engineering roles but made them more fun and doable for a lot of people, I’m expecting generative capabilities to do the same. They are great at searching many possibilities and quickly finding great options, but it’s still up to you to know the ins and outs of your consumers, your product, and your design. Models aren’t great at critical thinking, but they are good at coming up with ideas and finding shortcomings, at least <span class="No-Break">in words.</span></p>
			<p>Now that we’ve looked at other generative modalities<a id="_idIndexMarker824"/> at a very high level, let’s learn about AWS offerings for <span class="No-Break">foundation models!</span></p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor237"/>AWS offerings in foundation models</h1>
			<p>On AWS, as you’ve seen throughout<a id="_idIndexMarker825"/> the book, you have literally hundreds of ways to optimize<a id="_idIndexMarker826"/> your foundation model development and operationalization. Let’s now look at a few ways AWS is explicitly investing to improve the customer experience in <span class="No-Break">this domain:</span></p>
			<ul>
				<li><strong class="bold">SageMaker JumpStart Foundation Model Hub</strong>: Announced in preview at re:Invent 2022, this is an option for pointing<a id="_idIndexMarker827"/> to foundation models nicely packaged in the SageMaker environment. This includes both open source models such as BLOOM and Flan-T5 from Hugging Face, and proprietary models such as AI21 Jurassic. A list of all the foundation models is available here <em class="italic">(5)</em>. To date, we have nearly 20 foundation models, all available for hosting in your own secure environments. Any data you use to interact with or fine-tune models on the Foundation Model Hub is not shared with providers. You can also optimize costs by selecting the instances yourself. We have tens of example notebooks pointing to these models for training<a id="_idIndexMarker828"/> and hosting across a wide variety of use cases available here <em class="italic">(6)</em> and elsewhere. For more information about the data the models were trained on, you can read about that in the <span class="No-Break">playground directly.</span></li>
				<li><strong class="bold">Amazon Bedrock</strong>: If you have been watching AWS news<a id="_idIndexMarker829"/> closely in early 2023, you may have noticed a new service we announced for foundation models: Amazon Bedrock! As discussed in this blog post <em class="italic">(7)</em> by Swami Sivasubramanian, Bedrock is a service that lets you interact with a variety of foundation models through a serverless interface that stays secure. Said another way, Bedrock provides a point of entry for multiple foundation models, letting you get the best of all possible providers. This includes AI start-ups such as AI21, Anthropic, and Stability. Interacting with Bedrock means invoking a serverless experience, saving you from dealing with the lower-level infrastructure. You can also fine-tune your models <span class="No-Break">with Bedrock!</span></li>
				<li><strong class="bold">Amazon Titan</strong>: Another model that will be available<a id="_idIndexMarker830"/> through Bedrock is <em class="italic">Titan</em>, a new large language model that’s fully trained and managed by Amazon! This means we handle the training data, optimizations, tuning, debiasing, and all enhancements for getting you results with large language models. Titan will also be available <span class="No-Break">for fine-tuning.</span></li>
			</ul>
			<p>Amazon Code Whisperer: As you may have seen, Code Whisperer is an AWS service announced in 2022 and made generally available in 2023. Interestingly it seems to tightly couple with a given development environment, taking the entire context of the script you are writing and generating recommendations based on this. You can write pseudo-code, markdown, or other function starts, and using keyboard shortcuts invoke the model. This will send you a variety of options based on the context of your script, letting you ultimately select the script that makes the most sense for you! Happily, this is now supported for both Jupyter notebooks and SageMaker Studio; you can read more about these initiatives from AWS Sr Principal Technologist Brain Granger, co-founder of Project Jupyter. Here’s Brian’s blog post on the topic: <a href="https://aws.amazon.com/blogs/machine-learning/announcing-new-jupyter-contributions-by-aws-to-democratize-generative-ai-and-scale-ml-workloads/">https://aws.amazon.com/blogs/machine-learning/announcing-new-jupyter-contributions-by-aws-to-democratize-generative-ai-and-scale-ml-workloads/</a> Pro tip: Code Whisperer is free to individuals! Close readers of Swami’s blog post above will also notice updates to our latest ML infrastructure, like the second edition of the <em class="italic">inferentia chip</em>, <em class="italic">inf2</em>, and a trainium instance with more <span class="No-Break">bandwidth, </span><span class="No-Break"><em class="italic">trn1n</em></span><span class="No-Break">.</span></p>
			<p>Close readers of Swami’s blog post will also notice updates to our latest ML infrastructure, such as the second<a id="_idIndexMarker831"/> edition of the <em class="italic">inferentia chip</em>, <em class="italic">inf2</em>, and a Trainium instance with more<a id="_idIndexMarker832"/> bandwidth, <em class="italic">trn1n</em>. We also released our code generation service, <em class="italic">CodeWhisperer</em>, at no cost <span class="No-Break">to you!</span></p>
			<p>Now that we’ve learned about some of the AWS offerings in this space, let’s hypothesize about the future of <span class="No-Break">foundation models.</span></p>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor238"/>The future of foundation models</h1>
			<p>To me, a few key points<a id="_idIndexMarker833"/> seem incredibly obvious for where foundation models <span class="No-Break">are trending:</span></p>
			<ul>
				<li><em class="italic">Intense competition will continue between open source and proprietary model providers</em>. As mentioned previously, right now we are in a perfect storm of hyper-focus on foundation models from most of the technology industry worldwide. A key axis here is proprietary versus open source. As suggested by this leaked Google document on May 4 <em class="italic">(8)</em>, the capabilities of the open source world are advancing and in many cases, open source options are better than proprietary ones. They actually describe open source models as “pound-for-pound more capable.” This means that for the size of the model itself, the smaller ones produced by the open source world are better in a <span class="No-Break">per-byte-size comparison.</span></li>
				<li><em class="italic">Model consumers will get more options at a lower cost if they are flexible about model providers</em>. To me, the result of this intense competition is clear; you’ll get more and more options at a lower price point as time goes by! This means the clear right choice here, as a model consumer, is not getting locked into a single model. As the brave new world of foundation models swims on, put you and your teams in the best possible scenario and stay flexible to new models as <span class="No-Break">they emerge.</span></li>
				<li><em class="italic">Models are getting smaller and stronger</em>. In no small part thanks to the drive enthusiasm in the open-source community coming from Stable Diffusion, foundation models are now decreasing in size but increasing in accuracy. This means today you can accomplish with a 13B-parameter model, like <em class="italic">StableVicuna</em> (<em class="italic">28</em>) what a few years ago took a 175B-parameter model to do. Keep this in mind as you’re <span class="No-Break">designing apps!</span></li>
				<li><em class="italic">Make proprietary data and human feedback your strength</em>. To best take advantage of all this competition, I’d suggest leaning into your data. As long as you’re open to the latest and greatest model backbone, using your own proprietary data and human feedback as a key investment area lets you differentiate from the rest of the market. Make sure you and your teams are labeling data early and often, using as much of your unique perspectives and expertise <span class="No-Break">as possible.</span></li>
				<li><em class="italic">Security is critical for interacting with foundation models</em>. This signal shows up so clearly across the market, consumers very strongly prefer environments that secure their data and do not allow sharing with model providers, including both model queries and fine-tuning assets. This is in some sense a derivative of making proprietary data and human feedback your strength; in tomorrow’s world, your data becomes your selling point. Protecting this in the form of your own foundation model, in terms of keeping your model unbiased, safe from jailbreak attacks, and not able to generate hate speech will continue to be important. It seems like an entirely new portfolio of security measures is necessary to ensure foundation <span class="No-Break">model applications.</span></li>
				<li><em class="italic">Foundation models are becoming the new database, and natural language is the new programming language</em>. I am expecting most applications in the next twelve months to include foundation models as a new type of database. Rather than just storing your records on disk, now you can crunch through them with neural networks, learn mappings and relationships at scales that single humans can’t compete with, and interact with humans in natural language. Optimizing, scaling, debiasing, ensuring accuracy, and taking costs out of this equation will be the work of the next <span class="No-Break">few years.</span></li>
			</ul>
			<p>I’m also expecting many more foundation models, across modalities, user bases, languages, technologies, goals, and domains. As the cost of training and producing them goes down, thanks<a id="_idIndexMarker834"/> to intense competition, we might see more and more entry-level developers entering this market. With that in mind, let’s close out the book with parting thoughts on the future <span class="No-Break">of pretraining.</span></p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor239"/>The future of pretraining</h1>
			<p>In a full roundabout, let’s take<a id="_idIndexMarker835"/> a look at some skeptical trends that provide a sense of caution and critical evaluation of the recent goldrush in foundation models. One of the most prominent examples is Geoffrey Hinton quitting Google (<em class="italic">29</em>) to warn about the dangers of AI development, calling for a 6-month moratorium (<em class="italic">30</em>) on all foundation <span class="No-Break">model research.</span></p>
			<p>Personally, I think this pause fails to recognize what drove the hype in the first place: the invisible hand of AI economics. Landing high-paying tech jobs is not easy; getting and maintaining visas for the US and similar countries is non-trivial. Growing your career in an industry with some of the most intelligent and driven people, with ground-shattering fast-paced changes is anything but simple. Foundation models and all improvements in technology evolve because humans need to prove themselves, grow their careers, and provide for their families. Asking to pause experiments on large-scale training is almost the same as asking massive swaths of young people to stop putting all of their passion, skills, and time into some of the best possible options for their own career development. Obviously, that’s the opposite of what's in their best interest to do! </p>
			<p>However, even within the camp of those who do support continued research on foundation models, not everyone is overly optimistic about the continued value of pretraining Transformers. Yann LeCun, the same scientist we mentioned in <a href="B18942_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> who developed MNIST, considers self-supervised learning to be “<em class="italic">the dark matter of intelligence</em>. (<em class="italic">9</em>)” Since the viral ChatGPT global moment LeCun has been very critical about the bounded performance of large language and auto-regressive models, given their core strength is simply predicting the next most likely token in a sequence and not actually understanding the world in any reliable way. Instead, he suggests we build AI models that learn how to reason, including developing hierarchical representations of action plans. </p>
			<p>He’s not alone in this cautionary note. MacArthur Fellow at the University of Washington and the Allen Institute for Artificial Intelligence Yejin Choi <em class="italic">(10)</em> recently shared her thoughts in a TED talk about why “AI is incredibly smart – and shockingly stupid.” <em class="italic">(11)</em> Choi’s multiple decades of work on common sense reasoning shows that while NLP models may solve some limited tasks well, they still struggle with extremely basic human tasks, such as understanding the difference between reality and fiction or hypothesizing and planning, the value of simplicity, and basic logical mapping of the world <span class="No-Break">around them.</span></p>
			<p>To me, the divide here is clear. On the one hand, we have a decades-long push for the best and most impressive artificial representations of intelligence. On the other hand, we have massive industries of business, technology, and academia with millions of human beings actively trying to build applications and provide value that serve as the foundation for their careers and their team’s longevity. Will this economy of technological development create the most intelligent machines? Possibly. Will it build applications and businesses that provide value to consumers? Certainly. Will these two related and yet distinct vectors<a id="_idIndexMarker836"/> continue to overlap for decades to come? Without <span class="No-Break">a doubt.</span></p>
			<p>Before we close things out, let me mention a few interesting technical trends of note to pretraining <span class="No-Break">in particular:</span></p>
			<ul>
				<li><strong class="bold">Continuous pretraining</strong>: If your foundation model benefits<a id="_idIndexMarker837"/> from the latest data, and this is available in a constant stream of updates, why not build a continuous loop of ingestion and training to keep your application performant? That’s the core proposal in this 2022 paper <em class="italic">(12)</em>. I’d imagine that some applications benefit from this constant stream of training, especially when parameter-efficient fine-tuning (<em class="italic">31</em>) makes the cost of this <span class="No-Break">more appealing.</span></li>
				<li><strong class="bold">Retrieval pretraining</strong>: The demand for accurate generated text<a id="_idIndexMarker838"/> will continue increasing as generative AI applications expand. This approach, suggested by DeepMind, applies the retrieval process <em class="italic">(14)</em> to pretraining and gets similar performance to GPT-3 while using 25x fewer parameters, making it much more efficient and appealing for both training and hosting. I’m expecting this basic concept of retrieving tokens during pretraining to evolve with RAG <em class="italic">(15)</em> to create LLMs<a id="_idIndexMarker839"/> that provide much higher <span class="No-Break">accuracy guarantees.</span></li>
				<li><strong class="bold">More universal pretraining regimes</strong>: As you are probably aware, much of the media<a id="_idIndexMarker840"/> and open source attention seems to focus only on the models and new state-of-the-art performance in low-level tasks. I think this is a problem because it misses the foundation of where these models come from and how they are built: pretraining itself. This indicates that if pretraining can become more accessible and more universal, we can move from a fixation on the model to broad support for pretraining more generally. A few steps exist already in this direction, such as Data2Vec <em class="italic">(16)</em>, which proposes a general framework for self-supervised<a id="_idIndexMarker841"/> learning across vision, speech, and language. Another attempt is <strong class="bold">UL2</strong>, <strong class="bold">Unifying Language Learning Paradigms</strong>. This Google Brain team suggests combining diverse pretraining paradigms and then switching to different paradigms <span class="No-Break">during fine-tuning.</span></li>
				<li><strong class="bold">More languages, please</strong>! One of the first conferences I attended in person after the pandemic was the Association of Computational Linguists in 2022 <em class="italic">(18)</em>. I was happily surprised by their focus on being multi-lingual, strongly and admirably pushing for capabilities that bridge languages and extend NLP capabilities across endangered languages and communities worldwide. Admirably the UN declared 2022-2023 the International Decade of Indigenous Languages, estimating that at least 50% of spoken languages today will be extinct or seriously endangered by 2100. (<em class="italic">32</em>). This will continue to be an important topic in foundation models because it serves as a bottleneck for technical adoption and innovation. One step in this direction is Tsinghua’s GLM-130B <em class="italic">(19)</em>, a model explicitly pretrained with Chinese and English. Another notable bilingual model is Hugging Face’s BLOOM <em class="italic">(20), which was trained on 46 natural and 13 programming languages. </em>Other similar projects provide capabilities in singular non-English languages, such as LightOn’s French model PAGnol <em class="italic">(21)</em>, a Japanese masked-language model <em class="italic">(22)</em>, a German LLM <em class="italic">(23)</em>, and more. There are even calls for a BritGPT <em class="italic">(24)</em>, to bring generative capabilities into British styles<a id="_idIndexMarker842"/> of speaking <span class="No-Break">and conversing.</span></li>
			</ul>
			<p>On that note, let’s close out the book with the <span class="No-Break">final conclusion.</span></p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor240"/>Summary</h1>
			<p>What a journey! To those of you who made it to the end with me, thank you so much for the time, creativity, and energy you’ve put into studying my words and thoughts. I hope at least some of the insights were worth your time, and that the mistakes weren’t <span class="No-Break">too glaring.</span></p>
			<p>In this book, we walked through the entire process of pretraining foundation models, looking at key use cases and examples from vision and language, and understanding core capabilities on AWS to build your applications and projects. I love hearing from my audience, so please, reach out to me and stay in touch! I’m active on LinkedIn; you can always ping me with questions or comments. I run a weekly Twitch show on Generative AI, so you can always find there me and hop in with feedback or <span class="No-Break">comments. </span><span class="No-Break"><em class="italic">(25)</em></span></p>
			<p>And of course, you can always talk to your teams at AWS to reach out to me directly! I love meeting customers, thinking through architectural choices, communicating your needs to our service teams, and thinking big with you about how we can build a better tomorrow. Let me know what you’re itching <span class="No-Break">to build!</span></p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor241"/>References</h1>
			<p>Please go through the following content for more information on the topics covered in <span class="No-Break">the chapter:</span></p>
			<ol>
				<li>Retrieval-Augmented Generation for Knowledge-Intensive NLP <span class="No-Break">Tasks:</span><span class="No-Break"> </span><a href="https://arxiv.org/pdf/2005.11401.pdf"><span class="No-Break">https://arxiv.org/pdf/2005.11401.pdf</span></a></li>
				<li>HELM: <a href="https://crfm.stanford.edu/helm/latest/">https://crfm.stanford.edu/helm/latest/</a> </li>
				<li>Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play 1st <span class="No-Break">Edition: </span><a href="https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1492041947"><span class="No-Break">https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1492041947</span></a></li>
				<li><span class="No-Break">aws-samples/amazon-sagemaker-architecting-for-ml: </span><a href="https://github.com/aws-samples/amazon-sagemaker-architectingfor-ml/tree/master/Example-Project"><span class="No-Break">https://github.com/aws-samples/amazon-sagemaker-architectingfor-ml/tree/master/Example-Project</span></a></li>
				<li>Getting started with Amazon SageMaker <span class="No-Break">JumpStart: </span><a href="https://aws.amazon.com/sagemaker/jumpstart/gettingstarted/?sagemaker-jumpstart-cards.sort-by=item.additionalFields.priority&amp;sagemaker-jumpstart-cards.sort-order=asc&amp;awsf.sagemakerjumpstart-filter-product-type=*all&amp;awsf.sagemaker-jumpstartfilter-text=*all&amp;awsf.sagemaker-jumpstart-filter-vision=*all&amp;awsf.sagemaker-jumpstart-filter-tabular=*all&amp;awsf.sagemaker-jumpstartfilter-audio-tasks=*all&amp;awsf.sagemaker-jumpstart-filtermultimodal=*all&amp;awsf.sagemaker-jumpstart-filter-RL=*all"><span class="No-Break">https://aws.amazon.com/sagemaker/jumpstart/gettingstarted/?sagemaker-jumpstart-cards.sort-by=item.additionalFields.priority&amp;sagemaker-jumpstart-cards.sort-order=asc&amp;awsf.sagemakerjumpstart-filter-product-type=*all&amp;awsf.sagemaker-jumpstartfilter-text=*all&amp;awsf.sagemaker-jumpstart-filter-vision=*all&amp;awsf.sagemaker-jumpstart-filter-tabular=*all&amp;awsf.sagemaker-jumpstartfilter-audio-tasks=*all&amp;awsf.sagemaker-jumpstart-filtermultimodal=*all&amp;awsf.sagemaker-jumpstart-filter-RL=*all</span></a></li>
				<li><span class="No-Break">aws/amazon-sagemaker-examples: </span><a href="https://github.com/aws/amazon-sagemaker-examples/tree/main/"><span class="No-Break">https://github.com/aws/amazon-sagemaker-examples/tree/main/</span></a></li>
				<li>Announcing New Tools for Building with Generative AI on <span class="No-Break">AWS: </span><a href="https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/</span></a></li>
				<li>Google "We Have No Moat, And Neither Does <span class="No-Break">OpenAI": </span><a href="https://www.semianalysis.com/p/google-we-have-no-moat-and-neither"><span class="No-Break">https://www.semianalysis.com/p/google-we-have-no-moat-and-neither</span></a></li>
				<li>Self-supervised learning: The dark matter of <span class="No-Break">intelligence: </span><a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/"><span class="No-Break">https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/</span></a></li>
				<li>Yejin <span class="No-Break">Choi: </span><a href="https://homes.cs.washington.edu/~yejin/"><span class="No-Break">https://homes.cs.washington.edu/~yejin/</span></a></li>
				<li>Why AI is incredibly smart and shockingly <span class="No-Break">stupid: </span><a href="https://www.ted.com/talks/yejin_choi_why_ai_is_incredibly_smart_and_shockingly_stupid/c?language=en"><span class="No-Break">https://www.ted.com/talks/yejin_choi_why_ai_is_incredibly_smart_and_shockingly_stupid/c?language=en</span></a></li>
				<li>Continual PreTraining Mitigates Forgetting in Language and <span class="No-Break">Vision: </span><a href="https://arxiv.org/pdf/2205.09357.pdf"><span class="No-Break">https://arxiv.org/pdf/2205.09357.pdf</span></a></li>
				<li>LoRA: Low-Rank Adaptation of Large Language <span class="No-Break">Models: </span><a href="https://arxiv.org/abs/2106.09685"><span class="No-Break">https://arxiv.org/abs/2106.09685</span></a></li>
				<li>Improving language models by retrieving from trillions of tokens: <a href="https://arxiv.org/pdf/2112.04426.pdf">https://arxiv.org/pdf/2112.04426.pdf</a> </li>
				<li>Retrieval-Augmented Generation for Knowledge-Intensive NLP <span class="No-Break">Tasks: </span><a href="https://arxiv.org/pdf/2005.11401.pdf"><span class="No-Break">https://arxiv.org/pdf/2005.11401.pdf</span></a></li>
				<li>data2vec: A General Framework for Self-supervised Learning in Speech, Vision and <span class="No-Break">Language: </span><a href="https://arxiv.org/pdf/2202.03555.pdf"><span class="No-Break">https://arxiv.org/pdf/2202.03555.pdf</span></a></li>
				<li>UL2: Unifying Language Learning <span class="No-Break">Paradigms: </span><a href="https://arxiv.org/pdf/2205.05131.pdf"><span class="No-Break">https://arxiv.org/pdf/2205.05131.pdf</span></a></li>
				<li>ACL <span class="No-Break">2022: </span><a href="https://www.2022.aclweb.org/"><span class="No-Break">https://www.2022.aclweb.org/</span></a></li>
				<li>GLM-130B: AN OPEN BILINGUAL PRE-TRAINED <span class="No-Break">MODEL: </span><a href="https://openreview.net/pdf?id=-Aw0rrrPUF"><span class="No-Break">https://openreview.net/pdf?id=-Aw0rrrPUF</span></a></li>
				<li>BLOOM: A 176B-Parameter Open-Access Multilingual Language Model: <a href="https://arxiv.org/pdf/2211.05100.pdf">https://arxiv.org/pdf/2211.05100.pdf</a> </li>
				<li>LightOn releases PAGnol, the largest French Language <span class="No-Break">Model:</span><span class="No-Break"> </span><a href="https://medium.com/@LightOnIO/lighton-releases-pagnol-the-largest-french-language-model-f50b719352ab"><span class="No-Break">https://medium.com/@LightOnIO/lighton-releases-pagnol-the-largest-french-language-model-f50b719352ab</span></a></li>
				<li>A Japanese Masked Language Model for Academic <span class="No-Break">Domain: </span><a href="https://aclanthology.org/2022.sdp-1.16.pdf"><span class="No-Break">https://aclanthology.org/2022.sdp-1.16.pdf</span></a></li>
				<li>Cedille.ai launches the largest language model in German for text <span class="No-Break">generation: </span><a href="https://cedille.ai/blog/cedille-ai-launches-the-largest-language-model-in-german-for-text-generation"><span class="No-Break">https://cedille.ai/blog/cedille-ai-launches-the-largest-language-model-in-german-for-text-generation</span></a></li>
				<li>UK needs its own ‘BritGPT’ or will face an uncertain future, MPs <span class="No-Break">hear: </span><a href="https://www.theguardian.com/business/2023/feb/22/uk-needs-its-own-britgpt-or-will-face-an-uncertain-future-mps-hear"><span class="No-Break">https://www.theguardian.com/business/2023/feb/22/uk-needs-its-own-britgpt-or-will-face-an-uncertain-future-mps-hear</span></a></li>
				<li>AWS <span class="No-Break">Schedule: </span><a href="https://www.twitch.tv/aws/schedule?seriesID=340be301-27dc-42c6-890a-302cd13899af"><span class="No-Break">https://www.twitch.tv/aws/schedule?seriesID=340be301-27dc-42c6-890a-302cd13899af</span></a></li>
				<li><span class="No-Break">deepset-ai/haystack: </span><a href="https://github.com/deepset-ai/haystack"><span class="No-Break">https://github.com/deepset-ai/haystack</span></a></li>
				<li>AWS- <span class="No-Break">Overview: </span><a href="https://aws.amazon.com/codewhisperer/"><span class="No-Break">https://aws.amazon.com/codewhisperer/</span></a></li>
				<li>Hugging <span class="No-Break">Face: </span><span class="No-Break">https://huggingface.co/CarperAI/stable-vicuna-13b-delta</span></li>
				<li>Godfather of <span class="No-Break">AI: </span><a href="https://www.theguardian.com/technology/2023/may/02/geoffrey-hinton-godfather-of-ai-quits-google-warns-dangers-of-machine-learning"><span class="No-Break">https://www.theguardian.com/technology/2023/may/02/geoffrey-hinton-godfather-of-ai-quits-google-warns-dangers-of-machine-learning</span></a></li>
				<li>Pause Giant AI Experiments: An Open <span class="No-Break">Letter: </span><a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/"><span class="No-Break">https://futureoflife.org/open-letter/pause-giant-ai-experiments/</span></a></li>
				<li> <span class="No-Break">huggingface/peft: </span><a href="https://github.com/huggingface/peft"><span class="No-Break">https://github.com/huggingface/peft</span></a></li>
				<li>Department of Economic and Social AffairsIndigenous <span class="No-Break">Peoples:</span><span class="No-Break"> </span><a href="https://www.un.org/development/desa/indigenouspeoples/indigenous-languages.html"><span class="No-Break">https://www.un.org/development/desa/indigenouspeoples/indigenous-languages.html</span></a></li>
			</ol>
		</div>
	</body></html>