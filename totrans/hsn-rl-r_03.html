<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building Blocks of Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p>The main objective of algorithms based on reinforcement learning is to learn and adapt to environmental changes. To do this, we use external feedback signals (reward signals) generated by the environment according to the choices made by the algorithm. In this context, the right choice that's suggested by the algorithm will provide a reward while a wrong choice will result in a penalty. All of this is done in order to achieve the best result possible. In this chapter, you will discover agent-environment interface concepts for building models. By the end of this chapter, you will be ready to dive into working on the Markov decision process. <span>We will also discover the fundamental concepts of the policy and how to improve the results by applying a policy gradient.</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Agent-environment interface</li>
<li>Understanding the Markov decision process</li>
<li>Explaining the policy</li>
<li>Exploring policy gradient methods</li>
</ul>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Agent-environment interface</h1>
                </header>
            
            <article>
                
<p>In reinforcement learning, the agent learns and adapts to environmental changes. The basis of this programming technique arises from the concept of receiving external stimuli according to the choices of the algorithm. A correct choice returns a reward while an incorrect choice returns a penalty. The objective of the system is to achieve the best possible result.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>These mechanisms derive from the basic concepts of machine learning (learning from experience) in an attempt to simulate human reasoning. In fact, in our mind, we activate brain mechanisms that lead us to chase and repeat what produces feelings of gratification and well-being. Whenever we experience moments of pleasure (food, music, art, and so on), our brain produces some substances that work by reinforcing that same stimulus, emphasizing it. Along with this mechanism of neurochemical reinforcement, our memory remembers this experience so that we can recreate how we feel in the future. Evolution has provided us with this mechanism so that we can repeat experiences that are rewarding to us.</p>
<p>This is why we remember the important experiences of our lives, especially those that are powerfully rewarding, are part of our memories, and condition our future explorations. Learning from experience can be simulated by a numerical algorithm in various ways, depending on the nature of the signal that's used for learning or the type of feedback that's returned by the system.</p>
<p>In supervised learning, there is a teacher who tells the system what the correct output is. In reality, it isn't always possible to have a tutor who guides us in our choices. Often, we only have qualitative information that gives us feedback on how the environment responds to our actions. This information is called reinforcement signals. The system doesn't give us any information about how to update the agent's behavior (that is, the weights). You cannot define a cost function or a gradient. The goal of the system is to create smart agents that are able to learn from their experience. In the following diagram, we can see a flowchart that displays how reinforcement learning interacts with the environment:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-468 image-border" src="assets/b2b34caf-debe-423d-ad95-4a606f973573.png" style="width:22.25em;height:13.67em;"/></p>
<p>Scientific literature has taken an uncertain stance on the classification of learning by reinforcement as a paradigm. In fact, in its initial phase, it was considered as a special case of supervised learning before it was fully promoted as the third paradigm of machine learning algorithms. It is applied in different contexts in which supervised learning is inefficient: the problems of interacting with the environment is a clear example of this.</p>
<p>We need to follow these steps to correctly apply a reinforcement learning algorithm:</p>
<ol>
<li>Prepare the agent.</li>
<li>Observe the environment.</li>
<li>Select the optimal strategy.</li>
<li>Execute the actions.</li>
<li>Calculate the corresponding reward (or penalty).</li>
<li>Develop updated strategies (if necessary).</li>
<li>Repeat <em>steps 2 - 5</em> iteratively until the agent learns the optimal strategies.</li>
</ol>
<p>Reinforcement learning is based on the psychological theory that came about due to a series of experiments that were performed on animals. In particular, Edward Thorndike (an American psychologist) noted that if a cat is given a reward immediately after it behaves in a way that's considered correct, it increases the probability that the cat will repeat this behavior. In the face of unwanted behavior, however, applying punishment decreases the probability that the cat will repeat the behavior.</p>
<p>On the basis of this theory, reinforcement learning tries to maximize the rewards that are received when an action or set of actions are executed so that a certain goal can be reached.</p>
<p>Reinforcement learning can be seen as a special case of the interaction problem for achieving a goal. The entity that must reach the goal is called an agent. The entity that the agent must interact with is called the environment, which corresponds to everything that is external to the agent.</p>
<p>So far, we have focused on the term agent, but what does it represent? The agent (software) is a software entity that performs services on behalf of another program, usually automatically and invisibly. These pieces of software are also called smart agents.</p>
<p>The following are the most important features of an agent:</p>
<ul>
<li>It can choose an action to perform on the environment that's either continuous or discrete.</li>
<li>The action that's performed depends on the situation. The situation is summarized in the system state.</li>
<li>The agent continuously monitors the environment (input) and continuously changes the status.</li>
<li>The choice of the action isn't trivial and requires a certain degree of "intelligence".</li>
<li>The agent has a smart memory.</li>
</ul>
<p>The agent has a goal-directed behavior, but how it acts in an uncertain environment isn't known beforehand. An agent learns by interacting with the environment. Planning can be developed while the agent is learning about the environment through the measurements it makes. This strategy is close to the trial-and-error theory.</p>
<div class="packt_tip">
<p>The trial and error theory is a crucial method of problem-solving. The trial is repeated until the agent is successful or until the agent stops trying.</p>
</div>
<p>The agent-environment interaction is continuous since the agent chooses an action to be taken and, in response, the environment changes state by presenting a new situation that the agent will be faced with.</p>
<p>In the case of reinforcement learning, the environment provides the agent with a reward. It is essential that the source of the reward is the environment to avoid the formation of a personal reinforcement mechanism that would compromise learning.</p>
<p>The value of the reward is proportional to the influence that the action has in reaching the objective, so it is positive or high in the case of a correct action or negative or low for an incorrect action.</p>
<p>The following are some examples from real life where an agent and environment have interacted to solve a certain problem:</p>
<ul>
<li>A chess player, where each move provides information about the possible countermoves of the opponent can make.</li>
<li>A little giraffe that, in a few hours, can learn to get up and run at 50 km/h.</li>
<li>A truly autonomous robot learns to move in a room so that it can get out of it.</li>
<li>The parameters of a refinery (oil pressure, flow, and so on) are set in real-time so that we can obtain the maximum yield or maximum quality.</li>
</ul>
<p>All of these examples have the following characteristics in common:</p>
<ul>
<li>Interaction with the environment</li>
<li>The objective of the agent</li>
<li>Uncertainty or partial knowledge of the environment</li>
</ul>
<p class="mce-root"/>
<p>From this, it is possible to make the following observations:</p>
<ul>
<li>The agent learns from its own experience</li>
<li>The actions change the status (the situation) and how many changes can be made in the future (delayed reward).</li>
<li>The effect of an action cannot be completely predicted.</li>
<li>The agent has a global assessment of its behavior.</li>
<li>The agent must exploit this information to improve its choices. These choices improve with experience.</li>
<li>Problems can have a finite or infinite time horizon.</li>
</ul>
<p>Essentially, the agent receives sensations from the environment through its sensors. Depending on its feelings, the agent decides what actions to take in the environment. Based on the immediate result of its actions, the agent may be rewarded.</p>
<p>If you want to use an automatic learning method, you need to give a formal description of the environment. It isn't important to know exactly how the environment is made <span>–</span> what's interesting is to make general assumptions about the properties that the environment has. In reinforcement learning, it is usually assumed that the environment can be described by a Markov decision process. Let's learn how.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the Markov decision process</h1>
                </header>
            
            <article>
                
<p>To avoid load problems and computational difficulties, the agent-environment interaction is considered as a Markov decision process. A <strong>Markov decision process</strong> is a discrete time stochastic control process.</p>
<div>
<p><strong>Stochastic processes</strong> are mathematical models that are used to study the evolution of phenomena following random or probabilistic laws. In all-natural phenomena, it is known that, by their very nature and by the observation errors, a random or accidental component is present. This component states that at every instant, <em>t</em>, the result of observing the phenomenon is a random number or random variables, <em>st</em>: it isn't possible to predict with certainty what the result will be; we can only state that it will take one of several possible values, each of which has a given probability.</p>
</div>
<p>A stochastic process is deemed Markovian when a certain instant, <em>t</em>, of the observation is chosen, the evolution of the process, starting with <em>t</em>, depends only on <em>t</em> while it doesn't depend on the previous instants in any way. Thus, a process is Markov when, given the moment of observation, the instant determines the future evolution of the process, while this evolution doesn't depend on the past.</p>
<p class="mce-root"/>
<p>In a Markov process, at each time step, the process is in some state, <em>s ∈ S</em>, and the agent may choose any action, <em>a ∈ A</em>, that is available in state <em>s</em>. The process responds at the next time step by randomly moving into a new state, <em>s'</em>, and giving the agent a corresponding reward <em>r(s,s')</em>.</p>
<p>In the following diagram, we can see the agent-environment interaction in a Markov decision process:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-470 image-border" src="assets/86d11815-26cd-481c-a7b4-5808afc8b778.png" style="width:30.75em;height:18.08em;"/></p>
<p>The agent-environment interaction shown in the preceding diagram can be summarized as follows:</p>
<ul>
<li>The agent and the environment interact at discrete intervals over time, t = 0, 1, 2… n.</li>
<li>At each interval, the agent receives a representation of the state <em>st</em> of the environment.</li>
<li>Each element <em>st S</em>, where <em>S</em> is the set of possible states.</li>
<li>Once the state is recognized, the agent must take an action at <em>A(st)</em>, where <em>A(st)</em> is the set of possible actions in the state, <em>st</em>.</li>
<li>The choice of the action to be taken depends on the objective to be achieved and is mapped through the policy indicated with the symbol <em>π</em> (discounted cumulative reward), which associates the action with <em>A(s)</em> for each state, <em>s</em>. The term <em>πt(s,a)</em> represents the probability that action <em>a</em> is carried out in the state, <em>s</em>.</li>
<li>During the next time interval, <em>t + 1</em>, as part of the consequence of the action, the agent receives a numerical reward, <em>rt + 1 R</em>, corresponding to the action that was taken previously.</li>
<li>Now, the consequence of the action represents the new state, <em>st</em>. At this point, the agent must code the state and make a choice in terms of the action that will take place.</li>
<li>This iteration repeats itself so that the objective is achieved by the agent.</li>
</ul>
<p>The definition of the status <em>st + 1</em> depends on the previous state and the action taken (MDP), as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/86ddc6a6-2400-4f4b-a2ae-8365595f33de.png" style="width:7.25em;height:1.33em;"/></p>
<p>In the formula, δ represents the status function.</p>
<p>In summary, we can state the following:</p>
<ul>
<li>In a Markov decision process, the agent can perceive the status, <em>s S</em> that they're in and has a set of actions at their disposal.</li>
<li>At each discrete interval <em>t</em> of time, the agent detects the current status <em>st</em> and decides to implement an action at <em>A</em>.</li>
<li>The environment responds by providing a reward (a reinforcement) <em>rt = r (st, at)</em> and moving into the state <em>st + 1 = δ (st, at)</em>.</li>
<li>The r and δ functions are part of the environment; they only depend on the current state and action (not the previous ones) and are not necessarily known to the agent.</li>
<li>The goal of reinforcement learning is to learn a policy that, for each state, <em>s,</em> in which the system is located, specifies an action to the agent so that it can maximize the total reinforcement it receives during the entire action sequence.</li>
</ul>
<p>Let's talk about some of the terms we used in more detail:</p>
<ul>
<li>A reward function defines the goal in a reinforcement learning problem. It maps the detected states of the environment into a single number, thus defining a reward. As we mentioned previously, the only goal is to maximize the total reward it receives in the long term. The reward function decides which actions are positive and negative for the agent. The reward function has the need to be correct, and it can be used as a basis for changing the policy. If an action that's suggested by the policy returns a low reward, in the next step, the policy can be modified to suggest other actions in the same situation.</li>
<li>A policy defines the behavior of the learning agent at a given time. It maps both the detected states of the environment and the actions to take when they are in those states. This corresponds to what would be called a set of rules or associations of stimulus response in psychology. The policy is the fundamental part of a reinforcing learning agent in the sense that it alone is enough to determine behavior.</li>
<li>A value function represents how good a state is for an agent. It is equal to the total reward that's expected for an agent from the status, <em>s</em>. The value function depends on the policy that the agent selects for the actions to be performed on.</li>
<li>An action-value function returns the value, that is, the expected return (overall reward) for using action, <em>a</em>, in a certain state, <em>s</em>, following a policy.</li>
</ul>
<p>In the next section, we will learn how to maximize the total reinforcement that's received during the learning process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discounted cumulative reward</h1>
                </header>
            
            <article>
                
<p>In the <em>Markov decision process</em> section, we mentioned that the goal of reinforcement learning is to learn a policy that, for each state, <em>s</em>, in which the system is located, specifies an action to the agent so that it can maximize the total reinforcement they receive during the entire action sequence. How can we maximize the total reinforcement that's received during the entire sequence of actions?</p>
<p>The total reinforcement that's derived from the policy is calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ce3c5ab6-1900-46e3-aa37-fcbac40d0e9c.png" style="width:19.08em;height:3.83em;"/></p>
<p>Here, r<sub>T</sub> represents the reward of the action that drives the environment in the terminal state, s<sub>T</sub>.</p>
<p>A possible solution to this problem is to associate the action that provides the highest reward to each individual state; that is, we must determine an optimal policy so that the previous quantity is maximized.</p>
<p>For problems that don't reach the goal or terminal state in a finite number of steps (continuing tasks), Rt continues to infinity.</p>
<p class="mce-root"/>
<p>In these cases, the sum of the rewards that we want to maximize diverges at the infinite, so this approach is not applicable. Due to this, it is necessary to develop an alternative reinforcement technique.</p>
<p>The technique that best suits the reinforcement learning paradigm turns out to be the discounted cumulative reward, which tries to maximize the following quantity:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a4f284ea-0813-4784-815b-48af5f19b057.png" style="width:24.25em;height:3.25em;"/></p>
<p>Here, γ is called a discount factor and represents the importance of future rewards. This parameter can take the values 0 ≤  γ ≤ 1, which have the following meanings:</p>
<ul>
<li>If γ &lt;1, the sequence, <em>rt</em>, will converge to a finite value.</li>
<li>If γ = 0, the agent will have no interest in future rewards but will try to maximize the reward for the current state.</li>
<li>If γ = 1, the agent will try to increase future rewards, even at the expense of the immediate ones.</li>
</ul>
<p>The discount factor can be modified during the learning process to highlight particular actions or states. An optimal policy can lead to the reinforcement that's obtained when performing a single action to be low (or even negative), provided that this leads to greater reinforcement. Exploring the environment is the right approach when you want to gather useful information. However, in some cases, it is also a computationally expensive process, so let's look at how to deal with this problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploration versus exploitation</h1>
                </header>
            
            <article>
                
<p>Ideally, the agent must associate with each action at the respective reward, <em>r</em>, in order to choose the most rewarded behavior for achieving the goal. This approach is impractical for complex problems in which the number of states is particularly high and the possible associations increase exponentially.</p>
<p>This problem is called the <strong>exploration-exploitation</strong> dilemma. Ideally, the agent must explore all the possible actions for each state and find the one that is rewarded the most when it's exploited.</p>
<p>Thus, decision-making involves a fundamental choice:</p>
<ul>
<li><strong>Exploitation</strong>: This makes the best decision given the current information.</li>
<li><strong>Exploration</strong>: This collects more information.</li>
</ul>
<p class="mce-root"/>
<p>In this process, the best long-term strategy can lead to considerable sacrifices in the short term. Therefore, it is necessary to gather enough information to make the best decisions.</p>
<p>The exploration-exploitation dilemma has something to offer whenever we try to learn something new. Often, we have to decide whether to choose what we already know (exploitation), thus leaving our cultural baggage unaltered, or choose something new and learn in this way instead (exploration). The second choice risks us making the wrong choices. This is an experience that we have often faced; think, for example, about the choices we make in a restaurant when we are asked to choose between the dishes on the menu:</p>
<ul>
<li>We can choose something that we already know about and that, in the past, has given us back a known reward with gratification (exploitation), such as pizza (who doesn't know the goodness of a Margherita pizza?).</li>
<li>We can try something new that we have never tasted before and see what we get (exploration), such as lasagne (alas, not everyone knows the magical taste of a lasagne bowl).</li>
</ul>
<p>The choice we make will depend on many boundary conditions: the price of the dishes, our level of hunger, our knowledge of dishes, and so on. What's important is that studying the best way to make this kind of choice has demonstrated that optimal learning requires that we sometimes make bad choices. This means that, sometimes, you have to choose to avoid the action you deem the most rewarding and take an action that you feel is less rewarding. The logic is that these actions are necessary to obtain a long-term benefit: sometimes, you need to get your hands dirty to learn more.</p>
<p>Here are some more examples of adopting this technique in real-life cases:</p>
<ul>
<li>Selecting a store:
<ul>
<li><strong>Exploitation</strong>: Go to your favorite store</li>
<li><strong>Exploration</strong>: Try a new store</li>
</ul>
</li>
</ul>
<ul>
<li>Choosing a route:
<ul>
<li><strong>Exploitation</strong>: Choose the best route you know of</li>
<li><strong>Exploration</strong>: Try a new route</li>
</ul>
</li>
</ul>
<p class="mce-root"/>
<p>In practice, in very complex problems, converging a very good strategy would be too slow. A good solution to this problem is to find a balance between exploration and exploitation:</p>
<ul>
<li>An agent who limits itself to exploring will always act in a casual way in every state and it is evident that converging to an optimal strategy is impossible.</li>
<li>If an agent explores a little, it will always use the actions it would always use, which may not be optimal.</li>
</ul>
<p class="mce-root">At every step, the agent has to choose between repeating what they have done so far or trying out new movements that could achieve better results.</p>
<p>Policies are essential when it comes to choosing an action to perform. In the next section, we will look at this further by analyzing the different approaches that can be used while searching for the best possible policy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Explaining the policy</h1>
                </header>
            
            <article>
                
<p>As we mentioned in the <em>Markov decision process</em> section, a policy defines the behavior of the learning agent at a given time. It maps both the detected states of the environment and the actions to take when they are in those states. The policy is the fundamental part of a reinforcing learning agent in the sense that it alone is enough to determine behavior. The policy is essential in the choices that the agent is required to make. In fact, once the observation has been obtained, the decision on what to do next is made on the basis of the policy. In this case, we don't need the value of the state or a particular action <span>– </span>we simply need the policy that considers the total reward.</p>
<p>Policies can be deterministic when the same action is taken for a given state, or probabilistic when the action is chosen based on some distribution calculation between the shares and the given state.</p>
<p>To fully understand the meaning of the policy, let's look at an example. Suppose we need to implement an algorithm to drive a delivery vehicle from the shop to the customer's home. We can define the following elements:</p>
<ul>
<li>The road map is the environment.</li>
<li>The current position of the vehicle is a state.</li>
<li>The policy is what the agent does to accomplish this task.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, we provide some examples of policies that our vehicle could adopt in order to complete a task, such as the delivery of goods:</p>
<ul>
<li><strong>Policy 1</strong>: Uncontrolled vehicles would move randomly until they accidentally end up in the right place (the customer's home). Here, it is possible that a lot of fuel will be consumed and that the delivery process will last a long time.</li>
<li><strong>Policy 2</strong>: Other vehicles could learn to travel on only the main roads, thus traveling more.</li>
<li><strong>Policy 3</strong>: The controlled vehicles will plan the route by choosing a route that will take them to their destination so that they drive fewer roads.</li>
</ul>
<p>Obviously, some policies are better than others and there are many ways to evaluate them, namely the function value state and the function's value-action. The goal is to learn the best policy. The policy can be approached in two ways: <strong>policy iteration</strong> and <strong>policy search</strong>. The main difference between these two techniques is in the use of the value function. In the upcoming sections, we will analyze both of these approaches in detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy iteration</h1>
                </header>
            
            <article>
                
<p>Policy iteration is a dynamic programming algorithm that uses a value function to model the expected return for each pair of action-states. Many techniques in reinforcement learning are based on this technique, including Q-learning, TD-learning, SARSA, QV-learning, and more. These techniques update the value functions using the immediate reward and the (discounted) value of the next state in a process called <strong>bootstrap</strong>. Therefore, they imply the storage of <em>Q (s, a)</em> in tables or with approximate function techniques.</p>
<div class="packt_tip">Policy iteration is generally applied to discrete Markov decision processes, where both the state space <em>S</em> and the action space <em>A</em> are discrete and finite sets.</div>
<p>Starting from an initial P<sub>0</sub> policy, the iteration of the policy alternates between the following two phases:</p>
<ol>
<li><strong>Policy evaluation</strong>: Given the current policy P, estimate the action-value function, Q<sub>P</sub>.</li>
<li><strong>Policy improvement</strong>: Calculate a better policy P ' based on Q<sub>P</sub>, then set P' as the new policy and return to the previous step.</li>
</ol>
<p class="mce-root"/>
<p>When the <span>action-</span>value function can be calculated for each action-state pair, the policy iteration with the greedy policy improvement leads to convergence by returning the optimal policy. Essentially, repeatedly executing these two processes converges the general process toward the optimal solution.</p>
<p>Unfortunately, the Q<sub>P</sub> value function can't always be calculated exactly; generally, it can be estimated from samples. In these cases, it is necessary to include a certain degree of randomness in the policy to ensure sufficient exploration of the state's space of action. These algorithms store the value function in a finite table (tabular approach). The limitation of these algorithms is that they cannot be applied to the case of continuous Markov decision processes. Moreover, these methods can be unusable in some discrete cases where the cardinality of the state-action space is too high. Let's look at another approach to this problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy search</h1>
                </header>
            
            <article>
                
<p>In the policy search approach, a parameterized policy is stored, but no value function is used or estimated. Policy search methods can rely on roll-out policies that use trajectory-based sampling. Other policy search methods use optimization techniques such as evolutionary algorithms to search for optimal policy parameters. Policy gradient methods are examples of policy search. Let's look at these in detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring policy gradient methods</h1>
                </header>
            
            <article>
                
<p>The policy gradient is a class of reinforcement learning algorithms based on the use of parameterized policies. The idea is to calculate the expected return gradient (reward) with respect to each parameter in order to change the parameters in a direction that increases the performance of the same. This method doesn't show the problems of traditional reinforcement learning such as the lack of guarantees of a value function, the problem resulting from the uncertainty of the state, and the complexities that arise from states and actions in continuous spaces. In the policy search method, no value function is used or estimated. The value function can be used to learn the policy parameter; however, it won't necessary for action selection. Policy gradient methods bypass all the problems that are connected to value function-based techniques by searching for the optimal policy directly.</p>
<p class="mce-root"/>
<p>The pros of applying policy gradient methods are as follows:</p>
<ul>
<li>Continuous states and actions can be treated as discrete cases and learning performance is often increased.</li>
<li>There's a great variety of different algorithms in the literature that have strong theoretical bases.</li>
<li>State uncertainty doesn't degrade the learning process, even if no particular state estimator is used.</li>
</ul>
<p>The cons of applying <span>policy gradient methods</span> are as follows:</p>
<ul>
<li>The learning rate can decide on the order of magnitude of the convergence speed.</li>
<li>They need to update the data very quickly to avoid the introduction of errors in the gradient estimator. This means that the use of sample data is not very efficient.</li>
</ul>
<p>In practice, a policy gradient returns the direction in which we have to modify the parameters of our algorithm to improve the policy in order to maximize the total accumulated rewards. The gradient is equal to the gradient of the logarithmic probability of the action that's chosen. In other words, we are looking to increase the probability of actions that return a good total reward and to reduce the probability of actions with negative final results <span>– </span>we keep what works and leave out what it doesn't.</p>
<p>As we mentioned in the <em>Markov decision process</em> section, the choice of the action to be taken depends on the objective to be achieved and is mapped through the policy indicated with the symbol π (discounted cumulative reward), which associates the action with a ∈ A(s) for each state, s. The term πt(s,a) represents the probability that action <em>a</em> is carried out in the state, <em>s</em>.</p>
<p>The purpose of this procedure is to parameterize the policy using a θ parameter, which allows us to determine the best action, <em>a</em>, in a state, <em>s</em>. This policy function will be defined as πθ (s,a).</p>
<p>To find the optimal policy, we will use a neural network that will input the state and output the probability of each action in that state. This probability will be used to sample an action from this distribution and perform that action in the state. Nothing tells us that the sampled action is the correct action to perform in the state. Then, we perform the action and keep the reward. This procedure will be repeated for each state. The data that's obtained will be our training data. At this point, to update the gradients, we will use an algorithm based on the descent of the gradient.</p>
<p>By doing this, the actions that return a high reward in a state will present a high probability, while the actions with a low reward will have a low probability.</p>
<p>Like in all gradient descent methods, the parameter vector is updated in the direction of the gradient of a performance measurement. In this case, the measurement for performance is given by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a0d4a2fb-5f46-4f71-b547-677149d2a4a0.png" style="width:7.83em;height:3.17em;"/></p>
<p><span>In the preceding formula,</span> <em>E</em> is the expected return and <em>r</em> is the reward. Our aim is to learn a policy that maximizes the cumulative future reward. The gradient of the expected return is called the policy gradient, and we'll use it to update the θ parameter, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8a939a7b-ece1-42c3-be19-90a141b1981f.png" style="width:7.92em;height:1.17em;"/></p>
<p>Here, we have the following:</p>
<ul>
<li>θ is the parameter</li>
<li>∇ is the gradient of the expected return</li>
<li>α is the learning rate</li>
</ul>
<p>A parameter update is performed at each learning iteration. The gradient descent algorithm guarantees convergence to at least one local optimum.</p>
<p>A general policy gradient algorithm can be summarized as follows:</p>
<pre>set initial policy parametrization<br/>repeat until converge<br/>    generate N trajectories following a policy πθ<br/>    compute policy gradient estimate<br/>    update the parameter θ</pre>
<p>In the next section, we will analyze some <span>methods based on the policy gradient, that is, the </span><strong>Monte Carlo policy gradient</strong> and <strong>actor-critic methods</strong>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Monte Carlo policy gradient</h1>
                </header>
            
            <article>
                
<p>The Monte-Carlo policy gradient, also called <strong>REINFORCE</strong>, is a class of associative RL algorithms for networks connected to stochastic units. It has been shown that, in tasks with immediate reinforcement and, with some limitations, even in some tasks with delayed reinforcement, these algorithms perform parameter adjustments in a direction that lies along the gradient of the expected reinforcement. Because of their nature, these algorithms can easily be integrated with other gradient descent methods and with backpropagation in particular. The major drawback is that they are unable to distinguish between local (global) maximums (minimum) and don't have a general convergence theory.</p>
<p>In this family of algorithms, the agent generates a trajectory of an episode using its current policy and uses it to update the policy parameter. This algorithm provides an off-policy update since a complete trajectory must be completed to build a sample space.</p>
<p>The following code is the pseudocode for the REINFORCE algorithm:</p>
<pre>Initialise θ randomly<br/>For each episode<br/>   For t=1:T-1<br/>     calculate return<br/>     update θ</pre>
<p>As we can see, no explicit exploration is required. In this case, having calculated the probabilities with a neural network, the exploration is performed automatically. First, random weights are used to initialize the network and a uniform probability distribution is returned. This distribution is equivalent to the behavior of a random agent.</p>
<div class="packt_tip">To reduce the variance of the gradient estimation while maintaining the bias, you can make a change to the REINFORCE algorithm by subtracting a base value from the return.</div>
<p>Now, let's look at another method based on the policy gradient.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Actor-critic methods</h1>
                </header>
            
            <article>
                
<p>Actor-critic methods use policy search and value function estimation in order to carry out low-variance gradient updates. These methods separate the memory structure to make the policy independent of the value function. The policy block is known as an <strong>actor</strong> because it chooses actions, while the estimated value function block is known as a <strong>critic</strong> in the sense that it criticizes the actions that are performed by the policy that is being followed. From this, it is clear that learning is an on-policy type where the critic learns and criticizes the work of politics.</p>
<p>Typically, the critic is a function of state evaluation. After each selection of an action, the critic assesses the new state to determine whether things have gone better or worse than expected.</p>
<p>The agent performs two jobs and performs two roles with two different networks:</p>
<ul>
<li>The <strong>actor</strong> network is the one that establishes the action to be performed in a certain state by updating the policy parameters in the direction proposed by the critic.</li>
<li>The <strong>critic</strong> network is the one that evaluates the consequences of this action, modifying the function value of the next time step, and updating the value function parameters.</li>
</ul>
<p>The environment is perceived and measured by input sensors. The system processes these inputs within the evaluation/estimation of the state. The status estimate and any rewards are then communicated to the agent.</p>
<p>The following is the pseudocode for the actor-critic algorithm:</p>
<pre>Initialize parameters randomly<br/>For t=1:T<br/>    Sample reward and next state<br/>    Sample the next action<br/>    Update the policy parameters<br/>    Compute the correction for action-value at time t<br/>    Use the correction to update the parameters of action-value function<br/>    Update action and state</pre>
<p>As we can see the critic network and the actor network are updated at each step.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have learned about the basics of building a model based on Markov processes. A Markov decision-making process is a stochastic process characterized by five elements: epoch, states, actions, transition probabilities, and rewards. There's also an agent present that controls the path of the stochastic process. At a certain point in the path and at a certain point, <em>t</em>, the agent intervenes and makes a decision that will influence the future evolution of the process. These moments are called epochs of decision, while the decisions that are made assume the connotation of actions.</p>
<p>Then, we discovered the policy that defines the behavior of the learning agent at a given time. It maps both the detected states of the environment and the actions to take when they are in those states. The policy is the fundamental part of a reinforcing learning agent in the sense that it alone is enough to determine behavior. The policy can be approached in two ways: <strong>policy iteration</strong> and <strong>policy search</strong>. The main difference between these two techniques is in their use of the value function.</p>
<p>Finally, we learned how to build a model based on policy gradient methods, that is, a class of reinforcement learning algorithms based on the use of parameterized policies. The idea is to calculate the expected return gradient (reward) with respect to each parameter in order to change the parameters in a direction that increases the performance of the same.</p>
<p>In the next chapter, we will see the Markov decision processes in action. We will get to grips with the concepts of the Markov decision process and understand the agent-environment interaction process. Then, we will learn how to use Bellman equations as consistency conditions for the optimal value functions to determine the optimal policy. Finally, we will discover and implement Markov chains and learn how to simulate random walks using them.</p>


            </article>

            
        </section>
    </body></html>