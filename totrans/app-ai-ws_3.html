<html><head></head><body>
		<div>
			<div id="_idContainer113" class="Content">
			</div>
		</div>
		<div id="_idContainer114" class="Content">
			<h1 id="_idParaDest-96">3. <a id="_idTextAnchor109"/>An Introduction to Classification</h1>
		</div>
		<div id="_idContainer130" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter introduces you to classification. You will implement various techniques, such as k-nearest neighbors and SVMs. You will use the Euclidean and Manhattan distances to work with k-nearest neighbors. You will apply these concepts to solve intriguing problems such as predicting whether a credit card applicant has a risk of defaulting and determining whether an employee would stay with a company for more than two years. By the end of this chapter, you will be confident enough to work with any data using classification and come to a certain conclusion.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor110"/>Introduction</h1>
			<p>In the previous chapter, you were introduced to regression models and learned how to fit a linear regression model with single or multiple variables, as well as with a higher-degree polynomial.</p>
			<p>Unlike regression models, which focus on learning how to predict continuous numerical values (which can have an infinite number of values), classification, which will be introduced in this chapter, is all about splitting data into separate groups, also called classes.</p>
			<p>For instance, a model can be trained to analyze emails and predict whether they are spam or not. In this case, the data is categorized into two possible groups (or classes). This type of classification is also called <strong class="bold">binary classification</strong>, which we will see a few examples of in this chapter. However, if there are more than two groups (or classes), you will be working on a <strong class="bold">multi-class classification</strong> (you will come across some examples of this in <em class="italic">Chapter 4</em>, <em class="italic">An Introduction to Decision Trees</em>). </p>
			<p>But what is a real-world classification problem? Consider a model that tries to predict a given user's rating for a movie where this score can only take values: <em class="italic">like</em>, <em class="italic">neutral</em>, or <em class="italic">dislike</em>. This is a classification problem.</p>
			<p>In this chapter, we will learn how to classify data using the k-nearest neighbors classifier and SVM algorithms. Just as we did for regression in the previous chapter, we will build a classifier based on cleaned and prepared training data and test the performance of our classifier using testing data.</p>
			<p>We'll begin by looking at the fundamentals of classification.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor111"/>The Fundamentals of Classification</h1>
			<p>As stated earlier, the goal of any classification problem is to separate the data into relevant groups accurately using a training set. There are a lot of applications of such projects in different industries, such as education, where a model can predict whether a student will pass or fail an exam, or healthcare, where a model can assess the level of severity of a given disease for each patient.</p>
			<p>A classifier is a model that determines the label (output) or value (class) of any data point that it belongs to. For instance, suppose you have a set of observations that contains credit-worthy individuals, and another one that contains individuals that are risky in terms of their credit repayment tendencies. </p>
			<p>Let's call the first group P and the second one Q. Here is an example of such data:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B16060_03_01.jpg" alt="Figure 3.1: Sample dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1: Sample dataset</p>
			<p>With this data, you will train a classification model that will be able to correctly classify a new observation into one of these two groups (this is binary classification). The model can find patterns such as a person with a salary above $60,000 being less risky or that having a mortgage/income ratio above ratio 10 makes an individual more at risk of not repaying their debts. This will be a <strong class="bold">multi-class classification</strong> exercise.</p>
			<p>Classification models can be grouped into different families of algorithms. The most famous ones are as follows:</p>
			<ul>
				<li>Distance-based, such as <strong class="bold">k-nearest neighbors</strong></li>
				<li>Linear models, such as <strong class="bold">logistic regression</strong> or <strong class="bold">SVMs</strong></li>
				<li>Tree-based, such as <strong class="bold">random forest</strong></li>
			</ul>
			<p>In this chapter, you will be introduced to two algorithms from the first two types of family: k-nearest neighbors (distance-based) and SVMs (linear models).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We'll walk you through tree-based algorithms such as random forest in <em class="italic">Chapter 4</em>, <em class="italic">An Introduction to Decision Trees</em>.</p>
			<p>But before diving into the models, we need to clean and prepare the dataset that we will be using in this chapter. </p>
			<p>In the following section, we will work on a German credit approvals dataset and perform all the data preparation required for the modeling stage. Let's start by loading the data.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor112"/>Exercise 3.01: Predicting Risk of Credit Card Default (Loading the Dataset)</h2>
			<p>In this exercise, we will be loading a dataset into a pandas DataFrame and exploring its contents. We will use the dataset of German credit approvals to determine whether an individual presents a risk of defaulting.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The CSV version of this dataset can be found on our GitHub repository:</p>
			<p class="callout"><a href="https://packt.live/3eriWTr">https://packt.live/3eriWTr</a>.</p>
			<p class="callout">The original dataset and information regarding the dataset can be found at <a href="https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29">https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29</a>.</p>
			<p class="callout">The data files are located at <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/">https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/</a>.</p>
			<p class="callout">Citation - <em class="italic">Dua, D., &amp; Graff, C.. (2017). UCI Machine Learning Repository</em>.</p>
			<ol>
				<li>Open a new Jupyter Notebook file.</li>
				<li>Import the <strong class="source-inline">pandas</strong> package as <strong class="source-inline">pd</strong>:<p class="source-code">import pandas as pd</p></li>
				<li>Create a new variable called <strong class="source-inline">file_url</strong>, which will contain the URL to the raw dataset file, as shown in the following code snippet:<p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/'\</p><p class="source-code">           'The-Applied-Artificial-Intelligence-Workshop/'\</p><p class="source-code">           'master/Datasets/german_credit.csv'</p></li>
				<li>Import the data using the <strong class="source-inline">pd.read_csv()</strong> method:<p class="source-code">df = pd.read_csv(file_url)</p></li>
				<li>Use <strong class="source-inline">df.head()</strong> to print the first five rows of the DataFrame:<p class="source-code">df.head()</p><p>The expected output is this:</p><div id="_idContainer116" class="IMG---Figure"><img src="image/B16060_03_02.jpg" alt="Figure 3.2: The first five rows of the dataset"/></div><p class="figure-caption">Figure 3.2: The first five rows of the dataset</p><p>As you can see, the output in the preceding screenshot shows us the features of the dataset, which can be either numerical or categorical (text).</p></li>
				<li>Now, use <strong class="source-inline">df.tail()</strong> to print the last five rows of the DataFrame:<p class="source-code">df.tail()</p><p>The expected output is this:</p><div id="_idContainer117" class="IMG---Figure"><img src="image/B16060_03_03.jpg" alt="Figure 3.3: The last five rows of the dataset"/></div><p class="figure-caption">Figure 3.3: The last five rows of the dataset</p><p>The last rows of the DataFrame are very similar to the first ones we saw earlier, so we can assume the structure is consistent across the rows.</p></li>
				<li>No<a id="_idTextAnchor113"/>w, use <strong class="source-inline">df.dtypes</strong> to print the list of columns and their data types:<p class="source-code">df.dtypes</p><p>The expected output is this:</p><div id="_idContainer118" class="IMG---Figure"><img src="image/B16060_03_04.jpg" alt="Figure 3.4: The list of columns and their data types&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.4: The list of columns and their data types</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hQXJEs">https://packt.live/3hQXJEs</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fN0DrT">https://packt.live/3fN0DrT</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>From the preceding output, we can see that this DataFrame has some numerical features ( <strong class="source-inline">int64</strong>) but also text (<strong class="source-inline">object</strong>). We can also see that most of these features are either personal details for an individual, such as their age, or financial information such as credit history or credit amount.</p>
			<p>By completing this exercise, we have successfully loaded the data into the DataFrame and had a first glimpse of the features and information it contains. </p>
			<p>In the topics ahead, we will be looking at preprocessing this data.</p>
			<h1 id="_idParaDest-100">Dat<a id="_idTextAnchor114"/>a Preprocessing</h1>
			<p>Before building a classifier, we need to format our data so that we can keep relevant data in the most suitable format for classification and remove all the data that we are not interested in. </p>
			<p>The following points are the best ways to achieve this:</p>
			<ul>
				<li><strong class="bold">Replacing or dropping values</strong>:<p>For instance, if there are <strong class="source-inline">N/A</strong> (or <strong class="source-inline">NA</strong>) values in the dataset, we may be better off substituting these values with a numeric value we can handle. Recall from the previous chapter that <strong class="source-inline">NA</strong> stands for <strong class="bold">Not Available</strong> and that it represents a missing value. We may choose to ignore rows with <strong class="source-inline">NA</strong> values or replace them with an outlier value. </p><p class="callout-heading">Note</p><p class="callout">An outlier value is a value such as -1,000,000 that clearly stands out from regular values in the dataset. </p><p>The <strong class="source-inline">fillna()</strong> method of a DataFrame does this type of replacement. The replacement of <strong class="source-inline">NA</strong> values with an outlier looks as follows:</p><p class="source-code">df.fillna(-1000000, inplace=True)</p><p>The <strong class="source-inline">fillna()</strong> method changes all <strong class="source-inline">NA</strong> values into numeric values.</p><p>This numeric value should be far from any reasonable value in the DataFrame. Minus one million is recognized by the classifier as an exception, assuming that only positive values are there, as mentioned in the preceding note.</p></li>
				<li><strong class="bold">Dropping rows or columns</strong>:<p>The alternative to replacing missing values with extreme values is simply dropping these rows:</p><p class="source-code">df.dropna(0, inplace=True)</p><p>The first argument (value <strong class="source-inline">0</strong>) specifies that we drop rows, not columns. The second argument (<strong class="source-inline">inplace=True</strong>) specifies that we perform the drop operation without cloning the DataFrame, and will save the result in the same DataFrame. This DataFrame doesn't have any missing values, so the <strong class="source-inline">dropna()</strong> method didn't alter the DataFrame. </p><p class="callout-heading">Note</p><p class="callout">Dropping the <strong class="source-inline">NA</strong> values is less desirable, as you often lose a reasonable chunk of your dataset.</p><p>If there is a column we do not want to include in the classification, we are better off dropping it. Otherwise, the classifier may detect false patterns in places where there is absolutely no correlation. </p><p>For instance, your phone number itself is very unlikely to correlate with your credit score. It is a 9 to 12-digit number that may very easily feed the classifier with a lot of noise. So, we can drop the <strong class="source-inline">telephone</strong> column, as shown in the following code snippet:</p><p class="source-code">df.drop(['telephone'], 1, inplace=True)</p><p>The second argument (value <strong class="source-inline">1</strong>) indicates that we are dropping columns, instead of rows. The first argument is an enumeration of the columns we would like to drop (here, this is <strong class="source-inline">['telephone']</strong>). The <strong class="source-inline">inplace</strong> argument is used so that the call modifies the original DataFrame.</p></li>
				<li><strong class="bold">Transforming data</strong>:<p>Often, the data format we are working with is not always optimal for the classification process. We may want to transform our data into a different format for multiple reasons, such as to highlight aspects of the data we are interested in (for example, Minmax scaling or normalization), to drop aspects of the data we are not interested in (for example, binarization), label encoding to transform categorical variables into numerical ones, and so on.</p><p>Minmax scaling scales each column in the data so that the lowest number in the column becomes 0, the highest number becomes 1, and all of the values in-between are proportionally scaled between 0 and 1.</p><p>This type of operation can be performed by the <strong class="source-inline">MinMaxScaler</strong> method of the scikit-learn <strong class="source-inline">preprocessing</strong> utility, as shown in the following code snippet:</p><p class="source-code">from sklearn import preprocessing</p><p class="source-code">import numpy as np</p><p class="source-code">data = np.array([[19, 65], \</p><p class="source-code">                 [4, 52], \</p><p class="source-code">                 [2, 33]])</p><p class="source-code">preprocessing.MinMaxScaler(feature_range=(0,1)).fit_transform(data)</p><p>The expected output is this:</p><p class="source-code">array([[1.        , 1.        ],</p><p class="source-code">       [0.11764706, 0.59375   ],</p><p class="source-code">       [0.        , 0.        ]])</p><p>Binarization transforms data into ones and zeros based on a condition, as shown in the following code snippet:</p><p class="source-code">preprocessing.Binarizer(threshold=10).transform(data)</p><p>The expected output is this:</p><p class="source-code">array([[1, 1],</p><p class="source-code">       [0, 1],</p><p class="source-code">       [0, 1]])</p></li>
			</ul>
			<p>In the preceding example, we transformed the original data <strong class="source-inline">([19, 65],[4, 52],[2, 33])</strong> into a binary form based on the condition of whether each value is greater than <strong class="source-inline">10</strong> or not (as defined by the <strong class="source-inline">threshold=10</strong> parameter). For instance, the first value, <strong class="source-inline">19</strong>, is above <strong class="source-inline">10</strong>, so it is replaced by <strong class="source-inline">1</strong> in the results.</p>
			<p>Label encoding is important for preparing your features (inputs) for the modeling stage. While some of your features are string labels, scikit-learn algorithms expect this data to be transformed into numbers.</p>
			<p>This is where the <strong class="source-inline">preprocessing</strong> library of scikit-learn comes into play.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You might have noticed that in the credit scoring example, there were two data files. One contained labels in string form, while the other contained labels in integer form. We loaded the data with string labels so that you got some experience of how to preprocess data properly with the label encoder.</p>
			<p>Label encoding is not rocket science. It creates a mapping between string labels and numeric values so that we can supply numbers to scikit-learn, as shown in the following example:</p>
			<p class="source-code">from sklearn import preprocessing</p>
			<p class="source-code">labels = ['Monday', 'Tuesday', 'Wednesday', \</p>
			<p class="source-code">          'Thursday', 'Friday']</p>
			<p class="source-code">label_encoder = preprocessing.LabelEncoder()</p>
			<p class="source-code">label_encoder.fit(labels)</p>
			<p>Let's enumerate the encoding:</p>
			<p class="source-code">[x for x in enumerate(label_encoder.classes_)]</p>
			<p>The expected output is this:</p>
			<p class="source-code">[(0, 'Friday'),</p>
			<p class="source-code"> (1, 'Monday'),</p>
			<p class="source-code"> (2, 'Thursday'),</p>
			<p class="source-code"> (3, 'Tuesday'),</p>
			<p class="source-code"> (4, 'Wednesday')]</p>
			<p>The preceding result shows us that scikit-learn has created a mapping for each day of the week to a respective number; for example, <strong class="source-inline">Friday</strong> will be <strong class="source-inline">0</strong> and <strong class="source-inline">Tuesday</strong> will be <strong class="source-inline">3</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">By default, scikit-learn assigned the mapping number by sorting the original values alphabetically. This is why <strong class="source-inline">Friday</strong> is mapped to <strong class="source-inline">0</strong>.</p>
			<p>Now, we can use this mapping (also called an encoder) to transform data. </p>
			<p>Let's try this out on two examples, <strong class="source-inline">Wednesday</strong> and <strong class="source-inline">Friday</strong>, using the <strong class="source-inline">transform()</strong> method:</p>
			<p class="source-code">label_encoder.transform(['Wednesday', 'Friday'])</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([4, 0], dtype=int64)</p>
			<p>As expected, we got the results <strong class="source-inline">4</strong> and <strong class="source-inline">0</strong>, which are the mapping values for <strong class="source-inline">Wednesday</strong> and <strong class="source-inline">Friday</strong>, respectively.</p>
			<p>We can also use this encoder to perform the inverse transformation with the <strong class="source-inline">inverse_transform</strong> function. Let's try this with the values <strong class="source-inline">0</strong> and <strong class="source-inline">4</strong>:</p>
			<p class="source-code">label_encoder.inverse_transform([0, 4])</p>
			<p>The expected output is this:</p>
			<p class="source-code">array(['Friday', 'Wednesday'], dtype='&lt;U9')</p>
			<p>As expected, we got back the values <strong class="source-inline">Friday</strong> and <strong class="source-inline">Wednesday</strong>. Now, let's practice what we've learned here on the German dataset. </p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor115"/>Exercise 3.02: Applying Label Encoding to Transform Categorical Variables into Numerical Variables</h2>
			<p>In this exercise, we will use one of the preprocessing techniques we just learned, label encoding, to transform all categorical variables into numerical ones. This step is necessary before training any machine learning model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We will be using the same dataset that we used in the previous exercise: the German credit approval dataset: <a href="https://packt.live/3eriWTr">https://packt.live/3eriWTr</a>.</p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Import the <strong class="source-inline">pandas</strong> package as <strong class="source-inline">pd</strong>:<p class="source-code">import pandas as pd</p></li>
				<li>Create a new variable called <strong class="source-inline">file_url</strong>, which will contain the URL to the raw dataset:<p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/'\</p><p class="source-code">           'The-Applied-Artificial-Intelligence-Workshop/'\</p><p class="source-code">           'master/Datasets/german_credit.csv'</p></li>
				<li>Load the data using the <strong class="source-inline">pd.read_csv()</strong> method:<p class="source-code">df = pd.read_csv(file_url)</p></li>
				<li>Import <strong class="source-inline">preprocessing</strong> from <strong class="source-inline">scikit-learn</strong>:<p class="source-code">from sklearn import preprocessing</p></li>
				<li>Define a function called <strong class="source-inline">fit_encoder()</strong> that takes a DataFrame and a column name as parameters and will fit a label encoder on the values of the column. You will use <strong class="source-inline">.LabelEncoder()</strong> and <strong class="source-inline">.fit()</strong> from <strong class="source-inline">preprocessing</strong> and <strong class="source-inline">.unique()</strong> from <strong class="source-inline">pandas</strong> (this will extract all the possible values of a DataFrame column):<p class="source-code">def fit_encoder(dataframe, column):</p><p class="source-code">    encoder = preprocessing.LabelEncoder()</p><p class="source-code">    encoder.fit(dataframe[column].unique())</p><p class="source-code">    return encoder</p></li>
				<li>Define a function called <strong class="source-inline">encode()</strong> that takes a DataFrame, a column name, and a label encoder as parameters and will transform the values of the column using the label encoder. You will use the <strong class="source-inline">.transform()</strong> method to do this:<p class="source-code">def encode(dataframe, column, encoder):</p><p class="source-code">    return encoder.transform(dataframe[column])</p></li>
				<li>Create a new DataFrame called <strong class="source-inline">cat_df</strong> that contains only non-numeric columns and print its first five rows. You will use the <strong class="source-inline">.select_dtypes()</strong> method from pandas and specify <strong class="source-inline">exclude='number'</strong>:<p class="source-code">cat_df = df.select_dtypes(exclude='number')</p><p class="source-code">cat_df.head()</p><p>The expected output (not all columns are shown) is this:</p><div id="_idContainer119" class="IMG---Figure"><img src="image/B16060_03_05.jpg" alt="Figure 3.5: First five rows of the DataFrame containing only non-numeric columns&#13;&#10;"/></div><p class="figure-caption">Figure 3.5: First five rows of the DataFrame containing only non-numeric columns</p></li>
				<li>Create a list called <strong class="source-inline">cat_cols</strong> that contains the column name of <strong class="source-inline">cat_df</strong> and print its content. You will use <strong class="source-inline">.columns</strong> from pandas to do this:<p class="source-code">cat_cols = cat_df.columns</p><p class="source-code">cat_cols</p><p>The expected output is this:</p><p class="source-code">Index(['account_check_status', 'credit_history', 'purpose', </p><p class="source-code">       'savings', 'present_emp_since', 'other_debtors', </p><p class="source-code">       'property', 'other_installment_plans', 'housing', </p><p class="source-code">       'job', 'telephone', 'foreign_worker'], dtype='object')</p></li>
				<li>Create a <strong class="source-inline">for</strong> loop that will iterate through each column from <strong class="source-inline">cat_cols</strong>, fit a label encoder using <strong class="source-inline">fit_encoder()</strong>, and transform the column with the <strong class="source-inline">encode()</strong> function:<p class="source-code">for col in cat_cols:</p><p class="source-code">    label_encoder = fit_encoder(df, col)</p><p class="source-code">    df[col] = encode(df, col, label_encoder)</p></li>
				<li>Print the first five rows of <strong class="source-inline">df</strong>:<p class="source-code">df.head()</p><p>The expected output is this:</p><div id="_idContainer120" class="IMG---Figure"><img src="image/B16060_03_06.jpg" alt="Figure 3.6: First five rows of the encoded DataFrame&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.6: First five rows of the encoded DataFrame</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Njh57h">https://packt.live/2Njh57h</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YZhtx5">https://packt.live/2YZhtx5</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>We have successfully encoded non-numeric columns. Now, our DataFrame contains only numeric values.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor116"/>Identifying Features and Labels</h2>
			<p>Before training our model, we still have to perform two final steps. The first one is to separate our features from the label (also known as a response variable or dependent variable). The <strong class="source-inline">label</strong> column is the one we want our model to predict. For the German credit dataset, in our case, it will be the column called <strong class="source-inline">default</strong>, which tells us whether an individual will present a risk of defaulting or not. </p>
			<p>The features are all the other columns present in the dataset. The model will use the information contained in those columns and find the relevant patterns in order to accurately predict the corresponding label. </p>
			<p>The scikit-learn package requires the labels and features to be stored in two different variables. Luckily, the pandas package provides a method to extract a column from a DataFrame called <strong class="source-inline">.pop()</strong>. </p>
			<p>We will extract the <strong class="source-inline">default</strong> column and store it in a variable called <strong class="source-inline">label</strong>:</p>
			<p class="source-code">label = df.pop('default')</p>
			<p class="source-code">label</p>
			<p>The expected output is this:</p>
			<p class="source-code">0      0</p>
			<p class="source-code">1      1</p>
			<p class="source-code">2      0</p>
			<p class="source-code">3      0</p>
			<p class="source-code">4      1</p>
			<p class="source-code">      ..</p>
			<p class="source-code">995    0</p>
			<p class="source-code">996    0</p>
			<p class="source-code">997    0</p>
			<p class="source-code">998    1</p>
			<p class="source-code">999    0</p>
			<p class="source-code">Name: default, Length: 1000, dtype: int64</p>
			<p>Now, if we look at the content of <strong class="source-inline">df</strong>, we will see that the <strong class="source-inline">default</strong> column is not present anymore:</p>
			<p class="source-code">df.columns</p>
			<p>The expected output is this:</p>
			<p class="source-code">Index(['account_check_status', 'duration_in_month',</p>
			<p class="source-code">       'credit_history', 'purpose', 'credit_amount',</p>
			<p class="source-code">       'savings', 'present_emp_since',</p>
			<p class="source-code">       'installment_as_income_perc', 'other_debtors',</p>
			<p class="source-code">       'present_res_since', 'property', 'age',</p>
			<p class="source-code">       'other_installment_plans', 'housing', </p>
			<p class="source-code">       'credits_this_bank', 'job', 'people_under_maintenance',</p>
			<p class="source-code">       'telephone', 'foreign_worker'],</p>
			<p class="source-code">      dtype='object')</p>
			<p>Now that we have our features and labels ready, we need to split our dataset into training and testing sets.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor117"/>Splitting Data into Training and Testing Using Scikit-Learn</h2>
			<p>The final step that's required before training a classifier is to split our data into training and testing sets. We already saw how to do this in <em class="italic">Chapter 2</em>, <em class="italic">An Introduction to Regression</em>:</p>
			<p class="source-code">from sklearn import model_selection</p>
			<p class="source-code">features_train, features_test, \</p>
			<p class="source-code">label_train, label_test = \</p>
			<p class="source-code">model_selection.train_test_split(df, label, test_size=0.1, \</p>
			<p class="source-code">                                 random_state=8)</p>
			<p>The <strong class="source-inline">train_test_split</strong> method shuffles and then splits our features and labels into a training dataset and a testing dataset. </p>
			<p>We can specify the size of the testing dataset as a number between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. A <strong class="source-inline">test_size</strong> of <strong class="source-inline">0.1</strong> means that <strong class="source-inline">10%</strong> of the data will go into the testing dataset. You can also specify a <strong class="source-inline">random_state</strong> so that you get the exact same split if you run this code again.</p>
			<p>We will use the training set to train our classifier and use the testing set to evaluate its predictive performance. By doing so, we can assess whether our model is overfitting and has learned patterns that are only relevant to the training set.</p>
			<p>In the next section, we will introduce you to the famous k-nearest neighbors classifier.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor118"/>The K-Nearest Neighbors Classifier</h1>
			<p>Now that we have our training and testing data, it is time to prepare our classifier to perform k-nearest neighbor classification. After being introduced to the k-nearest neighbor algorithm, we will use scikit-learn to perform classification.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor119"/>Introducing the K-Nearest Neighbors Algorithm (KNN)</h2>
			<p>The goal of classification algorithms is to divide data so that we can determine which data points belong to which group. </p>
			<p>Suppose that a set of classified points is given to us. Our task is to determine which class a new data point belongs to.</p>
			<p>In order to train a k-nearest neighbor classifier (also referred to as KNN), we need to provide the corresponding class for each observation on the training set, that is, which group it belongs to. The goal of the algorithm is to find the relevant relationship or patterns between the features that will lead to this class. The k-nearest neighbors algorithm is based on a proximity measure that calculates the distance between data points. </p>
			<p>The two most famous proximity (or distance) measures are the Euclidean and the Manhattan distance. We will go through more details in the next section.</p>
			<p>For any new given point, KNN will find its k nearest neighbor, see which class is the most frequent between those k neighbors, and assign it to this new observation. But what is k, you may ask? Determining the value of k is totally arbitrary. You will have to set this value upfront. This is not a parameter that can be learned by the algorithm; it needs to be set by data scientists. This kind of parameter is called a <strong class="bold">hyperparameter</strong>. Theoretically, you can set the value of k to between 1 and positive infinity.</p>
			<p>There are two main best practices to take into consideration:</p>
			<ul>
				<li>k should always be an odd number. The reason behind this is that we want to avoid a situation that ends in a tie. For instance, if you set <em class="italic">k=4</em> and it so happens that two of the neighbors of a point are from class A and the other two are from class B, then KNN doesn't know which class to choose. To avoid this situation, it is better to choose <em class="italic">k=3</em> or <em class="italic">k=5</em>.</li>
				<li>The greater k is, the more accurate KNN will be. For example, if we compare the cases between <em class="italic">k=1 </em>and <em class="italic">k=15</em>, the second one will give you more confidence that KNN will choose the right class as it will need to look at more neighbors before making a decision. On the other hand, with <em class="italic">k=1</em>, it only looks at the closest neighbor and assigns the same class to an observation. But how can we be sure it is not an outlier or a special case? Asking more neighbors will lower the risk of making the wrong decision. But there is a drawback to this: the higher k is, the longer it will take KNN to make a prediction. This is because it will have to perform more calculations to get the distance between all the neighbors of an observation. Due to this, you have to find the sweet spot that will give correct predictions without compromising too much on the time it takes to make a prediction. </li>
			</ul>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor120"/>Distance Metrics With K-Nearest Neighbors Classifier in Scikit-Learn</h2>
			<p>Many distance metrics could work with the k-nearest neighbors algorithm. We will present the two most frequently used ones: the Euclidean distance and the Manhattan distance of two data points.</p>
			<h3 id="_idParaDest-107"><a id="_idTextAnchor121"/>The Euclidean Distance</h3>
			<p>The distance between two points, <strong class="source-inline">A</strong> and <strong class="source-inline">B</strong>, with the coordinates <strong class="source-inline">A=(a1, a2, …, an)</strong> and <strong class="source-inline">B=(b1, b2, …, bn)</strong>, respectively, is the length of the line connecting these two points. For example, if A and B are two-dimensional data points, the Euclidean distance, <strong class="source-inline">d</strong>, will be as follows:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B16060_03_07.jpg" alt="Figure 3.7: Visual representation of the Euclidean distance between A and B&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7: Visual representation of the Euclidean distance between A and B</p>
			<p>The formula to calculate the Euclidean distance is as follows:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B16060_03_08.jpg" alt="Figure 3.8: Distance between points A and B&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8: Distance between points A and B</p>
			<p>As we will be using the Euclidean distance in this book, let's see how we can use scikit-learn to calculate the distance of multiple points. </p>
			<p>We have to import <strong class="source-inline">euclidean_distances</strong> from <strong class="source-inline">sklearn.metrics.pairwise</strong>. This function accepts two sets of points and returns a matrix that contains the pairwise distance of each point from the first and second sets of points. </p>
			<p>Let's take the example of an observation, Z, with coordinates (<strong class="source-inline">4, 4</strong>). Here, we want to calculate the Euclidean distance with 3 others points, A, B, and C, with the coordinates (<strong class="source-inline">2, 3</strong>), (<strong class="source-inline">3, 7</strong>), and (<strong class="source-inline">1, 6</strong>), respectively:</p>
			<p class="source-code">from sklearn.metrics.pairwise import euclidean_distances</p>
			<p class="source-code">observation = [4,4]</p>
			<p class="source-code">neighbors = [[2,3], [3,7], [1,6]]</p>
			<p class="source-code">euclidean_distances([observation], neighbors)</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([[2.23606798, 3.16227766, 3.60555128]])</p>
			<p>Here, the distance of Z=(<strong class="source-inline">4,4</strong>) and B=(<strong class="source-inline">3,7</strong>) is approximately <strong class="source-inline">3.162</strong>, which is what we got in the output. </p>
			<p>We can also calculate the Euclidean distances between points in the same set:</p>
			<p class="source-code">euclidean_distances(neighbors)</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([[0.        , 4.12310563, 3.16227766],</p>
			<p class="source-code">       [4.12310563, 0.        , 2.23606798],</p>
			<p class="source-code">       [3.16227766, 2.23606798, 0.        ]])</p>
			<p>The diagonal that contains value <strong class="source-inline">0</strong> corresponds to the Euclidean distance between each data point and itself. This matrix is symmetric from this diagonal as it calculates the distance of two points and its reverse. For example, the value <strong class="source-inline">4.12310563</strong> on the first row is the distance between A and B, while the same value on the second row corresponds to the distance between B and A.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor122"/>The Manhattan/Hamming Distance</h2>
			<p>The formula of the Manhattan (or Hamming) distance is very similar to the Euclidean distance, but rather than using the square root, it relies on calculating the absolute value of the difference of the coordinates of the data points:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B16060_03_09.jpg" alt="Figure 3.9: The Manhattan and Hamming distance&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9: The Manhattan and Hamming distance</p>
			<p>You can think of the Manhattan distance as if we're using a grid to calculate the distance rather than using a straight line:</p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B16060_03_10.jpg" alt="Figure 3.10: Visual representation of the Manhattan distance between A and B&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10: Visual representation of the Manhattan distance between A and B</p>
			<p>As shown in the preceding plot, the Manhattan distance will follow the path defined by the grid to point B from A. </p>
			<p>Another interesting property is that there can be multiple shortest paths between A and B, but their Manhattan distances will all be equal to each other. In the preceding example, if each cell of the grid equals a unit of 1, then all three of the shortest paths highlighted will have a Manhattan distance of 9.</p>
			<p>The Euclidean distance is a more accurate generalization of distance, while the Manhattan distance is slightly easier to calculate as you only need to find the difference between the absolute value rather than calculating the difference between squares and then taking the root.</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor123"/>Exercise 3.03: Illustrating the K-Nearest Neighbors Classifier Algorithm in Matplotlib</h2>
			<p>Suppose we have a list of employee data. Our features are the number of hours worked per week and the yearly salary. Our label indicates whether an employee has stayed with our company for more than 2 years. The length of stay is represented by zero if it is less than 2 years and one if it is greater than or equal to 2 years.</p>
			<p>We want to create a three-nearest neighbors classifier that determines whether an employee will stay with our company for at least 2 years. </p>
			<p>Then, we would like to use this classifier to predict whether an employee with a request to work 32 hours a week and earning 52,000 dollars per year is going to stay with the company for 2 years or not.</p>
			<p>Follow these steps to complete this exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The aforementioned dataset is available on GitHub at <a href="https://packt.live/2V5VaV9">https://packt.live/2V5VaV9</a>.</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Import the <strong class="source-inline">pandas</strong> package as <strong class="source-inline">pd</strong>:<p class="source-code">import pandas as pd</p></li>
				<li>Create a new variable called <strong class="source-inline">file_url()</strong>, which will contain the URL to the raw dataset:<p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/'\</p><p class="source-code">           'The-Applied-Artificial-Intelligence-Workshop/'\</p><p class="source-code">           'master/Datasets/employees_churned.csv'</p></li>
				<li>Load the data using the <strong class="source-inline">pd.read_csv()</strong> method:<p class="source-code">df = pd.read_csv(file_url)</p></li>
				<li>Print the rows of the DataFrame:<p class="source-code">df</p><p>The expected output is this:</p><div id="_idContainer125" class="IMG---Figure"><img src="image/B16060_03_11.jpg" alt="Figure 3.11: DataFrame of the employees dataset&#13;&#10;"/></div><p class="figure-caption">Figure 3.11: DataFrame of the employees dataset</p></li>
				<li>Import <strong class="source-inline">preprocessing</strong> from <strong class="source-inline">scikit-learn</strong>:<p class="source-code">from sklearn import preprocessing</p></li>
				<li>Instantiate a <strong class="source-inline">MinMaxScaler</strong> with <strong class="source-inline">feature_range=(0,1)</strong> and save it to a variable called <strong class="source-inline">scaler</strong>:<p class="source-code">scaler = preprocessing.MinMaxScaler(feature_range=(0,1))</p></li>
				<li>Scale the DataFrame using <strong class="source-inline">.fit_transform()</strong>, save the results in a new variable called <strong class="source-inline">scaled_employees</strong>, and print its content:<p class="source-code">scaled_employees = scaler.fit_transform(df)</p><p class="source-code">scaled_employees</p><p>The expected output is this:</p><p class="source-code">array([[0.        , 0.18518519, 0.        ],</p><p class="source-code">       [0.2       , 0.        , 0.        ],</p><p class="source-code">       [0.6       , 0.11111111, 0.        ],</p><p class="source-code">       [0.2       , 0.37037037, 0.        ],</p><p class="source-code">       [1.        , 0.18518519, 0.        ],</p><p class="source-code">       [1.        , 0.62962963, 1.        ],</p><p class="source-code">       [1.        , 0.11111111, 1.        ],</p><p class="source-code">       [0.6       , 0.37037037, 1.        ],</p><p class="source-code">       [1.        , 1.        , 1.        ],</p><p class="source-code">       [0.6       , 0.55555556, 1.        ]])</p><p>In the preceding code snippet, we have scaled our original dataset so that all the values range between 0 and 1.</p></li>
				<li>From the scaled data, extract each of the three columns and save them into three variables called <strong class="source-inline">hours_worked</strong>, <strong class="source-inline">salary</strong>, and <strong class="source-inline">over_two_years</strong>, as shown in the following code snippet:<p class="source-code">hours_worked = scaled_employees[:, 0]</p><p class="source-code">salary = scaled_employees[:, 1]</p><p class="source-code">over_two_years = scaled_employees[:, 2]</p></li>
				<li>Import the <strong class="source-inline">matplotlib.pyplot</strong> package as <strong class="source-inline">plt</strong>:<p class="source-code">import matplotlib.pyplot as plt</p></li>
				<li>Create two scatter plots with <strong class="source-inline">plt.scatter</strong> using <strong class="source-inline">hours_worked</strong> as the <em class="italic">x</em>-axis and <strong class="source-inline">salary</strong> as the <em class="italic">y</em>-axis, and then create different markers according to the value of <strong class="source-inline">over_two_years</strong>. You can add the labels for the <em class="italic">x</em> and <em class="italic">y</em> axes with <strong class="source-inline">plt.xlabel</strong> and <strong class="source-inline">plt.ylabel</strong>. Display the scatter plots with <strong class="source-inline">plt.show()</strong>:<p class="source-code">plt.scatter(hours_worked[:5], salary[:5], marker='+')</p><p class="source-code">plt.scatter(hours_worked[5:], salary[5:], marker='o')</p><p class="source-code">plt.xlabel("hours_worked")</p><p class="source-code">plt.ylabel("salary")</p><p class="source-code">plt.show()</p><p>The expected output is this:</p><div id="_idContainer126" class="IMG---Figure"><img src="image/B16060_03_12.jpg" alt="Figure 3.12: Scatter plot of the scaled data&#13;&#10;"/></div><p class="figure-caption">Figure 3.12: Scatter plot of the scaled data</p><p>In the preceding code snippet, we have displayed the data points of the scaled data on a scatter plot. The <strong class="source-inline">+</strong> points represent the employees that stayed less than 2 years, while the <strong class="source-inline">o</strong> ones are for the employees who stayed for more than 2 years. </p><p>Now, let's say we got a new observation and we want to calculate the Euclidean distance with the data from the scaled dataset.</p></li>
				<li>Create a new variable called <strong class="source-inline">observation</strong> with the coordinates <strong class="source-inline">[0.5, 0.26]</strong>:<p class="source-code">observation = [0.5, 0.26]</p></li>
				<li>Import the <strong class="source-inline">euclidean_distances</strong> function from <strong class="source-inline">sklearn.metrics.pairwise</strong>:<p class="source-code">from sklearn.metrics.pairwise import euclidean_distances</p></li>
				<li>Create a new variable called <strong class="source-inline">features</strong>, which will extract the first two columns of the scaled dataset:<p class="source-code">features = scaled_employees[:,:2]</p></li>
				<li>Calculate the Euclidean distance between <strong class="source-inline">observation</strong> and <strong class="source-inline">features</strong> using <strong class="source-inline">euclidean_distances</strong>, save it into a variable called <strong class="source-inline">dist</strong>, and print its value, as shown in the following code snippet:<p class="source-code">dist = euclidean_distances([observation], features)</p><p class="source-code">dist</p><p>The expected output is this:</p><p class="source-code">array([[0.50556627, 0.39698866, 0.17935412, 0.3196586 ,</p><p class="source-code">        0.50556627, 0.62179262, 0.52169714, 0.14893495,</p><p class="source-code">        0.89308454, 0.31201456]])</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3djY1jO">https://packt.live/3djY1jO</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3esx7HF">https://packt.live/3esx7HF</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>From the preceding output, we can see that the three nearest neighbors are as follows: </p>
			<ul>
				<li><strong class="source-inline">0.1564897</strong> for point <strong class="source-inline">[0.6, 0.37037037, 1.]</strong></li>
				<li><strong class="source-inline">0.17114358</strong> for point <strong class="source-inline">[0.6, 0.11111111, 0.]</strong></li>
				<li><strong class="source-inline">0.32150303</strong> for point <strong class="source-inline">[0.6, 0.55555556, 1.]</strong></li>
			</ul>
			<p>If we choose <strong class="source-inline">k=3</strong>, KNN will look at the classes for these three nearest neighbors and since two of them have a label of <strong class="source-inline">1</strong>, it will assign this class to our new observation, <strong class="source-inline">[0.5, 0.26]</strong>. This means that our three-nearest neighbors classifier will classify this new employee as being more likely to stay for at least 2 years.</p>
			<p>By completing this exercise, we saw how a KNN classifier will classify a new observation by finding its three closest neighbors using the Euclidean distance and then assign the most frequent class to it.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor124"/>Parameterization of the K-Nearest Neighbors Classifier in scikit-learn</h2>
			<p>The parameterization of the classifier is where you fine-tune the accuracy of your classifier. Since we haven't learned all of the possible variations of k-nearest neighbors, we will concentrate on the parameters that you will understand based on this topic:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can access the documentation of the k-nearest neighbors classifier here: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html</a>.</p>
			<ul>
				<li><strong class="source-inline">n_neighbors</strong>: This is the k value of the k-nearest neighbors algorithm. The default value is <strong class="source-inline">5</strong>.</li>
				<li><strong class="source-inline">metric</strong>: When creating the classifier, you will see a name – <strong class="source-inline">Minkowski</strong>. Don't worry about this name – you have learned about the first- and second-order Minkowski metrics already. This metric has a <strong class="source-inline">power</strong> parameter. For <strong class="source-inline">p=1</strong>, the Minkowski metric is the same as the Manhattan metric. For <strong class="source-inline">p=2</strong>, the Minkowski metric is the same as the Euclidean metric.</li>
				<li><strong class="source-inline">p</strong>: This is the power of the Minkowski metric. The default value is <strong class="source-inline">2</strong>.</li>
			</ul>
			<p>You have to specify these parameters once you create the classifier:</p>
			<p class="source-code">classifier = neighbors.KNeighborsClassifier(n_neighbors=50, p=2)</p>
			<p>Then, you will have to fit the KNN classifier with your training data:</p>
			<p class="source-code">classifier.fit(features, label)</p>
			<p>The <strong class="source-inline">predict()</strong> method can be used to predict the label for any new data point:</p>
			<p class="source-code">classifier.predict(new_data_point)</p>
			<p>In the next exercise, we will be using the KNN implementation from scikit-learn to automatically find the nearest neighbors and assign corresponding classes.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor125"/>Exercise 3.04: K-Nearest Neighbors Classification in scikit-learn</h2>
			<p>In this exercise, we will use scikit-learn to automatically train a KNN classifier on the German credit approval dataset and try out different values for the <strong class="source-inline">n_neighbors</strong> and <strong class="source-inline">p</strong> hyperparameters to get the optimal output values. We will need to scale the data before fitting KNN.</p>
			<p>Follow these steps to complete this exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This exercise is a follow up from <em class="italic">Exercise 3.02</em>, <em class="italic">Applying Label Encoding to Transform Categorical Variables into Numerical</em>. We already saved the resulting dataset from <em class="italic">Exercise 3.02</em>, <em class="italic">Applying Label Encoding to Transform Categorical Variables into Numerical</em> in the GitHub repository at <a href="https://packt.live/2Yqdb2Q">https://packt.live/2Yqdb2Q</a>.</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook.</li>
				<li>Import the <strong class="source-inline">pandas</strong> package as <strong class="source-inline">pd</strong>:<p class="source-code">import pandas as pd</p></li>
				<li>Create a new variable called <strong class="source-inline">file_url</strong>, which will contain the URL to the raw dataset:<p class="source-code">file_url = 'https://raw.githubusercontent.com/'\</p><p class="source-code">           'PacktWorkshops/'\</p><p class="source-code">           'The-Applied-Artificial-Intelligence-Workshop/'\</p><p class="source-code">           'master/Datasets/german_prepared.csv'</p></li>
				<li>Load the data using the <strong class="source-inline">pd.read_csv()</strong> method:<p class="source-code">df = pd.read_csv(file_url)</p></li>
				<li>Import <strong class="source-inline">preprocessing</strong> from <strong class="source-inline">scikit-learn</strong>:<p class="source-code">from sklearn import preprocessing</p></li>
				<li>Instantiate <strong class="source-inline">MinMaxScaler</strong> with <strong class="source-inline">feature_range=(0,1)</strong> and save it to a variable called <strong class="source-inline">scaler</strong>:<p class="source-code">scaler = preprocessing.MinMaxScaler(feature_range=(0,1))</p></li>
				<li>Fit the scaler and apply the corresponding transformation to the DataFrame using <strong class="source-inline">.fit_transform()</strong> and save the results to a variable called <strong class="source-inline">scaled_credit</strong>:<p class="source-code">scaled_credit = scaler.fit_transform(df)</p></li>
				<li>Extract the <strong class="source-inline">response</strong> variable (the first column) to a new variable called <strong class="source-inline">label</strong>:<p class="source-code">label = scaled_credit[:, 0]</p></li>
				<li>Extract the features (all the columns except for the first one) to a new variable called <strong class="source-inline">features</strong>:<p class="source-code">features = scaled_credit[:, 1:]</p></li>
				<li>Import <strong class="source-inline">model_selection.train_test_split</strong> from <strong class="source-inline">sklearn</strong>:<p class="source-code">from sklearn.model_selection import train_test_split</p></li>
				<li>Split the scaled dataset into training and testing sets with <strong class="source-inline">test_size=0.2</strong> and <strong class="source-inline">random_state=7</strong> using <strong class="source-inline">train_test_split</strong>:<p class="source-code">features_train, features_test, \</p><p class="source-code">label_train, label_test = \</p><p class="source-code">train_test_split(features, label, test_size=0.2, \</p><p class="source-code">                 random_state=7)</p></li>
				<li>Import <strong class="source-inline">neighbors</strong> from <strong class="source-inline">sklearn</strong>:<p class="source-code">from sklearn import neighbors</p></li>
				<li>Instantiate <strong class="source-inline">KNeighborsClassifier</strong> and save it to a variable called <strong class="source-inline">classifier</strong>:<p class="source-code">classifier = neighbors.KNeighborsClassifier()</p></li>
				<li>Fit the k-nearest neighbors classifier on the training set:<p class="source-code">classifier.fit(features_train, label_train)</p><p>Since we have not mentioned the value of k, the default is <strong class="source-inline">5</strong>.</p></li>
				<li>Print the accuracy score for the training set with <strong class="source-inline">.score()</strong>:<p class="source-code">acc_train = classifier.score(features_train, label_train)</p><p class="source-code">acc_train</p><p>You should get the following output:</p><p class="source-code">0.78625</p><p>With this, we've achieved an accuracy score of <strong class="source-inline">0.78625</strong> on the training set with the default hyperparameter values: <em class="italic">k=5 </em>and the Euclidean distance. </p><p>Let's have a look at the score for the testing set.</p></li>
				<li>Print the accuracy score for the testing set with <strong class="source-inline">.score()</strong>:<p class="source-code">acc_test = classifier.score(features_test, label_test)</p><p class="source-code">acc_test</p><p>You should get the following output:</p><p class="source-code">0.75</p><p>The accuracy score dropped to <strong class="source-inline">0.75</strong> on the testing set. This means our model is overfitting and doesn't generalize well to unseen data. In the next activity, we will try different hyperparameter values and see if we can improve this.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ATeluO">https://packt.live/2ATeluO</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2VbDTKx">https://packt.live/2VbDTKx</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we learned how to split a dataset into training and testing sets and fit a KNN algorithm. Our final model can accurately predict whether an individual is more likely to default or not 75% of the time. </p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor126"/>Activity 3.01: Increasing the Accuracy of Credit Scoring</h2>
			<p>In this activity, you will be implementing the parameterization of the k-nearest neighbors classifier and observing the end result. The accuracy of credit scoring is currently 75%. You need to find a way to increase it by a few percentage points.</p>
			<p>You can try different values for k (<strong class="source-inline">5</strong>, <strong class="source-inline">10</strong>, <strong class="source-inline">15</strong>, <strong class="source-inline">25</strong>, and <strong class="source-inline">50</strong>) with the Euclidean and Manhattan distances.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This activity requires you to complete <em class="italic">Exercise 3.04</em>, <em class="italic">K-Nearest Neighbors Classification in scikit-learn</em> first as we will be using the previously prepared data here. </p>
			<p>The following steps will help you complete this activity:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">neighbors</strong> from <strong class="source-inline">sklearn</strong>.</li>
				<li>Create a function to instantiate <strong class="source-inline">KNeighborsClassifier</strong> with hyperparameters specified, fit it with the training data, and return the accuracy score for the training and testing sets.</li>
				<li>Using the function you created, assess the accuracy score for k = (<strong class="source-inline">5</strong>, <strong class="source-inline">10</strong>, <strong class="source-inline">15</strong>, <strong class="source-inline">25</strong>, <strong class="source-inline">50</strong>) for both the Euclidean and Manhattan distances.</li>
				<li>Find the best combination of hyperparameters.</li>
			</ol>
			<p>The expected output is this:</p>
			<p class="source-code">(0.775, 0.785)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 343.</p>
			<p>In the next section, we will introduce you to another machine learning classifier: a <strong class="bold">Support Vector Machine</strong> (<strong class="bold">SVM</strong>).</p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor127"/>Classification with Support Vector Machines</h1>
			<p>We first used SVMs for regression in <em class="italic">Chapter 2</em>, <em class="italic">An Introduction to Regression</em>. In this topic, you will find out how to use SVMs for classification. As always, we will use scikit-learn to run our examples in practice.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor128"/>What Are Support Vector Machine Classifiers?</h2>
			<p>The goal of an SVM is to find a surface in an n-dimensional space that separates the data points in that space into multiple classes.</p>
			<p>In two dimensions, this surface is often a straight line. However, in three dimensions, the SVM often finds a plane. These surfaces are optimal in the sense that they are based on the information available to the machine so that it can optimize the separation of the n-dimensional spaces.</p>
			<p>The optimal separator found by the SVM is called the best separating hyperplane.</p>
			<p>An SVM is used to find one surface that separates two sets of data points. In other words, SVMs are <strong class="bold">binary classifiers</strong>. This does not mean that SVMs can only be used for binary classification. Although we were only talking about one plane, SVMs can be used to partition a space into any number of classes by generalizing the task itself.</p>
			<p>The separator surface is optimal in the sense that it maximizes the distance of each data point from the separator surface.</p>
			<p>A vector is a mathematical structure defined on an n-dimensional space that has a magnitude (length) and a direction. In two dimensions, you draw the vector (<em class="italic">x, y</em>) from the origin to the point (x, y). Based on geometry, you can calculate the length of the vector using the Pythagorean theorem and the direction of the vector by calculating the angle between the horizontal axis and the vector. </p>
			<p>For instance, in two dimensions, the vector (3, -4) has the following magnitude:</p>
			<p class="source-code">np.sqrt( 3 * 3 + 4 * 4 )</p>
			<p>The expected output is this:</p>
			<p class="source-code">5.0</p>
			<p>It has the following direction (in degrees):</p>
			<p class="source-code">np.arctan(-4/3) / 2 / np.pi * 360</p>
			<p>The expected output is this:</p>
			<p class="source-code">-53.13010235415597</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor129"/>Understanding Support Vector Machines</h2>
			<p>Suppose that two sets of points with two different classes, 0 and 1, are given. For simplicity, we can imagine a two-dimensional plane with two features: one mapped on the horizontal axis and one mapped on the vertical axis.</p>
			<p>The objective of the SVM is to find the best separating line that separates points <strong class="source-inline">A</strong>, <strong class="source-inline">D</strong>, <strong class="source-inline">C</strong>, <strong class="source-inline">B</strong>, and <strong class="source-inline">H</strong>, which all belong to class 0, from points <strong class="source-inline">E</strong>, <strong class="source-inline">F</strong>, and <strong class="source-inline">G</strong>, which are of class 1:</p>
			<p> </p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B16060_03_13.jpg" alt="Figure 3.13: Line separating red and blue members&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13: Line separating red and blue members</p>
			<p>But separation is not always that obvious. For instance, if there is a new point of class 0 in-between <strong class="source-inline">E</strong>, <strong class="source-inline">F</strong>, and <strong class="source-inline">G</strong>, there is no line that could separate all the points without causing errors. If the points from class 0 form a full circle around the class 1 points, there is no straight line that could separate the two sets:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B16060_03_14.jpg" alt="Figure 3.14: Graph with two outlier points&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14: Graph with two outlier points</p>
			<p>For instance, in the preceding graph, we tolerate two outlier points, <strong class="source-inline">O</strong> and <strong class="source-inline">P</strong>.</p>
			<p>In the following solution, we do not tolerate outliers, and instead of a line, we create the best separating path consisting of two half-lines:</p>
			<p> </p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B16060_03_15.jpg" alt="Figure 3.15: Graph removing the separation of the two outliers&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15: Graph removing the separation of the two outliers</p>
			<p>The perfect separation of all data points is rarely worth the resources. Therefore, the SVM can be regularized to simplify and restrict the definition of the best separating shape and allow outliers. </p>
			<p>The regularization parameter of an SVM determines the rate of errors to allow or forbid misclassifications. </p>
			<p>An SVM has a kernel parameter. A linear kernel strictly uses a linear equation to describe the best separating hyperplane. A polynomial kernel uses a polynomial, while an exponential kernel uses an exponential expression to describe the hyperplane.</p>
			<p>A margin is an area centered around the separator and is bounded by the points closest to the separator. A balanced margin has points from each class that are equidistant from the line.</p>
			<p>When it comes to defining the allowed error rate of the best separating hyperplane, a gamma parameter decides whether only the points near the separator count in determining the position of the separator, or whether the points farthest from the line count, too. The higher the gamma, the lower the number of points that influence the location of the separator.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor130"/>Support Vector Machines in scikit-learn</h2>
			<p>Our entry point is the end result of <em class="italic">Activity 3.02</em>, <em class="italic">Support Vector Machine Optimization in scikit-learn</em>. Once we have split the training and test data, we are ready to set up the classifier:</p>
			<p class="source-code">features_train, features_test, \</p>
			<p class="source-code">label_train, label_test = \</p>
			<p class="source-code">model_selection.train_test_split(scaled_features, label,\</p>
			<p class="source-code">                                 test_size=0.2)</p>
			<p>Instead of using the k-nearest neighbors classifier, we will use the <strong class="source-inline">svm.SVC()</strong> classifier:</p>
			<p class="source-code">from sklearn import svm</p>
			<p class="source-code">classifier = svm.SVC()</p>
			<p class="source-code">classifier.fit(features_train, label_train)</p>
			<p class="source-code">classifier.score(features_test, label_test)</p>
			<p>The expected output is this:</p>
			<p class="source-code">0.745</p>
			<p>It seems that the default SVM classifier of scikit-learn does a slightly better job than the k-nearest neighbors classifier.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor131"/>Parameters of the scikit-learn SVM</h2>
			<p>The following are the parameters of the scikit-learn SVM:</p>
			<ul>
				<li><strong class="source-inline">kernel</strong>: This is a string or callable parameter specifying the kernel that's being used in the algorithm. The predefined kernels are <strong class="source-inline">linear</strong>, <strong class="source-inline">poly</strong>, <strong class="source-inline">rbf</strong>, <strong class="source-inline">sigmoid</strong>, and <strong class="source-inline">precomputed</strong>. The default value is <strong class="source-inline">rbf</strong>.</li>
				<li><strong class="source-inline">degree</strong>: When using a polynomial, you can specify the degree of the polynomial. The default value is <strong class="source-inline">3</strong>.</li>
				<li><strong class="source-inline">gamma</strong>: This is the kernel coefficient for <strong class="source-inline">rbf</strong>, <strong class="source-inline">poly</strong>, and <strong class="source-inline">sigmoid</strong>. The default value is <strong class="source-inline">auto</strong>, which is computed as <em class="italic">1/number_of_features</em>. </li>
				<li><strong class="source-inline">C</strong>: This is a floating-point number with a default of <strong class="source-inline">1.0</strong> that describes the penalty parameter of the error term.<p class="callout-heading">Note</p><p class="callout">You can read about the parameters in the reference documentation at <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html</a>. </p></li>
			</ul>
			<p>Here is an example of an SVM:</p>
			<p class="source-code">classifier = svm.SVC(kernel="poly", C=2, degree=4, gamma=0.05)</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor132"/>Activity 3.02: Support Vector Machine Optimization in scikit-learn</h2>
			<p>In this activity, you will be us<a id="_idTextAnchor133"/>ing, comparing, and contrasting the different SVMs' classifier parameters. With this, you will find a set of parameters resulting in the highest classification data on the training and testing data that we loaded and prepared in <em class="italic">Activity 3.01,</em> <em class="italic">Increasing the Accuracy of Credit Scoring</em>. </p>
			<p>You must different combinations of hyperparameters for SVM:</p>
			<ul>
				<li><strong class="source-inline">kernel="linear"</strong></li>
				<li><strong class="source-inline">kernel="poly", C=1, degree=4, gamma=0.05</strong></li>
				<li><strong class="source-inline">kernel="poly", C=1, degree=4, gamma=0.05</strong></li>
				<li><strong class="source-inline">kernel="poly", C=1, degree=4, gamma=0.25</strong></li>
				<li><strong class="source-inline">kernel="poly", C=1, degree=4, gamma=0.5</strong></li>
				<li><strong class="source-inline">kernel="poly", C=1, degree=4, gamma=0.16</strong></li>
				<li><strong class="source-inline">kernel="sigmoid"</strong></li>
				<li><strong class="source-inline">kernel="rbf", gamma=0.15</strong></li>
				<li><strong class="source-inline">kernel="rbf", gamma=0.25</strong></li>
				<li><strong class="source-inline">kernel="rbf", gamma=0.5</strong></li>
				<li><strong class="source-inline">kernel="rbf", gamma=0.35</strong></li>
			</ul>
			<p>The following steps will help you complete this activity:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file and execute all the steps mentioned in the previous, <em class="italic">Exercise 3.04</em>, <em class="italic">K-Nearest Neighbor Classification in scikit-learn</em>.</li>
				<li>Import <strong class="source-inline">svm</strong> from <strong class="source-inline">sklearn</strong>.</li>
				<li>Create a function to instantiate an SVC with the hyperparameters specified, fit with the training data, and return the accuracy score for the training and testing sets.</li>
				<li>Using the function you created, assess the accuracy scores for the different hyperparameter combinations.</li>
				<li>Find the best combination of hyperparameters.</li>
			</ol>
			<p>The expected output is this:</p>
			<p class="source-code">(0.78125, 0.775)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 347.</p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor134"/>Summary</h1>
			<p>In this chapter, we learned about the basics of classification and the difference between regression problems. Classification is about predicting a response variable with limited possible values. As for any data science project, data scientists need to prepare the data before training a model. In this chapter, we learned how to standardize numerical values and replace missing values. Then, you were introduced to the famous k-nearest neighbors algorithm and discovered how it uses distance metrics to find the closest neighbors to a data point and then assigns the most frequent class among them. We also learned how to apply an SVM to a classification problem and tune some of its hyperparameters to improve the performance of the model and reduce overfitting.</p>
			<p>In the next chapter, we will walk you through a different type of algorithm, called decision trees.</p>
		</div>
	</body></html>