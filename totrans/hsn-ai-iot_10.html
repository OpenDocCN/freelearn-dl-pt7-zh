<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">AI for the Industrial IoT</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Companies from a diverse background today are realizing the importance of </span><strong><span class="koboSpan" id="kobo.3.1">Artificial Intelligence</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">AI</span></strong><span class="koboSpan" id="kobo.6.1">), and hence, are incorporating it into their ecosystems. </span><span class="koboSpan" id="kobo.6.2">This chapter focuses on some of the successful AI-powered </span><strong><span class="koboSpan" id="kobo.7.1">industrial IoT</span></strong><span class="koboSpan" id="kobo.8.1"> solutions. </span><span class="koboSpan" id="kobo.8.2">By the end of this chapter, you will have covered the following:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">How AI-powered IoT solutions are changing the industry</span></li>
<li><span class="koboSpan" id="kobo.10.1">Different industries offering AI-enabled analysis for their data to increase production, optimize logistics, and improve the customer experience</span></li>
<li><span class="koboSpan" id="kobo.11.1">Preventive maintenance</span></li>
<li><span class="koboSpan" id="kobo.12.1">Implementing a code to perform preventive maintenance based on aircraft engine sensors data</span></li>
<li><span class="koboSpan" id="kobo.13.1">Electrical load forecasting</span></li>
<li><span class="koboSpan" id="kobo.14.1">Implementing a TensorFlow code to perform short-term load forecasting</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Introduction to AI-powered industrial IoT</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The convergence of IoT, robotics, big data, and </span><strong><span class="koboSpan" id="kobo.3.1">machine learning</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">ML</span></strong><span class="koboSpan" id="kobo.6.1">) is creating enormous opportunities for industrial firms as well as significant challenges.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The availability of low-cost sensors, multiple cloud platforms, and powerful edge infrastructure is making it easier and profitable for industries to adopt AI. </span><span class="koboSpan" id="kobo.7.2">This AI-powered industrial IoT is transforming the way companies provide products and services or interact with customers and partners.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.8.1">One of the promising areas of the AI-powered industrial IoT is </span><strong><span class="koboSpan" id="kobo.9.1">preventive maintenance</span></strong><span class="koboSpan" id="kobo.10.1">. </span><span class="koboSpan" id="kobo.10.2">Until now, industrial firms used to be reactive concerning maintenance, in the sense that they will perform maintenance either as a part of a fixed schedule, such as every six months, or only when some equipment stops functioning. </span><span class="koboSpan" id="kobo.10.3">For instance, a logistics company may have biannual service checks of every vehicle in its fleet and replace certain parts or entire vehicles on a set schedule. </span><span class="koboSpan" id="kobo.10.4">This reactive maintenance often wastes time and can be expensive. </span><span class="koboSpan" id="kobo.10.5">Applying AI algorithms to predict anomalies and errors before they happen can save a lot of time.</span></p>
<p><span class="koboSpan" id="kobo.11.1">Another area where the AI-powered industrial IoT can achieve miracles is collaboration among humans and robots. </span><span class="koboSpan" id="kobo.11.2">Robots are already part of the industrial IoT ecosystem; working in assembly-lines and warehouses, they perform tasks that are especially repetitive or dangerous for human workers. </span><span class="koboSpan" id="kobo.11.3">The semi-autonomous trucks, trains, and loaders that are presently part of the mining industry are typically guided by pre-programmed routines, fixed tracks, and/or remote human operators.</span></p>
<p><span class="koboSpan" id="kobo.12.1">In many industry situations, the latency introduced by cloud computations may not be acceptable, in such situations, the edge of computation infrastructure is needed.</span></p>
<p><span><span class="koboSpan" id="kobo.13.1">To provide you with an idea of the spread and usage of AI-powered industrial IoT, the following is a list of some of the hot AI-powered startups providing industrial IoT services and solutions:</span></span></p>
<ul>
<li><span><strong><span class="koboSpan" id="kobo.14.1">Uptake Technologies Inc</span></strong><span class="koboSpan" id="kobo.15.1">: A Chicago based startup, co-founded in 2014 by Brad Keywell, makes software to monitor and analyze real-time data generated by industrial equipment and uses it to improve the performance and maintenance of the machinery. </span><span class="koboSpan" id="kobo.15.2">It is planning to expand its horizon to heavy target industries such as energy, railroads, oil and gas, mining, and wind power (</span><a href="https://www.uptake.com/"><span class="koboSpan" id="kobo.16.1">https://www.uptake.com/</span></a><span class="koboSpan" id="kobo.17.1">).</span></span></li>
<li><span><strong><span class="koboSpan" id="kobo.18.1">C3.ai</span></strong><span class="koboSpan" id="kobo.19.1">: A leading provider of big data, IoT, and AI applications, led by Thomas Siebel, has been declared a leader in the IoT platform by Forrester Research 2018 industrial IoT Wave report. Founded in the year 2009, it has successfully provided industries services in the field of energy management, network efficiency, fraud detection, and inventory optimization (</span><a href="https://c3.ai"><span class="koboSpan" id="kobo.20.1">https://c3.ai</span></a><span class="koboSpan" id="kobo.21.1">). </span></span></li>
<li><strong><span class="koboSpan" id="kobo.22.1">Alluvium</span></strong><span class="koboSpan" id="kobo.23.1">: Founded in 2015 by Drew Conway, the author of </span><em><span class="koboSpan" id="kobo.24.1">Machine Learning for Hackers</span></em><span class="koboSpan" id="kobo.25.1">, Alluvium uses ML and AI to help industrial companies achieve operation stability and improve their production. </span><span class="koboSpan" id="kobo.25.2">Their flagship product, Primer, helps companies identify the useful insights from the raw and distilled data from the sensors, allowing them to predict operational faults before they happen </span><span><span class="koboSpan" id="kobo.26.1">(</span></span><a href="https://alluvium.io"><span class="koboSpan" id="kobo.27.1">https://alluvium.io</span></a><span><span class="koboSpan" id="kobo.28.1">)</span></span><span class="koboSpan" id="kobo.29.1">.</span></li>
<li><strong><span class="koboSpan" id="kobo.30.1">Arundo Analytics</span></strong><span class="koboSpan" id="kobo.31.1">: </span><span><span class="koboSpan" id="kobo.32.1">Headed by Jakob Ramsøy, founded in the year 2015, Arundo Analytics provides services to connect live data to ML and other analytical models. </span><span class="koboSpan" id="kobo.32.2">They have products to scale deployed models, create and manage live data pipelines (</span><a href="https://www.arundo.com"><span class="koboSpan" id="kobo.33.1">https://www.arundo.com</span></a><span class="koboSpan" id="kobo.34.1">).</span></span></li>
<li><span><strong><span class="koboSpan" id="kobo.35.1">Canvass Analytics</span></strong><span class="koboSpan" id="kobo.36.1">: It helps industries make critical business decisions using predictive analytics based on real-time operational data. The Canvass AI Platform distils the millions of data points generated by industrial machines, sensors, and operations systems and identifies patterns and correlations within the data to create new insights. </span><span class="koboSpan" id="kobo.36.2">Headed by Humera Malik, Canvass Analytics was founded in 2016 (</span><a href="https://www.canvass.io"><span class="koboSpan" id="kobo.37.1">https://www.canvass.io</span></a><span class="koboSpan" id="kobo.38.1">). </span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.39.1">That is not the end software technology giants such as Amazon and Google are spending a lot of funds and infrastructure in industrial IoT. Google is using predictive modeling to reduce their data center cost, and PayPal is using ML to find fraudulent transactions.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Some interesting use cases</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">A large number of companies from diverse backgrounds are realizing the importance and impact of incorporating data analysis and AI into their eco-systems. </span><span class="koboSpan" id="kobo.2.2">From increasing their operations, supply chain, and maintenance efficiency to increasing employee productivity, to creating new business models, products, and services, there is not a facet where AI has not been explored. </span><span class="koboSpan" id="kobo.2.3">Following, we list some of the interesting use cases of AI-powered IoT in industries:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">Predictive maintenance</span></strong><span class="koboSpan" id="kobo.4.1">: In predictive maintenance, AI algorithms are used to predict future failures of equipment before the failure occurs. </span><span class="koboSpan" id="kobo.4.2">This allows the company to perform maintenance, and hence, reduce the downtime. </span><span class="koboSpan" id="kobo.4.3">In the successive section, we will go into more details of how preventive maintenance is helpful for the industries and what are the various ways in which it can be done.</span></li>
<li><strong><span class="koboSpan" id="kobo.5.1">Asset tracking</span></strong><span class="koboSpan" id="kobo.6.1">: Also called </span><strong><span class="koboSpan" id="kobo.7.1">asset management</span></strong><span class="koboSpan" id="kobo.8.1">, this is the method to keep track of key physical assets. </span><span class="koboSpan" id="kobo.8.2">Keeping track of key assets, a company can optimize logistics, maintain inventory levels, and detect any inefficiencies. </span><span class="koboSpan" id="kobo.8.3">Traditionally, asset tracking was limited to adding RFID or barcodes to the assets, and hence, keeping a tab on their location, however, with the AI algorithms at our perusal, it is now possible to do more active asset tracking. </span><span class="koboSpan" id="kobo.8.4">For instance, a windmill power station can sense the change in wind speed, its direction, and even the temperature and use these parameters to align the individual windmill in the best direction to maximize power generation. </span></li>
<li><strong><span class="koboSpan" id="kobo.9.1">Fleet management and maintenance</span></strong><span class="koboSpan" id="kobo.10.1">: The transport industry had been using AI for fleet management by optimizing routes for about a decade. </span><span class="koboSpan" id="kobo.10.2">The availability of many low-cost sensors and the advancement of edge computing devices have now made it possible for transport companies to collect and use the data received from these sensors to not only optimize the logistics by better vehicle to vehicle communication and preventive maintenance, but to accelerate safety. </span><span class="koboSpan" id="kobo.10.3">Installing systems such as drowsiness detection, the risky behavior caused due to fatigue or distraction can be detected, and the driver can be asked to take countermeasures.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Predictive maintenance using AI</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">Heavy machinery and equipment are the backbone of any industry and like all physical objects, they deteriorate, age, and fail. </span><span class="koboSpan" id="kobo.2.2">Initially, companies used to perform reactive maintenance, that is, maintenance was done once the equipment failure was reported. </span><span class="koboSpan" id="kobo.2.3">This used to cause unplanned downtime. </span></span><span class="koboSpan" id="kobo.3.1">For any industry, an unscheduled, unplanned downtime can cause significant resource crunch and drastically reduce efficiency, production, and hence, profits. </span><span class="koboSpan" id="kobo.3.2">To deal with these problems, industries shifted to preventive maintenance. </span></p>
<p><span class="koboSpan" id="kobo.4.1">In preventive maintenance, regular scheduled routine checks are performed at predetermined intervals. </span><span class="koboSpan" id="kobo.4.2">Preventive maintenance required keeping a record of equipment and their scheduled maintenance. </span><span class="koboSpan" id="kobo.4.3">The third </span><span><span class="koboSpan" id="kobo.5.1">industrial revolution, where computers were introduced into industries, made it easy to maintain and update these records. </span></span><span class="koboSpan" id="kobo.6.1">While preventive maintenance saves the industry from most unplanned downtimes, it still isn't the best alternative, since regular checks can be an unnecessary expenditure. </span><span class="koboSpan" id="kobo.6.2">The following diagram outlines an example of the four industrial revolutions:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1"><img class="aligncenter size-full wp-image-1133 image-border" src="assets/43713621-93d7-4bd5-a566-6b8ef8defa0c.png" style="width:29.25em;height:14.17em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.8.1"> Image shared under Creative Commons Attribution: (https://commons.wikimedia.org/wiki/File:Industry_4.0.png)</span></div>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.9.1">The current trend of automation and digitalization has lead to the fourth industrial revolution, also called </span><strong><span class="koboSpan" id="kobo.10.1">Industry 4.0</span></strong><span class="koboSpan" id="kobo.11.1">. </span><span class="koboSpan" id="kobo.11.2">This has allowed companies to deploy </span><strong><span class="koboSpan" id="kobo.12.1">machine-to-machine</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong><span class="koboSpan" id="kobo.14.1">M2M</span></strong><span class="koboSpan" id="kobo.15.1">) and </span><strong><span class="koboSpan" id="kobo.16.1">machine-to-human</span></strong><span class="koboSpan" id="kobo.17.1"> (</span><strong><span class="koboSpan" id="kobo.18.1">M2H</span></strong><span class="koboSpan" id="kobo.19.1">) communication, along with AI-powered analytical algorithms, enabling predictive maintenance, that predict the breakdown before it occurs using past data. </span><span class="koboSpan" id="kobo.19.2">Predictive maintenance strategies have enormously eased the maintenance and management of the company resources. </span></p>
<p><span class="koboSpan" id="kobo.20.1">The main idea behind predictive maintenance is to predict when equipment breakdown might occur based on condition-monitoring data. </span><span class="koboSpan" id="kobo.20.2">The sensors are used to monitor the condition and performance of equipment during their normal operation, depending on the equipment, different types of sensors may be used. </span><span class="koboSpan" id="kobo.20.3">Some of the common condition monitoring parameters/sensor values are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.21.1">Vibration sensors mainly used to detect misalignment, imbalance, mechanical looseness, or wear on pumps and motors</span></li>
<li><span class="koboSpan" id="kobo.22.1">Current/voltage sensors to measure the current and voltage supplied to an electric motor</span></li>
<li><span class="koboSpan" id="kobo.23.1">Ultrasound analysis to detect leakage in pipe systems or tanks, or mechanical malfunctions of movable parts and faults in electrical equipment</span></li>
<li><span class="koboSpan" id="kobo.24.1">Infrared thermography to identify temperature fluctuations</span></li>
<li><span class="koboSpan" id="kobo.25.1">Sensors to detect liquid quality (for example in the case of wine sensors to detect the presence of different elements in the wine) </span></li>
</ul>
<p><span class="koboSpan" id="kobo.26.1">To implement predictive maintenance, the most important thing is to identify the conditions that need to be monitored. </span><span class="koboSpan" id="kobo.26.2">The sensors required to monitor these conditions are then deployed. </span><span class="koboSpan" id="kobo.26.3">Finally, the data from the sensors is collected to build a model. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Predictive maintenance using Long Short-Term Memory</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">For demonstrating predictive maintenance, we'll use the simulated data provided in Azure ML (</span><a href="https://gallery.azure.ai/Collection/Predictive-Maintenance-Template-3"><span class="koboSpan" id="kobo.3.1">https://gallery.azure.ai/Collection/Predictive-Maintenance-Template-3</span></a><a href="https://gallery.azure.ai/Collection/Predictive-Maintenance-Template-3"><span class="koboSpan" id="kobo.4.1">).</span></a><span class="koboSpan" id="kobo.5.1"> The dataset consists of the following three files:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.6.1">Training data</span></strong><span class="koboSpan" id="kobo.7.1">: It contains the aircraft engine run to failure data. </span><span class="koboSpan" id="kobo.7.2">The download link for the data is </span><a href="http://azuremlsamples.azureml.net/templatedata/PM_train.txt"><span class="koboSpan" id="kobo.8.1">http://azuremlsamples.azureml.net/templatedata/PM_train.txt</span></a><span class="koboSpan" id="kobo.9.1">.</span></li>
<li><strong><span class="koboSpan" id="kobo.10.1">Testing data</span></strong><span class="koboSpan" id="kobo.11.1">: It contains the aircraft engine operating data without failure events recorded. </span><span class="koboSpan" id="kobo.11.2">The data can be loaded from the link: </span><a href="http://azuremlsamples.azureml.net/templatedata/PM_test.txt"><span class="koboSpan" id="kobo.12.1">http://azuremlsamples.azureml.net/templatedata/PM_test.txt</span></a><span class="koboSpan" id="kobo.13.1">.</span></li>
<li><strong><span class="koboSpan" id="kobo.14.1">Ground truth data</span></strong><span class="koboSpan" id="kobo.15.1">: Here, the information about the true remaining cycles for each engine in testing data is available. </span><span class="koboSpan" id="kobo.15.2">The link for the ground truth data is </span><a href="http://azuremlsamples.azureml.net/templatedata/PM_truth.txt"><span class="koboSpan" id="kobo.16.1">http://azuremlsamples.azureml.net/templatedata/PM_truth.txt</span></a><span class="koboSpan" id="kobo.17.1">.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.18.1">According to the data description provided at the data source, the training data (</span><kbd><span class="koboSpan" id="kobo.19.1">train_FD001.txt</span></kbd><span class="koboSpan" id="kobo.20.1">) consists of multiple multivariate time series with cycle as the time unit, together with 21 sensor readings for each cycle. </span><span class="koboSpan" id="kobo.20.2">Each time series can be assumed as being generated from a different engine of the same type. </span><span class="koboSpan" id="kobo.20.3">Each engine is assumed to start with different degrees of initial wear and manufacturing variation, and this information is unknown to the user. </span><span class="koboSpan" id="kobo.20.4">In this simulated data, the engine is assumed to be operating normally at the start of each time series. </span><span class="koboSpan" id="kobo.20.5">It starts to degrade at some point during the series of the operating cycles. </span><span class="koboSpan" id="kobo.20.6">This degrades the progresses and grows in magnitude. </span><span class="koboSpan" id="kobo.20.7">When a predefined threshold is reached, then the engine is considered unsafe for further operation. </span><span class="koboSpan" id="kobo.20.8">In other words, the last cycle in each time series can be considered as the failure point of the corresponding engine. </span><span class="koboSpan" id="kobo.20.9">Taking the sample training data as an example, the engine with </span><kbd><span class="koboSpan" id="kobo.21.1">id=1</span></kbd><span class="koboSpan" id="kobo.22.1"> fails at cycle </span><kbd><span class="koboSpan" id="kobo.23.1">192</span></kbd><span class="koboSpan" id="kobo.24.1">, and engine with </span><kbd><span class="koboSpan" id="kobo.25.1">id=2</span></kbd><span class="koboSpan" id="kobo.26.1"> fails at cycle </span><kbd><span class="koboSpan" id="kobo.27.1">287</span></kbd><span class="koboSpan" id="kobo.28.1">.</span></p>
<p><span class="koboSpan" id="kobo.29.1">The testing data (</span><kbd><span class="koboSpan" id="kobo.30.1">test_FD001.txt</span></kbd><span class="koboSpan" id="kobo.31.1">) has the same data schema as the training data. </span><span class="koboSpan" id="kobo.31.2">The only difference is that the data does not indicate when the failure occurs (in other words, the last time period does NOT represent the failure point). </span><span class="koboSpan" id="kobo.31.3">Taking the sample testing data, the engine with </span><kbd><span class="koboSpan" id="kobo.32.1">id=1</span></kbd><span class="koboSpan" id="kobo.33.1"> runs from cycle </span><kbd><span class="koboSpan" id="kobo.34.1">1</span></kbd><span class="koboSpan" id="kobo.35.1"> through cycle </span><kbd><span class="koboSpan" id="kobo.36.1">31</span></kbd><span class="koboSpan" id="kobo.37.1">. </span><span class="koboSpan" id="kobo.37.2">It is not shown how many more cycles this engine can last before it fails.</span></p>
<p><span class="koboSpan" id="kobo.38.1">The ground truth data (</span><kbd><span class="koboSpan" id="kobo.39.1">RUL_FD001.txt</span></kbd><span class="koboSpan" id="kobo.40.1">) provides the number of remaining working cycles for the engines in the testing data. </span><span class="koboSpan" id="kobo.40.2">Taking the sample ground truth data shown as an example, the engine with </span><kbd><span class="koboSpan" id="kobo.41.1">id=1</span></kbd><span class="koboSpan" id="kobo.42.1"> in the testing data can run another </span><kbd><span class="koboSpan" id="kobo.43.1">112</span></kbd><span class="koboSpan" id="kobo.44.1"> cycles before it fails.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.45.1">Since this is a time series data, we will use </span><strong><span class="koboSpan" id="kobo.46.1">Long Short-Term Memory</span></strong><span class="koboSpan" id="kobo.47.1"> (</span><strong><span class="koboSpan" id="kobo.48.1">LSTM</span></strong><span class="koboSpan" id="kobo.49.1">) to classify weather the engine will fail in a certain time period or not. </span><span class="koboSpan" id="kobo.49.2">The code presented here is based on the implementation provided at the GitHub link of Umberto Griffo: (</span><a href="https://github.com/umbertogriffo/Predictive-Maintenance-using-LSTM"><span class="koboSpan" id="kobo.50.1">https://github.com/umbertogriffo/Predictive-Maintenance-using-LSTM</span></a><span class="koboSpan" id="kobo.51.1">):</span></p>
<ol>
<li><span class="koboSpan" id="kobo.52.1">The modules needed to implement predictive maintenance are imported in the first step. </span><span class="koboSpan" id="kobo.52.2">We also set the seed for random calculations so that the result is reproducible:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.53.1">import keras</span><br/><span class="koboSpan" id="kobo.54.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.55.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.56.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.57.1">import os</span><br/><br/><span class="koboSpan" id="kobo.58.1"># Setting seed for reproducibility</span><br/><span class="koboSpan" id="kobo.59.1">np.random.seed(1234) </span><br/><span class="koboSpan" id="kobo.60.1">PYTHONHASHSEED = 0</span><br/><br/><span class="koboSpan" id="kobo.61.1">from sklearn import preprocessing</span><br/><span class="koboSpan" id="kobo.62.1">from sklearn.metrics import confusion_matrix, recall_score, precision_score</span><br/><span class="koboSpan" id="kobo.63.1">from keras.models import Sequential,load_model</span><br/><span class="koboSpan" id="kobo.64.1">from keras.layers import Dense, Dropout, LSTM</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.65.1">Let's read the data and assign column names, shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.66.1"># read training data - It is the aircraft engine run-to-failure data.</span><br/><span class="koboSpan" id="kobo.67.1">train_df = pd.read_csv('PM_train.txt', sep=" ",</span><br/><span class="koboSpan" id="kobo.68.1">         header=None)</span><br/><span class="koboSpan" id="kobo.69.1">train_df.drop(train_df.columns[[26, 27]], </span><br/><span class="koboSpan" id="kobo.70.1">        axis=1, </span><br/><span class="koboSpan" id="kobo.71.1">        inplace=True)</span><br/><span class="koboSpan" id="kobo.72.1">train_df.columns = ['id', 'cycle', 'setting1',</span><br/><span class="koboSpan" id="kobo.73.1">         'setting2', 'setting3', 's1', 's2',</span><br/><span class="koboSpan" id="kobo.74.1">         's3', 's4', 's5', 's6', 's7', 's8',</span><br/><span class="koboSpan" id="kobo.75.1">         's9', 's10', 's11', 's12', 's13', </span><br/><span class="koboSpan" id="kobo.76.1">        's14', 's15', 's16', 's17', 's18', </span><br/><span class="koboSpan" id="kobo.77.1">        's19', 's20', 's21']</span><br/><br/><span class="koboSpan" id="kobo.78.1">train_df = train_df.sort_values(['id','cycle'])</span><br/><br/><span class="koboSpan" id="kobo.79.1"># read test data - It is the aircraft engine operating data without failure events recorded.</span><br/><span class="koboSpan" id="kobo.80.1">test_df = pd.read_csv('PM_test.txt', </span><br/><span class="koboSpan" id="kobo.81.1">        sep=" ", header=None)</span><br/><span class="koboSpan" id="kobo.82.1">test_df.drop(test_df.columns[[26, 27]], </span><br/><span class="koboSpan" id="kobo.83.1">        axis=1, </span><br/><span class="koboSpan" id="kobo.84.1">        inplace=True)</span><br/><span class="koboSpan" id="kobo.85.1">test_df.columns = ['id', 'cycle', 'setting1', </span><br/><span class="koboSpan" id="kobo.86.1">        'setting2', 'setting3', 's1', 's2', 's3',</span><br/><span class="koboSpan" id="kobo.87.1">         's4', 's5', 's6', 's7', 's8', 's9', </span><br/><span class="koboSpan" id="kobo.88.1">        's10', 's11', 's12', 's13', 's14',</span><br/><span class="koboSpan" id="kobo.89.1">         's15', 's16', 's17', 's18', 's19', </span><br/><span class="koboSpan" id="kobo.90.1">        's20', 's21']</span><br/><br/><span class="koboSpan" id="kobo.91.1"># read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.</span><br/><span class="koboSpan" id="kobo.92.1">truth_df = pd.read_csv('PM_truth.txt', </span><br/><span class="koboSpan" id="kobo.93.1">        sep=" ", </span><br/><span class="koboSpan" id="kobo.94.1">        header=None)</span><br/><span class="koboSpan" id="kobo.95.1">truth_df.drop(truth_df.columns[[1]], </span><br/><span class="koboSpan" id="kobo.96.1">        axis=1, </span><br/><span class="koboSpan" id="kobo.97.1">        inplace=True)</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.98.1">As the first step, we make a prediction whether the engine will fail in the time period or not, hence our label will be </span><kbd><span class="koboSpan" id="kobo.99.1">1</span></kbd><span class="koboSpan" id="kobo.100.1"> or </span><kbd><span class="koboSpan" id="kobo.101.1">0</span></kbd><span class="koboSpan" id="kobo.102.1">, that is, this will be a binary classification problem. </span><span class="koboSpan" id="kobo.102.2">To create the binary label, we preprocess the data and we create a new label </span><strong><span class="koboSpan" id="kobo.103.1">remaining useful life</span></strong><span class="koboSpan" id="kobo.104.1"> (</span><strong><span class="koboSpan" id="kobo.105.1">RUL</span></strong><span class="koboSpan" id="kobo.106.1">). </span><span class="koboSpan" id="kobo.106.2">We also create a binary </span><kbd><span class="koboSpan" id="kobo.107.1">label1</span></kbd><span class="koboSpan" id="kobo.108.1"> variable telling if a specific engine is going to fail within </span><kbd><span class="koboSpan" id="kobo.109.1">w1</span></kbd><span class="koboSpan" id="kobo.110.1"> cycles or not. </span><span class="koboSpan" id="kobo.110.2">And finally, the data (non-sensor) is normalized, shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.111.1"># Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)</span><br/><span class="koboSpan" id="kobo.112.1">rul = pd.DataFrame(train_df.groupby('id')</span><br/><span class="koboSpan" id="kobo.113.1">        ['cycle'].max()).reset_index()</span><br/><span class="koboSpan" id="kobo.114.1">rul.columns = ['id', 'max']</span><br/><span class="koboSpan" id="kobo.115.1">train_df = train_df.merge(rul, </span><br/><span class="koboSpan" id="kobo.116.1">        on=['id'], </span><br/><span class="koboSpan" id="kobo.117.1">        how='left')</span><br/><span class="koboSpan" id="kobo.118.1">train_df['RUL'] = train_df['max'] -     train_df['cycle']</span><br/><span class="koboSpan" id="kobo.119.1">train_df.drop('max', </span><br/><span class="koboSpan" id="kobo.120.1">        axis=1, </span><br/><span class="koboSpan" id="kobo.121.1">        inplace=True)</span><br/><br/><span class="koboSpan" id="kobo.122.1"># Let us generate label columns for training data</span><br/><span class="koboSpan" id="kobo.123.1"># we will only use "label1" for binary classification, </span><br/><span class="koboSpan" id="kobo.124.1"># The question: is a specific engine going to fail within w1 cycles?</span><br/><span class="koboSpan" id="kobo.125.1">w1 = 30</span><br/><span class="koboSpan" id="kobo.126.1">w0 = 15</span><br/><span class="koboSpan" id="kobo.127.1">train_df['label1'] = np.where(train_df['RUL'] &lt;= w1, 1, 0 )</span><br/><br/><span class="koboSpan" id="kobo.128.1"># MinMax normalization (from 0 to 1)</span><br/><span class="koboSpan" id="kobo.129.1">train_df['cycle_norm'] = train_df['cycle']</span><br/><span class="koboSpan" id="kobo.130.1">cols_normalize = train_df.columns.difference</span><br/><span class="koboSpan" id="kobo.131.1">        (['id','cycle','RUL','label1'])</span><br/><span class="koboSpan" id="kobo.132.1">min_max_scaler = preprocessing.MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.133.1">norm_train_df = pd.DataFrame(min_max_scaler.</span><br/><span class="koboSpan" id="kobo.134.1">        fit_transform(train_df[cols_normalize]), </span><br/><span class="koboSpan" id="kobo.135.1">        columns=cols_normalize, </span><br/><span class="koboSpan" id="kobo.136.1">        index=train_df.index)</span><br/><span class="koboSpan" id="kobo.137.1">join_df = train_df[train_df.columns.</span><br/><span class="koboSpan" id="kobo.138.1">        difference(cols_normalize)].</span><br/><span class="koboSpan" id="kobo.139.1">        join(norm_train_df)</span><br/><span class="koboSpan" id="kobo.140.1">train_df = join_df.reindex(columns = train_df.columns)</span><br/><br/><span class="koboSpan" id="kobo.141.1">train_df.head()</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.142.1"><img src="assets/db9d096a-4f3b-4f94-8bfa-f43be21b0d05.png" style="width:49.75em;height:15.00em;"/></span></p>
<ol start="4">
<li><span class="koboSpan" id="kobo.143.1">Similar preprocessing is performed on the test dataset, with just one change—the RUL value is obtained from the ground truth data:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.144.1"># MinMax normalization (from 0 to 1)</span><br/><span class="koboSpan" id="kobo.145.1">test_df['cycle_norm'] = test_df['cycle']</span><br/><span class="koboSpan" id="kobo.146.1">norm_test_df = pd.DataFrame(</span><br/><span class="koboSpan" id="kobo.147.1">        min_max_scaler.</span><br/><span class="koboSpan" id="kobo.148.1">        transform(test_df[cols_normalize]), </span><br/><span class="koboSpan" id="kobo.149.1">        columns=cols_normalize,     </span><br/><span class="koboSpan" id="kobo.150.1">         index=test_df.index)</span><br/><span class="koboSpan" id="kobo.151.1">test_join_df = test_df[test_df.</span><br/><span class="koboSpan" id="kobo.152.1">        columns.difference(cols_normalize)].</span><br/><span class="koboSpan" id="kobo.153.1">        join(norm_test_df)</span><br/><span class="koboSpan" id="kobo.154.1">test_df = test_join_df.</span><br/><span class="koboSpan" id="kobo.155.1">        reindex(columns = test_df.columns)</span><br/><span class="koboSpan" id="kobo.156.1">test_df = test_df.reset_index(drop=True)</span><br/><br/><br/><span class="koboSpan" id="kobo.157.1"># We use the ground truth dataset to generate labels for the test data.</span><br/><span class="koboSpan" id="kobo.158.1"># generate column max for test data</span><br/><span class="koboSpan" id="kobo.159.1">rul = pd.DataFrame(test_df.</span><br/><span class="koboSpan" id="kobo.160.1">        groupby('id')['cycle'].max()).</span><br/><span class="koboSpan" id="kobo.161.1">        reset_index()</span><br/><span class="koboSpan" id="kobo.162.1">rul.columns = ['id', 'max']</span><br/><span class="koboSpan" id="kobo.163.1">truth_df.columns = ['more']</span><br/><span class="koboSpan" id="kobo.164.1">truth_df['id'] = truth_df.index + 1</span><br/><span class="koboSpan" id="kobo.165.1">truth_df['max'] = rul['max'] + truth_df['more']</span><br/><span class="koboSpan" id="kobo.166.1">truth_df.drop('more', </span><br/><span class="koboSpan" id="kobo.167.1">        axis=1, </span><br/><span class="koboSpan" id="kobo.168.1">        inplace=True)</span><br/><br/><span class="koboSpan" id="kobo.169.1"># generate RUL for test data</span><br/><span class="koboSpan" id="kobo.170.1">test_df = test_df.merge(truth_df, </span><br/><span class="koboSpan" id="kobo.171.1">        on=['id'], how='left')</span><br/><span class="koboSpan" id="kobo.172.1">test_df['RUL'] = test_df['max'] - test_df['cycle']</span><br/><span class="koboSpan" id="kobo.173.1">test_df.drop('max', </span><br/><span class="koboSpan" id="kobo.174.1">        axis=1, </span><br/><span class="koboSpan" id="kobo.175.1">        inplace=True)</span><br/><br/><span class="koboSpan" id="kobo.176.1"># generate label columns w0 and w1 for test data</span><br/><span class="koboSpan" id="kobo.177.1">test_df['label1'] = np.where</span><br/><span class="koboSpan" id="kobo.178.1">        (test_df['RUL'] &lt;= w1, 1, 0 )</span><br/><span class="koboSpan" id="kobo.179.1">test_df.head()</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.180.1"><img src="assets/af874577-0db2-4421-9166-23f0ab836a8a.png" style="width:49.50em;height:15.17em;"/></span></p>
<ol start="5">
<li><span class="koboSpan" id="kobo.181.1">Since we are using LSTM for time-series modeling, we create a function that will generate the sequence to be fed to the LSTM as per the window size. </span><span class="koboSpan" id="kobo.181.2">We have chosen the window size of </span><kbd><span class="koboSpan" id="kobo.182.1">50</span></kbd><span class="koboSpan" id="kobo.183.1">. </span><span class="koboSpan" id="kobo.183.2">We will also need a function to generate the corresponding label:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.184.1"># function to reshape features into </span><br/><span class="koboSpan" id="kobo.185.1"># (samples, time steps, features) </span><br/><span class="koboSpan" id="kobo.186.1">def gen_sequence(id_df, seq_length, seq_cols):</span><br/><span class="koboSpan" id="kobo.187.1">    """ Only sequences that meet the window-length</span><br/><span class="koboSpan" id="kobo.188.1">    are considered, no padding is used. </span><span class="koboSpan" id="kobo.188.2">This </span><br/><span class="koboSpan" id="kobo.189.1">    means for testing we need to drop those which </span><br/><span class="koboSpan" id="kobo.190.1">    are below the window-length. </span><span class="koboSpan" id="kobo.190.2">An alternative</span><br/><span class="koboSpan" id="kobo.191.1">    would be to pad sequences so that</span><br/><span class="koboSpan" id="kobo.192.1">    we can use shorter ones """</span><br/><br/><span class="koboSpan" id="kobo.193.1">    # for one id we put all the rows in a single matrix</span><br/><span class="koboSpan" id="kobo.194.1">    data_matrix = id_df[seq_cols].values</span><br/><span class="koboSpan" id="kobo.195.1">    num_elements = data_matrix.shape[0]</span><br/><span class="koboSpan" id="kobo.196.1">    # Iterate over two lists in parallel.</span><br/><span class="koboSpan" id="kobo.197.1">    # For example id1 have 192 rows and </span><br/><span class="koboSpan" id="kobo.198.1">    # sequence_length is equal to 50</span><br/><span class="koboSpan" id="kobo.199.1">    # so zip iterate over two following list of </span><br/><span class="koboSpan" id="kobo.200.1">    # numbers (0,112),(50,192)</span><br/><span class="koboSpan" id="kobo.201.1">    # 0 50 -&gt; from row 0 to row 50</span><br/><span class="koboSpan" id="kobo.202.1">    # 1 51 -&gt; from row 1 to row 51</span><br/><span class="koboSpan" id="kobo.203.1">    # 2 52 -&gt; from row 2 to row 52</span><br/><span class="koboSpan" id="kobo.204.1">    # ...</span><br/><span class="koboSpan" id="kobo.205.1">    # 111 191 -&gt; from row 111 to 191</span><br/><span class="koboSpan" id="kobo.206.1">    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):</span><br/><span class="koboSpan" id="kobo.207.1">        yield data_matrix[start:stop, :]</span><br/><br/><span class="koboSpan" id="kobo.208.1">def gen_labels(id_df, seq_length, label):</span><br/><span class="koboSpan" id="kobo.209.1">    # For one id we put all the labels in a </span><br/><span class="koboSpan" id="kobo.210.1">    # single matrix.</span><br/><span class="koboSpan" id="kobo.211.1">    # For example:</span><br/><span class="koboSpan" id="kobo.212.1">    # [[1]</span><br/><span class="koboSpan" id="kobo.213.1">    # [4]</span><br/><span class="koboSpan" id="kobo.214.1">    # [1]</span><br/><span class="koboSpan" id="kobo.215.1">    # [5]</span><br/><span class="koboSpan" id="kobo.216.1">    # [9]</span><br/><span class="koboSpan" id="kobo.217.1">    # ...</span><br/><span class="koboSpan" id="kobo.218.1">    # [200]] </span><br/><span class="koboSpan" id="kobo.219.1">    data_matrix = id_df[label].values</span><br/><span class="koboSpan" id="kobo.220.1">    num_elements = data_matrix.shape[0]</span><br/><span class="koboSpan" id="kobo.221.1">    # I have to remove the first seq_length labels</span><br/><span class="koboSpan" id="kobo.222.1">    # because for one id the first sequence of </span><br/><span class="koboSpan" id="kobo.223.1">    # seq_length size have as target</span><br/><span class="koboSpan" id="kobo.224.1">    # the last label (the previus ones are </span><br/><span class="koboSpan" id="kobo.225.1">    # discarded).</span><br/><span class="koboSpan" id="kobo.226.1">    # All the next id's sequences will have </span><br/><span class="koboSpan" id="kobo.227.1">    # associated step by step one label as target. </span><br/><span class="koboSpan" id="kobo.228.1">    return data_matrix[seq_length:num_elements, :]</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="6">
<li><span class="koboSpan" id="kobo.229.1">Let's now generate the training sequence and corresponding label for our data, shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.230.1"># pick a large window size of 50 cycles</span><br/><span class="koboSpan" id="kobo.231.1">sequence_length = 50</span><br/><br/><span class="koboSpan" id="kobo.232.1"># pick the feature columns </span><br/><span class="koboSpan" id="kobo.233.1">sensor_cols = ['s' + str(i) for i in range(1,22)]</span><br/><span class="koboSpan" id="kobo.234.1">sequence_cols = ['setting1', 'setting2', </span><br/><span class="koboSpan" id="kobo.235.1">        'setting3', 'cycle_norm']</span><br/><span class="koboSpan" id="kobo.236.1">sequence_cols.extend(sensor_cols)</span><br/><br/><span class="koboSpan" id="kobo.237.1"># generator for the sequences</span><br/><span class="koboSpan" id="kobo.238.1">seq_gen = (list(gen_sequence</span><br/><span class="koboSpan" id="kobo.239.1">        (train_df[train_df['id']==id], </span><br/><span class="koboSpan" id="kobo.240.1">        sequence_length, sequence_cols)) </span><br/><span class="koboSpan" id="kobo.241.1">        for id in train_df['id'].unique())</span><br/><br/><span class="koboSpan" id="kobo.242.1"># generate sequences and convert to numpy array</span><br/><span class="koboSpan" id="kobo.243.1">seq_array = np.concatenate(list(seq_gen)).</span><br/><span class="koboSpan" id="kobo.244.1">        astype(np.float32)</span><br/><span class="koboSpan" id="kobo.245.1">print(seq_array.shape)</span><br/><br/><span class="koboSpan" id="kobo.246.1"># generate labels</span><br/><span class="koboSpan" id="kobo.247.1">label_gen = [gen_labels(train_df[train_df['id']==id], </span><br/><span class="koboSpan" id="kobo.248.1">        sequence_length, ['label1']) </span><br/><span class="koboSpan" id="kobo.249.1">        for id in train_df['id'].unique()]</span><br/><span class="koboSpan" id="kobo.250.1">label_array = np.concatenate(label_gen).</span><br/><span class="koboSpan" id="kobo.251.1">        astype(np.float32)</span><br/><span class="koboSpan" id="kobo.252.1">print(label_array.shape)</span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.253.1">We now build an LSTM model with two LSTM layers and a fully connected layer. </span><span class="koboSpan" id="kobo.253.2">The model is trained for binary classification, and therefore, it tries to reduce the binary cross entropy loss. </span><span class="koboSpan" id="kobo.253.3">The </span><kbd><span class="koboSpan" id="kobo.254.1">Adam</span></kbd><span class="koboSpan" id="kobo.255.1"> optimizer is used to update the model parameters:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.256.1">nb_features = seq_array.shape[2]</span><br/><span class="koboSpan" id="kobo.257.1">nb_out = label_array.shape[1]</span><br/><br/><span class="koboSpan" id="kobo.258.1">model = Sequential()</span><br/><br/><span class="koboSpan" id="kobo.259.1">model.add(LSTM(</span><br/><span class="koboSpan" id="kobo.260.1">     input_shape=(sequence_length, nb_features),</span><br/><span class="koboSpan" id="kobo.261.1">     units=100,</span><br/><span class="koboSpan" id="kobo.262.1">     return_sequences=True))</span><br/><span class="koboSpan" id="kobo.263.1">model.add(Dropout(0.2))</span><br/><br/><span class="koboSpan" id="kobo.264.1">model.add(LSTM(</span><br/><span class="koboSpan" id="kobo.265.1">     units=50,</span><br/><span class="koboSpan" id="kobo.266.1">     return_sequences=False))</span><br/><span class="koboSpan" id="kobo.267.1">model.add(Dropout(0.2))</span><br/><br/><span class="koboSpan" id="kobo.268.1">model.add(Dense(units=nb_out,</span><br/><span class="koboSpan" id="kobo.269.1">     activation='sigmoid'))</span><br/><span class="koboSpan" id="kobo.270.1">model.compile(loss='binary_crossentropy', </span><br/><span class="koboSpan" id="kobo.271.1">    optimizer='adam', </span><br/><span class="koboSpan" id="kobo.272.1">    metrics=['accuracy'])</span><br/><br/><span class="koboSpan" id="kobo.273.1">print(model.summary())</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.274.1"><img class="aligncenter size-full wp-image-1134 image-border" src="assets/39a6a841-8fcb-499c-8944-b60c0a07e23e.png" style="width:45.42em;height:21.58em;"/></span></p>
<ol start="8">
<li><span class="koboSpan" id="kobo.275.1">We train the model, shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.276.1">history = model.fit(seq_array, label_array, </span><br/><span class="koboSpan" id="kobo.277.1">        epochs=100, batch_size=200, </span><br/><span class="koboSpan" id="kobo.278.1">        validation_split=0.05, verbose=2,</span><br/><span class="koboSpan" id="kobo.279.1">         callbacks = [keras.callbacks.</span><br/><span class="koboSpan" id="kobo.280.1">            EarlyStopping(monitor='val_loss', </span><br/><span class="koboSpan" id="kobo.281.1">            min_delta=0, patience=10, </span><br/><span class="koboSpan" id="kobo.282.1">            verbose=0, mode='min'),</span><br/><span class="koboSpan" id="kobo.283.1">        keras.callbacks.</span><br/><span class="koboSpan" id="kobo.284.1">            ModelCheckpoint</span><br/><span class="koboSpan" id="kobo.285.1">            (model_path,monitor='val_loss',     </span><br/><span class="koboSpan" id="kobo.286.1">            save_best_only=True, </span><br/><span class="koboSpan" id="kobo.287.1">            mode='min', verbose=0)])    </span></pre>
<ol start="9">
<li><span class="koboSpan" id="kobo.288.1">The trained model gives 98% accuracy on the test dataset and 98.9% accuracy on the validation dataset. </span><span class="koboSpan" id="kobo.288.2">The precision value is </span><kbd><span class="koboSpan" id="kobo.289.1">0.96</span></kbd><span class="koboSpan" id="kobo.290.1">, and there is a recall of </span><kbd><span class="koboSpan" id="kobo.291.1">1.0</span></kbd><span class="koboSpan" id="kobo.292.1"> and an F1 score of </span><kbd><span class="koboSpan" id="kobo.293.1">0.98</span></kbd><span class="koboSpan" id="kobo.294.1">. </span><span class="koboSpan" id="kobo.294.2">Not bad, right! </span><span class="koboSpan" id="kobo.294.3">The following diagram shows these results of the train model:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.295.1"><img class="aligncenter size-full wp-image-1135 image-border" src="assets/d3461ac4-07d5-4701-9a62-a70015086c06.png" style="width:46.58em;height:25.33em;"/></span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.296.1">We can use the same data to also </span><span><span class="koboSpan" id="kobo.297.1">predict the RUL of the aircraft engines, that is, predict the engines time to failure. </span><span class="koboSpan" id="kobo.297.2">This will be a regression problem now we can use the LSTM model to perform regression as well. </span><span class="koboSpan" id="kobo.297.3">The initial steps will be the same as before, but from the fifth step onwards we will have changes. </span><span class="koboSpan" id="kobo.297.4">While the input data sequence generated will remain the same as before, the target will no longer be the binary label, instead, we will use RUL as the target for our regression model:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.298.1">We create the target value using the same </span><kbd><span class="koboSpan" id="kobo.299.1">gen_labels()</span></kbd><span class="koboSpan" id="kobo.300.1"> function. </span><span class="koboSpan" id="kobo.300.2">We also create a validation set using the </span><kbd><span class="koboSpan" id="kobo.301.1">gen_sequence()</span></kbd><span class="koboSpan" id="kobo.302.1"> function:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.303.1"># generate labels</span><br/><span class="koboSpan" id="kobo.304.1">label_gen = [gen_labels(train_df[train_df['id']==id],</span><br/><span class="koboSpan" id="kobo.305.1">        sequence_length, ['RUL']) </span><br/><span class="koboSpan" id="kobo.306.1">        for id in train_df['id'].unique()]</span><br/><span class="koboSpan" id="kobo.307.1">label_array = np.concatenate(label_gen).astype(np.float32)</span><br/><br/><span class="koboSpan" id="kobo.308.1"># val is a list of 192 - 50 = 142 bi-dimensional array </span><br/><span class="koboSpan" id="kobo.309.1"># (50 rows x 25 columns)</span><br/><span class="koboSpan" id="kobo.310.1">val=list(gen_sequence(train_df[train_df['id']==1], </span><br/><span class="koboSpan" id="kobo.311.1">        sequence_length, sequence_cols))</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.312.1">Create an LSTM model. </span><span class="koboSpan" id="kobo.312.2">We are using </span><kbd><span class="koboSpan" id="kobo.313.1">r2</span></kbd><span class="koboSpan" id="kobo.314.1"> as the metrics during training, therefore, we use the Keras custom metric feature and our own metrics function:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.315.1">def r2_keras(y_true, y_pred):</span><br/><span class="koboSpan" id="kobo.316.1">     """Coefficient of Determination </span><br/><span class="koboSpan" id="kobo.317.1">     """</span><br/><span class="koboSpan" id="kobo.318.1">     SS_res = K.sum(K.square( y_true - y_pred ))</span><br/><span class="koboSpan" id="kobo.319.1">     SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )</span><br/><span class="koboSpan" id="kobo.320.1">     return ( 1 - SS_res/(SS_tot + K.epsilon()) )</span><br/><br/><span class="koboSpan" id="kobo.321.1"># Next, we build a deep network. </span><br/><span class="koboSpan" id="kobo.322.1"># The first layer is an LSTM layer with 100 units followed by </span><br/><span class="koboSpan" id="kobo.323.1"># another LSTM layer with 50 units. </span><br/><span class="koboSpan" id="kobo.324.1"># Dropout is also applied after each LSTM layer to control </span><br/><span class="koboSpan" id="kobo.325.1"># overfitting. </span><br/><span class="koboSpan" id="kobo.326.1"># Final layer is a Dense output layer with single unit and linear </span><br/><span class="koboSpan" id="kobo.327.1"># activation since this is a regression problem.</span><br/><span class="koboSpan" id="kobo.328.1">nb_features = seq_array.shape[2]</span><br/><span class="koboSpan" id="kobo.329.1">nb_out = label_array.shape[1]</span><br/><br/><br/><span class="koboSpan" id="kobo.330.1">model = Sequential()</span><br/><span class="koboSpan" id="kobo.331.1">model.add(LSTM(</span><br/><span class="koboSpan" id="kobo.332.1">     input_shape=(sequence_length, nb_features),</span><br/><span class="koboSpan" id="kobo.333.1">     units=100,</span><br/><span class="koboSpan" id="kobo.334.1">     return_sequences=True))</span><br/><span class="koboSpan" id="kobo.335.1">model.add(Dropout(0.2))</span><br/><span class="koboSpan" id="kobo.336.1">model.add(LSTM(</span><br/><span class="koboSpan" id="kobo.337.1">     units=50,</span><br/><span class="koboSpan" id="kobo.338.1">     return_sequences=False))</span><br/><span class="koboSpan" id="kobo.339.1">model.add(Dropout(0.2))</span><br/><span class="koboSpan" id="kobo.340.1">model.add(Dense(units=nb_out))</span><br/><span class="koboSpan" id="kobo.341.1">model.add(Activation("linear"))</span><br/><span class="koboSpan" id="kobo.342.1">model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])</span><br/><br/><br/><span class="koboSpan" id="kobo.343.1">print(model.summary())</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.344.1"><img src="assets/48c70894-7fbe-4469-be8a-77a44b46f56c.png" style="width:49.17em;height:21.08em;"/></span></p>
<ol start="3">
<li><span class="koboSpan" id="kobo.345.1">Train the model on the training dataset, shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.346.1"># fit the network</span><br/><span class="koboSpan" id="kobo.347.1">history = model.fit(seq_array, label_array, epochs=100, </span><br/><span class="koboSpan" id="kobo.348.1">    batch_size=200, validation_split=0.05, verbose=2,</span><br/><span class="koboSpan" id="kobo.349.1">    callbacks = [keras.callbacks.EarlyStopping</span><br/><span class="koboSpan" id="kobo.350.1">    (monitor='val_loss', min_delta=0, patience=10, </span><br/><span class="koboSpan" id="kobo.351.1">    verbose=0, mode='min'),</span><br/><span class="koboSpan" id="kobo.352.1">    keras.callbacks.ModelCheckpoint</span><br/><span class="koboSpan" id="kobo.353.1">    (model_path,monitor='val_loss', </span><br/><span class="koboSpan" id="kobo.354.1">    save_best_only=True, mode='min', </span><br/><span class="koboSpan" id="kobo.355.1">    verbose=0)])</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.356.1">The trained model provides an </span><kbd><span class="koboSpan" id="kobo.357.1">r2</span></kbd><span class="koboSpan" id="kobo.358.1"> value of 0.80 on test dataset and 0.72 on the validation dataset. </span><span class="koboSpan" id="kobo.358.2">We can improve our results by hypertuning the model parameters. </span><span class="koboSpan" id="kobo.358.3">Following, you can see the loss of the model for train and validation datasets during training:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.359.1"><img class="aligncenter size-full wp-image-1136 image-border" src="assets/6285b89e-5545-4260-9e20-e9297f4e6d08.png" style="width:43.17em;height:41.83em;"/></span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.360.1">To run this code, please ensure you have Tensorflow 1.4&lt;, and Keras 2.1.2. </span><span class="koboSpan" id="kobo.360.2">If you have a higher version of Keras, first uninstall it using </span><kbd><span class="koboSpan" id="kobo.361.1">pip uninstall keras</span></kbd><span class="koboSpan" id="kobo.362.1"> and then reinstall it using </span><kbd><span class="koboSpan" id="kobo.363.1">pip install keras==2.1.2</span></kbd><span class="koboSpan" id="kobo.364.1">.</span></div>
<p><span class="koboSpan" id="kobo.365.1">The complete code with both the binary classification and regression model are available at the GitHub repository, </span><kbd><span class="koboSpan" id="kobo.366.1">Chapter10/Predictive_Maintenance_using_LSTM.ipynb</span></kbd><span class="koboSpan" id="kobo.367.1">. </span><span class="koboSpan" id="kobo.367.2">We can also create a model to determine if the failure will occur in different time windows, for example, fails in the window </span><em><span class="koboSpan" id="kobo.368.1">(1,w</span><sub><span class="koboSpan" id="kobo.369.1">0</span></sub><span class="koboSpan" id="kobo.370.1">)</span></em><span class="koboSpan" id="kobo.371.1"> or fails in the window (</span><em><span class="koboSpan" id="kobo.372.1">w</span><sub><span class="koboSpan" id="kobo.373.1">0+1</span></sub><span class="koboSpan" id="kobo.374.1">, w</span><sub><span class="koboSpan" id="kobo.375.1">1</span></sub></em><span class="koboSpan" id="kobo.376.1">) days, and so on. </span><span class="koboSpan" id="kobo.376.2">This will then be a multi-classification problem, and data will need to be preprocessed accordingly. </span><span class="koboSpan" id="kobo.376.3">You can read more about this predictive maintenance template from Azure AI Gallery: </span><a href="https://gallery.azure.ai/Experiment/Predictive-Maintenance-Step-2A-of-3-train-and-evaluate-regression-models-2"><span class="koboSpan" id="kobo.377.1">https://gallery.azure.ai/Experiment/Predictive-Maintenance-Step-2A-of-3-train-and-evaluate-regression-models-2</span></a><span class="koboSpan" id="kobo.378.1">. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Predictive maintenance advantages and disadvantages</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">According to a survey report by GE (</span><a href="https://www.gemeasurement.com/sites/gemc.dev/files/ge_the_impact_of_digital_on_unplanned_downtime_0.pdf"><span class="koboSpan" id="kobo.3.1">https://www.gemeasurement.com/sites/gemc.dev/files/ge_the_impact_of_digital_on_unplanned_downtime_0.pdf</span></a><span class="koboSpan" id="kobo.4.1">). </span><span class="koboSpan" id="kobo.4.2">The downtime negatively affects the performance of the oil and gas industry. </span><span class="koboSpan" id="kobo.4.3">This is true, not only for the oil and gas industry, but all industries. </span><span class="koboSpan" id="kobo.4.4">Hence, to reduce downtime, and increase efficiency, it is important that predictive maintenance is adopted. </span><span class="koboSpan" id="kobo.4.5">However, the cost of establishing predictive maintenance is quite high, but once a predictive maintenance system has been properly established, it helps to provide several cost-effective benefits, such as the following:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.5.1">Minimized time is required for equipment maintenance</span></li>
<li><span class="koboSpan" id="kobo.6.1">Minimum production time is lost due to maintenance</span></li>
<li><span class="koboSpan" id="kobo.7.1">And finally, the spare parts cost is also minimized</span></li>
</ul>
<p><span><span class="koboSpan" id="kobo.8.1">Successful predictive maintenance can reshape the company as a whole in a positive way. </span></span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Electrical load forecasting in industry</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Electricity is presently the most important energy vector in both the domestic and industrial sectors. </span><span class="koboSpan" id="kobo.2.2">Since, unlike fuels, it is hard and expensive to store electricity, there is a need for a precise coupling between its generation and demand. </span><span class="koboSpan" id="kobo.2.3">Electrical energy load forecasting, hence, is very vital. </span><span class="koboSpan" id="kobo.2.4">Depending upon the time range (forecasting horizon) electrical load forecasting is classified into the following three categories:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">Short-term load forecasting</span></strong><span class="koboSpan" id="kobo.4.1">: The forecast is made for one hour to a few weeks</span></li>
<li><strong><span class="koboSpan" id="kobo.5.1">Medium-term load forecasting</span></strong><span class="koboSpan" id="kobo.6.1">: The forecast duration spreads from a few weeks to a few months</span></li>
<li><strong><span class="koboSpan" id="kobo.7.1">Long-term load forecasting</span></strong><span class="koboSpan" id="kobo.8.1">: Here, the forecasting is done from a few months to years</span></li>
</ul>
<p><span class="koboSpan" id="kobo.9.1">Depending upon the need and application one may have to plan either one or all of the previous load forecasting categories. </span><span class="koboSpan" id="kobo.9.2">In recent years, a lot of research work has been done in the area of </span><strong><span class="koboSpan" id="kobo.10.1">short-term load forecasting</span></strong><span class="koboSpan" id="kobo.11.1"> (</span><strong><span class="koboSpan" id="kobo.12.1">STLF</span></strong><span class="koboSpan" id="kobo.13.1">). </span><span class="koboSpan" id="kobo.13.2">STLF can assist industries by providing an accurate means to predict future load, which can help in precise planning, decrease in operating cost, and thus, increase profit and provide a more reliable electrical supply. </span><span class="koboSpan" id="kobo.13.3">STLF predicts the future energy demands based on historical data (acquired through smart meters) and predicted whether conditions. </span></p>
<p><span class="koboSpan" id="kobo.14.1">The load forecasting problem is a regression problem. </span><span class="koboSpan" id="kobo.14.2">It can be modeled as a time series problem or as a static model. </span><span class="koboSpan" id="kobo.14.3">Modeling load forecasting as a time series data is the most popular choice. </span><span class="koboSpan" id="kobo.14.4">With time series modeling, we can use the standard ML time series models like ARIMA, or we can make use of deep learning models such as recurrent neural networks and LSTM. </span></p>
<div class="packt_tip">
<div><span class="koboSpan" id="kobo.15.1">For a comprehensive review of various strategies and models used in the electrical load forecasting, refer to this paper:</span></div>
<div class="gs_citr"><span class="koboSpan" id="kobo.16.1">Fallah, S., Deo, R., Shojafar, M., Conti, M., and Shamshirband, S. </span><span class="koboSpan" id="kobo.16.2">(2018). </span><em><span class="koboSpan" id="kobo.17.1">Computational Intelligence Approaches for Energy Load forecasting in Smart Energy Management Grids: State of the Art, Future Challenges, and Research Directions</span></em><span class="koboSpan" id="kobo.18.1">. </span><span class="koboSpan" id="kobo.18.2">Energies, 11(3), 596.</span></div>
</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">STLF using LSTM</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Here, we present the code for performing a short-term load forecasting with the help of LSTM. </span><span class="koboSpan" id="kobo.2.2">The data for training and testing is taken from the UCI ML website (</span><a href="https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption#"><span class="koboSpan" id="kobo.3.1">https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption#</span></a><span class="koboSpan" id="kobo.4.1">). </span><span class="koboSpan" id="kobo.4.2">The code for STLF has been adapted from GitHub (</span><a href="https://github.com/demmojo/lstm-electric-load-forecast"><span class="koboSpan" id="kobo.5.1">https://github.com/demmojo/lstm-electric-load-forecast</span></a><span class="koboSpan" id="kobo.6.1">):</span></p>
<ol>
<li><span class="koboSpan" id="kobo.7.1">We import the necessary modules and set random seeds, shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.8.1">import time</span><br/><span class="koboSpan" id="kobo.9.1">from keras.layers import LSTM</span><br/><span class="koboSpan" id="kobo.10.1">from keras.layers import Activation, Dense, Dropout</span><br/><span class="koboSpan" id="kobo.11.1">from keras.models import Sequential, load_model</span><br/><span class="koboSpan" id="kobo.12.1">from numpy.random import seed</span><br/><br/><span class="koboSpan" id="kobo.13.1">from tensorflow import set_random_seed</span><br/><span class="koboSpan" id="kobo.14.1">set_random_seed(2) # seed random numbers for Tensorflow backend</span><br/><span class="koboSpan" id="kobo.15.1">seed(1234) # seed random numbers for Keras</span><br/><span class="koboSpan" id="kobo.16.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.17.1">import csv</span><br/><span class="koboSpan" id="kobo.18.1">import matplotlib.pyplot as plt</span><br/><br/><span class="koboSpan" id="kobo.19.1">%matplotlib inline</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.20.1">Define utility functions for loading the data and converting it into a sequence suited for LSTM input:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.21.1">def load_data(dataset_path, sequence_length=60, prediction_steps=5, ratio_of_data=1.0):</span><br/><span class="koboSpan" id="kobo.22.1">    # 2075259 is the total number of measurements </span><br/><span class="koboSpan" id="kobo.23.1">    # from Dec 2006 to Nov 2010</span><br/><span class="koboSpan" id="kobo.24.1">    max_values = ratio_of_data * 2075259</span><br/><br/><span class="koboSpan" id="kobo.25.1">    # Load data from file</span><br/><span class="koboSpan" id="kobo.26.1">    with open(dataset_path) as file:</span><br/><span class="koboSpan" id="kobo.27.1">        data_file = csv.reader(file, delimiter=";")</span><br/><span class="koboSpan" id="kobo.28.1">        power_consumption = []</span><br/><span class="koboSpan" id="kobo.29.1">        number_of_values = 0</span><br/><span class="koboSpan" id="kobo.30.1">        for line in data_file:</span><br/><span class="koboSpan" id="kobo.31.1">            try:</span><br/><span class="koboSpan" id="kobo.32.1">                power_consumption.append(float(line[2]))</span><br/><span class="koboSpan" id="kobo.33.1">                number_of_values += 1</span><br/><span class="koboSpan" id="kobo.34.1">            except ValueError:</span><br/><span class="koboSpan" id="kobo.35.1">                pass</span><br/><br/><span class="koboSpan" id="kobo.36.1">            # limit data to be considered by </span><br/><span class="koboSpan" id="kobo.37.1">            # model according to max_values</span><br/><span class="koboSpan" id="kobo.38.1">            if number_of_values &gt;= max_values: </span><br/><span class="koboSpan" id="kobo.39.1">                break</span><br/><br/><span class="koboSpan" id="kobo.40.1">    print('Loaded data from csv.')</span><br/><span class="koboSpan" id="kobo.41.1">    windowed_data = []</span><br/><span class="koboSpan" id="kobo.42.1">    # Format data into rolling window sequences</span><br/><span class="koboSpan" id="kobo.43.1">    # for e.g: index=0 =&gt; 123, index=1 =&gt; 234 etc.</span><br/><span class="koboSpan" id="kobo.44.1">    for index in range(len(power_consumption) - sequence_length): </span><br/><span class="koboSpan" id="kobo.45.1">            windowed_data.append(</span><br/><span class="koboSpan" id="kobo.46.1">            power_consumption[</span><br/><span class="koboSpan" id="kobo.47.1">            index: index + sequence_length])</span><br/><br/><span class="koboSpan" id="kobo.48.1">    # shape (number of samples, sequence length)</span><br/><span class="koboSpan" id="kobo.49.1">    windowed_data = np.array(windowed_data)</span><br/><br/><span class="koboSpan" id="kobo.50.1">    # Center data</span><br/><span class="koboSpan" id="kobo.51.1">    data_mean = windowed_data.mean()</span><br/><span class="koboSpan" id="kobo.52.1">    windowed_data -= data_mean</span><br/><span class="koboSpan" id="kobo.53.1">    print('Center data so mean is zero </span><br/><span class="koboSpan" id="kobo.54.1">            (subtract each data point by mean of value: ', </span><br/><span class="koboSpan" id="kobo.55.1">            data_mean, ')')</span><br/><span class="koboSpan" id="kobo.56.1">    print('Data : ', windowed_data.shape)</span><br/><br/><span class="koboSpan" id="kobo.57.1">    # Split data into training and testing sets</span><br/><span class="koboSpan" id="kobo.58.1">    train_set_ratio = 0.9</span><br/><span class="koboSpan" id="kobo.59.1">    row = int(round(train_set_ratio * windowed_data.shape[0]))</span><br/><span class="koboSpan" id="kobo.60.1">    train = windowed_data[:row, :]</span><br/><br/><span class="koboSpan" id="kobo.61.1">    # remove last prediction_steps from train set</span><br/><span class="koboSpan" id="kobo.62.1">    x_train = train[:, :-prediction_steps] </span><br/><span class="koboSpan" id="kobo.63.1">    # take last prediction_steps from train set</span><br/><span class="koboSpan" id="kobo.64.1">    y_train = train[:, -prediction_steps:] </span><br/><span class="koboSpan" id="kobo.65.1">    x_test = windowed_data[row:, :-prediction_steps]</span><br/><br/><span class="koboSpan" id="kobo.66.1">    # take last prediction_steps from test set</span><br/><span class="koboSpan" id="kobo.67.1">    y_test = windowed_data[row:, -prediction_steps:] </span><br/><br/><span class="koboSpan" id="kobo.68.1">    x_train = np.reshape(x_train, </span><br/><span class="koboSpan" id="kobo.69.1">            (x_train.shape[0], x_train.shape[1], 1))</span><br/><span class="koboSpan" id="kobo.70.1">    x_test = np.reshape(x_test, </span><br/><span class="koboSpan" id="kobo.71.1">            (x_test.shape[0], x_test.shape[1], 1))</span><br/><br/><span class="koboSpan" id="kobo.72.1">    return [x_train, y_train, x_test, y_test, data_mean]</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.73.1">Build the LSTM model, the model we have built contains two LSTM and one fully connected layer:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.74.1">def build_model(prediction_steps):</span><br/><span class="koboSpan" id="kobo.75.1">    model = Sequential()</span><br/><span class="koboSpan" id="kobo.76.1">    layers = [1, 75, 100, prediction_steps]</span><br/><span class="koboSpan" id="kobo.77.1">    model.add(LSTM(layers[1], </span><br/><span class="koboSpan" id="kobo.78.1">        input_shape=(None, layers[0]), </span><br/><span class="koboSpan" id="kobo.79.1">        return_sequences=True)) # add first layer</span><br/><span class="koboSpan" id="kobo.80.1">    model.add(Dropout(0.2)) # add dropout for first layer</span><br/><span class="koboSpan" id="kobo.81.1">    model.add(LSTM(layers[2], </span><br/><span class="koboSpan" id="kobo.82.1">        return_sequences=False)) # add second layer</span><br/><span class="koboSpan" id="kobo.83.1">    model.add(Dropout(0.2)) # add dropout for second layer</span><br/><span class="koboSpan" id="kobo.84.1">    model.add(Dense(layers[3])) # add output layer</span><br/><span class="koboSpan" id="kobo.85.1">    model.add(Activation('linear')) # output layer </span><br/><span class="koboSpan" id="kobo.86.1">    start = time.time()</span><br/><span class="koboSpan" id="kobo.87.1">    model.compile(loss="mse", optimizer="rmsprop")</span><br/><span class="koboSpan" id="kobo.88.1">    print('Compilation Time : ', time.time() - start)</span><br/><span class="koboSpan" id="kobo.89.1">    return model</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.90.1">Train the model, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.91.1">def run_lstm(model, sequence_length, prediction_steps):</span><br/><span class="koboSpan" id="kobo.92.1">    data = None</span><br/><span class="koboSpan" id="kobo.93.1">    global_start_time = time.time()</span><br/><span class="koboSpan" id="kobo.94.1">    epochs = 1</span><br/><span class="koboSpan" id="kobo.95.1">    ratio_of_data = 1 # ratio of data to use from 2+ million data points</span><br/><span class="koboSpan" id="kobo.96.1">    path_to_dataset = 'data/household_power_consumption.txt'</span><br/><br/><span class="koboSpan" id="kobo.97.1">    if data is None:</span><br/><span class="koboSpan" id="kobo.98.1">        print('Loading data... </span><span class="koboSpan" id="kobo.98.2">')</span><br/><span class="koboSpan" id="kobo.99.1">        x_train, y_train, x_test, y_test, result_mean = load_data(path_to_dataset, sequence_length,</span><br/><span class="koboSpan" id="kobo.100.1">                                                                  prediction_steps, ratio_of_data)</span><br/><span class="koboSpan" id="kobo.101.1">    else:</span><br/><span class="koboSpan" id="kobo.102.1">        x_train, y_train, x_test, y_test = data</span><br/><br/><span class="koboSpan" id="kobo.103.1">    print('\nData Loaded. </span><span class="koboSpan" id="kobo.103.2">Compiling...\n')</span><br/>    <br/><span class="koboSpan" id="kobo.104.1">    model.fit(x_train, y_train, batch_size=128, epochs=epochs, validation_split=0.05)</span><br/><span class="koboSpan" id="kobo.105.1">    predicted = model.predict(x_test)</span><br/><span class="koboSpan" id="kobo.106.1">    # predicted = np.reshape(predicted, (predicted.size,))</span><br/><span class="koboSpan" id="kobo.107.1">    model.save('LSTM_power_consumption_model.h5') # save LSTM model</span><br/><br/><span class="koboSpan" id="kobo.108.1">    plot_predictions(result_mean, prediction_steps, predicted, y_test, global_start_time)</span><br/><br/><span class="koboSpan" id="kobo.109.1">    return None</span><br/><br/><br/><span class="koboSpan" id="kobo.110.1">sequence_length = 10 # number of past minutes of data for model to consider</span><br/><span class="koboSpan" id="kobo.111.1">prediction_steps = 5 # number of future minutes of data for model to predict</span><br/><span class="koboSpan" id="kobo.112.1">model = build_model(prediction_steps)</span><br/><span class="koboSpan" id="kobo.113.1">run_lstm(model, sequence_length, prediction_steps)</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.114.1">We can see from the following graph that our model is making good predictions:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.115.1"><img class="aligncenter size-full wp-image-1137 image-border" src="assets/cb3ab15b-14e4-4a5b-9ed6-d6ff41d61856.png" style="width:28.08em;height:20.00em;"/></span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.116.1">The complete code is available at GitHub: </span><kbd><span class="koboSpan" id="kobo.117.1">Chapter10/Electrical_load_Forecasting.ipynb</span></kbd><span class="koboSpan" id="kobo.118.1">. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this chapter, we saw that AI-empowered IoT has had a significant impact on industries. </span><span class="koboSpan" id="kobo.2.2">From manufacturing, logistics, agriculture, and mining to creating new products and services, AI has touched every facet. </span><span class="koboSpan" id="kobo.2.3">We can hopefully assume that the AI-powered industrial IoT will alter and disrupt current business processes and models for the better.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The next chapter will showcase how AI and the IoT can help to shape better cities. </span></p>


            </article>

            
        </section>
    </body></html>