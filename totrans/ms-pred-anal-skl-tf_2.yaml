- en: Cross-validation and Parameter Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predictive analytics is about making predictions for unknown events. We use
    it to produce models that generalize data. For this, we use a technique called
    cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation is a validation technique for assessing the result of a statistical
    analysis that generalizes to an independent dataset that gives a measure of out-of-sample
    accuracy. It achieves the task by averaging over several random partitions of
    the data into training and test samples. It is often used for hyperparameter tuning
    by doing cross-validation for several possible values of a parameter and choosing
    the parameter value that gives the lowest cross-validation average error.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two kinds of cross-validation: exhaustive and non-exhaustive. K-fold
    is an example of non-exhaustive cross-validation. It is a technique for getting
    a more accurate assessment of the model''s performance. Using k-fold cross-validation,
    we can do hyperparameter tuning. This is about choosing the best hyperparameters
    for our models. Techniques such as k-fold cross-validation and hyperparameter
    tuning are crucial for building great predictive analytics models. There are many
    flavors or methods of cross-validation, such as holdout cross-validation and k-fold
    cross-validation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Holdout cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-fold cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing models with k-fold cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holdout cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In holdout cross-validation, we hold out a percentage of observations and so
    we get two datasets. One is called the training dataset and the other is called
    the testing dataset. Here, we use the testing dataset to calculate our evaluation
    metrics, and the rest of the data is used to train the model. This is the process
    of holdout cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of holdout cross-validation is that it is very easy to implement
    and it is a very intuitive method of cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this kind of cross-validation is that it provides a single
    estimate for the evaluation metric of the model. This is problematic because some
    models rely on randomness. So in principle, it is possible that the evaluation
    metrics calculated on the test sometimes they will vary a lot because of random
    chance. So the main problem with holdout cross-validation is that we get only
    one estimation of our evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: K-fold cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In k-fold cross-validation, we basically do holdout cross-validation many times.
    So in k-fold cross-validation, we partition the dataset into *k* equal-sized samples.
    Of these many *k* subsamples, a single subsample is retained as the validation
    data for testing the model, and the remaining *k−1* subsamples are used as training
    data. This cross-validation process is then repeated *k* times, with each of the
    *k* subsamples used exactly once as the validation data. The *k* results can then
    be averaged to produce a single estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows a visual example of 5-fold cross-validation
    (*k=5*) :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da18d7be-8cc3-4aa7-a092-e74201a63185.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we see that our dataset gets divided into five parts. We use the first
    part for testing and the rest for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps we follow in the 5-fold cross-validation method:'
  prefs: []
  type: TYPE_NORMAL
- en: We get the first estimation of our evaluation metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use the second part for testing and the rest for training, and we use that
    to get a second estimation of our evaluation metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use the third part for testing and the rest for training, and so on. In this
    way, we get five estimations of the evaluation metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In k-fold cross-validation, after the *k* estimations of the evaluation matrix
    have been observed, an average of them is taken. This will give us a better estimation
    of the performance of the model. So, instead of having just one estimation of
    this evaluation metric, we can get *n* number of estimations with k-fold cross-validation,
    and then we can take the average and get a better estimation for the performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: As seen here, the advantage of the k-fold cross-validation method is that it
    can be used not only for model evaluation but also for hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: In this validation method, the common values for *k* are 5 and 10.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the variants of k-fold cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Repeated cross-validation:** In repeated cross-validation, we perform k-fold
    cross-validation many times. So, if we want 30 estimations of our evaluation metrics,
    we can do 5-fold cross-validation six times. So then we will get 30 estimations
    of our evaluation metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Leave-One-Out (LOO) cross-validation:** In this method, we take the whole
    dataset for training except for one point. We use that one point for evaluation
    and then we repeat this process for every data point in our dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we have millions of points, this validation method will be really expensive
    computationally. We use repeated k-fold cross-validation in such cases, because
    this validation method will give us comparatively good results.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing k-fold cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take examples of a `diamond` dataset to understand the implementation
    of k-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For performing k-fold cross-validation in `scikit-learn`, we first have to
    import the libraries that we will use. The following code snippet shows the code
    used for importing the libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The second step is to prepare the dataset for the `diamond` dataset that we
    will use in this example. The following shows the code used to prepare data for
    this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After preparing the data, we will create the objects used for modeling. The
    following shows the code used to create the objects for modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is the same cell that we used in [Chapter 1](765f03c9-7f1e-4eb8-af81-5cf6b0f3d2ee.xhtml),
    *Ensemble Methods for Regression and Classification*. The difference here is that
    we are not using the `train_test_split` function. Here, we are producing the `X`
    matrix, which contains all the features and also has our target feature. So we
    have our `X` matrix and our `y` vector.
  prefs: []
  type: TYPE_NORMAL
- en: For training the model, we will instantiate our `RandomForestRegressor` function,
    which we found was the best model in [Chapter 1](765f03c9-7f1e-4eb8-af81-5cf6b0f3d2ee.xhtml), *Ensemble
    Methods for Regression and Classification*, for this dataset. The following shows
    the code used to instantiate the `RandomForestRegressor` function**:**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform k-fold cross-validation, we import the `cross_validate` function
    from the `model_selection` module in `scikit-learn`. The following shows the code
    used to import the `cross_validate` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This `cross_validate` function works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We provide the estimator, which will be the `RandomForestRegressor` function.
    The following shows the code used to apply the `RandomForestRegressor` function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we pass the `X` object and the `y` object.
  prefs: []
  type: TYPE_NORMAL
- en: We provide a set of metrics that we want to evaluate for this model and for
    this dataset. In this case, evaluation is done using the `mean_squared_error`
    function and the `r2` metrics, as shown in the preceding code. Here, we pass the
    value of *k* in `cv`. So, in this case, we will do tenfold cross-validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output that we get from this `cross_validate` function will be a dictionary
    with the corresponding matrix. For better understanding, the output is converted
    into a dataframe. The following screenshot shows the code use to visualize the
    scores in the dataframe and the dataframe output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9856da4f-8d3d-476e-a72e-661065f019ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we apply `test_mean_squared_error` and `test_r2`, which were the two metrics
    that we wanted to evaluate. After evaluating, we get the `train_mean_squared_error` value
    and the `test_r2` set. So, we are interested in the testing metrics.
  prefs: []
  type: TYPE_NORMAL
- en: To get a better assessment of the performance of a model, we will take an average
    (mean) of all of the individual measurements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the code for getting the `Mean test MSE` and `Mean test
    R-squared` values and output showing their values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: So here, on averaging, we see that the mean of the test MSE is the value that
    we have here and an average of the other metric, which was the R-squared evaluation
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing models with k-fold cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As k-fold cross-validation method proved to be a better method, it is more suitable
    for comparing models. The reason behind this is that k-fold cross-validation gives
    much estimation of the evaluation metrics, and on averaging these estimations,
    we get a better assessment of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the code used to import libraries for comparing models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After importing libraries, we''ll import the `diamond` dataset. The following
    shows the code used to prepare this `diamond` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to prepare objects for modeling after preparing the dataset for
    doing model comparison. The following shows the code used for preparing the objects
    for modeling. Here we have the `X` matrix, showing the features, and the `y` vector,
    which is the target for this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, we will compare the KNN model, the random forest model, and the bagging
    model. In these models, using tenfold cross-validation, we will use the `KNNRegressor`,
    `RandomForestRegressor`, and `AdaBoostRegressor` functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we will import the `cross_validate` function. The following shows the
    code used to import the `cross_validate` function for these three models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to compare the models using the `cross_validate` function.
    The following shows the code block used for comparing these three models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we see the result of the testing evaluation metrics that we will use
    for every model. We use the `mean_squared_error` function. For every model, we
    use tenfold cross-validation and after getting the result, we get the `test_score`
    variable. This `test_score` variable is the mean-squared error in this case, and
    is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the code for the result that we got after running
    the tenfold cross-validation for every model and also, the table showing the 10
    estimations of the evaluation metrics for the three models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3371935f-fe8f-49f7-97e0-b4f194856d59.png)'
  prefs: []
  type: TYPE_IMG
- en: This table shows a lot of variation in estimations of every model. To know the
    actual performance of any model, we take the average of the result. So, in the
    preceding figure, we take the mean of all the values and then plot it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the code used to take the mean and the graph
    showing the mean MSE values for every model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a47895c7-c5aa-4670-896b-0eddabf074f6.png)'
  prefs: []
  type: TYPE_IMG
- en: On averaging, the random forest model performs best out of the three models.
    So, after taking an average, the second place goes to the KNN model, and the boosting
    model takes last place.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the code used to get the box plot for these evaluation
    metrics and the box plot for the three models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc62d4f6-ce2f-4866-9f7c-6fb96e95b6a3.png)'
  prefs: []
  type: TYPE_IMG
- en: So, looking at the box plot for these evaluation metrics, we see that, the random
    forest performs the best.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the degree of variability of these models, we can analyze the standard
    deviation of the test MSE for regression models. The following screenshot shows
    the code used to check the degree of variability and the plot showing the standard
    deviation of these models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e98e483d-748c-4067-ab99-27a387d6d1c6.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows that the most model in this case was the KNN
    model, followed by the boosting model, and random forest model had the least variation.
    So, the random forest model is the best among these three models. Even for this
    dataset, the random forest model performs the best.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The method used to choose the best estimators for a particular dataset or choosing
    the best values for all hyperparameters is called **hyperparameter tuning**. Hyperparameters
    are parameters that are not directly learned within estimators. Their value is
    decided by the modelers.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the `RandomForestClassifier` object, there are a lot of hyperparameters,
    such as `n_estimators`, `max_depth`, `max_features`, and `min_samples_split`.
    Modelers decide the values for these hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Exhaustive grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most important and generally-used methods for performing hyperparameter
    tuning is called the **exhaustive grid search**. This is a brute-force approach
    because it tries all of the combinations of hyperparameters from a grid of parameter
    values. Then, for each combination of hyperparameters, the model is evaluated
    using k-fold cross-validation and any other specified metrics. So the combination
    that gives us the best metric is the one that is returned by the object that we
    will use in `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example of a hyperparameter grid. Here, we try three different
    values, such as 10, 30, and 50, for the `n_estimators` hyperparameter. We will
    try two options, such as auto and square root, for `max_features`, and assign
    four values—5, 10, 20, and 30—for `max_depth`. So, in this case, we will have
    24 hyperparameter combinations. These 24 will be evaluated. For every one of these
    24 combinations, in this case, we use tenfold cross-validation and the computer
    will be training and evaluating 240 models. The biggest shortcoming that grid
    search faces is the curse of dimensionality which will be covered in the coming
    chapters. The curse of dimensionality essentially means that the number of times
    you will have to evaluate your model increases exponentially with the number of
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: If certain combinations of hyperparameters are not tested, then different grids
    can be passed to the `GridSearchCV` object. Here, different grids can be passed
    in the form of a list of dictionaries because every grid is a dictionary in `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Don't ever use the entire dataset for tuning parameters, always perform train-test
    split when tuning parameters, otherwise the hyperparameters may be fit to that
    specific dataset and the model won't generalize well to new data.
  prefs: []
  type: TYPE_NORMAL
- en: So, we perform the train-test split and use one part of the dataset to learn
    the hyperparameters of our model; the part that we left for testing should be
    used for the final model evaluation, and later we use the whole dataset to fit
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take an example of the `diamond` dataset to understand hyperparameter
    tuning in `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform hyperparameter tuning, we first have to import the libraries that
    we will use. To import the libraries, we will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we perform the transformations to the `diamond` dataset that we will
    use in this example. The following shows the code used to prepare data for this
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After preparing the data, we will create the objects used for modeling. The
    following shows the code used to create the objects for modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: After performing and creating the objects used for modeling, we perform the
    `train_test_split` function. In the preceding codeblock, we will set 0.1(10%)
    of the data for testing, so this portion of the dataset will be used for model
    evaluation after tuning the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will tune the `RandomForestRegressor` model using the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: This parameter represents the number of trees in the forest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: This parameter represents the number of features to consider
    when looking for best split. The possible choices are `n_features`, which corresponds
    to the auto hyperparameter, or the square root of the `log2` of the number of
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: This parameter represents the maximum depth of the tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different values with a grid search will be used for these parameters. For `n_estimators`
    and `max_depth`, we will use four different values. For `n_estimators`, we will
    use `[25,50,75,100]` as values, and for `max_depth`, we will use `[10,15,20,30]`
    as values. For `max_features`, we will use the auto and square root.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will instantiate the `RandomForestRegressor` model explicitly without
    any hyperparameters. The following shows the code used to instantiate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameter grid is basically a dictionary where we pass the name of the
    hyperparameter and the values that we want to try. The following code block shows
    the parameter grid with different parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In total, we have four values for `n_estimators`, four values for `max_depth`,
    and two for `max_features`. So, on calculating, there are in total of 32 combinations
    of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform hyperparameter tuning in `scikit-learn`, we will use the `GridSearchCV`
    object. The following shows the code used to import the object from `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we pass the estimator we want to tune, in this case, `RandomForestRegressor`. The
    following screenshot shows the code used and the output that we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c374323-bcaa-4d95-b126-082372f965e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Then we pass the parameter grid that we want to try. Here, `refit` means that
    this estimator object will refit using the best parameters that it found using
    this process of grid-search and cross-validation. This is the evaluation metric
    that will be used by this object to evaluate all of the possible combinations
    of hyperparameters. In this case, we will use tenfold cross-validation. So after
    creating that, we can use the `fit` method and pass the training object. Since
    we are using tenfold cross-validation with 32 combinations, the model will evaluate
    320 models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the results using the `cv _results_` attribute from the object that
    we created using the `GridSearchCV` method. The following screenshot shows the
    code used to get the result and output showing the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c8cf2ba-e2f8-4301-96d2-f7e29f2876c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the most important thing is to get `best_params_`. So with `best_params_`,
    we can see the combination of parameters of all 32 combinations. The following
    screenshot shows the input used to get the parameters and the output showing combination
    of parameters that can give the best result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99ba11ae-8c28-44e3-bebe-433858fbe813.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that the combination that can give `best_params_` is `max_depth`
    with a value of `20`, `max _features` with a value of auto, and `n_estimators`
    with a value of `100`. So this is the best possible combination of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also get the `best_estimator_` object, and this is the complete list
    of hyperparameters. The following screenshot shows the code used to get `best_estimator_` and
    output showing the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e34b51a-55e9-4422-a75d-599120926bbf.png)'
  prefs: []
  type: TYPE_IMG
- en: So, as we tuned the hyperparameters of the random forest model, we got different
    values from the values that we got before; when we had values for `n_estimators`
    as `50`, `max_depth` as `16`, and `max_features` as `auto`, the parameters in
    that model were untuned.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing tuned and untuned models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can compare the best model that we got while tuning the parameters with
    the best model that we have been using without the help of tuning `n_estimators`
    with a value of `50`, `max_depth` with a value of `16`, and `max_features` as
    `auto`, and in both the cases it was random forest. The following code shows the
    values of the parameters of both the tuned and untuned models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To actually see the comparison between the tuned and untuned models, we can
    see the value of the mean-square error. The following screenshot shows the code
    to get the `mean_squared_error` value for the two models, and the plot showing
    the comparison of the MSE value of the two models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bb189e2-3e82-45bc-911a-302a14c799f4.png)'
  prefs: []
  type: TYPE_IMG
- en: We can clearly observe here that, on comparison, these tuned parameters perform
    better than the untuned parameters in both of the random forest models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the actual difference in values between the two models, we can do a
    little calculation. The following screenshot shows the calculation for getting
    the percentage of improvement and output showing the actual percentage value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de47b18f-6727-4197-8078-2b10a3457512.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we got an improvement of 4.6% for the tuned model over the untuned one
    and this is actually very good. In these models, an improvement of 1%-3% percent
    can also have huge practical implications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about cross-validation, and different methods of
    cross-validation, including holdout cross-validation and k-fold cross-validation.
    We came to know that k-fold cross-validation is nothing but doing holdout cross-validation
    many times. We implemented k-fold cross-validation using the `diamond` dataset.
    We also compared different models using k-fold cross-validation and found the
    best-performing model, which was the random forest model.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we discussed hyperparameter tuning. We came across the exhaustive grid-search
    method, which is used to perform hyperparameter tuning. We implemented hyperparameter
    tuning again using the `diamond` dataset. We also compared tuned and untuned models,
    and found that tuned parameters make the model perform better than untuned ones.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study feature selection methods, dimensionality
    reduction and **principle component analysis** (**PCA**), and feature engineering.
    We will also learn about a method to improve the model with feature engineering.
  prefs: []
  type: TYPE_NORMAL
