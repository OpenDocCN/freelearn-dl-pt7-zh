["```py\nimport keras\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n# Setting seed for reproducibility\nnp.random.seed(1234) \nPYTHONHASHSEED = 0\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score\nfrom keras.models import Sequential,load_model\nfrom keras.layers import Dense, Dropout, LSTM\n```", "```py\n# read training data - It is the aircraft engine run-to-failure data.\ntrain_df = pd.read_csv('PM_train.txt', sep=\" \",\n         header=None)\ntrain_df.drop(train_df.columns[[26, 27]], \n        axis=1, \n        inplace=True)\ntrain_df.columns = ['id', 'cycle', 'setting1',\n         'setting2', 'setting3', 's1', 's2',\n         's3', 's4', 's5', 's6', 's7', 's8',\n         's9', 's10', 's11', 's12', 's13', \n        's14', 's15', 's16', 's17', 's18', \n        's19', 's20', 's21']\n\ntrain_df = train_df.sort_values(['id','cycle'])\n\n# read test data - It is the aircraft engine operating data without failure events recorded.\ntest_df = pd.read_csv('PM_test.txt', \n        sep=\" \", header=None)\ntest_df.drop(test_df.columns[[26, 27]], \n        axis=1, \n        inplace=True)\ntest_df.columns = ['id', 'cycle', 'setting1', \n        'setting2', 'setting3', 's1', 's2', 's3',\n         's4', 's5', 's6', 's7', 's8', 's9', \n        's10', 's11', 's12', 's13', 's14',\n         's15', 's16', 's17', 's18', 's19', \n        's20', 's21']\n\n# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\ntruth_df = pd.read_csv('PM_truth.txt', \n        sep=\" \", \n        header=None)\ntruth_df.drop(truth_df.columns[[1]], \n        axis=1, \n        inplace=True)\n```", "```py\n# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\nrul = pd.DataFrame(train_df.groupby('id')\n        ['cycle'].max()).reset_index()\nrul.columns = ['id', 'max']\ntrain_df = train_df.merge(rul, \n        on=['id'], \n        how='left')\ntrain_df['RUL'] = train_df['max'] -     train_df['cycle']\ntrain_df.drop('max', \n        axis=1, \n        inplace=True)\n\n# Let us generate label columns for training data\n# we will only use \"label1\" for binary classification, \n# The question: is a specific engine going to fail within w1 cycles?\nw1 = 30\nw0 = 15\ntrain_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n\n# MinMax normalization (from 0 to 1)\ntrain_df['cycle_norm'] = train_df['cycle']\ncols_normalize = train_df.columns.difference\n        (['id','cycle','RUL','label1'])\nmin_max_scaler = preprocessing.MinMaxScaler()\nnorm_train_df = pd.DataFrame(min_max_scaler.\n        fit_transform(train_df[cols_normalize]), \n        columns=cols_normalize, \n        index=train_df.index)\njoin_df = train_df[train_df.columns.\n        difference(cols_normalize)].\n        join(norm_train_df)\ntrain_df = join_df.reindex(columns = train_df.columns)\n\ntrain_df.head()\n```", "```py\n# MinMax normalization (from 0 to 1)\ntest_df['cycle_norm'] = test_df['cycle']\nnorm_test_df = pd.DataFrame(\n        min_max_scaler.\n        transform(test_df[cols_normalize]), \n        columns=cols_normalize,     \n         index=test_df.index)\ntest_join_df = test_df[test_df.\n        columns.difference(cols_normalize)].\n        join(norm_test_df)\ntest_df = test_join_df.\n        reindex(columns = test_df.columns)\ntest_df = test_df.reset_index(drop=True)\n\n# We use the ground truth dataset to generate labels for the test data.\n# generate column max for test data\nrul = pd.DataFrame(test_df.\n        groupby('id')['cycle'].max()).\n        reset_index()\nrul.columns = ['id', 'max']\ntruth_df.columns = ['more']\ntruth_df['id'] = truth_df.index + 1\ntruth_df['max'] = rul['max'] + truth_df['more']\ntruth_df.drop('more', \n        axis=1, \n        inplace=True)\n\n# generate RUL for test data\ntest_df = test_df.merge(truth_df, \n        on=['id'], how='left')\ntest_df['RUL'] = test_df['max'] - test_df['cycle']\ntest_df.drop('max', \n        axis=1, \n        inplace=True)\n\n# generate label columns w0 and w1 for test data\ntest_df['label1'] = np.where\n        (test_df['RUL'] <= w1, 1, 0 )\ntest_df.head()\n```", "```py\n# function to reshape features into \n# (samples, time steps, features) \ndef gen_sequence(id_df, seq_length, seq_cols):\n    \"\"\" Only sequences that meet the window-length\n    are considered, no padding is used. This \n    means for testing we need to drop those which \n    are below the window-length. An alternative\n    would be to pad sequences so that\n    we can use shorter ones \"\"\"\n\n    # for one id we put all the rows in a single matrix\n    data_matrix = id_df[seq_cols].values\n    num_elements = data_matrix.shape[0]\n    # Iterate over two lists in parallel.\n    # For example id1 have 192 rows and \n    # sequence_length is equal to 50\n    # so zip iterate over two following list of \n    # numbers (0,112),(50,192)\n    # 0 50 -> from row 0 to row 50\n    # 1 51 -> from row 1 to row 51\n    # 2 52 -> from row 2 to row 52\n    # ...\n    # 111 191 -> from row 111 to 191\n    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n        yield data_matrix[start:stop, :]\n\ndef gen_labels(id_df, seq_length, label):\n    # For one id we put all the labels in a \n    # single matrix.\n    # For example:\n    # [[1]\n    # [4]\n    # [1]\n    # [5]\n    # [9]\n    # ...\n    # [200]] \n    data_matrix = id_df[label].values\n    num_elements = data_matrix.shape[0]\n    # I have to remove the first seq_length labels\n    # because for one id the first sequence of \n    # seq_length size have as target\n    # the last label (the previus ones are \n    # discarded).\n    # All the next id's sequences will have \n    # associated step by step one label as target. \n    return data_matrix[seq_length:num_elements, :]\n```", "```py\n# pick a large window size of 50 cycles\nsequence_length = 50\n\n# pick the feature columns \nsensor_cols = ['s' + str(i) for i in range(1,22)]\nsequence_cols = ['setting1', 'setting2', \n        'setting3', 'cycle_norm']\nsequence_cols.extend(sensor_cols)\n\n# generator for the sequences\nseq_gen = (list(gen_sequence\n        (train_df[train_df['id']==id], \n        sequence_length, sequence_cols)) \n        for id in train_df['id'].unique())\n\n# generate sequences and convert to numpy array\nseq_array = np.concatenate(list(seq_gen)).\n        astype(np.float32)\nprint(seq_array.shape)\n\n# generate labels\nlabel_gen = [gen_labels(train_df[train_df['id']==id], \n        sequence_length, ['label1']) \n        for id in train_df['id'].unique()]\nlabel_array = np.concatenate(label_gen).\n        astype(np.float32)\nprint(label_array.shape)\n```", "```py\nnb_features = seq_array.shape[2]\nnb_out = label_array.shape[1]\n\nmodel = Sequential()\n\nmodel.add(LSTM(\n     input_shape=(sequence_length, nb_features),\n     units=100,\n     return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(\n     units=50,\n     return_sequences=False))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=nb_out,\n     activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', \n    optimizer='adam', \n    metrics=['accuracy'])\n\nprint(model.summary())\n```", "```py\nhistory = model.fit(seq_array, label_array, \n        epochs=100, batch_size=200, \n        validation_split=0.05, verbose=2,\n         callbacks = [keras.callbacks.\n            EarlyStopping(monitor='val_loss', \n            min_delta=0, patience=10, \n            verbose=0, mode='min'),\n        keras.callbacks.\n            ModelCheckpoint\n            (model_path,monitor='val_loss',     \n            save_best_only=True, \n            mode='min', verbose=0)])    \n```", "```py\n# generate labels\nlabel_gen = [gen_labels(train_df[train_df['id']==id],\n        sequence_length, ['RUL']) \n        for id in train_df['id'].unique()]\nlabel_array = np.concatenate(label_gen).astype(np.float32)\n\n# val is a list of 192 - 50 = 142 bi-dimensional array \n# (50 rows x 25 columns)\nval=list(gen_sequence(train_df[train_df['id']==1], \n        sequence_length, sequence_cols))\n```", "```py\ndef r2_keras(y_true, y_pred):\n     \"\"\"Coefficient of Determination \n     \"\"\"\n     SS_res = K.sum(K.square( y_true - y_pred ))\n     SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n     return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n\n# Next, we build a deep network. \n# The first layer is an LSTM layer with 100 units followed by \n# another LSTM layer with 50 units. \n# Dropout is also applied after each LSTM layer to control \n# overfitting. \n# Final layer is a Dense output layer with single unit and linear \n# activation since this is a regression problem.\nnb_features = seq_array.shape[2]\nnb_out = label_array.shape[1]\n\nmodel = Sequential()\nmodel.add(LSTM(\n     input_shape=(sequence_length, nb_features),\n     units=100,\n     return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(\n     units=50,\n     return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=nb_out))\nmodel.add(Activation(\"linear\"))\nmodel.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])\n\nprint(model.summary())\n```", "```py\n# fit the network\nhistory = model.fit(seq_array, label_array, epochs=100, \n    batch_size=200, validation_split=0.05, verbose=2,\n    callbacks = [keras.callbacks.EarlyStopping\n    (monitor='val_loss', min_delta=0, patience=10, \n    verbose=0, mode='min'),\n    keras.callbacks.ModelCheckpoint\n    (model_path,monitor='val_loss', \n    save_best_only=True, mode='min', \n    verbose=0)])\n```", "```py\nimport time\nfrom keras.layers import LSTM\nfrom keras.layers import Activation, Dense, Dropout\nfrom keras.models import Sequential, load_model\nfrom numpy.random import seed\n\nfrom tensorflow import set_random_seed\nset_random_seed(2) # seed random numbers for Tensorflow backend\nseed(1234) # seed random numbers for Keras\nimport numpy as np\nimport csv\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n```", "```py\ndef load_data(dataset_path, sequence_length=60, prediction_steps=5, ratio_of_data=1.0):\n    # 2075259 is the total number of measurements \n    # from Dec 2006 to Nov 2010\n    max_values = ratio_of_data * 2075259\n\n    # Load data from file\n    with open(dataset_path) as file:\n        data_file = csv.reader(file, delimiter=\";\")\n        power_consumption = []\n        number_of_values = 0\n        for line in data_file:\n            try:\n                power_consumption.append(float(line[2]))\n                number_of_values += 1\n            except ValueError:\n                pass\n\n            # limit data to be considered by \n            # model according to max_values\n            if number_of_values >= max_values: \n                break\n\n    print('Loaded data from csv.')\n    windowed_data = []\n    # Format data into rolling window sequences\n    # for e.g: index=0 => 123, index=1 => 234 etc.\n    for index in range(len(power_consumption) - sequence_length): \n            windowed_data.append(\n            power_consumption[\n            index: index + sequence_length])\n\n    # shape (number of samples, sequence length)\n    windowed_data = np.array(windowed_data)\n\n    # Center data\n    data_mean = windowed_data.mean()\n    windowed_data -= data_mean\n    print('Center data so mean is zero \n            (subtract each data point by mean of value: ', \n            data_mean, ')')\n    print('Data : ', windowed_data.shape)\n\n    # Split data into training and testing sets\n    train_set_ratio = 0.9\n    row = int(round(train_set_ratio * windowed_data.shape[0]))\n    train = windowed_data[:row, :]\n\n    # remove last prediction_steps from train set\n    x_train = train[:, :-prediction_steps] \n    # take last prediction_steps from train set\n    y_train = train[:, -prediction_steps:] \n    x_test = windowed_data[row:, :-prediction_steps]\n\n    # take last prediction_steps from test set\n    y_test = windowed_data[row:, -prediction_steps:] \n\n    x_train = np.reshape(x_train, \n            (x_train.shape[0], x_train.shape[1], 1))\n    x_test = np.reshape(x_test, \n            (x_test.shape[0], x_test.shape[1], 1))\n\n    return [x_train, y_train, x_test, y_test, data_mean]\n```", "```py\ndef build_model(prediction_steps):\n    model = Sequential()\n    layers = [1, 75, 100, prediction_steps]\n    model.add(LSTM(layers[1], \n        input_shape=(None, layers[0]), \n        return_sequences=True)) # add first layer\n    model.add(Dropout(0.2)) # add dropout for first layer\n    model.add(LSTM(layers[2], \n        return_sequences=False)) # add second layer\n    model.add(Dropout(0.2)) # add dropout for second layer\n    model.add(Dense(layers[3])) # add output layer\n    model.add(Activation('linear')) # output layer \n    start = time.time()\n    model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n    print('Compilation Time : ', time.time() - start)\n    return model\n```", "```py\ndef run_lstm(model, sequence_length, prediction_steps):\n    data = None\n    global_start_time = time.time()\n    epochs = 1\n    ratio_of_data = 1 # ratio of data to use from 2+ million data points\n    path_to_dataset = 'data/household_power_consumption.txt'\n\n    if data is None:\n        print('Loading data... ')\n        x_train, y_train, x_test, y_test, result_mean = load_data(path_to_dataset, sequence_length,\n                                                                  prediction_steps, ratio_of_data)\n    else:\n        x_train, y_train, x_test, y_test = data\n\n    print('\\nData Loaded. Compiling...\\n')\n\n    model.fit(x_train, y_train, batch_size=128, epochs=epochs, validation_split=0.05)\n    predicted = model.predict(x_test)\n    # predicted = np.reshape(predicted, (predicted.size,))\n    model.save('LSTM_power_consumption_model.h5') # save LSTM model\n\n    plot_predictions(result_mean, prediction_steps, predicted, y_test, global_start_time)\n\n    return None\n\nsequence_length = 10 # number of past minutes of data for model to consider\nprediction_steps = 5 # number of future minutes of data for model to predict\nmodel = build_model(prediction_steps)\nrun_lstm(model, sequence_length, prediction_steps)\n```"]