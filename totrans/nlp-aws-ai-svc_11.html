<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer194">
			<h1 id="_idParaDest-113"><em class="italic"><a id="_idTextAnchor117"/>Chapter 9</em>: Extracting Metadata from Financial Documents</h1>
			<p>In the previous chapter, we learned how to build an intelligent solution for media content monetization using AWS AI services. We did this by talking about how our fictitious company <strong class="bold">LiveRight Holdings private limited</strong> requires a cost-effective expansion for content monetization. We designed an architecture using AWS AI services, media services, and the content delivery network for an end-to-end walkthrough of how to monetize content in video files.</p>
			<p>In this chapter, we will look at how AWS AI services can help us extract metadata for financial filing reports for <strong class="bold">LiveRight Holdings</strong>. This will allow their financial analysts to look into important information and make better decisions concerning financial events such as mergers, acquisitions, and IPOs.</p>
			<p>We will talk about what metadata is and why it is important to extract metadata. Then, we will cover how to use Amazon Comprehend entity extraction and how Amazon Comprehend events can be used to extract metadata from documents.</p>
			<p>In this chapter, we will be covering the following topics:</p>
			<ul>
				<li>Extracting metadata from financial documents</li>
				<li>Setting up the use case</li>
			</ul>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor118"/>Technical requirements</h1>
			<p>For this chapter, you will need access to an AWS account. Please make sure that you follow the instructions specified in the <em class="italic">Technical requirements</em> section of <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, to create your AWS account, and log into the AWS Management Console before trying the steps in the <em class="italic">Extracting metadata from financial documents</em> section.</p>
			<p>The Python code and sample datasets for our solution can be found at <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2009">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2009</a>. Please use the instructions in the following sections, along with the code in the aforementioned repository, to build the solution.</p>
			<p>Check out the following video to see the Code in Action at <a href="https://bit.ly/3jBxp3E">https://bit.ly/3jBxp3E</a>.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor119"/>Extracting metadata from financial documents</h1>
			<p>In this section, we <a id="_idIndexMarker556"/>will talk about a use case where <strong class="bold">LiveRight Holdings private limited</strong> is attempting to acquire AwakenLife Pvt Ltd. They are going to do a press release soon <a id="_idIndexMarker557"/>and financial analysts <a id="_idIndexMarker558"/>are curious to identify the important metadata such as the acquisition date, amount, organization, and so forth so that they can act according to the market. LiveRight analyzed the Amazon Whole Foods merger to determine what it can learn and how metadata extraction will be useful for its due diligence. We will use the Amazon Whole Foods merger sample dataset to understand how you can perform metadata extraction using the preceding architecture:</p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="Images/B17528_09_01.jpg" alt="Figure 9.1 – Metadata extraction architecture" width="661" height="160"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Metadata extraction architecture</p>
			<p>In this architecture, we will start with large financial documents for extracting metadata. We will show you how you can use Amazon Textract batch processing jobs to extract data from this large document and save this extracted data as a text file. Then, we will show you how to extract entities from this text file using Comprehend Events and visualize the relationships between the entity using a knowledge graph. Alternatively, you can use Amazon Neptune, which is a graph database that's used to visualize these relations.</p>
			<p>In the next section, we'll look at this architecture by using Jupyter Notebook code.</p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor120"/>Setting up the use case</h1>
			<p>In this section, we <a id="_idIndexMarker559"/>will cover how to get started and walk you through the architecture shown in the preceding diagram.</p>
			<p>We have broken down the solution code walkthrough into the following sections:</p>
			<ul>
				<li>Setting up the notebook code and S3 bucket creation</li>
				<li>Uploading sample documents and extracting text using Textract</li>
				<li>Metadata extraction using Comprehend</li>
				<li>Starting Comprehend Events job with the SDK</li>
				<li>Collecting the Comprehend Events job results from S3</li>
				<li>Analyzing the output of Comprehend Events</li>
			</ul>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor121"/>Setting up the notebook code and S3 Bucket creation</h2>
			<p>Follow <a id="_idIndexMarker560"/>these steps to set up the notebook:</p>
			<ol>
				<li>In the <a id="_idIndexMarker561"/>SageMaker Jupyter notebook you set up in the previous chapters, Git clone <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/</a>.</li>
				<li>Then, go to <strong class="source-inline">/Chapter 09/chapter 09 metadata extraction.ipynb</strong> and start running the notebook.</li>
				<li>Now that we have set up the notebook, we'll create an Amazon S3 bucket. Follow the steps provided in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, to create an Amazon S3 bucket.</li>
				<li>Copy the created bucket, open your sample code from <a href="B17528_09_Final_SB_ePub.xhtml#_idTextAnchor117"><em class="italic">Chapter 9</em></a><em class="italic">, Extracting Metadata from Financial Documents </em>(<strong class="source-inline">Chapter 09/chapter 09 metadata extraction.ipynb</strong>), and paste it into the following notebook cell to get started:<p class="source-code">bucket = '&lt;your s3 bucket name&gt;'</p><p class="callout-heading">Note</p><p class="callout">We assume that your notebook has IAM access for Amazon Comprehend full access, Amazon S3 full access, and Amazon Textract full access. If you do not have access, you will get an access denied exception.</p></li>
			</ol>
			<p>If you get <a id="_idIndexMarker562"/>an access denied exception while running any of <a id="_idIndexMarker563"/>the steps in this notebook, please go to <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, and set up the relevant IAM roles.</p>
			<p>In the next section, we will walk you through the code so that you understand how the architecture works.</p>
			<h3>Uploading sample documents and extracting text using Textract</h3>
			<p>In this section, we will walk you through how you can quickly set up the proposed architecture <a id="_idIndexMarker564"/>shown in <em class="italic">Figure 9.1</em>. We have already <a id="_idIndexMarker565"/>created an Amazon S3 bucket where your output and <a id="_idIndexMarker566"/>sample documents will be stored. We also pasted that S3 bucket's name in the notebook cell. If you haven't done this yet, please complete the preceding steps.</p>
			<p>We will refer to the following notebook: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2009/chapter%2009%20metadata%20extraction.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2009/chapter%2009%20metadata%20extraction.ipynb</a>. Let's get started:</p>
			<ol>
				<li value="1">First, we must download the sample PDF financial press release document from Amazon S3.<p>Now, we must upload it using the <strong class="source-inline">upload_file</strong> S3 command via the <strong class="source-inline">sample_financial_news_doc.pdf</strong> boto3 API to an S3 bucket for processing. The same bucket will be used to return service output:</p><p class="source-code">filename = "sample_financial_news_doc.pdf"</p><p class="source-code">s3_client.upload_file(filename, bucket, filename)</p><p class="callout-heading">Note</p><p class="callout">This PDF file consists of a press release statement of the Whole Foods and Amazon merger in 2017 and consists of 156 pages.</p></li>
				<li>Now, we will run Amazon Textract to convert this PDF into a text file; Amazon Comprehend accepts a text input file with UTF 8 encoding for metadata extraction as input. You can run the notebook code to start an asynchronous processing <a id="_idIndexMarker567"/>job to extract text from documents. We explained how the asynchronous Textract batch processing code <a id="_idIndexMarker568"/>works in detail in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>,<em class="italic"> Introducing Amazon Textract</em>. If you want to deep dive, please refer to that chapter. Run <a id="_idIndexMarker569"/>the following cell to get the job results:<p class="source-code">jobId = startJob(bucket, filename)</p><p class="source-code">print("Started job with id: {}".format(jobId))</p><p class="source-code">if(isJobComplete(jobId)):</p><p class="source-code">    response = getJobResults(jobId)</p><p>At this point, you will get a <strong class="source-inline">Job ID</strong>. Wait until the job's status changes from in progress to complete:</p><div id="_idContainer178" class="IMG---Figure"><img src="Images/B17528_09_02.jpg" alt="Figure 9.2 – Textract job status&#13;&#10;" width="1311" height="126"/></div><p class="figure-caption">Figure 9.2 – Textract job status</p></li>
				<li>Now, we will convert the extracted data from Amazon Textract into a UTF 8 text file for Amazon Comprehend by running the following notebook cell:<p class="source-code">text_filename = 'sample_finance_data.txt'</p><p class="source-code">doc = Document(response)</p><p class="source-code">with open(text_filename, 'w', encoding='utf-8') as f:</p><p class="source-code">    for page in doc.pages:</p><p class="source-code">        page_string = ''</p><p class="source-code">        for line in page.lines:</p><p class="source-code">            #print((line.text))</p><p class="source-code">            page_string += str(line.text)</p><p class="source-code">        #print(page_string)</p><p class="source-code">        f.writelines(page_string + "\n")</p><p>The financial press release document text will be extracted from the press release documents:</p></li>
			</ol>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="Images/B17528_09_03.jpg" alt="Figure 9.3 – Text extracted from the press release document using Amazon Textract" width="1404" height="498"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – Text extracted from the press release document using Amazon Textract</p>
			<p>In this section, we <a id="_idIndexMarker570"/>covered how to extract text data from <a id="_idIndexMarker571"/>a press release document (a 2017 press release about Amazon's acquisition of Whole Foods), which consists of <a id="_idIndexMarker572"/>156 pages, into text format using Amazon Textract. In the next section, we will talk about how to extract metadata from this document using Comprehend entity detection sync APIs and Comprehend events async jobs.</p>
			<h3>Metadata extraction using Comprehend</h3>
			<p>In this <a id="_idIndexMarker573"/>section, we will use <a id="_idIndexMarker574"/>the aforementioned text file to extract metadata using the Amazon Comprehend Events API. </p>
			<h4>Comprehend Events API</h4>
			<p>Amazon Comprehend Events is a very specific API that can help you analyze financial events <a id="_idIndexMarker575"/>such as mergers, acquisitions, IPO dates, press releases, bankruptcy, and more. It extracts important financial entities such as IPO dates, the merger parties' names, and so on from these events and establishes relationships so that financial analysts can act in real time on their financial models and make accurate predictions and quick decisions.</p>
			<p>Amazon Comprehend Events can help you analyze asynchronous jobs. To do this, you must ensure you do the following first:</p>
			<ul>
				<li>Set up an Amazon Comprehend Events job through the AWS Console.</li>
				<li>Set up an Amazon Comprehend Events job through the notebook (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2009/chapter%2009%20metadata%20extraction.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2009/chapter%2009%20metadata%20extraction.ipynb</a>) using boto3 Python APIs.<p class="callout-heading">Note</p><p class="callout">You can choose one of the aforementioned approaches to analyze your press release documents using Amazon Comprehend events.</p></li>
			</ul>
			<p>Let's start by setting up an Amazon Comprehend Events job using the Amazon Comprehend consol:</p>
			<ol>
				<li value="1">Open the Amazon Comprehend console by going to <a href="https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#home">https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#home</a>.</li>
				<li>Go to <strong class="bold">Analysis Jobs</strong> -&gt; <strong class="bold">Create Analysis Job</strong>.</li>
				<li>In <strong class="bold">Job Settings</strong>, enter <strong class="bold">Name: Test-events</strong>. You will see that we can choose from various types of analysis, such as <strong class="bold">Sentiment</strong>, <strong class="bold">PII</strong>, <strong class="bold">Entity</strong>, and <strong class="bold">Topic modeling</strong>. Choose <strong class="bold">Events</strong>. For <strong class="bold">Language</strong> choose <strong class="bold">English</strong>, while for <strong class="bold">Target Event Types</strong>, choose all the available options, as shown in the following screenshot:<div id="_idContainer180" class="IMG---Figure"><img src="Images/B17528_09_04.jpg" alt="Figure 9.4 – Creating a Comprehend events analysis job&#13;&#10;" width="1047" height="639"/></div><p class="figure-caption">Figure 9.4 – Creating a Comprehend events analysis job</p></li>
				<li>For <strong class="bold">Input data</strong>, select <strong class="bold">My documents</strong> and provide the <strong class="bold">S3 location</strong> information <a id="_idIndexMarker576"/>of your input text file; for example, <strong class="source-inline">s3://&lt;your bucket&gt;/ sample_finance_data.txt</strong>. Choose one document per line for <strong class="bold">Input format</strong>.<p class="callout-heading">Note</p><p class="callout">We are using one document per line as the input format instead of one document per file. This is because the total file size of this press release document is 655 KB and the limit for one document per file is 10 KB. One document per line format can have 5,000 lines in a single document; the press release document we are using for this demo contains 156 lines.</p><div id="_idContainer181" class="IMG---Figure"><img src="Images/B17528_09_05.jpg" alt="Figure 9.5 – Choosing Input data for the analysis job&#13;&#10;" width="865" height="396"/></div><p class="figure-caption">Figure 9.5 – Choosing Input data for the analysis job</p></li>
				<li>For <strong class="bold">Output Data</strong>, under <strong class="bold">S3 location</strong>, enter the location where you want your output <a id="_idIndexMarker577"/>to be saved. It will be the same bucket you created in the previous step; that is, <strong class="source-inline">s3://&lt;your-bucket&gt;</strong>:<div id="_idContainer182" class="IMG---Figure"><img src="Images/B17528_09_06.jpg" alt="Figure 9.6 – Choosing an output S3 location for the analysis job" width="870" height="246"/></div><p class="figure-caption">Figure 9.6 – Choosing an output S3 location for the analysis job</p></li>
				<li>For <strong class="bold">Access permissions</strong>, choose <strong class="bold">Create an IAM Role</strong>. For <strong class="bold">Name suffix</strong>, write <strong class="source-inline">events-role</strong>:<div id="_idContainer183" class="IMG---Figure"><img src="Images/B17528_09_07.jpg" alt="Figure 9.7 – Setting up access permissions by creating an IAM role&#13;&#10;" width="972" height="480"/></div><p class="figure-caption">Figure 9.7 – Setting up access permissions by creating an IAM role</p></li>
				<li>Click on the <strong class="bold">Create Job</strong> button to trigger an events job. Grab a coffee/tea as this job <a id="_idIndexMarker578"/>will take 15 minutes to complete.</li>
				<li>Once your job is complete, go to <strong class="bold">Events-job</strong>, copy the job ID, as highlighted in the following screenshot, and move back to the <strong class="bold">Collect Results from S3</strong> notebook section so that you can use this as the Events <strong class="source-inline">Job ID</strong>:</li>
			</ol>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="Images/B17528_09_08.jpg" alt="Figure 9.8 – Copying the job ID from the Job details page after creating a job" width="1052" height="523"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – Copying the job ID from the Job details page after creating a job </p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you are creating events using the Amazon Comprehend console, skip the <em class="italic">Start an asynchronous job with the SDK</em> section in the notebook and move on to the <em class="italic">Collect the results from S3</em> section.</p>
			<p>In this section, we <a id="_idIndexMarker579"/>covered how to create a Comprehend Events job using the <strong class="bold">AWS Console</strong> for a large financial press release document. Skip the next section if you have already set up using the console.</p>
			<h3>Starting Comprehend Events jobs with the SDK </h3>
			<p>In this <a id="_idIndexMarker580"/>section, we will switch back to our notebook to start an asynchronous job with the SDK. Let's get started:</p>
			<ol>
				<li value="1">Create an IAM role by going to the IAM console at <a href="https://console.aws.amazon.com/iam/home?region=us-east-1#/home">https://console.aws.amazon.com/iam/home?region=us-east-1#/home</a>. Ensure that you create an IAM role with access to Comprehend and have specified S3. Paste the following into the cell:<p class="source-code">job_data_access_role = 'arn:aws:iam::&lt;your account number&gt;:role/service-role/AmazonComprehendServiceRole-test-events-role'</p></li>
				<li>Run the following cell to set up other Events job parameters, such as event types and input data format:<p class="source-code">input_data_format = 'ONE_DOC_PER_LINE'</p><p class="source-code">job_uuid = uuid.uuid1()</p><p class="source-code">job_name = f"events-job-{job_uuid}"</p><p class="source-code">event_types = ["BANKRUPTCY", "EMPLOYMENT", "CORPORATE_ACQUISITION", </p><p class="source-code">               "INVESTMENT_GENERAL", "CORPORATE_MERGER", "IPO",</p><p class="source-code">               "RIGHTS_ISSUE", "SECONDARY_OFFERING", "SHELF_OFFERING",</p><p class="source-code">               "TENDER_OFFERING", "STOCK_SPLIT"]</p></li>
				<li>Run <a id="_idIndexMarker581"/>the following cell to trigger the Events analysis job. This job is calling the Python boto 3 starts event detection job API. Go to <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.start_events_detection_job">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.start_events_detection_job</a> to learn more:<p class="source-code">response = comprehend_client.start_events_detection_job(</p><p class="source-code">    InputDataConfig={'S3Uri': input_data_s3_path,</p><p class="source-code">                     'InputFormat': input_data_format},</p><p class="source-code">    OutputDataConfig={'S3Uri': output_data_s3_path},</p><p class="source-code">    DataAccessRoleArn=job_data_access_role,</p><p class="source-code">    JobName=job_name,</p><p class="source-code">    LanguageCode='en',</p><p class="source-code">    TargetEventTypes=event_types</p><p class="source-code">)</p><p class="source-code">events_job_id = response['JobId']</p></li>
			</ol>
			<p>In this <a id="_idIndexMarker582"/>section, we covered how to trigger Comprehend Events analysis jobs using the SDK. At this point, we have a job ID that we will use in the next section to collect the output and analyze the metadata.</p>
			<h3>Collecting the results from S3</h3>
			<p>In this section, we will <a id="_idIndexMarker583"/>analyze the output results of this job in Amazon S3. Let's get started:</p>
			<ol>
				<li value="1">If you used the Amazon Comprehend console previously, you must have copied the <strong class="source-inline">Job ID</strong> at the end of that section. Please paste it in the following cell by uncommenting it. Then, run the cell:<p class="source-code">events_job_id ="&lt;Job ID&gt;"</p></li>
				<li>If you used the Amazon Comprehend SDK to trigger Events analysis jobs, continue with the following cell to track the job's status:<p class="source-code">job = comprehend_client.describe_events_detection_job(JobId=events_job_id)</p><p class="source-code">waited = 0</p><p class="source-code">timeout_minutes = 30</p><p class="source-code">while job['EventsDetectionJobProperties']['JobStatus'] != 'COMPLETED':</p><p class="source-code">    sleep(60)</p><p class="source-code">    waited += 60</p><p class="source-code">    assert waited//60 &lt; timeout_minutes, "Job timed out after %d seconds." % waited</p><p class="source-code">    job = comprehend_client.describe_events_detection_job(JobId=events_job_id)</p></li>
				<li>Once the job is completed, you can get the output from Amazon S3 by running the following cell:<p class="source-code">output_data_s3_file = job['EventsDetectionJobProperties']['OutputDataConfig']['S3Uri'] + text_filename + '.out'</p><p class="source-code">results = []</p><p class="source-code">with smart_open.open(output_data_s3_file) as fi:</p><p class="source-code">    results.extend([json.loads(line) for line in fi.readlines() if line])</p></li>
			</ol>
			<p>In this section, we covered how to track a Comprehend Events job's completion using SDKs and <a id="_idIndexMarker584"/>collect the output from Amazon S3. Now that we have collected the results, we will analyze the results and metadata that have been extracted.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor122"/>Analyzing the output of Comprehend Events</h2>
			<p>In this section, we will show you different ways you can analyze the output of Comprehend <a id="_idIndexMarker585"/>Events. This output can be used by financial analysts to predict market trends or look up key information in large datasets. But first, let's understand the <strong class="bold">Comprehend Events</strong> system's output (<a href="https://docs.aws.amazon.com/comprehend/latest/dg/how-events.html">https://docs.aws.amazon.com/comprehend/latest/dg/how-events.html</a>): </p>
			<ul>
				<li>The system returns JSON output for each submitted document. The structure of the response is shown here:<p class="source-code">result = results[0]</p><p class="source-code">result</p><p>In the response, you get entities, as well as entities grouped as <strong class="source-inline">mentions</strong>, <strong class="source-inline">arguments</strong>, and <strong class="source-inline">triggers</strong>, along with the confidence score. We will see these terms being used throughout the notebook:</p></li>
			</ul>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="Images/B17528_09_09.jpg" alt="Figure 9.9 – Comprehend events JSON output&#13;&#10;" width="859" height="448"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.9 – Comprehend events JSON output</p>
			<ul>
				<li><strong class="source-inline">Events</strong> are groups of <strong class="source-inline">triggers</strong>. The API's output includes the text, character offset, and type of each trigger, along with the confidence score. The confidence of event <a id="_idIndexMarker586"/>group membership is provided by <strong class="source-inline">GroupScore</strong>. Run the following notebook cell to take a look at these:<p class="source-code">result['Events'][1]['Triggers']</p><p>The following is the output of the preceding code:</p></li>
			</ul>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="Images/B17528_09_10.jpg" alt="Figure 9.10 – Comprehend events triggers&#13;&#10;" width="524" height="432"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.10 – Comprehend events triggers</p>
			<p><strong class="source-inline">acquire</strong> and <strong class="source-inline">transaction</strong> are related to the <strong class="source-inline">CORPORATE_ACQUISTION</strong> type event.</p>
			<ul>
				<li>Arguments are linked to entities by <strong class="source-inline">EntityIndex</strong>, along with the classification confidence of the role assignment. It talks about how the entity is related to the event. Run the following code to understand this:<p class="source-code">result['Events'][1]['Arguments']</p><p>The output of <strong class="source-inline">arguments</strong> will look as follows: </p></li>
			</ul>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="Images/B17528_09_11.jpg" alt="Figure 9.11 – Comprehend events arguments &#13;&#10;" width="796" height="151"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.11 – Comprehend events arguments </p>
			<p><strong class="source-inline">Investee</strong>, <strong class="source-inline">Amount</strong>, and <strong class="source-inline">Date</strong> are roles with entity indexes and confidence scores.</p>
			<ul>
				<li>Entities are <a id="_idIndexMarker587"/>groups of <strong class="source-inline">Mentions</strong> that consist of <strong class="source-inline">text</strong>, character <strong class="source-inline">offset</strong>, and <strong class="source-inline">type</strong> of each mention, along with their confidence scores. The confidence of the entity group's membership is provided by <strong class="source-inline">Group Scores</strong>. Let's run the following cell to understand this:<p class="source-code">result['Entities'][5]['Mentions']</p><p>The following output shows what the <strong class="source-inline">Mention</strong> entity looks like:</p></li>
			</ul>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="Images/B17528_09_12.jpg" alt="Figure 9.12 – Comprehend events mentions&#13;&#10;" width="439" height="293"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.12 – Comprehend events mentions</p>
			<p><strong class="source-inline">entityIndex</strong> 5 refers to the <strong class="source-inline">Type</strong> <strong class="source-inline">Monetary_Value</strong> in the output.</p>
			<p>Now that we know what <strong class="source-inline">entity</strong>, <strong class="source-inline">arguments</strong>, and <strong class="source-inline">mentions</strong> are, let's visualize the relationships between them.</p>
			<h3>Visualizing events and entities</h3>
			<p>In the remainder of the notebook, we'll provided several tabulations and visualizations to help <a id="_idIndexMarker588"/>you understand what the API is returning. First, we'll look at <strong class="source-inline">spans</strong>, both <strong class="source-inline">triggers</strong> and entity <strong class="source-inline">mentions</strong>. One of the most essential visualization tasks for sequence labeling tasks is highlighting tagged text in documents. For demonstration purposes, we'll do this with <strong class="bold">displaCy</strong>, which is a built-in dependency visualizer that lets you check your model's predictions in your browser (<a href="https://explosion.ai/demos/displacy">https://explosion.ai/demos/displacy</a>):</p>
			<ol>
				<li value="1">Run the following code to convert <strong class="source-inline">entity</strong> into displaCy format. Convert the output of <strong class="source-inline">Events</strong> into displaCy format:<p class="source-code">entities = [</p><p class="source-code">    {'start': m['BeginOffset'], 'end': m['EndOffset'], 'label': m['Type']}</p><p class="source-code">    for e in result['Entities']</p><p class="source-code">    for m in e['Mentions']</p><p class="source-code">]</p></li>
				<li>Use the following code to map <strong class="source-inline">triggers</strong>:<p class="source-code">triggers = [</p><p class="source-code">    {'start': t['BeginOffset'], 'end': t['EndOffset'], 'label': t['Type']}</p><p class="source-code">    for e in result['Events']</p><p class="source-code">    for t in e['Triggers']</p><p class="source-code">]</p></li>
				<li>Run the following code so that <strong class="source-inline">spans</strong> is <strong class="source-inline">sorted</strong> so that displaCy can process it correctly:<p class="source-code">spans = sorted(entities + triggers, key=lambda x: x['start'])</p><p class="source-code">tags = [s['label'] for s in spans]</p><p class="source-code">output = [{"text": raw_texts[0], "ents": spans, "title": None, "settings": {}}]</p></li>
				<li>Now, we will render all <strong class="source-inline">entities</strong> participating in the event by running the following notebook code:<p class="source-code">displacy.render(output, style="ent", options={"colors": color_map}, manual=True)</p><p>The following is the output of running the preceding code:</p></li>
			</ol>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="Images/B17528_09_13.jpg" alt="Figure 9.13 – Comprehend events and entities" width="1350" height="614"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.13 – Comprehend events and entities</p>
			<p>We have <a id="_idIndexMarker589"/>color-coded the events based on the relationships that were found. Just by looking at the highlighted entities and relationships that are the same color, we can see that John Mackey is the co-founder and CEO and that he will remain employed.</p>
			<h3>Rendering tabular data</h3>
			<p>Many financial <a id="_idIndexMarker590"/>users use <strong class="source-inline">Events</strong> to create structured data from unstructured text. In this section, we'll demonstrate how to do this with pandas. </p>
			<p>First, we must flatten the hierarchical JSON data into a pandas DataFrame by doing the following:</p>
			<ol>
				<li value="1">Create the <strong class="source-inline">entities</strong> DataFrame. The entity indices must be explicitly created:<p class="source-code">entities_df = pd.DataFrame([</p><p class="source-code">    {"EntityIndex": i, **m}</p><p class="source-code">    for i, e in enumerate(result['Entities'])</p><p class="source-code">    for m in e['Mentions']</p><p class="source-code">])</p></li>
				<li>Create <a id="_idIndexMarker591"/>the <strong class="source-inline">events</strong> DataFrame. The <strong class="source-inline">Event</strong> indices must be explicitly created:<p class="source-code">events_df = pd.DataFrame([</p><p class="source-code">    {"EventIndex": i, **a, **t}</p><p class="source-code">    for i, e in enumerate(result['Events'])</p><p class="source-code">    for a in e['Arguments']</p><p class="source-code">    for t in e['Triggers']</p><p class="source-code">])</p></li>
				<li>The following code will join the two tables into one flat data structure:<p class="source-code">events_df = events_df.merge(entities_df, on="EntityIndex", suffixes=('Event', 'Entity'))</p><p>The following is the output of <strong class="source-inline">EntityIndex</strong> as a tabular structure:</p></li>
			</ol>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="Images/B17528_09_14.jpg" alt="Figure 9.14 – Comprehend events entity as a DataFrame&#13;&#10;" width="1472" height="613"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.14 – Comprehend events entity as a DataFrame</p>
			<p>We can see that its easy to analyze and extract important events and metadata respective to those events such as Date and time as a python pandas dataframe. Once your data is in dataframe this can be easily saved into downstream applications such as a database or a graph database for furthur analysis.</p>
			<h3>Tabular representation of analytics</h3>
			<p>We're <a id="_idIndexMarker592"/>primarily interested in the <em class="italic">event structure</em>, so let's make that more transparent by creating a new table with <strong class="source-inline">Roles</strong> as a column header, grouped by event: </p>
			<ul>
				<li>The following code will do this for us:<p class="source-code">def format_compact_events(x):</p><p class="source-code">    This code will take the most commonly occurring EventType and the set of triggers.</p><p class="source-code">    d = {"EventType": Counter(x['TypeEvent']).most_common()[0][0],</p><p class="source-code">         "Triggers": set(x['TextEvent'])}</p><p class="source-code">    This code will loop for each argument Role, collect the set of mentions in the group.</p><p class="source-code">    for role in x['Role']:</p><p class="source-code">        d.update({role: set((x[x['Role']==role]['TextEntity']))})</p><p class="source-code">    return d</p></li>
				<li>The following code will group data by <strong class="source-inline">EventIndex</strong> and format:<p class="source-code">event_analysis_df = pd.DataFrame(</p><p class="source-code">    events_df.groupby("EventIndex").apply(format_compact_events).tolist()</p><p class="source-code">).fillna('')</p><p class="source-code">event_analysis_df</p><p>The following screenshot shows the output of the DataFrame representing the tabular format of Comprehend Events:</p></li>
			</ul>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="Images/B17528_09_15.jpg" alt="Figure 9.15 – Comprehend events tabular representation&#13;&#10;" width="1290" height="581"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.15 – Comprehend events tabular representation</p>
			<p>In <a id="_idIndexMarker593"/>the preceding output, we have a tabular representation of the event type, date, investee, investor, employer, employee, and title, all of which can easily be used by financial analysts to look into the necessary metadata.</p>
			<h3>Graphing event semantics</h3>
			<p>The most <a id="_idIndexMarker594"/>striking representation of the output of Comprehend Events can be found in a semantic graph, which is a network of the entities and events that have been referenced in a document(s). The code we will cover shortly (please open the <strong class="source-inline">pyvis</strong> link for this) uses two open source libraries: <strong class="source-inline">Networkx</strong> and <strong class="source-inline">pyvis</strong>. <strong class="source-inline">Networkx</strong> is a Python package that's used to create, manipulate, and study the structure, dynamics, and functions of complex networks (<a href="https://networkx.org/">https://networkx.org/</a>), while <strong class="source-inline">pyvis</strong> (<a href="https://pyvis.readthedocs.io/en/latest/">https://pyvis.readthedocs.io/en/latest/</a>) is a library that allows you to quickly generate visual <a id="_idIndexMarker595"/>networks to render events system output. The vertices represent entity <strong class="source-inline">mentions</strong> and <strong class="source-inline">triggers</strong>, while the edges are the argument roles held by the entities concerning the <strong class="source-inline">triggers</strong> in the graph.</p>
			<h3>Formatting data</h3>
			<p>The system output must be conformed to the node (that is, the vertex) and edge list format required by Networkx. This requires iterating over <strong class="source-inline">triggers</strong>, <strong class="source-inline">entities</strong>, and <strong class="source-inline">argument</strong> structural relations. Note that we can use the <strong class="source-inline">GroupScore</strong> and <strong class="source-inline">Score</strong> keys on various <a id="_idIndexMarker596"/>objects to prune nodes and edges where the model has less confidence. We can also use various strategies to pick a "canonical" mention from each <strong class="source-inline">mention</strong> group to appear in the graph; here, we have chosen the mention with the longest string-wise extent. Run the following code to format it:</p>
			<ul>
				<li>Entities are associated with events by group, not individual <strong class="source-inline">mentions</strong> for simplicity. The following method assumes that the canonical mention is the longest one:<p class="source-code">def get_canonical_mention(mentions):</p><p class="source-code">    extents = enumerate([m['Text'] form in mentions])</p><p class="source-code">    longest_name = sorted(extents, key=lambda x: len(x[1]))</p><p class="source-code">    return [mentions[longest_name[-1][0]]]</p></li>
				<li>Set a global confidence threshold:<p class="source-code">thr = 0.5</p></li>
				<li>In the following code, we are representing nodes as (<strong class="source-inline">id</strong>, <strong class="source-inline">type</strong>, <strong class="source-inline">tag</strong>, <strong class="source-inline">score</strong>, <strong class="source-inline">mention_type</strong>) tuples:<p class="source-code">trigger_nodes = [</p><p class="source-code">    ("tr%d" % i, t['Type'], t['Text'], t['Score'], "trigger")</p><p class="source-code">    for i, e in enumerate(result['Events'])</p><p class="source-code">    for t in e['Triggers'][:1]</p><p class="source-code">    if t['GroupScore'] &gt; thr</p><p class="source-code">]</p><p class="source-code">entity_nodes = [</p><p class="source-code">    ("en%d" % i, m['Type'], m['Text'], m['Score'], "entity")</p><p class="source-code">    for i, e in enumerate(result['Entities'])</p><p class="source-code">    for m in get_canonical_mention(e['Mentions'])</p><p class="source-code">    if m['GroupScore'] &gt; thr</p><p class="source-code">]</p></li>
				<li>In the <a id="_idIndexMarker597"/>following code, we are representing edges as (<strong class="source-inline">trigger_id</strong>, <strong class="source-inline">node_id</strong>, <strong class="source-inline">role</strong>, <strong class="source-inline">score</strong>) tuples:<p class="source-code">argument_edges = [</p><p class="source-code">    ("tr%d" % i, "en%d" % a['EntityIndex'], a['Role'], a['Score'])</p><p class="source-code">    for i, e in enumerate(result['Events'])</p><p class="source-code">    for a in e['Arguments']</p><p class="source-code">    if a['Score'] &gt; thr</p></li>
				<li>To create a compact graph, once the nodes and edges have been defined, we can create and visualize the graph by using the following code block:<p class="source-code">G = nx.Graph()</p></li>
				<li>Iterate over the <strong class="source-inline">triggers</strong> and entity <strong class="source-inline">mentions</strong>, as follows:<p class="source-code">for mention_id, tag, extent, score, mtype in trigger_nodes + entity_nodes:</p><p class="source-code">    label = extent if mtype.startswith("entity") else tag</p><p class="source-code">    G.add_node(mention_id, label=label, size=score*10, color=color_map[tag], tag=tag, group=mtype)</p></li>
				<li>The following <a id="_idIndexMarker598"/>code iterates over the argument role assignments:<p class="source-code">for event_id, entity_id, role, score in argument_edges:</p><p class="source-code">    G.add_edges_from(</p><p class="source-code">        [(event_id, entity_id)],</p><p class="source-code">        label=role,</p><p class="source-code">        weight=score*100,</p><p class="source-code">        color="grey"</p><p class="source-code">    )</p></li>
				<li>The following code drops <strong class="source-inline">mentions</strong> that don't participate in events:<p class="source-code">G.remove_nodes_from(list(nx.isolates(G)))</p><p class="source-code">nt = Network("600px", "800px", notebook=True, heading="")</p><p class="source-code">nt.from_nx(G)</p><p class="source-code">nt.show("compact_nx.html")</p><p>The following is the output in graph format:</p></li>
			</ul>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="Images/B17528_09_16.jpg" alt="Figure 9.16 – Comprehend events knowledge graph representation&#13;&#10;" width="806" height="649"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.16 – Comprehend events knowledge graph representation</p>
			<p>In the <a id="_idIndexMarker599"/>preceding output, if we traverse this graph, we can see the relationships between the entity, known as Whole Foods, which is a participant in the corporate merger, and its employer. This is John Macey, whose title is CEO.</p>
			<h3>A more complete graph</h3>
			<p>The preceding graph is compact and only relays essential event type and argument role information. We can <a id="_idIndexMarker600"/>use a slightly more complicated set of functions to graph all of the information returned by the API.</p>
			<p>This convenient function in <strong class="source-inline">events_graph.py</strong>. It plots a complete graph of the document, showing all <strong class="source-inline">events</strong>, <strong class="source-inline">triggers</strong>, and <strong class="source-inline">entities</strong>, as well as their groups:</p>
			<p class="source-code">import events_graph as evg</p>
			<p class="source-code">evg.plot(result, node_types=['event', 'trigger', 'entity_group', 'entity'], thr=0.5)</p>
			<p>The following <a id="_idIndexMarker601"/>is the output in graph format:</p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="Images/B17528_09_17.jpg" alt="Figure 9.17 – Comprehend Events knowledge graph visualization" width="811" height="647"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.17 – Comprehend Events knowledge graph visualization</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can use <strong class="bold">Amazon Neptune</strong> for <a id="_idIndexMarker602"/>large-scale knowledge graph analysis with Amazon Comprehend Events.</p>
			<p>Here, we have extracted the metadata and analyzed it in a tabular manner and showed how we can present it in a graph. You can use Amazon Neptune for large-scale knowledge graph analysis with Amazon Comprehend Events, as we covered in <em class="italic">Figure 9.1</em>.</p>
			<p>To deep dive into how you can do this using Amazon Neptune, please refer to the <em class="italic">Further reading</em> section for the relevant blog, which will walk you through how you can build a knowledge graph in Amazon Neptune using Amazon Comprehend Events.</p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="source-inline">Entities</strong> extracted with Comprehend Events are going to be different than Comprehend detect entity API as events are specific to the financial event's entity and relationship extraction.</p>
			<p>You can <a id="_idIndexMarker603"/>also extract metadata from Amazon Comprehend for Word or PDF documents using either a detect entity, a custom entity, or even <a id="_idIndexMarker604"/>Comprehend Events in the case of financial documents and enrich the document labeling process using <strong class="bold">SageMaker Ground Truth</strong>. SageMaker Ground Truth is a service that is primarily used for labeling data. </p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor123"/>Summary</h1>
			<p>In this chapter, we learned why metadata extraction is really important before looking at the use case for <strong class="bold">LiveRight</strong>, our fictitious bank, which had acquisitions that made a press release statement. Financial analysts wanted to quickly evaluate the events and entities concerning this press release and wanted to make market predictions. We looked at an architecture to help you accomplish this. In the architecture shown in <em class="italic">Figure 1.1</em>, we spoke about how you can use AWS AI services such as Amazon Textract to extract text from the sample press release documents. Then, we saved all the text with utf-8 encoding in the Amazon S3 bucket for Amazon Comprehend entity or metadata extractions jobs.</p>
			<p>We used an Amazon Comprehend Events job to extract entities and relationships between the entity. We have provided a <em class="italic">walkthrough video</em> link of the <em class="italic">Comprehend Events feature</em> in the <em class="italic">Further reading</em> section if you wish to learn more. We also provided two ways to configure Comprehend Events job; that is, use either the AWS console or AWS Python boto3 APIs. Finally, we talked about how you can visualize this relationship between extracted metadata using either a graph API such as <strong class="source-inline">displayCy</strong>, <strong class="source-inline">Networkx</strong>, or <strong class="source-inline">pyvis,</strong> or using Amazon Neptune's graph database. We also suggested that this metadata can be further used as an input to data labeling using <strong class="bold">Amazon SageMaker Ground Truth</strong>.</p>
			<p>In the next chapter, we will talk about how you can perform content monetization for your cool websites.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor124"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the following resources:</p>
			<ul>
				<li><em class="italic">Building a knowledge graph in Amazon Neptune using Amazon Comprehend Events</em>, by Brian O'Keefe, Graham Horwood, and Navtanay Sinha (<a href="https://aws.amazon.com/blogs/database/building-a-knowledge-graph-in-amazon-neptune-using-amazon-comprehend-events/">https://aws.amazon.com/blogs/database/building-a-knowledge-graph-in-amazon-neptune-using-amazon-comprehend-events/</a>).</li>
				<li><em class="italic">Announcing the launch of Amazon Comprehend Events</em>, by Graham Horwood, Sameer Karnik, and Ben Snively (<a href="https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-amazon-comprehend-events/">https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-amazon-comprehend-events/</a>).</li>
				<li><em class="italic">Fintech Snacks 2 – Extracting Market-Moving Events with Amazon Comprehend Events</em>, by Mona Mona and Evan Peck (<a href="https://www.youtube.com/watch?v=QvmVT_8y7-Y">https://www.youtube.com/watch?v=QvmVT_8y7-Y</a>).</li>
			</ul>
		</div>
	</div></body></html>