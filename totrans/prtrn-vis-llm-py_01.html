<html><head></head><body>
		<div id="_idContainer010">
			<h1 id="_idParaDest-17" class="chapter-number"><a id="_idTextAnchor016"/>1</h1>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>An Introduction to Pretraining Foundation Models</h1>
			<p class="author-quote">The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin … The only thing that matters in the long run is the leveraging of computation.</p>
			<p class="author-quote">– Richard Sutton, “The Bitter Lesson,” 2019 (1)</p>
			<p>In this chapter, you’ll be introduced to foundation models, the backbone of many artificial intelligence and machine learning systems today. In particular, we<a id="_idIndexMarker000"/> will dive into their creation process, also called pretraining, and understand where it’s competitive to improve the accuracy of your models. We will discuss the core transformer architecture underpinning state-of-the-art models such as Stable Diffusion, BERT, Vision Transformers, OpenChatKit, CLIP, Flan-T5, and more. You will learn about the encoder and decoder frameworks, which work to solve a variety of <span class="No-Break">use cases.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>The art of pretraining <span class="No-Break">and fine-tuning</span></li>
				<li>The Transformer <span class="No-Break">model architecture</span></li>
				<li>State-of-the-art vision and <span class="No-Break">language models</span></li>
				<li>Encoders <span class="No-Break">and decoders</span></li>
			</ul>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>The art of pretraining and fine-tuning</h1>
			<p>Humanity is one<a id="_idIndexMarker001"/> of Earth’s most interesting<a id="_idIndexMarker002"/> creatures. We are capable of producing the greatest of beauty and asking the most profound questions, and yet fundamental aspects about us are, in many cases, largely unknown. What exactly is consciousness? What is the human mind, and where does it reside? What does it mean to be human, and how do <span class="No-Break">humans learn?</span></p>
			<p>While scientists, artists, and thinkers from countless disciplines grapple with these complex questions, the field of computation<a id="_idIndexMarker003"/> marches forward to replicate (and in some cases, surpass) human<a id="_idIndexMarker004"/> intelligence. Today, applications from self-driving cars to writing screenplays, search engines, and question-answering systems have one thing in common – they all use a model, and sometimes many different kinds of models. Where do these models come from, how do they acquire intelligence, and what steps can we take to apply them for maximum impact? Foundation models are essentially compact representations of massive sets of data. The representation comes about through applying a <em class="italic">pretraining objective</em> onto the dataset, from predicting masked tokens to completing sentences. Foundation models are useful because once they have<a id="_idIndexMarker005"/> been created, through the process called pretraining, they can either be deployed directly or fine-tuned for a downstream task. An example<a id="_idIndexMarker006"/> of a foundation model deployed directly is <strong class="bold">Stable Diffusion</strong>, which was pretrained on billions of image-text pairs and generates useful images from text immediately after pretraining. An example of a fine-tuned foundation model is <strong class="bold">BERT</strong>, which was pretrained on large language<a id="_idIndexMarker007"/> datasets, but is most useful when adapted for a downstream domain, such <span class="No-Break">as classification.</span></p>
			<p>When applied in natural language processing, these models can complete sentences, classify text into different categories, produce summarizations, answer questions, do basic math, and generate creative artifacts such as poems and titles. In computer vision, foundation models are useful everywhere from image classification to generation, pose estimation to object detection, pixel mapping, <span class="No-Break">and more.</span></p>
			<p>This comes<a id="_idIndexMarker008"/> about because of defining a <strong class="bold">pretraining objective</strong>, which we’ll learn about in detail in this book. We’ll also cover its peer method, <strong class="bold">fine-tuning</strong>, which helps the model learn<a id="_idIndexMarker009"/> more about a specific domain. This<a id="_idIndexMarker010"/> more generally falls under the category of <strong class="bold">transfer learning</strong>, the practice of taking a pretrained neural network and supplying it with a novel dataset with the hope of enhancing its knowledge in a certain dimension. In both vision and language, these terms have some overlap and some clear distinctions, but don’t worry, we’ll cover them more throughout the chapters. I’m using the term <em class="italic">fine-tuning</em> to include the whole set of techniques to adapt a model to another domain, outside of the one where it was trained, not in the narrow, classic sense of <span class="No-Break">the term.</span></p>
			<p class="callout-heading">Fundamentals – pretraining objectives</p>
			<p class="callout">The heart of large-scale pretraining revolves around this core concept. A <strong class="bold">pretraining objective</strong> is a method that leverages<a id="_idIndexMarker011"/> information readily available in the dataset without requiring extensive human labeling. Some pretraining objectives involve masking, providing a unique <strong class="source-inline">[MASK]</strong> token in place of certain words, and training the model to fill in those words. Others take a different route, using the left-hand side of a given text string to attempt to generate the <span class="No-Break">right-hand side.</span></p>
			<p class="callout">The training process happens through a <strong class="bold">forward pass</strong>, sending your raw training data through the neural network<a id="_idIndexMarker012"/> to produce some output word. The loss function then computes the difference between this predicted word and the one found in the data. This difference between the predicted values<a id="_idIndexMarker013"/> and the actual values then serves as the basis for the <strong class="bold">backward pass</strong>. The backward pass itself usually leverages a type of stochastic gradient descent to update the parameters of the neural network with respect to that same loss function, ensuring that, next time around, it’s more likely to get a lower <span class="No-Break">loss function.</span></p>
			<p class="callout">In the case <a id="_idIndexMarker014"/>of BERT<em class="italic">(2)</em>, the pretraining objective is called a <strong class="bold">masked token loss</strong>. For generative textual models<a id="_idIndexMarker015"/> of the GPT <em class="italic">(3)</em> variety, the pretraining objective is called <strong class="bold">causal language loss</strong>. Another way of thinking about this entire process is <strong class="bold">self-supervised learning</strong>, utilizing content already available<a id="_idIndexMarker016"/> in a dataset to serve as a signal to the model. In computer<a id="_idIndexMarker017"/> vision, you’ll also see this referred to as a <strong class="bold">pretext task</strong>. More on state-of-the-art models in the <span class="No-Break">sections ahead!</span></p>
			<p>Personally, I think pretraining<a id="_idIndexMarker018"/> is one of the most exciting developments<a id="_idIndexMarker019"/> in machine learning research. Why? Because, as Richard Sutton suggests controversially at the start of the chapter, it’s computationally efficient. Using pretraining, you can build a model from massive troves of information available on the int<a id="_idTextAnchor019"/>ernet, then combine all of this knowledge using your own proprietary data and apply it to as many applications as you can dream of. On top of that, pretraining opens the door for tremendous collaboration across company, country, language, and domain lines. The industry is truly just getting started in developing, perfecting, and exploiting the <span class="No-Break">pretraining paradigm.</span></p>
			<p>We know that pretraining is interesting and effective, but where is it competitive in its own right? Pretraining your own model is useful <em class="italic">when your own proprietary dataset is very large and different from common research datasets, and primarily unlabeled</em>. Most of the models we will learn about in this book are trained on similar corpora – Wikipedia, social media, books, and popular internet sites. Many of them focus on the English language, and few of them consciously use the rich interaction between visual and textual data. Throughout the book, we will learn about the nuances and different advantages of selecting and perfecting your <span class="No-Break">pretraining strategies.</span></p>
			<p>If your business or research hypothesis hinges on non-standard natural languages, such as financial or legal terminology, non-English languages, or rich knowledge from another domain, you may want to consider pretraining your own model from scratch. The core question you want to ask yourself is, <em class="italic">How valuable is an extra one percentage point of accuracy in my model?</em> If you do not know the answer to this question, then I strongly recommend spending some time getting yourself to an answer. We will spend time discussing how to do this in <a href="B18942_02.xhtml#_idTextAnchor034"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. Once you can confidently say an increase in the accuracy of my model is worth at least a few hundred thousand dollars, and even possibly a few million, then you are ready to begin pretraining your <span class="No-Break">own model.</span></p>
			<p>Now that we have learned<a id="_idIndexMarker020"/> about foundation<a id="_idIndexMarker021"/> models, how they come about through a process called pretraining, and how to adapt them to a specific domain through fine-tuning, let’s learn more about the Transformer <span class="No-Break">model architecture.</span></p>
			<h1 id="_idParaDest-20"><a id="_idTextAnchor020"/>The Transformer model architecture and self-attention</h1>
			<p>The Transformer model, presented<a id="_idIndexMarker022"/> in the now-famous 2017 paper <em class="italic">Attention is all you need</em>, marked a turning<a id="_idIndexMarker023"/> point for the machine learning industry. This is primarily because it used an existing mathematical technique, self-attention, to solve problems in NLP related<a id="_idIndexMarker024"/> to sequences. The Transformer<a id="_idIndexMarker025"/> certainly wasn’t the first attempt at modeling sequences, previously, <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>) and even <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) were popular <span class="No-Break">in language.</span></p>
			<p>However, the Transformer made headlines because its training cost was a small fraction of the existing techniques. This is because the Transformer is fundamentally easier to parallelize, due to its core self-attention process, than previous techniques. It also set new world records in machine translation. The original Transformer used both an encoder and decoder, techniques we will dive into later throughout this chapter. This joint encoder-decoder pattern was followed directly by other models focused on similar text-to-text tasks, such <span class="No-Break">as T5.</span></p>
			<p>In 2018, Alex Radford and his team presented <strong class="bold">Generative Pretrained Transformers</strong>, a method inspired by the 2017 Transformer, but using<a id="_idIndexMarker026"/> only the decoder. Called <strong class="bold">GPT</strong>, this model handled large-scale unsupervised<a id="_idIndexMarker027"/> pretraining well, and it was paired<a id="_idIndexMarker028"/> with supervised fine-tuning to perform well on downstream tasks. As we mentioned previously, this <em class="italic">causal language modeling</em> technique optimizes the log probability of tokens, giving us a left-to-right ability to find the most probable word in <span class="No-Break">a sequence.</span></p>
			<p>In 2019, Jacob Devlin and his team presented <em class="italic">BERT: Pretraining of Deep Bidirectional Transformers</em>. BERT also adopted the pretraining, fine-tuning paradigm, but implemented a masked language modeling loss function that helped the model learn the impact of tokens both before and after them. This proved useful in disambiguating the meaning of words in different contexts and has aided encoder-only tasks such as classification <span class="No-Break">ever since.</span></p>
			<p>Despite their names, neither<a id="_idIndexMarker029"/> GPT nor BERT uses the full encoder-decoder as presented in the original Transformer paper but instead leverages the <strong class="bold">self-attention mechanism</strong> as core steps throughout the learning process. Thus, it is in fact the self-attention process we <span class="No-Break">should understand.</span></p>
			<p>First, remember that each word, or token, is<a id="_idIndexMarker030"/> represented<a id="_idIndexMarker031"/> as an embedding. This embedding is created simply by using a <strong class="bold">tokenizer</strong>, a pretrained data object for each model that maps the word to its appropriate dense vector. Once<a id="_idIndexMarker032"/> we have the embedding per token, we use <strong class="bold">learnable weights</strong> to generate three new vectors: <strong class="bold">key</strong>, <strong class="bold">query</strong>, and <strong class="bold">value</strong>. We then use matrix multiplication and a few steps to interact with the key and the query, using the value at the very end to determine what was most informative in the sequence overall. Throughout the training loop, we update these weights to get better and better interactions, as determined by your <span class="No-Break">pretraining objective.</span></p>
			<p>Your pretraining objective serves as a directional guide for how to update the model parameters. Said another way, your pretraining objective provides the primary signal to your stochastic gradient descent updating procedure, changing the weights of your model based on how incorrect your model predictions are. When you train for long periods of time, the parameters should reflect a decrease in loss, giving you an overall increase <span class="No-Break">in accuracy.</span></p>
			<p>Interestingly, the type of transformer<a id="_idIndexMarker033"/> heads will change slightly based<a id="_idIndexMarker034"/> on the different types of pretraining objectives you’re using. For example, a normal self-attention block uses information from both the left- and right-hand sides of a token to predict it. This is to provide the most informative contextual information for the prediction and is useful in masked language modeling. In practice, the self-attention heads are stacked to operate on full matrices of embeddings, giving us multi-head attention. Casual language modeling, however, uses a different type of attention head: masked self-attention. This limits the scope of predictive information to only the left-hand side of the matrix, forcing the model to learn a left-to-right procedure. This is in contrast to the more traditional self-attention, which has access to both the left and right sides of the sequence to <span class="No-Break">make predictions.</span></p>
			<p>Most of the time, in practice, and certainly throughout this book, you won’t need to code any transformers or self-attention heads from scratch. Through this book, we will, however, be diving into many model architectures, so it’s helpful to have this conceptual knowledge as <span class="No-Break">a base.</span></p>
			<p>From an intuitive perspective, what you’ll need to understand about transformers and self-attention <span class="No-Break">is fewfold:</span></p>
			<ul>
				<li><strong class="bold">The transformer itself is a model entirely built upon a self-attention function</strong>: The self-attention function takes a set of inputs, such as embeddings, and performs mathematical operations to combine these. When combined with token (word or subword) masking, the model can effectively learn how significant certain parts of the embeddings, or the sequence, are to the other parts. This is the meaning of self-attention; the model is trying to understand which parts of the input dataset are most relevant to the <span class="No-Break">other parts.</span></li>
				<li><strong class="bold">Transformers perform exceedingly well using sequences</strong>: Most of the benchmarks they’ve blown past in recent years are from NLP, for a good reason. The pretraining objectives for these include token masking and sequence completion, both of which rely on not just individual data points but the stringing of them together, and their combination. This is good news for those of you who already work with sequential data and an interesting challenge for those <span class="No-Break">who don’t.</span></li>
				<li><strong class="bold">Transformers operate very well at large scales</strong>: The underlying attention head is easily parallelizable, which gives it a strong leg-up in reference to other candidate sequence-based neural network architectures such as RNNs, including <strong class="bold">Long Short-Term Memory</strong> (<strong class="bold">LSTM</strong>)  based networks. The self-attention head can be set to trainable in the case of pretraining, or untrainable in the case of fine-tuning. When attempting to actually train the self-attention heads, as we’ll do throughout this book, the best performance you’ll see is when the transformers are applied on large datasets. How large they need to be, and what trade-offs you can make when electing to fine-tune or pretrain, is the subject of <span class="No-Break">future chapters.</span></li>
			</ul>
			<p>Transformers are not the only<a id="_idIndexMarker035"/> means of pretraining. As we’ll see throughout<a id="_idIndexMarker036"/> the next section, there are many different types of models, particularly in vision and multimodal cases, which can deliver <span class="No-Break">state-of-the-art performance.</span></p>
			<h1 id="_idParaDest-21"><a id="_idTextAnchor021"/>State-of-the-art vision and language models</h1>
			<p>If you’re new to machine<a id="_idIndexMarker037"/> learning, then<a id="_idIndexMarker038"/> there is a key concept you will eventually want to learn how to master, that is, <strong class="bold">state of the art</strong>. As you are aware, there are many different types of machine learning tasks, such as object detection, semantic segmentation, pose detection, text classification, and question answering. For each of these, there are many different research datasets. Each of these datasets provides labels, frequently for train, test, and validation splits. The datasets tend to be hosted by academic institutions, and each of these is purpose-built to train machine learning models that solve each of these types <span class="No-Break">of problems.</span></p>
			<p>When releasing a new dataset, researchers will frequently also release a new model that has been trained on the train set, tuned on the validation set, and separately evaluated on the test set. Their evaluation score on a new test set establishes a new state of the art for this specific type of modeling problem. When publishing certain types of papers, researchers will frequently try to improve performance in this area – for example, by trying to increase accuracy by a few percentage points on a handful <span class="No-Break">of datasets.</span></p>
			<p>The reason state-of-the-art performance matters for you is that it is a strong indication of how well your model is likely to perform in the best possible scenario. It isn’t easy to replicate most research results, and frequently, labs will have developed special techniques to improve performance that may not be easily observed and replicated by others. This is especially true when datasets and code repositories aren’t shared publicly, as is the case with GPT-3. This is acutely true when training methods aren’t disclosed, as <span class="No-Break">with GPT-4.</span></p>
			<p>However, given sufficient resources, it is possible to achieve similar performance as reported in top papers. An excellent place to find state-of-the-art performance at any given point in time is an excellent website, <em class="italic">Papers With Code</em>, maintained by Meta and enhanced by the community. By using this free tool, you can easily find top papers, datasets, models, and GitHub sites with example code. Additionally, they have great historical views, so you can see how the top models in different datasets have evolved <span class="No-Break">over time.</span></p>
			<p>In later chapters on preparing datasets<a id="_idIndexMarker039"/> and picking models, we’ll go into more detail <a id="_idIndexMarker040"/>on how to find the right examples<a id="_idTextAnchor022"/> for you, including how to determine how similar to and different from your own goals they are. Later in the book, we’ll also help you determine the op<a id="_idTextAnchor023"/>timal models, and sizes for them. Right now, let’s look at some models that, as of this writing, are currently sitting at the top of their <span class="No-Break">respective leaderboards.</span></p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor024"/>Top vision models as of April 2023</h2>
			<p>First, let’s take a quick<a id="_idIndexMarker041"/> look at the models performing the best today within image tasks such as classification <span class="No-Break">and generation.</span></p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Dataset</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Best model</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">From Transformer</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Performance</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">ImageNet</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Basic-L (<span class="No-Break">Lion fine-tuned)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p>91.10% top <span class="No-Break">1% accuracy</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">CIFAR-10</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">ViT-H/14 </span><span class="No-Break"><em class="italic">(1)</em></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">99.5% correct</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">COCO</span></p>
						</td>
						<td class="No-Table-Style">
							<p>InternImage-H (M3I <span class="No-Break">Pre-training: </span><a href="https://paperswithcode.com/paper/internimage-exploring-large-scale-vision"><span class="No-Break">https://paperswithcode.com/paper/internimage-exploring-large-scale-vision</span></a><span class="No-Break">)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">No</span></p>
						</td>
						<td class="No-Table-Style">
							<p>65.0 <span class="No-Break">Box AP</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">STL-10</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Diffusion ProjectedGAN</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">No</span></p>
						</td>
						<td class="No-Table-Style">
							<p>6.91 <span class="No-Break">FID (generation)</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">ObjectNet</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">CoCa</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p>82.7% top <span class="No-Break">1% accuracy</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">MNIST</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Heterogeneous ensemble with simple <span class="No-Break">CNN </span><span class="No-Break"><em class="italic">(1)</em></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">No</span></p>
						</td>
						<td class="No-Table-Style">
							<p>99.91% accuracy (<span class="No-Break">0.09% error)</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.1 – Top image results</p>
			<p>At first glance, these numbers may seem intimidating. After all, many of them are near or close to 99% accurate! Isn’t that too high of a bar for beginning or intermediate machine <span class="No-Break">learning practitioners?</span></p>
			<p>Before we get too carried away<a id="_idIndexMarker042"/> with doubt and fear, it’s helpful to understand that most of these accuracy scores came at least five years after the research dataset was published. If we analyze the historical graphs available on <em class="italic">Paper With Code</em>, it’s easy to see that when the first researchers published their datasets, initial accuracy scores were closer to 60%. Then, it took many years of hard work, across diverse organizations and teams, to finally produce models capable of hitting the 90s. So, don’t lose heart! If you put in the time, you too can train a model that establishes a new state-of-the-art performance<a id="_idTextAnchor025"/> in a given area. This part is science, <span class="No-Break">not magic.</span></p>
			<p>You’ll notice that while some of these models do in fact adopt a Transformer-inspired backend, some do not. Upon closer inspection, you’ll also see that some of these models rely on the pretrain and fine-tune paradigm we’ll be learning about in this book, but not all of them. If you’re new to machine learning, then this discrepancy is something to start getting comfortable with! Robust and diverse scientific debate, perspectives, insights, and observations are critical aspects of maintaining healthy communities and increasing the quality of outcomes across the field as a whole. This means that you can, and should, expect some divergence in methods you come across, and that’s a <span class="No-Break">good thing.</span></p>
			<p>Now that you have a better understanding<a id="_idIndexMarker043"/> of top models in computer vision these days, let’s explore one of the earliest methods combining techniques from large language models with vision: contrastive pretraining and natural <span class="No-Break">language supervision.</span></p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor026"/>Contrastive pretraining and natural language supervision</h2>
			<p>What’s interesting about both modern and classic image datasets, from Fei-Fei Li’s 2006 ImageNet to the LAION-5B as used in 2022 Stable Diffusion, is that the labels themselves are composed of natural language. Said another way, because the scope of the images includes objects from the physical world, the labels<a id="_idIndexMarker044"/> necessarily are more nuanced than single digits. Broadly speaking, this type of problem framing is called <strong class="bold">natural </strong><span class="No-Break"><strong class="bold">language supervision</strong></span><span class="No-Break">.</span></p>
			<p>Imagine having a large dataset of tens of millions of images, each provided with captions. Beyond simply naming the objects, a caption gives you more information about the content of the images. A caption can be anything from <em class="italic">Stella sits on a yellow couch</em> to <em class="italic">Pepper, the Australian pup</em>. In just a few words we immediately get more context than simply describing the objects. Now, imagine using a pretrained model, such as an encoder, to process the language into a dense vector representation. Then, combine this with another pretrained model, this time an image encoder, to process the image into another dense vector representation. Combine both of these in a learnable matrix, and you are on your way to contrastive pretraining! Also presented by Alex Radford and the team, just a few years after their work on GPT, this method gives us both<a id="_idIndexMarker045"/> a way to jointly learn about the relationship between both images and language and a model well suited to do so. The model is called <strong class="bold">Contrastive Language-Image </strong><span class="No-Break"><strong class="bold">Pretraining</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CLIP</strong></span><span class="No-Break">).</span></p>
			<p>CLIP certainly isn’t the only vision-language pretraining task that uses natural language supervision. One year<a id="_idIndexMarker046"/> earlier, in 2019, a research team from China proposed a <strong class="bold">Visual-Linguistic BERT</strong> model attempting a similar goal. Since then, the joint training of vision-and-language foundation models has become very popular, with Flamingo, Imagen, and Stable Diffusion all presenting <span class="No-Break">interesting work.</span></p>
			<p>Now that we’ve learned a little bit about joint vision-and-language contrastive pretraining, let’s explore today’s top models <span class="No-Break">in language.</span></p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor027"/>Top language models as of April 2023</h2>
			<p>Now, let’s evaluate some<a id="_idIndexMarker047"/> of today’s best-in-class models for a task extremely pertinent to foundation models, and thus this book: language modeling. This table shows a set of language model benchmark results across a variety <span class="No-Break">of scenarios.</span></p>
			<table id="table002" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Dataset</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Best model</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">From Transformer</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Performance</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">WikiText-103</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Hybrid H3 (<span class="No-Break">2.7B params)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">No</span></p>
						</td>
						<td class="No-Table-Style">
							<p>10.60 <span class="No-Break">test perplexity</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Penn Treebank (<span class="No-Break">Word Level)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>GPT-3 (<span class="No-Break">Zero-Shot) </span><span class="No-Break"><em class="italic">(1)</em></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p>20.5 <span class="No-Break">test perplexity</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">LAMBADA</span></p>
						</td>
						<td class="No-Table-Style">
							<p>PaLM-540B (<span class="No-Break">Few-Shot) </span><span class="No-Break"><em class="italic">(1)</em></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">89.7% accuracy</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Penn Treebank (<span class="No-Break">Character Level)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Mogrifer LSTM + dynamic <span class="No-Break">eval </span><span class="No-Break"><em class="italic">(1)</em></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">No</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1.083 bit <span class="No-Break">per character</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>C4 (Colossal Clean <span class="No-Break">Crawled Corpus)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Primer</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">No</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">12.35 perplexity</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.2 – Top language modeling results</p>
			<p>First, let’s try to answer a fundamental question. What is language modeling, and why does it matter? Language modeling as known today appears to have been formalized in two cornerstone papers: BERT <em class="italic">(9)</em> and GPT <em class="italic">(10)</em>. The core concept that inspired both papers is deceptively simple: how do we better use unsupervised <span class="No-Break">natural language?</span></p>
			<p>As is no doubt unsurprising to you, <a id="_idTextAnchor028"/>the vast majority of natural language in our world has no direct digital label. Some natural language lends itself well to concrete labels, such as cases where objectivity is beyond doubt. This can include accuracy in answering questions, summarization, high-level sentiment analysis, document retrieval, <span class="No-Break">and more.</span></p>
			<p>But the process of finding these labels and producing the datasets necessary for them can be prohibitive, as it is entirely manual. At the same time, many unsupervised datasets get larger by the minute. Now that much of the global dialog is online, datasets rich in variety are easy to access. So, how can ML researchers position themselves to benefit from these large, <span class="No-Break">unsupervised datasets?</span></p>
			<p>This is exactly the problem<a id="_idIndexMarker048"/> that language modeling seeks to solve. Language modeling is a process to apply mathematical techniques on large corpora of unlabelled text, relying on a variety of pretraining<a id="_idIndexMarker049"/> objectives to enable the model to <em class="italic">teach itself</em> about the text. Also called <strong class="bold">self-supervision</strong>, the precise method of learning varies based on the model at hand. BERT applies a mask randomly throughout the dataset and learns to predict the word hidden by the mask, using an encoder. GPT uses a decoder to predict left-to-right, starting at the beginning of a sentence, for example, and learning how to predict the end of the sentence. Models in the T5 family use both encoders and decoders to learn text-to-text tasks, such as translation<a id="_idIndexMarker050"/> and search. As proposed in ELECTRA <em class="italic">(11)</em>, another alternative is a <strong class="bold">token replacement</strong> objective, which opts to inject new tokens into the original text, rather than <span class="No-Break">masking them.</span></p>
			<p class="callout-heading">Fundamentals – fine-tuning</p>
			<p class="callout">Foundational language models<a id="_idIndexMarker051"/> are only useful in applications when paired with their peer method, fine-tuning. The intuition behind fine-tuning is very understandable; we want to take a foundational model pretrained elsewhere and apply a much smaller set of data to make it more focused<a id="_idIndexMarker052"/> and useful for our specific task. We can also call this <strong class="bold">domain adaptation</strong> – adapting a pretrained model to an entirely different domain that was not included in its <span class="No-Break">pretraining task.</span></p>
			<p class="callout">Fine-tuning tasks are everywhere! You can take a base language model, such as BERT, and fine-tune it for text classification. Or question answering. Or named entity recognition. Or you could take a different model, GPT-2 for example, and fine-tune it for summarization. Or you could take something like T5 and fine-tune it for translation. The basic idea is that you are leveraging the intelligence of the foundation model. You’re leveraging the compute, the dataset, the large neural network, and ultimately, the distribution method the researchers leveraged simply by inheriting their pretrained artifact. Then, you can optionally add extra layers to the network yourself, or more likely, use a software framework such as Hugging Face to simplify the process. Hugging Face has done an amazing job building an extremely popular open source framework with tens of thousands of pretrained models, and we’ll see in future chapters how to best utilize their examples to build our own models in both vision and language. There are many different types of fine-tuning, from parameter-efficient fine-tuning to instruction-fine-tuning, chain of thought, and even methods that don’t strictly update the core model parameters<a id="_idIndexMarker053"/> such as retrieval augmented generation. We’ll discuss these later in <span class="No-Break">the book.</span></p>
			<p>As we will discover in future chapters, foundational language and vision models are not without their negative aspects. For starters, their extremely large compute requirements place significant energy demands on service providers. Ensuring that energy is met through sustainable means and that the modeling process is as efficient as possible are top goals for the models of the future. These large compute requirements are also obviously quite expensive, posing inherent challenges for those without sufficient resources. I would argue, however, that the core techniques you’ll learn throughout this book are relevant across a wide spectrum of computational needs and resourcing. Once you’ve demonstrated success at a smaller scale of pretraining, it’s usually much easier to justify the <span class="No-Break">additional ask.</span></p>
			<p>Additionally, as we will see in future chapters, large models are infamous for their ability to inherit social biases present in their training data. From associating certain employment aspects with gender to classifying criminal likelihood based on race, researchers have identified hundreds <em class="italic">(9)</em> of ways bias can creep into NLP systems. As with all technology, designers and developers must be aware of these risks and take steps to mitigate them. In later chapters, I’ll identify<a id="_idIndexMarker054"/> a variety of steps you can take today to reduce <span class="No-Break">these risks.</span></p>
			<p>Next, let’s learn about a core technique used in defining appropriate experiments for language models: the <span class="No-Break">scaling laws!</span></p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor029"/>Language technique spotlight – causal modeling and the scaling laws</h2>
			<p>You’ve no<a id="_idIndexMarker055"/> doubt heard<a id="_idIndexMarker056"/> of the<a id="_idIndexMarker057"/> now-infamous model <strong class="bold">ChatGPT</strong>. For a few years, a San<a id="_idIndexMarker058"/> Francisco-based<a id="_idIndexMarker059"/> AI firm, OpenAI, developed research with a mission to improve humanity’s outcomes around artificial intelligence. Toward that end, they made bold leaps in scaling language models, deriving formulas as one might in physics to explain the performance of LLMs at scale. They originally positioned themselves as a non-profit, releasing their core insights and the code to reproduce them. Four years after its founding, however, they pivoted to cutting exclusive billion-dollar deals with Microsoft.  Now, their 600-strong R&amp;D teams focus on developing proprietary models and techniques, and many open source projects attempt to replicate and improve on their offerings. Despite this controversial pivot, the team at OpenAI gave the industry a few extremely useful insights. The first is GPT, and the second is the <span class="No-Break">scaling laws.</span></p>
			<p>As mentioned previously, GPT-based models use <strong class="bold">causal language modeling</strong> to learn how best to<a id="_idIndexMarker060"/> complete text. This means using a left-to-right completion learning<a id="_idIndexMarker061"/> criteria, which updates<a id="_idIndexMarker062"/> the model’s learnable parameters<a id="_idIndexMarker063"/> until the text is completely accurate. While the first GPT model of 2018 was itself useful, the real excitement came years later in two phases. First, Jared Kaplan lead a team at OpenAI to suggest a novel concept: using formulas inspired by his work in physics to estimate the impact the size of the model, dataset, and overall compute environment will have on the loss of the model. These <em class="italic">Scaling Laws for Neural Language Models</em> <em class="italic">(9)</em> suggested that the optimal model size for a given compute environment <span class="No-Break">was massive.</span></p>
			<p>The original GPT model of 2018 was only 117 million parameters, and its second version, aptly named GPT-2, increased the model size by up to 10x. This increase in parameter size more than doubled the overall accuracy of the model. Encouraged by these results, and fuelled by Kaplan’s theoretical and empirical findings, OpenAI boldly increased the model parameter size by another 10x, giving <span class="No-Break">us GPT-3.</span></p>
			<p>As the model increased in size, from 1.3 billion parameters to 13 billion, ultimately hitting 175 billion parameters, accuracy also took a huge leap! This result catalyzed the field of NLP, unleashing new use cases and a flurry of new work exploring and extending these impacts. Since then, new work has explored both larger (PaLM <em class="italic">(9)</em>) and smaller (Chinchilla <em class="italic">(10)</em>) models, with Chinchilla presenting an update to the scaling laws entirely. Yann LeCunn’s team at Meta has also presented smaller models that outperform the larger ones in specific areas, such as question-answering (Atlas <em class="italic">(9)</em>). Amazon has also presented two models that outperform GPT-3: the AlexaTM and MM-COT. Numerous teams have also undertaken efforts to produce open source versions of GPT-3, such as Hugging Face’s BLOOM, EleutherAI’S GPT-J, and <span class="No-Break">Meta’s OPT.</span></p>
			<p>The rest of this book is dedicated to discussing these models – where they come from, what they are good for, and especially how to train your own! While much excellent work has covered using these pretrained models in production through fine-tuning, such as Hugging Face’s own <em class="italic">Natural Language Processing with Transformers</em> (Tunstall et al., 2022), I continue to believe that pretraining your own foundation model is probably the most interesting computational<a id="_idIndexMarker064"/> intellectual exercise you can embark<a id="_idIndexMarker065"/> on today. I also believe it’s one<a id="_idIndexMarker066"/> of the most profitable. But more<a id="_idIndexMarker067"/> on <span class="No-Break">that ahead!</span></p>
			<p>Next, let’s learn about two key model components you’ll need to understand in detail: encoders <span class="No-Break">and decoders.</span></p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor030"/>Encoders and decoders</h1>
			<p>Now, I’d like to briefly<a id="_idIndexMarker068"/> introduce you to two key<a id="_idIndexMarker069"/> topics that you’ll see in the discussion of transformer-based models: encoders and decoders. Let’s establish some basic intuition to help you understand what they are all about. An encoder is simply a computational graph (or neural network, function, or object depending on your background), which takes an input with a larger feature space and returns an object with a smaller feature space. We hope (and demonstrate computationally) that the encoder is able to learn what is most essential about the provided <span class="No-Break">input data.</span></p>
			<p>Typically, in large language and vision models, the encoder itself is composed of a number of multi-head self-attention objects. This means that in transformer-based models, an encoder is usually a number of self-attention steps, learning what is most essential about the provided input data and passing this onto the downstream model. Let’s look at a <span class="No-Break">quick visual:</span></p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/B18942_01_01.jpg" alt="Figure 1.1 – ﻿Encoders and decoders"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Encoders and decoders</p>
			<p>Intuitively, as you<a id="_idIndexMarker070"/> can see in the preceding<a id="_idIndexMarker071"/> figure, the encoder starts with a larger input space and iteratively compresses this to a smaller latent space. In the case of classification, this is just a classification head with output allotted for each class. In the case of masked language modeling, encoders are stacked on top of each other to better predict tokens to replace the masks. This mea<a id="_idTextAnchor031"/>ns the encoders output an embedding, or a numerical representation of that token, and after prediction, the tokenizer is reused to translate that embedding back into <span class="No-Break">natural language.</span></p>
			<p>One of the earliest large language models, BERT, is an encoder-only model. Most other BERT-based models, such as DeBERTa, DistiliBERT, RoBERTa, DeBERTa, and others in this family use encoder-only model architectures. Decoders operate exactly in reverse, starting with a compressed representation and iteratively recomposing that back into a larger feature space. Both encoders and decoders can be combined, as in the original Transformer, to solve <span class="No-Break">text-to-text problems.</span></p>
			<p>To make it easier, here’s a short table quickly summarizing the three types of self-attention blocks we’ve looked at, encoders, decoders, and <span class="No-Break">their combination.</span></p>
			<table id="table003" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Size of inputs <span class="No-Break">and outputs</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Type of <span class="No-Break">self-attention blocks</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Machine <span class="No-Break">learning tasks</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Example models</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Long <span class="No-Break">to short</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Encoder</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Classification, any <span class="No-Break">dense representation</span></p>
						</td>
						<td class="No-Table-Style">
							<p>BERT, DeBERTa, DistiliBERT, RoBERTa, XLM, AlBERT, CLIP, VL-BERT, <span class="No-Break">Vision Transformer</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Short <span class="No-Break">to long</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Decoder</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Generation, summarization, question-answering, any <span class="No-Break">sparse representation</span></p>
						</td>
						<td class="No-Table-Style">
							<p>GPT, GPT-2, GPT-Neo, GPT-J, ChatGPT, GPT-4, <span class="No-Break">BLOOM, OPT</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Equal</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Encoder-decoder</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Machine translation, <span class="No-Break">style translation</span></p>
						</td>
						<td class="No-Table-Style">
							<p>T5, BART, BigBird, FLAN-T5, <span class="No-Break">Stable Diffusion</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.3 – Encoders, decoders, and their combination</p>
			<p>Now that you have a better<a id="_idIndexMarker072"/> understanding of encoders, decoders, and the models they create, let’s close<a id="_idIndexMarker073"/> out the chapter with a quick recap of all the concepts you just <span class="No-Break">learned about.</span></p>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor032"/>Summary</h1>
			<p>We’ve covered a lot in just this first chapter! Let’s quickly recap some of the top themes before moving on. First, we looked at the art of pretraining and fine-tuning, including a few key pretraining objects such as masked language and causal language modeling. We learned about the Transformer model architecture, including the core self-attention mechanism with its variant. We looked at state-of-the-art vision and language models, including spotlights on contrastive pretraining from natural language supervision, and scaling laws for neural language models. We learned about encoders, decoders, and their combination, which are useful throughout the vision and language <span class="No-Break">domains today.</span></p>
			<p>Now that you have a great conceptual and applied basis to understand pretraining foundation models, let’s look at preparing your dataset: <span class="No-Break">part one.</span></p>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor033"/>References</h1>
			<p>Please go through the following content for more information on a few topics covered in <span class="No-Break">the chapter:</span></p>
			<ol>
				<li><em class="italic">The Bitter Lesson</em>, Rich Sutton, March 13, <span class="No-Break">2019: </span><a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html"><span class="No-Break">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</span></a></li>
				<li>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. <em class="italic">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>: <a href="https://aclanthology.org/N19-1423/">https://aclanthology.org/N19-1423/</a>. In <em class="italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186, Minneapolis, Minnesota. Association for <span class="No-Break">Computational Linguistics.</span></li>
				<li>Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario. 2020. <em class="italic">Language Models are Few-Shot Learners</em>. In <em class="italic">Advances in Neural Information Processing Systems, Volume 33</em>. Pages 1877-1901. Curran <span class="No-Break">Associates, Inc.</span></li>
				<li><em class="italic">AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT </em><span class="No-Break"><em class="italic">SCALE</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2010.11929v2.pdf"><span class="No-Break">https://arxiv.org/pdf/2010.11929v2.pdf</span></a></li>
				<li><em class="italic">AN ENSEMBLE OF SIMPLE CONVOLUTIONAL NEURAL NETWORK MODELS FOR MNIST DIGIT </em><span class="No-Break"><em class="italic">RECOGNITION</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2008.10400v2.pdf"><span class="No-Break">https://arxiv.org/pdf/2008.10400v2.pdf</span></a></li>
				<li><em class="italic">Language Models are Few-Shot </em><span class="No-Break"><em class="italic">Learners</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2005.14165v4.pdf "><span class="No-Break">https://arxiv.org/pdf/2005.14165v4.pdf</span></a></li>
				<li><em class="italic">PaLM: Scaling Language Modeling with </em><span class="No-Break"><em class="italic">Pathways</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2204.02311v3.pdf"><span class="No-Break">https://arxiv.org/pdf/2204.02311v3.pdf</span></a></li>
				<li><em class="italic">MOGRIFIER </em><span class="No-Break"><em class="italic">LSTM</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/1909.01792v2.pdf"><span class="No-Break">https://arxiv.org/pdf/1909.01792v2.pdf</span></a></li>
				<li><em class="italic">BERT: Pre-training of Deep Bidirectional Transformers for Language </em><span class="No-Break"><em class="italic">Understanding</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/1810.04805.pdf"><span class="No-Break">https://arxiv.org/pdf/1810.04805.pdf</span></a></li>
				<li><em class="italic">Improving Language Understanding by Generative </em><span class="No-Break"><em class="italic">Pre-Training</em></span><span class="No-Break">: </span><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"><span class="No-Break">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</span></a></li>
				<li><em class="italic">ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN </em><span class="No-Break"><em class="italic">GENERATORS</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2003.10555.pdf"><span class="No-Break">https://arxiv.org/pdf/2003.10555.pdf</span></a></li>
				<li><em class="italic">Language (Technology) is Power: A Critical Survey of “Bias” in </em><span class="No-Break"><em class="italic">NLP</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2005.14050.pdf"><span class="No-Break">https://arxiv.org/pdf/2005.14050.pdf</span></a></li>
				<li><em class="italic">Scaling Laws for Neural Language </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2001.08361.pdf"><span class="No-Break">https://arxiv.org/pdf/2001.08361.pdf</span></a></li>
				<li><em class="italic">PaLM: Scaling Language Modeling with </em><span class="No-Break"><em class="italic">Pathways</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2204.02311.pdf"><span class="No-Break">https://arxiv.org/pdf/2204.02311.pdf</span></a></li>
				<li><em class="italic">Training Compute-Optimal Large Language </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2203.15556.pdf"><span class="No-Break">https://arxiv.org/pdf/2203.15556.pdf</span></a></li>
				<li><em class="italic">Atlas: Few-shot Learning with Retrieval Augmented Language </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href=" https://arxiv.org/pdf/2208.03299.pdf"><span class="No-Break">https://arxiv.org/pdf/2208.03299.pdf</span></a></li>
			</ol>
		</div>
	</body></html>