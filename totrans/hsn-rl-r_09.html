<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Temporal Difference Learning</h1>
                </header>
            
            <article>
                
<p><strong>Temporal difference</strong> (<strong>TD</strong>) learning algorithms are based on reducing the differences between estimates that are made by the agent at different times. It is a combination of the ideas of the <strong>Monte Carlo</strong>¬†(<strong>MC</strong>)¬†method¬†and <strong>dynamic programming</strong> (<strong>DP</strong>). <span><span>The algorithm¬†</span></span>can learn directly from raw data, without a model of the dynamics of the environment (like MC). Update estimates are based, in part, on other learned estimates, without waiting for the result (bootstrap, like DP). In this chapter, we will learn how to use TD learning algorithms to resolve the vehicle routing problem.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Understanding TD methods</li>
<li>Introducing graph theory and its implementation in R</li>
<li>Implementing TD methods to the vehicle routing problem</li>
</ul>
<p class="mce-root">By the end of this chapter, you will have learned about the different types of TD learning algorithms and how to use them to predict the future behavior of a system. We will learn about the basic concepts of the Q-learning algorithm and use them to generate system behavior through the current best policy estimate. Finally, we will differentiate between SARSA and the Q-learning approach.</p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2YTB7dD">http://bit.ly/2YTB7dD</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding TD methods</h1>
                </header>
            
            <article>
                
<p>TD methods are based on reducing the differences between the estimates that are made by the agent at different times. Q-learning, which we will learn about in the following section, is a TD algorithm, but it is based on the difference between states in immediate adjacent instants. TD is more generic and may consider moments and states that are further away.</p>
<p>TD methods are a combination of the ideas of the MC method and DP, which, as you may recall, can be summarized as follows:</p>
<ul>
<li>MC methods allow us to solve reinforcement learning problems based on the average of the results obtained.</li>
<li>DP represents a set of algorithms that can be used to calculate an optimal policy when given a perfect model of the environment in the form of an MDP.</li>
</ul>
<p>The TD methods, on the one hand, inherit the idea of learning directly from the experience accumulated interacting with the system, without the dynamics of the system itself, from the Monte Carlo method. While they inherit from the DP methods, the idea is to update the estimate of functions in a state from the estimates made in other states (bootstrap). TD methods are suitable for learning without a model of dynamic environments. You need to converge using a fixed policy if the time step is sufficiently small or if it reduces over time.</p>
<p>Such methods differ from other techniques because they try to minimize the error of consecutive time forecasts. To achieve this goal, these methods rewrite the update of the value function in the form of a Bellman equation, thereby improving the prediction by bootstrapping. Here, the variance of the forecast is reduced in each update step. To get a backpropagation of updates in order to save memory, an eligibility vector is applied. Example trajectories are used more efficiently, resulting in good learning rates.</p>
<p>The methods based on time differences allow us to manage the problem of control (that is, to search for the optimal policy) by letting us update the value functions based on the results of the transition to the next state. At every step, the function <em>Q</em> (action-value function) is updated based on the value it has assumed for the next state-action pair and the reward that's obtained through the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4fc3644e-c504-4a0e-a6cb-af4305e45efe.png" style="width:18.50em;height:1.33em;"/></p>
<p>By adopting a one-step look-ahead, it is clear that a two-step formula can also be used, as shown in the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/099626e5-5156-4773-a468-7559189b2dab.png" style="width:28.00em;height:1.67em;"/></p>
<div class="packt_tip">The term look-ahead specifies the procedure that tries to predict the effects of choosing a branching variable in the evaluation of one of its values. This procedure has the following purposes: to choose a variable to be evaluated later and to evaluate the order of the values to be assigned to it.</div>
<p class="mce-root">More generally, with <em>n</em>-step look-ahead, we obtain the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6807f18d-87c3-400d-b7e9-3eb6a25f8909.png" style="width:40.42em;height:1.83em;"/></p>
<p>An aspect of characterizing the different types of algorithms based on temporal difference is the methodology of choosing an action. There are "on-policy"¬†<span>methods,</span>¬†in which the update is made based on the results of actions that have been determined by the selected policy, and "off-policy" methods, in which various policies can be assessed through hypothetical actions, that aren't actually undertaken. Unlike "on-policy" methods, the latter can separate the problem of exploration from that of control, and learning tactics aren't necessarily applied during the learning phase.</p>
<p>In the following sections, we will learn how to implement TD methods through two approaches: SARSA and Q-learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SARSA</h1>
                </header>
            
            <article>
                
<p>As we anticipated in <a href="2362715d-f8f2-435a-9c00-975ed61986a8.xhtml">Chapter 1</a>,¬†<em>Overview of Reinforcement Learning with R</em>, the SARSA algorithm implements an on-policy TD method, in which the update of the action-value function (<em>Q</em>) is performed based on the results of the transition from the state <em>s = s (t)</em> to the state <em>s' = s (t + 1)</em> by the action <em>a (t)</em>, which is taken based on a selected policy <em>œÄ (s, a)</em>.</p>
<p>Some policies always choose the action providing the maximum reward and nondeterministic policies (Œµ-greedy, Œµ-soft, or softmax), which ensure an element of exploration in the learning phase.</p>
<p>In SARSA, it is necessary to estimate the action-value function ùëû (ùë†, ùëé) because the total value of a state ùë£ (ùë†) (value function) is not sufficient in the absence of an environment model to allow the policy to determine, given a state, which action is performed the best. In this case, however, the values are estimated step by step by following the Bellman equation with the update parameter ùë£ (ùë†), while considering¬†<span>the state-action pair</span>¬†in place of a state.</p>
<p>Due to its on-policy nature, SARSA estimates the action-value function based on the behavior of the œÄ policy, and at the same time, modifies the greedy behavior of the policy with respect to the updated estimates from the action-value function. The convergence of SARSA, and more generally of all TD methods, depends on the nature of policies.</p>
<p class="mce-root"/>
<p>The following code block shows the pseudo-code for the SARSA algorithm:</p>
<pre>Initialize<br/>   arbitrary action-value function<br/>Repeat (for each episode)<br/>   Initialize s<br/>   choose a from s using policy from action-value function<br/>   Repeat (for each step in episode)<br/>      take action a<br/>      observe r, s'<br/>     choose a' from s' using policy from action-value function<br/>      update action-value function<br/>      update s,a</pre>
<p>The <kbd>update</kbd> rule of the action-value function uses all five elements (<kbd>s<sub>t</sub></kbd>, <kbd>a<sub>t</sub></kbd>, <kbd>r<sub>t + 1</sub></kbd>, <kbd>s<sub>t + 1</sub></kbd>, and¬†<kbd>a<sub>t + 1</sub></kbd>)¬† and for this reason, it is called <strong>State-Action-Reward-State-Action</strong> (<strong>SARSA</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Q-learning</h1>
                </header>
            
            <article>
                
<p>Q-learning is one of the most used reinforcement learning algorithms. This is due to its ability to compare the expected utility of the available actions without requiring an environment model. Thanks to this technique, it is possible to find an optimal action for every given state in a finished MDP.</p>
<p>A general solution <span>to the reinforcement learning problem is to estimate an evaluation function during the learning proce</span>ss. This function must be able to evaluate the convenience or otherwise of a particular policy <span>through the sum of the rewards</span>. In fact, Q-learning tries to maximize the value of the Q function (action-value function), which represents the maximum discounted future reward when we perform actions <em>a</em> in the state <em>s</em>.</p>
<p>Q-learning, like SARSA, estimates the function value ùëû (ùë†, ùëé) incrementally, updating the value of the state-action pair at each step of the environment, following the logic of updating the general formula for estimating the values for the TD methods. Q-learning, unlike SARSA, has off-policy characteristics. That is, while the policy is improved according to the values estimated by ùëû (ùë†, ùëé), the value function updates the estimates following a strictly greedy secondary policy: given a state, the chosen action is always the one that maximizes the value <em>max</em>ùëû (ùë†, ùëé). However, the œÄ policy has an important role in estimating values because <span>the state-action pairs to be visited and updated are determined¬†</span>through it.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p><span>The following code block shows pseudo-code</span> for the Q-learning algorithm:</p>
<pre>Initialize<br/>   arbitrary action-value function<br/>Repeat (for each episode)<br/>   Initialize s<br/>   choose a from s using policy from action-value function<br/>   Repeat (for each step in episode)<br/>      take action a<br/>      observe r, s'<br/>      update action-value function<br/>      update s</pre>
<p>Q-learning uses a table to store each state-action couple. At each step, the agent observes the current state of the environment and using the œÄ policy selects and executes the action. By executing the action, the agent obtains the reward, ùëÖ<sub>ùë°+1</sub>, and the new state, ùëÜ<sub>ùë°+1</sub>. At this point, the agent can calculate ùëÑ (s<sub>ùë°</sub>, a<sub>ùë°</sub>), updating the estimate.</p>
<p>In the following section, the basis of graph theory will be given and how this technology can be addressed in R.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing graph theory and implementing it in R</h1>
                </header>
            
            <article>
                
<p>Graphs are data structures that are widely used in optimization problems. A graph is represented by a vertex and an edge structure. The vertices can be events from which different alternatives (the edges) depart. Typically, graphs are used to represent a network unambiguously: vertices represent individual calculators, road intersections, or bus stops, and edges are electrical connections or roads. Edges can connect vertices in any way possible.</p>
<p>Graph theory is a branch of mathematics that allows you to describe sets of objects together with their relationships; it was invented in 1700 by Leonhard Euler.</p>
<p>A graph is indicated in a compact way with <em>G = (V, E)</em>, where <em>V</em> indicates the set of vertices and <em>E</em> the set of edges that constitute it. The number of vertices is <em>|V|</em> and the number of edges is <em>|E|</em>.¬†The number of vertices of the graph, or of a subpart of it, is obviously the fundamental quantity to define its dimensions; the number and distribution of edges describe their connectivity.</p>
<p class="mce-root"/>
<p>There are different types of edges: we are talking about undirected edges for which the edges do not have a direction in comparison with those directed. A directed edge is called an arc and the relative graph is called a¬†<strong>digraph</strong>. For example, undirected edges are used to represent computer networks with synchronous links for data transmission (as shown in the following diagram), directed graphs can represent road networks, allowing the representation of double-senses and unique senses.</p>
<p>The following diagram represents a simple graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-488 image-border" src="assets/9f6d2eaf-d7ed-4e54-8342-da449764a46b.png" style="width:31.17em;height:20.42em;"/></p>
<p><span>We say the graph is connected if we can reach all of the other vertices of the graph from any given vertex</span>. Weighted graphs are graphs if a weight is associated with each edge, which is normally defined by a weight function (<em>w</em>). The weight can be seen as a cost or the distance between the two knots that the bow unites. The cost can be dependent on the flow that crosses the edge through a law. In this sense, the function w can be linear or not and depends on the flow that crosses the edge (non-congested networks) or also on the flow of nearby edges (congested networks).</p>
<p>A vertex is characterized by its degree, which is equal to the number of edges that end on the vertex itself. Depending on the degree, the vertices are as follows:</p>
<ul>
<li>A vertex of order 0 is called an isolated vertex.</li>
<li>A vertex of order 1 is called a leaf vertex.</li>
</ul>
<p>The following diagram shows¬†a graph with vertices labeled by degree:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-489 image-border" src="assets/62f921fa-7336-4572-b0dd-c3c5dc96e5f9.png" style="width:27.67em;height:9.83em;"/></p>
<p>In a directed graph, we can distinguish the outdegree (number of outgoing edges) from the indegree (number of incoming edges). Based on this assumption, a<span>¬†vertex with an indegree of zero is called a source vertex and a</span><span>¬†vertex with an outdegree of zero is called a sink vertex.</span></p>
<p>Finally, a simplicial vertex is one whose neighbors form a clique: every two neighbors are adjacent. A universal vertex is a vertex that is adjacent to every other vertex in the graph.</p>
<p>To represent a graph, different approaches are available, such as the following:</p>
<ul>
<li>Graphic representation (as shown in the previous diagram)</li>
<li>Adjacency matrix</li>
<li>List of vertices <em>V</em> and of arcs <em>E</em></li>
</ul>
<p>The first way to represent a graph was clearly introduced through a practical example (see the previous diagram). In the graphical representation, circles are used to represent the vertices and lines to indicate the connections between two vertices if they are connected. If this connection has a direction, then it is indicated by adding an arrow. In the following section, we will analyze the other two ways of representing a graph.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adjacency matrix</h1>
                </header>
            
            <article>
                
<p>So far, we have represented a graph through vertices and edges. When the number of vertices is small, this way of representing a graph is the best one because it allows us to analyze its structure intuitively. When the number of vertices becomes large, the graphic representation becomes confusing. In this case, it is better to represent the graph through the adjacency matrix. By adjacency matrix or connection matrix, we mean a data structure that's commonly used in graph representation. It is widely used in the drafting of algorithms that operate on graphs and in their computer representation. If it is a sparse matrix, the use of the adjacency list is preferable to the matrix.</p>
<p class="mce-root"/>
<p>Given any graph, its adjacency matrix is ‚Äã‚Äãmade up of a square binary matrix that has the names of the vertices of the graph as rows and columns. In the place (<em>i, j</em>) of the matrix, there is a 1 if and only if an edge that goes from the vertex <em>i</em> to the vertex <em>j</em> exists in the graph; otherwise, there is a 0. In the case of the representation of undirected graphs, the matrix is symmetric with respect to the main diagonal. For example, check out the graph represented in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-490 image-border" src="assets/3d3da994-fab2-4b91-bb9e-a5e2d0417f04.png" style="width:35.58em;height:13.58em;"/></p>
<p>The preceding graph can be represented through the following adjacency matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-491 image-border" src="assets/dcd3ca6f-e442-4d4e-bebb-d73792d20ad0.png" style="width:6.67em;height:9.50em;"/></p>
<p>As anticipated, the matrix is symmetric with respect to the main diagonal being undirected. If instead of the 1 in the matrix, there are numbers; these are to be interpreted as the weight attributed to each connection (edge). Here, the matrix is called Markov's matrix, as it is applicable to a Markov process. For example, if the set of vertices of the graph represents a series of points on a map, the weight of the edges can be interpreted as the distance of the points that they connect.</p>
<p>One of the fundamental characteristics of this matrix is that it obtains the number of paths from a node <em>i</em> to a node <em>j</em>, which must cross <em>n</em> vertices. To obtain all of this, it is sufficient to make the <em>n</em> power of the matrix and see the number that appears in place <em>i, j</em>. Another way of representing graphs is using adjacency lists. Let's see how.¬†</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adjacency list</h1>
                </header>
            
            <article>
                
<p><strong>Adjacency lists</strong> are a mode of graph representation in memory. This is probably the simplest representation to implement, although, in general, it is not the most efficient in terms of the space that's occupied.</p>
<p>Let's analyze a simple graph; next to each vertex is its list of adjacencies. The idea of representation is simply that every vertex <em>Vi</em> is associated with a list containing all of the vertices <em>Vj</em> so that there is the edge from <em>Vi</em> to <em>Vj</em>.</p>
<p>Assuming that you memorize all of the pairs of the type (<em>Vi, L</em>), where <em>L</em> is the adjacency list of the <em>Vi</em> vertex, we obtain a unique description of the graph. Alternatively, if you decide to sort adjacency lists, you do not need to explicitly store the vertexes as well.</p>
<p>Let's take an example‚Äîwe will use the same graph adopted in the previous section, which is represented in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-492 image-border" src="assets/30f10bb6-661c-4f8c-ab4a-031e07f376c2.png" style="width:39.75em;height:14.08em;"/></p>
<p>From this, we will build the list of adjacencies according to what has been said so far. The graph¬†in the previous diagram can be represented as follows:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>adjacent to</p>
</td>
<td>
<p>2,3</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p>adjacent to</p>
</td>
<td>
<p>1,3</p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p>adjacent to</p>
</td>
<td>
<p>1,2,4</p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p>adjacent to</p>
</td>
<td>
<p>3</p>
</td>
</tr>
</tbody>
</table>
<p>An adjacency list is made up of pairs. There is a pair for each vertex in the graph. The first element of the pair is the vertex that is being analyzed, and the second is the set formed by all of the vertices adjacent to it, which is connected to it by one side.</p>
<p>Assuming that we have a graph with <em>n</em> vertices and <em>m</em> edges (directed) that unite them, and supposing the adjacency lists are<span>¬†memorized¬†</span><span>in the order (so as not to explicitly memorize the indices), we will have each edge appear in one and one list of adjacencies, and it appears as the number of the vertex to which it points. Due to this, it's necessary to memorize a total of <em>m</em> numbers less than or equal to <em>n</em>, for a total cost of</span> <em>mlog2n</em><span>.</span></p>
<p>There is no obvious way to optimize this representation for non-oriented graphs; each arc must be memorized in the adjacency lists of both vertices that it connects, hence halving the efficiency. The same argument holds if the graph is oriented, but we need an efficient method to know the arcs entering a certain vertex. In this case, it is convenient to associate two lists¬†<span>to each vertex</span>: that of the incoming arcs and that of the outgoing arcs.</p>
<p>As far as time efficiency is concerned, representation by adjacency lists behaves quite well both in access and in insertion, carrying out the main operations in time <em>O(n)</em>. So far, we have analyzed the graphic representation techniques. Now, let's learn how to use them in the R environment.¬†</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling graphs in R</h1>
                </header>
            
            <article>
                
<p>In R, the set of nodes (<em>V</em>) and the set of arcs (<em>E</em>) are data structures of different types. For <em>V</em>, once we assign a unique identifier to¬†<span>each node,¬†</span>then we can access every node without ambiguity. Hence, it is like saying that the data structure hosting the properties of the nodes is one-dimensional and, therefore, is a vector.</p>
<p>On the contrary, the data structure for the set of arcs (links between nodes) <em>E</em> cannot be a vector and¬†it does not express the characteristics of single objects but expresses relations between pairs of objects (pairs of nodes in this case). So if, for example, in <em>V</em> (the set of nodes) there are 10 nodes, then the dimensions of <em>E</em> will be 10 √ó 10 or all of the relationships between all of the possible pairs of nodes. Ultimately, <em>E</em> has not one but two dimensions and therefore it is not a vector but a matrix.</p>
<p>In the matrix <em>E</em>, we have several rows equal to the number of nodes present in <em>V</em> and several columns equal to the number of nodes present in <em>V</em>. This represents the adjacency matrix analyzed in detail in the <em>Adjacency matrix</em> section.</p>
<p>To address the graph in R, we can use the <kbd>igraph</kbd> package‚Äîthis package contains functions for simple graphs and network analysis. It can handle large graphs very well and provides functions for generating random and regular graphs, graph visualization, centrality methods, and much more.</p>
<p>The following table gives some information about this package:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Package</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><kbd>igraph</kbd></p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Date</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">2019-22-04</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Version</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">1.2.4.1</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Title</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Network Analysis and Visualization</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Maintainer</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">G√°bor Cs√°rdi</p>
</td>
</tr>
</tbody>
</table>
<p>¬†</p>
<p>To start using the available tools, we will analyze a simple example. Suppose we have a graph consisting of four nodes and <span>four</span> edges. The first thing to do is to define the links between the <span>four</span> nodes; to do this, we will use the <kbd>graph</kbd> function (remember to load the <kbd>igraph</kbd> library after installing it):</p>
<pre>library(igraph)<br/>Graph1 &lt;- graph(edges=c(1,2, 2,3, 3, 1, 3,4), n=4, directed=F)</pre>
<p>The <kbd>graph</kbd> function is part of the <kbd>graph.constructors</kbd> methods that offer various methods for creating graphs: empty graphs, graphs with the given edges, graphs from adjacency matrices, star graphs, lattices, rings, and trees. The method we used defines the graph by indicating edges using a numeric vector defining the edges as follows: the first edge from the first element to the second, the second edge from the third to the fourth, and so on.</p>
<p>In fact, we can see that four pairs of values have been passed: the first defines the connection between nodes 1 and 2, the second between nodes 2 and 3, the third between nodes 3 and 1, and finally, the fourth between nodes 4 and 2. To better understand the connections between the nodes of the graph, we will draw it:</p>
<pre>plot(Graph1)</pre>
<p class="mce-root"/>
<p>The following graph was plotted:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-493 image-border" src="assets/529c85a6-f9a7-4819-a753-17a7dab6a4b4.png" style="width:35.58em;height:34.83em;"/></p>
<p>The graph we created is an object with features that we can analyze as follows:</p>
<pre>Graph1</pre>
<p>The following results are returned:</p>
<pre><strong>IGRAPH 143ffd1 U--- 4 4 --</strong><br/><strong>+ edges from 143ffd1:</strong><br/><strong>[1] 1--2 2--3 1--3 3--4</strong></pre>
<p><span>The edges between the nodes are indicated.¬†</span>We then calculate the shortest path between node 1 and node 4:</p>
<pre>get.shortest.paths(Graph1, 1, 4)</pre>
<p><kbd>get.shortest.paths()¬†</kbd> calculates a single shortest path between the source vertex to the target vertices. This function uses a breadth-first search for unweighted graphs and Dijkstra's algorithm for weighted graphs. In our case, having added the weight attribute, the Dijkstra algorithm was used.</p>
<p>The following result is returned:</p>
<pre>$vpath<br/>$vpath[[1]]<br/><strong>+ 3/4 vertices, from 1b5d9f3:</strong><br/><strong>[1] 1 3 4</strong></pre>
<p>Now, calculate the distance between the two points for this path:</p>
<pre>distances(Graph1, 1, 4)</pre>
<p><span>The following result is returned:</span></p>
<pre><strong>[1,]    2</strong></pre>
<p>The graph we have represented so far has limited usefulness to identify the shortest path between two locations, which represents our goal. To calculate the best route, it is necessary to introduce the concept of edge weight. In our case, we can see this attribute as a measure of the length of the path between two nodes; in this way, we can evaluate the distance between two nodes through a path. To do this, we'll use the attribute weights as follows:</p>
<pre>WeightsGraph1&lt;- c(1,1,4,1)<br/>E(Graph1)$weight &lt;- WeightsGraph1</pre>
<p>First, we defined the weights with a vector, confirming the sequence of edges defined in the creation of the graph. So, we added the weight attribute to the previously created graph. Now, each edge has its own length. What happens if weights are not defined? Simply, they are all set equal to 1; in this case, the shortest path would be the one with the least number of nodes.</p>
<p>Now, let's calculate again the shortest path between node 1 and node 4:</p>
<pre>get.shortest.paths(Graph1, 1, 4)</pre>
<p class="mce-root"/>
<p><span>The following result is returned:</span></p>
<pre>$vpath<br/>$vpath[[1]]<br/><strong>+ 4/4 vertices, from 1b5d9f3:</strong><br/><strong>[1] 1 2 3 4</strong></pre>
<p>We can see that the path now involves multiple nodes. We will verify the distance between the two nodes:</p>
<pre>distances(Graph1, 1, 4)</pre>
<p><span>The following result is returned:</span></p>
<pre><strong>[1,]    3</strong></pre>
<p>In this way, we have verified that the indicated path is the shortest one since the longest connection has been avoided. In the next section, we see how it is possible to find the best route using Dijkstra's algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dijkstra's algorithm</h1>
                </header>
            
            <article>
                
<p>Dijkstra's algorithm is used to solve the problem of finding the shortest path from the source <em>s</em> to all of the nodes. The algorithm maintains a label <em>d(i)</em> to the nodes representing an upper bound on the length of the shortest path of the node <em>i</em>.</p>
<p>At each step, the algorithm partitions the nodes in <em>V</em> into two sets: the set of permanently labeled nodes and the set of nodes that are still temporarily labeled. The distance of permanently labeled nodes represents the shortest path distance from the source to these nodes, whereas the temporary labels contain a value that can be greater than or equal to the shortest path length.</p>
<p>The basic idea of the algorithm is to start from the source and try to permanently label the successor nodes. In the beginning, the algorithm places the value of the source distance to zero and initializes the other distances to an arbitrarily high value (by convention, we will set the initial value of the distances <em>d[i] = + ‚àû, ‚àÄi ‚àà V</em>). At each iteration, the node label <em>i</em> is the value of the minimum distance along a path from the source that contains, apart from <em>i</em>, only permanently labeled nodes. The algorithm selects the node whose label has the lowest value among those labeled temporarily, labels it permanently, and updates all of the labels of the nodes adjacent to it. The algorithm terminates when all of the nodes have been permanently labeled.</p>
<p class="mce-root"/>
<p>From the execution of this algorithm for each destination node <em>v</em> of <em>V</em>, <span>we obtain¬†</span>a shortest path <em>p</em> (from <em>s</em> to <em>v</em>) and we calculate the following:</p>
<ul>
<li><em>d [v]</em>:¬†Distance of node <em>v</em> from source node <em>s</em> long <em>p</em></li>
<li><em>œÄ [v]</em>: Predecessor of node <em>v</em> long <em>p</em></li>
</ul>
<p>For the initialization of¬†each node v of V, we will use the following procedure:</p>
<ul>
<li><em>d [v] = ‚àû if v ‚â† s</em>, otherwise <em>d [s] = 0</em></li>
<li><em>œÄ [v] = √ò</em></li>
</ul>
<p>During the execution, we use the relaxation technique of a generic edge <em>(u, v)</em> of <em>E</em>, which serves to improve the estimation of <em>d</em>.</p>
<p>The relaxation of an edge <em>(u, v)</em> of <em>E</em>, consists in evaluating whether, using <em>u</em> as a predecessor of <em>v</em>, the current value of distance <em>d [v]</em> can be improved and, in this case, they update <em>d [v]</em> and <em>œÄ [v]</em>. The procedure is as follows:</p>
<ol>
<li>If <em>d[v]&gt; d[u] + w (u, v)</em> then</li>
<li><em>d[v] = d[u] + w (u, v)</em></li>
<li><em>œÄ [v] = u</em></li>
</ol>
<p>The algorithm basically performs two operations: a node selection operation and an operation to update the distances. The first selects the node with the value of the lowest label <span>at each step</span>; the other verifies the condition <em>d[v]&gt; d[u] + w(u, v)</em> and, if so, updates the value of the label placing <em>d[v] = d[u] + w (u, v)</em>.</p>
<p>In the following section, we will implement a TD method to address a real-life application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing TD methods to the¬†vehicle routing problem¬†</h1>
                </header>
            
            <article>
                
<p>Given a weighted graph and a designated vertex <em>V</em>, it is often requested to find the path from a node to each of the other vertices in the graph. Identifying a path connecting two or more nodes of a graph is a problem that appears as a subproblem of many other problems of discrete optimization and has numerous applications in the real world.</p>
<p class="mce-root"/>
<p>Consider, for example, the problem of identifying a route between two locations shown on a road map, where the vertices are the localities, while the edges are the roads that connect them. In this case, each cost is associated with the length in kilometers of the road or the average time needed to cover it. If instead of any path, we want to identify one of the minimum total cost, then the resulting problem is known as the problem of the shortest path in a graph. In other words, the shortest path between two vertices of a graph is that path that connects these vertices and minimizes the sum of the costs associated with crossing each edge.</p>
<p>So, let's take a practical example‚Äîconsider a tourist visiting Italy by car who wants to reach Venice from Rome. Having a map of Italy available in which, for each direct link between the cities, its length<span>¬†</span><span>is marked</span><span>, how can the tourist find the shortest path?</span></p>
<p>The system can be schematized with a graph in which each city corresponds to a vertex, and the roads correspond to the connecting arcs between the vertices. You need to determine the shortest path between the source vertex and the target vertex of the graph.</p>
<p>A solution to the problem is to number all possible routes from Rome to Venice. For each route, calculate the total length and then select the shortest. This solution is not the most efficient because there are millions of paths to analyze.</p>
<p>In practice, we will model the map of Italy as a weighted oriented graph <em>G = (V, E)</em>, where each vertex represents a city, each edge <em>(u, v)</em> represents a direct path from <em>u</em> to <em>v</em> and each weight <em>w (u, v)</em> corresponding to an edge <em>(u, v)</em> represents the distance between <em>u</em> and <em>v</em>. So, the problem to be solved is that of finding the shortest path that connects the vertex corresponding to Rome with that corresponding to Venice.</p>
<p>Given a weighted directed graph <em>G = (V, E)</em>, the weight of a path <em>p = (v0, v1, ..., vk)</em> is given by the sum of the weights of the edges that constitute it, as shown in the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9e8dcd20-9e19-4a85-a2e6-2af524fe1dd3.png" style="width:14.42em;height:4.75em;"/></p>
<p>The shortest path from node <em>u</em> to node <em>v</em> of <em>V</em> is a path <em>p = (u, v1, v2, ..., v)</em> so that <em>w(p)</em> is minimal, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4cc61d83-cabb-4482-b856-f6fff9032a19.png" style="width:10.25em;height:3.92em;"/></p>
<p class="mce-root"/>
<p>The cost of the minimum path from <em>u</em> to <em>v</em> is denoted by <em>Œ¥(u, v)</em>. If there is no path from <em>u</em> to <em>v</em> then <em>Œ¥ (u, v) = ‚àû</em>.</p>
<p>Given a connected weighted graph <em>G = (V, E)</em> and a source node <em>s</em> of <em>V</em>, there are several algorithms to find a shortest path from <em>s</em> toward each other node of <em>V</em>. In the previous section, we analyzed the Dijkstra algorithm, now the time has come to tackle the problem using algorithms based on reinforcement learning.</p>
<p>As anticipated at the beginning of this chapter, the Vehicle Routing Problem (VRP) is a typical distribution and transport problem, which consists of optimizing the use of a set of vehicles with limited capacity to pick up and deliver goods or people to geographically distributed stations. Managing these operations in the best possible way can significantly reduce costs. Before tackling the problem with Python code, let's analyze the basic characteristics of the topic to understand possible solutions.</p>
<p>Based on what has been said so far, it is clear that a problem of this type is configured as a path optimization procedure that can be conveniently dealt with using graph theory.</p>
<p>Suppose we have the following graph with the distances between vertices indicated on the edges:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-494 image-border" src="assets/cec69904-6c1f-479c-8e23-f1f49bcb12c0.png" style="width:31.33em;height:20.92em;"/></p>
<p class="mce-root"/>
<p>It is easy to see that the shortest path from 1 to 6 is¬†1 ‚Äì 2 ‚Äì 5 ‚Äì 4 - 6.</p>
<p>In the <em>Understanding TD methods</em> section, we have seen that the method of choosing an action diversifies the types of algorithms based on the TD. In on-policy based methods (SARSA), the update is carried out based on the results of the actions determined by the selected policy, while in the off-policy methods (Q-learning), the policies are evaluated through hypothetical actions, not actually undertaken. We will address the problem just introduced through both approaches, highlighting the merits and defects of the solutions obtained.¬†So, let's see how to deal with the problem of vehicle routing using Q-learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Q-learning approach</h1>
                </header>
            
            <article>
                
<p>As we said in the¬†<em>Q-learning</em> section, Q-learning tries to maximize the value of the Q function (action-value function), which represents the maximum discounted future reward when we perform actions <em>a</em> in the state <em>s</em>.</p>
<p>The following block is an implementation of R code that allows us to research this path through the technique of Q-learning:</p>
<pre>N &lt;- 1000<br/>gamma &lt;- 0.9<br/>alpha &lt;- 1<br/>FinalState &lt;- 6<br/><br/>RMatrix &lt;- matrix(c(-1,50,1,-1,-1,-1,<br/>                      -1,-1,-1,1,50,-1,<br/>                      -1,-1,-1,1,-1,-1,<br/>                      -1,-1,-1,-1,-1,100,<br/>                      -1,-1,-1,50,-1,-1,<br/>                      -1,-1,-1,-1,-1,100),nrow=6,byrow = TRUE)<br/><br/>print(RMatrix)<br/><br/>QMatrix &lt;- matrix(rep(0,length(RMatrix)), nrow=nrow(RMatrix))<br/><br/>for (i in 1:N) {<br/>  CurrentState &lt;- sample(1:nrow(RMatrix), 1)<br/> repeat {<br/>    AllNS &lt;- which(RMatrix[CurrentState,] &gt; -1)<br/>    if (length(AllNS)==1)<br/>      NextState &lt;- AllNS<br/>    else<br/>      NextState &lt;- sample(AllNS,1)<br/>    QMatrix[CurrentState,NextState] &lt;- QMatrix[CurrentState,NextState] + alpha*(RMatrix[CurrentState,NextState] + gamma*max(QMatrix[NextState, which(RMatrix[NextState,] &gt; -1)]) - QMatrix[CurrentState,NextState])<br/><br/>    if (NextState == FinalState) break<br/>    CurrentState &lt;- NextState<br/>  }<br/>}<br/><br/>print(QMatrix)</pre>
<p>We will analyze the code line by line, starting from the setting of the following parameters:</p>
<pre>N &lt;- 1000<br/>gamma &lt;- 0.9<br/>alpha &lt;- 1<br/>FinalState &lt;- 6</pre>
<p>Here, we have the following:</p>
<ul>
<li><kbd>N</kbd>: the number of episodes to iterate</li>
<li><kbd>gamma</kbd>: the discount factor</li>
<li><kbd>alpha</kbd>: the learning rate</li>
<li><kbd>FinalState</kbd>: the target node</li>
</ul>
<p>Let's move on the reward matrix setting:</p>
<pre>RMatrix &lt;- matrix(c(-1,50,1,-1,-1,-1,<br/>                      -1,-1,-1,1,50,-1,<br/>                      -1,-1,-1,1,-1,-1,<br/>                      -1,-1,-1,-1,-1,100,<br/>                      -1,-1,-1,50,-1,-1,<br/>                      -1,-1,-1,-1,-1,100),nrow=6,byrow = TRUE)</pre>
<p>We see the matrix as it appears:</p>
<pre>print(RMatrix)</pre>
<p>The following matrix is printed:</p>
<pre><strong>     [,1] [,2] [,3] [,4] [,5] [,6]</strong><br/><strong>[1,]   -1   50    1   -1   -1   -1</strong><br/><strong>[2,]   -1   -1   -1    1   50   -1</strong><br/><strong>[3,]   -1   -1   -1    1   -1   -1</strong><br/><strong>[4,]   -1   -1   -1   -1   -1  100</strong><br/><strong>[5,]   -1   -1   -1   50   -1   -1</strong><br/><strong>[6,]   -1   -1   -1   -1   -1  100</strong></pre>
<p>Let's try to understand how we set this matrix. It's all very simple: we have associated a high reward at the most convenient edges, those with a lower weight (which means shorter). We then associated the highest reward (100) to the edge that leads us to the goal. Finally, we associated a negative reward to non-existent links. The following chart shows how we set the rewards:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-495 image-border" src="assets/637421dd-564f-49df-8343-4d268a577cb8.png" style="width:35.67em;height:23.75em;"/></p>
<p>We have simply replaced the weight of the edges with the rewards set to the value of the lengths. Edges with longer lengths return a low reward, edges with smaller lengths return a high reward. Finally, the maximum reward is achieved when the target is reached.</p>
<p>As we said in the <em>Q-learning</em> section, our goal is to estimate an evaluation function that evaluates the convenience of a policy based on the sum of the rewards. The Q-learning algorithm tries to maximize the value of the Q function (action-value function), which represents the maximum discounted future reward when we perform actions <em>a</em> in the state <em>s</em>.</p>
<p>Let's analyze again the procedure that we have to implement using R:</p>
<pre>Initialize<br/>   arbitrary action-value function<br/>Repeat (for each episode)<br/>   Initialize s<br/>   choose a from s using policy from action-value function<br/>   Repeat (for each step in episode)<br/>      take action a<br/>      observe r, s'<br/>      update action-value function<br/>      update s</pre>
<p>At each step, the agent observes the current state of the environment and using the œÄ policy selects and executes the action. By executing the action, the agent obtains the reward ùëÖùë° <em>+ 1</em> and the new state ùëÜùë° <em>+ 1</em>. At this point, the agent can calculate ùëÑ (<em>s</em>ùë°, <em>a</em>ùë°) updating the estimate.</p>
<p>Therefore, the Q function represents the essential element of the procedure; it is a matrix of the same dimensions as the rewards matrix. First, let's initialize it with all zeros:</p>
<pre>  QMatrix &lt;- matrix(rep(0,length(RMatrix)), nrow=nrow(RMatrix))</pre>
<p>At this point, we have to set up a cycle that will repeat the operations for each episode:</p>
<pre>for (i in 1:N) {</pre>
<p>The initial part of the cycle is used to set the initial state and the initial policy; in our case, we will choose an initial state randomly:</p>
<pre>  CurrentState &lt;- sample(1:nrow(RMatrix), 1)</pre>
<p>¬†After setting the initial state, we must insert a cycle that will be repeated until the final state, that is, our target, has been reached:</p>
<pre>  repeat {</pre>
<p>Now we must choose the next state according to the possible actions available in the current state. To move to the next node, what actions can we take? If only one possible action is available, we will choose that one. Otherwise, we will choose one at random, only to then analyze the others:</p>
<pre>    AllNS &lt;- which(RMatrix[CurrentState,] &gt; -1)<br/>    if (length(AllNS)==1)<br/>      NextState &lt;- AllNS<br/>    else<br/>      NextState &lt;- sample(AllNS,1)</pre>
<p>¬†¬†¬† Based on the results obtained, we can update the action-value function (<kbd>QMatrix</kbd>):</p>
<pre>    QMatrix[CurrentState,NextState] &lt;- QMatrix[CurrentState,NextState] + alpha*(RMatrix[CurrentState,NextState] + gamma*max(QMatrix[NextState, which(RMatrix[NextState,] &gt; -1)]) - QMatrix[CurrentState,NextState])</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The formula used for updating the Q function is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/49844e1b-bcec-4e0b-8109-ca1c62ad23b7.png" style="width:35.00em;height:1.58em;"/></p>
<p>Now, we will check the status achieved: if we have reached our target, then we will exit the cycle with the break command; otherwise, we will set the next status as the current one (<kbd>NextState</kbd>):</p>
<pre>    if (NextState == FinalState) break<br/>    CurrentState &lt;- NextState<br/>  }<br/>}</pre>
<p>Once the procedure is finished, we will print the Q matrix:</p>
<pre>print(QMatrix)</pre>
<p>The following result is returned:</p>
<pre><strong>     [,1]  [,2]  [,3] [,4] [,5] [,6]</strong><br/><strong>[1,]    0 864.5 811.9    0    0    0</strong><br/><strong>[2,]    0   0.0   0.0  901  905    0</strong><br/><strong>[3,]    0   0.0   0.0  901    0    0</strong><br/><strong>[4,]    0   0.0   0.0    0    0 1000</strong><br/><strong>[5,]    0   0.0   0.0  950    0    0</strong><br/><strong>[6,]    0   0.0   0.0    0    0 1000</strong></pre>
<p>Let's try to understand what this matrix tells us. To start, we can say that this matrix allows us to calculate the shortest path starting from any state, therefore, not necessarily from node 1. In our case, we will start from node 1 in order to confirm what was obtained visually. Recall that each row of the matrix represents a state and each value in the column tells us what the reward is in the transition to the state marked by the column index.</p>
<p>In the following flow path, we have the following:</p>
<ol>
<li>Starting from the first line, we see that the maximum value is in correspondence of the second column, so the best path takes us from state 1 to 2.</li>
<li>We then pass to the state 2 identified by the second row; here, we see that the greatest value of reward is in correspondence of the fifth column, therefore, the best path takes us from state 2 to 5.</li>
<li>Let's then move on to state 5 identified by the fifth line. Here, we see that the greatest value of reward is in correspondence with the fourth column, therefore, the best path takes us from state 5 to 4.</li>
</ol>
<p>¬†</p>
<ol start="4">
<li>Finally, we pass to the state 4 identified by the fourth line. Here, we see that the greatest value of reward is in correspondence with the sixth column, therefore, the best path leads us from state 4 to 6.</li>
</ol>
<p>We have reached the target and by doing so we have traced the path shorter from node 1 to 6, which is the following:</p>
<p>1 ‚Äì 2 - 5 ‚Äì 4 ‚Äì 6</p>
<p>This path coincides with the one obtained visually at the beginning of the section. The procedure for extracting the shortest path of the <kbd>QMatrix</kbd> matrix can be easily automated as follows:</p>
<pre>RowMaxPos&lt;-apply(QMatrix, 1, which.max)<br/>ShPath &lt;- list(1)<br/>i=1<br/>while (i!=6) {<br/> IndRow&lt;- RowMaxPos[i]<br/> ShPath&lt;-append(ShPath,IndRow)<br/> i= RowMaxPos[i]<br/>}<br/><br/>print(ShPath)</pre>
<p>The following results are returned:</p>
<pre><strong>[[1]]</strong><br/><strong>[1] 1</strong><br/><strong>[[2]]</strong><br/><strong>[1] 2</strong><br/><strong>[[3]]</strong><br/><strong>[1] 5</strong><br/><strong>[[4]]</strong><br/><strong>[1] 4</strong><br/><strong>[[5]]</strong><br/><strong>[1] 6</strong></pre>
<p>As we can see, the same result has been returned.¬†Now, let's see what happens if we try to tackle the same problem but with a different approach.¬†</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The SARSA approach</h1>
                </header>
            
            <article>
                
<p>As we anticipated in SARSA, starting from the current state <em>St</em> an action <em>At</em> is taken and the agent gets a reward R. In this way, the agent is transferred to the next state <em>St + 1</em> and takes an action <em>At + 1</em> in <em>St + 1</em>. In fact, SARSA is the acronym of the tuple (<em>S, A</em>, <em>R</em>, <em>St + 1</em>, <em>At + 1</em>).</p>
<p>The following is the whole code for the SARSA approach:</p>
<pre>N &lt;- 1000<br/>gamma &lt;- 0.9<br/>alpha &lt;- 1<br/>FinalState &lt;- 6<br/><br/>RMatrix &lt;- matrix(c(-1,50,1,-1,-1,-1,<br/>                      -1,-1,-1,1,50,-1,<br/>                      -1,-1,-1,1,-1,-1,<br/>                      -1,-1,-1,-1,-1,100,<br/>                      -1,-1,-1,50,-1,-1,<br/>                      -1,-1,-1,-1,-1,100),nrow=6,byrow = TRUE)<br/><br/>print(RMatrix)<br/><br/>QMatrix &lt;- matrix(rep(0,length(RMatrix)), nrow=nrow(RMatrix))<br/><br/>for (i in 1:N) {<br/>  CurrentState &lt;- sample(1:nrow(RMatrix), 1)<br/> repeat {<br/>    AllNS &lt;- which(RMatrix[CurrentState,] &gt; -1)<br/>    if (length(AllNS)==1)<br/>      NextState &lt;- AllNS<br/>    else<br/>      NextState &lt;- sample(AllNS,1)<br/><br/>    AllNA &lt;- which(RMatrix[NextState,] &gt; -1)<br/>    if (length(AllNA)==1)<br/>      NextAction &lt;- AllNA<br/>    else<br/>      NextAction &lt;- sample(AllNA,1)<br/><br/>    QMatrix[CurrentState,NextState] &lt;- QMatrix[CurrentState,NextState] + alpha*(RMatrix[CurrentState,NextState] + gamma*QMatrix[NextState,NextAction]  - QMatrix[CurrentState,NextState])<br/><br/>    if (NextState == FinalState) break<br/>    CurrentState &lt;- NextState<br/>  }<br/>}<br/><br/>print(QMatrix)<br/><br/>RowMaxPos&lt;-apply(QMatrix, 1, which.max)<br/>ShPath &lt;- list(1)<br/>i=1<br/>while (i!=6) {<br/> IndRow&lt;- RowMaxPos[i]<br/> ShPath&lt;-append(ShPath,IndRow)<br/> i= RowMaxPos[i]<br/>}<br/><br/>print(ShPath)</pre>
<p>As you can see, much of the code is similar to the previous case (Q-learning), as there are many similarities between the two approaches. We will only analyze the differences between the two approaches. In the first part of the code, the initial parameters are set and the rewards matrix is defined:</p>
<pre>N &lt;- 1000<br/>gamma &lt;- 0.9<br/>alpha &lt;- 1<br/>FinalState &lt;- 6<br/><br/>RMatrix &lt;- matrix(c(-1,50,1,-1,-1,-1,<br/>                      -1,-1,-1,1,50,-1,<br/>                      -1,-1,-1,1,-1,-1,<br/>                      -1,-1,-1,-1,-1,100,<br/>                      -1,-1,-1,50,-1,-1,<br/>                      -1,-1,-1,-1,-1,100),nrow=6,byrow = TRUE)<br/><br/>print(RMatrix)</pre>
<p>Now, let's move on to initialize the matrix Q and set the cycle that will allow us to update the action-value function:</p>
<pre>QMatrix &lt;- matrix(rep(0,length(RMatrix)), nrow=nrow(RMatrix))<br/><br/>for (i in 1:N) {<br/>  CurrentState &lt;- sample(1:nrow(RMatrix), 1)<br/><br/> repeat {<br/>    AllNS &lt;- which(RMatrix[CurrentState,] &gt; -1)<br/>    if (length(AllNS)==1)<br/>      NextState &lt;- AllNS<br/>    else<br/>      NextState &lt;- sample(AllNS,1)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Up to this point, nothing has changed compared to the formulation analyzed in the previous example. But now there are essential changes. In the <em>SARSA</em> section, we saw the pseudo-code of the algorithm; for convenience, we repeat it here:</p>
<pre>Initialize<br/>   arbitrary action-value function<br/>Repeat (for each episode)<br/>   Initialize s<br/>   choose a from s using policy from action-value function<br/>   Repeat (for each step in episode)<br/>      take action a<br/>      observe r, s'<br/>      choose a' from s' using policy from action-value function<br/>      update action-value function<br/>      update s,a</pre>
<p>Comparing it with the one proposed in the previous section (the Q-learning approach), we can see that the substantial difference between the two methods lies in the formula used for updating the action-value function and in the calculation of the action to be followed in the next state. The next action we will follow in the next state is calculated as follows:</p>
<pre>AllNA &lt;- which(RMatrix[NextState,] &gt; -1)<br/>    if (length(AllNA)==1)<br/>      NextAction &lt;- AllNA<br/>    else<br/>      NextAction &lt;- sample(AllNA,1)</pre>
<p>The approach is used to evaluate the next state. The formula we will use will be the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d669691c-1ceb-4ff7-bd8e-1992a2684e0f.png" style="width:29.67em;height:1.33em;"/></p>
<p>This formula in code R becomes this:</p>
<pre>     QMatrix[CurrentState,NextState] &lt;- QMatrix[CurrentState,NextState] + alpha*(RMatrix[CurrentState,NextState] + gamma*QMatrix[NextState,NextAction]  - QMatrix[CurrentState,NextState])</pre>
<p>¬†¬†¬† The rest of the code is similar to the previous section:</p>
<pre>    if (NextState == FinalState) break<br/>    CurrentState &lt;- NextState<br/>  }<br/>}<br/><br/>print(QMatrix)<br/><br/>RowMaxPos&lt;-apply(QMatrix, 1, which.max)<br/>ShPath &lt;- list(1)<br/>i=1<br/>while (i!=6) {<br/> IndRow&lt;- RowMaxPos[i]<br/> ShPath&lt;-append(ShPath,IndRow)<br/> i= RowMaxPos[i]<br/>}<br/>print(ShPath)</pre>
<p>We can then analyze the results:</p>
<pre><strong>     [,1] [,2] [,3] [,4] [,5]     [,6]</strong><br/><strong>[1,]    0   50    1    0    0   0.0000</strong><br/><strong>[2,]    0    0    0    1   50   0.0000</strong><br/><strong>[3,]    0    0    0    1    0   0.0000</strong><br/><strong>[4,]    0    0    0    0    0 999.9999</strong><br/><strong>[5,]    0    0    0   50    0   0.0000</strong><br/><strong>[6,]    0    0    0    0    0 999.9999</strong></pre>
<p>The shortest path is as follows:</p>
<pre><strong>[[1]]</strong><br/><strong>[1] 1</strong><br/><strong>[[2]]</strong><br/><strong>[1] 2</strong><br/><strong>[[3]]</strong><br/><strong>[1] 5</strong><br/><strong>[[4]]</strong><br/><strong>[1] 4</strong><br/><strong>[[5]]</strong><br/><strong>[1] 6</strong></pre>
<p>The result is identical to the one obtained in the previous example. Let's understand how the two approaches are different from each other.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Differentiating SARSA and Q-learning</h1>
                </header>
            
            <article>
                
<p>From the algorithmic point of view, the substantial difference between the two approaches we analyzed in the previous sections lies in the two equations we used to update the action-value function. Let's compare them to understand them better:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3feba7be-e270-4ece-af07-033418ce16e7.png" style="width:31.50em;height:1.42em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ec0f3f35-3304-4f66-962f-8fcedf1701f1.png" style="width:33.25em;height:1.50em;"/></p>
<p class="mce-root"/>
<p>Q-learning calculates the difference between <em>Q (s, a)</em> and the maximum value of the action, while SARSA calculates the difference between <em>Q (s, a)</em> and the value of the next action. In doing this, you can highlight the following points:</p>
<ul>
<li>SARSA uses the policy used by the agent to generate experience in the environment (such as epsilon-greedy), in order to select an additional action <em>A t + 1</em>. Then, it uses¬†<em>Q (S t + 1, A t +1)</em> to discount the gamma factor as expected future returns in the calculation of the update target.</li>
<li>Q-learning does not use this policy to select an additional action <em>A t + 1</em>. Instead, it estimates the expected future returns in the update rule as <em>max Q (S t + 1, A)</em> for all actions.</li>
</ul>
<p>The two approaches converge to different solutions:</p>
<ul>
<li>SARSA converges to the optimal solution by following the same policy that was used to generate the experience. This will have elements of randomness to ensure convergence.</li>
<li>Q-learning converges into an optimal solution after generating experience and training by following a greedy policy.</li>
</ul>
<p>SARSA is advisable when we need to guarantee the agent's performance during the learning process. This is where, in the learning process,<span>¬†</span>we must guarantee a low number of errors that are expensive for the equipment we are using. Hence, we care about its performance during the learning process.</p>
<p>An algorithm like Q-learning is advisable in cases where we do not care about the agent's performance during the learning process and we only want the agent to learn about an optimal greedy policy that we will adopt at the end of the process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, TD learning algorithms were introduced. These algorithms are based on reducing the differences between the estimates that are made by the agent at different times. The SARSA algorithm implements an on-policy TD method, while Q-learning has off-policy characteristics.</p>
<p>Then, the basics of graph theory were addressed‚Äîthe adjacency matrix and adjacency list topics were covered. We have seen how to represent graphs in R using the <kbd>igraph</kbd> package. By doing this, we addressed the shortest path problem. We also analyzed the Dijkstra algorithm in R.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Finally, the vehicle routing problem was resolved using the Q-learning and SARSA algorithms. The differences between the two approaches to solve the problem were analyzed in detail.</p>
<p>In the next chapter, we will about learn the fundamental concepts of game theory. We will learn how to install and configure the OpenAI Gym library and understand how it works. We will learn about the difference between the Q-learning and SARSA algorithms and understand how to make a learning and a testing phase. Finally, we will learn how to develop OpenAI Gym applications using R.</p>


            </article>

            
        </section>
    </body></html>