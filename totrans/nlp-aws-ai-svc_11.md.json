["```py\n    bucket = '<your s3 bucket name>'\n    ```", "```py\n    filename = \"sample_financial_news_doc.pdf\"\n    s3_client.upload_file(filename, bucket, filename)\n    ```", "```py\n    jobId = startJob(bucket, filename)\n    print(\"Started job with id: {}\".format(jobId))\n    if(isJobComplete(jobId)):\n        response = getJobResults(jobId)\n    ```", "```py\n    text_filename = 'sample_finance_data.txt'\n    doc = Document(response)\n    with open(text_filename, 'w', encoding='utf-8') as f:\n        for page in doc.pages:\n            page_string = ''\n            for line in page.lines:\n                #print((line.text))\n                page_string += str(line.text)\n            #print(page_string)\n            f.writelines(page_string + \"\\n\")\n    ```", "```py\n    job_data_access_role = 'arn:aws:iam::<your account number>:role/service-role/AmazonComprehendServiceRole-test-events-role'\n    ```", "```py\n    input_data_format = 'ONE_DOC_PER_LINE'\n    job_uuid = uuid.uuid1()\n    job_name = f\"events-job-{job_uuid}\"\n    event_types = [\"BANKRUPTCY\", \"EMPLOYMENT\", \"CORPORATE_ACQUISITION\", \n                   \"INVESTMENT_GENERAL\", \"CORPORATE_MERGER\", \"IPO\",\n                   \"RIGHTS_ISSUE\", \"SECONDARY_OFFERING\", \"SHELF_OFFERING\",\n                   \"TENDER_OFFERING\", \"STOCK_SPLIT\"]\n    ```", "```py\n    response = comprehend_client.start_events_detection_job(\n        InputDataConfig={'S3Uri': input_data_s3_path,\n                         'InputFormat': input_data_format},\n        OutputDataConfig={'S3Uri': output_data_s3_path},\n        DataAccessRoleArn=job_data_access_role,\n        JobName=job_name,\n        LanguageCode='en',\n        TargetEventTypes=event_types\n    )\n    events_job_id = response['JobId']\n    ```", "```py\n    events_job_id =\"<Job ID>\"\n    ```", "```py\n    job = comprehend_client.describe_events_detection_job(JobId=events_job_id)\n    waited = 0\n    timeout_minutes = 30\n    while job['EventsDetectionJobProperties']['JobStatus'] != 'COMPLETED':\n        sleep(60)\n        waited += 60\n        assert waited//60 < timeout_minutes, \"Job timed out after %d seconds.\" % waited\n        job = comprehend_client.describe_events_detection_job(JobId=events_job_id)\n    ```", "```py\n    output_data_s3_file = job['EventsDetectionJobProperties']['OutputDataConfig']['S3Uri'] + text_filename + '.out'\n    results = []\n    with smart_open.open(output_data_s3_file) as fi:\n        results.extend([json.loads(line) for line in fi.readlines() if line])\n    ```", "```py\n    result = results[0]\n    result\n    ```", "```py\n    result['Events'][1]['Triggers']\n    ```", "```py\n    result['Events'][1]['Arguments']\n    ```", "```py\n    result['Entities'][5]['Mentions']\n    ```", "```py\n    entities = [\n        {'start': m['BeginOffset'], 'end': m['EndOffset'], 'label': m['Type']}\n        for e in result['Entities']\n        for m in e['Mentions']\n    ]\n    ```", "```py\n    triggers = [\n        {'start': t['BeginOffset'], 'end': t['EndOffset'], 'label': t['Type']}\n        for e in result['Events']\n        for t in e['Triggers']\n    ]\n    ```", "```py\n    spans = sorted(entities + triggers, key=lambda x: x['start'])\n    tags = [s['label'] for s in spans]\n    output = [{\"text\": raw_texts[0], \"ents\": spans, \"title\": None, \"settings\": {}}]\n    ```", "```py\n    displacy.render(output, style=\"ent\", options={\"colors\": color_map}, manual=True)\n    ```", "```py\n    entities_df = pd.DataFrame([\n        {\"EntityIndex\": i, **m}\n        for i, e in enumerate(result['Entities'])\n        for m in e['Mentions']\n    ])\n    ```", "```py\n    events_df = pd.DataFrame([\n        {\"EventIndex\": i, **a, **t}\n        for i, e in enumerate(result['Events'])\n        for a in e['Arguments']\n        for t in e['Triggers']\n    ])\n    ```", "```py\n    events_df = events_df.merge(entities_df, on=\"EntityIndex\", suffixes=('Event', 'Entity'))\n    ```", "```py\n    def format_compact_events(x):\n        This code will take the most commonly occurring EventType and the set of triggers.\n        d = {\"EventType\": Counter(x['TypeEvent']).most_common()[0][0],\n             \"Triggers\": set(x['TextEvent'])}\n        This code will loop for each argument Role, collect the set of mentions in the group.\n        for role in x['Role']:\n            d.update({role: set((x[x['Role']==role]['TextEntity']))})\n        return d\n    ```", "```py\n    event_analysis_df = pd.DataFrame(\n        events_df.groupby(\"EventIndex\").apply(format_compact_events).tolist()\n    ).fillna('')\n    event_analysis_df\n    ```", "```py\n    def get_canonical_mention(mentions):\n        extents = enumerate([m['Text'] form in mentions])\n        longest_name = sorted(extents, key=lambda x: len(x[1]))\n        return [mentions[longest_name[-1][0]]]\n    ```", "```py\n    thr = 0.5\n    ```", "```py\n    trigger_nodes = [\n        (\"tr%d\" % i, t['Type'], t['Text'], t['Score'], \"trigger\")\n        for i, e in enumerate(result['Events'])\n        for t in e['Triggers'][:1]\n        if t['GroupScore'] > thr\n    ]\n    entity_nodes = [\n        (\"en%d\" % i, m['Type'], m['Text'], m['Score'], \"entity\")\n        for i, e in enumerate(result['Entities'])\n        for m in get_canonical_mention(e['Mentions'])\n        if m['GroupScore'] > thr\n    ]\n    ```", "```py\n    argument_edges = [\n        (\"tr%d\" % i, \"en%d\" % a['EntityIndex'], a['Role'], a['Score'])\n        for i, e in enumerate(result['Events'])\n        for a in e['Arguments']\n        if a['Score'] > thr\n    ```", "```py\n    G = nx.Graph()\n    ```", "```py\n    for mention_id, tag, extent, score, mtype in trigger_nodes + entity_nodes:\n        label = extent if mtype.startswith(\"entity\") else tag\n        G.add_node(mention_id, label=label, size=score*10, color=color_map[tag], tag=tag, group=mtype)\n    ```", "```py\n    for event_id, entity_id, role, score in argument_edges:\n        G.add_edges_from(\n            [(event_id, entity_id)],\n            label=role,\n            weight=score*100,\n            color=\"grey\"\n        )\n    ```", "```py\n    G.remove_nodes_from(list(nx.isolates(G)))\n    nt = Network(\"600px\", \"800px\", notebook=True, heading=\"\")\n    nt.from_nx(G)\n    nt.show(\"compact_nx.html\")\n    ```", "```py\nimport events_graph as evg\nevg.plot(result, node_types=['event', 'trigger', 'entity_group', 'entity'], thr=0.5)\n```"]