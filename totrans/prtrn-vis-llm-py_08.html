<html><head></head><body>
		<div id="_idContainer070">
			<h1 id="_idParaDest-107" class="chapter-number"><a id="_idTextAnchor127"/>8</h1>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor128"/>Large-Scale Training on SageMaker</h1>
			<p>In this chapter, we will cover the key features and functionality available with Amazon SageMaker to run highly optimized distributed training. You’ll learn how to optimize your script for SageMaker training, along with key usability features. You’ll also learn about backend optimizations for distributed training with SageMaker, such as GPU health checks, resilient training, checkpointing, and <span class="No-Break">script mode.</span></p>
			<p>We are going to cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Optimizing your script for <span class="No-Break">SageMaker training</span></li>
				<li>Top usability features for <span class="No-Break">SageMaker training</span></li>
			</ul>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor129"/>Optimizing your script for SageMaker training</h1>
			<p>So far in this book, you<a id="_idIndexMarker405"/> have learned quite a lot! We have covered everything from the foundations of pretraining to GPU optimization, picking the right use case, dataset and model preparation, parallelization basics, finding the right hyperparameters, and so on. The vast majority of this is that these are applicable in any compute environment you choose to apply them to. This chapter, however, is exclusively scoped to AWS and SageMaker especially. Why? So that you can master all the nuances included in at least one compute platform. Once you have learned how to become proficient in one compute platform, then you will be able to use that to work on any project you like! When, for various reasons, you need to transition onto another platform, you will at least have the basic concepts you need to know about to look for and consider <span class="No-Break">the transition.</span></p>
			<p>First, let us look at your scripts. The core of most SageMaker training scripts has at least <span class="No-Break">three things:</span></p>
			<ul>
				<li><span class="No-Break">Package imports</span></li>
				<li><span class="No-Break">Argument parsing</span></li>
				<li>Function definitions <span class="No-Break">and usage</span></li>
			</ul>
			<p>Let’s break<a id="_idIndexMarker406"/> these <span class="No-Break">down next.</span></p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor130"/>Importing packages</h2>
			<p>As we’ve covered <a id="_idIndexMarker407"/>previously, you can install and access really any package you need. You have many different ways to make these accessible within SageMaker training. At a minimum, when you define your job, you can bring a <strong class="source-inline">requirements.txt</strong> file with the packages defined. SageMaker will then use <strong class="source-inline">pip install</strong> to install these on your training compute for you, making <span class="No-Break">them available.</span></p>
			<p>Alternatively, you can build a base container with all of these pre-installed. This is certainly the fastest option, since it saves time during the training process. Rather than using <em class="italic">pip install</em>, you can use a pre-built image with all of the packages available. Another option is to import your own Python packages, sending your entire project to the SageMaker training environment. Then, you can import whatever code you’re <span class="No-Break">working on.</span></p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor131"/>Argument parsing</h2>
			<p>An extremely<a id="_idIndexMarker408"/> common package we use in the SageMaker training environment is <strong class="source-inline">argparse</strong>. If you’re not familiar with this, let me introduce you <span class="No-Break">to it.</span></p>
			<p>Once you’ve built a Python script, you might need to run it with different flags, settings, or arguments. Some of these might be with different hyperparameters, modes, or features that you want your script to run for you. The <strong class="source-inline">argparse</strong> package is a great way to do this in Python. First, in your script, you’ll need to <em class="italic">explicitly add a line of code for each argument you want to use</em>. In SageMaker, you might start with something <span class="No-Break">like this.</span></p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B18942_Figure_8.01.jpg" alt="Figure 8.1 – A basic arg parsing function"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – A basic arg parsing function</p>
			<p>As you can see in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em>, I’m simply importing <strong class="source-inline">argparse</strong>, creating the <strong class="source-inline">parser</strong> object, and then adding an argument called <strong class="source-inline">train_folder</strong>. This will default to looking up <em class="italic">my environment variable</em>, which, as you may remember, is how SageMaker training injects information into your environment. If you’re curious, you can step through the CloudWatch logs for any of your SageMaker training jobs to see a list of all of the available environment variables. These will include all of the metadata for your job, all of your hyperparameters, and <span class="No-Break">so on.</span></p>
			<p>In this brief <a id="_idIndexMarker409"/>example, I’m pointing to my <em class="italic">train channel</em>. I created this by pointing to S3, or an optional FSx for Lustre, when I created my training job. That’s my training data. First, I upload it to S3. Then, I point to it when I configure my job. SageMaker copies that onto my SageMaker training instance and loads it onto a local path. The local path is usually something like <strong class="source-inline">/opt/ml/input/data/train/</strong>. When you want to point to that local path on your training container, you call <strong class="source-inline">args.train_folder</strong>, or however you’ve defined it. To read the file, you can either list the name from the folder or pass the name as <span class="No-Break">another argument.</span></p>
			<p>My personal favorite way to keep my script clean and tidy is to wrap all of my <strong class="source-inline">arg</strong> parsing in a dedicated function. Then, this will neatly return the <strong class="source-inline">args</strong> object. Here’s the <span class="No-Break">full script.</span></p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B18942_Figure_8.02.jpg" alt="Figure 8.2 – Invoking the arg parsing function in your main script"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Invoking the arg parsing function in your main script</p>
			<p>Another common argument you might pass is <strong class="source-inline">model_dir</strong>. You can point this to the <strong class="source-inline">SM_MODEL_DIR</strong> SM environment variable. SageMaker will write your model from the training container to S3 after the job <span class="No-Break">is finished.</span></p>
			<p>You can add any other hyperparameter you want, using the <strong class="source-inline">hyperparameters</strong> argument in the job config. Then, you can use these in your scripts. I’ve built arguments to point to things<a id="_idIndexMarker410"/> such as my data index, how to run my scripts, a path to checkpoint my model, and countless other arguments you may need for <span class="No-Break">your project.</span></p>
			<h3>Functions definition and usage</h3>
			<p>At the risk of <a id="_idIndexMarker411"/>stating<a id="_idIndexMarker412"/> the obvious here, you can write whatever software you want to write. You can copy directly to and from any accessible data source, spawn other jobs, initiate other cloud resources, or use open source packages – the possibilities <span class="No-Break">are endless.</span></p>
			<h3>Invoke your script with mpi</h3>
			<p>When you’re using<a id="_idIndexMarker413"/> distributed training, SageMaker invokes your script with <strong class="source-inline">mpi</strong>. As you learned earlier, this is a core library useful to run distributed training. We’ll use <strong class="source-inline">mpirun</strong> or <strong class="source-inline">smddprun</strong> to invoke your script. As you can see, we’ll invoke your script with all of the <span class="No-Break">relevant parameters.</span></p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B18942_Figure_8.03.jpg" alt="Figure 8.3 – How SageMaker training invokes your script"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – How SageMaker training invokes your script</p>
			<p>This is from a very complex example, training tens of billions of parameters for GPT-2, but it shows you many of the available ways you can configure your distributed training cluster <span class="No-Break">on SageMaker.</span></p>
			<h4>Logging and CloudWatch</h4>
			<p>As you may be<a id="_idIndexMarker414"/> aware, you have many<a id="_idIndexMarker415"/> options<a id="_idIndexMarker416"/> for logging. print statements are a great way to debug, but as you grow, you may move to something more managed such as the <strong class="source-inline">logging</strong> package. Remember that all of these are sent to CloudWatch logs, so you can easily view and debug your scripts. Open up the training job view in the AWS console, scroll to the bottom, and click the <em class="italic">View logs</em>. This takes you to CloudWatch, giving you one log stream per node in your cluster, each called <strong class="source-inline">algo</strong>. Usually, the top log stream is the leader node, but all of the streams will try to connect to the leader, so just see which algo they are trying to connect to. The logs will start after your instances are online and the script has been invoked, so it may take a few minutes after the job starts to <span class="No-Break">see these.</span></p>
			<h4>Checkpointing</h4>
			<p>One last parameter <a id="_idIndexMarker417"/>to be aware of in your SageMaker training<a id="_idIndexMarker418"/> scripts is <strong class="bold">checkpointing</strong>. In SageMaker, this actually serves a different role than just the model path. The model path will be copied to S3 at the end of your training job, but your checkpoints <em class="italic">will be copied throughout</em>. This makes them a great candidate for in-job debugging, running TensorBoard <em class="italic">(2)</em>, and restarting from the <span class="No-Break">latest checkpoint.</span></p>
			<p>Implementing a restart from your checkpoint is an extremely efficient technique to learn and perfect. It’s not hard – just look in S3 for the right path, configure your job, and then make sure you’re looking in the right directory for your base model. For large-scale jobs, we recommend you checkpoint at least every 2–3 hours. This makes it easy for you to get through any hardware, software, networking, data, or other issues that will almost certainly arise throughout your <span class="No-Break">training process.</span></p>
			<p>For a detailed example of this, take a look at our GPT-2 training example at <em class="italic">(3)</em> in the <em class="italic">References</em> section. It <a id="_idIndexMarker419"/>implements a <strong class="source-inline">load_partial</strong> parameter that points to the S3 path you can provide <span class="No-Break">for checkpointing.</span></p>
			<h3>Configuring your job via the SageMaker estimator</h3>
			<p>While you do <a id="_idIndexMarker420"/>have multiple ways of running your SageMaker job, notably through the UI, the CLI, and <strong class="source-inline">boto3</strong>, probably the most popular way of doing this is through the <span class="No-Break">Python SDK.</span></p>
			<p>Here’s an example of what this might <span class="No-Break">look like:</span></p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B18942_Figure_8.04.jpg" alt="Figure 8.4 – Using the SageMaker estimator to run your remote training job"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Using the SageMaker estimator to run your remote training job</p>
			<p>Note that we’re<a id="_idIndexMarker421"/> pointing to a base image, actually through the <strong class="source-inline">PyTorch</strong> object. This points to a base AWS Deep Learning Container, defined by what framework version you specify. You can override this by pointing to <strong class="source-inline">image_uri</strong>, which will need to be a Docker container in Amazon ECS. In this estimator, you can also pass key parameters such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">instance_count</strong> and <strong class="source-inline">instance_type</strong> to configure your <span class="No-Break">training resource.</span></li>
				<li>Your entry point script and its source directory. This is where SageMaker will look for <strong class="source-inline">requirements.txt</strong> and your main script to execute both of <span class="No-Break">the files.</span></li>
				<li>Your hyperparameters – again, you define them based on what <span class="No-Break">you need.</span></li>
				<li>Your distribution<a id="_idIndexMarker422"/> parameters. We’ll cover them in the last section of <span class="No-Break">this chapter.</span></li>
			</ul>
			<p>Next, let’s take a look at some interesting usability features for <span class="No-Break">SageMaker training.</span></p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor132"/>Top usability features for SageMaker training</h1>
			<p>Now that you <a id="_idIndexMarker423"/>have some sense of how to integrate your scripts with SageMaker training, let’s learn about a few key aspects of SageMaker that make it especially easy and fun to <span class="No-Break">work with.</span></p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor133"/>Warm pools for rapid experimentation</h2>
			<p>Once your<a id="_idIndexMarker424"/> SageMaker job is online, it moves through the <span class="No-Break">following phases:</span></p>
			<ul>
				<li><span class="No-Break">Initializing resources</span></li>
				<li>Downloading <span class="No-Break">your data</span></li>
				<li>Downloading your <span class="No-Break">training image</span></li>
				<li>Invoking your <span class="No-Break">main script</span></li>
				<li>Uploading the model artifact to S3 <span class="No-Break">on completion</span></li>
			</ul>
			<p>You might be wondering, what happens if my job breaks and I need to update a few lines of code? Do I need to completely restart the entire cluster <span class="No-Break">from scratch?</span></p>
			<p>Fortunately for you, the answer is no! Definitely not. You can use managed warm pools. Just add one extra hyperparameter, <strong class="source-inline">keep_alive_period_in_seconds</strong>, and it’ll keep your job online even after your script either fails or finishes completely. This is useful because, in many cases, that upfront job initialization actually is the largest bottleneck in your flow. It can take anywhere from a few minutes for smaller CPU-based instances to as much as 8 minutes or more for larger GPU-based instances <span class="No-Break">to initialize.</span></p>
			<p>On the upside, that wait time for GPU instances is ultimately saving you money and time because we’re running GPU health checks on the backend to ensure you get only good news. On the downside, 8 minutes is a long time to wait between development iterations. This is particularly painful if you’re updating something embarrassingly simple, such as a basic <span class="No-Break">syntax error.</span></p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B18942_Figure_8.05.jpg" alt="Figure 8.5 – Viewing your training jobs in the console"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Viewing your training jobs in the console</p>
			<p>Managed warm <a id="_idIndexMarker425"/>pools solve this problem for you <span class="No-Break">as follows:</span></p>
			<ol>
				<li>First, add that hyperparameter to your <span class="No-Break">job configuration.</span></li>
				<li>Next, once the job finishes training, either successfully or with an error, the warm pool status should <span class="No-Break">show </span><span class="No-Break"><strong class="bold">Available</strong></span><span class="No-Break">.</span></li>
				<li>Afterward, when you submit another job with a matching image URI, instance type, and instance count, this will show the <strong class="bold">In Use</strong> status and then ultimately <strong class="bold">Reused</strong>, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></li>
			</ol>
			<p>While saving a few minutes through using managed warm pools may not seem like a huge gain, it truly adds up a scale. When you’re working up against a deadline and every hour of the day counts, using warm pools may be the difference between hitting your deadline and not. It means that in a single hour, you can update your script easily hundreds <a id="_idIndexMarker426"/>of times, whereas before you may only have been able to do this up to about 10 times in <span class="No-Break">an hour.</span></p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor134"/>SSM and SSH into training instances</h2>
			<p>Once your<a id="_idIndexMarker427"/> job is up and running <a id="_idIndexMarker428"/>successfully, especially a long-running job with lots of complex steps along the way, you can imagine how useful it would be to connect to the instance directly, view it, and run <span class="No-Break">debug commands.</span></p>
			<p>Fortunately, we have a solution for that – a group of our very own ML SAs built out a custom design pattern that helps you enable this in your own environments <em class="italic">(1)</em>. They listened closely to customers, iterated on requirements, and developed a very <span class="No-Break">nice project.</span></p>
			<p>You can follow the steps in the following repository to install this in your own SageMaker resources, allowing you to easily connect to running jobs and analyze them <span class="No-Break">in flight.</span></p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B18942_Figure_8.06.jpg" alt="Figure 8.6 – SSH in SageMaker training jobs"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – SSH in SageMaker training jobs</p>
			<p>From a systems architecture perspective, there are two key paths forward in evaluating this solution. On the one hand, you can use the fully-managed service, AWS Systems Manager. This is generally more secure than SSH but is a bit more limited in functionality. If all you need is to open up a terminal onto a remote instance, run some debug commands, and view the output in progress, this is probably the solution for you. Setting it up isn’t too hard; you just need to configure the IAM and SSM resources accordingly. When used in combination with warm pools, this is <span class="No-Break">really powerful!</span></p>
			<p>On the other hand, you can also use SSH directly. SSH is generally less secure than SSM. This is because SSM uses a managed AWS service, while SSH opens up the possibility that any malicious user could connect to the nodes using port forwarding. This means that in an enterprise environment, in many cases you’re better off starting with SSM. SSH will, however, let you update a local script and use port forwarding. This means if you want something to take your local script and send it to the remote training instance seamlessly, SSH is the way to go. However, given that you now have warm pools, it’s questionable <a id="_idIndexMarker429"/>whether you’d need <a id="_idIndexMarker430"/>this. The SSH solution is really nice if your IDE supports remote connection points, such as VS Code <span class="No-Break">or PyCharm.</span></p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor135"/>Track jobs and experiments to replicate results</h2>
			<p>One of my<a id="_idIndexMarker431"/> personal favorite features of SageMaker training is, honestly, its most basic – storing everything about your job and keeping it searchable by default! That’s called the <strong class="bold">metadata</strong>. All the <a id="_idIndexMarker432"/>hyperparameters, input data locations, images, variables, and other information about your job are stored every time you submit them. This means you can easily track your jobs over time, logging in  to view the CloudWatch logs, downloading the model from S3 whenever you need to, adding tags to specify other details, and <span class="No-Break">so on.</span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B18942_Figure_8.07.jpg" alt="Figure 8.7 – Viewing your training job metadata in the AWS console"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Viewing your training job metadata in the AWS console</p>
			<p>All of this data is in your account for the long haul, without your paying for any of it. You also use SageMaker Search to find jobs with the highest accuracy from a given S3 path, an instance <a id="_idIndexMarker433"/>type or count, a hyperparameter, or any available value. Just recently we’ve launched a few new features that make using SageMaker Training much easier. One of them is a hosted TensorBoard (https://aws.amazon.com/about-aws/whats-new/2023/04/amazon-sagemaker-hosted-tensorboard/) which lets you easily track and compare experiments. The second is a new @remote decorator that lets you transition local functions to remote jobs very <span class="No-Break">easily! (</span><a href="https://aws.amazon.com/blogs/machine-learning/run-your-local-machine-learning-code-as-amazon-sagemaker-training-jobs-with-minimal-code-changes/?sc_channel=sm&amp;sc_campaign=Machine_Learning&amp;sc_publisher=LINKEDIN&amp;sc_geo=GLOBAL&amp;sc_outcome=awareness&amp;sc_content=ml_services&amp;trk=machine_learning&amp;linkId=211795861"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/run-your-local-machine-learning-code-as-amazon-sagemaker-training-jobs-with-minimal-code-changes/?sc_channel=sm&amp;sc_campaign=Machine_Learning&amp;sc_publisher=LINKEDIN&amp;sc_geo=GLOBAL&amp;sc_outcome=awareness&amp;sc_content=ml_services&amp;trk=machine_learning&amp;linkId=211795861</span></a><span class="No-Break">)</span></p>
			<p>Now, let’s close out the chapter by learning about <span class="No-Break">backend optimizations!</span></p>
			<h3>Backend optimizations for distributed training with SageMaker</h3>
			<p>You’ve learned<a id="_idIndexMarker434"/> how to update your training scripts for SageMaker, and you’ve taken a closer look at some of the ways SageMaker is pretty fun and friendly to work with. Let’s finish by exploring ways that SageMaker optimizes the backend for large-scale <span class="No-Break">distributed training.</span></p>
			<p>As you have probably guessed, SageMaker can spin up anywhere from a few to a few thousand GPUs. This is due to the core service offering for training – the ability to turn on, orchestrate, and manage all of these GPUs on your behalf. You define this cluster when you define a training job, and as you learned earlier in this chapter, you use <em class="italic">mpi</em> to communicate between all of the nodes. You can store all of the hyperparameters and job metadata, stream all of your logs to CloudWatch, plug into your favorite operations tooling, ensure the nodes are healthy, connect to your data in S3, download and run your image, and so on. This <em class="italic">large-scale cluster orchestration</em> is completely elastic, easily flowing from one to hundreds <span class="No-Break">of instances.</span></p>
			<p>However, orchestrating this cluster is not especially useful, unless you have healthy GPUs. As you learned earlier in the book, writing software to successfully orchestrate all the tens of thousands of cores in a single GPU is no small task. Even when you have updated CUDA, drivers, and the latest deep learning frameworks, the bad news is that you may still get a bad GPU. Hardware fails, and GPU failures are incredibly common. As you scale up your training jobs to more GPUs, the odds of you getting a GPU failure even once in that massive pool of compute increases. This is why the <em class="italic">GPU health checks</em> that SageMaker brings to the table are incredibly useful! We can track down the latest GPU errors and have integrated checks for these in our job orchestrator. It means that when you get a node on SageMaker, it is much more likely to <span class="No-Break">be healthy.</span></p>
			<p>Even with extensive GPU health checks and large-scale job orchestration, it is still possible that your job will error out even before it starts. You might get something like an <em class="italic">insufficient capacity error</em>, indicating that there are simply not enough of your requested instance type in your requested region. You could also get an <em class="italic">internal service error</em>, unsurprisingly telling you that something went wrong at your end. For these and other cases, it is extremely <a id="_idIndexMarker435"/>useful to have <strong class="bold">resilient training</strong>. Adding this is simple – you just need to bring an extra parameter to your training job configuration. Set <strong class="source-inline">max_retry_attempts</strong> to your preference; personally, I just max it out at 30 every time I am running something with more than <span class="No-Break">8 instances.</span></p>
			<p>While this is useful to get a job successfully started, I also have customers who <em class="italic">implement another job restart</em>. This might come into play when stepping through your mini-batches during your training loop. While the <strong class="source-inline">bf16</strong> data type has proven extremely useful to improve the stability of large-scale distributed GPU training, it is still not uncommon to see the loss of your model spontaneously spike, plateau, or drop. You might also see <a id="_idIndexMarker436"/>your total job throughput unexpectedly change. If any of these things happen, it’s wise to trigger an emergency checkpoint, kill the job, and then start again from that same checkpoint and step number. A combination of a few extra functions in your training script and a Lambda function listening via the <strong class="source-inline">EventBridge</strong> would be a natural way to do this. For a recent summary of some best practices, take a look at the blog post <em class="italic">(4)</em> in the <span class="No-Break"><em class="italic">References</em></span><span class="No-Break"> section.</span></p>
			<h3>Distributed training libraries – a model and data parallel</h3>
			<p>As you’ve learned<a id="_idIndexMarker437"/> previously, AWS has optimizations for distributed training. These are extremely effective methods to scale up to hundreds and thousands of GPUs on SageMaker. Let’s take one more look at them <span class="No-Break">in detail.</span></p>
			<p>Remember that AlexNet only achieved groundbreaking results because it used multiple GPUs? Historically speaking, one of the earliest approaches to a multi-node deep learning process was<a id="_idIndexMarker438"/> called a <strong class="bold">parameter server</strong>. Parameter servers, as you can see in the following diagram, are a simple and effective way to orchestrate distributed gradient descent at scale. One node operates as the leader. It synchronizes gradients with the worker nodes, checking their health, and maintaining one globally consistent version of the model. Parameter servers can be somewhat slow, but they are actually more efficient in terms of the bandwidth they consume. Let’s explore <span class="No-Break">this visually.</span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B18942_Figure_8.08.jpg" alt="Figure 8.7 – Historical approaches to distributed gradient descent"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Historical approaches to distributed gradient descent</p>
			<p>This slowness, however, led to a slightly different approach. Ring-based topologies used the <strong class="source-inline">AllReduce</strong> algorithm under the hood to communicate between all nodes, collecting the average of the gradients and distributing the result to each of the nodes. This is the same basic approach common in Horovod and PyTorch DistributedDataParallel, popularized by their increase in speed over their <span class="No-Break">older cousin.</span></p>
			<p>However, <strong class="source-inline">AllReduce</strong> as a basic collective does <em class="italic">not</em> perform well at scale. Every additional node increases the bandwidth consumed during the <strong class="source-inline">AllReduce</strong> step. This means that your scaling efficiency gets worse as you add more instances, ultimately leading to poor utilization of your instances and, thus, your <span class="No-Break">compute budget.</span></p>
			<p>To counter this negative impact, AWS developed <em class="italic">custom collectives for data parallel</em>. These are the single best way to get the highest performance on the AWS Cloud. This was introduced<a id="_idIndexMarker439"/> as <strong class="bold">SageMaker Distributed Data Parallel</strong> (<strong class="bold">SMDDP</strong>) <em class="italic">(5)</em>, available as an SDK, in your container, and for any supported SageMaker job. Use SMDDP to ensure your large-scale GPU jobs are running as quickly and efficiently as possible, using them as the backend for any supported distributed software. SMDDP also integrates with Amazon’s Elastic Fabric Adapter, a low-jitter low-latency communication enhancement on AWS. Generally, SMDDP makes it easy for you to point to it from deep learning frameworks, setting it as your <span class="No-Break">distributed backend.</span></p>
			<p>Fortunately for you, as of the December 2022 release, this is now also available in the <em class="italic">model parallel</em> family. Now, you can set a <strong class="source-inline">ddp</strong> backend in the <strong class="source-inline">smp_options</strong> object, with <strong class="source-inline">ddp_dist_backend:auto</strong>. When this new backend option is combined with the <em class="italic">sharded data parallel</em> configuration<a id="_idIndexMarker440"/> we discussed in <a href="B18942_05.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, this gives you another <span class="No-Break">30% boost!</span></p>
			<p>Now, let’s close out the chapter with a <span class="No-Break">quick recap.</span></p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor136"/>Summary</h1>
			<p>In this chapter, we learned about the key features of Amazon SageMaker for large-scale distributed training. We looked at how to optimize your script, from importing packages to parsing arguments, writing code, invoking your script with <strong class="source-inline">mpi</strong>, writing to CloudWatch logs, checkpointing, working with the SM estimator, and so on. We covered key usability features to make SageMaker more fun and friendly to work with, such as warm pools for rapid experimentation, SSM and SSH in training instances, and tracking jobs. Finally, we learned about backend optimizations for distributed training, such as SMDDP collectives, using it both standalone and in combination with the model <span class="No-Break">parallel package.</span></p>
			<p>In the next chapter, we’ll explore even more advanced topics in <span class="No-Break">distributed training!</span></p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor137"/>References</h1>
			<p>Please go through the following content for more information on a few topics covered in <span class="No-Break">the chapter:</span></p>
			<ol>
				<li><span class="No-Break"><em class="italic">aws-sample/sagemaker-ssh-helper</em></span><span class="No-Break">: </span><a href="https://github.com/aws-samples/sagemaker-ssh-helper"><span class="No-Break">https://github.com/aws-samples/sagemaker-ssh-helper</span></a></li>
				<li><em class="italic">Use TensorBoard in Amazon SageMaker </em><span class="No-Break"><em class="italic">Studio</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tensorboard.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tensorboard.html</span></a></li>
				<li><span class="No-Break"><em class="italic">aws/amazon-sagemaker-examples</em></span><span class="No-Break">: </span><a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt2/train_gpt_simple.py"><span class="No-Break">https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt2/train_gpt_simple.py</span></a></li>
				<li><em class="italic">Training large language models on Amazon SageMaker: Best </em><span class="No-Break"><em class="italic">practices</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/blogs/machine-learning/training-large-language-models-on-amazon-sagemaker-best-practices/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/training-large-language-models-on-amazon-sagemaker-best-practices/</span></a></li>
				<li><em class="italic">Introduction to SageMaker’s Distributed Data Parallel </em><span class="No-Break"><em class="italic">Library</em></span><span class="No-Break">: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-intro.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-intro.html</span></a></li>
			</ol>
		</div>
	</body></html>