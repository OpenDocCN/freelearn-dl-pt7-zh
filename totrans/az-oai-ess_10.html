<html><head></head><body>
		<div id="_idContainer165">
			<h1 id="_idParaDest-110" class="chapter-number"><a id="_idTextAnchor109"/>10</h1>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor110"/>Creating a Multimodal Multi-Agent Framework with the Azure OpenAI Assistants API</h1>
			<p>In this chapter, we will explore how to build a system using the Azure OpenAI Assistants API that allows multiple intelligent agents to work together. These agents use advanced language models to understand and generate natural language, making them capable of performing tasks independently and making decisions on <span class="No-Break">their own.</span></p>
			<p>For example, imagine a team of AI agents collaborating to create and enhance images based on user input. One agent could generate an initial image, another agent could refine the details, and a third agent could adjust the colors and textures. This collaborative process results in high-quality and intricate outputs that a single agent might struggle to achieve alone. This example serves as a comprehensive guide for developers looking to harness the capabilities of multiple AI agents working in concert. It shows how agents can communicate and collaborate to process complex tasks, particularly focusing on generative <span class="No-Break">AI applications.</span></p>
			<p>Microsoft recently <a id="_idIndexMarker462"/>introduced <strong class="bold">Azure AI Agent Service</strong>, a fully managed platform designed to help developers efficiently build, deploy, and scale high-quality AI agents. This service eliminates the complexity of managing compute and storage resources, allowing developers to focus on creating intelligent and extensible AI solutions. Traditionally, implementing client-side function-calling required writing hundreds of lines of code. With <strong class="bold">Azure AI Agent Service</strong>, this process is now streamlined to just a few lines, significantly reducing development time and effort. Since Azure AI Agent Service follows the same wire protocol <a id="_idIndexMarker463"/>as <strong class="bold">Azure OpenAI Assistants</strong>, developers <a id="_idIndexMarker464"/>can seamlessly create and run agents using<a id="_idIndexMarker465"/> either <strong class="bold">OpenAI SDKs</strong> or <strong class="bold">Azure AI </strong><span class="No-Break"><strong class="bold">Foundry SDKs</strong></span><span class="No-Break">.</span></p>
			<p>While the service is <a id="_idIndexMarker466"/>optimized for <strong class="bold">single-agent</strong> implementations, orchestrating multiple agents<a id="_idIndexMarker467"/> as a <strong class="bold">multi-agent system</strong> requires additional tooling. For research and proof-of-concept projects, <strong class="bold">AutoGen</strong> is a good <a id="_idIndexMarker468"/>choice. However, for production-grade multi-agent systems, <strong class="bold">Semantic Kernel</strong> offers<a id="_idIndexMarker469"/> a better and more scalable <span class="No-Break">multi-agent framework.</span></p>
			<p>This chapter is especially relevant for developers and tech enthusiasts interested in exploring the frontiers of generative AI and multi-agent systems using the Azure OpenAI Assistants API. Specifically, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>The <span class="No-Break">problem statement</span></li>
				<li><span class="No-Break">Technical requirements</span></li>
				<li><span class="No-Break">Architecture diagram</span></li>
				<li>Creating a multimodal <span class="No-Break">multi-agent framework</span></li>
			</ul>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor111"/>Problem statement</h1>
			<p>As AI technology advances, we need smarter systems to handle complicated tasks in areas such as robotics, gaming, and self-driving cars. Single-agent systems often struggle with these complex problems because they can’t manage everything on their own. To solve this, we aim to create a system where multiple intelligent agents work together. Using the Azure OpenAI Assistants API, we’ll build a framework that combines the language skills of advanced AI models with the decision-making abilities of <span class="No-Break">smart agents.</span></p>
			<p>For example, in robotics, a team of robots could work together to clean up a large park. One robot might pick up trash, another might mow the grass, and a third might water the plants. These robots need to communicate and coordinate their tasks to ensure the park is cleaned efficiently <span class="No-Break">and thoroughly.</span></p>
			<p>Another example is in gaming, where multiple AI characters can collaborate to create more dynamic and challenging gameplay. One character could be a strategist, planning the next moves, while another could be a scout, gathering information about the environment. Similarly, in self-driving cars, multiple autonomous vehicles need to communicate to avoid collisions and optimize traffic flow. By working together, they can make better decisions than a single car acting on <span class="No-Break">its own.</span></p>
			<p>In this chapter, we will guide you through the process of building a multi-agent framework using the Azure OpenAI Assistants API. We will start with an overview of the key concepts and technologies involved, including intelligent agents and LLMs. Next, we will walk you through practical examples and case studies to demonstrate how to implement and integrate these agents. You will learn how to set up communication between agents, enable them to collaborate on tasks, and leverage the capabilities of LLMs for enhanced decision-making <span class="No-Break">and problem-solving.</span></p>
			<p>By the end of this chapter, you will understand how to create and use a multi-agent system that combines the strengths of language models and autonomous agents. This system will be capable of handling a wide range of tasks, showcasing its potential applications in various fields such as robotics, gaming, and <span class="No-Break">autonomous systems.</span></p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor112"/>Technical requirements</h1>
			<p>To follow along with the practical exercises in this chapter, access the source code available in this chapter’s GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Azure-OpenAI-Essentials/blob/main/Chapter10.ipynb"><span class="No-Break">https://github.com/PacktPublishing/Azure-OpenAI-Essentials/blob/main/Chapter10.ipynb</span></a><span class="No-Break">.</span></p>
			<p>You will need the following tools on your local machine to start working on <span class="No-Break">the solution:</span></p>
			<ul>
				<li>Python 3.9, 3.10, or <span class="No-Break">3.11: </span><a href="https://www.python.org/downloads/"><span class="No-Break">https://www.python.org/downloads/</span></a></li>
				<li>An Azure account: If you’re new to Azure, get an Azure account for free and you’ll get some free Azure credits to <span class="No-Break">get started</span></li>
				<li>An Azure subscription with access enabled for the Azure <span class="No-Break">OpenAI service</span></li>
				<li><strong class="bold">Azure OpenAI</strong> (<strong class="bold">AOAI</strong>) connection and model information: Create an Azure OpenAI using Sweden Central as <span class="No-Break">the region</span></li>
				<li>Assistants models <span class="No-Break">with GPT-4o</span></li>
				<li>DALL-E 3 model with <span class="No-Break">version 3.0</span></li>
			</ul>
			<p>In addition to the system requirements just mentioned, it is crucial to have a solid foundation in fundamental Azure services and a basic proficiency in the Python programming language, equivalent to a beginner level (Python 100). These skills are vital for efficiently harnessing and leveraging Azure services in the context of <span class="No-Break">this chapter.</span></p>
			<p>Rest assured that even if you are new to the Azure environment, we have designed this chapter to be beginner-friendly. It offers clear explanations and includes detailed screenshots to facilitate your learning and get you started on the <span class="No-Break">right track.</span></p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor113"/>Architecture design</h1>
			<p>We will <a id="_idIndexMarker470"/>be using a multi-agent system where the User Proxy Assistant acts as the main orchestrator, facilitating communication between the user and specialized agents. The DALL-E Assistant is<a id="_idIndexMarker471"/> responsible for generating images based on user queries, while the Vision Assistant analyzes these images using the GPT-4o model. The system maintains ongoing communication through persistent threads and utilizes function calls to perform specific tasks, such as image generation <span class="No-Break">and analysis.</span></p>
			<p>In a typical workflow, the user sends a query to the User Proxy Assistant, which forwards it to the DALL-E Assistant to create an image. This image is then analyzed by the Vision Assistant, which provides feedback that can be used to refine the image further. This coordinated effort between specialized agents ensures efficient handling of <span class="No-Break">complex tasks.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">This is just one way to implement multi-agent systems. You could also use other frameworks such as AutoGen, Semantic Kernel, CrewAI, <span class="No-Break">and LangGraph.</span></p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B21019_10_01.jpg" alt="Figure 10.1: Architecture diagram"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1: Architecture diagram</p>
			<p><span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> illustrates an example of the overall architectural framework. In this scenario, the User Proxy Assistant acts as the central coordinator, facilitating communication between the DALL-E Assistant and the <span class="No-Break">Vision Assistant.</span></p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor114"/>Creating a multimodal multi-agent framework</h1>
			<p>In this <a id="_idIndexMarker472"/>section, we’ll walk through a practical example of building a multi-agent framework using the AOAI Assistants API. We’ll focus on how these agents can interact and collaborate to handle complex tasks, such as generating and refining images through iterative improvements based on <span class="No-Break">user input.</span></p>
			<p>Each assistant is structured as illustrated in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.2</em>, showcasing their respective roles and interactions within <span class="No-Break">the system:</span></p>
			<ul>
				<li><strong class="bold">User Proxy Assistant</strong>: This<a id="_idIndexMarker473"/> assistant is initialized as the primary orchestrator, enabling seamless communication between the user and specialized agents in the group chat. It ensures messages are properly routed and maintains an ongoing conversation to guarantee that user tasks are <span class="No-Break">successfully executed.</span></li>
				<li><strong class="bold">DALL·E Assistant</strong>: This <a id="_idIndexMarker474"/>assistant is set up to handle image generation tasks. It connects with the DALL·E model to create visual content based on <span class="No-Break">user prompts.</span></li>
				<li><strong class="bold">Vision Assistant</strong>: This <a id="_idIndexMarker475"/>assistant is configured to perform image analysis. It utilizes the GPT-4 Vision model to process and interpret visual inputs, extracting meaningful insights for <span class="No-Break">the user.</span></li>
			</ul>
			<p>Together, these assistants form a cohesive system where the User Proxy Assistant coordinates interactions, the DALL·E Assistant generates images, and the Vision Assistant analyzes them, ensuring an efficient and responsive <span class="No-Break">AI-driven workflow.</span></p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B21019_10_02.jpg" alt="Figure 10.2: Assistant details"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2: Assistant details</p>
			<p>Before you begin, ensure that the Azure OpenAI service is set up and that both the DALL·E 3 and GPT-4o models are deployed, as illustrated in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B21019_10_03.jpg" alt="Figure 10.3: Model deployment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3: Model deployment</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Details for setting up the Azure account with an active subscription and the Azure OpenAI Service resource are provided in <a href="B21019_02.xhtml#_idTextAnchor023"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break"><em class="italic">.</em></span></p>
			<p>Now, set <a id="_idIndexMarker476"/>up all three Assistants using the <span class="No-Break">following configuration:</span></p>
			<ul>
				<li>First, set up the <strong class="source-inline">user_proxy</strong> agent using the <span class="No-Break">following code:</span></li>
			</ul>
			<pre class="source-code">
user_proxy = assistant_client.beta.assistants.create(
    name=name_pa,
    instructions=instructions_pa,
    model=assistant_deployment_name,
    tools=tools
)</pre>			<ul>
				<li>Next, set up the <strong class="source-inline">dalle_assistant</strong> agent using the <span class="No-Break">following code:</span></li>
			</ul>
			<pre class="source-code">
dalle_assistant = assistant_client.beta.assistants.create(
    name=name_dl,
    instructions=instructions_dl,
    model=assistant_deployment_name,
    tools=tools
)</pre>			<ul>
				<li>Finally, set<a id="_idIndexMarker477"/> up the <strong class="source-inline">vision_assistant</strong> agent using the <span class="No-Break">following code:</span></li>
			</ul>
			<pre class="source-code">
dalle_assistant = assistant_client.beta.assistants.create(
    name=name_dl,
    instructions=inst<a id="_idTextAnchor115"/>ructions_dl,
    model=assistant_deployment_name,
    tools=tools
)</pre>			<p>The <a id="_idIndexMarker478"/>User Proxy Assistant thread functions as the primary communication channel, facilitating message exchanges between agents to fulfill the user’s request, as illustrated in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B21019_10_04.jpg" alt="Figure 10.4: User Proxy Agent thread"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4: User Proxy Agent thread</p>
			<p>Initialize the User Proxy Assistant thread using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
thread = assistant_client.beta.threads.create()</pre>			<p>The only input necessary is the user’s query, which drives the overall task execution across the assistants. This query is transmitted to the User Proxy Assistant thread via the <strong class="source-inline">dispatch_message</strong> function <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
user_message = input("User Query: ")
message = dispatch_message(user_message, user_proxy, thread)</pre>			<p>The <a id="_idIndexMarker479"/>User Proxy Assistant is designed to route messages to the appropriate agents to ensure <a id="_idIndexMarker480"/>task completion. The <strong class="source-inline">send_message</strong> function facilitates this process by leveraging the <strong class="source-inline">agents_threads</strong> structure, which maintains a record of agents and their respective threads throughout the entire conversation, ensuring seamless communication and <span class="No-Break">task execution:</span></p>
			<pre class="source-code">
agents_threads: Dict[str, Dict[str, Optional[str]]] = {
    "dalle_assistant": {"agent": dalle_assistant, "thread": None},
    "vision_assistant": {"agent": vision_assistant, "thread": None},
}</pre>			<p>If a thread does not already exist between the User Proxy Assistant and the other agents, a new one is created to initiate the conversation, ensuring smooth communication and <span class="No-Break">task execution:</span></p>
			<pre class="source-code">
# Create a new thread if user proxy and agent thread does not exist
    if not recipient_info["thread"]:
        thread_object = assistant_client.beta.threads.create()
        recipient_info["thread"] = thread_object</pre>			<p>For instance, when the User Proxy Assistant needs to interact with the DALL·E Assistant, it establishes a new thread between the two agents to facilitate communication, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B21019_10_05.jpg" alt="Figure 10.5: User Proxy and DALL-E Assistant communication"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5: User Proxy and DALL-E Assistant communication</p>
			<p>The <strong class="source-inline">send_message</strong> function <a id="_idIndexMarker481"/>invokes another function responsible for creating and dispatching a message to the appropriate agent, ensuring the task is <span class="No-Break">executed efficiently:</span></p>
			<pre class="source-code">
message = dispatch_message(query, recipient_info["agent"]</pre>			<p>The <strong class="source-inline">dispatch_message</strong> function identifies and retrieves all available functions for the specified assistant. It utilizes the <strong class="source-inline">agent.tool</strong> function to register these capabilities and adds them to the assistant’s available function dictionary <span class="No-Break">for execution:</span></p>
			<pre class="source-code">
available_functions = {}
# Iterate through each tool in the agent.tools list
    for tool in agent.tools:
        # Check if the tool has a 'function' attribute
        if hasattr(tool, 'function'):
            function_name = tool.function.name
            # Attempt to retrieve the function by its name and add it 
            # to the available functions dictionary
            if function_name in globals():
                available_functions[function_name] = globals()
                    [function_name]
        else:
# Handle the case where the tool does not have a 'function' attribute
            print("This tool does not have a 'function' attribute.")</pre>			<p>The <strong class="source-inline">dispatch_message</strong> function then directs a message to the appropriate assistant to carry out the task by generating a new message within the thread and invoking the<a id="_idIndexMarker482"/> assistant’s <strong class="source-inline">create</strong> function. This same process is applied when handling the user’s query within the User Proxy <span class="No-Break">Assistant thread:</span></p>
			<pre class="source-code">
# Draft a new message as part of the ongoing conversation.
message = assistant_client.beta.threads.messages.create(
    thread_id=thread.id, role="user", content=message)
    # Carry out the tasks delineated in this discussion thread.
    run = assistant_client.beta.threads.runs.create(
        thread_id=thread.id,
        assistant_id=agent.id,
    )</pre>			<p>The assistant thread’s <strong class="source-inline">run</strong> function assesses whether a specific tool or function is needed for the given task. If a requirement is identified, it assigns the relevant function to the assistant for execution. Once invoked, the function processes the task and returns the output, which is then stored in the <span class="No-Break"><strong class="source-inline">tool_response</strong></span><span class="No-Break"> object:</span></p>
			<pre class="source-code">
# Execute the proper function to execute the task if an action is required
        if run.status == "requires_action":
            tool_calls = run.required_action.submit_tool_outputs
                            .tool_calls
            tool_outputs = []
            for tool_call in tool_calls:
                tool_responses = []
                if (
                    run.required_action.type == "submit_tool_outputs"
                    and run.required_action.submit_tool_outputs
                           .tool_calls is not None
                ):
                tool_calls = run.required_action.submit_tool_outputs
                                .tool_calls
                for call in tool_calls:
                    if call.type == "function":
                        if call.function.name not in 
                            available_functions:
                        raise Exception("Function requested by the 
                            model does not exist")
#associate the proper function to the agent to call
                    function_to_call = available_functions[
                        call.function.name]
                    tool_response = function_to_call(**json.loads(
                        call.function.arguments))
                    tool_responses.append({"tool_call_id": call.id, 
                        "output": tool_response})
 # submit tool outputs
            run = client.beta.threads.runs.submit_tool_outputs(
                thread_id=thread.id,
                run_id=run.id,
                tool_outputs=tool_responses
            )</pre>			<p>The output is sent back to the User Proxy Assistant as a message via the <strong class="source-inline">submit_tool_outputs</strong> function, ensuring seamless communication and <span class="No-Break">task completion.</span></p>
			<p>For instance, as <a id="_idIndexMarker483"/>illustrated in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.6</em>, the DALL·E Assistant returns the generated image to the User Proxy Assistant thread, ensuring the output is relayed back for further processing or <span class="No-Break">user interaction.</span></p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B21019_10_06.jpg" alt="Figure 10.6: The message and output between assistants"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6: The message and output between assistants</p>
			<p>The <a id="_idIndexMarker484"/>User Proxy Assistant then initiates a request to the Vision Assistant to analyze the generated image and provide feedback. It follows the same process as before but now directs the request to the Vision Assistant, creating a new thread linked to the existing User Proxy Assistant thread. This cycle can be repeated iteratively, calling different assistants as needed, until the overall task is fully completed, as <span class="No-Break">illustrated here:</span></p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B21019_10_07.jpg" alt="Figure 10.7: Multi-agent architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7: Multi-agent architecture</p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor116"/>Example output</h1>
			<p>Now, let’s <a id="_idIndexMarker485"/>conduct end-to-end testing with all three assistants working together to complete the task by deploying the full solution from this chapter’s <span class="No-Break">GitHub repository.</span></p>
			<p><em class="italic">User Agent</em>: Generate an image of a boat drifting in the water and analyze it and enhance <span class="No-Break">the image.</span></p>
			<p><em class="italic">Dalle Assistant</em>: Creating <span class="No-Break">the image...</span></p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B21019_10_08.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p><em class="italic">Vision Assistant</em>: Analyzing <span class="No-Break">the image...</span></p>
			<p><em class="italic">Vision Assistant</em>: The image<a id="_idIndexMarker486"/> presents a serene and minimalistic composition featuring a solitary wooden<a id="_idIndexMarker487"/> boat floating on calm waters. The overall tone is muted and ethereal, conveying a sense of peace and tranquility. The boat, capped with a small canopy and a flag, is perfectly positioned to catch the viewer’s focus. The reflection in the water is a nice touch that adds depth to the scene. The light is beautifully managed, likely depicting either dawn or dusk, which gives the scene a dreamy, <span class="No-Break">soft glow.</span></p>
			<p><span class="No-Break"><strong class="bold">Critique</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="bold">Lack of Detail</strong>: While simplicity is part of the image’s charm, the boat itself lacks detailed texture or distinctive features. Adding some details or ornaments on the boat could enhance visual interest without disturbing <span class="No-Break">the tranquility.</span></li>
				<li><strong class="bold">Foreground Wave</strong>: The wave in the foreground is slightly distracting. It draws attention away from the boat and seems inconsistent with the otherwise calm setting. A more uniform water surface would emphasize <span class="No-Break">the stillness.</span></li>
				<li><strong class="bold">Sky Gradient</strong>: The sky lacks a gradient typical of dawn or dusk scenes, which could add to the atmosphere significantly. Introducing some color transitions in the sky might enhance the <span class="No-Break">ethereal effect.</span></li>
				<li><strong class="bold">Background Content</strong>: The background is very plain, and while this focuses attention on the boat, incorporating subtle elements such as distant landmasses or birds might create a more dynamic and contextually <span class="No-Break">rich composition.</span></li>
			</ul>
			<p><em class="italic">Enhanced prompt for DALL-E</em>: Generate an ethereal image of a wooden boat with detailed texture, floating on calm waters at dawn. The boat should feature a small canopy and a<a id="_idIndexMarker488"/> decorative flag. The water should appear still, with no waves interrupting the serene atmosphere. Capture a soft and colorful gradient in the sky to reflect an early morning or evening time, possibly including distant, faint silhouettes of land or birds to add subtle complexity and depth to <span class="No-Break">the scene.</span></p>
			<p><em class="italic">Dalle Assistant</em>: Creating <span class="No-Break">the image...</span></p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/B21019_10_09.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p><em class="italic">Vision Assistant</em>: Analyzing <span class="No-Break">the image...</span></p>
			<p><em class="italic">Vision Assistant</em>: The image showcases a beautiful and serene landscape capturing a traditional wooden boat on a calm body of water during a picturesque sunrise or sunset. The water is so still that the boat and distant silhouettes of other boats and some kind of structures stand <a id="_idIndexMarker489"/>reflected perfectly, enhancing the image’s overall tranquility. The warm color palette of the sky with hues of orange and pink transitioning into the blues gives the scene a dreamy and peaceful atmosphere. The thin line of birds flying across the sky adds a<a id="_idIndexMarker490"/> dynamic element and conveys a sense of journey <span class="No-Break">and freedom.</span></p>
			<p><span class="No-Break"><strong class="bold">Critique</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="bold">Details and Texture</strong>: While the overall composition is compelling, the boat’s texture could be enhanced for more realism. Currently, it appears a bit smooth, which could detract from potential wood <span class="No-Break">grain detailing.</span></li>
				<li><strong class="bold">Lighting</strong>: The ambient lighting is well done, but more contrast could be applied to make the boat’s features pop and increase the scene’s depth, especially for the structure in <span class="No-Break">the background.</span></li>
				<li><strong class="bold">Background Elements</strong>: The structures present in the background are intriguing and add depth but lack clarity. Offering more details or suggesting a specific type of structure could enhance the scene’s <span class="No-Break">storytelling aspect.</span></li>
				<li><strong class="bold">Foreground Interest</strong>: Perhaps including additional elements in the foreground, like subtle ripples in the water or nearby wildlife, might add more interest and life to the scene without detracting from <span class="No-Break">its simplicity.</span></li>
			</ul>
			<p><em class="italic">Enhanced prompt for DALL-E</em>: Create a serene scene of a traditional wooden boat with detailed wood grain textures on a calm lake during a vivid sunset. The sky should be a gradient of warm oranges and pinks blending into cool blues, casting a soft reflective glow on the still water. In the background, faint structures on stilts suggest a mysterious setting, with enough clarity to hint at their cultural significance. Introduce gentle ripples in the water around the boat to imply movement and perhaps integrate a few wildlife figures subtly in the foreground to add lively elements without compromising the tranquility of <span class="No-Break">the scene.</span></p>
			<p><em class="italic">Dalle Assistant</em>: Creating <span class="No-Break">the image...</span></p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/B21019_10_10.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p>With just one user <a id="_idIndexMarker491"/>input, you can see how DALL·E and the Vision Assistant collaborate to enhance image quality—demonstrating the power of a <span class="No-Break">multi-agent system.</span></p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor117"/>Summary</h1>
			<p>In this chapter, we explored building a multi-agent system using the Azure OpenAI Assistants API. Multi-agent framework with the Azure AI Assistants API offers valuable insights into the evolving landscape of AI-driven collaboration. By harnessing the capabilities of LLM-based agents within the Azure ecosystem, we can build a network of specialized agents that enhance system intelligence, enabling them to tackle complex tasks and make informed decisions dynamically. By establishing assistants, managing communication threads, sending messages, and executing functions, we have outlined a structured approach that developers <span class="No-Break">can adopt.</span></p>
			<p>In the next chapter, we will shift our focus to the crucial topics of privacy and security as related to AOAI. Having explored the development and implementation of practical solutions using AOAI alongside various Azure AI services in previous chapters, we will now delve into essential considerations for safeguarding data and ensuring compliance. This chapter will cover topics such as Azure OpenAI Service compliance, data privacy, content filtering, managed identity, virtual network configurations, private endpoint configurations, data encryption, and responsible AI practices for Azure OpenAI. These areas are critical for building secure and trustworthy AI solutions in <span class="No-Break">enterprise environments.</span></p>
		</div>
	

		<div id="_idContainer166" class="Content">
			<h1 id="_idParaDest-118" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor118"/>Part 3: Mastering Governance, Operations, and AI Optimization with Azure OpenAI</h1>
			<p>In this final part, we shift focus from creating solutions to mastering the critical elements of governance, deployment, and advanced optimization strategies for Azure OpenAI. This part equips you with the knowledge to safeguard their implementations, operationalize their AI solutions effectively, and maximize the potential of generative AI through advanced prompt <span class="No-Break">engineering techniques.</span></p>
			<p>You’ll explore essential privacy and security practices to ensure compliance and data protection, dive into operational best practices for managing resources efficiently, and gain insights into crafting precise and impactful prompts. Whether you’re deploying AI at scale or refining its behavior for specific use cases, this section provides the tools to transform your Azure OpenAI solutions into robust, reliable, and <span class="No-Break">innovative systems.</span></p>
			<p>This part has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B21019_11.xhtml#_idTextAnchor119"><em class="italic">Chapter 11</em></a>, <em class="italic">Privacy and Security</em></li>
				<li><a href="B21019_12.xhtml#_idTextAnchor150"><em class="italic">Chapter 12</em></a>, <em class="italic">Operationalizing Azure OpenAI</em></li>
				<li><a href="B21019_13.xhtml#_idTextAnchor164"><em class="italic">Chapter 13</em></a>, <em class="italic">Advanced Prompt Engineering</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer167">
			</div>
		</div>
	</body></html>