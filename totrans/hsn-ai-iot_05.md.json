["```py\npip install deap\n```", "```py\nimport string\nimport random\n\nfrom deap import base, creator, tools\n```", "```py\ncreator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  \n```", "```py\ncreator.create(\"Individual\", list, fitness=creator.FitnessMax)\n```", "```py\ntoolbox = base.Toolbox()\n# Gene Pool\ntoolbox.register(\"attr_string\", random.choice, \\\n               string.ascii_letters + string.digits )\n```", "```py\n#Number of characters in word\n# The word to be guessed\nword = list('hello')\nN = len(word)\n# Initialize population\ntoolbox.register(\"individual\", tools.initRepeat, \\\n         creator.Individual, toolbox.attr_string, N )\ntoolbox.register(\"population\",tools.initRepeat, list,\\\n         toolbox.individual)\n```", "```py\ndef evalWord(individual, word):\n    return sum(individual[i] == word[i] for i in\\\n            range(len(individual))),    \n```", "```py\ntoolbox.register(\"evaluate\", evalWord, word)\ntoolbox.register(\"mate\", tools.cxTwoPoint)\ntoolbox.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.05)\ntoolbox.register(\"select\", tools.selTournament, tournsize=3)\n```", "```py\ndef main():\n    random.seed(64)\n    # create an initial population of 300 individuals \n    pop = toolbox.population(n=300)\n    # CXPB is the crossover probability \n    # MUTPB is the probability for mutating an individual\n    CXPB, MUTPB = 0.5, 0.2\n\n    print(\"Start of evolution\")\n\n    # Evaluate the entire population\n    fitnesses = list(map(toolbox.evaluate, pop))\n    for ind, fit in zip(pop, fitnesses):\n        ind.fitness.values = fit\n\n    print(\" Evaluated %i individuals\" % len(pop))\n\n    # Extracting all the fitnesses of individuals in a list\n    fits = [ind.fitness.values[0] for ind in pop]\n    # Variable keeping track of the number of generations\n    g = 0\n\n    # Begin the evolution\n    while max(fits) < 5 and g < 1000:\n        # A new generation\n        g += 1\n        print(\"-- Generation %i --\" % g)\n\n        # Select the next generation individuals\n        offspring = toolbox.select(pop, len(pop))\n        # Clone the selected individuals\n        offspring = list(map(toolbox.clone, offspring))\n\n        # Apply crossover and mutation on the offspring\n        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n            # cross two individuals with probability CXPB\n            if random.random() < CXPB:    \n            toolbox.mate(child1, child2)\n            # fitness values of the children\n            # must be recalculated later\n            del child1.fitness.values\n            del child2.fitness.values\n        for mutant in offspring:\n            # mutate an individual with probability MUTPB\n            if random.random() < MUTPB:\n                toolbox.mutate(mutant)\n                del mutant.fitness.values\n\n        # Evaluate the individuals with an invalid fitness\n        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n        fitnesses = map(toolbox.evaluate, invalid_ind)\n        for ind, fit in zip(invalid_ind, fitnesses):\n        ind.fitness.values = fit\n\n        print(\" Evaluated %i individuals\" % len(invalid_ind))\n\n        # The population is entirely replaced by the offspring\n        pop[:] = offspring\n\n        # Gather all the fitnesses in one list and print the stats\n        fits = [ind.fitness.values[0] for ind in pop]\n\n        length = len(pop)\n        mean = sum(fits) / length\n        sum2 = sum(x*x for x in fits)\n        std = abs(sum2 / length - mean**2)**0.5\n\n        print(\" Min %s\" % min(fits))\n        print(\" Max %s\" % max(fits))\n        print(\" Avg %s\" % mean)\n        print(\" Std %s\" % std)\n\n    print(\"-- End of (successful) evolution --\")\n\n    best_ind = tools.selBest(pop, 1)[0]\n    print(\"Best individual is %s, %s\" % (''.join(best_ind),\\\n             best_ind.fitness.values))\n```", "```py\nimport random\nimport numpy as np\n\nfrom deap import base, creator, tools, algorithms\nfrom scipy.stats import bernoulli\nfrom dag import DAG, DAGValidationError\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n```", "```py\nmnist = input_data.read_data_sets(\"mnist_data/\", one_hot=True)\ntrain_imgs   = mnist.train.images\ntrain_labels = mnist.train.labels\ntest_imgs    = mnist.test.images\ntest_labels  = mnist.test.labels\n\ntrain_imgs = np.reshape(train_imgs,[-1,28,28,1])\ntest_imgs = np.reshape(test_imgs,[-1,28,28,1])\n```", "```py\nSTAGES = np.array([\"s1\",\"s2\",\"s3\"]) # S\nNUM_NODES = np.array([3,4,5]) # K\n\nL =  0 # genome length\nBITS_INDICES, l_bpi = np.empty((0,2),dtype = np.int32), 0 # to keep track of bits for each stage S\nfor nn in NUM_NODES:\n    t = nn * (nn - 1)\n    BITS_INDICES = np.vstack([BITS_INDICES,[l_bpi, l_bpi + int(0.5 * t)]])\n    l_bpi = int(0.5 * t)\n    L += t\nL = int(0.5 * L)\n\nTRAINING_EPOCHS = 20\nBATCH_SIZE = 20\nTOTAL_BATCHES = train_imgs.shape[0] // BATCH_SIZE\n```", "```py\ndef weight_variable(weight_name, weight_shape):\n    return tf.Variable(tf.truncated_normal(weight_shape, stddev = 0.1),name = ''.join([\"weight_\", weight_name]))\n\ndef bias_variable(bias_name,bias_shape):\n    return tf.Variable(tf.constant(0.01, shape = bias_shape),name = ''.join([\"bias_\", bias_name]))\n\ndef linear_layer(x,n_hidden_units,layer_name):\n    n_input = int(x.get_shape()[1])\n    weights = weight_variable(layer_name,[n_input, n_hidden_units])\n    biases = bias_variable(layer_name,[n_hidden_units])\n    return tf.add(tf.matmul(x,weights),biases)\n\ndef apply_convolution(x,kernel_height,kernel_width,num_channels,depth,layer_name):\n    weights = weight_variable(layer_name,[kernel_height, kernel_width, num_channels, depth])\n    biases = bias_variable(layer_name,[depth])\n    return tf.nn.relu(tf.add(tf.nn.conv2d(x, weights,[1,2,2,1],padding = \"SAME\"),biases)) \n\ndef apply_pool(x,kernel_height,kernel_width,stride_size):\n    return tf.nn.max_pool(x, ksize=[1, kernel_height, kernel_width, 1], \n            strides=[1, 1, stride_size, 1], padding = \"SAME\")\n```", "```py\ndef generate_dag(optimal_indvidual,stage_name,num_nodes):\n    # create nodes for the graph\n    nodes = np.empty((0), dtype = np.str)\n    for n in range(1,(num_nodes + 1)):\n        nodes = np.append(nodes,''.join([stage_name,\"_\",str(n)]))\n\n    # initialize directed asyclic graph (DAG) and add nodes to it\n    dag = DAG()\n    for n in nodes:\n        dag.add_node(n)\n\n    # split best indvidual found via genetic algorithm to identify vertices connections and connect them in DAG \n    edges = np.split(optimal_indvidual,np.cumsum(range(num_nodes - 1)))[1:]\n    v2 = 2\n    for e in edges:\n        v1 = 1\n        for i in e:\n            if i:\n                dag.add_edge(''.join([stage_name,\"_\",str(v1)]),''.join([stage_name,\"_\",str(v2)])) \n            v1 += 1\n        v2 += 1\n\n    # delete nodes not connected to anyother node from DAG\n    for n in nodes:\n        if len(dag.predecessors(n)) == 0 and len(dag.downstream(n)) == 0:\n            dag.delete_node(n)\n            nodes = np.delete(nodes, np.where(nodes == n)[0][0])\n\n    return dag, nodes\n```", "```py\ndef generate_tensorflow_graph(individual,stages,num_nodes,bits_indices):\n    activation_function_pattern = \"/Relu:0\"\n\n    tf.reset_default_graph()\n    X = tf.placeholder(tf.float32, shape = [None,28,28,1], name = \"X\")\n    Y = tf.placeholder(tf.float32,[None,10],name = \"Y\")\n\n    d_node = X\n    for stage_name,num_node,bpi in zip(stages,num_nodes,bits_indices):\n        indv = individual[bpi[0]:bpi[1]]\n\n        add_node(''.join([stage_name,\"_input\"]),d_node.name)\n        pooling_layer_name = ''.join([stage_name,\"_input\",activation_function_pattern])\n\n        if not has_same_elements(indv):\n            # ------------------- Temporary DAG to hold all connections implied by genetic algorithm solution ------------- #  \n\n            # get DAG and nodes in the graph\n            dag, nodes = generate_dag(indv,stage_name,num_node) \n            # get nodes without any predecessor, these will be connected to input node\n            without_predecessors = dag.ind_nodes() \n            # get nodes without any successor, these will be connected to output node\n            without_successors = dag.all_leaves()\n\n            # ----------------------------------------------------------------------------------------------- #\n\n            # --------------------------- Initialize tensforflow graph based on DAG ------------------------- #\n\n            for wop in without_predecessors:\n                add_node(wop,''.join([stage_name,\"_input\",activation_function_pattern]))\n\n            for n in nodes:\n                predecessors = dag.predecessors(n)\n                if len(predecessors) == 0:\n                    continue\n                elif len(predecessors) > 1:\n                    first_predecessor = predecessors[0]\n                    for prd in range(1,len(predecessors)):\n                        t = sum_tensors(first_predecessor,predecessors[prd],activation_function_pattern)\n                        first_predecessor = t.name\n                    add_node(n,first_predecessor)\n                elif predecessors:\n                    add_node(n,''.join([predecessors[0],activation_function_pattern]))\n\n            if len(without_successors) > 1:\n                first_successor = without_successors[0]\n                for suc in range(1,len(without_successors)):\n                    t = sum_tensors(first_successor,without_successors[suc],activation_function_pattern)\n                    first_successor = t.name\n                add_node(''.join([stage_name,\"_output\"]),first_successor) \n            else:\n                add_node(''.join([stage_name,\"_output\"]),''.join([without_successors[0],activation_function_pattern])) \n\n            pooling_layer_name = ''.join([stage_name,\"_output\",activation_function_pattern])\n            # ------------------------------------------------------------------------------------------ #\n\n        d_node =  apply_pool(tf.get_default_graph().get_tensor_by_name(pooling_layer_name), \n                                 kernel_height = 16, kernel_width = 16,stride_size = 2)\n\n    shape = d_node.get_shape().as_list()\n    flat = tf.reshape(d_node, [-1, shape[1] * shape[2] * shape[3]])\n    logits = linear_layer(flat,10,\"logits\")\n\n    xentropy =  tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y)\n    loss_function = tf.reduce_mean(xentropy)\n    optimizer = tf.train.AdamOptimizer().minimize(loss_function) \n    accuracy = tf.reduce_mean(tf.cast( tf.equal(tf.argmax(tf.nn.softmax(logits),1), tf.argmax(Y,1)), tf.float32))\n\n    return  X, Y, optimizer, loss_function, accuracy\n\n# Function to add nodes\ndef add_node(node_name, connector_node_name, h = 5, w = 5, nc = 1, d = 1):\n    with tf.name_scope(node_name) as scope:\n        conv = apply_convolution(tf.get_default_graph().get_tensor_by_name(connector_node_name), \n                   kernel_height = h, kernel_width = w, num_channels = nc , depth = d, \n                   layer_name = ''.join([\"conv_\",node_name]))\n\ndef sum_tensors(tensor_a,tensor_b,activation_function_pattern):\n    if not tensor_a.startswith(\"Add\"):\n        tensor_a = ''.join([tensor_a,activation_function_pattern])\n\n    return tf.add(tf.get_default_graph().get_tensor_by_name(tensor_a),\n                 tf.get_default_graph().get_tensor_by_name(''.join([tensor_b,activation_function_pattern])))\n\ndef has_same_elements(x):\n    return len(set(x)) <= 1\n```", "```py\ndef evaluateModel(individual):\n    score = 0.0\n    X, Y, optimizer, loss_function, accuracy = generate_tensorflow_graph(individual,STAGES,NUM_NODES,BITS_INDICES)\n    with tf.Session() as session:\n        tf.global_variables_initializer().run()\n        for epoch in range(TRAINING_EPOCHS):\n            for b in range(TOTAL_BATCHES):\n                offset = (epoch * BATCH_SIZE) % (train_labels.shape[0] - BATCH_SIZE)\n                batch_x = train_imgs[offset:(offset + BATCH_SIZE), :, :, :]\n                batch_y = train_labels[offset:(offset + BATCH_SIZE), :]\n                _, c = session.run([optimizer, loss_function],feed_dict={X: batch_x, Y : batch_y})\n\n        score = session.run(accuracy, feed_dict={X: test_imgs, Y: test_labels})\n        #print('Accuracy: ',score)\n    return score,\n```", "```py\npopulation_size = 20\nnum_generations = 3\ncreator.create(\"FitnessMax\", base.Fitness, weights = (1.0,))\ncreator.create(\"Individual\", list , fitness = creator.FitnessMax)\ntoolbox = base.Toolbox()\ntoolbox.register(\"binary\", bernoulli.rvs, 0.5)\ntoolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.binary, n = L)\ntoolbox.register(\"population\", tools.initRepeat, list , toolbox.individual)\ntoolbox.register(\"mate\", tools.cxOrdered)\ntoolbox.register(\"mutate\", tools.mutShuffleIndexes, indpb = 0.8)\ntoolbox.register(\"select\", tools.selRoulette)\ntoolbox.register(\"evaluate\", evaluateModel)\npopl = toolbox.population(n = population_size)\n\nimport time\nt = time.time()\nresult = algorithms.eaSimple(popl, toolbox, cxpb = 0.4, mutpb = 0.05, ngen = num_generations, verbose = True)\nt1 = time.time() - t\nprint(t1)\n```", "```py\nbest_individuals = tools.selBest(popl, k = 3)\nfor bi in best_individuals:\n    print(bi)\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split as split\n\nfrom keras.layers import LSTM, Input, Dense\nfrom keras.models import Model\n\nfrom deap import base, creator, tools, algorithms\nfrom scipy.stats import bernoulli\nfrom bitstring import BitArray\n\nnp.random.seed(1120)\n```", "```py\ndata = pd.read_csv('train.csv')\ndata = np.reshape(np.array(data['wp1']),(len(data['wp1']),1))\n\ntrain_data = data[0:17257]\ntest_data = data[17257:]\n```", "```py\ndef prepare_dataset(data, window_size):\n    X, Y = np.empty((0,window_size)), np.empty((0))\n    for i in range(len(data)-window_size-1):\n        X = np.vstack([X,data[i:(i + window_size),0]])\n        Y = np.append(Y,data[i + window_size,0])   \n    X = np.reshape(X,(len(X),window_size,1))\n    Y = np.reshape(Y,(len(Y),1))\n    return X, Y\n```", "```py\ndef train_evaluate(ga_individual_solution):   \n    # Decode genetic algorithm solution to integer for window_size and num_units\n    window_size_bits = BitArray(ga_individual_solution[0:6])\n    num_units_bits = BitArray(ga_individual_solution[6:]) \n    window_size = window_size_bits.uint\n    num_units = num_units_bits.uint\n    print('\\nWindow Size: ', window_size, ', Num of Units: ', num_units)\n\n    # Return fitness score of 100 if window_size or num_unit is zero\n    if window_size == 0 or num_units == 0:\n        return 100, \n\n    # Segment the train_data based on new window_size; split into train and validation (80/20)\n    X,Y = prepare_dataset(train_data,window_size)\n    X_train, X_val, y_train, y_val = split(X, Y, test_size = 0.20, random_state = 1120)\n\n    # Train LSTM model and predict on validation set\n    inputs = Input(shape=(window_size,1))\n    x = LSTM(num_units, input_shape=(window_size,1))(inputs)\n    predictions = Dense(1, activation='linear')(x)\n    model = Model(inputs=inputs, outputs=predictions)\n    model.compile(optimizer='adam',loss='mean_squared_error')\n    model.fit(X_train, y_train, epochs=5, batch_size=10,shuffle=True)\n    y_pred = model.predict(X_val)\n\n    # Calculate the RMSE score as fitness score for GA\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    print('Validation RMSE: ', rmse,'\\n')\n\n    return rmse,\n```", "```py\npopulation_size = 4\nnum_generations = 4\ngene_length = 10\n\n# As we are trying to minimize the RMSE score, that's why using -1.0\\. \n# In case, when you want to maximize accuracy for instance, use 1.0\ncreator.create('FitnessMax', base.Fitness, weights = (-1.0,))\ncreator.create('Individual', list , fitness = creator.FitnessMax)\n\ntoolbox = base.Toolbox()\ntoolbox.register('binary', bernoulli.rvs, 0.5)\ntoolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n = gene_length)\ntoolbox.register('population', tools.initRepeat, list , toolbox.individual)\n\ntoolbox.register('mate', tools.cxOrdered)\ntoolbox.register('mutate', tools.mutShuffleIndexes, indpb = 0.6)\ntoolbox.register('select', tools.selRoulette)\ntoolbox.register('evaluate', train_evaluate)\n\npopulation = toolbox.population(n = population_size)\nr = algorithms.eaSimple(population, toolbox, cxpb = 0.4, mutpb = 0.1, ngen = num_generations, verbose = False)\n```", "```py\nbest_individuals = tools.selBest(population,k = 1)\nbest_window_size = None\nbest_num_units = None\n\nfor bi in best_individuals:\n    window_size_bits = BitArray(bi[0:6])\n    num_units_bits = BitArray(bi[6:]) \n    best_window_size = window_size_bits.uint\n    best_num_units = num_units_bits.uint\n    print('\\nWindow Size: ', best_window_size, ', Num of Units: ', best_num_units)\n```", "```py\nX_train,y_train = prepare_dataset(train_data,best_window_size)\nX_test, y_test = prepare_dataset(test_data,best_window_size)\n\ninputs = Input(shape=(best_window_size,1))\nx = LSTM(best_num_units, input_shape=(best_window_size,1))(inputs)\npredictions = Dense(1, activation='linear')(x)\nmodel = Model(inputs = inputs, outputs = predictions)\nmodel.compile(optimizer='adam',loss='mean_squared_error')\nmodel.fit(X_train, y_train, epochs=5, batch_size=10,shuffle=True)\ny_pred = model.predict(X_test)\n\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint('Test RMSE: ', rmse)\n```"]