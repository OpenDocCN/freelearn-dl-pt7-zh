<html><head></head><body>
		<div>
			<div id="_idContainer146" class="Content">
			</div>
		</div>
		<div id="_idContainer147" class="Content">
			<h1 id="_idParaDest-142">5. <a id="_idTextAnchor158"/>Artificial Intelligence: Clustering</h1>
		</div>
		<div id="_idContainer170" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter will introduce you to the fundamentals of clustering, an unsupervised learning approach in contrast with the supervised learning approaches seen in the previous chapters. You will be implementing different types of clustering, including flat clustering with the k-means algorithm and hierarchical clustering with the mean shift algorithm and the agglomerative hierarchical model. You will also learn how to evaluate the performance of your clustering model using intrinsic and extrinsic approaches. By the end of this chapter, you will be able to analyze data using clustering and apply this skill to solve challenges across a variety of fields.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor159"/>Introduction</h1>
			<p><a id="_idTextAnchor160"/>In the previous chapter, you were introduced to decision trees and their applications in classification. You were also introduced to regression in <em class="italic">Chapter 2</em>, <em class="italic">An Introduction to Regression</em>. Both regression and classification are part of the supervised learning approach. However, in this chapter, we will be looking at the unsupervised learning approach; we will be dealing with datasets that don't have any labels (outputs). It is up to the machines to tell us what the labels will be based on a set of parameters that we define. In this chapter, we will be performing unsupervised learning by using clustering algorithms.</p>
			<p>We will use clustering to analyze data to find certain patterns and create groups. Apart from that, clustering can be used for many purposes:</p>
			<ul>
				<li>Market segmentation detects the best stocks in the market you should be focusing on.</li>
				<li>Customer segmentation detects customer cohorts using their consumption patterns to recommend products better.</li>
				<li>In computer vision, image segmentation is performed using clustering. Using this, we can find different objects in an image.</li>
				<li>Clustering can be also be combined with classification to generate a compact representation of multiple features (inputs), which can then be fed to a classifier.</li>
				<li>Clustering can also filter data points by detecting outliers.</li>
			</ul>
			<p>Regardless of whether we are applying clustering to genetics, videos, images, or social networks, if we analyze data using clustering, we may find similarities between data points that are worth treating uniformly.</p>
			<p>For instance, consider a store manager, who is responsible for ensuring the profitability of their store. The products in the store are divided into different categories, and there are different customers who prefer different items. Each customer has their own preferences, but they have some similarities between them. You might have a customer who is interested in bio products, who tends to choose organic products, which are also of interest to a vegetarian customer. Even if they are different, they have similarities in their preferences or patterns as they both tend to buy organic vegetables. This can be treated as an example of clustering.</p>
			<p>In <em class="italic">Chapter 3</em>, <em class="italic">An Introduction to Classification</em>, you learned about classification, which is a part of the supervised learning approach. In a classification problem, we use labels to train a model in order to be able to classify data points. With clustering, as we do not have labels for our features, we need to let the model figure out the clusters to which these features belong. This is usually based on the distance between each data point.</p>
			<p>In this chapter, you will learn about the k-means algorithm, which is the most widely used algorithm for clustering, but first, we need to define what the clustering problem is.</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor161"/>Defining the Clustering Problem</h1>
			<p>We shall define the clustering problem so that we will be able to find similarities between our data points. For instance, suppose we have a dataset that consists of points. Clustering helps us understand this structure by describing how these points are distributed.</p>
			<p>Let's look at an example of data points in a two-dimensional space in <em class="italic">Figure 5.1</em>:</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B16060_05_01.jpg" alt="Figure 5.1: Data points in a two-dimensional space&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1: Data points in a two-dimensional space</p>
			<p>Now, have a look, at <em class="italic">Figure 5.2</em>. It is evident that there are <strong class="bold">three</strong> clusters:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B16060_05_02.jpg" alt="Figure 5.2: Three clusters formed using the data points in a two-dimensional space&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2: Three clusters formed using the data points in a two-dimensional space</p>
			<p>The three clusters were easy to detect because the points are close to one another. Here, you can see that clustering determines the data points that are close to each other. You may have also noticed that the data points <strong class="source-inline">M</strong><span class="subscript">1</span>, <strong class="source-inline">O</strong><span class="subscript">1</span>, and <strong class="source-inline">N</strong><span class="subscript">1</span> do not belong to any cluster; these are the <strong class="bold">outlier points</strong>. The clustering algorithm you build should be prepared to treat these outlier points properly, without moving them into a cluster.</p>
			<p>While it is easy to recognize clusters in a two-dimensional space, we normally have multidimensional data points, which is where we have more than two features. Therefore, it is important to know which data points are close to one other. Also, it is important to define the distance metrics that detect whether data points are close to each other. One well-known distance metric is Euclidean distance, which we learned about in <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Artificial Intelligence</em>. In mathematics, we often use Euclidean distance to measure the distance between two points. Therefore, Euclidean distance is an intuitive choice when it comes to clustering algorithms so that we can determine the proximity of data points when locating clusters.</p>
			<p>However, there is one drawback to most distance metrics, including Euclidean distance: the more we increase the dimensions, the more uniform these distances will become compared to each other. When we only have a few dimensions or features, it is easy to see which point is the closest to another one. However, when we add more features, the relevant features get embedded with all the other data and it becomes very hard to distinguish the relevant features from the others as they act as noise for our model. Therefore, getting rid of these noisy features may greatly increase the accuracy of our clustering model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Noise in a dataset can be irrelevant information or randomness that is unwanted.</p>
			<p>In the next section, we will be looking at two different clustering approaches.</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor162"/>Clustering Approaches</h1>
			<p>There are two types of clustering:</p>
			<ul>
				<li><strong class="bold">Flat</strong></li>
				<li><strong class="bold">Hierarchical</strong></li>
			</ul>
			<p>In flat clustering, we specify the number of clusters we would like the machine to find. One example of flat clustering is the k-means algorithm, where <em class="italic">k</em> specifies the number of clusters we would like the algorithm to use.</p>
			<p>In hierarchical clustering, however, the machine learning algorithm itself finds out the number of clusters that are needed.</p>
			<p>Hierarchical clustering also has two approaches:</p>
			<ul>
				<li><strong class="bold">Agglomerative or bottom-up hierarchical clustering</strong> treats each point as a cluster to begin with. Then, the closest clusters are grouped together. The grouping is repeated until we reach a single cluster with every data point.</li>
				<li><strong class="bold">Divisive or top-down hierarchical clustering</strong> treats data points as if they were all in one single cluster at the start. Then the cluster is divided into smaller clusters by choosing the furthest data points. The splitting is repeated until each data point becomes its own cluster.</li>
			</ul>
			<p><em class="italic">Figure 5.3</em> gives you a much more accurate description of these two clustering approaches.</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B16060_05_03.jpg" alt="Figure 5.3: Figure showing the two approaches&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3: Figure showing the two approaches</p>
			<p>Now that we are familiar with the different clustering approaches, let's take a look at the different clustering algorithms supported by scikit-learn.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor163"/>Clustering Algorithms Supported by scikit-learn</h2>
			<p>In this chapter, we will learn about two clustering algorithms supported by scikit-learn: </p>
			<ul>
				<li>The k-means algorithm</li>
				<li>The mean shift algorithm</li>
			</ul>
			<p><strong class="bold">K-means</strong> is an example of flat clustering, where we must specify the number of clusters in advance. k-means is a general-purpose clustering algorithm that performs well if the number of clusters is not too high and the size of the clusters is uniform. </p>
			<p><strong class="bold">Mean shift</strong> is an example of hierarchical clustering, where the clustering algorithm determines the number of clusters. Mean shift is used when we do not know the number of clusters in advance. In contrast with k-means, mean shift supports use cases where there may be many clusters present, even if the size of the clusters greatly varies.</p>
			<p>Scikit-learn contains many other algorithms, but we will be focusing on the k-means and mean shift algorithms in this chapter.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For a complete description of clustering algorithms, including performance comparisons, visit the clustering page of scikit-learn at <a href="http://scikit-learn.org/stable/modules/clustering.html">http://scikit-learn.org/stable/modules/clustering.html</a>.</p>
			<p>In the next section, we begin with the k-means algorithm.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor164"/>The K-Means Algorithm</h1>
			<p>The k-means algorithm is a flat clustering algorithm, as mentioned previously. It works as follows:</p>
			<ul>
				<li>Set the value of <em class="italic">k</em>.</li>
				<li>Choose <em class="italic">k</em> data points from the dataset that are the initial centers of the individual clusters.</li>
				<li>Calculate the distance from each data point to the chosen center points and group each point in the cluster whose initial center is the closest to the data point.</li>
				<li>Once all the points are in one of the <em class="italic">k</em> clusters, calculate the center point of each cluster. This center point does not have to be an existing data point in the dataset; it is simply an average.</li>
				<li>Repeat this process of assigning each data point to the cluster whose center is closest to the data point. Repetition continues until the center points no longer move.</li>
			</ul>
			<p>To ensure that the k-means algorithm terminates, we need the following:</p>
			<ul>
				<li>A maximum threshold value at which the algorithm will then terminate</li>
				<li>A maximum number of repetitions of shifting the moving points</li>
			</ul>
			<p>Due to the nature of the k-means algorithm, it will have a hard time dealing with clusters that greatly vary in size.</p>
			<p>The k-means algorithm has many use cases that are part of our everyday lives, such as:</p>
			<ul>
				<li><strong class="bold">Market segmentation</strong>: Companies gather all sorts of data on their customers. Performing k-means clustering analysis on their customers will reveal customer segments (clusters) with defined characteristics. Customers belonging to the same segment can be seen as having similar patterns or preferences.</li>
				<li><strong class="bold">Tagging of content</strong>: Any content (videos, books, documents, movies, or photos) can be assigned tags in order to group together similar content or themes. These tags are the result of clustering.</li>
				<li><strong class="bold">Detection of fraud and criminal activities</strong>: Fraudsters often leave clues in the form of unusual behaviors compared to other customers. For instance, in the car insurance industry, a normal customer will make a claim for a damaged car arising from an incident, whereas fraudsters will make claims for deliberate damage. Clustering can help detect whether the damage has arisen from a real accident or from a fake accident.</li>
			</ul>
			<p>In the next exercise, we will be implementing the k-means algorithm in scikit-learn.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor165"/>Exercise 5.01: Implementing K-Means in scikit-learn</h2>
			<p>In this exercise, we will be plotting a dataset in a two-dimensional plane and performing clustering on it using the k-means algorithm.</p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li>Open a new Jupyter Notebook file.</li>
				<li>Now create an artificial dataset as a NumPy array to demonstrate the k-means algorithm. The data points are shown in the following code snippet:<p class="source-code">import numpy as np</p><p class="source-code">data_points = np.array([[1, 1], [1, 1.5], [2, 2], \</p><p class="source-code">                        [8, 1], [8, 0], [8.5, 1], \</p><p class="source-code">                        [6, 1], [1, 10], [1.5, 10], \</p><p class="source-code">                        [1.5, 9.5], [10, 10], [1.5, 8.5]])</p></li>
				<li>Now, plot these data points in the two-dimensional plane using <strong class="source-inline">matplotlib.pyplot</strong>, as shown in the following code snippet:<p class="source-code">import matplotlib.pyplot as plot</p><p class="source-code">plot.scatter(data_points.transpose()[0], \</p><p class="source-code">             data_points.transpose()[1])</p><p>The expected output is this:</p><div id="_idContainer151" class="IMG---Figure"><img src="image/B16060_05_04.jpg" alt="Figure 5.4: Graph showing the data points on a two-dimensional plane using &#13;&#10;matplotlib.pyplot&#13;&#10;"/></div><p class="figure-caption">Figure 5.4: Graph showing the data points on a two-dimensional plane using matplotlib.pyplot</p><p class="callout-heading">Note</p><p class="callout">We used the <strong class="source-inline">transpose</strong> array method to get the values of the first feature and the second feature. We could also use proper array indexing to access these columns: <strong class="source-inline">dataPoints[:,0]</strong>, which is equivalent to <strong class="source-inline">dataPoints.transpose()[0]</strong>.</p><p>Now that we have the data points, it is time to execute the k-means algorithm on them. </p></li>
				<li>Define <strong class="source-inline">k</strong> as <strong class="source-inline">3</strong> in the k-means algorithm. We expect a cluster in the bottom-left, top-left, and bottom-right corners of the graph. Add <strong class="source-inline">random_state = 8</strong> in order to reproduce the same results:<p class="source-code">from sklearn.cluster import KMeans</p><p class="source-code">k_means_model = KMeans(n_clusters=3,random_state=8)</p><p class="source-code">k_means_model.fit(data_points)</p><p>In the preceding code snippet, we have used the <strong class="source-inline">KMeans</strong> module from <strong class="source-inline">sklearn.cluster</strong>. As always with <strong class="source-inline">sklearn</strong>, we need to define a model with the parameter and then fit the model on the dataset. </p><p>The expected output is this:</p><p class="source-code">KMeans(algorithm='auto', copy_x=True, init='k-means++', </p><p class="source-code">       max_iter=300, n_clusters=3, n_init=10, n_jobs=None,</p><p class="source-code">       precompute_distances='auto',</p><p class="source-code">       random_state=8, tol=0.0001, verbose=0)</p><p>The output shows all the parameters for our k-means models, but the important ones are:</p><p><strong class="source-inline">max_iter</strong>: Represents the maximum number of times the k-means algorithm will iterate through.</p><p><strong class="source-inline">n_clusters</strong>: Represents the number of clusters to be formed by the k-means algorithm.</p><p><strong class="source-inline">n_init</strong>: Represents the number of times the k-means algorithm will initialize a random point.</p><p><strong class="source-inline">tol</strong>: Represents the threshold for checking whether the k-means algorithm can terminate.</p></li>
				<li>Once the clustering is done, access the center point of each cluster as shown in the following code snippet:<p class="source-code">centers = k_means_model.cluster_centers_</p><p class="source-code">centers</p><p>The output of <strong class="source-inline">centers</strong> will be as follows:</p><p class="source-code">array([[7.625     , 0.75      ],</p><p class="source-code">       [3.1       , 9.6       ],</p><p class="source-code">       [1.33333333, 1.5       ]])</p><p>This output is showing the coordinates of the center of our three clusters. If you look back at <em class="italic">Figure 5.4</em>, you will see that the center points of the clusters appear to be in the bottom-left, (<strong class="source-inline">1.3, 1.5</strong>), the top-left (<strong class="source-inline">3.1, 9.6</strong>), and the bottom-right (<strong class="source-inline">7.265, 0.75</strong>) corners of the graph. The <em class="italic">x</em> coordinate of the top-left cluster is <strong class="source-inline">3.1</strong>, most likely because it contains our outlier data point at <strong class="source-inline">[10, 10]</strong>.</p></li>
				<li>Next, plot the clusters with different colors and their center points. To find out which data point belongs to which cluster, we must query the <strong class="source-inline">labels</strong> property of the k-means classifier, as shown in the following code snippet:<p class="source-code">labels = k_means_model.labels_</p><p class="source-code">labels</p><p>The output of <strong class="source-inline">labels</strong> will be as follows:</p><p class="source-code">array([2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1])</p><p>The output array shows which data point belongs to which cluster. This is all we need to plot the data.</p></li>
				<li>Now, plot the data as shown in the following code snippet:<p class="source-code">plot.scatter(centers[:,0], centers[:,1])</p><p class="source-code">for i in range(len(data_points)):</p><p class="source-code">    plot.plot(data_points[i][0], data_points[i][1], \</p><p class="source-code">              ['k+','kx','k_'][k_means_model.labels_[i]])</p><p class="source-code">plot.show()</p><p>In the preceding code snippets, we used the <strong class="source-inline">matplotlib</strong> library to plot the data points with the center of each coordinate. Each cluster has its marker (<strong class="source-inline">x</strong>, <strong class="source-inline">+</strong>, and <strong class="source-inline">-</strong>), and its center is represented by a filled circle.</p><p>The expected output is this:</p><p> </p><div id="_idContainer152" class="IMG---Figure"><img src="image/B16060_05_05.jpg" alt="Figure 5.5: Graph showing the center points of the three clusters&#13;&#10;"/></div><p class="figure-caption">Figure 5.5: Graph showing the center points of the three clusters</p><p>Having a look at <em class="italic">Figure 5.5</em>, you can see that the center points are inside their clusters, which are represented by the <strong class="source-inline">x</strong>, <strong class="source-inline">+</strong>, and <strong class="source-inline">-</strong> marks.</p></li>
				<li>Now, reuse the same code and choose only two clusters instead of three:<p class="source-code">k_means_model = KMeans(n_clusters=2,random_state=8)</p><p class="source-code">k_means_model.fit(data_points)</p><p class="source-code">centers2 = k_means_model.cluster_centers_</p><p class="source-code">labels2 = k_means_model.labels_</p><p class="source-code">plot.scatter(centers2[:,0], centers2[:,1])</p><p class="source-code">for i in range(len(data_points)):</p><p class="source-code">    plot.plot(data_points[i][0], data_points[i][1], \</p><p class="source-code">              ['k+','kx'][labels2[i]])</p><p class="source-code">plot.show()</p><p>The expected output is this:</p><p> </p><div id="_idContainer153" class="IMG---Figure"><img src="image/B16060_05_06.jpg" alt="Figure 5.6: Graph showing the data points of the two clusters&#13;&#10;"/></div><p class="figure-caption">Figure 5.6: Graph showing the data points of the two clusters</p><p>This time, we only have <strong class="source-inline">x</strong> and <strong class="source-inline">+</strong> points, and we can clearly see a bottom cluster and a top cluster. Interestingly, the top cluster in the second try contains the same points as the top cluster in the first try. The bottom cluster of the second try consists of the data points joining the bottom-left and the bottom-right clusters of the first try.</p></li>
				<li>Finally, use the k-means model for prediction as shown in the following code snippet. The output will be an array containing the cluster numbers belonging to each data point:<p class="source-code">predictions = k_means_model.predict([[5,5],[0,10]])</p><p class="source-code">predictions</p><p>The output of <strong class="source-inline">predictions</strong> is as follows:</p><p class="source-code">array([0, 1], dtype=int32)</p><p>This means that our first point belongs to the first cluster (at the bottom) and the second point belongs to the second cluster (at the top).</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2CpvMDo">https://packt.live/2CpvMDo</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2Nnv7F2">https://packt.live/2Nnv7F2</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>By completing this exercise, you were able to use a simple k-means clustering model on sample data points.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor166"/>The Parameterization of the K-Means Algorithm in scikit-learn</h2>
			<p>Like the classification and regression models in <em class="italic">Chapter 2</em>, <em class="italic">An Introduction to Regression</em>, <em class="italic">Chapter 3</em>, <em class="italic">An Introduction to Classification</em>, and <em class="italic">Chapter 4</em>, <em class="italic">An Introduction to Decision Trees</em>, the k-means algorithm can also be parameterized. The complete list of parameters can be found at <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html</a>.</p>
			<p>Some examples are as follows:</p>
			<ul>
				<li><strong class="source-inline">n_clusters</strong>: The number of clusters into which the data points are separated. The default value is <strong class="source-inline">8</strong>.</li>
				<li><strong class="source-inline">max_iter</strong>: The maximum number of iterations.</li>
				<li><strong class="source-inline">tol</strong>: The threshold for checking whether we can terminate the k-means algorithm. </li>
			</ul>
			<p>We also used two attributes to retrieve the cluster center points and the clusters themselves:</p>
			<ul>
				<li><strong class="source-inline">cluster_centers_</strong>: This returns the coordinates of the cluster center points.</li>
				<li><strong class="source-inline">labels_</strong>: This returns an array of integers representing the number of clusters the data point belongs to. Numbering starts from zero.</li>
			</ul>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor167"/>Exercise 5.02: Retrieving the Center Points and the Labels</h2>
			<p>In this exercise, you will be able to understand the usage of <strong class="source-inline">cluster_centers_</strong> and <strong class="source-inline">labels_</strong>.</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Next, create the same 12 data points from <em class="italic">Exercise 5.01</em>, <em class="italic">Implementing K-Means in scikit-learn</em>, but here, perform k-means clustering with four clusters, as shown in the following code snippet:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plot</p><p class="source-code">from sklearn.cluster import KMeans</p><p class="source-code">data_points = np.array([[1, 1], [1, 1.5], [2, 2], \</p><p class="source-code">                        [8, 1], [8, 0], [8.5, 1], \</p><p class="source-code">                        [6, 1], [1, 10], [1.5, 10], \</p><p class="source-code">                        [1.5, 9.5], [10, 10], [1.5, 8.5]])</p><p class="source-code">k_means_model = KMeans(n_clusters=4,random_state=8)</p><p class="source-code">k_means_model.fit(data_points)</p><p class="source-code">centers = k_means_model.cluster_centers_</p><p class="source-code">centers</p><p>The output of <strong class="source-inline">centers</strong> is as follows:</p><p class="source-code">array([[ 7.625     ,  0.75      ],</p><p class="source-code">       [ 1.375     ,  9.5       ],</p><p class="source-code">       [ 1.33333333,  1.5       ],</p><p class="source-code">       [10.        , 10.        ]])</p><p>The output of the <strong class="source-inline">cluster_centers_</strong> property shows the <em class="italic">x</em> and <em class="italic">y</em> coordinates of the center points.</p><p>From the output, we can see the <strong class="source-inline">4</strong> centers, which are bottom right (<strong class="source-inline">7.6, 0.75</strong>), top left (<strong class="source-inline">1.3, 9.5</strong>), bottom left (<strong class="source-inline">1.3, 1.5</strong>), and top right (<strong class="source-inline">10, 10</strong>). We can also note that the fourth cluster (the top-right cluster) is only made of a single data point. This data point can be assumed to be an <strong class="bold">outlier</strong>.</p></li>
				<li>Now, apply <strong class="source-inline">labels_ property</strong> on the cluster:<p class="source-code">labels = k_means_model.labels_</p><p class="source-code">labels</p><p>The output of <strong class="source-inline">labels</strong> is as follows:</p><p class="source-code">array([2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 3, 1], dtype=int32)</p><p>The <strong class="source-inline">labels_</strong> property is an array of length <strong class="source-inline">12</strong>, showing the cluster of each of the <strong class="source-inline">12</strong> data points it belongs to. The first cluster is associated with the number 0, the second is associated with 1, the third is associated with 2, and so on (remember that Python indexes always start from 0 and not 1).</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3dmHsDX">https://packt.live/3dmHsDX</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2B0ebld">https://packt.live/2B0ebld</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>By completing this exercise, you were able to retrieve the coordinates of a cluster's center. You were also able to see which label (cluster) each data point has been assigned to.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor168"/>K-Means Clustering of Sales Data</h2>
			<p>In the upcoming activity, we will be looking at sales data, and we will perform k-means clustering on that sales data. </p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor169"/>Activity 5.01: Clustering Sales Data Using K-Means</h2>
			<p>In this activity, you will work on the Sales Transaction Dataset Weekly dataset, which contains the weekly sales data of 800 products over 1 year. Our dataset won't contain any information regarding the product except sales. </p>
			<p>Your goal will be to identify products with similar sales trends using the k-means clustering algorithm. You will have to experiment with the number of clusters in order to find the optimal number of clusters.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be found at <a href="https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly">https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly</a>.</p>
			<p class="callout">The dataset file can also be found in our GitHub repository: <a href="https://packt.live/3hVH42v">https://packt.live/3hVH42v</a>.</p>
			<p class="callout">Citation: <em class="italic">Tan, S., &amp; San Lau, J. (2014). Time series clustering: A superior alternative for market basket analysis. In Proceedings of the First International Conference on Advanced Data and Information Engineering (DaEng-2013) (pp. 241–248)</em>.</p>
			<p>The following steps will help you complete this activity:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Load the dataset as a DataFr<a id="_idTextAnchor170"/>ame and inspect the data.</li>
				<li>Create a new DataFrame without the unnecessary columns using the <strong class="source-inline">drop</strong> function from pandas (that is, the first <strong class="source-inline">55</strong> columns of the dataset) and use the <strong class="source-inline">inplace</strong> parameter, which is a part of pandas.</li>
				<li>Create a k-means clustering model with <strong class="source-inline">8</strong> clusters and with <strong class="source-inline">random_state = 8</strong>.</li>
				<li>Retrieve the labels from the first clustering model.</li>
				<li>From the first DataFrame, <strong class="source-inline">df</strong>, keep only the <strong class="source-inline">W</strong> columns and the labels as a new column.</li>
				<li>Perform the required aggregation using the <strong class="source-inline">groupby</strong> function from pandas in order to obtain the yearly average sale of each cluster.</li>
			</ol>
			<p>The expected output is this:</p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B16060_05_07.jpg" alt="Figure 5.7: Expected output on the Sales Transaction Data using k-means&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7: Expected output on the Sales Transaction Data using k-means</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity is available on page 363.</p>
			<p>Now that you have seen the k-means algorithm in detail, we will move on to another type of clustering algorithm, the mean shift algorithm.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor171"/>The Mean Shift Algorithm</h1>
			<p>Mean shift is a hierarchical clustering algorithm that assigns data points to a cluster by calculating a cluster's center and moving it towards the mode at each iteration. The mode is the area with the most data points. At the first iteration, a random point will be chosen as the cluster's center and then the algorithm will calculate the mean of all nearby data points within a certain radius. The mean will be the new cluster's center. The second iteration will then begin with the calculation of the mean of all nearby data points and setting it as the new cluster's center. At each iteration, the cluster's center will move closer to where most of the data points are. The algorithm will stop when it is not possible for a new cluster's center to contain more data points. When the algorithm stops, each data point will be assigned to a cluster.</p>
			<p>The mean shift algorithm will also determine the number of clusters needed, in contrast with the k-means algorithm. This is advantageous as we rarely know how many clusters we are looking for.</p>
			<p>This algorithm also has many use cases. For instance, the Xbox Kinect device detects human body parts using the mean shift algorithm. Each main body part (head, arms, legs, hands, and so on) is a cluster of data points assigned by the mean shift algorithm.</p>
			<p>In the next exercise, we will be implementing the mean shift algorithm.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor172"/>Exercise 5.03: Implementing the Mean Shift Algorithm</h2>
			<p>In this exercise, we will implement clustering by using the mean shift algorithm.</p>
			<p>We will use the <strong class="source-inline">scipy.spatial</strong> library in order to compute the Euclidean distance, seen in <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Artificial Intelligence</em>. This library simplifies the calculation of distances (such as Euclidean or Manhattan) between a list of coordinates. More details about this library can be found at <a href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html#module-scipy.spatial.distance">https://docs.scipy.org/doc/scipy/reference/spatial.distance.html#module-scipy.spatial.distance</a>.</p>
			<p>The following steps will help you complete the exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Let's use the data points from <em class="italic">Exercise 5.01</em>, <em class="italic">Implementing K-Means in scikit-learn</em>:<p class="source-code">import numpy as np</p><p class="source-code">data_points = np.array([[1, 1], [1, 1.5], [2, 2], \</p><p class="source-code">                        [8, 1], [8, 0], [8.5, 1], \</p><p class="source-code">                        [6, 1], [1, 10], [1.5, 10], \</p><p class="source-code">                        [1.5, 9.5], [10, 10], [1.5, 8.5]])</p><p class="source-code">import matplotlib.pyplot as plot</p><p class="source-code">plot.scatter(data_points.transpose()[0], \</p><p class="source-code">             data_points.transpose()[1])</p><p>Our task now is to find point P (x, y), for which the number of data points within radius R from point P is maximized. The points are distributed as follows:</p><div id="_idContainer155" class="IMG---Figure"><img src="image/B16060_05_08.jpg" alt="Figure 5.8: Graph showing the data points from the data_points array&#13;&#10;"/></div><p class="figure-caption">Figure 5.8: Graph showing the data points from the data_points array</p></li>
				<li>Equate point <strong class="source-inline">P1</strong> to the first data point, <strong class="source-inline">[1, 1]</strong> of our list:<p class="source-code">P1 = [1, 1]</p></li>
				<li>Find the points that are within a distance of <strong class="source-inline">r = 2</strong> from this point. We will use the <strong class="source-inline">scipy</strong> library, which simplifies mathematical calculations, including spatial distance:<p class="source-code">from scipy.spatial import distance</p><p class="source-code">r = 2</p><p class="source-code">points1 = np.array([p0 for p0 in data_points if \</p><p class="source-code">                    distance.euclidean(p0, P1) &lt;= r])</p><p class="source-code">points1</p><p>In the preceding code snippet, we used the Euclidean distance to find all the points that fall within the <strong class="source-inline">r</strong> radius of point <strong class="source-inline">P1</strong>.</p><p>The output of <strong class="source-inline">points1</strong> will be as follows:</p><p class="source-code">array([[1. , 1. ],</p><p class="source-code">       [1. , 1.5],</p><p class="source-code">       [2. , 2. ]])</p><p>From the output, we can see that we found three points that fall within the radius of <strong class="source-inline">P1</strong>. They are the three points at the bottom left of the graph we saw earlier, in <em class="italic">Figure 5.8</em> of this chapter.</p></li>
				<li>Now, calculate the mean of the data points to obtain the new coordinates of <strong class="source-inline">P2</strong>:<p class="source-code">P2 = [np.mean( points1.transpose()[0] ), \</p><p class="source-code">      np.mean(points1.transpose()[1] )]</p><p class="source-code">P2</p><p>In the preceding code snippet, we have calculated the mean of the array containing the three data points in order to obtain the new coordinates of <strong class="source-inline">P2</strong>.</p><p>The output of <strong class="source-inline">P2</strong> will be as follows:</p><p class="source-code">[1.3333333333333333, 1.5]</p></li>
				<li>Now that the new <strong class="source-inline">P2</strong> has been calculated, retrieve the points within the given radius again, as shown in the following code snippet:<p class="source-code">points2 = np.array([p0 for p0 in data_points if \</p><p class="source-code">                    distance.euclidean( p0, P2) &lt;= r])</p><p class="source-code">points2</p><p>The output of <strong class="source-inline">points</strong> will be as follows:</p><p class="source-code">array([[1. , 1. ],</p><p class="source-code">       [1. , 1.5],</p><p class="source-code">       [2. , 2. ]])</p><p>These are the same three points that we found in <em class="italic">Step 4</em>, so we can stop here. Three points have been found around the mean of <strong class="source-inline">[1.3333333333333333, 1.5]</strong>. The points around this center within a radius of <strong class="source-inline">2</strong> form a cluster.</p></li>
				<li>Since data points <strong class="source-inline">[1, 1.5]</strong> and <strong class="source-inline">[2, 2]</strong> are already in a cluster with <strong class="source-inline">[1,1]</strong>, we can directly continue with the fourth point in our list, <strong class="source-inline">[8, 1]</strong>:<p class="source-code">P3 = [8, 1]</p><p class="source-code">points3 = np.array( [p0 for p0 in data_points if \</p><p class="source-code">                     distance.euclidean(p0, P3) &lt;= r])</p><p class="source-code">points3</p><p>In the preceding code snippet, we used the same code as <em class="italic">Step 4</em> but with a new <strong class="source-inline">P3</strong>.</p><p>The output of <strong class="source-inline">points3</strong> will be as follows:</p><p class="source-code">array([[8. , 1. ],</p><p class="source-code">       [8. , 0. ],</p><p class="source-code">       [8.5, 1. ],</p><p class="source-code">       [6. , 1. ]])</p><p>This time, we found four points inside the radius <strong class="source-inline">r</strong> of <strong class="source-inline">P4</strong>. </p></li>
				<li>Now, calculate the mean, as shown in the following code snippet:<p class="source-code">P4 = [np.mean(points3.transpose()[0]), \</p><p class="source-code">      np.mean(points3.transpose()[1])]</p><p class="source-code">P4</p><p>In the preceding code snippet, we calculated the mean of the array containing the four data points in order to obtain the new coordinates of <strong class="source-inline">P4</strong>, as in <em class="italic">Step 5</em>.</p><p>The output of <strong class="source-inline">P4</strong> will be as follows:</p><p class="source-code">[7.625, 0.75]</p><p>This mean will not change because in the next iteration, we will find the same data points.</p></li>
				<li>Notice that we got lucky with the selection of point <strong class="source-inline">[8, 1]</strong>. If we started with <strong class="source-inline">P = [8, 0]</strong> or <strong class="source-inline">P = [8.5, 1]</strong>, we would only find three points instead of four. Let's try with <strong class="source-inline">P5 = [8, 0]</strong>:<p class="source-code">P5 = [8, 0]</p><p class="source-code">points4 = np.array([p0 for p0 in data_points if \</p><p class="source-code">                   distance.euclidean(p0, P5) &lt;= r])</p><p class="source-code">points4</p><p>In the preceding code snippet, we used the same code as in <em class="italic">Step 4</em> but with a new <strong class="source-inline">P5</strong>.</p><p>The output of <strong class="source-inline">points4</strong> will be as follows:</p><p class="source-code">array([[8. , 1. ],</p><p class="source-code">       [8. , 0. ],</p><p class="source-code">       [8.5, 1. ]])</p><p>This time, we found three points inside the radius <strong class="source-inline">r</strong> of <strong class="source-inline">P5</strong>. </p></li>
				<li>Now, rerun the distance calculation with the shifted mean as shown in <em class="italic">Step 5</em>:<p class="source-code">P6 = [np.mean(points4.transpose()[0]), \</p><p class="source-code">      np.mean(points4.transpose()[1])]</p><p class="source-code">P6</p><p>In the preceding code snippet, we calculated the mean of the array containing the three data points in order to obtain the new coordinates of <strong class="source-inline">P6</strong>.</p><p>The output of <strong class="source-inline">P6</strong> will be as follows:</p><p class="source-code">[8.166666666666666, 0.6666666666666666]</p></li>
				<li>Now do the same again but with <strong class="source-inline">P7 = [8.5, 1]</strong>:<p class="source-code">P7 = [8.5, 1]</p><p class="source-code">points5 = np.array([p0 for p0 in data_points if \</p><p class="source-code">                    distance.euclidean(p0, P7) &lt;= r])</p><p class="source-code">points5</p><p>In the preceding code snippet, we used the same code as in <em class="italic">Step 4</em> but with a new <strong class="source-inline">P7</strong>.</p><p>The output of <strong class="source-inline">points5</strong> will be as follows:</p><p class="source-code">array([[8. , 1. ],</p><p class="source-code">       [8. , 0. ],</p><p class="source-code">       [8.5, 1. ]])</p><p>This time, we found the same three points again inside the radius <strong class="source-inline">r</strong> of <strong class="source-inline">P</strong>. This means that starting from <strong class="source-inline">[8,1]</strong>, we got a larger cluster than starting from <strong class="source-inline">[8, 0]</strong> or <strong class="source-inline">[8.5, 1]</strong>. Therefore, we must take the center point that contains the maximum number of data points.</p></li>
				<li>Now, let's see what would happen if we started the discovery from the fourth data point, that is, <strong class="source-inline">[6, 1]</strong>:<p class="source-code">P8 = [6, 1]</p><p class="source-code">points6 = np.array([p0 for p0 in data_points if \</p><p class="source-code">                    distance.euclidean(p0, P8) &lt;= r])</p><p class="source-code">points6</p><p>In the preceding code snippet, we used the same code as in <em class="italic">Step 4</em> but with a new <strong class="source-inline">P8</strong>.</p><p>The output of <strong class="source-inline">points6</strong> will be as follows:</p><p class="source-code">array([[8., 1.],</p><p class="source-code">       [6., 1.]])</p><p>This time, we found only two data points inside the radius <strong class="source-inline">r</strong> of <strong class="source-inline">P8</strong>. We successfully found the data point <strong class="source-inline">[8, 1]</strong>.</p></li>
				<li>Now, shift the mean from <strong class="source-inline">[6, 1]</strong> to the calculated new mean:<p class="source-code">P9 = [np.mean(points6.transpose()[0]), \</p><p class="source-code">      np.mean(points6.transpose()[1]) ]</p><p class="source-code">P9</p><p>In the preceding code snippet, we calculated the mean of the array containing the three data points in order to obtain the new coordinates of <strong class="source-inline">P9</strong>, as in <em class="italic">Step 5</em>.</p><p>The output of <strong class="source-inline">P9</strong> will be as follows:</p><p class="source-code">[7.0, 1.0]</p></li>
				<li>Check whether you have obtained more points with this new <strong class="source-inline">P9</strong>:<p class="source-code">points7 = np.array([p0 for p0 in data_points if \</p><p class="source-code">                    distance.euclidean(p0, P9) &lt;= r])</p><p class="source-code">points7</p><p>In the preceding code snippet, we used the same code as in <em class="italic">Step 4</em> but with a new <strong class="source-inline">P9</strong>.</p><p>The output of <strong class="source-inline">points7</strong> will be as follows:</p><p class="source-code">array([[8. , 1. ],</p><p class="source-code">       [8. , 0. ],</p><p class="source-code">       [8.5, 1. ],</p><p class="source-code">       [6. , 1. ]])</p><p>We successfully found all four points. Therefore, we have successfully defined a cluster of size <strong class="source-inline">4</strong>. The mean will be the same as before: <strong class="source-inline">[7.625, 0.75]</strong>.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3drUZtE">https://packt.live/3drUZtE</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2YoSu78">https://packt.live/2YoSu78</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>This was a simple clustering example that applied the mean shift algorithm. We only illustrated what the algorithm considers when finding clusters. </p>
			<p>However, there is still one question, and that is what will the value of the radius be?</p>
			<p>Note that if the radius of <strong class="source-inline">2</strong> was not set, we could simply start either with a huge radius that includes all data points and then reduce the radius, or we could start with a tiny radius, making sure that each data point is in its cluster, and then increase the radius until we get the desired result.</p>
			<p>In the next section, we will be looking at the mean shift algorithm but using scikit-learn.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor173"/>The Mean Shift Algorithm in scikit-learn</h2>
			<p>Let's use the same data points we used with the k-means algorithm:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">data_points = np.array([[1, 1], [1, 1.5], [2, 2], \</p>
			<p class="source-code">                        [8, 1], [8, 0], [8.5, 1], \</p>
			<p class="source-code">                        [6, 1], [1, 10], [1.5, 10], \</p>
			<p class="source-code">                        [1.5, 9.5], [10, 10], [1.5, 8.5]])</p>
			<p>The syntax of the mean shift clustering algorithm is like the syntax for the k-means clustering algorithm:</p>
			<p class="source-code">from sklearn.cluster import MeanShift</p>
			<p class="source-code">mean_shift_model = MeanShift()</p>
			<p class="source-code">mean_shift_model.fit(data_points)</p>
			<p>Once the clustering is done, we can access the center point of each cluster:</p>
			<p class="source-code">mean_shift_model.cluster_centers_</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([[ 1.375     ,  9.5       ],</p>
			<p class="source-code">       [ 8.16666667,  0.66666667],</p>
			<p class="source-code">       [ 1.33333333,  1.5       ],</p>
			<p class="source-code">       [10.        , 10.        ],</p>
			<p class="source-code">       [ 6.        ,  1.        ]])</p>
			<p>The mean shift model found five clusters with the centers shown in the preceding code.</p>
			<p>Like k-means, we can also get the labels:</p>
			<p class="source-code">mean_shift_model.labels_</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([2, 2, 2, 1, 1, 1, 4, 0, 0, 0, 3, 0], dtype=int64)</p>
			<p>The output array shows which data point belongs to which cluster. This is all we need to plot the data:</p>
			<p class="source-code">import matplotlib.pyplot as plot</p>
			<p class="source-code">plot.scatter(mean_shift_model.cluster_centers_[:,0], \</p>
			<p class="source-code">             mean_shift_model.cluster_centers_[:,1])</p>
			<p class="source-code">for i in range(len(data_points)): </p>
			<p class="source-code">    plot.plot(data_points[i][0], data_points[i][1], \</p>
			<p class="source-code">              ['k+','kx','kv', 'k_', 'k1']\</p>
			<p class="source-code">              [mean_shift_model.labels_[i]])</p>
			<p class="source-code">plot.show()</p>
			<p>In the preceding code snippet, we made a plot of the data points and the centers of the five clusters. Each data point belonging to the same cluster will have the same marker. The cluster centers are marked as a dot.</p>
			<p>The expected output is this:</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B16060_05_09.jpg" alt="Figure 5.9: Graph showing the data points of the five clusters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9: Graph showing the data points of the five clusters</p>
			<p>We can see that three clusters contain more than a single dot (the top left, the bottom left, and the bottom right). The two single data points that are also their own cluster can be seen as outliers, as mentioned previously, as they are too far from the other clusters to be part of any of them.</p>
			<p>Now that we have learned about the mean shift algorithm, we can have look at hierarchical clustering, and more specifically at agglomerative hierarchical clustering (the <em class="italic">bottom-up</em> approach).</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor174"/>Hierarchical Clustering</h2>
			<p>Hierarchical clustering algorithms fall into two categories: </p>
			<ul>
				<li>Agglomerative (or bottom-up) hierarchical clustering</li>
				<li>Divisive (or top-down) hierarchical clustering</li>
			</ul>
			<p>We will only talk about agglomerative hierarchical clustering in this chapter, as it is the most widely used and most efficient of the two approaches.</p>
			<p>Agglomerative hierarchical clustering treats each data point as a single cluster in the beginning and then successively merges (or agglomerates) the closest clusters together in pairs. In order to find the closest data clusters, agglomerative hierarchical clustering uses a heuristic such as the Euclidean or Manhattan distance to define the distance between data points. A linkage function will also be required to aggregate the distance between data points in clusters in order to define a unique value of the closeness of clusters. </p>
			<p>Examples of linkage functions include single linkage (simple distance), average linkage (average distance), maximum linkage (maximum distance), and Ward linkage (square difference). The pairs of clusters with the smallest value of linkage will be grouped together. The grouping is repeated until we reach a single cluster containing every data point. In the end, this algorithm terminates when there is only a single cluster left.</p>
			<p>In order to visually represent the hierarchy of clusters, a dendrogram can be used. A dendrogram is a tree where the leaves at the bottom represent data points. Each intersection between two leaves is the grouping of these two leaves. The root (top) represents a unique cluster that contains all the data points. Have a look at <em class="italic">Figure 5.10</em>, which represents a dendrogram.</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B16060_05_10.jpg" alt="Figure 5.10: Example of a dendrogram&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10: Example of a dendrogram</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor175"/>Agglomerative Hierarchical Clustering in scikit-learn</h2>
			<p>Have a look at the following example, where we use the same data points as we used with the k-means algorithm:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">data_points = np.array([[1, 1], [1, 1.5], [2, 2], \</p>
			<p class="source-code">                        [8, 1], [8, 0], [8.5, 1], \</p>
			<p class="source-code">                        [6, 1], [1, 10], [1.5, 10], \</p>
			<p class="source-code">                        [1.5, 9.5], [10, 10], [1.5, 8.5]])</p>
			<p>In order to plot a dendrogram, we need to first import the <strong class="source-inline">scipy</strong> library:</p>
			<p class="source-code">from scipy.cluster.hierarchy import dendrogram</p>
			<p class="source-code">import scipy.cluster.hierarchy as sch</p>
			<p>Then we can plot a dendrogram using SciPy with the <strong class="source-inline">ward</strong> linkage function, as it is the most commonly used linkage function:</p>
			<p class="source-code">dendrogram = sch.dendrogram(sch.linkage(data_points, \</p>
			<p class="source-code">                            method='ward'))</p>
			<p>The output of the dendrogram will be as follows:</p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B16060_05_11.jpg" alt="Figure 5.11: Dendrogram based on random data points&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.11: Dendrogram based on random data points</p>
			<p>With the dendrogram, we can generally guess what will be a good number of clusters by simply drawing a horizontal line as shown in <em class="italic">Figure 5.12</em>, in the area with the highest vertical distance, and counting the number of intersections. In this case, it should be two clusters, but we will go to the next biggest area as two is too small a number.</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B16060_05_12.jpg" alt="Figure 5.12: Division on clusters in the dendrogram&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.12: Division on clusters in the dendrogram</p>
			<p>The <em class="italic">y</em> axis represents the measure of closeness, and the <em class="italic">x</em> axis represents the index of each data point. So our first three data points (<strong class="source-inline">0,1,2</strong>) are parts of the same cluster, then another cluster is made of the next four points (<strong class="source-inline">3,4,5,6</strong>), data point <strong class="source-inline">10</strong> is a cluster on its own, and the remaining data points (<strong class="source-inline">7,8,9,11</strong>) form the last cluster.</p>
			<p>The syntax of the agglomerative hierarchical clustering algorithm is similar to the k-means clustering algorithm except that we need to specify the number type of <strong class="source-inline">affinity</strong> (here, we choose the Euclidean distance) and the linkage (here, we choose the <strong class="source-inline">ward</strong> linkage):</p>
			<p class="source-code">from sklearn.cluster import AgglomerativeClustering</p>
			<p class="source-code">agglomerative_model = AgglomerativeClustering(n_clusters=4, \</p>
			<p class="source-code">                                              affinity='euclidean', \</p>
			<p class="source-code">                                              linkage='ward')</p>
			<p class="source-code">agglomerative_model.fit(data_points)</p>
			<p>The output is:</p>
			<p class="source-code">AgglomerativeClustering(affinity='euclidean', </p>
			<p class="source-code">                        compute_full_tree='auto',</p>
			<p class="source-code">                        connectivity=None, </p>
			<p class="source-code">                        distance_threshold=None,</p>
			<p class="source-code">                        linkage='ward', memory=None,</p>
			<p class="source-code">                        n_clusters=4, pooling_func='deprecated')</p>
			<p>Similar to k-means, we can also get the labels as shown in the following code snippet:</p>
			<p class="source-code">agglomerative_model.labels_</p>
			<p>The expected output is this:</p>
			<p class="source-code">array([2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 3, 1], dtype=int64)</p>
			<p>The output array shows which data point belongs to which cluster. This is all we need to plot the data:</p>
			<p class="source-code">import matplotlib.pyplot as plot</p>
			<p class="source-code">for i in range(len(data_points)): </p>
			<p class="source-code">    plot.plot(data_points[i][0], data_points[i][1], \</p>
			<p class="source-code">              ['k+','kx','kv', 'k_'][agglomerative_model.labels_[i]])</p>
			<p class="source-code">plot.show()</p>
			<p>In the preceding code snippet, we made a plot of the data points and the four clusters' centers. Each data point belonging to the same cluster will have the same marker.</p>
			<p>The expected output is this:</p>
			<p> </p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B16060_05_13.jpg" alt="Figure 5.13: Graph showing the data points of the four clusters &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.13: Graph showing the data points of the four clusters </p>
			<p>We can see that, in contrast with the result from the mean shift method, agglomerative clustering was able to properly group the data point at (<strong class="source-inline">6,1</strong>) with the bottom-right cluster instead of having his own cluster. In situations like this one, where we have a very small amount of data, agglomerative hierarchical clustering and mean shift will work better than k-means. However, they have very expensive computational time requirements, which will make them struggle on very large datasets. However, k-means is very fast and is a better choice for very large datasets.</p>
			<p>Now that we have learned about a few different clustering algorithms, we need to start evaluating these models and comparing them in order to choose the best model for clustering.</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor176"/>Clustering Performance Evaluation</h1>
			<p>Unlike supervised learning, where we always have the labels to evaluate our predictions with, unsupervised learning is a bit more complex as we do not usually have labels. In order to evaluate a clustering model, two approaches can be taken depending on whether the label data is available or not:</p>
			<ul>
				<li>The first approach is the extrinsic method, which requires the existence of label data. This means that in absence of label data, human intervention is required in order to label the data or at least a subset of it. </li>
				<li>The other approach is the intrinsic approach. In general, the extrinsic approach tries to assign a score to clustering, given the label data, whereas the intrinsic approach evaluates clustering by examining how well the clusters are separated and how compact they are.<p class="callout-heading">Note</p><p class="callout">We will skip the mathematical explanations as they are quite complicated.</p><p class="callout">You can find more mathematical details on the sklearn website at this URL: <a href="https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation ">https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation</a></p></li>
			</ul>
			<p>We will begin with the extrinsic approach (as it is the most widely used method) and define the following scores using sklearn on our k-means example:</p>
			<ul>
				<li>The adjusted Rand index</li>
				<li>The adjusted mutual information</li>
				<li>The homogeneity</li>
				<li>The completeness</li>
				<li>The V-Measure</li>
				<li>The Fowlkes-Mallows score</li>
				<li>The contingency matrix</li>
			</ul>
			<p>Let's have a look at an example in which we first need to import the <strong class="source-inline">metrics</strong> module from <strong class="source-inline">sklearn.cluster</strong>:</p>
			<p class="source-code">from sklearn import metrics</p>
			<p>We will be reusing the code from our k-means example in <em class="italic">Exercise 5.01</em>, <em class="italic">Implementing K-Means in scikit-learn:</em></p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import matplotlib.pyplot as plot</p>
			<p class="source-code">from sklearn.cluster import KMeans</p>
			<p class="source-code">data_points = np.array([[1, 1], [1, 1.5], [2, 2], \</p>
			<p class="source-code">                        [8, 1], [8, 0], [8.5, 1], \</p>
			<p class="source-code">                        [6, 1], [1, 10], [1.5, 10], \</p>
			<p class="source-code">                        [1.5, 9.5], [10, 10], [1.5, 8.5]])</p>
			<p class="source-code">k_means_model = KMeans(n_clusters=3,random_state = 8)</p>
			<p class="source-code">k_means_model.fit(data_points)</p>
			<p class="source-code">k_means_model.labels_</p>
			<p>The output of our predicted labels using <strong class="source-inline">k_means_model.labels_</strong> was:</p>
			<p class="source-code">array([2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1, 1])</p>
			<p>Finally, define the true labels of this dataset, as shown in the following code snippet:</p>
			<p class="source-code">data_labels = np.array([0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 3, 1])</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor177"/>The Adjusted Rand Index</h2>
			<p>The adjusted Rand index is a function that measures the similarity between the cluster predictions and the labels while ignoring permutations. The adjusted Rand index works quite well when the labels are large equal-sized clusters.</p>
			<p>The adjusted Rand index has a range between <strong class="bold">[-1.1]</strong>, where negative values are not desirable. A negative score means that our model is performing worse than if we were to randomly assign labels. If we were to randomly assign them, our score would be close to 0. However, the closer we are to 1, the better our clustering model is at predicting the right label.</p>
			<p>With <strong class="source-inline">sklearn</strong>, we can easily compute the adjusted Rand index by using this code:</p>
			<p class="source-code">metrics.adjusted_rand_score(data_labels, k_means_model.labels_)</p>
			<p>The expected output is this:</p>
			<p class="source-code">0.8422939068100358</p>
			<p>In this case, the adjusted Rand index indicates that our k-means model is not far from our true labels.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor178"/>The Adjusted Mutual Information</h2>
			<p>The adjusted mutual information is a function that measures the entropy between the cluster predictions and the labels while ignoring permutations.</p>
			<p>The adjusted mutual information has no defined range, but negative values are considered bad. The closer we are to 1, the better our clustering model is at predicting the right label.</p>
			<p>With <strong class="source-inline">sklearn</strong>, we can easily compute it by using this code:</p>
			<p class="source-code">metrics.adjusted_mutual_info_score(data_labels, \</p>
			<p class="source-code">                                   k_means_model.labels_)</p>
			<p>The expected output is this:</p>
			<p class="source-code">0.8769185235006342</p>
			<p>In this case, the adjusted mutual information indicates that our k-means model is quite good and not far from our true labels.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor179"/>The V-Measure, Homogeneity, and Completeness </h2>
			<p>The V-Measure is defined as the harmonic mean of homogeneity and completeness. The harmonic mean is a type of average (other types are the arithmetic mean and the geometric mean) using reciprocals (a reciprocal is the inverse of a number. For example the reciprocal of 2 is <img src="image/B16060_05_13a.png" alt="formula"/>, and the reciprocal of 3 is <img src="image/B16060_05_13b.png" alt="37"/>). </p>
			<p>The formula of the harmonic mean is as follows:</p>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/B16060_05_14.jpg" alt="Figure 5.14: The harmonic mean formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.14: The harmonic mean formula</p>
			<p><img src="image/B16060_05_14a.png" alt="formula 3"/> is the number of values and <img src="image/B16060_05_14b.png" alt="formula 4"/> is the value of each point.</p>
			<p>In order to calculate the V-Measure, we first need to define homogeneity and completeness.</p>
			<p>Perfect homogeneity refers to a situation where each cluster has data points belonging to the same label. The homogeneity score will reflect how well each of our clusters is grouping data from the same label.</p>
			<p>Perfect completeness refers to the situation where all data points belonging to the same label are clustered into the same cluster. The homogeneity score will reflect how well, for each of our labels, its data points are all grouped inside the same cluster.</p>
			<p>Hence, the formula of V-Measure is as follows:</p>
			<div>
				<div id="_idContainer166" class="IMG---Figure">
					<img src="image/B16060_05_15.jpg" alt="Figure 5.15: The V-Measure formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.15: The V-Measure formula</p>
			<p><img src="image/B16060_05_15a.png" alt="formula 5"/> has a default value of <strong class="bold">1</strong>, but it can be changed to further emphasize either homogeneity or completeness.</p>
			<p>These three scores have a range between [<strong class="bold">0,1</strong>], with <strong class="bold">0</strong> being the worst possible score and <strong class="bold">1</strong> being the perfect score.</p>
			<p>With <strong class="source-inline">sklearn</strong>, we can easily compute these three scores by using this code:</p>
			<p class="source-code">metrics.homogeneity_score(data_labels, k_means_model.labels_)</p>
			<p class="source-code">metrics.completeness_score(data_labels, k_means_model.labels_)</p>
			<p class="source-code">metrics.v_measure_score(data_labels, k_means_model.labels_, \</p>
			<p class="source-code">                        beta=1)</p>
			<p>The output of <strong class="source-inline">homogeneity_score</strong> is as follows:</p>
			<p class="source-code">0.8378758055108827</p>
			<p>In this case, the homogeneity score indicates that our k-means model has clusters containing different labels.</p>
			<p>The output of <strong class="source-inline">completeness_score</strong> is as follows:</p>
			<p class="source-code">1.0</p>
			<p>In this case, the completeness score indicates that our k-means model has successfully put every data point of each label inside the same cluster.</p>
			<p>The output of <strong class="source-inline">v_measure_score</strong> is as follows:</p>
			<p class="source-code">0.9117871871412709</p>
			<p>In this case, the V-Measure indicates that our k-means model, while not being perfect, has a good score in general.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor180"/>The Fowlkes-Mallows Score</h2>
			<p>The Fowlkes-Mallows score is a metric measuring the similarity within a label cluster and the prediction of the cluster, and this is defined as the geometric mean of the precision and recall (you learned about this in <em class="italic">Chapter 4</em>,<em class="italic"> An Introduction to Decision Trees</em>).</p>
			<p>The formula of the Fowlkes-Mallows score is as follows:</p>
			<div>
				<div id="_idContainer168" class="IMG---Figure">
					<img src="image/B16060_05_16.jpg" alt="Figure 5.16: The Fowlkes-Mallows formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.16: The Fowlkes-Mallows formula</p>
			<p>Let's break this down:</p>
			<ul>
				<li>True positive (or <em class="italic">TP</em>): Are all the observations where the predictions are in the same cluster as the label cluster</li>
				<li>False positive (or <em class="italic">FP</em>): Are all the observations where the predictions are in the same cluster but not the same as the label cluster</li>
				<li>False negative (or <em class="italic">FN</em>): Are all the observations where the predictions are not in the same cluster but are in the same label cluster</li>
			</ul>
			<p>The Fowlkes-Mallows score has a range between [<strong class="bold">0, 1</strong>], with <strong class="bold">0</strong> being the worst possible score and <strong class="bold">1</strong> being the perfect score.</p>
			<p>With <strong class="source-inline">sklearn</strong>, we can easily compute it by using this code:</p>
			<p class="source-code">metrics.fowlkes_mallows_score(data_labels, k_means_model.labels_)</p>
			<p>The expected output is this:</p>
			<p class="source-code">0.8885233166386386</p>
			<p>In this case, the Fowlkes-Mallows score indicates that our k-means model is quite good and not far from our true labels.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor181"/>The Contingency Matrix</h2>
			<p>The contingency matrix is not a score, but it reports the intersection cardinality for every true/predicted cluster pair and the required label data. It is very similar to the <em class="italic">Confusion Matrix</em> seen in <em class="italic">Chapter 4</em>, <em class="italic">An Introduction to Decision Trees</em>. The matrix must be the same for the label and cluster name, so we need to be careful to give our cluster the same name as our label, which was not the case with the previously seen scores.</p>
			<p>We will modify our labels from this:</p>
			<p class="source-code">data_labels = np.array([0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 3, 1])</p>
			<p>To this:</p>
			<p class="source-code">data_labels = np.array([2, 2, 2, 1, 1, 1, 1, 0, 0, 0, 3, 0])</p>
			<p>Then, with <strong class="source-inline">sklearn</strong>, we can easily compute the contingency matrix by using this code:</p>
			<p class="source-code">from sklearn.metrics.cluster import contingency_matrix</p>
			<p class="source-code">contingency_matrix(k_means_model.labels_,data_labels)</p>
			<p>The output of <strong class="source-inline">contingency_matrix</strong> is as follows:</p>
			<p class="source-code">array([[0, 4, 0, 0],</p>
			<p class="source-code">       [4, 0, 0, 1],</p>
			<p class="source-code">       [0, 0, 3, 0]])</p>
			<p>The first row of the <strong class="source-inline">contingency_matrix</strong> output indicates that there are <strong class="source-inline">4</strong> data points whose true cluster is the first cluster (<strong class="source-inline">0</strong>). The second row indicates that there are also four data points whose true cluster is the second cluster (<strong class="source-inline">1</strong>); however, an extra <strong class="source-inline">1</strong> was incorrectly predicted in this cluster, but it belongs to the fourth cluster (<strong class="source-inline">3</strong>). The third row indicates that there are three data points whose true cluster is the third cluster (<strong class="source-inline">2</strong>).</p>
			<p>We will now look at the intrinsic approach, which is required when we do not have the label. We will define the following scores using sklearn on our k-means example:</p>
			<ul>
				<li>The Silhouette Coefficient</li>
				<li>The Calinski-Harabasz index</li>
				<li>The Davies-Bouldin index</li>
			</ul>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor182"/>The Silhouette Coefficient</h2>
			<p>The Silhouette Coefficient is an example of an intrinsic evaluation. It measures the similarity between a data point and its cluster when compared to other clusters.</p>
			<p>It comprises two scores:</p>
			<ul>
				<li><strong class="source-inline">a</strong>: The average distance between a data point and all other data points in the same cluster.</li>
				<li><strong class="source-inline">b</strong>: The average distance between a data point and all the data points in the nearest cluster.</li>
			</ul>
			<p>The Silhouette Coefficient formula is:</p>
			<div>
				<div id="_idContainer169" class="IMG---Figure">
					<img src="image/B16060_05_17.jpg" alt="Figure 5.17: The Silhouette Coefficient formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.17: The Silhouette Coefficient formula</p>
			<p>The Silhouette Coefficient has a range between [<strong class="bold">-1,1</strong>], with <strong class="bold">-1</strong> meaning an incorrect clustering. A score close to zero indicates that our clusters are overlapping. A score close to <strong class="bold">1</strong> indicates that all the data points are assigned to the appropriate clusters.</p>
			<p>Then, with <strong class="source-inline">sklearn</strong>, we can easily compute the silhouette coefficient by using this code:</p>
			<p class="source-code">metrics.silhouette_score(data_points, k_means_model.labels_)</p>
			<p>The output of <strong class="source-inline">silhouette_score</strong> is as follows:</p>
			<p class="source-code">0.6753568188872228</p>
			<p>In this case, the Silhouette Coefficient indicates that our k-means model has some overlapping clusters, and some improvements can be made by separating some of the data points from one of the clusters.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor183"/>The Calinski-Harabasz Index</h2>
			<p>The Calinski-Harabasz index measures how the data points inside each cluster are spread. It is defined as the ratio of the variance between clusters and the variance inside each cluster. The Calinski-Harabasz index doesn't have a range and starts from <strong class="bold">0</strong>. The higher the score is, the denser our clusters are. A dense cluster is an indication of a well-defined cluster.</p>
			<p>With <strong class="source-inline">sklearn</strong>, we can easily compute it by using this code:</p>
			<p class="source-code">metrics.calinski_harabasz_score(data_points, k_means_model.labels_)</p>
			<p>The output of <strong class="source-inline">calinski_harabasz_score</strong> is as follows:</p>
			<p class="source-code">19.52509172315154</p>
			<p>In this case, the Calinski-Harabasz index indicates that our k-means model clusters are quite spread out and suggests that we might have overlapping clusters.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor184"/>The Davies-Bouldin Index</h2>
			<p>The Davies-Bouldin index measures the average similarity between clusters. The similarity is a ratio of the distance between a cluster and its closest cluster and the average distance between each data point of a cluster and it's cluster's center. The Davies-Bouldin index doesn't have a range and starts from <strong class="bold">0</strong>. The closer the score is to <strong class="bold">0</strong> the better; it means the clusters are well separated, which is an indication of a good cluster.</p>
			<p>With <strong class="source-inline">sklearn</strong>, we can easily compute the Davis-Bouldin index by using this code:</p>
			<p class="source-code">metrics.davies_bouldin_score(data_points, k_means_model.labels_)</p>
			<p>The output of <strong class="source-inline">davies_bouldin_score</strong> is as follows:</p>
			<p class="source-code">0.404206621415983</p>
			<p>In this case, the Calinski-Harabasz score indicates that our k-means model has some overlapping clusters and an improvement could be made by better separating some of the data points in one of the clusters.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor185"/>Activity 5.02: Clustering Red Wine Data Using the Mean Shift Algorithm and Agglomerative Hierarchical Clustering</h2>
			<p>In this activity, you will work on the Wine Quality dataset and, more specifically, on red wine data. This dataset contains data on the quality of 1,599 red wines and the results of their chemical tests.</p>
			<p>Your goal will be to build two clustering models (using the mean shift algorithm and agglomerative hierarchical clustering) in order to identify whether wines of similar quality also have similar physicochemical properties. You will also have to evaluate and compare the two clustering models using extrinsic and intrinsic approaches.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be found at the following URL: <a href="https://archive.ics.uci.edu/ml/datasets/Wine+Quality">https://archive.ics.uci.edu/ml/datasets/Wine+Quality</a>.</p>
			<p class="callout">The dataset file can be found on our GitHub repository at <a href="https://packt.live/2YYsxuu">https://packt.live/2YYsxuu</a>.</p>
			<p class="callout">Citation: <em class="italic">P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009</em>.</p>
			<p>The following steps will help you complete the activity:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Load the dataset as a DataFrame with <strong class="source-inline">sep = ";"</strong> and inspect the data.</li>
				<li>Create a mean shift clustering model, then retrieve the model's predicted labels and the number of clusters created.</li>
				<li>Create an agglomerative hierarchical clustering model after creating a dendrogram and selecting the optimal number of clusters.</li>
				<li>Retrieve the labels from the first clustering model.</li>
				<li>Compute the following extrinsic approach scores for both models:<p>The adjusted Rand index</p><p>The adjusted mutual information</p><p>The V-Measure</p><p>The Fowlkes-Mallows score</p></li>
				<li>Compute the following intrinsic approach scores for both models:<p>The Silhouette Coefficient</p><p>The Calinski-Harabasz index</p><p>The Davies-Bouldin index</p></li>
			</ol>
			<p>The expected output is this:</p>
			<p>The values of each score for the mean shift clustering model will be as follows:</p>
			<ul>
				<li>The adjusted Rand index: <strong class="source-inline">0.0006771608724007207</strong></li>
				<li>The adjusted mutual information: <strong class="source-inline">0.004837187596124968</strong></li>
				<li>The V-Measure: <strong class="source-inline">0.021907254751144124</strong></li>
				<li>The Fowlkes-Mallows score: <strong class="source-inline">0.5721233634622408</strong></li>
				<li>The Silhouette Coefficient: <strong class="source-inline">0.32769323700400077</strong></li>
				<li>The Calinski-Harabasz index: <strong class="source-inline">44.62091774102674</strong></li>
				<li>The Davies-Bouldin index: <strong class="source-inline">0.8106334674570222</strong></li>
			</ul>
			<p>The values of each score for the agglomerative hierarchical clustering will be as follows:</p>
			<ul>
				<li>The adjusted Rand index: <strong class="source-inline">0.05358047852603172</strong></li>
				<li>The adjusted mutual information: <strong class="source-inline">0.05993098663692826</strong></li>
				<li>The V-Measure: <strong class="source-inline">0.07549735446050691</strong></li>
				<li>The Fowlkes-Mallows score: <strong class="source-inline">0.3300681478007641</strong></li>
				<li>The Silhouette Coefficient: <strong class="source-inline">0.1591882574407987</strong></li>
				<li>The Calinski-Harabasz index: <strong class="source-inline">223.5171774491095</strong></li>
				<li>The Davies-Bouldin index: <strong class="source-inline">1.4975443816135114</strong><p class="callout-heading">Note</p><p class="callout">The solution to this activity is available on page 368.</p></li>
			</ul>
			<p>By completing this activity, you performed mean shift and agglomerative hierarchical clustering on multiple columns for many products. You also learned how to evaluate a clustering model with an extrinsic and intrinsic approach. Finally, you used the results of your models and their evaluation to find an answer to a real-world problem.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor186"/>Summary</h1>
			<p>In this chapter, we learned the basics of how clustering works. Clustering is a form of unsupervised learning where the features are given, but not the labels. It is the goal of the clustering algorithms to find the labels based on the similarity of the data points.</p>
			<p>We also learned that there are two types of clustering, flat and hierarchical, with the first type requiring the number of clusters to find, whereas the second type finds the optimal number of clusters itself.</p>
			<p>The k-means algorithm is an example of flat clustering, whereas mean shift and agglomerative hierarchical clustering are examples of a hierarchical clustering algorithm.</p>
			<p>We also learned about the numerous scores to evaluate the performance of a clustering model, with the labels in the extrinsic approach or without the labels in the intrinsic approach.</p>
			<p>In <em class="italic">Chapter 6</em>, <em class="italic">Neural Networks and Deep Learning</em>, you will be introduced to a field that has become popular in this decade due to the explosion of computation power and cheap, scalable online server capacity. This field is the science of neural networks and deep learning.</p>
		</div>
		<div>
			<div id="_idContainer171" class="Content">
			</div>
		</div>
	</body></html>