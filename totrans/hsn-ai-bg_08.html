<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p>Along with generative networks, reinforcement learning algorithms have provided the most visible advances in <strong>Artificial Intelligence</strong> (<strong>AI</strong>) today. For many years, computer scientists have worked toward creating algorithms and machines that can perceive and react to their environment like a human would. Reinforcement learning is a manifestation of that, giving us the wildly popular AlphaGo and self-driving cars. In this chapter, we'll cover the foundations of reinforcement learning that will allow us to create advanced artificial agents later in this book. </p>
<p>Reinforcement learning plays off the human notion of learning from experience.<span> Like generative models, it learns based on </span><strong>evaluative feedback</strong><span>. Unlike instructive feedback ...</span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be utilizing TensorFlow in Python. We will also be using the OpenAI gym to test our algorithms. </p>
<p>OpenAI gym is an open source toolkit for developing and testing reinforcement learning algorithms. Written in Python, there are environments from Atari games to robot simulations. As we develop reinforcement learning algorithms in this chapter and later chapters, gym will give us access to test environments that would otherwise be very complicated to construct on our own.</p>
<p>You will need either a macOS or Linux environment to run gym. You can install gym by running a simple <kbd>pip install</kbd> command: </p>
<pre><strong>pip install gym</strong></pre>
<p>You should now have gym installed! If you've run into an error, you may have dependency issues. Check the official gym GitHub repo for the latest dependencies (<a href="https://github.com/openai/gym">https://github.com/openai/gym</a>).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Principles of reinforcement learning</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is based on the concept of learning from interaction with a surrounding environment and consequently rewarding positive actions taken in that environment. In reinforcement learning, we refer to our algorithm as the<span> </span><strong>agent</strong><span> </span>because it takes action on the world around it:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-292 image-border" src="Images/bb5e4352-6aa7-4258-adf7-dade399596c8.png" style="width:19.67em;height:9.25em;" width="1041" height="488"/></div>
<p>When an agent takes an action, it receives a reward or penalty depending on whether it took the <em>correct </em>action or not. Our goal in reinforcement learning is to let the agent learn to take actions that maximize the rewards it receives from its environment. These concepts are not at all new; in fact, they've been around ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Markov processes</h1>
                </header>
            
            <article>
                
<p>At the crux of reinforcement learning is the <strong>Markov Decision process</strong> (<strong>MDP</strong>). Markov processes are random strings of events where the future probabilities of events happening are determined by the probability of the most recent event. They extend the basic Markov Chain by adding rewards and decisions to the process. The fundamental problem of reinforcement learning can be modeled as an MDP. <strong>Markov models</strong> are a general class of models that are utilized to solve MDPs. </p>
<p><strong>Markov models</strong> rely on a very important property, called the <strong>Markov property</strong>, where the current state in a Markov process completely characterizes and explains the state of the world at that time; everything we need to know about predicting future events is dependent on where we are in the process. For instance, the following Markov process models the state of the stock market at any given time. There are three states– a <strong>Bull market</strong>, a <strong>Bear market</strong>, or a <strong>Stagnant market </strong>– and the respective probabilities for staying in each <strong>state</strong> or transitioning to another state: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-323 image-border" src="Images/5df44dcc-f24f-45d3-94cd-e6f3b9cc3cd1.png" style="width:22.58em;height:16.75em;" width="400" height="296"/></div>
<p>The entity that navigates an MDP is called an <strong>agent</strong>. In this case, the agent would be the stock market itself. We can remember the parameters of the Markov process by SAP: </p>
<ul>
<li><strong>Set of possible states</strong> (<strong>S</strong>):<strong> </strong>The possible states of being that an agent can be in at any given time. When we talk about states in reinforcement learning, this is what we are referring to.</li>
<li><strong>Set of possible actions</strong> (<strong>A</strong>): All of the possible actions that an agent can take in its environment. These are the lines between the states; what actions can happen between two states?</li>
<li><strong>Transition probability</strong> (<strong>P</strong>): The probability of moving to any of the new given states.</li>
</ul>
<p><span>The goal of any reinforcement learning agent is to solve a given MDP by maximizing the <strong>reward</strong> it</span> receives<span> from taking specific actions.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Rewards</h1>
                </header>
            
            <article>
                
<p>As we mentioned previously, reinforcement learning algorithms seek to maximize their potential future reward. In deep learning languages, we call this the expected<span> </span><strong>reward</strong>. At each time step,<span> <em>t</em></span><span>, </span>in the training process of a reinforcement learning algorithm, we want to maximize the return,<span> <em>R</em>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/dddc4ed9-78e8-410a-8c21-4484f8c0a281.png" style="width:34.58em;height:3.42em;" width="5470" height="540"/></p>
<p>Our final reward is the summation of all of the expected rewards at each time step <span>– </span>we call this the <strong>cumulative reward</strong>. Mathematically, we can write the preceding equation as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e8b23352-c032-4d95-a205-32d3e3c353a2.png" style="width:5.75em;height:4.00em;" width="790" height="540"/></div>
<p>Theoretically, this process could go on forever; the termination ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Policies</h1>
                </header>
            
            <article>
                
<p>A policy, simply stated, is a way of acting; your place of employment or education has policies about how, what, and when you can do things. This term is no different when used in the context of reinforcement learning. We use policies to map states to potential actions that a reinforcement learning agent can take. Mathematically speaking, policies in reinforcement learning are represented by the Greek letter π, and they tell an agent what action to take at any given state in an MDP. Let's look at a simple MDP to examine this; imagine that you are up late at night, you are sleepy, but maybe you are stuck into a good movie. Do you stay awake or go to bed? In this scenario, we would have three states: </p>
<ul>
<li>Your initial state of sleepiness</li>
<li>Being well rested</li>
<li>Being sleep deprived</li>
</ul>
<p>Each of these states has a transition probability and reward associated with taking an action based on them. </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-329 image-border" src="Images/838e883c-5989-4ca8-b55f-ff3db83f0249.png" style="width:45.67em;height:12.58em;" width="962" height="264"/></div>
<p>For example, Let's say you decide to stay up. In this scenario, you'll find yourself in the <strong>Don't Sleep</strong> transition state. From here, there is only one place to go - a 100% chance of ending up in the <strong>Sleep Deprived</strong> State. </p>
<p>If you slept, there is a 90% chance of you being well rested and a 10% chance of you still being tired. You could return to your tired state by not sleeping as well. In this scenario, we'd want to choose the actions (sleeping) that maximize our rewards. That voice in your head that's telling you to go to sleep is the policy. </p>
<p>Our objective is to learn a policy (<img class="fm-editor-equation" src="Images/f069ed33-7df1-4b2e-8f86-580c65a7db09.png" style="width:1.08em;height:1.00em;" width="190" height="170"/>) that maximizes the network's reward; we call this the <strong>optimal policy</strong>. In this case, the optimal policy is <strong>deterministic</strong>, meaning that there is a clear optimal action to take at each state. Policies can also be <strong>stochastic</strong>, meaning that there is a distribution of possible actions that can be drawn from. </p>
<p>Reinforcement learning agents can learn <strong>on-policy</strong> or <strong>off-policy</strong>; when an algorithm is on-policy, it learns the policy from all of the agent's actions, including exploratory actions that it may take. It improves the <em>existing policy</em>. Off-policy learning is <em>off from the previous policy</em>, or in other words, evaluating or learning a policy that was different from the original policy. Off-policy learning happens independent of an agent's previous. Later on in this chapter, we'll discuss two different approaches to reinforcement learning <span>–</span> one on-policy (policy gradients) and one off-policy (Q-learning). </p>
<p>To help our algorithm learn an optimal policy, we utilize <strong>value functions</strong>. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Value functions</h1>
                </header>
            
            <article>
                
<p>A value function helps us measure the expected reward at certain states; it represents the expected cumulative reward from following a certain policy at any given state. There are two types of value functions used in the field of reinforcement learning; <strong>state value functions </strong><em>V</em>(<em>s</em>) and <strong>action value functions <img class="fm-editor-equation" src="Images/c08de824-20ac-4874-a897-77581021565b.png" style="width:3.00em;height:1.17em;" width="560" height="220"/></strong>.</p>
<p>The state value function describes the value of a state when following a policy. It is the expected return that an agent will achieve when starting at state <em>s</em> under a policy π. This function will give us the expected reward for an agent given that it starts following a policy at state<em>s</em>:</p>

<p>Let's break down what this function ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Bellman equation</h1>
                </header>
            
            <article>
                
<p>As one of the most important equations in the entire field of reinforcement learning, the Bellman equation is the cornerstone of solving reinforcement learning problems. Developed by applied mathematician Richard Bellman, it's less of a equation and more of a condition of optimization that models the reward of an agent's decision at a point in time based on the expected choices and rewards that could come from said decision. The Bellman equation can be derived for either the state value function or the action value function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6b28e04b-517e-4e36-a3b8-a3dac6d176e5.png" style="width:27.08em;height:4.00em;" width="3560" height="530"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/43056f1b-558f-4419-9e41-5644a86bc786.png" style="width:27.08em;height:4.17em;" width="3440" height="530"/></p>
<p>As usual, let's break down these equations. We're going to focus on the state value function. First, we have the summation of all policies for every state/action pair: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/bd04bdaf-5aa1-4575-9faa-20dd81d56f38.png" style="width:5.75em;height:2.92em;" width="820" height="420"/></p>
<p>Next, we have the <span>transition probability; it's the probability of being in state <em>s</em>, taking action <em>a</em>, and ending up in state </span><img class="fm-editor-equation" src="Images/825e8573-ef10-4033-8474-2e48bda6425a.png" style="width:0.83em;height:1.17em;" width="140" height="180"/>:</p>
<p class="CDPAlignCenter CDPAlign"><img style="font-size: 1em;width:2.42em;height:1.67em;" class="fm-editor-equation" src="Images/f6d0db5d-ba0c-4f29-9cef-64e094924511.png" width="300" height="210"/></p>
<p>Next is the cumulative reward that we discussed earlier:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/29d7d849-d342-487c-b4b3-32ff1fbec440.png" style="width:2.42em;height:1.67em;" width="320" height="210"/></p>
<p>Lastly, we have the discounted value of the function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/568aee06-6340-4d42-90c5-515f1b87864e.png" style="width:4.42em;height:1.58em;" width="640" height="230"/></p>
<p>Altogether, we're describing the entire reinforcement learning process; we want to find a state value function or an action value function that satisfies the Bellman equation. We're missing one key part here; how do we solve this in practice? One option is to use a paradigm called <strong>dynamic programming</strong>, which is an optimization method that was also developed by Bellman himself. One means of solving for the optimal policy with dynamic programming is to use the<span> </span><strong>value iteration method</strong>. In this manner, we use the Bellman equation as an iterative update function. We want to converge from Q to Q* by enforcing the Bellman equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/9ae2e7b8-896c-4e3d-b9b1-1921a1ea5b86.png" style="width:23.92em;height:3.00em;" width="3510" height="450"/></p>
<p>Let's see how this would work in Python by trying to solve the cartpole problem. With value iteration, we start with a random value function and then find an improved value function in an iterative process until we reach an optimal value function.</p>
<p>We can attempt this on the cartpole problem. Let's import <kbd>gym</kbd> and generate a random value function to begin with:</p>
<pre>import gym<br/>import numpy as np<br/><br/>def gen_random_policy():<br/> return (np.random.uniform(-1,1, size=4), np.random.uniform(-1,1))</pre>
<p>Next, let's turn that policy into an action:</p>
<pre>def policy_to_action(env, policy, obs):<br/> if np.dot(policy[0], obs) + policy[1] &gt; 0:<br/> return 1<br/> else:<br/> return 0</pre>
<p>Lastly, we can run the training process:</p>
<pre>def run_episode(env, policy, t_max=1000, render=False):<br/> obs = env.reset()<br/> total_reward = 0<br/> for i in range(t_max):<br/> if render:<br/> env.render()<br/> selected_action = policy_to_action(env, policy, obs)<br/> obs, reward, done, _ = env.step(selected_action)<br/> total_reward += reward<br/> if done:<br/> break<br/> return total_reward<br/><br/>if __name__ == '__main__':<br/> env = gym.make('CartPole-v0')<br/> <br/> n_policy = 500<br/> policy_list = [gen_random_policy() for _ in range(n_policy)]<br/> <br/> scores_list = [run_episode(env, p) for p in policy_list]<br/> <br/> print('Best policy score = %f' %max(scores_list))<br/> <br/> best_policy= policy_list[np.argmax(scores_list)]<br/> print('Running with best policy:\n')<br/> run_episode(env, best_policy, render=True)</pre>
<p>While value iterations work in this simplistic environment, we<span> can quickly run into problems when utilizing it in larger, more complex environments. As we have to individually compute the value for every state/value pair, many unstructured inputs such as images become impossibly large. Imagine how expensive it would be to compute this function for every single pixel in an advanced video game, every time our reinforcement learning algorithm tried to make a move!</span></p>
<p>For this reason, we utilize deep learning methods to do these computations for us. Deep Neural Networks can act as function approximators. There are two primary methods, called <strong>Deep Q</strong>-<strong>learning</strong> and <strong>policy gradients</strong>. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Q–learning</h1>
                </header>
            
            <article>
                
<p>Q-learning is a reinforcement learning method that utilizes the action value function, or Q function, to solve tasks. In this section, we'll talk about both traditional Q-learning as well as Deep Q-learning.</p>
<p>Standard Q-learning works off the core concept of the Q-table. You can think of the Q-table as a reference table; every row represents a state and every column represents an action. The values of the table are the expected future rewards that are received for a specific combination of actions and states. <span>Procedurally, we do the following:</span></p>
<ol>
<li>Initialize the Q-table</li>
<li>Choose an action</li>
<li>Perform that action</li>
<li>Measure the reward that was received</li>
<li>Update the Q- value</li>
</ol>
<p>Let's walk through each of these steps to better understand the algorithm. ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Policy optimization</h1>
                </header>
            
            <article>
                
<p><strong>Policy optimization methods</strong> are an alternative to Q-learning and value function approximation. Instead of learning the Q-values for state/action pairs, these methods directly learn a policy π that maps state to an action by calculating a gradient. Fundamentally, for a search such as for an <span>optimization problem, p</span><span>olicy methods are a means of learning the correct policy from a stochastic distribution of potential policy actions.</span> <span>Therefore, our network architecture changes a bit to learn a policy directly:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1349 image-border" src="Images/e760c7dd-ebb7-4f86-80a1-99b79ca3f002.png" style="width:23.08em;height:12.33em;" width="828" height="440"/></div>
<p>Because every state has a distribution of possible actions, the optimization problem becomes easier. We no longer have to compute exact rewards for specific actions. <span>Recall that deep learning methods rely on the concept of an episode. In the case of deep reinforcement learning, each episode represents a game or task, while </span><strong>trajectories</strong><span> represent plays or directions within that game or task. We can define a trajectory as a path of state/action/reward combinations, represented by the Greek letter tau (<em>Τ</em>):</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a2ccc04a-71d3-4348-9738-0e06229375e7.png" style="width:25.08em;height:1.58em;" width="3480" height="220"/></p>
<p><span>Think about a robot learning to walk; if we used Q-learning or another dynamic programming method for this task, our algorithms would need to learn exactly how much reward to assign to every single joint movement for every possible trajectory. The algorithm would</span> need<span> to learn timings, exact angles to bend the robotic limbs, and so on. By learning a policy directly, the algorithm can simply focus on the overall task of moving the robot's feet when it walks.</span></p>
<p>When utilizing policy gradient methods, we can define an individual policy in the same simple manner as we did with Q-learning; our policy is the expected sum of future discounted rewards:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/679aca0a-2d0d-4591-baf9-47de01f4025f.png" style="width:12.08em;height:3.67em;" width="1800" height="540"/></div>
<p>Therefore, the goal of our network becomes to <em>maximize</em> some policy, <em>J</em>, in order to maximize our expected future reward:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/effaee75-f1d0-4f0e-a1db-7efaede13cdf.png" style="width:9.08em;height:1.42em;" width="1410" height="220"/></div>
<p>One way to learn a policy is to use gradient methods, hence the name <em>policy</em> gradients. Since we want to find a policy that maximizes reward, we perform the opposite of gradient descent, <em>gradient ascent</em>, on a distribution of potential policies:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-large wp-image-333 image-border" src="Images/9f9110be-7efa-4c04-8d9d-ad8bbee171e1.png" style="width:31.50em;height:15.83em;" width="969" height="487"/></div>
<p>The parameters of the new neural network become the parameters of our policy, so by performing gradient ascent and updating the network parameters, <img class="fm-editor-equation" src="Images/1fe06433-f3bb-4efd-a882-95e32b284d69.png" style="width:1.25em;height:1.08em;" width="190" height="170"/>becomes <img class="fm-editor-equation" src="Images/449d7209-ba25-4190-8b0b-f499112d9a48.png" style="width:1.00em;height:1.00em;" width="170" height="170"/>. <span>Fully deriving policy gradients is outside the scope of this book, however, let's touch upon some high-level concepts of how policy gradient methods work in practice. Take a look at the following landscape; there are three potential paths with three potential end rewards: </span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-336 image-border" src="Images/f98503e9-e293-4df2-a122-80db07f2d5ec.png" style="width:39.50em;height:14.67em;" width="732" height="272"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft"><span>Policy gradient methods seek to increase the probability of taking the path that leads to the highest reward. Mathematically, the standard policy gradient method looks as follows:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/8bf327fa-4f88-42ab-89aa-1a183c69e620.png" style="width:12.92em;height:1.58em;" width="2130" height="260"/></div>
<p>As usual, let's break this down a bit. We'll start by saying that our gradient, <em>g</em>, is the expected value of the gradient <em>times</em> the log of our policy for a given action/state pair, times the <strong>advantage</strong>. Advantage is simply the action value function minus the state value function:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ef3592e7-994c-43bd-9d6b-cd714f9512e0.png" style="width:12.58em;height:1.42em;" width="1960" height="220"/></div>
<p><span>In implementation, vanilla policy gradient methods still run into problems. One particularly notable issue is called the credit assignment problem, in which a reward signal received from a long trajectory of interaction happens at the end of the trajectory. Policy gradient methods have a hard time ascertaining which action caused this reward and finds difficulty </span><em>assigning credit</em><span> for that action. </span></p>
<p><span>Policy gradients are an outline for an entire class of algorithms that can be built around this idea. The key to optimizing these methods is in the details of individual optimization procedures. In the next section, we'll look at several means to improve on vanilla policy gradient methods. </span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Extensions on policy optimization</h1>
                </header>
            
            <article>
                
<p><span>One common way to compute policy gradients is with the <strong>Reinforce</strong> </span><strong>algorithm</strong><span>. Reinforce is a Monte-Carlo policy gradient method that uses likelihood ratios to estimate the value of a policy at a given point. The algorithm can lead to high variance.</span></p>
<p>Vanilla policy gradient methods can be challenging as they are extremely sensitive to what you choose for your step size parameter. Choose a step size too big and the correct policy is overwhelmed by noise <span>–</span> too small and the training becomes incredibly slow. Our next class of reinforcement learning algorithms, <strong>proximal policy optimization</strong> (<strong>PPO</strong>), seeks to remedy this shortcoming of policy gradients. PPO is a new class of reinforcement learning algorithms that was ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned the important foundations of reinforcement learning, one of the most visible practices in the AI field.</p>
<p>Reinforcement learning is based on the concepts of agents acting in an environment and taking action based on what it sees in its surrounding environment. An agent's actions are guided by either policy optimization methods or dynamic programming methods that help it learn how to interact with its environment. We use dynamic programming methods when we care more about exploration and off-policy learning. On the other hand, we use policy optimization methods when we have dense, continuous problem spaces and we only want to optimize for what we care about. </p>
<p>We'll look at several different real-world applications of reinforcement learning in the upcoming chapter.</p>


            </article>

            
        </section>
    </div>



  </body></html>