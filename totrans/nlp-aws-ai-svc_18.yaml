- en: '*Chapter 15*: Classifying Documents and Setting up Human in the Loop for Active
    Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last chapter, we covered how you can use **Amazon Comprehend Custom Entity**
    to extract business entities from your documents, and we showed you how you can
    use humans in the loop with Amazon Augmented AI (A2I) to augment or improve entity
    predictions. Lastly, we showed you how you can retrain the Comprehend custom entity
    model with an augmented dataset to improve accuracy using Amazon A2I.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will talk about how you can use **Amazon Comprehend** custom
    classification to classify documents and then how you can set up active learning
    feedback with your custom classification model using Amazon A2I.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Using comprehend custom classification with human in the loop for active learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the document classification workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you will need access to an AWS account. Please make sure to
    follow the instructions specified in the *Technical requirements* section in [*Chapter
    2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027),*Introducing Amazon Textract*,
    to create your AWS account, and log in to the AWS Management Console before trying
    the steps in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code and sample datasets for setting up Comprehend custom classification
    with a human-in-the-loop solution are in the following link: [https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2015](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2015).'
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action at [https://bit.ly/3BiOjKt](https://bit.ly/3BiOjKt).
  prefs: []
  type: TYPE_NORMAL
- en: Please use the instructions in the following sections along with the code in
    the repository to build the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Using Comprehend custom classification with human in the loop for active learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Comprehend provides the capability to classify the data using Amazon
    Comprehend AutoML and bring your own custom training dataset. You can easily accomplish
    a lot with the Amazon Comprehend custom classification feature as it requires
    fewer documents to train Comprehend AutoML models. You are spending less time
    labeling the dataset and then worrying about setting up infrastructure or choosing
    the right algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: You can use Amazon Comprehend custom classification for a variety of use cases,
    such as classifying documents based on type, classifying news articles, or classifying
    movies based on type.
  prefs: []
  type: TYPE_NORMAL
- en: The fictitious company *LiveRight pvt ltd* wants to classify the documents submitted
    by the customers, such as whether the document submitted is an ID or a bank statement,
    even before analyzing the data inside the document. Moreover, if you are using
    a classification model to classify the documents based on the type of submitted
    document, you would also want to improve the accuracy of your predicted outcome
    in real time, based on the confidence score predicted by the Comprehend custom
    classification model. This is where humans in the loop with Amazon Augmented AI
    is going to help.
  prefs: []
  type: TYPE_NORMAL
- en: We covered Amazon A2I in [*Chapter 13*](B17528_13_Final_SB_ePub.xhtml#_idTextAnchor151),
    *Improving the Accuracy of Document Processing Workflows*. In this chapter, we
    will walk you through some reference architecture on how you can easily set up
    a custom classification model using Amazon Comprehend and have a feedback loop
    set up with Amazon A2I for active learning on your Comprehend custom model.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will walk you through the following architecture on how you can train
    a custom classification model and create a real-time endpoint for inferencing
    or classifying documents in near real time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1 – Comprehend custom classification training workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_15_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.1 – Comprehend custom classification training workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'This architecture walks through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Training documents, such as bank statements or pay stubs, are uploaded to Amazon
    S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amazon Textract extracts text from these documents and then some post-processing
    is done to create a labeled training file for Comprehend custom classification
    training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the training file, an Amazon Comprehend job is created to classify documents,
    such as bank statements or pay stubs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After training is completed, you have two options with Amazon Comprehend: either
    you can do batch inferencing on a batch of documents to classify them or you can
    create real-time endpoints. In the architecture, we are showing how you can set
    up a real-time endpoint to classify a document type.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are going to walk you through the preceding conceptual architecture using
    Jupyter Notebook and a few lines of Python code in the *Setting up to solve the
    use case* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have a near real-time document classification endpoint. We will show
    you how you can set up humans in the loop with this Amazon Comprehend custom classification
    endpoint and set up a model retraining or active-learning loop to improve your
    model accuracy using the following architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.2 – Real-time classification with model retraining'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_15_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.2 – Real-time classification with model retraining
  prefs: []
  type: TYPE_NORMAL
- en: 'In this architecture, we will walk you through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Client application** sends the document to Amazon Textract.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Amazon Textract** extracts the data or text in real-time API and extracted
    data is passed on to the Amazon Comprehend real-time classifier endpoint.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Amazon Comprehend custom classification endpoint classifies this document
    type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This classification endpoint is configured with Amazon A2I human in the loop.
    If the prediction of classification is **high confidence** based on your business
    threshold, which you can configure, the high-confidence predictions are directly
    sent to client applications.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For low-confidence predictions, such as anything below the 95% confidence, the
    score predicted is low confidence for you. A human loop is created, and these
    predictions are sent for human review. Refer to [*Chapter 3*](B17528_03_Final_SB_ePub.xhtml#_idTextAnchor049),
    *Introducing Amazon Comprehend*, to understand what a confidence score is and
    Comprehend custom features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The augmented or corrected data from human labelers are saved in an Amazon S3
    bucket as a **JSON** file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This data is then combined with the original training dataset and the Amazon
    Comprehend custom model is retrained for active learning using human feedback.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will walk you through *steps 1 to 6* using Jupyter Notebook in the *Setting
    up the use case section*. Feel free to combine the augmented classified labels
    with the original dataset and try retraining for your understanding. You can automate
    this architecture using step functions and Lambda functions. We will share with
    you the blogs that can help you set up this architecture using Lambda functions
    in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered the architecture for both model training and retraining
    or active learning. Now, let's move on to the next section to see these concepts
    with code.
  prefs: []
  type: TYPE_NORMAL
- en: Building the document classification workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will get right down to action and start executing the tasks
    to build our solution. But first, there are prerequisites we will have to take
    care of.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up to solve the use case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have not done so in the previous chapters, you will first have to create
    an Amazon SageMaker Jupyter notebook and set up `Chapter 15` folder, and open
    the `chapter15 classify documents with human in the loop.ipynb` notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move to the next section to show you how you can set up the libraries
    and upload training data to Amazon S3 using this notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up and uploading sample documents to Amazon S3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this step, we will follow instructions to set up an S3 bucket and upload
    documents:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the notebook and run the cells below `boto 3` for setup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Move on to the next cell and enter a bucket name to create an S3 bucket in
    your account. Make sure you add the current month and date in `MMDD` for `data_bucket`,
    as shown in the following code block, before executing this cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now run the following cell to upload or copy a sample bank statement or pay
    stub image as a training file from your local notebook to the S3 bucket that you
    just created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now run the next two cells in the notebook to list the training images we just
    copied in Amazon S3\. We created a function named `get_s3_bucket_items`. We are
    getting the image objects from S3 and saving them as images for Textract processing
    in future steps. Refer to the notebook to execute these steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following step to define a path or local directory structure to store
    data extracted from Amazon Textract:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We've covered how to create an S3 bucket and we have loaded training data. Now,
    let's move on to the next section to extract text.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting text from sample documents using Amazon Textract
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Go to the notebook and run the calls in **Step 2: Extract text from sample
    documents using Amazon Textract** to define a function using Amazon Textract to
    extract data from the sample images in Amazon S3\. We are using the DetectDocumentText
    sync API to do this extraction; you can also use *AsyncAPI* or *Textract batch
    APIs* to perform data extraction. Refer to [*Chapter 4*](B17528_04_Final_SB_ePub.xhtml#_idTextAnchor063),
    *Automating Document Processing Workflows*, to dive deep into these APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This function takes the *image's* path and returns the text and labels for the
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s call this function by passing the scanned document''s images by running
    the following cell in the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function extracts the data and saves it in the local directory
    structure you defined in the **Set up and Upload Sample Documents** step. The
    following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.3 – Textract output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_15_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.3 – Textract output
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have extracted the text and associated labels, for example, *0* for
    a bank statement and *1* for pay stubs. Now, let's move to the next section for
    Comprehend training.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Amazon Comprehend classification training job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have extracted the data and labels in the previous step from our sample
    of scanned documents in Amazon S3\. Now, let''s understand how to set up a Comprehend
    classification training job using **Step 3: Create Amazon Comprehend Classification
    training job** in the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first create a function to map the extracted data and labels into a
    pandas DataFrame so that we can convert that into a CSV training file in the next
    step. Run the following code to define the function, which takes the extracted
    data location and returns labels and text from it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will call the function we defined in the previous step by running the
    following cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get a pandas DataFrame with labels and documents, shown as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.4 – Labeled training DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17528_15_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 15.4 – Labeled training DataFrame
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will save this DataFrame as a CSV and upload it to Amazon S3 using
    S3\. Put the `boto3` API object as the Comprehend training file for Amazon Comprehend
    training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, go to the Amazon Comprehend console link (https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#classification)
    to create a classification job. Click on **Train Classifier**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In `doc-classifier`, and in `1`, and scroll down to select `csv file`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We have the choice to add versions for Amazon Comprehend custom models. To
    learn more about this feature, refer to this link: [https://docs.aws.amazon.com/comprehend/latest/dg/model-versioning.html](https://docs.aws.amazon.com/comprehend/latest/dg/model-versioning.html).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.5 – Amazon Comprehend custom classification UI'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17528_15_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 15.5 – Amazon Comprehend custom classification UI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the training data location, browse to the `doc-processing-bucket-MMDD` S3
    bucket created in the `s3://doc-processing-bucket-MMDD/comprehend_train_data.csv`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For `Autosplit`, which means Amazon Comprehend will automatically split the
    test data for you. You also have the choice to tune your model by bringing your
    own test dataset here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For output data, enter the `s3://doc-processing-bucket-MMDD` S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For access permissions, select `classifydoc` in **NameSuffix**.![Figure 15.6
    – Amazon Comprehend custom classification IAM setting
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17528_15_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 15.6 – Amazon Comprehend custom classification IAM setting
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scroll down and click on the **Train Classifier** button to start training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This training will take 30 minutes to complete as we have a large number of
    documents to train with in this chapter. You can use this time to set up a private
    workforce for setting up humans in the loop, which we did in [*Chapter 13*](B17528_13_Final_SB_ePub.xhtml#_idTextAnchor151),
    *Improving the Accuracy of Document Processing Workflows*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once your job is completed, move on to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Amazon Comprehend real-time endpoints and testing a sample document
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will show you how you can create a real-time endpoint with
    the trained model in the **AWS Management Console**. Comprehend uses the **Inference
    Unit** (**IU**) to analyze how many characters can be analyzed in real time per
    second. IU is a measure of the endpoint''s throughput. You can adjust the IU of
    an endpoint anytime. After creating the endpoint, we will then show you how you
    can call this endpoint to test a sample bank statement using the Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to this link, https://console.aws.amazon.com/comprehend/v2/home?region=us-east-1#endpoints,
    and click on **Create Endpoint.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter `classify-doc` as the endpoint name, set `doc-classifier`, which we trained
    in the previous step, and set **Inference units** to **1**.![Figure 15.7 – Amazon
    Comprehend Create real-time endpoint UI
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17528_15_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 15.7 – Amazon Comprehend Create real-time endpoint UI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scroll down and select **I Acknowledge** and click on **Create Endpoint**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delete this endpoint at the cleanup section in the notebook to avoid incurring
    a cost.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, copy the **ARN** of the endpoint, as shown in the next screenshot, and
    move to the Jupyter Notebook link:![Figure 15.8 – Comprehend custom classification
    endpoint ARN
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17528_15_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 15.8 – Comprehend custom classification endpoint ARN
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the notebook, enter the preceding copied endpoint arn in the notebook cell
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will take a sample test document or any pay stub not used in training
    for real-time classification. Run the following code to see the sample pay statement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.9 – Sample pay stub document'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17528_15_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 15.9 – Sample pay stub document
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the next two cells in the notebook under **Extract Text from this sample
    doc using Textract** to extract text from this sample document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell, which calls a Comprehend ClassifyDocument API. This
    method takes the extracted text and custom classification endpoint and returns
    a response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following response:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.10 – ClassifyDocument response'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_15_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.10 – ClassifyDocument response
  prefs: []
  type: TYPE_NORMAL
- en: As per the response, the model endpoint has classified the document as a pay
    stub with 99% confidence. We tested this endpoint, so now let's move on to the
    next section to set up a human loop.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up active learning with a Comprehend real-time endpoint using human
    in the loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we are going to show you a custom integration with a Comprehend
    classifier endpoint, which you can invoke using the A2I StartHumanLoop API. You
    can pass any type of AI/ML prediction response to this API to trigger a human
    loop. In [*Chapter 13*](B17528_13_Final_SB_ePub.xhtml#_idTextAnchor151), *Improving
    the Accuracy of Document Processing Workflows*, we showed you a native integration
    with the Textract Analyze document API by passing a human loop workflow ARN to
    the AnalyzeDocument API. Setting up a custom workflow includes the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a **worker task template**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a **human review workflow**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create and start an A2I human loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the human loop status and start labeling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To get started, you need to create a private workforce and copy the private
    ARN in the *Environment setup* step in the Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a private workforce, refer to the *Creating a private work team in
    AWS Console* section in [*Chapter 13*](B17528_13_Final_SB_ePub.xhtml#_idTextAnchor151),
    *Improving the Accuracy of Document Processing Workflows*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the next cell and move to the `Create worker task` template. This is the
    UI that the workers are going to view while labeling. We will show the prediction
    results in the UI and the original document data. We have used a pre-built classification
    template ([https://github.com/aws-samples/amazon-a2i-sample-task-uis/blob/master/text/document-classification.liquid.html](https://github.com/aws-samples/amazon-a2i-sample-task-uis/blob/master/text/document-classification.liquid.html))
    for this use case. Run the notebook cell to define the HTML template.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can create a custom UI HTML template based on what type of data you want
    to show to your labelers. For example, you can show the actual document on the
    right and entities highlighted on the left using custom UIs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We have defined or chosen the HTML template in the preceding step, in which
    we will create a function to create a UI task using the `create_human_task_ui`
    API by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Run the next cell to invoke the function to create the UI task defined in the
    previous step. You will get a `human task arn` response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we will define a human review workflow. This human review workflow needs
    the private workforce you created, the UI template task you created, and a data
    bucket where you want the output of human review. We will use the `sagemaker.create_flow_definition`
    API to create a flow definition or human review workflow by running the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will get the response from the Comprehend custom classifier endpoint
    for the sample document for pay stubs on the sample data and parse this response
    for the human loop setup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, using this preceding JSON response, we will set a confidence threshold.
    This `StartHumanloop` API needs the workflow ARN or flow definition ARN created
    in the previous step and the JSON response from the Comprehend classification
    to create a human loop. We are triggering this loop based on the confidence score
    threshold, as shown in the next code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding condition states anything greater than 90% confidence from your
    model endpoint will trigger a loop. This threshold is for demo purposes and needs
    to be changed for real use cases, such as anything below 90% that would trigger
    a human loop.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, run the following code to get the link to your private work team to start
    labeling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get a link to the following A2I portal:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.11 – Amazon A2I login console'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17528_15_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 15.11 – Amazon A2I login console
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select **Task title** and click on **Start working**; you will be redirected
    to the classification task UI.![Figure 15.12 – Amazon A2I sample classification
    task UI
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17528_15_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 15.12 – Amazon A2I sample classification task UI
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Review the data on the left in the previous screenshot and classify it by selecting
    the **Pay Stubs** category, and then click **Submit**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After submitting this classification task as a human reviewer, go back to the
    notebook and run the following code to get the completed tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will review the human-reviewed results from completed human reviews,
    which are stored automatically as a JSON file in Amazon S3 by running the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You get the following response:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Human-reviewed JSON response'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_15_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.13 – Human-reviewed JSON response
  prefs: []
  type: TYPE_NORMAL
- en: Using this data, you can augment or enrich your existing dataset used for training.
    Try combining this data with the Comprehend training data we created and try retraining
    your model to improve accuracy. We will point you to some blogs to accomplish
    this step in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Please delete the model and the Comprehend endpoints created for the steps we
    did in this notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered two things using a reference architecture as well
    as a code walkthrough. Firstly, we covered how you can extract data from various
    types of documents, such as pay stubs, bank statements, or identification cards
    using Amazon Textract. Then, we learned how you can perform some post-processing
    to create a labeled training file for Amazon Comprehend custom classification
    training.
  prefs: []
  type: TYPE_NORMAL
- en: We showed you that even with 36 bank statement documents and 24 pay stubs as
    a training sample, you can achieve really good accuracy using Amazon Comprehend
    transfer-learning capabilities and AutoML with document or text classification.
    Obviously, the accuracy improves with more data.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you learned how to set up a training job in the AWS Management Console
    and how to set up a real-time classification endpoint using the AWS Management
    Console.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, you learned how you can set up humans in the loop with the real-time
    classification endpoint to review/verify and validate what the model has classified.
    We then also discussed how you can retrain your existing model by adding this
    data with your existing training data and set up a retraining or active-learning
    loop. Please refer to the *Further reading* section to automate this workflow
    using Lambda functions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover how you can improve the accuracy of **PDF
    batch processing** with Amazon Textract and humans in the loop. So, stay tuned!
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Active learning workflow for Amazon Comprehend custom classification models
    – Part 2*, *Shanthan Kesharaju, Joyson Neville Lewis, and Mona Mona* ([https://aws.amazon.com/blogs/machine-learning/active-learning-workflow-for-amazon-comprehend-custom-classification-part-2/)](https://aws.amazon.com/blogs/machine-learning/active-learning-workflow-for-amazon-comprehend-custom-classification-part-2/)%20)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Creating and Using Custom Classifiers (*[https://docs.aws.amazon.com/comprehend/latest/dg/getting-started-document-classification.html](https://docs.aws.amazon.com/comprehend/latest/dg/getting-started-document-classification.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
