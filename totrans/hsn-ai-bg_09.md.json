["```py\nimport os\n def loadText(fileName, fields):\n ''' Function to Load the Movie Lines text '''\n     lines = {}\n     with open(fileName, 'r', encoding='iso-8859-1') as f:\n     for line in f:\n         values = line.split(\" +++$+++ \")\n         lineObj = {}\n         for i, field in enumerate(fields):\n             lineObj[field] = values[i]\n        lines[lineObj['lineID']] = lineObj\n\n return lines\n```", "```py\nlines = {}\nmovie_lines = [\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\nlines = loadText(\"/users/patricksmith/desktop/glove/movie_lines.txt\", movie_lines)\n```", "```py\nclass GloVeModel():\n    def __init__(self, embedding_size, window_size, max_vocab_size=100000, min_occurrences=1,\n                 scaling_factor=3/4, cooccurrence_cap=100, batch_size=512, learning_rate=0.05):\n        self.embedding_size = embedding_size\n        if isinstance(context_size, tuple):\n            self.left_context, self.right_context = context_size\n        elif isinstance(context_size, int):\n            self.left_context = self.right_context = context_size\n\n        self.max_vocab_size = max_vocab_size\n        self.min_occurrences = min_occurrences\n        self.scaling_factor = scaling_factor\n        self.cooccurrence_cap = cooccurrence_cap\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.__words = None\n        self.__word_to_id = None\n        self.__cooccurrence_matrix = None\n        self.__embeddings = None\n\ndef fit_to_corpus(self, corpus):\n     self.__fit_to_corpus(corpus, self.max_vocab_size,             self.min_occurrences,\n     self.left_context, self.right_context)\n     self.__build_graph()\n\ndef __fit_to_corpus(self, corpus, vocab_size, min_occurrences, left_size, right_size):\n     word_counts = Counter()\n     cooccurrence_counts = defaultdict(float)\n     for region in corpus:\n         word_counts.update(region)\n         for l_context, word, r_context in _context_windows(region, left_size, right_size):\n             for i, context_word in enumerate(l_context[::-1]):\n                 cooccurrence_counts[(word, context_word)] += 1 / (i + 1)\n             for i, context_word in enumerate(r_context):\n                 cooccurrence_counts[(word, context_word)] += 1 / (i + 1)\n     if len(cooccurrence_counts) == 0:\n         raise ValueError(\"No coccurrences in corpus. Did you try to reuse a generator?\")\n         self.__words = [word for word, count in word_counts.most_common(vocab_size)\n         if count >= min_occurrences]\n     self.__word_to_id = {word: i for i, word in         enumerate(self.__words)}\n     self.__cooccurrence_matrix = {\n     (self.__word_to_id[words[0]], self.__word_to_id[words[1]]): count\n         for words, count in cooccurrence_counts.items()\n         if words[0] in self.__word_to_id and words[1] in self.__word_to_id}\n```", "```py\ndef __build_graph(self):\n self.__graph = tf.Graph()\n with self.__graph.as_default(), self.__graph.device(_device_for_node):\n count_max = tf.constant([self.cooccurrence_cap], dtype=tf.float32,\n name='max_cooccurrence_count')\n scaling_factor = tf.constant([self.scaling_factor], dtype=tf.float32,\n name=\"scaling_factor\")\n\n self.__focal_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n name=\"focal_words\")\n self.__context_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n name=\"context_words\")\n self.__cooccurrence_count = tf.placeholder(tf.float32, shape=[self.batch_size],\n name=\"cooccurrence_count\")\n\n focal_embeddings = tf.Variable(\n tf.random_uniform([self.vocab_size, self.embedding_size], 1.0, -1.0),\n name=\"focal_embeddings\")\n context_embeddings = tf.Variable(\n tf.random_uniform([self.vocab_size, self.embedding_size], 1.0, -1.0),\n name=\"context_embeddings\")\n\n focal_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),\n name='focal_biases')\n context_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),\n name=\"context_biases\")\n\n focal_embedding = tf.nn.embedding_lookup([focal_embeddings], self.__focal_input)\n context_embedding = tf.nn.embedding_lookup([context_embeddings], self.__context_input)\n focal_bias = tf.nn.embedding_lookup([focal_biases], self.__focal_input)\n context_bias = tf.nn.embedding_lookup([context_biases], self.__context_input)\n\n weighting_factor = tf.minimum(\n 1.0,\n tf.pow(\n tf.div(self.__cooccurrence_count, count_max),\n scaling_factor))\n\n embedding_product = tf.reduce_sum(tf.multiply(focal_embedding, context_embedding), 1)\n\n log_cooccurrences = tf.log(tf.to_float(self.__cooccurrence_count))\n\n distance_expr = tf.square(tf.add_n([\n embedding_product,\n focal_bias,\n context_bias,\n tf.negative(log_cooccurrences)]))\n\n single_losses = tf.multiply(weighting_factor, distance_expr)\n self.__total_loss = tf.reduce_sum(single_losses)\n tf.summary.scalar(\"GloVe_loss\", self.__total_loss)\n self.__optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(\n self.__total_loss)\n self.__summary = tf.summary.merge_all()\n\n self.__combined_embeddings = tf.add(focal_embeddings, context_embeddings,\n name=\"combined_embeddings\")\n```", "```py\ndef batchify(batch_size, *sequences):\n     for i in range(0, len(sequences[0]), batch_size):\n         yield tuple(sequence[i:i+batch_size] for sequence in sequences) \n\ndef MakeBatches(self):\n     ''' Make Batches of Data to Feed The Model'''\n     cooccurrences = [(word_ids[0], word_ids[1], count)\n     for word_ids, count in self.__cooccurrence_matrix.items()]\n         i_indices, j_indices, counts = zip(*cooccurrences)\n         return list(batchify(self.batch_size, i_indices, j_indices, counts))\n```", "```py\n@property\ndef foo(self): return self._foo \n\n## Is the same as\n\ndef foo(self): \n    return self._foo\n\nfoo = property(foo)\n```", "```py\n @property\n     def vocab_size(self):\n         return len(self.__words)\n\n     @property\n     def words(self):\n         if self.__words is None:\n             raise NotFitToCorpusError(\"Need to fit model to corpus before accessing words.\")\n             return self.__words\n\n     @property\n     def embeddings(self):\n     if self.__embeddings is None:\n         raise NotTrainedError(\"Need to train model before accessing embeddings\")\n         return self.__embeddings\n\n     def id_for_word(self, word):\n         if self.__word_to_id is None:\n             raise NotFitToCorpusError(\"Need to fit model to corpus before looking up word ids.\")\n             return self.__word_to_id[word]\n```", "```py\n def ContextWindow(region, left_size, right_size):\n\n    for i, word in enumerate(region):\n         start_index = i - left_size\n         end_index = i + right_size\n         left_context = _window(region, start_index, i - 1)\n         right_context = _window(region, i + 1, end_index)\n         yield (left_context, word, right_context)\n\n## Function to Create the Window Itself\ndef window(region, start_index, end_index):                   \n     last_index = len(region) + 1\n     selected_tokens = region[max(start_index, 0):min(end_index, last_index) + 1]\n     return selected_tokens\n```", "```py\ndef train(self, num_epochs, log_dir=None, summary_batch_interval=1000):\n\n    ## Initialize the total steps variable, which will be incrementally adjusted in training \n    total_steps = 0\n\n    ## Start a TensorFlow session\n     with tf.Session(graph=self.__graph) as session:\n         if should_write_summaries:\n             summary_writer = tf.summary.FileWriter(log_dir, graph=session.graph)\n            ## Initialize the variables in TensorFlow\n             tf.global_variables_initializer().run()\n\n         for epoch in range(num_epochs):\n             shuffle(batches)\n\n         for batch_index, batch in enumerate(batches):\n             i_s, j_s, counts = batch\n\n         if len(counts) != self.batch_size:\n             continue\n             feed_dict = {\n             self.__focal_input: i_s,\n             self.__context_input: j_s,\n             self.__cooccurrence_count: counts}\n             session.run([self.__optimizer], feed_dict=feed_dict)\n```", "```py\nmodel = GloVeModel(embedding_size=300, context_size=1)\nmodel.fit_to_corpus(corpus) \nmodel.train(num_epochs=100)\n```"]