<html><head></head><body>
		<div>
			<div id="_idContainer131" class="Content">
			</div>
		</div>
		<div id="_idContainer132" class="Content">
			<h1 id="_idParaDest-120">4. <a id="_idTextAnchor135"/>An Introduction to Decision Trees</h1>
		</div>
		<div id="_idContainer145" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter introduces you to two types of supervised learning algorithms in detail. The first algorithm will help you classify data points using decision trees, while the other algorithm will help you classify data points using random forests. Furthermore, you'll learn how to calculate the precision, recall, and F<span class="subscript">1</span> score of models, both manually and automatically. By the end of this chapter, you will be able to analyze the metrics that are used for evaluating the utility of a data model and classify data points based on decision trees and random forest algorithms.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor136"/>Introduction</h1>
			<p>In the previous two chapters, we learned the difference between regression and classification problems, and we saw how to train some of the most famous algorithms. In this chapter, we will look at another type of algorithm: tree-based models.</p>
			<p>Tree-based models are very popular as they can model complex non-linear patterns and they are relatively easy to interpret. In this chapter, we will introduce you to decision trees and the random forest algorithms, which are some of the most widely used tree-based models in the industry.</p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor137"/>Decision Trees</h1>
			<p>A decision tree has leaves, branches, and nodes. Nodes are where a decision is made. A decision tree consists of rules that we use to formulate a decision (or prediction) on the prediction of a data point.</p>
			<p>Every node of the decision tree represents a feature, while every edge coming out of an internal node represents a possible value or a possible interval of values of the tree. Each leaf of the tree represents a label value of the tree.</p>
			<p>This may sound complicated, but let's look at an application of this. </p>
			<p>Suppose we have a dataset with the following features and the response variable is determining whether a person is creditworthy or not:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B16060_04_01.jpg" alt="Figure 4.1: Sample dataset to formulate the rules&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1: Sample dataset to formulate the rules</p>
			<p>A decision tree, remember, is just a group of rules. Looking at the dataset in <em class="italic">Figure 4.1</em>, we can come up with the following rules:</p>
			<ul>
				<li>All people with house loans are determined as creditworthy.</li>
				<li>If debtors are employed and studying, then loans are creditworthy.</li>
				<li>People with income above 75,000 a year are creditworthy.</li>
				<li>At or below 75,000 a year, people with car loans and who are employed are creditworthy.</li>
			</ul>
			<p>Following the order of the rules we just defined, we can build a tree, as shown in <em class="italic">Figure 4.2</em> and describe one possible credit scoring method:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B16060_04_02.jpg" alt="Figure 4.2: Decision tree for the loan type&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2: Decision tree for the loan type</p>
			<p>First, we determine the loan type. House loans are automatically creditworthy according to the first rule. Study loans are described by the second rule, resulting in a subtree containing another decision on employment. Since we have covered both house and study loans, there are only car loans left. The third rule describes an income decision, while the fourth rule describes a decision on employment.</p>
			<p>Whenever we must score a new debtor to determine whether they are creditworthy, we have to go through the decision tree from top to bottom and observe the true or false value at the bottom.</p>
			<p>Obviously, a model based on seven data points is highly inaccurate because we can't generalize rules that simply do not match reality. Therefore, rules are often determined based on large amounts of data.</p>
			<p>This is not the only way that we can create a decision tree. We can build decision trees based on other sequences of rules, too. Let's extract some other rules from the dataset in <em class="italic">Figure 4.1</em>.</p>
			<p><strong class="bold">Observation 1</strong>: Notice that individual salaries that are greater than 75,000 are all creditworthy.</p>
			<p><strong class="bold">Rule 1</strong>: <strong class="source-inline">Income &gt; 75,000 =&gt; CreditWorthy</strong> is true.</p>
			<p>Rule 1 classifies four out of seven data points (IDs C, E, F, G); we need more rules for the remaining three data points.</p>
			<p><strong class="bold">Observation 2</strong>: Out of the remaining three data points, two are not employed. One is employed (ID D) and is creditworthy. With this, we can claim the following rule:</p>
			<p><strong class="bold">Rule 2</strong>: Assuming <strong class="source-inline">Income &lt;= 75,000</strong>, the following holds true: <strong class="source-inline">Employed == true =&gt; CreditWorthy</strong>.</p>
			<p>Note that with this second rule, we can also classify the remaining two data points (IDs A and B) as not creditworthy. With just two rules, we accurately classified all the observations from this dataset:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B16060_04_03.jpg" alt="Figure 4.3: Decision tree for income&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3: Decision tree for income</p>
			<p>The second decision tree is less complex. At the same time, we cannot overlook the fact that the model says, <em class="italic">employed people with a lower income are less likely to pay back their loans</em>. Unfortunately, there is not enough training data available (there are only seven observations in this example), which makes it likely that we'll end up with false conclusions. </p>
			<p>Overfitting is a frequent problem in decision trees when making a decision based on a few data points. This decision is rarely representative.</p>
			<p>Since we can build decision trees in any possible order, it makes sense to define an efficient way of constructing a decision tree. Therefore, we will now explore a measure for ordering the features in the decision process.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor138"/>Entropy</h2>
			<p>In information theory, entropy measures how randomly distributed the possible values of an attribute are. The higher the degree of randomness is, the higher the entropy of the attribute. </p>
			<p>Entropy is the highest possibility of an event. If we know beforehand what the outcome will be, then the event has no randomness. So, entropy is <strong class="bold">zero</strong>.</p>
			<p>We use entropy to order the splitting of nodes in the decision tree. Taking the previous example, which rule should we start with? Should it be <strong class="source-inline">Income &lt;= 75000</strong> or <strong class="source-inline">is employed</strong>? We need to use a metric that can tell us that one specific split is better than the other. A good split can be defined by the fact it clearly split the data into two homogenous groups. One of these metrics is information gain, and it is based on entropy.</p>
			<p>Here is the formula for calculating entropy:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B16060_04_04.jpg" alt="Figure 4.4: Entropy formula&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4: Entropy formula</p>
			<p><em class="italic">p</em><span class="subscript">i</span> represents the probability of one of the possible values of the target variable occurring. So, if this column has <em class="italic">n</em> different unique values, then we will have the probability for each of them <em class="italic">([p</em><span class="subscript">1</span><em class="italic">, p</em><span class="subscript">2</span><em class="italic">, ..., p</em><span class="subscript">n</span><em class="italic">])</em> and apply the formula.</p>
			<p>To manually calculate the entropy of a distribution in Python, we can use the <strong class="source-inline">np.log2</strong> and <strong class="source-inline">np.dot()</strong> methods from the NumPy library. There is no function in <strong class="source-inline">numpy</strong> to automatically calculate entropy.</p>
			<p>Have a look at the following example:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">probabilities = list(range(1,4)) </p>
			<p class="source-code">minus_probabilities = [-x for x in probabilities]</p>
			<p class="source-code">log_probabilities = [x for x in map(np.log2, probabilities)]</p>
			<p class="source-code">entropy_value = np.dot(minus_probabilities, log_probabilities)</p>
			<p>The probabilities are given as a NumPy array or a regular list on <em class="italic">line 2</em>: <em class="italic">p</em><span class="subscript">i.</span></p>
			<p>We need to create a vector of the negated values of the distribution in <em class="italic">line 3</em>: - <em class="italic">p</em><span class="subscript">i.</span></p>
			<p>In <em class="italic">line 4</em>, we must take the base two logarithms of each value in the distribution list: log<span class="subscript">i</span> p<span class="subscript">i.</span></p>
			<p>Finally, we calculate the sum with the scalar product, also known as the dot product of the two vectors:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B16060_04_05.jpg" alt="Figure 4.5: Dot product of two vectors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5: Dot product of two vectors</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You learned about the dot product for the first time in <em class="italic">Chapter 2</em>, <em class="italic">An Introduction to Regression</em>. The dot product of two vectors is calculated by multiplying the <em class="italic">i</em><span class="subscript">th</span> coordinate of the first vector by the <em class="italic">i</em><span class="subscript">th</span> coordinate of the second vector, for each <em class="italic">i</em>. Once we have all the products, we sum the values:</p>
			<p class="callout"><em class="italic">np.dot([1, 2, 3], [4, 5, 6])</em></p>
			<p class="callout">This results in 1*4 + 2*5 + 3*6 = 32.</p>
			<p>In the next exercise, we will be calculating entropy on a small sample dataset.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor139"/>Exercise 4.01: Calculating Entropy</h2>
			<p>In this exercise, we will calculate the entropy of the features in the dataset in <em class="italic">Figure 4.6</em>: </p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B16060_04_06.jpg" alt="Figure 4.6: Sample dataset to formulate the rules&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6: Sample dataset to formulate the rules</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset file can also be found in our GitHub repository:</p>
			<p class="callout"><a href="https://packt.live/2AQ6Uo9">https://packt.live/2AQ6Uo9</a>.</p>
			<p>We will calculate entropy for the <strong class="source-inline">Employed</strong>, <strong class="source-inline">Income</strong>, <strong class="source-inline">LoanType</strong>, and <strong class="source-inline">LoanAmount</strong> features.</p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li>Open a new Jupyter Notebook file.</li>
				<li>Import the <strong class="source-inline">numpy</strong> package as <strong class="source-inline">np</strong>:<p class="source-code">import numpy as np</p></li>
				<li>Define a function called <strong class="source-inline">entropy()</strong> that receives an array of probabilities and then returns the calculated entropy value, as shown in the following code snippet:<p class="source-code">def entropy(probabilities):</p><p class="source-code">    minus_probabilities = [-x for x in probabilities]</p><p class="source-code">    log_probabilities = [x for x in map(np.log2, \</p><p class="source-code">                                        probabilities)]</p><p class="source-code">    return np.dot(minus_probabilities, log_probabilities)</p><p>Next, we will calculate the entropy of the <strong class="source-inline">Employed</strong> column. This column contains only two possible values: <strong class="source-inline">true</strong> or <strong class="source-inline">false</strong>. The <strong class="source-inline">true</strong> value appeared four times out of seven rows, so its probability is <strong class="source-inline">4/7</strong>. Similarly, the probability of the <strong class="source-inline">false</strong> value is <strong class="source-inline">3/7</strong> as it appeared three times in this dataset.</p></li>
				<li>Use the <strong class="source-inline">entropy()</strong> function to calculate the entropy of the <strong class="source-inline">Employed</strong> column with the probabilities <strong class="source-inline">4/7</strong> and <strong class="source-inline">3/7</strong>:<p class="source-code">H_employed = entropy([4/7, 3/7])</p><p class="source-code">H_employed</p><p>You should get the following output:</p><p class="source-code">0.9852281360342515</p><p>This value is quite close to zero, which means the groups are quite homogenous.</p></li>
				<li>Now, use the <strong class="source-inline">entropy()</strong> function to calculate the entropy of the <strong class="source-inline">Income</strong> column with its corresponding list of probabilities:<p class="source-code">H_income = entropy([1/7, 2/7, 1/7, 2/7, 1/7])</p><p class="source-code">H_income</p><p>You should get the following output:</p><p class="source-code">2.2359263506290326</p><p>Compared to the <strong class="source-inline">Employed</strong> column, the entropy for <strong class="source-inline">Income</strong> is higher. This means the probabilities of this column are more spread. </p></li>
				<li>Use the <strong class="source-inline">entropy</strong> function to calculate the entropy of the <strong class="source-inline">LoanType</strong> column with its corresponding list of probabilities:<p class="source-code">H_loanType = entropy([3/7, 2/7, 2/7])</p><p class="source-code">H_loanType</p><p>You should get the following output:</p><p class="source-code">1.5566567074628228</p><p>This value is higher than 0, so the probabilities for this column are quite spread.</p></li>
				<li>Let's use the <strong class="source-inline">entropy</strong> function to calculate the entropy of the <strong class="source-inline">LoanAmount</strong> column with its corresponding list of probabilities:<p class="source-code">H_LoanAmount = entropy([1/7, 1/7, 3/7, 1/7, 1/7])</p><p class="source-code">H_LoanAmount</p><p>You should get the following output:</p><p class="source-code">2.128085278891394</p><p>The entropy for <strong class="source-inline">LoanAmount</strong> is quite high, so its values are quite random.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/37T8DVz">https://packt.live/37T8DVz</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2By7aI6">https://packt.live/2By7aI6</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>Here, you can see that the <strong class="source-inline">Employed</strong> column has the lowest entropy among the four different columns because it has the least variation in terms of values.</p>
			<p>By completing this exercise, you've learned how to manually calculate the entropy for each column of a dataset.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor140"/>Information Gain</h2>
			<p>When we partition the data points in a dataset according to the values of an attribute, we reduce the entropy of the system.</p>
			<p>To describe information gain, we can calculate the distribution of the labels. Initially, in <em class="italic">Figure 4.1</em>, we had five creditworthy and two not creditworthy individuals in our dataset. The entropy belonging to the initial distribution is as follows:</p>
			<p class="source-code">H_label = entropy([5/7, 2/7])</p>
			<p class="source-code">H_label</p>
			<p>The output is as follows:</p>
			<p class="source-code">0.863120568566631</p>
			<p>Let's see what happens if we partition the dataset based on whether the loan amount is greater than 15,000 or not:</p>
			<ul>
				<li>In group 1, we get one data point belonging to the 15,000 loan amount. This data point is not creditworthy.</li>
				<li>In group 2, we have five creditworthy individuals and one non-creditworthy individual.</li>
			</ul>
			<p>The entropy of the labels in each group is as follows.</p>
			<p>For group 1, we have the following:</p>
			<p class="source-code">H_group1 = entropy([1]) </p>
			<p class="source-code">H_group1</p>
			<p>The output is as follows:</p>
			<p class="source-code">-0.0</p>
			<p>For group 2, we have the following:</p>
			<p class="source-code">H_group2 = entropy([5/6, 1/6]) </p>
			<p class="source-code">H_group2</p>
			<p>The output is as follows:</p>
			<p class="source-code">0.6500224216483541</p>
			<p>To calculate the information gain, let's calculate the weighted average of the group entropies:</p>
			<p class="source-code">H_group1 * 1/7 + H_group2 * 6/7</p>
			<p>The output is as follows:</p>
			<p class="source-code">0.5571620756985892</p>
			<p>Now, to find the information gain, we need to calculate the difference between the original entropy (<strong class="source-inline">H_label</strong>) and the one we just calculated:</p>
			<p class="source-code">Information_gain = 0.863120568566631 - 0.5572</p>
			<p class="source-code">Information_gain</p>
			<p>The output is as follows:</p>
			<p class="source-code">0.30592056856663097</p>
			<p>By splitting the data with this rule, we gain a little bit of information.</p>
			<p>When creating the decision tree, on each node, our job is to partition the dataset using a rule that maximizes the information gain.</p>
			<p>We could also use Gini Impurity instead of entropy-based information gain to construct the best rules for splitting decision trees.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor141"/>Gini Impurity</h2>
			<p>Instead of entropy, there is another widely used metric that can be used to measure the randomness of a distribution: Gini Impurity.</p>
			<p>Gini Impurity is defined as follows:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B16060_04_07.jpg" alt="Figure 4.7: Gini Impurity&#13;&#10; &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7: Gini Impurity</p>
			<p><em class="italic">p</em><span class="subscript">i</span> here represents the probability of one of the possible values of the target variable occurring.</p>
			<p>Entropy may be a bit slower to calculate because of the usage of the logarithm. Gini Impurity, on the other hand, is less precise when it comes to measuring randomness.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Some programmers prefer Gini Impurity because you don't have to calculate with logarithms. Computation-wise, none of the solutions are particularly complex, and so both can be used. When it comes to performance, the following study concluded that there are often just minimal differences between the two metrics: <a href="https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf">https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf</a>.</p>
			<p>With this, we have learned that we can optimize a decision tree by splitting the data based on information gain or Gini Impurity. Unfortunately, these metrics are only available for discrete values. What if the label is defined on a continuous interval such as a price range or salary range?</p>
			<p>We have to use other metrics. You can technically understand the idea behind creating a decision tree based on a continuous label, which was about regression. One metric we can reuse in this chapter is the mean squared error. Instead of Gini Impurity or information gain, we have to minimize the mean squared error to optimize the decision tree. As this is a beginner's course, we will omit this metric.</p>
			<p>In the next section, we will discuss the exit condition for a decision tree.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor142"/>Exit Condition</h2>
			<p>We can continuously split the data points according to more and more specific rules until each leaf of the decision tree has an entropy of zero. The question is whether this end state is desirable.</p>
			<p>Often, this is not what we expect, because we risk overfitting the model. When our rules for the model are too specific and too nitpicky, and the sample size that the decision was made on is too small, we risk making a false conclusion, thus recognizing a pattern in the dataset that simply does not exist in real life. </p>
			<p>For instance, if we spin a roulette wheel three times and we get 12, 25, and 12, this concludes that every odd spin resulting in the value 12 is not a sensible strategy. By assuming that every odd spin equals 12, we find a rule that is exclusively due to random noise.</p>
			<p>Therefore, posing a restriction on the minimum size of the dataset that we can still split is an exit condition that works well in practice. For instance, if you stop splitting as soon as you have a dataset that's lower than 50, 100, 200, or 500 in size, you avoid drawing conclusions on random noise, and so you minimize the risk of overfitting the model.</p>
			<p>Another popular exit condition is the maximum restriction on the depth of the tree. Once we reach a fixed tree depth, we classify the data points in the leaves.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor143"/>Building Decision Tree Classifiers Using scikit-learn</h2>
			<p>We have already learned how to load data from a <strong class="source-inline">.csv</strong> file, how to apply preprocessing to data, and how to split data into training and testing datasets. If you need to refresh yourself on this knowledge, you can go back to the previous chapters, where you can go through this process in the context of regression and classification.</p>
			<p>Now, we will assume that a set of training features, training labels, testing features, and testing labels have been given as a return value of the <strong class="source-inline">scikit-learn train-test-split</strong> call:</p>
			<p class="source-code">from sklearn import model_selection</p>
			<p class="source-code">features_train, features_test, \</p>
			<p class="source-code">label_train, label_test = \</p>
			<p class="source-code">model_selection.train_test_split(features, label, test_size=0.1, \</p>
			<p class="source-code">                                 random_state=8)</p>
			<p>In the preceding code snippet, we used <strong class="source-inline">train_test_split</strong> to split the dataset (features and labels) into training and testing sets. The testing set represents 10% of the observation (<strong class="source-inline">test_size=0.1</strong>). The <strong class="source-inline">random_state</strong> parameter is used to get reproducible results.</p>
			<p>We will not focus on how we got these data points because this process is exactly the same as in the case of regression and classification.</p>
			<p>It's time to import and use the decision tree classifier of scikit-learn:</p>
			<p class="source-code">from sklearn.tree import DecisionTreeClassifier</p>
			<p class="source-code">decision_tree = DecisionTreeClassifier(max_depth=6)</p>
			<p class="source-code">decision_tree.fit(features_train, label_train)</p>
			<p>We set one optional parameter in <strong class="source-inline">DecisionTreeClassifier</strong>, that is, <strong class="source-inline">max_depth</strong>, to limit the depth of the decision tree. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can read the official documentation for the full list of parameters: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a>. </p>
			<p>Some of the more important parameters are as follows:</p>
			<ul>
				<li><strong class="source-inline">criterion</strong>: Gini stands for Gini Impurity, while entropy stands for information gain. This will define which measure will be used to assess the quality of a split at each node.</li>
				<li><strong class="source-inline">max_depth</strong>: This is the parameter that defines the maximum depth of the tree.</li>
				<li><strong class="source-inline">min_samples_split</strong>: This is the minimum number of samples needed to split an internal node.</li>
			</ul>
			<p>You can also experiment with all the other parameters that were enumerated in the documentation. We will omit them in this section.</p>
			<p>Once the model has been built, we can use the decision tree classifier to predict data:</p>
			<p class="source-code">decision_tree.predict(features_test)</p>
			<p>You will build a decision tree classifier in the activity at the end of this section.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor144"/>Performance Metrics for Classifiers</h2>
			<p>After splitting the training and testing data, the decision tree model has a <strong class="source-inline">score</strong> method to evaluate how well testing data is classified by the model (also known as the accuracy score). We learned how to use the <strong class="source-inline">score</strong> method in the previous two chapters:</p>
			<p class="source-code">decision_tree.score(features_test, label_test)</p>
			<p>The return value of the <strong class="source-inline">score</strong> method is a number that's less than or equal to 1. The closer we get to 1, the better our model is.</p>
			<p>Now, we will learn about another way to evaluate the model. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Feel free to use this method on the models you constructed in the previous chapter as well.</p>
			<p>Suppose we have one test feature and one test label:</p>
			<p class="source-code">predicted_label = decision_tree.predict(features_test)</p>
			<p>Let's use the previous creditworthy example and assume we trained a decision tree and now have its predictions:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B16060_04_08.jpg" alt="Figure 4.8: Sample dataset to formulate the rules&#13; &#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8: Sample dataset to formulate the rules</p>
			<p>Our model, in general, made good predictions but had few errors. It incorrectly predicted the results for IDs <strong class="source-inline">A</strong>, <strong class="source-inline">D</strong>, and <strong class="source-inline">E</strong>. Its accuracy score will be 4 / 7 = 0.57.</p>
			<p>We will use the following definitions to define some metrics that will help you evaluate how good your classifier is:</p>
			<ul>
				<li><strong class="bold">True positive (or TP)</strong>: All the observations where the true label (the <strong class="source-inline">Creditworthy</strong> column, in our example) and the corresponding predictions both have the value <strong class="source-inline">Yes</strong>. In our example, IDs <strong class="source-inline">C</strong>, <strong class="source-inline">F</strong>, and <strong class="source-inline">G</strong> will fall under this category.</li>
				<li><strong class="bold">True negative (or TN)</strong>: All the observations where the true label and the corresponding predictions both have the value <strong class="source-inline">No</strong>. Only ID <strong class="source-inline">B</strong> will be classified as true negative.</li>
				<li><strong class="bold">False positive (or FP)</strong>: All the observations where the prediction is <strong class="source-inline">Yes</strong> but the true label is actually <strong class="source-inline">No</strong>. This will be the case for ID <strong class="source-inline">A</strong>.</li>
				<li><strong class="bold">False negative (or FN)</strong>: All the observations where the prediction is <strong class="source-inline">No</strong> but the true label is actually <strong class="source-inline">Yes</strong>, such as for IDs <strong class="source-inline">D</strong> and <strong class="source-inline">E</strong>.</li>
			</ul>
			<p>Using the preceding four definitions, we can define four metrics that describe how well our model predicts the target variable. The <strong class="source-inline">#( X )</strong> symbol denotes the number of values in <strong class="source-inline">X</strong>. Using technical terms, <strong class="source-inline">#( X )</strong> denotes the cardinality of <strong class="source-inline">X</strong>:</p>
			<p><strong class="bold">Definition (Accuracy)</strong>: <em class="italic">#( True Positives ) + #( True Negatives ) / #( Dataset )</em> </p>
			<p>Accuracy is a metric that's used for determining how many times the classifier gives us the correct answer. This is the first metric we used to evaluate the score of a classifier. </p>
			<p>In our previous example (<em class="italic">Figure 4.8</em>), the accuracy score will be TP + TN / total = (3 + 1) / 7 = 4/7.</p>
			<p>We can use the function provided by scikit-learn to calculate the accuracy of a model:</p>
			<p class="source-code">from sklearn.metrics import accuracy_score</p>
			<p class="source-code">accuracy_score(label_test, predicted_label)</p>
			<p><strong class="bold">Definition (Precision)</strong>: <em class="italic">#TruePositives / (#TruePositives + #FalsePositives)</em></p>
			<p>Precision centers around values that our classifier found to be positive. Some of these results are true positive, while others are false positive. High precision means that the number of false positive results is very low compared to the true positive results. This means that a precise classifier rarely makes a mistake when finding a positive result.</p>
			<p><strong class="bold">Definition (Recall)</strong>: <em class="italic">#True Positives / (#True Positives + #False Negatives)</em></p>
			<p>Recall centers around values that are positive among the test data. Some of these results are found by the classifier. These are the true positive values. Those positive values that are not found by the classifier are false negatives. A classifier with a high recall value finds most of the positive values.</p>
			<p>Using our previous example (<em class="italic">Figure 4.8</em>), we will get the following measures:</p>
			<ul>
				<li>Precision = TP / (TP + FP) = 4 / (4 + 1) = 4/6 = 0.8</li>
				<li>Recall = TP / (TP + FN) = 4 / (4 + 2) = 4/6 = 0.6667</li>
			</ul>
			<p>With these two measures, we can easily see where our model is performing better or worse. In this example, we know it tends to misclassify false negative cases. These measures are more granular than the accuracy score, which only gives you an overall score.</p>
			<p>The F<span class="subscript">1 </span>score is a metric that combines precision and recall scores. Its value ranges between 0 and 1. If the F<span class="subscript">1</span> score equals 1, it means the model is perfectly predicting the right outcomes. On the other hand, an F<span class="subscript">1</span> score of 0 means the model cannot predict the target variable accurately. The advantage of the F<span class="subscript">1</span> score is that it considers both false positives and false negatives. </p>
			<p>The formula for calculating the F<span class="subscript">1</span> score is as follows: </p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B16060_04_09.jpg" alt="Figure 4.9: Formula to calculate the F1 score&#13; &#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9: Formula to calculate the F<span class="subscript">1 </span>score</p>
			<p>As a final note, the scikit-learn package also provides a handy function that can show all these measures in one go: <strong class="source-inline">classification_report()</strong>. A classification report is useful to check the quality of our predictions:</p>
			<p class="source-code">from sklearn.metrics import classification_report</p>
			<p class="source-code">print(classification_report(label_test, predicted_label))</p>
			<p>In the next exercise, we will be practicing how to calculate these scores manually.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor145"/>Exercise 4.02: Precision, Recall, and F1 Score Calculation</h2>
			<p>In this exercise, we will calculate the precision, recall value, and the F<span class="subscript">1</span> score of two different classifiers on a simulated dataset.</p>
			<p>The following steps will help you complete this exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file.</li>
				<li>Import the <strong class="source-inline">numpy</strong> package as <strong class="source-inline">np</strong> using the following code:<p class="source-code">import numpy as np</p></li>
				<li>Create a <strong class="source-inline">numpy</strong> array called <strong class="source-inline">real_labels</strong> that contains the values [<strong class="source-inline">True, True, False, True, True]</strong>. This list will represent the true values of the target variable for our simulated dataset. Print its content:<p class="source-code">real_labels = np.array([True, True, False, True, True])</p><p class="source-code">real_labels</p><p>The expected output will be as follows:</p><p class="source-code">array([ True, True, False, True, True])</p></li>
				<li>Create a <strong class="source-inline">numpy</strong> array called <strong class="source-inline">model_1_preds</strong> that contains the values <strong class="source-inline">[True, False, False, False, False]</strong>. This list will represent the predicted values of the first classifier. Print its content:<p class="source-code">model_1_preds = np.array([True, False, False, False, False])</p><p class="source-code">model_1_preds</p><p>The expected output will be as follows:</p><p class="source-code">array([ True, False, False, False, False])</p></li>
				<li>Create another <strong class="source-inline">numpy</strong> array called <strong class="source-inline">model_2_preds</strong> that contains the values <strong class="source-inline">[True, True, True, True, True]</strong>. This list will represent the predicted values of the first classifier. Print its content:<p class="source-code">model_2_preds = np.array([True, True, True, True, True])</p><p class="source-code">model_2_preds</p><p>The expected output will be as follows:</p><p class="source-code">array([ True,  True,  True,  True,  True])</p></li>
				<li>Create a variable called <strong class="source-inline">model_1_tp_cond</strong> that will find the true positives for the first model:<p class="source-code">model_1_tp_cond = (real_labels == True) \</p><p class="source-code">                   &amp; (model_1_preds == True)</p><p class="source-code">model_1_tp_cond</p><p>The expected output will be as follows:</p><p class="source-code">array([ True, False, False, False, False])</p></li>
				<li>Create a variable called <strong class="source-inline">model_1_tp</strong> that will get the number of true positives for the first model by summing <strong class="source-inline">model_1_tp_cond</strong>:<p class="source-code">model_1_tp = model_1_tp_cond.sum()</p><p class="source-code">model_1_tp</p><p>The expected output will be as follows:</p><p class="source-code">1</p><p>There is only <strong class="source-inline">1</strong> true positive case for the first model.</p></li>
				<li>Create a variable called <strong class="source-inline">model_1_fp</strong> that will get the number of false positives for the first model:<p class="source-code">model_1_fp = ((real_labels == False) \</p><p class="source-code">               &amp; (model_1_preds == True)).sum()</p><p class="source-code">model_1_fp</p><p>The expected output will be as follows:</p><p class="source-code">0</p><p>There is no false positive for the first model.</p></li>
				<li>Create a variable called <strong class="source-inline">model_1_fn</strong> that will get the number of false negatives for the first model:<p class="source-code">model_1_fn = ((real_labels == True) \</p><p class="source-code">               &amp; (model_1_preds == False)).sum()</p><p class="source-code">model_1_fn</p><p>The expected output will be as follows:</p><p class="source-code">3</p><p>The first classifier presents <strong class="source-inline">3</strong> false negative cases.</p></li>
				<li>Create a variable called <strong class="source-inline">model_1_precision</strong> that will calculate the precision for the first model:<p class="source-code">model_1_precision = model_1_tp / (model_1_tp + model_1_fp)</p><p class="source-code">model_1_precision</p><p>The expected output will be as follows:</p><p class="source-code">1.0</p><p>The first classifier has a precision score of <strong class="source-inline">1</strong>, so it didn't predict any false positives.</p></li>
				<li>Create a variable called <strong class="source-inline">model_1_recall</strong> that will calculate the recall for the first model:<p class="source-code">model_1_recall = model_1_tp / (model_1_tp + model_1_fn)</p><p class="source-code">model_1_recall</p><p>The expected output will be as follows:</p><p class="source-code">0.25</p><p>The recall score for the first model is only <strong class="source-inline">0.25</strong>, so it is predicting quite a lot of false negatives.</p></li>
				<li>Create a variable called <strong class="source-inline">model_1_f1</strong> that will calculate the F<span class="subscript">1</span> score for the first model:<p class="source-code">model_1_f1 = 2*model_1_precision * model_1_recall\</p><p class="source-code">             / (model_1_precision + model_1_recall)</p><p class="source-code">model_1_f1</p><p>The expected output will be as follows:</p><p class="source-code">0.4</p><p>As expected, the F<span class="subscript">1</span> score is quite low for the first model.</p></li>
				<li>Create a variable called <strong class="source-inline">model_2_tp</strong> that will get the number of true positives for the second model:<p class="source-code">model_2_tp = ((real_labels == True) \</p><p class="source-code">               &amp; (model_2_preds == True)).sum()</p><p class="source-code">model_2_tp</p><p>The expected output will be as follows:</p><p class="source-code">4</p><p>There are <strong class="source-inline">4</strong> true positive cases for the second model.</p></li>
				<li>Create a variable called <strong class="source-inline">model_2_fp</strong> that will get the number of false positives for the second model:<p class="source-code">model_2_fp = ((real_labels == False) \</p><p class="source-code">               &amp; (model_2_preds == True)).sum()</p><p class="source-code">model_2_fp</p><p>The expected output will be as follows:</p><p class="source-code">1</p><p>There is only one false positive for the second model.</p></li>
				<li>Create a variable called <strong class="source-inline">model_2_fn</strong> that will get the number of false negatives for the second model:<p class="source-code">model_2_fn = ((real_labels == True) \</p><p class="source-code">               &amp; (model_2_preds == False)).sum()</p><p class="source-code">model_2_fn</p><p>The expected output will be as follows:</p><p class="source-code">0</p><p>There is no false negative for the second classifier.</p></li>
				<li>Create a variable called <strong class="source-inline">model_2_precision</strong> that will calculate precision for the second model:<p class="source-code">model_2_precision = model_2_tp / (model_2_tp + model_2_fp) </p><p class="source-code">model_2_precision</p><p>The expected output will be as follows:</p><p class="source-code">0.8</p><p>The precision score for the second model is quite high: <strong class="source-inline">0.8</strong>. It is not making too many mistakes regarding false positives.</p></li>
				<li>Create a variable called <strong class="source-inline">model_2_recall</strong> that will calculate recall for the second model:<p class="source-code">model_2_recall = model_2_tp / (model_2_tp + model_2_fn)</p><p class="source-code">model_2_recall</p><p>The expected output will be as follows:</p><p class="source-code">1.0</p><p>In terms of recall, the second classifier did a great job and didn't misclassify observations to false negatives.</p></li>
				<li>Create a variable called <strong class="source-inline">model_2_f1</strong> that will calculate the F<span class="subscript">1</span> score for the second model:<p class="source-code">model_2_f1 = 2*model_2_precision*model_2_recall \</p><p class="source-code">             / (model_2_precision + model_2_recall)</p><p class="source-code">model_2_f1</p><p>The expected output will be as follows:</p><p class="source-code">0.888888888888889</p><p>The F<span class="subscript">1</span> score is quite high for the second model.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3evqbtu">https://packt.live/3evqbtu</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2NoxLdo">https://packt.live/2NoxLdo</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we saw how to manually calculate the precision, recall, and F<span class="subscript">1</span> score for two different models. The first classifier has excellent precision but bad recall, while the second classifier has excellent recall and quite good precision.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor146"/>Evaluating the Performance of Classifiers with scikit-learn</h2>
			<p>The scikit-learn package provides some functions for automatically calculating the precision, recall, and F<span class="subscript">1</span> score for you. You will need to import them first:</p>
			<p class="source-code">from sklearn.metrics import recall_score, \</p>
			<p class="source-code">precision_score, f1_score</p>
			<p>To get the precision score, you will need to get the predictions from your model, as shown in the following code snippet:</p>
			<p class="source-code">label_predicted = decision_tree.predict(data)</p>
			<p class="source-code">precision_score(label_test, predicted_label, \</p>
			<p class="source-code">                average='weighted')</p>
			<p>Calculating the <strong class="source-inline">recall_score</strong> can be done like so:</p>
			<p class="source-code">recall_score(label_test, label_predicted, average='weighted')</p>
			<p>Calculating the <strong class="source-inline">f1_score</strong> can be done like so:</p>
			<p class="source-code">f1_score(label_test, predicted_label, average='weighted')</p>
			<p>In the next section, we will learn how to use another tool, called the confusion matrix, to analyze the performance of a classifier.</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor147"/>The Confusion Matrix</h1>
			<p>Previously, we learned how to use some calculated metrics to assess the performance of a classifier. There is another very interesting tool that can help you evaluate the performance of a multi-class classification model: the confusion matrix.</p>
			<p>A confusion matrix is a square matrix where the number of rows and columns equals the number of distinct label values (or classes). In the columns of the matrix, we place each test label value. In the rows of the matrix, we place each predicted label value. </p>
			<p>A confusion matrix looks like this: </p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B16060_04_10.jpg" alt="Figure 4.10: Sample confusion matrix&#13; &#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10: Sample confusion matrix</p>
			<p>In the preceding example, the first row of the confusion matrix is showing us that the model is doing the following:</p>
			<ul>
				<li>Correctly predicting class A <strong class="source-inline">88</strong> times</li>
				<li>Predicting class A when the true value is B <strong class="source-inline">3</strong> times</li>
				<li>Predicting class A when the true value is C <strong class="source-inline">2</strong> times</li>
			</ul>
			<p>We can also see the scenario where the model is making a lot of mistakes when it is predicting C while the true value is A (16 times). A confusion matrix is a powerful tool to quickly and easily spot which classes your model is performing well or badly for.</p>
			<p>The scikit-learn package provides a function to calculate and display a confusion matrix:</p>
			<p class="source-code">from sklearn.metrics import confusion_matrix</p>
			<p class="source-code">confusion_matrix(label_test, predicted_label)</p>
			<p>In the next activity, you will be building a decision tree that will classify cars as unacceptable, acceptable, good, and very good for customers.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor148"/>Activity 4.01: Car Data Classification</h2>
			<p>In this activity, you will build a reliable decision tree model that's capable of aiding a company in finding cars that clients are likely to buy. We will be assuming that the car rental agency is focusing on building a lasting relationship with its clients. Your task is to build a decision tree model that classifies cars into one of four categories: unacceptable, acceptable, good, and very good.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset file can also be found in our GitHub repository: <a href="https://packt.live/2V95I6h">https://packt.live/2V95I6h</a>.</p>
			<p class="callout">The dataset for this activity can be accessed here: <a href="https://archive.ics.uci.edu/ml/datasets/Car+Evaluation">https://archive.ics.uci.edu/ml/datasets/Car+Evaluation</a>. </p>
			<p class="callout">Citation – <em class="italic">Dua, D., &amp; Graff, C.. (2017). UCI Machine Learning Repository</em>. </p>
			<p>It is composed of six different features: <strong class="source-inline">buying</strong>, <strong class="source-inline">maintenance</strong>, <strong class="source-inline">doors</strong>, <strong class="source-inline">persons</strong>, <strong class="source-inline">luggage_boot</strong>, and <strong class="source-inline">safety</strong>. The target variable ranks the level of acceptability for a given car. It can take four different values: <strong class="source-inline">unacc</strong>, <strong class="source-inline">acc</strong>, <strong class="source-inline">good</strong>, and <strong class="source-inline">vgood</strong>.</p>
			<p>The following steps will help you complete this activity:</p>
			<ol>
				<li value="1">Load the dataset into Python and import the necessary libaries.</li>
				<li>Perform label encoding with <strong class="source-inline">LabelEncoder()</strong> from scikit-learn.</li>
				<li>Extract the <strong class="source-inline">label</strong> variable using <strong class="source-inline">pop()</strong> from pandas.</li>
				<li>Now, separate the training and testing data with <strong class="source-inline">train_test_spit()</strong> from scikit-learn. We will use 10% of the data as test data.</li>
				<li>Build the decision tree classifier using <strong class="source-inline">DecisionTreeClassifier()</strong> and its methods, <strong class="source-inline">fit()</strong> and <strong class="source-inline">predict()</strong>.</li>
				<li>Check the score of our model based on the test data with <strong class="source-inline">score()</strong>.</li>
				<li>Create a deeper evaluation of the model using <strong class="source-inline">classification_report()</strong> from scikit-learn.</li>
			</ol>
			<p>Expected output:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B16060_04_11.jpg" alt="Figure 4.11: Output showing the expected classification report&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11: Output showing the expected classification report</p>
			<p class="callout-heading">Note </p>
			<p class="callout">T<a id="_idTextAnchor149"/>he solution to this activity can be found on page 353.</p>
			<p>In the next section we will be looking at Random Forest Classifier.</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor150"/>Random Forest Classifier</h1>
			<p>If you think about the name random forest classifier, it can be explained as follows:</p>
			<ul>
				<li>A forest consists of multiple trees.</li>
				<li>These trees can be used for classification.</li>
				<li>Since the only tree we have used so far for classification is a decision tree, it makes sense that the random forest is a forest of decision trees.</li>
				<li>The random nature of the trees means that our decision trees are constructed in a randomized manner.</li>
			</ul>
			<p>Therefore, we will base our decision tree construction on information gain or Gini Impurity.</p>
			<p>Once you understand these basic concepts, you essentially know what a random forest classifier is all about. The more trees you have in the forest, the more accurate prediction is going to be. When performing prediction, each tree performs classification. We collect the results, and the class that gets the most votes wins.</p>
			<p>Random forests can be used for regression as well as for classification. When using random forests for regression, instead of counting the most votes for a class, we take the average of the arithmetic mean (average) of the prediction results and return it. Random forests are not as ideal for regression as they are for classification, though, because the models that are used to predict values are often out of control, and often return a wide range of values. The average of these values is often not too meaningful. Managing the noise in a regression exercise is harder than in classification.</p>
			<p>Random forests are often better than one simple decision tree because they provide redundancy. They treat outlier values better and have a lower probability of overfitting the model. Decision trees seem to behave great as long as you are using them on the data that was used when creating the model. Once you use them to predict new data, random forests lose their edge. Random forests are widely used for classification problems, whether it be customer segmentation for banks or e-commerce, classifying images, or medicine. If you own an Xbox with Kinect, your Kinect device contains a random forest classifier to detect your body.</p>
			<p>Random forest is an ensemble algorithm. The idea behind ensemble learning is that we take an aggregated view of a decision of multiple agents that potentially have different weaknesses. Due to the aggregated vote, these weaknesses cancel out, and the majority vote likely represents the correct result.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor151"/>Random Forest Classification Using scikit-learn</h2>
			<p>As you may have guessed, the scikit-learn package provides an implementation of the <strong class="source-inline">RandomForest</strong> classifier with the <strong class="source-inline">RandomForestClassifier</strong> class. This class provides the exact same methods as all the scikit-learn models you have seen so far – you need to instantiate a model, then fit it with the training set with <strong class="source-inline">.fit()</strong>, and finally make predictions with <strong class="source-inline">.predict()</strong>:</p>
			<p class="source-code">from sklearn.ensemble import RandomForestClassifier</p>
			<p class="source-code">random_forest_classifier = RandomForestClassifier()</p>
			<p class="source-code">random_forest_classifier.fit(features_train, label_train)</p>
			<p class="source-code">labels_predicted = random_forest_classifier.predict\</p>
			<p class="source-code">                   (features_test)</p>
			<p>In the next section, we will be looking at the parameterization of the random forest classifier.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor152"/>The Parameterization of the Random Forest Classifier</h2>
			<p>We will be considering a subset of the possible parameters, based on what we already know, which is based on the description of constructing random forests:</p>
			<ul>
				<li><strong class="source-inline">n_estimators</strong>: The number of trees in the random forest. The default value is 10.</li>
				<li><strong class="source-inline">criterion</strong>: Use Gini or entropy to determine whether you use Gini Impurity or information gain using the entropy in each tree. This will be used to find the best split at each node.</li>
				<li><strong class="source-inline">max_features</strong>: The maximum number of features considered in any tree of the forest. Possible values include an integer. You can also add some strings such as <strong class="source-inline">sqrt</strong> for the square root of the number of features. </li>
				<li><strong class="source-inline">max_depth</strong>: The maximum depth of each tree.</li>
				<li><strong class="source-inline">min_samples_split</strong>: The minimum number of samples in the dataset in a given node to perform a split. This may also reduce the tree's size.</li>
				<li><strong class="source-inline">bootstrap</strong>: A Boolean that indicates whether to use bootstrapping on data points when constructing trees.</li>
			</ul>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor153"/>Feature Importance</h2>
			<p>A random forest classifier gives you information on how important each feature in the data classification process is. Remember, we used a lot of randomly constructed decision trees to classify data points. We can measure how accurately these data points behave, and we can also see which features are vital when it comes to decision-making.</p>
			<p>We can retrieve the array of feature importance scores with the following query:</p>
			<p class="source-code">random_forest_classifier.feature_importances_</p>
			<p>In this six-feature classifier, the fourth and sixth features are clearly a lot more important than any other features. The third feature has a very low importance score.</p>
			<p>Feature importance scores come in handy when we have a lot of features and we want to reduce the feature size to avoid the classifier getting lost in the details. When we have a lot of features, we risk overfitting the model. Therefore, reducing the number of features by dropping the least significant ones is often helpful. </p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor154"/>Cross-Validation</h2>
			<p>Earlier, we learned how to use different metrics to assess the performance of a classifier, such as the accuracy, precision, recall, or the F<span class="subscript">1</span> score on a training and testing set. The objective is to have a high score on both sets that are very close to each other. In that case, your model is performant and not prone to overfitting.</p>
			<p>The test set is used as a proxy to evaluate whether your model can generalize well to unseen data or whether it learns patterns that are only relevant to the training set. </p>
			<p>But in the case of having quite a few hyperparameters to tune (such as for <strong class="source-inline">RandomForest</strong>), you will have to train a lot of different models and test them on your testing set. This kind of defeats the purpose of the testing set. Think of the testing set as the final exam that will define whether you pass a subject or not. You will not be allowed to pass and repass it over and over.</p>
			<p>One solution for avoiding using the testing set too much is creating a validation set. You will train your model on the training set and use the validation set to assess its score according to different combinations of hyperparameters. Once you find your best model, you will use the testing set to make sure it doesn't overfit too much. This is, in general, the suggested approach for any data science project.</p>
			<p>The drawback of this approach is that you are reducing the number of observations for the training set. If you have a dataset with millions of rows, it is not a problem. But for a small dataset, this can be problematic. This is where cross-validation comes in.</p>
			<p>The following <em class="italic">Figure 4.12</em>, shows that this is a technique where you create multiple splits of the training data. For each split, the training data is separated into folds (five, in this example) and one of the folds will be used as the validation set while the others will be used for training.</p>
			<p>For instance, for the top split, fold 5 will be used for validation and the four other folds (1 to 4) will be used to train the model. You will follow the same process for each split. After going through each split, you will have used the entire training data and the final performance score will be the average of all the models that were trained on each split:</p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B16060_04_12.jpg" alt="Figure 4.12: Cross-validation example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12: Cross-validation example</p>
			<p>With scikit-learn, you can easily perform cross-validation, as shown in the following code snippet:</p>
			<p class="source-code">from sklearn.ensemble import RandomForestClassifier</p>
			<p class="source-code">random_forest_classifier = RandomForestClassifier()</p>
			<p class="source-code">from sklearn.model_selection import cross_val_score</p>
			<p class="source-code">cross_val_score(random_forest_classifier, features_train, \</p>
			<p class="source-code">                label_train, cv=5, scoring='accuracy')</p>
			<p><strong class="source-inline">cross_val_score</strong> takes two parameters: </p>
			<ul>
				<li><strong class="source-inline">cv</strong>: Specifies the number of splits.</li>
				<li><strong class="source-inline">scoring</strong>: Defines which performance metrics you want to use. You can find the list of possible values here: <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter">https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter</a>.</li>
			</ul>
			<p>In the next section, we will look at a specific variant of <strong class="source-inline">RandomForest</strong>, called <strong class="source-inline">extratrees</strong>.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor155"/>Extremely Randomized Trees</h2>
			<p>Extremely randomized trees increase the randomization inside random forests by randomizing the splitting rules on top of the already randomized factors in random forests.</p>
			<p>Parameterization is like the random forest classifier. You can see the full list of parameters here: <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html</a>.</p>
			<p>The Python implementation is as follows:</p>
			<p class="source-code">from sklearn.ensemble import ExtraTreesClassifier</p>
			<p class="source-code">extra_trees_classifier = \</p>
			<p class="source-code">ExtraTreesClassifier(n_estimators=100, \</p>
			<p class="source-code">                     max_depth=6)</p>
			<p class="source-code">extra_trees_classifier.fit(features_train, label_train)</p>
			<p class="source-code">labels_predicted = extra_trees_classifier.predict(features_test)</p>
			<p>In the following activity, we will be optimizing the classifier built in <em class="italic">Activity 4.01</em>, <em class="italic">Car Data Classification</em>.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor156"/>Activity 4.02: Random Forest Classification for Your Car Rental Company</h2>
			<p>In this activity, you will optimize your classifier so that you satisfy your clients more when selecting future cars for your car fleet. We will be performing random forest and extreme random forest classification on the car dealership dataset that you worked on in the previous activity of this chapter.</p>
			<p>The following steps will help you complete this activity:</p>
			<ol>
				<li value="1">Follow <em class="italic">Steps 1 - 4</em> of the previous <em class="italic">Activity 4.01</em>, <em class="italic">Car Data Classification</em>.</li>
				<li>Create a random forest using <strong class="source-inline">RandomForestClassifier</strong>.</li>
				<li>Train the models using <strong class="source-inline">.fit()</strong>.</li>
				<li>Import the <strong class="source-inline">confusion_matrix</strong> function to find the quality of the <strong class="source-inline">RandomForest</strong>.</li>
				<li>Print the classification report using <strong class="source-inline">classification_report()</strong>.</li>
				<li>Print the feature importance with <strong class="source-inline">.feature_importance_</strong>.</li>
				<li>Repeat <em class="italic">Steps 2 to 6</em> with an <strong class="source-inline">extratrees</strong> model.</li>
			</ol>
			<p>Expected output:</p>
			<p class="source-code">array([0.08844544, 0.0702334 , 0.01440408, 0.37662014,</p>
			<p class="source-code">       0.05965896, 0.39063797])</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 357.</p>
			<p>By completing this activity, you've learned how to fit the <strong class="source-inline">RandomForest</strong> and <strong class="source-inline">extratrees</strong> models and analyze their classification report and feature importance. Now, you can try different hyperparameters on your own and see if you can improve their results.</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor157"/>Summary</h1>
			<p>In this chapter, we learned how to use decision trees for prediction. Using ensemble learning techniques, we created complex reinforcement learning models to predict the class of an arbitrary data point.</p>
			<p>Decision trees proved to be very accurate on the surface, but they were prone to overfitting the model. Random forests and extremely randomized trees reduce overfitting by introducing some random elements and a voting algorithm, where the majority wins.</p>
			<p>Beyond decision trees, random forests, and extremely randomized trees, we also learned about new methods for evaluating the utility of a model. After using the well-known accuracy score, we started using the precision, recall, and F<span class="subscript">1</span> score metrics to evaluate how well our classifier works. All of these values were derived from the confusion matrix.</p>
			<p>In the next chapter, we will describe the clustering problem and compare and contrast two clustering algorithms.</p>
		</div>
	</body></html>