<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Performing Speech-to-Text and Vice Versa with Amazon Transcribe and Polly</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will continue to develop the skills and intuition required for real-world artificial intelligence applications. We will build an application that can translate spoken speech from one language to another. We will leverage Amazon Transcribe and Amazon Polly to perform speech-to-text and text-to-speech tasks. We will also demonstrate how our reference architecture allows us to reuse the <span><span>service implementations</span></span> we implemented in the previous chapter projects.</p>
<p>We will cover the following topics:</p>
<ul>
<li>Performing speech-to-text with Amazon Transcribe</li>
<li>Performing text-to-speech with Amazon Polly</li>
<li>Building serverless AI applications with AWS services, RESTful APIs, and web user interface</li>
<li>Reusing existing AI service implementations within the reference architecture</li>
<li>Discussing user experience and product design decisions</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This book's GitHub repository, which contains the source code for this chapter, can be found at <a href="https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services">https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technologies from science fiction</h1>
                </header>
            
            <article>
                
<p>Google's recent entry into the headphone market, called the Pixel Buds, had a unique feature that wowed the reviewers. These headphones can translate conversations in real time for dozens of languages. This sounds like science fiction. What comes to mind is Star Trek's universal translator that allows Starfleet crews to communicate with almost any alien race. Even though the Pixel Buds are not as powerful as their science fiction counterpart, they are packed with some amazing <strong>Artificial Intelligence</strong> (<strong>AI</strong>) technologies. This product showcases what we can expect from AI capabilities to help us communicate with more people in more places.</p>
<p>We will be implementing a similar conversation translation feature using AWS AI services. Our application, modestly named the Universal Translator, will provide voice-to-voice translation between dozens of languages. However, our Universal Translator is <span>not exactly real time and it only supports dozens of human languages.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the architecture of Universal Translator</h1>
                </header>
            
            <article>
                
<p>Our Universal Translator application will provide a web user interface for the users to record a phrase in one language and then translate that phrase to another language. Here is the architecture design highlighting the layers and services of our application. The organization of the layers and components should be familiar to you from our previous projects:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ebd4b401-085c-4255-885a-53ab98fce7f0.png" style=""/></div>
<p>In this application, the web user interface will interact with three RESTful endpoints in the orchestration layer:</p>
<ul>
<li><strong>Upload Recording Endpoint</strong> will delegate the audio recording upload to our Storage Service, which provides an abstraction layer to AWS S3.</li>
<li><strong>Translate Recording Endpoint</strong> will use both the Amazon Transcription Service and the Amazon Translation Service. It first gets the transcription of the audio recording, and then translates the transcription text to the target language. The transcription service and the translation service abstract the Amazon Transcribe and Amazon Translate services respectively.</li>
<li><strong>Synthesize Speech Endpoint</strong> will delegate the speech synthesis of the translated text to the Speech Service, which is backed by the Amazon Polly service.</li>
</ul>
<p>As we will soon see in the project implementation, the Translation Service is reused from the Pictorial Translator project, without any modification. In addition, the upload file method in the Storage Service implementation is also reused from the previous projects. One of the benefits of separating the orchestration layer and the service implementations should be clear here. We can reuse and recombine various service implementations, without modification, by stitching them together in the orchestration layer. Each application's unique business logics are implemented in the orchestration layer while the capabilities are implemented without the knowledge of the application-specific business logic.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Component interactions of Universal Translator</h1>
                </header>
            
            <article>
                
<p>The following diagram walks through how the different components will interact with each other to form the business logic workflow of the Universal Translator application:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ee07ad0e-99d3-4c4a-89a0-34ae8e71593b.png" style=""/></div>
<p>The following is from the user's perspective:</p>
<ul>
<li>The user first selects the source and target languages of the speech translation in the web user interface.</li>
<li>The user then records a short audio speech with the on-screen controls.</li>
<li>This recording can be played back from the web user interface. <span>The user can use the playback feature to check the quality of the speech recording.</span></li>
<li>When the user is satisfied with the recording, it can be uploaded for translation.</li>
<li>Some time later, the web user interface will display both the transcription and translation texts in the web user interface.</li>
<li>Finally, a synthesized speech of the translated text will be available for audio playback from the web user interface.</li>
</ul>
<p>We decided to break the end-to-end translation process into three major steps:</p>
<ol>
<li>Upload the audio recording.</li>
<li>Get the translation text.</li>
<li>Synthesize the speech.</li>
</ol>
<p>This design decision allows the Universal Translator to display the translation text in the web user interface while the translation audio is being synthesized. This way, not only does the application appear more responsive to the user, but the user can also make use of the translation text in certain situations without waiting for the audio synthesis.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the project structure</h1>
                </header>
            
            <article>
                
<p>Let's create a similar base project structure with the steps outlined in <a href="042787e6-6f54-4728-8354-e22d87be0460.xhtml">Chapter 2</a>, <em>Anatomy of a Modern AI Application</em>, including <kbd>pipenv</kbd>,<span> </span><kbd>chalice</kbd>, and the web files:</p>
<ol>
<li>In the terminal, we will create the root project directory, and enter it with the following commands:</li>
</ol>
<pre class="p1" style="padding-left: 60px"><strong><span class="s1">$ mkdir </span>UniversalTranslator<span class="s1"><br/></span><span class="s1">$ cd </span>UniversalTranslator</strong></pre>
<ol start="2">
<li>We will create placeholders for the web frontend by creating a directory named <kbd>Website</kbd>, and, within this directory, we will create the <kbd>index.html</kbd> and <kbd>scripts.js</kbd> files, as shown as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ mkdir Website</strong><br/><strong>$ touch Website/index.html</strong><br/><strong>$ touch Website/scripts.js</strong></pre>
<ol start="3">
<li>We will create a Python 3 virtual environment with<span> </span><kbd>pipenv<span> </span></kbd>in the project's <kbd>root</kbd> directory. Our Python portion of the project needs two packages,<span> </span><kbd>boto3<span> </span></kbd>and<span> </span><kbd>chalice</kbd>. We can install them with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ pipenv --three</strong><br/><strong>$ pipenv install boto3</strong><br/><strong>$ pipenv install chalice</strong></pre>
<ol start="4">
<li>Remember that the Python packages installed via<span> </span><span><kbd>pipenv</kbd> are</span> only available if we activate the virtual environment. One way to do this is with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ pipenv shell</strong></pre>
<ol start="5">
<li>Next, while still in the virtual environment, we will create the orchestration layer as an AWS <kbd>chalice</kbd> project named <kbd>Capabilities</kbd>, with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ chalice new-project Capabilities</strong></pre>
<ol start="6">
<li>To create the <kbd>chalicelib</kbd> Python package, issue the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd Capabilities</strong><br/><strong>mkdir chalicelib</strong><br/><strong>touch chalicelib/__init__.py</strong><br/><strong>cd ..</strong></pre>
<p style="padding-left: 60px">The initial project structure for Universal Translator should look like the following:</p>
<pre style="padding-left: 60px">Project Structure<br/>------------<br/>├── UniversalTranslator/<br/>    ├── Capabilities/<br/>        ├── .chalice/<br/>            ├── config.json<br/>        ├── chalicelib/<br/>            ├── __init__.py<br/>        ├── app.py<br/>        ├── requirements.txt<br/>    ├── Website/<br/>        ├── index.html<br/>        ├── script.js<br/>    ├── Pipfile<br/>    ├── Pipfile.lock</pre>
<p><span>This is the project structure for the Universal Translator; it contains a user interface, orchestration, and the service implementation layers of the AI application architecture that we defined in <a href="042787e6-6f54-4728-8354-e22d87be0460.xhtml">Chapter 2</a>, <em>Anatomy of a Modern AI Application</em>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing services</h1>
                </header>
            
            <article>
                
<p>Let's implement this application layer by layer, starting with the service implementations that contain the crucial AI capabilities that make the Universal Translator tick.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transcription service – speech-to-text</h1>
                </header>
            
            <article>
                
<p>In the Universal Translator, we are going to translate spoken words from one language to another. The first step of this translation process is to know which words were spoken. For this, we are going to use the Amazon Transcribe service. Amazon Transcribe uses deep learning based <strong>Automatic Speech Recognition</strong> (<strong>ASR</strong>) algorithms to generate text from speech.</p>
<p>Let's use the AWS CLI to understand how the Transcribe service works. Issue the following command to start a transcription:</p>
<pre><strong>$ aws transcribe start-transcription-job</strong><br/><strong>  --transcription-job-name &lt;jobname&gt;</strong><br/><strong>  --language-code en-US</strong><br/><strong>  --media-format wav</strong><br/><strong>  --media MediaFileUri=https://s3.amazonaws.com/contents.aws.a/&lt;audio file&gt;.wav</strong><br/><strong>  --output-bucket-name contents.aws.a</strong><br/><strong>{</strong><br/><strong>    "TranscriptionJob": {</strong><br/><strong>        "TranscriptionJobName": "&lt;jobname&gt;",</strong><br/><strong>        "TranscriptionJobStatus": "IN_PROGRESS",</strong><br/><strong>        "LanguageCode": "en-US",</strong><br/><strong>        "MediaFormat": "wav",</strong><br/><strong>        "Media": {</strong><br/><strong>            "MediaFileUri": "https://s3.amazonaws.com/&lt;input bucket&gt;/&lt;audio file&gt;.wav"</strong><br/><strong>        },</strong><br/><strong>        "CreationTime": 1552752370.771</strong><br/><strong>    }</strong><br/><strong>}</strong></pre>
<p>Let's understand the parameters passed in the preceding command:</p>
<ul>
<li>The job name must be a unique ID for each transcription job.</li>
<li>The language code tells the service which language the audio speech is in. Supported languages, at the time of this writing, are <kbd>"en-US"</kbd>, <kbd>"es-US"</kbd>, <kbd>"en-AU"</kbd>, <kbd>"fr-CA"</kbd>, <kbd>"en-GB"</kbd>, <kbd>"de-DE"</kbd>, <kbd>"pt-BR"</kbd>, <kbd>"fr-FR"</kbd>, and <kbd>"it-IT"</kbd>.</li>
<li>The media format specifies the audio format of the speech; possible values are <kbd>"mp3"</kbd>, <kbd>"mp4"</kbd>, <kbd>"wav"</kbd>, and <kbd>"flac"</kbd>.</li>
<li>The media parameter takes a URI to the audio recording, for example, an S3 URL.</li>
<li>The output bucket name specifies in which S3 bucket the transcription output should be stored.</li>
</ul>
<p>You will need to upload an audio recording to an S3 bucket for this command to work. You can use any available software tool to record an audio clip, or you can skip ahead to the <em>Speech Service</em> section of this chapter to learn how to generate speech audio with the Amazon Polly service.</p>
<p>Interestingly, we don't get the transcript in the output of this command. In fact, we can see that the transcription job we just started hasn't finished yet. In the output, <kbd>TranscriptionJobStatus</kbd> is still <kbd>IN_PROGRESS</kbd>. The Amazon Transcribe service follows an asynchronous pattern, which is commonly use for longer running tasks.</p>
<p>So how do we know the job has finished? There is another command as the following shows. We can issue this command to check the status of the job we just started:</p>
<pre><strong>$ aws transcribe get-transcription-job --transcription-job-name &lt;jobname&gt;</strong><br/><strong>{</strong><br/><strong>    "TranscriptionJob": {</strong><br/><strong>        "TranscriptionJobName": "&lt;jobname&gt;",</strong><br/><strong>        "TranscriptionJobStatus": "COMPLETED",</strong><br/><strong>        "LanguageCode": "en-US",</strong><br/><strong>        "MediaSampleRateHertz": 96000,</strong><br/><strong>        "MediaFormat": "wav",</strong><br/><strong>        "Media": {</strong><br/><strong>            "MediaFileUri": "https://s3.amazonaws.com/&lt;input bucket&gt;/&lt;audio file&gt;.wav"</strong><br/><strong>        },</strong><br/><strong>        "Transcript": {</strong><br/><strong>            "TranscriptFileUri": "https://s3.amazonaws.com/&lt;output bucket&gt;/jobname.json"</strong><br/><strong>        },</strong><br/><strong>        "CreationTime": 1552752370.771,</strong><br/><strong>        "CompletionTime": 1552752432.731,</strong><br/><strong>        "Settings": {</strong><br/><strong>            "ChannelIdentification": false</strong><br/><strong>        }</strong><br/><strong>    }</strong><br/><strong>}</strong></pre>
<p>In the preceding command:</p>
<ul>
<li>The <kbd>get-transcription-job</kbd> command takes in one parameter here, which is the unique job name that we specified when we started the job.</li>
<li>When the job status becomes <kbd>"COMPLETED"</kbd>, <kbd>"TranscriptFileUri"</kbd> will point to the JSON output file sitting in the output bucket we specified earlier.</li>
</ul>
<p>This JSON file contains the actual transcription; here's an excerpt:</p>
<pre>{<br/>   "jobName":"jobname",<br/>   "accountId":"...",<br/>   "results":{<br/>      "transcripts":[<br/>         {<br/>            "transcript":"Testing, testing one two three"<br/>         }<br/>      ],<br/>      "items":[<br/>         ...<br/>      ]<br/>   },<br/>   "status":"COMPLETED"<br/>}</pre>
<p>This is the JSON output we need to parse in our service implementation to extract the transcription text.</p>
<p><span>The Transcription Service is implemented as </span><span>a Python class:</span></p>
<ul>
<li>The constructor, <kbd>__init__()</kbd>, creates a <kbd>boto3</kbd> client to the Transcribe service. The constructor also takes in the Storage Service as a dependency for later use.</li>
<li>The <kbd>transcribe_audio()</kbd> method contains the logic to work with the Amazon Transcribe service.</li>
<li>The <kbd>extract_transcript()</kbd> method is a helper method that contains the logic to parse the transcription JSON output from the Transcribe service.</li>
</ul>
<pre style="padding-left: 60px"><span class="s1">import boto3<br/></span><span class="s1">import datetime<br/></span><span class="s1">import time<br/></span><span class="s1">import json<br/><br/></span><span class="s1">class TranscriptionService:<br/></span><span class="s1"><span class="Apple-converted-space">    </span>def __init__(self, storage_service):<br/></span><span class="s1"><span class="Apple-converted-space">        </span>self.client = boto3.client('transcribe')<br/></span><span class="s1"><span class="Apple-converted-space">        </span>self.bucket_name = storage_service.get_storage_location()<br/></span><span class="s1"><span class="Apple-converted-space">        </span>self.storage_service = storage_service<br/><br/></span><span class="s1"><span class="Apple-converted-space">    </span>def transcribe_audio(self, file_name, language):<br/>        ...<br/><br/></span><span class="s1"><span class="Apple-converted-space">    </span>@staticmethod<br/></span><span class="s1"><span class="Apple-converted-space">    </span>def extract_transcript(transcription_output):<br/>        ...<br/></span></pre>
<p>Before we dive deeper into the implementation of the <kbd>"transcribe_audio()"</kbd> and <kbd>"extract_transcript()"</kbd> methods, let's first take a look at the APIs available for Transcribe in the <kbd>boto3</kbd> SDK.</p>
<p>As with the AWS CLI commands, the Amazon Transcribe APIs follow the same asynchronous pattern. We can call <kbd>"start_transcription_job()"</kbd> with <kbd>"TranscriptionJobName"</kbd>, which is the unique identifier for the transcription job. This method invokes the transcription process; however, it does not return the transcription text when the API call finishes. The <kbd>"start_transcription_job()"</kbd> API call also returns a <kbd>"TranscriptionJobStatus"</kbd> field within its response, which can be one of three values, <kbd>"IN_PROGRESS"</kbd>, <kbd>"COMPLETED"</kbd>, or <kbd>"FAILED"</kbd>. We can check on the transcription process with the API call to <kbd>"get_transcritpion_job()"</kbd> with the previously specified <kbd>"TranscriptionJobName"</kbd> to check on the job status.</p>
<p>When the job is completed, the transcription text is placed in an S3 bucket. We can either specify an S3 bucket with <kbd>"OutputBucketName"</kbd> when calling <kbd>"start_transcription_job()"</kbd> or Amazon Transcribe will place the transcription output in a default S3 bucket with a presigned URL to access the transcription text. If the job failed, another <kbd>"FailureReason"</kbd> field in the response of <kbd>"start_transcription_job()"</kbd> or <kbd>"get_transcription_job()"</kbd> will provide the information about why the job failed.</p>
<div class="packt_infobox">This asynchronous pattern is commonly used for longer running processes. Instead of blocking the API caller until the process is completed, it allows the caller to perform other tasks and check back on the process later. Think about the customer experience of ordering an item on Amazon.com. Instead of making the customer wait on the website for the item to be packaged, shipped, and delivered, the customer is immediately shown the confirmation message that the order has been placed, and the customer can check on the order status (with a unique order <span>ID</span>) at a later time.</div>
<p>The <kbd>"transcribe_audio()"</kbd> method in the <kbd>TranscriptionService</kbd> class works around this asynchronous pattern of the <span>Amazon Transcribe </span>APIs:</p>
<pre><span class="s1">def transcribe_audio(self, file_name, language):<br/></span><span class="s1">    POLL_DELAY = 5<br/><br/></span><span class="s1">    language_map = {<br/></span><span class="s1">        'en': 'en-US',<br/></span><span class="s1">        'es': 'es-US',<br/></span><span class="s1">        'fr': 'fr-CA'<br/></span><span class="s1">    }<br/><br/></span><span class="s1">    job_name = file_name + '-trans-' + datetime.datetime.now().strftime("%Y%m%d%H%M%S")<br/><br/></span><span class="s1">    response = self.client.start_transcription_job(<br/></span><span class="s1">        TranscriptionJobName = job_name,<br/></span><span class="s1">        LanguageCode = language_map[language],<br/></span><span class="s1">        MediaFormat = 'wav',<br/></span><span class="s1">        Media = {<br/></span><span class="s1">            'MediaFileUri': "http://" + self.bucket_name + ".s3.amazonaws.com/" + file_name<br/></span><span class="s1">        },<br/></span><span class="s1">        OutputBucketName = self.bucket_name<br/></span><span class="s1">    )<br/><br/></span><span class="s1">    transcription_job = {<br/></span><span class="s1">        'jobName': response['TranscriptionJob']['TranscriptionJobName'],<br/></span><span class="s1">        'jobStatus': 'IN_PROGRESS'<br/></span><span class="s1">    }<br/></span><span class="s1">    while transcription_job['jobStatus'] == 'IN_PROGRESS':<br/></span><span class="s1">        time.sleep(POLL_DELAY)<br/></span><span class="s1">        response = self.client.get_transcription_job(<br/></span><span class="s1">            TranscriptionJobName = transcription_job['jobName']<br/></span><span class="s1">        )<br/></span><span class="s1">        transcription_job['jobStatus'] = response['TranscriptionJob']<br/>                                                ['TranscriptionJobStatus']<br/><br/></span><span class="s1">    transcription_output = self.storage_service.get_file(job_name + '.json')<br/></span><span class="s1">    return self.extract_transcript(transcription_output)</span></pre>
<p>The following is an overview of the <span>preceding</span> <kbd><span>"transcribe_audio()"</span></kbd> implementation:</p>
<ul>
<li>We used a simple Python dictionary to store three pairs of language codes. The abbreviations "en", "es", and "fr" are the language codes used by our Universal Translator, and they are mapped to the language codes used by Amazon Transcribe, "en-US", "es-US", and "fr-CA" respectively.</li>
<li><span>This particular mapping is limited to only three languages to simplify our project. </span><span>However, it does demonstrate the technique for abstracting the language codes, an implementation detail used by third-party services from our application. This way, regardless of the underlying third-party services, we can always standardize the language codes used by our application.</span></li>
<li>We generated a unique name for each transcription job, called <kbd>"job_name"</kbd>. This name combines the audio filename and a string representation of the current time. This way, even if the method is called on the same file multiple times, the job name still remains unique.</li>
<li>This method then calls <kbd>"start_transcription_job()"</kbd> with the unique <kbd>"job_name"</kbd>, language code, media format, the S3 URI where the recording is located, and finally the output bucket name.</li>
<li><span>Even though the Transcribe API is asynchronous, we designed our <kbd>"transcribe_audio()"</kbd> method to be synchronous. To make our synchronous method work with Transcribe's asynchronous APIs, we added a wait loop. This loop waits for the <kbd>POLL_DELAY</kbd> seconds (set at 5 seconds) and then calls the <kbd>"get_transcription_job()"</kbd> method repeatedly to check on the job status. The loop runs while the job status is still <kbd>"IN_PROGRESS"</kbd></span>.</li>
<li>Finally, when the job completes or fails, we grab the contents of the JSON output file from the specified S3 bucket via the Storage Service. This is why we needed the Storage Service as a dependency in the constructor. We then parse the transcription output with an <kbd>"extract_transcript()"</kbd> helper method.</li>
</ul>
<p>Next, we implement the <kbd>"extract_transcript()"</kbd> helper method, previously used by the <kbd>"transcribe_audio()"</kbd> method to parse the Amazon Transcribe output:</p>
<pre><span class="s1">@staticmethod<br/></span><span class="s1">def extract_transcript(transcription_output):<br/></span><span class="s1">    transcription = json.loads(transcription_output)<br/><br/></span><span class="s1">    if transcription['status'] != 'COMPLETED':<br/></span><span class="s1">        return 'Transcription not available.'<br/><br/></span><span class="s1">    transcript = transcription['results']['transcripts'][0]['transcript']<br/></span><span class="s1">    return transcript</span></pre>
<p>We have seen the JSON output format earlier with the transcription job we issued with the AWS CLI. This helper method encapsulates the logic to parse this JSON output. It first checks if the job completed successfully, and, if not, the method returns an error message as the transcription text; otherwise, it returns the actual transcription text.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Translation Service – translating text</h1>
                </header>
            
            <article>
                
<p>Just like in <a href="504c5915-cf10-4cd0-8f5c-3c75466f7dc6.xhtml">Chapter 3</a>, <em>Detecting and Translating Text with Amazon Rekognition and Translat</em>e, the Pictorial Translator application, we are going to leverage the Amazon Translate service to provide the language translation capability. As stated earlier, we can reuse the same implementation of the translation service that we used in the Pictorial Translator project:</p>
<pre><span class="s1">import boto3<br/><br/></span><span class="s1">class TranslationService:<br/></span><span class="s1"><span class="Apple-converted-space">    </span>def __init__(self):<br/></span><span class="s1"><span class="Apple-converted-space">        </span>self.client = boto3.client('translate')<br/><br/></span><span class="s1"><span class="Apple-converted-space">    </span>def translate_text(self, text, source_language = 'auto', target_language = 'en'):<br/></span><span class="s1"><span class="Apple-converted-space">        </span>response = self.client.translate_text(<br/></span><span class="s1"><span class="Apple-converted-space">            </span>Text = text,<br/></span><span class="s1"><span class="Apple-converted-space">            </span>SourceLanguageCode = source_language,<br/></span><span class="s1"><span class="Apple-converted-space">            </span>TargetLanguageCode = target_language<br/></span><span class="s1"><span class="Apple-converted-space">        </span>)<br/><br/></span><span class="s1"><span class="Apple-converted-space">        </span>translation = {<br/></span><span class="s1"><span class="Apple-converted-space">            </span>'translatedText': response['TranslatedText'],<br/></span><span class="s1"><span class="Apple-converted-space">            </span>'sourceLanguage': response['SourceLanguageCode'],<br/></span><span class="s1"><span class="Apple-converted-space">            </span>'targetLanguage': response['TargetLanguageCode']<br/></span><span class="s1"><span class="Apple-converted-space">        </span>}<br/><br/></span><span class="s1"><span class="Apple-converted-space">        </span>return translation</span></pre>
<p><span>The preceding code is the exact same implementation of <kbd>TranslationService</kbd> from the previous project. For completion, we include this code here. For more details on its implementation and design choices, refer to <a href="https://cdp.packtpub.com/hands_on_artificial_intelligence_on_amazon_web_services/wp-admin/post.php?post=300&amp;action=edit#post_298">Chapter 3</a>, <em>Detecting and Translating Text with Amazon Rekognition and Translat</em>e.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speech Service – text-to-speech</h1>
                </header>
            
            <article>
                
<p>Once we have the translation text, we are going to leverage the Amazon Polly service to generate a spoken version of the translation.</p>
<p>Before we dive into the implementation, let's generate a short audio speech using this service with the following AWS CLI command:</p>
<pre><strong>$ aws polly start-speech-synthesis-task</strong><br/><strong>  --output-format mp3</strong><br/><strong>  --output-s3-bucket-name &lt;bucket&gt;</strong><br/><strong>  --text "testing testing 1 2 3"</strong><br/><strong>  --voice-id Ivy</strong><br/><strong>{</strong><br/><strong>    "SynthesisTask": {</strong><br/><strong>        "TaskId": "e68d1b6a-4b7f-4c79-9483-2b5a5932e3d1",</strong><br/><strong>        "TaskStatus": "scheduled",</strong><br/><strong>        "OutputUri": "https://s3.us-east-1.amazonaws.com/&lt;bucket&gt;/&lt;task id&gt;.mp3",</strong><br/><strong>        "CreationTime": 1552754991.114,</strong><br/><strong>        "RequestCharacters": 21,</strong><br/><strong>        "OutputFormat": "mp3",</strong><br/><strong>        "TextType": "text",</strong><br/><strong>        "VoiceId": "Ivy"</strong><br/><strong>    }</strong><br/><strong>}</strong></pre>
<p>This command has four mandatory parameters:</p>
<ul>
<li>The output format is the audio format:
<ul>
<li>For audio stream, this can be <kbd>"mp3"</kbd>, <kbd>"ogg_vorbis"</kbd>, or <kbd>"pcm"</kbd>.</li>
<li>For speech marks, this will be <kbd>"json"</kbd>.</li>
</ul>
</li>
</ul>
<ul>
<li>The output S3 bucket name is where the generated audio file will be placed.</li>
<li>The text is the text to be used for the text-to-speech synthesis.</li>
<li>The voice ID specifies one of many voices available in Amazon Polly. The voice ID indirectly specifies the language, as well as a female or male voice. We used Ivy, which is one of the female voices for US English.</li>
</ul>
<p>The Amazon Polly service follows a similar asynchronous pattern we saw earlier with Amazon Transcribe. To check on the status of the task we just started, we issue the following AWS CLI command:</p>
<pre><strong>$ aws polly get-speech-synthesis-task --task-id e68d1b6a-4b7f-4c79-9483-2b5a5932e3d1</strong><br/><strong>{</strong><br/><strong>    "SynthesisTask": {</strong><br/><strong>        "TaskId": "e68d1b6a-4b7f-4c79-9483-2b5a5932e3d1",</strong><br/><strong>        "TaskStatus": "completed",</strong><br/><strong>        "OutputUri": "https://s3.us-east-1.amazonaws.com/&lt;bucket&gt;/&lt;task id&gt;.mp3",</strong><br/><strong>        "CreationTime": 1552754991.114,</strong><br/><strong>        "RequestCharacters": 21,</strong><br/><strong>        "OutputFormat": "mp3",</strong><br/><strong>        "TextType": "text",</strong><br/><strong>        "VoiceId": "Ivy"</strong><br/><strong>    }</strong><br/><strong>}</strong></pre>
<p>In the preceding command, we have the following:</p>
<ul>
<li>The <kbd>"get-speech-synthesis-task"</kbd> command takes in just one parameter, the task ID that was passed back in the output of the <kbd>"start-speech-synthesis-task"</kbd> command.</li>
<li>When the job status becomes <kbd>"completed"</kbd>, the <kbd>"OutputUri"</kbd> will point to the audio file generated in the S3 bucket we specified earlier.</li>
<li>The audio filename is the task ID with the specified audio format file extension, for example,<kbd>"e68d1b6a-4b7f-4c79-9483-2b5a5932e3d1.mp3"</kbd><span> </span><span class="s1">for the MP3 format.</span></li>
</ul>
<p>Our Speech Service implementation is a Python class with a constructor, <kbd>"__init__()"</kbd>, and a method named <kbd>synthesize_speech()</kbd>. Here is its implementation as follows:</p>
<pre><span class="s1">import boto3<br/></span><span class="s1">import time<br/><br/></span><span class="s1">class SpeechService:<br/></span><span class="s1"><span class="Apple-converted-space">    </span>def __init__(self, storage_service):<br/></span><span class="s1"><span class="Apple-converted-space">        </span>self.client = boto3.client('polly')<br/></span><span class="s1"><span class="Apple-converted-space">        </span>self.bucket_name = storage_service.get_storage_location()<br/></span><span class="s1"><span class="Apple-converted-space">        </span>self.storage_service = storage_service<br/><br/></span><span class="s1"><span class="Apple-converted-space">    </span>def synthesize_speech(self, text, target_language):<br/></span><span class="s1"><span class="Apple-converted-space">        </span>POLL_DELAY = 5<br/></span><span class="s1"><span class="Apple-converted-space">        </span>voice_map = {<br/></span><span class="s1"><span class="Apple-converted-space">            </span>'en': 'Ivy',<br/></span><span class="s1"><span class="Apple-converted-space">            </span>'de': 'Marlene',<br/></span><span class="s1"><span class="Apple-converted-space">            </span>'fr': 'Celine',<br/></span><span class="s1"><span class="Apple-converted-space">            </span>'it': 'Carla',<br/></span><span class="s1"><span class="Apple-converted-space">            </span>'es': 'Conchita'<br/></span><span class="s1"><span class="Apple-converted-space">        </span>}<br/></span><span class="s1"><span class="Apple-converted-space"><br/>        </span>response = self.client.start_speech_synthesis_task(<br/></span><span class="s1"><span class="Apple-converted-space">            </span>Text = text,<br/></span><span class="s1"><span class="Apple-converted-space">            </span>VoiceId = voice_map[target_language],<br/></span><span class="s1"><span class="Apple-converted-space">            </span>OutputFormat = 'mp3',<br/></span><span class="s1"><span class="Apple-converted-space">            </span>OutputS3BucketName = self.bucket_name<br/></span><span class="s1"><span class="Apple-converted-space">        </span>)<br/><br/></span><span class="s1"><span class="Apple-converted-space">        </span>synthesis_task = {<br/></span><span class="s1"><span class="Apple-converted-space">            </span>'taskId': response['SynthesisTask']['TaskId'],<br/></span><span class="s1"><span class="Apple-converted-space">            </span>'taskStatus': 'inProgress'<br/></span><span class="s1"><span class="Apple-converted-space">        </span>}<br/><br/></span><span class="s1"><span class="Apple-converted-space">        </span>while synthesis_task['taskStatus'] == 'inProgress'\<br/></span><span class="s1"><span class="Apple-converted-space">                </span>or synthesis_task['taskStatus'] == 'scheduled':<br/></span><span class="s1"><span class="Apple-converted-space">            </span>time.sleep(POLL_DELAY)<br/><br/></span><span class="s1"><span class="Apple-converted-space">            </span>response = self.client.get_speech_synthesis_task(<br/></span><span class="s1"><span class="Apple-converted-space">                </span>TaskId = synthesis_task['taskId']<br/></span><span class="s1"><span class="Apple-converted-space">            </span>)<br/><br/></span><span class="s1"><span class="Apple-converted-space">            </span>synthesis_task['taskStatus'] = response['SynthesisTask']['TaskStatus']<br/></span><span class="s1"><span class="Apple-converted-space">            </span>if synthesis_task['taskStatus'] == 'completed':<br/></span><span class="s1"><span class="Apple-converted-space">                </span>synthesis_task['speechUri'] = response['SynthesisTask']['OutputUri']<br/></span><span class="s1"><span class="Apple-converted-space">                </span>self.storage_service.make_file_public(synthesis_task['speechUri'])<br/></span><span class="s1"><span class="Apple-converted-space">                </span>return synthesis_task['speechUri']<br/><br/></span><span class="s1"><span class="Apple-converted-space">        </span>return ''</span></pre>
<p>The constructor creates a <kbd>boto3</kbd> client for the Amazon Polly service, and takes in <kbd>StorageService</kbd> as a dependency for later use.</p>
<p>In the <kbd>synthesize_speech()</kbd> method, we used the Python <kbd>voice_map</kbd> dictionary to store five pairs of language codes. The language codes used by our Universal Translator application are <kbd>"en"</kbd>, <kbd>"de"</kbd>, <kbd>"fr"</kbd>, <kbd>"it"</kbd>, and <kbd>"es"</kbd>. Instead of language codes, Amazon Polly uses voice ids which are associated with languages as well as female/male voices. The following is an excerpt of Polly voice mappings:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Language</strong></p>
</td>
<td>
<p><strong>Female ID(s)</strong></p>
</td>
<td>
<p><strong>Male ID(s)</strong></p>
</td>
</tr>
<tr>
<td>
<p>English, British (en-GB)</p>
</td>
<td>
<p>Amy, Emma</p>
</td>
<td>
<p>Brian</p>
</td>
</tr>
<tr>
<td>
<p>German (de-DE)</p>
</td>
<td>
<p>Marlene, Vicki</p>
</td>
<td>
<p>Hans</p>
</td>
</tr>
<tr>
<td>
<p>French (fr-FR)</p>
</td>
<td>
<p>Celine, Lea</p>
</td>
<td>
<p>Mathieu</p>
</td>
</tr>
<tr>
<td>
<p>Italian (it-IT)</p>
</td>
<td>
<p>Carla, Bianca</p>
</td>
<td>
<p>Giorgio</p>
</td>
</tr>
<tr>
<td>
<p>Spanish, European (es-ES)</p>
</td>
<td>
<p>Conchita, Lucia</p>
</td>
<td>
<p>Enrique</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The <kbd>voice_map</kbd> dictionary in this method stored the first female voice ID of each language that the Universal Translator supports. This design choice is to simplify our project implementation. For a more polished voice-to-voice translation application, the developer can choose to support more languages and to provide customizations on the different voices. Again, <kbd>"voice_map"</kbd> abstracts third party service implementation details, the Amazon Polly voice ids, from our application.</p>
<p>Our choices of supported languages are not completely arbitrary here. We specifically picked US English, US Spanish, and Canadian French for the input voices of Amazon Transcribe, and a European variant of the output voices of Amazon Polly. We are targeting customers from North America who are traveling to Europe with our Universal Translator, at least for this <strong>MVP</strong> (<strong>minimal viable product</strong>) version.</p>
<p>The Amazon Polly service APIs follow the same asynchronous pattern as its AWS CLI commands, with the <kbd>"start_speech_synthesis_task()"</kbd> and <kbd>"get_speech_synthesis_task()"</kbd> API calls. The implementation to synthesize speech looks very similar to the transcription implementation. Once again, we call the <kbd>"start_speech_synthesis_task()"</kbd> method to start the long-running process, and then use a while loop to make our method implementation synchronous. This loop waits for the <kbd>POLL_DELAY</kbd> seconds (set at 5 seconds) and then calls the <kbd>"get_speech_synthesis_task()"</kbd> method to check on the job status, which can be <kbd>"scheduled"</kbd>, <kbd>"inProgress"</kbd>, <kbd>"completed"</kbd>, and <kbd>"failed"</kbd>. The loop runs while the status is still <kbd>"scheduled"</kbd> or <kbd>"inProgress"</kbd>.</p>
<p>Notice that, even amongst AWS APIs, the status values are not consistent from service to service. Our speech and transcription services shielded all these implementation details from the rest of our application. In the event that we want to swap in a different speech or transcription service implementation, the changes are isolated in the service implementation layer.</p>
<p>Finally, when the task has the status of <kbd>"completed"</kbd>, we grab the S3 URI of the synthesized audio translation. By default, the file in the S3 bucket will not be publicly accessible, and our web user interface will not be able to play the audio translation. Therefore, before we return the S3 URI, we used our Storage Service <kbd>"make_file_public()"</kbd> method to make the audio translation public. We will take a look at how that's done in the Storage Service implementation next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Storage Service – uploading and retrieving a file</h1>
                </header>
            
            <article>
                
<p>Most of the Storage Service implementation should look familiar from the previous chapter. The <kbd>__init__()</kbd>, constructor, the <kbd>"get_storage_location()"</kbd> method, and the <kbd>"upload_file()"</kbd> method are all exactly the same as in our previous implementations. We added two new methods to extend the functionalities of <kbd>StorageService</kbd>.</p>
<p>Here is the complete implementation:</p>
<pre><span class="s1">import boto3<br/><br/></span><span class="s1">class StorageService:<br/></span><span class="s1"><span class="Apple-converted-space">    </span>def __init__(self, storage_location):<br/></span><span class="s1"><span class="Apple-converted-space">        </span>self.client = boto3.client('s3')<br/></span><span class="s1"><span class="Apple-converted-space">        </span>self.bucket_name = storage_location<br/></span><span class="s1"><span class="Apple-converted-space"><br/>    </span>def get_storage_location(self):<br/></span><span class="s1"><span class="Apple-converted-space">        </span>return self.bucket_name<br/><br/></span><span class="s1"><span class="Apple-converted-space">    </span>def upload_file(self, file_bytes, file_name):<br/></span><span class="s1"><span class="Apple-converted-space">        </span>self.client.put_object(Bucket = self.bucket_name,<br/></span><span class="s1"><span class="Apple-converted-space">                               </span>Body = file_bytes,<br/></span><span class="s1"><span class="Apple-converted-space">                               </span>Key = file_name,<br/></span><span class="s1"><span class="Apple-converted-space">                               </span>ACL = 'public-read')<br/><br/></span><span class="s1"><span class="Apple-converted-space">        </span>return {'fileId': file_name,<br/></span><span class="s1"><span class="Apple-converted-space">                </span>'fileUrl': "http://" + self.bucket_name + ".s3.amazonaws.com/" + file_name}<br/><br/></span><span class="s1"><span class="Apple-converted-space">    </span>def get_file(self, file_name):<br/></span><span class="s1"><span class="Apple-converted-space">        </span>response = self.client.get_object(Bucket = self.bucket_name, Key = file_name)<br/><br/></span><span class="s1"><span class="Apple-converted-space">        </span>return response['Body'].read().decode('utf-8')<br/><br/></span><span class="s1"><span class="Apple-converted-space">    </span>def make_file_public(self, uri):<br/></span><span class="s1"><span class="Apple-converted-space">        </span>parts = uri.split('/')<br/></span><span class="s1"><span class="Apple-converted-space">        </span>key = parts[-1]<br/></span><span class="s1"><span class="Apple-converted-space">        </span>bucket_name = parts[-2]<br/><br/></span><span class="s1"><span class="Apple-converted-space">        </span>self.client.put_object_acl(Bucket = bucket_name,<br/></span><span class="s1"><span class="Apple-converted-space">                                   </span>Key = key,<br/></span><span class="s1"><span class="Apple-converted-space">                                   </span>ACL = 'public-read')</span></pre>
<p> Let's have a look at the two new class methods:</p>
<ul>
<li>The <kbd>get_file()</kbd> method takes a filename and returns that file's content as a string. We accomplish this by using the <kbd>boto3</kbd> S3 client to get the object by key (filename) from the bucket name (Storage Service's storage location), and then decode the file content as a UTF-8 string.</li>
<li>The <kbd>make_file_public()</kbd> method takes a file URI and changes the <strong>Access Control List</strong> (<strong>ACL</strong>) of the target file to allow public access. Since our Storage Service is backed by AWS S3, the method assumes the URI is an S3 URI and parses it accordingly to extract the bucket name and key. With the bucket name and key, it then changes the object's ACL to <kbd>'public-read'</kbd>.</li>
</ul>
<p>All of the methods within the Storage Service are designed to be generic, so that they are more likely to be reused by different applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing RESTful endpoints</h1>
                </header>
            
            <article>
                
<p>Now that the services are implemented, let's move on to the orchestration layer with the RESTful endpoints. Since all of the real work is done by the service implementations, the endpoints are used to stitch the capabilities together and to provide HTTP access for the user interface layer to use these capabilities. The implementation code, therefore, is concise and easy to understand.</p>
<p>The <kbd>app.py</kbd> file contains the RESTful endpoint implementations. Here's a snippet from <kbd>app.py</kbd> that includes the imports, configuration, and initialization code:</p>
<pre><span class="s1">from chalice import Chalice<br/>from chalicelib import storage_service<br/>from chalicelib import transcription_service<br/>from chalicelib import translation_service<br/>from chalicelib import speech_service<br/><br/>import base64<br/>import json<br/><br/>#####<br/># chalice app configuration<br/>#####<br/>app = Chalice(app_name='Capabilities')<br/>app.debug = True<br/><br/>#####<br/># services initialization<br/>#####<br/>storage_location = 'contents.aws.ai'<br/>storage_service = storage_service.StorageService(storage_location)<br/>transcription_service = transcription_service.TranscriptionService(storage_service)<br/>translation_service = translation_service.TranslationService()<br/>speech_service = speech_service.SpeechService(storage_service)<br/><br/><br/>#####<br/># RESTful endpoints<br/>#####<br/>...</span></pre>
<p>We will discuss each endpoint implementation in <kbd>app.py</kbd> in detail in the next few sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Translate recording endpoint</h1>
                </header>
            
            <article>
                
<p><span>The Translate Recording endpoint is an HTTP POST endpoint that takes JSON parameters in the request's body.</span> This endpoint takes the recording ID as a parameter, and <span>uses a JSON body to pass in the source and target languages of the translation</span>:</p>
<pre><span>@app.route</span>(<span>'/recordings/{recording_id}/translate-text'</span>, <span>methods </span>= [<span>'POST'</span>], <span>cors </span>= <span>True</span>)<br/><span>def </span>translate_recording(recording_id):<br/>    <span>"""transcribes the specified audio then translates the transcription text"""<br/></span><span>    </span>request_data = json.loads(app.current_request.raw_body)<br/>    from_lang = request_data[<span>'fromLang'</span>]<br/>    to_lang = request_data[<span>'toLang'</span>]<br/><br/>    transcription_text = transcription_service.transcribe_audio(recording_id, from_lang)<br/><br/>    translation_text = translation_service.translate_text(transcription_text,<br/>                                                        <span>target_language </span>= to_lang)<br/><br/>    <span>return </span>{<br/>        <span>'text'</span>: transcription_text,<br/>        <span>'translation'</span>: translation_text<br/>    }</pre>
<p><span>The annotation right above this function describes the HTTP request that can access the endpoint:</span></p>
<pre>POST &lt;server url&gt;/recordings/{recording_id}/translate-text<br/>{<br/>    "fromLang": &lt;SOURCE LANGUAGE&gt;,<br/>    "toLang": &lt;TARGET LANGUAGE&gt;<br/>}</pre>
<p>Let's look at the preceding code:</p>
<ul>
<li><span>The annotation right above the <kbd>"transcribe_recording()"</kbd></span> describes the HTTP POST request that can access the endpoint.</li>
<li>The function first gets the request data that contains the source language, <kbd>"fromLang",</kbd> and target language, <kbd>"toLang"</kbd>, for the translation.</li>
<li>The <kbd>"transcribe_recording()"</kbd> function calls to the Transcription Service to transcribe the audio recording.</li>
<li>Next, this function calls the Translation Service to translate the transcription text.</li>
<li>Finally, this function returns a JSON object containing both the transcription text and the translation information.</li>
</ul>
<p>Let's test this endpoint out by running <kbd>chalice local</kbd> in the Python virtual environment, and then issue the following <kbd>curl</kbd> command that specifies an audio clip that has already been uploaded to our S3 bucket:</p>
<pre><strong>$ curl --header "Content-Type: application/json" --request POST --data '{"fromLang":"en","toLang":"de"}' http://127.0.0.1:8000/recordings/&lt;recording id&gt;/translate-text</strong><br/><strong>[</strong><br/><strong>    {</strong><br/><strong>        "text": "&lt;transcription&gt;",</strong><br/><strong>        "translation": {</strong><br/><strong>            "translatedText": "&lt;translation&gt;",</strong><br/><strong>            "sourceLanguage": "en",</strong><br/><strong>            "targetLanguage": "de"</strong><br/><strong>        }</strong><br/><strong>    }</strong><br/><strong>]</strong></pre>
<p>The <kbd>&lt;recording id&gt;</kbd> identifies the filename of the audio file in our S3 bucket.</p>
<p>This is the JSON that our web user interface will receive and use to display the translation to the user.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Synthesize speech endpoint</h1>
                </header>
            
            <article>
                
<p>The Synthesize Speech endpoint is an HTTP POST endpoint that takes JSON parameters in the request's body. This endpoint uses JSON to pass in the target language and the text to be converted into speech. Even though Universal Translator is designed to translate short phrases, the text used to perform text-to-speech on can potentially be long, depending on the application. We are using a JSON payload here, as opposed to the URL parameters, because there's a limit to the length of URLs. This design decision makes the endpoint more reusable for other applications in the future. It is also a good practice to keep the URLs of your application short and clean:</p>
<pre><span class="s1">@app.route('/synthesize_speech', methods = ['POST'], cors = True)<br/>def synthesize_speech():<br/>    """performs text-to-speech on the specified text / language"""<br/>    request_data = json.loads(app.current_request.raw_body)<br/>    </span>text = request_data[<span>'text'</span>]<br/>    language = request_data[<span>'language'</span>]<span class="s1"><br/><br/>    translation_audio_url = speech_service.synthesize_speech(text, language)<br/><br/>    return {<br/>        'audioUrl': translation_audio_url<br/>    }</span></pre>
<p><span>The annotation right above this function describes the HTTP request that can access this endpoint:</span></p>
<pre>POST &lt;server url&gt;/synthesize_speech<br/>{<br/>    "text": &lt;TEXT&gt;,<br/>    "language": &lt;LANGUAGE&gt;<br/>}</pre>
<p>In the preceding code, we have the following:</p>
<ul>
<li><span>The <kbd>synthesize_speech()</kbd> function parses the request body as JSON data to get the text and the language for the speech synthesis.</span></li>
<li><span>The function then calls the Speech Service's <kbd>synthesize_speech()</kbd> method.</span></li>
<li><span>The function then returns the URL to the audio file. Remember that we already made this audio file publicly accessible before <kbd>synthesize_speech()</kbd> returned.</span></li>
</ul>
<p>Let's test this endpoint out by running <kbd>chalice local</kbd> in the Python virtual environment, and then issue the following <kbd>curl</kbd> command to pass in the JSON payload:</p>
<pre><strong>$ curl --header "Content-Type: application/json" --request POST --data '{"text":"Dies ist ein Test des Amazons Polly Service.","language":"de"}' http://127.0.0.1:8000/synthesize_speech</strong><br/><strong>{</strong><br/><strong>  "audioUrl": "https://s3.us-east-1.amazonaws.com/&lt;bucket&gt;/&lt;task id&gt;.mp3"</strong><br/><strong>}</strong></pre>
<p><span>This is the JSON that our web user interface will receive and use to update the audio player for the translation speech.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upload recording Endpoint</h1>
                </header>
            
            <article>
                
<p>This endpoint is essentially the same as the Upload Image Endpoint from <a href="https://cdp.packtpub.com/hands_on_artificial_intelligence_on_amazon_web_services/wp-admin/post.php?post=300&amp;action=edit#post_298">Chapter 3</a><span>, </span><em>Detecting and Translating Text with Amazon Rekognition and Translat</em><span>e</span><em>,</em> Pictorial Translator application. It uses the same two functions that we implemented in the project without modification. The only change is the <kbd>@app.route</kbd> annotation, where we created a different HTTP POST endpoint, <kbd>/recordings</kbd>, that takes uploads via Base64 encoding:</p>
<pre><span>@app.route</span>(<span>'/recordings'</span>, <span>methods </span>= [<span>'POST'</span>], <span>cors </span>= <span>True</span>)<br/><span>def </span>upload_recording():<br/>    """processes file upload and saves file to storage service"""<br/>    request_data = json.loads(app.current_request.raw_body)<br/>    file_name = request_data['filename']<br/>    file_bytes = base64.b64decode(request_data['filebytes'])<br/><br/>    file_info = storage_service.upload_file(file_bytes, file_name)<br/><br/>    <span>return </span>file_info</pre>
<p><span>For completion, we include the code for both the endpoint and its helper function here. For more details on their implementations, refer to <a href="504c5915-cf10-4cd0-8f5c-3c75466f7dc6.xhtml">Chapter 3</a>, <em>Detecting and Translating Text with Amazon Rekognition and Translate</em>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the Web User Interface</h1>
                </header>
            
            <article>
                
<p>Next, let's create a simple web user interface with HTML and JavaScript in the <kbd>index.html</kbd> and <kbd>scripts.js</kbd> files in the <kbd>Website</kbd> directory.</p>
<p>This is what the final web interface looks like:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ec702afc-5450-4647-9fcf-4e26e20704ba.png" style=""/></div>
<p>In this application, the user first selects the languages to translate from and to in the <span class="packt_screen">Select Languages</span> section. The user then records a short speech in the <span class="packt_screen">Record Audio</span> section. In this section, the user can also playback the recording to check for quality. Then the translation process kicks off. When the translation text becomes available, it is displayed to the user in the <span class="packt_screen">Translation Text</span> section. Then the text-to-speech generation process is started. When the generated audio translation becomes available, the audio player controls are enabled to allow playback of the translation.</p>
<p>We made the design decision to treat the steps of every translation as sequential, meaning only one translation can be performed at a time. There is a certain amount of wait time for each end-to-end translation, mostly due to the speed of Amazon Transcribe and Amazon Polly services. There are a few techniques to improve the user experience during the wait time:</p>
<ul>
<li>A most important technique is actually letting the user know that the application is processing. We employed spinners in the <span class="packt_screen">Translation Text</span> and <span class="packt_screen">Translation Audio</span> sections while the application is processing. The fact that we are displaying spinners gives the clue to the user that these steps are not instantaneous.</li>
<li>Another technique we employed is breaking up the text translation and audio translation steps. Even though the total amount of processing time stays about the same, the user sees progress and intermediate results. Psychologically, this significantly reduces the perception of the wait time for the user.</li>
<li>We could also reduce the <kbd>POLL_DELAY</kbd> in our Transcription Service and Speech Service implementations. Currently, the <kbd>POLL_DELAY</kbd> is set to 5 seconds in both. This results in some delay after the processing is completed, on average 2.5 seconds of delay in each step and on average 5 seconds in total. We can certainly reduce the delay. However, there is a tradeoff here: shorter <kbd>POLL_DELAY</kbd> will result in more AWS API calls to <kbd>"get_transcription_job()"</kbd> and <kbd>"get_speech_synthesis_task()"</kbd> functions.</li>
<li>Finally, we could use real-time services for faster processing if they are available. For example, Amazon Transcribe now supports real-time transcription with a feature called Streaming Transcription. This feature enables applications to pass in live audio streams and receive text transcripts in real time. Unfortunately, at the time of this writing, this feature is not available in the Python AWS SDK. A flexible architecture will allow future service implementations, AWS-based or otherwise, to be more easily swapped in for long term evolution of the application.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">index.html</h1>
                </header>
            
            <article>
                
<p>Following is the <kbd>index.html</kbd> file. We are using standard HTML tags here, so the code of the web page should be easy to follow:</p>
<pre>&lt;!doctype html&gt;<br/>&lt;html lang="en"/&gt;<br/><br/>&lt;head&gt;<br/>    &lt;meta charset="utf-8"/&gt;<br/>    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"/&gt;<br/><br/>    &lt;title&gt;Universal Translator&lt;/title&gt;<br/><br/>    &lt;link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css"&gt;<br/>    &lt;link rel="stylesheet" href="https://www.w3schools.com/lib/w3-theme-blue-grey.css"&gt;<br/>    &lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;<br/>&lt;/head&gt;<br/><br/>&lt;body class="w3-theme-14"&gt;<br/>    &lt;div style="min-width:400px"&gt;<br/>        &lt;div class="w3-bar w3-large w3-theme-d4"&gt;<br/>            &lt;span class="w3-bar-item"&gt;Universal Translator&lt;/span&gt;<br/>        &lt;/div&gt;<br/><br/>        ...<br/>    &lt;/div&gt;<br/><br/>    &lt;script src="https://github.com/streamproc/MediaStreamRecorder/releases/download/1.3.4/MediaStreamRecorder.js"&gt;&lt;/script&gt;<br/>   &lt;script src="scripts.js"&gt;&lt;/script&gt;<br/>&lt;/body&gt;<br/><br/>&lt;/html&gt;</pre>
<p>This code snippet shows the frame and the title of the web user interface:</p>
<ul>
<li>In addition to the W3 stylesheets we have used in previous projects, we also included the Font-Awesome CSS for the spinners.</li>
<li>At the bottom of <kbd>index.html</kbd>, we have included <kbd>MediaStreamRecorder.js</kbd> for the audio recording functionality in the web user interface.</li>
<li>The rest of the <kbd>index.html</kbd> code snippet goes inside the top level <kbd>&lt;div&gt;</kbd> tag within the <kbd>&lt;body&gt;</kbd></li>
</ul>
<pre style="padding-left: 30px">    ...<br/>        &lt;div class="w3-container w3-content"&gt;<br/>            &lt;p class="w3-opacity"&gt;&lt;b&gt;Select Languages&lt;/b&gt;&lt;/p&gt;<br/>            &lt;div class="w3-panel w3-white w3-card w3-display-container  <br/>            w3-center"&gt;<br/>                &lt;div&gt;<br/>                    &lt;b class="w3-opacity"&gt;From:&lt;/b&gt;<br/>                    &lt;select id="fromLang"&gt;<br/>                        &lt;option value="en"&gt;English&lt;/option&gt;<br/>                        &lt;option value="es"&gt;Spanish&lt;/option&gt;<br/>                        &lt;option value="fr"&gt;French&lt;/option&gt;<br/>                    &lt;/select&gt;<br/>                    &lt;hr&gt;<br/>                    &lt;b class="w3-opacity"&gt;To:&lt;/b&gt;<br/>                    &lt;select id="toLang"&gt;<br/>                        &lt;option value="de"&gt;German&lt;/option&gt;<br/>                        &lt;option value="fr"&gt;French&lt;/option&gt;<br/>                        &lt;option value="it"&gt;Italian&lt;/option&gt;<br/>                        &lt;option value="es"&gt;Spanish&lt;/option&gt;<br/>                    &lt;/select&gt;<br/>                &lt;/div&gt;<br/>            &lt;/div&gt;<br/>        &lt;/div&gt;<br/><br/>        &lt;div class="w3-container w3-content"&gt;<br/>            &lt;p class="w3-opacity"&gt;&lt;b&gt;Record Audio&lt;/b&gt;&lt;/p&gt;<br/>            &lt;div class="w3-panel w3-white w3-card w3-display-container <br/>            w3-center"&gt;<br/>                &lt;div&gt;<br/>                    &lt;audio id="recording-player" controls&gt;<br/>                        Your browser does not support the audio  <br/>                        element...<br/>                    &lt;/audio&gt;<br/>                &lt;/div&gt;<br/>                &lt;div&gt;<br/>                    &lt;input type="button" id="record-toggle" <br/>                    value="Record"<br/>                           onclick="toggleRecording()"/&gt;<br/>                    &lt;input type="button" id="translate" <br/>                     value="Translate"<br/>                           onclick="uploadAndTranslate()" disabled/&gt;<br/>                &lt;/div&gt;<br/>            &lt;/div&gt;<br/>        &lt;/div&gt;<br/>     ...</pre>
<p>In this code snippet, we have the following:</p>
<ul>
<li>We created the <kbd>Select Languages</kbd> and <span class="packt_screen">Record Audio</span> sections of the web user interface.</li>
<li>In the <span class="packt_screen">Select Languages</span> section, we hardcoded the supported <kbd>fromLang</kbd> and <kbd>toLang</kbd> in <kbd>dropdown</kbd> lists.</li>
<li>In the <span class="packt_screen">Record Audio</span> section, we used the <kbd>&lt;audio&gt;</kbd> tag to create an audio player with a couple of input buttons to control the recording and translation functions.</li>
<li>Most of the dynamic behaviors are implemented in <kbd>scripts.js</kbd>.</li>
</ul>
<p>To continue with the <kbd>index.html</kbd> code, execute the following command:</p>
<pre>...<br/>        &lt;div class="w3-container w3-content"&gt;<br/>            &lt;p class="w3-opacity"&gt;&lt;b&gt;Translation Text&lt;/b&gt;&lt;/p&gt;<br/>            &lt;div class="w3-panel w3-white w3-card w3-display-container <br/>            w3-center"&gt;<br/>                &lt;p id="text-spinner" hidden&gt;<br/>                    &lt;i class="fa fa-spinner w3-spin" style="font-<br/>                    size:64px"&gt;&lt;/i&gt;<br/>                &lt;/p&gt;<br/>                &lt;p class="w3-opacity"&gt;&lt;b&gt;Transcription:&lt;/b&gt;&lt;/p&gt;<br/>                &lt;div id="transcription"&gt;&lt;/div&gt;<br/>                &lt;hr&gt;<br/>                &lt;p class="w3-opacity"&gt;&lt;b&gt;Translation:&lt;/b&gt;&lt;/p&gt;<br/>                &lt;div id="translation"&gt;&lt;/div&gt;<br/>            &lt;/div&gt;<br/>        &lt;/div&gt;<br/><br/>        &lt;div class="w3-container w3-content"&gt;<br/>            &lt;p class="w3-opacity"&gt;&lt;b&gt;Translation Audio&lt;/b&gt;&lt;/p&gt;<br/>            &lt;div class="w3-panel w3-white w3-card w3-display-container <br/>            w3-center"&gt;<br/>                &lt;p id="audio-spinner" hidden&gt;<br/>                    &lt;i class="fa fa-spinner w3-spin" style="font-<br/>                     size:64px"&gt;&lt;/i&gt;<br/>                &lt;/p&gt;<br/>                &lt;audio id="translation-player" controls&gt;<br/>                    Your browser does not support the audio element...<br/>                &lt;/audio&gt;<br/>            &lt;/div&gt;<br/>        &lt;/div&gt;<br/>...</pre>
<p>In this code snippet, we have the following:</p>
<ul>
<li>We created the <kbd>Translation Text</kbd> and <span class="packt_screen">Translation Audio</span> sections of the web user interface.</li>
<li>In the <span class="packt_screen">Translation Text</span> section, we placed a spinner that's initially hidden from view and a couple of <kbd>&lt;div&gt;</kbd> that will be used to display the translation results.</li>
<li>In the <span class="packt_screen">Translation Audio</span> section, we placed another spinner that's also initially hidden from view, along with an audio player that will be used to play back the translation audio.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">scripts.js</h1>
                </header>
            
            <article>
                
<p>The following is the <kbd>scripts.js</kbd> file. Much of the Universal Translator's dynamic behaviors are implemented in JavaScript. <kbd>scripts.js</kbd> is interacting with the endpoints and stitching together the overall user experience of the application:</p>
<pre>"use strict";<br/><br/>const serverUrl = "http://127.0.0.1:8000";<br/><br/>...<br/><br/>class HttpError extends Error {<br/>    constructor(response) {<br/>        super(`${response.status} for ${response.url}`);<br/>        this.name = "HttpError";<br/>        this.response = response;<br/>    }<br/>}</pre>
<p>This snippet defines the <kbd>serverUrl</kbd> as the address of <kbd>chalice local</kbd>. It also defines the <kbd>HttpError</kbd> to handle the exceptions that might occur during the HTTP requests:</p>
<pre><span class="s1">let audioRecorder;<br/></span><span class="s1">let recordedAudio;<br/></span><span class="s1"><br/>const maxAudioLength = 30000;<br/></span><span class="s1">let audioFile = {};</span><span class="s1"><br/><br/></span><span class="s1">const mediaConstraints = {<br/></span><span class="s1"><span class="Apple-converted-space"> </span>audio: true<br/></span><span class="s1">};<br/><br/></span><span class="s1">navigator.getUserMedia(mediaConstraints, onMediaSuccess, onMediaError);</span><span class="s1"><br/><br/></span><span class="s1">function onMediaSuccess(audioStream) {<br/></span><span class="s1"><span class="Apple-converted-space"> </span>audioRecorder = new MediaStreamRecorder(audioStream);<br/></span><span class="s1"><span class="Apple-converted-space"> </span>audioRecorder.mimeType = "audio/wav";<br/></span><span class="s1"><span class="Apple-converted-space"> </span>audioRecorder.ondataavailable = handleAudioData;<br/></span><span class="s1">}<br/><br/></span><span class="s1">function onMediaError(error) {<br/></span><span class="s1"><span class="Apple-converted-space"> </span>alert("audio recording not available: " + error.message);<br/></span><span class="s1">}<br/><br/></span><span class="s1">function startRecording() {<br/></span><span class="s1"><span class="Apple-converted-space"> </span>recordedAudio = [];<br/></span><span class="s1"><span class="Apple-converted-space"> </span>audioRecorder.start(maxAudioLength);<br/></span><span class="s1">}<br/><br/></span><span class="s1">function stopRecording() {<br/></span><span class="s1"><span class="Apple-converted-space"> </span>audioRecorder.stop();<br/></span><span class="s1">}<br/><br/></span><span class="s1">function handleAudioData(audioRecording) </span><span class="s2">{<br/></span><span class="s1"><span class="Apple-converted-space"> </span>audioRecorder.stop();<br/></span><span class="s1"><span class="Apple-converted-space"> </span>audioFile = new File([</span><span class="s1">audioRecording]</span><span class="s1">,</span><span class="s1"> "recorded_audio.wav", {type: "audio/wav"});<br/><br/></span><span class="s1"> let audioElem = document.getElementById("recording-player");<br/></span><span class="s1"> audioElem.src = window.URL.createObjectURL(audioRecording);<br/></span><span class="s1">}</span></pre>
<p>This code snippet follows the recommended audio recording implementation from <a href="https://github.com/intercom/MediaStreamRecorder">https://github.com/intercom/MediaStreamRecorder</a> which we will not cover. There are a few details to point out:</p>
<ul>
<li>Universal Translator supports upto 30 seconds of audio recording, defined in the <kbd>maxAudioLength</kbd> constant. This length should be sufficient for translation of short phrases.</li>
<li>The audio recording format is set to <kbd>audio/wav</kbd>, which is one of the formats supported by Amazon Transcribe.</li>
<li>When the audio recording is completed, we perform two tasks:
<ul>
<li> We place the recorded bits in a JavaScript File object with the filename <kbd>recorded_audio.wav</kbd>; this will be the filename of the uploaded recording to S3. Since the recordings all have the same filename, a previously uploaded recording will be replaced when a new recording is uploaded.</li>
<li>We update the audio player in the <span class="packt_screen">Record Audio</span> section with an Object URL to the recorded audio for playback.</li>
</ul>
</li>
</ul>
<pre style="padding-left: 90px">let isRecording = false;<br/><br/>function toggleRecording() {<br/>    let toggleBtn = document.getElementById("record-toggle");<br/>    let translateBtn = document.getElementById("translate");<br/><br/>    if (isRecording) {<br/>        toggleBtn.value = 'Record';<br/>        translateBtn.disabled = false;<br/>        stopRecording();<br/>    } else {<br/>        toggleBtn.value = 'Stop';<br/>        translateBtn.disabled = true;<br/>        startRecording();<br/>    }<br/><br/>    isRecording = !isRecording;<br/>}</pre>
<p>The <kbd>toggleRecording</kbd> function in <kbd>scripts.js</kbd> makes the first input button beneath the audio player a toggle button. This toggle button starts or stops the audio recording with the <kbd>MediaStreamRecorder</kbd> implementation preceding.</p>
<p>Next, we define five functions:</p>
<ul>
<li><kbd>uploadRecording()</kbd>: Uploads the audio recording via Base64 encoding to our Upload Recording Endpoint</li>
<li><kbd>translateRecording()</kbd>: Calls our Translate Recording Endpoint to translate the audio recording</li>
<li><kbd>updateTranslation()</kbd>: Updates the <span class="packt_screen">Translation Text</span> section with the returned transcription and translation texts</li>
<li><kbd>synthesizeTranslation()</kbd>: Calls our Synthesize Speech Endpoint to generate audio speech of the translation text</li>
<li><kbd>updateTranslationAudio()</kbd>: Updates the audio player in the <span class="packt_screen">Translation Audio</span> section with the audio speech URL to enable playback</li>
</ul>
<p>These functions correspond to the sequential steps of the translation user experience. We broke them into individual functions to make the JavaScript code more modular and readable; each function performs a specific task. Let's go through the implementation details of these functions.</p>
<p><span>Let's have a look at the <kbd>uploadRecording()</kbd> function </span><span>as shown in the following code block</span><span>:</span></p>
<pre><span>async function uploadRecording() {<br/>    // encode recording file as base64 string for upload<br/>    let converter = new Promise(function(resolve, reject) {<br/>        const reader = new FileReader();<br/>        reader.readAsDataURL(audioFile);<br/>        reader.onload = () =&gt; resolve(reader.result<br/>            .toString().replace(/^data:(.*,)?/, ''));<br/>        reader.onerror = (error) =&gt; reject(error);<br/>    });<br/>    let encodedString = await converter;<br/><br/>    // make server call to upload image<br/>    // and return the server upload promise<br/>    return fetch(serverUrl + "/recordings", {<br/>        method: "POST",<br/>        headers: {<br/>            'Accept': 'application/json',<br/>            'Content-Type': 'application/json'<br/>        },<br/>        body: JSON.stringify({filename: audioFile.name, filebytes: encodedString})<br/>    }).then(response =&gt; {<br/>        if (response.ok) {<br/>            return response.json();<br/>        } else {<br/>            throw new HttpError(response);<br/>        }<br/>    })<br/>}</span></pre>
<p>In the preceding code, we have the following:</p>
<ul>
<li>The <kbd>uploadRecording()</kbd> function creates a Base64 encoded string from the File object that's holding the audio recording.</li>
<li>This function formats the JSON payload to include the <kbd>filename</kbd> and <kbd>filebytes</kbd> that our endpoint is expecting.</li>
<li>It then sends the HTTP POST request with the JSON payload to our Upload Recording Endpoint URL and returns the response JSON.</li>
<li>This is almost the same code as the <kbd>uploadImage()</kbd> function in the Pictorial Translator application; the only difference is the File is coming from the audio recorder.</li>
</ul>
<p><span>Let's have a look at the <kbd>translateRecording()</kbd> function </span><span>as shown in the following code block</span><span>:</span></p>
<pre>let fromLang;<br/>let toLang;<br/><br/><span>function </span><span>translateRecording</span>(audio) {<br/>    <span>let </span><span>fromLangElem </span>= <span>document</span>.<span>getElementById</span>(<span>"fromLang"</span>);<br/>    <span>fromLang </span>= <span>fromLangElem</span>[<span>fromLangElem</span>.<span>selectedIndex</span>].<span>value</span>;<br/>    <span>let </span><span>toLangElem </span>= <span>document</span>.<span>getElementById</span>(<span>"toLang"</span>);<br/>    <span>toLang </span>= <span>toLangElem</span>[<span>toLangElem</span>.<span>selectedIndex</span>].<span>value</span>;<br/><br/>    <span>// start translation text spinner<br/></span><span>    </span><span>let </span><span>textSpinner </span>= <span>document</span>.<span>getElementById</span>(<span>"text-spinner"</span>);<br/>    <span>textSpinner</span>.<span>hidden </span>= <span>false</span>;<br/><br/>    <span>// make server call to transcribe recorded audio<br/></span><span>    </span><span>return </span><span>fetch</span>(<span>serverUrl </span>+ <span>"/recordings/" </span>+ audio[<span>"fileId"</span>] + <span>"/translate-text"</span>, {<br/>        <span>method</span>: <span>"POST"</span>,<br/>        <span>headers</span>: {<br/>            <span>'Accept'</span>: <span>'application/json'</span>,<br/>            <span>'Content-Type'</span>: <span>'application/json'<br/></span><span>        </span>},<br/>        <span>body</span>: <span>JSON</span>.<span>stringify</span>({<span>fromLang</span>: <span>fromLang</span>, <span>toLang</span>: to<span>Lang</span>})<br/>    }).<span>then</span>(response =&gt; {<br/>        <span>if </span>(response.<span>ok</span>) {<br/>            <span>return </span>response.<span>json</span>();<br/>        } <span>else </span>{<br/>            <span>throw new </span>HttpError(response);<br/>        }<br/>    })<br/>}</pre>
<p>In the preceding code, we have the following:</p>
<ul>
<li>The <kbd>translateRecording()</kbd> function first grabs the values of the languages from the <kbd>fromLang</kbd> and <kbd>toLang </kbd>dropdowns in the web user interface.</li>
<li>The function then starts the spinner, signaling the start of the translation process to the user. It then calls our Translate Recording Endpoint and waits for the response.</li>
</ul>
<p><span>Let's have a look at the <kbd>updateTranslation()</kbd> function </span><span>as shown in the following code block:</span></p>
<pre>function updateTranslation(translation) {<br/>    // stop translation text spinner<br/>    let textSpinner = document.getElementById("text-spinner");<br/>    textSpinner.hidden = true;<br/><br/>    let transcriptionElem = document.getElementById("transcription");<br/>    transcriptionElem.appendChild(document.createTextNode(translation["text"]));<br/><br/>    let translationElem = document.getElementById("translation");<br/>    translationElem.appendChild(document.createTextNode(translation["translation"]<br/>                                                        ["translatedText"]));<br/><br/>    return translation<br/>}</pre>
<p>In the preceding code, the following applies:</p>
<ul>
<li>When the Translate Recording Endpoint responds, the <kbd>updateTranslation()</kbd> function hides the spinner.</li>
<li>The function then updates the <kbd>Translation Text</kbd> section with the transcription and translation texts.</li>
</ul>
<p><span>Let's have a look at the <kbd>synthesizeTranslation()</kbd> function </span><span>as shown in the following code block</span><span>:</span></p>
<pre>function synthesizeTranslation(translation) {<br/>    // start translation audio spinner<br/>    let audioSpinner = document.getElementById("audio-spinner");<br/>    audioSpinner.hidden = false;<br/><br/>    // make server call to synthesize translation audio<br/>    return fetch(serverUrl + "/synthesize_speech", {<br/>        method: "POST",<br/>        headers: {<br/>            'Accept': 'application/json',<br/>            'Content-Type': 'application/json'<br/>        },<br/>        body: JSON.stringify({text: translation["translation"]["translatedText"], language: toLang})<br/>    }).then(response =&gt; {<br/>        if (response.ok) {<br/>            return response.json();<br/>        } else {<br/>            throw new HttpError(response);<br/>        }<br/>    })<br/>}</pre>
<p>In the preceding code, we have the following:</p>
<ul>
<li>The <span><kbd>synthesizeTranslation()</kbd> function</span> starts the spinner to signal the start of the speech synthesis process to the user.</li>
<li>This function then calls the Synthesize Speech Endpoint and waits for the response. Remember this endpoint is expecting JSON parameters, which it is setting in the <kbd>fetch()</kbd> call.</li>
</ul>
<p><span>Let's have a look at the <kbd>updateTranslationAudio()</kbd> function </span><span>as shown in the following code block</span><span>:</span></p>
<pre>function updateTranslationAudio(audio) {<br/>    // stop translation audio spinner<br/>    let audioSpinner = document.getElementById("audio-spinner");<br/>    audioSpinner.hidden = true;<br/><br/>    let audioElem = document.getElementById("translation-player");<br/>    audioElem.src = audio["audioUrl"];<br/>}</pre>
<p>In the preceding code, we have the following:</p>
<ul>
<li>When the Synthesize Speech Endpoint responds, the <kbd>updateTranslationAudio()</kbd> function stops the audio synthesis spinner.</li>
<li>This function then updates the audio player with the URL of the synthesized translation audio.</li>
</ul>
<p>All preceding five functions are stitched together by the <kbd>uploadAndTranslate()</kbd> function as follows:</p>
<pre>function uploadAndTranslate() {<br/>    let toggleBtn = document.getElementById("record-toggle");<br/>    toggleBtn.disabled = true;<br/>    let translateBtn = document.getElementById("translate");<br/>    translateBtn.disabled = true;<br/><br/>    uploadRecording()<br/>        .then(audio =&gt; translateRecording(audio))<br/>        .then(translation =&gt; updateTranslation(translation))<br/>        .then(translation =&gt; synthesizeTranslation(translation))<br/>        .then(audio =&gt; updateTranslationAudio(audio))<br/>        .catch(error =&gt; {<br/>            alert("Error: " + error);<br/>        })<br/><br/>    toggleBtn.disabled = false;<br/>}</pre>
<p>Notice how clear the sequence of events are in the <kbd>uploadAndTranslate()</kbd> function. By way of a final step in this function, we enable the record toggle button so that the user can start the next translation.</p>
<p>The final project structure for the Universal Translator application should be as follows:</p>
<pre>├── Capabilities<br/>│ ├── app.py<br/>│ ├── chalicelib<br/>│ │ ├── __init__.py<br/>│ │ ├── speech_service.py<br/>│ │ ├── storage_service.py<br/>│ │ ├── transcription_service.py<br/>│ │ └── translation_service.py<br/>│ └── requirements.txt<br/>├── Pipfile<br/>├── Pipfile.lock<br/>└── Website<br/>    ├── index.html<br/>    └── scripts.js</pre>
<p><span>Now, we have completed the implementation of the Universal Translator application.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying the Universal Translator to AWS</h1>
                </header>
            
            <article>
                
<p><span>The deployment steps for the Universal Translator application is the same as the deployment steps of the projects in the previous chapters. We include them here for completion.</span></p>
<ol>
<li>First, let's tell Chalice to perform policy analysis for us by setting <kbd>"autogen_policy"</kbd> to <kbd>false</kbd> in the <kbd>config.json</kbd> file in the <kbd>.chalice</kbd> directory of the project structure:</li>
</ol>
<pre style="padding-left: 60px">{<br/>  "version": "2.0",<br/>  "app_name": "Capabilities",<br/>  "stages": {<br/>    "dev": {<br/>      "autogen_policy": false,<br/>      "api_gateway_stage": "api"<br/>    }<br/>  }<br/>}</pre>
<ol start="2">
<li><span>Next, we create a new file <kbd>policy-dev.json</kbd> in the <kbd>chalice</kbd> directory to manually specify the AWS services the project needs:</span></li>
</ol>
<pre style="padding-left: 60px">{<br/><span> "Version"</span>: <span>"2012-10-17"</span>,<br/><span> "Statement"</span>: [<br/> {<br/><span> "Effect"</span>: <span>"Allow"</span>,<br/><span> "Action"</span>: [<br/><span> "logs:CreateLogGroup"</span>,<br/><span> "logs:CreateLogStream"</span>,<br/><span> "logs:PutLogEvents"</span>,<br/><span> "s3:*"</span>,<br/><span> "translate:*",<br/> "transcribe:*",<br/> "polly:*"</span><span><br/></span> ],<br/><span> "Resource"</span>: <span>"*"<br/></span> }<br/> ]<br/>}</pre>
<ol start="3">
<li>Next, we deploy the Chalice backend to AWS by running the following command within the <kbd>Capabilities</kbd> directory:</li>
</ol>
<pre style="padding-left: 60px">$ chalice deploy<br/>Creating deployment package.<br/>Creating IAM role: Capabilities-dev<br/>Creating lambda function: Capabilities-dev<br/>Creating Rest API<br/>Resources deployed:<br/>  - Lambda ARN: arn:aws:lambda:us-east-1:&lt;UID&gt;:function:Capabilities-dev<br/>  - Rest API URL: https://&lt;UID&gt;.execute-api.us-east-1.amazonaws.com/api/</pre>
<p style="padding-left: 60px">When the deployment is complete, Chalice will output a RESTful API URL that looks similar to <kbd>https://&lt;UID&gt;.execute-api.us-east-1.amazonaws.com/api/</kbd> where the <kbd>&lt;UID&gt;</kbd> is a unique identifier string. This is the server URL your frontend app should hit to access the application backend running on AWS.</p>
<ol start="4">
<li>Next we will upload the <kbd>index.html</kbd> and <kbd>scripts.js</kbd> files to this S3 bucket<span>, and then </span><span>set the permissions to publicly readable</span>. Before we do that, we need to make a change in <kbd>scripts.js</kbd> as shown in the following. Remember, the website will be running in the cloud now, and won't have access to our local HTTP server. Replace the local server URL with the one from our backend deployment:</li>
</ol>
<pre style="padding-left: 60px">"use strict";<br/>const serverUrl = "https://&lt;UID&gt;.execute-api.us-east-1.amazonaws.com/api";<br/>...</pre>
<p>Now the Universal Translator application is accessible to everyone on the Internet!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discussing the project enhancement ideas</h1>
                </header>
            
            <article>
                
<p><span>At the end of each hands-on project in Part 2, we provide you with a few ideas to extend the intelligence-enabled application. Here are a couple of ideas to enhance the Universal Translator:</span></p>
<ul>
<li>Allow users to save default source language and output voice preferences within the application. The user is likely to use his or her native language as the source language and may prefer the translated speech to match the his or her gender and voice.</li>
<li>Add real-time transcription with Amazon Transcribe's <span>Streaming Transcription feature. This feature can greatly reduce the user's wait time for the voice translation. At the time of this writing, the Python SDK does not support this feature, so your implementation will need a different SDK. Our architecture does support a polyglot system, a system written in multiple languages.</span></li>
<li>The Universal Translator and the Pictorial Translator both provide translation capabilities. These two forms of translation capability can be combined into a single application for travelers and students, especially a mobile app that's always with the user in the real-world.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we built a Universal Translator application to <span>translate spoken speech from one language to another. We combined speech-to-text, language translation, and text-to-speech capabilities from AWS AI services, including Amazon Transcribe, Amazon Translate, and Amazon Polly. This hands-on project </span>continued our journey as AI practitioners to develop the skills and intuition for real-world AI applications. Along the way, we also d<span>iscussed user experience and product design decisions of our Universal Translator application. Additionally, we d</span>emonstrated clean code reuse of <span>Translation Service and Storage Service in the </span>reference architecture defined in <a href="042787e6-6f54-4728-8354-e22d87be0460.xhtml">Chapter 2</a><span>, </span><em>Anatomy of a Modern AI Application</em>.</p>
<p>In the next chapter, we will leverage more AWS AI services to create solutions that can simplify our lives. Being an AI practitioner is not just about knowing which services or APIs to use, but is also about being skilled in fusing good product and architecture designs with AI capabilities.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p>For more information on Performing Speech-to-Text and Vice-versa with Amazon Transcribe and Amazon Polly, please refer to the following links:</p>
<ul>
<li><a href="https://www.verizonwireless.com/wireless-devices/accessories/google-pixel-buds/">https://www.verizonwireless.com/wireless-devices/accessories/google-pixel-buds/</a></li>
<li><a href="https://www.washingtonpost.com/news/the-switch/wp/2017/11/15/i-tried-out-googles-translating-headphones-heres-what-i-found/?utm_term=.1cef17d669e2">https://www.washingtonpost.com/news/the-switch/wp/2017/11/15/i-tried-out-googles-translating-headphones-heres-what-i-found/?utm_term=.1cef17d669e2</a></li>
<li><a href="https://github.com/intercom/MediaStreamRecorder">https://github.com/intercom/MediaStreamRecorder</a></li>
</ul>


            </article>

            
        </section>
    </body></html>