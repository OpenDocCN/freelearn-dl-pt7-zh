<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Advanced Classifiers"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Advanced Classifiers</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A simple classifier</li><li class="listitem" style="list-style-type: disc">Language model classifier with tokens</li><li class="listitem" style="list-style-type: disc">Naïve Bayes</li><li class="listitem" style="list-style-type: disc">Feature extractors</li><li class="listitem" style="list-style-type: disc">Logistic regression</li><li class="listitem" style="list-style-type: disc">Multithreaded cross validation</li><li class="listitem" style="list-style-type: disc">Tuning parameters in logistic regression</li><li class="listitem" style="list-style-type: disc">Customizing feature extraction</li><li class="listitem" style="list-style-type: disc">Combining feature extractors</li><li class="listitem" style="list-style-type: disc">Classifier-building life cycle</li><li class="listitem" style="list-style-type: disc">Linguistic tuning</li><li class="listitem" style="list-style-type: disc">Thresholding classifiers</li><li class="listitem" style="list-style-type: disc">Train a little, learn a little – active learning</li><li class="listitem" style="list-style-type: disc">Annotation</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec30"/>Introduction</h1></div></div></div><p>This chapter introduces more sophisticated classifiers that use different learning techniques as well as richer observations about the data (features). We will also address the best practices for building machine-learning systems as well as data annotation and approaches that minimize the amount of training data needed.</p></div></div>
<div class="section" title="A simple classifier"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec31"/>A simple classifier</h1></div></div></div><p>This recipe is a <a class="indexterm" id="id235"/>thought experiment that should help make clear what machine learning does. Recall the <span class="emphasis"><em>Training your own language model classifier</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, to train your own sentiment classifier in the recipe. Consider what a conservative approach to the same problem might be—build <code class="literal">Map&lt;String,String&gt;</code> from the inputs to the correct class. This recipe will explore how this might work and what its consequences might be.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec70"/>How to do it...</h2></div></div></div><p>Brace yourself; this will be spectacularly stupid but hopefully informative.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Enter the following in the command line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.OverfittingClassifier</strong></span>
</pre></div></li><li class="listitem">The usual anemic prompt appears, with some user input:<div class="informalexample"><pre class="programlisting">Training
Type a string to be classified. Empty string to quit.
When all else fails #Disney
Category is: e</pre></div></li><li class="listitem">It correctly gets the language as <code class="literal">e</code> or English. However, everything else is about to fail. Next, we will use the following code:<div class="informalexample"><pre class="programlisting">Type a string to be classified. Empty string to quit.
When all else fails #Disne
Category is: n</pre></div><p>We just dropped the final <code class="literal">y</code> on <code class="literal">#Disney</code>, and as a result, we got a big confused classifier. What happened?</p></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec71"/>How it works...</h2></div></div></div><p>This section should really be called <span class="emphasis"><em>How it doesn't work</em></span>, but let's dive into the details anyway.</p><p>Just to be clear, this recipe <a class="indexterm" id="id236"/>is not recommended as an actual solution to a classification problem that requires any flexibility at all. However, it introduces a minimal example of how to work with LingPipe's <code class="literal">Classification</code> class as well as makes clear what an extreme case of overfitting looks like; this in turn, helps demonstrate how machine learning is different from most of standard computer engineering.</p><p>Starting with the <code class="literal">main()</code> method, we will get into standard code slinging that should be familiar to you from <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>:</p><div class="informalexample"><pre class="programlisting">String dataPath = args.length &gt; 0 ? args[0] : "data/disney_e_n.csv";
List&lt;String[]&gt; annotatedData = Util.readAnnotatedCsvRemoveHeader(new File(dataPath));

OverfittingClassifier classifier = new OverfittingClassifier();
System.out.println("Training");
for (String[] row: annotatedData) {
  String truth = row[Util.ANNOTATION_OFFSET];
  String text = row[Util.TEXT_OFFSET];
  classifier.handle(text,new Classification(truth));
}
Util.consoleInputBestCategory(classifier);</pre></div><p>Nothing novel is going on <a class="indexterm" id="id237"/>here—we are just training up a classifier, as shown in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, and then supplying the classifier to the <code class="literal">Util.consoleInputBestCategory()</code> method. Looking at the class code reveals what is going on:</p><div class="informalexample"><pre class="programlisting">public class OverfittingClassifier implements BaseClassifier&lt;CharSequence&gt; {

  Map&lt;String,Classification&gt; mMap 
         = new HashMap&lt;String,Classification&gt;();  

   public void handle(String text, Classification classification) {mMap.put(text, classification);
  }</pre></div><p>So, the <code class="literal">handle()</code> method <a class="indexterm" id="id238"/>takes the <code class="literal">text</code> and <code class="literal">classification</code> pair and stuffs them in <code class="literal">HashMap</code>. The classifier does nothing else to learn from the data so training amounts to memorization of the data:</p><div class="informalexample"><pre class="programlisting">@Override
public Classification classify(CharSequence text) {
  if (mMap.containsKey(text)) {
    return mMap.get(text);
  }
  return new Classification("n");
}</pre></div><p>The <code class="literal">classify()</code> method just does a lookup into <code class="literal">Map</code> and returns the value if there is one, otherwise, we will get the category <code class="literal">n</code> as the return classification.</p><p>What is good about the preceding code is that you have a minimalist example of a <code class="literal">BaseClassifier</code> implementation, and you can see how the <code class="literal">handle()</code> method adds data to the classifier.</p><p>What is bad about the preceding code is the utter rigidity of the mapping from training data to categories. If the exact example is not seen in training, then the <code class="literal">n</code> category is assumed.</p><p>This is an extreme example of overfitting, but it essentially conveys what it means to have an overfit model. An overfit model is tailored too close to the training data and cannot generalize well to new data.</p><p>Let's think a bit more about what is so wrong about the preceding classifier for language identification—the issue is that entire sentences/tweets are the wrong unit of processing. Words/tokens are a much better measure of what language is being used. Some improvements that will be borne <a class="indexterm" id="id239"/>out in the later recipes are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Break the text into words/tokens.</li><li class="listitem" style="list-style-type: disc">Instead of a match/no-match decision, consider a more nuanced approach. A simple <span class="emphasis"><em>which language matches more words</em></span> will be a huge improvement.</li><li class="listitem" style="list-style-type: disc">As languages get closer, for example, British versus American English, probabilities can be called for that. Pay attention to likely discriminating words.</li></ul></div><p>While this recipe might be comically inappropriate for the task at hand, consider trying a sentiment for an even more ludicrous example. It embodies a core assumption of much of computer science that the world of inputs is discrete and finite. Machine learning can be viewed as a response to a world where this is not the case.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec72"/>There's more…</h2></div></div></div><p>Oddly enough, we often have a need for such a classifier in commercial systems—we call it the management classifier; it runs preemptively on data. It has happened that a senior VP is unhappy with the system output for some example. This classifier then can be trained with the exact case that allows for immediate system fixing and satisfaction of the VP.</p></div></div>
<div class="section" title="Language model classifier with tokens"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec32"/>Language model classifier with tokens</h1></div></div></div><p>
<a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, covered classification without knowing what tokens/words were, with a <a class="indexterm" id="id240"/>language model per category—we <a class="indexterm" id="id241"/>used character slices or ngrams to model the text. <a class="link" href="ch02.html" title="Chapter 2. Finding and Working with Words">Chapter 2</a>, <span class="emphasis"><em>Finding and Working with Words</em></span>, discussed at length the process of finding tokens in text, and now we can use them to build a classifier. Most of the time, we use tokenized input to classifiers, so this recipe is an important introduction to the concept.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec73"/>How to do it...</h2></div></div></div><p>This recipe will tell us how to train and use a tokenized language model classifier, but it will ignore issues such as evaluation, serialization, deserialization, and so on. You can refer to the recipes in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, for examples. This code of this recipe is in <code class="literal">com.lingpipe.cookbook.chapter3.TrainAndRunTokenizedLMClassifier</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The exception of the following code is the same as found in the<span class="emphasis"><em> Training your own language model classifier</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>. The <code class="literal">DynamicLMClassifier</code> class provides a static method for the creation of a tokenized LM classifier. Some setup is required. The <code class="literal">maxTokenNgram</code> variable sets the largest size of token sequences used in the classifier—smaller datasets usually benefit from lower order (number of tokens) ngrams. Next, we will set up a <code class="literal">tokenizerFactory</code> method, selecting the workhorse tokenizer from <a class="link" href="ch02.html" title="Chapter 2. Finding and Working with Words">Chapter 2</a>, <span class="emphasis"><em>Finding and Working with Words</em></span>. Finally, we will specify the categories that the classifier uses:<div class="informalexample"><pre class="programlisting">int maxTokenNGram = 2;
TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
String[] categories = Util.getCategories(annotatedData);</pre></div></li><li class="listitem">Next, the classifier is constructed:<div class="informalexample"><pre class="programlisting">DynamicLMClassifier&lt;TokenizedLM&gt; classifier = DynamicLMClassifier.createTokenized(categories,tokenizerFactory,maxTokenNGram);</pre></div></li><li class="listitem">Run the code from the command line or your IDE:<div class="informalexample"><pre class="programlisting">java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.TrainAndRunTokenizedLMClassifier</pre></div></li></ol></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec74"/>There's more...</h2></div></div></div><p>In application, the <code class="literal">DynamicLMClassifier</code> classifier does not see a great deal of use in commercial application. This classifier might be a good choice for an author-identification classifier (that is, one <a class="indexterm" id="id242"/>that classifies whether a given piece <a class="indexterm" id="id243"/>of text is written by an author or by someone else) that was highly sensitive to turns of phrase and exact word usage. The Javadoc is well worth consulting to better understand what this class does.</p></div></div>
<div class="section" title="Na&#xEF;ve Bayes"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec33"/>Naïve Bayes</h1></div></div></div><p>Naïve Bayes is probably <a class="indexterm" id="id244"/>the world's most famous classification technology, and just to keep you on your toes, we provide two separate implementations with lots of configurability. One of the most well-known applications of a Naïve Bayes classifier is for spam filtering in an e-mail.</p><p>The reason the word <span class="emphasis"><em>naïve</em></span> is used is that the classifier assumes that words (features) occur independent of one another—this is clearly a naïve assumption, but lots of useful and not-so-useful technologies have been based on the approach. Some notable features of the traditional naïve Bayes include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Character <a class="indexterm" id="id245"/>sequences are converted to bags of tokens with counts. No whitespaces are considered, and the order of the tokens does not matter.</li><li class="listitem" style="list-style-type: disc">Naïve Bayes classifiers require two or more categories into which input texts are categorized. These categories must be both exhaustive and mutually exclusive. This indicates that a document used for training must only belong to one category.</li><li class="listitem" style="list-style-type: disc">The math is very simple: <code class="literal">p(category|tokens) = p(category,tokens)/p(tokens)</code>.</li><li class="listitem" style="list-style-type: disc">The class is configurable for various kinds of unknown token models.</li></ul></div><p>A naïve Bayes classifier estimates two things. First, it estimates the probability of each category, independent of any tokens. This is carried out based on the number of training examples presented for each category. Second, for each category, it estimates the probability of seeing each token in that category. Naïve Bayes is so useful and important that we will show you exactly how it works and plug through the formulas. The example we have is to classify hot and <a class="indexterm" id="id246"/>cold weather based on the text.</p><p>First, we will work out the math to calculate the probability of a category given a word sequence. Second, we will plug in an example and then verify it using the classifier we build.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec75"/>Getting ready</h2></div></div></div><p>Let's lay out the basic formula to calculate the probability of a category given a text input. A token-based naïve Bayes classifier computes the joint token count and category probabilities as follows:</p><div class="informalexample"><pre class="programlisting">p(tokens,cat) = p(tokens|cat) * p(cat)</pre></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Conditional probabilities are derived by applying Bayes's rule to invert the probability calculation:<div class="informalexample"><pre class="programlisting">p(cat|tokens) = p(tokens,cat) / p(tokens)
               = p(tokens|cat) * p(cat) / p(tokens)</pre></div></li><li class="listitem">Now, we will get to expand all these terms. If we look at <code class="literal">p(tokens|cat)</code>, this is where the naïve assumption comes into play. We assume that each token is independent, and thus, the probability of all the tokens is the product of the probability of each token:<div class="informalexample"><pre class="programlisting">p(tokens|cat) = p(tokens[0]|cat) * p(tokens[1]|cat) * . . . * p(tokens[n]|cat)</pre></div><p>The probability of the tokens themselves, that is, <code class="literal">p(tokens)</code>, the denominator in the preceding equation. This is just the sum of their probability in each category weighted by the probability of the category itself:</p><div class="informalexample"><pre class="programlisting">p(tokens) = p(tokens|cat1) * p(cat1) + p(tokens|cat2) * p(cat2) + . . . + p(tokens|catN) * p(catN)</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note06"/>Note</h3><p>When building a naïve Bayes classifier, <code class="literal">p(tokens)</code> doesn't need to be explicitly calculated. Instead, we can use <code class="literal">p(tokens|cat) * p(cat) </code>and assign the tokens to the category with the higher product.</p></div></div></li><li class="listitem">Now that we have laid out each element of our equation, we can look at how these probabilities <a class="indexterm" id="id247"/>are calculated. We can calculate both these probabilities using simple frequencies.<p>The probability of a category is calculated by counting the number of times the category showed up in the training instances divided by the total number of training instances. As we know that Naïve Bayes classifiers have exhaustive and mutually-exclusive categories, the sum of the frequency of each category must equal the total number of training instances:</p><div class="informalexample"><pre class="programlisting">p(cat) = frequency(cat) / (frequency(cat1) + frequency(cat2) + . . . + frequency(catN))</pre></div><p>The probability of a token in a category is computed by counting the number of times the token appeared in a category divided by the number of times all the other tokens appeared in this category:</p><div class="informalexample"><pre class="programlisting">p(token|cat) = frequency(token,cat)/(frequency(token1,cat) + frequency(token2,cat) + . . . + frequency(tokenN,cat)</pre></div><p>These probabilities are calculated to provide what is called the <span class="strong"><strong>maximum likelihood estimate</strong></span> of the model. Unfortunately, these estimates provide zero probability for tokens that were not seen during the training. You can see this very easily in the calculation of an unseen token probability. Since it wasn't seen, it will have a frequency count of 0, and the numerator of our original equation goes to 0.</p><p>In order to overcome this, we will use a technique known as <span class="strong"><strong>smoothing</strong></span> that <a class="indexterm" id="id248"/>assigns a prior and then computes a maximum a posteriori estimate rather than a maximum likelihood estimate. A very common smoothing technique is called additive smoothing, and it just involves adding a prior count to every count in the training data. Two sets of counts are added: the first is a token count added to all the token frequency calculations, and the second is a category count, which is added to all the category count calculations.</p><p>This obviously changes the <code class="literal">p(cat)</code> and the <code class="literal">p(token|cat)</code> values. Let's call the <code class="literal">alpha</code> prior that is added to the category count <a class="indexterm" id="id249"/>and the <code class="literal">beta</code> prior that is added to the token count. When we call the <code class="literal">alpha</code> prior, our previous calculations will change to:</p><div class="informalexample"><pre class="programlisting">p(cat) = frequency(cat) + alpha / [(frequency(cat1) + alpha) + (frequency(cat2)+alpha) + . . . + (frequency(catN) + alpha)]</pre></div><p>When we call the <code class="literal">beta</code> prior, the calculations will change to:</p><div class="informalexample"><pre class="programlisting">p(token|cat) = (frequency(token,cat)+beta) / [(frequency(token1,cat)+beta) + frequency(token2,cat)+beta) + . . . + (frequency(tokenN,cat) + beta)]</pre></div></li><li class="listitem">Now that we have set up our equations, let's look at a concrete example.<p>We'll build a classifier to classify whether the forecast calls for hot or cold weather based on a set of phrases:</p><div class="informalexample"><pre class="programlisting">hot : super steamy today
hot : boiling out
hot : steamy out

cold : freezing out
cold : icy</pre></div><p>There are a total of seven tokens in these five training items:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">super</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">steamy</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">today</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">boiling</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">out</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">freezing</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">icy</code></li></ul></div><p>Of these, all the tokens appear once, except <code class="literal">steamy</code>, which appears twice in the <code class="literal">hot</code> category and <code class="literal">out</code>, which appears once in each category. This is our training data. Now, let's calculate the probability of an input text being in the <code class="literal">hot</code> or <code class="literal">cold</code> category . Let's say our input is the word <code class="literal">super</code>. Let's set the category prior <code class="literal">alpha</code> to <code class="literal">1</code> and the token prior <code class="literal">beta</code> also to <code class="literal">1</code>.</p></li><li class="listitem">So, we will calculate <a class="indexterm" id="id250"/>the probabilities of <code class="literal">p(hot|super)</code> and <code class="literal">p(cold|super)</code>:<div class="informalexample"><pre class="programlisting">p(hot|super) = p(super|hot) * p(hot)/ p(super)

p(super|hot) = (freq(super,hot) + beta) / [(freq(super|hot)+beta) + (freq(steamy|hot) + beta) + . . . + (freq(freezing|hot)+beta)</pre></div><p>We will take into consideration all the tokens, including the ones that haven't been seen in the <code class="literal">hot</code> category:</p><div class="informalexample"><pre class="programlisting">freq(super|hot) + beta = 1 + 1 = 2
freq(steamy|hot) + beta = 2 + 1 = 3
freq(today|hot) + beta = 1 + 1 = 2
freq(boiling|hot) + beta = 1 + 1 = 2
freq(out|hot) + beta = 1 + 1 = 2
freq(freezing|hot) + beta = 0 + 1 = 1
freq(icy|hot) + beta = 0 + 1 = 1</pre></div><p>This will give us a denominator equal to a sum of these inputs:</p><div class="informalexample"><pre class="programlisting">2+3+2+2+2+1+1 = 13</pre></div></li><li class="listitem">Now, <code class="literal">p(super|hot) = 2/13</code> is one part of the equation. We still need to calculate <code class="literal">p(hot)</code> and <code class="literal">p (super)</code>:<div class="informalexample"><pre class="programlisting">p(hot) = (freq(hot) + alpha) / 
                    ((freq(hot) + alpha) + freq(cold)+alpha)) </pre></div><p>For the <code class="literal">hot</code> category, we have three documents or cases, and for the <code class="literal">cold</code> category, we have two documents in our training data. So, <code class="literal">freq(hot) = 3</code> and <code class="literal">freq(cold) = 2</code>:</p><div class="informalexample"><pre class="programlisting">p(hot) = (3 + 1) / (3 + 1) + (2 +1) = 4/7
Similarly p(cold) = (2 + 1) / (3 + 1) + (2 +1) = 3/7
Please note that p(hot) = 1 – p(cold)

p(super) = p(super|hot) * p(hot) + p(super|cold) + p(cold)</pre></div><p>To calculate <code class="literal">p(super|cold)</code>, we need to repeat the same steps:</p><div class="informalexample"><pre class="programlisting">p(super|cold) = (freq(super,cold) + beta) / [(freq(super|cold)+beta) + (freq(steamy|cold) + beta) + . . . + (freq(freezing|cold)+beta)

freq(super|cold) + beta = 0 + 1 = 1
freq(steamy|cold) + beta = 0 + 1 = 1
freq(today|cold) + beta = 0 + 1 = 1
freq(boiling|cold) + beta = 0 + 1 = 1
freq(out|cold) + beta = 1 + 1 = 2
freq(freezing|cold) + beta = 1 + 1 = 2
freq(icy|cold) + beta = 1 + 1 = 2

p(super|cold) = freq(super|cold)+beta/sum of all terms above

              = 0 + 1 / (1+1+1+1+2+2+2) = 1/10</pre></div><p>This gives us <a class="indexterm" id="id251"/>the probability of the token <code class="literal">super</code>:</p><div class="informalexample"><pre class="programlisting">P(super) = p(super|hot) * p(hot) + p(super|cold) * p(cold)
         = 2/13 * 4/7 + 1/10 * 3/7</pre></div><p>We now have all the pieces together to calculate <code class="literal">p(hot|super)</code> and <code class="literal">p(cold|super)</code>:</p><div class="informalexample"><pre class="programlisting">p(hot|super) = p(super|hot) * p(hot) / p(super)
             = (2/13 * 4/7) / (2/13 * 4/7 + 1/10 * 3/7)

             = 0.6722
p(cold|super) = p(super|cold) * p(cold) /p(super)
             = (1/10 * 3/7) / (2/13 * 4/7 + 1/10 * 3/7)
             = 0.3277

Obviously, p(hot|super) = 1 – p(cold|super)</pre></div><p>If we want to repeat this for the input stream <code class="literal">super super</code>, the following calculations can be used:</p><div class="informalexample"><pre class="programlisting">p(hot|super super) = p(super super|hot) * p(hot) / p(super super)
             = (2/13 * 2/13 * 4/7) / (2/13 * 2/13 * 4/7 + 1/10 * 1/10 * 3/7)
             = 0.7593
p(cold|super super) = p(super super|cold) * p(cold) /p(super super)
             = (1/10 * 1/10 * 3/7) / (2/13 * 2/13 * 4/7 + 1/10 * 1/10 * 3/7)
             = 0.2406</pre></div><p>Remember our naïve assumption: the probability of the tokens is the product of the probabilities, since we assume that they are independent of each other.</p></li></ol></div><p>Let's verify our calculations by <a class="indexterm" id="id252"/>training up the naïve Bayes classifier and using the same input.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec76"/>How to do it...</h2></div></div></div><p>Let's verify some of these calculations in code:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In your IDE, run the <code class="literal">TrainAndRunNaiveBayesClassifier</code> class in the code package of this chapter, or using the command line, type the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.TrainAndRunNaiveBayesClassifier</strong></span>
</pre></div></li><li class="listitem">In the prompt, let's use our first example, <code class="literal">super</code>:<div class="informalexample"><pre class="programlisting">Type a string to be classified
super
h 0.67   
c 0.33   </pre></div></li><li class="listitem">As we can see, our calculations were correct. For the case of a word, <code class="literal">hello</code>, that doesn't exist in our training; we will fall back to the prevalence of the categories modified by the category's prior counts:<div class="informalexample"><pre class="programlisting">Type a string to be classified
hello
h 0.57   
c 0.43</pre></div></li><li class="listitem">Again, for the case of <code class="literal">super super</code>, our calculations were correct.<div class="informalexample"><pre class="programlisting">Type a string to be classified
<span class="strong"><strong>super super</strong></span>
</pre></div><div class="informalexample"><pre class="programlisting">h 0.76   
c 0.24    </pre></div></li><li class="listitem">The source that generates the preceding output is in <code class="literal">src/com/lingpipe/chapter3/TrainAndRunNaiveBays.java</code>. The code should be straightforward, so we will not covering it in this recipe.</li></ol></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec77"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For more details on <a class="indexterm" id="id253"/>configuring naïve Bayes, including length normalizing, refer to the Javadoc at <a class="ulink" href="http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html">http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html</a></li><li class="listitem" style="list-style-type: disc">You can refer to the <a class="indexterm" id="id254"/>expectation maximization tutorial at <a class="ulink" href="http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html</a></li></ul></div></div></div>
<div class="section" title="Feature extractors"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec34"/>Feature extractors</h1></div></div></div><p>Up until now, we have <a class="indexterm" id="id255"/>been using characters and words to train our models. We are about to introduce a classifier (logistic regression) that allows for other observations about the data to inform the classifier—for example, whether a word is actually a date. Feature extractors are used in CRF taggers and K-means clustering. This recipe will introduce feature extractors independent of any technology that uses them.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec78"/>How to do it...</h2></div></div></div><p>There is not much to this recipe, but the upcoming <span class="emphasis"><em>Logistic regression</em></span> recipe has many moving parts, and this is one of them.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Fire up your IDE or type in the command line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter3.SimpleFeatureExtractor</strong></span>
</pre></div></li><li class="listitem">Type a string into our standard I/O loop:<div class="informalexample"><pre class="programlisting">Type a string to see its features
My first feature extraction!</pre></div></li><li class="listitem">Features are then produced:<div class="informalexample"><pre class="programlisting">!=1
My=1
extraction=1
feature=1
first=1</pre></div></li><li class="listitem">Note that there is no order information here. Does it keep a count or not?<div class="informalexample"><pre class="programlisting">Type a string to see its features
My my my what a nice feature extractor.
my=2
.=1
My=1
a=1
extractor=1
feature=1
nice=1
what=1</pre></div></li><li class="listitem">The feature extractor keeps count with <code class="literal">my=2</code>, and it does not normalize the case (<code class="literal">My</code> is different from <code class="literal">my</code>). Refer to the later recipes in this chapter on how to modify feature <a class="indexterm" id="id256"/>extractors—they are very flexible.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec79"/>How it works…</h2></div></div></div><p>LingPipe provides <a class="indexterm" id="id257"/>solid infrastructure for the creation of feature extractors. The code for this recipe is in <code class="literal">src/com/lingipe/chapter3/SimpleFeatureExtractor.java</code>:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException {
  TokenizerFactory tokFact 
    = IndoEuropeanTokenizerFactory.<span class="strong"><strong>INSTANCE</strong></span>;
  FeatureExtractor&lt;CharSequence&gt; tokenFeatureExtractor 
    = new TokenFeatureExtractor(tokFact);</pre></div><p>The preceding code constructs <code class="literal">TokenFeatureExtractor</code> with <code class="literal">TokenizerFactory</code>. It is one of the 13 <code class="literal">FeatureExtractor</code> implementations provided in LingPipe.</p><p>Next, we will apply the I/O loop and print out the feature, which is <code class="literal">Map&lt;String, ? extends Number&gt;</code>. The <code class="literal">String</code> element is the feature name. In this case, the actual token is the name. The second element of the map is a value that extends <code class="literal">Number</code>, in this case, the count of how many times the token was seen in the text.</p><div class="informalexample"><pre class="programlisting">BufferedReader reader 
  = new BufferedReader(new   InputStreamReader(System.<span class="strong"><strong>in</strong></span>));
while (true) {
  System.<span class="strong"><strong>out</strong></span>.println("\nType a string to see its features");
  String text = reader.readLine();
  Map&lt;String, ? extends Number &gt; features 
    = tokenFeatureExtractor.features(text);
  System.<span class="strong"><strong>out</strong></span>.println(features);
}</pre></div><p>The feature name needs to only be <a class="indexterm" id="id258"/>a unique name—we could have prepended each feature name with <code class="literal">SimpleFeatExt_</code> to keep track of where the feature came from, which is helpful in complex feature-extraction scenarios.</p></div></div>
<div class="section" title="Logistic regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec35"/>Logistic regression</h1></div></div></div><p>Logistic regression is <a class="indexterm" id="id259"/>probably responsible for the majority of industrial classifiers, with the possible exception of naïve Bayes classifiers. It almost certainly is one of the best performing classifiers available, albeit at the cost of slow training and considerable complexity in configuration and tuning.</p><p>Logistic regression is also known as maximum entropy, neural network classification with a <a class="indexterm" id="id260"/>single neuron, and others. So far in this book, the classifiers have been based on the underlying characters or tokens, but logistic regression uses unrestricted feature extraction, which allows for arbitrary observations of the situation to be encoded in the classifier.</p><p>This recipe closely <a class="indexterm" id="id261"/>follows a more complete tutorial at <a class="ulink" href="http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html</a>.</p><div class="section" title="How logistic regression works"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl3sec06"/>How logistic regression works</h2></div></div></div><p>All that logistic <a class="indexterm" id="id262"/>regression does is take a vector of feature weights over the data, apply a vector of coefficients, and do some simple math, which results in a probability for each class encountered in training. The complicated bit is in determining what the coefficients should be.</p><p>The following are some of the features produced by our training recipe for 21 tweets annotated for English <code class="literal">e</code> and non-English <code class="literal">n</code>. There are relatively few features because feature weights are being pushed to <code class="literal">0.0</code> by our prior, and once a weight is <code class="literal">0.0</code>, then the feature is removed. Note that one category, <code class="literal">n</code>, is set to <code class="literal">0.0</code> for all the features of the <code class="literal">n-1</code> category—this is a property of the logistic regression process that fixes once categories features to <code class="literal">0.0</code> and adjust all other categories features with respect to that:</p><div class="informalexample"><pre class="programlisting">FEATURE    e          n
I :   0.37    0.0
! :   0.30    0.0
Disney :   0.15    0.0
" :   0.08    0.0
to :   0.07    0.0
anymore : 0.06    0.0
isn :   0.06    0.0
' :   0.06    0.0
t :   0.04    0.0
for :   0.03    0.0
que :   -0.01    0.0
moi :   -0.01    0.0
_ :   -0.02    0.0
, :   -0.08    0.0
pra :   -0.09    0.0
? :   -0.09    0.0</pre></div><p>Take the string, <code class="literal">I luv Disney</code>, which will only have two non-zero features: <code class="literal">I=0.37</code> and <code class="literal">Disney=0.15</code> for <code class="literal">e</code> and zeros for <code class="literal">n</code>. Since there is no feature that matches <code class="literal">luv</code>, it is ignored. The probability that the tweet is English breaks down to:</p><p>
<span class="emphasis"><em>vectorMultiply(e,[I,Disney]) =  exp(.37*1 + .15*1) = 1.68</em></span>
</p><p>
<span class="emphasis"><em>vectorMultiply(n,[I,Disney]) =  exp(0*1 + 0*1) = 1</em></span>
</p><p>We will rescale to a <a class="indexterm" id="id263"/>probability by summing the outcomes and dividing it:</p><p>
<span class="emphasis"><em>p(e|,[I,Disney]) = 1.68/(1.68 +1) = 0.62</em></span>
</p><p>
<span class="emphasis"><em>p(e|,[I,Disney]) = 1/(1.68 +1) = 0.38</em></span>
</p><p>This is how the math works on running a logistic regression model. Training is another issue entirely.</p></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec81"/>Getting ready</h2></div></div></div><p>This recipe assumes the same framework that we have been using all along to get training data from <code class="literal">.csv</code> files, train the classifier, and run it from the command line.</p><p>Setting up to train the classifier is a bit complex because of the number of parameters and objects used in training. We will discuss all the 10 arguments to the training method as found in <code class="literal">com.lingpipe.cookbook.chapter3.TrainAndRunLogReg</code>.</p><p>The <code class="literal">main()</code> method starts with <a class="indexterm" id="id264"/>what should be familiar classes and methods—if they are not familiar, have a look at <span class="emphasis"><em>How to train and evaluate with cross validation</em></span> and <span class="emphasis"><em>Introduction to Introduction to tokenizer Factories – finding words in a character stream</em></span>, recipes from <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, and <a class="link" href="ch02.html" title="Chapter 2. Finding and Working with Words">Chapter 2</a>, <span class="emphasis"><em>Finding and Working with Words</em></span>, respectively:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>public static void</strong></span> main(String[] args) <span class="strong"><strong>throws</strong></span> IOException {
  String trainingFile = args.length &gt; 0 ? args[0] 
           : "data/disney_e_n.csv";
  List&lt;String[]&gt; training 
    = Util.<span class="strong"><strong>readAnnotatedCsvRemoveHeader</strong></span>(<span class="strong"><strong>new</strong></span> File(trainingFile));

  <span class="strong"><strong>int</strong></span> numFolds = 0;
  XValidatingObjectCorpus&lt;Classified&lt;CharSequence&gt;&gt; corpus 
    = Util.<span class="strong"><strong>loadXValCorpus</strong></span>(training,numFolds);

  TokenizerFactory tokenizerFactory 
    = IndoEuropeanTokenizerFactory.<span class="strong"><strong>INSTANCE</strong></span>;</pre></div><p>Note that we are using <code class="literal">XValidatingObjectCorpus</code> when a simpler implementation such as <code class="literal">ListCorpus</code> will do. We will not take advantage of any of its cross-validation features, because the <code class="literal">numFolds</code> param as <code class="literal">0</code> will have training visit the entire corpus. We are trying to keep the number of novel classes to a minimum, and we tend to always use this implementation in real-world gigs anyway.</p><p>Now, we will start to build the configuration for our classifier. The <code class="literal">FeatureExtractor&lt;E&gt;</code> interface provides a mapping from data to features; this will be used to train and run the classifier. In this case, we are using a <code class="literal">TokenFeatureExtractor()</code> method, which creates features based on the tokens found by the tokenizer supplied during construction. This is similar to what naïve Bayes reasons over. The previous recipe goes into more detail about what the feature extractor is doing if this is not clear:</p><div class="informalexample"><pre class="programlisting">FeatureExtractor&lt;CharSequence&gt; featureExtractor
  = <span class="strong"><strong>new</strong></span> TokenFeatureExtractor(tokenizerFactory);</pre></div><p>The <code class="literal">minFeatureCount</code> item is usually set to a number higher than 1, but with small training sets, this is needed to get any performance. The thought behind filtering feature counts is that logistic regression tends to overfit low-count features that, just by chance, exist in one category of training data. As training data grows, the <code class="literal">minFeatureCount</code> value is adjusted usually by paying attention to cross-validation performance:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>int</strong></span> minFeatureCount = 1;</pre></div><p>The <code class="literal">addInterceptFeature</code> <a class="indexterm" id="id265"/>Boolean controls whether a category feature exists that models the prevalence of the category in training. The default <a class="indexterm" id="id266"/>name of the intercept feature is <code class="literal">*&amp;^INTERCEPT%$^&amp;**</code>, and you will see it in the weight vector output if it is being used. By convention, the intercept feature is set to <code class="literal">1.0</code> for all inputs. The idea is that if a category is just very common or very rare, there should be a feature that captures just this fact, independent of other features that might not be as cleanly distributed. This models the category probability in naïve Bayes in some way, but the logistic regression algorithm will decide how useful it is as it does with all other features:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>boolean</strong></span> addInterceptFeature = <span class="strong"><strong>true</strong></span>;
<span class="strong"><strong>boolean</strong></span> noninformativeIntercept = <span class="strong"><strong>true</strong></span>;</pre></div><p>These Booleans control what happens to the intercept feature if it is used. Priors, in the following code, are typically not applied to the intercept feature; this is the result if this parameter is true. Set the Boolean to <code class="literal">false</code>, and the prior will be applied to the intercept.</p><p>Next is the <code class="literal">RegressionPrior</code> instance, which controls how the model is fit. What you need to know is that priors help prevent logistic regression from overfitting the data by pushing coefficients towards 0. There is a non-informative prior that does not do this with the consequence that if there is a feature that applies to just one category it will be scaled to infinity, because the model keeps fitting better as the coefficient is increased in the numeric estimation. Priors, in this context, function as a way to not be over confident in observations about the world.</p><p>Another dimension in the <code class="literal">RegressionPrior</code> instance is the expected variance of the features. Low variance will push coefficients to zero more aggressively. The prior returned by the static <code class="literal">laplace()</code> method tends to work well for NLP problems. For more information on what is going on <a class="indexterm" id="id267"/>here, consult the relevant Javadoc and the logistic regression tutorial referenced at the beginning of the recipe—there is a lot going on, but it can be managed without a deep theoretical understanding. Also, see the <span class="emphasis"><em>Tuning parameters in logistic regression</em></span> recipe in this chapter.</p><div class="informalexample"><pre class="programlisting">double <span class="strong"><strong>priorVariance</strong></span> = 2;
RegressionPrior prior 
  = RegressionPrior.laplace(priorVariance,
          noninformativeIntercept);</pre></div><p>Next, we will control how the algorithm searches for an answer.</p><div class="informalexample"><pre class="programlisting">AnnealingSchedule annealingSchedule
  = AnnealingSchedule.exponential(0.00025,0.999);
double minImprovement = 0.000000001;
int minEpochs = 100;
int maxEpochs = 2000;</pre></div><p>
<code class="literal">AnnealingSchedule</code> is best <a class="indexterm" id="id268"/>understood by consulting the Javadoc, but what it does is change how much the coefficients are allowed to vary when fitting the model. The <a class="indexterm" id="id269"/>
<code class="literal">minImprovement</code> parameter sets the amount the model fit has to improve to not terminate the search, because the algorithm has converged. The <code class="literal">minEpochs</code> parameter sets a minimal number of iterations, and <code class="literal">maxEpochs</code> sets an upper limit if the search does not converge as determined by <code class="literal">minImprovement</code>.</p><p>Next is some code that allows for basic reporting/logging. <code class="literal">LogLevel.INFO</code> will report a great deal of information about the progress of the classifier as it tries to converge:</p><div class="informalexample"><pre class="programlisting">PrintWriter progressWriter = new PrintWriter(System.out,true);
progressWriter.println("Reading data.");
Reporter reporter = Reporters.writer(progressWriter);
reporter.setLevel(LogLevel.INFO);  </pre></div><p>Here ends the <span class="emphasis"><em>Getting ready</em></span> section of one of our most complex classes—next, we will train and run the classifier.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec82"/>How to do it...</h2></div></div></div><p>It has been a bit of work <a class="indexterm" id="id270"/>setting up to train and run this class. We will just go through the steps to get it up and running; the upcoming recipes will address its tuning and evaluation:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Note that there is a more complex 14-argument train method as well the one that extends configurability. This is the 10-argument version:<div class="informalexample"><pre class="programlisting">LogisticRegressionClassifier&lt;CharSequence&gt; classifier
    = LogisticRegressionClassifier.
        &lt;CharSequence&gt;<span class="strong"><strong>train</strong></span>(corpus,
        featureExtractor,
        minFeatureCount,
        addInterceptFeature,
        prior,
        annealingSchedule,
        minImprovement,
        minEpochs,
        maxEpochs,
        reporter);</pre></div></li><li class="listitem">The <code class="literal">train()</code> method, depending on the <code class="literal">LogLevel</code> constant, will produce from nothing with <code class="literal">LogLevel.NONE</code> to the prodigious output with <code class="literal">LogLevel.ALL</code>.</li><li class="listitem">While we are not going to use it, we show how to serialize the trained model to disk. The <span class="emphasis"><em>How to serialize a LingPipe object – classifier example</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, explains what is going on:<div class="informalexample"><pre class="programlisting">AbstractExternalizable.<span class="strong"><strong>compileTo</strong></span>(classifier,
  <span class="strong"><strong>new</strong></span> File("models/myModel.LogisticRegression"));</pre></div></li><li class="listitem">Once trained, we will apply the standard classification loop with:<div class="informalexample"><pre class="programlisting">Util.consoleInputPrintClassification(classifier);</pre></div></li><li class="listitem">Run the preceding code in the IDE of your choice or use the command-line command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.TrainAndRunLogReg</strong></span>
</pre></div></li><li class="listitem">The result is a big dump of information about the training:<div class="informalexample"><pre class="programlisting">Reading data.
:00 Feature Extractor class=class com.aliasi.tokenizer.TokenFeatureExtractor
:00 min feature count=1
:00 Extracting Training Data
:00 Cold start
:00 Regression callback handler=null
:00 Logistic Regression Estimation
:00 Monitoring convergence=true
:00 Number of dimensions=233
:00 Number of Outcomes=2
:00 Number of Parameters=233
:00 Number of Training Instances=21
:00 Prior=LaplaceRegressionPrior(Variance=2.0, noninformativeIntercept=true)
:00 Annealing Schedule=Exponential(initialLearningRate=2.5E-4, base=0.999)
:00 Minimum Epochs=100
:00 Maximum Epochs=2000
:00 Minimum Improvement Per Period=1.0E-9
:00 Has Informative Prior=true
:00 epoch=    0 lr=0.000250000 ll=   -20.9648 lp= -232.0139 llp=  -252.9787 llp*=  -252.9787
:00 epoch=    1 lr=0.000249750 ll=   -20.9406 lp= -232.0195 llp=  -252.9602 llp*=  -252.9602</pre></div></li><li class="listitem">The <code class="literal">epoch</code> reporting <a class="indexterm" id="id271"/>goes on until either the number of epochs is met or the search converges. In the following case, the number of epochs was met:<div class="informalexample"><pre class="programlisting">:00 epoch= 1998 lr=0.000033868 ll=   -15.4568 lp=  -233.8125 llp=  -249.2693 llp*=  -249.2693
:00 epoch= 1999 lr=0.000033834 ll=   -15.4565 lp=  -233.8127 llp=  -249.2692 llp*=  -249.2692</pre></div></li><li class="listitem">Now, we can play with the classifier a bit:<div class="informalexample"><pre class="programlisting">Type a string to be classified. Empty string to quit.
I luv Disney
Rank  Category  Score  P(Category|Input)
0=e 0.626898085027528 0.626898085027528
1=n 0.373101914972472 0.373101914972472</pre></div></li><li class="listitem">This should look familiar; it is exactly the same result as the worked example at the start of the recipe.</li></ol></div><p>That's it! You have trained <a class="indexterm" id="id272"/>up and used the world's most relevant industrial classifier. However, there's a lot more to harnessing the power of this beast.</p></div></div>
<div class="section" title="Multithreaded cross validation"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec36"/>Multithreaded cross validation</h1></div></div></div><p>Cross <a class="indexterm" id="id273"/>validation (refer to the <span class="emphasis"><em>How to train and evaluate with cross validation</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>) can be very slow, which interferes with tuning systems. This recipe will show you a simple but effective way to access all the available cores on your system to more quickly process each fold.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec83"/>How to do it...</h2></div></div></div><p>This recipe explains multi-threaded cross validation in the context of the next recipe, so don't be confused by the fact that the same class is repeated.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Engage your IDE or type in the command line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.TuneLogRegParams</strong></span>
</pre></div></li><li class="listitem">The system then responds with the following output (you might have to scroll to the top of the window):<div class="informalexample"><pre class="programlisting">Reading data.
RUNNING thread Fold 5 (1 of 10)
RUNNING thread Fold 9 (2 of 10)
RUNNING thread Fold 3 (3 of 10)
RUNNING thread Fold 4 (4 of 10)
RUNNING thread Fold 0 (5 of 10)
RUNNING thread Fold 2 (6 of 10)
RUNNING thread Fold 8 (7 of 10)
RUNNING thread Fold 6 (8 of 10)
RUNNING thread Fold 7 (9 of 10)
RUNNING thread Fold 1 (10 of 10)
reference\response
          \e,n,
         e 11,0,
         n 6,4,</pre></div></li><li class="listitem">The default training data is 21 tweets annotated for English <code class="literal">e</code> and non-English <code class="literal">n</code>. In the preceding output, we saw a report of each fold that runs as a thread and the resulting confusion matrix. That's it! We just did multithreaded cross validation. Let's see how this works.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec84"/>How it works…</h2></div></div></div><p>All the action <a class="indexterm" id="id274"/>happens in the <code class="literal">Util.xvalLogRegMultiThread()</code> method, which we invoke from <code class="literal">src/com/lingpipe/cookbook/chapter3/TuneLogRegParams.java</code>. The details of <code class="literal">TuneLogRegParams</code> are covered in the next recipe. This recipe will focus on the <code class="literal">Util</code> method:</p><div class="informalexample"><pre class="programlisting">int numThreads = 2;
int numFolds = 10;
Util.xvalLogRegMultiThread(corpus,
        featureExtractor,
        minFeatureCount,
        addInterceptFeature,
        prior,
        annealingSchedule,
        minImprovement,
        minEpochs,
        maxEpochs,
        reporter,
        numFolds,
        numThreads,
        categories);</pre></div><p>All 10 parameters used to configure logistic regression are controllable (you can refer to the previous recipe for explanation), with the addition of <code class="literal">numFolds</code>, which controls how many folds there will be, <code class="literal">numThreads</code>, which controls how many threads can be run at the same time, and finally, <code class="literal">categories</code>.</p><p>If we look at the relevant <a class="indexterm" id="id275"/>method in <code class="literal">src/com/lingpipe/cookbook/Util.java</code>, we see:</p><div class="informalexample"><pre class="programlisting">public static &lt;E&gt; ConditionalClassifierEvaluator&lt;E&gt; xvalLogRegMultiThread(
    final XValidatingObjectCorpus&lt;Classified&lt;E&gt;&gt; corpus,
    final FeatureExtractor&lt;E&gt; featureExtractor,
    final int minFeatureCount, 
    final boolean addInterceptFeature,
    final RegressionPrior prior, 
    final AnnealingSchedule annealingSchedule,
    final double minImprovement, 
    final int minEpochs, final int maxEpochs,
    final Reporter reporter, 
    final int numFolds, 
    final int numThreads, 
    final String[] categories) {</pre></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The method starts with the matching arguments for configuration information of logistic regression and running cross validation. Since cross validation is most often used in system tuning, all the relevant bits are exposed to modification. Everything is final because we are using an anonymous inner class to create threads.</li><li class="listitem">Next, we will set up <code class="literal">crossFoldEvaluator</code> that will collect the results from each thread:<div class="informalexample"><pre class="programlisting">corpus.setNumFolds(numFolds);
corpus.permuteCorpus(new Random(11211));
final boolean storeInputs = true;
final ConditionalClassifierEvaluator&lt;E&gt; crossFoldEvaluator
  = new ConditionalClassifierEvaluator&lt;E&gt;(null, categories, storeInputs);</pre></div></li><li class="listitem">Now, we will <a class="indexterm" id="id276"/>get down to the business of creating threads for each fold, <code class="literal">i</code>:<div class="informalexample"><pre class="programlisting">List&lt;Thread&gt; threads = new ArrayList&lt;Thread&gt;();
for (int i = 0; i &lt; numFolds; ++i) {
  final XValidatingObjectCorpus&lt;Classified&lt;E&gt;&gt; fold 
    = corpus.itemView();
  fold.setFold(i);</pre></div><p>The <code class="literal">XValidatingObjectCorpus</code> class is set up for multithreaded access by creating a thread-safe version of the corpus for reads with the <code class="literal">itemView()</code> method. This method returns a corpus that can have the fold set, but no data can be added.</p><p>Each thread is a <code class="literal">runnable</code> object, where the actual work of training and evaluating the fold is handled in the <code class="literal">run()</code> method:</p><div class="informalexample"><pre class="programlisting">Runnable runnable 
  = new Runnable() {
    @Override
    public void run() {
    try {
      LogisticRegressionClassifier&lt;E&gt; classifier
        = LogisticRegressionClassifier.&lt;E&gt;train(fold,
                featureExtractor,
                minFeatureCount,
                addInterceptFeature,
                prior,
                annealingSchedule,
                minImprovement,
                minEpochs,
                maxEpochs,
                reporter);</pre></div><p>In this code, we started with training the classifier, which, in turn, requires a <code class="literal">try/catch</code> statement to handle <code class="literal">IOException</code> thrown by the <code class="literal">LogisticRegressionClassifier.train()</code> method. Next, we will create <code class="literal">withinFoldEvaluator</code> that will be populated within the thread without a synchronization issue:</p><div class="informalexample"><pre class="programlisting">ConditionalClassifierEvaluator&lt;E&gt; withinFoldEvaluator 
  = new ConditionalClassifierEvaluator&lt;E&gt;(classifier, categories, storeInputs);
fold.visitTest(withinFoldEvaluator);</pre></div><p>It is important that <code class="literal">storeInputs</code> be <code class="literal">true</code> so that the fold results can be added to <code class="literal">crossFoldEvaluator</code>:</p><div class="informalexample"><pre class="programlisting">addToEvaluator(withinFoldEvaluator,crossFoldEvaluator);</pre></div><p>This method, also in <code class="literal">Util</code>, iterates over all the true positives and false negatives for each category and adds them to <code class="literal">crossFoldEvaluator</code>. Note that this is synchronized: this means that only one thread can access the <a class="indexterm" id="id277"/>method at a time, but given that classification has already been done, it should not be much of a bottleneck:</p><div class="informalexample"><pre class="programlisting">public synchronized static &lt;E&gt; void addToEvaluator(BaseClassifierEvaluator&lt;E&gt; foldEval, ScoredClassifierEvaluator&lt;E&gt; crossFoldEval) {
  for (String category : foldEval.categories()) {
   for (Classified&lt;E&gt; classified : foldEval.truePositives(category)) {
    crossFoldEval.addClassification(category,classified.getClassification(),classified.getObject());
   }
   for (Classified&lt;E&gt; classified : foldEval.falseNegatives(category)) {
    crossFoldEval.addClassification(category,classified.getClassification(),classified.getObject());
   }
  }
 }</pre></div><p>The method takes the true positives and false negatives from each category and adds them to the <code class="literal">crossFoldEval</code> evaluator. These are essentially copy operations that do not take long to compute.</p></li><li class="listitem">Returning to <code class="literal">xvalLogRegMultiThread</code>, we will handle the exception and add the completed <code class="literal">Runnable</code> to our list of <code class="literal">Thread</code>:<div class="informalexample"><pre class="programlisting">    catch (Exception e) {
      e.printStackTrace();
    }
  }
};
threads.add(new Thread(runnable,"Fold " + i));</pre></div></li><li class="listitem">With all the threads set up, we will invoke <code class="literal">runThreads()</code> as well as print the confusion matrix that results. We will not go into the source of <code class="literal">runThreads()</code>, because it is a straightforward Java management of threads, and <code class="literal">printConfusionMatrix</code> has been covered in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>:<div class="informalexample"><pre class="programlisting">    
  runThreads(threads,numThreads); 
  printConfusionMatrix(crossFoldEvaluator.confusionMatrix());
}</pre></div></li></ol></div><p>That's it for really <a class="indexterm" id="id278"/>speeding up cross validation on multicore machines. It can make a big difference when tuning systems.</p></div></div>
<div class="section" title="Tuning parameters in logistic regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec37"/>Tuning parameters in logistic regression</h1></div></div></div><p>Logistic regression <a class="indexterm" id="id279"/>presents an intimidating array of <a class="indexterm" id="id280"/>parameters to tweak for better performance, and working with it is a bit of black art. Having built thousands of these classifiers, we are still learning how to do it better. This recipe will point you in the general right direction, but the topic probably deserves its own book.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec85"/>How to do it...</h2></div></div></div><p>This recipe involves extensive changes to the source of <code class="literal">src/com/lingpipe/chapter3/TuneLogRegParams.java</code>. We will just run one configuration of it here, with most of the exposition in the <span class="emphasis"><em>How it works…</em></span> section.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Engage your IDE or type the following in the command line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.TuneLogRegParams</strong></span>
</pre></div></li><li class="listitem">The system then responds with cross-validation output confusion matrix for our default data in <code class="literal">data/disney_e_n.csv</code>:<div class="informalexample"><pre class="programlisting">reference\response
          \e,n,
         e 11,0,
         n 6,4,</pre></div></li><li class="listitem">Next, we will report on false positives for each category—this will cover all the mistakes made:<div class="informalexample"><pre class="programlisting">False Positives for e
ES INSUPERABLE DISNEY !! QUIERO VOLVER:( : n
@greenath_ t'as de la chance d'aller a Disney putain : n 
jamais été moi. : n
@HedyHAMIDI au quartier pas a Disney moi: n
…</pre></div></li><li class="listitem">This output is <a class="indexterm" id="id281"/>followed by the features, their coefficients, and a count—remember that we will see <code class="literal">n-1</code> categories, because one of the category's features is set to <code class="literal">0.0</code> for all features:<div class="informalexample"><pre class="programlisting">Feature coefficients for category e
I : 0.36688604
! : 0.29588525
Disney : 0.14954419
" : 0.07897427
to : 0.07378086
…
Got feature count: 113</pre></div></li><li class="listitem">Finally, we <a class="indexterm" id="id282"/>have our standard I/O that allows for examples to be tested:<div class="informalexample"><pre class="programlisting">Type a string to be classified
I luv disney
Rank  Category  Score  P(Category|Input)
0=e 0.5907060507161321 0.5907060507161321
1=n 0.40929394928386786 0.40929394928386786</pre></div></li><li class="listitem">This is the basic structure that we will work with. In the upcoming sections, we will explore the impact of varying parameters more closely.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec86"/>How it works…</h2></div></div></div><p>This recipe assumes that you are familiar with logistic regression training and configuration from two recipes back and cross validation, which is the previous recipe. The overall structure of the code is presented in an outline form, with the tuning parameters retained. Modifying each parameter will be discussed later in the recipe—below we start with the <code class="literal">main()</code> method ignoring some code as indicated by '<code class="literal">...</code>' and the tunable code shown for tokenization and feature extraction:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException {
    …
  TokenizerFactory tokenizerFactory 
     = IndoEuropeanTokenizerFactory.INSTANCE;
  FeatureExtractor&lt;CharSequence&gt; featureExtractor
     = new TokenFeatureExtractor(tokenizerFactory);
  int minFeatureCount = 1;
  boolean addInterceptFeature = false;</pre></div><p>Next the priors are set up:</p><div class="informalexample"><pre class="programlisting">  boolean noninformativeIntercept = true;
  double priorVariance = 2 ;
  RegressionPrior prior 
    = RegressionPrior.laplace(priorVariance,
            noninformativeIntercept);</pre></div><p>Priors have a strong <a class="indexterm" id="id283"/>influence on the behavior coefficient assignment:</p><div class="informalexample"><pre class="programlisting">  AnnealingSchedule annealingSchedule
    = AnnealingSchedule.exponential(0.00025,0.999);
  double minImprovement = 0.000000001;
  int minEpochs = 10;
  int maxEpochs = 20;</pre></div><p>The preceding code controls the search space of logistic regression:</p><div class="informalexample"><pre class="programlisting">Util.xvalLogRegMultiThread(corpus,…);</pre></div><p>The preceding code runs cross validation to see how the system is doing—note the elided parameters with <code class="literal">...</code>.</p><p>In the following code, we will set the number of folds to <code class="literal">0</code>, which will have the train method visit the entire corpus:</p><div class="informalexample"><pre class="programlisting">corpus.setNumFolds(0);
LogisticRegressionClassifier&lt;CharSequence&gt; classifier
  = LogisticRegressionClassifier.&lt;CharSequence&gt;train(corpus,…</pre></div><p>Then, for each category, we will print out the features and their coefficients for the just trained classifier:</p><div class="informalexample"><pre class="programlisting">int featureCount = 0;
for (String category : categories) {
  ObjectToDoubleMap&lt;String&gt; featureCoeff 
    = classifier.featureValues(category);
  System.out.println("Feature coefficients for category " 
        + category);
  for (String feature : featureCoeff.keysOrderedByValueList()) {
    System.out.print(feature);
    System.out.printf(" :%.8f\n",featureCoeff.getValue(feature));
    ++featureCount;
  }
}
System.out.println("Got feature count: " + featureCount);</pre></div><p>Finally, we will have the <a class="indexterm" id="id284"/>usual console classifier I/O:</p><div class="informalexample"><pre class="programlisting">Util.consoleInputPrintClassification(classifier);    </pre></div><div class="section" title="Tuning feature extraction"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec07"/>Tuning feature extraction</h3></div></div></div><p>The features that are <a class="indexterm" id="id285"/>fed into logistic regression have a huge impact on the performance of the system. We will cover feature extraction in greater detail in the <a class="indexterm" id="id286"/>later recipes, but we will bring to bear one of the most useful and somewhat counter-intuitive approaches here, because it is very easy to execute—use character ngrams instead of words/tokens. Let's look at an example:</p><div class="informalexample"><pre class="programlisting">Type a string to be classified. Empty string to quit.
The rain in Spain
Rank  Category  Score  P(Category|Input)
0=e 0.5 0.5
1=n 0.5 0.5</pre></div><p>This output indicates that the classifier is tied between <code class="literal">e</code> English and <code class="literal">n</code> non-English as a decision. Scrolling back through the features, we will see that there are no matches for any of the words in the input. There are some substring matches on the English side. <code class="literal">The</code> has the substring <code class="literal">he</code> for the feature word <code class="literal">the</code>. For language ID, it makes sense to consider subsequences, but as a matter of experience, it can be a big help for sentiment and other problems as well.</p><p>Modifying the tokenizer to be two-to-four-character ngrams is done as follows:</p><div class="informalexample"><pre class="programlisting">int min = 2;
int max = 4;
TokenizerFactory tokenizerFactory 
  = new NGramTokenizerFactory(min,max);</pre></div><p>This results in the proper distinction being made:</p><div class="informalexample"><pre class="programlisting">Type a string to be classified. Empty string to quit.
The rain in Spain
Rank  Category  Score  P(Category|Input)
0=e 0.5113903651380305 0.5113903651380305
1=n 0.4886096348619695 0.4886096348619695</pre></div><p>The overall performance on cross validation drops a bit. For very small training sets, such as 21 tweets, this is not unexpected. Generally, the cross-validation performance with a consultation of what the mistakes look like and a look at the false positives will help guide this process.</p><p>In looking at the false positives, it is clear that <code class="literal">Disney</code> is a source of problems, because the coefficients on features show it to be evidence for English. Some of the false positives are:</p><div class="informalexample"><pre class="programlisting">False Positives for e
@greenath_ t'as de la chance d'aller a Disney putain j'y ai jamais été moi. : n
@HedyHAMIDI au quartier pas a Disney moi : n
Prefiro gastar uma baba de dinheiro pra ir pra cancun doq pra Disney por exemplo : n</pre></div><p>The following are the <a class="indexterm" id="id287"/>features for <code class="literal">e</code>:</p><div class="informalexample"><pre class="programlisting">Feature coefficients for category e
I : 0.36688604
! : 0.29588525
Disney : 0.14954419
" : 0.07897427
to : 0.07378086</pre></div><p>In the absence of more <a class="indexterm" id="id288"/>training data, the features <code class="literal">!</code>, <code class="literal">Disney</code>, and <code class="literal">"</code> should be removed to help this classifier perform better, because none of these features are language specific, whereas <code class="literal">I</code> and <code class="literal">to</code> are, although not unique to English. This can be done by filtering the data or creating the appropriate tokenizer factory, but the best move is to probably get more data.</p><p>The <code class="literal">minFeature</code> count becomes useful when there is much more data, and you don't want logistic regression focusing on a very-low-count phenomenon because it tends to lead to overfitting.</p><p>Setting the <code class="literal">addInterceptFeature</code> parameter to <code class="literal">true</code> will add a feature that always fires. This will allow logistic regression to have a feature sensitive to the number of examples for each category. It is not the marginal likelihood of the category, as logistic regression will adjust the weight like any other feature—but the following priors show how it can be further tuned:</p><div class="informalexample"><pre class="programlisting">de : -0.08864114
( : -0.10818647
*&amp;^INTERCEPT%$^&amp;** : -0.17089337</pre></div><p>The intercept is the strongest <a class="indexterm" id="id289"/>feature for <code class="literal">n</code> in the end, and the overall cross-validation performance suffered in this case.</p></div><div class="section" title="Priors"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec08"/>Priors</h3></div></div></div><p>The role of priors <a class="indexterm" id="id290"/>is to restrict the tendency of logistic regression to perfectly fit the training data. The ones we use try in varying degrees to push coefficients to zero. We <a class="indexterm" id="id291"/>will start with the <code class="literal">nonInformativeIntercept</code> prior, which controls whether the intercept feature is subject to the normalizing influences of the prior—if true, then the intercept is not subject to the prior, which was the case in the preceding example. Setting it to <code class="literal">false</code> moved it much closer to zero from <code class="literal">-0.17</code>:</p><div class="informalexample"><pre class="programlisting">*&amp;^INTERCEPT%$^&amp;** : -0.03874782</pre></div><p>Next, we will adjust the variance of the prior. This sets an expected variation for the weights. A low variance means that coefficients are expected not to vary much from zero. In the preceding code, the variance was set to <code class="literal">2</code>. This is the result of setting it to <code class="literal">.01</code>:</p><div class="informalexample"><pre class="programlisting">Feature coefficients for category e
' : -0.00003809
Feature coefficients for category n</pre></div><p>This is a drop from 104 features with variance <code class="literal">2</code> to one feature for variance <code class="literal">.01</code>, because once a feature has dropped to <code class="literal">0</code>, it is removed.</p><p>Increasing the variance changes our top <code class="literal">e</code> features from <code class="literal">2</code> to <code class="literal">4</code>:</p><div class="informalexample"><pre class="programlisting">Feature coefficients for category e
I : 0.36688604
! : 0.29588525
Disney : 0.14954419

I : 0.40189501
! : 0.31387376
Disney : 0.18255271</pre></div><p>This is a total of 119 features.</p><p>Consider a variance of <code class="literal">2</code> and a <code class="literal">gaussian</code> prior:</p><div class="informalexample"><pre class="programlisting">boolean noninformativeIntercept = false;
double priorVariance = 2;
RegressionPrior prior 
  = RegressionPrior.gaussian(priorVariance,
    noninformativeIntercept);</pre></div><p>We will get the following output:</p><div class="informalexample"><pre class="programlisting">I : 0.38866670
! : 0.27367013
Disney : 0.22699340</pre></div><p>Oddly, we spend very little time worrying about which prior we use, but variance has a big role in performance, because it can cut down the feature space quickly. Laplace is a commonly accepted prior <a class="indexterm" id="id292"/>for NLP applications.</p><p>Consult the Javadoc and <a class="indexterm" id="id293"/>logistic regression tutorial for more information.</p></div><div class="section" title="Annealing schedule and epochs"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec09"/>Annealing schedule and epochs</h3></div></div></div><p>As logistic <a class="indexterm" id="id294"/>regression converges, the annealing schedule controls how the search space is explored and terminated:</p><div class="informalexample"><pre class="programlisting">AnnealingSchedule annealingSchedule
    = AnnealingSchedule.exponential(0.00025,0.999);
  double minImprovement = 0.000000001;
  int minEpochs = 10;
  int maxEpochs = 20;</pre></div><p>When tuning, we will increase the first parameter to the annealing schedule by order of magnitude (<code class="literal">.0025,.025,..</code>) if the search is taking too long—often, we can increase the training speed without impacting the cross-validation performance. Also, the <code class="literal">minImprovement</code> value can be increased to have the convergence end earlier, which can both increase the training <a class="indexterm" id="id295"/>speed and prevent the model from overfitting—this is called <span class="strong"><strong>early stopping</strong></span>. Again, your guiding light in this situation is to look at the cross-validation performance when making changes.</p><p>The epochs required to achieve convergence can get quite high, so if the classifier is iterating to <code class="literal">maxEpochs -1</code>, this means that more epochs are required to converge. Be sure to set the <code class="literal">reporter.setLevel(LogLevel.INFO);</code> property or a more informative level to get the convergence report. This is another way to additionally force early stopping.</p><p>Parameter tuning is a black art that can only be learned through practice. The quality and quantity of training data is probably the dominant factor in classifier performance, but tuning can make a big difference as well.</p></div></div></div>
<div class="section" title="Customizing feature extraction"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec38"/>Customizing feature extraction</h1></div></div></div><p>Logistic regression <a class="indexterm" id="id296"/>allows for arbitrary features to be used. Features are any observations that can be made about data being classified. Some examples are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Words/tokens from the text.</li><li class="listitem" style="list-style-type: disc">We found that character ngrams work very well in lieu of words or stemmed words. For small data sets of less than 10,000 words of training, we will use 2-4 grams. Bigger training data can merit a longer gram, but we have never had good results above 8-gram characters.</li><li class="listitem" style="list-style-type: disc">Output from another component can be a feature, for example, a part-of-speech tagger.</li><li class="listitem" style="list-style-type: disc">Metadata known about the text, for example, the location of a tweet or time of the day it was created.</li><li class="listitem" style="list-style-type: disc">Recognition of dates and numbers abstracted from the actual value.</li></ul></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec87"/>How to do it…</h2></div></div></div><p>The source for this <a class="indexterm" id="id297"/>recipe is in <code class="literal">src/com/lingpipe/cookbook/chapter3/ContainsNumberFeatureExtractor.java</code>.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Feature extractors are straightforward to build. The following is a feature extractor that returns a <code class="literal">CONTAINS_NUMBER</code> feature with weight <code class="literal">1</code>:<div class="informalexample"><pre class="programlisting">public class ContainsNumberFeatureExtractor implements FeatureExtractor&lt;CharSequence&gt; {
  @Override
  public Map&lt;String,Counter&gt; features(CharSequence text) {
         ObjectToCounterMap&lt;String&gt; featureMap 
         = new ObjectToCounterMap&lt;String&gt;();
    if (text.toString().matches(".*\\d.*")) {
      featureMap.set("CONTAINS_NUMBER", 1);
    }
    return featureMap;  }</pre></div></li><li class="listitem">By adding a <code class="literal">main()</code> method, we can test the feature extractor:<div class="informalexample"><pre class="programlisting">public static void main(String[] args) {
  FeatureExtractor&lt;CharSequence&gt; featureExtractor 
         = new ContainsNumberFeatureExtractor();
  System.out.println(featureExtractor.features("I have a number 1"));
}</pre></div></li><li class="listitem">Now run the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.ContainsNumberFeatureExtractor</strong></span>
</pre></div></li><li class="listitem">The preceding code yields the following output:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>CONTAINS_NUMBER=1</strong></span>
</pre></div></li></ol></div><p> That's it. The next recipe will show you how to combine feature extractors.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec88"/>There's more…</h2></div></div></div><p>Designing features is a bit of an art. Logistic regression is supposed to be robust in the face of irrelevant features, but overwhelming it with really dumb features will likely detract from performance.</p><p>One way to think about what features you need is to wonder what evidence from the text or environment helps you, the human, decide what the correct classification is. Try and ignore your world knowledge <a class="indexterm" id="id298"/>when looking at the text. If world knowledge, that is, France is a country, is important, then try and model this world knowledge with a gazetteer to generate <code class="literal">CONTAINS_COUNTRY_MENTION</code>.</p><p>Be aware that features are strings, and the only notion of equivalence is the exact string match. The <code class="literal">12:01pm</code> feature is completely distinct from <code class="literal">12:02pm</code>, although, to a human, these strings are very close, because we understand time. To get the similarity of these two features, you must have something like a <code class="literal">LUNCH_TIME</code> feature that is computed using time.</p></div></div>
<div class="section" title="Combining feature extractors"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec39"/>Combining feature extractors</h1></div></div></div><p>Feature extractors can be <a class="indexterm" id="id299"/>combined in much the same way as tokenizers in <a class="link" href="ch02.html" title="Chapter 2. Finding and Working with Words">Chapter 2</a>, <span class="emphasis"><em>Finding and Working with Words</em></span>.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec89"/>How to do it…</h2></div></div></div><p>This recipe will show you how to combine the feature extractor from the previous recipe with a very common feature extractor over character ngrams.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We will start with a <code class="literal">main()</code> method in <code class="literal">src/com/lingpipe/cookbook/chapter3/CombinedFeatureExtractor.java</code> that we will use to run the feature extractor. The following lines set up features that result from the tokenizer using the LingPipe class, <code class="literal">TokenFeatureExtractor</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>public static void</strong></span> main(String[] args) {
   <span class="strong"><strong>int</strong></span> min = 2;
  <span class="strong"><strong>int</strong></span> max = 4;
  <span class="strong"><strong>TokenizerFactory</strong></span> tokenizerFactory 
     = <span class="strong"><strong>new</strong></span> NGramTokenizerFactory(min,max);
  FeatureExtractor&lt;CharSequence&gt; tokenFeatures 
= <span class="strong"><strong>new</strong></span> TokenFeatureExtractor(tokenizerFactory);</pre></div></li><li class="listitem">Then, we will construct the feature extractor from the previous recipe.<div class="informalexample"><pre class="programlisting">FeatureExtractor&lt;CharSequence&gt; numberFeatures 
= <span class="strong"><strong>new</strong></span> ContainsNumberFeatureExtractor();</pre></div></li><li class="listitem">Next, the LingPipe class joining feature extractors, <code class="literal">AddFeatureExtractor</code>, joins the two into a third:<div class="informalexample"><pre class="programlisting">FeatureExtractor&lt;CharSequence&gt; joinedFeatureExtractors 
  = <span class="strong"><strong>new</strong></span> AddFeatureExtractor&lt;CharSequence&gt;(
          tokenFeatures,numberFeatures);</pre></div></li><li class="listitem">The remaining code gets the features and prints them out:<div class="informalexample"><pre class="programlisting">String input = <span class="strong"><strong>"show me 1!"</strong></span>;
Map&lt;String,? <span class="strong"><strong>extends</strong></span> Number&gt; features 
   = joinedFeatureExtractors.features(input);
System.<span class="strong"><strong>out</strong></span>.println(features);</pre></div></li><li class="listitem">Run the following command<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.CombinedFeatureExtractor</strong></span>
</pre></div></li><li class="listitem">The output looks like this:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>{me =1.0,  m=1.0, me 1=1.0, e =1.0, show=1.0,  me =1.0, ho=1.0, ow =1.0, e 1!=1.0, sho=1.0,  1=1.0, me=1.0, how =1.0, CONTAINS_NUMBER=1.0, w me=1.0,  me=1.0, how=1.0,  1!=1.0, sh=1.0, ow=1.0, e 1=1.0, w m=1.0, ow m=1.0, w =1.0, 1!=1.0}</strong></span>
</pre></div></li></ol></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec90"/>There's more…</h2></div></div></div><p>The Javadoc references a broad range of feature extractors and combiners/filters to help manage the task of feature <a class="indexterm" id="id300"/>extraction. One slightly confusing aspect of the class is that the <code class="literal">FeatureExtractor</code> interface is in the <code class="literal">com.aliasi.util</code> package, and the implementing classes are all in <code class="literal">com.aliasi.features</code>.</p></div></div>
<div class="section" title="Classifier-building life cycle"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec40"/>Classifier-building life cycle</h1></div></div></div><p>At the top-level building, a <a class="indexterm" id="id301"/>classifier usually proceeds as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create training data—refer to the following recipe for more about this.</li><li class="listitem">Build training and evaluation infrastructure with sanity check.</li><li class="listitem">Establish baseline performance.</li><li class="listitem">Select optimization metric for classifier—this is what the classifier is trying to do and will guide tuning.</li><li class="listitem">Optimize classifier via techniques such as:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Parameter tuning</li><li class="listitem" style="list-style-type: disc">Thresholding</li><li class="listitem" style="list-style-type: disc">Linguistic tuning</li><li class="listitem" style="list-style-type: disc">Adding training data</li><li class="listitem" style="list-style-type: disc">Refining classifier definition</li></ul></div></li></ol></div><p>This recipe will present the first four steps in concrete terms, and there are recipes in this chapter for the optimization step.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec91"/>Getting ready</h2></div></div></div><p>Nothing happens without training data for classifiers. Look at the <span class="emphasis"><em>Annotation</em></span> recipe at the end of the chapter for tips on creating training data. You can also use an active learning framework to incrementally generate a training corpus (covered later in this chapter), which is the data used in this recipe.</p><p>Next, reduce the risk by starting with the dumbest possible implementation to make sure that the problem being solved is scoped correctly, and that the overall architecture makes sense. Connect the assumed inputs to assumed outputs with simple code. We promise that most of the time, one or the other will not be what you thought it would be.</p><p>This recipe assumes that you are familiar with the evaluation concepts in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, such as cross validation and confusion matrices, in addition to the logistic regression recipes covered so far.</p><p>The entire source is at <code class="literal">src/com/lingpipe/cookbook/chapter3/ClassifierBuilder.java</code>.</p><p>This recipe also assumes that you can compile and run the code within your preferred development environment. The result of all the changes we are making is in <code class="literal">src/com/lingpipe/cookbook/chapter3/ClassifierBuilderFinal.java</code>.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note07"/>Note</h3><p>Big caveat in this recipe—we are using a tiny dataset to make basic points on classifier building. The sentiment classifier we are trying to build would benefit from 10 times more data.</p></div></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec92"/>How to do it…</h2></div></div></div><p>We start with a <a class="indexterm" id="id302"/>collection of tweets that have been deduplicated and are the result of the <span class="emphasis"><em>Train a little, learn a little – active learning</em></span> recipe that will follow this recipe. The starting point of the recipe is the following code:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException {
  String trainingFile = args.length &gt; 0 ? args[0] 
    : "data/activeLearningCompleted/"
    + "disneySentimentDedupe.2.csv";
  int numFolds = 10;
  List&lt;String[]&gt; training 
    = Util.readAnnotatedCsvRemoveHeader(new File(trainingFile));
  String[] categories = Util.getCategories(training);
  XValidatingObjectCorpus&lt;Classified&lt;CharSequence&gt;&gt; corpus 
  = Util.loadXValCorpus(training,numFolds);
TokenizerFactory tokenizerFactory 
  = IndoEuropeanTokenizerFactory.INSTANCE;
PrintWriter progressWriter = new PrintWriter(System.out,true);
Reporter reporter = Reporters.writer(progressWriter);
reporter.setLevel(LogLevel.WARN);
boolean storeInputs = true;
ConditionalClassifierEvaluator&lt;CharSequence&gt; evaluator 
    = new ConditionalClassifierEvaluator&lt;CharSequence&gt;(null, categories, storeInputs);
corpus.setNumFolds(0);
LogisticRegressionClassifier&lt;CharSequence&gt; classifier = Util.trainLogReg(corpus, tokenizerFactory, progressWriter);
evaluator.setClassifier(classifier);
System.out.println("!!!Testing on training!!!");
Util.printConfusionMatrix(evaluator.confusionMatrix());
}</pre></div><div class="section" title="Sanity check – test on training data"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec10"/>Sanity check – test on training data</h3></div></div></div><p>The first thing <a class="indexterm" id="id303"/>to do is get the system running and test on training data:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We have left a print statement that advertises what is going on:<div class="informalexample"><pre class="programlisting">System.out.println("!!!Testing on training!!!");
corpus.visitTrain(evaluator);</pre></div></li><li class="listitem">Running <code class="literal">ClassifierBuilder</code> will yield the following:<div class="informalexample"><pre class="programlisting">!!!Testing on training!!!
reference\response
          \p,n,o,
         p 67,0,3,
         n 0,30,2,
         o 2,1,106,</pre></div></li><li class="listitem">The preceding confusion matrix is nearly a perfect system output, which validates that the system is basically working. This is the best system output you will ever see; never let management see it, or they will think this level of performance is either doable or done.</li></ol></div></div><div class="section" title="Establishing a baseline with cross validation and metrics"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec11"/>Establishing a baseline with cross validation and metrics</h3></div></div></div><p>Now <a class="indexterm" id="id304"/>it is time to <a class="indexterm" id="id305"/>see what is really going on.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"> If you have small data, then set the number of folds to <code class="literal">10</code> so that 90 percent of <a class="indexterm" id="id306"/>the data is <a class="indexterm" id="id307"/>used for training. If you have large data or are in a huge rush, then set it to <code class="literal">2</code>:<div class="informalexample"><pre class="programlisting">static int NUM_FOLDS = 10;</pre></div></li><li class="listitem">Comment out or remove the training on test code:<div class="informalexample"><pre class="programlisting">//System.out.println("!!!Testing on training!!!");
//corpus.visitTrain(evaluator);</pre></div></li><li class="listitem">Plumb in a cross-validation loop or just uncomment the loop in our source:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>corpus.setNumFolds(numFolds);</strong></span>
<span class="strong"><strong>for (int i = 0; i &lt; numFolds; ++i) {</strong></span>
<span class="strong"><strong>  corpus.setFold(i);</strong></span>
  LogisticRegressionClassifier&lt;CharSequence&gt; classifier 
     = Util.trainLogReg(corpus, tokenizerFactory, progressWriter);
  evaluator.setClassifier(classifier);
<span class="strong"><strong>  corpus.visitTest(evaluator);</strong></span>
}</pre></div></li><li class="listitem">Recompiling and running the code will give us the following output:<div class="informalexample"><pre class="programlisting">reference\response
          \p,n,o,
         p 45,8,17,
         n 16,13,3,
         o 18,3,88,</pre></div></li><li class="listitem">The classifier labels mean <code class="literal">p=positiveSentiment</code>, <code class="literal">n=negativeSentiment</code>, and <code class="literal">o=other</code>, which covered other languages or neutral sentiment. The first row of the confusion matrix indicates that the system gets <code class="literal">45</code> true positives, <code class="literal">8</code> false negatives that it thinks are <code class="literal">n</code>, and <code class="literal">17</code> false negatives that it thinks are <code class="literal">o</code>:<div class="informalexample"><pre class="programlisting">reference\response
      \p,n,o,
    p 45,8,17,</pre></div></li><li class="listitem">To get the false positives for <code class="literal">p</code>, we need to look at the first column. We see that the system thought that <code class="literal">16</code> <code class="literal">n</code> annotations were <code class="literal">p</code> and <code class="literal">18</code> <code class="literal">o</code> annotations were <code class="literal">p</code>:<div class="informalexample"><pre class="programlisting">reference\response
          \p,
         p 45
         n 16
         o 18</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip03"/>Tip</h3><p>The confusion <a class="indexterm" id="id308"/>matrix is the most honest and straightforward way to view/present results for classifiers. Performance <a class="indexterm" id="id309"/>metrics such as precision, recall, F-measure, and accuracy are all very slippery and often used incorrectly. When <a class="indexterm" id="id310"/>presenting results, always have a confusion matrix handy, because if we are in the audience or someone like us is, we will ask to see it.</p></div></div></li><li class="listitem">Perform <a class="indexterm" id="id311"/>the same analysis for the other categories, and you will have an <a class="indexterm" id="id312"/>assessment of system performance.</li></ol></div></div><div class="section" title="Picking a single metric to optimize against"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec12"/>Picking a single metric to optimize against</h3></div></div></div><p>Perform the <a class="indexterm" id="id313"/>following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">While the confusion matrix establishes the overall performance of the classifier, it is too complex to use as a tuning guide. You don't want to have to digest the entire matrix every time you adjust a feature. You and your team must agree on a single number that, if it goes up, the system is considered better. The <a class="indexterm" id="id314"/>following metrics apply to binary classifiers; if there are more than two categories, then you will have to sum them somehow. Some common metrics we see are:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>F-measure</strong></span>: F-measure is an attempt to reward reductions in false negatives and false positives at the same time: <p>
<span class="emphasis"><em>F-measure = 2*TP / (2*TP + FP + FN)</em></span>
</p><p>It is mainly an academic measure to declare that one system is better than another. It sees little use in industry.</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Recall at 90 percent precision</strong></span>: The goal is to provide as much coverage as possible while not making more than 10 percent false positives. This is when the system does not want to look bad very often; this applies to spell checkers, question answering systems, and sentiment dashboards.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Precision at 99.9 percent recall</strong></span>: This metric supports <span class="emphasis"><em>needle in the haystack</em></span> or <span class="emphasis"><em>needle in the needle stack</em></span> kind of problems. The user cannot afford to miss anything and is willing to perhaps slog through lots of false positives as long as they don't miss anything. The system is better if the false positive rate is lower. Use cases are intelligence analysts and medical researchers.</li></ul></div></li><li class="listitem">Determining this metric comes from a mixture of business/research needs, technical capability, available resources, and willpower. If a customer wants a high recall and high-precision system, our first question will be to ask what the budget is per document. If it is high enough, we will suggest hiring experts to correct system output, which is the best combination of what computers are good at (exhaustiveness) and what humans are good at (discrimination). Generally, budgets don't support this, so the balancing act begins, but we have deployed systems in just this way.</li><li class="listitem">For this recipe, we will pick a maximizing recall at 50-percent precision for <code class="literal">n</code> (negative), because we want to be sure to intercept any negative sentiment and will <a class="indexterm" id="id315"/>tolerate false positives. We will choose 65 percent for a <code class="literal">p</code> positive, because the good news is less actionable, <a class="indexterm" id="id316"/>and who doesn't love Disney? We don't care what <code class="literal">o</code> (other performance) is—the category exists for linguistic reasons, independent of the business use. This metric a likely metric for a sentiment-dashboard application. This means that the system will produce one mistake for every two guesses of a negative-sentiment category and 13 out of 20 for positive sentiment.</li></ol></div></div><div class="section" title="Implementing the evaluation metric"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec13"/>Implementing the evaluation metric</h3></div></div></div><p>Perform <a class="indexterm" id="id317"/>the following steps to implement the evaluation metric:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We will start with <a class="indexterm" id="id318"/>reporting precision/recall for all categories with the <code class="literal">Util.printPrecRecall</code> method after printing out the confusion matrix:<div class="informalexample"><pre class="programlisting">Util.printConfusionMatrix(evaluator.confusionMatrix());
<span class="strong"><strong>Util.printPrecRecall(evaluator);</strong></span>
</pre></div></li><li class="listitem">The output will now look like this:<div class="informalexample"><pre class="programlisting">reference\response
          \p,n,o,
         p 45,8,17,
         n 16,13,3,
         o 18,3,88,
Category p
Recall: 0.64
Prec  : 0.57
Category n
Recall: 0.41
Prec  : 0.54
Category o
Recall: 0.81
Prec  : 0.81</pre></div></li><li class="listitem">The precision for <code class="literal">n</code> exceeds our objective of <code class="literal">.5</code>–since we want to maximize recall at <code class="literal">.5</code>, we can make a few more mistakes before we get to the limit. You can refer to the <span class="emphasis"><em>Thresholding classifiers</em></span> recipe to find out how to do this.</li><li class="listitem">The precision for <code class="literal">p</code> is 57 percent, and this is too low for our business objective. Logistic regression classifiers, however, provide a conditional probability that might allow us to meet the precision needs just by paying attention to the probability. Adding the following line of code will allow us to see the results sorted by conditional probability:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Util.printPRcurve(evaluator);</strong></span>
</pre></div></li><li class="listitem">The preceding line of code starts by getting a <code class="literal">ScoredPrecisionRecallEvaluation</code> value from the evaluator. A double-scored curve (<code class="literal">[][])</code> is gotten from that object with the Boolean interpolate set to false, because we want the curve to be unadulterated. You can look at the Javadoc <a class="indexterm" id="id319"/>for what is going on. Then, we will use a print route from the same class to print <a class="indexterm" id="id320"/>out the curve. The output will look like this:<div class="informalexample"><pre class="programlisting">reference\response
          \p,n,o,
         p 45,8,17,
         n 16,13,3,
         o 18,3,88,
Category p
Recall: 0.64
Prec  : 0.57
Category n
Recall: 0.41
Prec  : 0.54
Category o
Recall: 0.81
Prec  : 0.81
PR Curve for Category: p
  PRECI.   RECALL    SCORE
0.000000 0.000000 0.988542
0.500000 0.014286 0.979390
0.666667 0.028571 0.975054
0.750000 0.042857 0.967286
0.600000 0.042857 0.953539
0.666667 0.057143 0.942158
0.571429 0.057143 0.927563
0.625000 0.071429 0.922381
0.555556 0.071429 0.902579
0.600000 0.085714 0.901597
0.636364 0.100000 0.895898
0.666667 0.114286 0.891566
0.615385 0.114286 0.888831
0.642857 0.128571 0.884803
0.666667 0.142857 0.877658
0.687500 0.157143 0.874135
0.647059 0.157143 0.874016
0.611111 0.157143 0.871183
0.631579 0.171429 0.858999
0.650000 0.185714 0.849296
0.619048 0.185714 0.845691
0.636364 0.200000 0.810079
0.652174 0.214286 0.807661
0.666667 0.228571 0.807339
0.640000 0.228571 0.799474
0.653846 0.242857 0.753967
0.666667 0.257143 0.753169
0.678571 0.271429 0.751815
0.655172 0.271429 0.747515
0.633333 0.271429 0.745660
0.645161 0.285714 0.744455
<span class="strong"><strong>0.656250 0.300000 0.738555</strong></span>
0.636364 0.300000 0.736310
0.647059 0.314286 0.705090
0.628571 0.314286 0.694125</pre></div></li><li class="listitem">The output is sorted by score, in the third column, which in this case, happens to be a conditional probability, so the max value is 1 and min value is 0. Notice that the recall grows as correct cases are found (the second line), and it never <a class="indexterm" id="id321"/>goes <a class="indexterm" id="id322"/>down. However, when a mistake is made like in the fourth line, precision drops to <code class="literal">.6</code>, because 3 out of 5 cases are correct so far. The precision actually goes below <code class="literal">.65</code> before the last value is found—in bold, with a score of <code class="literal">.73</code>.</li><li class="listitem">So, without any tuning, we can report that we can achieve 30 percent recall for <code class="literal">p</code> at our accepted precision limit of 65 percent. This requires that we threshold the classifier at <code class="literal">.73</code> for the category, which means if we reject scores less than <code class="literal">.73</code> for <code class="literal">p</code>, some comments are:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We got lucky. Usually, the first classifier runs do not reveal an immediately useful threshold with default values.</li><li class="listitem" style="list-style-type: disc">Logistic regression classifiers have a very nice property that they provide; they also provide conditional probability estimates for thresholding. Not all classifiers have this property—language models and naïve Bayes classifiers tend to push scores towards 0 or 1, making thresholding difficult.</li><li class="listitem" style="list-style-type: disc">As the training data is highly biased (this is from the <span class="emphasis"><em>Train a little, learn a little – active learning</em></span> recipe that follows), we cannot trust this threshold. The classifier will have to be pointed at fresh data to set the threshold. Refer to the <span class="emphasis"><em>Thresholding classifiers</em></span> recipe to see how this is done.</li><li class="listitem" style="list-style-type: disc">This classifier has seen very little data and will not be a good candidate for deployment despite the supporting evaluation. We would be more comfortable with at least 1,000 tweets from a diverse set of dates.</li></ul></div></li></ol></div><p>At this point in the process, we either accept the results by verifying that the performance is acceptable on <a class="indexterm" id="id323"/>fresh data or turn to improving the classifier by techniques covered by other recipes in this chapter. The final step of <a class="indexterm" id="id324"/>the recipe is to train up the classifier on all training data and write to disk:</p><div class="informalexample"><pre class="programlisting">corpus.setNumFolds(0);
LogisticRegressionClassifier&lt;CharSequence&gt; classifier 
  = Util.trainLogReg(corpus, tokenizerFactory, progressWriter);
AbstractExternalizable.compileTo(classifier, 
  new File("models/ClassifierBuilder.LogisticRegression"));</pre></div><p>We will use the resulting model in the <span class="emphasis"><em>Thresholding classifiers</em></span> recipe.</p></div></div></div>
<div class="section" title="Linguistic tuning"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec41"/>Linguistic tuning</h1></div></div></div><p>This recipe will <a class="indexterm" id="id325"/>address issues around tuning the classifier by paying attention to the mistakes made by the system and making linguistic adjustments by adjusting parameters and features. We will continue with the sentiment use case from the previous recipe and work with the same data. We will start with a fresh class at <code class="literal">src/com/lingpipe/cookbook/chapter3/LinguisticTuning.java</code>.</p><p>We have very little data. In the real world, we will insist on more training data—at least 100 of the smallest category, negative, are needed with a natural distribution of positives and others.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec93"/>How to do it…</h2></div></div></div><p>We will jump right in and run some data—the default is <code class="literal">data/activeLearningCompleted/disneySentimentDedupe.2.csv</code>, but you can specify your own file in the command line.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Run the following in your command line or IDE equivalent: <div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.LinguisticTuning</strong></span>
</pre></div></li><li class="listitem">For each fold, the <a class="indexterm" id="id326"/>features for the classifier will be printed. The output will look like the following for each category (just the first few features for each):<div class="informalexample"><pre class="programlisting">Training on fold 0
######################Printing features for category p NON_ZERO 
?: 0.52
!: 0.41
love: 0.37
can: 0.36
my: 0.36
is: 0.34
in: 0.29
of: 0.28
I: 0.28
old: 0.26
me: 0.25
My: 0.25
?: 0.25
wait: 0.24
?: 0.23
an: 0.22
out: 0.22
movie: 0.22
?: 0.21
movies: 0.21
shirt: 0.21
t: 0.20
again: 0.20
Princess: 0.19
i: 0.19 
…
######################Printing features for category o NON_ZERO 
:: 0.69
/: 0.52
*&amp;^INTERCEPT%$^&amp;**: 0.48
@: 0.41
*: 0.36
(: 0.35
…
######################Printing features for category n ZERO</pre></div></li><li class="listitem">Starting with the <code class="literal">n</code> category, note that there are no features. It is a property of logistic regression that one category's features are all set to <code class="literal">0.0</code>, and the remaining <code class="literal">n-1</code> category's features are offset accordingly. This cannot be controlled, which is a bit annoying because the <code class="literal">n</code> or negative category can be the focus of linguistic tuning given how badly it performs in the example. Not to be deterred, we will move on.</li><li class="listitem">Note that the output is intended to make it easy to use a <code class="literal">find</code> command to locate feature output in the extensive reporting output. To find a feature search on <code class="literal">category &lt;feature name&gt;</code> to see if there is a nonzeroed report, search on <code class="literal">category &lt;feature name&gt; NON_ZERO</code>.</li><li class="listitem">We are looking for a few things in these features. First of all, there are apparently odd features that are getting big scores—the output is ranked in positive to negative for the category. What we want to look for is some signal in the feature weights—so <code class="literal">love</code> makes sense as being associated with a positive sentiment. Looking at features like this can really be surprising and counter intuitive. The uppercase <code class="literal">I</code> and lowercase <code class="literal">i</code> suggest that the text should be downcased. We <a class="indexterm" id="id327"/>will make this change and see if it helps. Our current performance is:<div class="informalexample"><pre class="programlisting">Category p
Recall: 0.64
Prec  : 0.57</pre></div></li><li class="listitem">The code change is to add a <code class="literal">LowerCaseTokenizerFactory</code> item to the current <code class="literal">IndoEuropeanTokenizerFactory</code> class:<div class="informalexample"><pre class="programlisting">TokenizerFactory tokenizerFactory 
  = IndoEuropeanTokenizerFactory.INSTANCE;
tokenizerFactory = new   LowerCaseTokenizerFactory(tokenizerFactory);</pre></div></li><li class="listitem">Run the code, and we will pick up some precision and recall:<div class="informalexample"><pre class="programlisting">Category p
Recall: 0.69
Prec  : 0.59</pre></div></li><li class="listitem">The features are as follows:<div class="informalexample"><pre class="programlisting">Training on fold 0
######################Printing features for category p NON_ZERO 
?: 0.53
my: 0.49
love: 0.43
can: 0.41
!: 0.39
i: 0.35
is: 0.31
of: 0.28
wait: 0.27
old: 0.25
♥: 0.24
an: 0.22</pre></div></li><li class="listitem">What's the next move? The <code class="literal">minFeature</code> count is very low at <code class="literal">1</code>. Let's raise it to <code class="literal">2</code> and see what happens:<div class="informalexample"><pre class="programlisting">Category p
Recall: 0.67
Prec  : 0.58</pre></div></li><li class="listitem">This hurts performance by a few cases, so we will return to <code class="literal">1</code>. However, experience dictates that the minimum count goes up as more data is found to prevent overfitting.</li><li class="listitem">It is time for the secret sauce—change the tokenizer to <code class="literal">NGramTokenizer</code>; it tends to work better than standard tokenizers—we are now rolling with the following code:<div class="informalexample"><pre class="programlisting">TokenizerFactory tokenizerFactory 
  = new NGramTokenizerFactory(2,4);
tokenizerFactory 
= new LowerCaseTokenizerFactory(tokenizerFactory);</pre></div></li><li class="listitem">This worked. We <a class="indexterm" id="id328"/>will pick up a few more cases:<div class="informalexample"><pre class="programlisting">Category p
Recall: 0.71
Prec  : 0.64</pre></div></li><li class="listitem">However, the features are now pretty hard to scan:<div class="informalexample"><pre class="programlisting">#########Printing features for category p NON_ZERO 
ea: 0.20
!!: 0.20
ov: 0.17
n : 0.16
ne: 0.15
 ?: 0.14
al: 0.13
rs: 0.13
ca: 0.13
! : 0.13
ol: 0.13
lo: 0.13
 m: 0.13
re : 0.12
so: 0.12
i : 0.12
f : 0.12
 lov: 0.12 </pre></div></li><li class="listitem">We have found over the course of time that character ngrams are the features of choice for text-classifier problems. They seem to nearly always help, and they helped here. Look at the features, and you can recover that <code class="literal">love</code> is still contributing but in little bits, such as <code class="literal">lov</code>, <code class="literal">ov</code>, and <code class="literal">lo</code>.</li><li class="listitem">There is another approach that deserves a mention, which is some of the tokens produced by <code class="literal">IndoEuropeanTokenizerFactory</code> are most likely useless, and they are just confusing the issue. Using a stop-word list, focusing on more useful tokenization, and perhaps applying a stemmer such as the Porter stemmer might work as well. This has been the traditional approach to these kinds of problems—we have never had that much luck with them.</li><li class="listitem">It is a good time to <a class="indexterm" id="id329"/>check on the performance of the <code class="literal">n</code> category; we have been messing about with the model and should check it:<div class="informalexample"><pre class="programlisting">Category n
Recall: 0.41
Prec  : 0.72</pre></div></li><li class="listitem">The output also reports false positives for <code class="literal">p</code> and <code class="literal">n</code>. We really don't care much about <code class="literal">o</code>, except when it shows up as a false positive for the other categories:<div class="informalexample"><pre class="programlisting">False Positives for p
*&lt;category&gt; is truth category

I was really excited for Disney next week until I just read that it's "New Jersey" week. #noooooooooo
 p 0.8434727204351016
 o 0.08488521562829848
*n 0.07164206393660003

"Why worry? If you've done the best you can, worrying won't make anything better." ~Walt Disney
 p 0.4791823543407749
*o 0.3278392260935065
 n 0.19297841956571868</pre></div></li><li class="listitem">Looking at false positives, we can suggest changes to feature extraction. Recognizing quotes from <code class="literal">~Walt Disney</code> might help the classifier with <code class="literal">IS_DISNEY_QUOTE</code>.</li><li class="listitem">Also, looking at errors can point out errors in annotation, one can argue that the following is actually positive:<div class="informalexample"><pre class="programlisting">Cant sleep so im watching.. Beverley Hills Chihuahua.. Yep thats right, I'm watching a Disney film about talking dogs.. FML!!!
 p 0.6045997587907997
 o 0.3113342571409484
*n 0.08406598406825164</pre></div><p>At this point, the system is somewhat tuned. The configuration should be saved someplace and the next steps are considered. They include the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Declare victory and deploy. Before deploying, be sure to test on novel data using <a class="indexterm" id="id330"/>all training data to train. The <span class="emphasis"><em>Thresholding classifiers</em></span> recipe will be very useful.</li><li class="listitem" style="list-style-type: disc">Annotate more data. Use the active learning framework in the following recipe to help identify high-confidence cases that are wrong and right. This will likely help more than anything with performance, especially with low-count data such as the kind we have been working with.</li><li class="listitem" style="list-style-type: disc">Looking at the epoch report, the system is never converging on its own. Increase the limit to 10,000 and see if this helps things.</li></ul></div><p>The result of our tuning efforts was to improve the performance from:</p><div class="informalexample"><pre class="programlisting">reference\response
          \p,n,o,
         p 45,8,17,
         n 16,13,3,
         o 18,3,88,
Category p
Recall: 0.64
Prec  : 0.57
Category n
Recall: 0.41
Prec  : 0.54
Category o
Recall: 0.81
Prec  : 0.81</pre></div><p>To the following:</p><div class="informalexample"><pre class="programlisting">reference\response
          \p,n,o,
         p 50,3,17,
         n 14,13,5,
         o 14,2,93,
Category p
Recall: 0.71
Prec  : 0.64
Category n
Recall: 0.41
Prec  : 0.72
Category o
Recall: 0.85
Prec  : 0.81</pre></div><p>This is not a bad uptick in performance in exchange for looking at some data and <a class="indexterm" id="id331"/>thinking a bit about how to help the classifier do its job.</p></li></ol></div></div></div>
<div class="section" title="Thresholding classifiers"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec42"/>Thresholding classifiers</h1></div></div></div><p>Logistic <a class="indexterm" id="id332"/>regression classifiers are often deployed with a threshold rather than the provided <code class="literal">classifier.bestCategory()</code> method. This method picks the <a class="indexterm" id="id333"/>category with the highest conditional probability, which, in a 3-way classifier, can be just above one-third. This recipe will show you how to adjust classifier performance by explicitly controlling how the best category is determined.</p><p>This recipe will consider the 3-way case with the <code class="literal">p</code>, <code class="literal">n</code>, and <code class="literal">o</code> labels and work with the classifier produced by the <span class="emphasis"><em>Classifier-building life cycle</em></span> recipe earlier in this chapter. The cross-validation evaluation produced is:</p><div class="informalexample"><pre class="programlisting">Category p
Recall: 0.64
Prec  : 0.57
Category n
Recall: 0.41
Prec  : 0.54
Category o
Recall: 0.81
Prec  : 0.81</pre></div><p>We will run novel data to set thresholds.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec94"/>How to do it...</h2></div></div></div><p>Our business use case is that recall be maximized while <code class="literal">p</code> has <code class="literal">.65</code> precision and <code class="literal">n</code> has <code class="literal">.5</code> precision for reasons discussed in the <span class="emphasis"><em>Classifier-building life cycle</em></span> recipe. The <code class="literal">o</code> category is not important in this case. The <code class="literal">p</code> category appears to be too low with <code class="literal">.57</code>, and the <code class="literal">n</code> category can increase recall as the precision is above <code class="literal">.5</code>.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We cannot use the cross-validation results unless care has been taken to produce a proper distribution of annotations—the active learning approach used tends to not produce such distributions. Even with a good distribution, the fact that the classifier was likely tuned with cross validation means that it is most likely overfit to that dataset because tuning decisions were made to maximize performance of those sets that are not general to new data.</li><li class="listitem">We need to point <a class="indexterm" id="id334"/>the trained classifier at new data—the rule of thumb is to train by hook or crook but always threshold on fresh. We followed the <span class="emphasis"><em>Getting data from the Twitter API</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, and downloaded new data from Twitter with the <code class="literal">disney</code> query. Nearly a year has passed since our initial search, so the tweets are most likely non-overlapping. The resulting 1,500 tweets were put into <code class="literal">data/freshDisney.csv</code>.</li><li class="listitem">Ensure that you don't run this code on data that is not backed up. The I/O is simple rather than robust. The code overwrites the input file.</li><li class="listitem">Invoke <code class="literal">RunClassifier</code> on your IDE or run the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3/RunClassifier</strong></span>
<span class="strong"><strong>Data is: data/freshDisney.csv model is: models/ClassifierBuilder.LogisticRegression</strong></span>
<span class="strong"><strong>No annotations found, not evaluating</strong></span>
<span class="strong"><strong>writing scored output to data/freshDisney.csv</strong></span>
</pre></div></li><li class="listitem">Open the <code class="literal">.csv</code> file in your favorite spreadsheet. All tweets should have a score and a guessed category in the standard annotation format.</li><li class="listitem">Sort with the primary sort on the <code class="literal">GUESS</code> column in the ascending or descending order and then sort on <code class="literal">SCORE</code> in the descending order. The result should be each category with higher scores descending to lower scores. This is how we set up top-down annotations.<div class="mediaobject"><img alt="How to do it..." src="graphics/4672OS_03_01.jpg"/><div class="caption"><p>Setting up sort of data for top-down annotation. All categories are grouped together, and a descending sort of the score is established.</p></div></div></li><li class="listitem">For the categories that you care about, in this case, <code class="literal">p</code> and <code class="literal">n</code>, annotate truth from the highest score to the lowest scores until it is likely that the precision goal has been broached. For example, annotate <code class="literal">n</code> until you either run out of <code class="literal">n</code> guesses, or you have enough mistakes that you have <code class="literal">.50</code> precision. A mistake is when the <a class="indexterm" id="id335"/>truth is <code class="literal">o</code> or <code class="literal">p</code>. Do the same for <code class="literal">p</code> until you have a precision  of <code class="literal">.65</code>, or you run out of number of <code class="literal">p</code>. For our canned example, we have put the annotations in <code class="literal">data/freshDisneyAnnotated.csv</code>.</li><li class="listitem">Run the following command or the equivalent in your IDE (note that we are supplying the input file and not using the default):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3/RunClassifier data/freshDisneyAnnotated.csv</strong></span>
</pre></div></li><li class="listitem">This command will produce the following output:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Data is: data/freshDisneyAnnotated.csv model is: models/ClassifierBuilder.LogisticRegression</strong></span>
<span class="strong"><strong>reference\response</strong></span>
<span class="strong"><strong>          \p,n,o,</strong></span>
<span class="strong"><strong>         p 141,25,0,</strong></span>
<span class="strong"><strong>         n 39,37,0,</strong></span>
<span class="strong"><strong>         o 51,28,0,</strong></span>
<span class="strong"><strong>Category p</strong></span>
<span class="strong"><strong>Recall: 0.85</strong></span>
<span class="strong"><strong>Prec  : 0.61</strong></span>
<span class="strong"><strong>Category n</strong></span>
<span class="strong"><strong>Recall: 0.49</strong></span>
<span class="strong"><strong>Prec  : 0.41</strong></span>
<span class="strong"><strong>Category o</strong></span>
<span class="strong"><strong>Recall: 0.00</strong></span>
<span class="strong"><strong>Prec  : NaN</strong></span>
</pre></div></li><li class="listitem">First off, this is a surprisingly good system performance for our minimally trained classifier. <code class="literal">p</code> is very close to the target precision of <code class="literal">.65</code> without thresholding, and coverage is not bad: it is found as 141 true positives out of 1,500 tweets. As we have not annotated all 1,500 tweets, we cannot truly say what the recall of the classifier is, so the term is overloaded in common use. The <code class="literal">n</code> category is not doing as well, but it is still pretty good. Our annotation did no annotations for the <code class="literal">o</code> category, so the system column is all zeros.</li><li class="listitem">Next, we will look <a class="indexterm" id="id336"/>at the precision/recall/score curve for thresholding guidance:<div class="informalexample"><pre class="programlisting">PR Curve for Category: p
  PRECI.   RECALL    SCORE
1.000000 0.006024 0.976872
1.000000 0.012048 0.965248
1.000000 0.018072 0.958461
1.000000 0.024096 0.947749
1.000000 0.030120 0.938152
1.000000 0.036145 0.930893
1.000000 0.042169 0.928653
…
0.829268 0.204819 0.781308
0.833333 0.210843 0.777209
0.837209 0.216867 0.776252
0.840909 0.222892 0.771287
0.822222 0.222892 0.766425
0.804348 0.222892 0.766132
0.808511 0.228916 0.764918
0.791667 0.228916 0.761848
0.795918 0.234940 0.758419
0.780000 0.234940 0.755753
0.784314 0.240964 0.755314
…
0.649746 0.771084 0.531612
0.651515 0.777108 0.529871
0.653266 0.783133 0.529396
0.650000 0.783133 0.528988
0.651741 0.789157 0.526603
0.648515 0.789157 0.526153
0.650246 0.795181 0.525740
0.651961 0.801205 0.525636
0.648780 0.801205 0.524874</pre></div></li><li class="listitem">Most values have been elided to save space in the preceding output. We saw that the point at which the classifier passes <code class="literal">.65</code> precision has a score of <code class="literal">.525</code>. This means that we can expect 65-percent precision if we threshold at <code class="literal">.525</code> with a bunch of caveats:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">This is a single-point sample without a confidence estimate. There are more sophisticated ways to arrive at a threshold that is beyond the scope of this recipe.</li><li class="listitem" style="list-style-type: disc">Time is a big contributor to variance in performance.</li><li class="listitem" style="list-style-type: disc">10-percent variance in performance for well-developed classifiers is not uncommon in practice. Factor this into performance <a class="indexterm" id="id337"/>requirements.</li></ul></div></li><li class="listitem">The nice thing about the preceding curve is that it looks like we can provide a <code class="literal">.80</code> precision classifier at a threshold of <code class="literal">.76</code> with nearly 30 percent of the coverage of the <code class="literal">.65</code> precision classifier if we decide that higher precision is called for.</li><li class="listitem">The <code class="literal">n</code> case has a curve that looks like this:<div class="informalexample"><pre class="programlisting">PR Curve for Category: n
  PRECI.   RECALL    SCORE
1.000000 0.013158 0.981217
0.500000 0.013158 0.862016
0.666667 0.026316 0.844607
0.500000 0.026316 0.796797
0.600000 0.039474 0.775489
0.500000 0.039474 0.768295
…
0.468750 0.197368 0.571442
0.454545 0.197368 0.571117
0.470588 0.210526 0.567976
0.485714 0.223684 0.563354
0.500000 0.236842 0.552538
0.486486 0.236842 0.549950
0.500000 0.250000 0.549910
0.487179 0.250000 0.547843
0.475000 0.250000 0.540650
0.463415 0.250000 0.529589</pre></div></li><li class="listitem">It looks like a threshold of <code class="literal">.549</code> gets the job done. The rest of the recipe will show how you to set up the thresholded classifier now that we have the thresholds.</li></ol></div><p>The code behind <code class="literal">RunClassifier.java</code> offers nothing of novelty in the context of this chapter, so it is left to you to work through.</p></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec95"/>How it works…</h2></div></div></div><p>The goal is to create a classifier that will assign <code class="literal">p</code> to a tweet if it scores above <code class="literal">.525</code> for that category and <code class="literal">n</code> if scores above <code class="literal">.549</code> for that category; otherwise, it gets <code class="literal">o</code>. Wrong….management saw the p/r curve and now insists that <code class="literal">p</code> must be 80-percent precise, which means that the threshold will be <code class="literal">.76</code>.</p><p>The solution is very simple. If a score for <code class="literal">p</code> is below <code class="literal">.76</code>, then it will be rescored down to <code class="literal">0.0</code>. Likewise, if a score for <code class="literal">n</code> is below <code class="literal">.54</code>, then it will be rescored down to <code class="literal">0.0</code>. The effect of this is that <code class="literal">o</code> will <a class="indexterm" id="id338"/>be the best category for all below-threshold cases, because <code class="literal">.75</code> <code class="literal">p</code> can at best be <code class="literal">.25</code> <code class="literal">n</code>, which remains below the <code class="literal">n</code> threshold, and <code class="literal">.53</code> <code class="literal">n</code> can at most be <code class="literal">.47</code> <code class="literal">p</code>, which is below that category's threshold. This can get complicated if all categories are thresholded, or the thresholds are low.</p><p>Stepping back, we are taking a conditional classifier where all the category scores must sum to 1 and breaking this contract, because we will take any estimate for <code class="literal">p</code> that is below <code class="literal">.76</code> and bust it down to <code class="literal">0.0</code>. It is a similar story for <code class="literal">n</code>. The resulting classifier will now have to be <code class="literal">ScoredClassifier</code> because this is the next most specific contract in the LingPipe API that we can uphold.</p><p>The code for this class is in <code class="literal">src/com/lingpipe/cookbook/chapter3/ThresholdedClassifier</code>. At the top level, we have the class, relevant member variable, and constructor:</p><div class="informalexample"><pre class="programlisting">public class ThresholdedClassifier&lt;E&gt; implements  ScoredClassifier&lt;E&gt; {
  
  ConditionalClassifier&lt;E&gt; mNonThresholdedClassifier;

  public ThresholdedClassifier (ConditionalClassifier&lt;E&gt; classifier) {
    mNonThresholdedClassifier = classifier;
  }</pre></div><p>Next, we will implement the only required method for <code class="literal">ScoredClassification</code>, and this is where the magic happens:</p><div class="informalexample"><pre class="programlisting">@Override
public ScoredClassification classify(E input) {
  ConditionalClassification classification 
    = mNonThresholdedClassifier.classify(input);
  List&lt;ScoredObject&lt;String&gt;&gt; scores 
      = new ArrayList&lt;ScoredObject&lt;String&gt;&gt;();
  for (int i = 0; i &lt; classification.size(); ++i) {
    String category = classification.category(i);     Double score = classification.score(i);
     if (category.equals("p") &amp;&amp; score &lt; .76d) {
       score = 0.0;
     }
    if (category.equals("n") &amp;&amp; score &lt; .549d) {
       score = 0.0;
     }
     ScoredObject&lt;String&gt; scored 
      = new ScoredObject&lt;String&gt;(category,score);
    scores.add(scored);
  }
  ScoredClassification thresholded 
    = ScoredClassification.create(scores);
  return thresholded;
}</pre></div><p>The complicated bit about scored classifications is that scores have to be assigned to all categories even if the score is <code class="literal">0.0</code>. The mapping from a conditional classification, where all scores sum to <code class="literal">1.0</code>, does not lend itself to a generic solution, which is why the preceding ad hoc <a class="indexterm" id="id339"/>implementation is used.</p><p>There is also a <code class="literal">main()</code> method that spools up the relevant bits for <code class="literal">ThresholdedClassifier</code> and applies them:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3/ThresholdedClassifier data/freshDisneyAnnotated.csv </strong></span>
<span class="strong"><strong>Data is: data/freshDisneyAnnotated.csv model is: models/ClassifierBuilder.LogisticRegression</strong></span>

<span class="strong"><strong>reference\response</strong></span>
<span class="strong"><strong>          \p,n,o,</strong></span>
<span class="strong"><strong>         p 38,14,114,</strong></span>
<span class="strong"><strong>         n 5,19,52,</strong></span>
<span class="strong"><strong>         o 5,5,69,</strong></span>
<span class="strong"><strong>Category p</strong></span>
<span class="strong"><strong>Recall: 0.23</strong></span>
<span class="strong"><strong>Prec  : 0.79</strong></span>
<span class="strong"><strong>Category n</strong></span>
<span class="strong"><strong>Recall: 0.25</strong></span>
<span class="strong"><strong>Prec  : 0.50</strong></span>
<span class="strong"><strong>Category o</strong></span>
<span class="strong"><strong>Recall: 0.87</strong></span>
<span class="strong"><strong>Prec  : 0.29</strong></span>
</pre></div><p>The thresholds are doing exactly as designed; <code class="literal">p</code> is <code class="literal">.79</code> precision, which is close enough for consulting, and <code class="literal">n</code> is spot on. The source for the <code class="literal">main()</code> method should be straightforward given the context of this chapter.</p><p>That's it. Almost never do we release a nonthresholded classifier, and best practices require that thresholds be set on held-out data, preferably from later epochs than the training data. Logistic regression is quite robust against skewed training data, but the ointment that cleanses the flaws of skewed data is novel data annotated top down to precision objectives. Yes, it is possible to <a class="indexterm" id="id340"/>threshold with cross validation, but it suffers from the flaws that overfit due to tuning, and you would screw up your distributions. Recall-oriented objectives are another matter.</p></div></div>
<div class="section" title="Train a little, learn a little &#x2013; active learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec43"/>Train a little, learn a little – active learning</h1></div></div></div><p>Active <a class="indexterm" id="id341"/>learning is a super power to quickly develop classifiers. It has saved many a project in the real world. The idea is very simple and can be broken down as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Assemble a packet of raw data that is way bigger than you can annotate manually.</li><li class="listitem">Annotate an embarrassingly small amount of the raw data.</li><li class="listitem">Train the classifier on the embarrassingly small amount of training data.</li><li class="listitem">Run the trained classifier on all the data.</li><li class="listitem">Put the classifier output into a <code class="literal">.csv</code> file ranked by confidence of best category.</li><li class="listitem">Correct another embarrassingly small amount of data, starting with the most confident classifications.</li><li class="listitem">Evaluate the performance.</li><li class="listitem">Repeat the process until the performance is acceptable, or you run out of data.</li><li class="listitem">If successful, be sure to evaluate/threshold on fresh data, because the active learning process can introduce biases to the evaluation.</li></ol></div><p>What this process does is help the classifier distinguish the cases where it is making higher confidence mistakes and correcting it. It also works as a classification-driven search engine of sorts, where the positive training data functions as the query, and the remaining data functions as the index being searched.</p><p>Traditionally, active learning is applied to the near-miss cases where the classifier is unsure of the correct class. In this case, the corrections will apply to the lowest confidence classifications. We came up with the high-confidence correction approach because we were under pressure to increase precision with a thresholded classifier that only accepted high-confidence decisions.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec96"/>Getting ready</h2></div></div></div><p>What is going on here is that we are using the classifier to find more data that looks like what it knows about. For <a class="indexterm" id="id342"/>problems where the target classes are rare in the unannotated data, it can very quickly help the system identify more examples of the class. For example, in a binary-classification task with marginal probability of 1 percent for the target class in the raw data, this is almost certainly the way to go. You cannot ask annotators to reliably mark a 1-in-100 phenomenon over time. While this is the right way to do it, the end result is that it will not be done because of the effort involved.</p><p>Like most cheats, shortcuts, and super powers, the question to ask is what is the price paid. In the duality of precision and recall, recall suffers with this approach. This is because the approach biases annotation towards known cases. Cases that have very different wording are unlikely to be found, so coverage can suffer.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec97"/>How to do it…</h2></div></div></div><p>Let's get started with active learning:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Collect some training data in our <code class="literal">.csv</code> format from <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, or use our example data in <code class="literal">data/activeLearning/disneyDedupe.0.csv</code>. Our data builds on the Disney tweets from <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>. Sentiment is a good candidate for active learning, because it benefits from quality training data and creating quality training data can be difficult. Use the <code class="literal">.csv</code> file format from the Twitter search downloader if you are using your own data.</li><li class="listitem">Run the <code class="literal">.csv</code> deduplication routine from the <span class="emphasis"><em>Eliminate near duplicates with Jaccard distance</em></span> recipe of <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span> to get rid of near-duplicate tweets. We have already done this with our example data. We went from 1,500 tweets to 1,343.</li><li class="listitem">If you have your own data, annotate around 25 examples in the <code class="literal">TRUTH</code> column according to the standard annotation:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">p</code> stands for positive sentiment</li><li class="listitem" style="list-style-type: disc"><code class="literal">n</code> stands for negative sentiment</li><li class="listitem" style="list-style-type: disc"><code class="literal">o</code> stands for other, which means that no sentiment is expressed, or the tweet is not in English</li><li class="listitem" style="list-style-type: disc">Be sure to get a few examples of each category</li></ul></div><p>Our example data is <a class="indexterm" id="id343"/>already annotated for this step. If you are using your own data, be sure to use the format of the first file (that has the <code class="literal">0.csv</code> format), with no other <code class="literal">.</code> in the path.</p><div class="mediaobject"><img alt="How to do it…" src="graphics/4672OS_03_02.jpg"/><div class="caption"><p>Examples of tweets annotated. Note that all categories have examples.</p></div></div></li><li class="listitem">Run the following command. Do not do this on your own annotated data without backing up the file. Our I/O routine is written for simplicity, not robustness. You have been warned:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar: com.lingpipe.cookbook.chapter3.ActiveLearner </strong></span>
</pre></div></li><li class="listitem">Pointed at the supplied annotated data, this will print the following to the console with a final suggestion:<div class="informalexample"><pre class="programlisting">reference\response
          \p,n,o,
         p 7,0,1,
         n 1,0,3,
         o 2,0,11,
Category p
Recall: 0.88
Prec  : 0.70
Category n
Recall: 0.00
Prec  : NaN
Category o
Recall: 0.85
Prec  : 0.73
Writing to file: data/activeLearning/disneySentimentDedupe.1.csv
Done, now go annotate and save with same file name</pre></div></li><li class="listitem">This recipe will show you how to make it better, mainly by making it bigger in smart ways. Let's see <a class="indexterm" id="id344"/>where we stand:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The data has been annotated a bit for three categories</li><li class="listitem" style="list-style-type: disc">Of 1,343 tweets, there have been 25 annotated, 13 of which are <code class="literal">o</code>, which we don't particularly care about given the use case, but they still are important because they are not <code class="literal">p</code> or <code class="literal">n</code></li><li class="listitem" style="list-style-type: disc">This is not nearly enough annotated data to build a reliable classifier with, but we can use it to help annotate more data</li><li class="listitem" style="list-style-type: disc">The last line encourages more annotation and the name of a file to annotate</li></ul></div></li><li class="listitem">The precision and recall are reported for each category, that is, the result of cross validation over the training data. There is also a confusion matrix. At this point, we are not expecting very good performance, but <code class="literal">p</code> and <code class="literal">o</code> are doing quite well. The <code class="literal">n</code> category is not doing well at all.<p>Next, fire up a spreadsheet, and import and view the indicated <code class="literal">.csv</code> file using a UTF-8 encoding. OpenOffice shows us the following:</p><div class="mediaobject"><img alt="How to do it…" src="graphics/4672OS_03_03.jpg"/><div class="caption"><p>Initial output of the active learning approach</p></div></div></li><li class="listitem">Reading from the left-hand side to the right-hand side, we will see the <code class="literal">SCORE</code> column, which reflects the classifier's confidence; its most likely category, shown in the <code class="literal">GUESS</code> column, is correct. The next column is the <code class="literal">TRUTH</code> class as determined by a human. The last <code class="literal">TEXT</code> column is the tweet being classified.</li><li class="listitem">All 1,343 tweets have been classified in one of two ways:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If the tweet had an annotation, that is, an entry in the <code class="literal">TRUTH</code> column, then the annotation was made when the tweet was in the test fold of a 10-fold cross validation. Line 13 is just such a case. In this case, the <a class="indexterm" id="id345"/>classification was <code class="literal">o</code>, but the truth was <code class="literal">p</code>, so it would be a false negative for <code class="literal">p</code>.</li><li class="listitem" style="list-style-type: disc">If the tweet was not annotated, that is, no entry in the <code class="literal">TRUTH</code> column, then it was classified using all the available training data. All other examples in the shown spreadsheet are handled this way. They don't inform the evaluation at all. We will annotate these tweets to help improve classifier performance.</li></ul></div></li><li class="listitem">Next, we will annotate high-confidence tweets irrespective of category, as shown in the following screenshot:<div class="mediaobject"><img alt="How to do it…" src="graphics/4672OS_03_04.jpg"/><div class="caption"><p>Corrected output for active learning output. Note the dominance of the o category.</p></div></div></li><li class="listitem">Annotating down to line 19, we will notice that most of the tweets are <code class="literal">o</code> and are dominating the process. There are only three <code class="literal">p</code> and no <code class="literal">n</code>. We need to get some <code class="literal">n</code> annotations.</li><li class="listitem">We can focus on likely candidate <code class="literal">n</code> annotations by selecting the entire sheet, except for the headers, and sorting by column <span class="strong"><strong>B</strong></span> or <code class="literal">GUESS</code>. Scrolling to the <code class="literal">n</code> guesses, we should see the <a class="indexterm" id="id346"/>highest confidence examples. In the following screenshot, we have annotated all the <code class="literal">n</code> guesses because the category needs data. Our annotations are in <code class="literal">data/activeLearningCompleted/disneySentimentDedupe.1.csv</code>. If you want to exactly duplicate the recipe, you will have to copy this file to the <code class="literal">activeLearning</code> directory.<div class="mediaobject"><img alt="How to do it…" src="graphics/4672OS_03_05.jpg"/><div class="caption"><p>Annotations sorted by category with very few n or negative categories.</p></div></div></li><li class="listitem">Scrolling to the <code class="literal">p</code> <a class="indexterm" id="id347"/>guesses, we annotated a bunch as well.<div class="mediaobject"><img alt="How to do it…" src="graphics/4672OS_03_06.jpg"/><div class="caption"><p>Positive labels with corrections and surprising number of negatives</p></div></div></li><li class="listitem">There are eight negative cases that we found in the <code class="literal">p</code> guesses mixed in with lots of <code class="literal">p</code> and some <code class="literal">o</code> annotations.</li><li class="listitem">We will  save the file without changing the filename and run the same program as we did earlier:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar: com.lingpipe.cookbook.chapter3.ActiveLearner </strong></span>
</pre></div></li><li class="listitem">The output will <a class="indexterm" id="id348"/>be as follows:<div class="informalexample"><pre class="programlisting">First file: data/activeLearning2/disneySentimentDedupe.0.csv
Reading from file data/activeLearning2/disneySentimentDedupe.1.csv
reference\response
          \p,n,o,
         p 17,1,20,
         n 9,1,5,
         o 9,1,51,
Category p
Recall: 0.45
Prec  : 0.49
Category n
Recall: 0.07
Prec  : 0.33
Category o
Recall: 0.84
Prec  : 0.67
Corpus is: 114
Writing to file: data/activeLearning2/disneySentimentDedupe.2.csv
Done, now go annotate and save with same file name</pre></div></li><li class="listitem">This is a fairly typical output early in the annotation process. Positive <code class="literal">p</code>, the easy category, is dragging along at 49-percent precision and 45-percent recall. Negative <code class="literal">n</code> is even worse. Undaunted, we will do another round of annotation on the output file indicating focus on <code class="literal">n</code> guesses to help that category improve performance. We will save and rerun the file:<div class="informalexample"><pre class="programlisting">First file:  data/activeLearning2/disneySentimentDedupe.0.csv
Reading from file data/activeLearning2/disneySentimentDedupe.2.csv
reference\response
          \p,n,o,
         p 45,8,17,
         n 16,13,3,
         o 18,3,88,
Category p
Recall: 0.64
Prec  : 0.57
Category n
Recall: 0.41
Prec  : 0.54
Category o
Recall: 0.81
Prec  : 0.81</pre></div></li><li class="listitem">This last round of annotation got us over the edge (remember to copy over our annotation from <code class="literal">activeLearningCompleted/disneySentimentDedupe.2.csv</code> if you are mirroring the recipe exactly). We annotated high-confidence examples from both <code class="literal">p</code> and <code class="literal">n</code>, adding nearly 100 examples. The first best annotation for <code class="literal">n</code> is above 50-percent precision with 41-percent recall. We assume that there will be a tunable threshold that meets our 80-percent requirement for <code class="literal">p</code> and declares victory in 211 moves, which is much less than the total 1,343 annotations.</li><li class="listitem">That's it. This is a real-world example and the first example we have tried for the book, so the data is not <a class="indexterm" id="id349"/>cooked. The approach tends to work, although no promises; some data resists even the most focused efforts of a well-equipped computational linguist.</li><li class="listitem">Be sure to store the final <code class="literal">.csv</code> file some place safe. It would be a shame to lose all that directed annotation.</li><li class="listitem">Before releasing this classifier we would want to run the classifier, which trains on all annotated data, on new text to verify performance and set thresholds. This annotation process introduces biases over the data that will not be reflected in the real world. In particular, we have biased annotation for <code class="literal">n</code> and <code class="literal">p</code> and added <code class="literal">o</code> as we saw them. This is not the actual distribution.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec98"/>How it works...</h2></div></div></div><p>This recipe has some <a class="indexterm" id="id350"/>subtlety because of the simultaneous evaluation and creation of ranked output for annotation. The code starts with constructs that should be familiar to you:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException {
  String fileName = args.length &gt; 0 ? args[0] 
    : "data/activeLearning/disneySentimentDedupe.0.csv"; 
  System.out.println("First file:  " + fileName);
  String latestFile = getLatestEpochFile(fileName);</pre></div><p>The <code class="literal">getLatestEpochFile</code> method looks for the highest numbered file that ends with <code class="literal">csv</code>, shares the root with the filename, and returns it. On no account will we use this routine for anything serious. The method is standard Java, so we won't cover it.</p><p>Once we have the latest file, we will do some reporting, read it in our standard <code class="literal">.csv </code>annotated files, and load a cross-validating corpus. All these routines are explained elsewhere in locations specified in the <code class="literal">Util</code> source. Finally, we will get the categories that were found in the <code class="literal">.csv</code> annotated file:</p><div class="informalexample"><pre class="programlisting">List&lt;String[]&gt; data 
  = Util.readCsvRemoveHeader(new File(latestFile));
int numFolds = 10;
XValidatingObjectCorpus&lt;Classified&lt;CharSequence&gt;&gt; corpus 
  = Util.loadXValCorpus(data,numFolds);
String[] categories = Util.getCategoryArray(corpus);</pre></div><p>Next, we will configure some <a class="indexterm" id="id351"/>standard logistic-regression-training parameters and create the evaluator for cross-fold evaluation. Note that the Boolean for <code class="literal">storeInputs</code> is <code class="literal">true</code>, which will facilitate recording results. The <span class="emphasis"><em>How to train and evaluate with cross validation</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, has a complete explanation:</p><div class="informalexample"><pre class="programlisting">PrintWriter progressWriter = new PrintWriter(System.out,true);
boolean storeInputs = true;
ConditionalClassifierEvaluator&lt;CharSequence&gt; evaluator 
  = new ConditionalClassifierEvaluator&lt;CharSequence&gt;(null, categories, storeInputs);
TokenizerFactory tokFactory 
  = IndoEuropeanTokenizerFactory.INSTANCE;</pre></div><p>Then, we will execute standard cross validation:</p><div class="informalexample"><pre class="programlisting">for (int i = 0; i &lt; numFolds; ++i) {
  corpus.setFold(i);
  final LogisticRegressionClassifier&lt;CharSequence&gt; classifier 
    = Util.trainLogReg(corpus,tokFactory, progressWriter);
  evaluator.setClassifier(classifier);
  corpus.visitTest(evaluator);
}</pre></div><p>At the end of cross validation, the evaluator has all the classifications stored in <code class="literal">visitTest()</code>. Next, we will transfer this data to an accumulator, which creates and stores rows that will be put into the output spreadsheet and redundantly stores the score; this score will be used in a sort to control the order of annotations printed out:</p><div class="informalexample"><pre class="programlisting">final ObjectToDoubleMap&lt;String[]&gt; accumulator 
  = new ObjectToDoubleMap&lt;String[]&gt;();</pre></div><p>Then, we will iterate over each category and create a list of the false negatives and true positives for the category—these are the cases that the truth category is the category label:</p><div class="informalexample"><pre class="programlisting">for (String category : categories) {
List&lt;Classified&lt;CharSequence&gt;&gt; inCategory
   = evaluator.truePositives(category);    
inCategory.addAll(evaluator.falseNegatives(category));</pre></div><p>Next, all the in-category test cases are used to create rows for the accumulator:</p><div class="informalexample"><pre class="programlisting">for (Classified&lt;CharSequence&gt; testCase : inCategory) {
   CharSequence text = testCase.getObject();
  ConditionalClassification classification 
    = (ConditionalClassification)                  testCase.getClassification();
  double score = classification.conditionalProbability(0);
  String[] xFoldRow = new String[Util.TEXT_OFFSET + 1];
  xFoldRow[Util.SCORE] = String.valueOf(score);
  xFoldRow[Util.GUESSED_CLASS] = classification.bestCategory();
  xFoldRow[Util.ANNOTATION_OFFSET] = category;
  xFoldRow[Util.TEXT_OFFSET] = text.toString();
  accumulator.set(xFoldRow,score);
}</pre></div><p>Next, the code will print out <a class="indexterm" id="id352"/>some standard evaluator output:</p><div class="informalexample"><pre class="programlisting">Util.printConfusionMatrix(evaluator.confusionMatrix());
Util.printPrecRecall(evaluator);  </pre></div><p>All the mentioned steps only apply to annotated data. We will now turn to getting best category and scores for all the unannotated data in the <code class="literal">.csv</code> file.</p><p>First, we will set the number of folds on the cross-validating corpus to <code class="literal">0</code>, which means that <code class="literal">vistTrain()</code> will visit the entire corpus of annotations—unannotated data is not contained in the corpus. The logistic regression classifier is trained in the usual way:</p><div class="informalexample"><pre class="programlisting">corpus.setNumFolds(0);
final LogisticRegressionClassifier&lt;CharSequence&gt; classifier
  = Util.trainLogReg(corpus,tokFactory,progressWriter);</pre></div><p>Armed with a classifier, the code iterates over all the <code class="literal">data</code> items, one row at a time. The first step is to check for an annotation. If the value is not the empty string, then the data was in the aforementioned corpus and used as training data so that the loop skips to the next row:</p><div class="informalexample"><pre class="programlisting">for (String[] csvData : data) {
   if (!csvData[Util.ANNOTATION_OFFSET].equals("")) {
    continue;
   }
   ScoredClassification classification = classifier.classify(csvData[Util.TEXT_OFFSET]);
   csvData[Util.GUESSED_CLASS] = classification.category(0);
   double estimate = classification.score(0);
   csvData[Util.SCORE] = String.valueOf(estimate);
   accumulator.set(csvData,estimate);
  }</pre></div><p>If the row is unannotated, then the score and <code class="literal">bestCategory()</code> method is added at the appropriate points, and the row is added to the accumulator with the score.</p><p>The rest of the code <a class="indexterm" id="id353"/>increments the index of the filename and writes out the accumulator data with a bit of reporting:</p><div class="informalexample"><pre class="programlisting">String outfile = incrementFileName(latestFile);
Util.writeCsvAddHeader(accumulator.keysOrderedByValueList(), 
        new File(outfile));    
System.out.println("Corpus size: " + corpus.size());
System.out.println("Writing to file: " + outfile);
System.out.println("Done, now go annotate and save with same" 
          + " file name");</pre></div><p>This is how it works. Remember that the biases that can be introduced by this approach invalidate evaluation numbers. Always run on fresh held-out data to get a proper sense of the classifier's performance.</p></div></div>
<div class="section" title="Annotation"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec44"/>Annotation</h1></div></div></div><p>One of the most <a class="indexterm" id="id354"/>valuable services we provide is teaching our customers how to create gold-standard data, also known as training data. Nearly every successful-driven NLP project we have done has involved a good deal of customer-driven annotation. The quality of the NLP is entirely dependent on the quality of the training data. Creating training data is a fairly straightforward process, but it requires attention to detail and significant resources. From a budget perspective, you can expect to spend as much as the development team on annotation, if not more.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec99"/>How to do it...</h2></div></div></div><p>We will use sentiment over tweets as our example, and we will assume a business context, but even academic efforts will have similar dimensions.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Get 10 examples of what you expect the system to do. For our example, this means getting 10 tweets that reflect the scope of what the system is expected to do.</li><li class="listitem">Make some effort to pick from the range of what you expect as inputs/outputs. Feel free to cherry-pick strong examples, but do not make up examples. Humans are terrible at creating example data. Seriously, don't do it.</li><li class="listitem">Annotate these tweets for the expected categories.</li><li class="listitem">Have a meeting with all the stakeholders in the annotation. This includes user-experience designers, business folks, developers, and end users. The goal of this meeting is to expose all the relevant parties to what the system will actually do—the system will take the 10 examples and produce the category label. You will be amazed at how much clarity this step establishes. Here are the kinds of clarity:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Upstream/downstream users of the classifier will have a clear idea of what they are expected to produce or consume. For example, the system consumes UTF-8-encoded English tweets and produces an ASCII single character of <code class="literal">p</code>, <code class="literal">n</code>, or <code class="literal">u</code>.</li><li class="listitem" style="list-style-type: disc">For a sentiment, people tend to want a severity score, which is very hard to get. You <a class="indexterm" id="id355"/>can expect annotation costs to double at least. Is it worth it? A score of confidence can be provided, but that is confidence that the category is correct <span class="emphasis"><em>not</em></span> the severity of the sentiment. This meeting will force the discussion.</li><li class="listitem" style="list-style-type: disc">During this meeting explain that each category will likely need at least 100 examples, if not 500, to do a reasonable job. Also explain that switching domains might require new annotations. NLP is extremely easy for your human colleagues, and as a result, they tend to underestimate what it takes to build systems.</li><li class="listitem" style="list-style-type: disc">Don't neglect to include whoever is paying for all this. I suppose you should not have your parents involved if this is your undergraduate thesis.</li></ul></div></li><li class="listitem">Write down an annotation standard that explains the intention behind each category. It doesn't need to be very complex, but it needs to exist. The annotation standard should be circulated around the stakeholders. Bonus points if you have one at the mentioned meetings; if so, it will likely be different at the end, but this is fine. An example is:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A tweet is positive <code class="literal">p</code> if the sentiment is unambiguously positive about Disney. A positive sentiment that applies to a non-Disney tweet is not <code class="literal">p</code> but <code class="literal">u</code>. An example is the <code class="literal">n</code> tweet indicates clearly negative intent towards Disney. Examples include that all other tweets are <code class="literal">u</code>.</li><li class="listitem" style="list-style-type: disc">Examples in the annotation standard do the best job of communicating the intent. Humans do a better job with examples rather than descriptions in our experience.</li></ul></div></li><li class="listitem">Create your collection of unannotated data. The best practice here is for the data to be random from the expected source. This works fine for categories with noticeable prevalence in data, say 10 percent or more, but we have built classifiers that occur at a rate of 1/2,000,000 for question-answering systems. For rare categories, you can use a search engine to help find instances of the category—for example, search for <code class="literal">luv</code> to find positive tweets. Alternatively, you can use a classifier trained on a few examples, run it on data, and look at the high-scoring positives—we covered this in the previous recipe.</li><li class="listitem">Recruit at least two annotators to annotate data. The reason we need at least two is that the task has to be shown to be reproducible by humans. If people can't reliably do the task, then you can't expect a computer to do it. This is where we execute some code. Type in the following command in the command line or invoke <a class="indexterm" id="id356"/>your annotators in you IDE—this will run with our default files:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter3.InterAnnotatorAgreement</strong></span>
</pre></div><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>data/disney_e_n.csv treated as truth </strong></span>
<span class="strong"><strong>data/disney1_e_n.csv treated as response</strong></span>
<span class="strong"><strong>Disagreement: n x e for: When all else fails #Disney</strong></span>
<span class="strong"><strong>Disagreement: e x n for: </strong></span><span class="strong"><strong>昨日の幸せな気持ちのまま今日は</strong></span><span class="strong"><strong>LAND</strong></span><span class="strong"><strong>にいっ</strong></span>
<span class="strong"><strong>reference\response</strong></span>
<span class="strong"><strong>          \e,n,</strong></span>
<span class="strong"><strong>         e 10,1,</strong></span>
<span class="strong"><strong>         n 1,9, </strong></span>
<span class="strong"><strong>Category: e Precision: 0.91, Recall: 0.91 </strong></span>
<span class="strong"><strong>Category: n Precision: 0.90, Recall: 0.90</strong></span>
</pre></div></li><li class="listitem">The code reports disagreements and prints out a confusion matrix. Precision and recall are useful metrics as well.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec100"/>How it works…</h2></div></div></div><p>There is little novel data in the code in <code class="literal">src/com/lingpipe/cookbook/chapter3/InterAnnotatorAgreement.java</code>. One slight twist is that we used <code class="literal">BaseClassifierEvaluator</code> to do the evaluation work without a classifier ever being specified—the creation is as follows:</p><div class="informalexample"><pre class="programlisting">BaseClassifierEvaluator&lt;CharSequence&gt; evaluator 
  = new BaseClassifierEvaluator&lt;CharSequence&gt;(null, 
                categories, storeInputs);</pre></div><p>The evaluator is populated with classifications directly rather than the usual <code class="literal">Corpus.visitTest()</code> method, as done elsewhere in the book:</p><div class="informalexample"><pre class="programlisting">evaluator.addClassification(truthCategory, 
          responseClassification, text);</pre></div><p>If the recipe requires further <a class="indexterm" id="id357"/>explanation, consult the <span class="emphasis"><em>Evaluation of classifiers—the confusion matrix</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec101"/>There's more…</h2></div></div></div><p>Annotation is a very complex area that deserves its own book, and fortunately, there is a good one, <span class="emphasis"><em>Natural Language Annotation for Machine Learning</em></span>, <span class="emphasis"><em>James Pustejovsky and Amber Stubbs</em></span>, <span class="emphasis"><em>O'Reilly Media</em></span>. To get annotations done, there is Amazon's Mechanical Turk service as well as companies that specialize in the creation of training data such as CrowdFlower. However, be careful of outsourcing because classifiers are very dependent on the quality of data.</p><p>Conflict resolution between annotators is a challenging area. Many errors will be due to attention lapses, but some will persist as legitimate areas of disagreement. Two easy resolution strategies are either to throw out the data or keep both annotations.</p></div></div></body></html>