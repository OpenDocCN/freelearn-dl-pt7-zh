["```py\nimport math\nfrom time import time\nfrom enum import Enum\n```", "```py\nclass TradingPosition(object):\n   ''' Class that manages the trading position of our platform'''\n\n    def __init__(self, code, buy_price, amount, next_price):\n        self.code = code ## Status code for what action our algorithm is taking\n        self.amount = amount ## The amount of the trade\n        self.buy_price = buy_price ## The purchase price of a trade\n        self.current_price = buy_price ## Buy price of the trade\n        self.current_value = self.current_price * self.amount\n        self.pro_value = next_price * self.amount\n```", "```py\ndef TradeStatus(self, current_price, next_price, amount):\n        ''' Manages the status of a trade that is in action '''\n        self.current_price = current_price ## updates the current price variable that is maintained within the class\n        self.current_value = self.current_price * amount\n        pro_value = next_price * amount\n```", "```py\ndef BuyStock(self, buy_price, amount, next_price):\n ''' Function to buy a stock '''\n     self.buy_price = ((self.amount * self.buy_price) + (amount * buy_price)) / (self.amount + amount)\n self.amount += amount\n     self.TradeStatus(buy_price, next_price)\n```", "```py\ndef SellStock(self, sell_price, amount, next_price):\n''' Function to sell a stock '''\n     self.current_price = sell_price\n     self.amount -= amount\n     self.TradeStatus(sell_price, next_price)\n```", "```py\n def HoldStock(self, current_price, next_price):\n ''' Function to hold a stock '''\n     self.TradeStatus(current_price, next_price)\n```", "```py\nclass Trader(object): ''' An Artificial Trading Agent '''     def __init__(self, market, cash=100000000.0):         ## Initialize all the variables we need for our trader         self.cash = cash ## Our Cash Variable         self.market = market ##         self.codes = market.codes         self.reward = 0         self.positions = []         self.action_times = 0         self.initial_cash = cash         self.max_cash = cash * 3 self.total_rewards ...\n```", "```py\nclass MarketHandler(object):\n ''' Class for handling our platform's interaction with market data'''\n     Running = 0\n     Done = -1\n\n     def __init__(self, codes, start_date=\"2008-01-01\", end_date=\"2018-05-31\", **options):\n         self.codes = codes\n         self.index_codes = []\n         self.state_codes = []\n         self.dates = []\n         self.t_dates = []\n         self.e_dates = []\n         self.origin_frames = dict()\n         self.scaled_frames = dict()\n         self.data_x = None\n         self.data_y = None\n         self.seq_data_x = None\n         self.seq_data_y = None\n         self.next_date = None\n         self.iter_dates = None\n         self.current_date = None\n\n         ## Initialize the stock data that will be fed in \n         self._init_data(start_date, end_date)\n\n         self.state_codes = self.codes + self.index_codes\n         self.scaler = [scaler() for _ in self.state_codes]\n         self.trader = Trader(self, cash=self.init_cash)\n         self.doc_class = Stock if self.m_type == 'stock' else Future\n```", "```py\ndef _init_data_frames(self, start_date, end_date):\n     self._validate_codes()\n     columns, dates_set = ['open', 'high', 'low', 'close', 'volume'], set()\n     ## Load the actual data\n     for index, code in enumerate(self.state_codes):\n         instrument_docs = self.doc_class.get_k_data(code, start_date, end_date)\n         instrument_dicts = [instrument.to_dic() for instrument in instrument_docs]\n         dates = [instrument[1] for instrument in instrument_dicts]\n         instruments = [instrument[2:] for instrument in instrument_dicts]\n         dates_set = dates_set.union(dates)\n         scaler = self.scaler[index]\n         scaler.fit(instruments)\n         instruments_scaled = scaler.transform(instruments)\n         origin_frame = pd.DataFrame(data=instruments, index=dates, columns=columns)\n         scaled_frame = pd.DataFrame(data=instruments_scaled, index=dates, columns=columns)\n         self.origin_frames[code] = origin_frame\n self.scaled_frames[code] = scaled_frame\n         self.dates = sorted(list(dates_set))\n    for code in self.state_codes:\n         origin_frame = self.origin_frames[code]\n         scaled_frame = self.scaled_frames[code]\n         self.origin_frames[code] = origin_frame.reindex(self.dates, method='bfill')\n         self.scaled_frames[code] = scaled_frame.reindex(self.dates, method='bfill')\n```", "```py\ndef _init_env_data(self):\n     if not self.use_sequence:\n         self._init_series_data()\n     else:\n         self._init_sequence_data()\n```", "```py\nself._init_data_frames(start_date, end_date)\n```", "```py\nimport tensorflow as tffrom sklearn.preprocessing import MinMaxScalerimport loggingimport os\n```", "```py\nclass TradingRNN(): ''' An RNN Model for ...\n```", "```py\nts = TimeSeries(key='YOUR_API_KEY', output_format='pandas')\ndata, meta_data = ts.get_intraday(symbol='TICKER',interval='1min', outputsize='full')\n```", "```py\nfrom collections import Counter, defaultdictimport osfrom random import shuffleimport tensorflow as tfimport nltk\nclass GloVeModel(): def __init__(self, embedding_size, window_size, max_vocab_size=100000, min_occurrences=1, scaling_factor=3/4, cooccurrence_cap=100, batch_size=512, learning_rate=0.05): self.embedding_size = embedding_size#First we define the hyper-parameters of our model if isinstance(context_size, tuple): self.left_context, self.right_context = context_size elif isinstance(context_size, int): self.left_context = self.right_context = context_size   self.max_vocab_size = max_vocab_size self.min_occurrences ...\n```", "```py\ndef NTN(batch_placeholders, corrupt_placeholder, init_word_embeds,     entity_to_wordvec,\\\n num_entities, num_relations, slice_size, batch_size, is_eval, label_placeholders):\n     d = 100 \n     k = slice_size\n     ten_k = tf.constant([k])\n     num_words = len(init_word_embeds)\n     E = tf.Variable(init_word_embeds) \n     W = [tf.Variable(tf.truncated_normal([d,d,k])) for r in range(num_relations)]\n     V = [tf.Variable(tf.zeros([k, 2*d])) for r in range(num_relations)]\n     b = [tf.Variable(tf.zeros([k, 1])) for r in range(num_relations)]\n     U = [tf.Variable(tf.ones([1, k])) for r in range(num_relations)]\n\n     ent2word = [tf.constant(entity_i)-1 for entity_i in entity_to_wordvec]\n     entEmbed = tf.pack([tf.reduce_mean(tf.gather(E, entword), 0) for entword in ent2word])\n```", "```py\npredictions = list()\nfor r in range(num_relations):\n     e1, e2, e3 = tf.split(1, 3, tf.cast(batch_placeholders[r], tf.int32)) #TODO: should the split dimension be 0 or 1?\n     e1v = tf.transpose(tf.squeeze(tf.gather(entEmbed, e1, name='e1v'+str(r)),[1]))\n     e2v = tf.transpose(tf.squeeze(tf.gather(entEmbed, e2, name='e2v'+str(r)),[1]))\n     e3v = tf.transpose(tf.squeeze(tf.gather(entEmbed, e3, name='e3v'+str(r)),[1]))\n     e1v_pos = e1v\n     e2v_pos = e2v\n     e1v_neg = e1v\n     e2v_neg = e3v\n     num_rel_r = tf.expand_dims(tf.shape(e1v_pos)[1], 0)\n     preactivation_pos = list()\n     preactivation_neg = list()\n```", "```py\nfor slice in range(k):\n     preactivation_pos.append(tf.reduce_sum(e1v_pos*tf.matmul(W[r][:,:,slice], e2v_pos), 0))\n     preactivation_neg.append(tf.reduce_sum(e1v_neg*tf.matmul( W[r][:,:,slice], e2v_neg), 0))\n\npreactivation_pos = tf.pack(preactivation_pos)\npreactivation_neg = tf.pack(preactivation_neg)\n\ntemp2_pos = tf.matmul(V[r], tf.concat(0, [e1v_pos, e2v_pos]))\ntemp2_neg = tf.matmul(V[r], tf.concat(0, [e1v_neg, e2v_neg]))\n\npreactivation_pos = preactivation_pos+temp2_pos+b[r]\npreactivation_neg = preactivation_neg+temp2_neg+b[r]\n\nactivation_pos = tf.tanh(preactivation_pos)\nactivation_neg = tf.tanh(preactivation_neg)\n\nscore_pos = tf.reshape(tf.matmul(U[r], activation_pos), num_rel_r)\nscore_neg = tf.reshape(tf.matmul(U[r], activation_neg), num_rel_r)\nif not is_eval:\n    predictions.append(tf.pack([score_pos, score_neg]))\nelse:\n    predictions.append(tf.pack([score_pos,             tf.reshape(label_placeholders[r], num_rel_r)]))\n```", "```py\npredictions = tf.concat(1, predictions)\n\nreturn predictions\n```", "```py\ndef loss(predictions, regularization):\n     temp1 = tf.maximum(tf.sub(predictions[1, :], predictions[0, :]) + 1, 0)\n     temp1 = tf.reduce_sum(temp1)\n     temp2 = tf.sqrt(sum([tf.reduce_sum(tf.square(var)) for var in     tf.trainable_variables()]))\n     temp = temp1 + (regularization * temp2)\n     return temp\n```", "```py\ndef training(loss, learningRate):\n    return tf.train.AdagradOptimizer(learningRate).minimize(loss)\n```", "```py\ndef eval(predictions):\n     print(\"predictions \"+str(predictions.get_shape()))\n     inference, labels = tf.split(0, 2, predictions)\n     return inference, labels\n```", "```py\nimport numpy as np\nimport tensorflow as tf from tensorflow.contrib.rnn import LSTMCell\n```", "```py\nibb = defaultdict(defaultdict)\nibb_full = pd.read_csv('data/ibb.csv', index_col=0).astype('float32')\n\nibb_lp = ibb_full.iloc[:,0] \nibb['calibrate']['lp'] = ibb_lp[0:104]\nibb['validate']['lp'] = ibb_lp[104:]\n\nibb_net = ibb_full.iloc[:,1] \nibb['calibrate']['net'] = ibb_net[0:104]\nibb['validate']['net'] = ibb_net[104:]\n\nibb_percentage = ibb_full.iloc[:,2] \nibb['calibrate']['percentage'] = ibb_percentage[0:104]\nibb['validate']['percentage'] = ibb_percentage[104:]\n```", "```py\nclass AutoEncoder():\n    ''' AutoEncoder for Data Drive Portfolio Allocation '''\n    def __init__(self, config):\n        \"\"\"First, let's set up our hyperparameters\"\"\"\n        num_layers = tf.placeholder('int')\n        hidden_size = tf.placeholder('int')\n        max_grad_norm = tf.placeholder('int')\n        batch_size = tf.placeholder('int')\n        crd = tf.placeholder('int')\n        num_l = tf.placeholder('int')\n        learning_rate = tf.placeholder('float')\n        self.batch_size = batch_size\n\n        ## sl will represent the length of an input sequence, which we would like to eb dynamic based on the data \n        sl = tf.placeholder(\"int\")\n        self.sl = sl\n```", "```py\nself.x = tf.placeholder(\"float\", shape=[batch_size, sl], name='Input_data')\nself.x_exp = tf.expand_dims(self.x, 1)\nself.keep_prob = tf.placeholder(\"float\")\n```", "```py\n## Create the Encoder as a TensorFlow Scope\nwith tf.variable_scope(\"Encoder\") as scope:\n     ## For the encoder, we will use an LSTM cell with Dropout\n     EncoderCell = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])\n     EncoderCell = tf.contrib.rnn.DropoutWrapper(EncoderCell, output_keep_prob=self.keep_prob)\n\n     ## Set the initial hidden state of the encoder\n     EncInitialState = EncoderCell.zero_state(batch_size, tf.float32)\n\n     ## Weights Factor\n     W_mu = tf.get_variable('W_mu', [hidden_size, num_l])\n\n     ## Outputs of the Encoder Layer\n     outputs_enc, _ = tf.contrib.rnn.static_rnn(cell_enc,\n     inputs=tf.unstack(self.x_exp, axis=2),\n     initial_state=initial_state_enc)\n     cell_output = outputs_enc[-1]\n\n     ## Bias Factor\n     b_mu = tf.get_variable('b_mu', [num_l])\n\n     ## Mean of the latent space variables\n     self.z_mu = tf.nn.xw_plus_b(cell_output, W_mu, b_mu, name='z_mu') \n\n     lat_mean, lat_var = tf.nn.moments(self.z_mu, axes=[1])\n     self.loss_lat_batch = tf.reduce_mean(tf.square(lat_mean) + lat_var - tf.log(lat_var) - 1)\n```", "```py\n## Layer to Generate the Initial Hidden State from the Encoder\n with tf.name_scope(\"Initial_State\") as scope:\n ## Weights Parameter State\n W_state = tf.get_variable('W_state', [num_l, hidden_size])\n\n ## Bias Paramter State\n b_state = tf.get_variable('b_state', [hidden_size])\n\n ## Hidden State\n z_state = tf.nn.xw_plus_b(self.z_mu, W_state, b_state, name='hidden_state')\n```", "```py\n## Decoder Layer \n with tf.variable_scope(\"Decoder\") as scope:\n\n     DecoderCell = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])\n\n     ## Set an initial state for the decoder layer\n     DecState = tuple([(z_state, z_state)] * num_layers)\n     dec_inputs = [tf.zeros([batch_size, 1])] * sl\n\n     ## Run the decoder layer\n     outputs_dec, _ = tf.contrib.rnn.static_rnn(cell_dec, inputs=dec_inputs, initial_state=DecState)\n```", "```py\n## Output Layer\n with tf.name_scope(\"Output\") as scope:\n     params_o = 2 * crd \n     W_o = tf.get_variable('W_o', [hidden_size, params_o])\n     b_o = tf.get_variable('b_o', [params_o])\n     outputs = tf.concat(outputs_dec, axis=0) \n     h_out = tf.nn.xw_plus_b(outputs, W_o, b_o)\n     h_mu, h_sigma_log = tf.unstack(tf.reshape(h_out, [sl, batch_size, params_o]), axis=2)\n     h_sigma = tf.exp(h_sigma_log)\n     dist = tf.contrib.distributions.Normal(h_mu, h_sigma)\n     px = dist.log_prob(tf.transpose(self.x))\n loss_seq = -px\n self.loss_seq = tf.reduce_mean(loss_seq)\n```", "```py\n## Train the AutoEncoder\n with tf.name_scope(\"Training\") as scope:\n\n     ## Global Step Function for Training\n     global_step = tf.Variable(0, trainable=False)\n\n     ## Exponential Decay for the larning rate\n     lr = tf.train.exponential_decay(learning_rate, global_step, 1000, 0.1, staircase=False)\n\n     ## Loss Function for the Network\n     self.loss = self.loss_seq + self.loss_lat_batch\n\n     ## Utilize gradient clipping to prevent exploding gradients\n     grads = tf.gradients(self.loss, tvars)\n grads, _ = tf.clip_by_global_norm(grads, max_grad_norm)\n     self.numel = tf.constant([[0]])\n\n     ## Lastly, apply the optimization process\n     optimizer = tf.train.AdamOptimizer(lr)\n     gradients = zip(grads, tvars)\n     self.train_step = optimizer.apply_gradients(gradients, global_step=global_step)\n     self.numel = tf.constant([[0]])\n\n```", "```py\nif True:\n     sess.run(model.init_op)\n     writer = tf.summary.FileWriter(LOG_DIR, sess.graph) # writer for Tensorboard\n\n step = 0 # Step is a counter for filling the numpy array perf_collect\n for i in range(max_iterations):\n     batch_ind = np.random.choice(N, batch_size, replace=False)\n     result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.train_step],\n feed_dict={model.x: X_train[batch_ind], model.keep_prob: dropout})\n\n if i % plot_every == 0:\n     perf_collect[0, step] = loss_train = result[0]\n     loss_train_seq, lost_train_lat = result[1], result[2]\n\n batch_ind_val = np.random.choice(Nval, batch_size, replace=False)\n\n result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.merged],\n feed_dict={model.x: X_val[batch_ind_val], model.keep_prob: 1.0})\n perf_collect[1, step] = loss_val = result[0]\n loss_val_seq, lost_val_lat = result[1], result[2]\n summary_str = result[3]\n writer.add_summary(summary_str, i)\n writer.flush()\n\n print(\"At %6s / %6s train (%5.3f, %5.3f, %5.3f), val (%5.3f, %5.3f,%5.3f) in order (total, seq, lat)\" % (\n i, max_iterations, loss_train, loss_train_seq, lost_train_lat, loss_val, loss_val_seq, lost_val_lat))\n step += 1\nif False:\n\n start = 0\n label = [] # The label to save to visualize the latent space\n z_run = []\n\n while start + batch_size < Nval:\n run_ind = range(start, start + batch_size)\n z_mu_fetch = sess.run(model.z_mu, feed_dict={model.x: X_val[run_ind], model.keep_prob: 1.0})\n z_run.append(z_mu_fetch)\n start += batch_size\n\n z_run = np.concatenate(z_run, axis=0)\n label = y_val[:start]\n\n plot_z_run(z_run, label)\n\nsaver = tf.train.Saver()\nsaver.save(sess, os.path.join(LOG_DIR, \"model.ckpt\"), step)\nconfig = projector.ProjectorConfig()\n\nembedding = config.embeddings.add()\nembedding.tensor_name = model.z_mu.name\n```", "```py\ncommunal_information = []\n\nfor i in range(0,83):\n    diff = np.linalg.norm((data.iloc[:,i] - reconstruct[:,i])) # 2 norm difference\n    communal_information.append(float(diff))\n\nprint(\"stock #, 2-norm, stock name\")\nranking = np.array(communal_information).argsort()\nfor stock_index in ranking:\n    print(stock_index, communal_information[stock_index], stock['calibrate']['net'].iloc[:,stock_index].name) # print stock name from lowest different to highest\n```", "```py\nwhich_stock = 1\n\nstock_autoencoder = copy.deepcopy(reconstruct[:, which_stock])\nstock_autoencoder[0] = 0\nstock_autoencoder = stock_autoencoder.cumsum()\nstock_autoencoder += (stock['calibrate']['lp'].iloc[0, which_stock])\n\npd.Series(stock['calibrate']['lp'].iloc[:, which_stock].as_matrix(), index=pd.date_range(start='01/06/2012', periods=104, freq='W')).plot(label='stock original', legend=True)\npd.Series(stock_autoencoder, index=pd.date_range(start='01/06/2012', periods = 104,freq='W')).plot(label='stock autoencoded', legend=True)\n```"]