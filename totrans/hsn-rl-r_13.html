<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">TD Learning in Healthcare</h1>
                </header>
            
            <article>
                
<p class="mce-root">Artificial intelligence and medicine—two worlds that have decidedly distant origins, but which have already started a close relationship of collaboration, with the fundamental objective of improving the health and life expectancy of the world population. This is a relationship that is destined to be further strengthened over the next few decades, especially since the challenges facing medicine in the 21<sup>st</sup> century are extremely demanding. In this chapter, we will address some of these problems with reinforcement learning. <span>We will learn how to use reinforcement learning in healthcare. Then, we will learn how to model healthcare insurance as a Markov decision process. Finally, we will understand how to plan sanitation in operating rooms using TD learning.</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Introducing reinforcement learning in healthcare</li>
<li>Modeling healthcare insurance plans</li>
<li>Using the transition model in health insurance</li>
<li>Operating room sanitation scheduling</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2PE4NHe">http://bit.ly/2PE4NHe</a></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing reinforcement learning in healthcare</h1>
                </header>
            
            <article>
                
<p>In recent years, enormous attention to artificial intelligence has been paid by the healthcare world—in fact, we have observed the collaboration between Google's DeepMind project and the United Kingdom's National Health Service, as well as IBM's continuous investments in the areas of genomics and drug discovery through the assistance of articular intelligence.</p>
<p>Many ask themselves what concrete improvements can be brought by the innovations introduced by machine learning in healthcare to patients' lives. In the span of 5-10 years, it is believed that healthcare will be totally revolutionized. Will executives, doctors, nurses, and others be prepared for this new challenge?</p>
<p>Numerous experts predict that, between 2025 and 2030, the world of health will be invaded by new technologies based on artificial intelligence and machine learning. This time frame, in reality, coincides with many other forecasts that indicate how the use of digital intelligence will influence many other fields of culture and human activities.</p>
<p>At this moment, in the western world, there are many companies actively involved in the development of machine learning systems aimed specifically at healthcare uses. In addition to these established companies, we also have a certain number of university start-ups and spin-offs that, almost daily, turn their attention to the world of medicine.</p>
<p>We'll take a look at some of the possible uses of machine learning <span>in the field of healthcare in the following sections</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Diagnosis of diseases</h1>
                </header>
            
            <article>
                
<p>Diagnosing a disease is a difficult process and requires a great deal of experience. It serves to recognize a disease or psychopathology based on symptoms or phenomena, the former being subjective manifestations present in the patient, and the latter also being evident to the doctor or psychologist. The set of symptoms and signs of which some are specific or pathognomonic and others are more or less generic characterize the clinical picture of a disease. The set of diagnostic methods is called diagnostics. Diagnostics is known to be instrumental when it uses special equipment or instruments, such as in imaging or clinical diagnostics, when it is based on the direct examination of the patient by the doctor.</p>
<p>Information technology offers great support for diagnosis. Various centers around the world make use of the space and computing power of the most powerful supercomputers in the world to obtain fast and accurate diagnoses, which draw and process information starting from the enormous knowledge base available in search engines and internet databases, including the same anonymous electronic patient records.</p>
<p>In recent years, a large contribution has been made by machine-based algorithms. Through these algorithms, the identification and classification of a pathology are less affected by errors. In this way, the early diagnosis of a disorder can make treatment much more effective.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prediction of epidemic events</h1>
                </header>
            
            <article>
                
<p>The term epidemic means the onset of a disease that rapidly spreads by direct or indirect infection, to the point of affecting a large number of people in a more or less vast territory, and is extinguished after a more or less long duration. This definition generally applies to the case of infectious diseases, although currently there is a tendency to transfer this terminology to the field of non-infectious diseases, especially since their frequency in certain populations has undergone a clear and unexpected increase, coinciding with quantitative variations and qualitative of causal factors.</p>
<p>The spread of diseases depends on phenomena characterized by territorial influences and population displacement. Correlating all of this information becomes a burdensome task. For this, we need the aid of artificial intelligence, which can predict how an epidemic can expand by taking into account the place in which it developed. The results of this analysis can help the authorities to prepare pandemic risk containment plans.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experimentation with new drugs</h1>
                </header>
            
            <article>
                
<p>Machine learning has a very important utility with regards to the identification of new drugs, starting from their study of the composition up to predicting their effects.</p>
<p>The Royal Society, which is based in the United Kingdom, states that machine learning is an optimal solution in the production of drugs through biological methods. In this way, pharmaceutical companies are important helpers in the drug production process, especially, with regards to timing and lowering production costs.</p>
<p>In precision medicine, the MIT Clinical Machine Learning Group uses algorithms to identify the best ways to produce drugs and cures, mainly for diabetes.</p>
<p><span>Another example</span> is given by the Hanover project by Microsoft, which uses machine learning in several cases, especially in technologies for therapies aimed at treating cancer and, more specifically, the identification of personalized treatments for acute myeloid leukemia.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DeepMind Health</h1>
                </header>
            
            <article>
                
<p>DeepMind can process millions of pieces of medical information in just a few minutes, greatly speeding up medical processes that are clinical in nature, such as folder archiving and diagnostics. The researchers at DeepMind are also working on models to emulate the ability to imagine the consequences of an action before undertaking it—practically speaking, they are trying to understand what intelligence and imagination are to convert them into algorithms. Verily, the Google branch of life science, is also working on a project called the <strong>Baseline Study</strong> to collect genetic data. The aim is to adopt some of Google's algorithms to analyze what allows people to be healthy. For this project, researchers also use disease monitoring technologies, such as intelligent contact lenses to measure blood sugar levels.</p>
<p>Now, the time has come to apply reinforcement learning algorithms to practical cases related to health care.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modeling healthcare insurance plans</h1>
                </header>
            
            <article>
                
<p>The term <strong>long term care</strong> (<strong>LTC</strong>) refers to a set of interventions that are necessary to guarantee adequate assistance to<span>, mainly elderly,</span><span> </span><span>individuals in conditions of non-self-sufficiency caused by an accident or a disease, but also only by aging. </span>These interventions, provided by public or private institutions, may fall within the overall health services and/or in the context of the complex of welfare services, both at home and in specific institutions designed to face these risks.</p>
<p>The importance of these forms of insurance, both private and public, is essentially due to the demographic aging process that has affected all industrialized countries. This process is creating strong financial and coverage problems in various sectors of the welfare state. Examples include the pension system, the demand for health services and, more particularly, the demand for those long-term social and health services that the elderly need in the event of total or partial loss of their autonomy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Health insurance basis</h1>
                </header>
            
            <article>
                
<p><strong>Health insurance</strong> means a wide range of insurance coverage that allows an insurer to intervene when the health condition of the insured is affected due to illness or injury, resulting in high medical expenses and inability to generate income, thereby causing economic damage to the insured.</p>
<p>The objective of these forms of insurance is to protect the insured from those risks connected to their health conditions and to their relative working capacity, which, due to the onset of diseases or the occurrence of accidents, can be interrupted for longer or shorter <span>periods</span><span>, temporarily or permanently.</span></p>
<p>Some insurances companies provide financial support for alternative health solutions to those offered by public health systems, such as medical expenses reimbursement insurance; others provide revenue in the form of income or capital, which compensates, at least in part, for the loss of income caused by periods of incapacity for work.</p>
<p>To monetize an insurance contract, the intervention of a specialist is required to study the technical organization of life insurance companies and, in general, the social security institutions  by establishing the bases and checking the results from a statistical, financial, and mathematical<span> </span><span>point of view</span><span>. This figure is called the actuary.</span></p>
<p>The first actuarial calculation models for life insurance are, in fact, dated between the end of the seventeenth century and the first half of the eighteenth century. Over the centuries, different models have been formulated. In the next section, we will analyze a model based on multiple decrements.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing multiple decrement models</h1>
                </header>
            
            <article>
                
<p>The theory of multiple decrements developed from the second half of the <span>eighteenth</span> century. The theory of multiple decrements has posed, in terms of mathematical formulation, an issue that is also relevant to any other type of insurance activity. It first developed in the field of continuous-time models and only later, toward the end of the eighteenth century, in the context of discrete-time models. The latter were mainly taken into consideration at the end of the nineteenth century for the first applications of actuarial calculation to public pension systems.</p>
<p>The calculation of insurance premiums is based on the rule of large numbers that states the proportion of successes in <em>n</em> independent realizations of an event <em>E</em> converges, for what tends to infinity, to the probability that the event <em>E</em> takes place. This means that, when the reference sample is sufficiently numerous and the probabilities of disease are independent of each other, the proportion of individuals who contract the disease approaches the probability of contracting the disease so that this percentage of the population will contract the disease. Thanks to this principle, the insurance company can calculate the expected repayments and increase the homogeneity. The number of the insured also increases the accuracy of the estimate. Therefore, the insurance company does not know who will contract the disease but only the percentage of individuals who will fall ill.</p>
<p>The calculation of premiums must be accepted by both parties, that is, the insured and the insurer. In the simplest situation, it is assumed that the interest rate is constant throughout the active period of the policy and the benefit due to the individual depends on the event of death. In the model based on common collectivity, the service is paid in the same way to each group. In such cases, it is necessary to define when to pay the benefit. Two possible solutions are payment on the first death in the group and payment on the last death in the group. In both cases, the only random variable is represented by the future life expectancy.</p>
<p>Life tables are used to calculate life expectancy. A life table is a table that shows, for each age, what the probability is that a person of that age will die before their next birthday. Therefore, in the life table, there is a single exit mode. This table is defined as a single decrement table. When we expect different causes of decrement on a group of individuals, we can model the system with a more general family of models based on multiple decrements.</p>
<p>In multiple decrement models, there is a simultaneous different causes of decrement. A life ends because of one of these decreases functioning. In the following section, we will calculate the benefit that will be paid to an insured person by modeling the problem as a Markov decision process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a transition model in health insurance</h1>
                </header>
            
            <article>
                
<p>A decrement model can be seen as a special case of multi-state models. In fact, just like what happens in a multi-state model, we can consider an individual of age <em>x</em> who, at time <em>t</em>, is in one of the <em>n + 1</em> potential states. In a multiple decrement model, starting from the state 0 (active), all of the other transition states are defined as absorbent states, which, once reached, can no longer be exited.</p>
<p>In our case, three states are provided: active, unable to work, and dead. The starting point is obviously represented by the active state, while the other two (unable and dead) are absorbent states, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-575 image-border" src="assets/607a8e60-ab4b-46cb-92ee-d0894e9ce96a.png" style="width:22.00em;height:11.75em;"/></p>
<p>We have previously stated that all the transition states except the active state are absorbent. This means that it is no longer possible to return to the active state. A transition from the unable to work state is possible—unfortunately, it's not pleasant as it involves death. Now, let's learn how to set up a decrease table.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the decrement table</h1>
                </header>
            
            <article>
                
<p>To deal with this problem, it is essential to have a decrement table. The decrement table contains the probabilities that the individual passes from one state to another, which sounds like a transition matrix. We analyzed transition matrices in detail in <a href="7ee860fd-cd4c-4034-8dd6-9c803e129418.xhtml">Chapter 3</a>, <em>Markov Decision Processes in Action</em>.</p>
<p>To begin, we will learn how to construct the table of decrements. You may recall that the decrement matrix differs from the life expectancy table in that, in addition to containing the probability of death as a function of the subject's age, it also contains the probabilities of transition in the other predicted states:</p>
<ol>
<li>We will create vectors that contain this information and start from age:</li>
</ol>
<pre style="padding-left: 60px">age&lt;-seq(from = 30,  to = 50, by=1)</pre>
<p style="padding-left: 60px">Here, we used the <kbd>seq()</kbd> function, which generates regular sequences. Three parameters are used, as follows:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><kbd>from</kbd><span>: The starting values of the sequence.</span></li>
<li><kbd>to</kbd><span>: The end values of the sequence.</span></li>
<li><kbd>by</kbd><span>: The increment of the sequence. By default, this is 1.</span></li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">In our case, we could have omitted the <kbd>by</kbd> parameter, but we have inserted it to make the code more readable. We considered only a part of working life, that is,the one that starts from 30 to reach 50. Of course, we could have extended this interval to the whole working life but, for our purposes, it is fine. Now, we can create vectors that contain the probabilities of transition from one state to another. We start from the active state, and we have three transition states: from active to unable, from active to dead, and from active to active.</p>
<ol start="2">
<li>We will use the <kbd>seq()</kbd> function again, as follows:</li>
</ol>
<pre style="padding-left: 60px">ProbA2U&lt;-seq(0.0006,0.0060,length.out = 21)<br/>ProbA2D&lt;-seq(0.0005,0.0020,length.out = 21)<br/>ProbA2A&lt;-1-(ProbA2U+ProbA2D)</pre>
<p style="padding-left: 60px">Once again, we used the <kbd>seq()</kbd> function, but this time, we used a new parameter: <kbd>length.out</kbd>. This parameter was used to set the desired length of the sequence.</p>
<p style="padding-left: 60px">Let's analyze the newly created vectors. The first vector is the probability of transition from the active to unable state and presents increasing values with age. The second vector represents the probability of transition from the active to dead state and presents increasing values with age, once again. Finally, the third vector represents the probability of transition from the active <span>state</span><span> </span><span>to active and was obtained as a difference between 1 and the sum of the previous values. This is because the sum of the three transition probabilities must return 1.</span></p>
<ol start="3">
<li>Now, let's move on to the unable <span>state</span>. Once again, we have three transitions: unable to active, unable to dead, and unable to unable. Let's create the three vectors:</li>
</ol>
<pre style="padding-left: 60px">ProbU2A&lt;-seq(0,0,length.out = 21)<br/>ProbU2D&lt;-seq(0.1219,0.1879,length.out = 21)<br/>ProbU2U&lt;-1-(ProbU2A+ProbU2D)</pre>
<p style="padding-left: 60px">As we can see, the first vector contains only zeros. This is because, as we anticipated, the unable state does not allow transit to the active state; in this sense, it is an absorbing state. The second vector represents the probability of transition from the unable to dead state and presents increasing values with age. Finally, the third  vector represents the probability of transition from the unable <span>state </span>to unable and was obtained as a difference between 1 and the sum of the previous values. This is because the sum of the three transition probabilities must return 1.</p>
<ol start="4">
<li>Finally, let's move on to the dead state. Once again, we have three transitions: dead to active, dead to unable, and dead to dead. Let's create the three vectors:</li>
</ol>
<pre style="padding-left: 60px">ProbD2A&lt;-seq(0,0,length.out = 21)<br/>ProbD2U&lt;-seq(0,0,length.out = 21)<br/>ProbD2D&lt;-1-(ProbD2A+ProbD2U)</pre>
<p style="padding-left: 60px">The dead state is definitely absorbent compared to the other two states. To confirm this, the first two vectors contain zeros, while the third contains ones. This is because the rule that the sum of the probabilities of state transitions must return 1 remains valid.</p>
<ol start="5">
<li>At this point, we just have to use these vectors to create the decrement table, as follows:</li>
</ol>
<pre style="padding-left: 60px">DecrementsTable&lt;- data.frame(age,ProbA2A,ProbA2U,ProbA2D,ProbU2A,ProbU2D,ProbU2U,ProbD2A,ProbD2U,ProbD2A)</pre>
<ol start="6">
<li>Let's take a look at the table we just created; we can use the <kbd>str()</kbd> function, which shows a compact display of the internal structure of an R object:</li>
</ol>
<pre style="padding-left: 60px">str(DecrementsTable)</pre>
<p style="padding-left: 60px">The results are shown in the following code block:</p>
<pre style="padding-left: 60px"><strong>'data.frame':  21 obs. of  10 variables:</strong><br/><strong> $ age    : num  30 31 32 33 34 35 36 37 38 39 ...</strong><br/><strong> $ ProbA2A: num  0.999 0.999 0.998 0.998 0.998 ...</strong><br/><strong> $ ProbA2U: num  0.0006 0.00087 0.00114 0.00141 0.00168 0.00195 0.00222 0.00249 0.00276 0.00303 ...</strong><br/><strong> $ ProbA2D: num  0.0005 0.000575 0.00065 0.000725 0.0008 ...</strong><br/><strong> $ ProbU2A: num  0 0 0 0 0 0 0 0 0 0 ...</strong><br/><strong> $ ProbU2D: num  0.122 0.125 0.128 0.132 0.135 ...</strong><br/><strong> $ ProbU2U: num  0.878 0.875 0.871 0.868 0.865 ...</strong><br/><strong> $ ProbD2A: num  0 0 0 0 0 0 0 0 0 0 ...</strong><br/><strong> $ ProbD2U: num  0 0 0 0 0 0 0 0 0 0 ...</strong><br/><strong> $ ProbD2D: num  1 1 1 1 1 1 1 1 1 1 ...</strong></pre>
<p style="padding-left: 60px">The dataframe contains 21 observations of 10 variables. Each observation represents ages from 30 to 50. The variables are age and nine transition probabilities between the three states: active, unable, and dead. As we can see, the variables contained in the table are identified by their names. To refer to variables with their names in a dataframe, we can interpose a dollar sign ($) between the data frame's name and the variable's name. Furthermore, we can read other information: the type attributed to each variable. In this case, we have 10 numerical type variables.</p>
<ol start="7">
<li>More information can be obtained using the <kbd>summary()</kbd> function, which produces summaries of the distribution of the variables:</li>
</ol>
<pre style="padding-left: 60px">summary(TransMatrix)</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-576 image-border" src="assets/a03c9bb8-04a0-44e4-a33d-1742ae2b8449.png" style="width:35.42em;height:24.58em;"/></p>
<p style="padding-left: 60px">The <kbd>summary()</kbd> function invokes particular methods that depend on the class of the first argument. For each feature, the following descriptors are returned—minimum, first quartile, median, mean third quartile, and maximum. A quick look at these values allows us to understand the statistical distribution of the values assumed by the variable.</p>
<ol start="8">
<li>To understand how this table appears, we can print the head of the decrement table, as follows:</li>
</ol>
<pre style="padding-left: 60px">head(DecrementsTable)</pre>
<p style="padding-left: 60px">The following results are printed:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-577 image-border" src="assets/62682d37-5036-4d5a-b6c5-22ba98b8656d.png" style="width:43.67em;height:8.58em;"/></p>
<p>As we can see, every observation corresponds to a worker's age, and for each age, the probabilities of transition from one state to another are reported. <span>Now, we can set the basics of the model in the next sectio</span>n.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Markov decision process model</h1>
                </header>
            
            <article>
                
<p>Our goal is to treat the problem as a Markov decision process. To do this, we must define the states. Previously, we said that there are three states in which a worker can be found—active, unable to work, and dead. Let's follow these steps:</p>
<ol>
<li>We will create a variable that contains this information:</li>
</ol>
<pre style="padding-left: 60px">WorkesStates&lt;-c("Active","Unable","Dead")</pre>
<p style="padding-left: 60px">A Markovian process is represented by a transition matrix that defines the probabilities of transition from one state to another. We have said that our system is defined by three states, so the transition matrix will be 3x3 in size. The information we need is contained in the decrements table.</p>
<ol start="2">
<li>Then, we will extract the transition matrix at a specific age of the worker:</li>
</ol>
<pre style="padding-left: 60px">TransMatrix35&lt;-matrix(as.numeric(DecrementsTable[DecrementsTable$age==35,2:10]),nrow = 3,ncol = 3, byrow = TRUE, dimnames = list(WorkesStates, WorkesStates))</pre>
<p style="padding-left: 60px">To do this, we simply extracted the final nine columns of the decrement table in correspondence with the observation identified by the value of the age variable equal to 35. We have used the <kbd>matrix</kbd> () function to arrange the extracted data into a 3x3 matrix (<kbd>nrow = 3, ncol = 3</kbd>) and we have fixed the values for progressive lines (<kbd>byrow = TRUE</kbd>). Let's see the result:</p>
<pre style="padding-left: 60px"><strong>       Active   Unable  Dead   </strong><br/><strong>Active 0.997175 0.00195 0.000875</strong><br/><strong>Unable 0        0.8616  0.1384 </strong><br/><strong>Dead   0        0       1       </strong> </pre>
<p style="padding-left: 60px">By analyzing the preceding matrix, we can see that it has a particular shape; in fact, all of the elements below the main diagonal are equal to zero. This type of matrix is called the upper triangular and has the following properties:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><span>The rank is equal to the number of non-null elements present in the main diagonal.</span></li>
<li>The determinant is equal to the product of the elements present in the main diagonal.</li>
<li>The eigenvalues are represented by the elements present in the main diagonal.</li>
</ul>
</li>
</ul>
<ol start="3">
<li>At this point, we can define the model:</li>
</ol>
<pre style="padding-left: 60px">MCModel35&lt;-new("markovchain", transitionMatrix = TransMatrix35, states = WorkesStates , name="MCModel35")</pre>
<p style="padding-left: 60px">The <kbd>markovchain</kbd> class has been designed to handle homogeneous Markov chain processes.</p>
<p style="padding-left: 60px">The following arguments are passed:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><kbd>transitionMatrix</kbd><span>: This is the square transition matrix containing the probabilities of the transition matrix.</span></li>
<li><kbd>states</kbd><span>: This is the name of the states and must be the same as the <kbd>colnames</kbd> and <kbd>rownames</kbd> of the transition matrix. This is a character vector, listing the states for which transition probabilities are defined.</span></li>
<li><kbd>name</kbd><span>: This is an optional character element to name the discrete-time Markov chains.</span></li>
</ul>
</li>
</ul>
<ol start="4">
<li>To show a summary of the model we've just created, let's use the following command:</li>
</ol>
<pre style="padding-left: 60px">MCModel35</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px"><strong>MCModel35</strong><br/><strong> A 3 - dimensional discrete Markov Chain defined by the following states:</strong><br/><strong> Active, Unable, Dead</strong><br/><strong> The transition matrix  (by rows)  is defined as follows:</strong><br/><strong>         Active  Unable     Dead</strong><br/><strong>Active 0.997175 0.00195 0.000875</strong><br/><strong>Unable 0.000000 0.86160 0.138400</strong><br/><strong>Dead   0.000000 0.00000 1.000000</strong></pre>
<ol start="5">
<li>As we saw in <a href="7ee860fd-cd4c-4034-8dd6-9c803e129418.xhtml">Chapter 3</a>, <em>Markov Decision Processes in Action</em>, to get the states of the <kbd>markovchain</kbd> object, we can use the <kbd>states</kbd> method, as follows:</li>
</ol>
<pre style="padding-left: 60px">states(MCModel35)</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>[1] "Active" "Unable" "Dead"</strong> </pre>
<ol start="6">
<li>To get the dimension of the <kbd>markovchain</kbd> object, we can use the <kbd>dim()</kbd> method, as follows:</li>
</ol>
<pre style="padding-left: 60px">dim(MCModel35)</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>[1] 3</strong></pre>
<ol start="7">
<li>To see which elements are contained in the object we have created, we can use the <kbd>str()</kbd> function, which shows a compact view of the internal structure of an object, R:</li>
</ol>
<pre style="padding-left: 60px">str(MCModel35)</pre>
<p style="padding-left: 60px">The following results are printed:</p>
<pre style="padding-left: 60px"><strong>Formal class 'markovchain' [package "markovchain"] with 4 slots</strong><br/><strong>  ..@ states          : chr [1:3] "Active" "Unable" "Dead"</strong><br/><strong>  ..@ byrow           : logi TRUE</strong><br/><strong>  ..@ transitionMatrix: num [1:3, 1:3] 0.99718 0 0 0.00195 0.8616 ...</strong><br/><strong>  .. ..- attr(*, "dimnames")=List of 2</strong><br/><strong>  .. .. ..$ : chr [1:3] "Active" "Unable" "Dead"</strong><br/><strong>  .. .. ..$ : chr [1:3] "Active" "Unable" "Dead"</strong><br/><strong>  ..@ name            : chr "MCModel35"</strong></pre>
<ol start="8">
<li>To retrieve the elements contained in each one, it will be enough to use the name of the object (<kbd>MCModel35</kbd>), followed by the name of the slot, separated by the <kbd>@</kbd> symbol. For example, to print the transition matrix, we will write the following:</li>
</ol>
<pre style="padding-left: 60px">MCModel35@transitionMatrix</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px"><strong>         Active  Unable     Dead</strong><br/><strong>Active 0.997175 0.00195 0.000875</strong><br/><strong>Unable 0.000000 0.86160 0.138400</strong><br/><strong>Dead   0.000000 0.00000 1.000000</strong></pre>
<ol start="9">
<li>We can evaluate the absorbent state like so:</li>
</ol>
<pre style="padding-left: 60px">absorbingStates(MCModel35)</pre>
<p style="padding-left: 60px">The following state is returned:</p>
<pre style="padding-left: 60px"><strong>[1] "Dead"</strong></pre>
<ol start="10">
<li>As we know, from this state, we can't go back. At this point, we can predict the status of the worker. It's a simulation, so let's see what happens:</li>
</ol>
<pre style="padding-left: 60px">set.seed(5)<br/>WorkerStatePred35&lt;- rmarkovchain(n = 1000, object = MCModel35, t0 ="Active")</pre>
<p style="padding-left: 60px">The <kbd>set.seed()</kbd> command defines the seed of the random number generator. In this way, all of the random numbers that are used in the algorithm will always be the same each time the code is executed, making the example reproducible.</p>
<ol start="11">
<li>We will extract the statistics from the results:</li>
</ol>
<pre style="padding-left: 60px">table(WorkerStatePred35)</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px"><strong>WorkerStatePred35</strong><br/><strong>Active</strong><br/><strong> 1000</strong></pre>
<p style="padding-left: 30px">Let's see what happens to a 50-year-old worker:</p>
<ol>
<li>First, we will extract the transition matrix at the worker's age of 50:</li>
</ol>
<pre style="padding-left: 60px">TransMatrix50&lt;-matrix(as.numeric(DecrementsTable[DecrementsTable$age==50,2:10]),nrow = 3,ncol = 3, byrow = TRUE, dimnames = list(WorkesStates, WorkesStates))</pre>
<ol start="2">
<li>Then, we will set the model:</li>
</ol>
<pre style="padding-left: 60px">MCModel50&lt;-new("markovchain", transitionMatrix = TransMatrix50, states = WorkesStates, name="MCModel50")</pre>
<ol start="3">
<li>Finally, we will simulate the working life:</li>
</ol>
<pre style="padding-left: 60px">WorkerStatePred50&lt;- rmarkovchain(n = 1000, object = MCModel50, t0 ="Active")</pre>
<ol start="4">
<li>Now, we can extract the results:</li>
</ol>
<pre style="padding-left: 60px">table(WorkerStatePred50)</pre>
<p style="padding-left: 60px">The following results are printed:</p>
<pre style="padding-left: 60px"><strong>WorkerStatePred50</strong><br/><strong>Active   Dead Unable</strong><br/><strong>    33    956     11</strong></pre>
<p>Now, we can make predictions about working life, simulating the status of a worker over time:</p>
<ol>
<li>The first thing we need to do is create a <kbd>markovchain</kbd> object for each available working age. We will refer to the interval between 30 and 50 years since we have the data for it.</li>
<li>To speed up this procedure, we will create an iterative cycle and insert each <kbd>markovchain</kbd> object in a list:</li>
</ol>
<pre style="padding-left: 60px">MCModelsList=list()<br/>j=1<br/>for(i in 30:50){<br/>  TransMatrix&lt;-matrix(as.numeric(DecrementsTable[DecrementsTable$age==i,2:10]),nrow = 3,ncol = 3, byrow = TRUE, dimnames = list(WorkesStates, WorkesStates))<br/>  MCModelsList[[j]]&lt;-new("markovchain", transitionMatrix = TransMatrix, states = WorkesStates)<br/>  j=j+1<br/>}</pre>
<ol start="3">
<li>To start, we have initialized the list and a counter that will allow us to update the list.</li>
<li>Then, we extracted the values contained in the decrements table to obtain the transition matrix at each age.</li>
<li>Finally, we created the <kbd>markovchain</kbd> object for that age. At this point, we can create a <kbd>markovchainList</kbd> object, as follows:</li>
</ol>
<pre style="padding-left: 60px">MCList30to50&lt;-new("markovchainList", markovchains = MCModelsList,name="MCList30to50")</pre>
<p style="padding-left: 60px">A <kbd>markovchainlist</kbd> object is a list of <kbd>markovchain</kbd> objects. These objects can be used to model non-homogeneous discrete-time Markov chains when transition probabilities change by time.</p>
<ol start="6">
<li>Now, we can simulate the transition between the states where the worker can be found:</li>
</ol>
<pre style="padding-left: 60px">StatesSequence&lt;-rmarkovchain(n=10000, object=MCList30to50,t0="Active")</pre>
<p style="padding-left: 60px">The <kbd>rmarkovchain()</kbd> function generates a sequence of states from homogeneous or non-homogeneous Markov chains. In our case, we have generated a sequence of 10,000 simulations.</p>
<ol start="7">
<li>Let's see what the newly created object contains:</li>
</ol>
<pre style="padding-left: 60px">str(StatesSequence)</pre>
<p style="padding-left: 60px">The following results are printed:</p>
<pre style="padding-left: 60px"><strong>'data.frame':  210000 obs. of  2 variables:</strong><br/><strong> $ iteration: num  1 1 1 1 1 1 1 1 1 1 ...</strong><br/><strong> $ values   : Factor w/ 3 levels "Active","Dead",..: 1 1 1 1 1 1 1 1 1 1 ...</strong></pre>
<p style="padding-left: 60px">This is a dataframe containing 210,000 observations of two variables. We are interested in the second value variable, which contains the states of the simulations.</p>
<ol start="8">
<li>Next, we will extract some simple statistics that allow us to obtain the count of the occurrences of each state in all of the simulations that are carried out:</li>
</ol>
<pre style="padding-left: 60px">StatesOccurences&lt;-table(StatesSequence$value)</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px"><strong>Active   Dead Unable</strong><br/><strong>202189   4889   2922</strong></pre>
<ol start="9">
<li>With the data available, it is really easy to calculate the expected times a worker will be in the <kbd>Unable</kbd> state:</li>
</ol>
<pre style="padding-left: 60px">ExpectedUnableOccurence&lt;-StatesOccurences[3]/nrow(StatesSequence)</pre>
<p style="padding-left: 60px">Here, we have divided the number of unable occurrences by the number of rows contained in the <kbd>StatesSequence</kbd> object. The number of rows contained in the <kbd>StatesSequence</kbd> object is equal to the product of the number of simulations (10,000) for the number of years of working life (21, from 30 to 50).</p>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>    Unable</strong><br/><strong>0.01391429</strong></pre>
<p>Starting from the simulations we've carried out, the insurance companies can forecast the expected premiums.</p>
<p>In the next section, we learn to optimize the use of an operating room in a hospital.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operating room sanitation planning</h1>
                </header>
            
            <article>
                
<p>Every year, around 8,000 deaths are associated with surgical room infections in the United States. Infections increase the duration of hospitalization, hence increasing costs and initiating legal action by patients. The hospital is a building in which the appropriate cleaning conditions contribute both to a better quality of life for the patient and those who work there and to decrease the probability of spreading microorganisms.</p>
<p>Environmental sanitation concerns the complexity of practical and sanitary procedures, and operations aimed at making a plant or a specific environment healthy through cleaning and detergent activities. When a disinfectant is used later, it is called sanitation. An adequate cleaning cycle must be performed before disinfection, and in any case, combined with it. Each environment has an optimal standard which is a consequence of the intended use of the environment itself. Hence, the sanitation of the operating rooms is completely different from that of the hospital rooms, which in turn is different from those of the common areas. The hospital can be divided into three areas of infectious risk: low, medium, and high. Low-risk areas include common areas such as corridors, offices, and waiting rooms. Areas of medium risk include hospital rooms, clinics, and laboratories. High-risk areas include operating rooms, intensive care rooms, resuscitation rooms, and recovery rooms.</p>
<p>Prevention of infections in a surgical room is a significant and current problem since it represents an important and frequent complication of surgery with serious consequences for patients' health and increased costs, understood as hospital and extra-hospital. For the patient, a hospital infection results in additional illness; for the doctor or the nurse, these infections can invalidate the efficacy of the treatment carried out, question their professionalism, and make them responsible for increased mortality in the treated patients. For these reasons, the need to implement preventive interventions aimed at containing infections must represent a common and shared goal.</p>
<p>We will frame the problem by defining the data we have available and what we want to achieve.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the context</h1>
                </header>
            
            <article>
                
<p>In this example, we will deal with the problem of planning the sanitation and sterilization activities of an operating room. This is a routine operation that has costs in terms of resources used and the time in which the operating room remains inactive. In the planning of hygiene maintenance activities in the operating room, two main objectives must be pursued: the first objective is to avoid infection by patients treated in the room, while the second goal is to save on the costs of sanitizing and sterilizing operations.</p>
<p>The two objectives are, in any case, correlated since a possible infection in the operating room activates an intervention protocol that requires additional operations that impose a stop to the activities for 9 hours. On the other hand, standard activities provide a 3-hour stop in activities. Sanitation and sterilization operations are carried out every 30 days. Given the latest cases of infection, the operating room manager wants to know whether the planning of additional operations before the deadline can lead to an improvement. Two actions are available: do not perform additional operations (NoSS = No Sanitation-Sterilization) or perform additional operations (SS = Sanitation-Sterilization).</p>
<p>The interval between two operations (30 days) was divided into three sub-intervals that correspond to three states in which the operating theater is located:</p>
<ul>
<li>state 1: Interval between 0-10 days from the last intervention</li>
<li>state 2: Interval between 11-20 days from the last intervention</li>
<li>state 3: Interval between 21-30 days from the last intervention</li>
</ul>
<p>Immediately after a Sanitation-Sterilization operation, the room enters state 1. If no additional operations are performed at the end of the first 10 days, the room goes to state 2, and finally, after 10 more days, it passes to state 3, at the end of which<span> </span><span>an intervention</span><span> is expected in all cases. If, between one operation and the other,  patient becomes infected, the ongoing operations in the operating room are interrupted and an emergency operation is carried out, bringing the system back to state 1. The problem is how to manage the Sanitation-Sterilization operations in addition to those already foreseen in a long-term vision to maximize the reward.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transition probability and rewards</h1>
                </header>
            
            <article>
                
<p>This problem can be treated as a Markov decision-making process. <span>T</span><span>o begin, we must define the transition matrix <em>P</em> (<em>s</em>, <em>s'</em>, <em>a</em>). Remember that it tells us what the probabilities are of passing from one state to another. Since two actions are available (NoSS and SS), we will define two matrices of transitions. We denote the probability of infection with <em>ps</em>, whose probability depends on the state of the operating room. The longer the time spent by the last intervention, the higher the probability that there is an infection. In this regard, we will define three probabilities of infection: p1 = 0.2, p2 = 0.3, and p3 = 0.4. Recall that since there's two possible actions, we will define two transition matrices. The transition matrix relating to the choice of action 1 (NoSS) will be as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/eefb173c-7d26-40af-9b90-bae2102d3a79.png" style="width:13.83em;height:3.83em;"/></p>
<p>If we are in state 1, then we will have a probability <em>p1</em> that remains in that state if an infection occurs. The remaining probability <em>1-p1</em> involves moving to the next state if no infection occurs. While the probability of passing to the state 3 is equal to 0, it is not possible to pass directly from state 1 to 3. Recall that, in the matrix of transitions, the sum of all of the probabilities in a state that is in a row must be equal to 1. If we are in state 2, we will have a probability <em>p2</em> that passes into state 1 if an infection occurs. The remaining probability <em>1-p2</em> involves moving to the next state, such as 3 if no infection occurs. In this case, the probability of remaining in state 2 is equal to 0. Finally, if we are in state 3, we will have a probability of <em>p3</em> of going to state 1 if an infection occurs.</p>
<p>The remaining probability <em>1-p3</em> expects to remain in state 3 if no infection occurs. After this state, a Sanitation-Sterilization operation is still performed, while the probability of passing to state 2 is equal to 0.</p>
<p>By substituting the values of the three probabilities associated with the three states in the transition matrix, we obtain the following matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4cb7aeac-aee5-45e3-be83-a08cd93c162f.png" style="width:14.42em;height:4.83em;"/></p>
<p>Now, let's define the transition matrix related to the choice of action 2 (SS):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4c2551b8-8dcf-42b3-b3c6-59131e2ec38e.png" style="width:12.42em;height:5.17em;"/></p>
<p>In this case, the form of the transition matrix is more immediate: it is the action that foresees a Sanitation-Sterilization operation, which, as we anticipated, reports the room in any case in state 1. This is the person who died in all three states we have, which means that the probability that we move to state 1 is unitary.</p>
<p>With the transition matrices, we are finished. Now, we must define the rewards matrix. It is a 3x2 matrix: three states and two actions. Action 1 (NoSS) plans not to carry out additional operations; in this case, we will have a decreasing reward as we approach the end of the period in which the routine operation is scheduled (30 days). Then, the first column of the rewards matrix will take the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0e714634-9520-4bc9-92dc-2f638082b64b.png" style="width:7.08em;height:4.67em;"/></p>
<p>Its meaning is obvious: if the chosen action is not to carry out the Sanitation-Sterilization operations, then we will have 3 of the reward for the first state, 2 of the reward for the second state, and the minimum reward for state 3. In the case where the <span>chosen</span><span> </span><span>action is to carry out Sanitation-Sterilization operations, we will have the following </span><span>instead</span><span>:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/56d4446b-14be-426b-a4f4-6df0b5207a96.png" style="width:7.08em;height:4.67em;"/></p>
<p>As we can see, things have been reversed with respect to the previous case: if the action that's chosen is to carry out Sanitation-Sterilization operations, we will have 1 reward for the first state, 2 for the second state, and the maximum reward for the state 3.</p>
<p>Under these conditions, the rewards matrix becomes the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c91f8b3e-9a6f-4a42-b4c5-20f9535be667.png" style="width:9.67em;height:4.75em;"/></p>
<p>After defining the indispensable tools to be able to treat the problem as a Markov process, let's elaborate about the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model setting</h1>
                </header>
            
            <article>
                
<p>As we already mentioned, our goal is to calculate a policy that allows us to obtain the maximum prize based on the settings we just developed. This means reducing the risk of infection by minimizing the costs of Sanitation-Sterilization operations in the operating room. Let's get started:</p>
<ol>
<li>Let's take a look at the code that allows us to do this:</li>
</ol>
<pre style="padding-left: 60px">library(MDPtoolbox)<br/>P &lt;- array(0, c(3,3,2))<br/>P[,,1] &lt;- matrix(c(0.2, 0.8, 0, 0.3,0,0.7,0.4,0,0.6), 3, 3, byrow=TRUE)<br/>P[,,2] &lt;- matrix(c(1, 0, 0, 1, 0, 0,1, 0, 0), 3, 3, byrow=TRUE)<br/>R &lt;- matrix(c(3, 1, 2, 2,1,3), 3, 2, byrow=TRUE)<br/>mdp_check(P, R)<br/>QLearnModel=mdp_Q_learning(P=P, R=R, discount = 0.95)<br/>print(QLearnModel$Q)<br/>print(QLearnModel$V)<br/>print(QLearnModel$policy)<br/>print(QLearnModel$mean_discrepancy)</pre>
<ol start="2">
<li>We will analyze the code line by line to understand the meaning of each command. Let's start by importing the library:</li>
</ol>
<pre style="padding-left: 60px">library(MDPtoolbox)</pre>
<p style="padding-left: 60px">The <strong>Markov decision process</strong> (<strong>MDP</strong>) toolbox contains many functions that allow us to tackle the resolution of discrete-time Markov decision processes. We introduced the package in <a href="7ee860fd-cd4c-4034-8dd6-9c803e129418.xhtml">Chapter 3</a>, <em>Markov Decision Processes in Action</em>.</p>
<p style="padding-left: 60px">The first thing to do is define the matrices <em>P</em> for the transition matrix and <em>R</em> for the reward matrix. Both have been adequately introduced in the transition probability and rewards section.</p>
<ol start="3">
<li>The transition matrix is a 3x3 matrix. To start, we will create a new matrix and initialize it to zero:</li>
</ol>
<pre style="padding-left: 60px">P &lt;- array(0, c(3,3,2))</pre>
<ol start="4">
<li>We need to define the transition matrix related to the action 1 (NoSS):</li>
</ol>
<pre style="padding-left: 60px">P[,,1] &lt;- matrix(c(0.2, 0.8, 0, 0.3,0,0.7,0.4,0,0.6), 3, 3, byrow=TRUE)</pre>
<p style="padding-left: 60px">The following matrix is defined:</p>
<pre style="padding-left: 60px">&gt; P[,,1]<br/><strong>     [,1] [,2] [,3]</strong><br/><strong>[1,]  0.2  0.8  0.0</strong><br/><strong>[2,]  0.3  0.0  0.7</strong><br/><strong>[3,]  0.4  0.0  0.6</strong></pre>
<ol start="5">
<li>Let's define the transition matrix related to the action 2 (SS):</li>
</ol>
<pre style="padding-left: 60px">P[,,2] &lt;- matrix(c(1, 0, 0, 1, 0, 0,1, 0, 0), 3, 3, byrow=TRUE)</pre>
<p style="padding-left: 60px">The following matrix is defined:</p>
<pre style="padding-left: 60px">&gt; P[,,2]<br/><strong>     [,1] [,2] [,3]</strong><br/><strong>[1,]    1    0    0</strong><br/><strong>[2,]    1    0    0</strong><br/><strong>[3,]    1    0    0</strong></pre>
<ol start="6">
<li>Let's move on to the definition of the rewards matrix, as follows:</li>
</ol>
<pre style="padding-left: 60px">R &lt;- matrix(c(3, 1, 2, 2,1,3), 3, 2, byrow=TRUE)</pre>
<p style="padding-left: 60px">The following matrix is introduced:</p>
<pre style="padding-left: 60px">&gt; R<br/><strong>     [,1] [,2]</strong><br/><strong>[1,]    3    1</strong><br/><strong>[2,]    2    2</strong><br/><strong>[3,]    1    3</strong></pre>
<p style="padding-left: 60px">Before proceeding with the development of the model, it is necessary to verify that <kbd>P</kbd> and <kbd>R</kbd> satisfy the criteria necessary for the problem to be of the MDP type.</p>
<ol start="7">
<li>To do this, we'll use the <kbd>mdp_check()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">mdp_check(P, R)</pre>
<p style="padding-left: 60px">This function performs a check on the MDP process that we defined by setting the transition probability matrix (<em>P</em>) and the reward matrix (<em>R</em>). If <kbd>P</kbd> and <kbd>R</kbd> have been set correctly, then the function returns an empty message. If <kbd>P</kbd> and <kbd>R</kbd> were not set correctly, then the function returns an error message describing the problem. The following result is returned:</p>
<pre style="padding-left: 60px">&gt; mdp_check(P, R)<br/><strong>[1] ""</strong></pre>
<ol start="8">
<li>So, the problem has been set. Now, we can try to solve the problem by using the <kbd>mdp_Q_learning()</kbd> function, as follows:</li>
</ol>
<pre style="padding-left: 60px">QLearnModel=mdp_Q_learning(P=P, R=R, discount = 0.95)</pre>
<p style="padding-left: 60px">The <kbd>mdp_Q_learning()</kbd> function solves discounted MDP with the Q-learning algorithm. As we mentioned in <a href="9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml">Chapter 7</a>, <em>Temporal Difference Learning</em>, Q-learning is one of the most used reinforcement learning algorithms. Thanks to this technique, it is possible to find an optimal action for every given state in a finished MDP.</p>
<p style="padding-left: 60px">A general solution to the reinforcement learning problem is to estimate, thanks to the learning process, an evaluation function. This function must be able to evaluate, through the sum of the rewards, the convenience or otherwise of a particular policy. In fact, Q-learning tries to maximize the value of the Q function (action-value function), which represents the maximum discounted future reward when we perform actions <em>a</em> in the state <em>s</em>.</p>
<p style="padding-left: 60px">Q-learning estimates the function value 𝑞 (𝑠, 𝑎) incrementally, updating the value of the state-action pair at each step of the environment, by following the logic of updating the general formula for estimating the values for the TD methods. Q-learning has off-policy characteristics, that is, while the policy is improved according to the values estimated by 𝑞 (𝑠, 𝑎), the value function updates the estimates by following a strictly greedy secondary policy: given a state, the chosen action is always the one that maximizes the value max𝑞 (𝑠, 𝑎). However, the π policy has an important role in estimating values because through it, the state-action pairs to be visited and updated are determined.</p>
<p>The following arguments are passed:</p>
<ul>
<li><strong>P</strong>: Transition probability array</li>
<li><strong>R</strong>: Reward array</li>
<li><strong>discount</strong>: Discount factor—discount is a real number, which belongs to ]0; 1[</li>
</ul>
<p>The <kbd>mdp_Q_learning()</kbd> function computes the Q matrix, the mean discrepancy, and gives the optimal value function and the optimal policy when allocated enough iterations. The following results are returned:</p>
<ul>
<li><strong>Q</strong>: Action-value function</li>
<li><strong>V</strong>: Value function</li>
<li><strong>policy</strong>: Policy</li>
<li><kbd>mean_discrepancy</kbd>: Discrepancy means over 100 iterations</li>
</ul>
<p>Now, we will look at the results:</p>
<ol>
<li>Let's analyze the results we've obtained, starting with the action-value function:</li>
</ol>
<pre style="padding-left: 60px">print(QLearnModel$Q)</pre>
<p style="padding-left: 60px">The following table is printed:</p>
<pre style="padding-left: 60px">&gt; print(QLearnModel$Q)<br/><br/><strong> [,1] [,2]</strong><br/><strong>[1,] 47.14339 40.10113</strong><br/><strong>[2,] 46.59996 37.67135</strong><br/><strong>[3,] 29.04791 47.32842</strong></pre>
<p style="padding-left: 60px">The Q function represents the essential element of the procedure; it is a matrix of the same dimensions as the rewards matrix, that is, an SxA matrix. The value-action function returns the utility we expect to achieve by taking a given action in a given state and following an optimal policy.</p>
<ol start="2">
<li>Let's print the value function:</li>
</ol>
<pre style="padding-left: 60px">print(QLearnModel$V)</pre>
<p style="padding-left: 60px">The following vector is printed:</p>
<pre style="padding-left: 60px">&gt; print(QLearnModel$V)<br/><strong>[1] 47.14339 46.59996 47.32842</strong></pre>
<p style="padding-left: 60px">A value function represents how good a state is for an agent. It is equal to the total reward expected for an agent from the status <em>s</em>. The value function depends on the policy that the agent selects the actions to be performed with. <em>V</em> is an <em>S</em> length vector.</p>
<ol start="3">
<li>Then, we will extract the policy:</li>
</ol>
<pre style="padding-left: 60px">print(QLearnModel$policy)</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px">&gt; print(QLearnModel$policy)<br/><strong>[1] 1 1 2</strong></pre>
<p style="padding-left: 60px">A policy is an <em>S</em> length vector. Each element is an integer corresponding to an action that maximizes the value function. A policy defines the behavior of the learning agent at a given time. It maps the detected states of the environment and the actions to take when they are in those states. This corresponds to what, in psychology, would be called a set of rules or associations of stimulus response. The policy is the fundamental part of a reinforcing learning agent in the sense that it alone is enough to determine behavior.</p>
<p style="padding-left: 60px">So, the policy suggested by the method is not to carry out the Sanitation-Sterilization operations in the first two states, but to execute it if we are in state 3. This is due to the low probability of developing an infection in the first two states, which becomes important in state 3.</p>
<ol start="4">
<li>Finally, we print the discrepancy means over 100 iterations:</li>
</ol>
<pre style="padding-left: 60px">print(QLearnModel$mean_discrepancy)</pre>
<p style="padding-left: 60px">The following table is printed:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-578 image-border" src="assets/15318d02-95bf-41bb-a472-89d352bee333.png" style="width:38.00em;height:11.25em;"/></p>
<p>The <kbd>mean_discrepancy</kbd> value is a vector of <em>V</em> discrepancy mean over 100 iterations. Here, the length of the vector for the default value of <em>N</em> is 100.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to use reinforcement learning in the field of health care. We began by analyzing an overview of the uses of machine learning-based algorithms in healthcare. We saw how these algorithms are used for the diagnosis of diseases, the prediction of epidemic events, and the testing of new drugs.</p>
<p>We then dealt with two practical cases. First, we saw how to tackle a problem related to healthcare insurance by modeling it as a Markov process. In this way, it is possible to foresee the probabilities with which a worker can be injured and quantify the premium that will have to be paid. In the second example, we determined the best policy to be adopted in the planning of operations for sanitizing an operating theater. The problem was solved through the use of the Q-learning technique.</p>
<p>In the next chapter, we will explore the world of deep reinforcement learning and how neural networks can be used to make the best policy research operations even more efficient.</p>


            </article>

            
        </section>
    </body></html>