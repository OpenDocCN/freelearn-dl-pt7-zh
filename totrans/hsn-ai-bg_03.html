<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Platforms and Other Essentials</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll discuss important libraries and frameworks that one needs to get started in <strong>Artificial Intelligence</strong> (<strong>AI</strong>). We'll cover the basic functions of the three most popular deep learning frameworks—TensorFlow, PyTorch, and Keras—show you how to get up and running in each of these frameworks, as we will be utilizing them in the following chapters. We'll touch upon computing for AI, and discuss how GPUs and other advanced memory units can improve it. Lastly, we'll discuss the fundamentals of two popular cloud computing frameworks for deep learning: AWS and Google Cloud. </p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Essential libraries for deep learning in Python: TensorFlow, PyTorch, and Keras</li>
<li>CPUs, GPUs, and compute frameworks that are used for AI</li>
<li>The fundamentals of AWS and Google Cloud</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>We will be working with TensorFlow, PyTorch, and Keras in Python 3. It is recommended that you have an NVIDIA GPU on your computer. The following models are recommended:</p>
<ul>
<li><span>GTX 1080 Ti</span></li>
<li><span>GTX 1070</span></li>
<li><span>GTX 1060 </span></li>
</ul>
<p>If you do not have an NVIDIA GPU, please follow the prompts in the <em>Cloud Computing</em> section to utilize a GPU instance on AWS. </p>
<p>You must also have an AWS and Google Cloud account; both are free, and you can sign up at their respective websites.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">TensorFlow, PyTorch, and Keras</h1>
                </header>
            
            <article>
                
<p>In this section, we'll introduce three of the most popular deep learning frameworks: TensorFlow, PyTorch, and Keras. While we'll look at the basic functionality of each of the packages, we'll learn about specific deep learning functions for each of the frameworks in later chapters as part of our hands-on approach to learning AI. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">TensorFlow</h1>
                </header>
            
            <article>
                
<p><strong>TensorFlow</strong> is the most popular, and most contributed to, deep learning library. Originally developed by Google Brain for use on Google's own AI products, it was open sourced in 2015 and has since become the standard for deep learning. TensorFlow underlies all of Google's own deep learning based products such as Google Translate and the Google Cloud Platform's machine learning APIs. Google has setup TensorFlow specifically to be parallelized, and as such it performs really well in distributed environments. </p>
<p>TensorFlow provides APIs for Python, C++, Java, and others; however, in this book we are going to stick to utilizing Python. TensorFlow can be installed from PyPy with the simple: <kbd>pip install tensorflow</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic building blocks</h1>
                </header>
            
            <article>
                
<p>As you may have guessed from the name, TensorFlow relies on the algebraic concept of tensors that we learned about in the previous chapter. Everything, from input data to parameters, is stored in a tensor in TensorFlow. As such, TensorFlow has its own functions for many of the basic operations normally handled by NumPy. </p>
<p>When writing tensors in TensorFlow, we're really writing everything in an array structure. Remember how an array can be a rank 1 tensor? That is exactly what we are passing in the preceding example. If we wanted to pass a rank 3 tensor, we'd simply write <kbd>x = tf.constant([1,2,3,4],[5,6,7,8],[9,10,11,12])</kbd>. You'll notice that we defined constants in the following code; these are just one of three types of data structure we can use in TensorFlow: </p>
<ul>
<li><strong>Constants</strong>: Defined values that cannot change</li>
<li><strong>Placeholders</strong>: Objects that will be assigned a value during a TensorFlow <strong>session</strong></li>
<li><strong>Variables</strong>: Like constants, only the values can change</li>
</ul>
<p>Alright, back to the code. If we had run the <span><span>following</span></span> code block, we would have been left with a <strong>TensorFlow object</strong>, which looks something like tensor (<kbd>"Mul:0"</kbd>, <kbd>shape=(4,), dtype=int32</kbd>). </p>
<pre>## Import Tensorflow at tf for simplicity<br/>import tensorflow as tf<br/><br/>## Define two constants <br/>const1 = tf.constant([4])<br/>const2 = tf.constant([5])<br/><br/>## Multiply the constants<br/>product = tf.multiply(const1, const2)</pre>
<p><span>Why? because TensorFlow runs on the concept of </span><strong>sessions</strong><span>. The underlying code of TensorFlow is written in C++, and a session allows a high-level TensorFlow package to communicate with the low-level C++ runtime. </span></p>
<div class="packt_infobox"><span>Before we run a TensorFlow session, we need to tell it to initialize all of the variables we declared, and then run the initialization## In Tensorflow, we must first initialize a session object</span></div>
<pre>## Variable Initializer<br/>init = initialize_all_variables()<br/><br/>## Initialize the session<br/>sess = tf.Session()<br/>sess.run(init)<br/><br/>## Run the session<br/>print(sess.run(product)) <br/><br/>## Close the session<br/>sess.close()</pre>
<p>One last important concept in TensorFlow is that of <strong>scopes</strong>. Scopes help us control various operational blocks within our model:</p>
<pre class="graf graf--pre graf-after--p">with tf.name_scope("my_scope"):<br/>            ## Define two constants <br/>            const1 = tf.constant([4])<br/>            const2 = tf.constant([5])<br/><br/>            ## Multiply the constants<br/>            product = tf.multiply(const1, const2)</pre>
<p>That's it! We've successfully performed out first operation in TensorFlow. We'll also be learning more about the in-depth operations of TensorFlow in the next chapter on building <strong>Artificial Neural Networks </strong>(<strong>ANNs</strong>).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The TensorFlow graph</h1>
                </header>
            
            <article>
                
<p>One of the more important and powerful features of TensorFlow is its graph. When you define one of the three types of TensorFlow data structures previously described, you automatically add a<span> </span><strong>node</strong><span> </span>and an<span> </span><strong>edge</strong><span> </span>to your graph. Nodes represent operations and edges represent tensors, so if we were to do basic multiplication such as the preceding example,<span> </span><kbd>const1</kbd><span> </span>and<span> </span><kbd>const2</kbd><span> </span>would represent edges in the graph,<span> </span><kbd>tf.multiply</kbd><span> </span>would represent a node, and<span> </span><kbd>product</kbd><span> </span>would represent an outgoing edge from that node. TensorFlow's graph is <strong>static</strong>, which means we cannot change it at runtime.</p>
<p><span>Remember, an ANN performs hundreds of computations; computing and interpreting at each step would be extremely compute-intensive. The TensorFlow graph ...</span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">PyTorch</h1>
                </header>
            
            <article>
                
<p><span><strong>PyTorch</strong> is a newer, but growing deep learning library that is based on the Torch framework used for Facebook's deep learning algorithms. Unlike TensorFlow, PyTorch is not a wrapper that compiles to an underlying language, but is written to mimic native Python. If you have had any experience with Python programming, PyTorch will feel extremely familiar to you. </span></p>
<p>PyTorch can be easily installed with:</p>
<div class="command">
<div>
<pre class="text"><strong>conda install pytorch torchvision -c pytorch</strong></pre></div>
</div>
<p><span>Currently, PyTorch does not have a Windows distribution, which may make it out of reach for some users. </span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic building blocks</h1>
                </header>
            
            <article>
                
<p>Such as TensorFlow, PyTorch represents data in tensor form. Torch tensors are defined as standard data types, such as <kbd>torch.FloatTensor()</kbd> , <kbd>torch.charTensor()</kbd>, and <kbd>torch.intTensor()</kbd>. As mentioned, operations in PyTorch are highly Pythonic. To repeat the exact same multiplication operation that we performed in preceding TensorFlow: </p>
<pre>import torch x = torch.IntTensor([4])y = torch.IntTensor([5])product = x * y</pre>
<p>As a result of it native Python feel, PyTorch allows for easy interaction between standard numpy arrays and PyTorch tensors. It's easy to switch back and forth between the two:</p>
<pre>import torch import numpy as np## Create a numpy arraynumpy_array = np.random.randn(20,20)##Convert the numpy array to a pytorch tensorpytorch_tensor ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The PyTorch graph</h1>
                </header>
            
            <article>
                
<p>PyTorch seems more Pythonic because of its <strong>dynamic graph compute structure</strong>. Since Python is an interpreted language, meaning that operations are executed at runtime, PyTorch's graphing feature seeks to replicate this by allowing us to alter variables in the graph at runtime. In simpler words, PyTorch's graphs are created at the time you actually execute the code, not defined statically beforehand like in TensorFlow. Architecturally, this means that you can actually change your network architecture during training, which means PyTorch can accommodate a lot more cutting edge, dynamic architectures. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Keras</h1>
                </header>
            
            <article>
                
<p><strong>Keras</strong> is the most high-level deep learning library available, and is often where people begin on their AI journey. While we will focus on applications with TensorFlow in this book, it is important to introduce Keras because of its ubiquity and ease of use. </p>
<p>Written by François Chollet at Google, Keras is a wrapper that can run on top of TensorFlow or other libraries such as Apache, MXNet, or Theano. Like the other libraries, it is available through PyPy by running <kbd>pip install keras</kbd> in your terminal or command line. Functionally, it's very similar to the way the scikit-learn works, and hence is a popular library for those who wish to get their hands dirty with deep learning as quickly as possible. </p>
<p>Like PyTorch, Keras was designed to ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic building blocks</h1>
                </header>
            
            <article>
                
<p>As Keras is designed as a model-level library, it does not contain methods for doing basic operations as PyTorch of base TensorFlow does. Instead, it utilizes TensorFlow as a backend. As such, its basic operations are the same as basic TensorFlow operations: </p>
<pre>import keras.backend as K<br/>x = K.constant(5)<br/>y = K.constant(6)<br/>product = x * y<br/><br/></pre>
<p>Keras also uses the same graph structure as Tensorflow. We'll learn more about Keras model building methods in the next chapter on <em>Your First Artificial Neural Networks</em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Wrapping up</h1>
                </header>
            
            <article>
                
<p>So, what is the best library to use? As you can see in the following screenshot, one benchmark places PyTorch firmly in the lead when compared with other deep learning libraries when running an ANN:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-large wp-image-924 image-border" src="Images/60eaf104-5f98-440f-af9d-4444555f98d5.png" style="width:54.75em;height:39.25em;" width="1024" height="735"/></div>
<p>Ultimately, your choice of library mostly comes down to personal preference; however, in general: </p>
<ul>
<li><strong>Keras</strong>: Best for beginners or those looking to do <em>quick and dirty</em> work on ANNs </li>
<li><strong>TensorFlow</strong>: Widely used, there are great code bases and tutorials available online and it is widely integrated into cloud machine images and all types of computing frameworks </li>
<li><strong>PyTorch</strong>: Provides exceptional speed and ease of use, but is largely still underdeveloped, ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cloud computing essentials</h1>
                </header>
            
            <article>
                
<p>Often, on-premise GPU clusters are not always available or practical. More often than not, many businesses are migrating their AI applications to the cloud, utilizing popular cloud provider services such as <strong>Amazon Web Services</strong> (<strong>AWS</strong>) or the <strong>Google Cloud Platform</strong> (<strong>GCP</strong>). When we talk about the cloud, we are really talking about database and compute resources, offered as a service. Cloud solution providers such as AWS and GCP have data centers across the world that store data and run computing jobs for people remotely. When your data is in the cloud, or when you are running a program in the cloud, you are really running or storing in one of these data centers. In cloud terminology, we call these data centers or cluster of data centers <strong>regions</strong>.<strong> </strong></p>
<p>Cloud services are divided into three different offering structures: </p>
<ul>
<li><strong>Infrastructure as a Service</strong> (<strong>IaaS</strong>): Raw computing and network resources that you can use to build infrastructure, just as you would locally</li>
<li><strong>Platform as a Service</strong> (<strong>PaaS</strong>): Managed services that obfuscate away infrastructure components</li>
<li><strong>Software as a Service</strong> (<strong>SaaS</strong>): Fully managed solutions, such as online email</li>
</ul>
<p>In this section, we'll cover both IaaS solutions, as well as PaaS solutions. While cloud providers do offer SaaS solutions for AI, they are a bit too high level for our needs. <span>In this section, we'll discuss the basic tools that you'll need to utilize the compute power of the cloud. Towards the end of this chapter, we'll discuss cloud computing in more detail in the </span><em>Maintaining AI applications</em><span> section. </span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">AWS basics</h1>
                </header>
            
            <article>
                
<p><strong>AWS</strong> is the most popular cloud computing provider on the market. In this section, we'll explore the basics for getting set up in the cloud, including creating and connecting to EC2 instances (Amazon's main cloud computing framework), as well as how to set up virtual machines in the cloud.</p>
<p>We'll also touch upon how to utilize Amazon's bulk storage component, S3. <span>While AWS offers several machine learning services, we're going to focus solely on the basic need to utilize AWS cloud computing architectures to power your AI systems.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">EC2 and virtual machines</h1>
                </header>
            
            <article>
                
<p>The building block for AWS systems is the <strong>Elastic Cloud Compute </strong>(<strong>EC2</strong>) instance; it is a virtual server that allows you to run applications in the cloud. In this chapter, EC2 will be the basis for our cloud computing work. For developers and data scientists, Amazon has a suite of virtual machines called <strong>Amazon Machine Images</strong> (<strong>AMI</strong>) that come preloaded with everything you need to get up and running with deep learning in the cloud. For our purposes, Amazon has both an Ubuntu AMI as well as an Amazon Linux distribution AMI, which are preloaded with Python 3 and TensorFlow, PyTorch, and Keras.</p>
<p>To get started with utilizing EC2 for deep learning, we'll just have to follow a few steps: </p>
<ol>
<li class="CDPAlignLeft CDPAlign">Log in to your Amazon Web Services Account.</li>
<li class="CDPAlignLeft CDPAlign">Search for EC2 in the Search bar and select the service to open a new console.</li>
<li class="CDPAlignLeft CDPAlign">Choose the <span class="packt_screen">Launch Instance</span> <span>button and search for the AWS deep learning AMI in the AWS Marketplace. You can select either the Ubuntu version or Amazon Linux. </span></li>
<li class="CDPAlignLeft CDPAlign">Select a GPU instance to run your image on. We suggest either a G2 or P2 instance. Choose Next on each page until you reach Configure Security Group. Under Source, choose My IP to allow access using only your IP address.</li>
<li class="CDPAlignLeft CDPAlign">Click <span class="packt_screen">Launch Instance.</span></li>
<li class="CDPAlignLeft CDPAlign">Create a new Private Key and store this somewhere locally; this will help you connect to your instance later on. </li>
</ol>
<p>Now, you should have your AMI set up and ready to utilize. If you already have an EC2 instance up and running 0<span>n your AWS account, select the instance and right-click on <span class="packt_screen">Image</span>, <span class="packt_screen">Create Image</span> under the dropdown for that instance:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1394 image-border" src="Images/59fb62db-96e8-42cf-b8a7-fb7a62dc4784.png" style="width:30.83em;height:21.50em;" width="558" height="389"/></div>
<p>Follow the prompts and select <span class="packt_screen">Create Image</span>. Afterwards, you can find that AMI by selecting <span class="packt_screen">EC2 -&gt; AMIs</span> under the main Explorer toolbar. If you still can't see your AMI, you can find more detailed instructions on AWS website <a href="https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/tkv-create-ami-from-instance.html">https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/tkv-create-ami-from-instance.html</a>.</p>
<p>To utilize your new virtual machine, first launch the instance on AWS. Here <kbd>ssh</kbd> is initialized by utilizing the following command (make sure you are in the same directory as the <kbd>pem</kbd> key file you just downloaded):</p>
<pre><strong><span>cd /Users/</span><span>your_username</span><span>/</span><span>Downloads</span><span>/<br/></span><span>ssh -L localhost:8888:localhost:8888 -i </span><span>&lt;your .pem file name&gt;</span><span> ubuntu@</span><span>&lt;Your instance DNS&gt;</span></strong></pre>
<p>Once you've connected with your terminal or command line, you can utilize the interface just as you would the command line on your local computer. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">S3 Storage </h1>
                </header>
            
            <article>
                
<p><strong>Amazon Simple Storage Service</strong> (<span><strong>Amazon S3</strong>)</span>, is AWS's bulk cloud storage solution. S3 is designed to be simple, cheap, and efficient - it works just like a local directory on your computer would. These storage locations are known as <strong>Buckets</strong>, and can store up to 5 Terabytes of data.</p>
<p>To setup an S3, log onto your AWS console, find the S3 service, and click <span class="packt_screen">Create bucket</span> </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1395 image-border" src="Images/5003b9fd-4865-4250-9f01-dc2f33954891.png" style="width:29.33em;height:11.67em;" width="798" height="318"/></p>
<p>You can set permissions for who can and cannot access the data in an S3 bucket, should you need to restrict access. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">AWS Sagemaker</h1>
                </header>
            
            <article>
                
<p><span>SageMaker is Amazon's fully managed cloud machine learning offering. As a <strong>Platform as a Service</strong> (<strong>PaaS</strong>) product, SageMaker is one of the simplest ways in which you can deploy a machine learning model. Unlike it's competitors, Amazon SageMaker only runs Python 2.7. </span>SageMaker has two options for handling machine learning services in the cloud:</p>
<ol>
<li>Creating and training your model in a hosted Jupyter notebook </li>
<li>Training from a dockerized version of the model</li>
</ol>
<p>We'll be diving into how to train and deploy models with SageMaker in the coming sections. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Google Cloud Platform basics</h1>
                </header>
            
            <article>
                
<p>While AWS is the major player in the cloud marketplace and has been for some time now, over the past few years the <strong>Google Cloud Platform</strong> (<strong>GCP</strong>) has been gaining popularity, especially in the field of machine learning. You can sign up for GCP for free by navigating to <a href="https://cloud.google.com/free/" target="_blank">https://cloud.google.com/free/</a>, and entering the console. Keep in mind that you do need a Google user account, such as a gmail account, to sign up for Google services. While many small tasks can be completed within the platform's free tier, GCP offers a $300.00 credit to new users to get started. </p>
<p>All services in GCP run under the umbrella of a project. Projects are tools for organizing computing tools, users and access rights, as well as billing. ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">GCP cloud storage</h1>
                </header>
            
            <article>
                
<p>Cloud storage is a simple, bucket-structured storage option that is similar to AWS S3. Like AWS, GCP cloud storage holds up to 5 Terabytes of data. As opposed to competitors such as AWS, or even Microsoft Azure, GCP's cloud storage has upload and download speeds for large files that are about three times faster than it's competitors. Cloud storage also has some of the fastest <strong>throughput</strong> on the market. Throughput, a cloud concept that measures how much data is processed at a given time - in simpler words, how fast can data be processed. When creating certain applications that rely on streaming data, this can be critical. <span>Cloud storage also has the option to create buckets that span across service regions, which helps with fault tolerance and availability of your data.</span></p>
<p>To setup a Cloud storage bucket, log-on to the GCP console, search for storage, and click <span class="packt_screen">CREATE BUCKET</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1398 image-border" src="Images/a568ade9-aab8-439a-bc9a-15ef47c75f14.png" style="width:53.67em;height:8.25em;" width="995" height="153"/></p>
<p>In addition to their standard compute and storage services, the GCP has another tool, ML engine, which provides seamless training and deployment operations for machine learning models. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">GCP Cloud ML Engine</h1>
                </header>
            
            <article>
                
<p>Google Cloud Platform's Cloud ML Engine is Google's equivalent to AWS SageMaker. As a managed PaaS, Cloud ML handles the training and deployment processes for machine learning algorithms. If you're thinking - what about a basic compute service like EC2 on AWS? GCP has that as well. <span>Compute Engine is GCP's answer to Amazon EC2; it provides basic, scalable cloud compute services. While we could use Compute Engine to setup AI platforms, GCP has made it extremely simple for us to build with Cloud ML Engine and as such, we will note be covering the basic Compute Engine. </span></p>
<p>Let's dive into the details. Cloud ML engine allows you to: </p>
<ul>
<li>Train scikit-learn and TensorFlow models both locally for testing and in the cloud</li>
<li>Create retrainable ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">CPUs, GPUs, and other compute frameworks</h1>
                </header>
            
            <article>
                
<p>Progress in AI has always been tied to our compute abilities. In this section, we will discuss CPUs and GPUs for powering AI applications, and how to set up your system to work with accelerated GPU processing. </p>
<p>The main computational hardware in your computer is known as the <strong>central processing unit </strong>(<strong>CPUs</strong>); CPUs are designed for general computing workloads. While your local CPU can be used to train a deep learning model, you might find your computer hanging up on the training process for hours. When training AI applications on hardware, it's smarter to use the CPU's cousin, the <strong>Graphics Processing Unit</strong> (<strong>GPU</strong>). GPUs are designed to process in parallel, just as an ANN process in parallel. As we learned in the last chapter, AI applications require many linear algebra operations, the exact same type of operations that are required for video games. GPUs, originally designed for the gaming industry, provide us with thousands of cores to process these operations as well as parallelize them. In this manner, they lend themselves naturally to constructing deep learning algorithms. </p>
<p><span>When selecting a GPU to utilize in deep learning applications, we're looking at three main characteristics:</span></p>
<ul>
<li><strong>Processing power</strong>: How fast the GPU can compute; defined as <em>cores</em> x <em>speed</em></li>
<li><strong>Memory</strong>: The ability of the GPU to handle various sizes of data</li>
<li><strong>RAM</strong>: The amount of data you can have on your GPU at any given time</li>
</ul>
<p><span>In this section, we'll be focusing on utilizing the most popular GPU brand for deep learning, <strong>NVIDIA</strong>, whose CUDA toolkit makes out-of-the-box deep learning an easy task. As the major competitor to NVIDIA, Radeon AMD GPUs utilize a toolkit called <strong>OpenCL</strong>, which does not have direct compatibility with most deep learning libraries out of the box. While AMD GPUs provide great hardware at a reasonable price, it is best to go with an NVIDIA product to make getting up to speed easy. </span></p>
<p><span>Should you have another GPU on your computer or no GPU at all, it is recommended that you utilize a GPU instance on AWS to follow the steps. </span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Installing GPU libraries and drivers</h1>
                </header>
            
            <article>
                
<p>Now, let's get our computers set up for building AI applications. If you wish to do this task on your local computer, it's advised that you have either Windows or a Linux distribution installed. Unfortunately, most macOS are not built to accommodate GPUs, and therefore we will not be touching on macOS in this section. If you do not have an NVIDIA GPU on your computer, please perform the following steps. If you do have an NVIDIA GPU, you may choose to follow along with the AWS-based section, or skip to the following section.</p>
<p>If you're not sure whether you have an NVIDIA GPU, you can check for its existence with the following terminal command: </p>
<pre class="graf graf--pre graf-after--p"><strong>lspci -nnk | grep -i nvidia</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">With Linux (Ubuntu)</h1>
                </header>
            
            <article>
                
<p>Linux and AI go together such as natural complements. When you talk to Google Assistant on your Android phone, talk to Cortana on your Windows device, or wonder how Watson won a round at Jeopardy on TV, it's all based on Linux.</p>
<p>When we talk about Linux in this chapter, we'll be talking about a particular distribution called <strong>Ubuntu</strong>,<strong> </strong>one of the most popular distributions of the Linux operating system. It's recommended that you stick with an older, more stable version of Ubuntu (Version 14.04), as although it will do wonders for your AI applications; it's certainly not as stable as your standard Windows OS. </p>
<p>If you'd like to use Ubuntu on your local machine, check out Ubuntu's tutorials for installing on a Windows machine <span>(</span><span><a href="https://tutorials.ubuntu.com/tutorial/tutorial-ubuntu-on-windows#0">https://tutorials.ubuntu.com/tutorial/tutorial-ubuntu-on-windows#0</a>)</span>. If you don't have a PC or you'd like to set up a virtual instance, AWS has a great tutorial (<a href="https://aws.amazon.com/getting-started/tutorials/launch-a-virtual-machine/">https://aws.amazon.com/getting-started/tutorials/launch-a-virtual-machine/</a>) to walk you through the steps:</p>
<ol>
<li>To get started with deep learning for GPUs on Ubuntu, we first have to install the GPU's driver. In this example, we're going to utilize <kbd>wget</kbd> and <kbd>chmod</kbd> to retrieve and set up read/write access: </li>
</ol>
<pre style="padding-left: 60px" class="graf graf--pre graf-after--p"><strong>wget http://us.download.nvidia.com/XFree86/Linuxx86_64/367.44/NVIDIA-Linux-x86_64-367.44.run</strong><br/><br/><strong>sudo chmod +x NVIDIA-Linux-x86_64-367.35.run</strong><br/><strong>./NVIDIA-Linux-x86_64-367.35.run --silent</strong></pre>
<ol start="2">
<li>Once the installation finishes, you can check if it was intalled correctly with a simple <kbd>nvidia-smi</kbd> command.</li>
<li>Next, let's install NVIDIA CUDA. CUDA is a NVIDIA package that allows us to run TensorFlow models on our GPUs: </li>
</ol>
<div>
<pre style="padding-left: 60px"><span><strong><span class="cm-builtin">wget</span></strong><span class="cm-string"><strong>"http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.44-1_amd64.deb"</strong><br/><strong>## Install the drivers</strong><br/><strong>sudo chmod +x cuda_7.5.18_linux.run</strong><br/><strong>./cuda_7.5.18_linux.run --driver --silent</strong><br/><strong>./cuda_7.5.18_linux.run --toolkit --silent</strong><br/><strong>./cuda_7.5.18_linux.run --samples --silent</strong><br/></span></span></pre></div>
<ol start="4">
<li class="CodeMirror-linenumber CodeMirror-gutter-elt">Next, let's add the library to our system path:</li>
</ol>
<pre style="padding-left: 60px" class="graf graf--pre graf-after--p"><strong>echo ‘export LD_LIBRARY_PATH=”$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64"’ &gt;&gt; ~/.bashrc</strong></pre>
<ol start="5">
<li>Lastly, we need to install a higher-level package called cuNN, which <span>is a specific library that sits on top of CUDA and provides highly-tuned procedures for typical deep learning operations:</span></li>
</ol>
<pre style="padding-left: 60px" class="graf graf--pre graf-after--blockquote"><strong>sudo scp cudnn-7.0-linux-x64-v4.0-prod.tgz</strong></pre>
<ol start="6">
<li>One last step to move the files to the correct place:</li>
</ol>
<pre style="padding-left: 60px" class="graf graf--pre graf-after--p"><strong>tar -xzvf cudnn-7.0-linux-x64-v4.0-prod.tgz</strong><br/><strong>cp cuda/lib64/* /usr/local/cuda/lib64/</strong><br/><strong>cp cuda/include/cudnn.h /usr/local/cuda/include/</strong></pre>
<ol start="7">
<li>And there you are, we're set up on Ubuntu for GPU acceleration. Our last step is to simply install the GPU-enabled version of TensorFlow with Python 3:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip3 install tensorflow-gpu</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">With Windows</h1>
                </header>
            
            <article>
                
<p>To set up your GPU for deep learning on Windows, you must be running Windows 7 or higher. You can verify that you have a CUDA-capable GPU through the <strong>display adapters</strong> section in the <strong>Windows Device Manager</strong>. Here, you will find the vendor name and model of your graphics card.</p>
<ol>
<li>To see what type of GPU you have, open a command line prompt and run:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>control /name Microsoft.DeviceManager</span></strong></pre>
<ol start="2">
<li>If you do not have the driver installed for your NVIDIA GPU, or would like to update the drivers, you can find the correct driver based on your device on the NVIDIA website: <a href="http://www.nvidia.com/Download/index.aspx?lang=en-us">http://www.nvidia.com/Download/index.aspx?lang=en-us</a>. </li>
<li>Next, go ahead and grab the CUDA toolkit (<a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a>) form NVIDIA. Select <strong>CUDA Version ...</strong></li></ol></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic GPU operations</h1>
                </header>
            
            <article>
                
<p>Now that we have our GPUs set up for deep learning, let's learn how to utilize them. In TensorFlow, GPUs are represented as strings:</p>
<ul>
<li><kbd>/cpu:0</kbd>: The CPU of your machine</li>
<li><kbd>/device:GPU:0</kbd>: The GPU of your machine, if you have one</li>
<li><kbd>/device:GPU:1</kbd>: The second GPU of your machine, and so on</li>
</ul>
<p><strong>Distributed training</strong> is the practice of training a network across several GPUs, and it's becoming an increasingly common way to train models in the AI field. TensorFlow, Keras, and PyTorch all support distributed training.</p>
<p><strong>Logging</strong> is an operation in TensorFlow to assign a particular set of commands to a particular GPU or CPU on your system. With logging, we can also <strong>parallelize</strong> our operations, meaning that we can distribute training across several GPUs at the same time. To do this, we utilize a simple loop structure:</p>
<div>
<pre><span>my_list = []<br/>## Iterate through the available GPUs<br/>for device in ['/gpu:0', '/gpu:1']:<br/>    ## Utilize the TensorFlow device manager<br/>    with tf.device(device):<br/>        x = tf.constant([1,2,3], shape=[1,3])<br/>        y = tf.constant([1,2,3],shape [3,1]) <br/>        my_list.append(tf.matmul(x, y))<br/>    <br/>    with tf.device('/cpu:0'):<br/>        sum_operation = tf.add(x,y)<br/>    <br/>    ## Run everything through a session<br/>    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))<br/>    sess.run(sum_operation)<br/></span><span><br/></span></pre></div>
<p>We first create an empty set, and use an iterator to assign the matrix multiplication procedure across two GPUs: GPU1 and GPU2. Another procedure, a simple sum, gets assigned to our CPU. We then run both through a TensorFlow session to execute the operations as we did before. </p>
<p>Keep in mind that this type of device management is for local devices, and that management is different for cloud architectures. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The future – TPUs and more</h1>
                </header>
            
            <article>
                
<p>Google has recently released a new piece of hardware known as the <strong>Tensor Processing Unit </strong><span>(</span><strong>TPU</strong><span>),</span> which is specifically designed for AI applications. These TPUs deliver 15-30x higher performance and 30-80x higher performance-per-watt than a CPU or GPU can deliver. Weights in large scale, production-level AI applications can number from five million to 100 million, and these TPUs excel in performing these operations. </p>
<p>TPUs are specifically tailored for TensorFlow processing, and are currently available on Google Cloud for use. If you're interested in exploring their functionality, GCP has a great tutorial (<a href="https://cloud.google.com/tpu/docs/quickstart" target="_blank">https://cloud.google.com/tpu/docs/quickstart</a>) on how to get started. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>The landscape for AI platforms and methods is diverse; in this chapter, we've outlined some of the most promising and well regarded technologies in the AI arena. For deep learning packages, we learned about TensorFlow, PyTorch, and Keras. TensorFlow, released by Google, is the most popular and robust of the deep learning libraries. It utilizes sessions and static graphs to compile to its underlying C++ code. PyTorch, on the other hand, is a newer library that features a dynamic graph for runtime execution, which allows it to feel like native Python. Lastly, Keras is a high-level library that runs on top of TensorFlow, and can be useful for creating straightforward networks where customization is not needed. </p>
<p>We also discussed cloud computing, utilizing AWS as the most popular cloud computing service, with its primary workhorses being the EC2 instance and the s3 Bucket. EC2 consists of virtual servers that can be used for scaling AI applications where hardware doesn't exist/is not needed. S3, Amazon's Simple Storage Service gives us the ability to store data and other resources that are necessary for running our applications. Lastly, we walked through enabling your computer and languages for GPU-accelerated deep learning. In the next chapter, we'll put this to use by creating our first ANNs.</p>
<p>In the next chapter, we'll put together the fundamental knowledge that learned in <a href="c72aa49d-41f1-4a15-bee5-9efc9190f282.xhtml" target="_blank">Chapter 2</a>, <em>Machine Learning Basics</em>, with the platform knowledge from this chapter to create our first ANNs. </p>


            </article>

            
        </section>
    </div>



  </body></html>