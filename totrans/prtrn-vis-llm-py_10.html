<html><head></head><body>
		<div id="_idContainer089">
			<h1 id="_idParaDest-132" class="chapter-number"><a id="_idTextAnchor152"/>10</h1>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor153"/>Fine-Tuning and Evaluating</h1>
			<p>In this chapter, you’ll learn how to fine-tune your model on use case-specific datasets, comparing its performance to that of off-the-shelf public models. You should be able to see a quantitative and qualitative boost from your pretraining regime. You’ll dive into some examples involving language, text, and everything in between. You’ll also learn how to think about and design a human-in-the-loop evaluation system, including the same RLHF that makes ChatGPT tick! This chapter focuses on updating the trainable weights of the model. For techniques that mimic learning but don’t update the weights, such as prompt tuning and standard retrieval augmented generation, see <a href="B18942_13.xhtml#_idTextAnchor198"><span class="No-Break"><em class="italic">Chapter 13</em></span></a> on <span class="No-Break">prompt engineering.</span></p>
			<p>We are going to cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Fine-tuning for language, text, and everything <span class="No-Break">in between</span></li>
				<li>LLM fine-tuning breakdown – instruction fine-tuning, parameter efficient fine-tuning, and reinforcement learning with <span class="No-Break">human feedback</span></li>
				<li><span class="No-Break">Vision fine-tuning</span></li>
				<li>Evaluating foundation models in vision, language, and <span class="No-Break">joint tasks</span></li>
			</ul>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor154"/>Fine-tuning for language, text, and everything in between</h1>
			<p>At this point in the book, we’ve already covered a lot of ground. We’ve focused primarily on the pretraining aspect, looking at everything from finding the right use cases and datasets to defining your loss functions, preparing your models and datasets, defining progressively larger experiments, parallelization basics, working with GPUs, finding the right hyperparameters, advanced concepts, and more! Here, we’ll explore how to make your models even more targeted to a <a id="_idIndexMarker489"/>specific <span class="No-Break">application: </span><span class="No-Break"><strong class="bold">fine-tuning</strong></span><span class="No-Break">.</span></p>
			<p>Presumably, if you are embarking on a<a id="_idIndexMarker490"/> large-scale training project, you might have one of the <span class="No-Break">following goals:</span></p>
			<ul>
				<li>You might be pretraining your own <span class="No-Break">foundation model</span></li>
				<li>You might be designing a novel method for <span class="No-Break">autonomous vehicles</span></li>
				<li>You might be classifying and segmenting 3D data, such as in real estate <span class="No-Break">or manufacturing</span></li>
				<li>You might be training a large text classification model or designing a novel image-text <span class="No-Break">generation model</span></li>
				<li>You might be building a text-to-music generator, or working on a completely new jointly trained modality, as yet undiscovered by the machine <span class="No-Break">learning community</span></li>
				<li>You might be training a large language model to solve general-purpose search and qquestion and answering for the entire world, or for specific communities, languages, organizations, <span class="No-Break">and purposes</span></li>
			</ul>
			<p>All these use cases have something in common; they use one large-scale model that achieves general-purpose intelligence through learning patterns in extreme-scale datasets and model sizes. In many cases, however, <em class="italic">these models only become extraordinarily useful when fine-tuned to solve a specific problem</em>. This is not to say that you cannot simply deploy one of them and use specialized <em class="italic">prompt engineering</em> to immediately get useful results, because you can. In fact, we will dive into that later in this book. But prompt engineering alone can only get you so far. It is much more common to <em class="italic">combine prompt engineering with fine-tuning</em> to both focus your model on a target application and use all your creativity and skill to solve the actual <span class="No-Break">human problem.</span></p>
			<p>I like to think about this pretraining and<a id="_idIndexMarker491"/> fine-tuning paradigm almost like the difference between general and specialized education, whether that is in a formal undergraduate and graduate program, online coursework, or on-the-job training. General education is broad. When done well, it encompasses a broad array of skills across many disciplines. Arguably, the primary output of generalized education is critical <span class="No-Break">thinking itself.</span></p>
			<p>Specialized training is very different; it is hyper-focused on excellence in a sometimes narrow domain. Examples of specialized training include a master’s degree, a certificate, a seminar, or a boot camp. Its output is usually critical thinking applied to one <span class="No-Break">specific vertical.</span></p>
			<p>While this intuitive difference is easy to grasp, in practice, it is less obvious how to design machine learning applications and experiments that are gracefully optimized for both kinds of knowledge and keep this up to date. Personally, I would argue that a combination of pretrained and fine-tuned models presents the best solution for this to date. It is possible this will be how we continue to deal with ML for years, if not decades, <span class="No-Break">to come.</span></p>
			<p>As you should start to feel quite comfortable with by this point in the book, <em class="italic">the art of building a fantastic machine learning application or effective experiment lies in using the best of both general and specialized models</em>. Do not limit yourself to just a single model; this is not terribly different from limiting yourself to a single world view or perspective. Increasing the number of types of models you use has the possibility to increase the overall intelligence of your application. Just make sure you are phasing each experiment and sprint with clear objectives <span class="No-Break">and deliverables.</span></p>
			<p>You might use one single pretrained model, such as GPT-2, then fine-tune it to generate text in your vernacular. Or you might use a pretrained model to featurize your input text, such as in Stable Diffusion, and then pass it to a downstream model such <span class="No-Break">as KNN.</span></p>
			<p class="callout-heading">Hint</p>
			<p class="callout">This is a great way to solve an image search! Or you might use any of the following fine-tuning <span class="No-Break">regimes outlined.</span></p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor155"/>Fine-tuning a language-only model</h2>
			<p>If your model is <a id="_idIndexMarker492"/>a language-only project – something inspired by <a id="_idIndexMarker493"/>BERT or GPT – then once you’ve either finished pretraining or have reached a significant milestone (maybe a few hundred steps or so), you’ll want to fine-tune that pretra<a id="_idTextAnchor156"/>ined base model on a more specific dataset. I would start to think about which use case to apply my model to – likely wherever I have the most supervised training data. This will also likely be a part of my business that has the strongest customer impact – from customer support to search, question answering to translation. You might also explore product ideation, feature request prioritization, documentation generation, text autocompletion, and <span class="No-Break">so on.</span></p>
			<p>Collect your supervised data and follow the steps in the previous chapter about analyzing your data. I would have many notebooks comparing the datasets, running summary statistics, and comparing distributions on key characteristics. After this basic analysis, I’d run some training jobs! These could be full-fledged SageMaker training jobs, or just using your notebook instances or Studio resources. Usually, fine-tuning jobs are not huge; it’s more common to fine-tune only a few GBs or so. If you have significantly more than this, I would probably consider just adding this to my pretraining <span class="No-Break">dataset altogether.</span></p>
			<p>Your final model should combine both the output from your pretraining project and the generalized data with your target use case. If you’ve done your job right, you should have many use cases lined up, so fine-tuning will let you use your pretrained model with all <span class="No-Break">of them!</span></p>
			<p>Personally, I would use<a id="_idIndexMarker494"/> Hugging Face for a language-only project, pointing <a id="_idIndexMarker495"/>to my new pretrained model as the base object. You can follow steps from its SDK to point to different <em class="italic">downstream tasks</em>. What’s happening is that we’re using the pretrained model as the base of the neural network, then simply adding extra layers at the end to render the output tokens in a format that more closely resembles and solves the use case you want <span class="No-Break">to handle.</span></p>
			<p>You’ll get to choose all the hyperparameters again. This is another time hyperparameter tuning is extremely useful; make it your friend to easily loop through tens to hundreds of iterations of your model and find the <span class="No-Break">best version.</span></p>
			<p>Now, let’s break down different fine-tuning strategies for language that explicitly update the model parameters. Please note that the commentary that follows simply describes common scenarios for these techniques; I have no doubt that there are better ways of approaching these in <span class="No-Break">development today.</span></p>
			<table id="table001-3" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Name</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Method</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Result</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Classic fine-tuning</span></p>
						</td>
						<td class="No-Table-Style">
							<p>This takes a set of supervised text pairs and a pretrained foundation model and adds a new downstream head to the model. The new head, and possibly a few layers of the original model, <span class="No-Break">is updated.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>The new model performs well on the given task and dataset but fails outside <span class="No-Break">of this.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Instruction fine-tuning</span></p>
						</td>
						<td class="No-Table-Style">
							<p>This technique is essentially normal fine-tuning, but the crux is using a dataset with explicit instructions and the desired response provided. For a sample dataset, see Stanford’s Alpaca project <em class="italic">(1)</em>. The instructions are commands such as “tell me a story,” “create a plan,” or “summarize <span class="No-Break">an article.”</span></p>
						</td>
						<td class="No-Table-Style">
							<p>A base generative model produces arbitrary text and performs well only in few-shot learning cases with complex prompt engineering. Once the instructions have been fine-tuned, the model can respond well in zero-shot cases without any examples in the prompt itself. Naturally, humans strongly prefer this, as it is much easier and faster <span class="No-Break">to use.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Parameter-efficient </strong><span class="No-Break"><strong class="bold">fine-tuning</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">PEFT</strong></span><span class="No-Break">)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Instead of updating all the weights of the original model, PEFT-based techniques, as inspired by LoRA <em class="italic">(2)</em>, inject new trainable matrices into the original model. This makes training and storage much more efficient and cost-effective, as much as three <span class="No-Break">times so</span></p>
						</td>
						<td class="No-Table-Style">
							<p>For similar datasets, PEFT-based methods seem to meet accuracy levels of full fine-tuning, while requiring an order of magnitude less computation. The newly trained layers can be reused similarly to a classically fine-tuned model. Personally, I wonder whether this method could unlock hyperparameter tuning at scale for <span class="No-Break">foundation models!</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Domain adaptation</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Using largely unsupervised data, in language, this technique lets you continue pretraining the model. This is most relevant for focusing the performance of the model on a new domain, such as a particular industry vertical or <span class="No-Break">proprietary dataset.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>This results in an updated foundation model that should know new vocabulary and terminology based on its updated domain. It will still require task-specific fine-tuning to achieve the best performance on a <span class="No-Break">specific task.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Reinforcement learning with human </strong><span class="No-Break"><strong class="bold">feedback</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">RLHF</strong></span><span class="No-Break">)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>This technique lets you quantify human preferences on generated content at scale. The process puts multiple model responses in front of human labelers and asks them to rank them. This is used to train a reward model, which serves as the guide for a reinforcement learning procedure to train a new LLM. We discuss this in <span class="No-Break">detail shortly.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>OpenAI shows <em class="italic">(3)</em> that models trained with RLHF are consistently preferred by humans, even over instruction fine-tuning. This is because the reward model learns what a group of humans sees, on average, as better-generated content. This preference is then incorporated into an LLM <span class="No-Break">through RL.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>If you’d like to jump straight into these techniques, including how they work with examples, then head straight over to the repository. You can also jump right to <a href="B18942_15.xhtml#_idTextAnchor229"><span class="No-Break"><em class="italic">Chapter 15</em></span></a> for a deeper dive on parameter efficient fine tuning Remember, in <a href="B18942_13.xhtml#_idTextAnchor198"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, we’ll learn all about techniques that mimic learning but do so without updating any <a id="_idIndexMarker496"/>parameters in the model itself. This includes <a id="_idIndexMarker497"/>prompt-tuning, prompt engineering, prefix tuning, and more. For now, let’s learn about fine-tuning <span class="No-Break">vision-only models.</span></p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor157"/>Fine-tuning vision-only models</h2>
			<p>Vision is a completely<a id="_idIndexMarker498"/> different world than language in terms of <a id="_idIndexMarker499"/>fine-tuning. In language, it’s a somewhat reliable practice to take a large pretrained model, such as BERT or GPT, add an extra dataset, fine-tune it, and get reasonably good performance out of the box. This isn’t to say that there aren’t countless other nuances and issues in language, because there are, but the general likelihood of getting pretty good performance with simple fine-tuning <span class="No-Break">is high.</span></p>
			<p>In vision, the likelihood of getting good performance right away is not as high. You might have a model from ImageNet that you’d like to use as your base model, then pair with a different set of labeled images. If your images already look like they came from ImageNet, then you are in good shape. However, if your images are completely different, with a different style, tone, character, nuance, or mode, then it’s likely your model won’t perform as well immediately. This is an age-old problem in vision that predates <span class="No-Break">foundation models.</span></p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B18942_Figure_10_01.jpg" alt="Figure 10.1 – From Kate Saenko’s WACV Pretrain Workshop 2023 Keynote (4)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – From Kate Saenko’s WACV Pretrain Workshop 2023 Keynote (4)</p>
			<p>Leading<a id="_idIndexMarker500"/> vision researcher Kate Saenko pioneered approaches to solving this <a id="_idIndexMarker501"/>problem, which she calls <strong class="bold">distribution bias</strong>. As she <a id="_idIndexMarker502"/>identified in her first paper on this in 2010 <em class="italic">(5)</em>, the core issue is the massive gap <a id="_idIndexMarker503"/>between the <strong class="bold">domains</strong>. What happens in computer vision is that using a pretrained model, focused on one particular dataset, doesn’t translate as well to the downstream task. Even after fine-tuning the pretrained base model on a new set of labeled samples, the model is likely to simply overfit or not even learn the new <span class="No-Break">domain well.</span></p>
			<p>Kate’s work identified that, in fact, using a more recently pretrained foundation model is very helpful in overcoming this domain adaptation problem <em class="italic">(6)</em>. She found that “<em class="italic">simply using a state-of-the-art backbone outperforms existing state-of-the-art domain adaptation baselines and sets new baselines on OfficeHome and DomainNet improving by 10.7% and 5.5%</em>”. In this case, <em class="italic">backbone</em> refers to the model, here ConvNext-T, DeiT-S, <span class="No-Break">and Swin-S.</span></p>
			<p>In the same work, Kate also found that larger models tended to perform better. In the following visual, you can see that by increasing the model size by tens of millions of parameters, she was also <a id="_idIndexMarker504"/>able to<a id="_idIndexMarker505"/> <span class="No-Break">increase accuracy.</span></p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B18942_Figure_10_02.jpg" alt="Figure 10.2 – Saenko’s results on the impact of increasing model size in vision"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Saenko’s results on the impact of increasing model size in vision</p>
			<p>Now that we’ve learned about some fine-tuning regimes relating to vision-only, let’s explore fine-tuning regimes in the combination of vision <span class="No-Break">and language!</span></p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor158"/>Fine-tuning vision-language models</h2>
			<p>First, let’s recap a<a id="_idIndexMarker506"/> few interesting tasks that are unique to the explicit combination<a id="_idIndexMarker507"/> of vision and language. These include visual question-answering, text-to-image, text-to-music, and as Allie Miller likes to say, “text-to-everything”. They also include image captioning, video captioning, visual entailment, grounding, and more. You might use vision-language models in an e-commerce application to make sure the right product is on the page, or even in the film industry to generate new points on <span class="No-Break">a storyboard.</span></p>
			<p>Fine-tuning a pretrained vision-language model, at the most basic level, should follow the same pattern as we discussed for each of the other paradigms. You need a base model, then you need a set of data that follows the same labeling schema. If you’ve used the <em class="italic">Lensa</em> app, then you’ll already be somewhat familiar with fine-tuning a vision-language model! Lensa asks you to upload photos of yourself to its app. Just hypothesizing here, I would guess that it takes these photos and quickly fine-tunes Stable Diffusion on these new images of you. Then, it probably uses a prompting tool along with a content filter to send images back <span class="No-Break">to you.</span></p>
			<p>Another recent case study of vision-language fine-tuning I’m really impressed by is <em class="italic">Riffusion</em> <em class="italic">(7)</em>. As of right now, you can use their free website to listen to music generated from text, and it’s pretty good! They built an open source framework that takes audio clips and converts them into images. The images are <a id="_idIndexMarker508"/>called <strong class="bold">spectrograms</strong>, which are the result of using a <em class="italic">Short-time Fourier transform</em>, which is an approximation for converting the audio into a two-dimensional image. This then serves as a visual signature for the sound itself. Spectrograms can also be transformed back into the audio itself, producing <span class="No-Break">the sound.</span></p>
			<p>Naturally, they used short textual descriptions of the audio clips as the textual label for each image, and voila! They had a labeled dataset to fine-tune Stable Diffusion. Using the spectrograms and textual descriptions of these, they fine-tuned the model and hosted it. Now you can quite literally write a textual prompt such as “Jamaican dancehall vocals” or “Sunrise DJ Set”, and their model will generate that audio <span class="No-Break">for you!</span></p>
			<p>I love this project<a id="_idIndexMarker509"/> because the authors went a step further: they<a id="_idIndexMarker510"/> designed a novel smoothing function to seamlessly transition from one spectrogram to another. This means when you’re using their website, you can very naturally transition from one musical mode to another. All of this was made possible by using the large pretrained Stable Diffusion base model and fine-tuning it with their novel image/text dataset. For the record, there are quite a few other music generation projects, including MusicLM <em class="italic">(8)</em>, DiffusionLM <em class="italic">(9)</em>, MuseNet <em class="italic">(10)</em>, <span class="No-Break">and others.</span></p>
			<p>Now that you’ve learned about the great variety of pretraining and fine-tuning regimes, you should be getting pretty excited about identifying ways to use the model you’ve been working on training until now. Let’s learn how to compare performance with open <span class="No-Break">source models!</span></p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor159"/>Evaluating foundation models</h1>
			<p>As we’ve discussed many times<a id="_idIndexMarker511"/> in this book so far, the primary reason to engage in large-scale training is that open source models aren’t cutting it for you. Before you start your own large-scale training project, you should have already completed the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Tested an open source model on your specific <span class="No-Break">use case</span></li>
				<li>Identified <span class="No-Break">performance gaps</span></li>
				<li>Fine-tuned that same open source model on a <em class="italic">small</em> subset of <span class="No-Break">your data</span></li>
				<li>Identified <em class="italic">smaller</em> <span class="No-Break">performance gaps</span></li>
			</ol>
			<p>The point is that you should have some empirical reason to believe that the open source model solves <em class="italic">some</em> of your business problem but not <em class="italic">all</em> of it. You need to also empirically prove that small-scale fine-tuning is in the same boat; it should increase system performance but still leave room for improvement. This entire next section is about evaluating that room for improvement. Let’s try to understand how we can evaluate <span class="No-Break">foundation models.</span></p>
			<p>As you are no doubt suspecting, evaluating foundation models falls into two phases. First, we care about the pretraining performance. You want to see the pretraining loss drop, be that masked language modeling loss, causual modeling loss, diffusion loss, perplexity, FID, or anything else. Second, we care about the downstream performance. That can be classification, named entity recognition, recommendation, pure generation, question answering, chat, or anything else. We covered evaluating the pretraining loss function in earlier chapters. In the following section, we’ll mostly cover the evaluation of downstream tasks. Let’s start with some top terms in <span class="No-Break">vision models.</span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor160"/>Model evaluation metrics for vision</h2>
			<p>In vision projects, as in<a id="_idIndexMarker512"/> all machine learning, evaluation completely <a id="_idIndexMarker513"/>depends on the task at hand. Common vision tasks include image classification, object detection, classification, segmentation, facial recognition, pose estimation, segmentation maps, and more. For an image classification problem, you’ll be happy to know the primary evaluation metric tends to be accuracy! Precision and recall also continue to be relevant here, as they are with any <span class="No-Break">classification task.</span></p>
			<p>For object detection, as you can see in the figure, the question is much harder. It’s not enough to know whether the given class is in the image anywhere; you need the model to also know which part of the image includes <span class="No-Break">the object.</span></p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B18942_Figure_10_03.jpg" alt="Figure 10.3 – Intersection over union"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Intersection over union</p>
			<p>Object detection is useful in self-driving, manufacturing, security, retail, and other applications. Usually, it’s not enough to just identify an object; you want to jointly minimize the amount of incorrect pixels your box consumes while maximizing the amount of <span class="No-Break">correct pixels.</span></p>
			<p>Here, we are <a id="_idIndexMarker514"/>using a term called <strong class="bold">IOU</strong>, which literally means <strong class="bold">intersection over union</strong>. As you can see, the term corresponds to the area of the overlap of the two <a id="_idIndexMarker515"/>bounding boxes, divided by the area of the union of the two. As you might imagine, a larger IOU is better, because it means your <a id="_idIndexMarker516"/>bounding boxes are more consistent. A smaller IOU means there is still a wide degree of difference between the two, and your classifiers may not be capturing similar amounts of information. You might find this interesting if your object detector has many different classes and you want to compare these. You can also take the weighted average IOU of all classes, giving you <a id="_idIndexMarker517"/>the <strong class="bold">mean </strong><span class="No-Break"><strong class="bold">IOU</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">mIOU</strong></span><span class="No-Break">).</span></p>
			<p>Another common way of aggregating the overall performance of the many classifiers in your object detection <a id="_idIndexMarker518"/>algorithm is <strong class="bold">mAP</strong>, or <strong class="bold">mean average precision</strong>. For a single model, you’d call this the average precision, because it’s an average of the results across all classification thresholds. For multiple models, you’d take the average of each class, hence <strong class="bold">mean average </strong><span class="No-Break"><strong class="bold">precision</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">mAP</strong></span><span class="No-Break">).</span></p>
			<p>Another really interesting<a id="_idIndexMarker519"/> vision solution in the foundation model space is <strong class="bold">Segment</strong> <strong class="bold">Anything Model</strong> (<strong class="bold">SAM</strong>) by Meta <em class="italic">(9)</em>. As shown in the following figure from its work, it presents a novel task, dataset, and model to <em class="italic">enable prompt-driven mask generation</em>. A segmentation map is a helpful construct in computer vision to identify pixels of an image that belong to a certain class. In this work, SAM learns how to generate new segmentation maps from both a given image and a natural language prompt. It then isolates pixels provided in the image that solve the question posed by the natural <span class="No-Break">language prompt.</span></p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B18942_Figure_10_04.jpg" alt="Figure 10.4 – Meta’s ﻿SAM"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Meta’s SAM</p>
			<p>To <a id="_idIndexMarker520"/>evaluate the segmentation maps generated by the model, the Meta team randomly sampled 50,000 masks and asked their professional annotators to improve the quality of these masks using image-editing tools, such as “brush” and “eraser”. Then, they computed the IoU <a id="_idIndexMarker521"/>between the original and the <span class="No-Break">final map.</span></p>
			<p>Now that we’ve looked at a few evaluation examples in vision, let’s do the same <span class="No-Break">for language.</span></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor161"/>Model evaluation metrics in language</h2>
			<p>While many of the<a id="_idIndexMarker522"/> classification-type metrics still apply to <a id="_idIndexMarker523"/>language, the question of how to evaluate generated language is inherently challenging. One could argue that many disciplines within the humanities, such as literary criticism, history, and philosophy, come down to evaluating a given corpora of written text. It’s not immediately obvious how to apply all this learning to improve the outputs of large <span class="No-Break">language models.</span></p>
			<p>One attempt at providing a standardized framework for this is the <em class="italic">HELM</em> <em class="italic">(12)</em> project from Stanford’s Center for Research on<a id="_idIndexMarker524"/> Foundation Models. <strong class="bold">HELM</strong> stands for <strong class="bold">Holistic Evaluation of Language Models</strong>. It provides an incredibly rich taxonomy of multiple evaluation metrics, including accuracy, fairness, bias, toxicity, and so on, along with results from nearly 30 LLMs available today. The following is a short example of this from their work <em class="italic">(13)</em>. It standardizes metrics evaluated across models <span class="No-Break">and datasets.</span></p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B18942_Figure_10_05.jpg" alt="Figure 10.5 – Taxonomy of multiple metrics from HELM"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Taxonomy of multiple metrics from HELM</p>
			<p>The HELM ratings are open sourced and available both in a web interface <em class="italic">(14)</em> and a GitHub repository <em class="italic">(15)</em>. In many cases, when you are hunting for the best model to use as your base, HELM is a great starting point. Now, let’s explore a few more of these evaluation metrics in detail. We’ll start with translation, then move on to summarization, question answering, and finally, <span class="No-Break">pure generation.</span></p>
			<p>One of the first applications for natural language processing was translation, also known as <em class="italic">machine translation</em>. Quite literally, this means training a large language model to learn the relationships between strings you provide in pairs, such as translations across natural languages, such as English into German. One early metric used to compare the quality of the generated translations is <strong class="bold">bleu</strong>, which was proposed at the ACL conference in 2002 by a team from IBM <em class="italic">(16)</em>. They <a id="_idIndexMarker525"/>called their approach a <strong class="bold">bilingual evaluation understudy</strong>, hence <strong class="bold">bleu</strong>. Generally, this refers to comparing the precise words generated by the model, and whether or not that exact same set of words appears in the <span class="No-Break">target sentence.</span></p>
			<p>Bleu has many drawbacks, however, such as not being able to adequately handle synonyms, small variants of the same word, the importance of words, or their order. For those reasons, many practitioners use more recently developed evaluation metrics, such as rouge <em class="italic">(17)</em>. Rather than anticipating a literal translation, as bleu does, rouge anticipates a summary of a text. This counts the number of lapping sub-words, word sequences, and <span class="No-Break">word pairs.</span></p>
			<p>Evaluation in <a id="_idIndexMarker526"/>question answering is interesting because, fundamentally, you can break the problem into two parts. First, based on a provided<a id="_idIndexMarker527"/> question, usually, you want to retrieve a document that relates to that question. Years ago, this was commonly solved with term frequency/inverse document frequency terms (TF-IDF scoring), which of course was famously ousted by Google’s page rank, which up-voted pages based on how many times they were linked by pages that linked other high-quality sites. Today, the NLP start-up deepset has an interesting solution, called <strong class="bold">haystack</strong>, which <a id="_idIndexMarker528"/>provides a convenient wrapper around your own pretrained NLP models to retrieve the document most relevant to <span class="No-Break">the question.</span></p>
			<p>The second part of evaluating a question-answering system is really the quality of your rendered text as the answer. You might simply try to find the part of the original document that most relates to the question, using techniques from information retrieval. Or you might try to summarize the document, or some part of the document, that most closely resembles the question. If you have large amounts of labeled data, such as clickstream data, you can actually point exactly to the part of the document that receives the most click-through data and provide that as the answer. Obviously, this appears to be what Google <span class="No-Break">does today.</span></p>
			<p>Evaluating the quality of generated text is especially challenging. While classification and some other ML problems have an inherently objective answer, where the human label is clearly right or wrong, this really isn’t the case in literary analysis. There are many right answers, because reasonable people have different perspectives on how they interpret a given story or text. This is an example <span class="No-Break">of subjectivity.</span></p>
			<p>How do we handle this discrepancy between subjectivity and objectivity in evaluating generated text? I can think of at least three ways. First, I’m quite fond of training a discriminator. This could be a classifier trained with positive and negative samples in cases where that is accurate, such as trying to mimic a certain author’s style. You can easily fine-tune a BERT-based model with a small sample of an author’s work, as compared with random outputs from GPT-3, for example, and get a very reliable way to evaluate generated text. Another interesting project is GPTScore, which uses zero-shot prompting to test other <span class="No-Break">LLMs: </span><a href="https://arxiv.org/pdf/2302.04166.pdf"><span class="No-Break">https://arxiv.org/pdf/2302.04166.pdf</span></a></p>
			<p>You might also <a id="_idIndexMarker529"/>have humans label the responses, and then<a id="_idIndexMarker530"/> simply aggregate the labels. Personally, I am really impressed by ChatGPT’s interesting approach to this problem. They simply asked humans to rank the responses from their GPT-3 model, and then trained the model to be optimized for the best of all responses using reinforcement learning! We’ll dive into that in the last section of <span class="No-Break">this chapter.</span></p>
			<p>Now that you’ve learned about a few evaluation metrics in language, let’s explore the same in jointly trained <span class="No-Break">vision-language tasks.</span></p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor162"/>Model evaluation metrics in joint vision-language tasks</h2>
			<p>As you’ve no <a id="_idIndexMarker531"/>doubt witnessed since<a id="_idIndexMarker532"/> their release, diffusion-based models are a fascinating space to watch. These models typically are jointly trained vision-and-language models that learn how to generate images using a process <a id="_idIndexMarker533"/>called <strong class="bold">diffusion</strong>. This process learns about the relationship between the provided words and the image itself, enabling the consumer to then easily produce a new image simply by providing a new set of words. After achieving a low loss during training on the validation set, evaluation is typically done manually by the consumer offline. Most people simply guess and check, testing the model with a few different hyperparameters and ultimately just picking their favorite picture. An ambitious team might train a discriminator, similar to what I mentioned previously for evaluating <span class="No-Break">generated text.</span></p>
			<p>But what if you wanted to focus on a specific object, and simply put that object onto a different background? Or stylize that object? Or change its emotion or pose? Fortunately, now <a id="_idIndexMarker534"/>you can! Nataniel Ruiz from <a id="_idIndexMarker535"/>Boston University, while interning at<a id="_idIndexMarker536"/> Google, developed a project to do just that called <span class="No-Break"><strong class="bold">DreamBooth</strong></span><span class="No-Break"> </span><span class="No-Break"><em class="italic">(18)</em></span><span class="No-Break">.</span></p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B18942_Figure_10_06.jpg" alt="Figure 10.6 – Preserving loss with DreamBooth fine-tuning"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Preserving loss with DreamBooth fine-tuning</p>
			<p>DreamBooth accomplishes this through custom tokens and a <em class="italic">specialized loss function</em>. The loss function is<a id="_idIndexMarker537"/> called a <strong class="bold">prior preservation</strong>, and it’s built to counter the overfitting that can commonly happen in vision fine-tuning, along with the <em class="italic">language-drift</em> issue that is known to happen in language fine-tuning. I’ll spare you the mathematical details of the loss function, but if you’re curious, please feel free to read the paper directly! Generally speaking, this new loss function retains its own generated samples during the fine-tuning process and uses these during supervision. This helps it retain the prior. They found that roughly 200 epochs and just 3-5 input training images were enough to deliver excellent images as a result. You could consider this custom loss function another type of evaluation for <span class="No-Break">image-text models.</span></p>
			<p>Now that we’ve explored a large variety of evaluation methods for vision and language models, let’s learn about ways to keep humans in <span class="No-Break">the loop!</span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor163"/>Incorporating the human perspective with labeling through SageMaker Ground Truth</h2>
			<p>Obviously, a critical way to incorporate the human perspective into your work is with labeling! At AWS, we have both a low-level labeling service called <strong class="bold">Mechanical Turk</strong> (<strong class="bold">MTurk</strong>) and<a id="_idIndexMarker538"/> a more managed feature called <strong class="bold">SageMaker Ground Truth</strong>. As we’ve<a id="_idIndexMarker539"/> discussed, MTurk has already impacted the ML domain by being used to create datasets as famous as ImageNet! Personally, I’m a fan of SageMaker Ground Truth because it’s much easier to use for pre-built image labeling tasks such as object detection, image classification, and semantic segmentation. It comes with tasks for NLP, such as text classification and named entity <a id="_idIndexMarker540"/>recognition, tasks for video, and tasks for 3D <span class="No-Break">point clouds.</span></p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B18942_Figure_10_07.jpg" alt="Figure 10.7 – Manage data labeling with SageMaker Ground Truth"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Manage data labeling with SageMaker Ground Truth</p>
			<p>You can bring your own HTML frame to any arbitrary ML task and even make use of the <em class="italic">active labelling</em> <em class="italic">(19)</em> feature to dynamically train a model using records you’ve already labeled and speed up the whole labeling process. For supported built-in tasks such as image classification, semantic segmentation, object detection, and text classification, this means you will actually train an ML model on the data you’ve already labeled, then run inference against the unlabeled data. When the model is at least 80% confident in its response, it’s considered a labeled sample. When it’s not, it’s routed to the manual teams to label. Overall, this can dramatically reduce the cost of <span class="No-Break">your project.</span></p>
			<p>Another nice feature of SageMaker Ground Truth is that it automatically consolidates any discrepancies in labelers on your behalf. You can define how many people you want to label your objects, and it will look at, on average, how accurate each of those labelers is. Then, it’ll use that per-person average accuracy to consolidate the votes <span class="No-Break">per object.</span></p>
			<p>For hosted models, you can also connect them to SageMaker Ground Truth via the <em class="italic">augmented artificial intelligence</em> solution. This means you can set a trigger to route model inference responses to a team of manual labelers, via SageMaker Ground Truth, to audit the<a id="_idIndexMarker541"/> response and ensure it’s accurate and not harmful <span class="No-Break">to humans.</span></p>
			<p>Now that you have some idea of how to incorporate human labeling across your ML projects, let’s break down the method that makes <span class="No-Break">ChatGPT tick!</span></p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor164"/>Reinforcement learning from human feedback</h1>
			<p>At least two things are undeniable about ChatGPT. First, its launch was incredibly buzzy. If you follow ML topics on social and general media, you probably remember being overloaded with content about people using it for everything from writing new recipes to start-up growth plans, and from website code to Python data analysis tips. However, there’s a good reason for the buzz. It’s actually so much better in terms of performance than any other prompt-based NLP solution the world has seen before. It establishes a new state of the art in question answering, text generation, classification, and so many other domains. It’s so good, in some cases it’s even better than a basic Google search! How did they do this? <strong class="bold">RLHF</strong> is <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker542"/></span><span class="No-Break"> answer!</span></p>
			<p>While RLHF is not a new concept in and of itself, certainly the most obviously successful application of RLHF in the large language model domain is ChatGPT. The predecessor to ChatGPT was InstructGPT <em class="italic">(20)</em>, where OpenAI developed a novel framework to improve the model responses from GPT-3. Despite being 100x smaller in terms of parameters, InstructGPT actually outperforms GPT-3 in many text-generation scenarios. ChatGPT takes this a step further by adding an explicit dialogue framework to the training data. This dialogue helps maintain the context of the entire chat, referring the model back to the top data points provided by <span class="No-Break">the consumer.</span></p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B18942_Figure_10_08.jpg" alt="Figure 10.8 – Reinforcement learning from human feedback"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Reinforcement learning from human feedback</p>
			<p>Let’s break down the reinforcement <a id="_idIndexMarker543"/>learning! To simplify the process, we can break it down into three <span class="No-Break">key steps:</span></p>
			<ol>
				<li>First, collect data from a pretrained model hosted live with humans. From actual questions provided by humans, OpenAI sends these same questions to a team of manual labelers. This set of labeled data is then used to fine-tune the large GPT-3 type model – in this case, it was <span class="No-Break">GPT-3.5 specifically.</span></li>
				<li>Next, take the fine-tuned model and submit prompts. After this, OpenAI asks the human labelers to <em class="italic">simply rank the outputs</em>. I love this approach because it does a good job of bridging the inherently subjective task of labeling freeform generated tasks, with the objective goal of producing a better ML model. When humans rank the responses from best to worst, it avoids the subjective question <em class="italic">“Is this good or not?”</em> and replaces it with an objective question, <em class="italic">“Which is your favourite?”</em> Armed with these ranked responses, it trains a <em class="italic">reward model</em>, which is just an ML model that takes a given prompt and rates it based on human responses. I’d imagine this is a regression model, though classification would <span class="No-Break">also work.</span></li>
				<li>Finally, OpenAI uses a reinforcement learning algorithm, PPO specifically, to connect the dots. It generates a prompt response from the LLM, and in the reinforcement learning literature, we’d call that <em class="italic">takes an action</em>. The reward for this response is produced by running it against the reward model we just trained in the previous step. That reward is used to update the PPO algorithm, which in turn ensures that the next response it provides is closer to the highest reward it <span class="No-Break">can get.</span></li>
			</ol>
			<p>And that is <a id="_idIndexMarker544"/>RLHF in a nutshell! I think it’s a brilliant way of integrating nuanced human preferences with machine learning models, and I can’t wait to try it in my <span class="No-Break">next project.</span></p>
			<p>Before we move on to the next chapter, in which we will detect and mitigate bias, let’s do a quick recap of all the concepts we’ve covered in this chapter. Hint: there are <span class="No-Break">a lot!</span></p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor165"/>Summary</h1>
			<p>The goal of this chapter was to give you a better understanding of fine-tuning and evaluating ML models overall, comparing them with open source options, and ultimately keeping humans in <span class="No-Break">the loop.</span></p>
			<p>We started with a recap of fine-tuning for language, text, and everything in between, discussing the benefits of both general and specialized knowledge. We learned about fine-tuning a language-only model, and how generally this is possible with even a small amount of data. We also talked about fine-tuning vision-only models, and how generally it is much more likely to overfit, making it a challenging proposition. We looked at fine-tuning jointly trained vision-language models, including Stable Diffusion and an interesting open source project called Riffusion. We talked about comparing performance with off-the-shelf public models. We learned about model evaluation metrics for vision specifically, along with language, and the emerging joint vision-language space. We also looked at a variety of ways to keep humans in the loop across this entire spectrum, culminating in a discussion about RLHF as used <span class="No-Break">in ChatGPT!</span></p>
			<p>Now, you’re ready to learn about detecting and mitigating bias in your ML projects in the <span class="No-Break">next chapter.</span></p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor166"/>References</h1>
			<ol>
				<li><span class="No-Break">tatsu-lab/stanford_alpaca: </span><a href="https://github.com/tatsu-lab/stanford_alpaca"><span class="No-Break">https://github.com/tatsu-lab/stanford_alpaca</span></a></li>
				<li>LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE <span class="No-Break">MODELS: </span><a href="https://arxiv.org/pdf/2106.09685.pdf"><span class="No-Break">https://arxiv.org/pdf/2106.09685.pdf</span></a></li>
				<li>Training language models to follow instructions with human <span class="No-Break">feedback: </span><a href="https://arxiv.org/pdf/2203.02155.pdf"><span class="No-Break">https://arxiv.org/pdf/2203.02155.pdf</span></a></li>
				<li>Keynote <span class="No-Break">Speakers: </span><a href="https://sites.google.com/view/wacv2023-workshop/speakers"><span class="No-Break">https://sites.google.com/view/wacv2023-workshop/speakers</span></a></li>
				<li>Adapting Visual Category Models to New <span class="No-Break">Domains: </span><a href="https://link.springer.com/content/pdf/10.1007/978-3-642-15561-1_16.pdf?pdf=inline%20link"><span class="No-Break">https://link.springer.com/content/pdf/10.1007/978-3-642-15561-1_16.pdf?pdf=inline%20link</span></a></li>
				<li>A Broad Study of Pre-training for Domain Generalization and <span class="No-Break">Adaptation: </span><a href="https://arxiv.org/pdf/2203.11819.pdf"><span class="No-Break">https://arxiv.org/pdf/2203.11819.pdf</span></a></li>
				<li><span class="No-Break">RIFFUSION: </span><a href="https://www.riffusion.com/about"><span class="No-Break">https://www.riffusion.com/about</span></a></li>
				<li>MusicLM: Generating Music From <span class="No-Break">Text: </span><a href="https://arxiv.org/pdf/2301.11325.pdf"><span class="No-Break">https://arxiv.org/pdf/2301.11325.pdf</span></a></li>
				<li>Diffusion-LM on Symbolic Music Generation with <span class="No-Break">Controllability: </span><a href="http://cs230.stanford.edu/projects_fall_2022/reports/16.pdf"><span class="No-Break">http://cs230.stanford.edu/projects_fall_2022/reports/16.pdf</span></a></li>
				<li><span class="No-Break">OpenAI: </span><a href="https://openai.com/research/musenet"><span class="No-Break">https://openai.com/research/musenet</span></a></li>
				<li>Segment <span class="No-Break">Anything </span><a href="https://arxiv.org/pdf/2304.02643.pdf"><span class="No-Break">https://arxiv.org/pdf/2304.02643.pdf</span></a></li>
				<li><span class="No-Break">HELM: </span><a href="https://crfm.stanford.edu/helm/latest/"><span class="No-Break">https://crfm.stanford.edu/helm/latest/</span></a></li>
				<li>Holistic Evaluation of Language <span class="No-Break">Models: </span><a href="https://arxiv.org/pdf/2211.09110.pdf"><span class="No-Break">https://arxiv.org/pdf/2211.09110.pdf</span></a></li>
				<li><span class="No-Break">HELM: </span><a href="https://crfm.stanford.edu/helm/latest/?groups=1"><span class="No-Break">https://crfm.stanford.edu/helm/latest/?groups=1</span></a></li>
				<li><span class="No-Break">stanford-crfm/helm: </span><a href="https://github.com/stanford-crfm/helm"><span class="No-Break">https://github.com/stanford-crfm/helm</span></a></li>
				<li>BLEU: a Method for Automatic Evaluation of Machine <span class="No-Break">Translation: </span><a href="https://aclanthology.org/P02-1040.pdf"><span class="No-Break">https://aclanthology.org/P02-1040.pdf</span></a></li>
				<li>ROUGE: A Package for Automatic Evaluation of <span class="No-Break">Summaries: </span><a href="https://aclanthology.org/W04-1013.pdf"><span class="No-Break">https://aclanthology.org/W04-1013.pdf</span></a></li>
				<li>DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven <span class="No-Break">Generation: </span><a href="https://arxiv.org/pdf/2208.12242.pdf"><span class="No-Break">https://arxiv.org/pdf/2208.12242.pdf</span></a></li>
				<li>Automate Data <span class="No-Break">Labeling: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-automated-labeling.html</span></a></li>
				<li>Training language models to follow instructions with human <span class="No-Break">feedback: </span><a href="https://arxiv.org/pdf/2203.02155.pdf"><span class="No-Break">https://arxiv.org/pdf/2203.02155.pdf</span></a></li>
			</ol>
		</div>
	</body></html>