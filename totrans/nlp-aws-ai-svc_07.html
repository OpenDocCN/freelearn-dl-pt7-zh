<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer113">
			<h1 id="_idParaDest-71"><em class="italic"><a id="_idTextAnchor074"/>Chapter 5</em>: Creating NLP Search</h1>
			<p>In the previous chapters, you were introduced to Amazon Textract for extracting text from documents, and Amazon Comprehend to extract insights with no prior <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) experience as a prerequisite. In the last chapter, we showed you how you can combine these features together to solve a real-world use case for document automation by giving an example of loan processing.</p>
			<p>In this chapter, we will use the Amazon Textract and Amazon Comprehend services to show you how you can quickly set up an intelligent search solution with the integration of powerful elements, such as <strong class="bold">Amazon Elasticsearch</strong>, which is a managed service to set up search and log analytics, and <strong class="bold">Amazon Kendra</strong>, which is an intelligent managed search solution powered by ML for natural language search.</p>
			<p>We will cover the following topics in this chapter:</p>
			<ul>
				<li>Going over search use cases and choices for search solutions</li>
				<li>Building a search solution for scanned images using Amazon Elasticsearch</li>
				<li>Setting up an enterprise search solution using Amazon Kendra</li>
			</ul>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor075"/>Technical requirements</h1>
			<p>For this chapter, you will need access to an AWS account. Before getting started we recommend that you create an AWS account by going through these steps here:</p>
			<ol>
				<li>Open <a href="https://portal.aws.amazon.com/billing/signup">https://portal.aws.amazon.com/billing/signup</a>.</li>
				<li>Please go through and execute the steps provided on the web page to sign up.</li>
				<li>Log in to your AWS account when prompted in the sections.</li>
			</ol>
			<p>The Python code and sample datasets for the Amazon Textract examples are provided on the book's GitHub repo at <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2005">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2005</a>.</p>
			<p>Check out the following video to see the Code in Action at <a href="https://bit.ly/3nygP5S">https://bit.ly/3nygP5S</a>.</p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor076"/>Creating NLP-powered smart search indexes</h1>
			<p>Every organization has lots of documents in the form of paper and in their archives too. The challenge <a id="_idIndexMarker317"/>is that these documents lie mostly in separate silos and not all in one place. So, for these organizations to make a business decision based on the hidden information in their siloed documents is extremely challenging. Some approaches these organizations take to make their documents searchable is putting the documents in a data lake. However, extracting meaningful information from these documents is another challenge as it would require a lot of NLP expertise, ML skills, and infrastructure to set that up. Even if you were able to extract insights from these documents, another challenge will then be setting up a scalable search solution.</p>
			<p>In this section, we will address these challenges by using the AWS AI services we introduced in previous chapters and then talk about how they can be used to set up a centralized document store.</p>
			<p>Once all the documents are in a centralized storage service such as Amazon S3, which is a scalable and durable object store similar to Dropbox, we can use <em class="italic">Amazon Textract</em> as covered in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a><em class="italic">,</em> <em class="italic">Introducing Amazon Textract,</em> to extract text from these documents, and use <em class="italic">Amazon Comprehend</em> as covered in <a href="B17528_03_Final_SB_ePub.xhtml#_idTextAnchor049"><em class="italic">Chapter 3</em></a><em class="italic">,</em> <em class="italic">Introducing Amazon Comprehend,</em> to extract NLP-based insights such as entities, keywords, sentiments, and more. Moreover, we can then quickly index the insights and the text and send it to Amazon Elasticsearch or Amazon Kendra to set up a smart search solution.</p>
			<p>The following diagram shows the architecture we will cover in this section:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="Images/B17528_05_01.jpg" alt="Figure 5.1 – Creating an NLP-powered search index&#13;&#10;&#13;&#10;" width="1119" height="446"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Creating an NLP-powered search index</p>
			<p>In <em class="italic">Figure 5.1</em>, you can <a id="_idIndexMarker318"/>see the two options we have to build a search index. The options are as follows:</p>
			<ol>
				<li value="1">Using Amazon Elasticsearch to build a search on top of your document processing pipeline with Amazon Textract and Amazon Comprehend</li>
				<li>Using Amazon Kendra to build a serverless intelligent search on top of your existing document processing pipeline with Amazon Textract and Amazon Comprehend</li>
			</ol>
			<p>If you are <a id="_idIndexMarker319"/>looking for a natural language-based search solution powered by ML where you can ask human-like questions rather than searching for keywords, you can choose Amazon Kendra for the search, as Amazon Kendra is an AWS AI service powered by ML. Amazon Kendra offers natural language search functionality and will provide you with NLP-based answers, meaning human-like contextual answers. For example, imagine you are setting up the search function on your IT support documents in Salesforce. Using Amazon Kendra you can ask direct questions such as <em class="italic">"where is the IT desk located?"</em> and Amazon Kendra will give you an exact response, such as "<em class="italic">the sixth floor</em>," whereas in Amazon Elasticsearch you can only perform keyword-based search.</p>
			<p>Moreover, you can also integrate Amazon Kendra into Amazon Lex, which is a service to create chatbots. You can deploy a smart search chatbot on your website powered by Amazon Lex and Amazon Kendra. Also, Amazon Kendra comes with a lot of connectors to discover and index your data for search, including Amazon S3, OneDrive, Google Drive, Salesforce, relational databases such as RDS, and many more supported by third-party vendors.</p>
			<p>You can set up a search on many different interesting use cases, for example, for financial analysts searching for financial events, as they have to scroll through tons of SEC filing reports and look for meaningful financial entities such as mergers and acquisitions. Using the proposed pipeline along with Amazon Comprehend Events can easily reduce the time and noise while scrolling through these documents and update their financial models in case of any financial events such as mergers or acquisitions.</p>
			<p>For healthcare companies, they can use the set of services and options offered by Amazon Comprehend Medical to create a smart search for healthcare data, where a doctor can log in and search for relevant keywords or information from the centralized patient data in Amazon HealthLake. We will cover more on this use case in this chapter.</p>
			<p>We all <a id="_idIndexMarker320"/>know finding jobs is extremely difficult. It's harder even for talent acquisition companies hunting for good candidates to search for relevant skills across thousands of resumes. You can use the proposed solution to set up a resume processing pipeline where you can upload the resumes of various candidates in Amazon S3 and search for relevant skills based on the jobs you are looking for.</p>
			<p>In this section, we covered two options with which to set up smart search indexes. In the next section, we will show you how you can set up this architecture to create an NLP-powered <a id="_idIndexMarker321"/>search application where <strong class="bold">Human Resources</strong> (<strong class="bold">HR</strong>) admin users can quickly upload candidates' scanned resumes and other folks can log in and search for relevant skill sets based on open job positions.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor077"/>Building a search solution for scanned images using Amazon Elasticsearch</h1>
			<p>In the previous chapters, we spoke about how you can use Amazon Lambda functions to create <a id="_idIndexMarker322"/>a serverless application. In this section, we will walk you through the following architecture to set up a scanned image-based search solution by calling the Amazon Textract <a id="_idIndexMarker323"/>and Amazon Comprehend APIs using an Amazon Lambda function. We are going to use Amazon <a id="_idIndexMarker324"/>Elasticsearch for this use case. However, you can also replace Amazon Elasticsearch with Amazon Kendra to create an ML-based search solution where you can use natural language to ask questions while searching.</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="Images/B17528_05_02.jpg" alt="Figure 5.2 – Building NLP search using Amazon Elasticsearch&#13;&#10;" width="1157" height="648"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 5.2 – Building NLP search using Amazon Elasticsearch</p>
			<p>The AWS service <a id="_idIndexMarker325"/>used in the previous architecture is <strong class="bold">Amazon Cognito</strong> to set up the login for your backend users.</p>
			<p>Amazon S3 is used for centralized storage. Amazon Lambda functions are used as serverless event <a id="_idIndexMarker326"/>triggers when the scanned resumes are uploaded to Amazon S3, and then we <a id="_idIndexMarker327"/>use both Amazon Textract and Amazon Comprehend to extract text and insights such as key phrases and entities. Then we index everything into Amazon Elasticsearch. Your end users can log in through Cognito, and will access Amazon Elasticsearch through a Kibana dashboard that comes integrated with Amazon Elasticsearch for visualization.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor078"/>Prerequisites</h2>
			<p>We will use <a id="_idIndexMarker328"/>an AWS CloudFormation template to spin up the resources needed for this chapter. CloudFormation templates are scripts written <a id="_idIndexMarker329"/>in YAML or JSON format to spin up resources or <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>). AWS CloudFormation templates write IaC and set all the necessary permissions for you:</p>
			<ol>
				<li value="1">Click <a href="https://forindexing.s3.eu-west-1.amazonaws.com/template-export-textract.yml">https://forindexing.s3.eu-west-1.amazonaws.com/template-export-textract.yml</a> to download and deploy an AWS CloudFormation template.<div id="_idContainer083" class="IMG---Figure"><img src="Images/B17528_05_03.jpg" alt="Figure 5.3 – CloudFormation template stack &#13;&#10;" width="1649" height="953"/></div><p class="figure-caption"> </p><p class="figure-caption">Figure 5.3 – CloudFormation template stack </p></li>
				<li>Scroll down to <strong class="bold">Parameters</strong>, enter your email address in the relevant field, and enter <strong class="source-inline">documentsearchapp</strong> for <strong class="bold">DOMAINNAME</strong> as shown in the following screenshot:<div id="_idContainer084" class="IMG---Figure"><img src="Images/B17528_05_04.jpg" alt="Figure 5.4 – Enter parameters&#13;&#10;" width="1177" height="352"/></div><p class="figure-caption">Figure 5.4 – Enter parameters</p></li>
				<li>Scroll down <a id="_idIndexMarker330"/>and check all three acknowledgments under <strong class="bold">Capabilities and transforms</strong>, then click <strong class="bold">Create stack</strong>.<div id="_idContainer085" class="IMG---Figure"><img src="Images/B17528_05_05.jpg" alt="Figure 5.5 – The Capabilities and transforms section&#13;&#10;" width="1382" height="486"/></div><p class="figure-caption">Figure 5.5 – The Capabilities and transforms section</p></li>
				<li>You will see your stack creation in progress. Wait till it's completed as shown in the following screenshot – you can refresh to see the changing status. It might take 20 minutes to deploy this stack so go grab a quick coffee:<div id="_idContainer086" class="IMG---Figure"><img src="Images/B17528_05_06.jpg" alt="Figure 5.6 – CloudFormation resources creation complete" width="1342" height="479"/></div><p class="figure-caption">Figure 5.6 – CloudFormation resources creation complete</p><p class="callout-heading">Note:</p><p class="callout">You will get <a id="_idIndexMarker331"/>an email with the login details to Cognito while your stack is being created. Make sure you check the same email you provided while creating this stack. An admin can add multiple users' email addresses through the Amazon Cognito console once it's deployed. Those emails can be sent to end users for logging in to the system once the resumes' data has been uploaded to Amazon S3.</p></li>
				<li>Go to the <strong class="bold">Outputs</strong> tab, and scroll down to the <strong class="bold">Outputs</strong> section.<div id="_idContainer087" class="IMG---Figure"><img src="Images/B17528_05_07.jpg" alt="Figure 5.7 – CloudFormation outputs&#13;&#10;" width="1348" height="733"/></div><p class="figure-caption">Figure 5.7 – CloudFormation outputs</p></li>
				<li>Copy the values for <strong class="bold">S3KeyPhraseBucket</strong> and <strong class="bold">KibanaLoginURL</strong> from the <strong class="bold">Value</strong> section. We are going to use these links for this section while walking through this app.</li>
			</ol>
			<p>Now you have <a id="_idIndexMarker332"/>set up up the infrastructure, including an Amazon S3 bucket, Lambda functions, the Cognito login, Kibana, and the Amazon Elasticsearch cluster using CloudFormation. You have the output from CloudFormation for your S3 bucket and Kibana dashboard login URLs. In the next section, we will walk you through how you can upload scanned images to interact with this application as an admin user.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor079"/>Uploading documents to Amazon S3</h2>
			<p>We'll <a id="_idIndexMarker333"/>start with the following steps for uploading documents to Amazon S3:</p>
			<ol>
				<li value="1">Click on the S3 link copied from the CloudFormation template output in the previous section. Then download the sample resume at <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2005/resume_sample.PNG">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2005/resume_sample.PNG</a>, and upload it in S3 by clicking on the <strong class="bold">Upload</strong> button followed by<strong class="bold"> Add files</strong>.<div id="_idContainer088" class="IMG---Figure"><img src="Images/B17528_05_08.jpg" alt="Figure 5.8 – Scanned image in Amazon S3&#13;&#10;" width="1650" height="497"/></div><p class="figure-caption">Figure 5.8 – Scanned image in Amazon S3</p></li>
				<li>This <a id="_idIndexMarker334"/>upload triggers an Amazon S3 event notification to the AWS Lambda function. To check that, go to the <strong class="bold">Properties</strong> tab and then scroll down to <strong class="bold">Event notifications</strong> as shown in the following screenshot:<div id="_idContainer089" class="IMG---Figure"><img src="Images/B17528_05_09.jpg" alt="Figure 5.9 – S3 event notifications to notify the AWS Lambda function&#13;&#10;" width="1370" height="256"/></div><p class="figure-caption">Figure 5.9 – S3 event notifications to notify the AWS Lambda function</p></li>
				<li>Click on the Lambda function link shown under <strong class="bold">Destination</strong>. We will inspect this Lambda function in the next section.</li>
			</ol>
			<p>We have uploaded the sample scanned resume to Amazon S3, and also showed you where you can find the S3 event notifications that trigger a Lambda function. In the next section, let's explore what is happening in the Lambda function.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor080"/>Inspecting the AWS Lambda function</h2>
			<p>In this <a id="_idIndexMarker335"/>section, we will inspect the code blocks of AWS Lambda and the API calls made to Amazon Textract and Amazon Comprehend along with Amazon Elasticsearch.</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="Images/B17528_05_10.jpg" alt="Figure 5.10 – AWS Lambda function&#13;&#10;" width="1559" height="641"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10 – AWS Lambda function</p>
			<p>The deployment code is too large for this function to show up in this AWS Lambda console. You can access the code through through the following GitHub repo instead, at <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2005/lambda/index.py">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2005/lambda/index.py</a>:</p>
			<ol>
				<li value="1">First, we are getting the files through Amazon S3 events as shown in the following code block from the main Lambda handler. In Lambda, all code blocks are executed from this main handler. The <strong class="source-inline">handler</strong> method is invoked by Lambda for each function invocation and acts as an entry point. The code outside the handler contains functions that can be called from the main handler and some global variables:<p class="source-code">def handler(event, context):</p><p class="source-code">bucket = event['Records'][0]['s3']['bucket']['name']</p><p class="source-code">key = unquote_plus(event['Records'][0]['s3']['object']<a id="_idTextAnchor081"/>['key'])</p></li>
				<li>The following code downloads the file from Amazon S3 to process it with Textract and Comprehend:<p class="source-code">       s3.Bucket(bucket).download_file(Key=key,Filename='/tmp/{}')</p><p class="source-code">        with open('/tmp/{}', 'rb') as document:</p><p class="source-code">            imageBytes = bytearray(document.read())</p><p class="source-code">        print("Object downloaded")</p></li>
				<li>After <a id="_idIndexMarker336"/>getting the objects or scanned resumes from S3 events and reading through a Lambda function, we will call the Amazon Textract AnalyzeDocument API, a real-time API to extract the text, using the following code:<p class="source-code">response = textract.analyze_document(Document={'Bytes': imageBytes},FeatureTypes=["TABLES", "FORMS"])</p><p class="source-code">document = Document(response)</p></li>
				<li>We will parse the response to extract the lines of text to be sent to Amazon Comprehend:<p class="source-code"> blocks=response['Blocks']</p><p class="source-code">        for block in blocks:</p><p class="source-code">            if block['BlockType'] == 'LINE':</p><p class="source-code">                 text += block['Text']+"\n"</p><p class="source-code">          print(text)</p></li>
				<li>Once we have extracted text, we will call the Comprehend Keyphrase API by putting it in a list variable to be indexed later:<p class="callout-heading">Note:</p><p class="callout">Comprehend sync APIs allow up to 5,000 characters as input so make sure your text is not more than 5,000 characters long.</p><p class="source-code">keyphrase_response = comprehend.detect_key_phrases(Text=text, LanguageCode='en')</p><p class="source-code">KeyPhraseList=keyphrase_response.get("KeyPhrases")</p><p class="source-code"> for s in KeyPhraseList:</p><p class="source-code">                            textvalues.append(s.get("Text")</p></li>
				<li>Now we <a id="_idIndexMarker337"/>will extract entities using the Comprehend DetectEntities API and save it in a map data structure variable to be indexed later:<p class="source-code">detect_entity= comprehend.detect_entities(Text=text, LanguageCode='en')</p><p class="source-code">EntityList=detect_entity.get("Entities")</p><p class="source-code">for s in EntityList:</p><p class="source-code">                                         textvalues_entity.update([(s.get("Type").strip('\t\n\r'),s.get("Text").strip('\t\n\r'))]</p></li>
				<li>Now we will create an Amazon S3 URL to be indexed:<p class="source-code">s3url='https://s3.console.aws.amazon.com/s3/object/'+bucket+'/'+key+'?region='+region</p></li>
				<li>We have the text, keyphrases, and entities, as well as the S3 link of the uploaded document. Now we will index it all and upload it in Elasticsearch:<p class="source-code">searchdata={'s3link':s3url,'KeyPhrases':textvalues,'Entity':textvalues_entity,'text':text, 'table':table, 'forms':forms}</p><p class="source-code">print(searchdata)</p><p class="source-code">print("connecting to ES")</p><p class="source-code">es=connectES()</p><p class="source-code">es.index(index="document", doc_type="_doc", body=searchdata)</p><p class="callout-heading">Note:</p><p class="callout">In case the resumes have tables or forms, we have prepared to index them as well. Moreover, this solution can also be used for <strong class="bold">invoice search.</strong></p></li>
			</ol>
			<p>In this section, we walked you through how you can extract text and insights from the <a id="_idIndexMarker338"/>documents uploaded to Amazon S3. We also indexed the data into Amazon Elasticsearch. In the next section, we will walk you through how you can log in to Kibana using your admin login email setup while creating CloudFormation templates and visualize the data in the Kibana dashboard.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor082"/>Searching for and discovering data in the Kibana console</h2>
			<p>In this section, we will cover how you can sign up to Kibana through Amazon Cognito by using the <a id="_idIndexMarker339"/>email you entered as the admin while deploying the resources through AWS CloudFormation. Then <a id="_idIndexMarker340"/>we will walk you through how you can set <a id="_idIndexMarker341"/>up your index in Kibana. We will cover how you <a id="_idIndexMarker342"/>can discover and search the data in the Kibana dashboard based on entity, keyword, and table filters from Amazon Comprehend. Lastly, you can download the searched resume link from Amazon S3.</p>
			<p>We will cover walkthroughs including signing up to the Kibana console, making the index discoverable for the search functionality, and searching for insights in Kibana.</p>
			<h3>Signing up to the Kibana console</h3>
			<p>In these <a id="_idIndexMarker343"/>steps, we will walk you through how you can log in to Kibana using the CloudFormation-generated output link:</p>
			<ol>
				<li value="1">Click on the Kibana login link you got from the CloudFormation output as shown in the following screenshot:<div id="_idContainer091" class="IMG---Figure"><img src="Images/B17528_05_11.jpg" alt="Figure 5.11 – CloudFormation output – Kibana URL&#13;&#10;" width="1336" height="563"/></div><p class="figure-caption">Figure 5.11 – CloudFormation output – Kibana URL</p></li>
				<li>This link <a id="_idIndexMarker344"/>will redirect you to this console:<div id="_idContainer092" class="IMG---Figure"><img src="Images/B17528_05_12.jpg" alt="Figure 5.12 – Kibana sign-in dialog&#13;&#10;" width="511" height="407"/></div><p class="figure-caption">Figure 5.12 – Kibana sign-in dialog</p><p class="callout-heading">Note:</p><p class="callout">You can sign up additional end users using the <strong class="bold">Sign up</strong> button shown in the previous screenshot.</p></li>
				<li>You should have got an email with a username and temporary password – enter those details in the preceding dialog, and click on <strong class="bold">Sign in</strong>.<div id="_idContainer093" class="IMG---Figure"><img src="Images/B17528_05_13.jpg" alt="Figure 5.13 – Verification and password login email&#13;&#10;" width="1374" height="289"/></div><p class="figure-caption">Figure 5.13 – Verification and password login email</p></li>
				<li>It will ask <a id="_idIndexMarker345"/>you to change your password the first time you sign in. After changing your password, you will be redirected to the Kibana console.</li>
			</ol>
			<p>We have covered how to sign up for Kibana. In the next section, we will walk you through setting up the index in Kibana.</p>
			<h3>Making the index discoverable for the search functionality</h3>
			<p>In this <a id="_idIndexMarker346"/>section, we will walk you through setting up an index in Kibana for searching:</p>
			<ol>
				<li value="1">Click on <strong class="bold">Discover</strong> when you reach the Kibana console and we will walk you through setting up your index in Kibana.<div id="_idContainer094" class="IMG---Figure"><img src="Images/B17528_05_14.jpg" alt="Figure 5.14 – Kibana Create index pattern page" width="1650" height="753"/></div><p class="figure-caption">Figure 5.14 – Kibana Create index pattern page</p></li>
				<li>Enter <strong class="source-inline">document</strong> in <a id="_idIndexMarker347"/>the <strong class="bold">Index pattern</strong> field, as shown in the following screenshot, then click <strong class="bold">Next step</strong>:<div id="_idContainer095" class="IMG---Figure"><img src="Images/B17528_05_15.jpg" alt="Figure 5.15 – Define index pattern&#13;&#10;" width="1354" height="603"/></div><p class="figure-caption">Figure 5.15 – Define index pattern</p></li>
				<li>Click on <strong class="bold">Create index pattern</strong>. This will make your Elasticsearch index discoverable.</li>
			</ol>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="Images/B17528_05_16.jpg" alt="Figure 5.16 – Create index pattern&#13;&#10;" width="1342" height="469"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.16 – Create index pattern</p>
			<p>We <a id="_idIndexMarker348"/>have created an index. Now we will start searching for insights.</p>
			<h3>Searching for insights in Kibana</h3>
			<p>In this <a id="_idIndexMarker349"/>section, we will walk you through searching for insights in Kibana:</p>
			<ol>
				<li value="1">Click on <strong class="bold">Discover</strong> and on the left-hand side you will find entities and key phrases that can be added to your search filters under <strong class="bold">Available Fields</strong>.<div id="_idContainer097" class="IMG---Figure"><img src="Images/B17528_05_17.jpg" alt="Figure 5.17 – Kibana's Discover dashboard (a)&#13;&#10;" width="1646" height="710"/></div><p class="figure-caption">Figure 5.17 – Kibana's Discover dashboard (a)</p><p>Let's look <a id="_idIndexMarker350"/>at another output shown in the following screenshot:</p><div id="_idContainer098" class="IMG---Figure"><img src="Images/B17528_05_18.jpg" alt="Figure 5.18 – Kibana's Discover dashboard (b)" width="1650" height="712"/></div><p class="figure-caption">Figure 5.18 – Kibana's Discover dashboard (b)</p></li>
				<li><strong class="bold">Entity search</strong>: Let's search for a candidate by date and title by adding the available fields of <strong class="bold">Entity.TITLE</strong> and <strong class="bold">Entity.dATE</strong> for a quick search. You can click on <strong class="bold">Add a filter</strong> and these filters will get added as seen in the following screenshot. You can see that it found someone with the big data analytics title in July 2017:<div id="_idContainer099" class="IMG---Figure"><img src="Images/B17528_05_19.jpg" alt="Figure 5.19 – Adding an entity filter to selected fields&#13;&#10;" width="1338" height="357"/></div><p class="figure-caption">Figure 5.19 – Adding an entity filter to selected fields</p></li>
				<li><strong class="bold">Keyword search using the keyphrases and table</strong>: Add the <strong class="bold">KeyPhrases</strong> and <strong class="bold">table filters</strong> from <strong class="bold">available fields</strong> and you will get a table summary of <a id="_idIndexMarker351"/>all the skills you are looking for, along with keyphrases about the candidate.<div id="_idContainer100" class="IMG---Figure"><img src="Images/B17528_05_20.jpg" alt="Figure 5.20 – Keyword and table fields search" width="1618" height="906"/></div><p class="figure-caption">Figure 5.20 – Keyword and table fields search</p></li>
				<li><strong class="bold">Doing a generic keyword search</strong>: Now I am looking for someone with both Amazon SageMaker and MySQL skills. Let's enter <strong class="source-inline">Amazon Sagemaker and MySQL</strong> in the search field and see whether we have a candidate resume matching our needs. We are able to find a candidate resume with both these skills as highlighted in the following screenshot:<div id="_idContainer101" class="IMG---Figure"><img src="Images/B17528_05_21.jpg" alt="Figure 5.21 – Keyword search with AND condition&#13;&#10;" width="1619" height="445"/></div><p class="figure-caption">Figure 5.21 – Keyword search with AND condition</p></li>
				<li><strong class="bold">Downloading the resume of the candidate matched</strong>: We can download the resume <a id="_idIndexMarker352"/>of the matched candidate by adding an S3 link on <strong class="bold">selected fields</strong> as follows:</li>
			</ol>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="Images/B17528_05_22.jpg" alt="Figure 5.22 – S3 link to download the resume&#13;&#10;" width="1498" height="317"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.22 – S3 link to download the resume</p>
			<p>In this section, we gave you an architecture overview of the search solution for scanned images where an admin user uploads the scanned documents in Amazon S3, and then showed how to sign up for the Kibana dashboard and search for keywords to gain meaningful insights from the scanned documents.</p>
			<p>We walked you through the steps to set up the architecture using AWS CloudFormation template one-click deploy, and you can check the <em class="italic">Further reading</em> section to learn more about how to create these templates. We also showed how you can interact with this application by uploading some sample documents. We guided you on how to set up the Kibana dashboard and provide some sample queries to gain insights from the keywords and entities as filters.</p>
			<p>In the <a id="_idIndexMarker353"/>next section, we will explore a Kendra-powered search solution. Let's get started exploring Amazon Kendra and what you can uncover by using it to power Textract and Comprehend in your document processing workflows.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor083"/>Setting up an enterprise search solution using Amazon Kendra</h1>
			<p>In this <a id="_idIndexMarker354"/>section, we will cover <a id="_idIndexMarker355"/>how you can quickly create an end-to-end serverless document search application using Amazon Kendra.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor084"/>In this <a id="_idIndexMarker356"/>section, we will cover the steps to get started.</h2>
			<h3>Git cloning the notebook</h3>
			<p>We will walk through the steps to git clone the notebook and show code samples to set up the kendra based search architecture using simple boto3 APIs.</p>
			<ol>
				<li value="1">In the <a id="_idIndexMarker357"/>SageMaker Jupyter notebook you set up in the previous chapters, Git clone <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/</a>.</li>
				<li>Go to <strong class="source-inline">Chapter 05/Ch05-Kendra Search.ipynb</strong> and start running the notebook.<p class="callout-heading">Note:</p><p class="callout">Please add Kendra IAM access to the SageMaker notebook IAM role so that you can call Kendra APIs through this notebook. In previous chapters, you already added IAM access to Amazon Comprehend and Textract APIs from the SageMaker notebook.</p></li>
			</ol>
			<h3>Creating an Amazon S3 bucket</h3>
			<p>We will show you how you can create a Amazon S3 bucket. We will use this bucket as a Kendra datasource and also to store extracted data from Amazon Textract.</p>
			<ol>
				<li value="1">Create <a id="_idIndexMarker358"/>an Amazon S3 bucket by going to the Amazon S3 console at <a href="https://s3.console.aws.amazon.com/s3/home?region=us-east-1">https://s3.console.aws.amazon.com/s3/home?region=us-east-1</a>.<span class="hidden"> </span></li>
				<li>Click on the <strong class="bold">Create bucket</strong> button and enter any bucket name as shown in the following screenshot:<div id="_idContainer103" class="IMG---Figure"><img src="Images/B17528_05_23.jpg" alt="Figure 5.23 – Create an Amazon S3 bucket&#13;&#10;" width="670" height="429"/></div><p class="figure-caption">Figure 5.23 – Create an Amazon S3 bucket</p></li>
				<li>Scroll down and click on <strong class="bold">Create bucket</strong>.</li>
				<li>Copy the created bucket name, open <strong class="source-inline">Chapter 05/Ch05-Kendra Search.ipynb,</strong> and paste it in the following cell in place of <strong class="source-inline">'&lt;your s3 bucket name&gt;'</strong> to get started:<p class="source-code"># Define IAM role</p><p class="source-code">role = get_execution_role()</p><p class="source-code">print("RoleArn: {}".format(role))</p><p class="source-code">sess = sagemaker.Session()</p><p class="source-code">s3BucketName = '&lt;your s3 bucket name&gt;'</p><p class="source-code">prefix = 'chapter5'</p></li>
			</ol>
			<p>We <a id="_idIndexMarker359"/>have the notebook ready and the Amazon S3 bucket created for this section's solution. Let's see a quick architecture walkthrough in the next section to understand the key components and then we will walk you through the code in the notebook you have set up.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor085"/>Walking through the solution</h2>
			<p>Setting up an enterprise-level search can be hard. That's why we have Amazon Kendra, which <a id="_idIndexMarker360"/>can crawl data from various data connectors to create a quick and easy search solution. In the following architecture, we will walk you through how you can set up a document search when you have your PDF documents in Amazon S3. We will extract the data using Amazon Textract from these PDF documents and send it to Amazon Comprehend to extract some key entities such as <strong class="bold">ORGANIZATION</strong>, <strong class="bold">TITLE</strong>, <strong class="bold">DATE</strong>, and so on. These entities will be used as filters while we sync the documents directly into Amazon Kendra for search.</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="Images/B17528_05_24.jpg" alt="Figure 5.24 – Architecture for the Amazon Kendra-powered search with Textract and Comprehend" width="737" height="364"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.24 – Architecture for the Amazon Kendra-powered search with Textract and Comprehend </p>
			<p>So, we <a id="_idIndexMarker361"/>gave you a high-level implementation architecture in the previous diagram. In the next section, we will walk you through how you can build this out with few lines of code and using the Python Boto3 APIs.</p>
			<h3>Code walkthrough</h3>
			<p>In this <a id="_idIndexMarker362"/>section, we will walk you through how you can quickly set up the proposed architecture:</p>
			<ol>
				<li value="1">We will refer to this notebook: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2005/Ch05-Kendra%20Search.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2005/Ch05-Kendra%20Search.ipynb</a>. The following code presents the Boto3 client setup for Comprehend, Kendra, and Textract APIs<p class="source-code">comprehend = boto3.client('comprehend')</p><p class="source-code">textract= boto3.client('textract')</p><p class="source-code">kendra= boto3.client('kendra')</p></li>
				<li>Now we will upload the PDF document at <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2005/resume_Sample.pdf">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2005/resume_Sample.pdf</a> from this repo to Amazon S3.<p class="callout-heading">Note:</p><p class="callout">You can upload as many documents for search as you wish. For this demonstration, we are providing just one sample. Please feel free to play around by uploading your documents to Amazon S3 and generating metadata files before you start syncing your documents to Amazon Kendra.</p><p>For extracting text from the PDF uploaded to Amazon S3, we will use the same code as we used for the asynchronous processing covered in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a><em class="italic">, Introducing Amazon Textract</em>.</p></li>
				<li>The <a id="_idIndexMarker363"/>following code shows text extraction from Amazon Textract:<p class="source-code">text=""</p><p class="source-code">for resultPage in response:</p><p class="source-code">    for item in resultPage["Blocks"]:</p><p class="source-code">        if item["BlockType"] == "LINE":</p><p class="source-code">            #print ('\033[94m' +  item["Text"] + '\033[0m')</p><p class="source-code">            text += item['Text']+"\n"</p><p class="source-code">print(text)</p><p>The sample results shown in the following screenshot contain the text from the PDF:</p><div id="_idContainer105" class="IMG---Figure"><img src="Images/B17528_05_25.jpg" alt="Figure 5.25 – Extracted text response from Amazon Textract for the resume data&#13;&#10;" width="534" height="145"/></div><p class="figure-caption">Figure 5.25 – Extracted text response from Amazon Textract for the resume data</p></li>
				<li>Now <a id="_idIndexMarker364"/>we will send this text to Amazon Comprehend for entity extraction by running the following code:<p class="source-code">entities= comprehend.detect_entities(Text=text, LanguageCode='en')</p></li>
				<li>Now we will create an Amazon Kendra index. Go to the Kendra console at https://console.aws.amazon.com/kendra/home?region=us-east-1#indexes and click the <strong class="bold">Create index</strong> button. Specify <strong class="source-inline">Search</strong> for <strong class="bold">Index name</strong>, then scroll down and click on <strong class="bold">Create a new role (Recommended)</strong>, shown highlighted in the following screenshot:<div id="_idContainer106" class="IMG---Figure"><img src="Images/B17528_05_26.jpg" alt="Figure 5.26 – Create a new role for the Kendra index" width="598" height="398"/></div><p class="figure-caption">Figure 5.26 – Create a new role for the Kendra index</p></li>
				<li>Enter <strong class="source-inline">AmazonKendra-us-east-1-kendra</strong> as the role name and click on <strong class="bold">Next</strong>. Your <a id="_idIndexMarker365"/>role name will be prefixed with <strong class="source-inline">AmazonKendra-us-east-1-</strong>.</li>
				<li>For <strong class="bold">Configure user access control</strong>, Use <strong class="bold">tokens for access control</strong>? select <strong class="bold">No</strong> and click <strong class="bold">Next</strong>.</li>
				<li>For <strong class="bold">Specify provisioning</strong>, choose <strong class="bold">Developer Edition</strong> and click on <strong class="bold">Create</strong>. Alternatively, you can run the following notebook cell after creating an IAM role to create the index programmatically:<p class="callout-heading">Note:</p><p class="callout">If you created the index using the console, please skip the programmatic creation and avoid running the following notebook cell to create the index.</p><p class="source-code">response = kendra.create_index(</p><p class="source-code">    Name='Search',</p><p class="source-code">    Edition='DEVELOPER_EDITION',</p><p class="source-code">    RoleArn='&lt;enter IAM role by creating IAM role in IAM console')</p><p class="source-code">print(response)</p><p class="callout-heading">Note:</p><p class="callout">Index creation can take up to 30 minutes.</p></li>
				<li>After <a id="_idIndexMarker366"/>creating the index, we need to get the index ID to run through this notebook.Once the index is created, click on <strong class="bold">Index</strong> and go to <strong class="bold">Index Settings</strong> to copy the index ID.<div id="_idContainer107" class="IMG---Figure"><img src="Images/B17528_05_27.jpg" alt="Figure 5.27 – Copying the Kendra index ID from the Kendra console" width="587" height="312"/></div><p class="figure-caption">Figure 5.27 – Copying the Kendra index ID from the Kendra console</p><p>Alternatively, if you created the index programmatically using the <em class="italic">CreateIndex API</em>, its response will contain an index ID of 36 digits that you need to copy and paste to run the next piece of code to update the search filters based on the Comprehend entities.</p></li>
				<li>Copy <a id="_idIndexMarker367"/>and paste the Kendra index ID over the placeholder in the following cell, then run the cell to update the index we created with filters for search. Refer to the notebook for the complete code to add all the filters:<p class="source-code">response = kendra.update_index(</p><p class="source-code">    Id="&lt;paste Index Id from Create Index response&gt;",</p><p class="source-code">    DocumentMetadataConfigurationUpdates=[</p><p class="source-code">        {</p><p class="source-code">            'Name':'ORGANIZATION',</p><p class="source-code">            'Type':'STRING_LIST_VALUE',</p><p class="source-code">            'Search': {</p><p class="source-code">                'Facetable': True,</p><p class="source-code">                'Searchable': True,</p><p class="source-code">                'Displayable': True</p><p class="source-code">            }</p><p class="source-code">        }}</p></li>
				<li>Now we will define the list of categories recognized by Comprehend:<p class="source-code">categories = ["ORGANIZATION", "PERSON", "DATE", "COMMERCIAL_ITEM", "OTHER", "TITLE", "QUANTITY"]</p></li>
				<li>Now <a id="_idIndexMarker368"/>we will iterate over the entities and generate a metadata file to populate the filters based on the entities from Amazon Comprehend:<p class="source-code">for e in entities["Entities"]:</p><p class="source-code">    if (e["Text"].isprintable()) and (not "\"" in e["Text"]) and (not e["Text"].upper() in category_text[e["Type"]]):</p><p class="source-code">                #Append the text to entity data to be used for a Kendra custom attribute</p><p class="source-code">                entity_data[e["Type"]].add(e["Text"])</p><p class="source-code">                #Keep track of text in upper case so that we don't treat the same text written in different cases differently</p><p class="source-code">                category_text[e["Type"]].append(e["Text"].upper())</p><p class="source-code">                #Keep track of the frequency of the text so that we can take the text with highest frequency of occurrance</p><p class="source-code">                text_frequency[e["Type"]][e["Text"].upper()] = 1</p><p class="source-code">    elif (e["Text"].upper() in category_text[e["Type"]]):</p><p class="source-code">                #Keep track of the frequency of the text so that we can take the text with highest frequency of occurrance</p><p class="source-code">                text_frequency[e["Type"]][e["Text"].upper()] += 1</p><p class="source-code">print(entity_data)</p></li>
				<li>You will get a response back detailing the Comprehend entity types and values detected in the text from the PDF document.<div id="_idContainer108" class="IMG---Figure"><img src="Images/B17528_05_28.jpg" alt="Figure 5.28 – Comprehend's extracted entities" width="1494" height="233"/></div><p class="figure-caption">Figure 5.28 – Comprehend's extracted entities</p></li>
				<li>Populate the <a id="_idIndexMarker369"/>Kendra metadata list from the previous entities for Amazon Kendra attributes filter:<p class="source-code">elimit = 10</p><p class="source-code">for et in categories:</p></li>
				<li>Take the <strong class="source-inline">elimit</strong> number of recognized text strings that have the highest frequency of occurrence:<p class="source-code">    el = [pair[0] for pair in sorted(text_frequency[et].items(), key=lambda item: item[1], reverse=True)][0:elimit]</p><p class="source-code">    metadata[et] = [d for d in entity_data[et] if d.upper() in el]</p><p class="source-code">metadata["_source_uri"] = documentName</p><p class="source-code">attributes["Attributes"] = metadata</p></li>
				<li>The last step is to save this file with the <strong class="source-inline">metadata.json</strong>. Make sure the filename is the original PDF document filename followed by <strong class="source-inline">metadata.json</strong> in the Amazon S3 bucket where your PDF document is uploaded:<p class="source-code">s3 = boto3.client('s3')</p><p class="source-code">prefix= 'meta/'</p><p class="source-code">with open("metadata.json", "rb") as f:</p><p class="source-code">    s3.upload_file( "metadata.json", s3BucketName,'%s/%s' % ("meta","resume_Sample.pdf.metadata.json"))</p></li>
			</ol>
			<p>We gave you a code walkthrough on how to upload a PDF document and extract data from it <a id="_idIndexMarker370"/>using Amazon Textract and then use Amazon Comprehend to extract entities. We then created a metadata file using the filters or entities extracted by Comprehend and uploaded it into Amazon S3. In the next section, we will walk you through how you can set up Amazon Kendra sync with the S3 document you uploaded, and how you can create a <strong class="source-inline">meta</strong> folder and place your metadata files there so that Amazon Kendra picks them up as metadata filters during the Kendra sync.</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor086"/>Searching in Amazon Kendra with enriched filters from Comprehend</h2>
			<p>In this <a id="_idIndexMarker371"/>section, we will walk you through how you can sync the documents to the index you have created, along with the filters in the metadata file:</p>
			<ol>
				<li value="1">Set the Kendra data source as the Amazon S3 bucket to which you uploaded your documents. Navigate to <strong class="bold">Amazon Kendra</strong> | <strong class="bold">Indexes</strong> | <strong class="bold">&lt;Name of the Index&gt;</strong><em class="italic"> </em>| <strong class="bold">Data sources |</strong> <strong class="bold">Add data source |</strong> <strong class="bold">Amazon S3</strong>, as shown in the following screenshot:<div id="_idContainer109" class="IMG---Figure"><img src="Images/B17528_05_29.jpg" alt="Figure 5.29 – Configuring Amazon Kendra sync" width="1129" height="671"/></div><p class="figure-caption">Figure 5.29 – Configuring Amazon Kendra sync</p></li>
				<li>Enter <strong class="source-inline">s3://&lt;your bucket name&gt;</strong> in the <strong class="bold">Enter the data source location</strong> field, and <a id="_idIndexMarker372"/>under <strong class="bold">Metadata files prefix folder location - optional</strong>, enter <strong class="source-inline">meta/</strong> as shown in the previous screenshot.</li>
				<li>In the <strong class="bold">IAM role</strong> section, choose <strong class="bold">Create a new role</strong> and enter <strong class="source-inline">AmazonKendra-s3</strong> in the <strong class="bold">Role name</strong> field.<div id="_idContainer110" class="IMG---Figure"><img src="Images/B17528_05_30.jpg" alt="Figure 5.30 – The run-on-demand schedule for Kendra" width="1211" height="809"/></div><p class="figure-caption">Figure 5.30 – The run-on-demand schedule for Kendra</p></li>
				<li>Then set the frequency for the sync run schedule to be <strong class="bold">Run on demand</strong> and click <strong class="bold">Next</strong>.</li>
				<li>Click on <strong class="bold">Review</strong> + Cr<strong class="bold">eate</strong>.</li>
				<li>After your data source has been created, click on <strong class="bold">Sync now</strong>.</li>
			</ol>
			<p>Once the <a id="_idIndexMarker373"/>sync is successful, all your documents in Amazon S3 will be synced and the Kendra filters will be populated with the metadata attributes extracted by Amazon Comprehend.</p>
			<p>In the next section, we will walk you through how you can navigate to the Amazon Kendra console to search.</p>
			<h3>Searching in Amazon Kendra</h3>
			<p>Amazon <a id="_idIndexMarker374"/>Kendra comes with a built-in search UI that can be used for testing the search functionality.</p>
			<p>You can also deploy this UI in a React app after testing. The page at <a href="https://docs.aws.amazon.com/kendra/latest/dg/deploying.html">https://docs.aws.amazon.com/kendra/latest/dg/deploying.html</a> has the deployment UI code available, which can be integrated with any serverless application using API Gateway and Lambda.</p>
			<p>You can also use the <strong class="source-inline">Kendra.query()</strong> API to retrieve results from the index you created in Kendra.</p>
			<p>In this section, we will walk you through using the built-in Kendra search console:</p>
			<ol>
				<li value="1">Navigate to <strong class="bold">Amazon Kendra</strong> | <strong class="bold">Indexes</strong> | <strong class="bold">Search</strong> | <strong class="bold">Search console</strong> and you will find a Kendra-powered built-in search UI as shown in the following screenshot. Enter <strong class="source-inline">person with cloud skills</strong> in the search field:<div id="_idContainer111" class="IMG---Figure"><img src="Images/B17528_05_31.jpg" alt="Figure 5.31 – Kendra query results" width="1326" height="685"/></div><p class="figure-caption">Figure 5.31 – Kendra query results</p><p>Amazon Kendra is able to give you a contextual answer containing Jane Doe, whose resume we indexed.</p><p>It also provides you with filters based on Comprehend entities on the left-hand side to quickly sort individuals based on entities such as <strong class="bold">ORGANIZATION</strong>, <strong class="bold">TITLE</strong>, <strong class="bold">DATE</strong>, and their word count frequencies.</p><p>You can also create <em class="italic">Comprehend custom entities</em>, as we covered in <a href="B17528_04_Final_SB_ePub.xhtml#_idTextAnchor063"><em class="italic">Chapter 4</em></a><em class="italic">, Automated Document Processing Workflows</em>, to enrich your metadata filters based on your business needs.</p></li>
				<li>Next, type <a id="_idIndexMarker375"/>the <strong class="source-inline">person with 10 years of experience</strong> query into the Kendra Search console.</li>
			</ol>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="Images/B17528_05_32.jpg" alt="Figure 5.32 – Kendra query results with filters on the left from Comprehend's metadata enrichment&#13;&#10;" width="1308" height="672"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.32 – Kendra query results with filters on the left from Comprehend's metadata enrichment</p>
			<p>Amazon Kendra is able to provide you with the exact contextual answer. You can also boost the response in Kendra based on relevance and provide feedback using the thumbs-up <a id="_idIndexMarker376"/>and thumbs-down buttons to improve your Kendra model.</p>
			<p class="callout-heading">Note:</p>
			<p class="callout">Amazon Kendra supports the use of PDF, Word, JSON, TXT, PPT, and HTML documents for the search functionality. Feel free to add more documents through this pipeline for better search results and accuracy.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor087"/>Summary</h1>
			<p>In this chapter, we covered two options to set up an intelligent search solution for your document-processing workflow. The first option involved setting up an NLP-based search quickly using Amazon Textract, Amazon Comprehend, and Amazon Elasticsearch using a Lambda function in a CloudFormation template for your scanned resume analysis, and can be used with anything scanned, such as images, invoices, or receipts. For the second option, we covered how you can set up an enterprise-level serverless scalable search solution with Amazon Kendra for your PDF documents. We also walked you through how you can enrich the Amazon Kendra search with additional attributes or metadata generated from Amazon Comprehend named entities.</p>
			<p>In the next chapter, we will talk about how you can use AI to improve customer service in your contact center.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor088"/>Further reading</h1>
			<ul>
				<li><em class="italic">Building an NLP-powered search index with Amazon Textract and Amazon Comprehend </em>by Mona Mona and Saurabh Shrivastava (<a href="https://aws.amazon.com/blogs/machine-learning/building-an-nlp-powered-search-index-with-amazon-textract-and-amazon-comprehend/">https://aws.amazon.com/blogs/machine-learning/building-an-nlp-powered-search-index-with-amazon-textract-and-amazon-comprehend/</a>)</li>
				<li><em class="italic">Build an intelligent search solution with automated content enrichment </em>by Abhinav Jawadekar and Udi Hershkovich (<a href="https://aws.amazon.com/blogs/machine-learning/build-an-intelligent-search-solution-with-automated-content-enrichment/">https://aws.amazon.com/blogs/machine-learning/build-an-intelligent-search-solution-with-automated-content-enrichment/</a>)</li>
			</ul>
		</div>
	</div></body></html>