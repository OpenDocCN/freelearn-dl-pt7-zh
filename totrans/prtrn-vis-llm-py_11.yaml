- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Detecting, Mitigating, and Monitoring Bias
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别、缓解和监控偏见
- en: In this chapter, we’ll analyze leading bias identification and mitigation strategies
    for large vision, language, and multimodal models. You’ll learn about the concept
    of bias, both in a statistical sense and how it impacts human beings in critical
    ways. You’ll understand key ways to quantify and remedy this in vision and language
    models, eventually landing on monitoring strategies that enable you to reduce
    any and all forms of harm when applying your **machine learning** (**ML**) models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将分析大规模视觉、语言和多模态模型的主要偏见识别与缓解策略。你将了解偏见的概念，包括统计学意义上的偏见，以及它如何在关键方面影响人类。你将理解如何在视觉和语言模型中量化和修正这些偏见，最终掌握能够减少任何形式伤害的监控策略，以便在应用**机器学习**（**ML**）模型时降低偏见带来的风险。
- en: 'We will cover the following topics in the chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Detecting bias in ML models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别 ML 模型中的偏见
- en: Mitigating bias in vision and language models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓解视觉和语言模型中的偏见
- en: Monitoring bias in ML models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控 ML 模型中的偏见
- en: Detecting, mitigating, and monitoring bias with SageMaker Clarify
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker Clarify 识别、缓解和监控偏见
- en: Detecting bias in ML models
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别 ML 模型中的偏见
- en: At this point in the book, we’ve covered many of the useful, interesting, and
    impressive aspects of large vision and language models. Hopefully, some of my
    passion for this space has started rubbing off on you, and you’re beginning to
    realize why this is as much of an art as it is a science. Creating cutting-edge
    ML models takes courage. Risk is inherently part of the process; you hope a given
    avenue will pay off, but until you’ve followed the track all the way to the end,
    you can’t be positive. Study helps, as does discussion with experts to try to
    validate your designs ahead of time, but personal experience ends up being the
    most successful tool in your toolbelt.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书已经涵盖了大规模视觉和语言模型中许多有用、有趣和令人印象深刻的方面。希望我对这一领域的热情已经开始影响到你，你也开始意识到这不仅是一门科学，还是一门艺术。创建最前沿的ML模型需要勇气。风险是过程的一部分；你希望某条路径能够带来回报，但直到你沿着这条路走到最后，你无法确定。研究有所帮助，和专家讨论可以提前验证你的设计，但个人经验最终成为你工具箱中最有效的工具。
- en: 'This entire chapter is dedicated to possibly the most significant Achilles
    heel in ML and **artificial intelligence** (**AI**): **bias**. Notably, here we
    are most interested in bias toward and against specific groups of human beings.
    You’ve probably already heard about **statistical bias**, an undesirable scenario
    where a given model has a statistical preference for part of the dataset, and
    thereby naturally against another part. This is an inevitable stage of every data
    science project: you need to carefully consider which datasets you are using and
    grapple with how that represents the world to your model. If you are over- or
    under-representing any facets of your datasets, this will invariably impact your
    model’s behavior. In the previous chapter, we looked at an example of credit card
    fraud and began to understand how the simple act of extracting and constructing
    your dataset can lead you in completely the wrong direction. Now, we’ll follow
    a similar exercise but focus on people.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的全部内容都集中在可能是 ML 和**人工智能**（**AI**）中最重要的致命弱点——**偏见**。特别地，我们关注的是针对特定群体的人类偏见。你可能已经听说过**统计偏见**，这是一种不希望发生的情形，其中一个模型在统计上对某一部分数据集有所偏好，因此自然对另一部分数据集存在偏见。这是每个数据科学项目中不可避免的阶段：你需要仔细考虑所使用的数据集，并思考这些数据如何向你的模型展示世界。如果你的数据集某些方面的表现过多或过少，这无疑会影响你的模型行为。在上一章中，我们探讨了信用卡欺诈的例子，开始理解提取和构建数据集的简单行为如何将你引入完全错误的方向。现在，我们将进行类似的练习，但这次重点关注人类群体。
- en: When ML and data science started becoming popular in business leadership circles,
    as with any new phenomena, there were naturally a few misunderstandings. A primary
    one of these in terms of ML was the mistaken belief that computers would naturally
    have fewer biased preferences than people. More than a few projects were inspired
    by this falsity. From hiring to performance evaluations, credit applications to
    background checks, and even for sentencing in the criminal justice system, countless
    data science projects were started with an intention of reducing biased outcomes.
    What these projects failed to realize is that *every dataset is limited by historical
    records*. When we train ML models on these records naively, we necessarily introduce
    those same limitations on the output space of the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: This means that records from criminal justice to human resources, financial
    services, and imaging systems when naively used to train ML models codified that
    bias and presented it in a digital format. When used at scale—for example, to
    make hundreds to thousands of digital decisions—this actually increases the scale
    of biased decision-making rather than reduces it. Classic examples of this include
    large-scale image classification systems failing to detect African Americans *(1)*
    or resume screening systems that developed a bias against anything female *(2)*.
    While all of these organizations immediately took action to right their wrongs,
    the overall problem was still shockingly public for the entire world to watch.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Detecting bias in large vision and language models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you may already suspect, large models trained on massive datasets from internet
    crawls are ripe with bias. This includes everything from types of people more
    likely to produce or not to produce content on the internet, to languages and
    styles, topics, the accuracy of the content, depth of analysis, personalities,
    backgrounds, histories, interests, professions, educational levels, and more.
    It includes visual representations of other people, modes, cultural styles, places,
    events, perspectives, objects, sexuality, preferences, religion—the list goes
    on and on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'In most projects, to use a phrase from Amazon, I find it helpful to *work backward*
    from my final application. Here, that refers to some large vision-language model—for
    example, **Stable Diffusion**. Then, I’ll ask myself: *Who* is likely to use this
    model, and how? Try to write down a list of types of people you think might use
    your model; in a bias context, you need to push yourself to think outside of your
    comfort zone. This is another place where having diverse teams is incredibly useful;
    ideally, ask someone with a background different from yours about their perspective.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have a target list of types of people who might use your model, think
    to yourself: Are these people represented in my dataset? How are they represented?
    Are they represented in a full spectrum of different emotional states and outcomes,
    or are they only represented in a tiny slice of humanity? If my dataset were the
    sole input to a computational process designed to learn patterns—that is, an ML
    algorithm—would many people from this group consider it a fair and accurate representation?
    Or, would they get angry at me and say: That’s so biased!?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: You can get started with even one or two groups of people, and usually, you
    want to think about certain scenarios where you know there’s a big gap in your
    dataset. For me, I tend to look right at gender and employment. You can also look
    at religion, race, socioeconomic status, sexuality, age, and so on. Try to stretch
    yourself and find an intersection. An intersection would be a place where someone
    from this group is likely to be, or not to be, within another category. This second
    category can be employment, education, family life, ownership of specific objects,
    accomplishments, criminal history, medical status, and so on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: A model is biased when it exhibits a clear “preference”, or measurable habit,
    of placing or not placing certain types of people in certain types of groups.
    Bias shows up when your model empirically places or does not place people from
    one of your *A* categories into one of your *B* categories.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say you’re using a text generation model in the GPT family.
    You might send your GPT-based model a prompt such as, “Akanksha works really hard
    as a …”. A biased model might fill in the blank as “nurse”, “secretary”, “homemaker”,
    “wife”, or “mother”. An unbiased model might fill in the blank as “doctor”, “lawyer”,
    “scientist”, “banker”, “author”, or “entrepreneur”. Imagine using that biased
    model for a resume-screening classifier, an employment chat helpline, or a curriculum-planning
    assistant. It would unwittingly, but very measurably, continue to make subtle
    recommendations against certain careers for women! Let’s look at a few more examples
    in the context of language:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Biased inference results from GPT-J 6B](img/Figure_11.1_B18942.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Biased inference results from GPT-J 6B
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I simply used my own name as a prompt into the GPT-J 6B model, and it
    thought I was a makeup artist. Alternatively, if I used the name “John”, it thought
    I was a software developer:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Biased inference results from GPT-J 6B (continued)](img/Figure_11.2_B18942.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Biased inference results from GPT-J 6B (continued)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Once you try this again, however, the responses obviously change. This is because
    the random seed isn’t set in the Hugging Face model playground, so the output
    from the **neural network** (**NN**) can change. When I tried it again for John,
    it still gave a response of “software developer”. When I tried it again for myself,
    it responded with “a freelance social media consultant”.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你再次尝试时，回应显然发生了变化。这是因为在 Hugging Face 模型平台中，随机种子并未设置，因此**神经网络**（**NN**）的输出可能会变化。当我再次为
    John 尝试时，它仍然给出了“软件开发人员”的回答。当我再次为我自己尝试时，它的回答是“自由职业社交媒体顾问”。
- en: 'Some of you might be wondering: Why is this biased? Isn’t this just holding
    to the statistical representation in the dataset? The answer is that the dataset
    itself is biased. There are more examples of male software engineers, fewer examples
    of female entrepreneurs, and so on. When we train AI/ML models on these datasets,
    we bring that bias directly into our applications. This means that if we use a
    biased model to screen resumes, suggest promotions, stylize text, assign credit,
    predict healthy indicators, determine criminal likelihoods, and more, we perpetuate
    that same bias systematically. And that is a big problem—one we need to actively
    fight against.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的一些人可能会想：为什么这有偏见？这不就是依赖数据集中统计表示吗？答案是数据集本身就有偏见。数据集中男性软件工程师的例子更多，女性企业家的例子更少，等等。当我们在这些数据集上训练AI/ML模型时，我们将这种偏见直接带入我们的应用中。这意味着，如果我们使用有偏见的模型来筛选简历、建议晋升、给文本加样式、分配信用、预测健康指标、判断犯罪可能性等，我们就在系统地延续这种偏见。这是一个大问题——我们需要积极应对。
- en: This guess-and-check process we’re doing right now with the pretrained model
    is called **detecting bias** or **identifying bias**. We are taking a pretrained
    model and providing it with specific scenarios at the intersection of our groups
    of interest defined previously, to empirically determine how well it performs.
    Once you’ve found a few empirical examples of bias, it’s helpful to also run summary
    statistics on this to understand how regularly this occurs in your dataset. Amazon
    researchers proposed a variety of metrics to do so here *(3)*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在正在用预训练模型进行的这种猜测和检验过程叫做**检测偏见**或**识别偏见**。我们正在使用一个预训练模型，并在之前定义的感兴趣群体交集的具体场景中进行测试，以实证地确定其表现如何。一旦你找到一些偏见的实证例子，运行汇总统计数据来了解其在数据集中发生的频率也是有帮助的。亚马逊的研究人员在这里提出了多种度量方法来做到这一点*(3)*。
- en: You might do a similar process with a pretrained vision model, such as **Stable
    Diffusion**. Ask your Stable Diffusion model to generate images of people, of
    workers, of different scenarios in life. Try phrasing your prompt to force the
    model to categorize a person around one of the points of intersection, and these
    days you are almost guaranteed to find empirical evidence of bias.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以对预训练的视觉模型做类似的处理，例如**稳定扩散**。让你的稳定扩散模型生成不同场景下的人物图像、工人图像等。尝试调整提示语，强迫模型将一个人分类为交集的某一类别，现如今你几乎可以保证会找到偏见的实证证据。
- en: Fortunately, more models are using “safety filters”, which explicitly bar the
    model from being able to produce violent or explicit content, but as you’ve learned
    in this chapter, that is far from being without bias.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，越来越多的模型正在使用“安全过滤器”，这些过滤器明确禁止模型生成暴力或露骨内容，但正如你在本章中所学到的，这远非没有偏见。
- en: By now, you should have a good idea of what bias means in the context of your
    application. You should know for which groups of people you want to design, and
    in which categories you want to improve your model’s performance. Make sure you
    spend a decent amount of time empirically evaluating bias in your model, as this
    will help you demonstrate that the following techniques actually improve the outcomes
    you care about.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经对应用中偏见的含义有了清晰的认识。你应该知道自己想要为哪些群体设计，在哪些类别中希望提升模型的表现。确保你花足够的时间在实证上评估模型中的偏见，因为这将帮助你证明以下技术确实能改善你关心的结果。
- en: Mitigating bias in vision and language models
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少视觉和语言模型中的偏见
- en: Now that you’ve learned about detecting bias in your vision and language models,
    let’s explore methods to mitigate this. Generally, this revolves around updating
    your dataset in various ways, whether through sampling, augmentation, or generative
    methods. We’ll also look at some techniques to use during the training process
    itself, including the concept of fair loss functions and other techniques.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了如何检测视觉和语言模型中的偏差，接下来让我们探讨如何减轻这种偏差。通常，这涉及通过各种方式更新数据集，无论是通过采样、增强还是生成方法。我们还将讨论一些在训练过程中使用的技术，包括公平损失函数的概念以及其他技术。
- en: As you are well aware by now, there are two key training phases to stay on top
    of. The first is the **pretraining process**, and the second is the **fine-tuning**
    or **transfer** **learning** (**TL**). In terms of bias, a critical point is how
    much bias transfer your models exhibit. That is to say, if your pretrained model
    was built on a dataset with bias, does that bias then transfer into your new model
    after you’ve done some fine-tuning?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，目前有两个关键的训练阶段需要关注。第一个是**预训练过程**，第二个是**微调**或**迁移****学习**（**TL**）。在偏差方面，一个关键点是你的模型表现出多少偏差转移。也就是说，如果你的预训练模型是基于带有偏差的数据集构建的，那么在你进行一些微调后，这些偏差是否会转移到新的模型中？
- en: 'A research team out of MIT delivered an interesting study on the effects of
    bias transfer in vision as recently as 2022 *(4)*, where they concluded: “*bias
    in pretrained models remained present even after fine-tuning these models on downstream
    target tasks. Crucially these biases can persist even when the target dataset
    used for fine-tuning did not contain such biases.*” This indicates that in vision,
    it is critical to ensure that your upstream pretrained dataset is bias-free. They
    found that the bias carried through into the downstream task.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 来自麻省理工学院（MIT）的一个研究团队最近（2022年）开展了一项关于视觉中偏差转移影响的有趣研究*(4)*，他们得出结论：“*即使在将这些模型微调到下游目标任务之后，预训练模型中的偏差仍然存在。关键是，即使微调使用的目标数据集本身不包含这些偏差，这些偏差依然会存在。*”这表明，在视觉领域，确保上游的预训练数据集没有偏差是至关重要的。研究发现，偏差会传递到下游任务中。
- en: 'A similar study for language found exactly the opposite of this! (*11*) Using
    a regression analysis in their work, the researchers realized that a better explanation
    for the presence of bias *was in the fine-tuned dataset* rather than the pretrained
    one. They concluded: “*attenuating downstream bias via upstream interventions—including
    embedding-space bias mitigation—is mostly futile.*” In language, the recommendation
    is to mostly mitigate bias in your downstream task, rather than upstream.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言的类似研究得出了完全相反的结论！(*11*) 研究人员在他们的工作中使用回归分析，意识到偏差的存在更好的解释是*微调数据集*中的偏差，而不是预训练数据集。他们得出结论：“*通过上游干预来减轻下游偏差——包括嵌入空间中的偏差减轻——大多数时候是徒劳的。*”在语言领域，建议主要在下游任务中减轻偏差，而不是在上游。
- en: How interesting is this?! Similar work in two different domains reached opposite
    conclusions about the most effective place to focus for mitigating bias. This
    means if you are working on a vision scenario, you should spend your time optimizing
    your pretrained dataset to remove bias. Alternatively, if you’re on a language
    project, you should focus on reducing bias in the fine-tuning dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这有多有趣？！在两个不同领域的类似工作中，关于减轻偏差的最有效聚焦点得出了相反的结论。这意味着，如果你在做视觉场景的工作，你应该花时间优化你的预训练数据集，消除偏差。相反，如果你在做语言项目，你应该专注于减少微调数据集中的偏差。
- en: Perhaps this means that vision models on average carry more context and background
    knowledge into their downstream performance, such as correlating objects with
    nearby objects and patterns as a result of the convolution, while language only
    applies this context learning at a much smaller sentence-level scope.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 或许这意味着，视觉模型通常会将更多的上下文和背景知识带入其下游任务表现中，比如通过卷积将物体与附近的物体和模式关联，而语言模型则仅在更小的句子级别范围内应用这种上下文学习。
- en: Bias mitigation in language – counterfactual data augmentation and fair loss
    functions
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言中的偏差减轻——反事实数据增强和公平损失函数
- en: In language, many bias mitigation techniques center on *creating counterfactuals*.
    Remember—a counterfactual is a hypothetical scenario that did not happen in the
    real world but could have. For example, this morning you had many options for
    eating breakfast. You might have had coffee with a muffin. You also might have
    had breakfast cereal with orange juice. You might have gone out to a restaurant
    for breakfast with a friend, or maybe you skipped breakfast entirely. One of them
    really happened to you, but the other ones are completely fabricated. Possible,
    but fabricated. Each of these different scenarios could be considered *counterfactual*.
    They represent different scenarios and chains of events that did not actually
    happen but are reasonably likely to occur.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言学中，许多偏见缓解技术侧重于*创造反事实*。记住——反事实是指在现实世界中没有发生但本可以发生的假设情境。例如，今天早上你有很多选择可以吃早餐。你可能选择了咖啡配松饼。你也可能选择了早餐麦片配橙汁。你还可能和朋友一起去餐馆吃早餐，或者完全跳过了早餐。上述其中一个情况确实发生了，而其他情况完全是编造的。它们是可能的，但却是虚构的。每一种不同的情景都可以被视为*反事实*。它们代表了不同的情景和事件链，这些事件并未实际发生，但合理地可能会发生。
- en: 'Now, consider this: what if you wanted to represent each scenario as being
    equally likely to occur? In the dataset of your life, you’ve established certain
    habits of being. If you wanted to train a model to consider all habits as equally
    likely, you’d need to create counterfactuals to equally represent all other possible
    outcomes. This type of dataset-hacking is exactly what we’re doing when we try
    to augment a dataset to debias it, or to mitigate the bias. First, we identify
    the top ways that bias creeps into our models and datasets, and then we mitigate
    that bias by creating more examples of what we don’t have enough examples for,
    creating counterfactuals.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑这个问题：如果你想让每个情景都有相等的发生概率，该怎么办？在你生活的数据集中，你已经建立了某些习惯。如果你想训练一个模型，使得所有习惯都被视为同样可能发生的，你就需要创造反事实，以平衡所有其他可能的结果。这种类型的数据集修改正是我们在尝试通过增强数据集来去偏见，或减少偏见时所做的。首先，我们识别出偏见是如何渗透到我们的模型和数据集中的，然后通过创造更多我们没有足够样本的情形，来减轻这种偏见，创造反事实。
- en: 'One study presenting these methods is available in reference *(5)* in the *References*
    section—it includes researchers from Amazon, UCLA, Harvard, and more. As mentioned
    previously, they focused on the intersection of gender and employment. Let’s take
    a look at an example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍这些方法的研究可以在*参考文献*部分的参考文献*(5)*中找到——这项研究包括了来自亚马逊、UCLA、哈佛等的研究人员。如前所述，他们专注于性别与就业的交集。让我们看一个例子：
- en: '![Figure 11.3 – Comparing responses from normal and debiased models](img/Figure_11.3_B18942.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – 比较普通模型与去偏见模型的响应](img/Figure_11.3_B18942.jpg)'
- en: Figure 11.3 – Comparing responses from normal and debiased models
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 比较普通模型与去偏见模型的响应
- en: To generate counterfactual samples for their fine-tuning dataset, the researchers
    used a common technique of switching pronouns. In particular, they “*use a curated
    dictionary of gender words with male <-> female mapping, for instance, father
    -> mother, she-> he, him-> her, and so on*”. With this pronoun dictionary, they
    generated new sequences and included these in the fine-tuning dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为他们的微调数据集生成反事实样本，研究人员使用了一个常见的技术，即交换代词。具体来说，他们“*使用一个精心编制的性别词典，其中包含男性<->女性的映射，例如父亲
    -> 母亲，她 -> 他，他 -> 她，等等*”。利用这个代词词典，他们生成了新的序列，并将这些序列包含进了微调数据集中。
- en: They also defined a fair knowledge distillation loss function. We’ll learn all
    about knowledge distillation in the upcoming chapter, but at a high level, what
    you need to know is that it’s the process of training a smaller model to mimic
    the performance of a larger model. Commonly this is done to shrink model sizes,
    giving you ideally the same performance as your large model, but on something
    much smaller you can use to deploy in single-GPU environments.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还定义了一个公平的知识蒸馏损失函数。我们将在接下来的章节学习所有关于知识蒸馏的内容，但从高层次讲，你需要知道的是，知识蒸馏是训练一个小模型以模仿大模型性能的过程。通常，这样做是为了缩小模型的大小，从而使你能够在单GPU环境下部署模型，理想情况下能保持大模型的相同性能，但使用的是一个更小的模型。
- en: 'Here, the researchers developed a novel distillation strategy to *equalize
    probabilities*. In generic distillation, you want the student model to learn the
    same probabilities for a given pattern:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，研究人员开发了一种新颖的蒸馏策略来*平衡概率*。在通用的蒸馏中，你希望学生模型学习到相同的概率分布，以应对给定的模式：
- en: '![Figure 11.4 – Equalizing distributions through distillation](img/Figure_11.4_B18942.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Equalizing distributions through distillation
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Here, the researchers knew this would lead to the student model learning exactly
    the same biased behavior they wanted to avoid. In response, they developed a novel
    distillation loss function to weight both the original and the counterfactual
    distributions as the same. This equalizing loss function helped their model learn
    to see both outcomes as equally likely and enabled the fair prompt responses you
    just saw! Remember—in order to build AI/ML applications that do not perpetuate
    the bias inherent in the datasets, we need to equalize how people are treated
    in the model itself.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned about a few ways to overcome bias in language, let’s
    do the same for vision.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Bias mitigation in vision – reducing correlation dependencies and solving sampling
    issues
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In vision scenarios, you have at least two big problems to tackle, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: First, not having enough pictures of under-represented groups of people
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, realizing after it’s too late that your pictures are all correlated
    with underlying objects or styles
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the first scenario, your model is likely to not learn that class at all.
    In the second scenario, your model learns a correlated confounding factor. It
    might learn more about objects in the background, overall colors, the overall
    style of the image, and so much more than it does about the objects you think
    it’s detecting. Then, it continues to use those background objects or traces to
    make classification guesses, where it clearly underperforms. Let’s explore a 2021
    study *(6)* from Princeton to learn more about these topics:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Correct and incorrect vision classifications](img/Figure_11.5_B18942.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Correct and incorrect vision classifications
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentally, what these images show is the correlation problem in computer
    vision. Here the model is simply trying to classify males and females in images.
    However, due to underlying correlations in these datasets, the model makes basic
    mistakes. In terms of sports uniforms, the researchers found that “*males tend
    to be represented as playing outdoor sports like baseball, while females tend
    to be portrayed as playing an indoor sport like basketball or in a swimsuit*”.
    This means the model thought everyone wearing a sports uniform indoors was female,
    and everyone wearing a sports uniform outdoors was male! Alternatively, for flowers,
    the researchers found a “*drastic difference in how males and females are portrayed,
    where males pictured with a flower are in formal, official settings, whereas females
    are in staged settings or paintings*." Hopefully, you can immediately see why
    this is a problem; even the model thinks that everyone in a formal setting is
    male, simply due to a lack of available training data!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we solve this problem? One angle the researchers explored was geography.
    They realized that—consistent with previous analyses—the images’ countries of
    origin were overwhelmingly the United States and European nations. This was true
    across the multiple datasets they analyzed that are common in vision research.
    In the following screenshot, you can see the model learns an association of the
    word “dish” with food items from Eastern Asia while failing to detect plates or
    satellite dishes, which were more common images from other regions:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们该如何解决这个问题呢？研究人员探讨的一个角度是地理因素。他们意识到——与先前的分析一致——图像的原产国主要是美国和欧洲国家。这在他们分析的多个常见视觉研究数据集中都是如此。在以下截图中，您可以看到模型将“dish”一词与东亚的食物物品关联起来，而未能检测到其他地区更常见的盘子或卫星天线：
- en: '![Figure 11.6 – Visual bias in the meaning of “dish” geographically](img/Figure_11.6_B18942.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – “dish”一词在地理意义上的视觉偏见](img/Figure_11.6_B18942.jpg)'
- en: Figure 11.6 – Visual bias in the meaning of “dish” geographically
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – “dish”一词在地理意义上的视觉偏见
- en: 'The Princeton team developed and open sourced a tool called *REVISE: REvealing
    VIsual biaSEs* *(7)* that any Python developer can use to analyze their own visual
    datasets and identify candidate objects and problems that will cause correlation
    issues. This actually uses Amazon’s Rekognition service in the backend to run
    large-scale classification and object detection on the dataset! You can, however,
    modify it to use open source classifiers if you prefer. The tool automatically
    suggests actions to take to reduce bias, many of which revolve around searching
    for additional datasets to increase the learning for that specific class. The
    suggested actions can also include adding extra tags, reconciling duplicate annotations,
    and more.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '普林斯顿团队开发并开源了一个名为*REVISE: REvealing VIsual biaSEs* *(7)*的工具，任何Python开发人员都可以用来分析自己的视觉数据集，并识别可能导致关联问题的候选对象和问题。该工具实际上在后台使用了亚马逊的Rekognition服务来对数据集进行大规模分类和对象检测！不过，如果您愿意，也可以修改它，使用开源分类器。该工具会自动建议采取减少偏见的措施，其中许多建议围绕着寻找额外的数据集，以增加对特定类别的学习。这些建议的行动还可能包括添加额外标签、整合重复注释等。'
- en: Now that we’ve learned about multiple ways to mitigate bias in vision and language
    models, let’s explore ways to monitor this in your applications.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了多种减轻视觉和语言模型偏见的方法，让我们来探讨如何在您的应用程序中监控这些偏见。
- en: Monitoring bias in ML models
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控机器学习模型中的偏见
- en: At this point in the book, for beginners, you are probably starting to realize
    that in fact, we are just at the tip of the iceberg in terms of identifying and
    solving bias problems. Implications for this range from everything from poor model
    performance to actual harm to humans, especially in domains such as hiring, criminal
    justice, financial services, and more. These are some of the reasons Cathy O’Neil
    raised these important issues in her 2016 book, *Weapons of Math Destruction*
    *(8)*. She argues that while ML models can be useful, they can also be quite harmful
    to humans when designed and implemented carelessly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，对于初学者来说，您可能已经开始意识到，实际上我们仅仅是在识别和解决偏见问题的冰山一角。其影响从模型性能差到对人类的实际伤害不等，尤其是在招聘、刑事司法、金融服务等领域。这也是凯西·奥尼尔在她2016年的书《数学毁灭武器》*(8)*中提出这些重要问题的一部分原因。她认为，尽管机器学习模型可以有用，但如果设计和实施不谨慎，它们也可能对人类造成相当大的伤害。
- en: This raises core issues about ML-driven innovation. How good is good enough
    in a world full of biases? As an ML practitioner myself who is passionate about
    large-scale innovation, and also as a woman who is on the negative end of some
    biases, while certainly on the positive side of others, I grapple with these questions
    a lot.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了关于机器学习驱动创新的核心问题。在充满偏见的世界里，“足够好”到底有多好？作为一名热衷于大规模创新的机器学习从业者，同时也是一个在某些偏见中处于负面端、在其他偏见中处于正面端的女性，我常常会在这些问题上深思。
- en: Personally, there are some data science projects I just refuse to work on because
    of bias. For me, this includes at least hiring and resume screening, performance
    reviews, criminal justice, and some financial applications. Maybe someday we’ll
    have balanced data and truly unbiased models, but based on what I can see, we
    are far away from that. I encourage every ML practitioner to develop a similar
    personal ethic about projects that have the potential for a negative impact on
    humans. You can imagine that even something as seemingly innocuous as online advertising
    can lead to large-scale discrepancies among people. Ads for everything from jobs
    to education, networking to personal growth, products to financial tools, psychology,
    and business advice, can in fact perpetuate large-scale social biases.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: At a higher level, I believe as an industry we can continue to evolve. While
    some professions require third-party certification, such as medical, legal, and
    education experts, ours still does not. Some service providers provide ML-specific
    certification, which is certainly a step in the right direction but doesn’t fully
    address the core dichotomy between pressure to deliver results from your employer
    with potential unknown and unidentified harm to your customers. Certainly, I’m
    not claiming to have any answers here; I can see the merits of both sides of this
    argument and can sympathize with the innovators just as well as with the end consumers.
    I’m just submitting that this is in fact a massive challenge for the entire industry,
    and I hope we can develop better mechanisms for this in the future.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'On a more immediately actionable note, for those of you who have a project
    to deliver on in the foreseeable future, I would suggest the following steps:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Identify a broad picture of your customer base, ideally with the help of a diverse
    team.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify what outcomes your model will impose on your customer; push yourself
    to think beyond the immediate impact on your business and your team. To use a
    phrase from Amazon, think big!
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to find empirical examples of your best- and worst-case scenarios—best case
    being where your model leads to win-wins, and worst case where it leads to lose-lose.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the techniques we learned throughout this book to make the win-wins more
    common, and the lose-lose as infrequent as you can. Remember—this usually comes
    down to analyzing your data, learning about its flaws and inherent perspectives,
    and remedying these either through the data itself or through your model and learning
    process.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add transparency. As O’Neil points out in her book, part of the industry-wide
    problem is applications that impact humans in major ways not explaining which
    features actually drive their final classification. To solve this, you can add
    simple feature importance testing through LIME *(9)*, or pixel and token mapping,
    as we’ll see next with SageMaker Clarify.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to develop quantitative measures for especially your worst-case scenario
    outcomes and monitor these in your deployed application.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As it turns out, one way to detect, mitigate, and monitor bias in your models
    is SageMaker Clarify!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Detecting, mitigating, and monitoring bias with SageMaker Clarify
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SageMaker Clarify is a feature within the SageMaker service you can use for
    bias and explainability across your ML workflows. It has a nice integration with
    SageMaker’s Data Wrangler, a fully managed UI for tabular data analysis and exploration.
    This includes nearly 20 bias metrics, statistical terms you can study and use
    to get increasingly more precise about how your model interacts with humanity.
    I’ll spare you the mathematics here, but feel free to read more about them in
    my blog post on the topic here: [https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72](https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72)
    *(10)*!'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Arguably more relevant for this book are Clarify’s *vision and language features*!
    This includes explaining image classification and object detection, along with
    language classification and regression. This should help you immediately understand
    what is driving your discriminative models’ output, and help you take steps to
    rectify any biased decisioning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Actually, the modularity of large pretrained models in combination with smaller
    outputs, such as using Hugging Face to easily add a classification output to a
    pretrained **large language model** (**LLM**), might be a way we could debias
    pretrained models using Clarify, ultimately using them for generation. A strong
    reason to use Clarify is that you can monitor both bias metrics and model explainability!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: In the next section of the book, *Part 5*, we will dive into key questions around
    deployment. Particularly, in [*Chapter 14*](B18942_14.xhtml#_idTextAnchor217),
    we’ll dive into ongoing operations, monitoring, and maintenance of models deployed
    into production. We’ll cover SageMaker Clarify’s monitoring features extensively
    there, especially discussing how you can connect these both to audit teams and
    automatic retraining workflows.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we dove into the concept of bias in ML, and especially explored
    angles in vision and language. We opened with a general discussion of human bias
    and introduced a few ways these empirically manifest in technology systems, frequently
    without intention. We introduced the concept of “intersectional bias”, and how
    commonly your first job in detecting bias is listing a few common types of intersections
    you want to be wary of, including gender or race and employment, for example.
    We demonstrated how this can easily creep into large vision and language models
    trained on datasets crawled from the internet. We also explored methods to mitigate
    bias in ML models. In language, we presented counterfactual data augmentation
    along with fair loss functions. In vision, we learned about the problem of correlational
    dependencies, and how you can use open source tools to analyze your vision dataset
    and solve sampling problems.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned about monitoring bias in ML models, including a large discussion
    about both personal and professional ethics, and actional steps for your projects.
    We closed out with a presentation of SageMaker Clarify, which you can use to detect,
    mitigate, and monitor bias in your ML models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s dive into *Part Five: Deployment!*. In the next chapter, we will
    learn about how to deploy your model on SageMaker.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please go through the following content for more information on a few topics
    covered in the chapter:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '*Google apologises for Photos app’s racist* *blunder*: [https://www.bbc.com/news/technology-33347866](https://www.bbc.com/news/technology-33347866)'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Amazon scraps secret AI recruiting tool that showed bias against* *women*:
    [https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation*:
    [https://assets.amazon.science/bd/b6/db8abad54b3d92a2e8857a9a543c/bold-dataset-and-metrics-for-measuring-biases-in-open-ended-language-generation.pdf](https://assets.amazon.science/bd/b6/db8abad54b3d92a2e8857a9a543c/bold-dataset-and-metrics-for-measuring-biases-in-open-ended-language-generation.pdf)'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*When does Bias Transfer in Transfer* *Learning?*: [https://arxiv.org/pdf/2207.02842.pdf](https://arxiv.org/pdf/2207.02842.pdf)'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Mitigating Gender Bias in Distilled Language Models via Counterfactual Role*
    *Reversal*: [https://aclanthology.org/2022.findings-acl.55.pdf](https://aclanthology.org/2022.findings-acl.55.pdf)'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*REVISE: A Tool for Measuring and Mitigating Bias in Visual* *Datasets*: [https://arxiv.org/pdf/2004.07999.pdf](https://arxiv.org/pdf/2004.07999.pdf)'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*princetonvisualai/revise-tool*: [https://github.com/princetonvisualai/revise-tool](https://github.com/princetonvisualai/revise-tool)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Weapons of Math Destruction: How Big Data Increases Inequality and Threatens
    Democracy Hardcover – September 6,* *2016*: [https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815)'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Why Should I Trust You?” Explaining the Predictions of Any* *Classifier*:
    [https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Dive into Bias Metrics and Model Explainability with Amazon SageMaker* *Clarify*:
    [https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72](https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72)'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis
    in Pre-Trained Language* *Models:* [https://aclanthology.org/2022.acl-long.247.pdf](https://aclanthology.org/2022.acl-long.247.pdf)'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
