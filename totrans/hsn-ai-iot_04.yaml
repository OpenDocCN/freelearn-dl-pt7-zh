- en: Deep Learning for IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last chapter, we learned about different **machine learning** (**ML**)
    algorithms. The focus of this chapter is neural networks based on multiple layered
    models, also known as deep learning models. They have become a buzzword in the
    last few years and an absolute favorite of investors in the field of artificial-intelligence-based
    startups. Achieving above human level accuracy in the task of object detection
    and defeating the world''s Dan Nine Go master are some of the feats possible by
    **deep** **learning** (**DL**). In this chapter and a few subsequent chapters,
    we will learn about the different DL models and how to use DL on our IoT generated
    data. In this chapter, we will start with a glimpse into the journey of DL, and
    learn about four popular models, the **multilayered perceptron** (**MLP**), the
    **convolutional neural network** (**CNN**), **recurrent neural network** (**RNN**),
    and autoencoders. Specifically, you will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The history of DL and the factors responsible for its present success
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial neurons and how they can be connected to solve non-linear problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The backpropagation algorithm and using it to train the MLP model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The different optimizers and activation functions available in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the CNN works and the concept behind kernel, padding, and strides
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using CNN model for classification and recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNNs and modified RNN and long short-term memory and gated recurrent units
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture and functioning of autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The human mind has always intrigued philosophers, scientists, and engineers
    alike. The desire to imitate and replicate the intelligence of the human brain
    by man has been written about over many years; Galatea by Pygmalion of Cyprus
    in Greek mythology, Golem in Jewish folklore, and Maya Sita in Hindu mythology
    are just a few examples. Robots with **Artificial Intelligence** (**AI**) are
    a favorite of (science) fiction writers since time immemorial.
  prefs: []
  type: TYPE_NORMAL
- en: AI, as we know today, was conceived parallel with the idea of computers. The
    seminal paper, *A Logical Calculus Of The Ideas Immanent In Nervous Activity*,
    in the year 1943 by McCulloch and Pitts proposed the first neural network model—the
    threshold devices that could perform logical operations such as AND, OR, AND-NOT.
    In his pioneering work, *Computing Machinery and Intelligence,* published in the
    year 1950, Alan Turing proposed a **Turing test**; a test to identify whether
    a machine has intelligence or not. Rosenblatt, in 1957, laid the base for networks
    that could learn from experience in his report, *The Perceptron—a perceiving and
    recognizing automaton*. These ideas were far ahead of their time; while the concepts
    looked theoretically possible, computational resources at that time severely limited
    the performance you could get through these models that could do logic and learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'While these papers seem old and irrelevant, they are very much worth reading
    and give great insight into the vision these initial thinkers had. Following,
    are the links to these papers for interested readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A Logical Calculus Of The Ideas Immanent In Nervous Activity*, McCulloch and
    Pitts: [https://link.springer.com/article/10.1007%2FBF02478259](https://link.springer.com/article/10.1007%2FBF02478259)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Computing Machinery and Intelligence*, Alan Turing: [http://phil415.pbworks.com/f/TuringComputing.pdf](http://phil415.pbworks.com/f/TuringComputing.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Perceptron—a perceiving and recognizing automaton*, Rosenblatt: [https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another interesting paper one by Wang and Raj from Carnegie Melon University,
    *On the Origin of Deep Learning*; the 72-page paper covers in detail the history
    of DL, starting from the McCulloch Pitts model to the latest attention models:
    [https://arxiv.org/pdf/1702.07800.pdf](https://arxiv.org/pdf/1702.07800.pdf).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Two AI winters and a few successes later (with the breakthrough in 2012, when
    Alex Krizhvesky, Ilya Sutskever, and Geoffrey Hinton''s AlexNet entry in the annual
    ImageNet challenge achieved an error rate of 16%), today we stand at a place where
    DL has outperformed most of the existing AI techniques. The following screenshot
    from Google Trends shows that, roughly around 2014, **Deep** **Learning** became
    popular and had been growing since then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/902e63f9-5ec8-4568-b10e-9f18e2a3a5f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep learning in Google Trends from 2004 to April 2018
  prefs: []
  type: TYPE_NORMAL
- en: Let's see the reasons behind this growing trend and analyze whether it's just
    hype or whether there's more to it.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning—why now?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of the core concepts in the field of DL were already in place by the 80s
    and 90s, and therefore, the question arises why suddenly we see an increase in
    the applications of DL to solve different problems from image classification and
    image inpainting, to self-driving cars and speech generation. The major reason
    is twofold, outlined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Availability of large high-quality dataset**: The internet resulted in the
    generation of an enormous amount of datasets in terms of images, video, text,
    and audio. While most of it''s unlabeled, by the effort of many leading researchers
    (for example, Fei Fei Li creating the ImageNet dataset), we finally have access
    to large labeled datasets. If DL is a furnace lighting your imagination, data
    is the fuel burning it. The greater the amount and variety of the data, the better
    the performance of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability of parallel computing using graphical processing units**: In
    DL models, there are mainly two mathematical matrix operations that play a crucial
    role, namely, matrix multiplication and matrix addition. The possibility of parallelizing
    these processes for all the neurons in a layer with the help of **graphical processing
    units** (**GPUs**) made it possible to train the DL models in reasonable time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the interest in DL grew, people came up with further improvements, like
    better optimizers for the gradient descent (the necessary algorithm used to calculate
    weight and bias update in DL models), for example, Adam and RMSprop; new regularization
    techniques such as dropout and batch normalization that help, not only in overfitting,
    but can also reduce the training time, and last, but not the least, availability
    of DL libraries such as TensorFlow, Theano, Torch, MxNet, and Keras, which made
    it easier to define and train complex architectures.
  prefs: []
  type: TYPE_NORMAL
- en: According to Andrew Ng, founder of [deeplearning.ai](https://www.deeplearning.ai/),
    despite plenty of hype and frantic investment, we won't see another AI winter,
    because improvements in the computing devices *will keep the performance advances
    and breakthroughs coming for the foreseeable future*, Andrew Ng said this at EmTech
    Digital in 2016, and true to his prediction, we have seen advancements in the
    processing hardware with Google's **Tensor Processing Unit** (**TPUs**), Intel
    Movidius, and NVIDIA's latest GPUs. Moreover, there are cloud computing GPUs that
    are available today at as low as 0.40 cents per hour, making it affordable for
    all.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read the complete article *AI Winter Isn''t Coming*, published in MIT
    Technology Review: [https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/](https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/).
    Here Andrew Ng answers different queries regarding the future of AI.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For DL, GPU processing power is a must; there are a large number of companies
    offering cloud computing services for the same. But in case you are starting in
    the field, you can use one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Colaboratory**: It provides a browser-based, GPU enabled Jupyter Notebook—like
    interface. It gives free access to the GPU computing power for 12 continuous hours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kaggle**: Kaggle too provides a Jupyter Notebook style interface with GPU
    computing power for roughly six continuous hours free of cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fundamental component of all DL models is an artificial neuron. The artificial
    neuron is inspired by the working of biological neurons. It consists of some inputs
    connected via weights (also called **synaptic connections**), the weighted sum
    of all the inputs goes through a processing function (called the **activation
    function**) and generates a non-linear output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows **A biological Neuron** and **An Artificial
    Neuron**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/533935b8-b03c-47b0-835f-3315a1a9361a.png)'
  prefs: []
  type: TYPE_IMG
- en: A biological neuron and an artificial neuron
  prefs: []
  type: TYPE_NORMAL
- en: 'If *X[i]* is the *i*^(th) input to the artificial neuron (*j*) connected via
    the synaptic connection *w[ij]*, then, the net input to the neuron, commonly called
    the **activity of the neuron**, can be defined as the weighted sum of all its
    contains, and is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d7fd5ac-b237-47c6-ba7a-c7d2a56f0415.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, *N* is the total number of inputs to the *j*^(th)
    neuron, and *θ[j]* is the threshold of the *j*^(th) neuron; the output of the
    neuron is then given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94f64405-a749-4631-8e50-1bf6d36bca10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding, *g* is the activation function. The following point lists
    different activation functions used in different DL models, along with their mathematical
    and graphical representations:'
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid: ![](img/3a88f5a9-2bb0-4f1e-89b3-117b96da4945.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c64270c0-bc9b-4917-81cf-3f050fb76f44.png)'
  prefs: []
  type: TYPE_IMG
- en: Hyperbolic Tangent: *g(h[j])= tanh(h[j])*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a8bc9f89-bf33-4b53-aae5-edf3fe12c45e.png)'
  prefs: []
  type: TYPE_IMG
- en: ReLU: *g(h[j])= max(0,h[j])*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1d0fe159-f070-4193-bb05-61dc1f5045d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Softmax: ![](img/7c124da7-6c92-4b9b-9d4c-d79ccae0cc89.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4197a960-7e58-4b01-890f-e611b8ee4062.png)'
  prefs: []
  type: TYPE_IMG
- en: Leaky ReLU: ![](img/71459d3e-9c52-47bf-b038-9db5e584f5a8.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/e2167991-26bb-4f98-8325-3663bb75cdac.png)'
  prefs: []
  type: TYPE_IMG
- en: ELU: ![](img/1ae102ff-1354-456c-a111-6c2f99642980.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9854bc26-8a85-49c1-b321-c25993a7f369.png)'
  prefs: []
  type: TYPE_IMG
- en: Threshold: ![](img/b6ede6c5-55ce-4340-8fdb-cfecc66880ad.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7585376a-d5a9-4212-9916-d07b03419737.png)'
  prefs: []
  type: TYPE_IMG
- en: Modelling single neuron in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Can we use this single neuron and make it learn? The answer is yes, the process
    of learning involves adapting the weights such that a predefined loss function
    (*L*) reduces. If we update the weights in the direction opposite to the gradient
    of the loss function with respect to weights, it will ensure that loss function
    decreases with each update. This algorithm is called the **gradient descent**
    algorithm, and is at the heart of all DL models. Mathematically, if *L* is the
    loss function and *η* the learning rate, then the weight *w[ij]* is updated and
    represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0ba28af-252f-4fd5-9a6c-77ac7ac240de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we have to model the single artificial neuron, we need first to decide the
    following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning rate parameter**: Learning rate parameter determines how fast we
    descent the gradient. Conventionally, it lies between *0* and *1*. If learning
    rate is too high, the network may either oscillate around the correct solution
    or completely diverge from the solution. On the other hand, when learning rate
    is too low, it will take a long time to converge to the solution finally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function**: The activation function decides how the output of
    the neuron varies with its activity. Since the weight update equation involves
    a derivative of the loss function, which in turn will depend on the derivative
    of the activation function, we prefer a continuous-differentiable function as
    the activation function for the neuron. Initially, sigmoid and hyperbolic tangent
    were used, but they suffered from slow convergence and vanishing gradients (the
    gradient becoming zero, and hence, no learning, while the solution hasn''t been
    reached). In recent years, **rectified linear units** (**ReLU**) and its variants
    such as leaky ReLU and ELU are preferred since they offer fast convergence and
    at the same time, help in overcoming the vanishing gradient problem. In ReLU,
    we sometimes have a problem of **dead neurons**, that is some neurons never fire
    because their activity is always less than zero, and hence, they never learn.
    Both leaky ReLU and ELU overcome the problem of dead neurons by ensuring a non-zero
    neuron output, even when the activity is negative. The lists of the commonly used
    activation functions, and their mathematical and graphical representations is
    explained before this section. (You can play around with the `activation_functions.ipynb` code ,
    which uses TensorFlow defined activation functions.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**: Loss function is the parameter our network tries to minimize,
    and so choosing the right loss function is crucial for the learning. As you will
    delve deep into DL, you will find many cleverly defined loss functions. You will
    see how, by properly defining loss functions, we can make our DL model create
    new images, visualize dreams, or give a caption to an image, and much more. Conventionally,
    depending on the type of task regression or classification, people use **mean
    square error** (**MSE**) or **categorical-cross entropy** loss function. You will
    learn these loss functions as we progress through the book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we know the basic elements needed to model an artificial neuron, let''s
    start with the coding. We will presume a regression task, and so we will use MSE
    loss function. If *y[j]* is the output of our single neuron for the input vector
    *X* and ![](img/ae1b66ec-7c89-4339-b04e-9c4f506272e1.png) is the output we desire
    for output neuron *j*, then the MSE error is mathematically expressed as (mean
    of the square of the error ![](img/a6adaa8f-16fa-49c4-af70-e3844c7cd9d8.png)),
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b1d1c78-aff6-46b5-bdd6-05c9058ff9f0.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding, *M* is the total number of training sample (input-output pair).
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you were to implement this artificial neuron without using TensorFlow
    (to be specific without using any of the DL libraries mentioned earlier), then
    you will need to calculate the gradient yourself, for example, you will write
    a function or a code that will first compute the gradient of loss function, and
    then you will have to write a code to update all of the weights and biases. For
    a single neuron with the MSE loss function, calculating derivative is still straightforward,
    but as the complexity of the network increases, calculating the gradient for the
    specific loss function, implementing it in code, and then finally updating weights
    and biases can become a very cumbersome act.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow makes this whole process easier by using automatic differentiation.
    TensorFlow specifies all the operations in a TensorFlow graph; this allows it
    to use the chain rule and go complicated in the graph assigning the gradients.
  prefs: []
  type: TYPE_NORMAL
- en: And so, in TensorFlow we build the execution graph, and define our loss function,
    then it calculates the gradient automatically, and it supports many different
    gradients, calculating algorithms (optimizers), which we can conveniently use.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can learn more about the concept of automatic differentiation through this
    link: [http://www.columbia.edu/~ahd2125/post/2015/12/5/](http://www.columbia.edu/~ahd2125/post/2015/12/5/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now with all this basic information, we build our single neuron in TensorFlow
    with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step, in every Python code, is always importing the modules one will
    need in the rest of the program. We will import TensorFlow to build the single
    artificial neuron. Numpy and pandas are there for any supporting mathematical
    calculations and for reading the data files. Beside this, we are also importing
    some useful functions (for normalization of data, splitting it into train, validation,
    and shuffling the data) from scikit-learn, we have already used these functions
    in the earlier chapters and know that normalization and shuffling is an important
    step in any AI pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As explained earlier, validation helps in knowing if the model has learned or
    it's overfitting or underfitting
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow, we first build a model graph and then execute it. This might,
    when starting, seem complicated, but once you get the hang of it, it''s very convenient
    and allows us to optimize the code for production. So, let''s first define our
    single neuron graph. We define `self.X` and `self.y` as placeholders to pass on
    the data to the graph, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The weights and biases are defined as variables so that the automatic differentiation
    automatically updates them. TensorFlow provides a graphical interface to support
    TensorBoard to see the graph structure, as well as different parameters, and how
    they change during training. It''s beneficial for debugging and understanding
    how your model is behaving. In the following code, we, therefore, add code lines
    to create histogram summaries for both weights and biases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we perform the mathematical operations, the matrix multiplication, between
    input and weights, add the bias, and calculate the activity of the neuron and
    its output, denoted by `self.y_hat` shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the loss function that we want our model to minimize, and use the
    TensorFlow optimizer to minimize it, and update weights and biases using the gradient
    descent optimizer, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We complete the `init` function by defining a TensorFlow Session and initializing
    all the variables. We also add code to ensure that TensorBoard writes all the
    summaries at the specified place, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the `train` function where the graph we previously built is executed,
    as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To make a prediction, we also include a `predict` method, as shown in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, like in the previous chapter, we read the data, normalize it using scikit-learn
    functions, and split it into training and validation set, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the artificial neuron we created to make the energy output prediction.
    `Training Loss` and `Validation Loss` are plotted as the artificial neuron learns,
    as shown in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d4f9e5f6-53e1-4a90-8053-4dc1f12be2f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Mean square error for training and validation data as the single artificial
    neuron learns to predict the energy output
  prefs: []
  type: TYPE_NORMAL
- en: The complete code with data reading, data normalization, training, and so on
    is given in the `single_neuron_tf.ipynb` Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayered perceptrons for regression and classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, you learned about a single artificial neuron and used it
    to predict the energy output. If we compare it with the linear regression result
    of [Chapter 3](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml), *Machine Learning
    for IoT*, we can see that though the single neuron did a good job, it was not
    as good as linear regression. The single neuron architecture had an MSE value
    of 0.078 on the validation dataset as compared 0.01 of linear regression. Can
    we make it better, with maybe more epochs, or different learning rate, or perhaps
    more single neurons. Unfortunately not, single neurons can solve only linearly
    separable problems, for example, they can provide a solution only if there exists
    a straight line separating the classes/decision.
  prefs: []
  type: TYPE_NORMAL
- en: The network with a single layer of neurons is called **simple perceptron**.
    The perceptron model was given by Rosenblatt in 1958 ([htt](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)[p://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;rep=rep1&amp;type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)).
    The paper created lots of ripples in the scientific community and lots of research
    was initiated in the field. It was first implemented in hardware for the task
    of image recognition. Although perceptron seemed very promising initially, the
    book *Perceptrons* by Marvin Minsky and Seymour Papert proved that simple perceptron
    can solve only linearly separable problems ([https://books.google.co.in/books?hl=en&amp;lr=&amp;id=PLQ5DwAAQBAJ&amp;oi=fnd&amp;pg=PR5&amp;dq=Perceptrons:+An+Introduction+to+Computational+Geometry&amp;ots=zyEDwMrl__&amp;sig=DfDDbbj3es52hBJU9znCercxj3M#v=onepage&amp;q=Perceptrons%3A%20An%20Introduction%20to%20Computational%20Geometry&amp;f=false](https://books.google.co.in/books?hl=en&lr=&id=PLQ5DwAAQBAJ&oi=fnd&pg=PR5&dq=Perceptrons:+An+Introduction+to+Computational+Geometry&ots=zyEDwMrl__&sig=DfDDbbj3es52hBJU9znCercxj3M#v=onepage&q=Perceptrons%3A%20An%20Introduction%20to%20Computational%20Geometry&f=false)).
  prefs: []
  type: TYPE_NORMAL
- en: 'So what do we do? We can use multiple layers of single neurons, in other words,
    use MLP. Just as in real life, we solve a complex problem by breaking it into
    small problems, each neuron in the first layer of the MLP breaks the problem into
    small linearly separable. Since the information flows here in one direction from
    the input layer to the output layer via hidden layers, this network is also called
    a **feedforward** network. In the following diagram, we see how the **XOR** problem
    is solved using two neurons in the first layer, and a single neuron in the **Output
    Layer**. The network breaks the non-linearly separable problem into three linearly
    separable problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91008d5d-b096-4775-a7e0-778e0a396226.png)'
  prefs: []
  type: TYPE_IMG
- en: Previous diagram can be explained as XOR solved using MLP with one hidden layer
    with neurons and one neuron in the output layer. Red points represent zero and
    blue points represent one. We can see that the hidden neurons separate the problem
    into two linearly separable problems (AND and OR), the output neuron then implements
    another linearly separable logic the AND-NOT logic, combining them together we
    are able to solve the XOR, which is not linearly separable
  prefs: []
  type: TYPE_NORMAL
- en: The hidden neurons transform the problem into a form that output layer can use.
    The idea of multiple layers of neurons was given by McCulloch and Pitts earlier,
    but while Rosenblatt gave the learning algorithm for simple perceptrons, he had
    no way of training multiple layered percetrons. The major difficulty was that,
    while for the output neurons we know what should be the desired output and so
    can calculate the error, and hence, the loss function and weight updates using
    gradient descent, there was no way to know the desired output of hidden neurons.
    Hence, in the absence of any learning algorithm, MLPs were never explored much.
    This changed in 1982 when Hinton proposed the backpropagation algorithm ([https://www.researchgate.net/profile/Yann_Lecun/publication/2360531_A_Theoretical_Framework_for_Back-Propagation/links/0deec519dfa297eac1000000/A-Theoretical-Framework-for-Back-Propagation.pdf](https://www.researchgate.net/profile/Yann_Lecun/publication/2360531_A_Theoretical_Framework_for_Back-Propagation/links/0deec519dfa297eac1000000/A-Theoretical-Framework-for-Back-Propagation.pdf)),
    which can be used to calculate the error, and hence, the weight updates for the
    hidden neurons. They employed a neat and straightforward mathematical trick of
    differentiation using the chain rule, and solved the problem of passing the errors
    at the output layer back to the hidden neurons, and in turn, boosted life back
    to neural networks. Today, backpropagation algorithm is at the heart of almost
    all DL models.
  prefs: []
  type: TYPE_NORMAL
- en: The backpropagation algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first gain a little understanding of the technique behind the backpropagation
    algorithm. If you remember from the previous section, the loss function at the
    output neuron is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfa9f5dc-ecb4-45ef-9369-63e2229da5ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can see that it''s unchanged, and so the weight connecting hidden neuron
    *k* to the output neuron *j* would be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/078b9093-0555-493a-af10-c031448c46a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Applying the chain rule of differentiation, this reduces to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26448358-62bd-4f0b-9a6d-c89e12e8c77e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, *O[k]* is the output of the hidden neuron *k*. Now
    the weight update connecting input neuron *i* to the hidden neuron *k* of hidden
    layer *n* can be written as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13803de4-5171-4668-ad06-bf08334d91ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Again applying the chain rule, it reduces to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd1d50af-50cd-43a9-b6bf-97fd3f629ef1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *O[i]* is the output of the hidden neuron *i* in the *n-1^(th)* hidden
    layer. Since we are using TensorFlow, we need not bother with calculating these
    gradients, but still, it's a good idea to know the expressions. From these expressions,
    you can see why it's important that the activation function is differentiable.
    The weight updates depend heavily on the derivative of the activation function,
    as well as the inputs to the neurons. Therefore, a smooth derivative function
    like that in the case of ReLU and ELU result in faster convergence. If the derivative
    becomes too large, we have the problem of exploding gradients, and if the derivative
    becomes almost zero, we have the problem of vanishing gradients. In both cases,
    the network does not learn optimally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Universal approximation theorem: in 1989 Hornik et al. and George Cybenko independently
    proved the universal approximation theorem. The theorem, in its simplest form,
    states that a large enough feedforward multilayered perceptron, under mild assumptions
    on activation function, with a single hidden layer, can approximate any Borel
    measurable function with any degree of accuracy we desire.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In simpler words, it means that the neural network is a universal approximator,
    and we can approximate any function, listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We can do so using a single hidden layer feedforward network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can do so provided the network is large enough (that is add more hidden neurons
    if needed).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cybenko proved it for sigmoid activation function at the hidden layer, and linear
    activation function at the output layer. Later, Hornik et al showed that it's
    actually the property of MLPs and can be proved for other activation functions
    too
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The theorem gives a guarantee that MLP can solve any problem, but does not give
    any measure on how large the network should be. Also, it does not guarantee learning
    and convergence.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the papers using the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hornik et al.: [https://www.sciencedirect.com/science/article/pii/0893608089900208](https://www.sciencedirect.com/science/article/pii/0893608089900208)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cybenko: [https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf](https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we can describe the steps involved in the backpropagation algorithm, listed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the input to the network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Propagate the input forward and calculate the output of the network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the loss at the output, and then using the preceding expressions,
    calculate weight updates for output layer neuron
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the weighted errors at output layers, calculate the weight updates for
    hidden layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update all the weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the steps for other training examples
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Energy output prediction using MLPs in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now see how good an MLP is for predicting energy output. This will be
    a regression problem. We will be using a single hidden layer MLP and will predict
    the net hourly electrical energy output from a combined cycle power plant. The
    description of the dataset is provided in [Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml),
    *Principles and foundations of IoT and AI*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since it''s a regression problem, our loss function remains the same as before.
    The complete code implementing the `MLP` class is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Before using it, let''s see the differences between previous code and the code
    we made earlier for the single artificial neuron. Here the dimensions of weights
    of hidden layer is `#inputUnits × #hiddenUnits`; the bias of the hidden layer
    will be equal to the number of hidden units (`#hiddenUnits`). The output layer
    weights have the dimensions `#hiddenUnits × #outputUnits`; the bias of output
    layer is of the dimension of the number of units in the output layer (`#outputUnits`).'
  prefs: []
  type: TYPE_NORMAL
- en: In defining the bias, we have used only the column dimensions, not row. This
    is because TensorFlow like `numpy` broadcasts the matrices according to the operation
    to be performed. And by not fixing the row dimensions of bias, we are able to
    maintain the flexibility of the number of input training samples (batch-size)
    we present to the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following screenshot shows  matrix multiplication and addition dimensions while
    calculating activity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5f59276a-59a7-490a-8c0b-09153b41bb2b.png)'
  prefs: []
  type: TYPE_IMG
- en: The matrix multiplication and addition dimensions while calculating activity
  prefs: []
  type: TYPE_NORMAL
- en: 'The second difference that you should note is in the definition of loss, we
    have added here the `l2` regularization term to reduce overfitting as discussed
    in [Chapter 3](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml), *Machine Learning
    for IoT*, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After reading the data from the `csv` file and separating it into training
    and validation like before, we define the `MLP` class object with `4` neurons
    in the input layer, `15` neurons in the hidden layer, and `1` neuron in the output
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code we train the model on training dataset for `6000` epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This trained network gives us an MSE of 0.016 and an *R²* value of 0.67\. Both
    are better than what we obtained from a single neuron, and comparable to the ML
    methods we studied in [Chapter 3](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml),
    *Machine Learning for IoT*. The complete code can be accessed in the file named
    `MLP_regresssion.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can play around with hyperparameters namely: the number of hidden neurons,
    the activation functions, the learning rate, the optimizer, and the regularization
    coefficient, and can obtain even better results.'
  prefs: []
  type: TYPE_NORMAL
- en: Wine quality classification using MLPs in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLP can be used to do classification tasks as well. We can reuse the MLP class
    from the previous section with minor modifications to perform the task of classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to make the following two major changes:'
  prefs: []
  type: TYPE_NORMAL
- en: The target in the case of classification will be one-hot encoded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The loss function will now be categorical cross-entropy loss: `tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y_hat,
    labels=self.y))`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So let''s now see the complete code, which is also available at GitHub in the
    file `MLP_classification`. We will be classifying the red wine quality, to make
    it convenient, we use only two wine classes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the necessary modules namely: TensorFlow, Numpy, Matplotlib, and
    certain functions from scikit-learn, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We defined our `MLP` class, it''s very similar to the `MLP` class you saw earlier,
    the only difference is in the definition of the loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we read the data, normalize it, and preprocess it so that wine quality
    is one-hot encoded with two labels. We also divide the data into training and
    validation set, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We define an `MLP` object and train it, demonstrated in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Following, you can see the results of training, the cross-entropy loss decreases
    as the network learns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/83e8116c-41c8-4e3e-a3c8-d1fb3a3f30cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The trained network, when tested on the validation dataset, provides an accuracy
    of 77.8%. The `confusion_matrix` on the validation set is shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a480aca2-4bdf-4a12-96cd-1916ece2f3aa.png)'
  prefs: []
  type: TYPE_IMG
- en: These results are again comparable to the results we obtained using ML algorithms.
    We can make it even better by playing around with the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLPs were fun, but as you must have observed while playing with MLP codes in
    the previous section, the time to learn increases as the complexity of input space
    increases; moreover, the performance of MLPs is just second to the ML algorithms.
    Whatever you can do with MLP, there's a high probability you can do it slightly
    better using ML algorithms you learned in [Chapter 3](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml),
    *Machine Learning for IoT*. Precisely for this reason, despite backpropagation
    algorithm being available in the 1980s, we observed the second AI winter roughly
    from 1987 to 1993.
  prefs: []
  type: TYPE_NORMAL
- en: This all changed, and the neural networks stopped playing the second fiddle
    to ML algorithms, in the 2010s with the development of deep neural networks. Today
    DL has achieved human level or more than human level performance in varied tasks
    of computer vision like recognizing traffic signals ([http://people.idsia.ch/~juergen/cvpr2012.pdf](http://people.idsia.ch/~juergen/cvpr2012.pdf)),
    faces ([https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf)),
    handwritten digits, ([https://cs.nyu.edu/~wanli/dropc/dropc.pdf](https://cs.nyu.edu/~wanli/dropc/dropc.pdf))
    and so on. The list is continuously growing.
  prefs: []
  type: TYPE_NORMAL
- en: CNN has been a major part of this success story. In this section, you will learn
    about CNN, the maths behind CNN, and some of the popular CNN architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Different layers of CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CNN consists of three main types of neuron layers: convolution layers, pooling
    layers, and fully connected layers. Fully connected layers are nothing but layers
    of MLP, they are always the last few layers of the CNN, and perform the final
    task of classification or regression. Let''s see how the convolution layer and
    max pooling layers work.'
  prefs: []
  type: TYPE_NORMAL
- en: The convolution layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the core building block of CNNs. It performs the mathematical operation
    similar to convolution (cross-correlation to be precise) on its input, normally
    a 3D image. It's defined by kernels (filters). The basic idea is that these filters
    stride through the entire image and extract specific features from the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before going into further details, let''s first see the convolution operation
    on a two-dimensional matrix for simplicity. The following diagram shows the operation
    when one pixel placed at position [2, 2] of a 5×5 **2D image** matrix is convolved
    with a 3×3 filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73305444-5f38-4e55-b2e1-d901a0aacfc8.png)'
  prefs: []
  type: TYPE_IMG
- en: Convolution operation at a single pixel
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation involves placing the filter with the pixel at the
    center, then performing element-wise multiplication between the filter elements
    and the pixel, along with its neighbors. Finally, summing the product. Since convolution
    operation is performed on a pixel, the filters are conventionally odd-sized like
    5×5, 3×3, or 7×7, and so on. The size of the filters specify how much neighboring
    area it's covering.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important parameters when designing the convolution layers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The size of the filters (k×k).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of filters in the layer, also called **channels**. The input color
    image is present in the three RGB channels. The number of channels are conventionally
    increased in the higher layers. Resulting in deeper information in higher layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of pixels the filter strides (s) through the image. Conventionally,
    the stride is of one pixel so that the filter covers the entire image starting
    from top-left to bottom-right.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The padding to be used while convolving. Traditionally, there are two options,
    either valid or same. In **valid** padding, there''s no padding at all, and thus
    the size of the convolved image is less than that of the original. In **same**,
    the padding of zeros is done around the boundary pixels, so that the size of the
    convolved image is the same as that of the original image. The following screenshot
    shows the complete **Convolved Image**. The green square of size 3×3 is the result
    when padding is valid, the complete 5×5 matrix on the right will be the result
    when padding is the same:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/180e138e-d181-4379-9469-50ec5fddde35.png)'
  prefs: []
  type: TYPE_IMG
- en: Convolution operation applied on a 5×5 image
  prefs: []
  type: TYPE_NORMAL
- en: The green square on the right will be the result of **valid** padding. For the
    **same** padding, we will get the complete 5×5 matrix shown on the right-hand
    side.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The convolution layer is followed conventionally by a pooling layer. The purpose
    of the pooling layer is to progressively reduce the size of the representation,
    and thus, reduce the number of parameters and computations in the network. Thus,
    it down samples the information as it propagates through the network in feed forward
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here again, we have a filter, traditionally people prefer a filter of size
    2×2, and it moves with a stride of two pixels in both directions. The pooling
    process replaces the four elements under the 2×2 filter by either the maximum
    value of the four (**Max Pooling**) or the average value of the four (**Average
    Pooling**). In the following diagram, you can see the result of pooling operation
    on a **2D single channel slice of an image**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d703e932-41f1-4053-bd23-c56664cc31db.png)'
  prefs: []
  type: TYPE_IMG
- en: Max pooling and average pooling operation on a two-dimensional single depth
    slice of an image
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple convolution pooling layers are stacked together to form a deep CNN.
    As the image propagates through the CNN, each convolutional layer extracts specific
    features. The lower layers extract the gross feature like shape, curves, lines,
    and so on, while the higher layers extract more abstract features like eyes, lips,
    and so on. The image, as it propagates through the network, reduces in dimensions,
    but increases in depth. The output from the last convolutional layer is flattened
    and passed to fully connected layers, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c80f02d4-6331-41f7-9fbb-df3ea4545697.png)'
  prefs: []
  type: TYPE_IMG
- en: The basic architecture of a CNN network
  prefs: []
  type: TYPE_NORMAL
- en: The values of filter matrix are also called **weights** and they are shared
    by the whole image. This sharing reduces the number of training parameters. The
    weights are learned by the network using the backpropagation algorithm. Since
    we will be using the auto-differentiation feature of TensorFlow, we are not calculating
    the exact expression for weight update for convolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: Some popular CNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a list of some of the popular CNN models available:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LeNet**: LeNet was the first successful CNN applied to recognize handwritten
    digits. It was developed by Yann LeCun in the 1990s. You can know more about LeNet
    architecture and its related publications at Yann LeCun''s home page ([http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VGGNet**: This was the runner-up in ILSVRC 2014, developed by Karen Simonyan
    and Andrew Zisserman. Its first version contains 16 Convolution+FC layers and
    was called **VGG16**, later they brought VGG19 with 19 layers. The details about
    its performance and publications can be accessed from the University of Oxford
    site ([http://www.robots.ox.ac.uk/~vgg/research/very_deep/](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ResNet**: Developed by Kaiming He et al., ResNet was the winner of ILSVRC
    2015\. It made use of new feature called **residual learning** and **batch normalization**.
    It''s a very deep network with more than 100 layers. It''s known that adding more
    layers will improve the performance, but adding layers also introduced the problem
    of vanishing gradients. ResNet solved this issue by making use of identity shortcut
    connection, where the signal skips one or more layers. You can read the original
    paper for more information ([https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GoogleNet**: This was the winning architecture of ILSVRC 2014\. It has 22
    layers, and introduced the idea of inception layer. The basic idea is to cover
    a bigger area, while at the same time, keep a fine resolution for small information
    on the images. As a result instead of one size filters, at each layer, we have
    filter ranging from 1×1 (for fine detailing) to 5×5\. The result of all the filters
    are concatenated and passed to next layer, the process is repeated in the next
    inception layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeNet to recognize handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the chapters ahead, we will be using some of these popular CNNs and their
    variants to solve image and video processing tasks. Right now, let's use the LeNet
    architecture proposed by Yann LeCun to recognize handwritten digits. This architecture
    was used by the US Postal Service to recognize handwritten ZIP codes on the letters
    they received ([http://yann.lecun.com/exdb/publis/pdf/jackel-95.pdf](http://yann.lecun.com/exdb/publis/pdf/jackel-95.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'LeNet consists of five layers with two convolutional max pool layers and three
    fully connected layers. The network also uses dropout feature, that is while training,
    some of the weights are turned off. This forces the other interconnections to
    compensate for them, and hence helps in overcoming overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: We import the necessary modules, shown as follows
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the class object `LeNet`, which will have the necessary CNN
    architecture and modules to train and make the prediction. In the `__init__` method,
    we define all the needed placeholders to hold input images and their output labels.
    We also define the loss, since this is a classification problem, we use cross-entropy
    loss, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `model` method is the one where the convolutional network architecture
    graph is actually build. We use the TensorFlow `tf.nn.conv2d` function to build
    the convolutional layers. The function takes an argument the filter matrix defined
    as weights and computes the convolution between the input and the filter matrix.
    We also use biases to give us a high degree of freedom. After the two convolution
    layers, we flatten the output and pass it to the fully connected layers, shown
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fit` method performs the batch-wise training, and `predict` method provides
    the output for given input, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the handwritten digits dataset and download it from Kaggle ([https://www.kaggle.com/c/digit-recognizer/data](https://www.kaggle.com/c/digit-recognizer/data)).
    The dataset is available in `.csv` format. We load the `.csv` files and preprocess
    the data. The following are the sample training diagrams:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1c22399b-ebc8-4748-a2d9-961e33840c5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here we will be training the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the `LeNet` object and train it on the training data. The obtain
    is 99.658% on the training dataset and 98.607% on the validation dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Impressive! You can predict the output for the test dataset and make a submission
    at Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The models that we have studied till now respond only present input. You present
    them an input, and based on what they have learned, they give you a corresponding
    output. But this is not the way we humans work. When you are reading a sentence,
    you do not interpret each word individually, you take the previous words into
    account to conclude its semantic
  prefs: []
  type: TYPE_NORMAL
- en: meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs are able to address this issue. They use the feedback loops, which preserves
    the information. The feedback loop allows the information to be passed from the
    previous steps to the present. The following diagram shows the basic architecture
    of an RNN and how the feedback allows the passing of information from one step
    of the network to the next (**Unroll**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3053a864-2e59-4b74-af9e-b0baa5861d37.png)'
  prefs: []
  type: TYPE_IMG
- en: Recurrent neural network
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, *X* represents the inputs. It''s connected to the
    neurons in the hidden layer by weights *W[hx]*, the output of the hidden layer,
    *h*, is fed back to the hidden layer via weights *W[hh]*, and also contributes
    to the output, *O,* via weights *W[yh]*. We can write the mathematical relationships
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08a55795-65bb-469e-9035-07107aa07d3d.png)![](img/2dfa68b4-ad21-402e-ad0a-dc306c307622.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *g* is the activation function, *b[h]* and *b[y]* are the biases of hidden
    and output neurons, respectively. In the a preceding relation all *X*, *h*, and
    *O* are vectors; *W[hx]*, *W[hh],* and *W[yh]* are matrices. The dimensions of
    the input *X* and the output *O* depends upon the dataset you are working on,
    and the number of units in hidden layer *h* are decided by you; you will find
    many papers where researchers have used 128 number of hidden units. The preceding
    architecture shows only one hidden layer, but we can have as many hidden layers
    as we want. RNNs have been applied in the field of natural language processing,
    they have also been applied to analyze the time series data, like stock prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs learn via an algorithm called as **backpropagation** **through time**
    (**BPTT**), it''s a modification of backpropagation algorithm that takes into
    account the time series nature of data. Here, the loss is defined as the sum of
    all the loss functions at times *t*=*1* to *t*=*T* (number of time steps to be
    unrolled), for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5edc1524-7b79-44e4-8ef6-097685d2548d.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *L^((t))* is the loss at time *t*, we apply the chain rule of differentiation
    like before, and derive the weight updates for weights *W[hx]*, *W[hh]**,* and
    *W[yh.]*
  prefs: []
  type: TYPE_NORMAL
- en: 'We are not deriving the expression for weight updates in this book, because
    we will not be coding it. TensorFlow provides an implementation for RNN and BPTT.
    But for the readers interested in going into the mathematical details, following
    are some references:'
  prefs: []
  type: TYPE_NORMAL
- en: '*On the difficulty of training Recurrent Neural Networks,* Razvan Pascanu,
    Tomas Mikolov, and Yoshua Bengio ([https://arxiv.org/pdf/1211.5063.pdf](https://arxiv.org/pdf/1211.5063.pdf))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning Long-Term Dependencies with Gradient Descent is Difficult*, Yoshua
    Bengio, Patrice Simard, and Paolo Frasconi ([www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf](http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, it will be incomplete not to mention Colah's blog ([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))
    and Andrej Karpathy's blog ([http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))
    for an excellent explanation of RNNs and some of their cool applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present the RNN with one input each timestep and predict the corresponding
    output. BPTT works by unrolling all input timesteps. The errors are calculated
    and accumulated for each timestep, later the network is rolled back to update
    the weights. One of the disadvantages of BPTT is that when the number of time
    steps increases, the computation also increases. This makes the overall model
    computationally expensive. Moreover, due to multiple gradient multiplications,
    the network is prone to the vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this issue, a modified version of BPTT, the truncated-BPTT is often
    used. In the truncated-BPTT, the data is processed one timestep at a time and
    the BPTT weight update is performed periodically for a fixed number of time steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can enumerate the steps of the truncated-BPTT algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Present the sequence of *K[1]* time steps of input and output pairs to the network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate and accumulate the errors across *K[2]* time steps by unrolling the
    network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weights by rolling up the network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The performance of the algorithm depends on two hyperparameters *K[1]* and *K[2]*.
    The number of forwarding pass timesteps between updates is represented by *K[1]*,
    it affects how fast or slow the training will be training and the frequency of
    the weight updates. *K[2]* on the other hand, represents the number of timesteps
    that apply to BPTT, it should be large enough to capture the temporal structure
    of the input data.
  prefs: []
  type: TYPE_NORMAL
- en: Long short-term memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber in 1997 proposed a modified RNN model, called the
    **long short-term memory** (**LSTM**) as a solution to overcome the vanishing
    gradient problem. The hidden layer in the RNNs is replaced by an LSTM cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LSTM cell consists of three gates: forget gate, input gate, and the output
    gate. These gates control the amount of long-term memory and the short-term memory
    generated and retained by the cell. The gates all have the `sigmoid` function,
    which squashes the input between *0* and *1*. Following, we see how the outputs
    from various gates are calculated, in case the expressions seem daunting to you,
    do not worry, we will be using the TensorFlow `tf.contrib.rnn.BasicLSTMCell` and
    `tf.contrib.rnn.static_rnn` to implement the LSTM cell, shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/200afd47-761e-48f2-b1f0-96ae371de2c6.png)'
  prefs: []
  type: TYPE_IMG
- en: The basic LSTM cell, *x* is the input to the cell, *h* the short-term memory
    and *c* the long-term memory. The subscript refers to the time
  prefs: []
  type: TYPE_NORMAL
- en: 'At each time step, *t*, the LSTM cell takes three inputs: the input *x[t]*,
    the short-term memory *h*[*t-1*], and the long-term memory *c[t-1]*, and outputs
    the long-term memory *c[t]* at and short-term memory *h[t]*. The subscript to
    *x*, *h,* and *c* refer to the timestep.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Forget Gate** *f*(.) controls the amount of short-term memory, *h*, to
    be remembered for further flow in the present time step. Mathematically we can
    represent Forget Gate *f(.)* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca432207-ce53-40bd-8444-41504c254c53.png)'
  prefs: []
  type: TYPE_IMG
- en: Where σ represents the sigmoid activation function, *W[fx]* and *W[fh]* are
    the weights controlling the influence of input *x[t],* short-term memory *h*[*t*-1],
    and *b[f]* the bias of the forget gate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Input Gate** *i*(.) controls the amount of input and working memory influencing
    the output of the cell. We can express it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1dacd13e-4362-4ae5-ae0a-2fc1ddd995d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **Output Gate** *o*(.) controls the amount of information that''s used
    for updating the short-term memory, and given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a136d33-ba2b-4d30-9184-10a4e4258714.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Beside these three gates, the LSTM cell also computes the candidate hidden
    state ![](img/9e3aa387-e3c4-413f-a38a-1ffc05da350b.png) , which along with the
    input and forget gate, is used to compute the amount of long term memory *c[t]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b0d751d8-781b-4a9c-a4a7-e4a5707b2960.png)![](img/55ff027e-d862-415c-b288-e72b31beaf81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The circle represents the element wise multiplication. The new value of the
    short-term memory is then computed as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3f46d9b-122e-4d86-a254-2be3800cd613.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s now see how we can implement LSTM in TensorFlow in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are using the following modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a class LSTM where we construct the graph and define the LSTM layer
    with the help of TensorFlow `contrib`. To take care of memory, we first clear
    the default graph stack and reset the global default graph using `tf.reset_default_graph()`.
    The input goes directly to the LSTM layer with `num_units` number of hidden units.
    It''s followed by a fully connected output layer with the `out_weights` weights
    and `out_bias` bias. Create the placeholders for input `self.x` and `self.y` label.
    The input is reshaped and fed to the LSTM cell. To create the LSTM layer, we first
    define the LSTM cell with `num_units` hidden units and forget bias set to `1.0`.
    This adds the biases to the forget gate in order to reduce the scale of forgetting
    in the beginning of the training. Reshape the output from the LSTM layer and feed
    it to the fully connected layer, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the methods to train and predict, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In the coming chapters, we will be using the RNN for handling time series production
    and text processing.
  prefs: []
  type: TYPE_NORMAL
- en: Gated recurrent unit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Gated recurrent unit** (**GRU**) is another modification of RNN. It has a
    simplified architecture compared to LSTM and overcomes the vanishing gradient
    problem. It takes only two inputs, the input *x[t]* at time *t* and memory *h[t-1]*
    from time *t*-1\. There are only two gates, **Update G****ate** and **Reset Gate**,
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/914f4d89-33d5-4db6-950b-ba58385b38a9.png)'
  prefs: []
  type: TYPE_IMG
- en: The architecture of a basic GRU cell
  prefs: []
  type: TYPE_NORMAL
- en: 'The update gate controls how much previous memory to keep, and the reset gate
    determines how to combine the new input with previous memory. We can define the
    complete GRU cell by the following four equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae7dba9c-7ee0-4ff2-ada7-42a0a0eec746.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/2acac066-bd31-4b34-b736-ba63b17fc36c.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/ec5aa1a8-224d-4ed5-98c7-27ffa72ed61a.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/37499e4d-e92a-404a-b41c-f4951d582cee.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: Both GRU and LSTM give a comparable performance, but GRU has fewer training
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The models we have learned up to now were learning using supervised learning.
    In this section, we will learn about autoencoders. They are feedforward, non-recurrent
    neural network, and learn through unsupervised learning. They are the latest buzz,
    along with generative adversarial networks, and we can find applications in image
    reconstruction, clustering, machine translation, and much more. They were initially
    proposed in the 1980s by Geoffrey E. Hinton and the PDP group ([http://www.cs.toronto.edu/~fritz/absps/clp.pdf](http://www.cs.toronto.edu/~fritz/absps/clp.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The autoencoder basically consists of two cascaded neural networks—the first
    network acts as an encoder; it takes the input *x* and encodes it using a transformation
    *h* to encoded signal *y*, shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03a3d50f-7635-4a27-ac5d-9ed153780372.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second neural network uses the encoded signal *y* as its input and performs
    another transformation *f* to get a reconstructed signal *r*, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e8c72478-363b-4d88-98d8-8247494a6503.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss function is the MSE with error *e* defined as the difference between
    the original input *x* and the reconstructed signal *r:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ac31af7-19b0-4f8b-b7a8-e1f7d6b3e542.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/6b841e32-381c-4f57-9cd7-47229f08c5f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic architecture of an autoencoder
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram shows an autoencoder with **Encoder** and **Decoder**
    highlighted separately. Autoencoders may have weight sharing, that is, weights
    of decoder and encoder are shared. This is done by simply making them a transpose
    of each other; this helps the network learn faster as the number of training parameters
    is less. There are a large variety of autoencoders for example: sparse autoencoders,
    denoising autoencoders, convolution autoencoders, and variational autoencoders.'
  prefs: []
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A denoising autoencoder learns from a corrupted (noisy) input; we feed the encoder
    network the noisy input and the reconstructed image from the decoder is compared
    with the original denoised input. The idea is that this will help the network
    learn how to denoise an input. The network does not just make a pixel-wise comparison,
    instead, in order to denoise the image, the network is forced to learn the information
    of neighboring pixels as well.
  prefs: []
  type: TYPE_NORMAL
- en: Once the autoencoder has learned the encoded features *y*, we can remove the
    decoder part of the network and use only the encoder part to achieve dimensionality
    reduction. The dimensionally reduced input can be fed to some other classification
    or regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Variational autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another very popular autoencoder is **variational autoencoders** (**VAE**).
    They are a mix of the best of both worlds: DL and the Bayesian inference.'
  prefs: []
  type: TYPE_NORMAL
- en: VAEs have an additional stochastic layer; this layer, after the encoder network,
    samples the data using a Gaussian distribution, and the one after the decoder
    network samples the data using Bernoulli's distribution.
  prefs: []
  type: TYPE_NORMAL
- en: VAEs can be used to generate images. VAEs allow one to set complex priors in
    the latent and learn powerful latent representations. We will learn more about
    them in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered some basic and useful deep neural network models.
    We started with a single neuron, saw its power and its limitations. The multilayered
    perceptron was built for both regression and classification tasks. The backpropagation
    algorithm was introduced. The chapter progressed to CNN, with an introduction
    to the convolution layers and pooling layers. We learned about some of the successful
    CNN and used the first CNN LeNet to perform handwritten digits recognition. From
    the feed forward MLPs and CNNs, we moved forward to RNNs. LSTM and GRU networks
    were introduced. We made our own LSTM network in TensorFlow and finally learned
    about autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will start with a totally new type of AI model genetic
    algorithms. Like neural networks, they too are inspired by nature. We will be
    using what we learned in this chapter and the coming few chapters in the case
    studies we'll do in later chapters.
  prefs: []
  type: TYPE_NORMAL
