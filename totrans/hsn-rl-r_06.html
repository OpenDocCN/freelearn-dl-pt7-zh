<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Multi-Armed Bandit Models</h1>
                </header>
            
            <article>
                
<p>The multi-armed bandit problem is a classic reinforcement learning problem that exemplifies the exploration versus exploitation dilemma. When we have a limited set of resources to base our choices on, it becomes essential to adopt a method to establish which of the alternative competing choices allow us to maximize the expected profit. The name <strong>multi</strong>-<strong>armed bandit</strong> derives from the example of a gambler struggling with a row of slot machines who must decide whether to continue with the current machine or try a different machine.</p>
<p><span>In this chapter, we will get an overview of the basic concepts of the multi-armed bandit model, discover the different techniques that are available to help resolve this problem, and discover the meaning of the action-value implementation. Then, we will learn how to address this problem using a contextual approach, how to implement asynchronous actor-critic agents, and how to implement a multi-armed bandit problem in R.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Multi-armed bandit model</li>
<li>Multi-armed bandit applications</li>
<li>Action-value implementation</li>
<li>Understanding problem solution techniques</li>
<li>Implementing the contextual approach</li>
<li>Understanding asynchronous actor-critic agents</li>
<li>Online advertising using the MAB model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-armed bandit model</h1>
                </header>
            
            <article>
                
<p>A common problem in learning theory is to identify the best option among a set of options, without knowing a priori what benefits each one can give, while minimizing the cost of doing so.</p>
<p class="mce-root"/>
<p>The <strong>multi-armed bandit</strong> (<strong>MAB</strong>) problem takes its name from a known problem faced in decision theory. A gambler must choose which slot machine to play among the many he has in front of him. After playing, he will have a certain degree of knowledge about the rewards that are distributed by some machines, but he will not know anything about the others, so he will be forced to choose between machines that are partly known and machines that are not known.</p>
<p>This problem is ideal for modeling the compromises between the exploitation of known opportunities and the exploration of unknown opportunities, as well as to test strategies in the presence of a high degree of ignorance and uncertainty.</p>
<p>In more technical terms, each slot machine is modeled as a probability distribution, with an average value and one standard deviation. These two parameters can vary over time, either dependently or independently, for example, to model the evolution over time or the dynamism of the competitive scenario, or in response to choices that are made by one or more players to model competition or influence on the context.</p>
<p>The distribution of probabilities is obviously not known to the players but can be learned over time as values ​​are obtained from each slot machine.</p>
<div class="packt_infobox">
<p>MAB problems were introduced by H. Robbins to model decision-making under uncertainty when the environment is unknown. These problems were treated in the following paper: <em>Some Aspects of the Sequential Design of Experiments</em>, <span>Robbins, H. (1952), </span>Bulletin of the American Mathematical Society, 55, 527–535.</p>
</div>
<p>At each time step, only one of the k levers is played and a stochastic reward is observed, where each play of a lever k generates independent and identically distributed samples resulting from some unknown distribution. The goal is to maximize the sum of the rewards that are obtained during all the time steps for a defined temporary interval.</p>
<p>Each algorithm that specifies which leverage should be played, given the past history of the rewards already obtained, represents our policy. The metric that's usually used to measure the performance of the latter is called regret, which is a measure that indicates how much less gain we get, in anticipation, following the chosen policy, than to follow the optimal one, in which the average of the distributions is known a priori of earnings.</p>
<p>Since these distributions are unknown to our policy, it is necessary to learn them through multiple plays, but at the same time, we also want to maximize the reward by choosing to play with machines that have already been rated as good. These two conflicting objectives <span>–</span> exploration of the unknown and exploitation of what is known <span>–</span> exemplify a fundamental trade-off that's present in a wide class of machine learning problems.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mathematical model</h1>
                </header>
            
            <article>
                
<p>A K-armed bandit problem is defined by the random variables <em>X<sub>i, n</sub></em> for 1 ≤ i ≤ K and n ≥ 1 (n is the number of plays), where i is the index that identifies a slot machine lever. By lowering the lever i, the rewards X<sub>i, 1</sub>, X<sub>i, 2</sub>, · · · are obtained, which are independent and identically distributed according to an unknown probability law with an unknown expectation µi (expected value). Also, the rewards between different levers maintain independence, that is, X<sub>i, s</sub> and X<sub>j, t</sub> are independent (and generally not identically distributed) for each 1 ≤ i &lt;j ≤ K and for every s, t ≥ 1. This problem is formally equivalent to a one-state Markov decision-making process.</p>
<p>An allocation strategy is an algorithm that chooses the next lever to be lowered based on the sequence of previous bets and the rewards obtained. As we mentioned previously, the concept of regret is used to measure the performance of the model, which measures the accumulated loss of gains that are obtained by following the chosen policy.</p>
<p>Let <em>T<sub>i</sub> (n)</em> be the number of times the lever <em>i</em> is played by the strategy in the first <em>n</em> plays. Here, the regret of the strategy after <em>n</em> plays is defined by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2ec52c5a-667a-4124-aa7c-b4737119a8b7.png" style="width:17.67em;height:3.83em;"/></p>
<p>Here, we have the following:</p>
<ul>
<li><em>µ* = max<sub>i=1;:::k</sub> µ<sub>i</sub></em></li>
<li><em>Ε(T<sub>k</sub>(T))</em> is the expectation about the number of times the policy will play machine k</li>
</ul>
<p>The goal is to minimize this regret or, equivalently, to maximize the sum of the rewards obtained after n plays. In fact, we can interpret regret as the difference between the maximum possible gain (having thrown the best lever n times, by definition the one that returns µ* as a reward) and the actual gain. </p>
<p>In the next section, you will see practical examples of applications that can be addressed with this technology.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-armed bandit applications</h1>
                </header>
            
            <article>
                
<p class="mce-root">So far, we have seen how to tackle a MAB problem from a mathematical point of view. What are the real applications that can be modeled in this way? We'll look at some examples in the following subsections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Online advertising</h1>
                </header>
            
            <article>
                
<p class="mce-root">In online advertising, a MAB solution uses machine learning algorithms to dynamically allocate advertisements to pages of websites that are performing well, while avoiding ads that show lower performance. In theory, MABs should produce faster results since there is no need to wait for a single winning variant.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">News allocation system</h1>
                </header>
            
            <article>
                
<p class="mce-root">Another application of MAB concerns news sites: when a new user accesses the site, they must choose a news item from a series of articles and the site receives the reward every time the user clicks on the article. Since the site's goal is to maximize the revenue, it wants to show the items that are most likely to get a click. Naturally, the choice depends on the user's characteristics. The problem is that we do not know the probability that an article is clicked, which is the parameter we want to learn about. We can clearly see that the exploration and exploitation dilemma presented in the preceding scenario<span> </span>can be modeled as a MAB problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Health care</h1>
                </header>
            
            <article>
                
<p class="mce-root">Various treatments are available in this area. The manager needs to decide which treatment to use while minimizing the patient's losses. The treatments are experimental and imply that the capacity of the treatment must be learned by performing it on the patients. The aforementioned problem can be modeled as a MAB problem where treatments such as arms and treatment efficiency must be learned. The manager can explore their arms to learn about their success rate (efficiency) or choose to exploit the arm with the best success rate so far.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Staff recruitment</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the selection of new personnel, the application of this methodology represents a powerful tool that's available to employers or requesting services to complete activities in a timely and economical manner. The employer's goal is to maximize the number of tasks completed. The workers that are available act as weapons in this case since they have qualities that are not known to the employer. So, this problem can be posed as a MAB problem, in which the employer can explore their arms to learn their qualities, or choose to exploit the best arm that's been identified so far.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selection of a financial portfolio</h1>
                </header>
            
            <article>
                
<p class="mce-root">The selection of an optimal portfolio is a typical decision problem, and as such, its solution consists of the following elements: the identification of a set of alternatives, using selection criteria to sort through the different possibilities, and the solution of the problem. In order to optimize a financial portfolio, we start by measuring the yield and risk of the products available. The risk-return variables can be considered two sides of the same coin since a certain level of risk will correspond to a given return.</p>
<p class="mce-root">The return can be defined as the sum of the results that are produced by the investment in relation to the capital employed, while the concept of risk can be translated into the degree of variability of returns associated with a given financial instrument. This problem can be modeled as a MAB problem with financial products such as arms and product performance as a result.</p>
<p>In the next section, we will learn how to estimate the action-value function in order to implement the MAB algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Action-value implementation</h1>
                </header>
            
            <article>
                
<p>A general solution to the problem of reinforcement learning is to estimate a value function using the learning process. This function must be able to evaluate, through the sum of the rewards, the convenience or otherwise of a specific policy. To start, we will define the state-value function. </p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">State-value function</h1>
                </header>
            
            <article>
                
<p>A value function represents how good a state is for an agent. It is equal to the total reward that's expected for an agent from the status s. The value function depends on the policy that the agent selects the actions to be performed on.</p>
<p>A policy π associates the probability π (s, a) <span>to the pair (s, a), </span>thus returning the probability that the action a is executed in the state s. Based on this, we can define a value function Vπ (s) as the expected value of the total reinforcement R<sub>t</sub> following the policy π starting from the state s:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fadd9841-a3b9-4187-b6cc-f72c5f23dc87.png" style="width:9.50em;height:3.42em;"/></p>
<p>Here, the sequence of rt is generated following the π policy starting from the s state. In other words, the examples of the training pattern must guide the learning process toward the evaluation of the optimal π * policy, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6ccd9d16-54bb-4191-a50e-6cfcb2f71ea2.png" style="width:12.00em;height:2.17em;"/></p>
<p>Assuming δ (s, a) the function that determines the new state generated by the pair (s, a), we can perform a lookahead search to choose the best action starting from the s state since we can express the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3a4e33aa-cecc-470e-9dd2-6eefbbc57fbc.png" style="width:20.83em;height:2.17em;"/></p>
<p>Here, r (s, a) represents the reward that's obtained from executing an action <span>a </span>in the state <span>s</span>. This solution is only acceptable if the functions are known, as shown in the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/37dd3b9c-c5cb-42b3-862d-46671bd3d51e.png" style="width:5.50em;height:0.83em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3d4d0efc-3f05-4ee5-ae31-a01c95de43af.png" style="width:5.67em;height:0.83em;"/></p>
<p>However, this scenario isn't always respected.</p>
<p>Now that we understand the role that's played by the state-value function, we can move on to the definition of the action-value function.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Action-value function</h1>
                </header>
            
            <article>
                
<p>When this scenario doesn't happen, it is necessary to define a new function similar to Vπ ∗:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ea4da394-538a-44c2-8b43-6ec90a2de576.png" style="width:14.42em;height:1.33em;"/></p>
<p>If the agent is able to estimate the function Q, it is possible to choose the optimal action s, even without knowing the function δ:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d2fe1391-eca0-4b32-9e17-3b6737e94f11.png" style="width:9.67em;height:2.00em;"/></p>
<p>The function <span>Q</span> is usually referred to as an action-value function. Following a policy <span>π, </span>the action-value-function returns the expected reward for using action a in a certain state s.</p>
<p>The main difference between the state-value and the action-value functions is that the Q value allows you <span>– </span>at least in the first phase <span>–</span> to take a different action than the one that was envisaged by the policy. This is because Q reasons in terms of total reward, so in a specific state, it can also return a reward lower than that paid by another action. The state-value function contains the value of reaching a certain state, while the action-value-function contains the value for choosing an action in a state. How is the best action chosen? Let's take a look.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing an action</h1>
                </header>
            
            <article>
                
<p>Suppose we select n-actions: a = a<sub>1</sub>… .a<sub>n</sub>. Each of these actions has its own value of the action-value function. Its estimated value at t-t<sub>h</sub> pitch (play) is Q<sub>t</sub> (a<sub>k</sub>). Recall that the true value of an action is the average reward received when that action is chosen. A natural way to estimate this value is to calculate the average of the rewards that was actually received when the action was chosen. In other words, if at the t-th game the action a was chosen k times before t, obtaining the rewards r<sub>1</sub>, r<sub>2</sub>, ..., r<sub>ka</sub>, then its value is estimated to be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8e395896-9a16-48b1-9d9f-89d358fd694c.png" style="width:12.83em;height:2.42em;"/></p>
<p>Here, we have the following:</p>
<ul>
<li>For k<sub>a</sub> = 0, we set Q<sub>t</sub> (a<sub>k</sub>) to a default value, Q<sub>0</sub> (a<sub>k</sub>) = 0 (no estimate available).</li>
<li>For k<sub>a</sub> → ∞, <span>Q</span><sub>t</sub><span> (a</span><sub>k</sub><span>) </span>→ Q <sup>*</sup> (a<sub>k</sub>) (for the law of large numbers).</li>
</ul>
<p class="mce-root"/>
<p>In all of these cases, the action-value function is calculated as an average (sample-average method).</p>
<p>Now that we've explored the basic concepts of this technology, we will move on and explore the possible solutions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding problem solution techniques</h1>
                </header>
            
            <article>
                
<p>So far, we have formalized the problem and we have seen what tools are available to make the choice of actions that give us the least regret. Now, we can formulate selection methods. We will look at the following problem-solving techniques in the following subsections:</p>
<ul>
<li>Greedy methods</li>
<li>Upper confidence bound</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Greedy methods</h1>
                </header>
            
            <article>
                
<p>This problem, which is much more complex than it may seem, can be tackled by using a very naive strategy, though it's not very effective:</p>
<ol>
<li>Initially, each of the levers is played.</li>
<li>The lever is played that has returned, on average, the highest reward.</li>
</ol>
<p>In this algorithm, we can observe the following:</p>
<ul>
<li>We allow the agent to have memory</li>
<li>We store the value associated with different actions</li>
<li>We choose the action that gave the greatest reward</li>
</ul>
<p>The best action is called the <strong>greedy action</strong> and the algorithm based on this is called the <strong>greedy</strong> method. The following steps are performed:</p>
<ol>
<li>At time step t, estimate a value for each action, as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/86e88567-c9b8-4e0b-8b8c-777cea11e2b1.png" style="width:22.33em;height:2.33em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Select the action with the maximum value:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7aff49af-1728-4099-8810-0c4a40e4f02b.png" style="width:8.08em;height:1.67em;"/></p>
<p>In greedy methods, no alternative solutions are explored through this method. Why do we have to choose an action that doesn't look the best? This is because we explore different solutions since the reward is not deterministic. This implies that we could achieve more with other actions because what matters is not the instant reward but the sum of the rewards that's obtained. In the greedy solution, the algorithm is completely based on exploitation. To improve the performance of the model, it is necessary to introduce exploration. Let's see how.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ε-greedy methods</h1>
                </header>
            
            <article>
                
<p>Here, it is necessary to maintain a predisposition to explore different actions. Once again, we find ourselves dealing with a problem based on the exploration-exploitation dilemma we addressed in detail in <a href="aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml">Chapter 2</a>, <em>Building Blocks of Reinforcement Learning</em>.</p>
<p>Ideally, the agent must explore all the possible actions for each state, finding the one that is actually most rewarded for exploiting it in achieving its goal. Thus, decision-making involves a fundamental choice:</p>
<ul>
<li><strong>Exploitation</strong>: Making the best decision given the current information</li>
<li><strong>Exploration</strong>: Collecting more information</li>
</ul>
<p>In this process, the best long-term strategy can lead to considerable sacrifices in the short term. Therefore, it is necessary to gather enough information to make the best decisions.</p>
<p>I suppose that, with probability ε, a different action is chosen. This action is chosen with a uniform probability between the n possible actions available. The following steps are performed:</p>
<ol>
<li>At time step t, a value for each action is estimated, as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/723adfc6-c091-4d4d-a1fb-0802a8bfddc0.png" style="width:23.17em;height:2.42em;"/></p>
<ol start="2">
<li>With probability 1- 𝜀, the action with the maximum value is selected, as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3d5ca4b6-5d36-44e8-b0a1-3ab2de6d6134.png" style="width:9.67em;height:2.00em;"/></p>
<ol start="3">
<li>With probability 𝜀, an action from all the actions with equal probability is selected randomly.</li>
</ol>
<p>The parameter value that's used the most is 𝜀 =0.1, but this can vary depending on the context. In this approach, we introduce an element of exploration that improves performance. However, if two actions have a very small difference between their Q values, this algorithm will also choose the action that has a higher probability than the others.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ε-greedy methods with a progressive decrease</h1>
                </header>
            
            <article>
                
<p>The exploration that was introduced with the adoption of 𝜀 offers us the opportunity to experiment with options that, so far, are unknown. Nevertheless, the random component of the strategy means that actions that have already been taken that have received poor results can be explored again. Such inefficient exploration can be avoided by progressively reducing the random exploration component.</p>
<p>In other words, by reducing the parameter ε over time, we could explore even less since we have gained confidence in the action, which has a strong potential of optimal value. This strategy offers a highly exploratory behavior in the beginning and a highly exploitative behavior in the end. Let's learn how to carry out exploitation and exploration together.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Upper confidence bound</h1>
                </header>
            
            <article>
                
<p>At the beginning of the game, we don't know which the best arm is. Therefore, we cannot characterize any arm. Thus, the UCB algorithm states that all arms have the same observed average value. So, a confidence limit for each arm will be created and an arm will be selected at random.</p>
<p>In this context, each arm will either give a reward or won't. If the arm that's selected returns a mistake, the average that will be observed by the arm will decrease, as well as the confidence limit. If the arm that's selected returns a reward, the observed average <span>and the confidence limit </span>will increase. By taking advantage of the best, we are decreasing the confidence limit. Adding more and more rounds, the likelihood that the other arms are doing well also increases.</p>
<p class="mce-root"/>
<p>To implement this strategy, follow these steps:</p>
<ol>
<li>At each round, two variables are computed:
<ul>
<li><em>R<sub>i</sub>(n)</em>: The sum of the rewards obtained by the lever <em>i</em> after <em>n</em> plays</li>
<li><span><em>T</em></span><em><sub>i</sub></em><span><em>(n)</em>: The number of times the lever <em>i</em> is played by the strategy in the first <em>n</em> plays</span></li>
</ul>
</li>
<li>We calculate the average rewards obtained by the lever <em>i</em> after <em>n</em> plays using the following formula:</li>
</ol>
<p class="CDPAlignLeft CDPAlign">                                                         <img src="assets/ec54ea62-c453-4a2c-b253-41aab5679c82.png" style="width:6.83em;height:2.75em;"/></p>
<ol start="3">
<li>We calculate the confidence interval after <em>n</em> plays using the following formula:</li>
</ol>
<p class="CDPAlignLeft CDPAlign">                                                      <img src="assets/044dd621-9e6e-4245-8f96-f97a5124f599.png" style="width:8.42em;height:2.92em;"/></p>
<ol start="4">
<li>We select the lever <em>i</em> that returns the maximum UCB as follows:</li>
</ol>
<p class="CDPAlignLeft CDPAlign">                                                          <img src="assets/6edfa64f-85e2-449f-a9a5-09f9aa3311b0.png" style="width:6.75em;height:1.33em;"/></p>
<p>Both of these algorithms keep track of how much they know of any available arm and pay no attention except to how much reward they have obtained from the arms. On the contrary, the algorithms that we've analyzed so far have under-explored the options whose initial experiences have not returned significant rewards, even if they do not have enough data to be sure of those arms.</p>
<p>In the next section, we will introduce the concept of state, which represents a description of the environment that the agent can use to perform targeted actions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the contextual approach</h1>
                </header>
            
            <article>
                
<p>So far, in addressing the problem of the MAB, we have generated an action but we have not exploited any information on the state of the environment (context). The range of actions that are available to the agent consists of pulling one or more arms of the bandit. In this way, a reward of +1 or -1 is received.</p>
<p>The problem is considered to be solved if the agent chooses the arm that increasingly returns a positive reward. In this case, we can design an agent that completely ignores the state of the environment since, in effect, there is always only one immutable state.</p>
<p>In the contextual bandit, the concept of state is introduced, which represents a description of the environment that the agent can use to carry out targeted actions. This model extends the original one by linking the decision to the state of the environment.</p>
<p>The following diagram shows diagrams of the two models:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-474 image-border" src="assets/e65917dc-f7e7-4900-aa21-89c343e8bbec.png" style="width:19.08em;height:15.58em;"/></p>
<p>The original problem is substantially modified in the sense that instead of a single bandit, which we've considered so far, in the new formalization of the problem, there are more bandits. The state of the environment tells us what bandit we're dealing with and the agent's goal is to learn the best action for any available bandit. Since each bandit will have different probabilities of reward for each arm, our agent will have to learn to condition their action on the state of the environment. Unless they do, they will not get the maximum possible reward over time.</p>
<p>With the <strong>contextual bandit</strong> model, you not only optimize the decision based on the previous observations but also personalize the decisions for each situation.</p>
<p>From the preceding model, we can observe the following:</p>
<ul>
<li>The algorithm observes a context.</li>
<li>The algorithm makes a decision by choosing an action from a series of alternative actions.</li>
<li>We can observe the result of this decision, which returns a reward.</li>
<li>The goal is to maximize the average reward.</li>
</ul>
<p class="mce-root"/>
<p>An example of applying this algorithm to the real world is the problem of selecting advertisements to be displayed on a website to optimize the clickthrough rate. The context is information about the user: where it comes from, information about the device that was used, pages of the site that were previously visited, geolocation, and so on. An action corresponds to the choice of which ad to display. One result is whether the user has clicked on a banner or not. A reward is binary: 0 if there is no click, 1 if there is a click. Now, let's learn how to alternate the evaluation of the policy with the improvement of the policy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding asynchronous actor-critic agents</h1>
                </header>
            
            <article>
                
<p>Actor–critic methods implement a generalized policy iteration, alternating between a policy evaluation and a policy improvement step. There are two closely related processes of actor improvement that aim at improving the current policy and critic evaluation by evaluating the current policy. If the process that's defined by the critic has the bootstrap, then the variance is reduced. By doing this, the learning of the algorithm becomes more stable with respect to the methods of the policy gradient.</p>
<p>These methods have the characteristic of separating the memory structure to make the policy independent of the value function. The policy block is known as an actor because it chooses actions, while the estimated value function is called the critic in the sense that it criticizes the actions that are performed by the policy that is being followed. From this, we understand that learning is an on-policy type <span>–</span> in fact, the critic learns and criticizes the work of politics.</p>
<p>We have already introduced the actor-critic model, so now, we will explain the term asynchronous. This is very simple and has effective intuitions, such as the following:</p>
<ul>
<li>The model has several agents exploring the environment at the same time (each agent has a copy of the entire environment).</li>
<li>The model gives different starting policies so that the agents are not related.</li>
<li>In the model, the global status is updated with the contributions of each agent and the process restarts.</li>
</ul>
<p>In 2016, the Google DeepMind group proposed an algorithm named <strong>asynchronous advantage actor-critic</strong> (<strong>A3C</strong>). The algorithm proved to be faster and simpler than most existing algorithms.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In this algorithm, multiple instances of agents are treated, which have been initialized differently in their separate environments. Each agent who begins to act and learn gathers their own experiences. These experiences are then used to update the global neural network shared by all agents. This network affects all the agent actions and each new experience of each agent improves the overall network faster. Because there are multiple instances of this agent, the training will be much faster and more effective.</p>
<p>In the next section, we will apply the concepts we've learned about so far by addressing a practical case.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Online advertising using the MAB model</h1>
                </header>
            
            <article>
                
<p>Online advertising falls into the new media category and takes advantage of the web's ability to reach a significant number of people. Advertising plays a decisive role for companies, which can easily reach a wide audience with lower costs than traditional means. One of the main advantages of internet advertising is the traceability of results or the effect it has on the public. This happens thanks to the ad servers that, in the case of banners, measure the number of views and the effective number of users, clicks.</p>
<p>In online advertising, we can distinguish between the following two types of macros:</p>
<ul>
<li>Contextual</li>
<li>Behavioral advertising</li>
</ul>
<p>In contextual advertising, Google is the typical case where you can place ads according to the words on the page or the type of site or topic that characterizes the website. In behavioral advertising, we select the target using the information we've collected regarding the behavior of each user on the web and on the app (pages visited, searches made) in order to identify their interests and needs, and then submit advertisements in line with them.</p>
<p>In both cases, it is clear the reference to the context needs to interact with the environment. It's also clear that we won't know a priori how the user will behave before an advertisement. These problems can be addressed through a model based on MAB, as follows:</p>
<ul>
<li>The context is represented by the characteristics of visitors and web pages.</li>
<li>Arms are represented by the types of ads that are available.</li>
<li>An action is equivalent to the type of ad to be shown.</li>
<li>The rewards are returned by the visitor's behavior. By clicking on the ad shown, you receive a reward of 1, while by not clicking on the ad, you receive a reward of 0.</li>
</ul>
<p class="mce-root"/>
<p>To make this discussion as understandable as possible, we will limit the number of ads we want to evaluate to three and aim to find which strategy offers the maximum total click rate after a certain number of impressions. So, let's learn how to tackle this problem by adopting the contextual approach. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the contextual package</h1>
                </header>
            
            <article>
                
<p>In the <em>Contextual approach</em> section, we said that, in the contextual bandit, the concept of state is introduced, which represents a description of the environment that the agent can use to carry out targeted actions. This model extends the original one by linking the decision to the state of the environment. In this section, we will see some examples of applications of the armed bandit problem addressed with the contextual approach. To do this, we will use the contextual package.</p>
<p>This package facilitates the simulation and evaluation of context-free and contextual MAB policies or algorithms to ease the implementation, evaluation, and dissemination of existing and new bandit algorithms and policies.</p>
<p>A brief description of the <kbd>diagram</kbd> package, which has been extracted from the official documentation, is shown in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Version</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">0.9.8.2</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Date</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">2019-07-08</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Maintainer</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Robin van Emden</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">License</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">GPL-3</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Authors</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Robin van Emden, Maurits Kaptein</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_infobox">The <kbd>contextual</kbd> package was presented by the authors in the following paper: van Emden, R. and Kaptein, M., 2018. <em>Contextual: Evaluating Contextual Multi-Armed Bandit Problems in R</em>. arXiv preprint arXiv:1811.01926.</div>
<p>In this package, the MAB problems are addressed under the following assumptions:</p>
<ul>
<li>The bandit is a set of arms where each arm is defined by some reward function mapping dimensional context vector returning a reward for every time step until the horizon.</li>
<li>The function of politics is the maximization of the cumulative reward. This function is carried out by selecting one of the currently available bandit arms in the sequence.</li>
<li>During the learning process, the policy observes the current state of the environment, which is represented by the vectors of the context characteristics. Afterward, the policy selects one of the available actions using an arm selection strategy. As a result, it receives a reward. This procedure allows the policy to update the selection of strategic arms. This procedure is then repeated <em>T</em> times.</li>
</ul>
<p>To represent what has been said in the algorithm, we can say that, for each round, a policy does the following:</p>
<ul>
<li>Observes the current context feature vectors</li>
<li>Selects an action</li>
<li>Receives a reward</li>
<li>Updates the arm-selection strategy parameters</li>
</ul>
<p>The goal of the policy is to optimize its cumulative reward.</p>
<p>Then, we apply the functions contained in the contextual package to a practical case.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Online advertising context-free policies</h1>
                </header>
            
            <article>
                
<p>The first simulation we will perform will not consider the context. We will simply evaluate how many clicks each advertisement receives after a certain number of impressions. First, we need to set the initial settings. As anticipated, we will only consider three announcements that correspond in the MAB formulation to three arms of the bandit, each with a different probability of generating a click. Let's get started:</p>
<ol>
<li>We will use the following code to perform the analysis:</li>
</ol>
<pre style="padding-left: 60px">library(contextual)<br/>horizon &lt;- 500<br/>simulations &lt;- 1000<br/>ProbClick &lt;- c(0.1, 0.3, 0.7)<br/>bandit &lt;- BasicBernoulliBandit$new(weights = ProbClick)<br/>policy &lt;- EpsilonFirstPolicy$new(time_steps = 100)<br/>agent &lt;- Agent$new(policy, bandit)<br/>simulator &lt;- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)<br/>history &lt;- simulator$run()<br/>plot(history, type = "average", regret = FALSE, lwd = 2, legend_position = "bottomright")<br/>plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)<br/>plot(history, type = "arms")</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="2">
<li>We will analyze this code line by line. The first line loads the library:</li>
</ol>
<pre style="padding-left: 60px">library(contextual)</pre>
<p style="padding-left: 60px">This library contains many functions that allow for the simulation and evaluation of context-free and contextual MAB policies or algorithms. </p>
<ol start="3">
<li>Now, let's fix some necessary parameters to set the problem as MAB:</li>
</ol>
<pre style="padding-left: 60px">horizon &lt;- 500</pre>
<ol start="4">
<li>The horizon is the number of rounds that must be played. Let's set the number of simulations:</li>
</ol>
<pre style="padding-left: 60px">simulations &lt;- 1000</pre>
<p style="padding-left: 60px"><span>The number of simulations indicates how many times to repeat the simulation. </span></p>
<ol start="5">
<li><span>Let's move on and set the probability that a user clicks on an advertisement:</span></li>
</ol>
<pre style="padding-left: 60px">ProbClick &lt;- c(0.1, 0.3, 0.7)</pre>
<p style="padding-left: 60px">A vector has been defined and contains the probabilities that each ad is clicked. In MAB terms, they represent the probabilities associated with the three arms. At this point, the initial parameters are fixed, so we can move on to defining the objects.</p>
<p style="padding-left: 60px">The first will be the bandit:</p>
<pre style="padding-left: 60px">bandit &lt;- BasicBernoulliBandit$new(weights = ProbClick)</pre>
<p style="padding-left: 60px">Here, we used the <kbd>BasicBernoulliBandit()</kbd> function. This function simulates k Bernoulli arms, where each arm issues a reward of one with uniform probability p, otherwise a reward of zero. In a bandit scenario, this can be used to simulate a hit or miss event, such as if a user clicks on a headline, ad, or recommended product. Only one argument is expected (weights): it is a numeric vector that represents the probability of reward values for each of the bandit's k arms. The <kbd>new<span>()</span></kbd> method generates and instantiates a new <kbd>BasicBernoulliBandit</kbd> instance. </p>
<ol start="6">
<li>Let's move on to defining the policy:</li>
</ol>
<pre style="padding-left: 60px">policy &lt;- EpsilonFirstPolicy$new(time_steps = 100)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">Here, we used the <kbd>EpsilonFirstPolicy()</kbd> function to implement a naive policy where a pure exploration phase is followed by a pure exploitation phase. Exploration happens within the first N time steps that are defined. During this time, at each time step t, <kbd>EpsilonFirstPolicy</kbd> selects an arm at random. Exploitation happens in the following steps, where we select the best arm up until N for either the remaining N trials or horizon T. Here, we used the <kbd>new<span>()</span></kbd> method, which generates a new <kbd>EpsilonFirstPolicy</kbd> object. Let's move on: </p>
<pre style="padding-left: 60px">agent &lt;- Agent$new(policy, bandit)</pre>
<p style="padding-left: 60px">The agent class is responsible for running one bandit/policy pair. The following arguments are available:</p>
<ul>
<li style="padding-left: 60px"><kbd>policy</kbd>: A policy instance.</li>
<li style="padding-left: 60px"><kbd>bandit</kbd>: A bandit instance.</li>
<li style="padding-left: 60px"><kbd>name character</kbd>: Sets the name of the Agent. If NULL (default), the agent generates a name based on its policy instance's name.</li>
<li style="padding-left: 60px"><kbd>sparse numeric</kbd>: Artificially reduces the data size by setting a sparsity level for the current bandit and policy pair.</li>
</ul>
<p style="padding-left: 60px">In our case, only two arguments are passed: <kbd>policy</kbd> and <kbd>bandit</kbd>. Let's move on to running the simulation:</p>
<pre style="padding-left: 60px">simulator &lt;- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)</pre>
<p style="padding-left: 60px">This is the entry point of any simulation. The <kbd>Simulator</kbd> class encloses one or more agents, creates an agent clone for each to be repeated simulation, runs the agents, and saves the log of all agent interactions in a history object. The following arguments are passed:</p>
<ul>
<li style="padding-left: 60px"><kbd>agent</kbd>: An agent instance or a list of agent instances</li>
<li style="padding-left: 60px"><kbd>horizon</kbd>: The number of pulls or time steps to run each agent, where t = 1, . . . , T <span>(integer value)</span></li>
<li style="padding-left: 60px"><kbd>simulations</kbd>: How many times to repeat each agent's simulation over t = 1, . . . , T, with a new seed on each repeat <span>(integer value)</span></li>
<li style="padding-left: 60px"><kbd>do_parallel</kbd>: If running simulator processes in parallel (logical value)</li>
</ul>
<div class="packt_infobox">For a detailed list of all the topics that are covered by the R6 class simulator, please refer to the official documentation: <a href="https://CRAN.R-project.org/package=contextual">https://CRAN.R-project.org/package=contextual</a>.</div>
<p><span>Here, we used the </span><kbd>new()</kbd><span> method to generate a new simulator object. It's time to run the simulation:</span></p>
<pre>history &lt;- simulator$run()</pre>
<p>The <kbd>run()</kbd> method simply runs a simulator instance. At this point, we have all the history of the simulation recorded in the history variable. Now, we can use this data to draw graphs. First, we will analyze the average reward:</p>
<pre>plot(history, type = "average", regret = FALSE, lwd = 2, legend_position = "bottomright")</pre>
<p>The <kbd>plot()</kbd> function generates plots from the history data. The following plot types are available:</p>
<ul>
<li><kbd>cumulative</kbd>: Plots the cumulative regret or reward <span>over time. If</span> regret=TRUE is passed, a cumulative regret is returned; if <span>regret=FALSE is passed, a cumulative reward is returned</span>.</li>
<li><kbd>average</kbd>: Plots the average regret or reward.<span> If</span><span> regret=TRUE is passed, a cumulative regret is returned; if </span><span>regret=FALSE is passed, a cumulative reward is returned</span><span>.</span></li>
<li><kbd>arms</kbd>: Plots the percentage of simulations per time step where each arm was chosen over time. If multiple agents have been run, it only plots the first agent.</li>
</ul>
<p>The following plot is printed:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-475 image-border" src="assets/24635d77-26f1-40f5-acbe-d84ea5160ff6.png" style="width:34.25em;height:22.42em;"/></p>
<p class="mce-root"/>
<p>Note that, after the first phase of exploration in which all the arms are tested in equal measure, we pass this to the exploitation phase, in which the arms that return the greatest rewards are preferred. The passage between the two phases can be seen by the net increase in the average reward. Let's move on the cumulative plot:</p>
<pre>plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2)</pre>
<p><span>The following plot is returned:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-476 image-border" src="assets/a7bc6048-9dcf-420a-85f0-305197e410d5.png" style="width:38.50em;height:23.67em;"/></p>
<p>The transition between the exploration and exploitation phases in this graph is even more evident. After the first 100 steps in which only the exploration phase has been used, the cumulative regret starts increasing with a logarithmic profile. Finally, we will plot the arms type:</p>
<pre>plot(history, type = "arms")</pre>
<p>This type of plot returns <span>the percentage of simulations per time step each arm was chosen over </span><span>time. The following plot is printed:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-477 image-border" src="assets/0cf25153-6a72-484d-917b-fe1127469258.png" style="width:33.67em;height:20.50em;"/></p>
<p>From the preceding graph, we can see that the most chosen arm is the number 3. In fact, even in this case, after the first exploration phase in which the three arms are chosen with comparable percentages, the choice of arm that's passed to the exploitation phase falls exclusively on number 3.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Online advertising ε-greedy-based policies</h1>
                </header>
            
            <article>
                
<p>Now, we will address the same problem by adopting an ε-greedy policy. As we mentioned previously, <span>with this approach, we introduce an element of exploration that improves performance. </span><span>The following code shows the analysis process when using the ε-greedy approach:</span></p>
<pre>library(contextual)<br/>horizon &lt;- 500<br/>simulations &lt;- 1000<br/>ProbClick &lt;- c(0.1, 0.3, 0.7)<br/>bandit &lt;- BasicBernoulliBandit$new(weights = ProbClick)<br/>policy &lt;- EpsilonGreedyPolicy$new(epsilon = 0.1)<br/>agent &lt;- Agent$new(policy,bandit)<br/>simulator &lt;- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)<br/>history &lt;- simulator$run()<br/>plot(history, type = "average", regret = FALSE, lwd = 2, legend_position = "bottomright")<br/>plot(history, type = "cumulative", regret = FALSE, rate = TRUE, lwd = 2, legend_position = "bottomright")<br/>plot(history, type = "arms", legend_position = "topright")</pre>
<p class="mce-root"/>
<p>We will analyze this code line by line. The first line loads the library:</p>
<pre>library(contextual)</pre>
<p>Now, let's fix some necessary parameters to set the problem as MAB:</p>
<pre>horizon &lt;- 500</pre>
<p>The horizon is the number of rounds that must be played. Let's set the number of simulations:</p>
<pre>simulations &lt;- 1000</pre>
<p><span> Let's move on and set the probability that a user clicks on an advertisement:</span></p>
<pre>ProbClick &lt;- c(0.1, 0.3, 0.7)</pre>
<p>The same probability vector that we used in the previous example has been adopted. Let's <span>move on and define the objects:</span></p>
<pre>bandit &lt;- BasicBernoulliBandit$new(weights = ProbClick)</pre>
<p>To define the bandit, we used the <kbd>BasicBernoulliBandit()</kbd> function. This function <span>simulates k Bernoulli arms, where each arm issues a reward of one with a uniform probability p, and otherwise a reward of zero. </span>Let's move on to defining the policy:</p>
<pre class="mce-root">policy &lt;- EpsilonGreedyPolicy$new(epsilon = 0.1)</pre>
<p>Here, we used the <kbd>EpsilonGreedyPolicy()</kbd> function. This function chooses an arm at random with a probability epsilon in the exploration phase; otherwise, it greedily chooses the arm with the highest estimated reward <span>in the exploitation phase</span>. Only the epsilon argument is passed, indicating the probability that the arms are selected at random. The <kbd>new()</kbd> method is used to generate a new <kbd>EpsilonGreedyPolicy</kbd> object. Finally, we will create an agent object, as follows:</p>
<pre class="mce-root">agent &lt;- Agent$new(policy,bandit)</pre>
<p><span>The agent class is responsible for running one Bandit/Policy pair. Now, we can run the simulation:</span></p>
<pre>simulator &lt;- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)</pre>
<p class="mce-root"/>
<p>The following results are printed:</p>
<pre><strong>Simulation horizon: 500</strong><br/><strong>Number of simulations: 1000</strong><br/><strong>Number of batches: 1</strong><br/><strong>Starting main loop.</strong><br/><strong>data.table 1.12.0 Latest news: r-datatable.com</strong><br/><strong>Finished main loop.</strong><br/><strong>Completed simulation in 0:01:25.191</strong><br/><strong>Computing statistics.</strong></pre>
<p>Now, we can <span>run the simulation:</span></p>
<pre class="mce-root">history &lt;- simulator$run()</pre>
<p><span>The history of the simulation that we carried out is recorded in the history variable. We can use this data to draw graphs. First, the regret average is plotted:</span></p>
<pre class="mce-root">plot(history, type = "average", regret = TRUE, lwd = 2, legend_position = "topright")</pre>
<p><span>The following plot is returned:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-478 image-border" src="assets/16345297-ced0-4d21-9e49-785df4a4b960.png" style="width:34.83em;height:21.33em;"/></p>
<p>Then, we plot the cumulative regret:</p>
<pre class="mce-root">plot(history, type = "cumulative", regret = TRUE, rate = TRUE, lwd = 2,legend_position = "topright")</pre>
<p><span>The following plot is returned:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-479 image-border" src="assets/7d982342-a28b-48ef-998e-b4d89fb3be17.png" style="width:35.50em;height:21.92em;"/></p>
<p>Finally, the arm choice in percent is plotted:</p>
<pre class="mce-root">plot(history, type = "arms")</pre>
<p><span>The following plot is returned:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-480 image-border" src="assets/22cd39d2-2341-4381-967e-3b0d35639a76.png" style="width:36.17em;height:22.17em;"/></p>
<p>In the preceding three graphs, a feature is highlighted (<strong>Arm choice %</strong>). The reduction of the regret over time is progressive and does not undergo a discontinuity like it did in the previous simulation (context-free policies-based example). This is due to the fact that the algorithm simultaneously carries out the exploration and exploitation phase. On one hand, it uniformly explores one of the advertisements randomly ε of the time, while on the other hand, it exploits the ad with the best current click rate 1-ε of the time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Online advertising context-based policies</h1>
                </header>
            
            <article>
                
<p>What happens if visitors are divided into two categories, male and female, or young and adult? We can introduce a reference to the context in the models we've analyzed so far. Recall that, in the contextual bandit, the concept of state was introduced, which represents a description of the environment that the agent can use to carry out targeted actions. This model extends the original one by linking the decision to the state of the environment. <span>The following code performs an analysis using the context approach:</span></p>
<pre>library(contextual)<br/>horizon &lt;- 500<br/>simulations &lt;- 1000<br/>ProbClickContx &lt;- matrix(c(0.1, 0.3, 0.7, 0.8, 0.4, 0.1),<br/> nrow = 2, ncol = 3, byrow = TRUE)<br/>bandit &lt;- ContextualBinaryBandit$new(weights = ProbClickContx)<br/>policy &lt;- EpsilonGreedyPolicy$new(epsilon = 0.1)<br/>agent &lt;- Agent$new(policy,bandit)<br/>simulator &lt;- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)<br/>history &lt;- simulator$run()<br/>plot(history, type = "arms", legend_position = "bottomright")</pre>
<p>We will analyze this code line by line. The first line loads the library:</p>
<pre>library(contextual)</pre>
<p>Now, let's fix some of the necessary parameters to set the problem as MAB:</p>
<pre>horizon &lt;- 500</pre>
<p>The horizon is the number of rounds that must be played. Let's set the number of simulations:</p>
<pre>simulations &lt;- 1000</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p><span>Let's move on and set the probability that a user clicks on an advertisement. In this case, the reference to a binary context is introduced. Two possible probability distributions are defined, both of which correspond to two user profiles:</span></p>
<pre>ProbClickContx &lt;- matrix(c(0.1, 0.3, 0.7, 0.8, 0.4, 0.1),<br/>                    nrow = 2, ncol = 3, byrow = TRUE)</pre>
<p>The first profile has a probability vector equal to (0.1, 0.3, 0.7), while the second has a probability vector equal to (0.8, 0.4, 0.1). Let's <span>move on and define the objects:</span></p>
<pre>bandit &lt;- ContextualBinaryBandit$new(weights = ProbClickContx)</pre>
<p>To define the bandit, we used the <kbd>ContextualBinaryBandit()</kbd> function. This function <span>simulates a contextual Bernoulli MAB problem, where at least one context feature is active at a time. In this case, the weights argument contains a d x k numeric matrix with probabilities of reward for d contextual features per k arms. </span>Let's move on and define the policy:</p>
<pre class="mce-root">policy &lt;- EpsilonGreedyPolicy$new(epsilon = 0.1)</pre>
<p>Here, we used the <kbd>EpsilonGreedyPolicy()</kbd> function. Finally, we will create an agent object, as follows:</p>
<pre class="mce-root">agent &lt;- Agent$new(policy,bandit)</pre>
<p><span>The agent class is responsible for running one Bandit/Policy pair. Now, we can run the simulation:</span></p>
<pre>simulator &lt;- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)</pre>
<p>The following results are printed:</p>
<pre><strong>Simulation horizon: 500</strong><br/><strong>Number of simulations: 1000</strong><br/><strong>Number of batches: 1</strong><br/><strong>Starting main loop.</strong><br/><strong>Finished main loop.</strong><br/><strong>Completed simulation in 0:01:43.277</strong><br/><strong>Computing statistics.</strong></pre>
<p>Now, we can <span>store the simulation:</span></p>
<pre class="mce-root">history &lt;- simulator$run()</pre>
<p><span>The history of the simulation is recorded in the history variable. Now, we can use this data to draw graphs. In this case, only the arm choice in percent is plotted:</span></p>
<pre class="mce-root">plot(history, type = "arms", legend_position = "bottomright")</pre>
<p><span>The following plot is returned:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-481 image-border" src="assets/d992f12f-5ab6-4a6c-b505-3067ad9155b7.png" style="width:32.75em;height:20.17em;"/></p>
<p>From this analysis, it is clear that the most chosen arm in percentage is number 1, then number 3, and finally number 2.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparison between solution techniques </h1>
                </header>
            
            <article>
                
<p>Several policies are available for the solution of the MAB problem. In the<em> Problem solution techniques</em> section, we analyzed some of them. The contextual package proposes some policies. The following is a list of those available policies:</p>
<ul>
<li><kbd>ContextualEpochGreedyPolicy</kbd></li>
<li><kbd>ContextualEpsilonGreedyPolicy</kbd></li>
<li><kbd>ContextualLogitBTSPolicy</kbd></li>
<li><kbd><kbd>ContextualTSProbitPolicy</kbd></kbd></li>
<li><kbd>EpsilonFirstPolicy</kbd></li>
<li><kbd>EpsilonGreedyPolicy</kbd></li>
<li><kbd>Exp3Policy</kbd></li>
<li><kbd>FixedPolicy</kbd></li>
<li><kbd>GittinsBrezziLaiPolicy</kbd></li>
<li><kbd>GradientPolicy</kbd></li>
<li><kbd>LifPolicy</kbd></li>
<li><kbd>LinUCBDisjointOptimizedPolicy</kbd></li>
<li><kbd>LinUCBDisjointPolicy</kbd></li>
<li><kbd>LinUCBGeneralPolicy</kbd></li>
<li><kbd>LinUCBHybridOptimizedPolicy</kbd></li>
<li><kbd>LinUCBHybridPolicy</kbd></li>
<li><kbd>OraclePolicy</kbd></li>
<li><kbd>RandomPolicy</kbd></li>
<li><kbd>SoftmaxPolicy</kbd></li>
<li><kbd>ThompsonSamplingPolicy</kbd></li>
<li><kbd>UCB1Policy</kbd></li>
<li><kbd>UCB2Policy</kbd></li>
</ul>
<div class="packt_infobox">For a detailed description of these policies, please refer to the official documentation of the package, which is available at the following URL: <a href="https://cran.r-project.org/package=contextual">https://CRAN.R-project.org/package=contextual</a>.</div>
<p>Some of these have already been adopted in the examples we've looked at in this chapter. To analyze the characteristics of some of these, we can make a comparison based on the results we've obtained. Here is the full code:</p>
<pre>library(contextual)<br/>horizon &lt;- 500<br/>simulations &lt;- 1000<br/>ProbClick &lt;- c(0.8, 0.3, 0.1)<br/>bandit &lt;- BasicBernoulliBandit$new(weights = ProbClick)<br/><br/>agents &lt;- list(Agent$new(OraclePolicy$new(), bandit),<br/> Agent$new(UCB1Policy$new(), bandit),<br/> Agent$new(ThompsonSamplingPolicy$new(1.0, 1.0), bandit),<br/> Agent$new(EpsilonGreedyPolicy$new(0.1), bandit),<br/> Agent$new(SoftmaxPolicy$new(tau = 0.1),bandit),<br/> Agent$new(Exp3Policy$new(0.1), bandit))<br/><br/>simulation &lt;- Simulator$new(agents, horizon, simulations)<br/>history &lt;- simulation$run()<br/><br/>plot(history, type = "cumulative", regret = FALSE)</pre>
<p>We will analyze this code line by line. The first line loads the library:</p>
<pre>library(contextual)</pre>
<p>Now, let's fix some of the necessary parameters to set the problem as MAB:</p>
<pre>horizon &lt;- 500<br/>simulations &lt;- 1000</pre>
<p><span> Let's move on and set the probability that a user clicks on an advertisement:</span></p>
<pre>ProbClick &lt;- c(0.8, 0.3, 0.1)</pre>
<p>Let's move on and define the bandit object:</p>
<pre>bandit &lt;- BasicBernoulliBandit$new(weights = ProbClick)</pre>
<p>Here, we used the <kbd>BasicBernoulliBandit()</kbd> function. Now, <span>we will create a list of agent objects, as follows:</span></p>
<pre>agents &lt;- list(Agent$new(OraclePolicy$new(), bandit),<br/>               Agent$new(UCB1Policy$new(), bandit),<br/>               Agent$new(ThompsonSamplingPolicy$new(1.0, 1.0), bandit),<br/>               Agent$new(EpsilonGreedyPolicy$new(0.1), bandit),<br/>               Agent$new(SoftmaxPolicy$new(tau = 0.1),bandit),<br/>               Agent$new(Exp3Policy$new(0.1), bandit))</pre>
<p>Six agents were defined with different policies, as follows:</p>
<ul>
<li class="mce-root"><kbd>OraclePolicy</kbd>: This policy knows the reward probabilities at all times, and will always play the optimal arm. It is often used as a baseline to compare other policies.</li>
<li class="mce-root"><kbd>UCB1Policy</kbd>: <span>This policy </span>constructs an optimistic estimate in the form of an Upper Confidence Bound to create an estimate of the expected payoff of each action and picks the action with the highest estimate. If the guess is wrong, the optimistic guess quickly decreases until another action has a higher estimate.</li>
<li class="mce-root"><kbd>ThompsonSamplingPolicy</kbd>: The procedure that's followed by this policy exploits the memory of the average rewards of the weapons. To do this, use a beta-binomial model with alpha and beta parameters, sample the values for each arm from the previous step, and select the arm with the highest value. When an arm is pulled and a Bernoulli reward is observed, it modifies the prior based on the reward. This procedure is repeated for the next arm pull.</li>
<li class="mce-root"><kbd>EpsilonGreedyPolicy</kbd>: <span>The procedure that's followed by this policy foresees the random choice of an arm with epsilon probability to explore the environment. Otherwise, an arm is chosen greedily, with the highest estimated reward. By doing this, the agent exploits the information that was acquired in the previous steps.</span></li>
<li class="mce-root"><kbd>SoftmaxPolicy</kbd>: T<span>his policy selects an arm based on the probability from the Boltmann distribution. It makes use of a temperature parameter, tau, which specifies how many arms we can explore. When tau is high, all the arms are explored equally, and, when tau is low, the arms offering higher rewards will be chosen.</span></li>
<li class="mce-root"><kbd>Exp3Policy</kbd>: <span>The procedure that's followed by this policy uses a probability distribution, which is a mixture of a uniform distribution. It also uses a distribution that assigns each action an exponential mass of probability in the cumulative reward that was estimated for that action.</span></li>
</ul>
<p><span>Now, we can run the simulation and store its history:</span></p>
<pre>simulation &lt;- Simulator$new(agents, horizon, simulations)<br/>history &lt;- simulation$run()</pre>
<p>Finally, we will plot the cumulative regrets of the agents that we simulated:</p>
<pre>plot(history, type = "cumulative")</pre>
<p><span>The following plot is returned:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-482 image-border" src="assets/dd38e685-12b0-4d9c-be60-e1c1a832ae86.png" style="width:36.58em;height:22.75em;"/></p>
<p>As we anticipated, the OraclePolicy is the one that presents the lowest values in absolute of the regret, which means it succeeds in making better use of the arm that supplies better results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have learned about the basic concepts of the multi-armed bandit model. This model is based on the dilemma of exploration and exploitation. In the case of limited resources, which is what we base our choices on, it is essential to know which competitive alternatives allow us to maximize the expected profit. The name derives from the example of a player struggling with a row of slot machines, who must decide whether to continue with the current machine or try another machine. A mathematical model of the problem was described. Then, we discovered the meaning of the action-value implementation and how it differs from the value function. The state-value-function contains the value of reaching a certain state, while the action-value-function contains the value for choosing an action in a state.</p>
<p>Several problem solution techniques were analyzed and the contextual approach was addressed. We also looked at a list of practical applications of the MAB problem. Finally, online advertising was treated using several MAB models.</p>
<p>In the next chapter, we will learn about the basic concepts of optimization techniques. We will learn how to decompose a problem into subproblems and how to implement the various optimization techniques. Then, we will understand the difference between recursion and memoization and discover how to use the dynamic programming approach to make the most convenient choices.</p>


            </article>

            
        </section>
    </body></html>