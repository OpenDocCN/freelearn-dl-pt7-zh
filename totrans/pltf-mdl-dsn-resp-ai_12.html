<html><head></head><body>
		<div id="_idContainer252">
			<h1 id="_idParaDest-223" class="chapter-nu ber"><a id="_idTextAnchor243"/>12</h1>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor244"/>Building Sustainable Enterprise-Grade AI Platforms</h1>
			<p>The primary objective of this chapter is to inform you about sustainability best practices, along with model governance techniques, to align them with your organizational goals and initiatives. You will be made aware of the environmental consequences of training <strong class="bold">Deep Learning (DL)</strong> models and possible remediating actions that you could take. You will also become accustomed to different metrics of sustainability for different platforms, which will lead to sustainable model training <span class="No-Break">and deployment.</span></p>
			<p>By the end of this chapter, you will have been equipped with the collaborative and decentralized learning techniques involved in <strong class="bold">Federated Learning (FL)</strong>, whether model training happens on the network, at the edge, or in the cloud. Finally, you will also be aware of the tools that can help to track carbon <span class="No-Break">emission statistics.</span></p>
			<p>In this chapter, these topics will be covered in the <span class="No-Break">following sections:</span></p>
			<ul>
				<li>The key to sustainable enterprise-grade <span class="No-Break">AI platforms</span></li>
				<li>Sustainability practices and metrics across different <span class="No-Break">cloud platforms</span></li>
				<li>Carbon <span class="No-Break">emission trackers</span></li>
				<li>Adopting sustainable model training and deployment <span class="No-Break">with FL</span></li>
			</ul>
			<h1 id="_idParaDest-225"><a id="_idTextAnchor245"/>Technical requirements</h1>
			<p>This chapter requires you to have Python 3.8, along with some Python packages, listed <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Keras 2.7.0 and <span class="No-Break">TensorFlow 2.7.0</span></li>
				<li><strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install CodeCarbon</strong></span></li>
			</ul>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor246"/>The key to sustainable enterprise-grade AI platforms</h1>
			<p>Sustainability <a id="_idIndexMarker1466"/>has a very important role to play in the era of ethical AI, as the ability to predict energy emissions can support initiatives to protect the environment and conserve resources. AI-enabled platforms can facilitate emission reductions and carbon dioxide (CO<span class="subscript">2</span>) removal, which can foster greener transportation networks, as well as monitor and control deforestation. Thus, by effectively using AI solutions in a sustainable manner, we can try to prevent extreme <span class="No-Break">weather conditions.</span></p>
			<p>As a first step, we should understand why organizations have set up a vision to go carbon free. Along with this, it is equally important to understand the leadership vision to properly align teams with the sustainable mission goals set forth by their senior leadership <span class="No-Break">and CXOs.</span></p>
			<p>Hence, as the first step, we will look at the motivation of organizations to restructure their roadmaps toward building sustainable <span class="No-Break">AI solutions.</span></p>
			<h2 id="_idParaDest-227"><a id="_idTextAnchor247"/>Sustainable solutions with AI as an organizational roadmap</h2>
			<p>We should be<a id="_idIndexMarker1467"/> extremely careful to build solutions that are environmentally friendly. Substantial research indicates that just 40 days of research training can emit 96 tons of CO<span class="subscript">2</span>. The amount is so large that it can be compared to 1,000 hours of air travel, roughly equivalent to the carbon footprint of 23 American homes (<a href="https://www.analyticsinsight.net/new-mit-neural-network-architecture-may-reduce-carbon-footprint-ai/">https://www.analyticsinsight.net/new-mit-neural-network-architecture-may-reduce-carbon-footprint-ai/</a>, <a href="https://inhabitat.com/mit-moves-toward-greener-more-sustainable-artificial-intelligence/">https://inhabitat.com/mit-moves-toward-greener-more-sustainable-artificial-intelligence/</a>). The carbon emissions involved in running a neural network search have been as high as 600,000 CO<span class="subscript">2</span>e (lbs), equivalent to the lifetime of 5 cars. Training algorithms such as large <strong class="bold">Natural Language Processing (NLP)</strong> models (or transformers) generate so much energy that data scientists need to be careful when designing the architectures of this kind of DL neural network (<a href="https://aclanthology.org/P19-1355.pdf">https://aclanthology.org/P19-1355.pdf</a>). This kind of training can be threatening to the environment. If we continue to build these AI solutions in this way, future generations will face adverse <span class="No-Break">environmental consequences.</span></p>
			<p>Organizations that want to apply AI to sustainability should not only focus on sustainable banking, energy consumption, or healthcare but also develop best practices to quantify the CO<span class="subscript">2</span>e by measuring carbon footprints and the computational power required to train and evaluate the methods for managing data <span class="No-Break">centers efficiently.</span></p>
			<p>In addition, data scientists <a id="_idIndexMarker1468"/>and engineers should first evaluate the <a id="_idIndexMarker1469"/>need for DL models before choosing them. For certain use cases, similar performance can be guaranteed with a standard model without needing to train a DL model. So, correct practices and audits need to be established in an organization before training DL models, to avoid a large <span class="No-Break">carbon footprint.</span></p>
			<p>Once the data science team finalizes the model (traditional versus DL in either centralized or FL setups), the selection of the number of hyperparameters and careful tuning play important roles in carbon emissions. Tuning hyperparameters for FL can become expensive in terms of energy consumed, as this leads to tuning hundreds of different models (with local models in <span class="No-Break">the clients).</span></p>
			<p>The tuning process in FL may become increasingly complex due to parameterization in the aggregation strategy, as well as heterogeneity in the individual datasets of the clients. The complexity of hyperparameter tuning in both types of learning must be carefully designed to minimize <span class="No-Break">CO</span><span class="No-Break"><span class="subscript">2</span></span><span class="No-Break">e release.</span></p>
			<p>The team should be aware of the choice of the dataset and the extent of preprocessing and feature engineering involved. One such example is when feature engineering techniques become more exhaustive and expensive in developing recommender systems using financial and retail data rather than using only <span class="No-Break">retail data.</span></p>
			<p>Let us study the organizational standards that can enable the building of <span class="No-Break">such frameworks.</span></p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor248"/>Organizational standards for sustainable frameworks</h2>
			<p>CXOs <a id="_idIndexMarker1470"/>and leadership must also be involved in mandating sustainable data and model practices as organizational objectives. These sustainable practices for scalable cloud platforms originate from setting the right objectives <a id="_idIndexMarker1471"/>and <strong class="bold">Service-Level Agreements</strong> (<strong class="bold">SLAs</strong>) based on the current business requirements. This may require selecting the right trade-off to affect sustainability metrics by prioritizing business-critical functions and allowing lower service levels for non-critical functions. Critically, businesses should handle siloed data<a id="_idIndexMarker1472"/> across departments by defining architectural design patterns that are suitable for sustainable model training <span class="No-Break">and deployment.</span></p>
			<p>Now, let us begin looking into the metrics measured across different <span class="No-Break">cloud platforms.</span></p>
			<h1 id="_idParaDest-229"><a id="_idTextAnchor249"/>Sustainability practices and metrics across different cloud platforms</h1>
			<p>In this<a id="_idIndexMarker1473"/> section, we will explore how we can evaluate <span class="No-Break">the following:</span></p>
			<ul>
				<li>Useful emission metrics on <span class="No-Break">Google Cloud</span></li>
				<li>Best practices and strategies for <span class="No-Break">carbon-free energy</span></li>
				<li>The energy efficiency of <span class="No-Break">data centers</span></li>
			</ul>
			<p>First of all, we will discuss some of the metrics of Google Cloud and also cite some of the initiatives and tools promoted <span class="No-Break">by Microsoft.</span></p>
			<p>Googleâ€™s role in sustainable cloud solutions has been overwhelming, and it has been the leading cloud provider in terms of purchasing sufficient renewable energy, more than any other organization. Starting with a pledge in 2007 to become the first major carbon-neutral company by 2017, 100% of its electricity consumption now comes from renewable energy. In the absence of enough wind and solar power or renewable resources, Google draws power from the local grid to run operations at a local data center. Conversely, when sufficient power is available, the excess is fed back to the local grid to be used elsewhere. With a vision of 24/7 carbon-free energy by 2030, it uses the guidelines mentioned in the following sections to make its data centers and electricity grids <span class="No-Break">carbon free.</span></p>
			<p>Microsoft also has its own Power BI application for Azure, popularly known as the Microsoft Sustainability Calculator, which provides insights into the amount of carbon emissions associated with the data centers in each region. Even initiatives such as the purchase of carbon-neutral renewable energy help to drive its annual <span class="No-Break">cloud consumption.</span></p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor250"/>Emission metrics on Google Cloud</h2>
			<p>To move<a id="_idIndexMarker1474"/> to carbon-free energy by 2030, Google Cloud has come up with an initiative to empower its customers to leverage carbon-free energy 24/7. Customers can consider the carbon impact when designing their solutions by analyzing a metric <a id="_idIndexMarker1475"/>called <strong class="bold">CFE%</strong>. This metric quantifies the amount of energy consumed every hour that is carbon free by broadly dividing it into two main categories: CFE% and carbon intensity. CFE% is calculated based on the generated energy that feeds the grid at any time, along with the clean energy attributions (through the supply of renewable energy resources) made available by Google that are applied to <span class="No-Break">the grid:</span></p>
			<ul>
				<li><strong class="bold">Google CFE%</strong>: This<a id="_idIndexMarker1476"/> provides a measure of the average percentage of carbon-free energy. This measure, when computed per location on an hourly basis, provides customers with an estimate of the amount of time that their apps can run on carbon-free energy, depending on the investments made into carbon-free energy that apply to the grid at the <span class="No-Break">same location.</span></li>
				<li><strong class="bold">Grid carbon intensity (gCO</strong><span class="subscript">2</span><strong class="bold">eq/kWh)</strong>: This<a id="_idIndexMarker1477"/> metric quantifies the average life cycle of gross emissions per unit of energy from a grid at a specific location. With it, we can compare the carbon intensity of the electricity at different locations within a local grid that behave similarly. Using this, we can select the region of deployment of production apps when two or more locations demonstrate a similar CFE%. To cite an example, we will often see that Frankfurt and the Netherlands exhibit similar CFE% scores, while the Netherlands experiences higher rates <span class="No-Break">of emissions.</span></li>
			</ul>
			<p>Now, let's discuss the best practices involved in utilizing <span class="No-Break">carbon-free energy.</span></p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor251"/>Best practices and strategies for carbon-free energy</h2>
			<p>As <a id="_idIndexMarker1478"/>Google Cloud Platform actively concentrates on increasing the CFE% for each of the Google Cloud regions, a higher percentage of carbon-free energy increases the sustainability of deployments. Some of the unique propositions for cloud AI specialists and architects are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Selecting a lower-carbon region</strong>: Building and running new applications in low-carbon regions (such as Finland or Sweden) with a higher CFE% helps to achieve the target carbon-free <span class="No-Break">energy emissions.</span></li>
				<li><strong class="bold">Running batch jobs in a low-carbon region</strong>: Running batch workloads through careful planning can help to maximize the use of carbon-free energy. This means we don't need to run low-to-medium workload jobs separately and ensures high CPU utilization by combining jobs that have high (as well as <span class="No-Break">low) workloads.</span></li>
				<li><strong class="bold">Driving organizational policies for greener cloud applications</strong>: Leadership teams and organizations should set priorities to allow the usage of resources and services in certain regions while restricting access and usage in <span class="No-Break">other regions.</span></li>
				<li><strong class="bold">Efficient use of services</strong>: Effectively strategizing the VM sizing, along with the use <a id="_idIndexMarker1479"/>of serverless products such as Cloud Run and Cloud Functions, can further reduce carbon emissions, as they auto-scale based on workload and conserve energy as much <span class="No-Break">as possible.</span></li>
			</ul>
			<h2 id="_idParaDest-232"><a id="_idTextAnchor252"/>The energy efficiency of data centers</h2>
			<p>To run<a id="_idIndexMarker1480"/> highly efficient data centers, we need to carefully plan and select the placement of VMs in a multisite cloud. In addition to selecting the right data center for hosting a VM, VM sizing is equally important, as it prevents the wastage of additional computational resources and improves the execution speed of an application. We can increase a data centerâ€™s efficiency by allowing parallelization and employing the right techniques to consolidate the cloud resources to reduce <span class="No-Break">energy consumption.</span></p>
			<p><strong class="bold">Physical Machines</strong> (<strong class="bold">PMs</strong>) constitute <a id="_idIndexMarker1481"/>35% of the total energy consumption of data centers. The energy consumed by them can be broken into <strong class="bold">static</strong> (the energy consumed when the VM is idle) and <strong class="bold">dynamic</strong> parts. While the dynamic power consumed primarily depends on the utilization of each component and constitutes more than 50% of the maximum power consumed, the idle power consumed remains lower than 50% of the maximum power consumed. In addition, a nominal amount of power is also consumed in sleep mode. We need the right balance between the amount of idle and dynamic power consumed to achieve high utilization workloads so that PMs contribute to a <a id="_idIndexMarker1482"/>high energy efficiency ratio by maintaining energy consumption proportional to the utilization load. We need to consider optimization to reduce energy consumption so that idle power consumption is at 0 W (when PMs are not in use), and at other times, it corresponds with an increasing workload until the maximum utilization load is achieved. Virtualized resources have the flexibility to consolidate user tasks into fewer PMs. We have to also carefully decide how we reduce the usage of PMs so that the idle power and, consequently, total consumption are reduced. Besides PMs, while determining the overall efficiency of a data center, some of the metrics of interest are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The <strong class="bold">Power Usage Effectiveness</strong> (<strong class="bold">PUE</strong>) metric <a id="_idIndexMarker1483"/>is a variable factor dependent on the cloud provider that signifies the energy efficiency of a data center. It estimates the overhead per computation cycle, which is obtained by evaluating the fraction of the total amount of power entering a data center required to make IT equipment fully functional. A highly efficient data center would have an ideal value close <span class="No-Break">to 1.0.</span></li>
				<li>The <strong class="bold">Total Power Usage Effectiveness</strong> (<strong class="bold">TUE</strong>) metric serves as an offset to reduce<a id="_idIndexMarker1484"/> the gaps created by the PUE metric. The TUE is obtained by taking the ratio of the total energy consumed by a data center to the energy consumed by the individual compute elements. It is thus more helpful for assessing environmental impact as it only considers the energy consumed for executed workloads instead of the whole <span class="No-Break">IT facility.</span></li>
				<li>The <strong class="bold">Green Energy Coefficient</strong> (<strong class="bold">GEC</strong>) metric <a id="_idIndexMarker1485"/>gives an estimate as a percentage of the energy consumed that is obtained from green energy sources, some of which are dependent on the weather, such as solar panels and wind turbines. Renewable-energy-driven data centers have a lower detrimental impact on the environment and are hence <span class="No-Break">more desirable.</span></li>
			</ul>
			<p>Ensuring a data centerâ€™s efficiency also requires following some of the best practices or techniques related to how we structure our programs to run on the cloud, and how we measure power consumption. It is worthwhile following some of the best practices <span class="No-Break">mentioned </span><span class="No-Break"><a id="_idIndexMarker1486"/></span><span class="No-Break">here:</span></p>
			<ul>
				<li>DevOps and cloud architects need not take into consideration the startup costs of the VMs as it <span class="No-Break">is negligible.</span></li>
				<li>Developers should ensure that all programs exploit all the cores available in the <span class="No-Break">VMs automatically.</span></li>
				<li>Specifying the compute resources is highly recommended at each step of the workflow to provision the creation of a VM. The best way to achieve this is to ensure sufficient disk space and memory availability to support the execution of the program. Allow applications to start processing whenever the available resources are ready to execute <span class="No-Break">the tasks.</span></li>
				<li>Allow submitted applications to execute all the steps of the task until <span class="No-Break">the end.</span></li>
				<li>Enable <strong class="bold">Dynamic Voltage and Frequency Scaling</strong> (<strong class="bold">DVFS</strong>) for PMs that host VMs to<a id="_idIndexMarker1487"/> reduce power consumption by dynamically adjusting the voltage and frequency of <span class="No-Break">the CPU.</span></li>
				<li>Forbid the over-commitment of resources on PMs so that they do not disturb the execution speed of <span class="No-Break">resource-intensive applications.</span></li>
			</ul>
			<p>Now let us estimate how we can track the amount of <span class="No-Break">carbon emitted.</span></p>
			<h1 id="_idParaDest-233"><a id="_idTextAnchor253"/>Carbon emission trackers</h1>
			<p>With a <a id="_idIndexMarker1488"/>thorough understanding of the carbon metrics on Google Cloud, our next lesson will focus on the computation mechanisms for the energy utilization of individual VMs, which we have illustrated with an example on GitHub. Our next step is to account for the carbon emissions by embedding carbon trackers, which can give us a detailed analysis of the emission statistics from within our <span class="No-Break">source code.</span></p>
			<p>Now, let us explore a few emission tools that can be used for FL and <span class="No-Break">centralized learning.</span></p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor254"/>The FL carbon calculator</h2>
			<p>The <a id="_idIndexMarker1489"/>amount of carbon emitted during FL from a <a id="_idIndexMarker1490"/>pool of devices can be tracked using a kind of FL carbon calculator (<a href="https://mlsys.cst.cam.ac.uk/carbon_fl/">https://mlsys.cst.cam.ac.uk/carbon_fl/</a>, <a href="https://github.com/mlco2/codecarbon">https://github.com/mlco2/codecarbon</a>), where the following parameters need to <span class="No-Break">be specified:</span></p>
			<ul>
				<li><strong class="bold">Devices</strong>: The<a id="_idIndexMarker1491"/> type of hardware being used by <span class="No-Break">the devices.</span></li>
				<li><strong class="bold">Country</strong>: The level of energy production resulting from the burning of fossil fuels is dependent on a country. The first parameter helps to access the electricity/carbon conversion rate for the country while the second parameter is needed to estimate the amount of carbon emissions resulting from communications between the client and <span class="No-Break">the server.</span></li>
				<li><strong class="bold">Dataset</strong>: This <a id="_idIndexMarker1492"/>helps to specify balanced, non-<strong class="bold">Independent Identical Distribution</strong> (<strong class="bold">IID</strong>) datasets such as ImageNet and CIFAR-10. Unlike IID datasets, non-IID datasets have random variables that are not mutually independent on each other, nor are they identically distributed. This makes them quite different in that they do not seem to come from the <span class="No-Break">same distribution.</span></li>
				<li><strong class="bold">The number of rounds</strong>: This<a id="_idIndexMarker1493"/> is used to specify the total number of iterations used to build the global model by the central server through the <span class="No-Break">aggregation process.</span></li>
				<li><strong class="bold">The number of Local Epochs</strong> (<strong class="bold">LEs</strong>): This<a id="_idIndexMarker1494"/> is used to specify the iterations at each client end to train their local model before sending them to <span class="No-Break">the server.</span></li>
				<li><strong class="bold">The number of active devices</strong>: This<a id="_idIndexMarker1495"/> is used to specify the count of active client devices in each round, which is usually a fraction of the total <span class="No-Break">devices used.</span></li>
				<li><strong class="bold">Network</strong>: This helps to define the internet upload/download speeds for a given set <span class="No-Break">of devices.</span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.1</em> demonstrates the amount of carbon emitted by 5 clients overall at the rate of 2.51 gCO<span class="subscript">2</span>eq, with 10 LEs and 10 rounds in an FL setup. Here, CO<span class="subscript">2</span>eq is responsible for measuring the energy utilization from hardware such as the CPU and GPU, and determining where the hardware is located. This, along with the regionâ€™s average CO<span class="subscript">2</span> emission (measured in gCO<span class="subscript">2</span>eq/KWh) and the ML modelâ€™s architecture, helps finalize the region of the deployment. For<a id="_idIndexMarker1496"/> example, deployments with high energy requirements can choose to select regions such as Quebec, Canada, but in certain other situations, they may be driven to deploy their solutions in Iowa, US, which has CO<span class="subscript">2</span> emissions as high as <span class="No-Break">735.6 gCO</span><span class="No-Break"><span class="subscript">2</span></span><span class="No-Break">eq/KWh:</span></p>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="image/Figure_12.1_B18681.jpg" alt="Figure 12.1 â€“ Carbon emissions from an FL setup with 5 clients with 10 LEs and 10 rounds"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 â€“ Carbon emissions from an FL setup with 5 clients with 10 LEs and 10 rounds</p>
			<p>Now, let us see how we compute the CO<span class="subscript">2</span>e for <span class="No-Break">centralized learning.</span></p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor255"/>Centralized learning carbon emissions calculator</h2>
			<p>Just as<a id="_idIndexMarker1497"/> with FL, we <a id="_idIndexMarker1498"/>can do the same for centralized learning, as illustrated in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.1</em>, using<a id="_idIndexMarker1499"/> an ML CO<span class="subscript">2</span> impact calculator (<a href="https://mlco2.github.io/impact/">https://mlco2.github.io/impact/</a> or <a href="https://github.com/mlco2/codecarbon">https://github.com/mlco2/codecarbon</a>)<span class="superscript"> </span>for different <span class="No-Break">cloud platforms.</span></p>
			<p>We also demonstrate in the following example how to measure carbon emissions using the<a id="_idIndexMarker1500"/> CodeCarbon tool, which can easily be integrated with a natural workflow. It captures emission <a id="_idIndexMarker1501"/>metrics from within the code and can help developers to track metrics at the function or <span class="No-Break">module level:</span></p>
			<ol>
				<li>As the first step, let us import all the <span class="No-Break">necessary libraries:</span><pre class="console">
import tensorflow as tf
from codecarbon import EmissionsTracker
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold,RandomizedSearchCV</pre></li>
				<li>In the<a id="_idIndexMarker1502"/> next step, the following code demonstrates training a DL model using the <span class="No-Break">MNIST dataset:</span><pre class="console">
def train_model():
Â Â Â Â mnist = tf.keras.datasets.mnist
Â Â Â Â (x_train, yv_train), (x_test, y_test) = mnist.load_data()
Â Â Â Â x_train, x_test = x_train / 255.0, x_test / 255.0
Â Â Â Â model = tf.keras.models.Sequential(
Â Â Â Â Â Â Â Â [
Â Â Â Â Â Â Â Â Â Â Â Â tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation=<strong class="source-inline">"</strong>relu<strong class="source-inline">"</strong>),
Â Â Â Â Â Â Â Â Â Â Â Â tf.keras.layers.Dropout(0.2),
Â Â Â Â Â Â Â Â Â Â Â Â tf.keras.layers.Dense(10),
Â Â Â Â Â Â Â Â ])
Â Â Â Â loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
Â Â Â Â model.compile(optimizer=<strong class="source-inline">"</strong>adam<strong class="source-inline">"</strong>, loss=loss_fn, metrics=[<strong class="source-inline">"</strong>accuracy<strong class="source-inline">"</strong>])
Â Â Â Â model.fit(x_train, y_train, epochs=10)
Â Â Â Â return model</pre></li>
				<li>Lastly, with <a id="_idIndexMarker1503"/>the following code, we can track the emission numbers for training the <span class="No-Break">DL code:</span><pre class="console">
with EmissionsTracker(project_name="mnist") as tracker:
Â Â Â Â model = train_model()
print(tracker.final_emissions)</pre></li>
			</ol>
			<p>Here is the output in kg CO<span class="subscript">2</span>:</p>
			<pre class="console">
0.0001063754808656955</pre>
			<p>We<a id="_idIndexMarker1504"/> can see that the DL model produces .0001063 kg of <span class="No-Break">CO</span><span class="No-Break"><span class="subscript">2</span></span><span class="No-Break"> emissions.</span></p>
			<p>We are now equipped with sustainable ML metrics for controlling our energy consumption. However, research has revealed that FL plays an important role in controlling CO<span class="subscript">2</span>e emissions in different circumstances. Let us now see how we can incorporate sustainability during model training and deployment <span class="No-Break">using FL.</span></p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor256"/>Adopting sustainable model training and deployment with FL</h1>
			<p>With an <a id="_idIndexMarker1505"/>exponential rise in ML training, over 300,000x from 2012 to 2018 â€“ that is, with a 3-4-month doubling period (well exceeding Mooreâ€™s 2-year doubling period) â€“ data scientists and algorithm researchers have increasingly investigated decentralized approaches of model training to try and curb the tremendous heat generated from DL models running on specialized hardware accelerators in data centers. This specialized hardware uses enormous amounts of energy (200 <strong class="bold">Terawatt-Hours</strong> (<strong class="bold">TWh</strong>)), higher than the national electricity consumption of some countries and contributing to 0.3% of global carbon emissions (as cited in the journal <em class="italic">Nature</em> in 2018). For example, Googleâ€™s AlphaGo Zero and training NLP models have released tons of CO<span class="subscript">2</span>e, demonstrating the urgency of adopting a <span class="No-Break">decentralized mechanism.</span></p>
			<p>The exponential rise in the number of mobile and IoT devices means that FL and its collaborative method of training have been instrumental in lowering power consumption and carbon emissions. Furthermore, it has been found that the design of FL has a greener environmental impact due to fewer carbon emissions when compared with <span class="No-Break">centralized learning.</span></p>
			<p>Now, let us understand the emission metrics in the model <span class="No-Break">training process.</span></p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor257"/>CO2e emission metrics</h2>
			<p>To <a id="_idIndexMarker1506"/>quantify the effect of training DL models in data centers or edge servers, the true environmental impact can only be<a id="_idIndexMarker1507"/> understood when we account for the following metrics, as energy consumption is truly translated into CO<span class="subscript">2</span>e emissions based on <span class="No-Break">geographical location:</span></p>
			<ul>
				<li>The total amount of energy consumed by the hardware for both centralized learning and FL systems, along with the communication energy for <span class="No-Break">FL systems</span></li>
				<li>The amount of energy consumed by the data centers due to cooling effects, particularly in the case of <span class="No-Break">centralized learning</span></li>
			</ul>
			<p>Now, let us isolate the factors that are responsible for energy consumption for centralized learning and FL-based <span class="No-Break">training factors.</span></p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor258"/>Comparing emission factors â€“ centralized learning versus FL</h2>
			<p>The<a id="_idIndexMarker1508"/> following factors summarize <a id="_idIndexMarker1509"/>the principal differentiating points between energy consumption in centralized learning <span class="No-Break">versus FL.</span></p>
			<h3>Hardware dependency</h3>
			<p>The energy consumption<a id="_idIndexMarker1510"/> in the training process for centralized learning can be derived by sampling the GPU and CPU power consumption at training time. The NVIDIA System Management Interface can be queried to average the GPU power consumption of all samples during training. However, for FL, besides the aggregation server and clients equipped with a GPU, this part of energy consumption can be <span class="No-Break">safely eliminated.</span></p>
			<p>In an FL environment with a distributed training setup and multiple communication rounds from heterogeneous edge devices, the training time can far exceed that of centralized training. This is because centralized training converges much more quickly. For FL, the training time, and hence the energy consumption, is closely related to the number of clients selected in each iteration, the data distribution of clients (which is most often non-IID), and the heterogeneous client devices with varying computational power that take part in the training process, as clients exhibit varying <span class="No-Break">computational power.</span></p>
			<p>In addition to these factors, the actual energy consumed is also affected by the hardware devices (RAM or HDD) employed at each client device, the infrastructure, and the device distribution. When scaling FL setups, we need to carefully evaluate all the possible options before making the proper selection for the FL clientâ€™s hardware options. Hence, to differentiate the energy consumption metrics for centralized learning and FL, we need to benchmark the hardware for FL clients to validate the <span class="No-Break">comparison metrics.</span></p>
			<h3>Data center cooling</h3>
			<p>The <a id="_idIndexMarker1511"/>data center cooling process plays a significant role in overall energy utilization, with contributions reaching as high as 40% of the total energy consumed (Capozzoli and Primiceri, 2015: <a href="https://www.researchgate.net/publication/290010399_Cooling_Systems_in_Data_Centers_State_of_Art_and_Emerging_Technologies0">https://www.researchgate.net/publication/290010399_Cooling_Systems_in_Data_Centers_State_of_Art_and_Emerging_Technologies0</a>). It is primarily dictated by the data centerâ€™s efficiency and estimated using the PUE ratio, which records an average of 1.67 for 2019 at a global level and is known for its variation across different cloud providers (recorded as 1.11 by Google, 1.2 by Amazon, and 1.125 by Microsoft in 2020). The FL training process does not have an associated cooling process. However, the central aggregation server can also be deployed with a <span class="No-Break">cooling functionality.</span></p>
			<p>Certain<a id="_idIndexMarker1512"/> data center cooling techniques employ optimal control mechanisms through the use of hot/cold aisle arrangement (not separating hot and cold aisles to promote free air mixing), containment (by isolating hot and cold air), rack placement (to promote heat circulation from rack hotspots), cable organization (to allow uninterrupted airflow in the data centers), and the usage of blanking panels (to block hot air from entering the data centerâ€™s airflow). These are efficient cooling methods that sustain the temperatures in the data centers. Even deploying monitoring tools to manage the data centerâ€™s airflow, humidity levels, temperature, air pressure, and hotspots can result in greater efficiency due to better temperature and pressure control of the <span class="No-Break">data center.</span></p>
			<h3>Energy utilization during data exchange</h3>
			<p>When training <a id="_idIndexMarker1513"/>happens on a central server, there is certainly no data exchange involved. In contrast, in an FL ecosystem, the data and models are transferred, downloaded, or uploaded between the central aggregation server and the distributed clients. In addition to this, energy utilization also takes into consideration the energy utilized by routers and hardware on account of downloads <span class="No-Break">and uploads.</span></p>
			<h2 id="_idParaDest-239"><a id="_idTextAnchor259"/>Illustrating how FL works better than centralized learning</h2>
			<p>The result of <a id="_idIndexMarker1514"/>experiments done by the Xinchi group (<a href="https://arxiv.org/pdf/2102.07627.pdf">https://arxiv.org/pdf/2102.07627.pdf</a>) explains, compares, and evaluates the impact of centralized learning <span class="No-Break">versus FL:</span></p>
			<ul>
				<li>The number of target communication rounds needed to attain the <span class="No-Break">target accuracy</span></li>
				<li>The energy associated with both centralized learning <span class="No-Break">and FL</span></li>
				<li>The training times for both centralized learning <span class="No-Break">and FL</span></li>
				<li>The different training optimizers used <span class="No-Break">with FL</span></li>
			</ul>
			<p>In the FL setup, each<a id="_idIndexMarker1515"/> client was engaged in training a small dataset with low-power, GPU-enabled edge devices. The setup achieved the target accuracy within one or five LEs. In contrast, in a centralized training environment, one LE translates to a standard epoch, which was applied during the training process on the <span class="No-Break">entire dataset.</span></p>
			<p>Furthermore, it has been demonstrated that the energy utilization is even lower in an FL setup than in a centralized learning setup using an energy-efficient training optimizer such <a id="_idIndexMarker1516"/>as FedAdam. FedAdam is well known for demonstrating faster initial convergence than <strong class="bold">Federated Averaging</strong> (<strong class="bold">FedAvg</strong>) and<a id="_idIndexMarker1517"/> even performing better than FedAvg along with non-adaptive optimizers. This adaptive optimization technique does not result in additional client storage or communication costs but rather ensures compatibility with <span class="No-Break">cross-device FL.</span></p>
			<p>In FedAvg, clients engaged in training <a id="_idIndexMarker1518"/>do so through several <strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>) steps to share local updates to the server. This method underperforms in heterogeneous settings (such as NLP domains with different users and different vocabularies) as it uses a simple average of the shared update to upgrade the <span class="No-Break">initial model.</span></p>
			<p>A gradient-based optimization technique such as FedAdam focuses on an adaptive server optimization procedure using per-coordinate methods to average the clientsâ€™ model updates. Here, the optimization at the server end aims to optimize the aggregated model from a global perspective using FedAvg and server momentum. On the other end, individual clients use a client optimizer over multiple epochs during local training. In this way, it minimizes the overall loss of local data during its limited <span class="No-Break">participation process.</span></p>
			<p>In FL, more often than centralized learning, a higher number of LEs results in the faster convergence of ML models in fewer FL rounds. Even though it does not ensure lower energy utilization, adaptive aggregation strategies such as FedAdam work better in terms of global model convergence speed. </p>
			<p>In addition, non-IID datasets need more FL rounds than IID datasets, which converge more quickly than <span class="No-Break">non-IID datasets.</span></p>
			<p>The effectiveness of the FedADAM optimizer is evident in the Xinchi group study (the experiments illustrated are taken from the research results of <em class="italic">A First Look into the Carbon Footprint of Federated Learning</em>, <a href="https://arxiv.org/pdf/2102.07627.pdf">https://arxiv.org/pdf/2102.07627.pdf</a>), where we see that it performed better than FedAvg for both the CIFAR-10 and SpeechCmd datasets. Furthermore, using five LEs, the amount of CO<span class="subscript">2</span>e emissions using FedAdam was lower than for centralized learning and FL <a id="_idIndexMarker1519"/>using FedAvg. For ImageNet experiments, to achieve the required test accuracy in the non-IID data case, the emissions were higher in FedAdam than FedAvg. This happened because the dataset is naturally unbalanced, which leads to rounds of training being required to reach the target test accuracy. This contributes to longer training times, leading to a higher level of CO<span class="subscript">2</span>e emissions. For example, the IID SpeechCmd dataset exhibits significantly lower emissions using the <span class="No-Break">FedAdam optimizer.</span></p>
			<p>Thus we see energy efficiency is dependent on a number of factors in both centralized learning and FL, a primary one being the optimizer used in FL. It is often possible, with non-IID datasets (for example, SpeechCmd) with the FedAdam optimizer, less complex model architectures, and fewer communication rounds, that FL yields lower CO<span class="subscript">2</span> emissions than centralized learning. Thus we can conclude that with lightweight neural networks employed, FL appears to be a better choice in terms of energy efficiency than <span class="No-Break">centralized learning.</span></p>
			<p>Let us understand how we can bring down CO<span class="subscript">2</span> emissions by making a trade-off and optimizing the following factors <span class="No-Break">in FL.</span></p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor260"/>The CO<span class="subscript">2</span> footprint of FL</h2>
			<p>Some of the<a id="_idIndexMarker1520"/> leading factors in deciding the CO<span class="subscript">2</span> footprint of centralized learning and FL training are as follows, resulting in varying levels of <span class="No-Break">CO</span><span class="No-Break"><span class="subscript">2</span></span><span class="No-Break">e emissions:</span></p>
			<ul>
				<li><strong class="bold">Geolocation of hardware</strong>: For example, training in France has been found to produce the lowest CO<span class="subscript">2</span>e emissions, as nuclear energy is the main source of energy in France, which consequently leads to the lowest energy-to-CO<span class="subscript">2</span>e <span class="No-Break">conversion rate.</span></li>
				<li><strong class="bold">The type of DL model, specifically the model architecture</strong>: For example, FedAvg-based FL optimizations have been found to emit more CO<span class="subscript">2</span> for image-based tasks using ResNet-18 than modern GPUs and centralized training. The same FL-based optimizations have been found to emit less CO<span class="subscript">2</span> on the SpeechCmd dataset when training an LSTM model. If the local training tasks on FL are lightweight, with less communication and data exchange, studies demonstrate that it will lead to lower <span class="No-Break">CO</span><span class="No-Break"><span class="subscript">2</span></span><span class="No-Break"> emissions.</span></li>
				<li><strong class="bold">Hardware efficiency</strong>: Chips such as Tegra X2 are likely to be embedded into smartphones, tablets, and other IoT devices. FL will continue to reduce emissions when using this type of <span class="No-Break">advanced chip.</span></li>
				<li><strong class="bold">Cooling needs and availability in data centers</strong>: FL does not have a centralized cooling process in a data center; those cooling requirements are distributed across the devices included in the federation and vary based on each deviceâ€™s needs. Distributed setups of centralized learning need a cooling facility. Advanced<a id="_idIndexMarker1521"/> GPUs or <strong class="bold">TPUs</strong> (short for <strong class="bold">Tensor Processing Units</strong>) requiring high <a id="_idIndexMarker1522"/>computational power demand more requirements for cooling and, consequently, high <span class="No-Break">energy utilization.</span></li>
				<li><strong class="bold">Communication rounds for FL</strong>: One LE in centralized learning yields more CO<span class="subscript">2</span> than five LEs, irrespective of the aggregation strategy or the device in place. This happens due to the fact that a lower number of local training epochs causes increased time for the model to converge. This ultimately leads to more data exchange and communication rounds between the local clients and the global server. With five LEs, individual devices train for longer, leading to fewer communication rounds and <span class="No-Break">lower emissions.</span></li>
				<li><strong class="bold">Data exchanges in FL</strong>: As we have seen before, the percentage of CO<span class="subscript">2</span>e emission is also <a id="_idIndexMarker1523"/>primarily driven by <strong class="bold">Wide-Area Networking</strong> (<strong class="bold">WAN</strong>) emissions due to exchanges between datasets and FL setups. For example, the energy utilization of communication may yield up to 0.4% (ImageNet with five LEs) and 95% (CIFAR-10 with one LE) of <span class="No-Break">total emissions.</span></li>
			</ul>
			<p>To bring parity and equivalency when comparing CO<span class="subscript">2</span>e emissions, the communication rounds in FL are converted into centralized epochs. CO<span class="subscript">2</span>e emissions are directly correlated to the number of centralized epochs. Some of the key factors in the increase in CO<span class="subscript">2</span> emissions put forward by the Xinchi group are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The number of <span class="No-Break">centralized epochs</span></li>
				<li>The thermal design power <span class="No-Break">of hardware</span></li>
				<li>An increase in the model size, increasing the energy used <span class="No-Break">for communication</span></li>
				<li>The size of the <span class="No-Break">training dataset</span></li>
				<li>The use of a better DL-based optimizer, such as FedAdam, which can outperform <span class="No-Break">centralized learning</span></li>
			</ul>
			<p>We <a id="_idIndexMarker1524"/>know how the devastating impacts of climate change over the last decade have required us to rethink and reconsider our emission metrics before deploying an architecture. Climate change and concern for a greener planet will drive research and innovation when designing new metrics. All these factors, when assembled, motivate us to choose better hardware that can provide visibility into the CO<span class="subscript">2</span> footprint and offer recommendations for possible remediations. It is now important for us to know how we can deploy popular design patterns for training <span class="No-Break">FL models.</span></p>
			<p>We should also look at other ways of generating energy from renewable sources, which can play an important role in convincing organizations to control their emissions and encourage investment in <span class="No-Break">renewable energy.</span></p>
			<p>Hence, let us walk through ways to compensate for <span class="No-Break">CO</span><span class="No-Break"><span class="subscript">2</span></span><span class="No-Break">e emissions.</span></p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor261"/>How to compensate for equivalent CO<span class="subscript">2</span>e emissions</h2>
			<p>To <a id="_idIndexMarker1525"/>compensate for CO<span class="subscript">2</span>e emissions, we need to understand how the information regarding the energy grid or the conversion rate from energy into CO<span class="subscript">2</span>e can be handled. As there is also a dearth of public sources of information, there are certain recommended practices to bear <span class="No-Break">in mind:</span></p>
			<ul>
				<li>We must move with the assumption that all data centers and edge devices connected to a local grid are directly associated with their physical location. Electricity-specific CO<span class="subscript">2</span>e emission factors are expressed in kg CO<span class="subscript">2</span>e/kWh, with varying factors associated with each country, such as in France (0.0790), the US (0.5741), and <span class="No-Break">China (0.9746).</span></li>
				<li>The emission technology (with suitable metrics to compute the energy lost when transmitting and distributing electricity) and the throughput or productivity of heat plants can impact the overall <span class="No-Break">energy utilization.</span></li>
				<li>Evaluating the conversion factor for both FL and centralized learning with the aforementioned assumptions in mind can help us compute and target an energy utilization metric that has the least CO<span class="subscript">2</span> emissions. In FL, the energy used for communication varies based on the type of data partitions (more energy is required for IID datasets), the number of communication rounds, the hardware type, the location, and other factors that we <span class="No-Break">have discussed.</span></li>
				<li>Our <a id="_idIndexMarker1526"/>next goal as sustainability experts is to compensate for the resultant CO<span class="subscript">2</span> emissions by allocating renewable energy sources through <a id="_idIndexMarker1527"/>buying <strong class="bold">Renewable Energy Credits</strong> (<strong class="bold">RECs</strong>) in the US or <strong class="bold">Tradable Green Certificates</strong> (<strong class="bold">TGCs</strong>) in <a id="_idIndexMarker1528"/>the EU. Even initiatives to support environment-friendly projects, such as renewable energy initiatives or massive tree-planting activities, can <span class="No-Break">be encouraged.</span></li>
			</ul>
			<p>We now need to consider how we train FL models by considering different parameters that impact <span class="No-Break">CO</span><span class="No-Break"><span class="subscript">2</span></span><span class="No-Break"> emissions.</span></p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor262"/>Design patterns of FL-based model training</h2>
			<p>Till <a id="_idIndexMarker1529"/>now, we have been concentrating on how FL is a great methodology from a sustainability standpoint. However, apart from sustainability, FL also offers a great benefit from a data privacy standpoint, in relation to Responsible AI. In FL, clients are able to share anonymized learning from their local data, instead of having to share potentially sensitive data with a centralized process. We will learn more about this in this section as we learn more about the different design patterns <span class="No-Break">of FL.</span></p>
			<p>FL has different design and deployment strategies that will impact its CO<span class="subscript">2</span> emission metrics. Some of the key metrics are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>The size of the non-IID datasets in <span class="No-Break">each client</span></li>
				<li>The number of <span class="No-Break">participating clients</span></li>
				<li>The wait time before clients engage in training that impacts the <span class="No-Break">model convergence</span></li>
				<li>The training time for local clients and the time to aggregate the <span class="No-Break">global model</span></li>
			</ul>
			<p>Let us now see the different modes of <span class="No-Break">FL-based training.</span></p>
			<p>When we<a id="_idIndexMarker1530"/> think about model training and data processing patterns using FL, we need to consider the type of data, along with the motivation of the clients to participate in local training. Hence, we can primarily divide them into <span class="No-Break">three types:</span></p>
			<ul>
				<li>A multi-task model trainer to train <span class="No-Break">non-IID datasets</span></li>
				<li>A heterogeneous data handler known for training <span class="No-Break">heterogeneous datasets</span></li>
				<li>An incentive registry, which motivates clients through a <span class="No-Break">reward system</span></li>
			</ul>
			<p>In each of the illustrated figures corresponding to each of the design patterns, we used the sequence of FL, which is <span class="No-Break">as follows:</span></p>
			<ol>
				<li>A generic global model is trained by the <span class="No-Break">central server.</span></li>
				<li>Each client selected in the specific training round downloads the global model and kicks off the local <span class="No-Break">training process.</span></li>
				<li>The locally trained model from the client end is then updated on the <span class="No-Break">global server.</span></li>
				<li>The server aggregates the global models using the FedAvg algorithm to improve the shared version of <span class="No-Break">the model.</span></li>
				<li>The local devices are then updated with the newly retrained global model, which prepares them for local retraining and updates them for <span class="No-Break">successive iterations.</span></li>
			</ol>
			<h3>The multi-task model trainer</h3>
			<p>This mode of<a id="_idIndexMarker1531"/> training has the primary <a id="_idIndexMarker1532"/>objective of improving learning efficiency and model performance metrics. Furthermore, it is most suitable for training separate related models on local devices. Specifically, we use this when the data distribution patterns of clients differ and the global model falls short of representing the data pattern exhibited by every client. For example, as the following figure illustrates, training separate models on computer <a id="_idIndexMarker1533"/>vision, reinforcement learning, speech recognition, and NLP is essential for modeling and demonstrating the purpose of <span class="No-Break">each model:</span></p>
			<div>
				<div id="_idContainer242" class="IMG---Figure">
					<img src="image/Figure_12.02_B18681.jpg" alt="Figure 12.2 â€“ FL model training design pattern â€“ a multi-task model trainer"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 â€“ FL model training design pattern â€“ a multi-task model trainer</p>
			<p>One<a id="_idIndexMarker1534"/> real-world example would be to use ML to solve a next-word prediction task (performs related machine learning tasks involving NLP) by using device data such as text messages, web browser search strings, <span class="No-Break">and emails.</span></p>
			<p>To obtain higher accuracy, this design involves more training time, computation, and energy resources in every round, as expected with other conventional FL techniques. The major challenge of this method is the vulnerability of local clients to data privacy threats and the limitations of the training to convex <span class="No-Break">loss functions.</span></p>
			<h3>A heterogeneous data handler</h3>
			<p>This <a id="_idIndexMarker1535"/>mode of training preserves<a id="_idIndexMarker1536"/> data privacy and finds better vanilla FL usage with non-IID and skewed data distributions by applying special processing techniques such as data augmentation and adversarial training with a generative <span class="No-Break">adversarial network:</span></p>
			<div>
				<div id="_idContainer243" class="IMG---Figure">
					<img src="image/Figure_12.03_B18681.jpg" alt="Figure 12.3 â€“ FL model training design pattern â€“ a heterogeneous data handler"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 â€“ FL model training design pattern â€“ a heterogeneous data handler</p>
			<p>As the personalized data of clients results in imbalanced and skewed data distributions, local models trained on devices tend to lower the global model accuracy upon aggregation. Hence, it <a id="_idIndexMarker1537"/>becomes essential to promote data efficiency by plugging in a heterogeneous data handler that<a id="_idIndexMarker1538"/> can correctly augment and distill the federated data and still preserve data privacy. The distillation process equips the client devices to gather information from other participating devices at intervals without any direct access to other <span class="No-Break">clientsâ€™ data.</span></p>
			<p>This mechanism tries to yield better model performance metrics at the cost of the training time and computational resources, which, ultimately, results in lower energy efficiency and a lower <span class="No-Break">sustainability metric.</span></p>
			<h3>An incentive registry</h3>
			<p>The <a id="_idIndexMarker1539"/>training model shown in the following<a id="_idIndexMarker1540"/> figure rewards each participating client based on their contribution in terms of data volume, model performance, and computation resources, among other things. This is a measure to motivate clients and improve the performance of the global model. The following figure demonstrates a blockchain and a smart-contract-based <span class="No-Break">incentive mechanism:</span></p>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<img src="image/Figure_12.04_B18681.jpg" alt="Figure 12.4 â€“ FL model training design pattern â€“ an incentive registry"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 â€“ FL model training design pattern â€“ an incentive registry</p>
			<p>Motivation energizes more clients to participate, as this training strategy is not driven by the complete participation of each and every client in every iteration. Moreover, the incentive scheme needs a mutual agreement between the clients and the learning coordinator to decide the evaluation criteria. Some of the common ways to formulate incentive schemes are reinforcement learning, blockchain/smart contracts, and the Stackelberg game model. One specific employment of blockchain-based FL incorporating incentives is FLChain, which supports collaborative training and a marketplace for model trading. This model also suffers from the challenge of long training times <a id="_idIndexMarker1541"/>and <span class="No-Break">computation resources.</span></p>
			<p>In this section, we <a id="_idIndexMarker1542"/>learned about important factors that control the FL training setup, including the parameters, the number of clients, time, and the epoch period of training. All these factors contribute to the energy used in training environments, which contributes to global warming. Beyond model training, let us also learn how different modes of model deployment also have an impact on energy emissions. In the following section, we will primarily discuss a strategy in an FL environment in which only selected clients are engaged in training. Although we will refer to individual clients taking part in the training process, our main goal is to understand the operational or deployment modes that enable clients to trigger the local <span class="No-Break">training process.</span></p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor263"/>Sustainability in model deployments</h2>
			<p>In the <a id="_idIndexMarker1543"/>previous section, we learned how FL plays an important role in sustainability. Now, let us explore a sustainable ML framework for FL. We will see how to use rechargeable devices that are capable of accumulating energy from the ambient environment and effectively utilizing it during intermittent <span class="No-Break">training periods.</span></p>
			<p>This kind of framework <a id="_idIndexMarker1544"/>can be extended to cross-device and cross-silo FL settings, including FL in wireless edge networks, IoT, and the <strong class="bold">Internet of Medical Devices</strong> (<strong class="bold">IoMD</strong>). Individual local device training can drive model convergence, as well as make it adaptable to the <span class="No-Break">following settings:</span></p>
			<ul>
				<li>Enabling the random selection of a small number of clients to minimize the communication overhead during <span class="No-Break">every iteration.</span></li>
				<li>Enabling the selection of clients to maximize the learning rate to speed up the <span class="No-Break">convergence rate.</span></li>
				<li>Enabling a client selection process based on the energy arrival process at each client. This is to address intermittent and non-homogeneous energy arrival patterns to effectively manage clients <span class="No-Break">dropping out.</span></li>
			</ul>
			<p>This deployment framework can rightly fit scenarios in which heterogeneous devices are available and each training round works independently. This makes the framework much more flexible, as each device participating in the training process in one round is not held accountable or dependent on future consecutive rounds. The training process (driven by random energy availability) is coordinated by the central server, which aggregates the local models of the clients to send them the updated global model. The individual clients are engaged in the local training with their own datasets, as well as updating the global model received using multiple SGD iterations over their local dataset. The framework not only benefits from energy-efficient sustainable FL training but also minimizes the total energy cost of training in scenarios where devices generate energy through an intermittent and non-homogeneous <span class="No-Break">renewal process.</span></p>
			<p>The <a id="_idIndexMarker1545"/>framework allows clients of different types to have varying levels of energy generation to participate in training, with two <span class="No-Break">different configurations:</span></p>
			<ul>
				<li>A biased model prediction strategy, where<a id="_idIndexMarker1546"/> the global model is biased toward clients that have more frequent energy availability. This kind of model sees a performance loss in the accuracy of the predicted outcomes but yields a better <span class="No-Break">convergence rate.</span></li>
				<li>An unbiased model prediction strategy, where<a id="_idIndexMarker1547"/> there is a wait associated with letting all the clients generate enough energy before each iteration of participating in the training process. This kind of model suffers from the longest wait time based on the slowest client. However, this kind of training has better performance metrics, despite having a slow <span class="No-Break">convergence rate.</span></li>
			</ul>
			<p>The proposed framework is equipped with four different entities, as described here, to facilitate an energy-aware client scheduling and <span class="No-Break">training strategy:</span></p>
			<ul>
				<li><strong class="bold">The energy profile of clients</strong>: Accounting for the energy availability of all clients <a id="_idIndexMarker1548"/>is the most important factor, as it governs the client participation based on the energy received from the ambient environment, such as solar, kinetic, ambient light, or RF energy. Profiling is a mechanism for studying energy patterns to infer the availability of sufficient energy to train the local model and send updates to the central server. Any participating client starting the training process at a global round starting at the initial time instant <em class="italic">t</em> ensures the client participates for the whole duration of that global round, <em class="italic">{t, . . ., t+T âˆ’1}</em>, where the client is engaged in training the local model. In addition, clients taking part in the training process for any global round <a id="_idIndexMarker1549"/>remain constant during the entire duration of training for that specified <span class="No-Break">global round.</span></li>
				<li><strong class="bold">Client scheduling</strong>: The framework comes with the flexibility of allowing clients to <a id="_idIndexMarker1550"/>decide whether they want to participate in the training process at that specific iteration based on their energy profile. The <a id="_idIndexMarker1551"/>stochastic participation process incorporates clients after locally estimating the energy arrival process to maximize the convergence rate or reduce the communication overhead of training. The scheduling process does not necessitate coordination among clients, making it easier to scale in <span class="No-Break">large networks.</span></li>
				<li><strong class="bold">Local training at the client</strong>: This phase allows participating clients to train their local datasets using SGD and then update them on the <span class="No-Break">central server.</span></li>
				<li><strong class="bold">Model updates by the server</strong>: This phase allows the server to aggregate the local models to consolidate the global model to be sent to <span class="No-Break">individual devices.</span></li>
			</ul>
			<p>Having studied the principal influencing factors behind energy-intelligent client scheduling, here, we will discuss how the scheduling process works based on the energy availability <span class="No-Break">of clients.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.5</em> illustrates the two different prediction strategies, where clients either take part as soon as energy arrives or wait until all clients have accumulated the energy required to start training. The left-hand part of the figure is optimistic about model convergence and starts client scheduling immediately when the energy is available (as represented by different colors of <em class="italic">t</em><span class="subscript">1</span>, <em class="italic">t</em><span class="subscript">2</span>, <em class="italic">t</em><span class="subscript">3</span>, and <em class="italic">t</em><span class="subscript">4</span>). On the other hand, the right-hand subfigure is conservative and waits until energy is available for all the clients to schedule them together, as represented by them all being the same color, orange, where <em class="italic">t</em><span class="subscript">1</span>, <em class="italic">t</em><span class="subscript">2</span>, <em class="italic">t</em><span class="subscript">3</span>, and <em class="italic">t</em><span class="subscript">4</span> are different <span class="No-Break">time instances:</span></p>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<img src="image/Figure_12.05_B18681.jpg" alt="Figure 12.5 â€“ Client scheduling based on energy arrival patterns"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 â€“ Client scheduling based on energy arrival patterns</p>
			<p>In this <a id="_idIndexMarker1552"/>section, we have learned how sustainable FL can be <a id="_idIndexMarker1553"/>deployed in large-scale networks using stochastic energy arrival processes. Furthermore, adapting model quantization and compression techniques with a completely random energy availability among clients promises a better characterization of the relationship between the energy renewal processes and the <span class="No-Break">training performance.</span></p>
			<p>We have understood scheduling the clientâ€™s training process based on energy. Now, let us see how these training patterns can be scheduled according to the architecture and how to scale <span class="No-Break">FL models.</span></p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor264"/>Design patterns of FL-based model deployments</h2>
			<p>Let <a id="_idIndexMarker1554"/>us deep-dive into how FL models<a id="_idIndexMarker1555"/> can be managed and aggregated once they are live in production. Here, we will discuss two types of architectural designs that come into play, mainly from the perspective of data communication, model management, and governance, and how these models are aggregated and distributed to <span class="No-Break">local clients:</span></p>
			<ul>
				<li><strong class="bold">FL-based model management patterns</strong>: We need the model management patterns to<a id="_idIndexMarker1556"/> establish the rules and processes related to the local clientâ€™s data or model size. The size plays a critical role in the data or model exchange and, hence, the amount of energy consumed. In addition, the frequency of replacing the model and updating the global model serves as a deciding factor in the CO<span class="subscript">2</span>e and the sustainability <span class="No-Break">of FL.</span></li>
				<li><strong class="bold">FL-based model aggregation patterns</strong>: We need model aggregation patterns in<a id="_idIndexMarker1557"/> FL to consolidate learning from individual clients to create an updated global model. The mode of the mÂ­odel aggregation process from the deployed clients â€“ be it asynchronous, hierarchical, or decentralized â€“ impacts the timing and the latency. All these factors contribute to varying levels <span class="No-Break">of CO</span><span class="No-Break"><span class="subscript">2</span></span><span class="No-Break">e.</span></li>
			</ul>
			<p>Let us discuss these designs in the <span class="No-Break">following sections.</span></p>
			<h3>FL-based model management patterns</h3>
			<p>Model management patterns <a id="_idIndexMarker1558"/>are responsible for model transmission, deployment, and governance. Based on how messages are transferred or models are stored locally by clients, we can divide them into four broad categories, as <span class="No-Break">described here:</span></p>
			<ul>
				<li>A message compressor, which reduces the transmitted <span class="No-Break">message size</span></li>
				<li>A model co-versioning registry, involved in model version management by receiving model updates from clients, helping to facilitate <span class="No-Break">model aggregation</span></li>
				<li>A model replacement unit, which monitors the global modelâ€™s performance and initiates new training when the model starts to show a <span class="No-Break">reduced performance</span></li>
				<li>A deployment selector, which pushes the improved global model to <span class="No-Break">the clients</span></li>
			</ul>
			<p>Let us discuss each of these four categories <span class="No-Break">in detail.</span></p>
			<h4>A message compressor</h4>
			<p>This<a id="_idIndexMarker1559"/> design pattern, as demonstrated in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.6</em>, compresses the message to reduce its data size during the model exchange process during <span class="No-Break">every round:</span></p>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="image/Figure_12.06_B18681.jpg" alt=" Figure 12.6 â€“ FL model deployment design pattern â€“ message compressor"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 12.6 â€“ FL model deployment design pattern â€“ message compressor</p>
			<p>This process helps to increase the efficiency of communication by effectively compressing model parameters or gradients in limited-bandwidth scenarios. However, the pattern incurs additional computation costs when the server must aggregate sizeable model parameters. Moreover, it also adds an overhead for message compression and decompression and may involve the loss of <span class="No-Break">essential information.</span></p>
			<h3>A model co-versioning registry</h3>
			<p>The <a id="_idIndexMarker1560"/>model co-versioning registry, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.7</em>, aids the local model governance process by <a id="_idIndexMarker1561"/>tracking each clientâ€™s model version and aligning it with the global model of that <span class="No-Break">corresponding iteration:</span></p>
			<div>
				<div id="_idContainer247" class="IMG---Figure">
					<img src="image/Figure_12.07_B18681.jpg" alt="Figure 12.7 â€“ FL model deployment design pattern â€“ model co-versioning registry"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7 â€“ FL model deployment design pattern â€“ model co-versioning registry</p>
			<p>This facilitates synchronous and asynchronous model updates, client dropouts, model selection, and stopping complex model training early. Furthermore, this pattern has the unique advantage of tracking the model quality (including global models, client device updates, and<a id="_idIndexMarker1562"/> the version of the application or the device OS/firmware), and adversarial activities by clients (where a client acts as an adversary) to strengthen the accountability of the system. The<a id="_idIndexMarker1563"/> registry is used to collect local model updates and map them to the global model so that the model version number and client IDs stay immutable. This pattern can also be implemented using a blockchain to provide model provenance and co-versioning. The added storage volume required by this architecture to store all versions of the global and local models comes with the increased benefit of offering system security to detect dishonest clients that may cause a system failure. One prime example of this pattern is the MLflow Model Registry built on Databricks, which has an efficient centralized model store for tracking the chronological model lineage, versioning, and <span class="No-Break">stage transitions.</span></p>
			<h4>A model replacement trigger</h4>
			<p>This <a id="_idIndexMarker1564"/>FL design pattern initiates the replacement of the model by setting a trigger on a new model task whenever the modelâ€™s performance degrades below an acceptable threshold. To address issues such as a drop in the modelâ€™s accuracy, this pattern provides mechanisms to investigate the reason for this reduced accuracy before establishing the new model training process. The retraining process is set only when the degradation in model performance is noticed for a few consecutive rounds to strongly infer that the performance reduction is global. Retraining the model adds a cost both in terms of communication and computation but is effective in handling clients in an FL environment with heterogenous non-IID datasets where clients have personalized datasets and exhibit  faster decays in the global model's performance over successive iterations. Microsoft Azure Machine Learning designer and Amazon SageMaker are well-known platforms for triggering model retraining based on the degradation of <span class="No-Break">model performance.</span></p>
			<h4>A deployment selector</h4>
			<p>This <a id="_idIndexMarker1565"/>deployment strategy, as demonstrated in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.8</em>, enables the convergence of the global model and pushes the global model to selected clients based on the application on which they run. Hence, different groups of clients receive different versions of converged models, and this aims to improve the performance metrics of the models. The model selection addresses the non-IID data distribution among clients so that different customized converged models serve different groups of clients better. However, this procedure comes with added overhead on the server, as the central server needs to properly identify the clients to provide them with different versions of the global model and, at the same time, train and store different models for diverse clients. Despite the increased training cost, we get better-generalized models based on the ML task of the clients. This strategy needs to have extra privacy measures built in to prevent privacy leakage when the server tries to group clients. This type of FL training is available on Amazon SageMaker and Google Cloud, which trains and manages multiple model versions and deploys them at <span class="No-Break">different endpoints:</span></p>
			<div>
				<div id="_idContainer248" class="IMG---Figure">
					<img src="image/Figure_12.08_B18681.jpg" alt="Figure 12.8 â€“ FL model deployment design pattern â€“ model deployment selector"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.8 â€“ FL model deployment design pattern â€“ model deployment selector</p>
			<h3>FL-based model aggregation patterns</h3>
			<p>This <a id="_idIndexMarker1566"/>design pattern employs different tactics for <a id="_idIndexMarker1567"/>model aggregation to reduce aggregation latency and increase the systemâ€™s efficiency, reliability, and accountability. The primary objective is to improve the model performance metrics by effectively utilizing optimal resources. They can be broadly divided into the following four types <span class="No-Break">of patterns:</span></p>
			<ul>
				<li>An asynchronous <span class="No-Break">secure aggregator</span></li>
				<li>A <span class="No-Break">decentralized aggregator</span></li>
				<li>A <span class="No-Break">security aggregator</span></li>
				<li>A <span class="No-Break">hierarchical aggregator</span></li>
			</ul>
			<p>We <a id="_idIndexMarker1568"/>have<a id="_idIndexMarker1569"/> illustrated the four patterns in the following subsections; the last two patterns can be combined into a hybrid hierarchical <span class="No-Break">secure aggregator.</span></p>
			<h4>An asynchronous aggregator</h4>
			<p>This <a id="_idIndexMarker1570"/>asynchronous global model aggregation strategy, as shown in the following figure, enables us to speed up the model aggregation with the arrival of a new model update without waiting for the models that are trained locally <span class="No-Break">the clients.</span></p>
			<div>
				<div id="_idContainer249" class="IMG---Figure">
					<img src="image/Figure_12.09_B18681.jpg" alt="Figure 12.9 â€“ FL model deployment design pattern â€“ asynchronous aggregator"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.9 â€“ FL model deployment design pattern â€“ asynchronous aggregator</p>
			<p>This mechanism differs from conventional FL by allowing clients to skip the first aggregation round, where clients engage in updating models asynchronously during the next (second or successive) aggregation phase. Hence, it accommodates the variability of clients in terms of their computational resources, bandwidth availability, and communication capacities. The clients enjoy a definite advantage when participating in global model aggregation where the model convergence process by the server is not impacted <span class="No-Break">by delays.</span></p>
			<p>The challenge of employing this strategy results in a bias in the global model, which may impact the modelâ€™s quality if important information is eliminated. In addition, the maximum aggregation latency at the server is dictated by when the slowest client sends <a id="_idIndexMarker1571"/>the updates, leading to slow <span class="No-Break">model convergence.</span></p>
			<p>However, some of the key benefits include centralized aggregation happening at each round instead of waiting for the slowest client. We also see reduced bandwidth usage during each round, as few clients send their <span class="No-Break">updates asynchronously.</span></p>
			<p>Examples<a id="_idIndexMarker1572"/> of design <a id="_idIndexMarker1573"/>patterns include <strong class="bold">Asynchronous Online Federated Learning</strong> and <strong class="bold">Asynchronous </strong><span class="No-Break"><strong class="bold">Federated Optimization</strong></span><span class="No-Break">.</span></p>
			<h4>A decentralized aggregator</h4>
			<p>This <a id="_idIndexMarker1574"/>aggregation strategy, as demonstrated in the following figure, follows a decentralized FL approach by removing the dependency on a central server that turns out to be the single point of failure. Here, the central server may experience additional loads, as it receives and aggregates updates from all clients. The figure illustrates how a blockchain and a smart contract can be used to receive model updates when updates are shared with neighboring devices. In conventional FL training, the system may suffer from privacy limitations when not all participating clients trust the <span class="No-Break">central server.</span></p>
			<div>
				<div id="_idContainer250" class="IMG---Figure">
					<img src="image/Figure_12.10_B18681.jpg" alt="Figure 12.10 â€“ FL model deployment design pattern â€“ decentralized aggregator"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.10 â€“ FL model deployment design pattern â€“ decentralized aggregator</p>
			<p>This decentralized approach is recommended where model updates are allowed between peer client devices. However, this architecture also requires a proper definition of a decentralized model management system, where peer systems can collect, store, examine, and aggregate the local models. Moreover, we also need to define system ownership, through which a random client is selected to perform the aggregation from the local nearby clients and send the aggregated model to the client network. The blockchain serves as one of the best options for decentralized FL, where it can store immutable models and the learning coordinator can maintain the blockchain. A blockchain mechanism is reliable, accountable, and trustworthy, thereby increasing resiliency to adversarial attacks, trust, <span class="No-Break">and transparency.</span></p>
			<p>One of the <a id="_idIndexMarker1575"/>major drawbacks of this system is the latency involved due to the blockchain consensus protocols during the model aggregation process. In addition, client devices may also experience power drainage due to their parallel participation in training and model aggregation. Even this kind of decentralized architecture runs the risk of clients exposing sensitive information about their peers to others. One practical example of this mode of peer-to-peer learning is BrainTorrent, in which clients engage in direct learning and communication with <span class="No-Break">each other.</span></p>
			<h4>A security aggregator</h4>
			<p>This <a id="_idIndexMarker1576"/>aggregation strategy supports built-in security protocols to protect the model better than vanilla FL without any support for data encryption. This is in contrast to conventional FL, which does not take data encryption and the secure exchange of parameters into consideration and leaves room for unauthorized data access. To prevent malicious clients from joining the training process and curb poisoning attacks on the data/model, we need to ensure that the proper security protocols are in place as and when models send updates to the central server. The best way to ensure that model parameters and gradients are not accessible to third parties is to employ secure, multi-party computation for model exchanges and aggregations to guarantee that each participating client is aware of its model input and output. We can also employ homomorphic encryption to allow the client to encrypt and the server to decrypt the model. Application-level security mechanisms such as pairwise masking and differential privacy help reduce data leakages to <span class="No-Break">external adversaries.</span></p>
			<p>Some of the drawbacks of this mode of architecture include the reduction in system efficiency and lower model accuracy at the cost of the addition of extra security protocols and encryption methods. Hence, we need to choose the right privacy thresholds to balance the trade-off between the modelâ€™s performance and privacy. The Secure Aggregation protocol (developed by Google) serves as one of the prime examples of a secured aggregation protocol <span class="No-Break">in FL.</span></p>
			<h4>A hierarchical aggregator</h4>
			<p>This <a id="_idIndexMarker1577"/>aggregation technique adds an extra hierarchical aggregator layer (such as an edge server) to reduce the impact of non-IID on the global model and increase the systemâ€™s efficiency. The following figure illustrates how we might employ edge network 1 and edge network 2 to carry out partial aggregation from nearby client devices before triggering the global aggregation process. This design pattern, with intermediate servers at the edge networks, has been introduced to handle slow communication between clients and distant servers to improve the systemâ€™s efficiency. Involving a hierarchical aggregator in the process helps scale FL systems and facilitates a better global model aggregation, as the server can assemble local models from clients that have similar <span class="No-Break">data heterogeneity.</span></p>
			<p>The main drawback of this kind of architecture is the system reliability when devices are disconnected from the edge servers, which impacts the model training and performance. In addition, we need to be extra cautious about the security protocols, as edge servers can have security breaches and are more often subjected to network <span class="No-Break">security threats.</span></p>
			<p>One real-world example of an FL model that directly uses this architecture is Hierarchical FedAvg, where multiple edge servers are employed for partial model aggregation depending on updates that are incrementally received from <span class="No-Break">the clients:</span></p>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="image/Figure_12.11_B18681.jpg" alt="Figure 12.11 â€“ FL model deployment design pattern â€“ hierarchical aggregator"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.11 â€“ FL model deployment design pattern â€“ hierarchical aggregator</p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor265"/>Summary</h1>
			<p>In this chapter, we have learned about the rising importance of sustainability when developing AI solutions, as well as the desired cloud metrics and architectural and operational know-how that can help us to reduce CO<span class="subscript">2</span> emissions. We saw a detailed overview of how FL models can be trained based on energy availability at the clients' end to reduce the detrimental impact of higher CO<span class="subscript">2</span> emissions. We are now aware of efficient FL-based design patterns, whether training, model management, or model <span class="No-Break">aggregation patterns.</span></p>
			<p>In the context of training and deployment strategies for FL, we also got the chance to explore the benefits of different DL-based training optimizers and their impact on the sustainability of solutions. Furthermore, we walked through the sustainability factors in centralized learning versus FL, which can help us to estimate the CO<span class="subscript">2</span> footprint of the GPU compute using the specification of the type of hardware, its efficiency, the active runtime period, the model architecture, cooling needs, the cloud provider, and the region. We explored how to calculate the total energy consumed, the different emission metrics we can use, and the best practices we can follow to maximize our data <span class="No-Break">centersâ€™ efficiency.</span></p>
			<p>In the next chapter, we will go deeper into exploring how to ensure sustainability while creating <span class="No-Break">feature stores.</span></p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor266"/>Further reading</h1>
			<ul>
				<li><em class="italic">Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine </em><span class="No-Break"><em class="italic">Learning</em></span><span class="No-Break">: </span><a href="https://jmlr.org/papers/volume21/20-312/20-312.pdf"><span class="No-Break">https://jmlr.org/papers/volume21/20-312/20-312.pdf</span></a></li>
				<li><em class="italic">How Can I Calculate CO</em>2<em class="italic">eq emissions for my Azure </em><span class="No-Break"><em class="italic">VM?</em></span><span class="No-Break">: </span><a href="https://devblogs.microsoft.com/sustainable-software/how-can-i-calculate-CO2eq-emissions-for-my-azure-vm/"><span class="No-Break">https://devblogs.microsoft.com/sustainable-software/how-can-i-calculate-CO2eq-emissions-for-my-azure-vm/</span></a></li>
				<li><em class="italic">A Framework for Sustainable Federated Learning, </em>B. GÃ¼ler and A. Yener: <a href="https://dl.ifip.org/db/conf/wiopt/wiopt2021/WiOpt_2021_paper_100-invited.pdf">https://dl.ifip.org/db/conf/wiopt/wiopt2021/WiOpt_2021_paper_100-invited.pdf</a>, 2021 19th International Symposium on Modeling and Optimization in Mobile, Ad hoc, and Wireless <span class="No-Break">Networks (WiOpt)</span></li>
				<li><em class="italic">Sustainable federated </em><span class="No-Break"><em class="italic">learning</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2102.11274.pdf"><span class="No-Break">https://arxiv.org/pdf/2102.11274.pdf</span></a></li>
				<li><em class="italic">Cloud </em><span class="No-Break"><em class="italic">sustainability</em></span><span class="No-Break">: </span><a href="https://cloud.google.com/sustainability"><span class="No-Break">https://cloud.google.com/sustainability</span></a></li>
				<li><em class="italic">How carbon-free is your cloud? New data lets you </em><span class="No-Break"><em class="italic">know</em></span><span class="No-Break">: </span><a href="https://cloud.google.com/blog/topics/sustainability/sharing-carbon-free-energy-percentage-for-google-cloud-regions"><span class="No-Break">https://cloud.google.com/blog/topics/sustainability/sharing-carbon-free-energy-percentage-for-google-cloud-regions</span></a></li>
				<li><em class="italic">CO</em><span class="subscript">2</span><em class="italic"> in Federated </em><span class="No-Break"><em class="italic">Learning</em></span><span class="No-Break">: </span><a href="https://mlsys.cst.cam.ac.uk/carbon_fl/"><span class="No-Break">https://mlsys.cst.cam.ac.uk/carbon_fl/</span></a></li>
				<li><em class="italic">Can Federated Learning Save The </em><span class="No-Break"><em class="italic">Planet?</em></span><span class="No-Break"> </span><a href="https://arxiv.org/pdf/2010.06537.pdf"><span class="No-Break">https://arxiv.org/pdf/2010.06537.pdf</span></a></li>
				<li><em class="italic">Architectural Patterns for the Design of Federated Learning </em><span class="No-Break"><em class="italic">Systems</em></span><span class="No-Break">: </span><a href="https://www.researchgate.net/publication/348316341_Architectural_Patterns_for_the_Design_of_Federated_Learning_Systems"><span class="No-Break">https://www.researchgate.net/publication/348316341_Architectural_Patterns_for_the_Design_of_Federated_Learning_Systems</span></a></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer253" class="IMG---Figure">
			</div>
		</div>
	</body></html>