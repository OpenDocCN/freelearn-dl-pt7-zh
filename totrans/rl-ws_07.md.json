["```py\n    alpha = 0.02\n    epsilon = 0.05\n    ```", "```py\n    q = np.ones((16,4))\n    ```", "```py\n    for i in range(nb_episodes):\n            s = env.reset()\n            a = action_epsilon_greedy(q, s, epsilon=epsilon)\n    ```", "```py\nwhile not done:\n            new_s, reward, done, info = env.step(a)\n            new_a = action_epsilon_greedy(q, new_s, epsilon=epsilon)\n            q[s, a] = q[s, a] + alpha * (reward + gamma \\\n                      * q[new_s, new_a] - q[s, a])\n            s = new_s\n            a = new_a\n```", "```py\n    import numpy as np\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    import gym\n    ```", "```py\n    env = gym.make('FrozenLake-v0', is_slippery=False)\n    ```", "```py\n    print(\"Action space = \", env.action_space)\n    print(\"Observation space = \", env.observation_space)\n    ```", "```py\n    Action space =  Discrete(4)\n    Observation space =  Discrete(16)\n    ```", "```py\n    actionsDict = {}\n    actionsDict[0] = \" L \"\n    actionsDict[1] = \" D \"\n    actionsDict[2] = \" R \"\n    actionsDict[3] = \" U \"\n    actionsDictInv = {}\n    actionsDictInv[\"L\"] = 0\n    actionsDictInv[\"D\"] = 1\n    actionsDictInv[\"R\"] = 2\n    actionsDictInv[\"U\"] = 3\n    ```", "```py\n    env.reset()\n    env.render()\n    ```", "```py\n    optimalPolicy = [\"R/D\",\" R \",\" D \",\" L \", \\\n                     \" D \",\" - \",\" D \",\" - \", \\\n                     \" R \",\"R/D\",\" D \",\" - \", \\\n                     \" - \",\" R \",\" R \",\" ! \",]\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Optimal policy:  \n    R/D  R   D  L\n     D   -   D  -\n     R  R/D  D  -\n     -   R   R  !\n    ```", "```py\n    def action_epsilon_greedy(q, s, epsilon=0.05):\n        if np.random.rand() > epsilon:\n            return np.argmax(q[s])\n        return np.random.randint(4)\n    def get_action_epsilon_greedy(epsilon):\n        return lambda q,s: action_epsilon_greedy\\\n                           (q, s, epsilon=epsilon)\n    ```", "```py\n    def greedy_policy(q, s):\n        return np.argmax(q[s])\n    ```", "```py\n    def average_performance(policy_fct, q):\n        acc_returns = 0.\n        n = 500\n        for i in range(n):\n            done = False\n            s = env.reset()\n            while not done:\n                a = policy_fct(q, s)\n                s, reward, done, info = env.step(a)\n                acc_returns += reward\n        return acc_returns/n\n    ```", "```py\n    nb_episodes = 80000\n    STEPS = 2000\n    epsilon_param = [[0.2, 0.001, int(nb_episodes/2)]]\n    ```", "```py\n    def sarsa(alpha = 0.02, \\\n              gamma = 1., \\\n              epsilon_start = 0.1, \\\n              epsilon_end = 0.001, \\\n              epsilon_annealing_stop = int(nb_episodes/2), \\\n              q = None, \\\n              progress = None, \\\n              env=env):\n        if q is None:\n            q = np.ones((16,4))\n            # Set q(terminal,*) equal to 0\n            q[5,:] = 0.0\n            q[7,:] = 0.0\n            q[11,:] = 0.0\n            q[12,:] = 0.0\n            q[15,:] = 0.0\n    ```", "```py\n        for i in range(nb_episodes):\n    ```", "```py\n            inew = min(i,epsilon_annealing_stop)\n            epsilon = (epsilon_start \\\n                       *(epsilon_annealing_stop - inew)\\\n                       +epsilon_end * inew) / epsilon_annealing_stop\n    ```", "```py\n            done = False\n            s = env.reset()\n            a = action_epsilon_greedy(q, s, epsilon=epsilon)\n    ```", "```py\n            while not done:\n    ```", "```py\n                new_s, reward, done, info = env.step(a)\n    ```", "```py\n                new_a = action_epsilon_greedy\\\n                        (q, new_s, epsilon=epsilon)\n                q[s, a] = q[s, a] + alpha * (reward + gamma \\\n                          * q[new_s, new_a] - q[s, a])\n                s = new_s\n                a = new_a\n    ```", "```py\n            if progress is not None and i%STEPS == 0:\n                progress[i//STEPS] = average_performance\\\n                                     (get_action_epsilon_greedy\\\n                                     (epsilon), q=q)\n        return q, progress\n    ```", "```py\n    sarsa_performance = np.ndarray(nb_episodes//STEPS)\n    q, sarsa_performance = sarsa(alpha = 0.02, gamma = 0.9, \\\n                                 progress=sarsa_performance, \\\n                                 epsilon_start=epsilon_param[0][0],\\\n                                 epsilon_end=epsilon_param[0][1], \\\n                                 epsilon_annealing_stop = \\\n                                 epsilon_param[0][2])\n    ```", "```py\n    plt.plot(STEPS*np.arange(nb_episodes//STEPS), sarsa_performance)\n    plt.xlabel(\"Epochs\")\n    plt.title(\"Learning progress for SARSA\")\n    plt.ylabel(\"Average reward of an epoch\")\n    ```", "```py\n    Text(0, 0.5, 'Average reward of an epoch')\n    ```", "```py\n    greedyPolicyAvgPerf = average_performance(greedy_policy, q=q)\n    print(\"Greedy policy SARSA performance =\", greedyPolicyAvgPerf)\n    ```", "```py\n    Greedy policy SARSA performance = 1.0\n    ```", "```py\n    q = np.round(q,3)\n    print(\"(A,S) Value function =\", q.shape)\n    print(\"First row\")\n    print(q[0:4,:])\n    print(\"Second row\")\n    print(q[4:8,:])\n    print(\"Third row\")\n    print(q[8:12,:])\n    print(\"Fourth row\")\n    print(q[12:16,:])\n    ```", "```py\n    (A,S) Value function = (16, 4)\n    First row \n    [[0.505 0.59  0.54  0.506]\n     [0.447 0.002 0.619 0.494]\n     [0.49  0.706 0.487 0.562]\n     [0.57  0.379 0.53  0.532]]\n    Second row\n    [[0.564 0.656 0\\.    0.503]\n     [0\\.    0\\.    0\\.    0\\.   ]\n     [0.003 0.803 0.002 0.567]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Third row\n    [[0.62  0\\.    0.728 0.555]\n     [0.63  0.809 0.787 0\\.   ]\n     [0.707 0.899 0\\.    0.699]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Fourth row\n    [[0\\.    0\\.    0\\.    0\\.   ]\n     [0\\.    0.791 0.9   0.696]\n     [0.797 0.895 1\\.    0.782]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    ```", "```py\n    policyFound = [actionsDict[np.argmax(q[0,:])],\\\n                   actionsDict[np.argmax(q[1,:])], \\\n                   actionsDict[np.argmax(q[2,:])], \\\n                   actionsDict[np.argmax(q[3,:])], \\\n                   actionsDict[np.argmax(q[4,:])], \\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[6,:])], \\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[8,:])], \\\n                   actionsDict[np.argmax(q[9,:])], \\\n                   actionsDict[np.argmax(q[10,:])], \\\n                   \" - \",\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[13,:])], \\\n                   actionsDict[np.argmax(q[14,:])], \\\n                   \" ! \"]\n    print(\"Greedy policy found:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(policyFound[idx+0], policyFound[idx+1], \\\n              policyFound[idx+2], policyFound[idx+3])\n    print(\" \")\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Greedy policy found: \n     D   R   D  L\n     D   -   D  -\n     R   D   D  -\n     -   R   R  !  \n\n    Optimal policy:  \n    R/D  R   D  L\n     D   -   D  -\n     R  R/D  D  -\n     -   R   R  !\n    ```", "```py\n    import numpy as np\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    import gym\n    ```", "```py\n    env = gym.make('FrozenLake-v0', is_slippery=True)\n    ```", "```py\n    print(\"Action space = \", env.action_space)\n    print(\"Observation space = \", env.observation_space)\n    ```", "```py\n    Action space =  Discrete(4)\n    Observation space =  Discrete(16)\n    ```", "```py\n    actionsDict = {}\n    actionsDict[0] = \"  L  \"\n    actionsDict[1] = \"  D  \"\n    actionsDict[2] = \"  R  \"\n    actionsDict[3] = \"  U  \"\n    actionsDictInv = {}\n    actionsDictInv[\"L\"] = 0\n    actionsDictInv[\"D\"] = 1\n    actionsDictInv[\"R\"] = 2\n    actionsDictInv[\"U\"] = 3\n    ```", "```py\n    env.reset()\n    env.render()\n    ```", "```py\n    optimalPolicy = [\"L/R/D\",\"  U  \",\"  U  \",\"  U  \",\\\n                     \"  L  \",\"  -  \",\" L/R \",\"  -  \",\\\n                     \"  U  \",\"  D  \",\"  L  \",\"  -  \",\\\n                     \"  -  \",\"  R  \",\"  D  \",\"  !  \",]\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Optimal policy:  \n      L/R/D  U    U    U\n        L    -   L/R   -\n        U    D    L    -\n        -    R    D    !\n    ```", "```py\n    def action_epsilon_greedy(q, s, epsilon=0.05):\n        if np.random.rand() > epsilon:\n            return np.argmax(q[s])\n        return np.random.randint(4)\n    def get_action_epsilon_greedy(epsilon):\n        return lambda q,s: action_epsilon_greedy\\\n                           (q, s, epsilon=epsilon)\n    ```", "```py\n    def greedy_policy(q, s):\n        return np.argmax(q[s])\n    ```", "```py\n    def average_performance(policy_fct, q):\n        acc_returns = 0.\n        n = 100\n        for i in range(n):\n            done = False\n            s = env.reset()\n            while not done:\n                a = policy_fct(q, s)\n                s, reward, done, info = env.step(a)\n                acc_returns += reward\n        return acc_returns/n\n    ```", "```py\n    nb_episodes = 80000\n    STEPS = 2000\n    epsilon_param = [[0.2, 0.001, int(nb_episodes/2)]]\n    ```", "```py\n    def sarsa(alpha = 0.02, \\\n              gamma = 1., \\\n              epsilon_start = 0.1,\\\n              epsilon_end = 0.001,\\\n              epsilon_annealing_stop = int(nb_episodes/2),\\\n              q = None, \\\n              progress = None, \\\n              env=env):\n        if q is None:\n            q = np.ones((16,4))\n            # Set q(terminal,*) equal to 0\n            q[5,:] = 0.0\n            q[7,:] = 0.0\n            q[11,:] = 0.0\n            q[12,:] = 0.0\n            q[15,:] = 0.0\n    ```", "```py\n        for i in range(nb_episodes):\n    ```", "```py\n            inew = min(i,epsilon_annealing_stop)\n            epsilon = (epsilon_start \\\n                       * (epsilon_annealing_stop - inew)\\\n                       + epsilon_end * inew) \\\n                       / epsilon_annealing_stop\n            done = False\n            s = env.reset()\n            a = action_epsilon_greedy(q, s, epsilon=epsilon)\n    ```", "```py\n            while not done:\n    ```", "```py\n                new_s, reward, done, info = env.step(a)\n    ```", "```py\n                new_a = action_epsilon_greedy\\\n                        (q, new_s, epsilon=epsilon)\n                q[s, a] = q[s, a] + alpha \\\n                          * (reward + gamma \\\n                             * q[new_s, new_a] - q[s, a])\n                s = new_s\n                a = new_a\n    ```", "```py\n            if progress is not None and i%STEPS == 0:\n                progress[i//STEPS] = average_performance\\\n                                     (get_action_epsilon_greedy\\\n                                     (epsilon), q=q)\n        return q, progress\n    ```", "```py\n    sarsa_performance = np.ndarray(nb_episodes//STEPS)\n    q, sarsa_performance = sarsa(alpha = 0.02, gamma = 1,\\\n                                 progress=sarsa_performance, \\\n                                 epsilon_start=epsilon_param[0][0],\\\n                                 epsilon_end=epsilon_param[0][1], \\\n                                 epsilon_annealing_stop = \\\n                                 epsilon_param[0][2])\n    ```", "```py\n    plt.plot(STEPS*np.arange(nb_episodes//STEPS), sarsa_performance)\n    plt.xlabel(\"Epochs\")\n    plt.title(\"Learning progress for SARSA\")\n    plt.ylabel(\"Average reward of an epoch\")\n    ```", "```py\n    Text(0, 0.5, 'Average reward of an epoch')\n    ```", "```py\n    greedyPolicyAvgPerf = average_performance(greedy_policy, q=q)\n    print(\"Greedy policy SARSA performance =\", greedyPolicyAvgPerf)\n    ```", "```py\n    Greedy policy SARSA performance = 0.75\n    ```", "```py\n    q = np.round(q,3)\n    print(\"(A,S) Value function =\", q.shape)\n    print(\"First row\")\n    print(q[0:4,:])\n    print(\"Second row\")\n    print(q[4:8,:])\n    print(\"Third row\")\n    print(q[8:12,:])\n    print(\"Fourth row\")\n    print(q[12:16,:])\n    ```", "```py\n    (A,S) Value function = (16, 4)\n    First row\n    [[0.829 0.781 0.785 0.785]\n     [0.416 0.394 0.347 0.816]\n     [0.522 0.521 0.511 0.813]\n     [0.376 0.327 0.378 0.811]]\n    Second row\n    [[0.83  0.552 0.568 0.549]\n     [0\\.    0\\.    0\\.    0\\.   ]\n     [0.32  0.195 0.535 0.142]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Third row\n    [[0.55  0.59  0.546 0.831]\n     [0.557 0.83  0.441 0.506]\n     [0.776 0.56  0.397 0.342]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Fourth row\n    [[0\\.    0\\.    0\\.    0\\.   ]\n     [0.528 0.619 0.886 0.506]\n     [0.814 0.943 0.877 0.844]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    ```", "```py\n    policyFound = [actionsDict[np.argmax(q[0,:])],\\\n                   actionsDict[np.argmax(q[1,:])],\\\n                   actionsDict[np.argmax(q[2,:])],\\\n                   actionsDict[np.argmax(q[3,:])],\\\n                   actionsDict[np.argmax(q[4,:])],\\\n                   \"  -  \",\\\n                   actionsDict[np.argmax(q[6,:])],\\\n                   \"  -  \",\\\n                   actionsDict[np.argmax(q[8,:])],\\\n                   actionsDict[np.argmax(q[9,:])],\\\n                   actionsDict[np.argmax(q[10,:])],\\\n                   \"  -  \",\\\n                   \"  -  \",\\\n                   actionsDict[np.argmax(q[13,:])],\\\n                   actionsDict[np.argmax(q[14,:])],\\\n                   \"  !  \"]\n    print(\"Greedy policy found:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(policyFound[idx+0], policyFound[idx+1], \\\n              policyFound[idx+2], policyFound[idx+3])\n    print(\" \")\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Greedy policy found:\n        L    U    U    U\n        L    -    R    -\n        U    D    L    -\n        -    R    D    !\n    Optimal policy:  \n      L/R/D  U    U    U\n        L    -   L/R   -\n        U    D    L    -\n        -    R    D    !\n    ```", "```py\n    alpha = 0.02\n    epsilon_expl = 0.2\n    ```", "```py\n    q = np.ones((16, 4))\n    # Set q(terminal,*) equal to 0\n    q[5,:] = 0.0\n    q[7,:] = 0.0\n    q[11,:] = 0.0\n    q[12,:] = 0.0\n    q[15,:] = 0.0\n    ```", "```py\n    for i in range(nb_episodes):\n        done = False\n        s = env.reset()\n    ```", "```py\n        while not done:\n            # behavior policy\n            a = action_epsilon_greedy(q, s, epsilon=epsilon_expl)\n    ```", "```py\n        new_s, reward, done, info = env.step(a)\n        a_max = np.argmax(q[new_s]) # estimation policy \n        q[s, a] = q[s, a] + alpha \\\n                  * (reward + gamma \\\n                     * q[new_s, a_max] -q[s, a])\n        s = new_s\n```", "```py\n    import numpy as np\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    import gym\n    ```", "```py\n    env = gym.make('FrozenLake-v0', is_slippery=False)\n    ```", "```py\n    print(\"Action space = \", env.action_space)\n    print(\"Observation space = \", env.observation_space)\n    ```", "```py\n    Action space =  Discrete(4)\n    Observation space =  Discrete(16)\n    ```", "```py\n    actionsDict = {}\n    actionsDict[0] = \" L \"\n    actionsDict[1] = \" D \"\n    actionsDict[2] = \" R \"\n    actionsDict[3] = \" U \"\n    actionsDictInv = {}\n    actionsDictInv[\"L\"] = 0\n    actionsDictInv[\"D\"] = 1\n    actionsDictInv[\"R\"] = 2\n    actionsDictInv[\"U\"] = 3\n    ```", "```py\n    env.reset()\n    env.render()\n    ```", "```py\n    optimalPolicy = [\"R/D\",\" R \",\" D \",\" L \",\\\n                     \" D \",\" - \",\" D \",\" - \",\\\n                     \" R \",\"R/D\",\" D \",\" - \",\\\n                     \" - \",\" R \",\" R \",\" ! \",]\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Optimal policy:  \n    R/D  R   D  L\n     D   -   D  -\n     R  R/D  D  -\n     -   R   R  !\n    ```", "```py\n    def action_epsilon_greedy(q, s, epsilon=0.05):\n        if np.random.rand() > epsilon:\n            return np.argmax(q[s])\n        return np.random.randint(4)\n    ```", "```py\n    def greedy_policy(q, s):\n        return np.argmax(q[s])\n    ```", "```py\n    def average_performance(policy_fct, q):\n        acc_returns = 0.\n        n = 500\n        for i in range(n):\n            done = False\n            s = env.reset()\n            while not done:\n                a = policy_fct(q, s)\n                s, reward, done, info = env.step(a)\n                acc_returns += reward\n        return acc_returns/n\n    ```", "```py\n    q = np.ones((16, 4))\n    # Set q(terminal,*) equal to 0\n    q[5,:] = 0.0\n    q[7,:] = 0.0\n    q[11,:] = 0.0\n    q[12,:] = 0.0\n    q[15,:] = 0.0\n    ```", "```py\n    nb_episodes = 40000\n    STEPS = 2000\n    alpha = 0.02\n    gamma = 0.9\n    epsilon_expl = 0.2\n    q_performance = np.ndarray(nb_episodes//STEPS)\n    ```", "```py\n    for i in range(nb_episodes):\n        done = False\n        s = env.reset()\n        while not done:\n            # behavior policy\n            a = action_epsilon_greedy(q, s, epsilon=epsilon_expl)\n            new_s, reward, done, info = env.step(a)\n            a_max = np.argmax(q[new_s]) # estimation policy \n            q[s, a] = q[s, a] + alpha \\\n                      * (reward + gamma \\\n                         * q[new_s, a_max] - q[s, a])\n            s = new_s\n        # for plotting the performance\n        if i%STEPS == 0:\n            q_performance[i//STEPS] = average_performance\\\n                                      (greedy_policy, q)\n    ```", "```py\n    plt.plot(STEPS * np.arange(nb_episodes//STEPS), q_performance)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Average reward of an epoch\")\n    plt.title(\"Learning progress for Q-Learning\")\n    ```", "```py\n    Text(0.5, 1.0, 'Learning progress for Q-Learning')\n    ```", "```py\n    greedyPolicyAvgPerf = average_performance(greedy_policy, q=q)\n    print(\"Greedy policy Q-learning performance =\", \\\n          greedyPolicyAvgPerf)\n    ```", "```py\n    Greedy policy Q-learning performance = 1.0\n    ```", "```py\n    q = np.round(q,3)\n    print(\"(A,S) Value function =\", q.shape)\n    print(\"First row\")\n    print(q[0:4,:])\n    print(\"Second row\")\n    print(q[4:8,:])\n    print(\"Third row\")\n    print(q[8:12,:])\n    print(\"Fourth row\")\n    print(q[12:16,:])\n    ```", "```py\n    (A,S) Value function = (16, 4)\n    First row\n    [[0.531 0.59  0.59  0.531]\n     [0.617 0.372 0.656 0.628]\n     [0.672 0.729 0.694 0.697]\n     [0.703 0.695 0.703 0.703]]\n    Second row\n    [[0.59  0.656 0\\.    0.531]\n     [0\\.    0\\.    0\\.    0\\.   ]\n     [0.455 0.81  0.474 0.754]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Third row\n    [[0.656 0\\.    0.729 0.59 ]\n     [0.656 0.81  0.81  0\\.   ]\n     [0.778 0.9   0.286 0.777]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Fourth row\n    [[0\\.    0\\.    0\\.    0\\.   ]\n     [0\\.    0.81  0.9   0.729]\n     [0.81  0.9   1\\.    0.81 ]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    ```", "```py\n    policyFound = [actionsDict[np.argmax(q[0,:])],\\\n                   actionsDict[np.argmax(q[1,:])],\\\n                   actionsDict[np.argmax(q[2,:])],\\\n                   actionsDict[np.argmax(q[3,:])],\\\n                   actionsDict[np.argmax(q[4,:])],\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[6,:])],\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[8,:])],\\\n                   actionsDict[np.argmax(q[9,:])],\\\n                   actionsDict[np.argmax(q[10,:])],\\\n                   \" - \",\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[13,:])],\\\n                   actionsDict[np.argmax(q[14,:])],\\\n                   \" ! \"]\n    print(\"Greedy policy found:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(policyFound[idx+0], policyFound[idx+1], \\\n              policyFound[idx+2], policyFound[idx+3])\n    print(\" \")\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Greedy policy found: \n     D   R   D  L\n     D   -   D  -\n     R   D   D  -\n     -   R   R  !  \n    Optimal policy:  \n    R/D  R   D  L\n     D   -   D  -\n     R  R/D  D  -\n     -   R   R  !\n    ```", "```py\nq[s, a] = q[s, a] + alpha * (reward + gamma * \n    (np.dot(pi[new_s, :],q[new_s, :]) - q[s, a])\n```", "```py\n    alpha = 0.02\n    n = 4\n    epsilon = 0.05\n    ```", "```py\n    q = np.ones((16,4))\n    ```", "```py\n    for i in range(nb_episodes):\n            s = env.reset()\n            a = action_epsilon_greedy(q, s, epsilon=epsilon)\n            T = 1e6\n    ```", "```py\n    while True:\n                new_s, reward, done, info = env.step(a)\n                if done:\n                    T = t+1\n    ```", "```py\n            new_a = action_epsilon_greedy(q, new_s, epsilon=epsilon)\n    ```", "```py\n    tau = t-n+1\n    ```", "```py\n    G = sum_n(q, tau, T, t, gamma, R, new_s, new_a)\n    q[s, a] = q[s, a] + alpha * (G- q[s, a]) \n    ```", "```py\n    alpha = 0.02\n    n = 4\n    ```", "```py\n    q = np.ones((16,4))\n    ```", "```py\n    for i in range(nb_episodes):\n            s = env.reset()\n            a = action_b_policy(q, s)\n            T = 1e6\n    ```", "```py\n    while True:\n                new_s, reward, done, info = env.step(a)\n                if done:\n                    T = t+1\n    ```", "```py\n            new_a = action_b_policy(q, new_s)\n    ```", "```py\n    tau = t-n+1\n    ```", "```py\n    rho = product_n(q, tau, T, t, R, new_s, new_a)\n    G = sum_n(q, tau, T, t, gamma, R, new_s, new_a)\n    q[s, a] = q[s, a] + alpha * rho * (G- q[s, a]) \n    ```", "```py\n    alpha = 0.02\n    lambda = 0.3\n    epsilon = 0.05\n    ```", "```py\n    q = np.ones((16,4))\n    ```", "```py\n    E = np.zeros((16, 4))\n    ```", "```py\n        state = env.reset()\n        action = action_epsilon_greedy(q, state, epsilon)\n        while True:\n    ```", "```py\n            E = eligibility_decay * gamma * E \n            E[state, action] += 1\n    ```", "```py\n            new_state, reward, done, info = env.step(action)\n            new_action = action_epsilon_greedy\\\n                         (q, new_state, epsilon)\n    ```", "```py\n            delta = reward + gamma \\\n                    * q[new_state, new_action] - q[state, action]\n            q = q + alpha * delta * E \n    ```", "```py\n            state, action = new_state, new_action\n            if done:\n                break\n    ```", "```py\n    import numpy as np\n    from numpy.random import random, choice\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    import gym\n    ```", "```py\n    env = gym.make('FrozenLake-v0', is_slippery=False)\n    ```", "```py\n    print(\"Action space = \", env.action_space)\n    print(\"Observation space = \", env.observation_space)\n    ```", "```py\n    Action space =  Discrete(4)\n    Observation space =  Discrete(16)\n    ```", "```py\n    actionsDict = {}\n    actionsDict[0] = \" L \"\n    actionsDict[1] = \" D \"\n    actionsDict[2] = \" R \"\n    actionsDict[3] = \" U \"\n    actionsDictInv = {}\n    actionsDictInv[\"L\"] = 0\n    actionsDictInv[\"D\"] = 1\n    actionsDictInv[\"R\"] = 2\n    actionsDictInv[\"U\"] = 3\n    ```", "```py\n    env.reset()\n    env.render()\n    ```", "```py\n    optimalPolicy = [\"R/D\",\" R \",\" D \",\" L \",\\\n                     \" D \",\" - \",\" D \",\" - \",\\\n                     \" R \",\"R/D\",\" D \",\" - \",\\\n                     \" - \",\" R \",\" R \",\" ! \",]\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Optimal policy:  \n    R/D  R   D  L\n     D   -   D  -\n     R  R/D  D  -\n     -   R   R  !\n    ```", "```py\n    def action_epsilon_greedy(q, s, epsilon=0.05):\n        if np.random.rand() > epsilon:\n            return np.argmax(q[s])\n        return np.random.randint(4)\n    def get_action_epsilon_greedy(epsilon):\n        return lambda q,s: action_epsilon_greedy\\\n                           (q, s, epsilon=epsilon)\n    ```", "```py\n    def greedy_policy(q, s):\n        return np.argmax(q[s])\n    ```", "```py\n    def average_performance(policy_fct, q):\n        acc_returns = 0.\n        n = 500\n        for i in range(n):\n            done = False\n            s = env.reset()\n            while not done:\n                a = policy_fct(q, s)\n                s, reward, done, info = env.step(a)\n                acc_returns += reward\n        return acc_returns/n\n    ```", "```py\n    # parameters for sarsa(lambda)\n    episodes = 30000\n    STEPS = 500\n    gamma = 0.9\n    alpha = 0.05\n    epsilon_start = 0.2\n    epsilon_end = 0.001\n    epsilon_annealing_stop = int(episodes/2)\n    eligibility_decay = 0.3\n    ```", "```py\n    q = np.zeros((16, 4))\n    # Set q(terminal,*) equal to 0\n    q[5,:] = 0.0\n    q[7,:] = 0.0\n    q[11,:] = 0.0\n    q[12,:] = 0.0\n    q[15,:] = 0.0\n    performance = np.ndarray(episodes//STEPS)\n    ```", "```py\n    for episode in range(episodes):\n    ```", "```py\n        inew = min(episode,epsilon_annealing_stop)\n        epsilon = (epsilon_start * (epsilon_annealing_stop - inew) \\\n                   + epsilon_end * inew) / epsilon_annealing_stop\n    ```", "```py\n        E = np.zeros((16, 4))\n    ```", "```py\n        state = env.reset()\n        action = action_epsilon_greedy(q, state, epsilon)\n        while True:\n    ```", "```py\n            E = eligibility_decay * gamma * E\n            E[state, action] += 1\n    ```", "```py\n            new_state, reward, done, info = env.step(action)\n    ```", "```py\n            new_action = action_epsilon_greedy\\\n                         (q, new_state, epsilon)\n    ```", "```py\n            delta = reward + gamma \\\n                    * q[new_state, new_action] - q[state, action]\n            q = q + alpha * delta * E \n    ```", "```py\n            state, action = new_state, new_action\n            if done:\n                break\n    ```", "```py\n        if episode%STEPS == 0:\n            performance[episode//STEPS] = average_performance\\\n                                          (get_action_epsilon_greedy\\\n                                          (epsilon), q=q)\n    ```", "```py\n    plt.plot(STEPS*np.arange(episodes//STEPS), performance)\n    plt.xlabel(\"Epochs\")\n    plt.title(\"Learning progress for SARSA\")\n    plt.ylabel(\"Average reward of an epoch\")\n    ```", "```py\n    Text(0, 0.5, 'Average reward of an epoch')\n    ```", "```py\n    greedyPolicyAvgPerf = average_performance(greedy_policy, q=q)\n    print(\"Greedy policy SARSA performance =\", greedyPolicyAvgPerf)\n    ```", "```py\n    Greedy policy SARSA performance = 1.0\n    ```", "```py\n    q = np.round(q,3)\n    print(\"(A,S) Value function =\", q.shape)\n    print(\"First row\")\n    print(q[0:4,:])\n    print(\"Second row\")\n    print(q[4:8,:])\n    print(\"Third row\")\n    print(q[8:12,:])\n    print(\"Fourth row\")\n    print(q[12:16,:])\n    ```", "```py\n    (A,S) Value function = (16, 4)\n    First row\n    [[0.499 0.59  0.519 0.501]\n     [0.474 0\\.    0.615 0.518]\n     [0.529 0.699 0.528 0.589]\n     [0.608 0.397 0.519 0.517]]\n    Second row\n    [[0.553 0.656 0\\.    0.489]\n     [0\\.    0\\.    0\\.    0\\.   ]\n     [0\\.    0.806 0\\.    0.593]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Third row\n    [[0.619 0\\.    0.729 0.563]\n     [0.613 0.77  0.81  0\\.   ]\n     [0.712 0.9   0\\.    0.678]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Fourth row\n    [[0\\.    0\\.    0\\.    0\\.   ]\n     [0.003 0.8   0.9   0.683]\n     [0.76  0.892 1\\.    0.787]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    ```", "```py\n    policyFound = [actionsDict[np.argmax(q[0,:])],\\\n                   actionsDict[np.argmax(q[1,:])],\\\n                   actionsDict[np.argmax(q[2,:])],\\\n                   actionsDict[np.argmax(q[3,:])],\\\n                   actionsDict[np.argmax(q[4,:])],\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[6,:])],\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[8,:])],\\\n                   actionsDict[np.argmax(q[9,:])],\\\n                   actionsDict[np.argmax(q[10,:])],\\\n                   \" - \",\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[13,:])],\\\n                   actionsDict[np.argmax(q[14,:])],\\\n                   \" ! \"]\n    print(\"Greedy policy found:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(policyFound[idx+0], policyFound[idx+1], \\\n              policyFound[idx+2], policyFound[idx+3])\n    print(\" \")\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Greedy policy found:\n     R   R   D   L \n     D   -   D   - \n     R   D   D   - \n     -   R   R   ! \n    Optimal policy:\n    R/D  R   D   L \n     D   -   D   - \n     R  R/D  D   - \n     -   R   R   !\n    ```", "```py\n    import numpy as np\n    from numpy.random import random, choice\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    import gym\n    ```", "```py\n    env = gym.make('FrozenLake-v0', is_slippery=True)\n    ```", "```py\n    print(\"Action space = \", env.action_space)\n    print(\"Observation space = \", env.observation_space)\n    ```", "```py\n    Action space =  Discrete(4)\n    Observation space =  Discrete(16)\n    ```", "```py\n    actionsDict = {}\n    actionsDict[0] = \"  L  \"\n    actionsDict[1] = \"  D  \"\n    actionsDict[2] = \"  R  \"\n    actionsDict[3] = \"  U  \"\n    actionsDictInv = {}\n    actionsDictInv[\"L\"] = 0\n    actionsDictInv[\"D\"] = 1\n    actionsDictInv[\"R\"] = 2\n    actionsDictInv[\"U\"] = 3\n    ```", "```py\n    env.reset()\n    env.render()\n    ```", "```py\n    optimalPolicy = [\"L/R/D\",\"  U  \",\"  U  \",\"  U  \",\\\n                     \"  L  \",\"  -  \",\" L/R \",\"  -  \",\\\n                     \"  U  \",\"  D  \",\"  L  \",\"  -  \",\\\n                     \"  -  \",\"  R  \",\"  D  \",\"  !  \",]\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Optimal policy:  \n      L/R/D  U    U    U\n        L    -   L/R   -\n        U    D    L    -\n        -    R    D    !\n    ```", "```py\n    def action_epsilon_greedy(q, s, epsilon=0.05):\n        if np.random.rand() > epsilon:\n            return np.argmax(q[s])\n        return np.random.randint(4)\n    def get_action_epsilon_greedy(epsilon):\n        return lambda q,s: action_epsilon_greedy\\\n                           (q, s, epsilon=epsilon)\n    ```", "```py\n    def greedy_policy(q, s):\n        return np.argmax(q[s])\n    ```", "```py\n    def average_performance(policy_fct, q):\n        acc_returns = 0.\n        n = 500\n        for i in range(n):\n            done = False\n            s = env.reset()\n            while not done:\n                a = policy_fct(q, s)\n                s, reward, done, info = env.step(a)\n                acc_returns += reward\n        return acc_returns/n\n    ```", "```py\n    # parameters for sarsa(lambda)\n    episodes = 80000\n    STEPS = 2000\n    gamma = 1\n    alpha = 0.02\n    epsilon_start = 0.2\n    epsilon_end = 0.001\n    epsilon_annealing_stop = int(episodes/2)\n    eligibility_decay = 0.3\n    ```", "```py\n    q = np.zeros((16, 4))\n    # Set q(terminal,*) equal to 0\n    q[5,:] = 0.0\n    q[7,:] = 0.0\n    q[11,:] = 0.0\n    q[12,:] = 0.0\n    q[15,:] = 0.0\n    performance = np.ndarray(episodes//STEPS)\n    ```", "```py\n    for episode in range(episodes):\n    ```", "```py\n        inew = min(episode,epsilon_annealing_stop)\n        epsilon = (epsilon_start * (epsilon_annealing_stop - inew) \\\n                   + epsilon_end * inew) / epsilon_annealing_stop\n    ```", "```py\n        E = np.zeros((16, 4))\n    ```", "```py\n        state = env.reset()\n        action = action_epsilon_greedy(q, state, epsilon)\n        while True:\n    ```", "```py\n            E = eligibility_decay * gamma * E\n            E[state, action] += 1\n    ```", "```py\n            new_state, reward, done, info = env.step(action)\n    ```", "```py\n            new_action = action_epsilon_greedy(q, new_state, epsilon)\n    ```", "```py\n            delta = reward + gamma \\\n                    * q[new_state, new_action] - q[state, action]\n            q = q + alpha * delta * E \n    ```", "```py\n            state, action = new_state, new_action\n            if done:\n                break\n    ```", "```py\n        if episode%STEPS == 0:\n            performance[episode//STEPS] = average_performance\\\n                                          (get_action_epsilon_greedy\\\n                                          (epsilon), q=q)\n    ```", "```py\n    plt.plot(STEPS*np.arange(episodes//STEPS), performance)\n    plt.xlabel(\"Epochs\")\n    plt.title(\"Learning progress for SARSA\")\n    plt.ylabel(\"Average reward of an epoch\")\n    ```", "```py\n    Text(0, 0.5, 'Average reward of an epoch')\n    ```", "```py\n    greedyPolicyAvgPerf = average_performance(greedy_policy, q=q)\n    print(\"Greedy policy SARSA performance =\", greedyPolicyAvgPerf)\n    ```", "```py\n    Greedy policy SARSA performance = 0.734\n    ```", "```py\n    q = np.round(q,3)\n    print(\"(A,S) Value function =\", q.shape)\n    print(\"First row\")\n    print(q[0:4,:])\n    print(\"Second row\")\n    print(q[4:8,:])\n    print(\"Third row\")\n    print(q[8:12,:])\n    print(\"Fourth row\")\n    print(q[12:16,:])\n    ```", "```py\n    (A,S) Value function = (16, 4)\n    First row\n    [[0.795 0.781 0.79  0.786]\n     [0.426 0.386 0.319 0.793]\n     [0.511 0.535 0.541 0.795]\n     [0.341 0.416 0.393 0.796]]\n    Second row\n    [[0.794 0.515 0.541 0.519]\n     [0\\.    0\\.    0\\.    0\\.   ]\n     [0.321 0.211 0.469 0.125]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Third row\n    [[0.5   0.514 0.595 0.788]\n     [0.584 0.778 0.525 0.46 ]\n     [0.703 0.54  0.462 0.365]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Fourth row\n    [[0\\.    0\\.    0\\.    0\\.   ]\n     [0.563 0.557 0.862 0.508]\n     [0.823 0.94  0.878 0.863]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    ```", "```py\n    policyFound = [actionsDict[np.argmax(q[0,:])],\\\n                   actionsDict[np.argmax(q[1,:])],\\\n                   actionsDict[np.argmax(q[2,:])],\\\n                   actionsDict[np.argmax(q[3,:])],\\\n                   actionsDict[np.argmax(q[4,:])],\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[6,:])],\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[8,:])],\\\n                   actionsDict[np.argmax(q[9,:])],\\\n                   actionsDict[np.argmax(q[10,:])],\\\n                   \" - \",\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[13,:])],\\\n                   actionsDict[np.argmax(q[14,:])],\\\n                   \" ! \"]\n    print(\"Greedy policy found:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(policyFound[idx+0], policyFound[idx+1], \\\n              policyFound[idx+2], policyFound[idx+3])\n    print(\" \")\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Greedy policy found: \n        L    U    U    U\n        L    -    R    -\n        U    D    L    -\n        -    R    D    !\n    Optimal policy:  \n      L/R/D  U    U    U\n        L    -   L/R   -\n        U    D    L    -\n        -    R    D    !\n    ```", "```py\nGreedy policy found:\n    L    U    U    U\n    L    -    R    -\n    U    D    L    -\n    -    R    D    !\nOptimal policy:  \n  L/R/D  U    U    U\n    L    -   L/R   -\n    U    D    L    -\n    -    R    D    !\n```"]