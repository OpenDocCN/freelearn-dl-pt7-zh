- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Introduction to Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 1.01: Measuring the Performance of a Random Agent'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the required libraries â€“ `abc`, `numpy`, and `gym`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the abstract class representing the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: An agent is represented by only a constructor and an abstract method, `pi`.
    This method is the actual policy; it takes as input the environment state and
    returns the selected action.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a continuous agent. A continuous agent has to initialize the probability
    distribution according to the action space passed as an input to the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the upper and lower bounds are infinite, the probability distribution is
    simply a normal distribution centered at 0, with a scale that is equal to 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the upper and lower bounds are both finite, the distribution is a uniform
    distribution defined in that range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the lower bound is ![1](img/B16182_01_a.png), the probability distribution
    is a shifted negative exponential distribution:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the upper bound is ![2](img/B16182_01_b.png), the probability distribution
    is a shifted exponential distribution:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `pi` method, which is simply a call to the distribution defined
    in the constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are ready to define the discrete agent. As before, the agent has to correctly
    initialize the action distribution according to the action space that is passed
    as a parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now it is useful to define a utility function to create the correct agent type
    based on the action space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The last step is to define the RL loop in which the agent interacts with the
    environment and collects rewards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the parameters, and then create the environment and the agent:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have to track the returns for each episode; to do this, we can use a simple
    list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start a loop for each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the variables for the calculation of the cumulated discount factor
    and the current episode return:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reset the environment and get the first observation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop for the number of timesteps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Render the environment, select the action, and then apply it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Increment the return, and calculate the cumulated discount factor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the episode is terminated, break from the timestep''s loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the timestep loop, we have to record the current return by appending
    it to the list of returns for each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After the episode loop, close the environment and calculate `statistics`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this activity, we implemented two different types of agents: a discrete
    agent, working with discrete environments, and a continuous agent, working with
    continuous environments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, you can render the episodes inside a notebook using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You can see the episode duration is not too long. This is because the actions
    are taken at random, so the pole falls after some timesteps.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3fbxR3Y](https://packt.live/3fbxR3Y).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete and continuous agents are two different possibilities when facing a
    new RL problem.
  prefs: []
  type: TYPE_NORMAL
- en: We have designed our agents in a very flexible way so that they can be applied
    to almost all environments without having to change the code.
  prefs: []
  type: TYPE_NORMAL
- en: We also implemented a simple RL loop and measured the performance of our agent
    on a classical RL problem.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Markov Decision Processes and Bellman Equations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 2.01: Solving Gridworld'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `visualization` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the possible actions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `Policy` class, representing the random policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `Environment` class and the `step` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop for all states and actions and build the transition and reward matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the correctness of the matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the expected reward for each state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the function to visualize the expected reward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The function visualizes the matrix using Matplotlib. You should see something
    similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.62: The expected reward for each state'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_02_62.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.62: The expected reward for each state'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The previous figure is a color representation of the expected reward associated
    with each state considering the current policy. Notice that the expected reward
    of bad states is exactly equal to `-1`. The expected reward of good states is
    exactly equal to `10` and `5`, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now set up the matrix form of the Bellman expectation equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Solve the Bellman equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 2.63: State values of Gridworld'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_02_63.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.63: State values of Gridworld'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the value of good states is less than the expected reward from those
    states. This is because landing states have an expected reward that is negative
    or because landing states are close to states for which the reward is negative.
    You can see that the state with the higher value is state ![a](img/B16182_02_63a.png),
    followed by state ![b](img/B16182_02_63b.png). It is also interesting to note
    the high value of the state in position (`0, 2`), which is close to the good states.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Al9xOB](https://packt.live/2Al9xOB).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2UChxBy](https://packt.live/2UChxBy).
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we experimented with the Gridworld environment, one of the
    most common toy RL environments. We defined a random policy, and we solved the
    Bellman expectation equation using `scipy.linalg.solve` to find the state values
    of the policy.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to visualize the results, when possible, to get a better understanding
    and to spot any errors.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Deep Learning in Practice with TensorFlow 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 3.01: Classifying Fashion Clothes Using a TensorFlow Dataset and TensorFlow
    2'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import all the required modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the Fashion MNIST dataset using TensorFlow datasets and split it into
    train and test splits. Then, create a list of classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Explore the dataset to get familiar with the input features, that is, shapes,
    labels, and classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Visualize some instances of the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It is also useful to take a look at how the images will appear. The following
    code snippet shows the first training set instance:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output image will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.30: First training image plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_03_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.30: First training image plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform feature normalization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s take a look at some instances of our training set by plotting `25`
    of them with their corresponding labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output image will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.31: A set of 25 training samples and their corresponding labels'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_03_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.31: A set of 25 training samples and their corresponding labels'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build the classification model. First, create a model using a layers'' sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, associate the model with an `optimizer`, a `loss` function, and a `metrics`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the deep neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last output lines will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Test the model's accuracy. The accuracy should be in excess of 88%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Evaluate the model on the test set and print the accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The accuracy may show slightly different values due to random sampling with
    a variable random seed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Perform inference and check the predictions against the ground truth.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As a first step, add a `softmax` layer to the model so that it outputs probabilities
    instead of logits. Then, print out the probabilities of the first test instance
    with the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, compare one model prediction (that is, the class with the highest predicted
    probability), the one on the first test instance, with its ground truth:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to perform a comparison that''s even clearer, create the following
    two functions. The first one plots the `i`-th test set instance image with a caption
    showing the predicted class with the highest probability, its probability in percent,
    and the ground truth between round brackets. This caption will be `blue` for correct
    predictions, and `red` for incorrect ones:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The second function creates a second image showing a bar plot of all classes''
    predicted probabilities. It will color the highest probable one in `blue` if the
    prediction is correct, or in `red` if it is incorrect. In this second case, the
    bar corresponding to the correct label is colored in `blue`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using these two functions, we can examine every instance of the test set. In
    the following snippet, the first test instance is being plotted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.32: First test instance, correctly predicted'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_03_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.32: First test instance, correctly predicted'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The very same approach can be used to plot a user-defined number of test instances,
    arranging the output in subplots, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.33: First 25 test instances with their predicted classes and ground
    truth comparison'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_03_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.33: First 25 test instances with their predicted classes and ground
    truth comparison'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3dXv3am](https://packt.live/3dXv3am).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Ux5JR5](https://packt.live/2Ux5JR5).
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we faced a problem that is quite similar to a real-world one.
    We had to deal with complex high dimensional inputs â€“ in our case, grayscale images
    â€“ and we wanted to build a model capable of autonomously grouping them into 10
    different categories. Thanks to the power of deep learning and state-of-the-art
    machine learning frameworks, we were able to build a fully connected neural network
    that achieves a classification accuracy in excess of 88%.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Getting started with OpenAI and TensorFlow for Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 4.01: Training a Reinforcement Learning Agent to Play a Classic Video
    Game'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import all the required modules from OpenAI Baselines and TensorFlow in order
    to use the `PPO` algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define and register a custom convolutional neural network for the policy network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function to build the environment in the format required by OpenAI
    Baselines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the `PongNoFrameskip-v4` environment, choose the required policy network
    parameters, and train it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While training, the model produces an output similar to the following (only
    a few lines have been reported here):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the trained agent in the environment and print the cumulative reward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following lines show the last part of the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It also renders the environment, showing what happens in the environment in
    real time:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14: One frame of the real-time environment, after rendering'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_04_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.14: One frame of the real-time environment, after rendering'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the built-in OpenAI Baselines run script to train PPO on the `PongNoFrameskip-v0`
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last few lines of the output will be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the built-in OpenAI Baselines run script to run the trained model on the
    `PongNoFrameskip-v0` environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the pretrained weights provided to see the trained agent in action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can read the `.tar` file by using the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the built-in OpenAI Baselines run script to train PPO on `PongNoFrameskip-v0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/30yFmOi](https://packt.live/30yFmOi).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this activity, we learned how to train a state-of-the-art reinforcement learning
    agent that, by only looking at screen pixels, is able to achieve better-than-human
    performance when playing a classic Atari video game. We made use of a convolutional
    neural network to encode environment observations and leveraged the state-of-the-art
    OpenAI tool to successfully train a PPO algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Dynamic Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 5.01: Implementing Policy and Value Iteration on the FrozenLake-v0
    Environment'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the environment and reset the current one. Set `is_slippery=False`
    in the initializer. Show the size of the action space and the number of possible
    states:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform policy evaluation iterations until the smallest change is less than
    `smallest_change`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take the action according to the current policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the Bellman optimality equation to update ![6](img/B16182_05_26a.png):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform policy improvement using the Bellman optimality equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the future reward by taking this action. Note that we are using the
    simplified equation because we don''t have non-one transition probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using `assert` statements, we can avoid getting into unwanted situations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the best action for this current state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the most optimal policy for the FrozenLake-v0 environment using policy
    iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform a test pass on the FrozenLake-v0 environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the maximum number of steps the agent is allowed to take. If it doesn''t
    reach a solution in this time, then we call it an episode and proceed ahead:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take the action that has the highest Q value in the current state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Step through the `FrozenLake-v0` environment randomly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform value iteration to find the most optimal policy for the FrozenLake-v0
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the value table randomly and initialize the policy randomly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the reward obtained if we were to perform this action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the code and make sure the output matches the expectation by running it
    in the `main` block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running this, you should be able to see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.27: FrozenLake-v0 environment output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_05_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.27: FrozenLake-v0 environment output'
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen from the output, we have successfully achieved the goal of retrieving
    the frisbee.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3fxtZuq](https://packt.live/3fxtZuq).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2ChI1Ss](https://packt.live/2ChI1Ss).
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Monte Carlo Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 6.01: Exploring the Frozen Lake Problem â€“ the Reward Function'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select the environment as `FrozenLake`. `is_slippery` is set to `False`. The
    environment is reset with the line `env.reset()` and rendered with the line `env.render()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.15: Frozen Lake state rendered'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_06_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.15: Frozen Lake state rendered'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is a text grid with the letters `S`, `F`, `G`, and `H` used to represent
    the current environment of `FrozenLake`. The highlighted cell `S` is the current
    state of the agent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the possible values in the observation space and the number of action
    values using the `print(env.observation_space)` and `print(env.action_space)`
    functions respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`16` is the number of cells in the grid, so `print(env.observation_space)`
    prints `16`. `4` is the number of possible actions, so `print(env.action_space)`
    prints `4`. `Discrete` shows the observation space and action space take only
    discrete values and do not take continuous values.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to define a function to generate a frozen lake episode. We
    initialize `episodes` and the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Navigate step by step and store `episode` and return `reward`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The action is obtained with `env.action_space.sample()`. `next_state`, `action`,
    and `reward` are obtained by calling the `env_step(action)` function. They are
    then appended to an episode. The `episode` is now a list of states, actions, and
    rewards.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The key is now to calculate the success rate, which is the likelihood of success
    for a batch of episodes. The way we do this is by calculating the total number
    of attempts in a batch of episodes. We calculate how many of them successfully
    reached the goal. The ratio of the agent successfully reaching the goal to the
    number of attempts made by the agent is the success ratio.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'First, we initialize the total reward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate the episode and reward for every iteration and calculate the total
    reward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The success ratio is calculated by dividing `total_reward` by `100` and is
    printed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The frozen lake prediction is calculated using the `frozen_lake_prediction`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.16: Output of Frozen Lake without learning'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_06_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.16: Output of Frozen Lake without learning'
  prefs: []
  type: TYPE_NORMAL
- en: The output prints the policy win ratio for the various episodes in batches of
    100\. The ratios are quite low as this is the simulation of an agent following
    a random policy. We will see in the next exercise how this can be improved by
    learning to a higher level by using a combination of a greedy policy and an epsilon
    soft policy.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Akh8Nm](https://packt.live/2Akh8Nm).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2zruU07](https://packt.live/2zruU07).
  prefs: []
  type: TYPE_NORMAL
- en: Activity 6.02 Solving Frozen Lake Using Monte Carlo Control Every Visit Epsilon
    Soft
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select the environment as `FrozenLake`. `is_slippery` is set to `False`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the `Q` value and `num_state_action` to zeros:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the value of `num_episodes` to `100000` and create `rewardsList`. We set
    `epsilon` to `0.30`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Setting epsilon to `0.30` means we will explore with a likelihood of 0.30 and
    be greedy with a likelihood of 1-0.30 or 0.70.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the loop till `num_episodes`. We initialize the environment, `results_List`,
    and `result_sum` to zero. Also, reset the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start a `while` loop, and check whether you need to pick a random action with
    a probability epsilon or greedy policy with a probability of 1-epsilon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now step through the `action` and get `new_state` and `reward`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result list is appended with the `state` and `action` pair. `result_sum`
    is incremented by the value of the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`new_state` is assigned to `state` and `result_sum` is appended to `rewardsList`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate `Q[s,a]` using the incremental method, as `Q[s,a] + (result_sum â€“
    Q[s,a]) / N(s,a)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the value of the success rates in batches of `1000`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the final success rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following output initially:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.17: Initial output of the Frozen Lake success rate'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_06_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.17: Initial output of the Frozen Lake success rate'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will get the following output finally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18: Final output of the Frozen Lake success rate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_06_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.18: Final output of the Frozen Lake success rate'
  prefs: []
  type: TYPE_NORMAL
- en: The success rate starts with a very low value close to 0% but with reinforcement
    learning, it learns, and the success rate increases incrementally going up to
    60%.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Ync9Dq](https://packt.live/2Ync9Dq).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3cUJLxQ](https://packt.live/3cUJLxQ).
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Temporal Difference Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 7.01: Using TD(0) Q-Learning to Solve FrozenLake-v0 Stochastic Transitions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the required modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `gym` environment called `FrozenLake-v0` using the `is_slippery`
    flag set to `True` in order to enable stochasticity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take a look at the action and observation spaces:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will print out the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create two dictionaries to easily translate the `actions` numbers into moves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reset the environment and render it to take a look at the grid problem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Its initial state is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.39: Environment''s initial state'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_07_39.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.39: Environment''s initial state'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the optimal policy for this environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the functions that will take Îµ-greedy actions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will take greedy actions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will calculate the agent''s average performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the Q-table so that all the values are equal to `1`, except for
    the values at the terminal states:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the number of total episodes, the number of steps representing the interval
    by which we''re evaluating the agent''s average performance, the learning rate,
    the discounting factor, the `Îµ` value for the exploration policy, and an array
    to collect all the agent''s performance evaluations during training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the Q-learning algorithm. Loop among all episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reset the environment and start the in-episode loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select the exploration action with an Îµ-greedy policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Step the environment with the selected exploration action and retrieval of
    the new state, reward, and done conditions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select a new action with the greedy policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the Q-table with the Q-learning TD(0) rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the state with a new value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the agent''s average performance for every step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the Q-learning agent''s mean reward history during training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This generates the following output, showing the learning progress for the
    Q-learning algorithm:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot for this can be visualized as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.40: Average reward of an epoch trend over training epochs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_07_40.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.40: Average reward of an epoch trend over training epochs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this case, as in the case of Q-learning applied to the deterministic environment,
    the plot shows how quickly Q-learning performance grows over epochs as the agent
    collects more and more experience. It also demonstrates that the algorithm is
    not capable of reaching 100% success after learning due to the limitations of
    stochasticity. When compared with using the SARSA method on a stochastic environment,
    as seen in *Figure 7.15*, the algorithm's performance grows faster and more steadily.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Evaluate the greedy policy''s performance for the trained agent (Q-table):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints out the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the Q-table values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print out the greedy policy that was found and compare it with the optimal
    policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This output shows that, as for all the exercises in this chapter, the off-policy,
    one-step Q-learning algorithm is able to find the optimal policy by simply exploring
    the environment, even in the context of stochastic environment transitions. As
    anticipated, for this setting, it is not possible to achieve the maximum reward
    100% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, for every state of the grid world that the greedy policy obtained
    with the Q-table that was calculated by our algorithm, it prescribes an action
    that is in accordance with the optimal policy that was defined by analyzing the
    environment problem. As we already saw, there are two states in which many different
    actions are equally optimal, and the agent correctly implements one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3elMxxu](https://packt.live/3elMxxu).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/37HSDWx](https://packt.live/37HSDWx).
  prefs: []
  type: TYPE_NORMAL
- en: 8\. The Multi-Armed Bandit Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 8.01: Queueing Bandits'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the necessary libraries and tools, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Declare the bandit object, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `N_CLASSES` variable will be used by our subsequent code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Implement the Greedy algorithm, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we are taking care to avoid choosing a class that does not have
    any customers left in it by checking if `queue_lengths[class_]` is greater than
    0 or not. The remaining code is analogous to what we had in our earlier discussion
    of Greedy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Subsequently, apply the algorithm to the bandit object, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate the following graph:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.24: Distribution of cumulative waiting time from Greedy'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_08_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: While these values might appear large compared to our earlier discussions, this
    is because the reward/cost distributions we are working with here take on higher
    values. We will use these values from Greedy as a frame of reference to analyze
    the performance of later algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Implement the Explore-then-commit algorithm using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the algorithm to the bandit object, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will produce the following graph:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.25: Distribution of cumulative waiting time from Explore-then-commit'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_08_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.25: Distribution of cumulative waiting time from Explore-then-commit'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This will also produce the max and average cumulative waiting times: `(1238591.3208636027,
    45909.77140562623)`. Compared to Greedy `(1218887.7924350922, 45155.236786598274)`,
    Explore-then-commit did relatively worse on this queueing bandit problem.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Implement Thompson Sampling, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Recall that in our initial discussion of Thompson Sampling, we draw random samples
    to estimate the reward expectation for each arm. Here, we drew random samples
    from the corresponding Gamma distributions (which are being used to model service
    rates) to estimate the rates (or the inverse job lengths) and choose the largest
    drawn sample.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This can be applied to solve the bandit problem using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following plot will be produced:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.26: Distribution of cumulative waiting time from Thompson Sampling'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_08_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.26: Distribution of cumulative waiting time from Thompson Sampling'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the max and mean waiting time `(1218887.7924350922, 45129.343871806814)`,
    we can see that Thompson Sampling is able to improve on Greedy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The modified version of Thompson Sampling can be implemented as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The initialization method of this class implementation has an additional attribute,
    `r`, which we will use to implement the exploitation logic.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the `decide()` method, right before we draw samples to estimate the rates,
    we check to see if the current time (`t`) is greater than the current queue length
    (the sum of `queue_lengths`). This Boolean indicates whether we have processed
    more than half of the customers or not. If so, we simply implement the logic of
    the Greedy algorithm and return the arm with the optimal average rate. Otherwise,
    we have our actual Thompson Sampling logic.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The `update()` method should be the same as the actual Thompson Sampling algorithm
    from the previous step, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, apply the algorithm to the bandit problem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will obtain the following graph:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.27: Distribution of cumulative waiting time from modified Thompson
    Sampling'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_08_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 8.27: Distribution of cumulative waiting time from modified Thompson
    Sampling'
  prefs: []
  type: TYPE_NORMAL
- en: Together with the max and mean waiting time `(1218887.7924350922, 45093.244027644556)`,
    we can see that this modified version of Thompson Sampling is more effective than
    the original at minimizing the cumulative waiting time across the experiments.
  prefs: []
  type: TYPE_NORMAL
- en: This speaks to the potential benefit of designing algorithms that are tailored
    to the contextual bandit problem that they are trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Yuw2IQ](https://packt.live/2Yuw2IQ).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3hnK5Z5](https://packt.live/3hnK5Z5).
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this activity, we have learned how to apply the approaches discussed
    in this chapter to a queueing bandit problem, that is, exploring an example of
    a potential contextual bandit process. Most notably, we have considered a variant
    of Thompson Sampling that has been modified to fit the context of the queueing
    problem, thus successfully lowering our cumulative regret compared to other algorithms.
    This activity also marks the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. What Is Deep Q-Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 9.01: Implementing a Double Deep Q Network in PyTorch for the CartPole
    Environment'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Open a new Jupyter notebook and import all of the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write code that will create a device based on the availability of a GPU environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `gym` environment using the `''CartPole-v0''` environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the `seed` for torch and the environment for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fetch the number of states and actions from the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set all of the hyperparameter values required for the DDQN process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the `calculate_epsilon` function, as described in the previous exercises:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a class, called `DQN`, that accepts the number of states as inputs and
    outputs Q values for the number of actions present in the environment, with the
    network that has a hidden layer of size `64`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the `ExperienceReplay` class, as described in the previous exercises:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `ExperienceReplay` class by passing the buffer size as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the DQN agent class with the changes discussed for the `optimize`
    function (from the code example given in the *Double Deep Q Network (DDQN)* section):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write the training process loop with the help of the following steps. First,
    instantiate the DQN agent using the class created earlier. Create a `steps_total`
    empty list to collect the total number of steps for each episode. Initialize `steps_counter`
    with zero and use it to calculate the decayed epsilon value for each step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Use two loops during the training process; the first one is to play the game
    for a certain number of steps. The second loop ensures that each episode goes
    on for a fixed number of steps. Inside the second `for` loop, the first step is
    to calculate the epsilon value for the current step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using the present state and epsilon value, you can select the action to perform.
    The next step is to take the action. Once you take the action, the environment
    returns the `new_state`, `reward`, and `done` flags.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using the `optimize` function, perform one step of gradient descent to optimize
    the DQN. Now make the new state the present state for the next iteration. Finally,
    check whether the episode is over. If the episode is over, then you can collect
    and record the reward for the current episode:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now observe the reward. As the reward is scalar feedback and gives an indication
    of how well the agent is performing, you should look at the average reward and
    the average reward for the last 100 episodes. Also, perform the graphical representation
    of rewards. Check how the agent is performing while playing more episodes and
    what the reward average is for the last 100 episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the rewards collected in the y axis and the number of episodes in the
    x axis to visualize how the rewards have been collected with the increasing number
    of episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.37: Plot for the rewards collected by the agent'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_09_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 9.37: Plot for the rewards collected by the agent'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3hnLDTd](https://packt.live/3hnLDTd).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/37ol5MK](https://packt.live/37ol5MK).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a comparison between different DQN techniques and DDQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vanilla DQN Outputs:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: '**DQN with Experience Replay and Target Network Outputs:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: '**DDQN Outputs:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding figure, along with the comparison of the results
    shown earlier, DDQN has the highest average reward, compared to other DQN implementations,
    and the average reward for the last 100 episodes is also higher. We can say that
    DDQN improves performance significantly in comparison to the other two DQN techniques.
    After completing this whole activity, we have learned how to combine a DDQN network
    with experience replay to overcome the issues of a vanilla DQN and achieve more
    stable rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Playing an Atari Game with Deep Recurrent Q-Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 10.01: Training a DQN with CNNs to Play Breakout'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Jupyter Notebook and import the relevant packages: `gym`, `random`,
    `tensorflow`, `numpy`, and `collections`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the seed for NumPy and TensorFlow to `168`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `DQN` class with the following methods: the `build_model()` method
    to instantiate a CNN, the `get_action()` method to apply the epsilon-greedy algorithm
    to choose the action to be played, the `add_experience()` method to store in memory
    the experience acquired by playing the game, the `replay()` method, which will
    perform experience replay by sampling experiences from the memory and train the
    DQN model with a callback to save the model every two episodes, and the `update_epsilon()`
    method to gradually decrease the epsilon value for epsilon-greedy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `initialize_env()` function, which will initialize the Breakout
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `preprocess_state()` function to preprocess the input images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `play_game()` function, which will play an entire game of Breakout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `train_agent()` function, which will iterate through a number of
    episodes where the agent will play a game and perform experience replay:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a Breakout environment called `env` with the `gym.make()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create two variables, `IMG_SIZE` and `SEQUENCE`, that will take the values
    `84` and `4`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `DQN` object called `agent`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `episodes` that will take the value `50`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `train_agent` function by providing `env`, `episodes`, and `agent`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output of the code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3hoZXdV](https://packt.live/3hoZXdV).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3dWLwfa](https://packt.live/3dWLwfa).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You just completed the first activity of this chapter. You successfully built
    and trained a DQN agent combined with CNNs to play the game Breakout. The performance
    of this model is very similar to the random agent (average score of 0.6). However,
    if you train it for longer (by increasing the number of episodes), it may achieve
    a better score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10.02: Training a DRQN to Play Breakout'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Jupyter Notebook and import the relevant packages: `gym`, `random`,
    `tensorflow`, `numpy`, and `collections`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the seed for NumPy and TensorFlow to `168`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `DRQN` class with the following methods: the `build_model()` method
    to instantiate a CNN combined with a RNN model, the `get_action()` method to apply
    the epsilon-greedy algorithm to choose the action to be played, the `add_experience()`
    method to store in memory the experience acquired by playing the game, the `replay()`
    method, which will perform experience replay by sampling experiences from the
    memory and train the DRQN model with a callback to save the model every two episodes,
    and the `update_epsilon()` method to gradually decrease the epsilon value for
    epsilon-greedy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `initialize_env()` function, which will initialize the Breakout
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `preprocess_state()` function to preprocess the input images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `combine_images()` function to stack the previous four screenshots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `play_game()` function, which will play an entire game of Breakout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `train_agent()` function, which will iterate through a number of
    episodes where the agent will play a game and perform experience replay:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a Breakout environment called `env` with `gym.make()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create two variables, `IMG_SIZE` and `SEQUENCE`, that will take the values
    `84` and `4`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `DRQN` object called `agent`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `episodes` that will take the value `200`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `train_agent` function by providing `env`, `episodes`, and `agent`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output of the code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2AjdgMx](https://packt.live/2AjdgMx).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/37mhlLM](https://packt.live/37mhlLM).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this activity, we added an LSTM layer and built a DRQN agent. It learned
    how to play the Breakout game, but didn't achieve satisfactory results even after
    200 episodes. It seems this is still at the exploratory stage. You may try to
    train it for more episodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10.03: Training a DARQN to Play Breakout'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Jupyter Notebook and import the relevant packages: `gym`, `random`,
    `tensorflow`, `numpy`, and `collections`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the seed for NumPy and TensorFlow to `168`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `DARQN` class and create the following methods: the `build_model()`
    method to instantiate a CNN combined with an RNN model, the `get_action()` method
    to apply the epsilon-greedy algorithm to choose the action to be played, the `add_experience()`
    method to store in memory the experience acquired by playing the game, the `replay()`
    method, which will perform experience replay by sampling experiences from the
    memory and train the DARQN model with a callback to save the model every two episodes,
    and the `update_epsilon()` method to gradually decrease the epsilon value for
    epsilon-greedy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `initialize_env()` function, which will initialize the Breakout
    environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `preprocess_state()` function to preprocess the input images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `combine_images()` function to stack the previous four screenshots:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `preprocess_state()` function to preprocess the input images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `train_agent()` function, which will iterate through a number of
    episodes where the agent will play a game and perform experience replay:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a Breakout environment called `env` with `gym.make()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create two variables, `IMG_SIZE` and `SEQUENCE`, that will take the values
    `84` and `4`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `DRQN` object called `agent`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `episodes` that will take the value `400`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `train_agent` function by providing `env`, `episodes`, and `agent`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output of the code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this activity, we built and trained a `DARQN` agent. It successfully learned
    how to play the Breakout game. It started with a score of `1.0` and achieved a
    final score of over `10` after `400` episodes, as shown in the preceding results.
    This is quite remarkable performance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2XUDZrH](https://packt.live/2XUDZrH).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2UDCsUP](https://packt.live/2UDCsUP).
  prefs: []
  type: TYPE_NORMAL
- en: 11\. Policy-Based Methods for Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 11.01: Creating an Agent That Learns a Model Using DDPG'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the necessary libraries (`os`, `gym`, and `ddpg`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, we create our Gym environment (`LunarLanderContinuous-v2`), as we did
    previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the agent with some sensible hyperparameters, as in *Exercise 11.02*,
    *Creating a Learning Agent*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Set up a random seed so that our experiments are reproducible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a blank array to story the scores; you can name it `history`. Iterate
    for at least `1000` episodes and in each episode, set a running score variable
    to `0` and the `done` flag to `False`, then reset the environment. Then, when
    the `done` flag is not `True`, carry out the following step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select the observations and get the new `state`, `reward`, and `done` flags.
    Save the `observation`, `action`, `reward`, `state_new`, and `done` flags. Call
    the `learn` function of the agent and add the current reward to the running score.
    Set the new state as the observation and finally, when the done flag is `True`,
    append `score` to `history`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can print out `score` and mean `score_history` results to see how the agent
    is learning over time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To observe the rewards, we can simply add the `print` statement. The rewards
    will be similar to those in the previous exercise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Run the code for at least 1,000 iterations and watch your lander attempt to
    land on the lunar surface.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To see the Lunar Lander simulation once the policy is learned, we just need
    to uncomment the `env.render()` code from the preceding code block. As seen in
    the previous exercise, this will open another window, where we will be able to
    see the game simulation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here''s a glimpse of how your lunar lander might behave once it has learned
    the policy:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.16: Screenshots from the environment after 1,000 rounds of training'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_11_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 11.16: Screenshots from the environment after 1,000 rounds of training'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/30X03Ul](https://packt.live/30X03Ul).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 11.02: Loading the Saved Policy to Run the Lunar Lander Simulation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the essential Python libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set your device using the `device` parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `ReplayBuffer` class, as we did in the previous exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `ActorCritic` class, as we did in the previous exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `Agent` class, as we did in the previous exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the Lunar Lander environment. Initialize the random seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE225]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the memory buffer and initialize the agent with hyperparameters, as
    in the previous exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the saved policy as an old policy from the `Exercise11.03` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, loop through your desired number of episodes. In every iteration,
    start by initializing the episode reward as `0`. Do not forget to reset the state.
    Run another loop, specifying the `max` timestamp. Get the `state`, `reward`, and
    `done` flags for each action taken and add the reward to the episode reward. Render
    the environment to see how your Lunar Lander is doing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the output of the code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE229]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You''ll see the reward oscillate in the positive zone as our Lunar Lander now
    has some idea of what a good policy can be. The reward may oscillate as there
    is more scope for learning. You might iterate over a few thousand more iterations
    to make your agent learn a better policy. Do not hesitate to tinker with the parameters
    specified in the code. The following screenshot shows the simulation output of
    some of the stages:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.17: The environment showing the simulation of the Lunar Lander'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_11_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 11.17: The environment showing the simulation of the Lunar Lander'
  prefs: []
  type: TYPE_NORMAL
- en: Before this activity, we explained some necessary concepts, such as creating
    a learning agent, training a policy, saving and loading the learned policies,
    and so on, in isolation. Through carrying out this activity, you learned how to
    build a complete RL project or a working prototype on your own by combining all
    that you have learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The complete simulation output can be found in the form of images at [https://packt.live/3ehPaAj](https://packt.live/3ehPaAj).
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2YhzrvD](https://packt.live/2YhzrvD).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  prefs: []
  type: TYPE_NORMAL
- en: 12\. Evolutionary Strategies for RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 12.01: Cart-Pole Activity'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the required packages as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE230]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the environment and the state and action space shapes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function to generate randomly selected initial network parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE232]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function to generate the neural network using the set of parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE233]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function to get the total reward for `300` steps when using the neural
    network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE234]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function to get the fitness scores for each element of the population
    when running the initial random selection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a mutation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a single-point crossover function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE237]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function for creating the next generation by selecting the pair with
    the highest rewards:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the current parameters for the weights and bias using a `for` loop to go
    through the indices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE239]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the neural network using the identified parameters and obtain a new reward
    based on the constructed neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE240]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function to output the convergence graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE241]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function for the genetic algorithm that outputs the parameters of
    the neural network based on the highest average reward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE242]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function that decodes the array of parameters to each neural network
    parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE243]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the generations to `50`, the number of trial tests to `15`, and the number
    of steps and trials to `500`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE244]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output (just the first few lines are shown here) will be similar to the
    following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE245]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output can be visualized in a plot as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 12.15: Rewards obtained over the generations'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_12_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 12.15: Rewards obtained over the generations'
  prefs: []
  type: TYPE_NORMAL
- en: 'The average of the rewards output (just the last few lines are shown here)
    will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: You will notice that depending on the start state, the convergence of the GA
    algorithm to the highest score will vary; also, the neural network model will
    not always achieve the optimal solution. The purpose of this activity was for
    you to implement the genetic algorithm techniques studied in this chapter and
    to see how you can combine evolutionary methods of neural network parameter tuning
    for action selection.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2AmKR8m](https://packt.live/2AmKR8m).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  prefs: []
  type: TYPE_NORMAL
