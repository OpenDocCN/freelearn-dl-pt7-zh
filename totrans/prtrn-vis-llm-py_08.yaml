- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large-Scale Training on SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover the key features and functionality available
    with Amazon SageMaker to run highly optimized distributed training. You’ll learn
    how to optimize your script for SageMaker training, along with key usability features.
    You’ll also learn about backend optimizations for distributed training with SageMaker,
    such as GPU health checks, resilient training, checkpointing, and script mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing your script for SageMaker training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Top usability features for SageMaker training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing your script for SageMaker training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, you have learned quite a lot! We have covered everything
    from the foundations of pretraining to GPU optimization, picking the right use
    case, dataset and model preparation, parallelization basics, finding the right
    hyperparameters, and so on. The vast majority of this is that these are applicable
    in any compute environment you choose to apply them to. This chapter, however,
    is exclusively scoped to AWS and SageMaker especially. Why? So that you can master
    all the nuances included in at least one compute platform. Once you have learned
    how to become proficient in one compute platform, then you will be able to use
    that to work on any project you like! When, for various reasons, you need to transition
    onto another platform, you will at least have the basic concepts you need to know
    about to look for and consider the transition.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us look at your scripts. The core of most SageMaker training scripts
    has at least three things:'
  prefs: []
  type: TYPE_NORMAL
- en: Package imports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Argument parsing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Function definitions and usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s break these down next.
  prefs: []
  type: TYPE_NORMAL
- en: Importing packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’ve covered previously, you can install and access really any package you
    need. You have many different ways to make these accessible within SageMaker training.
    At a minimum, when you define your job, you can bring a `requirements.txt` file
    with the packages defined. SageMaker will then use `pip install` to install these
    on your training compute for you, making them available.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can build a base container with all of these pre-installed.
    This is certainly the fastest option, since it saves time during the training
    process. Rather than using *pip install*, you can use a pre-built image with all
    of the packages available. Another option is to import your own Python packages,
    sending your entire project to the SageMaker training environment. Then, you can
    import whatever code you’re working on.
  prefs: []
  type: TYPE_NORMAL
- en: Argument parsing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An extremely common package we use in the SageMaker training environment is
    `argparse`. If you’re not familiar with this, let me introduce you to it.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve built a Python script, you might need to run it with different flags,
    settings, or arguments. Some of these might be with different hyperparameters,
    modes, or features that you want your script to run for you. The `argparse` package
    is a great way to do this in Python. First, in your script, you’ll need to *explicitly
    add a line of code for each argument you want to use*. In SageMaker, you might
    start with something like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – A basic arg parsing function](img/B18942_Figure_8.01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – A basic arg parsing function
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 8**.1*, I’m simply importing `argparse`, creating
    the `parser` object, and then adding an argument called `train_folder`. This will
    default to looking up *my environment variable*, which, as you may remember, is
    how SageMaker training injects information into your environment. If you’re curious,
    you can step through the CloudWatch logs for any of your SageMaker training jobs
    to see a list of all of the available environment variables. These will include
    all of the metadata for your job, all of your hyperparameters, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this brief example, I’m pointing to my *train channel*. I created this by
    pointing to S3, or an optional FSx for Lustre, when I created my training job.
    That’s my training data. First, I upload it to S3\. Then, I point to it when I
    configure my job. SageMaker copies that onto my SageMaker training instance and
    loads it onto a local path. The local path is usually something like `/opt/ml/input/data/train/`.
    When you want to point to that local path on your training container, you call
    `args.train_folder`, or however you’ve defined it. To read the file, you can either
    list the name from the folder or pass the name as another argument.
  prefs: []
  type: TYPE_NORMAL
- en: My personal favorite way to keep my script clean and tidy is to wrap all of
    my `arg` parsing in a dedicated function. Then, this will neatly return the `args`
    object. Here’s the full script.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Invoking the arg parsing function in your main script](img/B18942_Figure_8.02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Invoking the arg parsing function in your main script
  prefs: []
  type: TYPE_NORMAL
- en: Another common argument you might pass is `model_dir`. You can point this to
    the `SM_MODEL_DIR` SM environment variable. SageMaker will write your model from
    the training container to S3 after the job is finished.
  prefs: []
  type: TYPE_NORMAL
- en: You can add any other hyperparameter you want, using the `hyperparameters` argument
    in the job config. Then, you can use these in your scripts. I’ve built arguments
    to point to things such as my data index, how to run my scripts, a path to checkpoint
    my model, and countless other arguments you may need for your project.
  prefs: []
  type: TYPE_NORMAL
- en: Functions definition and usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the risk of stating the obvious here, you can write whatever software you
    want to write. You can copy directly to and from any accessible data source, spawn
    other jobs, initiate other cloud resources, or use open source packages – the
    possibilities are endless.
  prefs: []
  type: TYPE_NORMAL
- en: Invoke your script with mpi
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you’re using distributed training, SageMaker invokes your script with `mpi`.
    As you learned earlier, this is a core library useful to run distributed training.
    We’ll use `mpirun` or `smddprun` to invoke your script. As you can see, we’ll
    invoke your script with all of the relevant parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – How SageMaker training invokes your script](img/B18942_Figure_8.03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – How SageMaker training invokes your script
  prefs: []
  type: TYPE_NORMAL
- en: This is from a very complex example, training tens of billions of parameters
    for GPT-2, but it shows you many of the available ways you can configure your
    distributed training cluster on SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Logging and CloudWatch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As you may be aware, you have many options for logging. print statements are
    a great way to debug, but as you grow, you may move to something more managed
    such as the `logging` package. Remember that all of these are sent to CloudWatch
    logs, so you can easily view and debug your scripts. Open up the training job
    view in the AWS console, scroll to the bottom, and click the *View logs*. This
    takes you to CloudWatch, giving you one log stream per node in your cluster, each
    called `algo`. Usually, the top log stream is the leader node, but all of the
    streams will try to connect to the leader, so just see which algo they are trying
    to connect to. The logs will start after your instances are online and the script
    has been invoked, so it may take a few minutes after the job starts to see these.
  prefs: []
  type: TYPE_NORMAL
- en: Checkpointing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One last parameter to be aware of in your SageMaker training scripts is **checkpointing**.
    In SageMaker, this actually serves a different role than just the model path.
    The model path will be copied to S3 at the end of your training job, but your
    checkpoints *will be copied throughout*. This makes them a great candidate for
    in-job debugging, running TensorBoard *(2)*, and restarting from the latest checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a restart from your checkpoint is an extremely efficient technique
    to learn and perfect. It’s not hard – just look in S3 for the right path, configure
    your job, and then make sure you’re looking in the right directory for your base
    model. For large-scale jobs, we recommend you checkpoint at least every 2–3 hours.
    This makes it easy for you to get through any hardware, software, networking,
    data, or other issues that will almost certainly arise throughout your training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: For a detailed example of this, take a look at our GPT-2 training example at
    *(3)* in the *References* section. It implements a `load_partial` parameter that
    points to the S3 path you can provide for checkpointing.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring your job via the SageMaker estimator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While you do have multiple ways of running your SageMaker job, notably through
    the UI, the CLI, and `boto3`, probably the most popular way of doing this is through
    the Python SDK.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of what this might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Using the SageMaker estimator to run your remote training job](img/B18942_Figure_8.04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Using the SageMaker estimator to run your remote training job
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we’re pointing to a base image, actually through the `PyTorch` object.
    This points to a base AWS Deep Learning Container, defined by what framework version
    you specify. You can override this by pointing to `image_uri`, which will need
    to be a Docker container in Amazon ECS. In this estimator, you can also pass key
    parameters such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`instance_count` and `instance_type` to configure your training resource.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your entry point script and its source directory. This is where SageMaker will
    look for `requirements.txt` and your main script to execute both of the files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your hyperparameters – again, you define them based on what you need.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your distribution parameters. We’ll cover them in the last section of this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s take a look at some interesting usability features for SageMaker
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Top usability features for SageMaker training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have some sense of how to integrate your scripts with SageMaker
    training, let’s learn about a few key aspects of SageMaker that make it especially
    easy and fun to work with.
  prefs: []
  type: TYPE_NORMAL
- en: Warm pools for rapid experimentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once your SageMaker job is online, it moves through the following phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializing resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading your data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading your training image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invoking your main script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uploading the model artifact to S3 on completion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might be wondering, what happens if my job breaks and I need to update a
    few lines of code? Do I need to completely restart the entire cluster from scratch?
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately for you, the answer is no! Definitely not. You can use managed warm
    pools. Just add one extra hyperparameter, `keep_alive_period_in_seconds`, and
    it’ll keep your job online even after your script either fails or finishes completely.
    This is useful because, in many cases, that upfront job initialization actually
    is the largest bottleneck in your flow. It can take anywhere from a few minutes
    for smaller CPU-based instances to as much as 8 minutes or more for larger GPU-based
    instances to initialize.
  prefs: []
  type: TYPE_NORMAL
- en: On the upside, that wait time for GPU instances is ultimately saving you money
    and time because we’re running GPU health checks on the backend to ensure you
    get only good news. On the downside, 8 minutes is a long time to wait between
    development iterations. This is particularly painful if you’re updating something
    embarrassingly simple, such as a basic syntax error.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Viewing your training jobs in the console](img/B18942_Figure_8.05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Viewing your training jobs in the console
  prefs: []
  type: TYPE_NORMAL
- en: 'Managed warm pools solve this problem for you as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, add that hyperparameter to your job configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, once the job finishes training, either successfully or with an error,
    the warm pool status should show **Available**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Afterward, when you submit another job with a matching image URI, instance type,
    and instance count, this will show the **In Use** status and then ultimately **Reused**,
    as shown in *Figure 8**.5*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While saving a few minutes through using managed warm pools may not seem like
    a huge gain, it truly adds up a scale. When you’re working up against a deadline
    and every hour of the day counts, using warm pools may be the difference between
    hitting your deadline and not. It means that in a single hour, you can update
    your script easily hundreds of times, whereas before you may only have been able
    to do this up to about 10 times in an hour.
  prefs: []
  type: TYPE_NORMAL
- en: SSM and SSH into training instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once your job is up and running successfully, especially a long-running job
    with lots of complex steps along the way, you can imagine how useful it would
    be to connect to the instance directly, view it, and run debug commands.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, we have a solution for that – a group of our very own ML SAs built
    out a custom design pattern that helps you enable this in your own environments
    *(1)*. They listened closely to customers, iterated on requirements, and developed
    a very nice project.
  prefs: []
  type: TYPE_NORMAL
- en: You can follow the steps in the following repository to install this in your
    own SageMaker resources, allowing you to easily connect to running jobs and analyze
    them in flight.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – SSH in SageMaker training jobs](img/B18942_Figure_8.06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – SSH in SageMaker training jobs
  prefs: []
  type: TYPE_NORMAL
- en: From a systems architecture perspective, there are two key paths forward in
    evaluating this solution. On the one hand, you can use the fully-managed service,
    AWS Systems Manager. This is generally more secure than SSH but is a bit more
    limited in functionality. If all you need is to open up a terminal onto a remote
    instance, run some debug commands, and view the output in progress, this is probably
    the solution for you. Setting it up isn’t too hard; you just need to configure
    the IAM and SSM resources accordingly. When used in combination with warm pools,
    this is really powerful!
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, you can also use SSH directly. SSH is generally less secure
    than SSM. This is because SSM uses a managed AWS service, while SSH opens up the
    possibility that any malicious user could connect to the nodes using port forwarding.
    This means that in an enterprise environment, in many cases you’re better off
    starting with SSM. SSH will, however, let you update a local script and use port
    forwarding. This means if you want something to take your local script and send
    it to the remote training instance seamlessly, SSH is the way to go. However,
    given that you now have warm pools, it’s questionable whether you’d need this.
    The SSH solution is really nice if your IDE supports remote connection points,
    such as VS Code or PyCharm.
  prefs: []
  type: TYPE_NORMAL
- en: Track jobs and experiments to replicate results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of my personal favorite features of SageMaker training is, honestly, its
    most basic – storing everything about your job and keeping it searchable by default!
    That’s called the **metadata**. All the hyperparameters, input data locations,
    images, variables, and other information about your job are stored every time
    you submit them. This means you can easily track your jobs over time, logging
    in to view the CloudWatch logs, downloading the model from S3 whenever you need
    to, adding tags to specify other details, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Viewing your training job metadata in the AWS console](img/B18942_Figure_8.07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Viewing your training job metadata in the AWS console
  prefs: []
  type: TYPE_NORMAL
- en: All of this data is in your account for the long haul, without your paying for
    any of it. You also use SageMaker Search to find jobs with the highest accuracy
    from a given S3 path, an instance type or count, a hyperparameter, or any available
    value. Just recently we’ve launched a few new features that make using SageMaker
    Training much easier. One of them is a hosted TensorBoard (https://aws.amazon.com/about-aws/whats-new/2023/04/amazon-sagemaker-hosted-tensorboard/)
    which lets you easily track and compare experiments. The second is a new @remote
    decorator that lets you transition local functions to remote jobs very easily!
    ([https://aws.amazon.com/blogs/machine-learning/run-your-local-machine-learning-code-as-amazon-sagemaker-training-jobs-with-minimal-code-changes/?sc_channel=sm&sc_campaign=Machine_Learning&sc_publisher=LINKEDIN&sc_geo=GLOBAL&sc_outcome=awareness&sc_content=ml_services&trk=machine_learning&linkId=211795861](https://aws.amazon.com/blogs/machine-learning/run-your-local-machine-learning-code-as-amazon-sagemaker-training-jobs-with-minimal-code-changes/?sc_channel=sm&sc_campaign=Machine_Learning&sc_publisher=LINKEDIN&sc_geo=GLOBAL&sc_outcome=awareness&sc_content=ml_services&trk=machine_learning&linkId=211795861))
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s close out the chapter by learning about backend optimizations!
  prefs: []
  type: TYPE_NORMAL
- en: Backend optimizations for distributed training with SageMaker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ve learned how to update your training scripts for SageMaker, and you’ve
    taken a closer look at some of the ways SageMaker is pretty fun and friendly to
    work with. Let’s finish by exploring ways that SageMaker optimizes the backend
    for large-scale distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: As you have probably guessed, SageMaker can spin up anywhere from a few to a
    few thousand GPUs. This is due to the core service offering for training – the
    ability to turn on, orchestrate, and manage all of these GPUs on your behalf.
    You define this cluster when you define a training job, and as you learned earlier
    in this chapter, you use *mpi* to communicate between all of the nodes. You can
    store all of the hyperparameters and job metadata, stream all of your logs to
    CloudWatch, plug into your favorite operations tooling, ensure the nodes are healthy,
    connect to your data in S3, download and run your image, and so on. This *large-scale
    cluster orchestration* is completely elastic, easily flowing from one to hundreds
    of instances.
  prefs: []
  type: TYPE_NORMAL
- en: However, orchestrating this cluster is not especially useful, unless you have
    healthy GPUs. As you learned earlier in the book, writing software to successfully
    orchestrate all the tens of thousands of cores in a single GPU is no small task.
    Even when you have updated CUDA, drivers, and the latest deep learning frameworks,
    the bad news is that you may still get a bad GPU. Hardware fails, and GPU failures
    are incredibly common. As you scale up your training jobs to more GPUs, the odds
    of you getting a GPU failure even once in that massive pool of compute increases.
    This is why the *GPU health checks* that SageMaker brings to the table are incredibly
    useful! We can track down the latest GPU errors and have integrated checks for
    these in our job orchestrator. It means that when you get a node on SageMaker,
    it is much more likely to be healthy.
  prefs: []
  type: TYPE_NORMAL
- en: Even with extensive GPU health checks and large-scale job orchestration, it
    is still possible that your job will error out even before it starts. You might
    get something like an *insufficient capacity error*, indicating that there are
    simply not enough of your requested instance type in your requested region. You
    could also get an *internal service error*, unsurprisingly telling you that something
    went wrong at your end. For these and other cases, it is extremely useful to have
    `max_retry_attempts` to your preference; personally, I just max it out at 30 every
    time I am running something with more than 8 instances.
  prefs: []
  type: TYPE_NORMAL
- en: While this is useful to get a job successfully started, I also have customers
    who *implement another job restart*. This might come into play when stepping through
    your mini-batches during your training loop. While the `bf16` data type has proven
    extremely useful to improve the stability of large-scale distributed GPU training,
    it is still not uncommon to see the loss of your model spontaneously spike, plateau,
    or drop. You might also see your total job throughput unexpectedly change. If
    any of these things happen, it’s wise to trigger an emergency checkpoint, kill
    the job, and then start again from that same checkpoint and step number. A combination
    of a few extra functions in your training script and a Lambda function listening
    via the `EventBridge` would be a natural way to do this. For a recent summary
    of some best practices, take a look at the blog post *(4)* in the *References*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training libraries – a model and data parallel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you’ve learned previously, AWS has optimizations for distributed training.
    These are extremely effective methods to scale up to hundreds and thousands of
    GPUs on SageMaker. Let’s take one more look at them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that AlexNet only achieved groundbreaking results because it used multiple
    GPUs? Historically speaking, one of the earliest approaches to a multi-node deep
    learning process was called a **parameter server**. Parameter servers, as you
    can see in the following diagram, are a simple and effective way to orchestrate
    distributed gradient descent at scale. One node operates as the leader. It synchronizes
    gradients with the worker nodes, checking their health, and maintaining one globally
    consistent version of the model. Parameter servers can be somewhat slow, but they
    are actually more efficient in terms of the bandwidth they consume. Let’s explore
    this visually.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Historical approaches to distributed gradient descent](img/B18942_Figure_8.08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Historical approaches to distributed gradient descent
  prefs: []
  type: TYPE_NORMAL
- en: This slowness, however, led to a slightly different approach. Ring-based topologies
    used the `AllReduce` algorithm under the hood to communicate between all nodes,
    collecting the average of the gradients and distributing the result to each of
    the nodes. This is the same basic approach common in Horovod and PyTorch DistributedDataParallel,
    popularized by their increase in speed over their older cousin.
  prefs: []
  type: TYPE_NORMAL
- en: However, `AllReduce` as a basic collective does *not* perform well at scale.
    Every additional node increases the bandwidth consumed during the `AllReduce`
    step. This means that your scaling efficiency gets worse as you add more instances,
    ultimately leading to poor utilization of your instances and, thus, your compute
    budget.
  prefs: []
  type: TYPE_NORMAL
- en: To counter this negative impact, AWS developed *custom collectives for data
    parallel*. These are the single best way to get the highest performance on the
    AWS Cloud. This was introduced as **SageMaker Distributed Data Parallel** (**SMDDP**)
    *(5)*, available as an SDK, in your container, and for any supported SageMaker
    job. Use SMDDP to ensure your large-scale GPU jobs are running as quickly and
    efficiently as possible, using them as the backend for any supported distributed
    software. SMDDP also integrates with Amazon’s Elastic Fabric Adapter, a low-jitter
    low-latency communication enhancement on AWS. Generally, SMDDP makes it easy for
    you to point to it from deep learning frameworks, setting it as your distributed
    backend.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately for you, as of the December 2022 release, this is now also available
    in the *model parallel* family. Now, you can set a `ddp` backend in the `smp_options`
    object, with `ddp_dist_backend:auto`. When this new backend option is combined
    with the *sharded data parallel* configuration we discussed in [*Chapter 5*](B18942_05.xhtml#_idTextAnchor085),
    this gives you another 30% boost!
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s close out the chapter with a quick recap.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the key features of Amazon SageMaker for large-scale
    distributed training. We looked at how to optimize your script, from importing
    packages to parsing arguments, writing code, invoking your script with `mpi`,
    writing to CloudWatch logs, checkpointing, working with the SM estimator, and
    so on. We covered key usability features to make SageMaker more fun and friendly
    to work with, such as warm pools for rapid experimentation, SSM and SSH in training
    instances, and tracking jobs. Finally, we learned about backend optimizations
    for distributed training, such as SMDDP collectives, using it both standalone
    and in combination with the model parallel package.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll explore even more advanced topics in distributed
    training!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please go through the following content for more information on a few topics
    covered in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*aws-sample/sagemaker-ssh-helper*: [https://github.com/aws-samples/sagemaker-ssh-helper](https://github.com/aws-samples/sagemaker-ssh-helper)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Use TensorBoard in Amazon SageMaker* *Studio*:[https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tensorboard.html](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tensorboard.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*aws/amazon-sagemaker-examples*: [https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt2/train_gpt_simple.py](https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt2/train_gpt_simple.py)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Training large language models on Amazon SageMaker: Best* *practices*: [https://aws.amazon.com/blogs/machine-learning/training-large-language-models-on-amazon-sagemaker-best-practices/](https://aws.amazon.com/blogs/machine-learning/training-large-language-models-on-amazon-sagemaker-best-practices/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Introduction to SageMaker’s Distributed Data Parallel* *Library*: [https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-intro.html](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-intro.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
