<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Finding Coreference Between Concepts/People"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Finding Coreference Between Concepts/People</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Named entity coreference with a document</li><li class="listitem" style="list-style-type: disc">Adding pronouns to coreference</li><li class="listitem" style="list-style-type: disc">Cross-document coreference</li><li class="listitem" style="list-style-type: disc">The John Smith problem</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec80"/>Introduction</h1></div></div></div><p>Coreference<a class="indexterm" id="id623"/> is a basic mechanism in human language that allows two sentences to be about the same thing. It's a big deal for human communication—it functions much in the same way as variable names do in programming languages, with the additional subtly that scope is defined by very different rules than blocks. Coreference is less important commercially—maybe this chapter will help change that. Here is an example:</p><div class="informalexample"><pre class="programlisting">Alice walked into the garden. She was surprised.</pre></div><p>Coreference exists between <code class="literal">Alice</code> and <code class="literal">She</code>; the phrases talk about the same thing. It all gets very interesting when we start asking whether Alice in one document is the same as Alice in another.</p><p>Coreference, like word-sense disambiguation, is a next-generation industrial capacity. The challenges of coreference contribute to the insistence of the IRS to have a social security number that unambiguously identifies persons independent of their names. Many of the techniques discussed were developed to help track persons and organizations in text data with varying degrees of success.</p></div></div>
<div class="section" title="Named entity coreference with a document"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec81"/>Named entity coreference with a document</h1></div></div></div><p>As seen in <a class="link" href="ch05.html" title="Chapter 5. Finding Spans in Text – Chunking">Chapter 5</a>, <span class="emphasis"><em>Finding Spans in Text – Chunking</em></span>, LingPipe can use a variety of techniques to recognize<a class="indexterm" id="id624"/> proper nouns that correspond to persons, places, things, genes, and so on. However, chunking doesn't quite finish the job, because it doesn't help with finding an entity when two named entities are the same. Being able to say that John Smith is the same entity as Mr. Smith, John or even an exact repeat, John Smith, can be very useful—so useful that the idea was the basis of our company when we were a baby-defense contractor. Our novel contribution was the generation of sentences indexed by what entities they mentioned, which turned out to be an excellent way to summarize what was being said about that entity, particularly if the mapping spanned languages—we call it <a class="indexterm" id="id625"/>
<span class="strong"><strong>entity-based summarization</strong></span>.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>The idea for entity-based summarization came about as a result of a talk Baldwin gave at the University of Pennsylvania at a graduate student seminar. Mitch Marcus, the then department chair, thought that showing all sentences that mentioned an entity—including pronouns—will be an excellent summary of that entity. In some sense, this comment is why LingPipe exists. It led to Baldwin leading a UPenn DARPA project and then the creation of Alias-i. Lesson learned—talk to everybody about your ideas and research.</p></div></div><p>This recipe will take you through the basics of computing coreferences.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec184"/>Getting ready</h2></div></div></div><p>Lay your hands on some narrative text; we will use a simple example that we know works—coreference systems usually need a lot of tuning to the domain. Feel free to pick something else, but it will need to be in English.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec185"/>How to do it…</h2></div></div></div><p>As usual, we will take you through running code from the command line and then dive into what the code actually does. Off we go.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We will start with a simple text to illustrate coreference. The file is in <code class="literal">data/simpleCoref.txt</code>, and it contains:<div class="informalexample"><pre class="programlisting">John Smith went to Washington. Mr. Smith is a business man.</pre></div></li><li class="listitem">Get thee to a command line and a Java interpreter and reproduce the following:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter7.NamedEntityCoreference</strong></span>
</pre></div></li><li class="listitem">This results in:<div class="informalexample"><pre class="programlisting">Reading in file :data/simpleCoref.txt 
Sentence Text=John Smith went to Washington.
     mention text=John Smith type=PERSON id=0
     mention text=Washington type=LOCATION id=1
Sentence Text=Mr. Smith is a business man.
     mention text=Mr. Smith type=PERSON id=0</pre></div></li><li class="listitem">There are three named entities found. Note that there is an <code class="literal">ID</code> field in the output. The <code class="literal">John Smith</code> and <code class="literal">Mr. Smith </code>entities have the same ID, <code class="literal">id=0</code>. This means that the phrases are considered to be coreferent. The remaining entity <code class="literal">Washington</code> has a different ID, <code class="literal">id=1</code>, and is not coreferent with John Smith / Mr. Smith.</li><li class="listitem">Create your own text file, supply it as an argument on the command line, and see what gets computed.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec186"/>How it works…</h2></div></div></div><p>The coreference code in <a class="indexterm" id="id626"/>LingPipe is a heuristic system built on top of sentence detection and named-entity recognition. The overall flow is as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Tokenize the text.</li><li class="listitem">Detect sentences in the document, for each sentence, detect named entities in the sentence in the left-to-right order, and for each named entity, perform the following tasks:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a mention. A mention is a single instance of a named entity.</li><li class="listitem">Mentions can be added to the existing mention chains, or they can start their own mention chains.</li><li class="listitem">Try to resolve the mention to a mention chain that is already created. If a unique match is found, then add the mention to the mention chain; otherwise, create a new mention chain.</li></ol></div></li></ol></div><p>The code is in <code class="literal">src/com/lingpipe/cookbook/chapter7/NamedEntityCoreference.java</code>. The <code class="literal">main()</code> method starts by setting up the parts of this recipe, starting with a tokenizer factory, sentence chunker, and finally, a named-entity chunker:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) 
    throws ClassNotFoundException, IOException {
  String inputDoc = args.length &gt; 0 ? args[0] 
        : "data/simpleCoref.txt";
  System.out.println("Reading in file :" 
      + inputDoc);
  TokenizerFactory mTokenizerFactory
    = IndoEuropeanTokenizerFactory.INSTANCE;
  SentenceModel sentenceModel
    = new IndoEuropeanSentenceModel();
  Chunker sentenceChunker 
    = new SentenceChunker(mTokenizerFactory,sentenceModel);
   File modelFile  
    = new File("models/ne-en-news-"
      + "muc6.AbstractCharLmRescoringChunker");
  Chunker namedEntChunker
    = (Chunker) AbstractExternalizable.readObject(modelFile);</pre></div><p>Now, we have set up the basic infrastructure for the recipe. Next is a coreference-specific class:</p><div class="informalexample"><pre class="programlisting">MentionFactory mf = new EnglishMentionFactory();</pre></div><p>The <code class="literal">MentionFactory</code> class <a class="indexterm" id="id627"/>creates mentions from phrases and types—the current source is named <code class="literal">entities</code>. Next, the coreference class is created with <code class="literal">MentionFactory</code> as a parameter:</p><div class="informalexample"><pre class="programlisting">WithinDocCoref coref = new WithinDocCoref(mf);</pre></div><p>The <code class="literal">WithinDocCoref</code> class wraps all the mechanics of computing coreference. From <a class="link" href="ch05.html" title="Chapter 5. Finding Spans in Text – Chunking">Chapter 5</a>, <span class="emphasis"><em>Finding Spans in Text - Chunking</em></span>, you should be familiar with the code to get the document text, detect sentences, and iterate over the sentences that apply a named-entity chunker to each sentence:</p><div class="informalexample"><pre class="programlisting">File doc = new File(inputDoc);
String text = Files.readFromFile(doc,Strings.UTF8);
Chunking sentenceChunking
  = sentenceChunker.chunk(text);
Iterator sentenceIt 
  = sentenceChunking.chunkSet().iterator();

for (int sentenceNum = 0; sentenceIt.hasNext(); ++sentenceNum) {
  Chunk sentenceChunk = (Chunk) sentenceIt.next();
  String sentenceText 
    = text.substring(sentenceChunk.start(),
          sentenceChunk.end());
  System.out.println("Sentence Text=" + sentenceText);

  Chunking neChunking = namedEntChunker.chunk(sentenceText);</pre></div><p>In the context of the current sentence, the named entities from the sentence are iterated over in the left-to-right order as they would be read. We know this because the <code class="literal">ChunkingImpl</code> class returns chunks in the order that they were added, and our <code class="literal">HMMChunker</code> adds them in the left-to-right order:</p><div class="informalexample"><pre class="programlisting">Chunking neChunking = namedEntChunker.chunk(sentenceText);
for (Chunk neChunk : neChunking.chunkSet()) {</pre></div><p>The following <a class="indexterm" id="id628"/>code takes the information from the chunk—type and phrase, but <span class="emphasis"><em>not</em></span> the offset information, and creates a mention:</p><div class="informalexample"><pre class="programlisting">String mentionText
  = sentenceText.substring(neChunk.start(),
          neChunk.end());
String mentionType = neChunk.type();
Mention mention = mf.create(mentionText,mentionType);</pre></div><p>The next line runs coreference with the mention and what sentence it is in and returns its ID:</p><div class="informalexample"><pre class="programlisting">int mentionId = coref.resolveMention(mention,sentenceNum);

System.out.println("     mention text=" + mentionText
            + " type=" + mentionType
            + " id=" + mentionId);</pre></div><p>If the mention was resolved to an existing entity, it will have that ID, as we saw with Mr. Smith. Otherwise, it will get a distinct ID and itself be available as an antecedent for subsequent mentions.</p><p>This covers the mechanics of running within a document coreference. The upcoming recipes will cover the modification of this class. The next recipe will add pronouns and provide references.</p></div></div>
<div class="section" title="Adding pronouns to coreference"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec82"/>Adding pronouns to coreference</h1></div></div></div><p>The preceding recipe handled<a class="indexterm" id="id629"/> coreference between named entities. This <a class="indexterm" id="id630"/>recipe will add pronouns to the mix.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec187"/>How to do it…</h2></div></div></div><p>This recipe will use an interactive version to help you explore the properties of the coreference algorithm. The system is very dependent on the quality of the named-entity detection, so use examples that the HMM is likely to get right. This was trained on <span class="emphasis"><em>Wall Street Journal</em></span> articles from the '90s.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Saddle up your console and type the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter7.Coreference</strong></span>
</pre></div></li><li class="listitem">In the resulting command prompt, type this:<div class="informalexample"><pre class="programlisting">Enter text followed by new line
&gt;John Smith went to Washington. He was a senator.
Sentence Text=John Smith went to Washington.
mention text=John Smith type=PERSON id=0
mention text=Washington type=LOCATION id=1
Sentence Text= He was a senator.
mention text=He type=MALE_PRONOUN id=0</pre></div></li><li class="listitem">The shared ID between <code class="literal">He</code> and <code class="literal">John Smith</code> indicates the coreference between the two. More examples will follow, with comments. Note that each input is considered a distinct document with separate ID spaces.</li><li class="listitem">If pronouns are not resolved to a named entity, they get the index <code class="literal">-1</code> as shown here:<div class="informalexample"><pre class="programlisting">&gt;He went to Washington.
Sentence Text= He went to Washington.
mention text=He type=MALE_PRONOUN id=-1
mention text=Washington type=LOCATION id=0</pre></div></li><li class="listitem">The following <a class="indexterm" id="id631"/>case also results in a <code class="literal">-1</code> value for <code class="literal">id</code>, because <a class="indexterm" id="id632"/>there is not one unique person in the prior context but two. This is called a failed uniqueness presupposition:<div class="informalexample"><pre class="programlisting">&gt;Jay Smith and Jim Jones went to Washington. He was a senator.
Sentence Text=Jay Smith and Jim Jones went to Washington.
mention text=Jay Smith type=PERSON id=0
mention text=Jim Jones type=PERSON id=1
mention text=Washington type=LOCATION id=2
Sentence Text= He was a senator.
mention text=He type=MALE_PRONOUN id=-1</pre></div></li><li class="listitem">The following code shows that <code class="literal">John Smith</code> can be resolved to a female pronoun as well. This is because there is no data about what names indicate which genders. It can be added, but generally, the context will disambiguate. <code class="literal">John</code> could be a female name. The key here is that the pronoun will disambiguate the gender, and a following male pronoun will fail to match:<div class="informalexample"><pre class="programlisting">Frank Smith went to Washington. She was a senator. 
Sentence Text=Frank Smith went to Washington.
     mention text=Frank Smith type=PERSON id=0
     mention text=Washington type=LOCATION id=1
Sentence Text=She was a senator.
     mention text=She type=FEMALE_PRONOUN id=0</pre></div></li><li class="listitem">The gender assignment will block reference by an incorrect gender. The <code class="literal">He</code> pronoun in the following code is resolved to ID <code class="literal">-1</code>, because the only person is resolved to a female pronoun:<div class="informalexample"><pre class="programlisting">John Smith went to Washington. She was a senator. He is now a lobbyist.
Sentence Text=John Smith went to Washington.
     mention text=John Smith type=PERSON id=0
     mention text=Washington type=LOCATION id=1
Sentence Text=She was a senator.
     mention text=She type=FEMALE_PRONOUN id=0
Sentence Text=He is now a lobbyist.
     mention text=He type=MALE_PRONOUN id=-1</pre></div></li><li class="listitem">Coreference can happen inside a sentence as well:<div class="informalexample"><pre class="programlisting">&gt;Jane Smith knows her future.
Sentence Text=Jane Smith knows her future.
     mention text=Jane Smith type=PERSON id=0
     mention text=her type=FEMALE_PRONOUN id=0</pre></div></li><li class="listitem">The order of the <a class="indexterm" id="id633"/>mentions (ordered by the most recent <a class="indexterm" id="id634"/>mention) matters when resolving mentions. In the following code, <code class="literal">He</code> is resolved to <code class="literal">James</code>, not <code class="literal">John</code>:<div class="informalexample"><pre class="programlisting">John is in this sentence. Another sentence about nothing. James is in this sentence. He is here.
Sentence Text=John is in this sentence.
     mention text=John type=PERSON id=0
Sentence Text=Another sentence about nothing.
Sentence Text=James is in this sentence.
     mention text=James type=PERSON id=1
Sentence Text=He is here.
     mention text=He type=MALE_PRONOUN id=1</pre></div></li><li class="listitem">The same effect takes place with named-entity mentions. The <code class="literal">Mr. Smith</code> entity resolves to the last mention:<div class="informalexample"><pre class="programlisting">John Smith is in this sentence. Random sentence. James Smith is in this sentence. Mr. Smith is mention again here.
Sentence Text=John Smith is in this sentence.
     mention text=John Smith type=PERSON id=0
Sentence Text=Random sentence.
     mention text=Random type=ORGANIZATION id=1
Sentence Text=James Smith is in this sentence.
     mention text=James Smith type=PERSON id=2
Sentence Text=Mr. Smith is mention again here.
     mention text=Mr. Smith type=PERSON id=2</pre></div></li><li class="listitem">The distinction between <code class="literal">John</code> and <code class="literal">James</code> goes away if there are too many intervening sentences:<div class="informalexample"><pre class="programlisting">John Smith is in this sentence. Random sentence. James Smith is in this sentence. Random sentence. Random sentence. Mr. Smith is here.
Sentence Text=John Smith is in this sentence.
     mention text=John Smith type=PERSON id=0
Sentence Text=Random sentence.
     mention text=Random type=ORGANIZATION id=1
Sentence Text=James Smith is in this sentence.
     mention text=James Smith type=PERSON id=2
Sentence Text=Random sentence.
     mention text=Random type=ORGANIZATION id=1
Sentence Text=Random sentence.
     mention text=Random type=ORGANIZATION id=1
Sentence Text=Mr. Smith is here.
     mention text=Mr. Smith type=PERSON id=3</pre></div></li></ol></div><p>The preceding examples are meant to demonstrate the properties of the within-document coreference system.</p></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec188"/>How it works…</h2></div></div></div><p>The code changes <a class="indexterm" id="id635"/>to add pronouns are straightforward. The<a class="indexterm" id="id636"/> code for this recipe is in <code class="literal">src/com/lingpipe/cookbook/chapter7/Coreference.java</code>. The recipe assumes that you understood the previous recipe, so it just covers the addition of pronoun mentions:</p><div class="informalexample"><pre class="programlisting">Chunking mentionChunking
  = neChunker.chunk(sentenceText);
Set&lt;Chunk&gt; chunkSet = new TreeSet&lt;Chunk&gt; (Chunk.TEXT_ORDER_COMPARATOR);
chunkSet.addAll(mentionChunking.chunkSet());</pre></div><p>We added the <code class="literal">Mention</code> objects from multiple sources, so there are no order guarantees on the order of elements anymore. Correspondingly, we created <code class="literal">TreeSet</code> and the appropriate comparator and added all the chunkings from the <code class="literal">neChunker</code>.</p><p>Next, we will add the male and female pronouns:</p><div class="informalexample"><pre class="programlisting">addRegexMatchingChunks(MALE_EN_PRONOUNS,"MALE_PRONOUN",
        sentenceText,chunkSet);
addRegexMatchingChunks(FEMALE_EN_PRONOUNS,"FEMALE_PRONOUN",
        sentenceText,chunkSet);</pre></div><p>The <code class="literal">MALE_EN_PRONOUNS</code> constant is a regular expression, <code class="literal">Pattern</code>:</p><div class="informalexample"><pre class="programlisting">static Pattern MALE_EN_PRONOUNS =   Pattern.compile("\\b(He|he|Him|him)\\b");</pre></div><p>The following lines of code show the <code class="literal">addRegExMatchingChunks</code> subroutine. It adds chunks based on regular expression matches and removes the overlapping, existing HMM-derived chunks:</p><div class="informalexample"><pre class="programlisting">static void addRegexMatchingChunks(Pattern pattern, String type, String text, Set&lt;Chunk&gt; chunkSet) {
  
  java.util.regex.Matcher matcher = pattern.matcher(text);
  
  while (matcher.find()) {
    Chunk regexChunk 
    = ChunkFactory.createChunk(matcher.start(),
            matcher.end(),
            type);
    for (Chunk chunk : chunkSet) {
    if (ChunkingImpl.overlap(chunk,regexChunk)) {
      chunkSet.remove(chunk);
    }
    }
  chunkSet.add(regexChunk);
  }</pre></div><p>The one complex bit is that the type for the <code class="literal">MALE_PRONOUN</code> and <code class="literal">FEMALE_PRONOUN</code> pronouns will be used to match against <code class="literal">PERSON</code> entities, with the consequence that the resolution sets the gender of the resolved-to entity.</p><p>Other than that, the <a class="indexterm" id="id637"/>code should look very familiar with our <a class="indexterm" id="id638"/>standard I/O loop running the interaction in the command prompt.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec189"/>See also</h2></div></div></div><p>The algorithm behind the system is based on the PhD. thesis of Baldwin. The system was called <a class="indexterm" id="id639"/>CogNIAC, and the work is from the mid '90s and is not a current state-of-the-art coreference system. A more modern approach would most likely use a machine-learning framework to take the features generated by Baldwin's approach and many other features and use it to develop a better performing system. A paper on the system is at <a class="ulink" href="http://www.aclweb.org/anthology/W/W97/W97-1306.pdf">http://www.aclweb.org/anthology/W/W97/W97-1306.pdf</a>.</p></div></div>
<div class="section" title="Cross-document coreference"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec83"/>Cross-document coreference</h1></div></div></div><p>Cross-document coreference (XDoc)<a class="indexterm" id="id640"/> takes the <code class="literal">id</code> space of an individual document and makes it global to a larger universe. This universe typically includes other processed documents and databases of known entities. While the annotation is trivial, all that one needs to do is swap the document-scope IDs for the universe-scope IDs. The calculation of XDoc can be quite difficult.</p><p>This recipe will tell us how to use a lightweight implementation of XDoc developed over the course of deploying such systems over the years. We will provide a code overview for those who might want to extend/modify the code—but there is a lot going on, and the recipe is quite dense.</p><p>The input is in the XML format where each file can contain multiple documents:</p><div class="informalexample"><pre class="programlisting">&lt;doc id="1"&gt;
&lt;title/&gt;
&lt;content&gt;
Breck Baldwin and Krishna Dayanidhi wrote a book about LingPipe. 
&lt;/content&gt;
&lt;/doc&gt;

&lt;doc id="2"&gt;
&lt;title/&gt;
&lt;content&gt;
Krishna Dayanidhi is a developer. Breck Baldwin is too. 
&lt;/content&gt;
&lt;/doc&gt;

&lt;doc id="3"&gt;
&lt;title/&gt;
&lt;content&gt;
K-dog likes to cook as does Breckles.
&lt;/content&gt;
&lt;/doc&gt;</pre></div><p>The goal is to produce annotations where the mentions of Breck Baldwin share the same ID across documents as for Krishna. Note that both are mentioned by their nicknames in the last document.</p><p>A very common elaboration of XDoc is linking a <span class="strong"><strong>database</strong></span> (<span class="strong"><strong>DB</strong></span>)<a class="indexterm" id="id641"/> of known entities to text mentions of these entities. This bridges the divide between structured DB and unstructured data (text), which many consider to be the next big thing in business intelligence / voice of the customer / enterprise-knowledge management. We have built systems that linked DBs of genes/proteins to MEDLINE abstracts and persons-of-interest lists to free text, and so on. DBs also provide a natural way for human editors to control how XDoc behaves.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec190"/>How to do it...</h2></div></div></div><p>All the code for this <a class="indexterm" id="id642"/>recipe is in the <code class="literal">com.lingpipe.cookbook.chapter7.tracker</code> package.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Gain access to your IDE and run <code class="literal">RunTracker</code> or type the following command in the command line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter7.tracker.RunTracker</strong></span>
</pre></div></li><li class="listitem">The screen will scroll by with the analysis of documents, but we will go to the designated output file and examine it. Open <code class="literal">cookbook/data/xDoc/output/docs1.xml</code> in your favorite text editor. You will see a poorly formatted version of the example output, unless your editor automatically formats XML usefully—the Firefox web browser does a decent job of rendering XML. The output should look like this:<div class="informalexample"><pre class="programlisting">&lt;docs&gt;
&lt;doc id="1"&gt;
&lt;title/&gt;
&lt;content&gt;
&lt;s index="0"&gt;
&lt;entity id="1000000001" type="OTHER"&gt;Breck Baldwin&lt;/entity&gt; and &lt;entity id="1000000002" type="OTHER"&gt;Krishna Dayanidhi&lt;/entity&gt; wrote a book about &lt;entity id="1000000003" type="OTHER"&gt;LingPipe.&lt;/entity&gt;
&lt;/s&gt;
&lt;/content&gt;
&lt;/doc&gt;
&lt;doc id="2"&gt;
&lt;title/&gt;
&lt;content&gt;&lt;s index="0"&gt;
&lt;entity id="1000000002" type="OTHER"&gt;Krishna Dayanidhi&lt;/entity&gt; is a developer.
&lt;/s&gt;
&lt;s index="1"&gt;&lt;entity id="1000000001" type="OTHER"&gt;Breck Baldwin&lt;/entity&gt; is too.
&lt;/s&gt;
&lt;/content&gt;
&lt;/doc&gt;
&lt;doc id="3"&gt;&lt;title/&gt;&lt;content&gt;&lt;s index="0"&gt;K-dog likes to cook as does &lt;entity id="1000000004" start="28" type="OTHER"&gt;Breckles&lt;/entity&gt;.&lt;/s&gt;&lt;/content&gt;&lt;/doc&gt;
&lt;/docs&gt;</pre></div></li><li class="listitem"><code class="literal">Krishna</code> is recognized in the first two documents with the shared ID, <code class="literal">1000000002</code>, but the <a class="indexterm" id="id643"/>nickname, <code class="literal">K-dog</code>, is not recognized at all. <code class="literal">Breck</code> is recognized in all three documents, but since the ID on the third mention, <code class="literal">Breckles</code>, is different from the one in the first two mentions, the system does not consider them to be the same entity.</li><li class="listitem">Next, we will use a DB in the form of a dictionary to improve the recognition of the authors when they are mentioned via nicknames. There is a dictionary at <code class="literal">data/xDoc/author-dictionary.xml</code>; it looks like this:<div class="informalexample"><pre class="programlisting">&lt;dictionary&gt;
&lt;entity canonical="Breck Baldwin" id="1" speculativeAliases="0" type="MALE"&gt;
  &lt;alias xdc="1"&gt;Breck Baldwin&lt;/alias&gt;
  &lt;alias xdc="1"&gt;Breckles&lt;/alias&gt;
  &lt;alias xdc="0"&gt;Breck&lt;/alias&gt;
&lt;/entity&gt;

&lt;entity canonical="Krishna Dayanidhi" id="2" speculativeAliases="0" type="MALE"&gt;
  &lt;alias xdc="1"&gt;Krishna Dayanidhi&lt;/alias&gt;
  &lt;alias xdc="1"&gt;K-Dog&lt;/alias&gt;
  &lt;alias xdc="0"&gt;Krishna&lt;/alias&gt; 
&lt;/entity&gt;</pre></div></li><li class="listitem">The aforementioned dictionary contains nicknames for both authors, in addition to their first names. Aliases that have the <code class="literal">xdc=1</code> value will be used to link entities across documents. The <code class="literal">xdc=0</code> value will only apply within a document. All aliases will be used to identify named entities via a dictionary lookup.</li><li class="listitem">Run the following command, which specifies the entity dictionary or IDE equivalent:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter7.tracker.RunTracker data/xDoc/author-dictionary.xml</strong></span>
</pre></div></li><li class="listitem">The output in <code class="literal">xDoc/output/docs1.xml</code> is very different from that of the previous run. First, note that the IDs for us are now the same as specified in the dictionary file: <code class="literal">1</code> for <code class="literal">Breck</code> and <code class="literal">2</code> for <code class="literal">Krishna</code>. This is a link between the structured DB, such as the nature of the dictionary and unstructured text. Second, notice that both our nicknames have been correctly identified and assigned to the correct IDs. Third, note that the types are now <code class="literal">MALE</code> instead of <code class="literal">OTHER</code>:<div class="informalexample"><pre class="programlisting">&lt;docs&gt;
&lt;doc id="1"&gt;
&lt;title/&gt;
&lt;content&gt;
&lt;s index="0"&gt;
&lt;entity id="1" type="MALE"&gt;Breck Baldwin&lt;/entity&gt; and &lt;entity id="2" type="MALE"&gt;Krishna Dayanidhi&lt;/entity&gt; wrote a book about &lt;entity id="1000000001" type="OTHER"&gt;LingPipe.&lt;/entity&gt;
&lt;/s&gt;
&lt;/content&gt;
&lt;/doc&gt;
&lt;doc id="2"&gt;
&lt;title/&gt;
&lt;content&gt;
&lt;s index="0"&gt;
&lt;entity id="2" start="0" type="MALE"&gt;K-dog&lt;/entity&gt; likes to cook as does &lt;entity id="1" start="28" type="MALE"&gt;Breckles&lt;/entity&gt;.
&lt;/s&gt;
&lt;/content&gt;
&lt;/doc&gt;
&lt;/docs&gt;</pre></div></li></ol></div><p>This was a very quick introduction to how to run XDoc. In the next section, we will see how it works.</p></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec191"/>How it works…</h2></div></div></div><p>Up until this<a class="indexterm" id="id644"/> recipe, we have attempted to keep code simple, straightforward, and understandable without a deep dive into piles of source. This recipe is more complicated. The code that backs this recipe is not going to fit into the allocated space for complete explanation. The exposition assumes that you will explore entire classes on your own and that you will refer to other recipes in this book for explanation. We offer this recipe because XDoc coreference is a very interesting problem, and our existing infrastructure might help others explore the phenomenon. Welcome to the deep end of the pool.</p><div class="section" title="The batch process life cycle"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec21"/>The batch process life cycle</h3></div></div></div><p>The entire process is <a class="indexterm" id="id645"/>controlled by the <code class="literal">RunTracker.java</code> class. The overall flow of the <code class="literal">main()</code> method is as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Read the DB of known entities that will be a source of named-entity recognition via <code class="literal">Dictionary</code> and a known mapping from aliases to dictionary entries. Aliases come with instructions regarding whether they should be used for matching entities across documents via the <code class="literal">xdc=1</code> or <code class="literal">xdc=0</code> flag.</li><li class="listitem">Set up <code class="literal">EnitityUniverse</code>, which is the global data structure of IDs for what is found in the texts and from the mentioned dictionary of known entities.</li><li class="listitem">Set up what is needed for within-document coreference—things such as a tokenizer, sentence detector, and named-entity detector. It gets a bit fancy with a POS tagger and some word counts.</li><li class="listitem">There is a Boolean that controls whether speculative entities will be added. If this Boolean is <code class="literal">true</code>, it means that we will update our universe of cross-document entities with the ones that we have never seen before. It is a much tougher task to reliably compute with this set to <code class="literal">true</code>.</li><li class="listitem">All the mentioned configuration goes into creating a <code class="literal">Tracker</code> object.</li><li class="listitem">Then, the <code class="literal">main()</code> method reads in documents to process, hands them off to the <code class="literal">Tracker</code> object for processing, and writes them to disk. The major steps of the <code class="literal">Tracker.processDocuments()</code> method are as follows:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Take a set of documents in the XML format and get the individual documents.</li><li class="listitem">For each document, apply the <code class="literal">processDocument()</code> method, which runs within-document coreference using the dictionary to help find entities as well as the named-entity detector and returns <code class="literal">MentionChain[]</code>. Then, resolve the individual mentions' chains against the entity universe to update document-level IDs to entity universe IDs. The last step is to write the document to disk with the entity universe IDs.</li></ol></div></li></ol></div><p>That is all that we will say about <code class="literal">RunTracker</code>; there is nothing in there that you should not be able to handle in the context of this book. In the following sections, we will address the individual components that <code class="literal">RunTracker</code> uses.</p><div class="section" title="Setting up the entity universe"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec06"/>Setting up the entity universe</h4></div></div></div><p>The entity <a class="indexterm" id="id646"/>universe <code class="literal">EntityUniverse.java</code>, is an in-memory representation of the global entities mentioned in a document/database collection. The entity universe also contains various indexes into these entities, which support computing XDoc on individual documents.</p><p>The dictionary seeds the <code class="literal">EntityUniverse</code> file with known entities, and the documents processed subsequently are sensitive to these entities. The XDoc algorithm tries to merge with existing entities before creating new ones, so the dictionary entities are strong attractors for mentions of these entities.</p><p>Each entity consists of a unique long ID, a set of aliases partitioned into four separate lists and a type (person, location, and so on). Whether the entity is in the user-defined dictionary and whether speculative mentions are allowed to be added to the entity are also mentioned. The <code class="literal">toString()</code> method lists an entity as:</p><div class="informalexample"><pre class="programlisting">id=556 type=ORGANIZATION userDefined=true allowSpec=false user XDC=[Nokia Corp., Nokia] user non-XDC=[] spec XDC=[] spec non-XDC
=[]</pre></div><p>The global data structures are as follows:</p><div class="informalexample"><pre class="programlisting">    private long mLastId = FIRST_SYSTEM_ID;</pre></div><p>Entities need <a class="indexterm" id="id647"/>unique IDs, and we have a convention that the <code class="literal">FIRST_SYSTEM_ID</code> value is a large integer, such as <code class="literal">1,000,000</code>. This provides a space (IDs &lt; 1,000,000) for users to add new entities without collisions with entities found by the system.</p><p>We will instantiate a tokenizer for use across the tracker:</p><div class="informalexample"><pre class="programlisting">    private final TokenizerFactory mTokenizerFactory;</pre></div><p>There is a global mapping from unique entity IDs to the entities:</p><div class="informalexample"><pre class="programlisting">    private final Map&lt;Long,Entity&gt; mIdToEntity
        = new HashMap&lt;Long,Entity&gt;();</pre></div><p>Another important data structure is a mapping from aliases (phrases) to entities that have the alias—<code class="literal">mXdcPhraseToEntitySet</code>. Only phrases that are candidates for finding likely matches for cross-document coreference get added here. From the dictionary, the aliases that are <code class="literal">xdc=1</code> are added:</p><div class="informalexample"><pre class="programlisting">private final ObjectToSet&lt;String,Entity&gt; mXdcPhraseToEntitySet
        = new ObjectToSet&lt;String,Entity&gt;();</pre></div><p>For speculatively found aliases, if the alias has at least two tokens and is not already on another entity, it is added to this set. This reflects a heuristic that tries hard to not split the entities apart. The logic of this is quite twisted and beyond the scope of this tutorial. You can refer to <code class="literal">EntityUniverse.createEntitySpeculative</code> and <code class="literal">EntityUniverse.addPhraseToEntity</code> for the code.</p><p>Why are some aliases not used in finding candidate entities? Consider that <code class="literal">George</code> has very little descriptive content to discriminate entities in <code class="literal">EntityUniverse</code>, but <code class="literal">George H.W. Bush</code> has much more information to work with.</p></div><div class="section" title="ProcessDocuments() and ProcessDocument()"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec07"/>ProcessDocuments() and ProcessDocument()</h4></div></div></div><p>The interesting bits start to happen in the <code class="literal">Tracker.processDocuments()</code> method, which calls the XML parsing of each document and then incrementally calls the <code class="literal">processDocument()</code> method. The <a class="indexterm" id="id648"/>code is straightforward for the former, so we will move on to where the more task-specific work happens <a class="indexterm" id="id649"/>with the <code class="literal">processDocument()</code> method called:</p><div class="informalexample"><pre class="programlisting">public synchronized OutputDocument processDocument(
            InputDocument document) {

       WithinDocCoref coref
            = new WithinDocCoref(mMentionFactory);

        String title = document.title();
        String content = document.content();

        List&lt;String&gt; sentenceTextList = new ArrayList&lt;String&gt;();
        List&lt;Mention[]&gt; sentenceMentionList 
        = new ArrayList&lt;Mention[]&gt;();

        List&lt;int[]&gt; mentionStartList = new ArrayList&lt;int[]&gt;();
        List&lt;int[]&gt; mentionEndList = new ArrayList&lt;int[]&gt;();

        int firstContentSentenceIndex
            = processBlock(title,0,
                           sentenceTextList,
                           sentenceMentionList,
                           mentionStartList,mentionEndList,
                           coref);

        processBlock(content,firstContentSentenceIndex,
                     sentenceTextList,
                     sentenceMentionList,
                     mentionStartList,mentionEndList,
                     coref);

        MentionChain[] chains = coref.mentionChains();</pre></div><p>We used a document format that supports distinguishing the title from the body of the document. This is a good idea if title case is distinct from body case, as is usual with newswire. The <code class="literal">chains</code> variable will have chains from the title and body of the text, with possible coreference <a class="indexterm" id="id650"/>between them. The <code class="literal">mentionStartList</code> and <code class="literal">mentionEndList</code> arrays will make it possible to realign the<a class="indexterm" id="id651"/> document scoped IDs with the entity universe scoped IDs later in the method:</p><div class="informalexample"><pre class="programlisting">Entity[] entities  = mXDocCoref.xdocCoref(chains);</pre></div></div><div class="section" title="Computing XDoc"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec08"/>Computing XDoc</h4></div></div></div><p>The XDoc code is<a class="indexterm" id="id652"/> the result of many hours of hand-tuning the algorithm to work well on news-style data. It has been run on datasets in the 20,000 document range and is designed to support dictionary entries very aggressively. The code also attempts to prevent<a class="indexterm" id="id653"/> <span class="strong"><strong>short circuits</strong></span>, which occur when obviously different entities have been merged together. If you mistakenly make Barbara Bush and George Bush coreferent in your global database, then you will have embarrassingly bad results that users will see.</p><p>The other sort of error is having two entities in the global store when one will do. This is a sort of <span class="emphasis"><em>Superman/Clark Kent problem</em></span> that can also apply to multiple mentions of the same name.</p><p>We will begin with the top-level code:</p><div class="informalexample"><pre class="programlisting">    public Entity[] xdocCoref(MentionChain[] chains) { Entity[]
        entities = new Entity[chains.length];
     
        Map&lt;MentionChain,Entity&gt; chainToEntity
            = new HashMap&lt;MentionChain,Entity&gt;();
        ObjectToSet&lt;Entity,MentionChain&gt; entityToChainSet
            = new ObjectToSet&lt;Entity,MentionChain&gt;();

        
        for (MentionChain chain : chains)
            resolveMentionChain((TTMentionChain) chain,
                                chainToEntity, entityToChainSet);

        for (int i = 0; i &lt; chains.length; ++i) {
            TTMentionChain chain = (TTMentionChain) chains[i];
            Entity entity = chainToEntity.get(chain);

            if (entity != null) {
                if (Tracker.DEBUG) {
                    System.out.println("XDOC: resolved to" + entity);
         Set chainSetForEntity = entityToChainSet.get(entity);
                    if (chainSetForEntity.size() &gt; 1) 
                        System.out.println("XDOC: multiple chains resolved to same entity " + entity.id());
                }
                entities[i] = entity;
                if (entity.addSpeculativeAliases()) 
                    addMentionChainToEntity(chain,entity);
            } else {
                Entity newEntity = promote(chain);
                entities[i] = newEntity;
            }
        }
        return entities;
    }</pre></div><p>A document has a<a class="indexterm" id="id654"/> list of mention chains, and each mention chain will be either added to an existing entity, or the mention chain will be promoted to being a new entity. Mention chains must contain a mention that is not pronominal, which is handled at the within-document coreference level.</p><p>Three data structures are updated as each mention chain is processed:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">Entity[]</code> entities are returned by the <code class="literal">xdocCoref</code> method to support the inline annotation of the documents.</li><li class="listitem" style="list-style-type: disc"><code class="literal">Map&lt;MentionChain,Entity&gt; chainToEntity</code> maps from mention chains to entities.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ObjectToSet&lt;Entity,MentionChain&gt; entityToChainSet</code> is the converse of <code class="literal">chainToEntity</code>. It is possible that multiple chains in the same document get mapped to the same entity, so this data structure is sensitive to this possibility. This version of the code allows this to happen—in effect, XDoc is setting up a within-doc resolution as a side effect.</li></ul></div><p>Simple enough, if an entity is found, then the <code class="literal">addMentionChainToEntity()</code> method adds any new information from the mention chain to the entity. New information can include new aliases and type changes (that is, a person is moved to being male or female in virtue of a disambiguating pronoun reference). If no entity is found, then the mention chain goes to <code class="literal">promote()</code>, which creates a new entity in the entity universe. We will start with <code class="literal">promote()</code>.</p></div><div class="section" title="The promote() method"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec09"/>The promote() method</h4></div></div></div><p>The entity universe is a minimalist <a class="indexterm" id="id655"/>data structure that just keeps track of phrases, types, and IDs. The <code class="literal">TTMentionChain</code> class is a more complex representation of the mentions of a particular document:</p><div class="informalexample"><pre class="programlisting">private Entity promote(TTMentionChain chain) {
    Entity entity
        = mEntityUniverse.createEntitySpeculative(
          chain.normalPhrases(),
                            chain.entityType());
        if (Tracker.DEBUG)
            System.out.println("XDOC: promoted " + entity);
        return entity;
    }</pre></div><p>The call to <code class="literal">mEntityUniverse.createEntitySpeculative</code> only requires the phrases for the chain (in this case, normalized phrases that have been lowercased and in which all sequences of <a class="indexterm" id="id656"/>whitespaces converted into a single space) and the type of the entity. No record is kept of the document from which the mention chain came, counts, or other potentially useful information. This is to keep the memory representation as small as possible. If there is a need to find all the sentences or documents that an entity is mentioned in (a common task), then that mapping from entity IDs has to be stored elsewhere. The XML representation produced for the document after XDoc is run is a natural place to start addressing these needs.</p></div><div class="section" title="The createEntitySpeculative() method"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec10"/>The createEntitySpeculative() method</h4></div></div></div><p>Creation of a<a class="indexterm" id="id657"/> speculatively found new entity only requires determining which of its aliases are the good candidates to link mention chains. Those that are good for cross-document coreference go into the <code class="literal">xdcPhrases</code> set, and the others go into the <code class="literal">nonXdc</code> phrases:</p><div class="informalexample"><pre class="programlisting">    public Entity createEntitySpeculative(Set&lt;String&gt; phrases,
                                          String entityType) {
        Set&lt;String&gt; nonXdcPhrases = new HashSet&lt;String&gt;();
        Set&lt;String&gt; xdcPhrases = new HashSet&lt;String&gt;();
        for (String phrase : phrases) {
            if (isXdcPhrase(phrase,hasMultiWordPhrases)) 
                xdcPhrases.add(phrase);
            else
                nonXdcPhrases.add(phrase);
        }
        while (mIdToEntity.containsKey(++mLastId)) ; // move up to next untaken ID
        Entity entity = new Entity(mLastId,entityType,
                                  null,null,xdcPhrases,nonXdcPhrases);
        add(entity);
        return entity;
    }</pre></div><p>The <code class="literal">boolean</code> method, <code class="literal">XdcPhrase()</code>, plays a critical role in the XDoc process. The current approach supports a very conservative notion of what a good XDoc phrase is. Intuitively, in the domain of newswire, phrases such as <code class="literal">he</code>, <code class="literal">Bob</code>, and <code class="literal">John Smith</code> are poor indicators of a unique individual being talked about. Good phrases might be <code class="literal">Breckenridge Baldwin</code>, because that is likely a unique name. There are lots of fancy theories for what is going on here, see rigid designators (<a class="ulink" href="http://en.wikipedia.org/wiki/Rigid_designator">http://en.wikipedia.org/wiki/Rigid_designator</a>). The next few lines of code run roughshod over 2,000 years of philosophical thought:</p><div class="informalexample"><pre class="programlisting">public boolean isXdcPhrase(String phrase,
          boolean hasMultiWordPhrase) {

    if (mXdcPhraseToEntitySet.containsKey(phrase)) {
        return false;
    }  
    if (phrase.indexOf(' ') == -1 &amp;&amp; hasMultiWordPhrase) {
        return false;
    }
    if (PronounChunker.isPronominal(phrase)) {
        return false;
   }
    return true;
}</pre></div><p>This approach <a class="indexterm" id="id658"/>attempts to identify the bad phrases for XDoc rather than the good ones. The reasoning is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>There is already an entity associated with the phrase</strong></span>: This enforces an assumption that there is only one John Smith in the world. This worked very well for intelligence-gathering applications, where the analysts had little trouble teasing apart the <code class="literal">John Smith</code> cases. You can refer to the <span class="emphasis"><em>The John Smith problem</em></span> recipe at the end of this chapter for more about this.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The phrase is only one word, and there are multiword phrases associated with the mention chain or entity</strong></span>: This assumes that longer words are better for XDoc. Note that different orders of entity creation can result in one-word phrases having <code class="literal">xdc</code> to be <code class="literal">true</code> on entities with multiword aliases.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The phrase is a pronoun</strong></span>: This is a fairly safe assumption, unless we are in religious texts where <code class="literal">He</code> or <code class="literal">Him</code> capitalized in the middle of a sentence indicate reference to God.</li></ul></div><p>Once the sets of <code class="literal">xdc</code> and <code class="literal">nonXdc</code> phrases are known, then the entity is created. Refer to the source code for <code class="literal">Entity.java</code> to understand how entities are created.</p><p>Then, the entity is created, and an <code class="literal">add</code> method updates a mapping in the <code class="literal">EntityUniverse</code> file of <code class="literal">xdc</code> phrases to entity IDs:</p><div class="informalexample"><pre class="programlisting">public void add(Entity e) {
        if (e.id() &gt; mLastId)
            mLastId = e.id();
        mIdToEntity.put(new Long(e.id()),e);
        for (String phrase : e.xdcPhrases()) {
            mXdcPhraseToEntitySet.addMember(phrase,e);
        }
    }</pre></div><p>The <code class="literal">EntityUniverse</code> file's global <code class="literal">mXdcPhraseToEntitySet</code> variable is the key to finding<a class="indexterm" id="id659"/> candidate entities for XDoc as used in <code class="literal">xdcEntitiesToPhrase()</code>.</p></div><div class="section" title="The XDocCoref.addMentionChainToEntity() entity"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec11"/>The XDocCoref.addMentionChainToEntity() entity</h4></div></div></div><p>Returning to the <code class="literal">XDocCoref.xdocCoref()</code> method, we have covered how to create a new entity <a class="indexterm" id="id660"/>via <code class="literal">XDocCoref.promote()</code>. The next option to cover is what happens when a mention chain is resolved to an existing entity, namely <code class="literal">XDocCoref.addMentionChainToEntity()</code>. For the speculative mentions to be added, the entity must allow speculatively found mentions as provided by the <code class="literal">Entity.allowSpeculativeAliases()</code> method. This is a feature of the user-defined dictionary entities discussed in user-defined entities. If speculative entities are allowed, then the mention chains are added to the entity with a sensitivity to whether they are <code class="literal">xdc</code> phrases or not:</p><div class="informalexample"><pre class="programlisting">private void addMentionChainToEntity(TTMentionChain chain, 
                Entity entity) {
    for (String phrase : chain.normalPhrases()) {
             mEntityUniverse.addPhraseToEntity(normalPhrase,
                entity);
        }
    }</pre></div><p>The only change that adding a mention chain can add to an entity is the addition of a new phrase. The additional phrases are classified for whether they are <code class="literal">xdc</code> or not in the same way as was done in the promotion of a mention chain.</p><p>At this point, we have gone over the basics of how mention chains from documents are either promoted to speculative entities or are merged with existing entities in <code class="literal">EntityUniverse</code>. Next, we will take a look at how resolution occurs in <code class="literal">XDocCoref.resolveMentionChain()</code>.</p></div><div class="section" title="The XDocCoref.resolveMentionChain() entity"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec12"/>The XDocCoref.resolveMentionChain() entity</h4></div></div></div><p>The <code class="literal">XDocCoref.resolveMentionChain()</code> method assembles a covering set of entities that can possibly <a class="indexterm" id="id661"/>match the mention chain being resolved and then attempt to find a unique entity via a call to <code class="literal">XDocCoref.resolveCandates()</code>:</p><div class="informalexample"><pre class="programlisting">private void resolveMentionChain(TTMentionChain chain, Map&lt;MentionChain,Entity&gt; chainToEntity, ObjectToSet&lt;Entity,MentionChain&gt; entityToChainSet) {
        if (Tracker.DEBUG)
            System.out.println("XDOC: resolving mention chain " 
          + chain);
        int maxLengthAliasOnMentionChain = 0;
        int maxLengthAliasResolvedToEntityFromMentionChain = -1;
        Set&lt;String&gt; tokens = new HashSet&lt;String&gt;();
        Set&lt;Entity&gt; candidateEntities = new HashSet&lt;Entity&gt;();
        for (String phrase : chain.normalPhrases()) {
        String[] phraseTokens = mEntityUniverse.normalTokens(phrase);
         String normalPhrase 
      = mEntityUniverse.concatenateNormalTokens(phraseTokens);
         for (int i = 0; i &lt; phraseTokens.length; ++i) {
                    tokens.add(phraseTokens[i]);
    }
         int length = phraseTokens.length;       
         if (length &gt; maxLengthAliasOnMentionChain) {
                maxLengthAliasOnMentionChain = length;
        }
         Set&lt;Entity&gt; matchingEntities
           = mEntityUniverse.xdcEntitiesWithPhrase(phrase);
         for (Entity entity : matchingEntities) {
           if (null != TTMatchers.unifyEntityTypes(
            chain.entityType(),
            entity.type())) {
               if (maxLengthAliasResolvedToEntityFromMentionChain &lt; length) 
                        maxLengthAliasResolvedToEntityFromMentionChain = length;
  candidateEntities.add(entity);
}
}
}   
resolveCandidates(chain,
                  tokens,
                  candidateEntities,
                          maxLengthAliasResolvedToEntityFromMentionChain == maxLengthAliasOnMentionChain,
                          chainToEntity,
                          entityToChainSet);}</pre></div><p>The code assembles a set of entities by doing a lookup into the entity universe with <code class="literal">EntityUniverse.xdcEntitiesWithPhrase()</code>. All aliases for the mention chain are tried without consideration of whether they are good XDoc aliases. Before the entities are added<a class="indexterm" id="id662"/> to <code class="literal">candidateEntities</code>, the type returned must be consistent with the type of the mention chain as determined by <code class="literal">TTMatchers.unifyEntityTypes</code>. This way, <code class="literal">Washington</code>, a location is not resolved to <code class="literal">Washington</code>, a person. A bit of record keeping is done to determine whether the longest alias on the mention chain has matched an entity.</p></div><div class="section" title="The resolveCandidates() method"><div class="titlepage"><div><div><h4 class="title"><a id="ch07lvl4sec13"/>The resolveCandidates() method</h4></div></div></div><p>The <code class="literal">resolveCandidates()</code> method captures a key assumption that holds both for within-document <a class="indexterm" id="id663"/>and XDoc coreferences—this unambiguous reference is the only basis of resolution. In the within-document case, an example where humans have this problem is the sentence, <code class="literal">Bob and Joe were working together. He fell into the threshing machine.</code> Who is <code class="literal">he</code> referring to? The linguistic expectation that a singular referring term have a unique antecedent is called a uniqueness presupposition. An example XDoc case is as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Doc1</strong></span>: John Smith is a character from Pocohontas</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Doc2</strong></span>: John Smith is the chairman or GM</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Doc3</strong></span>: John Smith is admired</li></ul></div><p>Which <code class="literal">John Smith</code> does the <code class="literal">John Smith</code> from Doc3 go with? Perhaps, neither. The algorithm in this software requires that there should be a single possible entity that survives the matching criteria. If there is more than one or zero, then a new entity is created. The implementation is as follows:</p><div class="informalexample"><pre class="programlisting">        private void resolveCandidates(TTMentionChain chain,
                                   Set&lt;String&gt; tokens,
                                   Set&lt;Entity&gt; candidateEntities,
                               boolean resolvedAtMaxLength,
                               Map&lt;MentionChain,Entity&gt; chainToEntity,
                               ObjectToSet&lt;Entity,MentionChain&gt; entityToChainSet) {
        filterCandidates(chain,tokens,candidateEntities,resolvedAtMaxLength);
        if (candidateEntities.size() == 0)
            return;
        if (candidateEntities.size() == 1) {
            Entity entity = Collections.&lt;Entity&gt;getFirst(candidateEntities);
            chainToEntity.put(chain,entity);
            entityToChainSet.addMember(entity,chain);
            return;
        }
        // BLOWN Uniqueness Presupposition; candidateEntities.size() &gt; 1
        if (Tracker.DEBUG)
            System.out.println("Blown UP; candidateEntities.size()=" + candidateEntities.size());
    }</pre></div><p>The <code class="literal">filterCandidates</code> method<a class="indexterm" id="id664"/> eliminates all the candidate entities that fail for various semantic reasons. Coreference with an entity in the entity universe only happens if there is a single possible solution. There is not a distinction between too many candidate entities (more than one) or too few (zero). In a more advanced system, one could try and further disambiguate if there are too many entities via <code class="literal">context</code>.</p><p>This is the heart of the XDoc code. The rest of the code marks up the document with entity-universe-relevant indices as returned by the <code class="literal">xdocCoref</code> method, which we just covered:</p><div class="informalexample"><pre class="programlisting">Entity[] entities  = mXDocCoref.xdocCoref(chains);</pre></div><p>The following <code class="literal">for</code> loop iterates over the mention chains, which are aligned with <code class="literal">Entities[]</code> returned by <code class="literal">xdocCoref</code>. For each mention chain, the mention is mapped to its cross-document entity:</p><div class="informalexample"><pre class="programlisting">Map&lt;Mention,Entity&gt; mentionToEntityMap
     = new HashMap&lt;Mention,Entity&gt;();
for (int i = 0; i &lt; chains.length; ++i){ 
  for (Mention mention : chains[i].mentions()) {
         mentionToEntityMap.put(mention,entities[i]);
  }
}</pre></div><p>Next, the code will set up a bunch of mappings to create chunks that reflect the entity universe IDs:</p><div class="informalexample"><pre class="programlisting">String[] sentenceTexts
        = sentenceTextList
            .&lt;String&gt;toArray(new String[sentenceTextList.size()])
Mention[][] sentenceMentions
            = sentenceMentionList
            .&lt;Mention[]&gt;toArray(new Mention[sentenceMentionList.size()][]);
int[][] mentionStarts
         = mentionStartList
            .&lt;int[]&gt;toArray(new int[mentionStartList.size()][]);

int[][] mentionEnds
            = mentionEndList
            .&lt;int[]&gt;toArray(new int[mentionEndList.size()][]);</pre></div><p>The actual creation of the chunks happens next:</p><div class="informalexample"><pre class="programlisting">Chunking[] chunkings = new Chunking[sentenceTexts.length];
  for (int i = 0; i &lt; chunkings.length; ++i) {
   ChunkingImpl chunking = new ChunkingImpl(sentenceTexts[i]);
   chunkings[i] = chunking;
   for (int j = 0; j &lt; sentenceMentions[i].length; ++j) {
    Mention mention = sentenceMentions[i][j];
    Entity entity = mentionToEntityMap.get(mention);
    if (entity == null) {
     Chunk chunk = ChunkFactory.createChunk(mentionStarts[i][j],
       mentionEnds[i][j],
       mention.entityType()
       + ":-1");
     //chunking.add(chunk); //uncomment to get unresolved ents as -1 indexed.
    } else {
     Chunk chunk = ChunkFactory.createChunk(mentionStarts[i][j],
       mentionEnds[i][j],
       entity.type()
       + ":" + entity.id());
     chunking.add(chunk);
    }
   }
  }</pre></div><p>The chunkings are then used to create the relevant portions of the document, and <code class="literal">OutputDocument</code> is <a class="indexterm" id="id665"/>returned:</p><div class="informalexample"><pre class="programlisting">        // needless allocation here and last, but simple
        Chunking[] titleChunkings = new Chunking[firstContentSentenceIndex];
        for (int i = 0; i &lt; titleChunkings.length; ++i)
            titleChunkings[i] = chunkings[i];

        Chunking[] bodyChunkings = new Chunking[chunkings.length - firstContentSentenceIndex];
        for (int i = 0; i &lt; bodyChunkings.length; ++i)
            bodyChunkings[i] = chunkings[firstContentSentenceIndex+i];

        String id = document.id();

        OutputDocument result = new OutputDocument(id,titleChunkings,bodyChunkings);
        return result;
    }</pre></div><p>So, this is what we have to offer as a starting place for XDoc coreference. Hopefully, we have explained the intentions behind the more opaque methods. Good luck!</p></div></div></div></div>
<div class="section" title="The John Smith problem"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec84"/>The John Smith problem</h1></div></div></div><p>Different people, locations, and concepts can have the same orthographic representation but be distinct. There are multiple instances of "John Smith", "Paris", and "bank" in the world, and a proper cross-document coreference system should be able to handle it. For the case of concepts such as "bank" (a river bank versus a financial bank), the term of art is word-sense disambiguation. This recipe will demonstrate one approach to the problem that Baldwin developed <a class="indexterm" id="id666"/>back in the day with Amit Bagga for person disambiguation.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec192"/>Getting ready</h2></div></div></div><p>The code for this recipe closely follows the <a class="indexterm" id="id667"/>clustering tutorial at <a class="ulink" href="http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html</a> but changes it to more closely fit the original Bagga-Baldwin work. There is a fair amount of code but nothing very complicated. The source is in <code class="literal">src/com/lingpipe/cookbook/chapter7/JohnSmith.java</code>.</p><p>The class starts with the standard panoply of NLP tools for tokenization, sentence detection, and named-entity detection. Refer to the previous recipes if this stack is unfamiliar:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) 
      throws ClassNotFoundException, IOException {
    TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
    SentenceModel sentenceModel
    = new IndoEuropeanSentenceModel();
    SENTENCE_CHUNKER 
    = new SentenceChunker(tokenizerFactory,sentenceModel);
    File modelFile
    = new File("models/ne-en-news-muc6.AbstractCharLmRescoringChunker");
    NAMED_ENTITY_CHUNKER 
    = (Chunker) AbstractExternalizable.readObject(modelFile);</pre></div><p>Next up, we will revisit <code class="literal">TfIdfDistance</code>. However, the task requires that we wrap the class to operate over <code class="literal">Documents</code> rather than <code class="literal">CharSequences</code>, because we would like to retain the filename and be<a class="indexterm" id="id668"/> able to manipulate what text is used for the calculations to come:</p><div class="informalexample"><pre class="programlisting">TfIdfDocumentDistance tfIdfDist = new TfIdfDocumentDistance(tokenizerFactory);</pre></div><p>Dropping to the referenced class, we have the following code:</p><div class="informalexample"><pre class="programlisting">public class TfIdfDocumentDistance implements Distance&lt;Document&gt; {
  TfIdfDistance mTfIdfDistance;
  public TfIdfDocumentDistance (TokenizerFactory tokenizerFactory) {
  mTfIdfDistance = new TfIdfDistance(tokenizerFactory);
  }
        
   public void train(CharSequence text) {
      mTfIdfDistance.handle(text);
   }

  @Override
  public double distance(Document doc1, Document doc2) {
    return mTfIdfDistance.distance(doc1.mCoreferentText,
              doc2.mCoreferentText);
  }
  
}</pre></div><p>The <code class="literal">train</code> method interfaces with the <code class="literal">TfIdfDistance.handle()</code> method and provides an implementation of a <code class="literal">distance(Document doc1, Document doc2)</code> method that will drive the clustering code discussed below. All that the <code class="literal">train</code> method does is pull out the relevant text and hand it off to the <code class="literal">TfIdfDistance</code> class for the relevant value.</p><p>The reference class, <code class="literal">Document</code>, is an inner class in <code class="literal">JohnSmith</code>, and it is quite simple. It gets sentences that have entities which match the <code class="literal">.*John Smith.*</code> pattern and puts them in the <code class="literal">mCoreferentText</code> variable:</p><div class="informalexample"><pre class="programlisting">static class Document {
        final File mFile;
        final CharSequence mText; 
        final CharSequence mCoreferentText;
        Document(File file) throws IOException {
            mFile = file; // includes name
            mText = Files.readFromFile(file,Strings.UTF8);
            Set&lt;String&gt; coreferentSents 
      = getCoreferentSents(".*John "                        + "Smith.*",mText.toString());
            StringBuilder sb = new StringBuilder();
            for (String sentence : coreferentSents) {
              sb.append(sentence);
            }
            mCoreferentText = sb.toString();
        }

        public String toString() {
            return mFile.getParentFile().getName() + "/"  
            + mFile.getName();
        }
    }</pre></div><p>Going deeper into the <a class="indexterm" id="id669"/>code, we will now visit the <code class="literal">getCoreferentSents()</code> method:</p><div class="informalexample"><pre class="programlisting">static final Set&lt;String&gt; getCoreferentSents(String targetPhrase, String text) {
     Chunking sentenceChunking
    = SENTENCE_CHUNKER.chunk(text);
  Iterator&lt;Chunk&gt; sentenceIt 
    = sentenceChunking.chunkSet().iterator();
  int targetId = -2;
  MentionFactory mentionFactory = new EnglishMentionFactory();
  WithinDocCoref coref = new WithinDocCoref(mentionFactory);
  Set&lt;String&gt; matchingSentenceAccumulator 
  = new HashSet&lt;String&gt;();
for (int sentenceNum = 0; sentenceIt.hasNext(); ++sentenceNum) {
  Chunk sentenceChunk = sentenceIt.next();
  String sentenceText 
    = text.substring(sentenceChunk.start(),
          sentenceChunk.end());
  Chunking neChunking
    = NAMED_ENTITY_CHUNKER.chunk(sentenceText);
  Set&lt;Chunk&gt; chunkSet 
    = new TreeSet&lt;Chunk&gt;(Chunk.TEXT_ORDER_COMPARATOR);
  chunkSet.addAll(neChunking.chunkSet());      Coreference.addRegexMatchingChunks(
    Pattern.compile("\\bJohn Smith\\b"),
            "PERSON",sentenceText,chunkSet);
  Iterator&lt;Chunk&gt; neChunkIt = chunkSet.iterator();
  while (neChunkIt.hasNext()) {
    Chunk neChunk = neChunkIt.next();
    String mentionText
        = sentenceText.substring(neChunk.start(),
            neChunk.end());
    String mentionType = neChunk.type();
    Mention mention 
    = mentionFactory.create(mentionText,mentionType);
    int mentionId 
    = coref.resolveMention(mention,sentenceNum);
    if (targetId == -2 &amp;&amp; mentionText.matches(targetPhrase)) {
    targetId = mentionId;
    }
    if (mentionId == targetId) {                          matchingSentenceAccumulator.add(sentenceText);
     System.out.println("Adding " + sentenceText);      
     System.out.println("     mention text=" + mentionText
            + " type=" + mentionType
            + " id=" + mentionId);
     }
  }
}
if (targetId == -2) {
  System.out.println("!!!Missed target doc " + text);
}
return matchingSentenceAccumulator;
}</pre></div><p>Look at the <span class="emphasis"><em>Cross-document coreference</em></span> recipe for most of the moving parts of the preceding method. We<a class="indexterm" id="id670"/> will call out a few notable bits. We are cheating in some sense by using a regular expression chunker to find any string that has as a <code class="literal">John Smith</code> substring and adding it in as a <code class="literal">PERSON</code> entity. Like most kinds of cheating, this is quite useful if your sole purpose in life is tracking <code class="literal">John Smith</code>. The cheating we did in reality was to use dictionary matching to find all variations of high-value intelligence targets such as <code class="literal">Osama bin Laden</code>. In the end, we had over 40 versions of his name scouring openly available news sources as a part of the MiTAP project.</p><p>Further, as each sentence is processed, we will check all the mentions for a matching pattern for <code class="literal">John Smith</code>, and if so, we will collect any sentence that has a mention of this ID. This means that a sentence that refers back to <code class="literal">John Smith</code> with a pronoun will be included, as will the <code class="literal">Mr. Smith</code> cases if coreference is doing its job. Note that we need to see a match for <code class="literal">John Smith</code> before we start collecting contextual information, so we will miss the first sentence of <code class="literal">He awoke. John Smith was a giant cockroach</code>. Also note that if a second <code class="literal">John Smith</code> shows up with a different ID, it will be ignored—this can happen.</p><p>Finally, note that there is some error checking, in that if <code class="literal">John Smith</code> is not found, then an error is reported to <code class="literal">System.out</code>.</p><p>If we pop back to mundane I/O slinging in our <code class="literal">main()</code> method after setting up <code class="literal">TfIdfDocumentDistance</code>, we would have:</p><div class="informalexample"><pre class="programlisting">File dir = new File(args[0]);
       Set&lt;Set&lt;Document&gt;&gt; referencePartition
            = new HashSet&lt;Set&lt;Document&gt;&gt;();
        for (File catDir : dir.listFiles()) {
            System.out.println("Category from file=" + catDir);
            Set&lt;Document&gt; docsForCat = new HashSet&lt;Document&gt;();
            referencePartition.add(docsForCat);
            for (File file : catDir.listFiles()) {
                Document doc = new Document(file);
                tfIdfDist.train(doc.mText);
                docsForCat.add(doc);
            }
        }</pre></div><p>We have not <a class="indexterm" id="id671"/>discussed this, but the truth annotation of which document references which <code class="literal">Mr. Smith</code> is encoded in the directory structure of the data. Each subdirectory in the top <code class="literal">johnSmith</code> directory is treated as the truth cluster. So, <code class="literal">referencePartition</code> contains the truth. We could have wrapped this as a classification problem with each subdirectory, the correct classification. We will leave it as an exercise to you to stuff this into a cross-validating corpus with a logistic regression solution.</p><p>Moving on, we will construct the test set by flattening our previous categories into a single set of <code class="literal">Documents</code>. We could have done this in the previous step, but mixing tasks tends to produce bugs, and the extra <code class="literal">for</code> loop does very little damage to the execution speed:</p><div class="informalexample"><pre class="programlisting">        Set&lt;Document&gt; docSet = new HashSet&lt;Document&gt;();
        for (Set&lt;Document&gt; cluster : referencePartition) {
            docSet.addAll(cluster);
        }</pre></div><p>Next, we will tee up the clustering algorithms. We will do both <code class="literal">CompleteLink</code> and <code class="literal">SingleLink</code> driven by <code class="literal">TfIdfDocumentDistance</code> that runs the show:</p><div class="informalexample"><pre class="programlisting">        
        HierarchicalClusterer&lt;Document&gt; clClusterer
            = new CompleteLinkClusterer&lt;Document&gt;(tfIdfDist);
        Dendrogram&lt;Document&gt; completeLinkDendrogram
            = clClusterer.hierarchicalCluster(docSet);

        HierarchicalClusterer&lt;Document&gt; slClusterer
            = new SingleLinkClusterer&lt;Document&gt;(tfIdfDist);
        Dendrogram&lt;Document&gt; singleLinkDendrogram
            = slClusterer.hierarchicalCluster(docSet);</pre></div><p>The details of the clustering algorithms are covered in <a class="link" href="ch05.html" title="Chapter 5. Finding Spans in Text – Chunking">Chapter 5</a>, <span class="emphasis"><em>Finding Spans in Texts – Chunking</em></span>. Now, we will report performance based on the number of clusters varied from <code class="literal">1</code> to the number of<a class="indexterm" id="id672"/> inputs. The one fancy bit is that the <code class="literal">Cross</code> category uses <code class="literal">SingleLinkClusterer</code> as the reference and <code class="literal">CompleteLinkClusterer</code> as the response:</p><div class="informalexample"><pre class="programlisting">System.out.println();
System.out.println(" -------------------------------------------"
        + "-------------");
System.out.println("|  K  |  Complete      |  Single        | "
        + " Cross         |");
System.out.println("|     |  P    R    F   |  P    R    F   |  P"
        + "     R    F   |");
System.out.println(" -------------------------------------------"
        +"-------------");
for (int k = 1; k &lt;= docSet.size(); ++k) {
   Set&lt;Set&lt;Document&gt;&gt; clResponsePartition
       = completeLinkDendrogram.partitionK(k);
   Set&lt;Set&lt;Document&gt;&gt; slResponsePartition
       = singleLinkDendrogram.partitionK(k);

   ClusterScore&lt;Document&gt; scoreCL
       = new ClusterScore&lt;Document&gt;(referencePartition,
                                    clResponsePartition) PrecisionRecallEvaluation clPrEval 
      = scoreCL.equivalenceEvaluation();
   ClusterScore&lt;Document&gt; scoreSL
       = new ClusterScore&lt;Document&gt;(referencePartition,
                                     slResponsePartition);
PrecisionRecallEvaluation slPrEval 
  = scoreSL.equivalenceEvaluation();

ClusterScore&lt;Document&gt; scoreX
    = new ClusterScore&lt;Document&gt;(clResponsePartition
                                 slResponsePartition);
PrecisionRecallEvaluation xPrEval 
  = scoreX.equivalenceEvaluation();

System.out.printf("| %3d | %3.2f %3.2f %3.2f | %3.2f %3.2f %3.2f" 
      + " | %3.2f %3.2f %3.2f |\n",
                   k,
                   clPrEval.precision(),
                   clPrEval.recall(),
                   clPrEval.fMeasure(),
                   slPrEval.precision(),
                   slPrEval.recall(),
                   slPrEval.fMeasure(),
                   xPrEval.precision(),
                   xPrEval.recall(),
                   xPrEval.fMeasure());
 }
System.out.println(" --------------------------------------------"
         + "------------");
}</pre></div><p>That's all that we<a class="indexterm" id="id673"/> need to do to get ready for this recipe. This is a rare phenomenon to be computed, and this is a toy implementation, but the key concepts should be evident.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec193"/>How to do it...</h2></div></div></div><p>We will just run this code and then mess with it a bit:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Get yourself to a terminal and type:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter7.JohnSmith</strong></span>
</pre></div></li><li class="listitem">The result will be piles of information that indicate what sentences are being extracted for use in the clustering—remember that the truth annotation is determined by the directory that the files are in. The first cluster is <code class="literal">0</code>:<div class="informalexample"><pre class="programlisting">Category from file=data/johnSmith/0</pre></div></li><li class="listitem">The code reports sentences that contain references to <code class="literal">John Smith</code>:<div class="informalexample"><pre class="programlisting">Adding I thought John Smith marries Pocahontas.''
     mention text=John Smith type=PERSON id=5
Adding He's bullets , she's arrows.''
     mention text=He type=MALE_PRONOUN id=5</pre></div></li><li class="listitem">The pronominal reference to <code class="literal">John Smith</code> is the basis of inclusion of the second sentence.</li><li class="listitem">The system output goes on, and finally, we will get the results for a single-link clustering against the truth and a complete link against the truth. The <code class="literal">K</code> column indicates how many clusters the algorithm was allowed with precision, recall, and F-measure reported. The first row is in this case that there is only one cluster that will allow for 100 percent recall and 23 percent precision for both complete and single links. Looking down at the scores, we can see that the complete link reports the best F-measure with 11 clusters at <code class="literal">0.60</code>—in truth, there are 35 clusters. The single-link approach maxes out F-measure at 68 clusters with <code class="literal">0.78</code> and shows much greater robustness on varying numbers of clusters. The cross case shows that single<a class="indexterm" id="id674"/> link and complete link are quite different in direct comparison as well. Note that some <code class="literal">K</code> values have been eliminated for readability:<div class="informalexample"><pre class="programlisting">--------------------------------------------------------
|  K  |  Complete      |  Single        
|     |  P    R    F   |  P    R    F   
 --------------------------------------------------------
|   1 | 0.23 1.00 0.38 | 0.23 1.00 0.38 
|   2 | 0.28 0.64 0.39 | 0.24 1.00 0.38 
|   3 | 0.29 0.64 0.40 | 0.24 1.00 0.39 
|   4 | 0.30 0.64 0.41 | 0.24 1.00 0.39 
|   5 | 0.44 0.63 0.52 | 0.24 0.99 0.39 
|   6 | 0.45 0.63 0.52 | 0.25 0.99 0.39 
|   7 | 0.45 0.63 0.52 | 0.25 0.99 0.40 
|   8 | 0.49 0.62 0.55 | 0.25 0.99 0.40 
|   9 | 0.55 0.61 0.58 | 0.25 0.99 0.40 
|  10 | 0.55 0.61 0.58 | 0.25 0.99 0.41 
|  11 | 0.59 0.61 0.60 | 0.27 0.99 0.42 
|  12 | 0.59 0.61 0.60 | 0.27 0.98 0.42 
|  13 | 0.56 0.41 0.48 | 0.27 0.98 0.43 
|  14 | 0.71 0.41 0.52 | 0.27 0.98 0.43 
|  15 | 0.71 0.41 0.52 | 0.28 0.98 0.43 
|  16 | 0.68 0.34 0.46 | 0.28 0.98 0.44 
|  17 | 0.68 0.34 0.46 | 0.28 0.98 0.44 
|  18 | 0.69 0.34 0.46 | 0.29 0.98 0.44 
|  19 | 0.67 0.32 0.43 | 0.29 0.98 0.45 
|  20 | 0.69 0.29 0.41 | 0.29 0.98 0.45 
|  30 | 0.84 0.22 0.35 | 0.33 0.96 0.49 
|  40 | 0.88 0.18 0.30 | 0.61 0.88 0.72 
|  50 | 0.89 0.16 0.28 | 0.64 0.86 0.73 
|  60 | 0.91 0.14 0.24 | 0.66 0.77 0.71 
|  61 | 0.91 0.14 0.24 | 0.66 0.75 0.70 
|  62 | 0.93 0.14 0.24 | 0.87 0.75 0.81 
|  63 | 0.94 0.13 0.23 | 0.87 0.69 0.77 
|  64 | 0.94 0.13 0.23 | 0.87 0.69 0.77 
|  65 | 0.94 0.13 0.23 | 0.87 0.68 0.77 
|  66 | 0.94 0.13 0.23 | 0.87 0.66 0.75 
|  67 | 0.95 0.13 0.23 | 0.87 0.66 0.75 
|  68 | 0.95 0.13 0.22 | 0.95 0.66 0.78 
|  69 | 0.94 0.11 0.20 | 0.95 0.66 0.78 
|  70 | 0.94 0.11 0.20 | 0.95 0.65 0.77 
|  80 | 0.98 0.11 0.19 | 0.97 0.43 0.59 
|  90 | 0.99 0.10 0.17 | 0.97 0.30 0.46 
| 100 | 0.99 0.08 0.16 | 0.96 0.20 0.34 
| 110 | 0.99 0.07 0.14 | 1.00 0.11 0.19 
| 120 | 1.00 0.07 0.12 | 1.00 0.08 0.14 
| 130 | 1.00 0.06 0.11 | 1.00 0.06 0.12 
| 140 | 1.00 0.05 0.09 | 1.00 0.05 0.10 
| 150 | 1.00 0.04 0.08 | 1.00 0.04 0.08 
| 160 | 1.00 0.04 0.07 | 1.00 0.04 0.07 
| 170 | 1.00 0.03 0.07 | 1.00 0.03 0.07 
| 180 | 1.00 0.03 0.06 | 1.00 0.03 0.06 
| 190 | 1.00 0.02 0.05 | 1.00 0.02 0.05 
| 197 | 1.00 0.02 0.04 | 1.00 0.02 0.04 
 --------------------------------------------------------</pre></div></li><li class="listitem">The following <a class="indexterm" id="id675"/>output constrains clustering not by cluster size but by the max distance threshold. The output is for the single-link cluster with <code class="literal">.05</code> increases the distance and the evaluation is the B-cubed metric. The output is the distance, precision, recall, and the size of the resulting cluster. The performance at <code class="literal">.80</code> and <code class="literal">.9</code> is quite good, but beware of setting production thresholds in this after the fact fashion. In a production environment, we will want to see much more data before setting the threshold:<div class="informalexample"><pre class="programlisting">B-cubed eval
Dist: 0.00 P: 1.00 R: 0.77 size:189
Dist: 0.05 P: 1.00 R: 0.80 size:171
Dist: 0.10 P: 1.00 R: 0.80 size:164
Dist: 0.15 P: 1.00 R: 0.81 size:157
Dist: 0.20 P: 1.00 R: 0.81 size:153
Dist: 0.25 P: 1.00 R: 0.82 size:148
Dist: 0.30 P: 1.00 R: 0.82 size:144
Dist: 0.35 P: 1.00 R: 0.83 size:142
Dist: 0.40 P: 1.00 R: 0.83 size:141
Dist: 0.45 P: 1.00 R: 0.83 size:141
Dist: 0.50 P: 1.00 R: 0.83 size:138
Dist: 0.55 P: 1.00 R: 0.83 size:136
Dist: 0.60 P: 1.00 R: 0.84 size:128
Dist: 0.65 P: 1.00 R: 0.84 size:119
Dist: 0.70 P: 1.00 R: 0.86 size:108
Dist: 0.75 P: 0.99 R: 0.88 size: 90
Dist: 0.80 P: 0.99 R: 0.94 size: 60
Dist: 0.85 P: 0.95 R: 0.97 size: 26
Dist: 0.90 P: 0.91 R: 0.99 size:  8
Dist: 0.95 P: 0.23 R: 1.00 size:  1
Dist: 1.00 P: 0.23 R: 1.00 size:  1</pre></div></li><li class="listitem">The <a class="indexterm" id="id676"/>B-cubed (Bagga, Bierman, and Baldwin) evaluation was created to heavily penalize pushing large clusters together. It assumes that it is more of a problem to push lots of documents about George W. Bush together with George H. W. Bush, both large clusters, than to mistake George Bush, the mechanic who got mentioned once in the dataset. Other scoring metrics will count both the mistakes as equally bad. It is the standard scoring metric used in the literature for this phenomenon.</li></ol></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec194"/>See also</h2></div></div></div><p>There is a fair amount of work in the research literature on this exact problem. We were not the first ones to think about this, but we came up with the dominant evaluation metric, and we released a corpus for other groups to compare themselves with us and each other. Our contribution is <span class="emphasis"><em>Entity-based cross-document coreferencing using the Vector Space Model</em></span> by Bagga and Baldwin in <span class="emphasis"><em>ACL '98 Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</em></span>. There has been much progress since—there are more than 400 citations to this model on Google Scholar; they are worth a look if this problem is of importance to you.</p></div></div></body></html>