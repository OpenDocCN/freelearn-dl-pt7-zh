<html><head></head><body>
		<div id="_idContainer197">
			<h1 id="_idParaDest-178" class="chapter-nu ber"><a id="_idTextAnchor198"/>9</h1>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor199"/>Model Explainability</h1>
			<p class="author-quote">“If you can’t explain it simply, you don’t understand it well enough.”</p>
			<p class="author-quote">– Albert Einstein</p>
			<p><strong class="bold">Model explainability</strong> is an important topic in the fields of <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) and <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>). It refers to the ability to understand<a id="_idIndexMarker1127"/> and explain how a model makes predictions and decisions. Explainability is important because it allows us to identify potential biases or errors in a model, and it can improve the performance and trustworthiness of <span class="No-Break">AI models.</span></p>
			<p>In this chapter, we will explore different methods and techniques for explaining and interpreting ML models. We will also examine the challenges and limitations of model explainability and will consider potential solutions to improve the interpretability of <span class="No-Break">ML algorithms.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Introduction to <span class="No-Break">Explainable AI</span></li>
				<li><strong class="bold">Explain Like I’m </strong><span class="No-Break"><strong class="bold">Five</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ELI5</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Local Interpretable Model-Agnostic </strong><span class="No-Break"><strong class="bold">Explanations</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">LIME</strong></span><span class="No-Break">)</span></li>
				<li>Understanding churn modeling using <span class="No-Break">XAI techniques</span></li>
				<li><span class="No-Break">CausalNex</span></li>
				<li>DoWhy for <span class="No-Break">causal inference</span></li>
				<li>AI Explainability 360 for <span class="No-Break">interpreting models</span></li>
			</ul>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor200"/>Technical requirements</h1>
			<p>This chapter requires you to have Python 3.8 along with the following <span class="No-Break">Python packages:</span></p>
			<ul>
				<li><span class="No-Break">NumPy</span></li>
				<li><span class="No-Break">Matplotlib</span></li>
				<li><span class="No-Break">Scikit-learn</span></li>
				<li><span class="No-Break">pandas</span></li>
				<li>Install ELI5 <span class="No-Break">as follows:</span><pre class="console">
<strong class="bold">pip install eli5</strong></pre></li>
				<li>To install LIME, use <span class="No-Break">the following:</span><pre class="console">
<strong class="bold">pip install lime</strong></pre></li>
				<li>SHAP can be installed <span class="No-Break">using this:</span><pre class="console">
<strong class="bold">pip install shap</strong></pre></li>
				<li>Install DoWhy using <span class="No-Break">the following:</span><pre class="console">
<strong class="bold">pip install dowhy</strong></pre></li>
				<li>AI <span class="No-Break">Explainability 360</span></li>
			</ul>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor201"/>Introduction to Explainable AI</h1>
			<p>Consider a model that predicts<a id="_idIndexMarker1128"/> whether a patient is likely to develop a certain terminal disease, a model that assists in making decisions about whether the person on trial is guilty or not, and a model that assists banks to determine whether to give someone a loan or not. Each of these models makes decisions that can have a profound domino effect on multiple lives (unlike a model used by Netflix that recommends movies to watch). Therefore, it is important that institutions with models employed in decision-making processes can explain the reasoning behind their predictions<a id="_idIndexMarker1129"/> and decisions. Model explainability, or <strong class="bold">Explainable AI</strong> (<strong class="bold">XAI</strong>), deals with developing algorithms and techniques that allow us to understand and interpret the reasoning behind a model’s predictions and decisions. As we have seen in the preceding examples, XAI is especially important in domains such as healthcare, finance, and criminal justice, as the consequences of model decisions can have a significant impact on individuals <span class="No-Break">and society.</span></p>
			<p>Currently, many ML models are considered <strong class="bold">black boxes</strong> due to their complex inner workings and lack<a id="_idIndexMarker1130"/> of transparency. This can lead to concerns about accountability and bias and can hinder stakeholders’ adoption of and trust in these models. To address these issues, there is a growing need for methods and techniques that can provide explainability and interpretability for <span class="No-Break">ML models.</span></p>
			<p>XAI is an ML interpretability<a id="_idIndexMarker1131"/> technique that focuses on understanding the predictions made by an ML model and explaining how those predictions were reached in a way that’s understandable for humans in order to build trust in the model. As the name suggests, XAI broadly focuses on model explanations and provides interfaces for deciphering these explanations to bridge ML and human systems efficiently. It is a key component of broader human-centric responsible <span class="No-Break">AI practices.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1</em> shows the different fields that interact <a id="_idIndexMarker1132"/> to build a human-centric <span class="No-Break">XAI system:</span></p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/Figure_9.01_B18681.jpg" alt="Figure 9.1 – A human-centric XAI system"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – A human-centric XAI system</p>
			<p>The development of XAI has accelerated alongside that of applied AI/<strong class="bold">Deep Learning</strong> (<strong class="bold">DL</strong>) systems from<a id="_idIndexMarker1133"/> roughly 2015. Progressively complicated DL models, such as deep neural networks, have revealed new explainability challenges for both developers and model stakeholders. When it comes to the implementation of AI, not even engineers or data scientists have a clear picture of what’s going on behind the scenes because of the characteristic black-box nature of neural networks. As a result, it becomes difficult to understand a model’s predictions and trust them in <span class="No-Break">high-stakes situations.</span></p>
			<p>XAI is a solution<a id="_idIndexMarker1134"/> when we want to determine whether we can trust the result of our AI model and how confident we should be that the model is correct. It allows us to understand how the AI model came up with its result in a certain situation and, consequently, builds trust in <span class="No-Break">those outcomes.</span></p>
			<p>To achieve a reliable XAI system, we should focus on three main components: prediction accuracy, traceability, and decision understanding, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/Figure_9.02_B18681.jpg" alt="Figure 9.2 – Three main components of an XAI system"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Three main components of an XAI system</p>
			<p>Let’s discuss each of <span class="No-Break">these components:</span></p>
			<ul>
				<li><strong class="bold">Prediction accuracy</strong> addresses technology requirements<a id="_idIndexMarker1135"/> and is undoubtedly a vital component in the success of AI usage in day-to-day operations. Prediction accuracy can be determined by comparing XAI output to the results in the training/test dataset. For example, we can use algorithms such as LIME to explain predictions made by <span class="No-Break">AI/DL/ML classifiers.</span></li>
				<li><strong class="bold">Decision understanding</strong> addresses human requirements and is all about<a id="_idIndexMarker1136"/> educating and informing teams in order to overcome AI distrust and help them to understand how decisions were made. This information can be presented to end users in the form of a dashboard displaying the primary factors in the making of a certain decision and the extent to which each of those factors influenced <span class="No-Break">the decision.</span></li>
				<li><strong class="bold">Traceability</strong> can impact and limit decision-making, setting<a id="_idIndexMarker1137"/> up a narrower scope for ML rules and features. Techniques such as <strong class="bold">Deep Learning Important</strong> <strong class="bold">FeaTures</strong> (<strong class="bold">DeepLIFT</strong>) can be utilized to compare the activation<a id="_idIndexMarker1138"/> of each neuron in a neural network to its reference neuron by displaying traceability links <span class="No-Break">and dependencies.</span></li>
			</ul>
			<p>With no or little understanding of how AI decisions are derived, the user’s trust in the framework is harmed and it may ultimately lead to model rejection. Consider a system making recommendations to users about connections to add to their network or products to buy. An explanation of these recommendations will act as a catalyst in the adoption of ML systems by making information more significant to <span class="No-Break">the user.</span></p>
			<p>XAI is about more than building<a id="_idIndexMarker1139"/> trust in the model; it is also about troubleshooting and improving the model’s performance. It allows us to investigate model behavior by tracking model insights on deployment status, fairness, quality, and model drift. By using XAI, model performance can be analyzed and alerts can be generated when a model deviates from the intended outcomes and gives a <span class="No-Break">substandard </span><span class="No-Break"><a id="_idIndexMarker1140"/></span><span class="No-Break">performance.</span></p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor202"/>Scope of XAI</h2>
			<p>Having an understanding of models<a id="_idIndexMarker1141"/> is crucial for many tasks involved in building and operating ML systems, and XAI can be used to do <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Hone modeling and data collection processes</strong>: After aggregating and comparing across dataset splits, XAI provides a means to identify and alert users of common ML pitfalls such as data skew <span class="No-Break">and drift.</span></li>
				<li><strong class="bold">Debug model performance</strong>: It allows us to debug unexpected behavior from the model and monitor deeper feature-level insights to inform <span class="No-Break">corrective actions.</span></li>
				<li><strong class="bold">Build trust</strong>: It informs and supports the decision-making process by explaining predictions to build trust with end users, making model decisions equitable <span class="No-Break">and reliable.</span></li>
				<li><strong class="bold">Identify unexpected predictions</strong>: It verifies model behavior and informs on amendatory actions. For instance, it allows regulators to efficiently validate that ML decisions comply with laws to mitigate the risk of generating poor model outcomes for <span class="No-Break">end users.</span></li>
				<li><strong class="bold">Act as a catalyst</strong>: It acts as a catalyst for the adoption of ML systems by building user trust and presenting model outcomes in understandable forms <span class="No-Break">to stakeholders.</span></li>
			</ul>
			<p>In an ideal world, an explainable and transparent model can be used across distinct industries, such as healthcare, by accelerating diagnostics, processing images, streamlining pharmaceutical process approvals, and making critical decisions in manufacturing, finance, logistics, and criminal justice, such as accelerating the conclusions of DNA analysis or prison <span class="No-Break">population analysis.</span></p>
			<p>Explainability can help developers to ensure that the system is working as expected, meets regulatory standards, and even allows the person affected by a decision to challenge that outcome. The ability to understand and explain model predictions allows companies and institutions to make informed decisions, especially where stakeholders require<a id="_idIndexMarker1142"/> justifications, such as in strategic <span class="No-Break">business decisions.</span></p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor203"/>Challenges in XAI</h2>
			<p>Some of the challenging<a id="_idIndexMarker1143"/> dilemmas faced when implementing XAI solutions are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Algorithm confidentiality</strong>: Often, due to security concerns, the details of the model and algorithms used are confidential. In such a scenario, it becomes a challenging task to ensure that the AI system did not learn a biased perspective (or an unbiased perspective of a biased world) as a result of gaps in the training data, objective function, <span class="No-Break">or model.</span></li>
				<li><strong class="bold">Fairness</strong>: It is challenging for XAI to determine whether a decision taken or an output obtained from an AI framework is fair or not, as the impression of fairness is subjective and relies upon the data used to train the AI/ML model. Furthermore, the definition of fairness can change depending on the use case, culture, and <span class="No-Break">so on.</span></li>
				<li><strong class="bold">Reliability</strong>: Without evaluating the process of how the AI system reached a certain outcome, it becomes hard to rely on the system, as clarification is required on whether the system is legitimate <span class="No-Break">or not.</span></li>
			</ul>
			<p>Building a model that can be explained and interpreted easily is the key to overcoming the aforementioned challenges. Highly intricate algorithms can be recreated or replaced with simpler approaches, which are easier to explain with the help <span class="No-Break">of XAI.</span></p>
			<h3>Classification of XAI techniques</h3>
			<p>XAI techniques can be classified<a id="_idIndexMarker1144"/> based on two main criteria—scope and model. Refer to <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/Figure_9.03_B18681.jpg" alt="Figure 9.3 – Classification of XAI techniques"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Classification of XAI techniques</p>
			<p>Let’s discuss <span class="No-Break">each technique:</span></p>
			<ul>
				<li><strong class="bold">Scope</strong>: XAI techniques can be classified based<a id="_idIndexMarker1145"/> on the scope<a id="_idIndexMarker1146"/> of their explanation. There are two main categories of XAI techniques in terms of scope: <strong class="bold">local</strong> and <strong class="bold">global</strong>. Local explanations focus<a id="_idIndexMarker1147"/> on describing the behavior of an AI model for a specific input or output. These explanations provide insights into how the model arrived at a particular decision for a given input and are useful for understanding the model’s behavior<a id="_idIndexMarker1148"/> in specific cases. Global explanations, on the other hand, provide insights into the general behavior of an AI model across a wide range of input and output. These explanations are useful for understanding the overall decision-making process of an AI model and how it handles different types <span class="No-Break">of input.</span></li>
				<li><strong class="bold">Model</strong>: XAI techniques can also be classified <a id="_idIndexMarker1149"/>based on the type of model they are applied to. There are two main categories of XAI techniques in the context of models: <strong class="bold">model-specific</strong> and <strong class="bold">model-agnostic</strong>. Model-specific explanations<a id="_idIndexMarker1150"/> are tailored to the architecture and design of a particular AI model. These explanations are often local in scope, providing insights into the behavior of the model for a specific input or output. Examples of model-specific XAI techniques include feature importance analysis and saliency maps. Model-agnostic explanations, on the other hand, are not<a id="_idIndexMarker1151"/> specific to any particular AI model and can be applied to a wide range of different models. These explanations are often global in scope, providing insights into the overall behavior of an AI model across a wide range of input and output. Examples of model-agnostic XAI techniques include counterfactual analysis and <span class="No-Break">model distillation.</span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.4</em> expands on the differences between model-specific<a id="_idIndexMarker1152"/> and model-agnostic <span class="No-Break">XAI techniques:</span></p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/Figure_9.04_B18681.jpg" alt="Figure 9.4 – Model-specific versus model-agnostic XAI techniques"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Model-specific versus model-agnostic XAI techniques</p>
			<p>Overall, XAI techniques<a id="_idIndexMarker1153"/> can be classified based on their scope (local or global) and how they relate to the model they are applied to (model-specific or model-agnostic). Understanding the different types of XAI techniques and how they can be applied can help practitioners choose the most appropriate technique for a given situation and better understand the behavior of their <span class="No-Break">AI models.</span></p>
			<p>Let’s now explore some Python libraries that can be used for <span class="No-Break">model explainability.</span></p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor204"/>Explain Like I’m Five (ELI5)</h1>
			<p>This Python library enables<a id="_idIndexMarker1154"/> us to visualize and debug ML models. It displays functionality for both local and global interpretations. XGBoost, LightGBM, scikit-learn, Lightning, and CatBoost are some of the libraries supported by ELI5. The goal of ELI5 is to make the explanations accessible to a general audience, including those who may not have a background in AI <span class="No-Break">or ML.</span></p>
			<p>Here is an example of how ELI5 could be used to provide an explanation for the predictions made by an <span class="No-Break">AI model.</span></p>
			<p>Imagine that an AI model<a id="_idIndexMarker1155"/> has been trained to predict the likelihood of a patient developing a particular disease based on various medical characteristics, such as age, gender, and medical history. The model predicts that a particular patient has a high likelihood of developing the disease. To provide an ELI5 explanation for this prediction, the model could provide a simple and easy-to-understand explanation such as <em class="italic">Based on your age, gender, and medical history, our model predicts that you have a high chance of developing the disease. This is because people with similar characteristics have a high risk of developing the disease according to our data</em>. This ELI5 explanation provides a clear and concise explanation for the prediction made by the model, using language and concepts that are accessible to a general audience. It also highlights the factors that the model considered when making the prediction, which can help to increase the transparency and accountability of the <span class="No-Break">AI system.</span></p>
			<p>With the following, we present a general outline of how ELI5 can be implemented <span class="No-Break">in code:</span></p>
			<ol>
				<li>First, the AI model that you want to explain should be trained and deployed. This could be an ML model, a DL model, or any other type of <span class="No-Break">AI model.</span></li>
				<li>Next, you will need to select the input or output of the model that you want to explain. This could be a single prediction made by the model or a batch <span class="No-Break">of predictions.</span></li>
				<li>You will then need to choose an ELI5 explanation method that is appropriate for your model and the type of explanation you want to provide. There are many different ELI5 methods, including methods that provide feature importance explanations, saliency maps, <span class="No-Break">and counterfactuals.</span></li>
				<li>Once you have selected an ELI5 method, you can use it to generate an explanation for the input or output of the model. This may involve running the input or output through the ELI5 method and then processing the resulting explanation to make it simple and easy to understand for a <span class="No-Break">general audience.</span></li>
				<li>Finally, you can present the ELI5 explanation to the user clearly and concisely, using language and concepts that are accessible to a general audience. This could involve displaying the explanation in a user interface, printing it to the console, or saving it to <span class="No-Break">a file.</span></li>
			</ol>
			<p>However, there are some limitations to ELI5. One limitation is that it may not be possible to provide a simple and easy-to-understand explanation for every decision made by an AI model. In some cases, the decision-making process of an AI model may be too complex or nuanced to be explained simply. Another limitation is that ELI5 explanations may not always be sufficient to fully understand the behavior of an AI model, especially for those with a more technical background. In these cases, more detailed and technical explanations<a id="_idIndexMarker1156"/> may be necessary. Also note that it does not work with all models – for example, with scikit-learn, it works only on linear classifiers, regressors, and tree-based models. With Keras, it supports the explanation of image classifiers using <span class="No-Break">Grad-CAM visualizations.</span></p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor205"/>LIME</h1>
			<p><strong class="bold">LIME</strong> is a popular XAI technique that’s used to provide local explanations<a id="_idIndexMarker1157"/> for the predictions made by an AI model. The goal of LIME is to provide an explanation of how an AI model arrives at a particular prediction from a specific input that is simple and easy to understand for a <span class="No-Break">general audience.</span></p>
			<p>LIME is model-agnostic, which means that it can be applied to a wide range of different AI models, regardless of their architecture or design. This makes it a flexible and widely applicable tool for explaining the behavior of <span class="No-Break">AI models.</span></p>
			<p>When generating explanations with LIME, the technique<a id="_idIndexMarker1158"/> first generates a set of perturbed versions of the input data, called <strong class="bold">perturbations</strong>. These perturbations are obtained by randomly changing the values of some of the input features (one at a time) while keeping the other features unchanged. The AI model is then used to make predictions for each of the perturbations, and the resulting predictions are used to build a simple linear model that approximates the behavior of the AI model in the local region around the original input. This linear model is then used to provide an explanation for the prediction made by the AI model. The explanation can be presented to the user in the form of a list of the most important features that contributed to the prediction, along with the relative importance of <span class="No-Break">each feature.</span></p>
			<p>For example, consider an email classification system that uses an AI model to classify emails as spam or not spam. To explain the classification of a particular email using LIME, the algorithm might generate a set of perturbations by randomly altering the words in the email while keeping the other features unchanged. The AI model would then make predictions for each of the perturbations, and the resulting predictions would be used to build a simple linear model that approximates the behavior of the AI model in the local region around the <span class="No-Break">original email.</span></p>
			<p>The linear model generated by LIME could then be used to provide an explanation for the classification of the original email, such as a list of the most important words in the email that contributed to the classification, along with the relative importance of each word. This explanation would be locally faithful to the email, meaning that it would accurately reflect the behavior of the AI model in the local region around the email, and would be learned over an interpretable representation of the email (that is, a list <span class="No-Break">of words).</span></p>
			<p>LIME is a useful tool for understanding the behavior of AI models and for providing simple and easy-to-understand explanations for the predictions made by these models. However, it has some limitations, such as the fact that it is only able to provide local explanations and may not be able to capture<a id="_idIndexMarker1159"/> the full complexity of an AI model’s <span class="No-Break">decision-making process.</span></p>
			<p>Let’s now move on to the next tool in our <span class="No-Break">toolbox: SHAP.</span></p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor206"/>SHAP</h2>
			<p><strong class="bold">SHAP</strong> (short for <strong class="bold">SHapley Additive exPlanations</strong>) is another popular XAI<a id="_idIndexMarker1160"/> technique. The goal of SHAP is to explain the overall decision-making process of an AI model and how it handles different types of input and to provide an explanation for this process that is simple and easy to understand for a <span class="No-Break">general audience.</span></p>
			<p>SHAP is based on the concept of Shapley values from game theory, which provides a way to fairly distribute the contributions of different players toward a collective outcome. In the context of XAI, SHAP uses Shapley values to calculate the relative importance of each feature in an input data point to the prediction made by an <span class="No-Break">AI model.</span></p>
			<p>To generate explanations with SHAP, the technique first calculates the Shapley values of each feature in the input data. This is done by considering all possible combinations of the input features and the corresponding predictions made by the AI model and then using these combinations to calculate the relative contribution of each feature to <span class="No-Break">the prediction.</span></p>
			<p>The Shapley values calculated by SHAP are then used to provide an explanation for the prediction made by the AI model. This explanation can be presented to the user in the form of a list of the most important features that contributed to the prediction, along with the relative importance of <span class="No-Break">each feature.</span></p>
			<p class="callout-heading">Shapley values and cooperative games</p>
			<p class="callout">Shapley values<a id="_idIndexMarker1161"/> are based on the concept of a cooperative game<a id="_idIndexMarker1162"/> in which a group of participants collaborates to attain a shared goal. The Shapley value is a measure of a player’s contribution to the collective outcome, taking into consideration the efforts of all other members. Shapley values offer a fair value to each player based on that player’s contribution to the group outcome. This means that each participant receives credit solely for the value they provide to the outcome while accounting for the efforts of the other players. Consider a group of individuals who are collaborating to build a house. Taking into consideration the efforts of all other participants, the Shapley value of each participant would represent their contribution to the construction of the house. A person who builds the house’s foundations would receive credit for the foundations’ worth, but not for the roof or walls, and so on, which were contributed by <span class="No-Break">other people.</span></p>
			<p>SHAP is a powerful and widely applicable tool for understanding the behavior of AI models and for providing global explanations for the predictions made by these models. However, it has some limitations, such as the fact that it can be computationally expensive to calculate Shapley values for large datasets. In addition, SHAP explanations may not always be easy for users<a id="_idIndexMarker1163"/> with a limited technical background <span class="No-Break">to interpret.</span></p>
			<p>Now that we have seen some tools that can be used to explain model predictions, let’s see how we can <span class="No-Break">use them.</span></p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor207"/>Understanding churn modeling using XAI techniques</h1>
			<p>Now that you have an idea<a id="_idIndexMarker1164"/> of the ELI5, LIME, and SHAP<a id="_idIndexMarker1165"/> techniques, let’s use them on a real-life problem. For the purpose of demonstration, we will consider the problem of <span class="No-Break"><strong class="bold">churn modeling</strong></span><span class="No-Break">.</span></p>
			<p>Churn modeling is a type of predictive modeling used to identify<a id="_idIndexMarker1166"/> customers who are likely to stop using a company’s products or services, also known as <em class="italic">churning</em>. Churn modeling is commonly<a id="_idIndexMarker1167"/> used in industries such as telecommunications, financial services, and e-commerce, where customer retention is an important factor for <span class="No-Break">business success.</span></p>
			<p>Churn modeling typically involves building a predictive model using ML or other statistical techniques to identify the factors that are most likely to contribute to customer churn. The model is trained on data covering past customers, including information about their demographics, usage patterns, and churn status (that is, whether they churned or not). The model is then used to make predictions about the likelihood of future customers churning based on their specific characteristics <span class="No-Break">and behavior.</span></p>
			<p>It can be used to identify high-risk customers who are likely to churn so that the company can take proactive measures with those customers to retain their customers, such as offering discounts and incentives to encourage them to continue using the company’s products or services. It can also be used to understand the factors that contribute to customer churn more broadly so that companies can take steps to address these factors and improve general <span class="No-Break">customer retention.</span></p>
			<p>We will start by building a model to predict churn. For the purpose of this example, we are using the the Churn Modeling dataset available at <a href="https://github.com/sharmaroshan/Churn-Modeling-Dataset">https://github.com/sharmaroshan/Churn-Modeling-Dataset</a>. The data consists of 10,000 data points with 14 features. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.5</em> shows the results of some <span class="No-Break">data exploration:</span></p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/Figure_9.05_B18681.jpg" alt="Figure 9.5 – Data exploration on the churn-modeling dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Data exploration on the churn-modeling dataset</p>
			<p>Let’s start building <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor208"/>Building a model</h2>
			<p>Now that we<a id="_idIndexMarker1168"/> have seen the data, let’s start building a model to use it. Since our focus is on explaining how to use<a id="_idIndexMarker1169"/> the Python libraries discussed earlier, we will make a simple <strong class="bold">Random Forest</strong> classifier <span class="No-Break">using scikit-learn.</span></p>
			<p>Here are the steps <span class="No-Break">for it:</span></p>
			<ol>
				<li>First, we import all the packages needed to build the model, along with the packages required to build the explanation (namely, ELI5, LIME, <span class="No-Break">and SHAP):</span><pre class="console">
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score
import pickle
import eli5
from eli5.sklearn import PermutationImportance
import lime
from lime import lime_tabular
import shap</pre></li>
				<li>Next, we download<a id="_idIndexMarker1170"/> the data and get the features and output from it; then, we one-hot encode the categorical variables, <strong class="source-inline">Geography</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">Gender</strong></span><span class="No-Break">:</span><pre class="console">
dataset = pd.read_csv('https://raw.githubusercontent.com/krishnaik06/Lime-Model-Interpretation/main/Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
geography = pd.get_dummies(X["Geography"],
        drop_first=False)
gender=pd.get_dummies(X['Gender'],drop_first=False)</pre></li>
				<li>Let’s also drop all irrelevant columns from <span class="No-Break">the DataFrame:</span><pre class="console">
X=pd.concat([X,geography,gender],axis=1)
X=X.drop(['Geography','Gender'],axis=1)</pre></li>
				<li>Now, we split the data into training and <span class="No-Break">test datasets:</span><pre class="console">
X_train, X_test, y_train, y_test = train_test_split(X,
        y, test_size = 0.2, random_state = 0)</pre></li>
				<li>Let’s now define the Random Forest classifier and train it on the <span class="No-Break">training dataset:</span><pre class="console">
Classifier=RandomForestClassifier()
classifier.fit(X_train,y_train)</pre></li>
				<li>Next, we use the trained model to make predictions on the test dataset. We can see that it gives an accuracy of 86% on the <span class="No-Break">test dataset:</span><pre class="console">
y_predict = classifier.predict(X_test)
y_prob = [probs[1] for probs in
        classifier.predict_proba(X_test)]</pre></li>
				<li>We can also <a id="_idIndexMarker1171"/>see the ROC curve for the prediction on the <span class="No-Break">test dataset:</span><pre class="console">
# Compute area under the curve
fpr, tpr, _ = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='orange',
        lw=2, label='ROC curve (area = %0.2f)' %
        roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2,
        linestyle='—')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title("Churn Modeling")
plt.legend(loc="lower right")
plt.show()</pre></li>
			</ol>
			<p>This is how the <a id="_idIndexMarker1172"/>ROC curve appears:</p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/Figure_9.06_B18681.jpg" alt="Figure 9.6 – ROC curve"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – ROC curve</p>
			<p>The area under the ROC curve is ~0.87, which is conventionally considered a mark of a fairly good classifier.</p>
			<ol>
				<li value="8">Since we used a Random Forest<a id="_idIndexMarker1173"/> classifier, let’s explore the feature importance based on the default <strong class="bold">Mean Decrease in Impurity</strong> (<strong class="bold">MDI</strong>) criterion. The following is the code to obtain this. To make it easy to understand, we plot the feature importance<a id="_idIndexMarker1174"/> as a <span class="No-Break">bar chart:</span><pre class="console">
# Feature importance in a dataframe
imp_df = pd.DataFrame({
        'feature': X_train.columns.values,
        'importance':classifier.feature_importances_})
 # Reorder by importance
ordered_df = imp_df.sort_values(by='importance')
imp_range = range(1,len(imp_df.index)+1)
## Bar chart with confidence intervals
height = ordered_df['importance']
bars = ordered_df['feature']
y_pos = np.arange(len(bars))
plt.barh(y_pos, height)
plt.yticks(y_pos, bars)
plt.xlabel("Mean reduction in tree impurity in random forest")
plt.tight_layout()
plt.show()</pre></li>
			</ol>
			<p>This is how the bar chart appears:</p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/Figure_9.07_B18681.jpg" alt="Figure 9.7 – Feature importance using scikit-learn’s MDI criteria"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Feature importance using scikit-learn’s MDI criteria</p>
			<p>In the preceding figure, we can see that the top five features responsible for whether or not a given customer will stay or churn are their age, their estimated salary, the balance they have, their <a id="_idIndexMarker1175"/>credit score, and the number of products <span class="No-Break">they’ve bought.</span></p>
			<p>Next, we use ELI5 to understand <span class="No-Break">the classifier.</span></p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor209"/>Using ELI5 to understand classifier models</h2>
			<p>We use the <strong class="source-inline">PermutationImportance</strong> method of ELI5<a id="_idIndexMarker1176"/> to <a id="_idIndexMarker1177"/>explain<a id="_idIndexMarker1178"/> the classifier model<a id="_idIndexMarker1179"/> we have built. <strong class="source-inline">PermutationImportance</strong> is a feature importance measure implemented in ELI5<a id="_idIndexMarker1180"/> that can be used to get an estimate of feature importance for any <strong class="bold">black-box estimator</strong>. The idea behind <strong class="source-inline">PermutationImportance</strong> is to randomly permute the values of a single feature, and then measure the impact of the permutation on the chosen model’s performance. The more the model’s performance is affected by the permutation, the more important the feature is considered to be. We pass the trained model and dataset as input to the <strong class="source-inline">PermutationImportance</strong> class defined in ELI5. Let’s begin <span class="No-Break">with this:</span></p>
			<ol>
				<li>In the following code, we use the area under the ROC curve as a measure to evaluate the importance of different features in the <span class="No-Break">trained model:</span><pre class="console">
perm_test = PermutationImportance(classifier,
        scoring=make_scorer(roc_auc_score),
        n_iter=50, random_state=123, cv="prefit")</pre></li>
				<li>Let’s calculate the feature importance using the <span class="No-Break">training dataset:</span><pre class="console">
perm_test.fit(X_train, y_train)</pre></li>
				<li>We can now get<a id="_idIndexMarker1181"/> the permutation<a id="_idIndexMarker1182"/> importance of each feature<a id="_idIndexMarker1183"/> using the <strong class="source-inline">explain_weights</strong> or <span class="No-Break"><strong class="source-inline">show_weights</strong></span><span class="No-Break"> function:</span><pre class="console">
imp_df = eli5.explain_weights_df(perm_test)</pre></li>
				<li>The <strong class="source-inline">imp_df</strong> DataFrame contains the estimator parameters. Again, for ease of understanding, it would be good to reorder them and plot the results as a bar chart. Here, we have the relevant code followed by the <span class="No-Break">bar chart:</span><pre class="console">
label_df = pd.DataFrame({
        'feature': [ "x" + str(i) for i in range(
        len(X_test.columns))],
        'feature_name': X_test.columns.values})
imp_df = pd.merge(label_df, imp_df, on='feature',
        how='inner', validate="one_to_one")
# Reorder by importance
ordered_df = imp_df.sort_values(by='weight')
imp_range=range(1,len(imp_df.index)+1)
## Bar chart with confidence intervals
height = ordered_df['weight']
bars = ordered_df['feature_name']
y_pos = np.arange(len(bars))
plt.barh(y_pos, height)
plt.yticks(y_pos, bars)
plt.xlabel("Permutation feature importance training
        set (decrease in AUC)")
plt.tight_layout()
plt.show()</pre></li>
			</ol>
			<p>Here<a id="_idIndexMarker1184"/> is the<a id="_idIndexMarker1185"/> bar<a id="_idIndexMarker1186"/> chart:</p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/Figure_9.08_B18681.jpg" alt="Figure 9.8 – Feature importance using ELI5 PermutationImportance with AUC as the score"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – Feature importance using ELI5 PermutationImportance with AUC as the score</p>
			<p>According to this graph, age is the most important factor, followed by the number of products bought, the balance, and whether<a id="_idIndexMarker1187"/> the person<a id="_idIndexMarker1188"/> is an active <a id="_idIndexMarker1189"/>member <span class="No-Break">or not.</span></p>
			<p>Let’s now try and explain the model’s prediction <span class="No-Break">using LIME.</span></p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor210"/>Hands-on with LIME</h2>
			<p>You can use LIME for any type<a id="_idIndexMarker1190"/> of dataset, whether it’s tabular, textual, or image-based. In this <a id="_idIndexMarker1191"/>section, we will introduce the use of LIME to understand the predictions made by our Random Forest classifier for <span class="No-Break">churn modeling.</span></p>
			<p>Here, we have tabular data. By using LIME, we can gain a better understanding of how our model is making decisions and identify the factors that are most important in <span class="No-Break">predicting churn:</span></p>
			<ol>
				<li>Since it is tabular data, we will use the <strong class="source-inline">LimeTabularExplainer</strong> class defined <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">lime_tabular</strong></span><span class="No-Break">:</span></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Ideally, we should not one-hot encode the categorical features, as when LIME tries to make its predictions, it might give nonsensical input. For more information, please see the thread <span class="No-Break">at </span><a href="https://github.com/marcotcr/lime/issues/323"><span class="No-Break">https://github.com/marcotcr/lime/issues/323</span></a><span class="No-Break">.</span></p>
			<pre class="console">
interpretor = lime_tabular.LimeTabularExplainer(
    training_data=np.array(X_train),
    feature_names=X_train.columns,
    mode='classification')</pre>
			<ol>
				<li value="2">We can now use LIME<a id="_idIndexMarker1192"/> to<a id="_idIndexMarker1193"/> explain the prediction for an instance. We chose the <span class="No-Break">following instance:</span><pre class="console">
X_test.iloc[3]:
CreditScore           788.00
Age                    32.00
Tenure                  4.00
Balance            112079.58
NumOfProducts           1.00
HasCrCard               0.00
IsActiveMember          0.00
EstimatedSalary     89368.59
France                  1.00
Germany                 0.00
Spain                   0.00
Female                  0.00
Male                    1.00
Name: 5906, dtype: float64</pre></li>
				<li>Now, we use LIME’s <strong class="source-inline">explain_instance</strong> function to outline its interpretation of <span class="No-Break">the input:</span><pre class="console">
exp = interpretor.explain_instance(
    data_row=X_test.iloc[3], #new data
    predict_fn=classifier.predict_proba
)
exp.show_in_notebook(show_table=True)</pre></li>
			</ol>
			<p>This is how the output appears:</p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/Figure_9.09_B18681.jpg" alt="Figure 9.9 – Feature importance using the LIME explainer for the X_test.iloc[3] instance"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 – Feature importance using the LIME explainer for the X_test.iloc[3] instance</p>
			<p>The LIME explainer tells us that for the given customer, the probability of churn is negligible (0.01). Further, in the middle of the preceding results, we can see the factors that contribute to each class along with the nature of their contribution. Again, we find that age has the largest impact on <span class="No-Break">the prediction.</span></p>
			<p>We can see that <a id="_idIndexMarker1194"/>all tools<a id="_idIndexMarker1195"/> are pointing in the same direction: <strong class="source-inline">Age</strong> is an important feature for churn modeling. Let’s see what <span class="No-Break">SHAP says.</span></p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor211"/>SHAP in action</h2>
			<p>In this section, we<a id="_idIndexMarker1196"/> will show you how to use SHAP to<a id="_idIndexMarker1197"/> explain the predictions made by our churn-modeling model. SHAP is a powerful XAI technique that allows us to understand the relative importance of each feature in an input data point to the prediction made by an ML model. Using SHAP, we can gain a better understanding of how our model is making decisions and identify the factors that are most important when <span class="No-Break">predicting churn:</span></p>
			<ol>
				<li>We will use the SHAP <strong class="source-inline">TreeExplainer</strong> class, which is specifically designed for use with tree-based models. The <strong class="source-inline">TreeExplainer</strong> class works by traversing the decision tree of the model and calculating the contribution of each feature to the prediction made<a id="_idIndexMarker1198"/> at each tree node. The contributions are then aggregated to generate a global explanation for the prediction made by <span class="No-Break">the model:</span><pre class="console">
explainer = shap.TreeExplainer(classifier)
shap_values = explainer.shap_values(X_test)</pre></li>
				<li>Let’s now use it on the same instance that we used for <span class="No-Break">LIME, </span><span class="No-Break"><strong class="source-inline">X_test[3]</strong></span><span class="No-Break">:</span><pre class="console">
shap.initjs() #initialize javascript in cell
shap.force_plot(explainer.expected_value[0],
        shap_values[0][3,:], X_test.iloc[3,:])</pre></li>
			</ol>
			<p>This will generate a SHAP force plot for the given instance. The force plot visually explains the factors that contributed to the prediction of this specific instance. The relative importance of each feature in the input data is shown, along with how each feature influenced the prediction. The first argument, <strong class="source-inline">explainer.expected_value[0]</strong>, says that the expected value for the model for this instance is <strong class="source-inline">0</strong> – that is, it belongs to the 0 class, meaning no churn. The second argument, <strong class="source-inline">shap_values[0][3,:]</strong>, provides an array of SHAP values for our given instance. The SHAP values represent the contributions of each feature to the prediction made by the model for the 0 class. Here, you can see the plot:</p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/Figure_9.10_B18681.jpg" alt="Figure 9.10 – SHAP force plot for the X_test.iloc[3] instance"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 – SHAP force plot for the X_test.iloc[3] instance</p>
			<p>As we can see, the SHAP force plot shows a linear axis. This axis represents the model’s output. You should also see a set of horizontal bars that represent the features in the input data. The plot also includes a baseline value, which is the expected value of the model’s output for the class or label being explained. The length of each horizontal bar represents the relative importance of the corresponding feature to the prediction. A longer bar indicates greater importance, while a shorter bar indicates lower importance. The color of each bar indicates how the value of the corresponding feature influenced the prediction. A blue bar indicates that a higher value for the feature led to a higher prediction, while a red bar indicates that a higher value for the feature led to a lower prediction.</p>
			<ol>
				<li value="3">We can also <a id="_idIndexMarker1199"/>use dependence<a id="_idIndexMarker1200"/> plots with SHAP to get an understanding of the relationship between a specific feature in an input dataset and the predictions made by the model. The following command produces a plot showing the effect of the <strong class="source-inline">Age</strong> feature on the prediction for the <span class="No-Break">0 class:</span><pre class="console">
shap.dependence_plot("Age", shap_values[0], X_test)</pre></li>
			</ol>
			<p>Here is the plot for it:</p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/Figure_9.11_B18681.jpg" alt="Figure 9.11 – SHAP dependence plot for the Age feature"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.11 – SHAP dependence plot for the Age feature</p>
			<ol>
				<li value="4">The summary plot lets<a id="_idIndexMarker1201"/> us visualize<a id="_idIndexMarker1202"/> the feature importance of all the input features. Here is the code <span class="No-Break">for it:</span><pre class="console">
shap.summary_plot(shap_values[1], X_test, plot_type="violin")</pre></li>
			</ol>
			<p>And the plot is as follows:</p>
			<div>
				<div id="_idContainer194" class="IMG---Figure">
					<img src="image/Figure_9.12_B18681.jpg" alt="Figure 9.12 – SHAP summary plot"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.12 – SHAP summary plot</p>
			<p>The SHAP summary plot consists of a vertical axis that represents the model’s output (often labeled as <strong class="source-inline">f(x)</strong>) and a set of horizontal violin plots that represent the distribution of the SHAP values for each feature. The violin plots show the distribution of the SHAP values for each feature across all instances in the dataset and can be used to understand how the value of each feature influenced the predictions made by <span class="No-Break">the model.</span></p>
			<p>The libraries that we’ve examined<a id="_idIndexMarker1203"/> up to <a id="_idIndexMarker1204"/>now offer us many insights into our model’s predictions based on the importance of each feature involved. Let’s move on to libraries that provide <span class="No-Break">cause-effect analysis.</span></p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor212"/>CausalNex</h1>
			<p>CausalNex, an open source Python library, allows<a id="_idIndexMarker1205"/> us to develop models that help to infer causation rather than observing correlation. The <strong class="source-inline">what if</strong> library offered by CausalNex<a id="_idIndexMarker1206"/> is deployed to test scenarios utilizing <strong class="bold">Bayesian networks </strong>and develop causal reasoning. Some prominent features<a id="_idIndexMarker1207"/> of CausalNex are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Simplifying causality understanding in Bayesian networks via visualization</strong>: One of the main features of CausalNex is its ability to simplify the understanding of causality in Bayesian networks through visualizations. The library provides a range of tools for visualizing Bayesian networks, including network plots, influence plots, and decision plots, which allow users to see how different variables are connected and how they influence <span class="No-Break">each other.</span></li>
				<li><strong class="bold">Understanding conditional dependencies between variables</strong>: CausalNex also provides tools for understanding conditional dependencies between variables. The library includes state-of-the-art structure learning methods, which are algorithms that can automatically learn the structure of a Bayesian network from data. These methods allow users to identify the relationships between variables and understand how they are influenced by other variables in <span class="No-Break">the network.</span></li>
				<li><strong class="bold">Augmenting domain knowledge</strong>: CausalNex also provides tools for augmenting domain knowledge, which refers to the specific knowledge<a id="_idIndexMarker1208"/> and expertise that users bring to the modeling process. The library allows users to incorporate their domain knowledge into the structure of the Bayesian networks, which can help improve the accuracy and reliability of <span class="No-Break">the model.</span></li>
				<li><strong class="bold">Evaluating the quality of the model</strong>: CausalNex includes tools for evaluating the quality of the model, such as statistical checks and model selection methods. These tools allow users to ensure that the model they have built is accurate and appropriate for their <span class="No-Break">specific problem.</span></li>
				<li><strong class="bold">Building predictive models based on structural relationships</strong>: The library also includes tools for building predictive models based on the structural relationships in the Bayesian networks, which can be useful for making predictions about future outcomes or for testing<a id="_idIndexMarker1209"/> scenarios in a “what <span class="No-Break">if” manner.</span></li>
			</ul>
			<p>In the following figure, you can see a graph<a id="_idIndexMarker1210"/> showing the causal relationships between a student’s performance and the factors that might play a role <span class="No-Break">in it:</span></p>
			<div>
				<div id="_idContainer195" class="IMG---Figure">
					<img src="image/Figure_9.13_B18681.jpg" alt="Figure 9.13 – Subgraph showing causal relationships between different factors affecting a student’s performance"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.13 – Subgraph showing causal relationships between different factors affecting a student’s performance</p>
			<p>This result is obtained from the introductory tutorials on CausalNex at <a href="https://causalnex.readthedocs.io/en/latest/03_tutorial/01_first_tutorial.html">https://causalnex.readthedocs.io/en/latest/03_tutorial/01_first_tutorial.html</a>. We can see that excessive internet usage can result in an increase in absence from school. Similarly, if the student increases the time spent studying, their grades (<strong class="bold">G1</strong>) <span class="No-Break">go up.</span></p>
			<p>Overall, CausalNex is a powerful toolkit for causal reasoning using Bayesian networks. It offers a range of features for simplifying the understanding of causality, understanding conditional dependencies between variables, augmenting domain knowledge, evaluating the quality of the model, and building predictive models<a id="_idIndexMarker1211"/> based on <span class="No-Break">structural relationships.</span></p>
			<p>Let’s now explore the next Python library for causal inference, the <span class="No-Break">DoWhy library.</span></p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor213"/>DoWhy for causal inference</h1>
			<p>DoWhy is a Python library<a id="_idIndexMarker1212"/> for causal inference<a id="_idIndexMarker1213"/> and analysis. It is designed to support interoperability with other causal estimation libraries, such as Causal ML and EconML, allowing users to easily combine different methods and approaches in <span class="No-Break">their analysis.</span></p>
			<p>One of the main features of DoWhy is its focus on robustness checks and sensitivity analysis. The library includes a range of methods for evaluating the robustness of causal estimates, such as bootstrapping and placebo tests. These methods help users to ensure that their estimates are reliable and not subject to bias or <span class="No-Break">confounding factors.</span></p>
			<p>In addition to robustness checks, DoWhy also offers an API that follows the common steps involved in causal analysis. These steps include creating a causal model, identifying the effect of interest, estimating the effect using statistical estimators, and validating the estimate through sensitivity analysis and <span class="No-Break">robustness checks.</span></p>
			<p>To create a causal model, users can use DoWhy’s causal graph and structural assumption tools to represent the relationships between variables and specify the underlying assumptions of the model. Once the model has been created, users can use DoWhy’s identification tools to determine whether the expected effect is valid and then use the library’s estimation tools to estimate the effect. Finally, users can use DoWhy’s validation tools to make sure that the estimate is accurate and reliable using sensitivity analysis and <span class="No-Break">robustness checks.</span></p>
			<p>This library makes three main contributions to the field of <span class="No-Break">causal inference:</span></p>
			<ul>
				<li><strong class="bold">Model the problem as a causal graph</strong>: DoWhy allows users to represent their problems as causal graphs, which are graphical representations of the causal relationships between variables. By modeling the problem as a causal graph, users can make all of their assumptions explicit, which helps to ensure that the assumptions are transparent and can be <span class="No-Break">easily understood.</span></li>
				<li><strong class="bold">Unified interface</strong>: DoWhy combines the two major frameworks of graphical models and potential outcomes, providing a unified interface for many different causal inference methods. This allows users to easily combine different methods and approaches in their analysis, and to choose the method that is most appropriate for their <span class="No-Break">specific problem.</span></li>
				<li><strong class="bold">Automatic tests for the validity of assumptions</strong>: DoWhy includes a range of tools for testing the validity of assumptions, such as robustness checks and sensitivity analysis. By automatically testing for the validity of assumptions, users can ensure that their estimates are reliable and not subject to bias or <span class="No-Break">confounding factors.</span></li>
			</ul>
			<p>DoWhy breaks down the process of causal inference into four steps: <strong class="bold">modeling</strong>, <strong class="bold">identification</strong>, <strong class="bold">estimation</strong>, and <strong class="bold">refutation</strong>. During the modeling<a id="_idIndexMarker1214"/> step, users create a causal graph to encode their<a id="_idIndexMarker1215"/> assumptions. In the identification<a id="_idIndexMarker1216"/> step, users formulate what they want to estimate. During the estimation<a id="_idIndexMarker1217"/> step, users compute the estimate using statistical estimators. Finally, in the refutation<a id="_idIndexMarker1218"/> step, users validate<a id="_idIndexMarker1219"/> the assumptions through sensitivity analysis and <span class="No-Break">robustness checks.</span></p>
			<p>Let’s get our hands dirty and play a little <span class="No-Break">with DoWhy.</span></p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor214"/>DoWhy in action</h2>
			<p>We will use a simple synthetic dataset<a id="_idIndexMarker1220"/> to demonstrate the features of the <span class="No-Break">DoWhy library:</span></p>
			<ol>
				<li> Start by importing the DoWhy library and the components we will be using, <span class="No-Break">as follows:</span><pre class="console">
import dowhy
import dowhy.datasets
from dowhy import CausalModel</pre></li>
				<li>Next, using DoWhy’s <strong class="source-inline">linear_dataset</strong> function, we generate a synthetic dataset such that the relationship between a treatment<a id="_idIndexMarker1221"/> and the outcome of interest is linear, also called the linear treatment effect (in our case, we chose <strong class="source-inline">beta=10</strong>, so the true treatment effect is <strong class="source-inline">10</strong>). This creates a linear model with the specified treatment<a id="_idIndexMarker1222"/> effect and generates a set of synthetic data points that conform to this model. The resulting DataFrame contains a column representing the treatment, a column representing the outcome, and a set of columns representing the common causes and instruments. Additionally, the <strong class="source-inline">num_effect_modifiers</strong> parameter specifies the number of effect modifiers or variables that modify the treatment effect. The <strong class="source-inline">num_samples</strong> parameter specifies the number of samples in the dataset, and the <strong class="source-inline">treatment_is_binary</strong> parameter indicates whether the treatment is binary or continuous. If the treatment is binary, then it can take only two values, effective or not and on or off; if the treatment is continuous, it can take more than two values. The <strong class="source-inline">stddev_treatment_noise</strong> parameter specifies the standard deviation of the treatment noise, which is added to the treatment effect to create the synthetic data. The generated data is a <strong class="source-inline">dictdata type</strong>. The DataFrame (<strong class="source-inline">df</strong>) is then extracted <span class="No-Break">from it:</span><pre class="console">
# Generate data
data = dowhy.datasets.linear_dataset(beta=10,
        num_common_causes=5,
        num_instruments = 2,
        num_effect_modifiers=2,
        num_samples=6000,
        treatment_is_binary=True,
        stddev_treatment_noise=9,
        num_discrete_common_causes=1)
df = data["df"]</pre></li>
				<li>The <strong class="source-inline">df</strong> DataFrame will have 11 columns, with column <strong class="source-inline">v0</strong> as the treatment name, the outcome being in column <strong class="source-inline">y</strong>, <strong class="source-inline">W0-W4</strong> representing the five common causes, <strong class="source-inline">Z0</strong> and <strong class="source-inline">Z1</strong> for the two<a id="_idIndexMarker1223"/> instruments, and <strong class="source-inline">X0</strong> and <strong class="source-inline">X1</strong> for the two effect modifiers. Now, we use the <strong class="source-inline">CausalModel</strong> class to create a causal model for our synthetic data. The <strong class="source-inline">CausalModel</strong> class uses the <span class="No-Break">following parameters:</span><ol><li><strong class="source-inline">data</strong>: A pandas DataFrame that holds <span class="No-Break">the data.</span></li><li><strong class="source-inline">treatment</strong>: The column in the DataFrame to be treated as the <strong class="source-inline">treatment</strong> variable. It represents the intervention or action that is being taken to affect <span class="No-Break">the outcome.</span></li><li><strong class="source-inline">outcome</strong>: The column in the DataFrame to be treated as the <span class="No-Break">outcome variable.</span></li><li><strong class="source-inline">common_causes</strong>: The columns to be treated as common causes. These represent the variables that can affect both<a id="_idIndexMarker1224"/> the treatment and outcome and are also called <span class="No-Break"><strong class="bold">confounding variables</strong></span><span class="No-Break">.</span></li><li><strong class="source-inline">instruments</strong>: This represents the instruments. These are the variables that are used to <span class="No-Break">infer causality.</span></li></ol></li>
			</ol>
			<p>This creates a graphical model that represents the structure of the causal relationships in the data. The <strong class="source-inline">graph</strong> parameter specifies the causal graph, which encodes the structure of the causal relationships<a id="_idIndexMarker1225"/> between the variables in the model. Here, we are using the <strong class="bold">Graph Modeling Language</strong> (<strong class="bold">GML</strong>) file format of our causal graph:</p>
			<pre class="console">
# Input a causal graph in GML format
model=CausalModel(
        data = df,
        treatment=data["treatment_name"],
        outcome=data["outcome_name"],
        graph=data["gml_graph"]
        )</pre>
			<p>Let’s view the <strong class="source-inline">model.view_model()</strong> graph:</p>
			<div>
				<div id="_idContainer196" class="IMG---Figure">
					<img src="image/Figure_9.14_B18681.jpg" alt="Figure 9.14 – Causal graph for the dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.14 – Causal graph for the dataset</p>
			<ol>
				<li value="4">Next, we see the target variables<a id="_idIndexMarker1226"/> that are to be estimated and the assumptions needed to identify them. The result tells us the estimands (causal estimands as estimated by the model, which describe the causal effect of interest), their names, the expression (a mathematical expression of estimands in terms of variables of the model), and the <span class="No-Break">assumptions needed:</span><pre class="console">
# Identification
identified_estimand =
        model.identify_effect(
        proceed_when_unidentifiable=True)
print(identified_estimand)
&gt;&gt;&gt;
 Estimand type: EstimandType.NONPARAMETRIC_ATE
### Estimand : 1
Estimand name: backdoor
Estimand expression:
  d
─────(E[y|W0,W1,W3,W2,W4])
d[v₀]
Estimand assumption 1, Unconfoundedness: If U→{v0} and U→y then P(y|v0,W0,W1,W3,W2,W4,U) = P(y|v0,W0,W1,W3,W2,W4)
### Estimand : 2
Estimand name: iv
Estimand expression:
 ⎡                              -1 ⎤
 ⎢    d        ⎛    d          ⎞  ⎥
E⎢─────────(y)⋅ ─────────([v₀])    ⎥
 ⎣d[Z₁  Z₀]     ⎝d[Z₁  Z₀]       ⎠  ⎦
Estimand assumption 1, As-if-random: If U→→y then ¬(U →→{Z1,Z0})
Estimand assumption 2, Exclusion: If we remove {Z1,Z0}→{v0}, then ¬({Z1,Z0}→y)
### Estimand : 3
Estimand name: frontdoor
No such variable(s) found!</pre></li>
				<li>Let’s now use the <strong class="source-inline">estimate_effect</strong> method to identify the estimand to compute an estimate of the treatment<a id="_idIndexMarker1227"/> effect. We can see that the mean value of the estimate is <strong class="source-inline">9.162</strong>, which represents the average effect of the treatment on the outcome across all individuals in <span class="No-Break">the population:</span><pre class="console">
# Estimation
causal_estimate =
        model.estimate_effect(identified_estimand,
        method_name="backdoor.propensity_score_stratification")
print(causal_estimate)
&gt;&gt;&gt;
 *** Causal Estimate ***
## Identified estimand
Estimand type: EstimandType.NONPARAMETRIC_ATE
### Estimand : 1
Estimand name: backdoor
Estimand expression:
  d
─────(E[y|W0,W1,W3,W2,W4])
d[v₀]
Estimand assumption 1, Unconfoundedness: If U→{v0} and U→y then P(y|v0,W0,W1,W3,W2,W4,U) = P(y|v0,W0,W1,W3,W2,W4)
## Realized estimand
b: y~v0+W0+W1+W3+W2+W4
Target units: ate
## Estimate
Mean value: 7.151535367146138</pre></li>
				<li>You can employ the <strong class="source-inline">refute_estimate</strong> function to test the robustness of an estimate of the treatment effect to various types of perturbations in the data or assumptions of <span class="No-Break">the model.</span></li>
			</ol>
			<p>Overall, DoWhy is a powerful tool<a id="_idIndexMarker1228"/> for causal inference and analysis. It offers a range of features for creating causal models, identifying effects, estimating effects, and validating estimates, making it a valuable resource for researchers and analysts working in the field of <span class="No-Break">causal inference.</span></p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor215"/>AI Explainability 360 for interpreting models</h1>
			<p>AI Explainability 360 is an open source toolkit that offers<a id="_idIndexMarker1229"/> a variety of techniques<a id="_idIndexMarker1230"/> for explaining and interpreting ML models. It supports<a id="_idIndexMarker1231"/> both model-specific and model-agnostic approaches, as well as local and global explanations, providing users with a range of options for understanding their models. In addition, the toolkit is built on top of popular ML libraries, including scikit-learn and XGBoost, making it easy to integrate into <span class="No-Break">existing pipelines.</span></p>
			<p>Some of the features<a id="_idIndexMarker1232"/> of AI Explainability 360 include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Model-agnostic and model-specific explainability techniques</strong>: AI Explainability 360 provides both model-agnostic and model-specific explainability techniques that can be used to understand and explain the predictions of any AI model. Model-agnostic techniques, such as LIME and SHAP, can be used to explain the predictions of any model, while model-specific techniques, such as feature importance and partial dependence plots, are tailored to specific types <span class="No-Break">of models.</span></li>
				<li><strong class="bold">Local and global explanations</strong>: AI Explainability 360 provides both local and global explanations of AI models. Local explanations focus on understanding specific predictions made by the model for individual instances, while global explanations focus on understanding the overall behavior of <span class="No-Break">the model.</span></li>
				<li><strong class="bold">Support for multiple types of data</strong>: AI Explainability 360 supports explanations for a variety of data types, including tabular, text, image, and time series data. It provides a range of explainability techniques that are tailored to the specific characteristics of each <span class="No-Break">data type.</span></li>
				<li><strong class="bold">Integration with popular AI frameworks</strong>: AI Explainability 360 is designed to be easily integrated with popular AI frameworks, including TensorFlow, PyTorch, and scikit-learn, making it easy to use in <span class="No-Break">real-world applications.</span></li>
				<li><strong class="bold">Extensive documentation and examples</strong>: AI Explainability 360 comes with extensive documentation and examples to help users get started with explainability in their <span class="No-Break">own projects.</span></li>
			</ul>
			<p>Overall, AI Explainability 360 is a powerful toolkit for understanding and explaining the predictions made by AI<a id="_idIndexMarker1233"/> models, and for building<a id="_idIndexMarker1234"/> transparent, trustworthy, and fair <span class="No-Break">AI systems.</span></p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor216"/>Summary</h1>
			<p>The future of AI lies in enabling people to collaborate with machines to solve complex problems. Like any efficient collaboration, this requires good communication, trust, clarity, and understanding. XAI aims to address such challenges by combining the best of symbolic AI and <span class="No-Break">traditional ML.</span></p>
			<p>In this chapter, we explored a variety of XAI techniques that can be used to explain and interpret ML models. These techniques can be classified based on their scope (local or global) and their model type (model-specific <span class="No-Break">or model-agnostic).</span></p>
			<p>We covered various Python libraries that provide XAI features and explained how to use ELI5, LIME, and SHAP to explore feature importance in model prediction. LIME can provide instance-based explanations for predictions made by any classifier. LIME approximates the classifier locally with an interpretable model and generates a list of features that contribute to the prediction in a given instance. SHAP uses Shapley values to explain the contribution of each feature to a prediction, supports both local and global explanations, and can be used with a variety of <span class="No-Break">model types.</span></p>
			<p>DoWhy is another library for causal inference and analysis. It offers an API for the common steps in causal analysis, including modeling, identification, estimation, and refutation. Finally, we introduced AI Explainability 360, a comprehensive open source toolkit for explaining and interpreting ML models. It supports both model-specific and model-agnostic explanations, as well as local and <span class="No-Break">global explanations.</span></p>
			<p>In conclusion, there are a variety of tools and libraries available for explaining and interpreting ML models. These tools (such as ELI5, LIME, SHAP, CausalNex, DoWhy, and AI Explainability 360) offer a range of options for understanding how models make their predictions and can be useful for researchers and practitioners working in the field of XAI. However, it is important to note that there are still limitations and challenges in <span class="No-Break">this field.</span></p>
			<p>In the next chapter, we will move on to model risk management and explore best practices for <span class="No-Break">model governance.</span></p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor217"/>References</h1>
			<ul>
				<li><em class="italic">Black-box vs. white-box: Understanding their advantages and weaknesses from a practical point of view</em>, Loyola-Gonzalez, Octavio. IEEE Access 7 (<span class="No-Break">2019): 154096-154113.</span></li>
				<li><em class="italic">Opportunities and challenges in explainable artificial intelligence (xai): A survey</em>. arXiv preprint arXiv:2006.11371, Das, Arun and Rad Paul. (2020) </li>
				<li><em class="italic">A systematic review of human–computer interaction and explainable artificial intelligence in healthcare with artificial intelligence techniques</em>. IEEE Access 9 (2021): 153316-153348, Nazar, Mobeen, et al. </li>
				<li><em class="italic">Why should I trust you? Explaining the predictions of any classifier</em>. Ribeiro, Marco Tulio, Singh Sameer, and Guestrin Carlos. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data <span class="No-Break">Mining. 2016.</span></li>
				<li><em class="italic">A unified approach to interpreting model predictions</em>. Advances in Neural Information Processing Systems 30. Lundberg, Scott M. and Lee <span class="No-Break">Su-In (2017)</span></li>
				<li><em class="italic">Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems</em>. 2016 IEEE Symposium on Security and Privacy (SP).Datta, Anupam, Sen Shayak, and Zick Yair. IEEE, 2016. </li>
				<li><em class="italic">Explaining prediction models and individual predictions with feature contributions</em>. Knowledge and Information Systems 41.3 (2014): 647-665. Štrumbelj, Erik and Kononenko Igor. </li>
				<li><em class="italic">Bayesian networks</em>, Pearl, <span class="No-Break">Judea. (2011)</span></li>
				<li><em class="italic">A tutorial on learning with Bayesian networks</em>. Innovations in Bayesian Networks (2008): 33-82. <span class="No-Break">Heckerman, David.</span></li>
				<li><em class="italic">Probabilistic Graphical Models: Principles And Techniques</em>. MIT Press, 2009. Koller, Daphne and <span class="No-Break">Friedman Nir.</span></li>
				<li><em class="italic">Causal inference in statistics: An overview</em>. <em class="italic">Statistics surveys 3</em>: 96-146. Pearl, <span class="No-Break">Judea. (2009)</span></li>
				<li>CausalNex <span class="No-Break">docs: </span><a href="https://causalnex.readthedocs.io/en/latest/"><span class="No-Break">https://causalnex.readthedocs.io/en/latest/</span></a></li>
				<li>DoWhy GitHub <span class="No-Break">repo: </span><a href="https://github.com/py-why/dowhy"><span class="No-Break">https://github.com/py-why/dowhy</span></a></li>
				<li><em class="italic">Introducing AI Explainability </em><span class="No-Break"><em class="italic">360</em></span><span class="No-Break">: </span><a href="https://www.ibm.com/blogs/research/2019/08/ai-explainability-360/"><span class="No-Break">https://www.ibm.com/blogs/research/2019/08/ai-explainability-360/</span></a></li>
			</ul>
		</div>
	</body></html>