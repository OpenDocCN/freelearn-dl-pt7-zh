<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Monte Carlo Methods for Predictions</h1>
                </header>
            
            <article>
                
<p>The Monte Carlo methods for estimating the value function and discovering excellent policies do not require the presence of a model of the environment. You can learn through the use of the agent's experience alone, or from samples of state sequences, actions, and rewards obtained from the interactions between agent and environment. The experience can be acquired by the agent in line with the learning process, or emulated by a previously populated dataset. In this chapter, we will learn how to use Monte Carlo methods to predict an optimal strategy. </p>
<p><span>By the end of the chapter, you should be familiar with the basic concepts of forecasting techniques and should have learned how to apply Monte Carlo methods in order to forecast environment behavior. We will also learn the model-free approach to deal with reinforcement learning problems, how to estimate the action value, and how learning the value of the optimal policy regardless of the agent's actions constitutes an off-policy algorithm.</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Overview of forecasting</li>
<li>Understanding Monte Carlo methods</li>
<li>Approaching model-free algorithms</li>
<li>Estimation of action values</li>
<li>Blackjack strategy prediction using the Monte Carlo approach</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2qONScL">http://bit.ly/2qONScL</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"> Overview of forecasting</h1>
                </header>
            
            <article>
                
<p>An attempt to predict the future has ancient roots and has covered the entire history of humanity, adapting to the typical ways of different civilizations and to different religious settings. The need to foresee future events appears to be justifiable not only for purely speculative and cognitive purposes, but also for operational purposes. The aim is to choose the most appropriate behavior to address the problems that will arise and try to take full advantage of the future situation.</p>
<p><strong>Forecast </strong>and <strong>prediction</strong> are often used as synonyms, but it's always a good idea to make a distinction between the meanings of these two terms. Forecasting allows you to associate the probability of occurrence with future events, or to specify confidence intervals to estimate the size that will be observable and measurable in the future. Prediction, on the other hand, involves identifying the specific value that a measurable quantity will assume in the future. It is, therefore, easy to associate the corresponding forecasts with the predictions thus formulated, using the classical instruments of inferential statistics to derive the relative confidence intervals.</p>
<p>In the following diagram, we can see two examples to understand the differences between <span>forecast and prediction</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4fdf7f91-892a-46b6-a69e-5594729965dc.png" style="width:40.25em;height:12.75em;"/></p>
<p>The difficulties associated with forecasting derive from the uncertainty of the future, which, at the moment in which it is to be expected, is not yet determined. The strong relevance of the elements of uncertainty in the forecast means that the probability calculation tools are essential in the development of a good forecast.</p>
<p>In the elaboration of a forecast, some aspects must be observed, such as the following:</p>
<ul>
<li>The nature of the forecasts can be qualitative or quantitative, but often, both aspects appear for complex phenomena.</li>
<li>The object of the forecast can be the future value of a phenomenon that manifests itself with continuity, or the time in which a phenomenon occurs, or the modalities and characteristics of an event that will occur in the future.</li>
<li>The forecast horizon is usually classified in the short, medium and long term. The distinction is not clear and precise. In general, we talk about short-term forecasts when the structural conditions remain unchanged. This is because the event to be foreseen will be largely determined by actions and behaviors that have already been implemented at the time of forecasting. Instead, we talk about long-term forecasts if the fundamental conditions that determine the event to be forecast are still substantially uncertain.</li>
<li>Finally, with regard to the dimension, the forecast can only relate to one phenomenon (univariate forecast), or, at the same time, more connected phenomena (multivariate forecast), and, in this case, it can be based on causal links through which the behavior of a phenomenon determines, possibly with a certain time lag, the trend of others (causal forecast).</li>
</ul>
<p>Risk and uncertainty are crucial in the elaboration of a forecast. In fact, it is good practice to indicate the degree of uncertainty related to the forecasts. In any case, the data must be updated so that the forecast is as accurate as possible.</p>
<p>Three terms are linked to the concept of statistical forecasting: object, purpose, and method. Let's try to understand what is meant by <span>statistical forecasting</span>. Statistical forecasting applies to conceptually defined phenomena in order to be objectively measured. The phenomenon and the measurement method must therefore be specified and defined, and must remain unchanged for the entire duration of the survey. The purpose of a forecast is to study the future manifestations of a phenomenon, determined by the persistence of a stability in the general structural properties that have occurred in the past. Finally, the forecasting method represents the mathematical model that uses the developments of the calculation of the probabilities related to stochastic processes on the one hand, and on the other, the paradigm of statistical inference and the principles of decision theory in conditions of uncertainty. Let's now try to understand how to approach the different methods available by analyzing real examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Forecasting methods</h1>
                </header>
            
            <article>
                
<p>Forecasting methodologies differ mainly on the basis of the characteristics and objectives of the decisions for which they will be used. The length of the time horizon, the availability and the homogeneity of a wide historical database, and the characteristics of the product to which the forecasts refer, such as the life cycle stage, are some of the factors that influence the choice of a method.</p>
<p>Essentially, the forecasting methods are divided into two categories—<strong>qualitative</strong> and <strong>quantitative</strong>. <span>In the following diagram, we can see examples of the two categories:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-621 image-border" src="assets/9586f16d-bc1e-4d58-b724-06a4cfa0f30c.png" style="width:38.58em;height:11.67em;"/></p>
<p>In the following sections, we will deal with both types of methods. We will analyze concrete cases to understand on what basis this classification is carried out.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Qualitative methods</h1>
                </header>
            
            <article>
                
<p>Qualitative forecasting methods are used to predict future data on the basis of past data. They are adopted when past numerical data is available, and when it is reasonable to assume that some of the models in the data should continue. These methods are generally applied to short- or medium-range decisions. So, qualitative methods are based primarily on judgements, and therefore depend on the opinion and judgement of consumers and experts. They are used when there is limited or no quantitative information, but sufficient qualitative information exists.</p>
<p>The following bullet points include some examples of the application of qualitative methods:</p>
<ul>
<li><strong>Assessments of the sales department</strong>: Each sales agent estimates the future demand for its territory for the next period. The hypothesis underlying this method is that the people closest to the client know their future needs better than anyone else. This information is then aggregated to arrive at global forecasts for each geographic area or product family.</li>
<li><strong>Market surveys</strong>: Companies often turn to firms who specialize in market surveys to make this type of forecast. Information is obtained directly from customers or, more often, from a representative sample of them. This type of investigation, however, is mainly used to look for new ideas, to like or dislike existing products, to find out which are the favorite brands of a given product, and so on.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quantitative methods</h1>
                </header>
            
            <article>
                
<p>We talk about quantitative methods when quantitative information is adequately available. They are also used to predict future data based on past data. These methods are usually applied to short- or medium-term decisions.</p>
<p>Examples of quantitative prediction methods include the following:</p>
<ul>
<li><strong>Time series</strong>: The phenomenon to be expected is treated like a black box, because it does not try to identify the phenomena that can influence it. The goal of this approach is to identify the past evolution of the phenomenon, and to extrapolate the past to obtain a prediction. In other words, the phenomenon to be predicted is modeled with respect to time, and not with respect to an explanatory variable (consider sales trends, gross domestic product (GDP) trends, and so on).</li>
<li><strong>Explanatory methods</strong>: This assumes that the variable to be predicted can be related to one or more independent or explanatory variables. For example, the demand for consumer goods of a family depends on the income received, and the age of the goods.</li>
</ul>
<p>Such forecasting techniques employ regression methods and, therefore, the main phase of the analysis entails specifying and estimating a model that relates the variable to be predicted (response) and the explanatory variables (for example, the effect on sales of advertising and/or a price promotion).</p>
<p>These methods can be used in the following hypotheses:</p>
<ul>
<li>Sufficient information is available regarding the past evolution of the phenomenon.</li>
<li>This information can be quantified.</li>
<li>It can be assumed that the characteristics of the past evolution continue to exist in the future in order to make the forecast.</li>
</ul>
<p>Ultimately, quantitative methods are used when adequate quantitative information is available.</p>
<p>In the following section, we will introduce Monte Carlo methods. In particular, we will focus on the use of these methods to solve reinforcement learning problems. We will also see what it means to use these methods for prediction and control.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding Monte Carlo methods</h1>
                </header>
            
            <article>
                
<p>Monte Carlo methods for estimating the value function and discovering excellent policies do not require the presence of a model of the environment. These methods can learn using the agent's experience alone, or from samples of state sequences, actions, and rewards obtained from interactions between the agent and the environment. The experience can be acquired by the agent in line with the learning process, or it can be emulated by a previously populated dataset. The possibility of gaining experience during learning (online learning) is interesting, because it allows the acquisition of excellent behavior even in the absence of a priori knowledge of the dynamics of the environment. Even learning through an already-populated experience dataset can be interesting because, if combined with online learning, it makes automatic policy improvements induced by others' experiences possible.</p>
<p>To solve reinforcement learning problems, Monte Carlo methods estimate the value function based on the total sum of rewards obtained, on average, in past episodes. This assumes that the experience is divided into episodes, and that all episodes are composed of a finite number of transitions. This is because, in Monte Carlo methods, the policy update and value function estimate take place after an episode is completed. Indeed, Monte Carlo methods iteratively estimate policy and value functions. In this case, however, each iteration cycle is equivalent to completing an episode. So, the policy update and value function estimate occur episode by episode, as we just said.</p>
<p>Monte Carlo methods try to get the best policy by using example returns. An environment model capable of generating these example transitions is therefore sufficient. Unlike dynamic programming, it is not necessary to know the probability of all possible transitions. In many cases, it is, in fact, easy to generate samples that satisfy the desired probability distributions, while it is impractical to express explicitly the totality of the probability distributions. These algorithms simulate an example sequence called an episode, <span>and are based on observed values, ​​update values, and policy estimates</span>. While iterating for a sufficient number of episodes, the results obtained show a satisfactory accuracy.</p>
<p>Compared to algorithms based on dynamic programming, Monte Carlo algorithms do not require the complete system model. However, they offer the possibility to update value and policy only at the end of each simulation, unlike dynamic programming algorithms, which update the estimates at each step. </p>
<p>Now it is time to understand what it means to use Monte Carlo methods for prediction and control.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monte Carlo methods for prediction</h1>
                </header>
            
            <article>
                
<p>Monte Carlo prediction is used to estimate a value function. In this case, the expected total reward from any given state is predicted, given the policy. The procedure follows this flow:</p>
<ul>
<li>Gives the policy</li>
<li>Calculates the value function</li>
</ul>
<p>You will recall that the policy defines the agent's way of acting based on the current state, thus representing the probability of action, in a specific way when in a specific state. </p>
<p>A prediction task requires that the policy is provided with the aim of measuring its performance, that is, of forecasting the total reward provided by each given state, assuming that the policy is set a priori.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Monte Carlo methods for control</h1>
                </header>
            
            <article>
                
<p>Monte Carlo control is used to optimize the value function to make the value function more accurate than the estimation. In control, policy is not fixed, and the goal is to find the optimal policy. In this case, our aim is to find that policy that maximizes the total reward provided by each given state.</p>
<p>A control algorithm also works for prediction, which predicts the values of the action in different ways and adjusts the policy to choose the best actions at each stage. Thus, the output of these algorithms provides an approximately optimal policy and the expected future rewards for following that policy.</p>
<p>In the next section, we will understand how to differentiate between a model-free and a model-based algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Approaching model-free algorithms</h1>
                </header>
            
            <article>
                
<p>In the previous section, <em>Understanding Monte Carlo methods</em>, we said that Monte Carlo methods do not require the presence of a model of the environment to estimate the value function, or to discover excellent policies. This means that Monte Carlo is model-free: no knowledge of <strong>Markov decision process</strong> (<strong>MDP</strong>) transitions or rewards is required. So, we don't need to have modeled the environment <span>previously</span>, but the necessary information will be collected during an interaction with the environment (online learning). Monte Carlo methods learn directly from episodes of experience, where an episode of experience is a series of tuples (state, action, reward, and next state). </p>
<p><span>In the following screenshot, we can see a comparison between a model-based and a model-free approach:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fa0b4143-8c09-4725-8beb-3e383a63b79b.png" style="width:25.92em;height:17.33em;"/></p>
<p>Model-free methods can be applied to many reinforcement learning problems that do not require any model of the environment. Many model-free approaches try to learn the value function and infer from it the optimal policy, or by searching for the optimal policy directly in the policy parameter space itself. These approaches can also be classified as on-policy approaches or off-policy approaches. On-policy methods use current policy to generate actions, and to update the policy itself, while off-policy methods use a different exploration policy to generate actions with respect to the policy that is updated. What are the differences between the two approaches—model-free and model-based? In the following section, we will try to highlight the differences in order to choose the right approach to solve a problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model-free versus model-based</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will try to clarify</span> the difference between these two approaches with a view to solving a problem with reinforcement learning. In such problems, the agent does not know all the elements of the system, which prevents him from planning a solution. In particular, the agent does not know how the environment will change in response to his actions. This is because the transition function, <em>T</em>, is not known. Furthermore, he does not even know what immediate reward he will receive in response to his actions. This is because he has not yet noted the reward function. The agent will have to explore the environment by trying to act, observing the answers, and somehow finding a good policy in order to obtain the best possible final reward.</p>
<p class="mce-root">The following question arises: If the agent knows neither the transition function nor the reward function, how can he derive good policy? To do so, two approaches can be followed: model-based, and model-free.</p>
<p class="mce-root">In the first <span>approach </span>(model-based), the agent learns a model from the observations of the functioning of the environment from his observations, and then derives a solution using that model. For example, if the agent is in the state s1, and performs an action a1, he can observe the environmental transition that takes him to the state s2, thereby obtaining a reward r2. This information can be used to update the evaluation of the transition matrix T (s2 | s1, a1) and R (s1, a1). This update can be performed using the supervised learning paradigm. Once the agent has adequately modeled the environment, he can use that model to find a policy. The algorithms that adopt this approach are called model-based.</p>
<p class="mce-root">The second approach does not involve learning an environment model to find a good policy. One of the most classic examples is Q-learning, which we will analyze in detail in <a href="9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml">Chapter 7</a>, <em>Temporal Difference Learning</em>. This algorithm directly estimates the optimal values ​​of the usefulness of each action in each state, from which a policy can be derived by choosing the action with the highest value in the current state. Because these approaches do not learn an environmental model, they are called <strong>model-free</strong>.</p>
<p class="mce-root">Ultimately, if, after learning, the agent can make predictions about what will be the next status, and reward, before taking any action, then our algorithm is model-based. Otherwise, it is a model-free algorithm.</p>
<p>Now, let's see how the action values in the Monte Carlo methods are updated.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Estimation of action values</h1>
                </header>
            
            <article>
                
<p>In general, Monte Carlo methods depend on repeated random sampling to obtain numerical results. To do this, they use randomness to solve deterministic problems. In our case, we will use random sampling of states and action-state pairs, we will look at the rewards, and then we will review the policy in an iterative way. The iteration of the process will converge on optimal policy as we explore every possible action-state pair.</p>
<p>For example, we could use the following procedure:</p>
<ul>
<li>We assign a reward of +1 to a correct action, -1 to an incorrect action, and 0 to a draw.</li>
<li>We establish a table in which each key corresponds to a particular state-action pair, and each value is the value of that pair. This represents the average reward received for that action in that state.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To solve the reinforcement learning problem, Monte Carlo methods estimate the value function on the basis of the total sum of rewards, obtained on average in past episodes. This assumes that the experience is divided into episodes, and that all episodes are composed of a finite number of transitions. This is because, in Monte Carlo methods, the estimate of the new values and the modification of the policy takes place once an episode is completed. Monte Carlo methods iteratively estimate policy and value function. In this case, however, each iteration cycle is equivalent to completing an episode—the new estimates of policy and value function occur episode by episode, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-622 image-border" src="assets/cb5a3260-5fb6-454b-a839-f0e620188085.png" style="width:27.25em;height:15.83em;"/></p>
<p>The workflow includes the sampling of experience episodes and the subsequent updating of estimates at the end of each episode. Because of the many random decisions within each episode, these methods have a high variance, although these are unbiased.</p>
<p>You may recall two processes, called <strong>policy evaluation</strong> and <strong>policy improvement</strong>:</p>
<ul>
<li>Policy evaluation algorithms consist of applying an iterative method to the resolution of the Bellman equation. Since convergence is guaranteed to us only for k → ∞, we must be content to have good approximations by imposing a stopping condition.</li>
<li>Policy improvement algorithms improve policy based on current values.</li>
</ul>
<p>As we said, the new estimates of policy and value function occur episode by episode; hence, the policy is updated only at the end of an episode.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following code block shows the pseudocode for Monte Carlo policy evaluation:</p>
<pre>Initialize<br/>    arbitrary policy π<br/>    arbitrary state-value function<br/>Repeat<br/>    generate episode using π<br/>    for each state s in episode<br/>        the received reward R is added to the set of<br/>       reinforcers obtained so far<br/>        estimate the value function on the basis on the average<br/>        of the total sum of rewards obtained</pre>
<p>Usually, the term <strong>Monte Carlo</strong> is used for estimation methods, the operations of which involve random components. In this case, the term <strong>Monte Carlo</strong> refers to <span>reinforcement learning </span>methods based on total reward averages. Unlike dynamic programming methods, which calculate the values for each state, Monte Carlo methods calculate values for each state-action pair because, in the absence of a model, only state values are not sufficient to decide which action is best performed in a certain state.</p>
<p>After analyzing in detail how the Monte Carlo methods approach problems based on reinforcement learning, the time has come to see a practical case. To do this, we will use a very popular game—blackjack. We will see how to forecast the best game strategy using the Monte Carlo methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Blackjack strategy prediction using the Monte Carlo approach</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT"><span>Blackjack is a card game that takes place between the dealer and the players. Players who achieve a higher score than the dealer and do not exceed 21 win, while those players who exceed 21 <strong>bust</strong> and lose the game. Blackjack is usually played with a sabot made up of 2 French card decks, for a total of 104 cards. In the game, the ace can be worth 11, or 1, the pictures are worth 10, while the other cards are worth their face value. Seeds have no influence or value. The sum of the points, for the purpose of calculating the score, takes place by simple arithmetic calculation.</span></p>
<p class="mce-root"/>
<p class="NormalPACKT"><span>Once the players have made their bet, the dealer, proceeding from left to right, assigns each of the players an uncovered card in each location played, assigning the last one to himself. He then does a second round of uncovered cards, without attributing one to himself. Once the distribution has taken place, the dealer reads in order the score of each player inviting them to show their game: they can ask for cards (hit) or stay (stick), at their discretion. If a player exceeds 21, he loses, and the dealer will take the bet. Once the players have defined their scores, the dealer develops his game on the basis of a simple rule; that is, he finds a card if he has a score lower than 17, and once he gets or passes 17, he must stop. If he passes 21, the dealer <strong>busts</strong> and must pay all the bets left on the table. Once all the scores have been defined, the dealer compares his own score with that of the other players, pays the combinations higher than his, collects the lower ones, and leaves the ones in a draw. The payment of winning bets is at par.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Blackjack game as an MDP</h1>
                </header>
            
            <article>
                
<p>Blackjack can be treated as an MDP because the status of the players can be defined by the value of the cards in possession, regardless of the cards you have. The status of the dealer is defined entirely by the value of the individual card displayed, and therefore the finished state of the entire game is defined by the status of the player, and the status of the dealer. Finally, the next state of the game is stochastically defined in its entirety from the current state and the player's action.</p>
<p class="mce-root">We recall that a problem can be defined as MDP if the next state is a stochastic function of the current state only, and of the applied action. Furthermore, the MDPs are applicable to situations in which the decision-making space is limited and discreet, the results are uncertain, and the terminal status and the relative prizes are well defined. The solution—an MDP provides us with the optimal action to perform based on a process that aims to maximize the reward for each possible state.</p>
<p>In the following sections, we will explain the code line by line:</p>
<ol>
<li>We will start defining the actions available:</li>
</ol>
<pre style="padding-left: 60px">HIT &lt;- 1 <br/>STICK &lt;- 2</pre>
<p style="padding-left: 60px"><span>Let's look at the meaning of <em>hit</em> and <em>stick</em></span>:</p>
<ul>
<li style="padding-left: 60px"><kbd>HIT</kbd>: Take another card from the dealer.</li>
<li style="padding-left: 60px"><kbd>STICK</kbd>: Take no more cards.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Now, we have to simulate the drafting of the cards from the deck, this operation being carried out by the dealer who, as anticipated, distributes two cards for each player:</li>
</ol>
<pre style="padding-left: 60px">BJCard &lt;- function()<br/>  return(sample(10,1))</pre>
<p style="padding-left: 60px">The <kbd>sample()</kbd> function takes a sample of the specified size from the elements passed using either with (<kbd>1</kbd>) or without (<kbd>0</kbd>) replacement.</p>
<ol start="3">
<li>Now, let's start generating an initial state randomly:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">StateInput &lt;- function () {<br/>   return ( c(sample(10, 1), sample(10, 1), 0))<br/>}</pre>
<p>The initial state and, in general, every possible state, is therefore represented by a vector with the three elements specified here:</p>
<ul>
<li><strong>Dealer card</strong> (sample(10, 1)): A value between 1 and 10</li>
<li><strong>Player hand value </strong><span>(</span><span>sample(10, 1)): The sum of the player's card values</span></li>
<li><strong>Terminal state</strong> (0): A binary value (0-1) that tells us if the hand is over</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">State and reward update</h1>
                </header>
            
            <article>
                
<p>In the following code block that we will analyze, we will update the state and the reward returned from the environment.</p>
<ol>
<li>Now, we will create the function that allows us to execute a single step of the process (<kbd>StepFunc</kbd>), based on the state (s) and the action (a) passed that returns a new state and reward obtained:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">StepFunc &lt;- function (s, a) {<br/> if(s[3]==1)<br/> return(list(s, 0))<br/> <br/> NewState &lt;- s<br/> BJReward &lt;- 0</pre>
<p class="mce-root" style="padding-left: 60px">The first check entered verifies whether we are in the terminal state (hand over), in which case the cycle is exited and the current state is returned and a reward equal to zero.</p>
<p class="mce-root"/>
<ol start="2">
<li class="mce-root">Let's check the action passed:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"> if(a==1) { <br/>     NewState[2] &lt;- s[2] + BJCard() <br/>     if (NewState[2]&gt;21 || NewState[2]&lt;1) { <br/>         NewState[3] &lt;- 1<br/>         BJReward &lt;- -1<br/>     }<br/><span>}</span></pre>
<p style="padding-left: 60px">If the action to be performed is <kbd>HIT</kbd>, then a new card is discovered and the status and reward are updated.</p>
<ol start="3">
<li>Let's see what happens if the past action is <kbd>STICK</kbd>:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">else { <br/>    NewState[3] &lt;- 1<br/>     DealerWork &lt;- FALSE<br/>     DealerSum &lt;- s[1]<br/>     while(!DealerWork) { <br/>         DealerSum &lt;- DealerSum + BJCard()<br/>         if (DealerSum&gt;21) { <br/>             DealerWork &lt;- TRUE<br/>             BJReward &lt;- 1<br/>         } else if (DealerSum &gt;= 17) { <br/>             DealerWork &lt;- TRUE<br/>             if(DealerSum==s[2])<br/>                 BJReward &lt;- 0<br/>             else <br/>                 BJReward &lt;- 2*as.integer(DealerSum&lt;s[2])-1<br/>             }<br/>         }<br/>     }<br/> return(list(NewState, BJReward))<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Then, the hand passes to the dealer who executes his game. You start by updating the player's terminal status to 1. Then, the dealer status (<kbd>DealerWork</kbd> &lt;- <kbd>FALSE</kbd>) and its current score are updated (<kbd>DealerSum &lt;- s [1]</kbd>). From this point on, using a <kbd>while</kbd> loop runs the dealer game. To start, a new card is discovered. At this point, a first <kbd>IF</kbd> value is used to check whether the dealer has busted (<kbd>DealerSum</kbd><kbd>&gt; 21</kbd>). In this case, the game ends with the victory of the player. The status of the dealer game is updated (<kbd>DealerWork &lt;- TRUE</kbd>), as well as the total <kbd>BJReward</kbd> reward &lt;- 1. Otherwise, if <kbd>DealerSum&gt; = 17</kbd>, the dealer stops his game and checks the status of the player. If the scores are equal (<kbd>DealerSum == s [2]</kbd>), the game ends in a draw (<kbd>BJReward &lt;- 0</kbd>), otherwise, if the dealer's score is greater than that of the player, then <kbd>BJReward</kbd> of the player = -1 and the player loses. If the dealer 's score is less than that of the player,then <kbd>BJReward</kbd> of the player = 1 and the player wins. Finally, as already mentioned, the function returns the updated status and the final reward of the step. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Strategy prediction</h1>
                </header>
            
            <article>
                
<p>Now is the time to predict the best strategy to be successful in the game:</p>
<ol>
<li><span>After defining the function that updates the status and the reward, it is time to define the policy:</span></li>
</ol>
<pre class="mce-root" style="padding-left: 60px">ActionsEpsValGreedy &lt;- function(s, QFunc, EpsVal) {<br/> if(runif(1)&lt;EpsVal)<br/> return(sample(1:2, 1))<br/> else<br/> return(which.max(QFunc[s[1],s[2],])) <br/>}</pre>
<p style="padding-left: 60px">To define the policy, an <kbd>ActionsEpsValGreedy()</kbd> function was created. This function accepts the following inputs:</p>
<ul>
<li style="padding-left: 60px"><kbd>s</kbd>: The state</li>
<li style="padding-left: 60px"><kbd>QFunc</kbd>: The action-value function</li>
<li style="padding-left: 60px"><kbd>EpsVAl</kbd>: The numeric value for epsilon</li>
</ul>
<p style="padding-left: 60px">This returns the action to be followed.</p>
<p class="mce-root"/>
<p style="padding-left: 60px">As we said in <a href="80162fc2-33f6-4f5a-9f70-6d063b32d9c9.xhtml"/><a href="80162fc2-33f6-4f5a-9f70-6d063b32d9c9.xhtml"/><a href="80162fc2-33f6-4f5a-9f70-6d063b32d9c9.xhtml"/><a href="80162fc2-33f6-4f5a-9f70-6d063b32d9c9.xhtml">Chapter 4</a>, <em>Multi-Armed Bandit Models</em>, in the ε-greedy approach, we assume that, with a probability ε, a different action is chosen. This action is chosen with uniform probability between the n possible actions available. In this way, we introduce an element of exploration that improves performance. However, if two actions only exhibit a very small difference between their Q values, this algorithm will also choose only that action that has a higher probability than the others.   </p>
<ol start="2">
<li>Now, let's load the <kbd>foreach</kbd> library:</li>
</ol>
<pre style="padding-left: 60px"><span>library("foreach")</span></pre>
<p style="padding-left: 60px">This package handles the <kbd>foreach</kbd> loop construct. The <kbd>foreach</kbd> command allows you to scroll through items in a collection without using an explicit counter. We recommend using the package for its return value, rather than for its side effects. Used in this way, it is similar to the standard <kbd>lapply</kbd> function, but does not require the evaluation of a function. Thus, the use of <kbd>foreach</kbd> facilitates the execution of the cycle in parallel.</p>
<ol start="3">
<li>Finally, we can define the <kbd>MontecarloFunc</kbd> function that will guide us in solving the problem. This function accepts the following variable as input:</li>
</ol>
<ul>
<li style="padding-left: 60px"><kbd>NumEpisode</kbd>: Number of episodes to play</li>
</ul>
<p style="padding-left: 60px">The <span><kbd>MontecarloFunc()</kbd> function returns the following values:</span></p>
<ul>
<li style="padding-left: 60px"><kbd>QFunc</kbd>: Updated action-value function</li>
<li style="padding-left: 60px"><kbd>N</kbd>: Updated numbers of state-action visits</li>
</ul>
<ol start="4">
<li>Let's analyze this in detail:</li>
</ol>
<pre style="padding-left: 60px">MontecarloFunc &lt;- function(NumEpisode){<br/>  QFunc &lt;- array(0, dim=c(10, 21, 2))<br/>  N &lt;- array(0, c(10,21,2))<br/>  N0=100</pre>
<p style="padding-left: 60px">Once the inputs are passed, the following variables are initialized:</p>
<ul>
<li style="padding-left: 60px"><kbd>QFunc</kbd>: Action-value function as an array containing the following variables: all possible card values (dim=10), all possible sum values (dim=21), and all possible actions (dim=2)</li>
<li style="padding-left: 60px"><kbd>N</kbd>: Number of state-action visits</li>
<li style="padding-left: 60px"><kbd>N0</kbd>: Offset for N</li>
</ul>
<ol start="5">
<li class="mce-root">Then, we will pass to define a policy:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"> policy &lt;- function(s) {<br/>     ActionsEpsValGreedy(s, QFunc, N0/(sum(N[s[1], s[2],])+N0))<br/> }</pre>
<ol start="6">
<li>We will now use a cycle to perform all the episodes necessary to calculate the best strategy:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">foreach(i=1:NumEpisode) %do% {<br/> s &lt;- StateInput()<br/> SumReturns &lt;- 0<br/> N.episode &lt;- array(0, c(10,21,2))</pre>
<ol start="7">
<li class="mce-root">Now, let's play a game for each episode:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">while(s[3]==0) {<br/> a &lt;- policy(s)<br/> N.episode[s[1], s[2], a] &lt;- N.episode[s[1], s[2], a] + 1<br/> StateReward &lt;- StepFunc(s, a)<br/> s &lt;- StateReward[[1]]<br/> SumReturns &lt;- SumReturns + StateReward[[2]]<br/>}</pre>
<p style="padding-left: 60px">Until such time as the game's terminal status is 0 (game in progress), it performs the following operations:</p>
<ul>
<li style="padding-left: 60px">Chooses an action to perform based on the defined policy</li>
<li style="padding-left: 60px">Increases the visits counter</li>
<li style="padding-left: 60px">Performs a step by calling the <kbd>StepFunc()</kbd> function</li>
<li style="padding-left: 60px">Updates your rewards</li>
</ul>
<ol start="8">
<li class="mce-root">After doing this, we move on to update Q and N:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">IndexValue &lt;- which(N.episode!=0)<br/> N[IndexValue] &lt;- (N[IndexValue]+N.episode[IndexValue])<br/> QFunc[IndexValue] &lt;- QFunc[IndexValue] + (SumReturns-QFunc[IndexValue]) / N[IndexValue]<br/> }</pre>
<p style="padding-left: 60px"><span>The preceding code block includes </span>the key element of the entire process—the updating mode of the Q function. In this case, an incremental approach has been adopted.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="9">
<li>In fact, the function <em>q</em> is updated using the following function:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5cc73415-ea95-4a4c-b092-a4e1dd0bb02d.png" style="width:17.00em;height:2.42em;"/></p>
<p style="padding-left: 60px">Here:</p>
<ul>
<li style="padding-left: 60px"><em>G</em>: This is a sum of the rewards.</li>
<li style="padding-left: 60px"><span>Q: This is the action-value function.</span></li>
<li style="padding-left: 60px"><em>N</em>: This is the number of state-action visits.</li>
</ul>
<ol start="10">
<li class="mce-root">Finally, the following results are returned:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">return(list(QFunc=QFunc, N=N))<br/> }</pre>
<ol start="11">
<li>After defining all the necessary functions, it is time to run the simulation:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">MCModel &lt;- MontecarloFunc(NumEpisode=100000)</pre>
<p style="padding-left: 60px">We simply passed the number of episodes required to obtain a good forecast of the best strategy.</p>
<ol start="12">
<li>At this point, to analyze the results, we can draw a graph. However, it is necessary to adequately format the action-value function:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">StateValueFunc &lt;- apply(MCModel$QFunc, MARGIN=c(1,2), FUN=max)</pre>
<p class="mce-root" style="padding-left: 60px">To do this, we used the <kbd>apply()</kbd> function, which returns a vector or array, or a list of values obtained by applying a function to margins of an array or matrix. </p>
<ol start="13">
<li class="mce-root">Now, we can draw a chart:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">persp(StateValueFunc, x=1:10, y=1:21, theta=50, phi=35, d=1.9, expand=0.3, border=NULL, ticktype="detailed",<br/> shade=0.6, xlab="Dealer exposed card", ylab="Player sum", zlab="Value", nticks=10)</pre>
<p>The <kbd>persp()</kbd> function was used. <span>This function draws perspective plots of a surface over the x-y plane.</span></p>
<p><span>The following chart is plotted:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-451 image-border" src="assets/ea1fae42-e238-4354-bce7-d4b32b34f99e.png" style="width:51.08em;height:36.42em;"/></p>
<p>In this way, we have a good estimate of the value function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, the basic concepts of the Monte Carlo method were explored. The Monte Carlo method entails looking for the solution to a problem, representing it as a parameter of a hypothetical population, and estimating this parameter by examining a sample of the population obtained through sequences of random numbers. Later, we highlighted the differences between the different methods that this technology makes available to us. The Monte Carlo prediction is used to estimate the value function, while Monte Carlo control is used to optimize the value function to make the value function more accurate than the estimation.</p>
<p>We then moved on to analyze the differences between algorithms based on a model-free approach, and those based on a model-based approach. Furthermore, we analyzed step by step the procedure that allows us to carry out the Monte Carlo policy evaluation. Finally, as a practical case of the concepts learned, a Blackjack strategy prediction involving the Monte Carlo method was performed.</p>
<p>In the next chapter, we will learn about the different types of <strong>temporal difference</strong> (<strong>TD</strong>) learning algorithms. You will discover how to use TD algorithms to predict the future behavior of a system, and learn the basic concepts of the Q-learning algorithm. You will also learn to use the current best policy estimate to generate system behavior through the Q-learning algorithm.</p>


            </article>

            
        </section>
    </body></html>