- en: 11\. Policy-Based Methods for Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will implement different policy-based methods of **Reinforcement
    Learning** (**RL**), such as policy gradients, **Deep Deterministic Policy Gradients**
    (**DDPGs**), **Trust Region Policy Optimization** (**TRPO**), and **Proximal Policy**
    **Optimization** (**PPO**). You will be introduced to the math behind some of
    the algorithms and you'll also learn how to code policies for RL agents within
    the OpenAI Gym environment. By the end of this chapter, you will not only have
    a base-level understanding of policy-based RL methods but you'll also be able
    to create complete working prototypes using the previously mentioned policy-based
    RL methods.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The focus of this chapter is policy-based methods for RL. However, before diving
    into a formal introduction to policy-based methods for RL, let's spend some time
    understanding the motivation behind them. Let's go back a few hundred years when
    the globe was still mostly undiscovered and maps were incomplete. Brave sailors
    at that time sailed the great oceans with only indomitable courage and unyielding
    curiosity on their side. But they weren't completely blind in the vastness of
    the oceans. They looked up to the night sky for direction. The stars and planets
    in the night sky guided them to their destination. The night sky is viewed differently
    at different times of the year from different parts of the globe. This information,
    along with highly accurate maps of the night sky, guided these brave explorers
    to their destinations and sometimes to unknown, uncharted lands.
  prefs: []
  type: TYPE_NORMAL
- en: Now, you might question what this story has to do with RL at all. A map of the
    night sky wasn't always available to those sailors. They were created by globetrotters,
    sailors, skywatchers, and astronomers over centuries. Sailors actually voyaged
    blindly at one time. They looked at the stars during night time, and every time
    they took a turn, they marked their position relative to the position of the stars
    in the night sky. Upon reaching their destination, they evaluated each turn they
    took and worked out which was more effective during their voyage. Every other
    ship that sailed to the same destination could do the same. With time, they had
    a good assessment of the turns that are the most effective for reaching a certain
    destination with respect to a ship's position in the sea, as assessed by looking
    at the position of the stars in the night sky. You can think of this as computing
    the value function where you know the immediate best move. But once sailors had
    a complete map of the night sky, they could simply derive a policy that would
    lead them to their destination.
  prefs: []
  type: TYPE_NORMAL
- en: You can consider the sea and the night sky as the environment and the sailors
    as agents within it. Over the span of a few centuries, our agents (sailors) built
    a model of their environment and so were able to come up with a value function
    (calculating a ship's relative position) that would lead them to the immediate
    best possible step (immediate navigational step) and also helped them build the
    optimal policy (a complete navigation route).
  prefs: []
  type: TYPE_NORMAL
- en: In the last chapter, you learned about **Deep Recurrent Q Networks** (**DRQNs**)
    and their advantage over simple deep Q networks. You also modeled a DRQN network
    for playing the very popular Atari video game *Breakout*. In this chapter, you'll
    learn about policy-based approaches to RL.
  prefs: []
  type: TYPE_NORMAL
- en: We will also learn about the policy gradient, which will help you learn about
    the model in real time. We will then learn about a policy gradient technique called
    DDPG to understand the continuous action space. Here, we will also learn how to
    code the Lunar Lander simulation to understand DDPGs using classes such as the
    `OUActionNoise` class, the `ReplayBuffer` class, the `ActorNetwork` class, and
    the `CriticNetwork` class. We will learn about these classes in detail later in
    this chapter. Finally, we will learn how we can improve the policy gradient technique
    by using the TRPO, PPO, and **Advantage Actor-Critic** (**A2C**) techniques. These
    techniques will help us reduce the operating cost of training the model and so
    will improve the policy-gradient technique.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by learning about some basic concepts, such as value-based RL, model-based
    RL, actor-critic, and action space, in the following sub-sections.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Value-Based and Model-Based RL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While it is useful to have a good model of the environment to be able to predict
    whether a particular move is better with regard to other possible moves, you still
    need to assess all the possible moves from every possible state in order to come
    up with an optimal policy. This is a non-trivial problem and is also computationally
    expensive if, say, our environment is a simulation and our agent is **Artificial
    Intelligence** (**AI**). This approach of model-based learning, when applied within
    a simulation, can look like the following scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the game of *Pong* (*Figure 11.1*). (*Pong*—released in 1972—was one
    of the first arcade video games manufactured by Atari.) Now, let's see how the
    model-based learning approach could be beneficial for an optimal policy playing
    *Pong* and what could be its drawbacks. So, suppose our agent has learned how
    to play *Pong* by looking at the game environment—that is, by looking at the black
    and white pixels of each frame. We can then ask our agent to predict the next
    possible state given a certain frame of black and white pixels from the game environment.
    But if there is any background noise in the environment (for example, a random,
    unrelated video playing in the background), our agent would also take that into
    consideration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, in most cases, those background noises would not help us in our planning—that
    is, determining an optimal policy—but would still eat up our computational resources.
    Following is a screenshot of *Pong* game:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1: The Atari Pong game'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.1: The Atari Pong game'
  prefs: []
  type: TYPE_NORMAL
- en: A value-based approach is better than the model-based approach because while
    performing the transition from one state to another, a value-based approach would
    only care about the value of the action in terms of the cumulative reward that
    we are predicting for each action. It would deem any background noise as mostly
    irrelevant. A value-based approach is well-suited for deriving an optimal policy.
    Imagine you have learned an action-value function—a Q function. Then, you can
    simply look at the highest values in each state and that gives you the optimal
    policy. However, value-based functions could still be inefficient. Let me try
    to explain why with an example. In order to travel from Europe to North America,
    or from South Africa to the southern coasts of India, the optimal policy for our
    explorer ship might just be to go straight. However, the ship might encounter
    icebergs, small islands, or ocean currents that might set it off course temporarily.
    It might still be the optimal policy for the ship to head straight, but the value
    function might change arbitrarily. So, a value-based method, in this case, would
    try to approximate all the arbitrary values, while a policy can be blind and,
    therefore, be more efficient in terms of computational cost. So, in many cases,
    it might be less efficient to compute an optimal policy based on the value function.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Actor-Critic Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, we have briefly explained the trade-offs between the value-based and model-based
    approaches. Now, can we somehow take the best of both the worlds and create a
    hybrid of them? The actor-critic model will help us to do that. If we draw a Venn
    diagram (*Figure 11.2*), we will see that the actor-critic model lies at the intersection
    of the value-based and policy-based RL approaches. They can basically learn a
    value function as well as a policy. We will discuss actor-critic model further
    in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2: The relation between different RL approaches'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.2: The relation between different RL approaches'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, most of the time, we try to learn a policy based on the values
    yielded by the value function, but we actually learn the policy and the values
    simultaneously. To end this introduction to actor-critic, let me share a quote
    by Bertrand Russell. Russell, in his book *The Problems of Philosophy*, said:
    "*We can know the general proposition without inferring it from instances, although
    some instances are usually necessary to make clear to us what the general proposition
    means.*" Treat that as food for thought. The code on how to implement the actor-critic
    model is shown later in this chapter. Next, we will learn about action spaces,
    the basics of which we already covered in *Chapter 1*, *Introduction to Reinforcement
    Learning*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapters, we already covered the basic definition and the types
    of action spaces. Here, we will quickly revise the concept of action spaces. Action
    spaces define the properties of the game environment. Let''s look at the following
    diagram to understand the types:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3: Action spaces'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.3: Action spaces'
  prefs: []
  type: TYPE_NORMAL
- en: There are two types of action spaces—discrete and continuous. Discrete action
    spaces allow discrete inputs—for example, the buttons on a gamepad. These discrete
    actions can move in either the left or right direction, going either up or down,
    moving forward or backward, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, continuous action spaces allow continuous inputs—for example,
    inputs from a steering wheel or a joystick. In the following section, we will
    learn how to apply a policy gradient to a continuous action space.
  prefs: []
  type: TYPE_NORMAL
- en: Policy Gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have established the motivation behind favoring policy-based methods
    over value-based ones with the navigation example in the previous section, let''s
    begin our formal introduction to policy gradients. Unlike Q-learning, which uses
    a storage buffer to store past experiences, policy-gradient methods learn in real
    time (that is, they learn from the most recent experience or action). A policy
    gradient''s learning is driven by whatever the agent encounters in the environment.
    After each gradient update, the experience is discarded and the policy moves on.
    Let''s look at a pictorial representation of what we have just learned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4: The policy gradient method explained pictorially'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.4: The policy gradient method explained pictorially'
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing that should immediately catch our attention is that the policy gradient
    method is, in general, less sample-efficient than Q-learning because the experiences
    are discarded after each gradient update. The mathematical representation of the
    gradient estimator is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5: Mathematical representation of policy gradient estimator'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.5: Mathematical representation of policy gradient estimator'
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, ![a](img/B16182_11_05a.png) is the stochastic policy and ![b](img/B16182_11_05b.png)
    is our advantage estimation function at time ![c](img/B16182_11_05c.png) —the
    estimate of the relative value of the selected action. The expectation,![d](img/B16182_11_05d.png)
    , indicates the average over a finite batch of samples in our algorithm, where
    we perform sampling and optimization, alternatively. Here, ![e](img/B16182_11_05e.png)
    is the gradient estimator. The ![f](img/B16182_11_05f.png) and ![g](img/B16182_11_05g.png)
    variables define the action and state at the time interval, ![h](img/B16182_11_05h.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the policy gradient loss is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6: Policy gradient loss defined'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.6: Policy gradient loss defined'
  prefs: []
  type: TYPE_NORMAL
- en: In order to calculate the advantage function, ![a](img/B16182_11_06a.png) ,
    we need the **discounted reward** and the **baseline estimate**. The discounted
    reward is also known as the **return**, which is the weighted sum of all the rewards
    our agent got during the current episode. It is called the discounted reward as
    there is a discount factor associated with it that prioritizes the immediate rewards
    over the long-term ones. ![b](img/B16182_11_06b.png) is basically the difference
    between the discounted reward and the baseline estimate.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if you still have any problems with wrapping your head around the
    concept, then it's not a big problem. Just try to grasp the overall idea and you'll
    be able to grasp the full concept of it eventually. Having said that, let me also
    introduce you to a stripped-down version of the vanilla policy gradient algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by initializing the policy parameter, ![c](img/B16182_11_06c.png),
    and the baseline, ![d](img/B16182_11_06d.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: One suggestion would be to go through the algorithm multiple times, along with
    the initial explanation, in order to properly understand the concept of policy
    gradients. But again, an overall understanding of things should be your first
    priority.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before implementing the practical elements, please install OpenAI Gym and the
    Box2D environment (which includes environments such as Lunar Lander) using PyPI.
    To carry out the installation, type the following commands into Terminal/Command Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's implement an exercise using the policy gradient method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11.01: Landing a Spacecraft on the Lunar Surface Using Policy Gradients
    and the Actor-Critic Method'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will work on a toy problem (OpenAI Lunar Lander) and help
    land the Lunar Lander inside the OpenAI Gym Lunar Lander environment using vanilla
    policy gradients and actor-critic. The following are the steps to implement this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Jupyter Notebook, import all the necessary libraries (`gym`, `torch`,
    and `numpy`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `ActorCritic` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, during the initialization of the `ActorCritic` class in the preceding code,
    we are creating our action and value networks. We are also creating blank arrays
    for storing the log probabilities, state values, and rewards.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, create a function to pass our state through the layers and name it `forward`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we are taking the state and passing it through the value layer after a
    ReLU transformation to get the state value. Similarly, we are passing the state
    through the action layer, followed by a softmax function, to get the action probabilities.
    Then, we are transforming the probabilities to discrete values for the purpose
    of sampling. Finally, we are adding our log probabilities and state values to
    their respective arrays and returning an action item.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the `computeLoss` function to calculate a discounted reward first. This
    will help give greater priority to the immediate reward. Then, we will calculate
    the loss as described in the policy gradient loss equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a `clear` method to clear the arrays that store the log probabilities,
    state values, and rewards after each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s start with the main code, which will help us to call the classes
    that we defined previously in the exercise. We start by assigning a random seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we need to set up our environment and initialize our policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we iterate for at least `10000` iterations for proper convergence.
    In each iteration, we sample an action and get the state and reward for that action.
    Then, we update our policy based on that action and clear our observations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, when you run the code, you''ll see the running reward for each episode.
    The following is the reward for the first 20 episodes out of the total 10,000 episodes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The output for only the first 20 episodes is shown here for ease of presentation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3hDibst](https://packt.live/3hDibst).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This output indicates that our agent, the Lunar Lander, has started taking
    actions. The negative reward indicates that in the beginning, the agent is not
    smart enough to take the right actions and so it takes random actions, for which
    it is rewarded negatively. A negative reward is a penalty. With time, the agent
    will start getting positive rewards as it starts learning. Soon, you''ll see the
    game window popping up on your screen showing the real-time progress of your Lunar
    Lander, as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7: The real-time progress of the Lunar Lander'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.7: The real-time progress of the Lunar Lander'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look into DDPGs, which extend the idea of policy
    gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Deterministic Policy Gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will apply the DDPG technique to understand the continuous
    action space. Moreover, we will learn how to code a moon lander simulation to
    understand DDPGs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We suggest that you type all the code given in this section into your Jupyter
    notebook as we will be using it later, in *Exercise 11.02*, *Creating a Learning Agent*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to use the OpenAI Gym Lunar Lander environment for continuous
    action spaces here. Let''s start by importing the essentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will learn how to define some classes, such as the `OUActionNoise` class,
    the `ReplayBuffer` class, the `ActorNetwork` class, and the `CriticNetwork` class,
    which will help us to implement the DDGP technique. At the end of this section,
    you'll have the complete code base that applies the DDPG within our OpenAI Gym
    game environment.
  prefs: []
  type: TYPE_NORMAL
- en: Ornstein-Uhlenbeck Noise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will define a class that will provide us with something known as
    Ornstein-Uhlenbeck noise. This Ornstein–Uhlenbeck process, in physics, is used
    to model the velocity of a Brownian particle under the influence of friction.
    Brownian motion, as you may already know, is the random motion of particles when
    suspended in a fluid (liquid or gas) resulting from their collisions with other
    particles in the same fluid. Ornstein–Uhlenbeck noise gives you a type of noise
    that is temporally correlated and is centered on a mean of 0\. Since the agent
    has zero knowledge of the model, it becomes difficult to train it. Here, Ornstein–Uhlenbeck
    noise can be used as a sample to generate that knowledge. Let''s look at the code
    implementation of this class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we defined three different functions—that is, `_init_()`,
    `_call()_`, and `reset()`. In the next section, we will learn how to implement
    the `ReplayBuffer` class to store the agent's past learnings.
  prefs: []
  type: TYPE_NORMAL
- en: The ReplayBuffer Class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Replay buffer is a concept we have borrowed from Q-learning. This buffer is
    basically a space to store all of our agent''s past learnings, which will help
    us to train the model better. We will initialize the class by defining the memory
    size for our state, action, and rewards, respectively. So, the initialization
    would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to define the `store_transition` method. This method takes the
    state, action, reward, and new state as arguments and stores the transitions from
    one state to another. There''s also a `done` flag to indicate the terminal state
    of our agent. Note that the index here is just a counter that we initialized previously,
    and it starts from `0` when its value is equal to the maximum memory size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need the `sample_buffer` method, which will be used to randomly
    sample the buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the entire class, at a glance, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we learned how to store the agent's past learnings to train
    the model better. Next, we will learn in more detail about the actor-critic model,
    which we briefly explained in this chapter's introduction.
  prefs: []
  type: TYPE_NORMAL
- en: The Actor-Critic Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, in the DDPG technique, we will define the actor and critic networks. Now,
    we have already introduced actor-critic, but we haven't talked much about it.
    Take the actor as the current policy and the critic as the value. You may conceptualize
    the actor-critic model as a guided policy. We will define our actor-critic model
    using fully connected neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The `CriticNetwork` class starts with an initialization. First, we will explain
    the parameters. The `Linear` layer and we will initialize it using our input and
    output dimensions. Next, is the initialization of the weights and biases of our
    fully connected layer. This initialization restricts the values of the weights
    and biases to a very narrow band of the parameter space when we sample between
    the `-f1` to `f1` range, as seen in the following code. This helps our network
    to better converge. Our initial layer is followed by a batch normalization, which
    again helps to better converge our network. We will repeat the same process with
    our second fully connected layer. The `CriticNetwork` class will also get an action
    value. Finally, the output is a single scalar value, which we will initialize
    next with a constant initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will optimize our `CriticNetwork` class using the `Adam` optimizer with
    a learning rate beta:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to write the `forward` function for our network. This takes a
    state and an action as input. We get the state-action value from this method.
    So, our state goes through the first fully connected layer, followed by the batch
    normalization and the ReLU activation. The activation is passed through the second
    fully connected layer, followed by another batch normalization, and before the
    final activation, we take into account the action value. Notice that we are adding
    the state and action values together to form the state-action value. The state-action
    value is then passed through the final layer and there we have our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'So, finally, the `CriticNetwork` class would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define `ActorNetwork`. This would be mostly similar to the `CriticNetwork`
    class but with some minor yet important changes. Let''s code it first and then
    we will explain it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this is similar to our `CriticNetwork` class. The main difference
    here is that we don't have an action value here and that we have written the `forward`
    function in a slightly different way. Notice that the final output from the `forward`
    function is a `tanh` function, which will bind our output between `0` and `1`.
    This is necessary for the environment we are going to play around with. Let's
    implement an exercise that will help us to create a learning agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11.02: Creating a Learning Agent'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will write our `Agent` class. We are already familiar with
    the concept of a learning agent, so let's see how we can implement one. This exercise
    will conclude the DDPG example that we have been building. Please make sure that
    you have run all the example code in this section before starting the exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We have assumed you have typed the code presented in the preceding section into
    a new notebook. Specifically, we have assumed you already have the code for importing
    the necessary libraries and creating the `OUActionNoise`, `ReplayBuffer`, `CriticNetwork`,
    and `ActorNetwork` classes in your notebook. This exercise begins by creating
    the `Agent` class.
  prefs: []
  type: TYPE_NORMAL
- en: For convenience, the complete code for this exercise, including the code in
    the example, can be found at [https://packt.live/37Jwhnq](https://packt.live/37Jwhnq).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps to implement this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by using the `__init__` method and passing the alpha and the beta,
    which are the learning rates for our actor and critic networks, respectively.
    Then, pass the input dimensions and a parameter called `tau`, which we will explain
    in a bit. Then, we want to pass the environment, which is our continuous action
    space, gamma, which is the agent''s discount factor, which we talked about earlier.
    Then, the number of actions, the maximum size of the memory, the size of the two
    layers, and the batch size are passed. Then, initialize our actor and critic.
    Finally, we will introduce our noise and the `update_params` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `update_params` function updates our parameters, but there's a catch. We
    basically have a moving target. This means we are using the same network to calculate
    the action and the value of the action simultaneously as we are updating the estimate
    in every episode. Because we are using the same parameters for both, it may lead
    to divergence. To tackle that, we use the target network, which learns the value
    and the action combinations, and the other network is used to learn the policy.
    We will periodically update the target network's parameters with the parameters
    of the evaluation network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we have the `select_action` method. Here, we take the observation from
    our actor and pass it through the feed-forward network. `mu_prime` here is basically
    the noise we add to the network. It is also called exploration noise. Finally,
    we call `actor.train()` and return the `numpy` value for `mu_prime`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next comes our `remember` function, which is self-explanatory. This takes the
    `state`, `action`, `reward`, `new_state`, and `done` flags in order to store them
    in memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will define the `learn` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we first check whether we have enough samples in our memory buffer for
    learning. So, if our memory counter is less than the batch size—meaning we do
    not have the batch size number of samples in our memory buffer—we simply return
    the value. Otherwise, we sample from our memory buffer the `state`, `action`,
    `reward`, `new_state`, and `done` flags. Once sampled, we must convert all of
    these flags into tensors for implementation. Then, we need to calculate the target
    actions, followed by the calculation of the new critic value using the target
    action states and the new state. Next, we calculate the critic value, which is
    the value we met for the states and actions in the current replay buffer. After
    that, we calculate the targets. Note that the part where we multiply `gamma` with
    the new critic value becomes `0` when the `done` flag is `0`. This basically means
    that when the episode is over, we only take into account the reward from the current
    state. The target is then converted into a tensor and reshaped for implementation
    purposes. Now, we can calculate and backpropagate our loss for the critic. Then,
    we do the same for our actor network. Finally, we update the parameters for our
    target actor and target critic network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, define the `update_params` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, the `update_params` function takes a `tau` value, which basically allows
    us to update the target network in very small steps. The value for `tau` is typically
    very small, much smaller than `1`. One thing to note is that we start with `tau`
    equal to `1` but later, the value is reduced to a much smaller number. What the
    function does is that it first gets all the names of the parameters for the critic,
    actor, target critic, and target actor. It then updates those parameters with
    the target critic and target actor. Now, we can create the main part of our Python code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you have created the `Agent` class properly, then, along with the preceding
    example code, you''ll be able to initialize our learning agent with the following
    bit of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the output, you''ll see the reward for each episode. Here is the output
    for the first 10 episodes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: What you see in the preceding output is that the reward oscillates between negative
    and positive. That is because until now, our agent was sampling random actions
    from all the actions it could take.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/37Jwhnq](https://packt.live/37Jwhnq).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next activity, we will make the agent remember its past learnings and
    learn from them. Here''s how the game environment will look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.8: The output window showing the Lunar Lander hovering'
  prefs: []
  type: TYPE_NORMAL
- en: in the game environment
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.8: The output window showing the Lunar Lander hovering in the game
    environment'
  prefs: []
  type: TYPE_NORMAL
- en: However, you'll find that the Lander doesn't attempt to land, and rather, it
    hovers over the lunar surface in our game environment. That's because we haven't
    enabled the agent to learn yet. We will do that in the following activity.
  prefs: []
  type: TYPE_NORMAL
- en: In the next activity, we will create an agent that will help to learn a model
    using DDPG.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 11.01: Creating an Agent That Learns a Model Using DDPG'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, we will implement what we have learned in this section and
    create an agent that learns through DDPG.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We have created a Python file for the actual DDPG implementation to be imported
    as a module using `from ddpg import *`. The module and the code of the activity
    can be downloaded from GitHub at [https://packt.live/2YksdXX](https://packt.live/2YksdXX).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the steps to perform for this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary libraries (`os`, `gym`, and `ddpg`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, we create our Gym environment (`LunarLanderContinuous-v2`), as we did
    previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the agent with some sensible hyperparameters, as in *Exercise 11.02*,
    *Creating a Learning Agent*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up a random seed so that our experiments are reproducible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a blank array to story the scores; you can name it `history`. Iterate
    for at least `1000` episodes and in each episode, set a running score variable
    to `0` and the `done` flag to `False`, then reset the environment. Then, when
    the `done` flag is not `True`, carry out the following step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an action from the observations and get the new `state`, `reward`, and
    `done` flags. Save the `observation`, `action`, `reward`, `state_new`, and `done`
    flags. Call the `learn` function of the agent and add the current reward to the
    running score. Set the new state as the observation and finally, when the `done`
    flag is `True`, append `score` to `history`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To observe the rewards, we can simply add a `print` statement. The rewards will
    be similar to those in the previous exercise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following is the expected simulation output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.9: Screenshots from the environment after 1,000 rounds of training'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_11_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 11.9: Screenshots from the environment after 1,000 rounds of training'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 766.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how we can improve the policy gradient approach
    that we just implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Improving Policy Gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn the various approaches that will help us improve
    the policy gradient approach that we learned about in the previous section. We
    will learn about techniques such as TRPO and PPO.
  prefs: []
  type: TYPE_NORMAL
- en: We will also learn about the A2C technique in brief. Let's understand the TRPO
    optimization technique in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Trust Region Policy Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most cases, RL is very sensitive to the initialization of weights. Take,
    for instance, the learning rate. If our learning rate is too high, then it may
    so happen that our policy update takes our policy network to a region of the parameter
    space where the next batch of data it collects is gathered against a very poor
    policy. This might cause our network to never recover again. Now, we will talk
    about newer methods that try to get rid of this problem. But before we do that,
    let's have a quick recap of what we have already covered.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Policy Gradients* section, we defined the estimator of the advantage
    function, ![b](img/B16182_11_9a.png) , as the difference between the discounted
    reward and the baseline estimate. Intuitively, the advantage estimator quantifies
    how good the action taken by our agent in a certain state was compared to what
    would typically happen in that state. One problem with the advantage function
    is that if we simply keep on updating our weights based on one batch of samples
    using gradient descent, then our parameter updates might stray far from the range
    where the data was sampled from. That could lead to an inaccurate estimate of
    the advantage function. In short, if we keep running gradient descent on a single
    batch of experiences, we might corrupt our policy.
  prefs: []
  type: TYPE_NORMAL
- en: One way to make sure this problem doesn't occur is to ensure that the updated
    policy doesn't differ too much from the old policy. This is basically the main
    crux of TRPO.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already understand how the gradient estimator works for vanilla policy gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10: The vanilla policy gradient method'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.10: The vanilla policy gradient method'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how it looks for TRPO:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11: Mathematical representation of TRPO'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.11: Mathematical representation of TRPO'
  prefs: []
  type: TYPE_NORMAL
- en: The only change here is that the log operator in the preceding equation has
    been replaced by a division by ![b](img/B16182_11_11a.png). This is known as the
    TRPO objective and optimizing it yields the same result as the vanilla policy
    gradients. In order to ensure that the new and updated policy doesn't differ much
    from the old policy, TRPO introduces a constraint known as the KL constraint.
  prefs: []
  type: TYPE_NORMAL
- en: This constraint, in simple words, makes sure that our new policy doesn't stray
    too far from the old one. Note that the actual TRPO strategy, however, proposes
    a penalty instead of a constraint.
  prefs: []
  type: TYPE_NORMAL
- en: Proximal Policy Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It might seem like everything is fine and good for TRPO, but the introduction
    of the KL constraint introduces an additional operating cost to our policy. To
    address that problem, and to basically solve the problems with vanilla policy
    gradients once and for all, the researchers at OpenAI have introduced PPO, which
    we will look into now.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main motivations behind PPO are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ease of implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ease of parameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One thing to note is that the PPO method doesn't use a replay buffer to store
    past experiences and learns straight from whatever the agent encounters in the
    environment. This is also known as an `online` method of learning while the former—using
    a replay buffer to store past experiences—is known as an `offline` method of learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of PPO define a probability ratio, ![a](img/B16182_11_11b.png),
    which is basically the probability ratio between the new and the old policy. So,
    we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12: The probability ratio between the old and new policy'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.12: The probability ratio between the old and new policy'
  prefs: []
  type: TYPE_NORMAL
- en: 'When provided with a sampled batch of actions and states, this ratio would
    be greater than `1` if the action is more likely now than in the old policy. Otherwise,
    it would remain between `0` and `1`. Now, the final objective of PPO when written
    down looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13: The final objective of PPO'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.13: The final objective of PPO'
  prefs: []
  type: TYPE_NORMAL
- en: Let's explain. Like the vanilla policy gradient, PPO tries to optimize the expectation
    and so we compute this expectation operator over batches of trajectories. Now,
    this is a minimum value of the modified policy gradient objective that we saw
    in TRPO and the second part is a clipped version of it. The clipping operation
    keeps the policy gradient objective between ![a](img/B16182_11_13a.png) and ![b](img/B16182_11_13b.png).
    ![c](img/B16182_11_13c.png) here is a hyperparameter, often equal to `0.2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the function looks simple at a glance, its ingenuity is remarkable.
    ![d](img/B16182_11_13d.png) can be both negative and positive, suggesting a negative
    advantage estimation and a positive advantage estimation, respectively. This behavior
    of the advantage estimator determines how the `min` operator works. Here''s an
    illustration of the `clip` parameter from the actual PPO paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.14: The clip parameter illustration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16182_11_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.14: The clip parameter illustration'
  prefs: []
  type: TYPE_NORMAL
- en: The plot on the left is where the advantage is positive. This means our actions
    yielded results that were better than expected. On the other plot on the right
    are the cases where our actions yielded less than the expected return.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how on the plot on the left the loss flattens out when `r` is too high.
    This might occur when the current action is much more plausible under the current
    policy than the old one. In this case, the objective function is clipped here,
    thereby ensuring the gradient update doesn't go beyond a certain limit.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, when the objective function is negative, the loss flattens
    when `r` is approaching `0`. This relates to actions that are more unlikely under
    the current policy than the old one. It may be clear by now how the clipping operation
    keeps the updates to the network parameters within a desirable range. It would
    be a better approach to learn about the PPO technique while implementing it. So,
    let's start with an exercise to reduce the operating cost of our policy using
    PPO.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11.03: Improving the Lunar Lander Example Using PPO'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll implement the Lunar Lander example using PPO. We will
    follow almost the same structure as before and you''ll be able to follow through
    this exercise easily if you have gone through the previous exercises and examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Jupyter Notebook and import the necessary libraries (`gym`, `torch`,
    and `numpy`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set our device as we did in the DDPG example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create the `ReplayBuffer` class. Here, we will create arrays
    to store the actions, states, log probabilities, reward, and terminal states:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will define our `ActorCritic` class. We will define our `action` and
    `value` layers first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will define methods to sample from the action space and evaluate the
    log probabilities of the actions, state value, and entropy of the distribution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, the `ActorCritic` class looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will define our `Agent` class using the `__init__()` and `update()`
    functions. First let''s define `__init__()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let''s define the `update` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, normalize the rewards and convert them to tensors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, find the probability ratio, find the loss and propagate our loss backwards:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the old policy with the new weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, here in *steps 6-10* of this exercise we are defining an agent by starting
    with the initialization of our policy, the optimizer, and the old policy. Then,
    in the `update` function, we are at first taking the Monte Carlo estimate of the
    state rewards. After normalizing the rewards, we are converting them into tensors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we are carrying out the policy optimization for `K_epochs`. Here, we have
    to find the probability ratio, ![a](img/B16182_11_14a.png), which is the probability
    ratio between the new and the old policy, as described previously.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After that, we are finding the loss, ![b](img/B16182_11_14b.png), and propagating
    our loss backward. Finally, we are updating our old policy with the new weights.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we can run the simulation as we did in the previous exercise and save
    the policy for future use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the first 10 lines of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are saving our policy at certain intervals. This is useful if you
    want to load the policy at a later stage and simply run the simulation from there.
    The simulation output of this exercise would be the same as in *Figure 11.9*,
    only the operating cost is reduced here.
  prefs: []
  type: TYPE_NORMAL
- en: Here, if you look at the difference between the rewards, the points given in
    each consequent episode are much less as we have used the PPO technique. That
    means the learning is not going haywire as it was in *Exercise 11.01*, *Landing
    a Spacecraft on the Lunar Surface Using Policy Gradients and the Actor-Critic
    Method*, where the difference between the rewards was higher.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2zM1Z6Z](https://packt.live/2zM1Z6Z).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  prefs: []
  type: TYPE_NORMAL
- en: We have almost covered all the important topics relating to policy-based RL.
    So, now we will talk about the last topic, which is the A2C method.
  prefs: []
  type: TYPE_NORMAL
- en: The Advantage Actor-Critic Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already learned about the actor-critic method and the reason for using
    it in the introduction, and we have also seen it used in our coding examples.
    But, a quick recap—actor-critic methods lie at the intersection of the value-based
    and policy-based methods, where we simultaneously update our policy and our value,
    which acts as a judge quantifying how good our policy actually is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will learn how A2C works:'
  prefs: []
  type: TYPE_NORMAL
- en: We start by initializing the policy parameter, ![b](img/B16182_11_14c.png),
    with random weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we play *N* number of steps with the current policy, ![a](img/B16182_11_14d.png)
    , and store the state, action, reward, and transitions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set our reward to `0` if we reach the final episode of the state; otherwise,
    we set the reward to the value of the current state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we calculate the discounted reward, policy loss, and value loss by looping
    backward from the final episode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we apply **Stochastic Gradient Descent** (**SGD**) using the mean policy
    and value loss for each batch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the steps from *step 2* onward until it reaches convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have only briefly introduced A2C here. A detailed description and implementation
    of this method is beyond the scope of this book.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The first coding example (*Exercise 11.01*, *Landing a Spacecraft on the Lunar
    Surface Using Policy Gradients and the Actor-Critic Method*) that we covered follows
    the basic A2C method. However, there's another technique, called the **Asynchronous
    Advantage Actor-Critic** (**A3C**) method. Remember, our policy gradient methods
    work online. That is, we only train on the data obtained using the current policy
    and we do not keep track of past experiences. However, to keep our data independent
    and identically distributed, we need a large buffer of transitions. The solution
    provided by A3C is to run multiple training environments in parallel to acquire
    large amounts of training data. With multiprocessing in Python, this actually
    becomes very fast in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next activity, we will write code to run the Lunar Lander simulation
    that we learned about in *Exercise 11.03*, *Improving the Lunar Lander Example
    Using PPO*. We will also render the environment to see the Lunar Lander. To do
    that, we will have to import the PIL library. The code to render the image is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Let's begin with the implementation of our final activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 11.02: Loading the Saved Policy to Run the Lunar Lander Simulation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, we will combine multiple aspects of RL that we have explained
    in previous sections. We will use what we learned in *Exercise 11.03*, *Improving
    the Lunar Lander Example Using PPO*, to write simple code to load the saved policy.
    This activity combines all the essential components of building a working RL prototype—in
    our case, the Lunar Lander simulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to take are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Open Jupyter and in a new notebook, import the essential Python libraries, including
    the PIL library to save the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set your device using the device parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the `ReplayBuffer`, `ActorCritic`, and `Agent` classes. We already defined
    these in the previous exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the Lunar Lander environment. Initialize the random seed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the memory buffer and initialize the agent with hyperparameters, as in
    the previous exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the saved policy as an old policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, loop through your desired number of episodes. In every iteration, start
    by initializing the episode reward as `0`. Do not forget to reset the state. Run
    another loop, specifying the `max` timestamp. Get the `state`, `reward`, and `done`
    flags for each action taken and add the reward to the episode reward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Render the environment to see how your Lunar Lander is doing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the expected output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following screenshot shows the simulation output of some of the stages:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.15: The environment showing the simulation of the Lunar Lander'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16182_11_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 11.15: The environment showing the simulation of the Lunar Lander'
  prefs: []
  type: TYPE_NORMAL
- en: The complete simulation output can be found in the form of images at [https://packt.live/3ehPaAj](https://packt.live/3ehPaAj).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 769\.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about policy-based methods, principally the drawbacks
    to value-based methods such as Q-learning, which motivate the use of policy gradients.
    We discussed the purposes of policy-based methods of RL, along with the trade-offs
    of other RL approaches.
  prefs: []
  type: TYPE_NORMAL
- en: You learned about the policy gradients that help a model to learn in a real-time
    environment. Next, we learned how to implement the DDPG using the actor-critic
    model, the `ReplayBuffer` class, and Ornstein–Uhlenbeck noise to understand the
    continuous action space. We also learned how you can improve policy gradients
    by using techniques such as TRPO and PPO. Finally, we talked in brief about the
    A2C method, which is an advanced version of the actor-critic model.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in this chapter, we played around with the Lunar Lander environment in
    OpenAI Gym—for both continuous and discrete action spaces—and coded the multiple
    policy-based RL approaches that we discussed.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about a gradient-free method to optimize
    neural networks and RL-based algorithms. We will then discuss the limitations
    of gradient-based methods. The chapter presents an alternative optimization solution
    to gradient methods through genetic algorithms as they ensure global optimum convergence.
    We will also learn about the hybrid neural networks that use genetic algorithms
    to solve complex problems.
  prefs: []
  type: TYPE_NORMAL
