<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Sensing Market Sentiment for Algorithmic Marketing at Sell Side
                </header>
            
            <article>
                
<p>In the previous chapter, we learned about investment portfolio management. We also learned some of the portfolio management techniques, such as the Markowitz mean-variance model and the Treynor–Black model for portfolio construction. We also learned about how to predict a trend for a security. So, the previous chapter was based on the buy side of a market. It depicted the behavior of portfolio managers or asset managers.</p>
<p>In this chapter, we will look at the sell side of the market. We will understand the behavior of the counterpart of the portfolio managers. <span>Sell side refers to securities firms/investment banks and their main services, including sales, trading, and research. Sales refers to the marketing of securities to inform investors about the securities available for selling. Trading refers to the services that investors use to buy and sell off securities and the research performed to assist investors in evaluating securities. Being client-centric, one of the key functions of a bank is sensing the needs and sentiments of the end investors, who in turn push the asset managers to buy the product from banks. We will begin this chapter by looking at a few concepts and techniques. We will look at an example that illustrates how to sense the needs of an investor. We will look at another example to analyze the annual report and extract information from it.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Understanding sentiment analysis</li>
<li>Sensing market requirements using sentiment analysis</li>
<li>Network building and analysis using Neo4j</li>
</ul>
<h1 id="uuid-021c672d-b89c-4c2f-8598-904c2878d0e9">Understanding sentiment analysis</h1>
<p>Sentiment analysis is a technique in which text mining is done for contextual information. The contextual information is identified and extracted from the source material. It helps businesses understand the sentiment for their products, securities, or assets. It can be very effective to use the advanced techniques of artificial intelligence for in-depth research in the area of text analysis. It is important to classify the transactions around the following concepts:</p>
<ul>
<li>The aspect of security the buyers and sellers care about</li>
<li>Customers' intentions and reactions concerning the securities</li>
</ul>
<p>Sentiment analysis is known to be the most common text analysis and classification tool. It receives an incoming message or transaction and classifies it depending on whether the sentiment associated with the transaction is positive, negative, or neutral. By using the sentiment analysis technique, it is possible to input a sentence and understand the sentiment behind the sentence.</p>
<p>Now that we have understood what sentiment analysis is, let's find out how to sense market requirements in the following section.</p>
<h1 id="uuid-7f3cec8c-aed5-4420-93f8-12730e28f536">Sensing market requirements using sentiment analysis</h1>
<p>One of the key requirements of a security firm/investment bank on the sell side is to manufacture the relevant securities for the market. We have explored the fundamental behaviors and responsibilities of companies in <a href="0c281efb-a1b8-423f-976b-0fa47f5da990.xhtml">Chapter 4</a>, <em>Mechanizing Capital Market Decisions</em>, and <a href="bbb73cab-df58-462a-8b5e-c1574611aff2.xhtml">Chapter 5</a>, <em>Predicting the Future of Investment Bankers</em>. We learned about the momentum approach in <a href="0e7c4e25-941b-4bd6-a04a-55924bdbaa43.xhtml">Chapter 6</a>, <em>Automated Portfolio Management Using the Treynor–Black Model and ResNet</em>. While the market does not always act rationally, it could be interesting to hear about the market's feelings. That is what we will be doing in this chapter.</p>
<p>In this example, we will be playing the role of the salesperson of an investment bank on the trading floor, trading in equities. What we want to find out is the likes and dislikes regarding securities so that they can market the relevant securities, including derivatives. We got our insights from Twitter Search, and the stock price from Quandl. All of this data requires a paid license.</p>
<h2 id="uuid-760fdf3a-fa5c-4c36-8c1f-c94516a9de3c">Solution and steps</h2>
<p>There are a total of three major steps <span>to get the market sentiment using </span><span>coding implementation. The data is used as shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1046 image-border" src="assets/f4bbac19-98e7-4dc2-8e09-d4c83ca5d8d0.png" style="width:32.25em;height:22.67em;"/></p>
<p>The steps are as follows:</p>
<ol>
<li>Data will be retrieved from Twitter and be saved locally as a JSON file.</li>
<li>The JSON file will then be read, further processed by counting the positive and negative words, and input as records into a SQL Lite database.</li>
<li>Lastly, the sentiment will be read from the database and compared against stock prices retrieved from Quandl.</li>
</ol>
<p>We will elaborate on these steps in more detail in the following sections.</p>
<h3 id="uuid-b41291f4-d776-4180-be79-32f0133e53fa">Downloading the data from Twitter</h3>
<p>By using a <span>Twitter Search </span><span>commercial license, we download data on the same industry as defined by the Shalender (Quandl) industry classification. We will use the API key to search and download the latest 500 tweets containing or tagged with the company name, one by one. All tweets are received in JSON format, which looks like a Python dictionary. The JSON file will then be saved on the computer for further processing.</span></p>
<p>Sample Python codes can be found on<span> G</span>itHub (<a href="https://github.com/twitterdev/search-tweets-python">https://github.com/twitterdev/search-tweets-python</a>), especially regarding authentication. The following is the code snippet for downloading tweets from Twitter:<a href="https://github.com/twitterdev/search-tweets-python"/></p>
<pre>'''*************************************<br/>#1. Import libraries and key variable values<br/><br/>'''<br/>from searchtweets import ResultStream, gen_rule_payload, load_credentials<br/>from searchtweets import collect_results<br/>import json<br/>import os<br/><br/>script_dir = os.path.dirname(__file__)<br/>#Twitter search commerical accounts credential<br/>premium_search_args = load_credentials("~/.twitter_keys.yaml",<br/>                                      env_overwrite=False)<br/>MAX_RESULTS=500 #maximum at 500<br/><br/>#list of companies in the same industry<br/>...<br/><br/>'''*************************************<br/>#2. download tweets of each company<br/><br/>'''<br/>for comp in comp_list:<br/>   ...</pre>
<h3 id="uuid-7b39b043-dab1-41f5-92d5-3f936e32b847" class="mce-root">Converting the downloaded tweets into records</h3>
<p class="mce-root">The tweet's message and any linked page will then be loaded and read by a simple language processing program, which will count the number of positive and negative words in the message and linked page body. The parsed tweet will be converted to a structured SQL database format and stored in a SQL Lite database.</p>
<p>The following is the code snippet to convert tweets into records:</p>
<pre>'''*************************************<br/>#1. Import libraries and key variable values<br/><br/>'''<br/>import json<br/>import os<br/>import re<br/>import sqlite3<br/>import 7A_lib_cnt_sentiment as sentiment<br/><br/>#db file<br/>db_path = 'parsed_tweets.db'<br/>db_name = 'tweet_db'<br/><br/>#sql db<br/>...<br/>#load tweet json<br/>...<br/>#loop through the tweets<br/>    ...<br/>    for tweet in data:<br/>        ...<br/>        tweet_txt_pos,tweet_txt_neg = sentiment.cnt_sentiment(tweet_txt)<br/>        keywords,sentences_list,words_list = \<br/>                                           sentiment.NER_topics(tweet_txt)<br/>        ...<br/>        if len(url_link)&gt;0:<br/>            ...<br/>            url_txt = sentiment.url_to_string(url)<br/>            temp_tweet_link_txt_pos, temp_tweet_link_txt_neg = \<br/>                                           sentiment.cnt_sentiment(url_txt)<br/>            link_keywords,link_sentences_list,link_words_list = \<br/>                                           sentiment.NER_topics(tweet_txt)<br/>            ...</pre>
<p>There are three functions that are called by the preceding program. One is used to count the positive and negative words, one looks at the topic concerned, and one retrieves the text in the URL given in the tweet.</p>
<p>The following code snippet defines the functions used in the program:</p>
<pre>import os<br/>import requests<br/>from bs4 import BeautifulSoup<br/>import re<br/>import spacy<br/>import en_core_web_sm<br/>nlp = en_core_web_sm.load()<br/><br/>...<br/>#cal the positive and negative sentiment words given the text<br/>def cnt_sentiment(text_to_be_parsed):<br/>    ...<br/><br/>def noun_phrase(sentence,item_list,lower):<br/>   ...<br/><br/>#NER<br/>import spacy<br/>from spacy import displacy<br/>from collections import Counter<br/>import math<br/><br/>#text has to be less than 1000000<br/>def NER_topics(text_to_be_parsed):<br/>    ...<br/>    MAX_SIZE =100000<br/>    ...<br/>    for nlp_cnt in range(number_nlp):<br/>        start_pos = nlp_cnt*MAX_SIZE<br/>        end_pos = min(MAX_SIZE,txt_len-start_pos)+start_pos-1<br/>        txt_selected = text_to_be_parsed[start_pos:end_pos]<br/>        ...<br/>        sentences_list = [x for x in article.sents]<br/>        full_sentences_list+=sentences_list<br/>        for sent in sentences_list:<br/>            phrases_list =[]<br/>            phases_list,items_list = noun_phrase(sent, items_list, \<br/>                                                 lower=True)<br/>     ...<br/><br/>#convert the URL's content into string<br/>def url_to_string(url):<br/>    ...</pre>
<h3 id="uuid-ac3b6ad5-8a0b-47f8-80b4-30795d528bc5" class="mce-root">Performing sentiment analysis</h3>
<p class="mce-root">The database that stored the parsed tweet will be read by another program. For each record, the sentiment will be represented by aggregate sentiment on a daily basis. Each tweet's sentiment is calculated as the total number of negative sentiments subtracted from positive sentiments. The range of this sentiment score should be in the range of -1 to +1, with -1 representing a totally negative score and +1 a totally positive score. Each day’s sentiment score is calculated as the average of all the tweets' sentiment scores for the security. Sentiment scores of all securities in the same industry are plotted on a graph, similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1047 image-border" src="assets/2c1f9e69-f7f9-4bf3-a0d5-73c88093fa9c.png" style="width:148.75em;height:65.75em;"/></p>
<p><span>For example, in the short period of our coverage, Dominion Energy has one of the most favorable sentiment scores (between Oct 29 and Oct 30).</span></p>
<p>The sample output of Dominion Energy is shown in the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1048 image-border" src="assets/f65f340f-ad37-4161-92dc-02daf067caf3.png" style="width:151.25em;height:74.92em;"/></p>
<p><span>The sentiment is the orange line and the price is the blue line (please refer to the color graph provided in the graphic bundle of this book).</span></p>
<p>The following is the code snippet for sentiment analysis:</p>
<pre>'''*************************************<br/>#1. Import libraries and key variable values<br/><br/>'''<br/>import sqlite3<br/>import pandas as pd<br/>import plotly<br/>import plotly.graph_objs as go<br/>import quandl<br/>import json<br/><br/># Create your connection.<br/>db_path = 'parsed_tweets.db'<br/>cnx = sqlite3.connect(db_path)<br/>db_name = 'tweet_db'<br/><br/>'''*************************************<br/>#2. Gauge the sentiment of each security<br/><br/>'''<br/>...<br/>sql_str = ...<br/>...<br/>print('Sentiment across securities')<br/>field_list = ['positive','negative']<br/>for sec in sec_list:<br/>    ...</pre>
<h3 id="uuid-d2da7624-54df-45f8-b7dc-70afc3829b54">Comparing the daily sentiment against the daily price</h3>
<p>After we obtain the sentiment score for each stock, we also want to know the predictive power or the influence of the sentiment on the stock price. The stock price of the day is calculated by the middle-of-day high and low. For each stock, we plot and compare the sentiment and stock price over a period of time. The following screenshot is an i<span>llustration of PG&amp;E Corp's sentiment versus stock price:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1049 image-border" src="assets/057a323b-721b-4faa-810c-fbe1aac920d0.png" style="width:151.00em;height:70.50em;"/></p>
<p>The following is the code snippet for daily sentiment analysis data against the daily price:</p>
<pre>#run it on different companies<br/>print('Retrieve data')<br/>df_comp = pd.read_csv('ticker_companyname.csv')<br/>corr_results={}<br/><br/>for index, row in df_comp.iterrows():<br/>    tkr = row['ticker']<br/>    name = row['name']<br/><br/>    target_sec = '"'+name +'"data.json'<br/>    <br/>    corr_result = price_sentiment(tkr,target_sec,date_range)<br/>    try:<br/>        corr_results[name]=corr_result['close'][0]<br/>    except Exception:<br/>        continue<br/><br/>f_corr = open('corr_results.json','w')<br/>json.dump(corr_results,f_corr)<br/>f_corr.close()</pre>
<p>Congratulations! You have developed a program to assist sales in finding popular securities to develop products for.</p>
<p>From what we have seen, comparing this example to the technical analysis examples, we can see that the information from the sentiment is far higher than the technical trend. So far, we have only looked at the primary impact of the trend, fundamental, and sentiment; however, companies are interconnected in our society. So how can we model the linkage of firms and individuals? This brings us to the next topic—network analysis.</p>
<h1 id="uuid-ca78cec6-e8f1-4abb-b3b4-130e0f95f7b4">Network building and analysis using Neo4j</h1>
<p>As sell-side analysts, besides finding out the primary impact of news on the company, we should also find out the secondary effect of any news. In our example, we will find out the suppliers, customers, and competitors of any news on the stocks.</p>
<p class="mce-root">We can do this using three approaches:</p>
<ul>
<li class="mce-root">By means of direct disclosure, such as annual reports</li>
<li class="mce-root">By means of secondary sources (media reporting)</li>
<li class="mce-root">By means of industry inferences (for example, raw materials industries, such as oil industries, provide the output for transportation industries)</li>
</ul>
<p class="mce-root">In this book, we use direct disclosure from the company to illustrate the point.</p>
<p>We are playing the role of equity researchers for the company stock, and one of our key roles is to understand the relevant parties' connections to the company. We seek to find out the related parties of the company—Duke Energy—by reading the company's annual report.</p>
<h2 id="uuid-57bfe35f-ba88-4f34-9796-f0c5822cbca8" class="mce-root">Solution</h2>
<p>There are a total of four steps. The following diagram shows the data flow:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1050 image-border" src="assets/11af5ce3-eb8c-41bd-89dd-3add22b17e89.png" style="width:24.08em;height:17.75em;"/></p>
<p>We will now look at the steps in more detail in the following sections.</p>
<h3 id="uuid-f6e7f7a0-7fbd-497d-b53a-3036dad3896f" class="mce-root">Using PDFMiner to extract text from a PDF</h3>
<p class="mce-root">Besides storage, we also need to extract the relationship from text documents. Before we can start dealing with text, we need to convert the PDF data to text. To do this, we use a library called <strong>PDFMiner</strong> (specifically, the module is called<span> </span><strong>pdfminer.six</strong> (<a href="https://github.com/pdfminer/pdfminer.six">https://github.com/pdfminer/pdfminer.six</a>) for Python 3+). PDF is an open standard to describe a document. It stores the lines, text, images, and their exact locations in the document. We will only be using a basic function in PDFMiner to extract the texts from it. Even though we could extract the coordinates, we will skip this to simplify our work. Upon extracting the text, we append all lines into one super long line.</p>
<p>The following code snippet imports the necessary libraries and initializes a PDF file to be processed:</p>
<pre>'''*************************************<br/>#1. Import relevant libraries and variables<br/><br/>'''<br/>#custom made function<br/>import 7B_lib_entitiesExtraction as entitiesExtraction<br/>import 7B_lib_parser_pdf as pdf_parser<br/>import json<br/>import sqlite3<br/><br/>pdf_path = 'annualrpt/NYSE_DUK_2017.pdf'<br/>...</pre>
<h3 id="uuid-e76d593e-36ba-45f8-a15f-9a1f0f39e4b0" class="mce-root">Entity extractions</h3>
<p class="mce-root">We deploy a linguistic analysis approach called <strong>part-of-speech</strong> (<strong>POS</strong>) tagging to decide whether words X and Z are a company or person, and whether Y is a product or service. Because of the sentence structure, we know that these are nouns, not because we know what X, Y, and Z are.</p>
<p class="mce-root">However, it is still not enough to label the entity. An entity is a standalone subject or object. Since there are too many entities, we should only tag entities with an uppercase first letter as those unique organizations or assets that are pertinent to our work.</p>
<p>The types of entity include <kbd>ORG</kbd>, <kbd>PERSON</kbd>, <kbd>FAC</kbd>, <kbd>NORP</kbd>, <kbd>GPE</kbd>, <kbd>LOC</kbd>, and <kbd>PRODUCT</kbd>—that is, Organization, Person, Facilities, Nationalities or religious or political groups, Geo-spatial, Location, and Product, using the SpaCy model.</p>
<p>Upon getting the text chunk from the PDF of step 1, we run SpaCy to extract the entities from each of the sentences. For each sentence, we store the entity types and entities in a database record. SpaCy will have a technical limitation on the length of the documents it analyzes; therefore, we cut the very long text chunk into different chunks to respect the technical limitation. However, this comes with the price of chopping sentences at the cut-off point of the text chunk. Considering that we are handling hundreds of pages, we will take the short cut. Of course, the best way to cut this is to cut it approximately around the chunk, while respecting the punctuation in order to preserve the complete sentences.</p>
<p>The following code snippet depicts how to extract various entities:</p>
<pre>'''*************************************<br/>#2. NLP<br/><br/>'''<br/>#Named Entity Extraction<br/>print('ner')<br/>#see if we need to convert everything to lower case words - we keep the original format for this case<br/>lower=False<br/>common_words, sentences, words_list,verbs_list = entitiesExtraction.NER_topics(text,lower)<br/>entities_in_sentences = entitiesExtraction.org_extraction(text)<br/>...<br/>#create this list to export the list of ent and cleanse them<br/>...<br/>print('looping sentences')<br/>for sentence in entities_in_sentences:<br/>    ents_dict[sentence_cnt] = {}<br/>    for entity in sentence:<br/>        ...<br/>        if ent_type in( 'ORG','PERSON','FAC','NORP','GPE','LOC','PRODUCT'):<br/>        ...<br/>    #handle other type<br/>    ...</pre>
<div class="mce-root packt_infobox"><strong>Entity classification via the lexicon:</strong><span> </span>For our use case, we need to further classify the organizations as suppliers, customers, competitors, investors, governments, or sister companies/assets—for example, banks that are the credit investors of the company will first be classified as <strong>Banks</strong> before they are inferred as the Credit Investors/Bankers for the company in its annual report. So some of the relationships require us to check against a database of organizations to classify them further. Acquiring such knowledge requires us to download the relevant databases—in our case, we use Wikipedia to download the list of banks. Only when we check against the list of names of banks will we be able to classify the organization as banks or not. We did not perform this step in our example, as we do not have the lexicon set that is normally available to banks.</div>
<h3 id="uuid-ae193503-03b0-4f31-851f-d9ff449d427e" class="mce-root">Using NetworkX to store the network structure</h3>
<p class="mce-root">After processing the data, the entities will be stored in SQL databases and further analyzed by NetworkX—a Python package that handles network data. Edge and Node are the building blocks of any graph; however, there are a lot more indicators to measure and describe the graph, as well as the position of the node and edge within the graph. What matters for our work now is to see whether the nodes are connected to the company in focus, and the type of connection they have.</p>
<p class="mce-root">At the end of NetworkX, the graph data is still pretty abstract. We need better interactive software to query and handle the data. Therefore, we will output the data as a CSV for Neo4j to further handle, as it provides a user interface to interact with the data.</p>
<p class="mce-root">It is, however, still far from being used—a lot of time is required to cleanse the dataset and define the types of relationship involved. Neo4j is a full-blown graph database that could satisfy the complex relationship structures.</p>
<p>A relationship must be established between the entities mentioned in the company's annual report and the entities stored in the database. In our example, we did not do any filtering of entities as the NLP model in the previous step has a lift of<span> </span>85%, and so it does not have perfect performance when it comes to spotting the entities. We extract only the people and organizations as entities. For the type of relationship (edge), we do not differentiate between the different edge types.</p>
<p>After defining the network structure, we prepare a list that stores the nodes and edges and generates a graph via <kbd>matplotlib</kbd>, which itself is not sufficient for manipulation or visualization. Therefore,  we output the data from NetworkX to CSV files—one storing the nodes and the other one storing the edges.</p>
<p>The following is the code snippet for generating a network of entities:</p>
<pre>'''*************************************<br/>#1. Import relevant libraries and variables<br/><br/>'''<br/>#generate network<br/>import sqlite3<br/>import pandas as pd<br/>import networkx as nx<br/>import matplotlib.pyplot as plt<br/><br/>#db file<br/>db_path = 'parsed_network.db'<br/>db_name = 'network_db'<br/><br/>#sql db<br/>conn = sqlite3.connect(db_path)<br/>c = conn.cursor()<br/><br/>...<br/><br/>network_dict={}<br/>edge_list=[]<br/>curr_source =''<br/>curr_entity = ''<br/>org_list = []<br/>person_list = []<br/><br/>'''*************************************<br/>#2. generate the network with all entities connected to Duke Energy - whose annual report is parsed<br/><br/>'''<br/>target_name = 'Duke Energy'<br/>#loop through the database to generate the network format data<br/>for index, row in df_org.iterrows():<br/>    ...<br/><br/>#Generate the output in networkX<br/>print('networkx')<br/><br/>#output the network<br/>G = nx.from_edgelist(edge_list)<br/>pos = nx.spring_layout(G)<br/>nx.draw(G, with_labels=False, nodecolor='r',pos=pos, edge_color='b')<br/>plt.savefig('network.png')</pre>
<h3 id="uuid-6b976604-87f6-461c-b993-329e6f3330d4" class="mce-root">Using Neo4j for graph visualization and querying</h3>
<p class="mce-root">We will install Neo4j and import the CSV files to construct the data network in Neo4j—the industry-grade graph database. Unfortunately, Neo4j itself requires another set of programming languages to manipulate its data, called <strong>Cypher</strong>. This allows us to extract and search the data we need.</p>
<p>We generate the files required for Neo4j. The following code snippet initializes Neo4j:</p>
<pre>#Generate output for Neo4j<br/>print('prep data for Neo4j')<br/>f_org_node=open('node.csv','w+')<br/>f_org_node.write('nodename\n')<br/><br/>f_person_node=open('node_person.csv','w+')<br/>f_person_node.write('nodename\n')<br/><br/>f_vertex=open('edge.csv','w+')<br/>f_vertex.write('nodename1,nodename2,weight\n')<br/>...</pre>
<p>In the terminal, we copy the output files to the home directory of Neo4j. The following are the commands to be executed from the terminal:</p>
<pre><strong>sudo cp '[path]/edge.csv' /var/lib/Neo4j/import/edge.csv</strong><br/><strong>sudo cp '[path]/node.csv' /var/lib/Neo4j/import/node.csv</strong><br/><br/><strong>sudo service Neo4j restart</strong></pre>
<p>At Neo4j, we log in via the browser. The following is the URL to enter into the browser:</p>
<pre>http://localhost:7474/browser/</pre>
<p>The following is the sample code snippet for Neo4j Cypher: </p>
<pre>MATCH (n) DETACH DELETE n;<br/><br/>USING PERIODIC COMMIT<br/>LOAD CSV WITH HEADERS FROM "file:///node.csv" AS row<br/>CREATE (:ENTITY {node: row.nodename});<br/><br/>CREATE INDEX ON :ENTITY(node);<br/><br/><br/>USING PERIODIC COMMIT<br/>LOAD CSV WITH HEADERS FROM "file:///edge.csv" AS row<br/>MATCH (vertex1:ENTITY {node: row.nodename1})<br/>MATCH (vertex2:ENTITY {node: row.nodename2})<br/>MERGE (vertex1)-[:LINK]-&gt;(vertex2);<br/><br/>MATCH (n:ENTITY)-[:LINK]-&gt;(ENTITY) RETURN n;</pre>
<p>The following screenshot is the resulting output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1051 image-border" src="assets/be090852-8478-4554-b0f5-4e4c7293ee44.png" style="width:34.08em;height:34.58em;"/></p>
<p>Congratulations! You have managed to extract lots of important names/parties from the annual report that you need to focus your research on for further analysis.</p>
<h1 id="uuid-d09a6c93-aa17-4d56-b6b3-81ae57107c04">Summary</h1>
<p>In this chapter, we learned about the behavior of the sell side of a market. We learned about what sentiment analysis is and how to use it. We also looked at an example to sense market needs using sentiment analysis. We learned about network analysis using Neo4j, which is a NoSQL database technique. We learned about text mining using the PDF miner tool.</p>
<p>In the next chapter, we will learn how to use bank APIs to build personal wealth advisers. Consumer banking will be a focus of the chapter. We will learn how to access the Open Bank Project to retrieve financial health data. We will also learn about document layout analysis in the chapter. Let's jump into it without any further ado.</p>


            </article>

            
        </section>
    </body></html>