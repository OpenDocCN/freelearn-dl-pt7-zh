<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Building Personal Wealth Advisers with Bank APIs
                </header>
            
            <article>
                
<p>In the previous chapter, we analyzed the behavior of <span>a sell-side of the exchange. We also learned about sentiment analysis and gained in-depth knowledge of the subject by learning how to analyze market needs using sentiment analysis. We then learned a bit about Neo4j, which is a NoSQL database technique. We then used Neo4j to build and store a network of entities involved in security trading.</span></p>
<p>In this chapter, we will focus on consumer banking and understand the needs of managing customer's digital data. Then, we will learn how to access the Open Bank Project, an open source platform for open banking. After that, we'll look at an example of wrapping AI models around bank APIs. Finally, we will learn about document layout analysis.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Managing customer's digital data</li>
<li>The Open Bank Project</li>
<li>Performing document layout analysis</li>
<li>Cash flow projection using the Open Bank API</li>
<li>Using invoice entity recognition to track daily expenses</li>
</ul>
<p>Let's get started!</p>
<h1 id="uuid-02366aa2-b16a-469c-9f8a-c38635320001">Managing customer's digital data</h1>
<p>In this era of digitization, there is no reason that money cannot be 100% transparent or that money transfers can't happen in real time, 24/7. Consumers have the right to their data as it represents their identity. Whether it is possible to or not, we should be consolidating our own data <span>– </span>realistically, this should be happening today and in the coming few years. It is best to consolidate our banking data in one place; for example, our frequent flyer mileage. The key point is that there shall be two tiers of data architecture <span>– </span>one for consolidation (including storage) and another for running the artificial intelligence services that will be used to analyze the data through the use of a smart device, also known as <strong>mobile applications</strong>. It can be painful to design an AI algorithm without understanding what is going on at the data consolidation layer.</p>
<p>Here, our data source could be identity data, bio/psychometric data, financial data, events that could impact any of this static data, and social data, which represents our relationship with others (including humans, objects, living organisms, and so on). This is, in fact, very similar to a <strong>business-to-business</strong> (<strong>B2B</strong>) setting, where any corporation could be represented by its legal identity, shareholders/ownership structures, financial health, events, as well as its commercial relationship, as outlined in <a href="d29ff3a8-3879-4d50-8795-a39bae5cc793.xhtml">Chapter 7</a>, <em>Sensing Market Sentiment for Algorithmic Marketing at Sell-Side</em>. This also means that what we are learning in this chapter can help with your understanding of the previous chapters in this book.</p>
<p>However, for all individuals, including your, our financial needs are quite basic—<span>they include p</span>ayment, credit, and wealth. These spell out the core activities of financial services. Insurance is included as part of wealth as it aims to protect our wealth against undesirable events and risks—it's like the derivatives that hedge risked on procurement costs in <a href="9ee6480e-c259-4e5d-bc11-31057de1a173.xhtml">Chapter 2</a>, <em>Time Series Analysis</em>.</p>
<div>
<p>However, I am also of the opinion that the data that's derived from consumers is also owned by the bank processing the transaction. It's like parenthood—<span>a</span>ll decisions regarding data (the parent's children) are agreed upon between the data owner (the consumer) and the data producer (the bank). What is lacking today is the technology to quickly attribute the data and economic benefits of the use of this data to certain economic activities, such as marketing. If one organization (for example, a supermarket) is paying social media (for example, Google, Facebook, and Twitter) for consumer's data for marketing purposes, the data owner will be credited with a portion of the economic benefits. Without advances in data technology, mere legal regulations will not be practical.</p>
</div>
<h1 id="uuid-3682e95b-2287-46e6-b385-73a8506da1ff"><strong>The Open Bank Project</strong></h1>
<p>The world's most advanced policy that allows consumers to consolidate their own data is called the <strong>Open Banking Project</strong>. It started in the UK in 2016, following the European's Directive PSD2 <span>– the </span>revised Payment Services Directive (<a href="https://www.ecb.europa.eu/paym/intro/mip-online/2018/html/1803_revisedpsd.en.html">https://www.ecb.europa.eu/paym/intro/mip-online/2018/html/1803_revisedpsd.en.html</a>). This changed the competitive landscape of banks by lowering the entry barrier in terms of making use of banks' information for financial advisory reasons. This makes robo-advisors a feasible business as the financial data that banks contain is no longer segregated.</p>
<p>The challenge with this project is that the existing incumbent dominant banks have little incentive to open up their data. On the consumer side, the slowness in data consolidation impacts the economic values of this inter-connected network of financial data on banking services. This obeys Metcalfe's Law, which states that the value of a network is equivalent to the square number of connected users (in our case, banks). The following table analyzes the situation using Game Theory to anticipate the outcome for both banks and consumers—assuming that consumers have only two banks in the market with four possible outcomes:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 34.6563%" class="CDPAlignCenter CDPAlign">
<p><strong>Cell value = benefits of</strong><br/>
<strong>bank A/bank B/Consumer</strong></p>
</td>
<td style="width: 34.011%" class="CDPAlignCenter CDPAlign">
<p><strong>Bank B: Open Bank API</strong></p>
</td>
<td style="width: 30.2678%" class="CDPAlignCenter CDPAlign">
<p><strong>Bank B: Not Open Bank API</strong></p>
</td>
</tr>
<tr>
<td style="width: 34.6563%" class="CDPAlignCenter CDPAlign">
<p>Bank A: Open Bank API</p>
</td>
<td style="width: 34.011%" class="CDPAlignCenter CDPAlign">
<p>0.5\0.5\2</p>
</td>
<td style="width: 30.2678%" class="CDPAlignCenter CDPAlign">
<p>0.5\1\1</p>
</td>
</tr>
<tr>
<td style="width: 34.6563%" class="CDPAlignCenter CDPAlign">
<p>Bank A: Not Open Bank API</p>
</td>
<td style="width: 34.011%" class="CDPAlignCenter CDPAlign">
<p>1\0.5\1</p>
</td>
<td style="width: 30.2678%" class="CDPAlignCenter CDPAlign">
<p>1\1\1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>For the status quo (that is, without any Open Bank API), let's assume that both banks A and B will enjoy 1 unit of benefits while the consumers will also have 1 unit of benefits.</p>
<p>For any bank to develop an Open Bank API, they will need to consume 0.5 of its resources. Therefore, we will have two cells showing either bank A or B developing the Open Bank API while the other does not. The one developing Open Bank API will have fewer benefits since 0.5 of the original 1 unit will need to be spent as resources to maintain the API. In these two cases, consumers cannot enjoy any additional benefits as the data is not consolidated.</p>
<p>Only in the case where all banks are adopting the Open Bank API will the consumers see incremental benefits (let's assume there's one more unit so that there's two in total, just to be arbitrary), while both banks have fewer benefits. This, of course, could be wrong as the market as a whole shall be more competitive, which is what is happening in the UK with regard to virtual banking—a new sub-segment has been created because of this initiative! So, at the end of the day, all banks could have improved benefits.</p>
<p>Having said that, the reality for most incumbent banks is that they have to maintain two sets of banking services—one completely virtual while the other set of banking channels remains brick and mortar and not scalable. Perhaps the way forward is to build another banking channel outside of its existing one and transfer the clients there.</p>
<p>Since a truly ideal state hasn't been achieved yet, for the moment, to construct a Digital You, there needs to be data from<span> the </span><strong>Open Bank Project</strong><span> </span>(<strong>OBP</strong>) from the UK on financial transactions (<a href="https://uk.openbankproject.com/">https://uk.openbankproject.com/</a>), identity verification via <span>Digidentity from the EU</span><span> (<a href="https://www.digidentity.eu/en/home/">https://www.digidentity.eu/en/home/</a>)</span>, health records stored with<span> </span>IHiS from Singapore (<a href="https://www.ihis.com.sg/">https://www.ihis.com.sg/</a>), events and social data from Facebook, Twitter, Instagram, and LinkedIn, life events from insurance companies, and so on. In short, we still need to work on each respective system rollout before we unite all these data sources.</p>
<h2 id="uuid-bb5fb24b-a197-4804-ad6b-47256c471c57">Smart devices <span>– </span>using APIs with Flask and MongoDB as storage</h2>
<p>Your smart device is a personalized private banker: the software will interact with markets and yourself. Within the smart device, the core modules are the <strong>Holding</strong> and <strong>User Interaction</strong> modules. The <strong>Holding</strong> module will safeguard the investment of the user/customer, while the user's interactions and the user themselves are greeted and connected by the <strong>User Interaction</strong> module.</p>
<p>The <strong>Holding</strong> module handles the quantitative aspect of investments—this is exactly what we covered in the previous two chapters, but at a personal level—by managing the portfolio and capturing various market data. However, the difference is that we need to understand the user/customer better through behavioral data that's been captured in the <strong>User Interaction</strong> module. The <strong>Holding</strong> module is the cognitive brain of the smart device.</p>
<p>The <strong>User Interaction</strong> module provides, of course, the interaction aspect of a smart device—it understands the user's preferences on investment and interactions. These investment preferences are captured in the <strong>Investment Policy Statement</strong> (<strong>IPS</strong>). These interactions are then handled by the <strong>Behavior Analyzer</strong>, which analyzes the preferred time, channels, and messages to communicate, as well as the financial behaviors of the user regarding their actual personality and risk appetite, both of which are derived from the data that's obtained from the <strong>Data Feeds </strong>of external sources or user-generated data from using the device. Last but not least, the <strong>communication channels</strong> (<strong>Comm Channels</strong>) interact with the user either by voice, text, or perhaps physically via a physical robot.</p>
<p>This sums up nicely what we mentioned in <a href="1464aff7-9b01-4476-bce9-7da307ef0211.xhtml">Chapter 1</a>, <em>The Importance of AI in Banking</em>, as the definition of AI—a machine that thinks and acts like a human, either rationally or emotionally, or both. The <strong>Holding</strong> module is the rational brain of a human and acts accordingly in the market, while its emotions are handled by the <strong>User Interaction</strong> module <span>– </span>sympathized by the <strong>Behavior Analyzer</strong> and how they interact via the <strong>Comm Channels</strong>. The following diagram shows the market and user interaction through banking functions:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1161 image-border" src="assets/72ccb197-84bc-4ef9-8819-773962b6d241.png" style="width:39.92em;height:26.08em;"/></p>
<p>Since we've already talked about the <strong>Holding</strong> module in the previous two chapters, which focused on the investment process, here, we'll focus more on the <strong>User Interaction</strong> module. Specifically, we will dig deeper into <strong>IPS</strong>, which records the investment needs of the user.</p>
<h3 id="uuid-a5f83737-6e76-40f7-b8ff-b997e358e115">Understanding IPS</h3>
<p>As we mentioned in <a href="0e7c4e25-941b-4bd6-a04a-55924bdbaa43.xhtml">Chapter 6</a>, <em>Automated Portfolio Management Using Treynor-Black Model and ResNet</em>, we will start looking at the individual's investment policy statement here. To do this, we need to collect data to build up the IPS for an individual customer.</p>
<p>Here is what it takes to build an IPS for a family:</p>
<ul>
<li><strong>Return and risk objectives</strong>:</li>
</ul>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Objective</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Comment</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Return objectives</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>To be inputted by the investors and via the behavioral analyzer—personality profiling</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Ability to take risk</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>To be inputted by the investors and via the behavioral analyzer—personality profiling</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Willingness to take risk</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>To be inputted by the investors and via the behavioral analyzer—personality profiling</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<ul>
<li><strong>Constraints</strong>:</li>
</ul>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Constraint</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>Comment</strong></p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Liquidity</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Liquidity of assets can be determined by the prices within the smart device.</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Time horizon</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Plan for your children's future <span>– </span>their studies (where and which school, how much, and so on), housing plans, jobs, retirement, and so on.</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Taxes (both US citizens)</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Citizenship via Digidentity.</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Legal &amp; regulatory environment</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>This could be implicated via commercial transactions, citizenship, employment, and residential constraints.</p>
<p>You may also need to consider the legal entity that will manage your wealth, such as a family trust.</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Unique circumstances</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>Interests and special circumstances aren't made known, including social media or medical profiles that stand out from a <em>standard</em> user's <span>– </span>this needs to be compared across users anonymously to provide real, unique circumstances.</p>
</td>
</tr>
</tbody>
</table>
<h2 id="uuid-1d4928fc-9f72-431e-b730-588b47cad347">Behavioral Analyzer <span>– </span>expenditure analyzers</h2>
<p>Similar to <a href="9ee6480e-c259-4e5d-bc11-31057de1a173.xhtml">Chapter 2</a>, <em>Time Series Analysi</em>s, we are going to forecast the day-to-day cash flow in the market. Since the income for most cases (and even most of the population who are working on salary) are fixed on a monthly basis, the only moving parts are the expenditures. Within these expenditures, there could be regular spending costs for things such as groceries, as well as irregular spending costs for things such as buying white goods or even a car. For a machine to track and project regular spending habits, as well as infrequent spending, the practical approach is to record these habits efficiently when they occur.</p>
<h2 id="uuid-29f70d07-e000-4ff5-bb91-25a43e5aa87f">Exposing AI services as APIs</h2>
<p>While the portfolio optimization model we built in <a href="0e7c4e25-941b-4bd6-a04a-55924bdbaa43.xhtml">Chapter 6</a>, <em>Automated Portfolio Management Using Treynor-Black Model and ResNet</em>, was great, the key technology that will be addressed in this chapter will demonstrate how AI models are wrapped and provided to users via API. With regard to technical modeling skills, we are not going to add any new techniques to our repertoire in this chapter.</p>
<h1 id="uuid-a11f941f-8b4a-4f8b-84af-ef095efd06ec">Performing document layout analysis</h1>
<p>In ML, there is a discipline called <strong>document layout analysis</strong>. It is indeed about studying how humans understand documents. It includes computer vision, natural language processing, and knowledge graphs. The end game is to deliver an ontology that can allow any document to be navigated, similar to how word processors can, but in an automated manner. In a word processor, we have to define certain words that are found in headers, as well as within different levels of the hierarchy <span>– </span>for example, heading level 1, heading level 2, body text, paragraph, and so on. What's not defined manually by humans is sentences, vocabulary, words, characters, pixels, and so on. However, when we handle the images taken by a camera or scanner, the lowest level of data is a pixel.</p>
<h2 id="uuid-92664c90-0604-470d-9f9b-bf79d68324fe">Steps for document layout analysis</h2>
<p>In this section, we will learn how to perform document layout analysis. The steps are as follows:</p>
<ol>
<li><strong>Forming characters from pixels</strong>: <span>The technique, which is used to convert pixels into characters, is known as </span><strong>Optical Character Recognition</strong><span> (</span><strong>OCR</strong><span>). It is a well-known problem that can be solved by many examples of d</span><span>eep learning, including the </span><span>MNIST dataset. Alternatively, we could use Tesseract-OCR to perform OCR.</span></li>
<li><strong>Image rotation</strong>: <span>When the image is not located at the correct vertical orientation, it may create challenges for people to read the characters. Of course, new research in this area is occurring that seems to be able to skip this step.</span></li>
<li><strong>Forming words from characters</strong>: Practically, we cannot wait minutes upon minutes for this to happen; with human performance, we can get this right. How do we know that a character shall be banded together with other characters to form one word? We know this from spatial clues. Yet the distance between characters is not fixed, so how can we teach a machine to understand this spatial distance? This is perhaps the challenge most people suffering from dyslexia face. Machines also suffer from dyslexia by default.</li>
<li><strong>Building meaning from words</strong>: This requires us to know the topic of the paper and the spelling of the words, which helps us to check our various dictionaries to understand what the document is about. <em>Learning</em> (in terms of deep learning in this book) could mean just a topic related to education, and the reason we know that it because you understand that this book is a machine learning book by Packt <span>– </span>a publisher name that you learned about in the past. Otherwise, by merely reading the word Packt, we may guess that it is related to a packaging company (that is, PACK-t)? In addition, we also draw clues from the label words—<em>step 3</em> itself looks like label words that introduce the actual content on the right-hand side of it.</li>
</ol>
<p style="padding-left: 60px">Classifying words as various generic types of entities helps <span>– </span>for example, dates, serial numbers, dollar amount, time, and so on. These are the generic entities we typically see in open source spaces such as spaCy, which we used in <a href="d29ff3a8-3879-4d50-8795-a39bae5cc793.xhtml">Chapter 7</a>, <em>Sensing Market Sentiment for Algorithmic Marketing at Sell Side</em>.</p>
<p style="padding-left: 60px">With regard to spatial clues in the form of words, we may understand the importance of larger words while paying less attention to smaller words. The location of the words on the page matter too. For example, to read English, we normally read from top to bottom, left to right, while in some other languages, we need to read from right to left, top to bottom, such as ancient Chinese.</p>
<p class="mce-root"/>
<h2 id="uuid-17d417a1-36da-42ce-93db-cfbe3bc8edf9" class="mce-root">Using Gensim for topic modeling</h2>
<p class="mce-root">In our example of topic modeling, we will focus on <em>step 4</em> to limit our scope of work. We will do this while we take the prework from <em>steps 1</em> to <em>3</em> for granted and skip <em>steps 5</em> and <em>6</em>. The dataset image we will be using has already been cleaned, rotated, and OCRed <span>– this </span>included binding characters to form words. What we have at hand is a dataset with each record represented by a text block, which could include multiple words. Gensim is concerned with tracking nouns in text.</p>
<h2 id="uuid-b904658d-80d2-431a-93fa-e6a88c5d23b3">Vector dimensions of Word2vec</h2>
<p class="mce-root">Word2Vec defines words by their different features <span>– </span>the feature value of each word is defined by the distance between the words that appear in the same sentence. It is meant to quantify the similarity between concepts and topics. In our example of Word2vec, we will use a pre-trained model to convert text into words. However, for each text block, there may be several values involved. In such a case, a series of these vectors would be compressed into a value, using a value called an <strong>Eigenvalue</strong>. We will use this simple approach to perform dimension reduction, which we do when we want to reduce the number of features (dimensions) of a variable. The most common approach to dimension reduction is called <strong>Principal Component Analysis</strong> (<strong>PCA</strong>). It is mostly applied to scalars, not vectors of variables. Imagine that each word is represented by a vector. Here, one text block with two words will be represented by a matrix composed of two vectors. Therefore, the PCA may not be an ideal solution for this kind of dimension reduction task.</p>
<p>While interpreting the vectors that represent the topic of the word, it is important to analyze the dimensions involved, as each dimension represents one semantic/meaning group. In our example of Word2vec, we'll skip this step to avoid putting too many dimensions into the meaningful extraction process. This means we'll have smaller feature spaces for illustration purposes.</p>
<h1 id="uuid-dc37b03c-e863-43be-99c2-08231842dfed">Cash flow projection using the Open Bank API</h1>
<p>In the future, we will need robo-advisors to be able to understand our needs. The most basic step is to be able to pull our financial data from across banks. Here, we will assume that we are customers of consumer banking services from the US who are staying in the UK. We are looking for wealth planning for a family of four—a married couple and two kids. What we want is a robo-advisor to perform all our financial activities for us.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will retrieve all the necessary transaction data from the <strong>Open Bank Project</strong> (<strong>OBP</strong>) API to forecast our expenditure forecasting via Open Bank API. The data that we will be using will be simulated data that follows the format specified in the OBP. We are not going to dive deep into any of the software technologies while focusing on building the wealth planning engine. The family description we'll be using has been obtained from the Federal Reserve (<a href="https://www.federalreserve.gov/econresdata/2016-economic-well-being-of-us-households-in-2015-Income-and-Savings.htm">https://www.federalreserve.gov/econresdata/2016-economic-well-being-of-us-households-in-2015-Income-and-Savings.htm</a>) regarding American household financials.</p>
<p>The following table shows the typical values of households in the US, which helps us understand the general demand for consumer banking:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p><strong>Income/Expense</strong></p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p><strong>Value (In US $)</strong></p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign">
<p><strong>Data Source (OBS)</strong></p>
</td>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p><strong>Income</strong></p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p><strong>102.7</strong></p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p>Salaries from working people</p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p>102.7</p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign">
<p><span>Largest auto-payment every month, with fixed salaries on a monthly basis.</span></p>
</td>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p><strong>Living expenses</strong></p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p><strong>67.3</strong></p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p>Annual expenses</p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p>57.3</p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign">
<p><span>Retrieve all transactions from credit cards, savings, and current accounts.</span></p>
</td>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p><span>Debt repayment</span></p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p>10.0</p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign">
<p><span>Transactions related to debt account.</span></p>
</td>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p><strong><span>Net worth</span></strong></p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p><strong>97</strong></p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p><strong><span>Assets</span></strong></p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p><strong>189.9</strong></p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p><span>Financial assets</span></p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p>23.5</p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign">
<p>The outstanding balance of Securities account. No visibility of the 401 plan.</p>
</td>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p><span>Non-financial assets</span></p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p>158.9</p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign">
<p><span>Housing valuation provided by</span><span> </span>Zillow.</p>
</td>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p><strong><span>Liabilities</span></strong></p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p><strong>92.6</strong></p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign"/>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p>Mortgage</p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p>59.5</p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign">
<p><span>The outstanding balance of the debt account.</span></p>
</td>
</tr>
<tr>
<td style="width: 14%" class="CDPAlignCenter CDPAlign">
<p>Auto loans and educational debts</p>
</td>
<td style="width: 11.5457%" class="CDPAlignCenter CDPAlign">
<p>32.8</p>
</td>
<td style="width: 68.4543%" class="CDPAlignCenter CDPAlign">
<p>Auto loan: Outstanding balance of debt account; student loan (federal), with the counterpart being Federal Student loan's; Student loan (private): Outstanding balance of debt account.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<div class="packt_infobox">For more details about Zillow, please refer to this link: <a href="https://www.zillow.com/howto/api/APIOverview.htm">https://www.zillow.com/howto/api/APIOverview.htm</a>.</div>
<h2 id="uuid-41d96911-24c5-4e08-8eea-e0b7009ee60c" class="mce-root">Steps involved</h2>
<p>To use the Open Bank API, we will need to do the following:</p>
<ol>
<li>Register to use the Open Bank API.</li>
<li>Download the necessary data.</li>
<li>Create a database to store this data.</li>
<li>Set up an API for forecasting.</li>
</ol>
<p>Let's get started!</p>
<h3 id="uuid-6fd60075-152a-4f6d-981b-86915aa81350" class="">Registering to use Open Bank API</h3>
<p>There are several ways we can access the Open Banking Project—we will work on one such where we registered at <a href="https://demo.openbankproject.com/">https://demo.openbankproject.com/</a>. </p>
<h3 id="uuid-f2e58410-745c-4cba-b73b-74da4eaaddb6">Creating and downloading demo data</h3>
<p>The code for this section can be downloaded from<span> </span>GitHub (<a href="https://github.com/OpenBankProject/Hello-OBP-DirectLogin-Python">https://github.com/OpenBankProject/Hello-OBP-DirectLogin-Python</a>). Based on the <kbd>hello_obp.py</kbd> file from this repository, we have modified the program so that it downloads the required data. Use the following code snippet to download the demo data:</p>
<pre># -*- coding: utf-8 -*-<br/><br/>from __future__ import print_function    # (at top of module)<br/>import sys<br/>import time<br/>import requests<br/><br/><br/># Note: in order to use this example, you need to have at least one account<br/># that you can send money from (i.e. be the owner).<br/># All properties are now kept in one central place<br/><br/>from props.default import *<br/><br/><br/># You probably don't need to change those<br/>...<br/><br/>#add the following lines to hello-obp.py before running it<br/>#add lines to download the file<br/>print("")<br/>print(" --- export json")<br/>import json<br/>f_json = open('transactions.json','w+')<br/>json.dump(transactions,f_json,sort_keys=True, indent=4)</pre>
<h3 id="uuid-13467266-5d02-478a-80b8-365f165a5505">Creating a NoSQL database to store the data locally</h3>
<p>I prefer MongoDB for this due to its ability to import JSON files in a hierarchical manner, without us needing to define the structure in advance. Even though we will need to store the NoSQL file in SQL database format (as we did in the previous chapter) whenever we need to run predictions with the ML model, it is still useful for us to cache the downloaded data physically before we run the prediction.</p>
<p>So, you may be wondering why we need to store it in a NoSQL database for our purposes <span>–</span> can't we just save it as we did in the previous chapter, when we handled tweet data? No <span>– we want </span>to use a database because for quicker retrieval, given that we will be storing hundreds of thousands of JSON files with an infinite number of days versus batch downloads. This also depends on how frequently we want to download the data; if we wish to update our databases every day, we may not need to store the JSON data in a NoSQL database as we wouldn't have very many files to deal with. However, if we are querying the data or continuously adding new features to the training dataset, we might be better off storing the raw data on our side.</p>
<p>The following code is used to establish our connectivity with the MongoDB server:</p>
<pre>from pymongo import MongoClient<br/>import json<br/>import pprint<br/><br/>#client = MongoClient()<br/>client = MongoClient('mongodb://localhost:27017/')<br/>db_name = 'AIFinance8A'<br/>collection_name = 'transactions_obp'<br/><br/>f_json = open('transactions.json', 'r')<br/>json_data = json.loads(f_json)<br/><br/>...<br/><br/>#to check if all documents are inserted<br/>...</pre>
<p>The following code is used to create the database:</p>
<pre>#define libraries and variables<br/>import sqlite3<br/>from pymongo import MongoClient<br/>import json<br/>from flatten_dict import flatten<br/><br/>client = MongoClient('mongodb://localhost:27017/')<br/>db_name = 'AIFinance8A'<br/>collection_name = 'transactions_obp'<br/><br/>db = client[db_name]<br/>collection = db[collection_name]<br/>posts = db.posts<br/><br/>...<br/><br/>#flatten the dictionary<br/>...<br/><br/>#create the database schema<br/>#db file<br/>db_path = 'parsed_obp.db'<br/>db_name = 'obp_db'<br/><br/>#sql db<br/>...<br/>sqlstr = 'drop table '+db_name<br/>...<br/>print('create')<br/>...<br/>#loop through the dict and insert them into the db<br/>...<br/><br/>for cnt in dict_cnt:<br/>    ...<br/>    for fld in tuple_fields_list:<br/>        ...<br/>    ...<br/>    sqlstr = 'insert into '+ db_name+ '(' + str(fld_list_str)+') VALUES \<br/>                                       ('+question_len[1:]+')'<br/>    ...</pre>
<h3 id="uuid-a90572b0-5528-4937-b21b-8915a40b8af6">Setting up the API for forecasting</h3>
<p>To perform forecasting for payments, we need to know what kind of forecasting model we want to build. Do we want a time series model or the ML model? Of course, we want to have a model that provides more information.</p>
<p>In our example, we have not prepared any model for this as the method we'll be using will be similar to the model we used in <a href="9ee6480e-c259-4e5d-bc11-31057de1a173.xhtml">Chapter 2</a>, <em>Time Series Analysis</em>. The main point to illustrate here is how to set up the API server and how to use another program to consume the API. Please make sure these two programs are run simultaneously.</p>
<p>The server will be set up to listen to requests so that it can run predictions. We will simply load the model without running any predictions. The following code snippet is used to connect us to the Open Bank API server:</p>
<pre>#Libraries<br/>from flask import Flask, request, jsonify<br/>from sklearn.externals import joblib<br/>import traceback<br/>import pandas as pd<br/>import numpy as np<br/><br/># Your API definition<br/>app = Flask(__name__)<br/><br/>@app.route('/predict', methods=['POST'])<br/>def predict():<br/>    ...<br/><br/>#Run the server<br/>if __name__ == '__main__':<br/>    ...</pre>
<p>The following code snippet is used to create requests from the client application:</p>
<pre>import requests<br/><br/>host = 'http://127.0.0.1:12345/'<br/><br/>r = requests.post(host+'predict', json={"key": "value"})<br/>print(r)</pre>
<p class="mce-root">Congratulations! You have built a robot that can read data from banks and have built it so that it can run AI models on this data.</p>
<p>For a household, it becomes critical to limit expenses to increase their cash flow. In the next section, we will look at how to track daily expenses using the invoice entity recognition technique.</p>
<h1 id="uuid-840f636e-7ad5-40eb-9a07-e8d85caee513">Using invoice entity recognition to track daily expenses</h1>
<p>While we are always dreaming for the end game of digitization through AI in finance, the reality is that there is data that's trapped. And very often, these expenses come in the form of paper, not API feeds. Dealing with paper would be inevitable if we were to transform ourselves into a fully digital world where all our information is stored in JSON files or SQL databases. We cannot avoid handling existing paper-based information. Using an example of a paper-based document dataset, we are going to demonstrate how to build up the engine for the invoice entity extraction model.</p>
<p>In this example, we will assume you are developing your own engine to scan and transform the invoice into a structured data format. However, due to a lack of data, you will need to parse the Patent images dataset, which is<span> </span>available<span> </span>at <a href="http://machinelearning.inginf.units.it/data-and-tools/ghega-dataset">http://machinelearning.inginf.units.it/data-and-tools/ghega-dataset</a>. Within the dataset, there are images, text blocks, and the target results that we want to extract from. This is known as <strong>entity extraction</strong>. The challenge here is that these invoices are not in a standardized format. Different merchants issue invoices in different sizes and formats, yet we are still able to understand the visual clues (font size, lines, positions, and so on) and the languages of the words and the words surrounding it (called <strong>labels</strong>).</p>
<h2 id="uuid-32158a35-259e-48b4-9d3d-03ec807ddfd4" class="mce-root">Steps involved</h2>
<p>We have to follow six steps to track daily expenses using invoice entity recognition. These steps are as follows:</p>
<ol>
<li class="mce-root">Import the relevant libraries and define the variables. In this example, we're introducing topic modeling, including <strong>Word to Vector</strong> (<kbd>Word2vec</kbd>), using <kbd>gensim</kbd>, and regular expressions using <kbd>re</kbd>, a built-in module. The following code snippet is used to import the required libraries:</li>
</ol>
<pre style="padding-left: 60px">import os<br/>import pandas as pd<br/>from numpy import genfromtxt<br/>import numpy as np<br/>from gensim.models import Word2Vec<br/>from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors<br/>import gensim.downloader as api<br/>from gensim.parsing.preprocessing import remove_stopwords<br/>from gensim.parsing.preprocessing import preprocess_string, strip_tags, remove_stopwords,strip_numeric,strip_multiple_whitespaces<br/>from scipy import linalg as LA<br/>import pickle<br/>import re<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.neural_network import MLPClassifier<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.metrics import classification_report,roc_curve, auc,confusion_matrix,f1_score<br/><br/>#please run this in terminal: sudo apt-get install libopenblas-dev<br/>model_word2vec = api.load("text8") # load pre-trained words vectors</pre>
<ol start="2">
<li>Define the functions that will need to be used later. There will be two groups of functions—one, <kbd>2A</kbd>, is used to train and test the neural network, while the other, <kbd>2B</kbd>, aims at converting the text into numeric values. The following code snippet defines the functions that will be used for invoice entity recognition:</li>
</ol>
<pre style="padding-left: 60px">#2. Define functions relevant for works<br/>##<strong>2A</strong> Neural Network<br/>##2A_i. Grid search that simulate the performance of different neural network design<br/>def grid_search(X_train,X_test, Y_train,Y_test,num_training_sample):<br/>...<br/>##2A_ii train network<br/>def train_NN(X,Y,target_names):<br/>...<br/>#<strong>2B</strong>: prepare the text data series into numeric data series<br/>#2B.i: cleanse text by removing multiple whitespaces and converting to lower cases<br/>def cleanse_text(sentence,re_sub):<br/>...<br/>#2B.ii: convert text to numeric numbers<br/>def text_series_to_np(txt_series,model,re_sub):<br/>...<br/><br/></pre>
<ol start="3">
<li>Prepare the dataset. In this example, we will try to use <kbd>numpy</kbd> to store the features as they're quite big. We will also use <kbd>pandas</kbd> for each file as it is far easier to manipulate and select columns using a DataFrame, given that the size of each image isn't too large. The following code snippet is used to prepare the dataset:</li>
</ol>
<pre style="padding-left: 60px">#3. Loop through the files to prepare the dataset for training and testing<br/>#loop through folders (represent different sources)<br/>for folder in list_of_dir:<br/>    files = os.path.join(path,folder)<br/>    #loop through folders (represent different filing of the same <br/>     source)<br/>    for file in os.listdir(files):<br/>        if file.endswith(truth_file_ext):<br/>        #define the file names to be read<br/>         ...<br/><br/>        #merge ground truth (aka target variables) with the blocks <br/>         ...<br/> <br/>        #convert the text itself into vectors and lastly a single <br/>        value using Eigenvalue<br/>        text_df = f_df['text']<br/>        text_np = text_series_to_np(text_df,model_word2vec,re_sub)<br/><br/>        label_df = f_df['text_label'] <br/>        label_np = text_series_to_np(label_df, model_word2vec, \<br/>                                     re_sub)<br/>         ...<br/>Y_pd = pd.get_dummies(targets_df)<br/>Y_np = Y_pd.values</pre>
<ol start="4">
<li>Execute the model. Here, we execute the model we prepared using the functions we defined in previous steps. The following code snippet is used to execute the model:</li>
</ol>
<pre style="padding-left: 60px">#4. Execute the training and test the outcome<br/>NN_clf, f1_clf = train_NN(full_X_np,Y_np,dummy_header)<br/>...</pre>
<p style="padding-left: 60px">Congratulations! With that, you have built a model that can extract information from scanned images!</p>
<ol start="5">
<li>Draw the clues from the spatial and visual environment of the word. The preceding line clearly separates <em>steps 4</em> and <em>5</em>. Noticing how these lines are being projected also helps us group similar words together. For documents that require original copies, we may need to look at signatures and logos, as well as matching these against a true verified signature or stamp.</li>
<li>Construct a knowledge map of these documents. This is when we can build a thorough understanding of the knowledge embedded in the document. Here, we need to use the graph database to keep track of this knowledge (we covered this in the previous chapter).</li>
</ol>
<p>This concludes our example of tracking daily expenses, as well as this chapter.</p>
<h1 id="uuid-ab41bef6-160c-411e-a2ab-a543ee612949">Summary</h1>
<p>In this chapter, we covered how to extract data and provide AI services using APIs. We understood how important it is to manage customer's digital data. We also understood the Open Bank Project and document layout analysis. We learned about this through two examples—<span>one was about</span> projecting cash flows, while the other was about tracking daily expenses.</p>
<p class="mce-root">The next chapter will also focus on consumer banking. We will learn how to create proxy data for information that's missing in the customer's profile. We also will take a look at an example chatbot that we can use to serve and interact with customers. We will use graph and NLP techniques to create this chatbot.</p>


            </article>

            
        </section>
    </body></html>