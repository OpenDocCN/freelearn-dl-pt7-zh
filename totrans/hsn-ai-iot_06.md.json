["```py\npip install gym\n```", "```py\nfrom gym import envs\nprint(envs.registry.all())\n```", "```py\nimport gym\nenv = gym.make('Pong-v0')\nobs = env.reset()\nenv.render()\n```", "```py\nframes = [] # array to store state space at each step\nfor _ in range(300):\n    frames.append(env.render(mode='rgb_array'))\n    obs,reward,done, _ = env.render(env.action_space.sample())\n    if done:\n        break\n```", "```py\nimport matplotlib.animation as animation\nfrom JSAnimation.Ipython_display import display_animation\nfrom IPython.display import display\n\npatch = plt.imshow(frames[0])\nplt.axis('off')\n\ndef animate(i)\n    patch.set_data(frames[i])\n\nanim = animation.FuncAnimation(plt.gcf(), animate, \\\n        frames=len(frames), interval=100)\n\ndisplay(display_animation(anim, default_mode='loop')\n```", "```py\nimport gym\nfrom gym import wrappers\nenv = gym.make('Pong-v0')\nenv = wrappers.Monitor(env, '/save-mov', force=True)\n# Follow it with the code above where env is rendered and agent\n# selects a random action\n\n```", "```py\nenv.close()\n```", "```py\nimport gym\nimport numpy as np\nenv = gym.make('Taxi-v2')\nobs = env.reset()\nenv.render()\n```", "```py\nm = env.observation_space.n # size of the state space\nn = env.action_space.n # size of action space\nprint(\"The Q-table will have {} rows and {} columns, resulting in \\\n     total {} entries\".format(m,n,m*n))\n\n# Intialize the Q-table and hyperparameters\nQ = np.zeros([m,n])\ngamma = 0.97\nmax_episode = 1000\nmax_steps = 100\nalpha = 0.7\nepsilon = 0.3\n```", "```py\nfor i in range(max_episode):\n    # Start with new environment\n    s = env.reset()\n    done = False\n    for _ in range(max_steps):\n        # Choose an action based on epsilon greedy algorithm\n        p = np.random.rand()\n        if p > epsilon or (not np.any(Q[s,:])):\n            a = env.action_space.sample() #explore\n        else:\n            a = np.argmax(Q[s,:]) # exploit\n        s_new, r, done, _ = env.step(a) \n        # Update Q-table\n        Q[s,a] = (1-alpha)*Q[s,a] + alpha*(r + gamma*np.max(Q[s_new,:]))\n        #print(Q[s,a],r)\n        s = s_new\n        if done:\n            break\n```", "```py\ns = env.reset()\ndone = False\nenv.render()\n# Test the learned Agent\nfor i in range(max_steps):\n a = np.argmax(Q[s,:])\n s, _, done, _ = env.step(a)\n env.render()\n if done:\n break \n```", "```py\nclass QNetwork:\n    def __init__(self,m,n,alpha):\n        self.s = tf.placeholder(shape=[1,m], dtype=tf.float32)\n        W = tf.Variable(tf.random_normal([m,n], stddev=2))\n        bias = tf.Variable(tf.random_normal([1, n]))\n        self.Q = tf.matmul(self.s,W) + bias\n        self.a = tf.argmax(self.Q,1)\n\n        self.Q_hat = tf.placeholder(shape=[1,n],dtype=tf.float32)\n        loss = tf.reduce_sum(tf.square(self.Q_hat-self.Q))\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=alpha)\n        self.train = optimizer.minimize(loss)\n        init = tf.global_variables_initializer()\n\n        self.sess = tf.Session()\n        self.sess.run(init)\n\n    def get_action(self,s):\n        return self.sess.run([self.a,self.Q], feed_dict={self.s:s})\n\n    def learnQ(self,s,Q_hat):\n        self.sess.run(self.train, feed_dict= {self.s:s, self.Q_hat:Q_hat})\n\n    def Qnew(self,s):\n        return self.sess.run(self.Q, feed_dict={self.s:s})\n\n```", "```py\nQNN = QNetwork(m,n, alpha)\nrewards = []\nfor i in range(max_episode):\n # Start with new environment\n s = env.reset()\n S = np.identity(m)[s:s+1]\n done = False\n counter = 0\n rtot = 0\n for _ in range(max_steps):\n # Choose an action using epsilon greedy policy\n a, Q_hat = QNN.get_action(S) \n p = np.random.rand()\n if p > epsilon:\n a[0] = env.action_space.sample() #explore\n\n s_new, r, done, _ = env.step(a[0])\n rtot += r\n # Update Q-table\n S_new = np.identity(m)[s_new:s_new+1]\n Q_new = QNN.Qnew(S_new) \n maxQ = np.max(Q_new)\n Q_hat[0,a[0]] = r + gamma*maxQ\n QNN.learnQ(S,Q_hat)\n S = S_new\n #print(Q_hat[0,a[0]],r)\n if done:\n break\n rewards.append(rtot)\nprint (\"Total reward per episode is: \" + str(sum(rewards)/max_episode))\n```", "```py\n\nimport gym\nimport sys\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom scipy.misc import imresize\n```", "```py\ndef preprocess(img):\n    img_temp = img[31:195] # Choose the important area of the image\n    img_temp = img_temp.mean(axis=2) # Convert to Grayscale#\n    # Downsample image using nearest neighbour interpolation\n    img_temp = imresize(img_temp, size=(IM_SIZE, IM_SIZE), interp='nearest')\n    return img_temp\n```", "```py\ndef update_state(state, obs):\n    obs_small = preprocess(obs)\n    return np.append(state[1:], np.expand_dims(obs_small, 0), axis=0)\n```", "```py\ndef __init__(self, K, scope, save_path= 'models/atari.ckpt'):\n    self.K = K\n    self.scope = scope\n    self.save_path = save_path\n    with tf.variable_scope(scope):\n        # inputs and targets\n        self.X = tf.placeholder(tf.float32, shape=(None, 4, IM_SIZE, IM_SIZE), name='X')\n        # tensorflow convolution needs the order to be:\n        # (num_samples, height, width, \"color\")\n        # so we need to tranpose later\n        self.Q_target = tf.placeholder(tf.float32, shape=(None,), name='G')\n        self.actions = tf.placeholder(tf.int32, shape=(None,), name='actions')\n        # calculate output and cost\n        # convolutional layers\n        Z = self.X / 255.0\n        Z = tf.transpose(Z, [0, 2, 3, 1])\n        cnn1 = tf.contrib.layers.conv2d(Z, 32, 8, 4, activation_fn=tf.nn.relu)\n        cnn2 = tf.contrib.layers.conv2d(cnn1, 64, 4, 2, activation_fn=tf.nn.relu)\n        cnn3 = tf.contrib.layers.conv2d(cnn2, 64, 3, 1, activation_fn=tf.nn.relu)\n        # fully connected layers\n        fc0 = tf.contrib.layers.flatten(cnn3)\n        fc1 = tf.contrib.layers.fully_connected(fc0, 512)\n        # final output layer\n        self.predict_op = tf.contrib.layers.fully_connected(fc1, K)\n        Qpredicted = tf.reduce_sum(self.predict_op * tf.one_hot(self.actions, K),\n     reduction_indices=[1])\n        self.cost = tf.reduce_mean(tf.square(self.Q_target - Qpredicted))\n        self.train_op = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6).minimize(self.cost)\n```", "```py\ndef predict(self, states):\n    return self.session.run(self.predict_op, feed_dict={self.X: states})\n```", "```py\ndef sample_action(self, x, eps):\n    \"\"\"Implements epsilon greedy algorithm\"\"\"\n    if np.random.random() < eps:\n        return np.random.choice(self.K)\n    else:\n        return np.argmax(self.predict([x])[0])\n```", "```py\n def update(self, states, actions, targets):\n     c, _ = self.session.run(\n         [self.cost, self.train_op],\n         feed_dict={\n         self.X: states,\n         self.Q_target: targets,\n         self.actions: actions\n         })\n     return c\n```", "```py\ndef copy_from(self, other):\n    mine = [t for t in tf.trainable_variables() if t.name.startswith(self.scope)]\n    mine = sorted(mine, key=lambda v: v.name)\n    theirs = [t for t in tf.trainable_variables() if t.name.startswith(other.scope)]\n    theirs = sorted(theirs, key=lambda v: v.name)\n    ops = []\n    for p, q in zip(mine, theirs):\n        actual = self.session.run(q)\n        op = p.assign(actual)\n        ops.append(op)\n    self.session.run(ops)\n```", "```py\ndef load(self):\n    self.saver = tf.train.Saver(tf.global_variables())\n    load_was_success = True\n    try:\n        save_dir = '/'.join(self.save_path.split('/')[:-1])\n        ckpt = tf.train.get_checkpoint_state(save_dir)\n        load_path = ckpt.model_checkpoint_path\n        self.saver.restore(self.session, load_path)\n    except:\n        print(\"no saved model to load. starting new session\")\n        load_was_success = False\n    else:\n        print(\"loaded model: {}\".format(load_path))\n        saver = tf.train.Saver(tf.global_variables())\n        episode_number = int(load_path.split('-')[-1])\n\ndef save(self, n):\n    self.saver.save(self.session, self.save_path, global_step=n)\n    print(\"SAVED MODEL #{}\".format(n))\n\ndef set_session(self, session):\n    self.session = session\n    self.session.run(tf.global_variables_initializer())\n    self.saver = tf.train.Saver()\n```", "```py\ndef learn(model, target_model, experience_replay_buffer, gamma, batch_size):\n    # Sample experiences\n    samples = random.sample(experience_replay_buffer, batch_size)\n    states, actions, rewards, next_states, dones = map(np.array, zip(*samples))\n    # Calculate targets\n     next_Qs = target_model.predict(next_states)\n     next_Q = np.amax(next_Qs, axis=1)\n     targets = rewards +     np.invert(dones).astype(np.float32) * gamma * next_Q\n    # Update model\n     loss = model.update(states, actions, targets)\n     return loss\n```", "```py\n# Some Global parameters\nMAX_EXPERIENCES = 500000\nMIN_EXPERIENCES = 50000\nTARGET_UPDATE_PERIOD = 10000\nIM_SIZE = 80\nK = 4 # env.action_space.n\n\n# hyperparameters etc\ngamma = 0.97\nbatch_sz = 64\nnum_episodes = 2700\ntotal_t = 0\nexperience_replay_buffer = []\nepisode_rewards = np.zeros(num_episodes)\nlast_100_avgs = []\n# epsilon for Epsilon Greedy Algorithm\nepsilon = 1.0\nepsilon_min = 0.1\nepsilon_change = (epsilon - epsilon_min) / 700000\n\n# Create Atari Environment\nenv = gym.envs.make(\"Breakout-v0\")\n\n# Create original and target Networks\nmodel = DQN(K=K, scope=\"model\")\ntarget_model = DQN(K=K, scope=\"target_model\")\n```", "```py\nwith tf.Session() as sess:\n    model.set_session(sess)\n    target_model.set_session(sess)\n    sess.run(tf.global_variables_initializer())\n    model.load()\n    print(\"Filling experience replay buffer...\")\n    obs = env.reset()\n    obs_small = preprocess(obs)\n    state = np.stack([obs_small] * 4, axis=0)\n    # Fill experience replay buffer\n    for i in range(MIN_EXPERIENCES):\n        action = np.random.randint(0,K)\n        obs, reward, done, _ = env.step(action)\n        next_state = update_state(state, obs)\n        experience_replay_buffer.append((state, action, reward, next_state, done))\n        if done:\n            obs = env.reset()\n            obs_small = preprocess(obs)\n            state = np.stack([obs_small] * 4, axis=0)\n        else:\n            state = next_state\n        # Play a number of episodes and learn\n        for i in range(num_episodes):\n            t0 = datetime.now()\n            # Reset the environment\n            obs = env.reset()\n            obs_small = preprocess(obs)\n            state = np.stack([obs_small] * 4, axis=0)\n            assert (state.shape == (4, 80, 80))\n            loss = None\n            total_time_training = 0\n            num_steps_in_episode = 0\n            episode_reward = 0\n            done = False\n            while not done:\n                # Update target network\n                if total_t % TARGET_UPDATE_PERIOD == 0:\n                    target_model.copy_from(model)\n                    print(\"Copied model parameters to target network. total_t = %s, period = %s\" % (total_t, TARGET_UPDATE_PERIOD))\n                # Take action\n                action = model.sample_action(state, epsilon)\n                obs, reward, done, _ = env.step(action)\n                obs_small = preprocess(obs)\n                next_state = np.append(state[1:], np.expand_dims(obs_small, 0), axis=0)\n                episode_reward += reward\n                # Remove oldest experience if replay buffer is full\n                if len(experience_replay_buffer) == MAX_EXPERIENCES:\n                    experience_replay_buffer.pop(0)\n                    # Save the recent experience\n                    experience_replay_buffer.append((state, action, reward, next_state, done))\n\n                # Train the model and keep measure of time\n                t0_2 = datetime.now()\n                loss = learn(model, target_model, experience_replay_buffer, gamma, batch_sz)\n                dt = datetime.now() - t0_2\n                total_time_training += dt.total_seconds()\n                num_steps_in_episode += 1\n                state = next_state\n                total_t += 1\n                epsilon = max(epsilon - epsilon_change, epsilon_min)\n                duration = datetime.now() - t0\n                episode_rewards[i] = episode_reward\n                time_per_step = total_time_training / num_steps_in_episode\n                last_100_avg = episode_rewards[max(0, i - 100):i + 1].mean()\n                last_100_avgs.append(last_100_avg)\n                print(\"Episode:\", i,\"Duration:\", duration, \"Num steps:\", num_steps_in_episode, \"Reward:\", episode_reward, \"Training time per step:\", \"%.3f\" % time_per_step, \"Avg Reward (Last 100):\", \"%.3f\" % last_100_avg,\"Epsilon:\", \"%.3f\" % epsilon)\n                if i % 50 == 0:\n                    model.save(i)\n                sys.stdout.flush()\n\n#Plots\nplt.plot(last_100_avgs)\nplt.xlabel('episodes')\nplt.ylabel('Average Rewards')\nplt.show()\nenv.close()\n```", "```py\nenv = gym.envs.make(\"Breakout-v0\")\nframes = []\nwith tf.Session() as sess:\n    model.set_session(sess)\n    target_model.set_session(sess)\n    sess.run(tf.global_variables_initializer())\n    model.load()\n    obs = env.reset()\n    obs_small = preprocess(obs)\n    state = np.stack([obs_small] * 4, axis=0)\n    done = False\n    while not done:\n        action = model.sample_action(state, epsilon)\n        obs, reward, done, _ = env.step(action)\n        frames.append(env.render(mode='rgb_array'))\n        next_state = update_state(state, obs)\n        state = next_state\n```", "```py\nimport numpy as np\nimport gym\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom gym import wrappers\n%matplotlib inline\n```", "```py\nclass PolicyNetwork(object):\n    def __init__(self, N_SIZE, h=200, gamma=0.99, eta=1e-3, decay=0.99, save_path = 'models1/pong.ckpt' ):\n        self.gamma = gamma\n        self.save_path = save_path\n        # Placeholders for passing state....\n        self.tf_x = tf.placeholder(dtype=tf.float32, shape=[None, N_SIZE * N_SIZE], name=\"tf_x\")\n        self.tf_y = tf.placeholder(dtype=tf.float32, shape=[None, n_actions], name=\"tf_y\")\n        self.tf_epr = tf.placeholder(dtype=tf.float32, shape=[None, 1], name=\"tf_epr\")\n\n        # Weights\n        xavier_l1 = tf.truncated_normal_initializer(mean=0, stddev=1\\. / N_SIZE, dtype=tf.float32)\n        self.W1 = tf.get_variable(\"W1\", [N_SIZE * N_SIZE, h], initializer=xavier_l1)\n        xavier_l2 = tf.truncated_normal_initializer(mean=0, stddev=1\\. / np.sqrt(h), dtype=tf.float32)\n        self.W2 = tf.get_variable(\"W2\", [h, n_actions], initializer=xavier_l2)\n\n        #Build Computation\n        # tf reward processing (need tf_discounted_epr for policy gradient wizardry)\n        tf_discounted_epr = self.tf_discount_rewards(self.tf_epr)\n        tf_mean, tf_variance = tf.nn.moments(tf_discounted_epr, [0], shift=None, name=\"reward_moments\")\n        tf_discounted_epr -= tf_mean\n        tf_discounted_epr /= tf.sqrt(tf_variance + 1e-6)\n\n        #Define Optimizer, compute and apply gradients\n        self.tf_aprob = self.tf_policy_forward(self.tf_x)\n        loss = tf.losses.log_loss(labels = self.tf_y,\n        predictions = self.tf_aprob,\n        weights = tf_discounted_epr)\n        optimizer = tf.train.AdamOptimizer()\n        self.train_op = optimizer.minimize(loss)\n```", "```py\ndef set_session(self, session):\n    self.session = session\n    self.session.run(tf.global_variables_initializer())\n    self.saver = tf.train.Saver()\n\ndef tf_discount_rewards(self, tf_r): # tf_r ~ [game_steps,1]\n    discount_f = lambda a, v: a * self.gamma + v;\n    tf_r_reverse = tf.scan(discount_f, tf.reverse(tf_r, [0]))\n    tf_discounted_r = tf.reverse(tf_r_reverse, [0])\n    return tf_discounted_r\n\ndef tf_policy_forward(self, x): #x ~ [1,D]\n    h = tf.matmul(x, self.W1)\n    h = tf.nn.relu(h)\n    logp = tf.matmul(h, self.W2)\n    p = tf.nn.softmax(logp)\n    return p\n\ndef update(self, feed):\n    return self.session.run(self.train_op, feed)\n\ndef load(self):\n    self.saver = tf.train.Saver(tf.global_variables())\n    load_was_success = True \n    try:\n        save_dir = '/'.join(self.save_path.split('/')[:-1])\n        ckpt = tf.train.get_checkpoint_state(save_dir)\n        load_path = ckpt.model_checkpoint_path\n        print(load_path)\n        self.saver.restore(self.session, load_path)\n    except:\n        print(\"no saved model to load. starting new session\")\n        load_was_success = False\n    else:\n        print(\"loaded model: {}\".format(load_path))\n        saver = tf.train.Saver(tf.global_variables())\n        episode_number = int(load_path.split('-')[-1])\n\ndef save(self):\n    self.saver.save(self.session, self.save_path, global_step=n)\n    print(\"SAVED MODEL #{}\".format(n))\n\ndef predict_UP(self,x):\n    feed = {self.tf_x: np.reshape(x, (1, -1))}\n    aprob = self.session.run(self.tf_aprob, feed);\n    return aprob\n```", "```py\n# downsampling\ndef preprocess(I):\n    \"\"\" \n    prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \n    \"\"\"\n    I = I[35:195] # crop\n    I = I[::2,::2,0] # downsample by factor of 2\n    I[I == 144] = 0 # erase background (background type 1)\n    I[I == 109] = 0 # erase background (background type 2)\n    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n    return I.astype(np.float).ravel()\n```", "```py\n# Create Game Environment\nenv_name = \"Pong-v0\"\nenv = gym.make(env_name)\nenv = wrappers.Monitor(env, '/tmp/pong', force=True)\nn_actions = env.action_space.n # Number of possible actions\n# Initializing Game and State(t-1), action, reward, state(t)\nstates, rewards, labels = [], [], []\nobs = env.reset()\nprev_state = None\n\nrunning_reward = None\nrunning_rewards = []\nreward_sum = 0\nn = 0\ndone = False\nn_size = 80\nnum_episodes = 2500\n\n#Create Agent\nagent = PolicyNetwork(n_size)\n```", "```py\nwith tf.Session() as sess:\n    agent.set_session(sess)\n    sess.run(tf.global_variables_initializer())\n    agent.load()\n    # training loop\n    done = False\n    while not done and n< num_episodes:\n        # Preprocess the observation\n        cur_state = preprocess(obs)\n        diff_state = cur_state - prev_state if prev_state isn't None else np.zeros(n_size*n_size)\n        prev_state = cur_state\n\n        #Predict the action\n        aprob = agent.predict_UP(diff_state) ; aprob = aprob[0,:]\n        action = np.random.choice(n_actions, p=aprob)\n        #print(action)\n        label = np.zeros_like(aprob) ; label[action] = 1\n\n        # Step the environment and get new measurements\n        obs, reward, done, info = env.step(action)\n        env.render()\n        reward_sum += reward\n\n        # record game history\n        states.append(diff_state) ; labels.append(label) ; rewards.append(reward)\n\n        if done:\n            # update running reward\n            running_reward = reward_sum if running_reward is None else         running_reward * 0.99 + reward_sum * 0.01    \n            running_rewards.append(running_reward)\n            #print(np.vstack(rs).shape)\n            feed = {agent.tf_x: np.vstack(states), agent.tf_epr: np.vstack(rewards), agent.tf_y: np.vstack(labels)}\n            agent.update(feed)\n            # print progress console\n            if n % 10 == 0:\n                print ('ep {}: reward: {}, mean reward: {:3f}'.format(n, reward_sum, running_reward))\n            else:\n                print ('\\tep {}: reward: {}'.format(n, reward_sum))\n\n            # Start next episode and save model\n            states, rewards, labels = [], [], []\n            obs = env.reset()\n            n += 1 # the Next Episode\n\n            reward_sum = 0\n            if n % 50 == 0:\n                agent.save()\n            done = False\n\nplt.plot(running_rewards)\nplt.xlabel('episodes')\nplt.ylabel('Running Averge')\nplt.show()\nenv.close()\n```"]