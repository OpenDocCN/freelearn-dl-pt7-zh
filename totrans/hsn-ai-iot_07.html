<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Generative Models for IoT</span></h1>
                </header>
            
            <article>
                
<p><strong><span class="koboSpan" id="kobo.2.1">Machine learning</span></strong><span class="koboSpan" id="kobo.3.1"> (</span><strong><span class="koboSpan" id="kobo.4.1">ML</span></strong><span class="koboSpan" id="kobo.5.1">) and </span><strong><span class="koboSpan" id="kobo.6.1">Artificial Intelligence</span></strong><span class="koboSpan" id="kobo.7.1"> (</span><strong><span class="koboSpan" id="kobo.8.1">AI</span></strong><span class="koboSpan" id="kobo.9.1">) have touched almost all fields related to man. </span><span class="koboSpan" id="kobo.9.2">Agriculture, music, health, defense—you won't find a single field where AI hasn't left its mark. </span><span class="koboSpan" id="kobo.9.3">The enormous success of AI/ML, besides the presence of computational powers, also depends on the generation of a significant amount of data. </span><span class="koboSpan" id="kobo.9.4">The majority of the data generated is unlabeled, and hence understanding the inherent distribution of the data is an important ML task. </span><span class="koboSpan" id="kobo.9.5">It's here that generative models come into the picture.</span></p>
<p><span class="koboSpan" id="kobo.10.1">In the past few years, deep generative models have shown great success in understanding data distribution and have been used in a variety of applications. </span><span class="koboSpan" id="kobo.10.2">Two of the most popular generative models are </span><strong><span class="koboSpan" id="kobo.11.1">Variational Autoencoders</span></strong><span class="koboSpan" id="kobo.12.1"> (</span><strong><span class="koboSpan" id="kobo.13.1">VAEs</span></strong><span class="koboSpan" id="kobo.14.1">) and </span><strong><span class="koboSpan" id="kobo.15.1">Generative Adversarial Networks</span></strong><span class="koboSpan" id="kobo.16.1"> (</span><strong><span class="koboSpan" id="kobo.17.1">GANs</span></strong><span class="koboSpan" id="kobo.18.1">).</span></p>
<p><span class="koboSpan" id="kobo.19.1">In this chapter, we'll learn about both VAEs and GANs and use them to generate images. </span><span class="koboSpan" id="kobo.19.2">After reading this chapter, you'll have covered the following:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.20.1">Knowing the difference between generative networks and discriminative networks</span></li>
<li><span class="koboSpan" id="kobo.21.1">Learning about VAEs</span></li>
<li><span class="koboSpan" id="kobo.22.1">Understanding the intuitive functioning of GANs</span></li>
<li><span class="koboSpan" id="kobo.23.1">Implementing a vanilla GAN and using it to generate handwritten digits</span></li>
<li><span class="koboSpan" id="kobo.24.1">Knowing the most popular variation of GAN, the </span><span><span class="koboSpan" id="kobo.25.1">Deep Convolutional GAN</span></span></li>
<li><span class="koboSpan" id="kobo.26.1">Implementing the </span><span><span class="koboSpan" id="kobo.27.1">Deep Convolutional GAN</span></span><span class="koboSpan" id="kobo.28.1"> in TensorFlow and using it to generate faces</span></li>
<li><span class="koboSpan" id="kobo.29.1">Knowing further modifications and applications of GANs</span></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Introduction</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Generative models are an exciting new branch of deep learning models that learn through unsupervised learning. </span><span class="koboSpan" id="kobo.2.2">The main idea is to generate new samples having the same distribution as the given training data; for example, a network trained on handwritten digits can create new digits that aren't in the dataset but are similar to them. </span><span class="koboSpan" id="kobo.2.3">Formally, we can say that if the training data follows the distribution </span><em><span class="koboSpan" id="kobo.3.1">P</span></em><sub><span class="koboSpan" id="kobo.4.1">data</span></sub><span class="koboSpan" id="kobo.5.1">(</span><em><span class="koboSpan" id="kobo.6.1">x</span></em><span class="koboSpan" id="kobo.7.1">), then the goal of generative models is to estimate the probability density function </span><em><span class="koboSpan" id="kobo.8.1">P</span></em><sub><span class="koboSpan" id="kobo.9.1">model</span></sub><span class="koboSpan" id="kobo.10.1">(</span><em><span class="koboSpan" id="kobo.11.1">x</span></em><span class="koboSpan" id="kobo.12.1">), which is similar to </span><em><span class="koboSpan" id="kobo.13.1">P</span></em><sub><span class="koboSpan" id="kobo.14.1">data</span></sub><span><span class="koboSpan" id="kobo.15.1">(</span></span><em><span class="koboSpan" id="kobo.16.1">x</span></em><span><span class="koboSpan" id="kobo.17.1">).</span></span></p>
<p><span><span class="koboSpan" id="kobo.18.1">Generative models can be classified into two types: </span></span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.19.1">Explicit generative models</span></strong><span class="koboSpan" id="kobo.20.1">: Here, the probability density function </span><em><span class="koboSpan" id="kobo.21.1">P</span></em><sub><span class="koboSpan" id="kobo.22.1">model</span></sub><span><span class="koboSpan" id="kobo.23.1">(</span></span><em><span class="koboSpan" id="kobo.24.1">x</span></em><span><span class="koboSpan" id="kobo.25.1">) is explicitly defined and solved. </span><span class="koboSpan" id="kobo.25.2">The density function may be tractable as in the case of PixelRNN/CNN, or an approximation of the density function as in the case of VAE.</span></span></li>
<li><strong><span class="koboSpan" id="kobo.26.1">Implicit generative models</span></strong><span class="koboSpan" id="kobo.27.1">: In these, the network learns to generate a sample from </span><em><span class="koboSpan" id="kobo.28.1">P</span></em><sub><span class="koboSpan" id="kobo.29.1">model</span></sub><span><span class="koboSpan" id="kobo.30.1">(</span></span><em><span class="koboSpan" id="kobo.31.1">x</span></em><span><span class="koboSpan" id="kobo.32.1">) without explicitly defining it. </span><span class="koboSpan" id="kobo.32.2">GANs are an example of this type of generative model.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.33.1">In this chapter, we'll explore VAE, an explicit generative model, and GAN, an implicit generative model. </span><span class="koboSpan" id="kobo.33.2">Generative models can be instrumental in generating realistic samples, and they can be used to perform super-resolution, colorization, and so on. </span><span class="koboSpan" id="kobo.33.3">With time series data, we can even use them for simulation and planning. </span><span class="koboSpan" id="kobo.33.4">And last but not least, they can also help us in understanding the latent representation of data.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Generating images using VAEs</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">From </span><a href="cb9d27c5-e98d-44b6-a947-691b0bc64766.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 4</span></a><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Deep Learning for IOT</span></em><span class="koboSpan" id="kobo.6.1">, you should be familiar with autoencoders and their functions. </span><span class="koboSpan" id="kobo.6.2">VAEs are a type of autoencoder; here, we retain the (trained) </span><strong><span class="koboSpan" id="kobo.7.1">Decoder</span></strong><span class="koboSpan" id="kobo.8.1"> part, which can be used by feeding random latent features </span><strong><span class="koboSpan" id="kobo.9.1">z</span></strong><span class="koboSpan" id="kobo.10.1"> to generate data similar to the training data. </span><span class="koboSpan" id="kobo.10.2">Now, if you remember, in autoencoders, the </span><strong><span class="koboSpan" id="kobo.11.1">Encoder</span></strong><span class="koboSpan" id="kobo.12.1"> results in the generation of low-dimensional features, </span><strong><span class="koboSpan" id="kobo.13.1">z</span></strong><span class="koboSpan" id="kobo.14.1">:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.15.1"><img class="aligncenter size-full wp-image-969 image-border" src="assets/dd786420-2201-4898-975f-21491a2ed0b9.png" style="width:20.17em;height:23.58em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="koboSpan" id="kobo.16.1">The architecture of autoencoders</span></div>
<p><span class="koboSpan" id="kobo.17.1">The VAEs are concerned with finding the likelihood function </span><em><span class="koboSpan" id="kobo.18.1">p</span></em><span class="koboSpan" id="kobo.19.1">(</span><em><span class="koboSpan" id="kobo.20.1">x</span></em><span class="koboSpan" id="kobo.21.1">) from the latent features </span><em><span class="koboSpan" id="kobo.22.1">z</span></em><span class="koboSpan" id="kobo.23.1">: </span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.24.1"><img class="fm-editor-equation" src="assets/74b8ddce-feed-42ca-93ee-c4dcb085498b.png" style="width:13.83em;height:3.00em;"/></span></p>
<p><span class="koboSpan" id="kobo.25.1">This is an intractable density function, and it isn't possible to directly optimize it; instead, we obtain a lower bound by using a simple Gaussian prior </span><em><span class="koboSpan" id="kobo.26.1">p</span></em><span class="koboSpan" id="kobo.27.1">(</span><em><span class="koboSpan" id="kobo.28.1">z</span></em><span class="koboSpan" id="kobo.29.1">) and making both </span><strong><span class="koboSpan" id="kobo.30.1">Encoder</span></strong><span class="koboSpan" id="kobo.31.1"> and </span><strong><span class="koboSpan" id="kobo.32.1">Decoder</span></strong><span class="koboSpan" id="kobo.33.1"> networks probabilistic:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.34.1"><img src="assets/7cd997c5-acba-4732-9b5b-bb83ca6d0bab.png" style="width:33.08em;height:26.67em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.35.1"> Architecture of a VAE</span></div>
<p><span class="koboSpan" id="kobo.36.1">This allows us to define a tractable lower bound on the log likelihood, given by the following:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.37.1"><img class="fm-editor-equation" src="assets/18eabc97-25f9-481b-926c-b58f879c28d2.png" style="width:43.67em;height:2.00em;"/></span></p>
<p><span class="koboSpan" id="kobo.38.1">In the preceding, </span><em><span class="koboSpan" id="kobo.39.1">θ</span></em><span class="koboSpan" id="kobo.40.1"> represents the decoder network parameters and </span><em><span class="koboSpan" id="kobo.41.1">φ</span></em><span class="koboSpan" id="kobo.42.1"> the encoder network parameters. </span><span class="koboSpan" id="kobo.42.2">The network is trained by maximizing this lower bound: </span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.43.1"><img class="fm-editor-equation" src="assets/70d81705-2637-40ce-803f-b8c5ac9882b8.png" style="width:20.75em;height:4.33em;"/></span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.44.1">The first term in the lower bound is responsible for the reconstruction of the input data, and the second term for making the approximate posterior distribution close to prior. </span><span class="koboSpan" id="kobo.44.2">Once trained, the encoder network works as a recognition or inference network, and the decoder network acts as the generator. </span></p>
<div class="packt_tip packt_infobox"><span class="koboSpan" id="kobo.45.1">You can refer to the detailed derivation in the paper titled </span><em><span><span class="koboSpan" id="kobo.46.1">Auto-Encoding Variational Bayes</span></span></em><span class="koboSpan" id="kobo.47.1"> by Diederik P Kingma and Max Welling, presented at ICLR 2014 (</span><a href="https://arxiv.org/abs/1312.6114"><span class="koboSpan" id="kobo.48.1">https://arxiv.org/abs/1312.6114</span></a><span class="koboSpan" id="kobo.49.1">).</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">VAEs in TensorFlow</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's now see VAE in action. </span><span class="koboSpan" id="kobo.2.2">In this example code, we'll be using the standard MNIST dataset and train a VAE to generate handwritten digits. </span><span class="koboSpan" id="kobo.2.3">Since the MNIST dataset is simple, the encoder and decoder network will consist of only fully connected layers; this will allow us to concentrate on the VAE architecture. </span><span class="koboSpan" id="kobo.2.4">If you plan to generate complex images (such as CIFAR-10), you'll need to modify the encoder and decoder network to convolution and deconvolution networks:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.3.1">The first step as in all previous cases is to import all of the necessary modules. </span><span class="koboSpan" id="kobo.3.2">Here, we'll use the TensorFlow higher API, </span><kbd><span class="koboSpan" id="kobo.4.1">tf.contrib</span></kbd><span class="koboSpan" id="kobo.5.1">, to make the fully connected layers. </span><span class="koboSpan" id="kobo.5.2">Note that this saves us from the hassle of declaring weights and biases for each layer independently:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.6.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.7.1">import tensorflow as tf</span><br/><br/><span class="koboSpan" id="kobo.8.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.9.1">%matplotlib inline</span><br/><br/><span class="koboSpan" id="kobo.10.1">from tensorflow.contrib.layers import fully_connected</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.11.1">We read the data. </span><span class="koboSpan" id="kobo.11.2">The MNIST dataset is available in TensorFlow tutorials, so we'll take it directly from there:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.12.1"># Load MNIST data in a format suited for tensorflow.</span><br/><span class="koboSpan" id="kobo.13.1">from tensorflow.examples.tutorials.mnist import input_data</span><br/><span class="koboSpan" id="kobo.14.1">mnist = input_data.read_data_sets('MNIST_data', one_hot=True)</span><br/><span class="koboSpan" id="kobo.15.1">n_samples = mnist.train.num_examples</span><br/><span class="koboSpan" id="kobo.16.1">n_input = mnist.train.images[0].shape[0]</span></pre>
<p class="mce-root"/>
<ol start="3">
<li><span class="koboSpan" id="kobo.17.1">We define the </span><kbd><span class="koboSpan" id="kobo.18.1">VariationalAutoencoder</span></kbd><span class="koboSpan" id="kobo.19.1"> class; this class is the core code. </span><span class="koboSpan" id="kobo.19.2">It contains methods for defining the encoder and decoder network. </span><span class="koboSpan" id="kobo.19.3">The encoder generates the mean and variance of the latent feature </span><em><span class="koboSpan" id="kobo.20.1">z</span></em><span class="koboSpan" id="kobo.21.1"> as </span><kbd><span class="koboSpan" id="kobo.22.1">z_mu</span></kbd><span class="koboSpan" id="kobo.23.1"> and </span><kbd><span class="koboSpan" id="kobo.24.1">z_sigma</span></kbd><span class="koboSpan" id="kobo.25.1"> respectively. </span><span class="koboSpan" id="kobo.25.2">Using these, a sample </span><kbd><span class="koboSpan" id="kobo.26.1">Z</span></kbd><span class="koboSpan" id="kobo.27.1"> is taken. </span><span class="koboSpan" id="kobo.27.2">The latent feature </span><em><span class="koboSpan" id="kobo.28.1">z</span></em><span class="koboSpan" id="kobo.29.1"> is then passed to the decoder network to generate </span><kbd><span class="koboSpan" id="kobo.30.1">x_hat</span></kbd><span class="koboSpan" id="kobo.31.1">. </span><span class="koboSpan" id="kobo.31.2">The network minimizes the sum of the reconstruction loss and latent loss using the Adam optimizer. </span><span class="koboSpan" id="kobo.31.3">The class also defines methods for reconstruction, generation, transformation (to latent space), and training a single step:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.32.1">class VariationalAutoencoder(object):</span><br/><span class="koboSpan" id="kobo.33.1">    def __init__(self,n_input, n_z,</span><br/><span class="koboSpan" id="kobo.34.1">        learning_rate=0.001, batch_size=100):</span><br/><span class="koboSpan" id="kobo.35.1">        self.batch_size = batch_size</span><br/><span class="koboSpan" id="kobo.36.1">        self.n_input = n_input</span><br/><span class="koboSpan" id="kobo.37.1">        self.n_z = n_z</span><br/> <br/><span class="koboSpan" id="kobo.38.1">        # Place holder for the input </span><br/><span class="koboSpan" id="kobo.39.1">        self.x = tf.placeholder(tf.float32, shape = [None, n_input])</span><br/> <br/> <br/><span class="koboSpan" id="kobo.40.1">        # Use Encoder Network to determine mean and </span><br/><span class="koboSpan" id="kobo.41.1">        # (log) variance of Gaussian distribution in the latent space</span><br/><span class="koboSpan" id="kobo.42.1">        self.z_mean, self.z_log_sigma_sq = self._encoder_network()</span><br/><span class="koboSpan" id="kobo.43.1">        # Draw a sample z from Gaussian distribution</span><br/><span class="koboSpan" id="kobo.44.1">        eps = tf.random_normal((self.batch_size, n_z), 0, 1, dtype=tf.float32)</span><br/><span class="koboSpan" id="kobo.45.1">        # z = mu + sigma*epsilon</span><br/><span class="koboSpan" id="kobo.46.1">        self.z = tf.add(self.z_mean,tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))</span><br/><span class="koboSpan" id="kobo.47.1">        # Use Decoder network to determine mean of</span><br/><span class="koboSpan" id="kobo.48.1">        # Bernoulli distribution of reconstructed input</span><br/><span class="koboSpan" id="kobo.49.1">        self.x_hat = self._decoder_network()</span><br/> <br/> <br/><span class="koboSpan" id="kobo.50.1">        # Define loss function based variational upper-bound and </span><br/><span class="koboSpan" id="kobo.51.1">        # corresponding optimizer</span><br/><span class="koboSpan" id="kobo.52.1">        # define generation loss</span><br/><span class="koboSpan" id="kobo.53.1">        reconstruction_loss = \</span><br/><span class="koboSpan" id="kobo.54.1">            -tf.reduce_sum(self.x * tf.log(1e-10 + self.x_hat)</span><br/><span class="koboSpan" id="kobo.55.1">            + (1-self.x) * tf.log(1e-10 + 1 - self.x_hat), 1)</span><br/><span class="koboSpan" id="kobo.56.1">        self.reconstruction_loss = tf.reduce_mean(reconstruction_loss)</span><br/> <br/><span class="koboSpan" id="kobo.57.1">        latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq </span><br/><span class="koboSpan" id="kobo.58.1">            - tf.square(self.z_mean)- tf.exp(self.z_log_sigma_sq), 1)</span><br/><span class="koboSpan" id="kobo.59.1">        self.latent_loss = tf.reduce_mean(latent_loss)</span><br/><span class="koboSpan" id="kobo.60.1">        self.cost = tf.reduce_mean(reconstruction_loss + latent_loss) </span><br/><span class="koboSpan" id="kobo.61.1">        # average over batch</span><br/><span class="koboSpan" id="kobo.62.1">        # Define the optimizer</span><br/><span class="koboSpan" id="kobo.63.1">        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)</span><br/> <br/> <br/><span class="koboSpan" id="kobo.64.1">        # Initializing the tensor flow variables</span><br/><span class="koboSpan" id="kobo.65.1">        init = tf.global_variables_initializer()</span><br/><span class="koboSpan" id="kobo.66.1">        # Launch the session</span><br/><span class="koboSpan" id="kobo.67.1">        self.sess = tf.InteractiveSession()</span><br/><span class="koboSpan" id="kobo.68.1">        self.sess.run(init)</span><br/> <br/> <br/> <br/><span class="koboSpan" id="kobo.69.1">    # Create encoder network</span><br/><span class="koboSpan" id="kobo.70.1">    def _encoder_network(self):</span><br/><span class="koboSpan" id="kobo.71.1">        # Generate probabilistic encoder (inference network), which</span><br/><span class="koboSpan" id="kobo.72.1">        # maps inputs onto a normal distribution in latent space.</span><br/><span class="koboSpan" id="kobo.73.1">        layer_1 = fully_connected(self.x,500,activation_fn=tf.nn.softplus) </span><br/><span class="koboSpan" id="kobo.74.1">        layer_2 = fully_connected(layer_1, 500, activation_fn=tf.nn.softplus) </span><br/><span class="koboSpan" id="kobo.75.1">        z_mean = fully_connected(layer_2,self.n_z, activation_fn=None)</span><br/><span class="koboSpan" id="kobo.76.1">        z_log_sigma_sq = fully_connected(layer_2, self.n_z, activation_fn=None)</span><br/><span class="koboSpan" id="kobo.77.1">        return (z_mean, z_log_sigma_sq)</span><br/><br/><span class="koboSpan" id="kobo.78.1">    # Create decoder network</span><br/><span class="koboSpan" id="kobo.79.1">    def _decoder_network(self):</span><br/><span class="koboSpan" id="kobo.80.1">        # Generate probabilistic decoder (generator network), which</span><br/><span class="koboSpan" id="kobo.81.1">        # maps points in the latent space onto a Bernoulli distribution in the data space.</span><br/><span class="koboSpan" id="kobo.82.1">        layer_1 = fully_connected(self.z,500,activation_fn=tf.nn.softplus) </span><br/><span class="koboSpan" id="kobo.83.1">        layer_2 = fully_connected(layer_1, 500, activation_fn=tf.nn.softplus) </span><br/><span class="koboSpan" id="kobo.84.1">        x_hat = fully_connected(layer_2, self.n_input, activation_fn=tf.nn.sigmoid)</span><br/> <br/><span class="koboSpan" id="kobo.85.1">        return x_hat</span><br/> <br/> <br/> <br/><span class="koboSpan" id="kobo.86.1">    def single_step_train(self, X):</span><br/><span class="koboSpan" id="kobo.87.1">        _,cost,recon_loss,latent_loss = self.sess.run([self.optimizer,         self.cost,self.reconstruction_loss,self.latent_loss],feed_dict={self.x: X})</span><br/><span class="koboSpan" id="kobo.88.1">        return cost, recon_loss, latent_loss</span><br/> <br/><span class="koboSpan" id="kobo.89.1">    def transform(self, X):</span><br/><span class="koboSpan" id="kobo.90.1">        """Transform data by mapping it into the latent space."""</span><br/><span class="koboSpan" id="kobo.91.1">        # Note: This maps to mean of distribution, we could alternatively</span><br/><span class="koboSpan" id="kobo.92.1">        # sample from Gaussian distribution</span><br/><span class="koboSpan" id="kobo.93.1">        return self.sess.run(self.z_mean, feed_dict={self.x: X})</span><br/> <br/><span class="koboSpan" id="kobo.94.1">    def generate(self, z_mu=None):</span><br/><span class="koboSpan" id="kobo.95.1">        """ Generate data by sampling from latent space.</span><br/> <br/><span class="koboSpan" id="kobo.96.1">        If z_mu isn't None, data for this point in latent space is</span><br/><span class="koboSpan" id="kobo.97.1">        generated. </span><span class="koboSpan" id="kobo.97.2">Otherwise, z_mu is drawn from prior in latent </span><br/><span class="koboSpan" id="kobo.98.1">        space. </span><br/><span class="koboSpan" id="kobo.99.1">        """</span><br/><span class="koboSpan" id="kobo.100.1">        if z_mu is None:</span><br/><span class="koboSpan" id="kobo.101.1">            z_mu = np.random.normal(size=n_z)</span><br/><span class="koboSpan" id="kobo.102.1">            # Note: This maps to mean of distribution, we could alternatively    </span><br/><span class="koboSpan" id="kobo.103.1">            # sample from Gaussian distribution</span><br/><span class="koboSpan" id="kobo.104.1">        return self.sess.run(self.x_hat,feed_dict={self.z: z_mu})</span><br/> <br/><span class="koboSpan" id="kobo.105.1">    def reconstruct(self, X):</span><br/><span class="koboSpan" id="kobo.106.1">        """ Use VAE to reconstruct given data. </span><span class="koboSpan" id="kobo.106.2">"""</span><br/><span class="koboSpan" id="kobo.107.1">        return self.sess.run(self.x_hat, feed_dict={self.x: X})</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.108.1">With all ingredients in place, let's train our VAE. </span><span class="koboSpan" id="kobo.108.2">We do this with the help of the </span><kbd><span class="koboSpan" id="kobo.109.1">train</span></kbd><span class="koboSpan" id="kobo.110.1"> function:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.111.1">def train(n_input,n_z, learning_rate=0.001,</span><br/><span class="koboSpan" id="kobo.112.1">    batch_size=100, training_epochs=10, display_step=5):</span><br/><span class="koboSpan" id="kobo.113.1">    vae = VariationalAutoencoder(n_input,n_z, </span><br/><span class="koboSpan" id="kobo.114.1">        learning_rate=learning_rate, </span><br/><span class="koboSpan" id="kobo.115.1">        batch_size=batch_size)</span><br/><span class="koboSpan" id="kobo.116.1">    # Training cycle</span><br/><span class="koboSpan" id="kobo.117.1">    for epoch in range(training_epochs):</span><br/><span class="koboSpan" id="kobo.118.1">        avg_cost, avg_r_loss, avg_l_loss = 0., 0., 0.</span><br/><span class="koboSpan" id="kobo.119.1">        total_batch = int(n_samples / batch_size)</span><br/><span class="koboSpan" id="kobo.120.1">        # Loop over all batches</span><br/><span class="koboSpan" id="kobo.121.1">        for i in range(total_batch):</span><br/><span class="koboSpan" id="kobo.122.1">            batch_xs, _ = mnist.train.next_batch(batch_size)</span><br/><span class="koboSpan" id="kobo.123.1">            # Fit training using batch data</span><br/><span class="koboSpan" id="kobo.124.1">            cost,r_loss, l_loss = vae.single_step_train(batch_xs)</span><br/><span class="koboSpan" id="kobo.125.1">            # Compute average loss</span><br/><span class="koboSpan" id="kobo.126.1">            avg_cost += cost / n_samples * batch_size</span><br/><span class="koboSpan" id="kobo.127.1">            avg_r_loss += r_loss / n_samples * batch_size</span><br/><span class="koboSpan" id="kobo.128.1">            avg_l_loss += l_loss / n_samples * batch_size</span><br/><span class="koboSpan" id="kobo.129.1">        # Display logs per epoch step</span><br/><span class="koboSpan" id="kobo.130.1">        if epoch % display_step == 0:</span><br/><span class="koboSpan" id="kobo.131.1">            print("Epoch: {:4d} cost={:.4f} Reconstruction loss = {:.4f} Latent Loss = {:.4f}".format(epoch,avg_cost,avg_r_loss,avg_l_loss))</span><br/><span class="koboSpan" id="kobo.132.1">     return vae</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.133.1">In the following screenshot, you can see the reconstructed digits (left) and generated handwritten digits (right) for a VAE with the latent space of size 10:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.134.1"><img class="aligncenter size-full wp-image-970 image-border" src="assets/786be619-47b5-41ef-a066-a0167120a6e3.png" style="width:45.08em;height:20.17em;"/></span></p>
<ol start="6">
<li><span class="koboSpan" id="kobo.135.1">As discussed earlier, the encoder network reduces the dimensions of the input space. </span><span class="koboSpan" id="kobo.135.2">To make it clearer, we reduce the dimension of latent space to 2. </span><span class="koboSpan" id="kobo.135.3">In the following, you can see that each label is separated in the two-dimensional z-space:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.136.1"><img class="aligncenter size-full wp-image-971 image-border" src="assets/76b1c36f-0c79-4c05-ac28-458193f9e3ba.png" style="width:27.42em;height:22.25em;"/></span></p>
<ol start="7">
<li><span class="koboSpan" id="kobo.137.1">The reconstructed and generated digits from a VAE with a latent space of the dimension 2 are as follows:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.138.1"><img class="aligncenter size-full wp-image-1100 image-border" src="assets/412c3c17-5bea-4042-b19b-205bdd11b800.png" style="width:41.00em;height:18.33em;"/></span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.139.1">The interesting thing to note from the preceding screenshot (right) is how changing the values of the two-dimensional </span><em><span class="koboSpan" id="kobo.140.1">z</span></em><span class="koboSpan" id="kobo.141.1"> results in different strokes and different numbers. </span><span class="koboSpan" id="kobo.141.2">The complete code is on GitHub in </span><kbd><span class="koboSpan" id="kobo.142.1">Chapter 07</span></kbd><span class="koboSpan" id="kobo.143.1">, in the file named </span><kbd><span class="koboSpan" id="kobo.144.1">VariationalAutoEncoders_MNIST.ipynb</span></kbd><span class="koboSpan" id="kobo.145.1">: </span></p>
<pre><span class="koboSpan" id="kobo.146.1">tf.contrib.layers.fully_connected(</span><br/><span class="koboSpan" id="kobo.147.1">    inputs,</span><br/><span class="koboSpan" id="kobo.148.1">    num_outputs,</span><br/><span class="koboSpan" id="kobo.149.1">    activation_fn=tf.nn.relu,</span><br/><span class="koboSpan" id="kobo.150.1">    normalizer_fn=None,</span><br/><span class="koboSpan" id="kobo.151.1">    normalizer_params=None,</span><br/><span class="koboSpan" id="kobo.152.1">    weights_initializer=intializers.xavier_intializer(),</span><br/><span class="koboSpan" id="kobo.153.1">    weights_regularizer= None, </span><br/><span class="koboSpan" id="kobo.154.1">    biases_initializer=tf.zeros_intializer(),</span><br/><span class="koboSpan" id="kobo.155.1">    biases_regularizer=None,</span><br/><span class="koboSpan" id="kobo.156.1">    reuse=None,</span><br/><span class="koboSpan" id="kobo.157.1">    variables_collections=None,</span><br/><span class="koboSpan" id="kobo.158.1">    outputs_collections=None,</span><br/><span class="koboSpan" id="kobo.159.1">    trainable=True,</span><br/><span class="koboSpan" id="kobo.160.1">    scope=None</span><br/><span class="koboSpan" id="kobo.161.1">)</span></pre>
<div class="packt_infobox"><span><span class="koboSpan" id="kobo.162.1">The layers (c</span></span><span><span class="koboSpan" id="kobo.163.1">ontrib</span></span><span><span class="koboSpan" id="kobo.164.1">) is a higher level package included in TensorFlow. </span><span class="koboSpan" id="kobo.164.2">It provides </span></span><span class="koboSpan" id="kobo.165.1">operations</span><span><span class="koboSpan" id="kobo.166.1"> for building neural network layers, regularizers, summaries, and so on. </span><span class="koboSpan" id="kobo.166.2">In the preceding </span></span><span><span class="koboSpan" id="kobo.167.1">code</span></span><span><span class="koboSpan" id="kobo.168.1">, we used the </span></span><kbd><span class="koboSpan" id="kobo.169.1">tf.contrib.layers.fully_connected()</span></kbd><span><span class="koboSpan" id="kobo.170.1"> operation , defined in </span></span><a href="https://www.github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/contrib/layers/python/layers/layers.py"><span class="koboSpan" id="kobo.171.1">tensorflow/contrib/layers/python/layers/layers.py</span></a><span><span class="koboSpan" id="kobo.172.1">, which adds a fully connected layer. </span><span class="koboSpan" id="kobo.172.2">By default, it creates weights representing a fully connected interconnection matrix, initialized by default using the Xavier initialization. </span><span class="koboSpan" id="kobo.172.3">It also creates biases initialized to zero. </span><span class="koboSpan" id="kobo.172.4">It provides an option for choosing normalization and activation function as </span></span><span class="koboSpan" id="kobo.173.1">well</span><span><span class="koboSpan" id="kobo.174.1">. </span></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">GANs</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">GANs are implicit generative networks. </span><span class="koboSpan" id="kobo.2.2">During a session at Quora, Yann LeCun, </span><span><span class="koboSpan" id="kobo.3.1">Director of AI Research at Facebook and Professor at NYU,</span></span><span class="koboSpan" id="kobo.4.1"> described GANs as </span><em><span class="koboSpan" id="kobo.5.1">the most interesting idea in the last 10 years in ML</span></em><span class="koboSpan" id="kobo.6.1">. </span><span class="koboSpan" id="kobo.6.2">At present, lots of research is happening in GANs. </span><span class="koboSpan" id="kobo.6.3">Major AI/ML conferences conducted in the last few years have reported a majority of papers related to GANs.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.7.1">GANs were proposed by Ian J. </span><span class="koboSpan" id="kobo.7.2">Goodfellow and Yoshua Bengio in the paper </span><em><span class="koboSpan" id="kobo.8.1">Generative Adversarial Networks</span></em><span class="koboSpan" id="kobo.9.1"> in the year 2014 (</span><a href="https://arxiv.org/abs/1406.2661"><span class="koboSpan" id="kobo.10.1">https://arxiv.org/abs/1406.2661</span></a><span class="koboSpan" id="kobo.11.1">). </span><span class="koboSpan" id="kobo.11.2">They're inspired by the two-player game scenario. </span><span class="koboSpan" id="kobo.11.3">Like the two players of the game, in GANs, two networks—one called the </span><strong><span class="koboSpan" id="kobo.12.1">discriminative network</span></strong><span class="koboSpan" id="kobo.13.1"> and the other the </span><strong><span class="koboSpan" id="kobo.14.1">generative network</span></strong><span><span class="koboSpan" id="kobo.15.1">—</span></span><span class="koboSpan" id="kobo.16.1">compete with each other. </span><span class="koboSpan" id="kobo.16.2">The generative network tries to generate data similar to the input data, and the discriminator network has to identify whether the data it's seeing is real or fake (that is, generated by a generator). </span><span class="koboSpan" id="kobo.16.3">Every time the discriminator finds a difference between the distribution of true input and fake data, the generator adjusts its weights to reduce the difference. </span><span><span class="koboSpan" id="kobo.17.1">To summarize, the discriminative network tries to learn the boundary between counterfeit and real data, and the generative network tries to learn the distribution of training data. </span></span><span class="koboSpan" id="kobo.18.1">As the training ends, the generator learns to produce images exactly like the input data distribution, and the discriminator can no longer differentiate the two. </span><span class="koboSpan" id="kobo.18.2">The general architecture of a GAN is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.19.1"><img class="aligncenter size-full wp-image-972 image-border" src="assets/ea57cdf9-8c57-41a7-ba6d-0b89d7ca7c72.png" style="width:150.08em;height:68.83em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.20.1">Architecture of GANs</span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.21.1">Let's now delve deep into how GANs learn. </span><span class="koboSpan" id="kobo.21.2">Both the discriminator and generator take turns to learn. </span><span class="koboSpan" id="kobo.21.3">The learning can be divided into two steps:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.22.1">Here the </span><strong><span class="koboSpan" id="kobo.23.1">Discriminator</span></strong><span class="koboSpan" id="kobo.24.1">, </span><em><span class="koboSpan" id="kobo.25.1">D</span></em><span class="koboSpan" id="kobo.26.1">(</span><em><span class="koboSpan" id="kobo.27.1">x</span></em><span class="koboSpan" id="kobo.28.1">), learns. </span><span class="koboSpan" id="kobo.28.2">The </span><strong><span class="koboSpan" id="kobo.29.1">Generator</span></strong><span class="koboSpan" id="kobo.30.1">, </span><em><span class="koboSpan" id="kobo.31.1">G</span></em><span class="koboSpan" id="kobo.32.1">(</span><em><span class="koboSpan" id="kobo.33.1">z</span></em><span class="koboSpan" id="kobo.34.1">), is used to generate </span><strong><span class="koboSpan" id="kobo.35.1">Fake Images</span></strong><span class="koboSpan" id="kobo.36.1"> from random noise </span><strong><span class="koboSpan" id="kobo.37.1">z</span></strong><span class="koboSpan" id="kobo.38.1"> (which follows some </span><strong><span class="koboSpan" id="kobo.39.1">Prior</span></strong><span class="koboSpan" id="kobo.40.1"> distribution </span><em><span class="koboSpan" id="kobo.41.1">P</span></em><span class="koboSpan" id="kobo.42.1">(</span><em><span class="koboSpan" id="kobo.43.1">z</span></em><span class="koboSpan" id="kobo.44.1">)). </span><span class="koboSpan" id="kobo.44.2">The </span><strong><span class="koboSpan" id="kobo.45.1">Fake Images</span></strong><span class="koboSpan" id="kobo.46.1"> from the </span><strong><span class="koboSpan" id="kobo.47.1">Generator</span></strong><span class="koboSpan" id="kobo.48.1"> and the </span><strong><span class="koboSpan" id="kobo.49.1">Real Images</span></strong><span class="koboSpan" id="kobo.50.1"> from the training dataset are both fed to the </span><strong><span class="koboSpan" id="kobo.51.1">Discriminator</span></strong><span class="koboSpan" id="kobo.52.1"> and it performs supervised learning trying to separate fake from real. </span><span class="koboSpan" id="kobo.52.2">If </span><em><span class="koboSpan" id="kobo.53.1">P</span></em><sub><span class="koboSpan" id="kobo.54.1">data</span></sub><span class="koboSpan" id="kobo.55.1">(</span><em><span class="koboSpan" id="kobo.56.1">x</span></em><span class="koboSpan" id="kobo.57.1">) is the training dataset distribution, then the </span><strong><span class="koboSpan" id="kobo.58.1">Discriminator Network</span></strong><span class="koboSpan" id="kobo.59.1"> tries to maximize its objective so that </span><em><span class="koboSpan" id="kobo.60.1">D</span></em><span class="koboSpan" id="kobo.61.1">(</span><em><span class="koboSpan" id="kobo.62.1">x</span></em><span class="koboSpan" id="kobo.63.1">) is close to 1 when the input data is real, and close to 0 when the input data is fake. </span><span class="koboSpan" id="kobo.63.2">This can be achieved by performing the gradient ascent on the following objective function:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.64.1"><img class="fm-editor-equation" src="assets/e5e9e9e9-9f1c-49c5-82dd-b419df342f52.png" style="width:32.42em;height:2.00em;"/></span></p>
<ol start="2">
<li><span class="koboSpan" id="kobo.65.1">In the next step, the </span><strong><span class="koboSpan" id="kobo.66.1">Generator Network</span></strong><span class="koboSpan" id="kobo.67.1"> learns. </span><span class="koboSpan" id="kobo.67.2">Its goal is to fool the </span><strong><span class="koboSpan" id="kobo.68.1">Discriminator Network</span></strong><span class="koboSpan" id="kobo.69.1"> into thinking that generated </span><em><span class="koboSpan" id="kobo.70.1">G</span></em><span class="koboSpan" id="kobo.71.1">(</span><em><span class="koboSpan" id="kobo.72.1">z</span></em><span class="koboSpan" id="kobo.73.1">) is real, that is, force </span><em><span class="koboSpan" id="kobo.74.1">D</span></em><span class="koboSpan" id="kobo.75.1">(</span><em><span class="koboSpan" id="kobo.76.1">G</span></em><span class="koboSpan" id="kobo.77.1">(</span><em><span class="koboSpan" id="kobo.78.1">z</span></em><span class="koboSpan" id="kobo.79.1">)) close to 1. </span><span class="koboSpan" id="kobo.79.2">To achieve this, the </span><strong><span class="koboSpan" id="kobo.80.1">Generator Network</span></strong><span class="koboSpan" id="kobo.81.1"> minimizes the objective:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.82.1"><img class="fm-editor-equation" src="assets/8310f3e2-5c6d-4d72-9e72-0f2539a3d99d.png" style="width:20.83em;height:2.00em;"/></span></p>
<p><span class="koboSpan" id="kobo.83.1">The two steps are repeated sequentially. </span><span class="koboSpan" id="kobo.83.2">Once the training ends, the discriminator is no longer able to discriminate between real and fake data and the generator becomes a pro at creating data very similar to the training data. </span><span class="koboSpan" id="kobo.83.3">Well, it's easier said than done: as you experiment with GANs, you'll find that the training isn't very stable. </span><span class="koboSpan" id="kobo.83.4">It's an open research issue, and many variants of GAN have been proposed to rectify the problem.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Implementing a vanilla GAN in TensorFlow</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this section, we'll write a TensorFlow code to implement a GAN, as we learned in the previous section. </span><span class="koboSpan" id="kobo.2.2">We'll use simple MLP networks for both the discriminator and generator. </span><span class="koboSpan" id="kobo.2.3">And for simplicity, we'll use the MNIST dataset:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.3.1">As always, the first step is to add all of the necessary modules. </span><span class="koboSpan" id="kobo.3.2">Since we'll need to access and train the generator and discriminator parameters alternatively, we'll define our weights and biases in the present code for clarity. It's always better to initialize weights using the Xavier initialization and biases to all zeros. </span><span class="koboSpan" id="kobo.3.3">So, we also import from TensorFlow a method to perform Xavier initialization, from </span><kbd><span class="koboSpan" id="kobo.4.1">tensorflow.contrib.layers import xavier_initializer</span></kbd><span class="koboSpan" id="kobo.5.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.6.1"># import the necessaey modules</span><br/><span class="koboSpan" id="kobo.7.1">import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.8.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.9.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.10.1">import matplotlib.gridspec as gridspec</span><br/><span class="koboSpan" id="kobo.11.1">import os</span><br/><span class="koboSpan" id="kobo.12.1">from tensorflow.contrib.layers import xavier_initializer</span><br/><span class="koboSpan" id="kobo.13.1">%matplotlib inline</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.14.1">Let's read the data and define hyperparameters:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.15.1"># Load data</span><br/><span class="koboSpan" id="kobo.16.1">from tensorflow.examples.tutorials.mnist import input_data</span><br/><span class="koboSpan" id="kobo.17.1">data = input_data.read_data_sets('MNIST_data', one_hot=True)</span><br/><br/><span class="koboSpan" id="kobo.18.1"># define hyperparameters</span><br/><span class="koboSpan" id="kobo.19.1">batch_size = 128</span><br/><span class="koboSpan" id="kobo.20.1">Z_dim = 100</span><br/><span class="koboSpan" id="kobo.21.1">im_size = 28</span><br/><span class="koboSpan" id="kobo.22.1">h_size=128</span><br/><span class="koboSpan" id="kobo.23.1">learning_rate_D = .0005</span><br/><span class="koboSpan" id="kobo.24.1">learning_rate_G = .0006</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.25.1">We define the training parameters for both generator and discriminator. </span><span class="koboSpan" id="kobo.25.2">We also define the placeholders for </span><span><span class="koboSpan" id="kobo.26.1">input </span></span><kbd><span class="koboSpan" id="kobo.27.1">X</span></kbd><span class="koboSpan" id="kobo.28.1"> and latent </span><kbd><span class="koboSpan" id="kobo.29.1">Z</span></kbd><span class="koboSpan" id="kobo.30.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.31.1">#Create Placeholder for input X and random noise Z</span><br/><span class="koboSpan" id="kobo.32.1">X = tf.placeholder(tf.float32, shape=[None, im_size*im_size])</span><br/><span class="koboSpan" id="kobo.33.1">Z = tf.placeholder(tf.float32, shape=[None, Z_dim])</span><br/><span class="koboSpan" id="kobo.34.1">initializer=xavier_initializer()</span><br/><br/><span class="koboSpan" id="kobo.35.1"># Define Discriminator and Generator training variables</span><br/><span class="koboSpan" id="kobo.36.1">#Discriminiator</span><br/><span class="koboSpan" id="kobo.37.1">D_W1 = tf.Variable(initializer([im_size*im_size, h_size]))</span><br/><span class="koboSpan" id="kobo.38.1">D_b1 = tf.Variable(tf.zeros(shape=[h_size]))</span><br/><br/><span class="koboSpan" id="kobo.39.1">D_W2 = tf.Variable(initializer([h_size, 1]))</span><br/><span class="koboSpan" id="kobo.40.1">D_b2 = tf.Variable(tf.zeros(shape=[1]))</span><br/><br/><span class="koboSpan" id="kobo.41.1">theta_D = [D_W1, D_W2, D_b1, D_b2]</span><br/><br/><span class="koboSpan" id="kobo.42.1">#Generator</span><br/><span class="koboSpan" id="kobo.43.1">G_W1 = tf.Variable(initializer([Z_dim, h_size]))</span><br/><span class="koboSpan" id="kobo.44.1">G_b1 = tf.Variable(tf.zeros(shape=[h_size]))</span><br/><br/><span class="koboSpan" id="kobo.45.1">G_W2 = tf.Variable(initializer([h_size, im_size*im_size]))</span><br/><span class="koboSpan" id="kobo.46.1">G_b2 = tf.Variable(tf.zeros(shape=[im_size*im_size]))</span><br/><br/><span class="koboSpan" id="kobo.47.1">theta_G = [G_W1, G_W2, G_b1, G_b2]</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.48.1">Now that we have the placeholders and weights in place, we define functions for generating random noise from </span><kbd><span class="koboSpan" id="kobo.49.1">Z</span></kbd><span class="koboSpan" id="kobo.50.1">. </span><span class="koboSpan" id="kobo.50.2">Here, we're using a uniform distribution to generate noise; people have also experimented with using Gaussian noise—to do so, you just change the random function from </span><kbd><span class="koboSpan" id="kobo.51.1">uniform</span></kbd><span class="koboSpan" id="kobo.52.1"> to </span><kbd><span class="koboSpan" id="kobo.53.1">normal</span></kbd><span class="koboSpan" id="kobo.54.1">:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.55.1">def sample_Z(m, n):</span><br/><span class="koboSpan" id="kobo.56.1">    return np.random.uniform(-1., 1., size=[m, n])</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.57.1">We construct the discriminator and generator networks:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.58.1">def generator(z):</span><br/><span class="koboSpan" id="kobo.59.1">    """ Two layer Generator Network Z=&gt;128=&gt;784 """</span><br/><span class="koboSpan" id="kobo.60.1">    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)</span><br/><span class="koboSpan" id="kobo.61.1">    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2</span><br/><span class="koboSpan" id="kobo.62.1">    G_prob = tf.nn.sigmoid(G_log_prob)</span><br/><span class="koboSpan" id="kobo.63.1">    return G_prob</span><br/><br/><br/><span class="koboSpan" id="kobo.64.1">def discriminator(x):</span><br/><span class="koboSpan" id="kobo.65.1">    """ Two layer Discriminator Network X=&gt;128=&gt;1 """</span><br/><span class="koboSpan" id="kobo.66.1">    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)</span><br/><span class="koboSpan" id="kobo.67.1">    D_logit = tf.matmul(D_h1, D_W2) + D_b2</span><br/><span class="koboSpan" id="kobo.68.1">    D_prob = tf.nn.sigmoid(D_logit)</span><br/><span class="koboSpan" id="kobo.69.1">    return D_prob, D_logit</span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.70.1">We'll also need a helper function to plot the handwritten digits generated. </span><span class="koboSpan" id="kobo.70.2">The following function plots 25 samples generated in a grid of 5×5:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.71.1">def plot(samples):</span><br/><span class="koboSpan" id="kobo.72.1">    """function to plot generated samples"""</span><br/><span class="koboSpan" id="kobo.73.1">    fig = plt.figure(figsize=(10, 10))</span><br/><span class="koboSpan" id="kobo.74.1">    gs = gridspec.GridSpec(5, 5)</span><br/><span class="koboSpan" id="kobo.75.1">    gs.update(wspace=0.05, hspace=0.05)</span><br/><span class="koboSpan" id="kobo.76.1">    for i, sample in enumerate(samples):</span><br/><span class="koboSpan" id="kobo.77.1">        ax = plt.subplot(gs[i])</span><br/><span class="koboSpan" id="kobo.78.1">        plt.axis('off')</span><br/><span class="koboSpan" id="kobo.79.1">        ax.set_xticklabels([])</span><br/><span class="koboSpan" id="kobo.80.1">        ax.set_yticklabels([])</span><br/><span class="koboSpan" id="kobo.81.1">        ax.set_aspect('equal')</span><br/><span class="koboSpan" id="kobo.82.1">        plt.imshow(sample.reshape(28, 28), cmap='gray')</span><br/><span class="koboSpan" id="kobo.83.1">    return fig</span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.84.1">Now, we define the TensorFlow operations to generate a sample from the generator and a prediction from the discriminator for both fake and real input data:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.85.1">G_sample = generator(Z)</span><br/><span class="koboSpan" id="kobo.86.1">D_real, D_logit_real = discriminator(X)</span><br/><span class="koboSpan" id="kobo.87.1">D_fake, D_logit_fake = discriminator(G_sample)</span></pre>
<ol start="8">
<li><span class="koboSpan" id="kobo.88.1">Next, we define cross-entropy losses for the generator and discriminator network, and alternatively, minimize them, keeping the other weight parameters frozen:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.89.1">D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))</span><br/><span class="koboSpan" id="kobo.90.1">D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))</span><br/><span class="koboSpan" id="kobo.91.1">D_loss = D_loss_real + D_loss_fake</span><br/><span class="koboSpan" id="kobo.92.1">G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))</span><br/><br/><span class="koboSpan" id="kobo.93.1">D_solver = tf.train.AdamOptimizer(learning_rate=learning_rate_D).minimize(D_loss, var_list=theta_D)</span><br/><span class="koboSpan" id="kobo.94.1">G_solver = tf.train.AdamOptimizer(learning_rate=learning_rate_G).minimize(G_loss, var_list=theta_G)</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="9">
<li><span class="koboSpan" id="kobo.95.1">Finally, let's perform the training within a TensorFlow session:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.96.1">sess = tf.Session()</span><br/><span class="koboSpan" id="kobo.97.1">sess.run(tf.global_variables_initializer())</span><br/><span class="koboSpan" id="kobo.98.1">GLoss = []</span><br/><span class="koboSpan" id="kobo.99.1">DLoss = []</span><br/><span class="koboSpan" id="kobo.100.1">if not os.path.exists('out/'):</span><br/><span class="koboSpan" id="kobo.101.1">    os.makedirs('out/')</span><br/><br/><span class="koboSpan" id="kobo.102.1">for it in range(100000):</span><br/><span class="koboSpan" id="kobo.103.1">    if it % 100 == 0:</span><br/><span class="koboSpan" id="kobo.104.1">        samples = sess.run(G_sample, feed_dict={Z: sample_Z(25, Z_dim)})</span><br/><span class="koboSpan" id="kobo.105.1">        fig = plot(samples)</span><br/><span class="koboSpan" id="kobo.106.1">        plt.savefig('out/{}.png'.format(str(it).zfill(3)), bbox_inches='tight')</span><br/><span class="koboSpan" id="kobo.107.1">        plt.close(fig)</span><br/><span class="koboSpan" id="kobo.108.1">    X_mb, _ = data.train.next_batch(batch_size)</span><br/><span class="koboSpan" id="kobo.109.1">    _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(batch_size, Z_dim)})</span><br/><span class="koboSpan" id="kobo.110.1">    _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(batch_size, Z_dim)})</span><br/><span class="koboSpan" id="kobo.111.1">    GLoss.append(G_loss_curr)</span><br/><span class="koboSpan" id="kobo.112.1">    DLoss.append(D_loss_curr)</span><br/><span class="koboSpan" id="kobo.113.1">    if it % 100 == 0:</span><br/><span class="koboSpan" id="kobo.114.1">        print('Iter: {} D loss: {:.4} G_loss: {:.4}'.format(it,D_loss_curr, G_loss_curr))</span><br/> <br/><span class="koboSpan" id="kobo.115.1">print('Done')</span></pre>
<ol start="10">
<li><span class="koboSpan" id="kobo.116.1">In the following screenshot, you can see how the loss for both the generative and discriminatives network varies:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.117.1"><img class="aligncenter size-full wp-image-978 image-border" src="assets/46606566-7484-44ab-88c4-36848eeae909.png" style="width:28.08em;height:19.33em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="koboSpan" id="kobo.118.1">Loss for both generative and discriminatives network</span></div>
<ol start="11">
<li><span class="koboSpan" id="kobo.119.1">Let's also see the handwritten digits generated at different epochs:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.120.1"><img class="aligncenter size-full wp-image-979 image-border" src="assets/877d7611-dfc9-411e-87e9-9b024bde1493.png" style="width:40.92em;height:24.08em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="koboSpan" id="kobo.121.1">Handwritten digits</span></div>
<p class="mce-root"/>
<p><span><span class="koboSpan" id="kobo.122.1">While the handwritten digits are good enough, we can see that a lot of improvements can be made. </span><span class="koboSpan" id="kobo.122.2">Some approaches used by researchers to stabilize the performance are as follows:</span></span><br/></p>
<ul>
<li><span class="koboSpan" id="kobo.123.1">Normalize the input images from (0,1) to (-1,1). </span><span class="koboSpan" id="kobo.123.2">And, instead of the sigmoid as the activation function for the final output of the generator, use the tangent hyperbolic activation function.</span></li>
<li><span class="koboSpan" id="kobo.124.1">Instead of minimizing the generator loss minimum </span><kbd><span class="koboSpan" id="kobo.125.1">log 1-D</span></kbd><span class="koboSpan" id="kobo.126.1">, we can maximize the loss maximum </span><kbd><span class="koboSpan" id="kobo.127.1">log D</span></kbd><span class="koboSpan" id="kobo.128.1">; this can be achieved in TensorFlow by simply flipping the labels while training the generator, for example (convert real into fake and fake into real).</span></li>
<li><span class="koboSpan" id="kobo.129.1">Another approach is to store previously generated images and train the discriminator by choosing randomly from them. </span><span class="koboSpan" id="kobo.129.2">(Yes, you guessed right—it's similar to the experience replay buffer we learned in </span><a href="01e534ff-b0a2-4b5e-bc9a-fd65c527ac7d.xhtml"><span class="koboSpan" id="kobo.130.1">Chapter 6</span></a><span class="koboSpan" id="kobo.131.1">, </span><em><span class="koboSpan" id="kobo.132.1">Reinforcement Learning for IoT</span></em><span class="koboSpan" id="kobo.133.1">.)</span></li>
<li><span class="koboSpan" id="kobo.134.1">People have also experimented with updating the generator or discriminator only if their loss is above a certain threshold.</span></li>
<li><span class="koboSpan" id="kobo.135.1">Instead of the ReLU activation function for the hidden layers of the discriminator and generator, use Leaky ReLU. </span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Deep Convolutional GANs </span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In 2016, Alec Radford </span><em><span class="koboSpan" id="kobo.3.1">et al.</span></em><span class="koboSpan" id="kobo.4.1"> proposed a variation of the GAN called the </span><strong><span class="koboSpan" id="kobo.5.1">Deep Convolutional GAN</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong><span class="koboSpan" id="kobo.7.1">DCGAN</span></strong><span class="koboSpan" id="kobo.8.1">). </span><span class="koboSpan" id="kobo.8.2">(The link to the full paper is: </span><a href="https://arxiv.org/abs/1511.06434"><span class="koboSpan" id="kobo.9.1">https://arxiv.org/abs/1511.06434</span></a><span class="koboSpan" id="kobo.10.1">.) They replaced the MLP layers with convolutional layers. </span><span class="koboSpan" id="kobo.10.2">They also added batch normalization in both the generator and discriminator networks. </span><span class="koboSpan" id="kobo.10.3">We'll implement DCGAN here on a celebrity images dataset. </span><span class="koboSpan" id="kobo.10.4">You can download the ZIP file, </span><kbd><span><span class="koboSpan" id="kobo.11.1">img_align_celeba.zip</span></span></kbd><span class="koboSpan" id="kobo.12.1">, from </span><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html"><span class="koboSpan" id="kobo.13.1">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</span></a><span class="koboSpan" id="kobo.14.1">. </span><span class="koboSpan" id="kobo.14.2">We make use of the </span><kbd><span class="koboSpan" id="kobo.15.1">loader_celebA.py</span></kbd><span class="koboSpan" id="kobo.16.1"> file we made in </span><a href="4351f888-1bf0-4945-a8a6-ddd71bd464dd.xhtml"><span class="koboSpan" id="kobo.17.1">Chapter 2</span></a><span class="koboSpan" id="kobo.18.1">, </span><em><span class="koboSpan" id="kobo.19.1">Data Access and Distributed Processing for IoT,</span></em><span class="koboSpan" id="kobo.20.1"> to unzip and read the images:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.21.1">We'll import statements for all of the modules we'll be requiring:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.22.1">import loader</span><br/><span class="koboSpan" id="kobo.23.1">import os</span><br/><span class="koboSpan" id="kobo.24.1">from glob import glob</span><br/><span class="koboSpan" id="kobo.25.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.26.1">from matplotlib import pyplot</span><br/><span class="koboSpan" id="kobo.27.1">import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.28.1">%matplotlib inline</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.29.1">We use </span><kbd><span class="koboSpan" id="kobo.30.1">loader_celebA.py</span></kbd><span class="koboSpan" id="kobo.31.1"> to unzip </span><kbd><span class="koboSpan" id="kobo.32.1">img_align_celeba.zip</span></kbd><span class="koboSpan" id="kobo.33.1">. </span><span class="koboSpan" id="kobo.33.2">Since the number of images is very high, we use the </span><kbd><span class="koboSpan" id="kobo.34.1">get_batches</span></kbd><span class="koboSpan" id="kobo.35.1"> function defined in this file to generate batches for training the network:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.36.1">loader.download_celeb_a()</span><br/><br/><span class="koboSpan" id="kobo.37.1"># Let's explore the images</span><br/><span class="koboSpan" id="kobo.38.1">data_dir = os.getcwd()</span><br/><span class="koboSpan" id="kobo.39.1">test_images = loader.get_batch(glob(os.path.join(data_dir, 'celebA/*.jpg'))[:10], 56, 56)</span><br/><span class="koboSpan" id="kobo.40.1">pyplot.imshow(loader.plot_images(test_images))</span></pre>
<p style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.41.1">In the following, you can see the dataset images:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.42.1"><img class="aligncenter size-full wp-image-980 image-border" src="assets/39ae09d7-88db-49d3-8484-11960687886b.png" style="width:19.50em;height:18.83em;"/></span></p>
<ol start="3">
<li><span class="koboSpan" id="kobo.43.1">We define the discriminator network. </span><span class="koboSpan" id="kobo.43.2">It consists of three convolutional layers with </span><kbd><span class="koboSpan" id="kobo.44.1">64</span></kbd><span class="koboSpan" id="kobo.45.1">, </span><kbd><span class="koboSpan" id="kobo.46.1">128</span></kbd><span class="koboSpan" id="kobo.47.1">, and </span><kbd><span class="koboSpan" id="kobo.48.1">256</span></kbd><span class="koboSpan" id="kobo.49.1"> filters respectively, each of size 5×5. </span><span class="koboSpan" id="kobo.49.2">The first two layers use a stride of </span><kbd><span class="koboSpan" id="kobo.50.1">2</span></kbd><span class="koboSpan" id="kobo.51.1"> and the third convolutional layer uses a stride of </span><kbd><span class="koboSpan" id="kobo.52.1">1</span></kbd><span class="koboSpan" id="kobo.53.1">. </span><span class="koboSpan" id="kobo.53.2">All three convolutional layers use </span><kbd><span class="koboSpan" id="kobo.54.1">leakyReLU</span></kbd><span class="koboSpan" id="kobo.55.1"> as the activation function. Each convolutional layer is also followed by a batch normalization layer. </span><span class="koboSpan" id="kobo.55.2">The result of the third convolutional layer is flattened and passed to the last fully connected (dense) layer with the sigmoid activation function:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.56.1">def discriminator(images, reuse=False):</span><br/><span class="koboSpan" id="kobo.57.1">    """</span><br/><span class="koboSpan" id="kobo.58.1">    Create the discriminator network</span><br/><span class="koboSpan" id="kobo.59.1">    """</span><br/><span class="koboSpan" id="kobo.60.1">    alpha = 0.2</span><br/> <br/><span class="koboSpan" id="kobo.61.1">    with tf.variable_scope('discriminator', reuse=reuse):</span><br/><span class="koboSpan" id="kobo.62.1">        # using 4 layer network as in DCGAN Paper</span><br/> <br/><span class="koboSpan" id="kobo.63.1">        # First convolution layer</span><br/><span class="koboSpan" id="kobo.64.1">        conv1 = tf.layers.conv2d(images, 64, 5, 2, 'SAME')</span><br/><span class="koboSpan" id="kobo.65.1">        lrelu1 = tf.maximum(alpha * conv1, conv1)</span><br/> <br/><span class="koboSpan" id="kobo.66.1">        # Second convolution layer</span><br/><span class="koboSpan" id="kobo.67.1">        conv2 = tf.layers.conv2d(lrelu1, 128, 5, 2, 'SAME')</span><br/><span class="koboSpan" id="kobo.68.1">        batch_norm2 = tf.layers.batch_normalization(conv2, training=True)</span><br/><span class="koboSpan" id="kobo.69.1">        lrelu2 = tf.maximum(alpha * batch_norm2, batch_norm2)</span><br/>        <br/><span class="koboSpan" id="kobo.70.1">        # Third convolution layer</span><br/><span class="koboSpan" id="kobo.71.1">        conv3 = tf.layers.conv2d(lrelu2, 256, 5, 1, 'SAME')</span><br/><span class="koboSpan" id="kobo.72.1">        batch_norm3 = tf.layers.batch_normalization(conv3, training=True)</span><br/><span class="koboSpan" id="kobo.73.1">        lrelu3 = tf.maximum(alpha * batch_norm3, batch_norm3)</span><br/>       <br/><span class="koboSpan" id="kobo.74.1">        # Flatten layer</span><br/><span class="koboSpan" id="kobo.75.1">        flat = tf.reshape(lrelu3, (-1, 4*4*256))</span><br/>        <br/><span class="koboSpan" id="kobo.76.1">        # Logits</span><br/><span class="koboSpan" id="kobo.77.1">        logits = tf.layers.dense(flat, 1)</span><br/>        <br/><span class="koboSpan" id="kobo.78.1">        # Output</span><br/><span class="koboSpan" id="kobo.79.1">        out = tf.sigmoid(logits)</span><br/>        <br/><span class="koboSpan" id="kobo.80.1">        return out, logits</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.81.1">The generator network is the reverse of the discriminator; the input to the generator is first fed to a dense layer with 2×2×512 units. </span><span class="koboSpan" id="kobo.81.2">The output of the dense layer is reshaped so that we can feed it to the convolution stack. </span><span class="koboSpan" id="kobo.81.3">We use the </span><kbd><span class="koboSpan" id="kobo.82.1">tf.layers.conv2d_transpose()</span></kbd><span class="koboSpan" id="kobo.83.1"> method to get the transposed convolution output. </span><span class="koboSpan" id="kobo.83.2">The generator has three transposed convolutional layers. </span><span class="koboSpan" id="kobo.83.3">All of the layers except the last convolutional layer have </span><kbd><span class="koboSpan" id="kobo.84.1">leakyReLU</span></kbd><span class="koboSpan" id="kobo.85.1"> as the activation function. </span><span class="koboSpan" id="kobo.85.2">The last transposed convolution layer uses the tangent hyperbolic activation function so that output lies in the range (</span><kbd><span class="koboSpan" id="kobo.86.1">-1</span></kbd><span class="koboSpan" id="kobo.87.1"> to </span><kbd><span class="koboSpan" id="kobo.88.1">1</span></kbd><span class="koboSpan" id="kobo.89.1">):</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.90.1">def generator(z, out_channel_dim, is_train=True):</span><br/><span class="koboSpan" id="kobo.91.1">    """</span><br/><span class="koboSpan" id="kobo.92.1">    Create the generator network</span><br/><span class="koboSpan" id="kobo.93.1">    """</span><br/><span class="koboSpan" id="kobo.94.1">    alpha = 0.2</span><br/>    <br/><span class="koboSpan" id="kobo.95.1">    with tf.variable_scope('generator', reuse=False if is_train==True else True):</span><br/><span class="koboSpan" id="kobo.96.1">        # First fully connected layer</span><br/><span class="koboSpan" id="kobo.97.1">        x_1 = tf.layers.dense(z, 2*2*512)</span><br/>        <br/><span class="koboSpan" id="kobo.98.1">        # Reshape it to start the convolutional stack</span><br/><span class="koboSpan" id="kobo.99.1">        deconv_2 = tf.reshape(x_1, (-1, 2, 2, 512))</span><br/><span class="koboSpan" id="kobo.100.1">        batch_norm2 = tf.layers.batch_normalization(deconv_2, training=is_train)</span><br/><span class="koboSpan" id="kobo.101.1">        lrelu2 = tf.maximum(alpha * batch_norm2, batch_norm2)</span><br/>        <br/><span class="koboSpan" id="kobo.102.1">        # Deconv 1</span><br/><span class="koboSpan" id="kobo.103.1">        deconv3 = tf.layers.conv2d_transpose(lrelu2, 256, 5, 2, padding='VALID')</span><br/><span class="koboSpan" id="kobo.104.1">        batch_norm3 = tf.layers.batch_normalization(deconv3, training=is_train)</span><br/><span class="koboSpan" id="kobo.105.1">        lrelu3 = tf.maximum(alpha * batch_norm3, batch_norm3)</span><br/>        <br/>        <br/><span class="koboSpan" id="kobo.106.1">        # Deconv 2</span><br/><span class="koboSpan" id="kobo.107.1">        deconv4 = tf.layers.conv2d_transpose(lrelu3, 128, 5, 2, padding='SAME')</span><br/><span class="koboSpan" id="kobo.108.1">        batch_norm4 = tf.layers.batch_normalization(deconv4, training=is_train)</span><br/><span class="koboSpan" id="kobo.109.1">        lrelu4 = tf.maximum(alpha * batch_norm4, batch_norm4)</span><br/>        <br/><span class="koboSpan" id="kobo.110.1">        # Output layer</span><br/><span class="koboSpan" id="kobo.111.1">        logits = tf.layers.conv2d_transpose(lrelu4, out_channel_dim, 5, 2, padding='SAME')</span><br/>        <br/><span class="koboSpan" id="kobo.112.1">        out = tf.tanh(logits)</span><br/>        <br/><span class="koboSpan" id="kobo.113.1">        return out</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.114.1">We define functions to calculate the model loss; it defines both the generator and discriminator loss and returns them:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.115.1">def model_loss(input_real, input_z, out_channel_dim):</span><br/><span class="koboSpan" id="kobo.116.1">    """</span><br/><span class="koboSpan" id="kobo.117.1">    Get the loss for the discriminator and generator</span><br/><span class="koboSpan" id="kobo.118.1">    """</span><br/>    <br/><span class="koboSpan" id="kobo.119.1">    label_smoothing = 0.9</span><br/>    <br/><span class="koboSpan" id="kobo.120.1">    g_model = generator(input_z, out_channel_dim)</span><br/><span class="koboSpan" id="kobo.121.1">    d_model_real, d_logits_real = discriminator(input_real)</span><br/><span class="koboSpan" id="kobo.122.1">    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)</span><br/>    <br/><span class="koboSpan" id="kobo.123.1">    d_loss_real = tf.reduce_mean(</span><br/><span class="koboSpan" id="kobo.124.1">        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,</span><br/><span class="koboSpan" id="kobo.125.1">                                                labels=tf.ones_like(d_model_real) * label_smoothing))</span><br/><span class="koboSpan" id="kobo.126.1">    d_loss_fake = tf.reduce_mean(</span><br/><span class="koboSpan" id="kobo.127.1">        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,</span><br/><span class="koboSpan" id="kobo.128.1">                                                labels=tf.zeros_like(d_model_fake)))</span><br/>    <br/><span class="koboSpan" id="kobo.129.1">    d_loss = d_loss_real + d_loss_fake</span><br/>                                                  <br/><span class="koboSpan" id="kobo.130.1">    g_loss = tf.reduce_mean(</span><br/><span class="koboSpan" id="kobo.131.1">        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,</span><br/><span class="koboSpan" id="kobo.132.1">                                                labels=tf.ones_like(d_model_fake) * label_smoothing))</span><br/>    <br/>    <br/><span class="koboSpan" id="kobo.133.1">    return d_loss, g_loss</span></pre>
<p class="mce-root"/>
<ol start="6">
<li><span class="koboSpan" id="kobo.134.1">We next need to define optimizers to make the discriminator and generator learn sequentially. </span><span class="koboSpan" id="kobo.134.2">To achieve this, we make use of </span><kbd><span class="koboSpan" id="kobo.135.1">tf.trainable_variables()</span></kbd><span class="koboSpan" id="kobo.136.1"> to get a list of all training variables, and then first optimize only the discriminator training variables, and then the generator training variables:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.137.1">def model_opt(d_loss, g_loss, learning_rate, beta1):</span><br/><span class="koboSpan" id="kobo.138.1">    """</span><br/><span class="koboSpan" id="kobo.139.1">    Get optimization operations</span><br/><span class="koboSpan" id="kobo.140.1">    """</span><br/><span class="koboSpan" id="kobo.141.1">    t_vars = tf.trainable_variables()</span><br/><span class="koboSpan" id="kobo.142.1">    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]</span><br/><span class="koboSpan" id="kobo.143.1">    g_vars = [var for var in t_vars if var.name.startswith('generator')]</span><br/><br/><span class="koboSpan" id="kobo.144.1">    # Optimize</span><br/><span class="koboSpan" id="kobo.145.1">    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): </span><br/><span class="koboSpan" id="kobo.146.1">        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)</span><br/><span class="koboSpan" id="kobo.147.1">        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)</span><br/><br/><span class="koboSpan" id="kobo.148.1">    return d_train_opt, g_train_opt</span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.149.1">Now, we have all of the necessary ingredients to train the DCGAN. </span><span class="koboSpan" id="kobo.149.2">It's always good to keep an eye how the generator has learned, so we define a helper function to display the images generated by the generator network as it learns:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.150.1">def generator_output(sess, n_images, input_z, out_channel_dim):</span><br/><span class="koboSpan" id="kobo.151.1">    """</span><br/><span class="koboSpan" id="kobo.152.1">    Show example output for the generator</span><br/><span class="koboSpan" id="kobo.153.1">    """</span><br/><span class="koboSpan" id="kobo.154.1">    z_dim = input_z.get_shape().as_list()[-1]</span><br/><span class="koboSpan" id="kobo.155.1">    example_z = np.random.uniform(-1, 1, size=[n_images, z_dim])</span><br/><br/><span class="koboSpan" id="kobo.156.1">    samples = sess.run(</span><br/><span class="koboSpan" id="kobo.157.1">        generator(input_z, out_channel_dim, False),</span><br/><span class="koboSpan" id="kobo.158.1">        feed_dict={input_z: example_z})</span><br/><br/><span class="koboSpan" id="kobo.159.1">    pyplot.imshow(loader.plot_images(samples))</span><br/><span class="koboSpan" id="kobo.160.1">    pyplot.show()</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="8">
<li><span class="koboSpan" id="kobo.161.1">Finally, comes the training part. </span><span class="koboSpan" id="kobo.161.2">Here, we use the </span><kbd><span class="koboSpan" id="kobo.162.1">ops</span></kbd><span class="koboSpan" id="kobo.163.1"> defined previously to train the DCGAN, and the images are fed to the network in batches:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.164.1">def train(epoch_count, batch_size, z_dim, learning_rate, beta1, get_batches, data_shape, data_files):</span><br/><span class="koboSpan" id="kobo.165.1">    """</span><br/><span class="koboSpan" id="kobo.166.1">    Train the GAN</span><br/><span class="koboSpan" id="kobo.167.1">    """</span><br/><span class="koboSpan" id="kobo.168.1">    w, h, num_ch = data_shape[1], data_shape[2], data_shape[3]</span><br/><span class="koboSpan" id="kobo.169.1">    X = tf.placeholder(tf.float32, shape=(None, w, h, num_ch), name='input_real') </span><br/><span class="koboSpan" id="kobo.170.1">    Z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')</span><br/><span class="koboSpan" id="kobo.171.1">    #model_inputs(data_shape[1], data_shape[2], data_shape[3], z_dim)</span><br/><span class="koboSpan" id="kobo.172.1">    D_loss, G_loss = model_loss(X, Z, data_shape[3])</span><br/><span class="koboSpan" id="kobo.173.1">    D_solve, G_solve = model_opt(D_loss, G_loss, learning_rate, beta1)</span><br/>    <br/>    <br/>    <br/><span class="koboSpan" id="kobo.174.1">    with tf.Session() as sess:</span><br/><span class="koboSpan" id="kobo.175.1">        sess.run(tf.global_variables_initializer())</span><br/><span class="koboSpan" id="kobo.176.1">        train_loss_d = []</span><br/><span class="koboSpan" id="kobo.177.1">        train_loss_g = []</span><br/><span class="koboSpan" id="kobo.178.1">        for epoch_i in range(epoch_count):</span><br/><span class="koboSpan" id="kobo.179.1">            num_batch = 0</span><br/><span class="koboSpan" id="kobo.180.1">            lossD, lossG = 0,0</span><br/><span class="koboSpan" id="kobo.181.1">            for batch_images in get_batches(batch_size, data_shape, data_files):</span><br/>                <br/><span class="koboSpan" id="kobo.182.1">                # values range from -0.5 to 0.5 so we scale to range -1, 1</span><br/><span class="koboSpan" id="kobo.183.1">                batch_images = batch_images * 2</span><br/><span class="koboSpan" id="kobo.184.1">                num_batch += 1</span><br/>            <br/><span class="koboSpan" id="kobo.185.1">                batch_z = np.random.uniform(-1, 1, size=(batch_size, z_dim))</span><br/>                <br/><span class="koboSpan" id="kobo.186.1">                _,d_loss = sess.run([D_solve,D_loss], feed_dict={X: batch_images, Z: batch_z})</span><br/><span class="koboSpan" id="kobo.187.1">                _,g_loss = sess.run([G_solve,G_loss], feed_dict={X: batch_images, Z: batch_z})</span><br/>                <br/><span class="koboSpan" id="kobo.188.1">                lossD += (d_loss/batch_size)</span><br/><span class="koboSpan" id="kobo.189.1">                lossG += (g_loss/batch_size)</span><br/><span class="koboSpan" id="kobo.190.1">                if num_batch % 500 == 0:</span><br/><span class="koboSpan" id="kobo.191.1">                    # After every 500 batches</span><br/><span class="koboSpan" id="kobo.192.1">                    print("Epoch {}/{} For Batch {} Discriminator Loss: {:.4f} Generator Loss: {:.4f}".</span><br/><span class="koboSpan" id="kobo.193.1">                          format(epoch_i+1, epochs, num_batch, lossD/num_batch, lossG/num_batch))</span><br/>                          <br/>                    <br/><span class="koboSpan" id="kobo.194.1">                    generator_output(sess, 9, Z, data_shape[3])</span><br/><span class="koboSpan" id="kobo.195.1">            train_loss_d.append(lossD/num_batch)</span><br/><span class="koboSpan" id="kobo.196.1">            train_loss_g.append(lossG/num_batch)</span><br/>    <br/><span class="koboSpan" id="kobo.197.1">    return train_loss_d, train_loss_g</span></pre>
<p class="mce-root"/>
<ol start="9">
<li><span class="koboSpan" id="kobo.198.1">Let's now define the parameters of our data and train it:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.199.1"># Data Parameters</span><br/><span class="koboSpan" id="kobo.200.1">IMAGE_HEIGHT = 28</span><br/><span class="koboSpan" id="kobo.201.1">IMAGE_WIDTH = 28</span><br/><span class="koboSpan" id="kobo.202.1">data_files = glob(os.path.join(data_dir, 'celebA/*.jpg'))</span><br/><br/><span class="koboSpan" id="kobo.203.1">#Hyper parameters</span><br/><span class="koboSpan" id="kobo.204.1">batch_size = 16</span><br/><span class="koboSpan" id="kobo.205.1">z_dim = 100</span><br/><span class="koboSpan" id="kobo.206.1">learning_rate = 0.0002</span><br/><span class="koboSpan" id="kobo.207.1">beta1 = 0.5</span><br/><span class="koboSpan" id="kobo.208.1">epochs = 2</span><br/><span class="koboSpan" id="kobo.209.1">shape = len(data_files), IMAGE_WIDTH, IMAGE_HEIGHT, 3</span><br/><span class="koboSpan" id="kobo.210.1">with tf.Graph().as_default():</span><br/><span class="koboSpan" id="kobo.211.1">    Loss_D, Loss_G = train(epochs, batch_size, z_dim, learning_rate, beta1, loader.get_batches, shape, data_files)</span></pre>
<p><span class="koboSpan" id="kobo.212.1">After each batch, you can see that the generator output is improving: </span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.213.1"><img class="aligncenter size-full wp-image-981 image-border" src="assets/4e8528a7-4430-4938-bc2c-93cddef80907.png" style="width:135.08em;height:44.00em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.214.1"> DCGAN generator output as learning progresses</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Variants of GAN and its cool applications</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the last few years, a large number of variants of GANs have been proposed. </span><span class="koboSpan" id="kobo.2.2">You can access the complete list of different variants of GAN from the GAN Zoo GitHub: </span><a href="https://github.com/hindupuravinash/the-gan-zoo"><span class="koboSpan" id="kobo.3.1">https://github.com/hindupuravinash/the-gan-zoo</span></a><span class="koboSpan" id="kobo.4.1">. </span><span class="koboSpan" id="kobo.4.2">In this section, we'll list some of the more popular and successful variants.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Cycle GAN</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">At the beginning of the 2018, the </span></span><span class="koboSpan" id="kobo.3.1">Berkeley AI research lab published a paper entitled </span><em><span><span class="koboSpan" id="kobo.4.1">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</span></span><span class="koboSpan" id="kobo.5.1"> </span></em><span class="koboSpan" id="kobo.6.1">(arXiv link: </span><a href="https://arxiv.org/pdf/1703.10593.pdf"><span class="koboSpan" id="kobo.7.1">https://arxiv.org/pdf/1703.10593.pdf</span></a><span class="koboSpan" id="kobo.8.1">). </span><span class="koboSpan" id="kobo.8.2">This paper is special not only because it proposed a new architecture, CycleGAN, with improved stability, but also because they demonstrated that such an architecture can be used for complex image transformations. </span><span class="koboSpan" id="kobo.8.3">The following diagram shows the architecture of a cycle GAN; t</span><span><span class="koboSpan" id="kobo.9.1">he two sections highlight the</span></span> <strong><span class="koboSpan" id="kobo.10.1">Generator</span></strong> <span><span class="koboSpan" id="kobo.11.1">and</span></span> <strong><span class="koboSpan" id="kobo.12.1">Discriminators</span></strong> <span><span class="koboSpan" id="kobo.13.1">playing a role in calculating the two adversarial losses</span></span><span class="koboSpan" id="kobo.14.1">:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.15.1"><img class="aligncenter size-full wp-image-1101 image-border" src="assets/d5036aa6-77cf-41c1-8828-11436977198e.png" style="width:39.50em;height:28.50em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.16.1"> The architecture of CycleGAN</span></div>
<p><span class="koboSpan" id="kobo.17.1">The CycleGAN consists of two GANs. </span><span class="koboSpan" id="kobo.17.2">They are trained on two different datasets, </span><em><span class="koboSpan" id="kobo.18.1">x</span></em><span class="koboSpan" id="kobo.19.1">∼</span><em><span class="koboSpan" id="kobo.20.1">P</span></em><sub><span class="koboSpan" id="kobo.21.1">data</span></sub><span class="koboSpan" id="kobo.22.1">(</span><em><span class="koboSpan" id="kobo.23.1">x</span></em><span class="koboSpan" id="kobo.24.1">) and </span><em><span class="koboSpan" id="kobo.25.1">y</span></em><span class="koboSpan" id="kobo.26.1">∼</span><em><span class="koboSpan" id="kobo.27.1">P</span></em><sub><span class="koboSpan" id="kobo.28.1">data</span></sub><span class="koboSpan" id="kobo.29.1">(y). </span><span class="koboSpan" id="kobo.29.2">The generator is trained to perform the mappings, namely, </span><em><span class="koboSpan" id="kobo.30.1">G</span><sub><span class="koboSpan" id="kobo.31.1">A</span></sub><span class="koboSpan" id="kobo.32.1">: x→y</span></em><span class="koboSpan" id="kobo.33.1"> and </span><em><span class="koboSpan" id="kobo.34.1">G</span><sub><span class="koboSpan" id="kobo.35.1">B</span></sub><span class="koboSpan" id="kobo.36.1">: y→x</span></em><span class="koboSpan" id="kobo.37.1"> respectively. </span><span class="koboSpan" id="kobo.37.2">Each discriminator is trained so that it can differentiate between the image </span><em><span class="koboSpan" id="kobo.38.1">x</span></em><span class="koboSpan" id="kobo.39.1"> and transformed image </span><em><span class="koboSpan" id="kobo.40.1">G</span><sub><span class="koboSpan" id="kobo.41.1">B</span></sub><span class="koboSpan" id="kobo.42.1">(y)</span></em><span class="koboSpan" id="kobo.43.1">, hence resulting in the adversary loss functions for the two transformations, defined as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.44.1"><img class="fm-editor-equation" src="assets/6f374cc4-833a-4d25-b3df-960cdac65753.png" style="width:44.92em;height:1.83em;"/></span></p>
<p><span class="koboSpan" id="kobo.45.1">And, the second is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.46.1"><img class="fm-editor-equation" src="assets/f1ddf7ab-4912-476c-a8e0-71f929e061f7.png" style="width:44.83em;height:1.83em;"/></span></p>
<p><span class="koboSpan" id="kobo.47.1">The generators of the two GANs are connected to each other in a cyclic fashion, so that if the output of one is fed to another and the corresponding output fed back to the first one, we get the same data. </span><span class="koboSpan" id="kobo.47.2">Let's make it clearer with an example; let's say the </span><strong><span class="koboSpan" id="kobo.48.1">Generator A</span></strong><span class="koboSpan" id="kobo.49.1"> (</span><strong><span class="koboSpan" id="kobo.50.1">G</span><sub><span class="koboSpan" id="kobo.51.1">A</span></sub></strong><span class="koboSpan" id="kobo.52.1">) is fed an image </span><em><span class="koboSpan" id="kobo.53.1">x</span></em><span class="koboSpan" id="kobo.54.1">, so the output is a transformation G</span><sub><span class="koboSpan" id="kobo.55.1">A</span></sub><span class="koboSpan" id="kobo.56.1">(</span><em><span class="koboSpan" id="kobo.57.1">x</span></em><span class="koboSpan" id="kobo.58.1">). This transformed image now is fed to </span><strong><span class="koboSpan" id="kobo.59.1">Generator B</span></strong><span class="koboSpan" id="kobo.60.1"> (</span><strong><span class="koboSpan" id="kobo.61.1">G</span><sub><span class="koboSpan" id="kobo.62.1">B</span></sub></strong><span class="koboSpan" id="kobo.63.1">) </span><em><span class="koboSpan" id="kobo.64.1">G</span><sub><span class="koboSpan" id="kobo.65.1">B</span></sub><span class="koboSpan" id="kobo.66.1">(G</span><sub><span class="koboSpan" id="kobo.67.1">A</span></sub><span class="koboSpan" id="kobo.68.1">(x))≈x</span></em><span class="koboSpan" id="kobo.69.1"> and the result should be the initial image </span><em><span class="koboSpan" id="kobo.70.1">x</span></em><span class="koboSpan" id="kobo.71.1">. </span><span class="koboSpan" id="kobo.71.2">Similarly, we shall have G</span><sub><span class="koboSpan" id="kobo.72.1">A</span></sub><span class="koboSpan" id="kobo.73.1">(G</span><sub><span class="koboSpan" id="kobo.74.1">B</span></sub><span class="koboSpan" id="kobo.75.1">(</span><em><span class="koboSpan" id="kobo.76.1">y</span></em><span class="koboSpan" id="kobo.77.1">)≈</span><em><span class="koboSpan" id="kobo.78.1">y</span></em><span class="koboSpan" id="kobo.79.1">. </span><span class="koboSpan" id="kobo.79.2">This is made possible by introducing a cyclic loss term:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.80.1"><img class="fm-editor-equation" src="assets/bbc7c74a-a887-4084-82d7-bb139d4ce839.png" style="width:48.75em;height:1.83em;"/></span></p>
<p><span class="koboSpan" id="kobo.81.1">Hence, the net objective function is as follows: </span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.82.1"><img class="fm-editor-equation" src="assets/ca90af46-a700-49ca-beb6-5b27653e1502.png" style="width:36.08em;height:1.67em;"/></span></p>
<p><span class="koboSpan" id="kobo.83.1">Here, </span><em><span class="koboSpan" id="kobo.84.1">λ</span></em><span class="koboSpan" id="kobo.85.1"> controls the relative importance of the two objectives. </span><span class="koboSpan" id="kobo.85.2">They also retained previous images in an experience buffer to train the discriminator. </span><span class="koboSpan" id="kobo.85.3">In the following screenshot, you can see some of the results obtained from the CycleGANs as reported in the paper:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.86.1"><img class="aligncenter size-full wp-image-982 image-border" src="assets/63324eaf-fd29-46d0-a7cc-02b9c1f9e7fb.png" style="width:54.50em;height:51.33em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="koboSpan" id="kobo.87.1">Results of CycleGAN (taken from the original paper) </span></div>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.88.1">The authors showed that CycleGANs can be used for the following:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.89.1">Image transformation</span></strong><span class="koboSpan" id="kobo.90.1">: Such as changing horses to zebra and vice versa</span></li>
<li><strong><span class="koboSpan" id="kobo.91.1">Enhancing the resolution</span></strong><span class="koboSpan" id="kobo.92.1">: The CycleGAN, when trained by a dataset consisting of low-resolution and super-resolution images, could perform super-resolution when given with low-resolution images</span></li>
<li><strong><span class="koboSpan" id="kobo.93.1">Style transfer</span></strong><span class="koboSpan" id="kobo.94.1">: Given an image, it can be transformed into different painting styles </span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Applications of GANs</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">GANs are indeed interesting networks; besides the applications you've seen, GANs have been explored in many other exciting applications. </span><span class="koboSpan" id="kobo.2.2">In the following, we list a few:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">Music generation</span></strong><span class="koboSpan" id="kobo.4.1">: MIDINet, a convolutional GAN, has been demonstrated to generate melodies. </span><span class="koboSpan" id="kobo.4.2">You can refer to the paper here: </span><a href="https://arxiv.org/pdf/1703.10847.pdf"><span class="koboSpan" id="kobo.5.1">https://arxiv.org/pdf/1703.10847.pdf</span></a><span class="koboSpan" id="kobo.6.1">.</span></li>
<li><strong><span class="koboSpan" id="kobo.7.1">Medical anomaly detection</span></strong><span class="koboSpan" id="kobo.8.1">: </span><span><span class="koboSpan" id="kobo.9.1">AnoGAN is</span></span><span><span class="koboSpan" id="kobo.10.1"> a DCGAN shown by Thomas Schlegl et al</span><em><span class="koboSpan" id="kobo.11.1">.</span></em><span class="koboSpan" id="kobo.12.1"> to learn a manifold of normal anatomical variability. </span><span class="koboSpan" id="kobo.12.2">They were able to train the network to label anomalies on optical coherence tomography images of the retina. </span><span class="koboSpan" id="kobo.12.3">If the work interests you, you can see the related paper on arXiv at </span><a href="https://arxiv.org/pdf/1703.05921.pdf"><span class="koboSpan" id="kobo.13.1">https://arxiv.org/pdf/1703.05921.pdf</span></a><span class="koboSpan" id="kobo.14.1">.</span></span></li>
<li><strong><span class="koboSpan" id="kobo.15.1">Vector arithmetic on faces using GANs</span></strong><span class="koboSpan" id="kobo.16.1">: In the joint research paper by Indico Research and Facebook, they demonstrated that it's possible to use GANs and perform image arithmetic. </span><span class="koboSpan" id="kobo.16.2">For example, </span><em><span class="koboSpan" id="kobo.17.1">Man with glasses</span></em><span class="koboSpan" id="kobo.18.1">—</span><em><span class="koboSpan" id="kobo.19.1">Man without glasses</span></em><span class="koboSpan" id="kobo.20.1"> + </span><em><span class="koboSpan" id="kobo.21.1">Woman without glasses</span></em><span class="koboSpan" id="kobo.22.1"> = </span><em><span class="koboSpan" id="kobo.23.1">Woman with glasses</span></em><span class="koboSpan" id="kobo.24.1">. </span><span class="koboSpan" id="kobo.24.2">It's an interesting paper and you can read more about it on Arxiv (</span><a href="https://arxiv.org/pdf/1511.06434.pdf"><span class="koboSpan" id="kobo.25.1">https://arxiv.org/pdf/1511.06434.pdf</span></a><span class="koboSpan" id="kobo.26.1">).</span></li>
<li><strong><span class="koboSpan" id="kobo.27.1">Text to image synthesis</span></strong><span class="koboSpan" id="kobo.28.1">: GANs have been demonstrated to generate images</span><span><span class="koboSpan" id="kobo.29.1"> of birds and flowers from human-written textual descriptions. </span><span class="koboSpan" id="kobo.29.2">The model uses DCGAN along with a hybrid character level convolutional recurrent network. </span><span class="koboSpan" id="kobo.29.3">The details of the work are given in the paper, </span><em><span><span class="koboSpan" id="kobo.30.1">Generative Adversarial Text to Image Synthesis</span></span></em><span class="koboSpan" id="kobo.31.1">. </span><span class="koboSpan" id="kobo.31.2">The link to the paper is </span><a href="https://arxiv.org/pdf/1605.05396.pdf"><span class="koboSpan" id="kobo.32.1">https://arxiv.org/pdf/1605.05396.pdf</span></a><span class="koboSpan" id="kobo.33.1">.</span><a href="https://arxiv.org/pdf/1605.05396.pdf"><span class="koboSpan" id="kobo.34.1"> </span></a></span></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">This was an interesting chapter, and I hope you enjoyed reading it as much as I enjoyed writing it. </span><span class="koboSpan" id="kobo.2.2">It's at present the hot topic of research. </span><span class="koboSpan" id="kobo.2.3">This chapter introduced generative models and their classification, namely implicit generative models and explicit generative models. </span><span class="koboSpan" id="kobo.2.4">The first generative model that was covered is VAEs; they're an explicit generative model and try to estimate the lower bound on the density function. </span><span class="koboSpan" id="kobo.2.5">The VAEs were implemented in TensorFlow and were used to generate handwritten digits.</span></p>
<p><span class="koboSpan" id="kobo.3.1">This chapter then moved on to a more popular explicit generative model: GANs. </span><span class="koboSpan" id="kobo.3.2">The GAN architecture, especially how the discriminator network and generative network compete with each other, was explained. </span><span class="koboSpan" id="kobo.3.3">We implemented a GAN using TensorFlow for generating handwritten digits. </span><span class="koboSpan" id="kobo.3.4">This chapter then moved on to the more successful variation of GAN: the DCGAN. </span><span class="koboSpan" id="kobo.3.5">We implemented a DCGAN to generate celebrity images. </span><span class="koboSpan" id="kobo.3.6">This chapter also covered the architecture details of CycleGAN, a recently proposed GAN, and some of its cool applications. </span></p>
<p><span class="koboSpan" id="kobo.4.1">With this chapter, we mark the end of part one of this book. </span><span class="koboSpan" id="kobo.4.2">Till now, we concentrated on different ML and DL models, which we'll require to understand our data and use it for prediction/classification, and other tasks. </span><span class="koboSpan" id="kobo.4.3">From the next chapter onward, we'll be talking more about the data itself and how we can process the data in the present IoT-driven environment.</span></p>
<p><span class="koboSpan" id="kobo.5.1">In the next chapter, we'll move toward distributed processing, a necessity when dealing with a large amount of data, and explore two platforms that offer distributed processing.</span></p>


            </article>

            
        </section>
    </body></html>