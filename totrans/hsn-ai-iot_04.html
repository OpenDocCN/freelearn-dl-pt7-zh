<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Deep Learning for IoT</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">In the last chapter, we learned about different </span><strong><span class="koboSpan" id="kobo.3.1">machine learning</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">ML</span></strong><span class="koboSpan" id="kobo.6.1">) algorithms. </span><span class="koboSpan" id="kobo.6.2">The focus of this chapter is neural networks based on multiple layered models, also known as deep learning models. </span><span class="koboSpan" id="kobo.6.3">They have become a buzzword in the last few years and an absolute favorite of investors in the field of artificial-intelligence-based startups. </span><span class="koboSpan" id="kobo.6.4">Achieving above human level accuracy in the task of object detection and defeating the world's Dan Nine Go master are some of the feats possible by </span><strong><span class="koboSpan" id="kobo.7.1">deep</span></strong> <strong><span class="koboSpan" id="kobo.8.1">learning</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong><span class="koboSpan" id="kobo.10.1">DL</span></strong><span class="koboSpan" id="kobo.11.1">). </span><span class="koboSpan" id="kobo.11.2">In this chapter and a few subsequent chapters, we will learn about the different DL models and how to use DL on our IoT generated data. </span><span class="koboSpan" id="kobo.11.3">In this chapter, we will start with a glimpse into the journey of DL, and learn about four popular models, the </span><strong><span class="koboSpan" id="kobo.12.1">multilayered perceptron</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong><span class="koboSpan" id="kobo.14.1">MLP</span></strong><span class="koboSpan" id="kobo.15.1">), the </span><strong><span class="koboSpan" id="kobo.16.1">convolutional neural network</span></strong><span class="koboSpan" id="kobo.17.1"> (</span><strong><span class="koboSpan" id="kobo.18.1">CNN</span></strong><span class="koboSpan" id="kobo.19.1">), </span><strong><span class="koboSpan" id="kobo.20.1">recurrent neural network</span></strong><span class="koboSpan" id="kobo.21.1"> (</span><strong><span class="koboSpan" id="kobo.22.1">RNN</span></strong><span class="koboSpan" id="kobo.23.1">), and autoencoders. </span><span class="koboSpan" id="kobo.23.2">Specifically, you will learn about the following:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.24.1">The history of DL and the factors responsible for its present success</span></li>
<li><span class="koboSpan" id="kobo.25.1">Artificial neurons and how they can be connected to solve non-linear problems</span></li>
<li><span class="koboSpan" id="kobo.26.1">The backpropagation algorithm and using it to train the MLP model</span></li>
<li><span class="koboSpan" id="kobo.27.1">The different optimizers and activation functions available in TensorFlow</span></li>
<li><span class="koboSpan" id="kobo.28.1">How the CNN works and the concept behind kernel, padding, and strides</span></li>
<li><span class="koboSpan" id="kobo.29.1">Using CNN model for classification and recognition</span></li>
<li><span class="koboSpan" id="kobo.30.1">RNNs and modified RNN and long short-term memory and gated recurrent units</span></li>
<li><span class="koboSpan" id="kobo.31.1">The architecture and functioning of autoencoders</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Deep learning 101</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The human mind has always intrigued philosophers, scientists, and engineers alike. </span><span class="koboSpan" id="kobo.2.2">The desire to imitate and replicate the intelligence of the human brain by man has been written about over many years; Galatea by Pygmalion of Cyprus in Greek mythology, Golem in Jewish folklore, and Maya Sita in Hindu mythology are just a few examples. </span><span class="koboSpan" id="kobo.2.3">Robots with </span><strong><span class="koboSpan" id="kobo.3.1">Artificial Intelligence</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">AI</span></strong><span class="koboSpan" id="kobo.6.1">) are a favorite of (science) fiction writers since time immemorial.</span></p>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.7.1">AI, as we know today, was conceived parallel with the idea of computers. </span><span class="koboSpan" id="kobo.7.2">The seminal paper, </span><em><span class="koboSpan" id="kobo.8.1">A Logical Calculus Of The Ideas Immanent In Nervous Activity</span></em><span class="koboSpan" id="kobo.9.1">, in the year 1943 by McCulloch and Pitts proposed the first neural network model—the threshold devices that could perform logical operations such as AND, OR, AND-NOT. </span><span class="koboSpan" id="kobo.9.2">In his pioneering work, </span><span><em><span class="koboSpan" id="kobo.10.1">Computing Machinery and Intelligence,</span></em><span class="koboSpan" id="kobo.11.1"> published in the year 1950,</span></span><span class="koboSpan" id="kobo.12.1"> Alan Turing proposed a </span><strong><span class="koboSpan" id="kobo.13.1">Turing test</span></strong><span class="koboSpan" id="kobo.14.1">; a test to identify whether a machine has intelligence or not. </span><span class="koboSpan" id="kobo.14.2">Rosenblatt, in 1957, laid the base for networks that could learn from experience in his report, </span><span><em><span class="koboSpan" id="kobo.15.1">The Perceptron—a perceiving and recognizing automaton</span></em><span class="koboSpan" id="kobo.16.1">. </span><span class="koboSpan" id="kobo.16.2">These ideas were far ahead of their time; while the concepts looked theoretically possible, computational resources at that time severely limited the performance you could get</span></span><span class="koboSpan" id="kobo.17.1"> through these models that could do logic and learn.</span></p>
<div class="packt_tip"><span class="koboSpan" id="kobo.18.1">While these papers seem old and irrelevant, they are very much worth reading and give great insight into the vision these initial thinkers had. </span><span class="koboSpan" id="kobo.18.2">Following, are the links to these papers for interested readers:
</span><ul>
<li><em><span class="koboSpan" id="kobo.19.1">A Logical Calculus Of The Ideas Immanent In Nervous Activity</span></em><span class="koboSpan" id="kobo.20.1">, McCulloch and Pitts: </span><a href="https://link.springer.com/article/10.1007%2FBF02478259"><span class="koboSpan" id="kobo.21.1">https://link.springer.com/article/10.1007%2FBF02478259</span><br/></a></li>
<li><em><span class="koboSpan" id="kobo.22.1">Computing Machinery and Intelligence</span></em><span class="koboSpan" id="kobo.23.1">, Alan Turing: </span><a href="http://phil415.pbworks.com/f/TuringComputing.pdf"><span class="koboSpan" id="kobo.24.1">http://phil415.pbworks.com/f/TuringComputing.pdf</span><br/></a></li>
<li><em><span class="koboSpan" id="kobo.25.1">The Perceptron—a perceiving and recognizing automaton</span></em><span class="koboSpan" id="kobo.26.1">, Rosenblatt: </span><a href="https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf"><span class="koboSpan" id="kobo.27.1">https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf</span></a><br/>
<br/><span class="koboSpan" id="kobo.28.1">
Another interesting paper one by Wang and Raj from Carnegie Melon University, </span><em><span class="koboSpan" id="kobo.29.1">On the Origin of Deep Learning</span></em><span class="koboSpan" id="kobo.30.1">; the 72-page paper covers in detail the history of DL, starting from the McCulloch Pitts model to the latest attention models: </span><a href="https://arxiv.org/pdf/1702.07800.pdf"><span class="koboSpan" id="kobo.31.1">https://arxiv.org/pdf/1702.07800.pdf</span></a><span class="koboSpan" id="kobo.32.1">.</span></li>
</ul>
</div>
<p><span class="koboSpan" id="kobo.33.1">Two AI winters and a few successes later (with the breakthrough in 2012, when Alex Krizhvesky, Ilya Sutskever, and Geoffrey Hinton's AlexNet entry in the annual ImageNet challenge achieved an error rate of 16%), today we stand at a place where DL has outperformed most of the existing AI techniques. </span><span class="koboSpan" id="kobo.33.2">The following screenshot from Google Trends shows that, roughly around 2014, </span><strong><span class="koboSpan" id="kobo.34.1">Deep</span></strong> <strong><span class="koboSpan" id="kobo.35.1">Learning</span></strong><span class="koboSpan" id="kobo.36.1"> became popular and had been growing since then:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.37.1"><img class="aligncenter size-full wp-image-923 image-border" src="assets/902e63f9-5ec8-4568-b10e-9f18e2a3a5f0.png" style="width:90.33em;height:45.25em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.38.1">Deep learning in Google Trends from 2004 to April 2018</span></div>
<p><span class="koboSpan" id="kobo.39.1">Let's see the reasons behind this growing trend and analyze whether it's just hype or whether there's more to it.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Deep learning—why now?</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Most of the core concepts in the field of DL were already in place by the 80s and 90s, and therefore, the question arises why suddenly we see an increase in the applications of DL to solve different problems from image classification and image inpainting, to self-driving cars and speech generation. </span><span class="koboSpan" id="kobo.2.2">The major reason is twofold, outlined as follows:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">Availability of large high-quality dataset</span></strong><span class="koboSpan" id="kobo.4.1">: The internet resulted in the generation of an enormous amount of datasets in terms of images, video, text, and audio. </span><span class="koboSpan" id="kobo.4.2">While most of it's unlabeled, by the effort of many leading researchers (for example, Fei Fei Li creating the ImageNet dataset), we finally have access to large labeled datasets. </span><span class="koboSpan" id="kobo.4.3">If DL is a furnace lighting your imagination, data is the fuel burning it. </span><span class="koboSpan" id="kobo.4.4">The greater the amount and variety of the data, the better the performance of the model.</span></li>
<li><strong><span class="koboSpan" id="kobo.5.1">Availability of parallel computing using graphical processing units</span></strong><span class="koboSpan" id="kobo.6.1">: In DL models, there are mainly two mathematical matrix operations that play a crucial role, namely, matrix multiplication and matrix addition. </span><span class="koboSpan" id="kobo.6.2">The possibility of parallelizing these processes for all the neurons in a layer with the help of </span><strong><span class="koboSpan" id="kobo.7.1">graphical processing units</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong><span class="koboSpan" id="kobo.9.1">GPUs</span></strong><span class="koboSpan" id="kobo.10.1">) made it possible to train the DL models in reasonable time.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.11.1">Once the interest in DL grew, people came up with further improvements, like better optimizers for the gradient descent (the necessary algorithm used to calculate weight and bias update in DL models), for example, Adam and RMSprop; new regularization techniques such as dropout and batch normalization that help, not only in overfitting, but can also reduce the training time, and last, but not the least, availability of DL libraries such as TensorFlow, Theano, Torch, MxNet, and Keras, which made it easier to define and train complex architectures.</span></p>
<p><span class="koboSpan" id="kobo.12.1">According to Andrew Ng, founder of </span><a href="https://www.deeplearning.ai/"><span class="koboSpan" id="kobo.13.1">deeplearning.ai</span></a><span class="koboSpan" id="kobo.14.1">, despite plenty of hype and frantic investment, we won't see another AI winter, because improvements in the computing devices </span><em><span class="koboSpan" id="kobo.15.1">will keep the performance advances and breakthroughs coming for the foreseeable future</span></em><span class="koboSpan" id="kobo.16.1">, Andrew Ng said this at EmTech Digital in 2016, and true to his prediction, we have seen advancements in the processing hardware with Google's </span><strong><span class="koboSpan" id="kobo.17.1">Tensor Processing Unit</span></strong><span class="koboSpan" id="kobo.18.1"> (</span><strong><span class="koboSpan" id="kobo.19.1">TPUs</span></strong><span class="koboSpan" id="kobo.20.1">), Intel Movidius, and NVIDIA's latest GPUs. </span><span class="koboSpan" id="kobo.20.2">Moreover, there are cloud computing GPUs that are available today at as low as 0.40 cents per hour, making it affordable for all.</span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.21.1">You can read the complete article </span><em><span class="koboSpan" id="kobo.22.1">AI Winter Isn't Coming</span></em><span class="koboSpan" id="kobo.23.1">, published in MIT Technology Review: </span><a href="https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/"><span class="koboSpan" id="kobo.24.1">https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/</span></a><span class="koboSpan" id="kobo.25.1">. </span><span class="koboSpan" id="kobo.25.2">Here Andrew Ng answers different queries regarding the future of AI.</span></div>
<div class="packt_tip"><span class="koboSpan" id="kobo.26.1">For DL, GPU processing power is a must; there are a large number of companies offering cloud computing services for the same. </span><span class="koboSpan" id="kobo.26.2">But in case you are starting in the field, you can use one of the following:
</span><ul>
<li><strong><span class="koboSpan" id="kobo.27.1">Google Colaboratory</span></strong><span class="koboSpan" id="kobo.28.1">: It provides a browser-based, GPU enabled Jupyter Notebook—like interface. </span><span class="koboSpan" id="kobo.28.2">It gives free access to the GPU computing power for 12 continuous hours.</span></li>
<li><strong><span class="koboSpan" id="kobo.29.1">Kaggle</span></strong><span class="koboSpan" id="kobo.30.1">: Kaggle too provides a Jupyter Notebook style interface with GPU computing power for roughly six continuous hours free of cost.</span></li>
</ul>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Artificial neuron</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The fundamental component of all DL models is an artificial neuron. </span><span class="koboSpan" id="kobo.2.2">The artificial neuron is inspired by the working of biological neurons. </span><span class="koboSpan" id="kobo.2.3">It consists of some inputs connected via weights (also called </span><strong><span class="koboSpan" id="kobo.3.1">synaptic connections</span></strong><span class="koboSpan" id="kobo.4.1">), the weighted sum of all the inputs goes through a processing function (called the </span><strong><span class="koboSpan" id="kobo.5.1">activation function</span></strong><span class="koboSpan" id="kobo.6.1">) and generates a non-linear output.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The following screenshot shows </span><strong><span class="koboSpan" id="kobo.8.1">A biological Neuron</span></strong><span class="koboSpan" id="kobo.9.1"> and </span><strong><span class="koboSpan" id="kobo.10.1">An Artificial Neuron</span></strong><span class="koboSpan" id="kobo.11.1">:</span></p>
<p class="CDPAlignCenter CDPAlign"/>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.12.1"><img src="assets/533935b8-b03c-47b0-835f-3315a1a9361a.png"/></span></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.13.1">A biological neuron and an artificial neuron</span></div>
<p><span class="koboSpan" id="kobo.14.1">If </span><em><span class="koboSpan" id="kobo.15.1">X</span><sub><span class="koboSpan" id="kobo.16.1">i</span></sub></em><span class="koboSpan" id="kobo.17.1"> is the </span><em><span class="koboSpan" id="kobo.18.1">i</span></em><sup><span class="koboSpan" id="kobo.19.1">th</span></sup><span class="koboSpan" id="kobo.20.1"> input to the artificial neuron (</span><em><span class="koboSpan" id="kobo.21.1">j</span></em><span class="koboSpan" id="kobo.22.1">) connected via the synaptic connection </span><em><span class="koboSpan" id="kobo.23.1">w</span><sub><span class="koboSpan" id="kobo.24.1">ij</span></sub></em><span class="koboSpan" id="kobo.25.1">, then, the net input to the neuron, commonly called the </span><strong><span class="koboSpan" id="kobo.26.1">activity of the neuron</span></strong><span class="koboSpan" id="kobo.27.1">, can be defined as the weighted sum of all its contains, and is given by the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.28.1"><img class="fm-editor-equation" src="assets/2d7fd5ac-b237-47c6-ba7a-c7d2a56f0415.png" style="width:10.75em;height:3.83em;"/></span></div>
<p><span class="koboSpan" id="kobo.29.1">In the preceding equation, </span><em><span class="koboSpan" id="kobo.30.1">N</span></em><span class="koboSpan" id="kobo.31.1"> is the total number of inputs to the </span><em><span class="koboSpan" id="kobo.32.1">j</span></em><sup><span class="koboSpan" id="kobo.33.1">th</span></sup><span class="koboSpan" id="kobo.34.1"> neuron, and </span><em><span class="koboSpan" id="kobo.35.1">θ</span><sub><span class="koboSpan" id="kobo.36.1">j</span></sub></em><span class="koboSpan" id="kobo.37.1"> is the threshold of the </span><em><span class="koboSpan" id="kobo.38.1">j</span></em><sup><span class="koboSpan" id="kobo.39.1">th</span></sup><span class="koboSpan" id="kobo.40.1"> neuron; the output of the neuron is then given by the following:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><br/>
<span class="koboSpan" id="kobo.41.1"><img class="fm-editor-equation" src="assets/94f64405-a749-4631-8e50-1bf6d36bca10.png" style="width:6.25em;height:1.67em;"/></span></div>
<p><span class="koboSpan" id="kobo.42.1">In the preceding, </span><em><span class="koboSpan" id="kobo.43.1">g</span></em><span class="koboSpan" id="kobo.44.1"> is the activation function. </span><span class="koboSpan" id="kobo.44.2">The following point lists different activation functions used in different DL models, along with their mathematical and graphical representations:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.45.1">Sigmoid:</span><span><span class="koboSpan" id="kobo.46.1"> </span></span><span class="koboSpan" id="kobo.47.1"><img style="font-size: 1em;color: #333333;width:9.17em;height:2.92em;" class="fm-editor-equation" src="assets/3a88f5a9-2bb0-4f1e-89b3-117b96da4945.png"/></span><span><span class="koboSpan" id="kobo.48.1"> </span></span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.49.1"><img class="aligncenter size-full wp-image-924 image-border" src="assets/c64270c0-bc9b-4917-81cf-3f050fb76f44.png" style="width:33.67em;height:24.08em;"/></span></p>
<ul>
<li><span><span class="koboSpan" id="kobo.50.1">Hyperbolic Tangent: </span><em><span class="koboSpan" id="kobo.51.1">g(h</span><sub><span class="koboSpan" id="kobo.52.1">j</span></sub><span class="koboSpan" id="kobo.53.1">)= tanh(h</span><sub><span class="koboSpan" id="kobo.54.1">j</span></sub><span class="koboSpan" id="kobo.55.1">)</span></em></span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.56.1"><img class="aligncenter size-full wp-image-925 image-border" src="assets/a8bc9f89-bf33-4b53-aae5-edf3fe12c45e.png" style="width:33.92em;height:23.17em;"/></span></p>
<ul>
<li><span><span class="koboSpan" id="kobo.57.1">ReLU: </span><em><span class="koboSpan" id="kobo.58.1">g(h</span><sub><span class="koboSpan" id="kobo.59.1">j</span></sub><span class="koboSpan" id="kobo.60.1">)= max(0,h</span><sub><span class="koboSpan" id="kobo.61.1">j</span></sub><span class="koboSpan" id="kobo.62.1">)</span></em></span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.63.1"><img class="aligncenter size-full wp-image-926 image-border" src="assets/1d0fe159-f070-4193-bb05-61dc1f5045d2.png" style="width:32.33em;height:23.17em;"/></span></p>
<ul>
<li><span><span class="koboSpan" id="kobo.64.1">Softmax: </span><span class="koboSpan" id="kobo.65.1"><img class="fm-editor-equation" src="assets/7c124da7-6c92-4b9b-9d4c-d79ccae0cc89.png" style="width:9.33em;height:3.58em;"/></span></span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.66.1"><img class="aligncenter size-full wp-image-927 image-border" src="assets/4197a960-7e58-4b01-890f-e611b8ee4062.png" style="width:29.33em;height:20.58em;"/></span></p>
<ul>
<li><span><span class="koboSpan" id="kobo.67.1">Leaky ReLU: </span><span class="koboSpan" id="kobo.68.1"><img class="fm-editor-equation" src="assets/71459d3e-9c52-47bf-b038-9db5e584f5a8.png" style="width:11.08em;height:2.83em;"/></span></span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.69.1"><img class="aligncenter size-full wp-image-928 image-border" src="assets/e2167991-26bb-4f98-8325-3663bb75cdac.png" style="width:30.75em;height:22.00em;"/></span></p>
<ul>
<li><span class="koboSpan" id="kobo.70.1">ELU: </span><span class="koboSpan" id="kobo.71.1"><img class="fm-editor-equation" src="assets/1ae102ff-1354-456c-a111-6c2f99642980.png" style="width:15.42em;height:3.08em;"/></span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.72.1"><img class="aligncenter size-full wp-image-929 image-border" src="assets/9854bc26-8a85-49c1-b321-c25993a7f369.png" style="width:28.42em;height:20.33em;"/></span></p>
<ul>
<li><span><span class="koboSpan" id="kobo.73.1">Threshold: </span><span class="koboSpan" id="kobo.74.1"><img class="fm-editor-equation" src="assets/b6ede6c5-55ce-4340-8fdb-cfecc66880ad.png" style="width:11.17em;height:3.00em;"/></span></span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.75.1"><img class="aligncenter size-full wp-image-930 image-border" src="assets/7585376a-d5a9-4212-9916-d07b03419737.png" style="width:29.08em;height:20.75em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Modelling single neuron in TensorFlow</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Can we use this single neuron and make it learn? </span><span class="koboSpan" id="kobo.2.2">The answer is yes, the process of learning involves adapting the weights such that a predefined loss function (</span><em><span class="koboSpan" id="kobo.3.1">L</span></em><span class="koboSpan" id="kobo.4.1">) reduces. </span><span class="koboSpan" id="kobo.4.2">If we update the weights in the direction opposite to the gradient of the loss function with respect to weights, it will ensure that loss function decreases with each update. </span><span class="koboSpan" id="kobo.4.3">This algorithm is called the </span><strong><span class="koboSpan" id="kobo.5.1">gradient descent</span></strong><span class="koboSpan" id="kobo.6.1"> algorithm, and is at the heart of all DL models. </span><span class="koboSpan" id="kobo.6.2">Mathematically, if </span><em><span class="koboSpan" id="kobo.7.1">L</span></em><span class="koboSpan" id="kobo.8.1"> is the loss function and </span><em><span class="koboSpan" id="kobo.9.1">η</span></em><span class="koboSpan" id="kobo.10.1"> the learning rate, then the weight </span><em><span class="koboSpan" id="kobo.11.1">w</span><sub><span class="koboSpan" id="kobo.12.1">ij</span></sub></em><span class="koboSpan" id="kobo.13.1"> is updated and represented as:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.14.1"><img class="fm-editor-equation" src="assets/d0ba28af-252f-4fd5-9a6c-77ac7ac240de.png" style="width:13.67em;height:3.75em;"/></span></div>
<p><span class="koboSpan" id="kobo.15.1">If we have to model the single artificial neuron, we need first to decide the following parameters:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.16.1">Learning rate parameter</span></strong><span class="koboSpan" id="kobo.17.1">: Learning rate parameter determines how fast we descent the gradient. </span><span class="koboSpan" id="kobo.17.2">Conventionally, it lies between </span><em><span class="koboSpan" id="kobo.18.1">0</span></em><span class="koboSpan" id="kobo.19.1"> and </span><em><span class="koboSpan" id="kobo.20.1">1</span></em><span class="koboSpan" id="kobo.21.1">. </span><span class="koboSpan" id="kobo.21.2">If learning rate is too high, the network may either oscillate around the correct solution or completely diverge from the solution. </span><span class="koboSpan" id="kobo.21.3">On the other hand, when learning rate is too low, it will take a long time to converge to the solution finally.</span></li>
<li><strong><span class="koboSpan" id="kobo.22.1">Activation function</span></strong><span class="koboSpan" id="kobo.23.1">: The activation function decides how the output of the neuron varies with its activity. </span><span class="koboSpan" id="kobo.23.2">Since the weight update equation involves a derivative of the loss function, which in turn will depend on the derivative of the activation function, we prefer a continuous-differentiable function as the activation function for the neuron. </span><span class="koboSpan" id="kobo.23.3">Initially, sigmoid and hyperbolic tangent were used, but they suffered from slow convergence and vanishing gradients (the gradient becoming zero, and hence, no learning, while the solution hasn't been reached). </span><span class="koboSpan" id="kobo.23.4">In recent years, </span><strong><span><span class="koboSpan" id="kobo.24.1">rectified linear units</span></span></strong><span class="koboSpan" id="kobo.25.1"> (</span><strong><span><span class="koboSpan" id="kobo.26.1">ReLU</span></span></strong><span class="koboSpan" id="kobo.27.1">) and its variants such as leaky ReLU and ELU are preferred since they offer fast convergence and at the same time, help in overcoming the vanishing gradient problem. </span><span class="koboSpan" id="kobo.27.2">In ReLU, we sometimes have a problem of </span><strong><span class="koboSpan" id="kobo.28.1">dead neurons</span></strong><span class="koboSpan" id="kobo.29.1">, that is some neurons never fire because their activity is always less than zero, and hence, they never learn. </span><span class="koboSpan" id="kobo.29.2">Both leaky ReLU and ELU overcome the problem of dead neurons by ensuring a non-zero neuron output, even when the activity is negative. </span><span class="koboSpan" id="kobo.29.3">The lists of the commonly used activation functions, and their mathematical and graphical representations is explained before this section. </span><span class="koboSpan" id="kobo.29.4">(You can play around with the </span><kbd><span class="koboSpan" id="kobo.30.1">activation_functions.ipynb</span></kbd><span class="koboSpan" id="kobo.31.1"> code , which uses TensorFlow defined activation functions.)</span></li>
<li><strong><span class="koboSpan" id="kobo.32.1">Loss function</span></strong><span class="koboSpan" id="kobo.33.1">: Loss function is the parameter our network tries to minimize, and so choosing the right loss function is crucial for the learning. </span><span class="koboSpan" id="kobo.33.2">As you will delve deep into DL, you will find many cleverly defined loss functions. </span><span class="koboSpan" id="kobo.33.3">You will see how, by properly defining loss functions, we can make our DL model create new images, visualize dreams, or give a caption to an image, and much more. </span><span class="koboSpan" id="kobo.33.4">Conventionally, depending on the type of task regression or classification, people use </span><strong><span class="koboSpan" id="kobo.34.1">mean square error</span></strong><span class="koboSpan" id="kobo.35.1"> (</span><strong><span><span class="koboSpan" id="kobo.36.1">MSE</span></span></strong><span class="koboSpan" id="kobo.37.1">) or </span><strong><span class="koboSpan" id="kobo.38.1">categorical-cross entropy</span></strong><span class="koboSpan" id="kobo.39.1"> loss function. </span><span class="koboSpan" id="kobo.39.2">You will learn these loss functions as we progress through the book.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.40.1">Now that we know the basic elements needed to model an artificial neuron, let's start with the coding. </span><span class="koboSpan" id="kobo.40.2">We will presume a regression task, and so we will use MSE loss function. </span><span class="koboSpan" id="kobo.40.3">If </span><em><span class="koboSpan" id="kobo.41.1">y</span><sub><span class="koboSpan" id="kobo.42.1">j</span></sub></em><span class="koboSpan" id="kobo.43.1"> is the output of our single neuron for the input vector </span><em><span class="koboSpan" id="kobo.44.1">X</span></em><span class="koboSpan" id="kobo.45.1"> and </span><span class="koboSpan" id="kobo.46.1"><img src="assets/ae1b66ec-7c89-4339-b04e-9c4f506272e1.png" style="width:1.08em;height:1.33em;"/></span><span class="koboSpan" id="kobo.47.1"> is the output we desire for output neuron </span><em><span class="koboSpan" id="kobo.48.1">j</span></em><span class="koboSpan" id="kobo.49.1">, then the MSE error is mathematically expressed as (mean of the square of the error </span><span class="koboSpan" id="kobo.50.1"><img src="assets/a6adaa8f-16fa-49c4-af70-e3844c7cd9d8.png" style="width:3.50em;height:1.33em;"/></span><span class="koboSpan" id="kobo.51.1">), shown as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.52.1"><img class="fm-editor-equation" src="assets/3b1d1c78-aff6-46b5-bdd6-05c9058ff9f0.png" style="width:12.67em;height:3.58em;"/></span></div>
<p><span class="koboSpan" id="kobo.53.1">In the preceding, </span><em><span class="koboSpan" id="kobo.54.1">M</span></em><span class="koboSpan" id="kobo.55.1"> is the total number of training sample (input-output pair).</span></p>
<p><span class="koboSpan" id="kobo.56.1">Note that if you were to implement this artificial neuron without using TensorFlow (to be specific without using any of the DL libraries mentioned earlier), then you will need to calculate the gradient yourself, for example, you will write a function or a code that will first compute the gradient of loss function, and then you will have to write a code to update all of the weights and biases. </span><span class="koboSpan" id="kobo.56.2">For a single neuron with the MSE loss function, calculating derivative is still straightforward, but as the complexity of the network increases, calculating the gradient for the specific loss function, implementing it in code, and then finally updating weights and biases can become a very cumbersome act.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.57.1">TensorFlow makes this whole process easier by using automatic differentiation. </span><span class="koboSpan" id="kobo.57.2">TensorFlow specifies all the operations in a TensorFlow graph; this allows it to use the chain rule and go complicated in the graph assigning the gradients.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.58.1">And so, in TensorFlow we build the execution graph, and define our loss function, then it calculates the gradient automatically, and it supports many different gradients, calculating algorithms (optimizers), which we can conveniently use.</span><br/>
<br/><span class="koboSpan" id="kobo.59.1">
You can learn more about the concept of automatic differentiation through this link: </span><a href="http://www.columbia.edu/~ahd2125/post/2015/12/5/"><span class="koboSpan" id="kobo.60.1">http://www.columbia.edu/~ahd2125/post/2015/12/5/</span></a><span class="koboSpan" id="kobo.61.1">.</span></p>
<p><span class="koboSpan" id="kobo.62.1">Now with all this basic information, we build our single neuron in TensorFlow with the following steps:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.63.1">The first step, in every Python code, is always importing the modules one will need in the rest of the program. </span><span class="koboSpan" id="kobo.63.2">We will import TensorFlow to build the single artificial neuron. </span><span class="koboSpan" id="kobo.63.3">Numpy and pandas are there for any supporting mathematical calculations and for reading the data files. </span><span class="koboSpan" id="kobo.63.4">Beside this, we are also importing some useful functions (for normalization of data, splitting it into train, validation, and shuffling the data) from scikit-learn, we have already used these functions in the earlier chapters and know that normalization and shuffling is an important step in any AI pipeline:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.64.1">import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.65.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.66.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.67.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.68.1">from sklearn.utils import shuffle</span><br/><span class="koboSpan" id="kobo.69.1">from sklearn.preprocessing import MinMaxScaler</span><br/><span class="koboSpan" id="kobo.70.1">from sklearn.model_selection import train_test_split</span><br/><span class="koboSpan" id="kobo.71.1">% matplotlib inline</span></pre>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.72.1">As explained earlier, validation helps in knowing if the model has learned or it's overfitting or underfitting</span></p>
<ol start="2">
<li><span class="koboSpan" id="kobo.73.1">In TensorFlow, we first build a model graph and then execute it. </span><span class="koboSpan" id="kobo.73.2">This might, when starting, seem complicated, but once you get the hang of it, it's very convenient and allows us to optimize the code for production. </span><span class="koboSpan" id="kobo.73.3">So, let's first define our single neuron graph. </span><span class="koboSpan" id="kobo.73.4">We define </span><kbd><span class="koboSpan" id="kobo.74.1">self.X</span></kbd><span class="koboSpan" id="kobo.75.1"> and </span><kbd><span class="koboSpan" id="kobo.76.1">self.y</span></kbd><span class="koboSpan" id="kobo.77.1"> as placeholders to pass on the data to the graph, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.78.1">class ArtificialNeuron:</span><br/><span class="koboSpan" id="kobo.79.1">    def __init__(self,N=2, act_func=tf.nn.sigmoid, learning_rate= 0.001):</span><br/><span class="koboSpan" id="kobo.80.1">        self.N = N # Number of inputs to the neuron</span><br/><span class="koboSpan" id="kobo.81.1">        self.act_fn = act_func</span><br/><br/><span class="koboSpan" id="kobo.82.1">        # Build the graph for a single neuron</span><br/><span class="koboSpan" id="kobo.83.1">        self.X = tf.placeholder(tf.float32, name='X', shape=[None,N])</span><br/><span class="koboSpan" id="kobo.84.1">        self.y = tf.placeholder(tf.float32, name='Y')</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.85.1">The weights and biases are defined as variables so that the automatic differentiation automatically updates them. </span><span class="koboSpan" id="kobo.85.2">TensorFlow provides a graphical interface to support TensorBoard to see the graph structure, as well as different parameters, and how they change during training. </span><span class="koboSpan" id="kobo.85.3">It's beneficial for debugging and understanding how your model is behaving. </span><span class="koboSpan" id="kobo.85.4">In the following code, we, therefore, add code lines to create histogram summaries for both weights and biases:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.86.1">self.W = tf.Variable(tf.random_normal([N,1], stddev=2, seed = 0), name = "weights")</span><br/><span class="koboSpan" id="kobo.87.1">        self.bias = tf.Variable(0.0, dtype=tf.float32, name="bias")</span><br/><span class="koboSpan" id="kobo.88.1">        tf.summary.histogram("Weights",self.W)</span><br/><span class="koboSpan" id="kobo.89.1">        tf.summary.histogram("Bias", self.bias)</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.90.1">Next, we perform the mathematical operations, the matrix multiplication, between input and weights, add the bias, and calculate the activity of the neuron and its output, denoted by </span><kbd><span class="koboSpan" id="kobo.91.1">self.y_hat</span></kbd><span class="koboSpan" id="kobo.92.1"> shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.93.1">activity = tf.matmul(self.X, self.W) + self.bias</span><br/><span class="koboSpan" id="kobo.94.1">self.y_hat = self.act_fn(activity)</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.95.1">We define the loss function that we want our model to minimize, and use the TensorFlow optimizer to minimize it, and update weights and biases using the gradient descent optimizer, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.96.1">error = self.y - self.y_hat</span><br/> <br/><span class="koboSpan" id="kobo.97.1">self.loss = tf.reduce_mean(tf.square(error))</span><br/><span class="koboSpan" id="kobo.98.1">self.opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)</span><br/> </pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.99.1">We complete the </span><kbd><span class="koboSpan" id="kobo.100.1">init</span></kbd><span class="koboSpan" id="kobo.101.1"> function by defining a TensorFlow Session and initializing all the variables. </span><span class="koboSpan" id="kobo.101.2">We also add code to ensure that TensorBoard writes all the summaries at the specified place, shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.102.1">tf.summary.scalar("loss",self.loss)</span><br/><span class="koboSpan" id="kobo.103.1">init = tf.global_variables_initializer()</span><br/> <br/><span class="koboSpan" id="kobo.104.1">self.sess = tf.Session()</span><br/><span class="koboSpan" id="kobo.105.1">self.sess.run(init)</span><br/> <br/><span class="koboSpan" id="kobo.106.1">self.merge = tf.summary.merge_all()</span><br/><span class="koboSpan" id="kobo.107.1">self.writer = tf.summary.FileWriter("logs/",graph=tf.get_default_graph())</span></pre>
<ol start="7">
<li><span class="koboSpan" id="kobo.108.1">We define the </span><kbd><span class="koboSpan" id="kobo.109.1">train</span></kbd><span class="koboSpan" id="kobo.110.1"> function where the graph we previously built is executed, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.111.1">def train(self, X, Y, X_val, Y_val, epochs=100):</span><br/><span class="koboSpan" id="kobo.112.1">epoch = 0</span><br/><span class="koboSpan" id="kobo.113.1">X, Y = shuffle(X,Y)</span><br/><span class="koboSpan" id="kobo.114.1">loss = []</span><br/><span class="koboSpan" id="kobo.115.1">loss_val = []</span><br/><span class="koboSpan" id="kobo.116.1">while epoch &amp;amp;lt; epochs:</span><br/><span class="koboSpan" id="kobo.117.1">            # Run the optimizer for the whole training set batch wise (Stochastic Gradient Descent)     </span><br/><span class="koboSpan" id="kobo.118.1">            merge, _, l = self.sess.run([self.merge,self.opt,self.loss], feed_dict={self.X: X, self.y: Y})    </span><br/><span class="koboSpan" id="kobo.119.1">            l_val = self.sess.run(self.loss, feed_dict={self.X: X_val, self.y: Y_val})    </span><br/> <br/><span class="koboSpan" id="kobo.120.1">            loss.append(l)</span><br/><span class="koboSpan" id="kobo.121.1">            loss_val.append(l_val)</span><br/><span class="koboSpan" id="kobo.122.1">            self.writer.add_summary(merge, epoch)    </span><br/> <br/><span class="koboSpan" id="kobo.123.1">            if epoch % 10 == 0:</span><br/><span class="koboSpan" id="kobo.124.1">                print("Epoch {}/{} training loss: {} Validation loss {}".\    </span><br/><span class="koboSpan" id="kobo.125.1">                    format(epoch,epochs,l, l_val ))    </span><br/> <br/> <br/><span class="koboSpan" id="kobo.126.1">            epoch += 1</span><br/><span class="koboSpan" id="kobo.127.1">        return loss, loss_val</span></pre>
<ol start="8">
<li><span class="koboSpan" id="kobo.128.1">To make a prediction, we also include a </span><kbd><span class="koboSpan" id="kobo.129.1">predict</span></kbd><span class="koboSpan" id="kobo.130.1"> method, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.131.1">    def predict(self, X):</span><br/><span class="koboSpan" id="kobo.132.1">        return self.sess.run(self.y_hat, feed_dict={self.X: X})</span></pre>
<ol start="9">
<li><span class="koboSpan" id="kobo.133.1">Next, like in the previous chapter, we read the data, normalize it using scikit-learn functions, and split it into training and validation set, shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.134.1">filename = 'Folds5x2_pp.xlsx'</span><br/><span class="koboSpan" id="kobo.135.1">df = pd.read_excel(filename, sheet_name='Sheet1')</span><br/><span class="koboSpan" id="kobo.136.1">X, Y = df[['AT', 'V','AP','RH']], df['PE']</span><br/><span class="koboSpan" id="kobo.137.1">scaler = MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.138.1">X_new = scaler.fit_transform(X)</span><br/><span class="koboSpan" id="kobo.139.1">target_scaler = MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.140.1">Y_new = target_scaler.fit_transform(Y.values.reshape(-1,1))</span><br/><span class="koboSpan" id="kobo.141.1">X_train, X_val, Y_train, y_val = \</span><br/><span class="koboSpan" id="kobo.142.1">        train_test_split(X_new, Y_new, test_size=0.4, random_state=333)</span></pre>
<ol start="10">
<li><span class="koboSpan" id="kobo.143.1">We use the artificial neuron we created to make the energy output prediction. </span><kbd><span class="koboSpan" id="kobo.144.1">Training Loss</span></kbd><span class="koboSpan" id="kobo.145.1"> and </span><kbd><span class="koboSpan" id="kobo.146.1">Validation Loss</span></kbd><span class="koboSpan" id="kobo.147.1"> are plotted as the artificial neuron learns, as shown in the following:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.148.1">_, d = X_train.shape</span><br/><span class="koboSpan" id="kobo.149.1">model = ArtificialNeuron(N=d)</span><br/><br/><span class="koboSpan" id="kobo.150.1">loss, loss_val = model.train(X_train, Y_train, X_val, y_val, 30000)</span><br/><br/><span class="koboSpan" id="kobo.151.1">plt.plot(loss, label="Taining Loss")</span><br/><span class="koboSpan" id="kobo.152.1">plt.plot(loss_val, label="Validation Loss")</span><br/><span class="koboSpan" id="kobo.153.1">plt.legend()</span><br/><span class="koboSpan" id="kobo.154.1">plt.xlabel("Epochs")</span><br/><span class="koboSpan" id="kobo.155.1">plt.ylabel("Mean Square Error")</span></pre>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.156.1"><img class="aligncenter size-full wp-image-931 image-border" src="assets/d4f9e5f6-53e1-4a90-8053-4dc1f12be2f0.png" style="width:29.50em;height:19.83em;"/></span></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.157.1">Mean square error for training and validation data as the single artificial neuron learns to predict the energy output</span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.158.1">The complete code with data reading, data normalization, training, and so on is given in the </span><kbd><span class="koboSpan" id="kobo.159.1">single_neuron_tf.ipynb</span></kbd><span class="koboSpan" id="kobo.160.1"> Jupyter Notebook.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Multilayered perceptrons for regression and classification</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the last section, you learned about a single artificial neuron and used it to predict the energy output. </span><span class="koboSpan" id="kobo.2.2">If we compare it with the linear regression result of </span><a href="09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 3</span></a><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Machine Learning for IoT</span></em><span class="koboSpan" id="kobo.6.1">, we can see that though the single neuron did a good job, it was not as good as linear regression. </span><span class="koboSpan" id="kobo.6.2">The single neuron architecture had an MSE value of 0.078 on the validation dataset as compared 0.01 of linear regression. </span><span class="koboSpan" id="kobo.6.3">Can we make it better, with maybe more epochs, or different learning rate, or perhaps more single neurons. </span><span class="koboSpan" id="kobo.6.4">Unfortunately not, single neurons can solve only linearly separable problems, for example, they can provide a solution only if there exists a straight line separating the classes/decision.</span></p>
<div class="packt_infobox CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.7.1">The network with a single layer of neurons is called </span><strong><span class="koboSpan" id="kobo.8.1">simple perceptron</span></strong><span class="koboSpan" id="kobo.9.1">. </span><span class="koboSpan" id="kobo.9.2">The perceptron model was given by Rosenblatt in 1958 (</span><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;rep=rep1&amp;type=pdf"><span class="koboSpan" id="kobo.10.1">htt</span></a><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;rep=rep1&amp;type=pdf"><span class="koboSpan" id="kobo.11.1">p://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;amp;rep=rep1&amp;amp;type=pdf</span></a><span class="koboSpan" id="kobo.12.1">). </span><span class="koboSpan" id="kobo.12.2">The paper created lots of ripples in the scientific community and lots of research was initiated in the field. </span><span class="koboSpan" id="kobo.12.3">It was first implemented in hardware for the task of image recognition. </span><span class="koboSpan" id="kobo.12.4">Although perceptron seemed very promising initially, the book </span><em><span class="koboSpan" id="kobo.13.1">Perceptrons</span></em><span class="koboSpan" id="kobo.14.1"> by Marvin Minsky and Seymour Papert proved that simple perceptron can solve only linearly separable problems (</span><a href="https://books.google.co.in/books?hl=en&amp;lr=&amp;id=PLQ5DwAAQBAJ&amp;oi=fnd&amp;pg=PR5&amp;dq=Perceptrons:+An+Introduction+to+Computational+Geometry&amp;ots=zyEDwMrl__&amp;sig=DfDDbbj3es52hBJU9znCercxj3M#v=onepage&amp;q=Perceptrons%3A%20An%20Introduction%20to%20Computational%20Geometry&amp;f=false"><span class="koboSpan" id="kobo.15.1">https://books.google.co.in/books?hl=en&amp;amp;lr=&amp;amp;id=PLQ5DwAAQBAJ&amp;amp;oi=fnd&amp;amp;pg=PR5&amp;amp;dq=Perceptrons:+An+Introduction+to+Computational+Geometry&amp;amp;ots=zyEDwMrl__&amp;amp;sig=DfDDbbj3es52hBJU9znCercxj3M#v=onepage&amp;amp;q=Perceptrons%3A%20An%20Introduction%20to%20Computational%20Geometry&amp;amp;f=false</span></a><span class="koboSpan" id="kobo.16.1">).</span></div>
<p><span class="koboSpan" id="kobo.17.1">So what do we do? </span><span class="koboSpan" id="kobo.17.2">We can use multiple layers of single neurons, in other words, use MLP. </span><span class="koboSpan" id="kobo.17.3">Just as in real life, we solve a complex problem by breaking it into small problems, each neuron in the first layer of the MLP breaks the problem into small linearly separable. </span><span class="koboSpan" id="kobo.17.4">Since the information flows here in one direction from the input layer to the output layer via hidden layers, this network is also called a </span><strong><span class="koboSpan" id="kobo.18.1">feedforward</span></strong><span class="koboSpan" id="kobo.19.1"> network. </span><span class="koboSpan" id="kobo.19.2">In the following diagram, we see how the </span><strong><span class="koboSpan" id="kobo.20.1">XOR</span></strong><span class="koboSpan" id="kobo.21.1"> problem is solved using two neurons in the first layer, and a single neuron in the </span><strong><span class="koboSpan" id="kobo.22.1">Output Layer</span></strong><span class="koboSpan" id="kobo.23.1">. </span><span class="koboSpan" id="kobo.23.2">The network breaks the non-linearly separable problem into three linearly separable problems:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.24.1"><img src="assets/91008d5d-b096-4775-a7e0-778e0a396226.png" style="width:43.42em;height:27.92em;"/></span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.25.1">Previous diagram can be explained as XOR solved using MLP with one hidden layer with neurons and one neuron in the output layer. </span><span class="koboSpan" id="kobo.25.2">Red points represent zero and blue points represent one. </span><span class="koboSpan" id="kobo.25.3">We can see that the hidden neurons separate the problem into two linearly separable problems (AND and OR), the output neuron then implements another linearly separable logic the AND-NOT logic, combining them together we are able to solve the XOR, which is not linearly separable</span></p>
<p><span class="koboSpan" id="kobo.26.1">The hidden neurons transform the problem into a form that output layer can use. </span><span class="koboSpan" id="kobo.26.2">The idea of multiple layers of neurons was given by McCulloch and Pitts earlier, but while Rosenblatt gave the learning algorithm for simple perceptrons, he had no way of training multiple layered percetrons. </span><span class="koboSpan" id="kobo.26.3">The major difficulty was that, while for the output neurons we know what should be the desired output and so can calculate the error, and hence, the loss function and weight updates using gradient descent, there was no way to know the desired output of hidden neurons. </span><span class="koboSpan" id="kobo.26.4">Hence, in the absence of any learning algorithm, MLPs were never explored much. </span><span class="koboSpan" id="kobo.26.5">This changed in 1982 when Hinton proposed the backpropagation algorithm (</span><a href="https://www.researchgate.net/profile/Yann_Lecun/publication/2360531_A_Theoretical_Framework_for_Back-Propagation/links/0deec519dfa297eac1000000/A-Theoretical-Framework-for-Back-Propagation.pdf"><span class="koboSpan" id="kobo.27.1">https://www.researchgate.net/profile/Yann_Lecun/publication/2360531_A_Theoretical_Framework_for_Back-Propagation/links/0deec519dfa297eac1000000/A-Theoretical-Framework-for-Back-Propagation.pdf</span></a><span class="koboSpan" id="kobo.28.1">), which can be used to calculate the error, and hence, the weight updates for the hidden neurons. </span><span class="koboSpan" id="kobo.28.2">They employed a neat and straightforward mathematical trick of differentiation using the chain rule, and solved the problem of passing the errors at the output layer back to the hidden neurons, and in turn, boosted life back to neural networks. </span><span class="koboSpan" id="kobo.28.3">Today, backpropagation algorithm is at the heart of almost all DL models.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The backpropagation algorithm</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's first gain a little understanding of the technique behind the backpropagation algorithm. </span><span class="koboSpan" id="kobo.2.2">If you remember from the previous section, the loss function at the output neuron is as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img class="fm-editor-equation" src="assets/cfa9f5dc-ecb4-45ef-9369-63e2229da5ee.png" style="width:14.00em;height:4.08em;"/></span></div>
<p><span class="koboSpan" id="kobo.4.1">You can see that it's unchanged, and so the weight connecting hidden neuron </span><em><span class="koboSpan" id="kobo.5.1">k</span></em><span class="koboSpan" id="kobo.6.1"> to the output neuron </span><em><span class="koboSpan" id="kobo.7.1">j</span></em><span class="koboSpan" id="kobo.8.1"> would be given as follows:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.9.1"><img class="fm-editor-equation" src="assets/078b9093-0555-493a-af10-c031448c46a4.png" style="width:17.17em;height:3.17em;"/></span></div>
<p><span class="koboSpan" id="kobo.10.1">Applying the chain rule of differentiation, this reduces to the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.11.1"><img class="fm-editor-equation" src="assets/26448358-62bd-4f0b-9a6d-c89e12e8c77e.png" style="width:35.33em;height:3.33em;"/></span></div>
<p><span class="koboSpan" id="kobo.12.1">In the preceding equation, </span><em><span class="koboSpan" id="kobo.13.1">O</span><sub><span class="koboSpan" id="kobo.14.1">k</span></sub></em><span class="koboSpan" id="kobo.15.1"> is the output of the hidden neuron </span><em><span class="koboSpan" id="kobo.16.1">k</span></em><span class="koboSpan" id="kobo.17.1">. </span><span class="koboSpan" id="kobo.17.2">Now the weight update connecting input neuron </span><em><span class="koboSpan" id="kobo.18.1">i</span></em><span class="koboSpan" id="kobo.19.1"> to the hidden neuron </span><em><span class="koboSpan" id="kobo.20.1">k</span></em><span class="koboSpan" id="kobo.21.1"> of hidden layer </span><em><span class="koboSpan" id="kobo.22.1">n</span></em><span class="koboSpan" id="kobo.23.1"> can be written as the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.24.1"><img class="fm-editor-equation" src="assets/13803de4-5171-4668-ad06-bf08334d91ae.png" style="width:20.67em;height:2.42em;"/></span></div>
<p><span class="koboSpan" id="kobo.25.1">Again applying the chain rule, it reduces to the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.26.1"><img class="fm-editor-equation" src="assets/bd1d50af-50cd-43a9-b6bf-97fd3f629ef1.png" style="width:54.00em;height:4.33em;"/></span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.27.1">Here, </span><em><span class="koboSpan" id="kobo.28.1">O</span><sub><span class="koboSpan" id="kobo.29.1">i</span></sub></em><span class="koboSpan" id="kobo.30.1"> is the output of the hidden neuron </span><em><span class="koboSpan" id="kobo.31.1">i</span></em><span class="koboSpan" id="kobo.32.1"> in the </span><em><span class="koboSpan" id="kobo.33.1">n-1</span><sup><span class="koboSpan" id="kobo.34.1">th</span></sup></em> <span><span class="koboSpan" id="kobo.35.1">hidden layer. </span><span class="koboSpan" id="kobo.35.2">Since we are using TensorFlow, we need not bother with calculating these gradients, but still, it's a good idea to know the expressions. </span><span class="koboSpan" id="kobo.35.3">From these expressions, you can see why it's important that the activation function is differentiable. </span><span class="koboSpan" id="kobo.35.4">The weight updates depend heavily on the derivative of the activation function, as well as the inputs to the neurons. </span><span class="koboSpan" id="kobo.35.5">Therefore, a smooth derivative function like that in the case of ReLU and ELU result in faster convergence. </span><span class="koboSpan" id="kobo.35.6">If the derivative becomes too large, we have the problem of exploding gradients, and if the derivative becomes almost zero, we have the problem of vanishing gradients. </span><span class="koboSpan" id="kobo.35.7">In both cases, the network does not learn optimally.</span></span></p>
<p class="mce-root"/>
<div class="packt_infobox CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.36.1">Universal approximation theorem: in 1989 Hornik et al. </span><span class="koboSpan" id="kobo.36.2">and George Cybenko independently proved the universal approximation theorem. </span><span class="koboSpan" id="kobo.36.3">The theorem, in its simplest form, states that a large enough feedforward multilayered perceptron, under mild assumptions on activation function, with a single hidden layer, can approximate any Borel measurable function with any degree of accuracy we desire.</span><br/>
<br/><span class="koboSpan" id="kobo.37.1">
In simpler words, it means that the neural network is a universal approximator, and we can approximate any function, listed as follows:</span><br/>
<br/>
<ul>
<li><span class="koboSpan" id="kobo.38.1">We can do so using a single hidden layer feedforward network.</span></li>
</ul>
<ul>
<li><span class="koboSpan" id="kobo.39.1">We can do so provided the network is large enough (that is add more hidden neurons if needed).</span></li>
</ul>
<ul>
<li><span class="koboSpan" id="kobo.40.1">Cybenko proved it for sigmoid activation function at the hidden layer, and linear activation function at the output layer. </span><span class="koboSpan" id="kobo.40.2">Later, Hornik et al showed that it's actually the property of MLPs and can be proved for other activation functions too</span></li>
</ul>
<br/>
<br/><span class="koboSpan" id="kobo.41.1">
The theorem gives a guarantee that MLP can solve any problem, but does not give any measure on how large the network should be. </span><span class="koboSpan" id="kobo.41.2">Also, it does not guarantee learning and convergence.</span><br/>
<br/><span class="koboSpan" id="kobo.42.1">
You can refer to the papers using the following links:</span><br/>
<ul>
<li><span class="koboSpan" id="kobo.43.1">Hornik et al.: </span><a href="https://www.sciencedirect.com/science/article/pii/0893608089900208"><span class="koboSpan" id="kobo.44.1">https://www.sciencedirect.com/science/article/pii/0893608089900208</span></a></li>
<li><span class="koboSpan" id="kobo.45.1">Cybenko: </span><a href="https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf"><span class="koboSpan" id="kobo.46.1">https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf</span></a></li>
</ul>
</div>
<p class="mce-root"><span class="koboSpan" id="kobo.47.1">Now we can describe the steps involved in the backpropagation algorithm, listed as follows:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.48.1">Apply the input to the network</span></li>
<li><span class="koboSpan" id="kobo.49.1">Propagate the input forward and calculate the output of the network</span></li>
<li><span class="koboSpan" id="kobo.50.1">Calculate the loss at the output, and then using the preceding expressions, calculate weight updates for output layer neuron</span></li>
</ol>
<ol start="4">
<li><span class="koboSpan" id="kobo.51.1">Using the weighted errors at output layers, calculate the weight updates for hidden layer</span></li>
<li><span class="koboSpan" id="kobo.52.1">Update all the weights</span></li>
<li><span class="koboSpan" id="kobo.53.1">Repeat the steps for other training examples</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Energy output prediction using MLPs in TensorFlow</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Let's now see how good an MLP is for predicting energy output. </span><span class="koboSpan" id="kobo.2.2">This will be a regression problem. </span><span class="koboSpan" id="kobo.2.3">We will be using a single hidden layer MLP and will predict the net hourly electrical energy output from a combined cycle power plant. </span><span class="koboSpan" id="kobo.2.4">The description of the dataset is provided in </span><a href="fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 1</span></a><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Principles and foundations of IoT and AI</span></em><span class="koboSpan" id="kobo.6.1">.</span><br/><span class="koboSpan" id="kobo.7.1">
Since it's a regression problem, our loss function remains the same as before. </span><span class="koboSpan" id="kobo.7.2">The complete code implementing the </span><kbd><span class="koboSpan" id="kobo.8.1">MLP</span></kbd><span class="koboSpan" id="kobo.9.1"> class is given as follows:</span></p>
<pre><span class="koboSpan" id="kobo.10.1">class MLP:</span><br/><span class="koboSpan" id="kobo.11.1">    def __init__(self,n_input=2,n_hidden=4, n_output=1, act_func=[tf.nn.elu, tf.sigmoid], learning_rate= 0.001):</span><br/><span class="koboSpan" id="kobo.12.1">        self.n_input = n_input # Number of inputs to the neuron</span><br/><span class="koboSpan" id="kobo.13.1">        self.act_fn = act_func</span><br/><span class="koboSpan" id="kobo.14.1">        seed = 123</span><br/> <br/><span class="koboSpan" id="kobo.15.1">        self.X = tf.placeholder(tf.float32, name='X', shape=[None,n_input])</span><br/><span class="koboSpan" id="kobo.16.1">        self.y = tf.placeholder(tf.float32, name='Y')</span><br/> <br/><span class="koboSpan" id="kobo.17.1">        # Build the graph for a single neuron</span><br/><span class="koboSpan" id="kobo.18.1">        # Hidden layer</span><br/><span class="koboSpan" id="kobo.19.1">        self.W1 = tf.Variable(tf.random_normal([n_input,n_hidden],\</span><br/><span class="koboSpan" id="kobo.20.1">                 stddev=2, seed = seed), name = "weights")    </span><br/><span class="koboSpan" id="kobo.21.1">        self.b1 = tf.Variable(tf.random_normal([1, n_hidden], seed = seed),\</span><br/><span class="koboSpan" id="kobo.22.1">                    name="bias")    </span><br/><span class="koboSpan" id="kobo.23.1">        tf.summary.histogram("Weights_Layer_1",self.W1)</span><br/><span class="koboSpan" id="kobo.24.1">        tf.summary.histogram("Bias_Layer_1", self.b1)</span><br/> <br/> <br/><span class="koboSpan" id="kobo.25.1">        # Output Layer</span><br/><span class="koboSpan" id="kobo.26.1">        self.W2 = tf.Variable(tf.random_normal([n_hidden,n_output],\</span><br/><span class="koboSpan" id="kobo.27.1">                stddev=2, seed = 0), name = "weights")</span><br/><span class="koboSpan" id="kobo.28.1">        self.b2 = tf.Variable(tf.random_normal([1, n_output], seed = seed),\</span><br/><span class="koboSpan" id="kobo.29.1">                name="bias")</span><br/><span class="koboSpan" id="kobo.30.1">        tf.summary.histogram("Weights_Layer_2",self.W2)</span><br/><span class="koboSpan" id="kobo.31.1">        tf.summary.histogram("Bias_Layer_2", self.b2)</span><br/> <br/> <br/><span class="koboSpan" id="kobo.32.1">        activity = tf.matmul(self.X, self.W1) + self.b1</span><br/><span class="koboSpan" id="kobo.33.1">        h1 = self.act_fn[0](activity)</span><br/> <br/><span class="koboSpan" id="kobo.34.1">        activity = tf.matmul(h1, self.W2) + self.b2</span><br/><span class="koboSpan" id="kobo.35.1">        self.y_hat = self.act_fn[1](activity)</span><br/> <br/> <br/><span class="koboSpan" id="kobo.36.1">        error = self.y - self.y_hat</span><br/> <br/><span class="koboSpan" id="kobo.37.1">        self.loss = tf.reduce_mean(tf.square(error))\</span><br/><span class="koboSpan" id="kobo.38.1">                 + 0.6*tf.nn.l2_loss(self.W1) </span><br/><span class="koboSpan" id="kobo.39.1">        self.opt = tf.train.GradientDescentOptimizer(learning_rate\</span><br/><span class="koboSpan" id="kobo.40.1">                    =learning_rate).minimize(self.loss)        </span><br/> <br/> <br/><span class="koboSpan" id="kobo.41.1">        tf.summary.scalar("loss",self.loss)</span><br/><span class="koboSpan" id="kobo.42.1">        init = tf.global_variables_initializer()</span><br/> <br/><span class="koboSpan" id="kobo.43.1">        self.sess = tf.Session()</span><br/><span class="koboSpan" id="kobo.44.1">        self.sess.run(init)</span><br/> <br/><span class="koboSpan" id="kobo.45.1">        self.merge = tf.summary.merge_all()</span><br/><span class="koboSpan" id="kobo.46.1">        self.writer = tf.summary.FileWriter("logs/",\</span><br/><span class="koboSpan" id="kobo.47.1">                graph=tf.get_default_graph())</span><br/> <br/> <br/><span class="koboSpan" id="kobo.48.1">     def train(self, X, Y, X_val, Y_val, epochs=100):</span><br/><span class="koboSpan" id="kobo.49.1">        epoch = 0</span><br/><span class="koboSpan" id="kobo.50.1">        X, Y = shuffle(X,Y)</span><br/><span class="koboSpan" id="kobo.51.1">        loss = []</span><br/><span class="koboSpan" id="kobo.52.1">        loss_val = []</span><br/><span class="koboSpan" id="kobo.53.1">        while epoch &amp;amp;lt; epochs:</span><br/><span class="koboSpan" id="kobo.54.1">            # Run the optimizer for the training set </span><br/><span class="koboSpan" id="kobo.55.1">            merge, _, l = self.sess.run([self.merge,self.opt,self.loss],\</span><br/><span class="koboSpan" id="kobo.56.1">                     feed_dict={self.X: X, self.y: Y})</span><br/><span class="koboSpan" id="kobo.57.1">            l_val = self.sess.run(self.loss, feed_dict=\</span><br/><span class="koboSpan" id="kobo.58.1">                    {self.X: X_val, self.y: Y_val})</span><br/> <br/><span class="koboSpan" id="kobo.59.1">            loss.append(l)</span><br/><span class="koboSpan" id="kobo.60.1">            loss_val.append(l_val)</span><br/><span class="koboSpan" id="kobo.61.1">            self.writer.add_summary(merge, epoch)</span><br/> <br/><span class="koboSpan" id="kobo.62.1">            if epoch % 10 == 0:</span><br/><span class="koboSpan" id="kobo.63.1">                print("Epoch {}/{} training loss: {} Validation loss {}".\</span><br/><span class="koboSpan" id="kobo.64.1">                    format(epoch,epochs,l, l_val ))</span><br/> <br/> <br/><span class="koboSpan" id="kobo.65.1">            epoch += 1</span><br/><span class="koboSpan" id="kobo.66.1">        return loss, loss_val</span><br/> <br/><span class="koboSpan" id="kobo.67.1">    def predict(self, X):</span><br/><span class="koboSpan" id="kobo.68.1">        return self.sess.run(self.y_hat, feed_dict={self.X: X})</span></pre>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.69.1">Before using it, let's see the differences between previous code and the code we made earlier for the single artificial neuron. </span><span class="koboSpan" id="kobo.69.2">Here the dimensions of weights of hidden layer is </span><kbd><span class="koboSpan" id="kobo.70.1">#inputUnits × #hiddenUnits</span></kbd><span class="koboSpan" id="kobo.71.1">; the bias of the hidden layer will be equal to the number of hidden units (</span><kbd><span class="koboSpan" id="kobo.72.1">#hiddenUnits</span></kbd><span class="koboSpan" id="kobo.73.1">). </span><span class="koboSpan" id="kobo.73.2">The output layer weights have the dimensions </span><kbd><span class="koboSpan" id="kobo.74.1">#hiddenUnits × #outputUnits</span></kbd><span class="koboSpan" id="kobo.75.1">; the bias of output layer is of the dimension of the number of units in the output layer (</span><kbd><span class="koboSpan" id="kobo.76.1">#outputUnits</span></kbd><span class="koboSpan" id="kobo.77.1">).</span></p>
<div class="packt_tip CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.78.1">In defining the bias, we have used only the column dimensions, not row. </span><span class="koboSpan" id="kobo.78.2">This is because TensorFlow like </span><kbd><span class="koboSpan" id="kobo.79.1">numpy</span></kbd><span class="koboSpan" id="kobo.80.1"> broadcasts the matrices according to the operation to be performed. </span><span class="koboSpan" id="kobo.80.2">And by not fixing the row dimensions of bias, we are able to maintain the flexibility of the number of input training samples (batch-size) we present to the network.</span></div>
<p><span class="koboSpan" id="kobo.81.1">Following screenshot shows  </span><span><span class="koboSpan" id="kobo.82.1">matrix multiplication and addition dimensions while calculating activity:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.83.1"><img class="aligncenter size-full wp-image-932 image-border" src="assets/5f59276a-59a7-490a-8c0b-09153b41bb2b.png" style="width:34.33em;height:21.50em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.84.1">The matrix multiplication and addition dimensions while calculating activity</span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="koboSpan" id="kobo.85.1">The second difference that you should note is in the definition of loss, we have added here the </span><kbd><span class="koboSpan" id="kobo.86.1">l2</span></kbd><span class="koboSpan" id="kobo.87.1"> regularization term to reduce overfitting as discussed in </span><a href="09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml"><span class="koboSpan" id="kobo.88.1">Chapter 3</span></a><span class="koboSpan" id="kobo.89.1">, </span><em><span class="koboSpan" id="kobo.90.1">Machine Learning for IoT</span></em><span class="koboSpan" id="kobo.91.1">, shown as follows:</span></p>
<pre><span class="koboSpan" id="kobo.92.1">self.loss = tf.reduce_mean(tf.square(error)) + 0.6*tf.nn.l2_loss(self.W1) </span></pre>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.93.1">After reading the data from the </span><kbd><span class="koboSpan" id="kobo.94.1">csv</span></kbd><span class="koboSpan" id="kobo.95.1"> file and separating it into training and validation like before, we define the </span><kbd><span class="koboSpan" id="kobo.96.1">MLP</span></kbd><span class="koboSpan" id="kobo.97.1"> class object with </span><kbd><span class="koboSpan" id="kobo.98.1">4</span></kbd><span class="koboSpan" id="kobo.99.1"> neurons in the input layer, </span><kbd><span class="koboSpan" id="kobo.100.1">15</span></kbd><span class="koboSpan" id="kobo.101.1"> neurons in the hidden layer, and </span><kbd><span class="koboSpan" id="kobo.102.1">1</span></kbd><span class="koboSpan" id="kobo.103.1"> neuron in the output layer:</span></p>
<pre><span class="koboSpan" id="kobo.104.1">_, d = X_train.shape</span><br/><span class="koboSpan" id="kobo.105.1">_, n = Y_train.shape</span><br/><span class="koboSpan" id="kobo.106.1">model = MLP(n_input=d, n_hidden=15, n_output=n)</span></pre>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.107.1">In the following code we train the model on training dataset for </span><kbd><span class="koboSpan" id="kobo.108.1">6000</span></kbd><span class="koboSpan" id="kobo.109.1"> epochs:</span></p>
<pre><span class="koboSpan" id="kobo.110.1">loss, loss_val = model.train(X_train, Y_train, X_val, y_val, 6000)</span></pre>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.111.1">This trained network gives us an MSE of 0.016 and an </span><em><span class="koboSpan" id="kobo.112.1">R</span><sup><span class="koboSpan" id="kobo.113.1">2</span></sup></em><span class="koboSpan" id="kobo.114.1"> value of 0.67. </span><span class="koboSpan" id="kobo.114.2">Both are better than what we obtained from a single neuron, and comparable to the ML methods we studied in </span><a href="09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml"><span class="koboSpan" id="kobo.115.1">Chapter 3</span></a><span class="koboSpan" id="kobo.116.1">, </span><em><span class="koboSpan" id="kobo.117.1">Machine Learning for IoT</span></em><span class="koboSpan" id="kobo.118.1">. </span><span class="koboSpan" id="kobo.118.2">The complete code can be accessed in the file named </span><kbd><span class="koboSpan" id="kobo.119.1">MLP_regresssion.ipynb</span></kbd><span class="koboSpan" id="kobo.120.1">.</span></p>
<div class="packt_tip CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.121.1">You can play around with hyperparameters namely: the number of hidden neurons, the activation functions, the learning rate, the optimizer, and the regularization coefficient, and can obtain even better results.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Wine quality classification using MLPs in TensorFlow</span></h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.2.1">MLP can be used to do classification tasks as well. </span><span class="koboSpan" id="kobo.2.2">We can reuse the MLP class from the previous section with minor modifications to perform the task of classification.</span></p>
<p class="mce-root"/>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.3.1">We will need to make the following two major changes:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.4.1">The target in the case of classification will be one-hot encoded</span></li>
<li><span class="koboSpan" id="kobo.5.1">The loss function will now be categorical cross-entropy loss: </span><kbd><span class="koboSpan" id="kobo.6.1">tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y_hat, labels=self.y))</span></kbd></li>
</ul>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.7.1">So let's now see the complete code, which is also available at GitHub in the file </span><kbd><span class="koboSpan" id="kobo.8.1">MLP_classification</span></kbd><span class="koboSpan" id="kobo.9.1">. </span><span class="koboSpan" id="kobo.9.2">We will be classifying the red wine quality, to make it convenient, we use only two wine classes:</span></p>
<ol>
<li class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.10.1">We import the necessary modules namely: TensorFlow, Numpy, Matplotlib, and certain functions from scikit-learn, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.11.1">import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.12.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.13.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.14.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.15.1">from sklearn.utils import shuffle</span><br/><span class="koboSpan" id="kobo.16.1">from sklearn.preprocessing import MinMaxScaler</span><br/><span class="koboSpan" id="kobo.17.1">from sklearn.model_selection import train_test_split</span><br/><span class="koboSpan" id="kobo.18.1">% matplotlib inline</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.19.1">We defined our </span><kbd><span class="koboSpan" id="kobo.20.1">MLP</span></kbd><span class="koboSpan" id="kobo.21.1"> class, it's very similar to the </span><kbd><span class="koboSpan" id="kobo.22.1">MLP</span></kbd><span class="koboSpan" id="kobo.23.1"> class you saw earlier, the only difference is in the definition of the loss function:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.24.1">class MLP:</span><br/><span class="koboSpan" id="kobo.25.1">    def __init__(self,n_input=2,n_hidden=4, n_output=1, act_func=[tf.nn.relu, tf.nn.sigmoid], learning_rate= 0.001):</span><br/><span class="koboSpan" id="kobo.26.1">        self.n_input = n_input # Number of inputs to the neuron</span><br/><span class="koboSpan" id="kobo.27.1">        self.act_fn = act_func</span><br/><span class="koboSpan" id="kobo.28.1">        seed = 456</span><br/> <br/><span class="koboSpan" id="kobo.29.1">        self.X = tf.placeholder(tf.float32, name='X', shape=[None,n_input])</span><br/><span class="koboSpan" id="kobo.30.1">        self.y = tf.placeholder(tf.float32, name='Y')</span><br/> <br/><span class="koboSpan" id="kobo.31.1">        # Build the graph for a single neuron</span><br/><span class="koboSpan" id="kobo.32.1">        # Hidden layer</span><br/><span class="koboSpan" id="kobo.33.1">        self.W1 = tf.Variable(tf.random_normal([n_input,n_hidden],\</span><br/><span class="koboSpan" id="kobo.34.1">             stddev=2, seed = seed), name = "weights")</span><br/><span class="koboSpan" id="kobo.35.1">        self.b1 = tf.Variable(tf.random_normal([1, n_hidden],\</span><br/><span class="koboSpan" id="kobo.36.1">             seed = seed), name="bias")</span><br/><span class="koboSpan" id="kobo.37.1">        tf.summary.histogram("Weights_Layer_1",self.W1)</span><br/><span class="koboSpan" id="kobo.38.1">        tf.summary.histogram("Bias_Layer_1", self.b1)</span><br/> <br/> <br/><span class="koboSpan" id="kobo.39.1">        # Output Layer</span><br/><span class="koboSpan" id="kobo.40.1">        self.W2 = tf.Variable(tf.random_normal([n_hidden,n_output],\</span><br/><span class="koboSpan" id="kobo.41.1">            stddev=2, seed = seed), name = "weights")</span><br/><span class="koboSpan" id="kobo.42.1">        self.b2 = tf.Variable(tf.random_normal([1, n_output],\</span><br/><span class="koboSpan" id="kobo.43.1">             seed = seed), name="bias")    </span><br/><span class="koboSpan" id="kobo.44.1">        tf.summary.histogram("Weights_Layer_2",self.W2)</span><br/><span class="koboSpan" id="kobo.45.1">        tf.summary.histogram("Bias_Layer_2", self.b2)</span><br/> <br/> <br/><span class="koboSpan" id="kobo.46.1">        activity1 = tf.matmul(self.X, self.W1) + self.b1</span><br/><span class="koboSpan" id="kobo.47.1">        h1 = self.act_fn[0](activity1)</span><br/> <br/><span class="koboSpan" id="kobo.48.1">        activity2 = tf.matmul(h1, self.W2) + self.b2</span><br/><span class="koboSpan" id="kobo.49.1">        self.y_hat = self.act_fn[1](activity2)</span><br/> <br/> <br/><span class="koboSpan" id="kobo.50.1">        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\</span><br/><span class="koboSpan" id="kobo.51.1">                logits=self.y_hat, labels=self.y))</span><br/><span class="koboSpan" id="kobo.52.1">        self.opt = tf.train.AdamOptimizer(learning_rate=\</span><br/><span class="koboSpan" id="kobo.53.1">                learning_rate).minimize(self.loss)</span><br/> <br/> <br/><span class="koboSpan" id="kobo.54.1">        tf.summary.scalar("loss",self.loss)</span><br/><span class="koboSpan" id="kobo.55.1">        init = tf.global_variables_initializer()</span><br/> <br/><span class="koboSpan" id="kobo.56.1">        self.sess = tf.Session()</span><br/><span class="koboSpan" id="kobo.57.1">        self.sess.run(init)</span><br/> <br/><span class="koboSpan" id="kobo.58.1">        self.merge = tf.summary.merge_all()</span><br/><span class="koboSpan" id="kobo.59.1">        self.writer = tf.summary.FileWriter("logs/",\</span><br/><span class="koboSpan" id="kobo.60.1">             graph=tf.get_default_graph())</span><br/> <br/> <br/> <br/><span class="koboSpan" id="kobo.61.1">    def train(self, X, Y, X_val, Y_val, epochs=100):</span><br/><span class="koboSpan" id="kobo.62.1">        epoch = 0</span><br/><span class="koboSpan" id="kobo.63.1">        X, Y = shuffle(X,Y)</span><br/><span class="koboSpan" id="kobo.64.1">        loss = []</span><br/><span class="koboSpan" id="kobo.65.1">        loss_val = []</span><br/><span class="koboSpan" id="kobo.66.1">        while epoch &amp;amp;lt; epochs:</span><br/><span class="koboSpan" id="kobo.67.1">            # Run the optimizer for the training set </span><br/><span class="koboSpan" id="kobo.68.1">            merge, _, l = self.sess.run([self.merge,self.opt,self.loss],\</span><br/><span class="koboSpan" id="kobo.69.1">                 feed_dict={self.X: X, self.y: Y})        </span><br/><span class="koboSpan" id="kobo.70.1">            l_val = self.sess.run(self.loss, feed_dict={self.X: X_val, self.y: Y_val})</span><br/> <br/><span class="koboSpan" id="kobo.71.1">            loss.append(l)</span><br/><span class="koboSpan" id="kobo.72.1">            loss_val.append(l_val)</span><br/><span class="koboSpan" id="kobo.73.1">            self.writer.add_summary(merge, epoch)</span><br/> <br/><span class="koboSpan" id="kobo.74.1">            if epoch % 10 == 0:</span><br/><span class="koboSpan" id="kobo.75.1">                print("Epoch {}/{} training loss: {} Validation loss {}".\</span><br/><span class="koboSpan" id="kobo.76.1">                    format(epoch,epochs,l, l_val ))</span><br/> <br/> <br/><span class="koboSpan" id="kobo.77.1">            epoch += 1</span><br/><span class="koboSpan" id="kobo.78.1">        return loss, loss_val</span><br/> <br/><span class="koboSpan" id="kobo.79.1">    def predict(self, X):</span><br/><span class="koboSpan" id="kobo.80.1">        return self.sess.run(self.y_hat, feed_dict={self.X: X})</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.81.1">Next, we read the data, normalize it, and preprocess it so that wine quality is one-hot encoded with two labels. </span><span class="koboSpan" id="kobo.81.2">We also divide the data into training and validation set, shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.82.1">filename = 'winequality-red.csv' </span><br/><span class="koboSpan" id="kobo.83.1">#Download the file from https://archive.ics.uci.edu/ml/datasets/wine+quality</span><br/><span class="koboSpan" id="kobo.84.1">df = pd.read_csv(filename, sep=';')</span><br/><span class="koboSpan" id="kobo.85.1">columns = df.columns.values</span><br/><span class="koboSpan" id="kobo.86.1"># Preprocessing and Categorizing wine into two categories</span><br/><span class="koboSpan" id="kobo.87.1">X, Y = df[columns[0:-1]], df[columns[-1]]</span><br/><span class="koboSpan" id="kobo.88.1">scaler = MinMaxScaler()</span><br/><span class="koboSpan" id="kobo.89.1">X_new = scaler.fit_transform(X)</span><br/><span class="koboSpan" id="kobo.90.1">#Y.loc[(Y&amp;amp;lt;3.5)]=3</span><br/><span class="koboSpan" id="kobo.91.1">Y.loc[(Y&amp;amp;lt;5.5) ] = 2</span><br/><span class="koboSpan" id="kobo.92.1">Y.loc[(Y&amp;amp;gt;=5.5)] = 1</span><br/><span class="koboSpan" id="kobo.93.1">Y_new = pd.get_dummies(Y) # One hot encode</span><br/><span class="koboSpan" id="kobo.94.1">X_train, X_val, Y_train, y_val = \</span><br/><span class="koboSpan" id="kobo.95.1"> train_test_split(X_new, Y_new, test_size=0.2, random_state=333)</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.96.1">We define an </span><kbd><span class="koboSpan" id="kobo.97.1">MLP</span></kbd><span class="koboSpan" id="kobo.98.1"> object and train it, demonstrated in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.99.1">_, d = X_train.shape</span><br/><span class="koboSpan" id="kobo.100.1">_, n = Y_train.shape</span><br/><span class="koboSpan" id="kobo.101.1">model = MLP(n_input=d, n_hidden=5, n_output=n)</span><br/><span class="koboSpan" id="kobo.102.1">loss, loss_val = model.train(X_train, Y_train, X_val, y_val, 10000)</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.103.1">Following, you can see the results of training, the cross-entropy loss decreases as the network learns:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.104.1">plt.plot(loss, label="Taining Loss")</span><br/><span class="koboSpan" id="kobo.105.1">plt.plot(loss_val, label="Validation Loss")</span><br/><span class="koboSpan" id="kobo.106.1">plt.legend()</span><br/><span class="koboSpan" id="kobo.107.1">plt.xlabel("Epochs")</span><br/><span class="koboSpan" id="kobo.108.1">plt.ylabel("Cross Entropy Loss")</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.109.1"><img class="aligncenter size-full wp-image-934 image-border" src="assets/83e8116c-41c8-4e3e-a3c8-d1fb3a3f30cb.png" style="width:28.25em;height:19.00em;"/></span></p>
<ol start="6">
<li><span class="koboSpan" id="kobo.110.1">The trained network, when tested on the validation dataset, provides an accuracy of 77.8%. </span><span class="koboSpan" id="kobo.110.2">The </span><kbd><span class="koboSpan" id="kobo.111.1">confusion_matrix</span></kbd><span class="koboSpan" id="kobo.112.1"> on the validation set is shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.113.1">from sklearn.metrics import confusion_matrix, accuracy_score</span><br/><span class="koboSpan" id="kobo.114.1">import seaborn as sns</span><br/><span class="koboSpan" id="kobo.115.1">cm = confusion_matrix(np.argmax(np.array(y_val),1), np.argmax(Y_pred,1))</span><br/><span class="koboSpan" id="kobo.116.1">sns.heatmap(cm,annot=True,fmt='2.0f')</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.117.1"><img class="aligncenter size-full wp-image-933 image-border" src="assets/a480aca2-4bdf-4a12-96cd-1916ece2f3aa.png" style="width:26.08em;height:18.67em;"/></span></p>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.118.1">These results are again comparable to the results we obtained using ML algorithms. </span><span class="koboSpan" id="kobo.118.2">We can make it even better by playing around with the hyperparameters.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Convolutional neural networks</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">MLPs were fun, but as you must have observed while playing with MLP codes in the previous section, the time to learn increases as the complexity of input space increases; moreover, the performance of MLPs is just second to the ML algorithms. </span><span class="koboSpan" id="kobo.2.2">Whatever you can do with MLP, there's a high probability you can do it slightly better using ML algorithms you learned in </span><a href="09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 3</span></a><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Machine Learning for IoT</span></em><span class="koboSpan" id="kobo.6.1">. </span><span class="koboSpan" id="kobo.6.2">Precisely for this reason, despite backpropagation algorithm being available in the 1980s, we observed the second AI winter roughly from 1987 to 1993.</span></p>
<p><span class="koboSpan" id="kobo.7.1">This all changed, and the neural networks stopped playing the second fiddle to ML algorithms, in the 2010s with the development of deep neural networks. </span><span class="koboSpan" id="kobo.7.2">Today DL has achieved human level or more than human level performance in varied tasks of computer vision like recognizing traffic signals (</span><a href="http://people.idsia.ch/~juergen/cvpr2012.pdf"><span class="koboSpan" id="kobo.8.1">http://people.idsia.ch/~juergen/cvpr2012.pdf</span></a><span class="koboSpan" id="kobo.9.1">), faces (</span><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf"><span class="koboSpan" id="kobo.10.1">https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf</span></a><span class="koboSpan" id="kobo.11.1">), handwritten digits, (</span><a href="https://cs.nyu.edu/~wanli/dropc/dropc.pdf"><span class="koboSpan" id="kobo.12.1">https://cs.nyu.edu/~wanli/dropc/dropc.pdf</span></a><span class="koboSpan" id="kobo.13.1">) and so on. </span><span class="koboSpan" id="kobo.13.2">The list is continuously growing.</span></p>
<p><span class="koboSpan" id="kobo.14.1">CNN has been a major part of this success story. </span><span class="koboSpan" id="kobo.14.2">In this section, you will learn about CNN, the maths behind CNN, and some of the popular CNN architectures.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Different layers of CNN</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">CNN consists of three main types of neuron layers: convolution layers, pooling layers, and fully connected layers. </span><span class="koboSpan" id="kobo.2.2">Fully connected layers are nothing but layers of MLP, they are always the last few layers of the CNN, and perform the final task of classification or regression. </span><span class="koboSpan" id="kobo.2.3">Let's see how the convolution layer and max pooling layers work.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The convolution layer</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">This is the core building block of CNNs. </span><span class="koboSpan" id="kobo.2.2">It performs the mathematical operation similar to convolution (cross-correlation to be precise) on its input, normally a 3D image. </span><span class="koboSpan" id="kobo.2.3">It's defined by kernels (filters). </span><span class="koboSpan" id="kobo.2.4">The basic idea is that these filters stride through the entire image and extract specific features from the image.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Before going into further details, let's first see the convolution operation on a two-dimensional matrix for simplicity. </span><span class="koboSpan" id="kobo.3.2">The following diagram shows the operation when one pixel placed at position [2, 2] of a 5×5 </span><strong><span class="koboSpan" id="kobo.4.1">2D image</span></strong><span class="koboSpan" id="kobo.5.1"> matrix is convolved with a 3×3 filter:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.6.1"><img class="aligncenter size-full wp-image-935 image-border" src="assets/73305444-5f38-4e55-b2e1-d901a0aacfc8.png" style="width:29.00em;height:41.17em;"/></span></div>
<div class="CDPAlignCenter packt_figref CDPAlign"><span class="koboSpan" id="kobo.7.1">Convolution operation at a single pixel</span></div>
<p><span class="koboSpan" id="kobo.8.1">The convolution operation involves placing the filter with the pixel at the center, then performing element-wise multiplication between the filter elements and the pixel, along with its neighbors. </span><span class="koboSpan" id="kobo.8.2">Finally, summing the product. </span><span class="koboSpan" id="kobo.8.3">Since convolution operation is performed on a pixel, the filters are conventionally odd-sized like 5×5, 3×3, or 7×7, and so on. </span><span class="koboSpan" id="kobo.8.4">The size of the filters specify how much neighboring area it's covering.</span></p>
<p><span class="koboSpan" id="kobo.9.1">The important parameters when designing the convolution layers are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.10.1">The size of the filters (k×k).</span></li>
<li><span class="koboSpan" id="kobo.11.1">The number of filters in the layer, also called </span><strong><span class="koboSpan" id="kobo.12.1">channels</span></strong><span class="koboSpan" id="kobo.13.1">. </span><span class="koboSpan" id="kobo.13.2">The input color image is present in the three RGB channels. </span><span class="koboSpan" id="kobo.13.3">The number of channels are conventionally increased in the higher layers. </span><span class="koboSpan" id="kobo.13.4">Resulting in deeper information in higher layers.</span></li>
<li><span class="koboSpan" id="kobo.14.1">The number of pixels the filter strides (s) through the image. </span><span class="koboSpan" id="kobo.14.2">Conventionally, the stride is of one pixel so that the filter covers the entire image starting from top-left to bottom-right.</span></li>
<li><span class="koboSpan" id="kobo.15.1">The padding to be used while convolving. </span><span class="koboSpan" id="kobo.15.2">Traditionally, there are two options, either valid or same. </span><span class="koboSpan" id="kobo.15.3">In </span><strong><span class="koboSpan" id="kobo.16.1">valid</span></strong><span class="koboSpan" id="kobo.17.1"> padding, there's no padding at all, and thus the size of the convolved image is less than that of the original. </span><span class="koboSpan" id="kobo.17.2">In </span><strong><span class="koboSpan" id="kobo.18.1">same</span></strong><span class="koboSpan" id="kobo.19.1">, the padding of zeros is done around the boundary pixels, so that the size of the convolved image is the same as that of the original image. </span><span class="koboSpan" id="kobo.19.2">The following screenshot shows the complete </span><strong><span class="koboSpan" id="kobo.20.1">Convolved Image</span></strong><span class="koboSpan" id="kobo.21.1">. </span><span class="koboSpan" id="kobo.21.2">The green square of size 3×3 is the result when padding is valid, the complete 5×5 matrix on the right will be the result when padding is the same:</span></li>
</ul>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.22.1"><img class="aligncenter size-full wp-image-1157 image-border" src="assets/180e138e-d181-4379-9469-50ec5fddde35.png" style="width:32.58em;height:20.33em;"/></span></div>
<div class="CDPAlignCenter CDPAlign"><span><span class="koboSpan" id="kobo.23.1">Convolution operation applied on a 5×5 image</span></span></div>
<p><span class="koboSpan" id="kobo.24.1">The green square on the right will be the result of </span><strong><span class="koboSpan" id="kobo.25.1">valid</span></strong><span class="koboSpan" id="kobo.26.1"> padding. </span><span class="koboSpan" id="kobo.26.2">For the </span><strong><span class="koboSpan" id="kobo.27.1">same</span></strong><span class="koboSpan" id="kobo.28.1"> padding, we will get the complete 5×5 matrix shown on the right-hand side.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Pooling layer</span></h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.2.1">The convolution layer is followed conventionally by a pooling layer. </span><span class="koboSpan" id="kobo.2.2">The purpose of the pooling layer is to progressively reduce the size of the representation, and thus, reduce the number of parameters and computations in the network. </span><span class="koboSpan" id="kobo.2.3">Thus, it down samples the information as it propagates through the network in feed forward manner.</span></p>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.3.1">Here again, we have a filter, traditionally people prefer a filter of size 2×2, and it moves with a stride of two pixels in both directions. </span><span class="koboSpan" id="kobo.3.2">The pooling process replaces the four elements under the 2×2 filter by either the maximum value of the four (</span><strong><span class="koboSpan" id="kobo.4.1">Max Pooling</span></strong><span class="koboSpan" id="kobo.5.1">) or the average value of the four (</span><strong><span class="koboSpan" id="kobo.6.1">Average Pooling</span></strong><span class="koboSpan" id="kobo.7.1">). </span><span class="koboSpan" id="kobo.7.2">In the following diagram, you can see the result of pooling operation on a </span><strong><span class="koboSpan" id="kobo.8.1">2D single channel slice of an image</span></strong><span class="koboSpan" id="kobo.9.1">:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.10.1"><img class="aligncenter size-full wp-image-937 image-border" src="assets/d703e932-41f1-4053-bd23-c56664cc31db.png" style="width:31.08em;height:23.08em;"/></span></p>
<div class="CDPAlignCenter packt_figref CDPAlign"><span class="koboSpan" id="kobo.11.1">Max pooling and average pooling operation on a two-dimensional single depth slice of an image</span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.12.1">Multiple convolution pooling layers are stacked together to form a deep CNN. </span><span class="koboSpan" id="kobo.12.2">As the image propagates through the CNN, each convolutional layer extracts specific features. </span><span class="koboSpan" id="kobo.12.3">The lower layers extract the gross feature like shape, curves, lines, and so on, while the higher layers extract more abstract features like eyes, lips, and so on. </span><span class="koboSpan" id="kobo.12.4">The image, as it propagates through the network, reduces in dimensions, but increases in depth. </span><span class="koboSpan" id="kobo.12.5">The output from the last convolutional layer is flattened and passed to fully connected layers, as shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.13.1"><img class="aligncenter size-full wp-image-938 image-border" src="assets/c80f02d4-6331-41f7-9fbb-df3ea4545697.png" style="width:21.17em;height:16.17em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="koboSpan" id="kobo.14.1">The basic architecture of a CNN network</span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.15.1">The values of filter matrix are also called </span><strong><span class="koboSpan" id="kobo.16.1">weights</span></strong><span class="koboSpan" id="kobo.17.1"> and they are shared by the whole image. </span><span class="koboSpan" id="kobo.17.2">This sharing reduces the number of training parameters. </span><span class="koboSpan" id="kobo.17.3">The weights are learned by the network using the backpropagation algorithm. </span><span class="koboSpan" id="kobo.17.4">Since we will be using the auto-differentiation feature of TensorFlow, we are not calculating the exact expression for weight update for convolution layers.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Some popular CNN model</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The following is a list of some of the popular CNN models available:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">LeNet</span></strong><span class="koboSpan" id="kobo.4.1">: LeNet was the first successful CNN applied to recognize handwritten digits. </span><span class="koboSpan" id="kobo.4.2">It was developed by Yann LeCun in the 1990s. </span><span class="koboSpan" id="kobo.4.3">You can know more about LeNet architecture and its related publications at Yann LeCun's home page (</span><a href="http://yann.lecun.com/exdb/lenet/"><span class="koboSpan" id="kobo.5.1">http://yann.lecun.com/exdb/lenet/</span></a><span class="koboSpan" id="kobo.6.1">).</span></li>
<li><strong><span class="koboSpan" id="kobo.7.1">VGGNet</span></strong><span class="koboSpan" id="kobo.8.1">: This was the runner-up in ILSVRC 2014, developed by Karen Simonyan and Andrew Zisserman. </span><span class="koboSpan" id="kobo.8.2">Its first version contains 16 Convolution+FC layers and was called </span><strong><span class="koboSpan" id="kobo.9.1">VGG16</span></strong><span class="koboSpan" id="kobo.10.1">, later they brought VGG19 with 19 layers. </span><span class="koboSpan" id="kobo.10.2">The details about its performance and publications can be accessed from the University of Oxford site (</span><a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/"><span class="koboSpan" id="kobo.11.1">http://www.robots.ox.ac.uk/~vgg/research/very_deep/</span></a><span class="koboSpan" id="kobo.12.1">).</span></li>
<li><strong><span class="koboSpan" id="kobo.13.1">ResNet</span></strong><span class="koboSpan" id="kobo.14.1">: Developed by Kaiming He et al., ResNet was the winner of ILSVRC 2015. </span><span class="koboSpan" id="kobo.14.2">It made use of new feature called </span><strong><span class="koboSpan" id="kobo.15.1">residual learning</span></strong><span class="koboSpan" id="kobo.16.1"> and </span><strong><span class="koboSpan" id="kobo.17.1">batch normalization</span></strong><span class="koboSpan" id="kobo.18.1">. </span><span class="koboSpan" id="kobo.18.2">It's a very deep network with more than 100 layers. </span><span class="koboSpan" id="kobo.18.3">It's known that adding more layers will improve the performance, but adding layers also introduced the problem of vanishing gradients. </span><span class="koboSpan" id="kobo.18.4">ResNet solved this issue by making use of identity shortcut connection, where the signal skips one or more layers. </span><span class="koboSpan" id="kobo.18.5">You can read the original paper for more information (</span><a href="https://arxiv.org/abs/1512.03385"><span class="koboSpan" id="kobo.19.1">https://arxiv.org/abs/1512.03385</span></a><span class="koboSpan" id="kobo.20.1">).</span></li>
<li><strong><span class="koboSpan" id="kobo.21.1">GoogleNet</span></strong><span class="koboSpan" id="kobo.22.1">: This was the winning architecture of ILSVRC 2014. </span><span class="koboSpan" id="kobo.22.2">It has 22 layers, and introduced the idea of inception layer. </span><span class="koboSpan" id="kobo.22.3">The basic idea is to cover a bigger area, while at the same time, keep a fine resolution for small information on the images. </span><span class="koboSpan" id="kobo.22.4">As a result instead of one size filters, at each layer, we have filter ranging from 1×1 (for fine detailing) to 5×5. </span><span class="koboSpan" id="kobo.22.5">The result of all the filters are concatenated and passed to next layer, the process is repeated in the next inception layer.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">LeNet to recognize handwritten digits</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the chapters ahead, we will be using some of these popular CNNs and their variants to solve image and video processing tasks. </span><span class="koboSpan" id="kobo.2.2">Right now, let's use the LeNet architecture proposed by Yann LeCun to recognize handwritten digits. </span><span class="koboSpan" id="kobo.2.3">This architecture was used by the US Postal Service to recognize handwritten ZIP codes on the letters they received (</span><a href="http://yann.lecun.com/exdb/publis/pdf/jackel-95.pdf"><span class="koboSpan" id="kobo.3.1">http://yann.lecun.com/exdb/publis/pdf/jackel-95.pdf</span></a><span class="koboSpan" id="kobo.4.1">).</span></p>
<p><span class="koboSpan" id="kobo.5.1">LeNet consists of five layers with two convolutional max pool layers and three fully connected layers. </span><span class="koboSpan" id="kobo.5.2">The network also uses dropout feature, that is while training, some of the weights are turned off. </span><span class="koboSpan" id="kobo.5.3">This forces the other interconnections to compensate for them, and hence helps in overcoming overfitting:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.6.1">We import the necessary modules, shown as follows</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.7.1"># Import Modules</span><br/><span class="koboSpan" id="kobo.8.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.9.1">import pandas as pd</span><br/><span class="koboSpan" id="kobo.10.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.11.1">%matplotlib inline</span></pre>
<p class="mce-root"/>
<ol start="2">
<li><span class="koboSpan" id="kobo.12.1">Next, we create the class object </span><kbd><span class="koboSpan" id="kobo.13.1">LeNet</span></kbd><span class="koboSpan" id="kobo.14.1">, which will have the necessary CNN architecture and modules to train and make the prediction. </span><span class="koboSpan" id="kobo.14.2">In the </span><kbd><span class="koboSpan" id="kobo.15.1">__init__</span></kbd><span class="koboSpan" id="kobo.16.1"> method, we define all the needed placeholders to hold input images and their output labels. </span><span class="koboSpan" id="kobo.16.2">We also define the loss, since this is a classification problem, we use cross-entropy loss, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.17.1"># Define your Architecture here</span><br/><span class="koboSpan" id="kobo.18.1">import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.19.1">from tensorflow.contrib.layers import flatten</span><br/><span class="koboSpan" id="kobo.20.1">class my_LeNet:</span><br/><span class="koboSpan" id="kobo.21.1">    def __init__(self, d, n, mu = 0, sigma = 0.1, lr = 0.001):</span><br/><span class="koboSpan" id="kobo.22.1">        self.mu = mu</span><br/><span class="koboSpan" id="kobo.23.1">        self.sigma = sigma</span><br/><span class="koboSpan" id="kobo.24.1">        self.n = n</span><br/><span class="koboSpan" id="kobo.25.1">        # place holder for input image dimension 28 x 28</span><br/><span class="koboSpan" id="kobo.26.1">        self.x = tf.placeholder(tf.float32, (None, d, d, 1)) </span><br/><span class="koboSpan" id="kobo.27.1">        self.y = tf.placeholder(tf.int32, (None,n))</span><br/><span class="koboSpan" id="kobo.28.1">        self.keep_prob = tf.placeholder(tf.float32) # probability to keep units</span><br/> <br/> <br/><span class="koboSpan" id="kobo.29.1">        self.logits = self.model(self.x)</span><br/><span class="koboSpan" id="kobo.30.1">        # Define the loss function</span><br/><span class="koboSpan" id="kobo.31.1">        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.y,\</span><br/><span class="koboSpan" id="kobo.32.1">                        logits=self.logits)</span><br/><span class="koboSpan" id="kobo.33.1">        self.loss = tf.reduce_mean(cross_entropy)</span><br/><span class="koboSpan" id="kobo.34.1">        optimizer = tf.train.AdamOptimizer(learning_rate = lr)</span><br/><span class="koboSpan" id="kobo.35.1">        self.train = optimizer.minimize(self.loss)</span><br/><span class="koboSpan" id="kobo.36.1">        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.y, 1))</span><br/><span class="koboSpan" id="kobo.37.1">        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br/><span class="koboSpan" id="kobo.38.1">        init = tf.global_variables_initializer()</span><br/><span class="koboSpan" id="kobo.39.1">        self.sess = tf.Session()</span><br/><span class="koboSpan" id="kobo.40.1">        self.sess.run(init)</span><br/><span class="koboSpan" id="kobo.41.1">        self.saver = tf.train.Saver()</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li><span class="koboSpan" id="kobo.42.1">The </span><kbd><span class="koboSpan" id="kobo.43.1">model</span></kbd><span class="koboSpan" id="kobo.44.1"> method is the one where the convolutional network architecture graph is actually build. </span><span class="koboSpan" id="kobo.44.2">We use the TensorFlow </span><kbd><span class="koboSpan" id="kobo.45.1">tf.nn.conv2d</span></kbd><span class="koboSpan" id="kobo.46.1"> function to build the convolutional layers. </span><span class="koboSpan" id="kobo.46.2">The function takes an argument the filter matrix defined as weights and computes the convolution between the input and the filter matrix. </span><span class="koboSpan" id="kobo.46.3">We also use biases to give us a high degree of freedom. </span><span class="koboSpan" id="kobo.46.4">After the two convolution layers, we flatten the output and pass it to the fully connected layers, shown as follows:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.47.1">def model(self,x):</span><br/><span class="koboSpan" id="kobo.48.1">    # Build Architecture</span><br/><span class="koboSpan" id="kobo.49.1">    keep_prob = 0.7</span><br/><span class="koboSpan" id="kobo.50.1">    # Layer 1: Convolutional. </span><span class="koboSpan" id="kobo.50.2">Filter 5x5 num_filters = 6 Input_depth =1</span><br/><span class="koboSpan" id="kobo.51.1">    conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean \</span><br/><span class="koboSpan" id="kobo.52.1">                    = self.mu, stddev = self.sigma))</span><br/><span class="koboSpan" id="kobo.53.1">    conv1_b = tf.Variable(tf.zeros(6))</span><br/><span class="koboSpan" id="kobo.54.1">    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b</span><br/><span class="koboSpan" id="kobo.55.1">    conv1 = tf.nn.relu(conv1)</span><br/> <br/><span class="koboSpan" id="kobo.56.1">    # Max Pool 1</span><br/><span class="koboSpan" id="kobo.57.1">    self.conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1],\</span><br/><span class="koboSpan" id="kobo.58.1">                     strides=[1, 2, 2, 1], padding='VALID')</span><br/> <br/> <br/><span class="koboSpan" id="kobo.59.1">    # Layer 2: Convolutional. </span><span class="koboSpan" id="kobo.59.2">Filter 5x5 num_filters = 16 Input_depth =6</span><br/><span class="koboSpan" id="kobo.60.1">    conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), \</span><br/><span class="koboSpan" id="kobo.61.1">                    mean = self.mu, stddev = self.sigma))</span><br/><span class="koboSpan" id="kobo.62.1">    conv2_b = tf.Variable(tf.zeros(16))</span><br/><span class="koboSpan" id="kobo.63.1">    conv2 = tf.nn.conv2d(self.conv1, conv2_W, strides=[1, 1, 1, 1],\</span><br/><span class="koboSpan" id="kobo.64.1">                     padding='VALID') + conv2_b</span><br/><span class="koboSpan" id="kobo.65.1">    conv2 = tf.nn.relu(conv2)</span><br/><br/><span class="koboSpan" id="kobo.66.1">    # Max Pool 2.</span><br/><span class="koboSpan" id="kobo.67.1">    self.conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], \</span><br/><span class="koboSpan" id="kobo.68.1">                    strides=[1, 2, 2, 1], padding='VALID')</span><br/> <br/><span class="koboSpan" id="kobo.69.1">    # Flatten.</span><br/><span class="koboSpan" id="kobo.70.1">    fc0 = flatten(self.conv2)</span><br/><span class="koboSpan" id="kobo.71.1">    print("x shape:",fc0.get_shape())</span><br/> <br/><span class="koboSpan" id="kobo.72.1">    # Layer 3: Fully Connected. </span><span class="koboSpan" id="kobo.72.2">Input = fc0.get_shape[-1]. </span><span class="koboSpan" id="kobo.72.3">Output = 120.</span><br/><span class="koboSpan" id="kobo.73.1">    fc1_W = tf.Variable(tf.truncated_normal(shape=(256, 120), \</span><br/><span class="koboSpan" id="kobo.74.1">                mean = self.mu, stddev = self.sigma))</span><br/><span class="koboSpan" id="kobo.75.1">    fc1_b = tf.Variable(tf.zeros(120))</span><br/><span class="koboSpan" id="kobo.76.1">    fc1 = tf.matmul(fc0, fc1_W) + fc1_b</span><br/><span class="koboSpan" id="kobo.77.1">    fc1 = tf.nn.relu(fc1)</span><br/> <br/><span class="koboSpan" id="kobo.78.1">    # Dropout</span><br/><span class="koboSpan" id="kobo.79.1">    x = tf.nn.dropout(fc1, keep_prob)</span><br/><br/><span class="koboSpan" id="kobo.80.1">    # Layer 4: Fully Connected. </span><span class="koboSpan" id="kobo.80.2">Input = 120. </span><span class="koboSpan" id="kobo.80.3">Output = 84.</span><br/><span class="koboSpan" id="kobo.81.1">    fc2_W = tf.Variable(tf.truncated_normal(shape=(120, 84), \</span><br/><span class="koboSpan" id="kobo.82.1">                    mean = self.mu, stddev = self.sigma))</span><br/><span class="koboSpan" id="kobo.83.1">    fc2_b = tf.Variable(tf.zeros(84))</span><br/><span class="koboSpan" id="kobo.84.1">    fc2 = tf.matmul(x, fc2_W) + fc2_b</span><br/><span class="koboSpan" id="kobo.85.1">    fc2 = tf.nn.relu(fc2)</span><br/> <br/><span class="koboSpan" id="kobo.86.1">    # Dropout</span><br/><span class="koboSpan" id="kobo.87.1">    x = tf.nn.dropout(fc2, keep_prob)</span><br/><br/><span class="koboSpan" id="kobo.88.1">    # Layer 6: Fully Connected. </span><span class="koboSpan" id="kobo.88.2">Input = 120. </span><span class="koboSpan" id="kobo.88.3">Output = n_classes.</span><br/><span class="koboSpan" id="kobo.89.1">    fc3_W = tf.Variable(tf.truncated_normal(shape=(84, self.n), \</span><br/><span class="koboSpan" id="kobo.90.1">                    mean = self.mu, stddev = self.sigma))</span><br/><span class="koboSpan" id="kobo.91.1">    fc3_b = tf.Variable(tf.zeros(self.n))</span><br/><span class="koboSpan" id="kobo.92.1">    logits = tf.matmul(x, fc3_W) + fc3_b</span><br/><span class="koboSpan" id="kobo.93.1">    #logits = tf.nn.softmax(logits)</span><br/><span class="koboSpan" id="kobo.94.1">    return logits</span></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.95.1">The </span><kbd><span class="koboSpan" id="kobo.96.1">fit</span></kbd><span class="koboSpan" id="kobo.97.1"> method performs the batch-wise training, and </span><kbd><span class="koboSpan" id="kobo.98.1">predict</span></kbd><span class="koboSpan" id="kobo.99.1"> method provides the output for given input, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.100.1">def fit(self,X,Y,X_val,Y_val,epochs=10, batch_size=100):</span><br/><span class="koboSpan" id="kobo.101.1">    X_train, y_train = X, Y</span><br/><span class="koboSpan" id="kobo.102.1">    num_examples = len(X_train)</span><br/><span class="koboSpan" id="kobo.103.1">    l = []</span><br/><span class="koboSpan" id="kobo.104.1">    val_l = []</span><br/><span class="koboSpan" id="kobo.105.1">    max_val = 0</span><br/><span class="koboSpan" id="kobo.106.1">    for i in range(epochs):</span><br/><span class="koboSpan" id="kobo.107.1">        total = 0</span><br/><span class="koboSpan" id="kobo.108.1">        for offset in range(0, num_examples, batch_size): # Learn Batch wise</span><br/><span class="koboSpan" id="kobo.109.1">            end = offset + batch_size</span><br/><span class="koboSpan" id="kobo.110.1">            batch_x, batch_y = X_train[offset:end], y_train[offset:end]</span><br/><span class="koboSpan" id="kobo.111.1">            _, loss = self.sess.run([self.train,self.loss], \</span><br/><span class="koboSpan" id="kobo.112.1">                        feed_dict={self.x: batch_x, self.y: batch_y})</span><br/><span class="koboSpan" id="kobo.113.1">            total += loss</span><br/><span class="koboSpan" id="kobo.114.1">            l.append(total/num_examples)</span><br/><span class="koboSpan" id="kobo.115.1">            accuracy_val = self.sess.run(self.accuracy, \</span><br/><span class="koboSpan" id="kobo.116.1">                                feed_dict={self.x: X_val, self.y: Y_val})</span><br/><span class="koboSpan" id="kobo.117.1">            accuracy = self.sess.run(self.accuracy, feed_dict={self.x: X, self.y: Y})</span><br/><span class="koboSpan" id="kobo.118.1">            loss_val = self.sess.run(self.loss, feed_dict={self.x:X_val,self.y:Y_val})</span><br/><span class="koboSpan" id="kobo.119.1">            val_l.append(loss_val)</span><br/><span class="koboSpan" id="kobo.120.1">            print("EPOCH {}/{} loss is {:.3f} training_accuracy {:.3f} and \</span><br/><span class="koboSpan" id="kobo.121.1">                        validation accuracy is {:.3f}".\</span><br/><span class="koboSpan" id="kobo.122.1">                        format(i+1,epochs,total/num_examples, accuracy, accuracy_val))</span><br/><span class="koboSpan" id="kobo.123.1">            # Saving the model with best validation accuracy</span><br/><span class="koboSpan" id="kobo.124.1">            if accuracy_val &amp;amp;gt; max_val:</span><br/><span class="koboSpan" id="kobo.125.1">                save_path = self.saver.save(self.sess, "/tmp/lenet1.ckpt")</span><br/><span class="koboSpan" id="kobo.126.1">                print("Model saved in path: %s" % save_path)</span><br/><span class="koboSpan" id="kobo.127.1">                max_val = accuracy_val</span><br/><br/><span class="koboSpan" id="kobo.128.1">    #Restore the best model</span><br/><span class="koboSpan" id="kobo.129.1">    self.saver.restore(self.sess, "/tmp/lenet1.ckpt")</span><br/><span class="koboSpan" id="kobo.130.1">    print("Restored model with highest validation accuracy")</span><br/><span class="koboSpan" id="kobo.131.1">    accuracy_val = self.sess.run(self.accuracy, feed_dict={self.x: X_val, self.y: Y_val})</span><br/><span class="koboSpan" id="kobo.132.1">    accuracy = self.sess.run(self.accuracy, feed_dict={self.x: X, self.y: Y})</span><br/><span class="koboSpan" id="kobo.133.1">    return l,val_l, accuracy, accuracy_val</span><br/> <br/><span class="koboSpan" id="kobo.134.1">def predict(self, X):</span><br/><span class="koboSpan" id="kobo.135.1">    return self.sess.run(self.logits,feed_dict={self.x:X})</span></pre>
<ol start="5">
<li><span class="koboSpan" id="kobo.136.1">We use the handwritten digits dataset and download it from Kaggle (</span><a href="https://www.kaggle.com/c/digit-recognizer/data"><span class="koboSpan" id="kobo.137.1">https://www.kaggle.com/c/digit-recognizer/data</span></a><span class="koboSpan" id="kobo.138.1">). </span><span class="koboSpan" id="kobo.138.2">The dataset is available in </span><kbd><span class="koboSpan" id="kobo.139.1">.csv</span></kbd><span class="koboSpan" id="kobo.140.1"> format. </span><span class="koboSpan" id="kobo.140.2">We load the </span><kbd><span class="koboSpan" id="kobo.141.1">.csv</span></kbd><span class="koboSpan" id="kobo.142.1"> files and preprocess the data. </span><span class="koboSpan" id="kobo.142.2">The following are the sample training diagrams:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.143.1">def load_data():</span><br/><span class="koboSpan" id="kobo.144.1">    # Read the data and create train, validation and test dataset</span><br/><span class="koboSpan" id="kobo.145.1">    data = pd.read_csv('train.csv')</span><br/><span class="koboSpan" id="kobo.146.1">    # This ensures always 80% of data is training and </span><br/><span class="koboSpan" id="kobo.147.1">    # rest Validation unlike using np.random</span><br/><span class="koboSpan" id="kobo.148.1">    train = data.sample(frac=0.8, random_state=255) </span><br/><span class="koboSpan" id="kobo.149.1">    val = data.drop(train.index)</span><br/><span class="koboSpan" id="kobo.150.1">    test = pd.read_csv('test.csv')</span><br/><span class="koboSpan" id="kobo.151.1">    return train, val, test</span><br/> <br/><span class="koboSpan" id="kobo.152.1">def create_data(df):</span><br/><span class="koboSpan" id="kobo.153.1">    labels = df.loc[:]['label']</span><br/><span class="koboSpan" id="kobo.154.1">    y_one_hot = pd.get_dummies(labels).astype(np.uint8)</span><br/><span class="koboSpan" id="kobo.155.1">    y = y_one_hot.values # One Hot encode the labels</span><br/><span class="koboSpan" id="kobo.156.1">    x = df.iloc[:,1:].values</span><br/><span class="koboSpan" id="kobo.157.1">    x = x.astype(np.float)</span><br/><span class="koboSpan" id="kobo.158.1">    # Normalize data</span><br/><span class="koboSpan" id="kobo.159.1">    x = np.multiply(x, 1.0 / 255.0)</span><br/><span class="koboSpan" id="kobo.160.1">    x = x.reshape(-1, 28, 28, 1) # return each images as 96 x 96 x 1</span><br/><span class="koboSpan" id="kobo.161.1">    return x,y</span><br/><br/><span class="koboSpan" id="kobo.162.1">train, val, test = load_data()</span><br/><span class="koboSpan" id="kobo.163.1">X_train, y_train = create_data(train)</span><br/><span class="koboSpan" id="kobo.164.1">X_val, y_val = create_data(val)</span><br/><span class="koboSpan" id="kobo.165.1">X_test = (test.iloc[:,:].values).astype(np.float)</span><br/><span class="koboSpan" id="kobo.166.1">X_test = np.multiply(X_test, 1.0 / 255.0)</span><br/><span class="koboSpan" id="kobo.167.1">X_test = X_test.reshape(-1, 28, 28, 1) # return each images as 96 x 96 x 1</span><br/><br/><span class="koboSpan" id="kobo.168.1"># Plot a subset of training data</span><br/><span class="koboSpan" id="kobo.169.1">x_train_subset = X_train[:12]</span><br/> <br/> <br/><span class="koboSpan" id="kobo.170.1"># visualize subset of training data</span><br/><span class="koboSpan" id="kobo.171.1">fig = plt.figure(figsize=(20,2))</span><br/><span class="koboSpan" id="kobo.172.1">for i in range(0, len(x_train_subset)):</span><br/><span class="koboSpan" id="kobo.173.1">    ax = fig.add_subplot(1, 12, i+1)</span><br/><span class="koboSpan" id="kobo.174.1">    ax.imshow(x_train_subset[i].reshape(28,28), cmap='gray')</span><br/><span class="koboSpan" id="kobo.175.1">fig.suptitle('Subset of Original Training Images', fontsize=20)</span><br/><span class="koboSpan" id="kobo.176.1">plt.show()</span></pre>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.177.1"><img class="aligncenter size-full wp-image-1140 image-border" src="assets/1c22399b-ebc8-4748-a2d9-961e33840c5f.png" style="width:53.75em;height:6.50em;"/></span></p>
<p style="padding-left: 60px"><span class="koboSpan" id="kobo.178.1">Here we will be training the model:</span></p>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.179.1">n_train = len(X_train)</span><br/><span class="koboSpan" id="kobo.180.1"># Number of validation examples</span><br/><span class="koboSpan" id="kobo.181.1">n_validation = len(X_val)</span><br/> <br/><span class="koboSpan" id="kobo.182.1"># Number of testing examples.</span><br/><span class="koboSpan" id="kobo.183.1">n_test = len(X_test)</span><br/> <br/><span class="koboSpan" id="kobo.184.1"># What's the shape of an handwritten digits?</span><br/><span class="koboSpan" id="kobo.185.1">image_shape = X_train.shape[1:-1]</span><br/> <br/><span class="koboSpan" id="kobo.186.1"># How many unique classes/labels there are in the dataset.</span><br/><span class="koboSpan" id="kobo.187.1">n_classes = y_train.shape[-1]</span><br/><span class="koboSpan" id="kobo.188.1">print("Number of training examples =", n_train)</span><br/><span class="koboSpan" id="kobo.189.1">print("Number of Validation examples =", n_validation)</span><br/><span class="koboSpan" id="kobo.190.1">print("Number of testing examples =", n_test)</span><br/><span class="koboSpan" id="kobo.191.1">print("Image data shape =", image_shape)</span><br/><span class="koboSpan" id="kobo.192.1">print("Number of classes =", n_classes)</span><br/><br/><span class="koboSpan" id="kobo.193.1"># The result</span><br/><span class="koboSpan" id="kobo.194.1">## &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; Number of training examples = 33600</span><br/><span class="koboSpan" id="kobo.195.1">## &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; Number of Validation examples = 8400 </span><br/><span class="koboSpan" id="kobo.196.1">## &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; Number of testing examples = 28000 </span><br/><span class="koboSpan" id="kobo.197.1">## &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; Image data shape = (28, 28) </span><br/><span class="koboSpan" id="kobo.198.1">## &amp;amp;gt;&amp;amp;gt;&amp;amp;gt; Number of classes = 10</span><br/><br/><span class="koboSpan" id="kobo.199.1"># Define the data values</span><br/><span class="koboSpan" id="kobo.200.1">d = image_shape[0]</span><br/><span class="koboSpan" id="kobo.201.1">n = n_classes</span><br/><span class="koboSpan" id="kobo.202.1">from sklearn.utils import shuffle</span><br/><span class="koboSpan" id="kobo.203.1">X_train, y_train = shuffle(X_train,y_train)</span></pre>
<ol start="6">
<li><span class="koboSpan" id="kobo.204.1">We create the </span><kbd><span class="koboSpan" id="kobo.205.1">LeNet</span></kbd><span class="koboSpan" id="kobo.206.1"> object and train it on the training data. </span><span class="koboSpan" id="kobo.206.2">The obtain is 99.658% on the training dataset and 98.607% on the validation dataset:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.207.1"># Create the Model</span><br/><span class="koboSpan" id="kobo.208.1">my_model = my_LeNet(d, n)</span><br/><br/><span class="koboSpan" id="kobo.209.1">### Train model  here.</span><br/><span class="koboSpan" id="kobo.210.1">loss, val_loss, train_acc, val_acc = my_model.fit(X_train, y_train, \</span><br/><span class="koboSpan" id="kobo.211.1">    X_val, y_val, epochs=50) </span></pre>
<p><span><span class="koboSpan" id="kobo.212.1">Impressive! </span><span class="koboSpan" id="kobo.212.2">You can predict the output for the test dataset and make a submission at Kaggle.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Recurrent neural networks</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The models that we have studied till now respond only present input. </span><span class="koboSpan" id="kobo.2.2">You present them an input, and based on what they have learned, they give you a corresponding output. </span><span class="koboSpan" id="kobo.2.3">But this is not the way we humans work. </span><span class="koboSpan" id="kobo.2.4">When you are reading a sentence, you do not interpret each word individually, you take the previous words into account to conclude its semantic</span><br/><span class="koboSpan" id="kobo.3.1">
meaning.</span></p>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.4.1">RNNs are able to address this issue. </span><span class="koboSpan" id="kobo.4.2">They use the feedback loops, which preserves the information. </span><span class="koboSpan" id="kobo.4.3">The feedback loop allows the information to be passed from the previous steps to the present. </span><span class="koboSpan" id="kobo.4.4">The following diagram shows the basic architecture of an RNN and how the feedback allows the passing of information from one step of the network to the next (</span><strong><span class="koboSpan" id="kobo.5.1">Unroll</span></strong><span class="koboSpan" id="kobo.6.1">):</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1"><img class="aligncenter size-full wp-image-942 image-border" src="assets/3053a864-2e59-4b74-af9e-b0baa5861d37.png" style="width:39.42em;height:27.00em;"/></span></div>
<div class="CDPAlignCenter packt_figref CDPAlign"><span class="koboSpan" id="kobo.8.1">Recurrent neural network</span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.9.1">In the preceding diagram, </span><em><span class="koboSpan" id="kobo.10.1">X</span></em><span class="koboSpan" id="kobo.11.1"> represents the inputs. </span><span class="koboSpan" id="kobo.11.2">It's connected to the neurons in the hidden layer by weights </span><em><span class="koboSpan" id="kobo.12.1">W</span><sub><span class="koboSpan" id="kobo.13.1">hx</span></sub></em><span class="koboSpan" id="kobo.14.1">, the output of the hidden layer, </span><em><span class="koboSpan" id="kobo.15.1">h</span></em><span class="koboSpan" id="kobo.16.1">, is fed back to the hidden layer via weights </span><em><span class="koboSpan" id="kobo.17.1">W</span><sub><span class="koboSpan" id="kobo.18.1">hh</span></sub></em><span class="koboSpan" id="kobo.19.1">, and also contributes to the output, </span><em><span class="koboSpan" id="kobo.20.1">O,</span></em><span class="koboSpan" id="kobo.21.1"> via weights </span><em><span class="koboSpan" id="kobo.22.1">W</span><sub><span class="koboSpan" id="kobo.23.1">yh</span></sub></em><span class="koboSpan" id="kobo.24.1">. </span><span class="koboSpan" id="kobo.24.2">We can write the mathematical relationships as the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.25.1"><img class="fm-editor-equation" src="assets/08a55795-65bb-469e-9035-07107aa07d3d.png" style="width:16.17em;height:1.42em;"/></span></div>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.26.1"><img class="fm-editor-equation" src="assets/2dfa68b4-ad21-402e-ad0a-dc306c307622.png" style="width:8.08em;height:1.50em;"/></span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.27.1">Where </span><em><span class="koboSpan" id="kobo.28.1">g</span></em><span class="koboSpan" id="kobo.29.1"> is the activation function, </span><em><span class="koboSpan" id="kobo.30.1">b</span><sub><span class="koboSpan" id="kobo.31.1">h</span></sub></em><span class="koboSpan" id="kobo.32.1"> and </span><em><span class="koboSpan" id="kobo.33.1">b</span><sub><span class="koboSpan" id="kobo.34.1">y</span></sub></em><span class="koboSpan" id="kobo.35.1"> are the biases of hidden and output neurons, respectively. </span><span class="koboSpan" id="kobo.35.2">In the a preceding relation all </span><em><span class="koboSpan" id="kobo.36.1">X</span></em><span class="koboSpan" id="kobo.37.1">, </span><em><span class="koboSpan" id="kobo.38.1">h</span></em><span class="koboSpan" id="kobo.39.1">, and </span><em><span class="koboSpan" id="kobo.40.1">O</span></em><span class="koboSpan" id="kobo.41.1"> are vectors; </span><em><span class="koboSpan" id="kobo.42.1">W</span><sub><span class="koboSpan" id="kobo.43.1">hx</span></sub></em><span class="koboSpan" id="kobo.44.1">, </span><em><span class="koboSpan" id="kobo.45.1">W</span><sub><span class="koboSpan" id="kobo.46.1">hh</span></sub><span class="koboSpan" id="kobo.47.1">,</span></em><span class="koboSpan" id="kobo.48.1"> and </span><em><span class="koboSpan" id="kobo.49.1">W</span><sub><span class="koboSpan" id="kobo.50.1">yh</span></sub></em><span class="koboSpan" id="kobo.51.1"> are matrices. </span><span class="koboSpan" id="kobo.51.2">The dimensions of the input </span><em><span class="koboSpan" id="kobo.52.1">X</span></em><span class="koboSpan" id="kobo.53.1"> and the output </span><em><span class="koboSpan" id="kobo.54.1">O</span></em><span class="koboSpan" id="kobo.55.1"> depends upon the dataset you are working on, and the number of units in hidden layer </span><em><span class="koboSpan" id="kobo.56.1">h</span></em><span class="koboSpan" id="kobo.57.1"> are decided by you; you will find many papers where researchers have used 128 number of hidden units. </span><span class="koboSpan" id="kobo.57.2">The preceding architecture shows only one hidden layer, but we can have as many hidden layers as we want. </span><span class="koboSpan" id="kobo.57.3">RNNs have been applied in the field of natural language processing, they have also been applied to analyze the time series data, like stock prices.</span></p>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.58.1">RNNs learn via an algorithm called as </span><strong><span class="koboSpan" id="kobo.59.1">backpropagation</span></strong> <strong><span class="koboSpan" id="kobo.60.1">through time</span></strong><span class="koboSpan" id="kobo.61.1"> (</span><strong><span class="koboSpan" id="kobo.62.1">BPTT</span></strong><span class="koboSpan" id="kobo.63.1">), it's a modification of backpropagation algorithm that takes into account the time series nature of data. </span><span class="koboSpan" id="kobo.63.2">Here, the loss is defined as the sum of all the loss functions at times </span><em><span class="koboSpan" id="kobo.64.1">t</span></em><span class="koboSpan" id="kobo.65.1">=</span><em><span class="koboSpan" id="kobo.66.1">1</span></em><span class="koboSpan" id="kobo.67.1"> to </span><em><span class="koboSpan" id="kobo.68.1">t</span></em><span class="koboSpan" id="kobo.69.1">=</span><em><span class="koboSpan" id="kobo.70.1">T</span></em><span class="koboSpan" id="kobo.71.1"> (number of time steps to be unrolled), for example:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.72.1"><img class="fm-editor-equation" src="assets/5edc1524-7b79-44e4-8ef6-097685d2548d.png" style="width:6.17em;height:3.67em;"/></span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.73.1">Where </span><em><span class="koboSpan" id="kobo.74.1">L</span><sup><span class="koboSpan" id="kobo.75.1">(t)</span></sup></em><span class="koboSpan" id="kobo.76.1"> is the loss at time </span><em><span class="koboSpan" id="kobo.77.1">t</span></em><span class="koboSpan" id="kobo.78.1">, we apply the chain rule of differentiation like before, and derive the weight updates for weights </span><em><span class="koboSpan" id="kobo.79.1">W</span><sub><span class="koboSpan" id="kobo.80.1">hx</span></sub></em><span><span class="koboSpan" id="kobo.81.1">,</span></span> <em><span class="koboSpan" id="kobo.82.1">W</span><sub><span class="koboSpan" id="kobo.83.1">hh</span></sub></em><em><span class="koboSpan" id="kobo.84.1">,</span></em> <span><span class="koboSpan" id="kobo.85.1">and</span></span> <em><span class="koboSpan" id="kobo.86.1">W</span><sub><span class="koboSpan" id="kobo.87.1">yh.</span></sub></em></p>
<div class="packt_infobox CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.88.1">We are not deriving the expression for weight updates in this book, because we will not be coding it. </span><span class="koboSpan" id="kobo.88.2">TensorFlow provides an implementation for RNN and BPTT. </span><span class="koboSpan" id="kobo.88.3">But for the readers interested in going into the mathematical details, following are some references:</span><br/>
<ul>
<li><em><span class="koboSpan" id="kobo.89.1">On the difficulty of training Recurrent Neural Networks,</span></em><span class="koboSpan" id="kobo.90.1"> Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio (</span><a href="https://arxiv.org/pdf/1211.5063.pdf"><span class="koboSpan" id="kobo.91.1">https://arxiv.org/pdf/1211.5063.pdf</span></a><span class="koboSpan" id="kobo.92.1">)</span></li>
<li><em><span class="koboSpan" id="kobo.93.1">Learning Long-Term Dependencies with Gradient Descent is Difficult</span></em><span class="koboSpan" id="kobo.94.1">, Yoshua Bengio, Patrice Simard, and Paolo Frasconi (</span><a href="http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf"><span class="koboSpan" id="kobo.95.1">www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf</span></a><span class="koboSpan" id="kobo.96.1">)</span></li>
<li><span class="koboSpan" id="kobo.97.1">Also, it will be incomplete not to mention Colah's blog (</span><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"><span class="koboSpan" id="kobo.98.1">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</span></a><span class="koboSpan" id="kobo.99.1">) and Andrej Karpathy's blog (</span><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"><span class="koboSpan" id="kobo.100.1">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</span></a><span class="koboSpan" id="kobo.101.1">) for an excellent explanation of RNNs and some of their cool applications</span></li>
</ul>
</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.102.1">We present the RNN with one input each timestep and predict the corresponding output. </span><span class="koboSpan" id="kobo.102.2">BPTT works by unrolling all input timesteps. </span><span class="koboSpan" id="kobo.102.3">The errors are calculated and accumulated for each timestep, later the network is rolled back to update the weights. </span><span class="koboSpan" id="kobo.102.4">One of the disadvantages of BPTT is that when the number of time steps increases, the computation also increases. </span><span class="koboSpan" id="kobo.102.5">This makes the overall model computationally expensive. </span><span class="koboSpan" id="kobo.102.6">Moreover, due to multiple gradient multiplications, the network is prone to the vanishing gradient problem.</span></p>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.103.1">To solve this issue, a modified version of BPTT, the truncated-BPTT is often used. </span><span class="koboSpan" id="kobo.103.2">In the truncated-BPTT, the data is processed one timestep at a time and the BPTT weight update is performed periodically for a fixed number of time steps.</span></p>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.104.1">We can enumerate the steps of the truncated-BPTT algorithm as follows:</span></p>
<ol>
<li class="CDPAlignLeft CDPAlign"><span><span class="koboSpan" id="kobo.105.1">Present the sequence of</span></span> <em><span class="koboSpan" id="kobo.106.1">K</span><sub><span class="koboSpan" id="kobo.107.1">1</span></sub></em> <span><span class="koboSpan" id="kobo.108.1">time steps of input and output pairs to the network</span></span></li>
<li class="CDPAlignLeft CDPAlign"><span><span class="koboSpan" id="kobo.109.1">Calculate and accumulate the errors across</span></span> <em><span class="koboSpan" id="kobo.110.1">K</span><sub><span class="koboSpan" id="kobo.111.1">2</span></sub></em> <span><span class="koboSpan" id="kobo.112.1">time steps by unrolling the network</span></span></li>
<li class="CDPAlignLeft CDPAlign"><span><span class="koboSpan" id="kobo.113.1">Update the weights by rolling up the network</span></span></li>
</ol>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.114.1">The performance of the algorithm depends on two hyperparameters </span><em><span class="koboSpan" id="kobo.115.1">K</span><sub><span class="koboSpan" id="kobo.116.1">1</span></sub></em><span class="koboSpan" id="kobo.117.1"> and </span><em><span class="koboSpan" id="kobo.118.1">K</span><sub><span class="koboSpan" id="kobo.119.1">2</span></sub></em><span class="koboSpan" id="kobo.120.1">. </span><span class="koboSpan" id="kobo.120.2">The number of forwarding pass timesteps between updates is represented by </span><em><span class="koboSpan" id="kobo.121.1">K</span><sub><span class="koboSpan" id="kobo.122.1">1</span></sub></em><span class="koboSpan" id="kobo.123.1">, it affects how fast or slow the training will be training and the frequency of the weight updates. </span><em><span class="koboSpan" id="kobo.124.1">K</span><sub><span class="koboSpan" id="kobo.125.1">2</span></sub></em><span class="koboSpan" id="kobo.126.1"> on the other hand, represents the number of timesteps that apply to BPTT, it should be large enough to capture the temporal structure of the input data.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Long short-term memory</span></h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.2.1">Hochreiter and Schmidhuber in 1997 proposed a modified RNN model, called the </span><strong><span class="koboSpan" id="kobo.3.1">long short-term memory</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">LSTM</span></strong><span class="koboSpan" id="kobo.6.1">) as a solution to overcome the vanishing gradient problem. </span><span class="koboSpan" id="kobo.6.2">The hidden layer in the RNNs is replaced by an LSTM cell.</span><br/><span class="koboSpan" id="kobo.7.1">
The LSTM cell consists of three gates: forget gate, input gate, and the output gate. </span><span class="koboSpan" id="kobo.7.2">These gates control the amount of long-term memory and the short-term memory generated and retained by the cell. </span><span class="koboSpan" id="kobo.7.3">The gates all have the </span><kbd><span class="koboSpan" id="kobo.8.1">sigmoid</span></kbd><span class="koboSpan" id="kobo.9.1"> function, which squashes the input between </span><em><span class="koboSpan" id="kobo.10.1">0</span></em><span class="koboSpan" id="kobo.11.1"> and </span><em><span class="koboSpan" id="kobo.12.1">1</span></em><span class="koboSpan" id="kobo.13.1">. </span><span class="koboSpan" id="kobo.13.2">Following, we see how the outputs from various gates are calculated, in case the expressions seem daunting to you, do not worry, we will be using the TensorFlow </span><kbd><span class="koboSpan" id="kobo.14.1">tf.contrib.rnn.BasicLSTMCell</span></kbd><span class="koboSpan" id="kobo.15.1"> and </span><kbd><span class="koboSpan" id="kobo.16.1">tf.contrib.rnn.static_rnn</span></kbd><span class="koboSpan" id="kobo.17.1"> to implement the LSTM cell, shown in the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.18.1"><img src="assets/200afd47-761e-48f2-b1f0-96ae371de2c6.png"/></span></div>
<div class="CDPAlignCenter packt_figref CDPAlign"><span class="koboSpan" id="kobo.19.1">The basic LSTM cell, </span><em><span class="koboSpan" id="kobo.20.1">x</span></em><span class="koboSpan" id="kobo.21.1"> is the input to the cell, </span><em><span class="koboSpan" id="kobo.22.1">h</span></em><span class="koboSpan" id="kobo.23.1"> the short-term memory and </span><em><span class="koboSpan" id="kobo.24.1">c</span></em><span class="koboSpan" id="kobo.25.1"> the long-term memory. </span><span class="koboSpan" id="kobo.25.2">The subscript refers to the time</span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.26.1">At each time step, </span><em><span class="koboSpan" id="kobo.27.1">t</span></em><span class="koboSpan" id="kobo.28.1">, the LSTM cell takes three inputs: the input </span><em><span class="koboSpan" id="kobo.29.1">x</span><sub><span class="koboSpan" id="kobo.30.1">t</span></sub></em><span class="koboSpan" id="kobo.31.1">, the short-term memory </span><em><span class="koboSpan" id="kobo.32.1">h</span></em><sub><em><span class="koboSpan" id="kobo.33.1">t-1</span></em></sub><span class="koboSpan" id="kobo.34.1">, and the long-term memory </span><em><span class="koboSpan" id="kobo.35.1">c</span><sub><span class="koboSpan" id="kobo.36.1">t-1</span></sub></em><span class="koboSpan" id="kobo.37.1">, and outputs the long-term memory </span><em><span class="koboSpan" id="kobo.38.1">c</span><sub><span class="koboSpan" id="kobo.39.1">t</span></sub></em><span class="koboSpan" id="kobo.40.1"> at and short-term memory </span><em><span class="koboSpan" id="kobo.41.1">h</span><sub><span class="koboSpan" id="kobo.42.1">t</span></sub></em><span class="koboSpan" id="kobo.43.1">. </span><span class="koboSpan" id="kobo.43.2">The subscript to </span><em><span class="koboSpan" id="kobo.44.1">x</span></em><span class="koboSpan" id="kobo.45.1">, </span><em><span class="koboSpan" id="kobo.46.1">h,</span></em><span class="koboSpan" id="kobo.47.1"> and </span><em><span class="koboSpan" id="kobo.48.1">c</span></em><span class="koboSpan" id="kobo.49.1"> refer to the timestep.</span></p>
<p class="mce-root"/>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.50.1">The </span><strong><span class="koboSpan" id="kobo.51.1">Forget Gate</span></strong> <em><span class="koboSpan" id="kobo.52.1">f</span></em><span class="koboSpan" id="kobo.53.1">(.) controls the amount of short-term memory, </span><em><span class="koboSpan" id="kobo.54.1">h</span></em><span class="koboSpan" id="kobo.55.1">, to be remembered for further flow in the present time step. </span><span class="koboSpan" id="kobo.55.2">Mathematically we can represent Forget Gate </span><em><span class="koboSpan" id="kobo.56.1">f(.)</span></em><span class="koboSpan" id="kobo.57.1"> as:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.58.1"><img class="fm-editor-equation" src="assets/ca432207-ce53-40bd-8444-41504c254c53.png" style="width:19.83em;height:1.75em;"/></span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.59.1">Where σ represents the sigmoid activation function, </span><em><span class="koboSpan" id="kobo.60.1">W</span><sub><span class="koboSpan" id="kobo.61.1">fx</span></sub></em><span class="koboSpan" id="kobo.62.1"> and </span><em><span class="koboSpan" id="kobo.63.1">W</span><sub><span class="koboSpan" id="kobo.64.1">fh</span></sub></em><span class="koboSpan" id="kobo.65.1"> are the weights controlling the influence of input </span><em><span class="koboSpan" id="kobo.66.1">x</span><sub><span class="koboSpan" id="kobo.67.1">t</span></sub><span class="koboSpan" id="kobo.68.1">,</span></em><span class="koboSpan" id="kobo.69.1"> short-term memory </span><em><span class="koboSpan" id="kobo.70.1">h</span></em><sub><em><span class="koboSpan" id="kobo.71.1">t</span></em><span class="koboSpan" id="kobo.72.1">-1</span></sub><span class="koboSpan" id="kobo.73.1">, and </span><em><span class="koboSpan" id="kobo.74.1">b</span><sub><span class="koboSpan" id="kobo.75.1">f</span></sub></em><span class="koboSpan" id="kobo.76.1"> the bias of the forget gate.</span></p>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.77.1">The </span><strong><span class="koboSpan" id="kobo.78.1">Input Gate</span></strong> <em><span class="koboSpan" id="kobo.79.1">i</span></em><span class="koboSpan" id="kobo.80.1">(.) controls the amount of input and working memory influencing the output of the cell. </span><span class="koboSpan" id="kobo.80.2">We can express it as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.81.1"><img class="fm-editor-equation" src="assets/1dacd13e-4362-4ae5-ae0a-2fc1ddd995d8.png" style="width:18.08em;height:1.58em;"/></span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.82.1">The </span><strong><span class="koboSpan" id="kobo.83.1">Output Gate</span></strong> <em><span class="koboSpan" id="kobo.84.1">o</span></em><span class="koboSpan" id="kobo.85.1">(.) controls the amount of information that's used for updating the short-term memory, and given by the following:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.86.1"><img class="fm-editor-equation" src="assets/4a136d33-ba2b-4d30-9184-10a4e4258714.png" style="width:18.58em;height:1.58em;"/></span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.87.1">Beside these three gates, the LSTM cell also computes the candidate hidden state </span><span class="koboSpan" id="kobo.88.1"><img class="fm-editor-equation" src="assets/9e3aa387-e3c4-413f-a38a-1ffc05da350b.png" style="width:0.83em;height:1.17em;"/></span><span class="koboSpan" id="kobo.89.1"> , which along with the input and forget gate, is used to compute the amount of long term memory </span><em><span class="koboSpan" id="kobo.90.1">c</span><sub><span class="koboSpan" id="kobo.91.1">t</span></sub></em><span class="koboSpan" id="kobo.92.1">:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.93.1"><img class="fm-editor-equation" src="assets/b0d751d8-781b-4a9c-a4a7-e4a5707b2960.png" style="width:19.83em;height:1.50em;"/></span></div>
<div class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.94.1"><img class="fm-editor-equation" src="assets/55ff027e-d862-415c-b288-e72b31beaf81.png" style="width:15.92em;height:1.58em;"/></span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.95.1">The circle represents the element wise multiplication. </span><span class="koboSpan" id="kobo.95.2">The new value of the short-term memory is then computed as the following:</span></p>
<div class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.96.1"><img class="fm-editor-equation" src="assets/e3f46d9b-122e-4d86-a254-2be3800cd613.png" style="width:10.75em;height:1.50em;"/></span></div>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.97.1">Let's now see how we can implement LSTM in TensorFlow in the following steps:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.98.1">We are using the following modules:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.99.1">import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.100.1">from tensorflow.contrib import rnn</span><br/><span class="koboSpan" id="kobo.101.1">import numpy as np</span></pre>
<ol start="2">
<li><span class="koboSpan" id="kobo.102.1">We define a class LSTM where we construct the graph and define the LSTM layer with the help of TensorFlow </span><kbd><span class="koboSpan" id="kobo.103.1">contrib</span></kbd><span class="koboSpan" id="kobo.104.1">. </span><span class="koboSpan" id="kobo.104.2">To take care of memory, we first clear </span><span><span class="koboSpan" id="kobo.105.1">the default graph stack and reset the global default graph using </span><kbd><span class="koboSpan" id="kobo.106.1">tf.reset_default_graph()</span></kbd><span class="koboSpan" id="kobo.107.1">. </span><span class="koboSpan" id="kobo.107.2">The input goes directly to the LSTM layer with </span><kbd><span class="koboSpan" id="kobo.108.1">num_units</span></kbd><span class="koboSpan" id="kobo.109.1"> number of hidden units. </span><span class="koboSpan" id="kobo.109.2">It's followed by a fully connected output layer with the </span><kbd><span class="koboSpan" id="kobo.110.1">out_weights</span></kbd><span class="koboSpan" id="kobo.111.1"> weights and </span><kbd><span class="koboSpan" id="kobo.112.1">out_bias</span></kbd></span><span class="koboSpan" id="kobo.113.1"> bias</span><span><span class="koboSpan" id="kobo.114.1">. </span><span class="koboSpan" id="kobo.114.2">Create the placeholders for input </span><kbd><span class="koboSpan" id="kobo.115.1">self.x</span></kbd><span class="koboSpan" id="kobo.116.1"> and </span><kbd><span class="koboSpan" id="kobo.117.1">self.y</span></kbd></span><span class="koboSpan" id="kobo.118.1"> label. </span><span class="koboSpan" id="kobo.118.2">Th</span><span><span class="koboSpan" id="kobo.119.1">e input is reshaped and fed to the LSTM cell. </span><span class="koboSpan" id="kobo.119.2">To create the LSTM layer, we first define the LSTM cell with </span><kbd><span class="koboSpan" id="kobo.120.1">num_units</span></kbd><span class="koboSpan" id="kobo.121.1"> hidden units and forget bias set to </span><kbd><span class="koboSpan" id="kobo.122.1">1.0</span></kbd><span class="koboSpan" id="kobo.123.1">. </span><span class="koboSpan" id="kobo.123.2">This adds the biases to the forget gate in order to reduce the scale of forgetting in the beginning of the training. </span><span class="koboSpan" id="kobo.123.3">Reshape the output from the LSTM layer and feed it to the fully connected layer, shown as follows:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.124.1"> class LSTM:</span><br/><span class="koboSpan" id="kobo.125.1">    def __init__(self, num_units, n_classes, n_input,\</span><br/><span class="koboSpan" id="kobo.126.1">             time_steps, learning_rate=0.001,):    </span><br/><span class="koboSpan" id="kobo.127.1">        tf.reset_default_graph()</span><br/><span class="koboSpan" id="kobo.128.1">        self.steps = time_steps</span><br/><span class="koboSpan" id="kobo.129.1">        self.n = n_input</span><br/><span class="koboSpan" id="kobo.130.1">        # weights and biases of appropriate shape</span><br/><span class="koboSpan" id="kobo.131.1">        out_weights = tf.Variable(tf.random_normal([num_units, n_classes]))</span><br/><span class="koboSpan" id="kobo.132.1">        out_bias = tf.Variable(tf.random_normal([n_classes]))</span><br/><span class="koboSpan" id="kobo.133.1">        # defining placeholders</span><br/><span class="koboSpan" id="kobo.134.1">        # input placeholder</span><br/><span class="koboSpan" id="kobo.135.1">        self.x = tf.placeholder("float", [None, self.steps, self.n])</span><br/><span class="koboSpan" id="kobo.136.1">        # label placeholder</span><br/><span class="koboSpan" id="kobo.137.1">        self.y = tf.placeholder("float", [None, n_classes])</span><br/><span class="koboSpan" id="kobo.138.1">        # processing the input tensor from [batch_size,steps,self.n] to </span><br/><span class="koboSpan" id="kobo.139.1">        # "steps" number of [batch_size,self.n] tensors</span><br/><span class="koboSpan" id="kobo.140.1">        input = tf.unstack(self.x, self.steps, 1)</span><br/><br/><span class="koboSpan" id="kobo.141.1">        # defining the network</span><br/><span class="koboSpan" id="kobo.142.1">        lstm_layer = rnn.BasicLSTMCell(num_units, forget_bias=1)</span><br/><span class="koboSpan" id="kobo.143.1">        outputs, _ = rnn.static_rnn(lstm_layer, input, dtype="float32")</span><br/><span class="koboSpan" id="kobo.144.1">        # converting last output of dimension [batch_size,num_units] to </span><br/><span class="koboSpan" id="kobo.145.1">        # [batch_size,n_classes] by out_weight multiplication</span><br/><span class="koboSpan" id="kobo.146.1">        self.prediction = tf.matmul(outputs[-1], out_weights) + out_bias</span><br/><br/><span class="koboSpan" id="kobo.147.1">        # loss_function</span><br/><span class="koboSpan" id="kobo.148.1">        self.loss = tf.reduce_mean(tf.squared_difference(self.prediction, self.y))</span><br/><span class="koboSpan" id="kobo.149.1">        # optimization</span><br/><span class="koboSpan" id="kobo.150.1">        self.opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.loss)</span><br/><br/><span class="koboSpan" id="kobo.151.1">        # model evaluation</span><br/><span class="koboSpan" id="kobo.152.1">        correct_prediction = tf.equal(tf.argmax(self.prediction, 1), tf.argmax(self.y, 1))</span><br/><span class="koboSpan" id="kobo.153.1">        self._accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br/><br/><span class="koboSpan" id="kobo.154.1">        init = tf.global_variables_initializer()</span><br/><span class="koboSpan" id="kobo.155.1">        gpu_options = tf.GPUOptions(allow_growth=True)</span><br/><br/><span class="koboSpan" id="kobo.156.1">        self.sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))</span><br/><span class="koboSpan" id="kobo.157.1">        self.sess.run(init)</span></pre>
<ol start="3">
<li><span class="koboSpan" id="kobo.158.1">We create the methods to train and predict, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px"><span class="koboSpan" id="kobo.159.1">def train(self, X, Y, epochs=100,batch_size=128):</span><br/><span class="koboSpan" id="kobo.160.1">    iter = 1</span><br/><span class="koboSpan" id="kobo.161.1">    #print(X.shape)</span><br/><span class="koboSpan" id="kobo.162.1">    X = X.reshape((len(X),self.steps,self.n))</span><br/><span class="koboSpan" id="kobo.163.1">    while iter &amp;amp;lt; epochs:</span><br/><span class="koboSpan" id="kobo.164.1">        for i in range(int(len(X)/batch_size)):</span><br/><span class="koboSpan" id="kobo.165.1">            batch_x, batch_y = X[i:i+batch_size,:], Y[i:i+batch_size,:]</span><br/><span class="koboSpan" id="kobo.166.1">            #print(batch_x.shape)</span><br/><span class="koboSpan" id="kobo.167.1">            #batch_x = batch_x.reshape((batch_size, self.steps, self.n))    </span><br/><span class="koboSpan" id="kobo.168.1">            #print(batch_x.shape)    </span><br/><span class="koboSpan" id="kobo.169.1">            self.sess.run(self.opt, feed_dict={self.x: batch_x, self.y: batch_y})</span><br/><span class="koboSpan" id="kobo.170.1">            if iter % 10 == 0:</span><br/><span class="koboSpan" id="kobo.171.1">                acc = self.sess.run(self._accuracy, feed_dict={self.x: X, self.y: Y})</span><br/><span class="koboSpan" id="kobo.172.1">                los = self.sess.run(self.loss, feed_dict={self.x: X, self.y: Y})</span><br/><span class="koboSpan" id="kobo.173.1">                print("For iter ", iter)</span><br/><span class="koboSpan" id="kobo.174.1">                print("Accuracy ", acc)</span><br/><span class="koboSpan" id="kobo.175.1">                print("Loss ", los)    </span><br/><span class="koboSpan" id="kobo.176.1">                print("__________________")</span><br/><span class="koboSpan" id="kobo.177.1">            iter = iter + 1</span><br/><br/><span class="koboSpan" id="kobo.178.1">def predict(self,X):</span><br/><span class="koboSpan" id="kobo.179.1">    # predicting the output</span><br/><span class="koboSpan" id="kobo.180.1">    test_data = X.reshape((-1, self.steps, self.n))</span><br/><span class="koboSpan" id="kobo.181.1">    out = self.sess.run(self.prediction, feed_dict={self.x:test_data})</span><br/><span class="koboSpan" id="kobo.182.1">    return out</span></pre>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.183.1">In the coming chapters, we will be using the RNN for handling time series production and text processing.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Gated recurrent unit</span></h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign"><strong><span class="koboSpan" id="kobo.2.1">Gated recurrent unit</span></strong><span class="koboSpan" id="kobo.3.1"> (</span><strong><span class="koboSpan" id="kobo.4.1">GRU</span></strong><span class="koboSpan" id="kobo.5.1">) is another modification of RNN. </span><span class="koboSpan" id="kobo.5.2">It has a simplified architecture compared to LSTM and overcomes the vanishing gradient problem. </span><span class="koboSpan" id="kobo.5.3">It takes only two inputs, the input </span><em><span class="koboSpan" id="kobo.6.1">x</span><sub><span class="koboSpan" id="kobo.7.1">t</span></sub></em><span class="koboSpan" id="kobo.8.1"> at time </span><em><span class="koboSpan" id="kobo.9.1">t</span></em><span class="koboSpan" id="kobo.10.1"> and memory </span><em><span class="koboSpan" id="kobo.11.1">h</span><sub><span class="koboSpan" id="kobo.12.1">t-1</span></sub></em><span class="koboSpan" id="kobo.13.1"> from time </span><em><span class="koboSpan" id="kobo.14.1">t</span></em><span class="koboSpan" id="kobo.15.1">-1. </span><span class="koboSpan" id="kobo.15.2">There are only two gates, </span><strong><span class="koboSpan" id="kobo.16.1">Update G</span></strong><strong><span class="koboSpan" id="kobo.17.1">ate</span></strong><span class="koboSpan" id="kobo.18.1"> and </span><strong><span class="koboSpan" id="kobo.19.1">Reset Gate</span></strong><span class="koboSpan" id="kobo.20.1">, shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.21.1"><img class="aligncenter size-full wp-image-1088 image-border" src="assets/914f4d89-33d5-4db6-950b-ba58385b38a9.png" style="width:115.42em;height:73.33em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.22.1">The architecture of a basic GRU cell</span></div>
<p class="mce-root"/>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.23.1">The update gate controls how much previous memory to keep, and the reset gate determines how to combine the new input with previous memory. </span><span class="koboSpan" id="kobo.23.2">We can define the complete GRU cell by the following four equations:</span></p>
<ul>
<li class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.24.1"><img class="fm-editor-equation" src="assets/ae7dba9c-7ee0-4ff2-ada7-42a0a0eec746.png" style="width:17.08em;height:1.42em;"/></span></li>
<li class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.25.1"><img class="fm-editor-equation" src="assets/2acac066-bd31-4b34-b736-ba63b17fc36c.png" style="width:17.67em;height:1.50em;"/></span></li>
<li class="mce-root CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.26.1"><img class="fm-editor-equation" src="assets/ec5aa1a8-224d-4ed5-98c7-27ffa72ed61a.png" style="width:19.67em;height:1.50em;"/></span></li>
<li class="mce-root CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.27.1"><img class="fm-editor-equation" src="assets/37499e4d-e92a-404a-b41c-f4951d582cee.png" style="width:16.08em;height:1.58em;"/></span></li>
</ul>
<p class="CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.28.1">Both GRU and LSTM give a comparable performance, but GRU has fewer training parameters.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Autoencoders</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The models we have learned up to now were learning using supervised learning. </span><span class="koboSpan" id="kobo.2.2">In this section, we will learn about autoencoders. </span><span class="koboSpan" id="kobo.2.3">They are feedforward, non-recurrent neural network, and learn through unsupervised learning. </span><span class="koboSpan" id="kobo.2.4">They are the latest buzz, along with generative adversarial networks, and we can find applications in image reconstruction, clustering, machine translation, and much more. </span><span class="koboSpan" id="kobo.2.5">They were initially proposed in the 1980s by Geoffrey E. </span><span class="koboSpan" id="kobo.2.6">Hinton and the PDP group (</span><a href="http://www.cs.toronto.edu/~fritz/absps/clp.pdf"><span class="koboSpan" id="kobo.3.1">http://www.cs.toronto.edu/~fritz/absps/clp.pdf</span></a><span class="koboSpan" id="kobo.4.1">).</span></p>
<p><span><span class="koboSpan" id="kobo.5.1">The autoencoder basically consists of two cascaded neural networks—the first network acts as an encoder; it takes the input</span></span> <em><span><span class="koboSpan" id="kobo.6.1">x</span></span></em><span><span class="koboSpan" id="kobo.7.1"> and encodes it using a transformation</span></span> <em><span><span class="koboSpan" id="kobo.8.1">h</span></span></em> <span><span class="koboSpan" id="kobo.9.1">to encoded signal</span></span> <em><span><span class="koboSpan" id="kobo.10.1">y</span></span></em><span><span class="koboSpan" id="kobo.11.1">, shown in the following equation</span></span><span><span class="koboSpan" id="kobo.12.1">:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.13.1"><img class="fm-editor-equation" src="assets/03a3d50f-7635-4a27-ac5d-9ed153780372.png" style="width:4.17em;height:1.33em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.14.1">The second neural network uses the encoded signal</span></span> <em><span><span class="koboSpan" id="kobo.15.1">y</span></span></em> <span><span class="koboSpan" id="kobo.16.1">as its input and performs another transformation</span></span> <em><span><span class="koboSpan" id="kobo.17.1">f</span></span></em> <span><span class="koboSpan" id="kobo.18.1">to get a reconstructed signal</span></span> <em><span><span class="koboSpan" id="kobo.19.1">r</span></span></em><span><span class="koboSpan" id="kobo.20.1">, shown as follows</span></span><span><span class="koboSpan" id="kobo.21.1">:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.22.1"><img class="fm-editor-equation" src="assets/e8c72478-363b-4d88-98d8-8247494a6503.png" style="width:8.67em;height:1.25em;"/></span></p>
<p class="layoutArea CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.23.1">The loss function is the MSE with error </span><em><span class="koboSpan" id="kobo.24.1">e</span></em><span class="koboSpan" id="kobo.25.1"> defined as the difference between the original input </span><em><span class="koboSpan" id="kobo.26.1">x</span></em><span class="koboSpan" id="kobo.27.1"> and the reconstructed signal </span><em><span class="koboSpan" id="kobo.28.1">r:</span></em></p>
<p class="layoutArea CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.29.1"><img class="fm-editor-equation" src="assets/2ac31af7-19b0-4f8b-b7a8-e1f7d6b3e542.png" style="width:5.08em;height:1.08em;"/></span></p>
<p class="layoutArea CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.30.1"><img class="aligncenter size-full wp-image-944 image-border" src="assets/6b841e32-381c-4f57-9cd7-47229f08c5f7.png" style="width:31.92em;height:26.92em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="koboSpan" id="kobo.31.1">Basic architecture of an autoencoder</span></div>
<p class="page CDPAlignLeft CDPAlign"><span class="koboSpan" id="kobo.32.1">The preceding diagram shows an autoencoder with </span><strong><span class="koboSpan" id="kobo.33.1">Encoder</span></strong><span class="koboSpan" id="kobo.34.1"> and </span><strong><span class="koboSpan" id="kobo.35.1">Decoder</span></strong><span class="koboSpan" id="kobo.36.1"> highlighted separately. </span><span class="koboSpan" id="kobo.36.2">Autoencoders may have weight sharing, that is, weights of decoder and encoder are shared. </span><span class="koboSpan" id="kobo.36.3">This is done by simply making them a transpose of each other; this helps the network learn faster as the number of training parameters is less. </span><span class="koboSpan" id="kobo.36.4">There are a large variety of autoencoders for example: sparse autoencoders, denoising autoencoders, convolution autoencoders, and variational autoencoders.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Denoising autoencoders</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">A denoising autoencoder learns from a corrupted (noisy) input; we feed the encoder network the noisy input and the reconstructed image from the decoder is compared with the original denoised input. </span><span class="koboSpan" id="kobo.2.2">The idea is that this will help the network learn how to denoise an input. </span><span class="koboSpan" id="kobo.2.3">The network does not just make a pixel-wise comparison, instead, in order to denoise the image, the network is forced to learn the information of neighboring pixels as well.</span></span></p>
<p><span class="koboSpan" id="kobo.3.1">Once the autoencoder has learned the encoded features </span><em><span class="koboSpan" id="kobo.4.1">y</span></em><span class="koboSpan" id="kobo.5.1">, we can remove the decoder part of the network and use only the encoder part to achieve dimensionality reduction. </span><span class="koboSpan" id="kobo.5.2">The dimensionally reduced input can be fed to some other classification or regression model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Variational autoencoders</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">Another very popular autoencoder is </span><strong><span class="koboSpan" id="kobo.3.1">variational autoencoders</span></strong></span> <span><span class="koboSpan" id="kobo.4.1">(</span></span><strong><span><span class="koboSpan" id="kobo.5.1">VAE</span></span></strong><span><span class="koboSpan" id="kobo.6.1">). </span><span class="koboSpan" id="kobo.6.2">They are a mix of the best of both worlds: DL and the Bayesian inference.</span><br/></span></p>
<p><span><span class="koboSpan" id="kobo.7.1">VAEs have an additional stochastic layer; this layer</span></span><span><span class="koboSpan" id="kobo.8.1">, after the encoder network, samples the data using a Gaussian distribution, and the one after the decoder network samples the data using Bernoulli's distribution.</span></span></p>
<p class="CDPAlignLeft CDPAlign"><span><span class="koboSpan" id="kobo.9.1">VAEs can be used to generate images. </span><span class="koboSpan" id="kobo.9.2">VAEs allow one to set complex priors in the latent and learn powerful latent representations. </span><span class="koboSpan" id="kobo.9.3">We will learn more about them in a later chapter.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this chapter, we covered some basic and useful deep neural network models. </span><span class="koboSpan" id="kobo.2.2">We started with a single neuron, saw its power and its limitations. </span><span class="koboSpan" id="kobo.2.3">The multilayered perceptron was built for both regression and classification tasks. </span><span class="koboSpan" id="kobo.2.4">The backpropagation algorithm was introduced. </span><span class="koboSpan" id="kobo.2.5">The chapter progressed to CNN, with an introduction to the convolution layers and pooling layers. </span><span class="koboSpan" id="kobo.2.6">We learned about some of the successful CNN and used the first CNN LeNet to perform handwritten digits recognition. </span><span class="koboSpan" id="kobo.2.7">From the feed forward MLPs and CNNs, we moved forward to RNNs. </span><span class="koboSpan" id="kobo.2.8">LSTM and GRU networks were introduced. </span><span class="koboSpan" id="kobo.2.9">We made our own LSTM network in TensorFlow and finally learned about autoencoders.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In the next chapter, we will start with a totally new type of AI model genetic algorithms. </span><span class="koboSpan" id="kobo.3.2">Like neural networks, they too are inspired by nature. </span><span class="koboSpan" id="kobo.3.3">We will be using what we learned in this chapter and the coming few chapters in the case studies we'll do in later chapters.</span></p>


            </article>

            
        </section>
    </body></html>