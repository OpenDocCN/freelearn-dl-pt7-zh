["```py\n    !sudo apt install ffmpeg\n    !pip install -q cohere openai tiktoken\n    ffmpeg is used for audio file manipulation. The cohere and openai Python libraries offer various AI models for tiktoken is required as a supporting library for authentication or token handling in the context of API requests. We also installed the latest Whisper files from the official OpenAI GitHub repository. These steps ensure we have all the tools ready for our transcription tasks.\n    ```", "```py\n    import numpy as np\n    import torch\n    torch.cuda.is_available()\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using torch {torch.__version__} ({DEVICE})\")\n    ```", "```py\n    import whisper\n    model = whisper.load_model(\"medium\", device=DEVICE)\n    print(\n        f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n        f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n    )\n    ```", "```py\n    for audiofile in audiofiles:\n        # Load audio and pad/trim it to fit 30 seconds\n        audio = whisper.load_audio(audiofile)\n        audio = whisper.pad_or_trim(audio)\n        # Make log-Mel spectrogram and move to the same device as the model\n        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n        #Next we detect the language of your audio file\n        _, probs = model.detect_language(mel)\n        detected_language = max(probs, key=probs.get)\n        print(f\"----\\nDetected language: {detected_language}\")\n        # Set up the decoding options\n        options = whisper.DecodingOptions(language=detected_language, without_timestamps=True, fp16=(DEVICE == \"cuda\"))\n        # Decode the audio and print the recognized text\n        result = whisper.decode(model, mel, options)\n    ```", "```py\n    def process_file(audiofile, model, w_options, w_translate=False):\n        # Load audio\n        audio = whisper.load_audio(audiofile)\n        transcribe_options = dict(task=\"transcribe\", **w_options)\n        translate_options = dict(task=\"translate\", **w_options)\n        transcription = model.transcribe(audiofile, **transcribe_options)[\"text\"]\n        if w_translate:\n            translation = model.transcribe(audiofile, **translate_options)[\"text\"]\n        else:\n            translation = \"N/A\"\n        return transcription, translation\n    ```", "```py\n    w_options = dict(without_timestamps=True, fp16=(DEVICE == \"cuda\"))\n    audiofile = 'Learn_OAI_Whisper_Spanish_Sample_Audio01.mp3'\n    transcription, translation = process_file(audiofile, model, w_options, False)\n    print(\"------\\nTranscription of file '\" + audiofile + \"':\")\n    for sent in sent_tokenize(transcription):\n        print(sent)\n    print(\"------\\nTranslation of file '\" + audiofile + \"':\")\n    for sent in sent_tokenize(translation):\n        print(sent)\n    import ipywidgets as widgets\n    widgets.Audio.from_file(audiofile, autoplay=False, loop=False)\n    ```", "```py\n        ------\n        Transcription of file 'product_names.wav':\n        Welcome to Quirk, Quid, Quill, Inc., where finance meets innovation.\n        Explore diverse offerings from the P3 Quattro, a unique investment portfolio quadrant to the O3 Omni, a platform for intricate derivative trading strategies.\n        Delve into unconventional bond markets with our B3 Bond X and experience non-standard equity trading with E3 Equity.\n        Surpass your wealth management with W3 Rap Z and anticipate market trends with the O2 Outlier, our forward-thinking financial forecasting tool.\n        Explore venture capital world with U3 Unifund or move your money with the M3 Mover, our sophisticated monetary transfer module.\n        At Quirk, Quid, Quill, Inc., we turn complex finance into creative solutions.\n        Join us in redefining financial services.\n        ```", "```py\n    This results in the following:\n\n    ```", "```py\n        ------\n        Transcription of file 'bbq_plans.wav':\n        Hello, my name is Preston Tuggle.\n        I am based in New York City.\n        This weekend, I have really exciting plans with some friends of mine, Amy and Sean.\n        We're going to a barbecue here in Brooklyn.\n        Hopefully, it's actually going to be a little bit of kind of an odd barbecue.\n        We're going to have donuts, omelets.\n        It's kind of like a breakfast as well as whiskey.\n        So that should be fun.\n        And I'm really looking forward to spending time with my friends, Amy and Sean.\n        ```", "```py\n    This results in the following:\n\n    ```", "```py\n\n    ```", "```py\n\n    ```", "```py\n    !pip install -q cohere openai tiktoken\n    !pip install -q librosa\n    !pip install git+https://github.com/openai/whisper.git\n    Fleurs(torch.utils.data.Dataset) custom class is implemented to download, extract, and preprocess audio files from the selected language dataset, preparing the dataset for processing with Whisper. We extract 5% of the dataset for the selected language using that class. Notice that we are removing only a few records from the FLEURS dataset for a given language. For example, if we choose the Korean language as the dataset, we must download about 840 records. Thus, downloading just 5% (77 records) is more manageable and runs the demo code faster. Feel free to experiment with other percentage values:\n\n    ```", "```py\n\n    ```", "```py\n    options = dict(language=language, beam_size=5, best_of=5, temperature=0)\n    transcribe_options = dict(task=\"transcribe\", **options)\n    translate_options = dict(task=\"translate\", **options)\n    ```", "```py\n    import os\n    os.environ[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=on\"\n    print(os.getenv(\"CMAKE_ARGS\"))\n    !pip install llama-cpp-python==0.2.34\n    !huggingface-cli download TheBloke/stablelm-zephyr-3b-GGUF stablelm-zephyr-3b.Q5_K_S.gguf --local-dir . --local-dir-use-symlinks False\n    !pip install -q git+https://github.com/openai/whisper.git\n    !pip install -q gradio\n    !pip install -q gTTS\n    !ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.\n    ```", "```py\n    import datetime\n    import os\n    from rich.console import Console\n    console = Console(width=110)\n    ## Logger file\n    tstamp = datetime.datetime.now()\n    tstamp = str(tstamp).replace(' ','_')\n    logfile = f'{tstamp}_log.txt'\n    def writehistory(text):\n        with open(logfile, 'a', encoding='utf-8') as f:\n            f.write(text)\n            f.write('\\n')\n        f.close()\n    ```", "```py\n    warnings.filterwarnings(\"ignore\")\n    with console.status(\"Loading...\",spinner=\"dots12\"):\n      llm_gpu = Llama(\n        model_path=\"/content/stablelm-zephyr-3b.Q5_K_S.gguf\",  # Download the model file first\n        n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n        n_threads=8,            # The number of CPU threads to use, tailor to your system, and the resulting performance\n        n_gpu_layers=35         # The number of layers to offload to GPU if you have GPU acceleration available\n    )\n    ```", "```py\n    prompt=\"In a short response, what is the capital of France?\"\n    template = f\"<|user|>\\n{prompt}<|endoftext|>\\n<|assistant|>\"\n    start = datetime.datetime.now()\n    output = llm_gpu(\n        template, # Prompt\n        temperature=0,\n        max_tokens=512,  # Generate up to 512 tokens\n        stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n        echo=False        # Whether to echo the prompt\n    )\n    console.print(output['choices'][0]['text'])\n    ```", "```py\n    import re\n    def llm_call(input_text):\n        prompt = \"\"\"Act as Tatianna, a junior-level assistant characterized by your cheerful demeanor and unwavering helpfulness. \\\n        You are in a business setting; thus, always act professionally and courteously. \\\n        Respond succinctly to the following instructions and questions, and do not include information about yourself unless it is part of the action or question: \\\n        \"\"\" + input_text\n        template = f\"<|user|>\\n{prompt}<|endoftext|>\\n<|assistant|>\"\n        response = llm_gpu(\n            template, # Prompt\n            temperature=0.1,\n            max_tokens=200,  # Generate up to 512 tokens\n            stop=[\"</s>\"],   # Example stop token - not necessarily correct for this specific model! Please check before using.\n            echo=False        # Whether to echo the prompt\n        )\n        if response is not None:\n            match = re.search(r':\\s*(.*)', response['choices'][0]['text'])\n            if match:\n                reply = match.group(1).strip()\n            reply = response['choices'][0]['text']\n        else:\n            reply = \"No response generated.\"\n        return reply\n    ```", "```py\n    import whisper\n    model = whisper.load_model(\"medium\", device=DEVICE)\n    def transcribe(audio):\n        if audio is None or audio == '':\n            return ('','',None)  # Return empty strings and None audio file\n        language = 'en'\n        audio = whisper.load_audio(audio)\n        audio = whisper.pad_or_trim(audio)\n        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n        _, probs = model.detect_language(mel)\n        options = whisper.DecodingOptions()\n        result = whisper.decode(model, mel, options)\n        result_text = result.text\n        out_result = llm_call(result_text)\n        audioobj = gTTS(text = out_result,\n                        lang = language,\n                        slow = False)\n        audioobj.save(\"Temp.mp3\")\n        return [result_text, out_result, \"Temp.mp3\"]\n    ```", "```py\n    gr.Interface(\n        title = 'Learn OpenAI Whisper: Voice Assistance',\n        fn=transcribe,\n        inputs = gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n        outputs=[\n            gr.Textbox(label=\"Speech to Text\"),\n            gr.Textbox(label=\"ChatGPT Output\"),\n            gr.Audio(\"Temp.mp3\")\n        ],\n        live=True).launch(debug=True)\n    ```", "```py\n    !pip install -q -U transformers==4.37.2\n    !pip install -q bitsandbytes==0.41.3 accelerate==0.25.0\n    !pip install -q git+https://github.com/openai/whisper.git\n    !pip install -q gradio\n    !pip install -q gTTS\n    !ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3\n    ```", "```py\n    import torch\n    from transformers import BitsAndBytesConfig\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16\n    )\n    ```", "```py\n    from huggingface_hub import notebook_login\n    notebook_login()\n    from huggingface_hub import whoami\n    whoami()\n    # you should see something like {'type': 'user',  'id': '...',  'name': 'Wauplin', ...}\n    from transformers import pipeline\n    model_id = \"llava-hf/llava-1.5-7b-hf\"\n    pipe = pipeline(\"image-to-text\", model=model_id, model_kwargs={\"quantization_config\": quantization_config})\n    ```", "```py\n    import whisper\n    import gradio as gr\n    import time\n    import warnings\n    import os\n    from gtts import gTTS\n    for i in range(1, 11):\n        !wget -nv https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter05/images/LOAIW_ch05_image_{str(i).zfill(2)}.jpg\n    from PIL import Image\n    image_path = \"/content/LOAIW_ch05_image_03.jpg\"\n    image = Image.open((image_path))\n    image\n    ```", "```py\n    max_new_tokens = 200\n    prompt_instructions = \"\"\"\n    Describe the image using as much detail as possible. Is it a painting or a photograph? What colors are predominant? What is the image about?\n    \"\"\"\n    prompt = \"USER: <image>\\n\" + prompt_instructions + \"\\nASSISTANT:\"\n    outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n    # outputs\n    # print(outputs[0][\"generated_text\"])\n    for sent in sent_tokenize(outputs[0][\"generated_text\"]):\n        print(sent)\n    ```", "```py\n    import warnings\n    from gtts import gTTS\n    import numpy as np\n    import torch\n    torch.cuda.is_available()\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using torch {torch.__version__} ({DEVICE})\")\n    import whisper\n    model = whisper.load_model(\"medium\", device=DEVICE)\n    print(\n        f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n        f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n    )\n    import requests\n    import re\n    from PIL import Image\n    input_text = 'What color is the flag in the image?'\n    input_image = '/content/LOAIW_ch05_image_10.jpg'\n    # load the image\n    image = Image.open(input_image)\n    # print(input_text)\n    prompt_instructions = \"\"\"\n    Act as an expert in imagery descriptive analysis, using as much detail as possible from the image, respond to the following prompt:\n    \"\"\" + input_text\n    prompt = \"USER: <image>\\n\" + prompt_instructions + \"\\nASSISTANT:\"\n    img2txt() function in charge of converting image-to-text output using LlaVa; transcribe() does speech-to-text using Whisper, and text_to_speech() uses gTTS to convert text to speech. The result is saved as an audio file:\n\n    ```", "```py\n\n    ```"]