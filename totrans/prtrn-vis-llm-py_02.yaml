- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: 'Dataset Preparation: Part One'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集准备：第一部分
- en: In this chapter, we will begin to discuss what you’ll need in your dataset to
    start a meaningful pretraining project. This is the first of two parts on dataset
    preparation. It opens with some business guidance on finding a good use case for
    foundation modeling, where the data becomes instrumental. Then, focusing on the
    content of your dataset, we use qualitative and quantitative measures to compare
    it with datasets used to pretrain other top models. You’ll learn how to determine
    whether your datasets are “large enough” and “good enough” to boost accuracy while
    pretraining. We discuss bias identification and mitigation, along with multilingual
    and multimodal solutions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始讨论启动有意义的预训练项目所需的数据集内容。这是关于数据集准备的两部分内容中的第一部分。首先，我们提供一些关于如何为基础建模找到合适用例的业务指导，其中数据起着至关重要的作用。然后，专注于数据集的内容，我们使用定性和定量的标准将其与用于预训练其他顶级模型的数据集进行比较。你将学会如何判断你的数据集是否“足够大”和“足够好”，以在预训练时提高准确性。我们还将讨论偏差识别与缓解，以及多语言和多模态解决方案。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: A business-level discussion on finding datasets and use cases for foundation
    modeling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论如何为基础建模寻找数据集和用例的业务层面内容
- en: Evaluating your dataset by comparing it to ones available in the open source
    research community
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将你的数据集与开源研究社区中可用的数据集进行对比来评估它
- en: Using scaling laws to size your dataset appropriately
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用扩展定律来合理规划你的数据集规模
- en: Bias detection and mitigation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差检测与缓解
- en: Dataset enhancements – multilingual and augmentation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集增强——多语言和数据增强
- en: Finding a dataset and use case for foundation modeling
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为基础建模寻找数据集和用例
- en: Datasets – we love them, we struggle with them, we rely on them, and we ignore
    them, oftentimes all at once. Every transaction, every digital moment, every archive,
    and every snapshot is a candidate for inclusion in a dataset. If your organization
    has already gone through a digital transformation, or if you’re digitally native,
    odds are you are already heavily invested in some data storage solution. Whether
    it’s on-premises or in the cloud, every organization needs a secure, reliable,
    operational, and robust solution to store countless types of data. The major question
    for you right now is, how can I monetize that? How can I lean into what is most
    unique about my organization’s history and strengths, and capitalize on it to
    develop net-new capabilities that further my own competitive advantage?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集——我们热爱它们，挣扎于它们，依赖它们，也常常忽视它们，往往是同时发生的。每一笔交易、每一个数字瞬间、每一个档案、每一张快照都可能成为数据集的候选项。如果你的组织已经经历了数字化转型，或者你是数字原生企业，那么很可能你已经大量投资于某些数据存储解决方案。无论是本地存储还是云存储，每个组织都需要一个安全、可靠、可操作且强大的解决方案来存储各种数据类型。此时你面临的主要问题是，我如何通过这些数据进行盈利？我如何利用我组织历史和优势中最独特的部分，并通过它开发新的能力，进一步提升我的竞争优势？
- en: For companies that already have machine learning models deployed into production
    applications, an easy way to find a candidate dataset for a foundation modeling
    project is to ask yourself, what is the single common denominator in all of my
    models? What domains, what modalities, and what signals do those models rely on,
    and what resources can I draw on to increase the overall intelligence of these
    models?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于已经在生产应用中部署机器学习模型的公司，找到一个适合基础建模项目的候选数据集的简单方法是问自己，所有模型的共同点是什么？这些模型依赖于哪些领域、哪些模式、哪些信号，哪些资源可以用来提高这些模型的整体智能？
- en: 'One common mental exercise is to consider the interactions that are core to
    your business. From search to customer support, supply chain, product development
    and maintenance, marketing, and so on, each of your lines of business involve
    decision making on a regular basis. Now, ask yourself the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的思维练习是考虑与业务核心相关的互动。从搜索到客户支持、供应链、产品开发和维护、营销等，你的每一条业务线都涉及到定期的决策过程。现在，问问自己以下问题：
- en: What if I could improve the accuracy of this decision making by 1%?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能将决策准确性提高1%呢？
- en: What if I could increase my marketing lead conversion by 1%?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能将我的营销潜在客户转化率提高1%呢？
- en: What if I could recommend better content to customers by 1%?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能为客户推荐更好的内容，提高1%呢？
- en: What if I could increase my operational efficiency by 1%?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能将我的运营效率提高1%呢？
- en: What if I could answer questions more accurately by 1%?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能更准确地回答问题，提高1%呢？
- en: What if I could deliver my products faster by 1%?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能让我的产品交付速度提高1%，会怎样呢？
- en: Once you’ve found a certain aspect of your business that’s most interesting
    to you or where you think the impact of your investment could be the highest,
    try to quantify this number. Will an increase in accuracy by 1% give you $50,000?
    What about $500,000? Maybe even $1,000,000? Many multiples of that? I’m stating
    the obvious here, but all things being equal, higher is clearly better. You want
    to pick an area you think will have the absolute maximum return on your investment.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你找到自己最感兴趣的业务领域，或者你认为投资影响最大的一块，尝试量化这个数字。准确度提高1%会为你带来$50,000吗？那$500,000呢？甚至$1,000,000？或者更多倍的收益？我说的显而易见，但所有条件相同的情况下，显然数字越高越好。你要选择一个你认为能带来最大投资回报的领域。
- en: Now, once you have that area of your organization identified, take 10% of the
    total estimated earnings, or some other low percentage you feel more comfortable
    with. That is your maximum compute budget. Now, don’t worry – we’re not going
    to blow through that all at once. You may not even need to spend it all. As we
    step through this book, I’ll help you figure out how to get early signals that
    your project is going to be successful, such as training on 1% of your data to
    ensure the performance is better than open source models. You want to hit key
    milestones throughout your pretraining project, and as you hit those milestones,
    you will get closer to achieving your end goal. That overall goal is also the
    same number you’ll use to figure out how much time it’s worth for you to spend
    on this project, how many people you’ll want to pull in, how many sprints to use,
    and so on.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦你确定了组织中的那个领域，拿出总预估收入的10%，或者是你觉得更合适的其他低百分比。这就是你的最大计算预算。别担心——我们不会一次性花光这些预算。你甚至可能根本不需要花完它。随着我们逐步阅读本书，我将帮助你弄清楚如何通过早期信号判断你的项目是否会成功，例如在`1%`的数据上进行训练，以确保性能优于开源模型。在你的预训练项目中，你需要达到关键的里程碑，随着你完成这些里程碑，你会越来越接近最终目标。这个总体目标也将是你用来决定花费多少时间、需要多少人手、要使用多少次冲刺等的数字。
- en: Once you have this target application in mind, along with both the estimated
    return and costs, you are ready to start bringing it to life! Start listing any
    datasets your organization already stores that are related to the application
    you want to build. Do you have relational databases with transactions relevant
    to this? Customer history? Click-stream data? Search results? What about images?
    Do you have any videos? Any audio files? Any experimental results? Push yourself
    to be creative in listing as many candidate datasets as you have. Consider taking
    a solid hour just to look around and see what your organization already has stored
    around this candidate application area. If it’s already mission-critical, then
    most likely you have quite a bit stored already.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了这个目标应用，并且清楚了预估的回报和成本，你就可以开始将其付诸实践了！开始列出你的组织中已经存储的、与您想要构建的应用相关的任何数据集。你是否有与此相关的事务型关系数据库？客户历史记录？点击流数据？搜索结果？那图像呢？有没有视频？有没有音频文件？有没有实验结果？尽量发挥创意，列出你所拥有的尽可能多的候选数据集。考虑花一个小时四处看看，看看你的组织已经存储了哪些与这个候选应用领域相关的数据。如果它已经是关键任务系统，那么很可能你已经存储了不少数据。
- en: If you don’t already have at least a few GB of data or, even better, a few 10s
    of GB, then you might want to consider gathering a new dataset from open source
    solutions. These might include any combination of over 6,000 datasets available
    through the *Papers With Code* site *(1)*. You can also look at the 8,000 datasets
    available from the *Hugging Face Hub* *(2)*. Remember, these datasets are available
    at no cost! Open source datasets are an excellent way to start proving the concept
    of your idea. Common datasets for language pretraining are *The Pile*, *Common
    Crawl*, *Wikipedia*, *CodeParrot*, and so on *(3)*. You can also look at the OSCAR
    corpus for multimodal pretraining. The **Vision Transformer** (**ViT**) *(4)*
    model was trained from scratch on ImageNet. You have plenty of options! You can
    also use all of these open source datasets to enhance your original dataset, using
    the best of both open source and proprietary options.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Something else to remember is that *pretraining explicitly benefits from unlabeled
    data*. The best pretraining projects happen when they leverage large volumes of
    unlabeled data and smaller volumes of labeled data. This is largely why pretraining
    foundation models is popular – most data in the world isn’t labeled. However,
    when we use a pretraining objective, as we learned about in the first chapter,
    we can easily train models to learn about this. Then, we fine-tune them using
    supervised data, which is usually smaller in quantity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: So, if you find yourself in a scenario with multiple hundreds of GBs of data,
    such as images, files, records, transactions, metadata, and time-series data,
    you may want to consider that as a top candidate for custom pretraining.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Top pretraining use cases by industry
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s highlight a few top use cases to pretrain custom foundational models
    by industry. These are areas in our world where pretraining and finetuning are
    already having an impact today:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '**Software** **and internet**:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search and discovery
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating documentation and code
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Questioning/answering
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hospitality** **and travel**:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer support and service
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Booking recommendations
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumer electronics**:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design automation
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fashion design automation
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Financial services**:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document summarization and generation
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal forecasts
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Media** **and entertainment**:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creativity enhancement
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Speeding up the generation of creatives (new images, new movies, better artifacts,
    and so on)
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the best creative to move to finals (best shot, best sequence, best
    melody, and so on)
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Health care and** **life sciences**:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protein modeling, drug discovery, and experimental prioritization
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Notes synthesis and diagnosis confirmation
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual results confirmation and experiment result prioritization
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scientific literature synthesis and experimental design suggestion
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Manufacturing** **and agriculture**:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part defects and building error detection
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall design automation of parts and products
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Autonomous product design
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Public** **sector governance**:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公共** **部门治理**：'
- en: Policy impact analysis automation
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政策影响分析自动化
- en: Policy suggestion automation
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政策建议自动化
- en: Political and philosophical difference reconciliation
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政治和哲学差异调解
- en: Budget impact assessment and analysis automation
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预算影响评估和分析自动化
- en: Let’s now move on to see how different your dataset is.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续看看你的数据集有多不同。
- en: Delta – how different is your dataset?
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Delta ——你的数据集有多不同？
- en: 'Now that you have some idea of what use case you are most interested in, and
    what datasets will give your organization the most value, it’s time to understand
    how unique your dataset is. This analysis matters because it will answer two questions:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经有了对最感兴趣的应用场景的某种理解，并且知道哪些数据集能为你的组织带来最大价值，现在是时候了解你的数据集有多独特了。这一分析很重要，因为它将回答两个问题：
- en: First, which models are already on the table for you to use, due to having been
    trained on similar data?
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，哪些模型已经可以供你使用，因为它们已经在类似的数据上训练过？
- en: Second, how well have those models performed?
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，这些模型的表现如何？
- en: This insight will start to give you a clue toward what performance you can hope
    to achieve on your datasets as a best-case scenario. Then, we’ll plug that expected
    performance number back into our total project value and make sure we’re still
    on track. The next chapter is completely dedicated to answering those questions.
    Here, we’ll learn how to pick apart your dataset. This is a good section for those
    who are new to data analysis.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这一见解将开始为你提供一个线索，指引你在最佳情况下能期望在数据集上实现的性能。然后，我们将把这个预期的性能数值重新带入我们整体项目的价值中，确保我们仍然在正确的轨道上。下一章将完全致力于回答这些问题。在这里，我们将学习如何分析你的数据集。这一部分对于那些刚接触数据分析的人来说非常有帮助。
- en: First, it’s always a good idea to spend time really analyzing any dataset you’re
    touching. Whether you are starting from something custom in your own database,
    or working with an open source option, anticipate spending at least a few hours
    getting to know it in little detail. Probably the best phrase I’ve heard to inspire
    this process is, *a good data science team asks more questions than they answer*.
    This is because *the act of analyzing a dataset is a living process, not a finite
    state*. Getting to know a dataset is a little bit like getting to know a person;
    it’s just that the way you ask questions and make observations is totally different.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，花时间真正分析你正在处理的任何数据集总是一个好主意。无论你是从自己数据库中的定制数据开始，还是使用开源选项，预计至少会花上几个小时来详细了解它。我听过的最能激励这个过程的短语是，*一个好的数据科学团队提出的问题比他们回答的问题还要多*。这是因为*分析数据集的过程是一个动态的过程，而不是一个有限的状态*。了解一个数据集有点像了解一个人；只不过你提问和观察的方式完全不同。
- en: Start by verbally describing the contours of your dataset. How many rows does
    it have? How many columns? How large are the images? How many tokens does it have?
    What features does it have? Are they numeric or categorical? Is it based on time?
    What metadata does it have? Make sure you have a good picture in your mind of
    what this dataset looks like. Talk with other people on your team about it until
    you feel confident and can answer questions quickly about the basics of your dataset
    composition. Use common data analysis techniques, such as Jupyter notebooks, to
    produce summary statistics and charts, and perform exploratory data analysis.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从口头描述你的数据集的轮廓开始。它有多少行？有多少列？图像有多大？它有多少个标记？它有哪些特征？这些特征是数值型的还是类别型的？它是基于时间的吗？它有哪些元数据？确保你对这个数据集的样貌有清晰的认知。和你团队中的其他人讨论，直到你感到自信，并能快速回答有关数据集组成的基本问题。使用常见的数据分析技术，如Jupyter笔记本，生成总结性统计和图表，并进行探索性数据分析。
- en: Critically, ask yourself, what real-world process was this dataset drawn from?
    How was this dataset acquired? We call this a sampling mechanism. If you are new
    to data analysis, and especially new to data analysis in applied settings outside
    of theoretical research, the first thing you’ll need to understand is that “not
    all sampling mechanisms are perfect.” To put it another way, you should get into
    the practice of assuming that there may be something wrong with your dataset.
    You need to critically evaluate the way your dataset was developed. Was it randomly
    collected? Or does all of the data have some underlying similarities? Any errors?
    The most important part of your data analysis process is to disabuse yourself
    of any underlying errors, inconsistencies, oddities, and faults in the raw data
    itself. You need to gain certainty that the data itself is indeed valid and reliable.
    Why? Because this certainty serves as a fundamental guarantee for everything you
    produce from this dataset. If the data isn’t reliable, your work can never be
    reliable.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是要问自己，这个数据集是从哪个现实世界过程中抽取出来的？这个数据集是如何获取的？我们称这为抽样机制。如果您是数据分析新手，尤其是在应用设置中进行数据分析而不是在理论研究中，您首先需要了解的是“并非所有抽样机制都是完美的”。换句话说，您应该习惯于假设您的数据集可能存在问题。您需要批判性地评估数据集的开发方式。它是随机收集的吗？或者所有数据都具有某些潜在的相似性？有任何错误吗？数据分析过程中最重要的部分是消除原始数据本身的任何潜在错误、不一致性、奇怪之处和缺陷。您需要确信数据本身确实是有效且可靠的。为什么？因为这种确定性是您从这些数据集产生的一切内容的基本保证。如果数据不可靠，您的工作永远不可能是可靠的。
- en: When you have an idea about your dataset, before that idea is proven true by
    the results you empirically observe, it’s called a **hypothesis**. A hypothesis
    is a concept you believe may be true about your dataset, or about any real-world
    process. However, because you currently lack empirical evidence validating the
    certainty of this hypothesis, you can’t state at the current time that it is objectively
    true. That’s why we call it a hypothesis!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当您对数据集有了想法，但在您通过实证观察到的结果证明之前，这被称为**假设**。假设是您认为可能适用于您的数据集或任何现实世界过程的概念。然而，由于目前缺乏实证证据来验证此假设的确定性，因此您不能断言它在当前时间是客观真实的。这就是为什么我们称其为假设！
- en: A core part of the scientific process, and as a corollary, your own development
    in machine learning is learning how to state this hypothesis clearly. You can
    phrase it as a simple question, something as basic as “which model solves this
    problem the best?”, “what does it mean to solve this type of problem optimally?”,
    or even, “how can we improve upon the state of the art in this area?”
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 科学过程的核心部分，以及作为推论的，您在机器学习中自身的发展，是学会如何清晰地陈述这一假设。您可以将其表述为一个简单的问题，例如“哪种模型最好地解决了这个问题？”，“什么是最优解决这种类型问题的意义？”，或者甚至，“我们如何在这个领域改进现有技术水平？”
- en: Once you have a hypothesis, also called a **research objective**, clearly stated,
    you then want to learn “how to design experiments that answer this question.”
    Experimental design is a surprisingly challenging skill! This includes the work
    of evaluating current research in certain areas, considering open questions and
    results others have demonstrated, and attempting to build upon them empirically.
    At the end of your project, you want to have clear empirical results you can point
    to that validate your work. We’ll discuss this more in the following chapters
    on model evaluation, but it’s a critical topic to keep in mind.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了假设，也称为**研究目标**，清晰地表达出来，接下来您需要学习“如何设计实验来回答这个问题”。实验设计是一项令人惊讶地具有挑战性的技能！这包括评估某些领域的当前研究工作，考虑开放性问题和他人展示的结果，并试图从经验上构建。在项目结束时，您希望有明确的经验性结果可以证明您的工作。我们将在关于模型评估的后续章节中进一步讨论这一点，但这是一个需要牢记的关键话题。
- en: Next, let’s learn about sizing our datasets.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们学习如何调整我们的数据集大小。
- en: Use the scaling laws to size your datasets
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用缩放定律来确定您的数据集大小。
- en: At this point, you should have identified your datasets, have a basic understanding
    of them, and be able to describe how they are similar to and different from previous
    datasets and research work in your chosen domain. It’s helpful to have at least
    a handful of papers to refer to so that you can do so when you’re stuck.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该已经确定了您的数据集，并对其有了基本的理解，能够描述它们与以前的数据集及您选择的领域中的研究工作的相似之处和不同之处。有几篇论文可以参考会很有帮助，这样在遇到困难时就能派上用场。
- en: In this section, we’ll explore how large your dataset should be in order to
    produce the expected results on a pretraining or fine-tuning project, which clearly
    validates the time and compute expenses you’ll be racking up. We’ll also discuss
    certain characteristics you’ll want this dataset to have, such as sufficient variety,
    quality, and lack of duplicates. The entirety of the next chapter is dedicated
    to picking the right model, including the size and scope, but for now, we’ll focus
    on the dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨数据集应该有多大，才能在预训练或微调项目中产生预期的结果，从而清楚地验证你将投入的时间和计算开销。我们还将讨论你希望这个数据集具备的某些特征，比如足够的多样性、质量和没有重复项。下一章的内容将完全专注于选择合适的模型，包括模型的大小和范围，但现在我们将重点讨论数据集。
- en: First, it’s helpful to know that there is a very large gray area between so-called
    large and small models and the corresponding size in datasets that they tend to
    run on. Under no circumstances should you think that you only need multiple terabytes
    and/or petabytes to think about pretraining, or even models that don’t fit on
    a single GPU. You can produce meaningful results with unsupervised data simply
    by continuing to pretrain your model, rather than necessarily starting pretraining
    from scratch, and still hit your business and intellectual goals. Depending on
    your project, and how niche and interesting it may be, you can easily showcase
    some useful work on just under 1 GB of data. So, don’t hesitate just because you
    aren’t sitting on the Fort Knox of all web data; just start from where you are!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，了解所谓的大模型和小模型之间，以及它们通常运行的数据集大小之间有一个非常大的灰色地带是很有帮助的。在任何情况下，你都不应该认为，只有多达数TB和/或PB的数据才能考虑预训练，或者认为即使是无法放入单个GPU的模型也不值得考虑。你可以通过继续预训练你的模型，而不必从零开始预训练，单凭无监督数据就能产生有意义的结果，并且仍然可以实现你的商业和知识目标。根据你的项目以及它可能的独特性和有趣程度，你完全可以在不到1
    GB的数据上展示一些有用的工作。因此，不要因为没有大量网络数据而犹豫不决；从你现有的资源开始吧！
- en: Next, you’ll need to understand something called the scaling laws *(6)*. These
    are a set of theories and formulas about how large models behave at different
    scales, notably as power laws. These formulas themselves are derived from empirical
    behavior at varying scales. You can use them to determine what model and dataset
    sizes are optimal for a given compute budget, and vice versa. To some degree,
    these laws, and their updated versions as presented in *Chinchilla*, are independent
    of the model architecture itself. This implies that the biggest way to improve
    model accuracy is scaling up the size, rather than alterations in the model architecture
    itself. Kaplan originally presented scaling laws explicitly within the context
    of language models leveraging the transformer model architecture. However, given
    the 10x increase in accuracy that this validated hypothesis gave rise to in the
    GPT-3 paper, I and many others *(7)* believe there is a reason to explore this
    basic relationship outside of **large language models** (**LLMs**), including
    vision and multimodal especially.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要理解一种叫做扩展法则的东西 *(6)*。这些是关于大模型在不同规模下如何表现的理论和公式，尤其是作为幂律。这些公式本身是通过不同规模的经验行为推导出来的。你可以用它们来确定给定计算预算下，什么样的模型和数据集大小是最优的，反之亦然。在某种程度上，这些法则，以及在*Chinchilla*中提出的更新版本，与模型架构本身是独立的。这意味着，提高模型准确性的最大方式是扩大规模，而不是改变模型架构本身。Kaplan最初在语言模型的背景下，明确提出了扩展法则，并利用了变换器模型架构。然而，考虑到这一被验证的假设在GPT-3论文中引发了准确度的10倍提升，我和许多其他人
    *(7)* 相信，有理由在**大语言模型**（**LLMs**）之外探索这种基本关系，特别是在视觉和多模态领域。
- en: You might be thinking, so what? Why is this such a big deal? Obviously, there’s
    some balance you’d want to achieve across your dataset, compute size, and model,
    so what gives? The reason Kaplan’s work was such a breakthrough is that *having
    a valid formula to quantify the optimal compute, data, and model values lets you
    estimate what range of loss your model might achieve*. To put it another way,
    now that we have scaling laws, we can figure out mathematically what loss we should
    expect at the end of our model training run, within a given range. And for training
    runs that can send compute costs into the hundreds of thousands of dollars, if
    not millions, this knowledge is incredibly valuable. OpenAI has validated this
    in its GPT-4 technical report, claiming to be able to accurately forecast its
    model’s loss given changes in scale.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: This opens a new area of questions. What other aspects of machine learning have
    empirically observable laws? In what other ways can we be inspired by physics
    to discover formulaic patterns that rely on mathematical relationships, beyond
    the inner workings of the model itself? This matters because, today, the vast
    majority of machine learning is trial and error. We hope something works, we try
    it out, learn from our experiment, and then take another step. However, I believe
    scaling laws point to a future where machine learning is increasingly enhanced
    with simple, efficient, and fast checks, rather than long-running computational
    experiments. What if we’ve simply been thinking about this in the wrong way for
    decades?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentals – scaling laws of neural language models
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you take a look at the original paper: *Scaling Laws of Neural Language
    Laws*, you’ll find out that one core concept is central to their analysis – proportionality.
    Kaplan et al here argue that changes in your dataset size or model size should
    be accompanied by proportional changes in the companion quantity. To put it another
    way, if you use a bigger dataset, you should use a bigger model, and vice versa.
    Now, exactly how strong this relationship is, what describes it, what constants
    are involved, and precisely how much scaling should be undertaken up or down is
    entirely at the heart of their paper.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'While it is helpful to know that a relationship is proportional, it is insufficient.
    Kaplan et al suggest and find empirically that the optimal scaling of neural language
    models follows a power law. Power laws are actually quite simple; they’re just
    about exponents at the end of the day. If two quantities follow a power law, you
    can assume that one side of the equation follows exponential change:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18942_02_001.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: To estimate the early-stopped test loss of a transformer-based training regime,
    given the size of both a dataset and a model, Kaplan et al suggest the following.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to unpack this in very simple terms – first, the left-hand side:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '**L** = the final loss of your model, stopped early, on your test set'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**N** = the number of trainable parameters in your model'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**N** = 你模型中可训练参数的数量'
- en: '**D** = the size of your dataset in tokens (for language)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**D** = 数据集中令牌的大小（对于语言）'
- en: Now, you understand that the entire equation is about computing the potential
    loss of your model, which would be great to know ahead of time! The rest of the
    terms on the right-hand side are about how to get there. All four of ![](img/B18942_02_002.png),
    ![](img/B18942_02_003.png), ![](img/B18942_02_004.png), and ![](img/B18942_02_005.png)
    describe constants that must be discovered from the dataset and training regime.
    Think of these as hyperparameters; we want to find some constant terms that describe
    our specific dataset and model. In many cases, however, we can simply use the
    constants as presented in their work.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经理解整个方程是关于计算模型的潜在损失，这样你就可以提前知道了！右侧的其余项是关于如何达到这个目标的。所有这四个项：![](img/B18942_02_002.png)、![](img/B18942_02_003.png)、![](img/B18942_02_004.png)
    和 ![](img/B18942_02_005.png) 都描述了必须从数据集和训练过程中发现的常数。可以把它们看作超参数；我们希望找到一些常数项，用以描述我们的特定数据集和模型。然而，在许多情况下，我们可以直接使用他们工作中呈现的常数。
- en: Kaplan et al found constant values for each of these in their training runs
    by fitting the loss curves with their scaling law functions. Using mathematical
    extensions of their core equations, they were able to accurately fit their learning
    curves. Making that fit helped them discover constants that proved useful throughout
    the rest of their studies.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Kaplan 等人在他们的训练过程中通过拟合损失曲线与其缩放法则函数，发现了每个常数的值。利用他们核心方程的数学扩展，他们能够准确地拟合学习曲线。进行这种拟合帮助他们发现了在其余研究中都非常有用的常数。
- en: In the real world, once you’ve performed some preliminary data analysis and
    have a good idea of what characteristics you’ll need to train an adequate model,
    most data science teams will immediately move on to training your first model.
    This is because the machine learning process is generally iterative; you’ll test
    a variety of methods, see which ones are the most promising at a given point in
    time, scale and evaluate, and then try again. For the purposes of a larger book
    on the topic, I’ll go into more detail on two key topics that can help you improve
    your dataset. These are steps you probably wouldn’t implement right at the beginning
    of your data science journey but that you should come back to over time to increase
    the overall quality of your work. The first is bias detection and mitigation,
    and the second is dataset enhancements.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，一旦你进行了一些初步的数据分析，并对你需要的特征有了清晰的了解，大多数数据科学团队会立即开始训练你的第一个模型。这是因为机器学习过程通常是迭代的；你会测试多种方法，看看在特定时间点哪些方法最有前景，然后扩大规模并进行评估，接着再尝试。为了更全面地阐述这个主题，我将详细介绍两个可以帮助你改善数据集的关键话题。这些步骤你可能不会在数据科学旅程的最初就实施，但随着时间的推移，你应该不断回顾它们，以提高工作整体质量。第一个是偏差检测与缓解，第二个是数据集增强。
- en: Bias detection and mitigation
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差检测与缓解
- en: The trajectory of the word “bias” is interesting in that, in the last 15 years,
    it’s come full circle. Originally, *bias* was arguably a statistical term. Formally,
    it implied that a sample size was improperly constructed, giving excessive weight
    to certain variables. Statisticians developed numerous methods to identify and
    reduce bias to evaluate studies properly, such as those used in randomized control
    trials in public health or policy evaluations in econometrics. Basic tactics include
    making sure that the treatment and control groups are roughly the same size and
    have roughly the same characteristics. Without a guarantee of that basic mathematical
    equivalence, or more realistically as close to it as the research team can get,
    it’s difficult to trust that the results of a study are truly valid. The results
    themselves are subject to bias, simply indicating the presence or absence of basic
    characteristics, rather than implying anything meaningful about the treatment
    itself.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: “偏差”一词的轨迹非常有趣，因为在过去的15年里，它已经走完了一圈。最初，*偏差*无疑是一个统计学术语。正式来说，它意味着样本大小不当，给某些变量赋予了过大的权重。统计学家们开发了许多方法来识别和减少偏差，以正确评估研究，例如公共卫生中的随机对照试验或计量经济学中的政策评估所使用的方法。基本策略包括确保处理组和控制组大致相同大小，并且具有大致相同的特征。如果没有保证这种基本的数学等价性，或者更现实地说，研究团队尽可能接近它，就很难相信研究结果是真正有效的。结果本身会受到偏差的影响，简单地指示基本特征的存在与否，而不是暗示关于治疗本身的任何有意义的信息。
- en: In the last 5 years, however, numerous studies have demonstrated the inability
    of machine learning models to perform adequately for certain groups of people
    under certain scenarios. The most egregious examples include facial recognition,
    image detection, employment, judicial decision-making, and countless others. Large
    technology companies have been the first to come under fire here, with financial
    institutions and even public policy organizations also coming in tow. These accusations
    are valid. While bias in datasets has always been a big problem in machine learning,
    the impact on human lives across the world is now so obvious that it deserves
    significant dialogue, discussion, solutioning, and monitoring. If bias is present
    in any dataset, it is almost certain to creep into the model itself. Models certainly
    are not objective; they are effectively children of the datasets they were trained
    on. Bias has now come full circle, starting in statistics, resonating with human
    rights, and now driving machine learning research.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '*The word bias has now come full circle; starting in statistics, resonating
    with human rights, and now driving machine* *learning research.*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of developing and attempting to deploy a machine learning model,
    and especially a large one with its own pretraining regime, you need to know a
    few things. First, the most reliable way to mitigate bias is by increasing and
    decreasing the different aspects of your datasets. This is especially obvious
    in computer vision. If you add more images of certain groups – for example, African
    Americans – your model will be able to recognize them. If you don’t have those
    images in sufficient numbers, your model won’t be able to recognize them in applications.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: For natural language, this question ends up being even more challenging. This
    is because most of the data in language isn’t already tabulated into different
    social categories, such as gender, race, religion, and sexuality. For all of those
    types that we care about and know we want to protect, we need to introduce our
    own methods to identify, compare, and synthesize them across our datasets. Just
    doing this alone is tough, as you can imagine.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Identifying bias is the first critical step in your journey toward responsible
    ML. Right at the beginning, you need to be able to answer two critical questions
    about your dataset:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: First, what types of bias are present in my dataset currently?
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, how much risk does this bias expose to my organization?
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think about risk in terms of impact on your customers, particularly the predictions
    from a biased ML model. If your model has the potential to cause harm to your
    customers, such as denying a loan, downgrading an employment submission, recommending
    harmful content, or even denying bail or other legal sentencing, then by all means
    make bias detection and mitigation your highest priority!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'While there are a variety of frameworks concerning responsible AI, I like to
    boil these down to four key actions to take. In terms of bias in ML models trained
    on biased datasets, your four key steps are **expect**, **identify**, **mitigate**,
    and **monitor**:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '**Expect**: When picking ML projects and datasets, expect that every dataset
    will have some type of bias at the root. Ask yourself, what problem is my ML model
    trying to solve, and what issues will I run into if I don’t have enough of certain
    types of data?'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Identify**: Then, use a variety of techniques to identify the bias present
    in your dataset. Make sure you know right at the outset how many attributes within
    certain groups you do or do not have. Keep working on this until you can quantify
    at least a handful of different types of bias metrics. See the following note
    box for some suggestions on how to identify bias.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mitigate**: Once you’ve identified the bias in your dataset numerically,
    take steps to mitigate the bias. Increase or decrease certain aspects of your
    dataset. Use augmentation, up- or down-sampling, and data transformations to drive
    down your bias metrics until they hit a less dangerous threshold.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Monitor**: Once you’ve deployed your ML model, the adventure continues. You
    can use the same bias detection methods you leveraged in *step 2* to monitor the
    model deployed in your application. Ensure that your application and overall system
    design include statistical monitoring and set thresholds for acceptable statistical
    levels. When the model starts to meet or exceed your thresholds, start manually
    reviewing the model predictions and initiate your training pipeline. Keeping humans
    in the loop, particularly those who are both knowledgeable and caring, is the
    best way to reduce the risk of biased predictions.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do bias detection and monitoring
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know bias is important, how do we find it mathematically? And once
    we’ve done that, how do we mitigate and monitor? There are many ways of doing
    this, and we can categorize these in their respective domains:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '**Tabular**: Detecting bias in tabular data amounts to computing some statistics.
    First, you’ll need to have some ground truth label in your dataset, indicating
    the status inside or outside of a certain group. Notably, for many teams, this
    alone presents a sizeable problem. However, the logical counter to this is simple.
    Expect your data to be biased, regardless of whether or not you have a column
    labeling it as members of certain groups. Introducing this label *is the only
    way to identify bias intrinsic to your dataset and, ultimately, remove it*. Otherwise,
    try to use a proxy, although these are known to be faulty.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Assuming you have a label, such as gender or race, then you have two types of
    metrics – pretraining and post-training metrics. One simple pretraining statistic
    is **class imbalance**. Class imbalance is simply the number of observations from
    your advantaged group, minus the number in the disadvantaged group, divided by
    your overall dataset size. If your class imbalance is too high, your dataset and
    subsequent model are certain to be biased.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: One common post-training metric is disparate impact, which is defined simply
    as the number of positive predicted labels in your disadvantaged group, divided
    by the same in your advantaged group. Intuitively, this measures your model’s
    likelihood of predicting positive for different groups, which as you can imagine
    is critical in certain domains such as employment or law. There is some legal
    precedent for using 4/5, or 80%, as the lower threshold here.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '**Vision and language**: Lastly, both vision and language have different approaches.
    In language, it’s common to evaluate a language model’s learned preference to
    suggest a given category under certain conditions, such as placing “he” or “her”
    under some employment criteria.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: With vision, you might use a pretrained text classifier to ensure that datasets
    are balanced before training. Also, you can clearly indicate a model’s poor behavior
    in detecting certain classes – for example, certain groups in image recognition.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing your dataset – multilingual, multimodal, and augmentations
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, now that you’ve learned how to pick a dataset, compare it with research
    datasets, determine the right approximate size, and evaluate bias, let’s dive
    into enhancing the dataset. In particular, we’ll look at a few dimensions – **multilingual**,
    **multimodal**, and **augmentations**. All three of these typically come a bit
    later in your ML projects, especially after the first few versions of your models
    have been trained and you’re looking for the next idea to give you a boost.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Personally, I think there are few applications in the world where multilingually
    *isn’t* a strong added value. *Multilingual* just means multiple languages. While
    many of the state-of-the-art language models were originally trained on English-only
    text, researchers in the last few years have made strong efforts to increase the
    lingual diversity of these corpora. That means they’re adding support for a lot
    of languages. In 2022, Hugging Face led a massive worldwide effort to democratize
    the creation of large language models, calling their program *Big Science* *(8)*.
    This led to the creation of a novel model they named the **BigScience Open-Science
    Open-Access Multilingual Language Model** (**BLOOM**). Hugging Face hopes to improve
    upon the state of the art in multilingual use cases especially, such as zero-shot
    language translation. However, the model was shown to perform worse than GPT-3
    in many cases, leading us to believe that the best models may be single-language
    only.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Frankly, being multilingual is just good business. For any product you develop,
    any program you run, and any service you offer, you are limited in interacting
    with your potential consumer through language at the end of the day. Think of
    a language as a market. While you’re developing your product, you want to bring
    it to as many markets as you can. Ultimately, that means as many languages as
    you can. For this reason, I’m optimistic that the industry will find a better
    way to incorporate multiple languages in possibly the same model without worsening
    results. Perhaps this is as simple as formatting a dataset appropriately, as in
    the case of chain-of-thought or instruction tuning.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Briefly, let’s explore adding additional modalities. Simply put, this means
    different types of datasets, such as adding vision to text, or vice versa. I introduced
    this concept in more detail at the close of [*Chapter 1*](B18942_01.xhtml#_idTextAnchor016).
    Here, I’d like to simply point out that *if you have text with images, or images
    with your text, try to use it*. Once you’re invested in a project, with many hours
    spent on analyzing the data, training, evaluating models, deploying these, and
    so on, why would you not go the extra mile to explore adding other modalities?
    Particularly when it has the potential of raising accuracy, which it does. From
    the perspective of a model, another modality is just another type of embedding.
    You’ll most likely want to use some model pretrained elsewhere to convert a raw
    image into embeddings – that is, before adding them as another input to your model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: There are trade-offs here; increasing the size of your model will increase its
    runtime. Increasing your dataset also increases your data transformation costs.
    Adding another step in the data transformation makes hosting more complex, meaning
    you may need to revisit the system design to deploy your model. All of these trade-offs
    are worthy of discussion, and ultimately, you’ll need to prioritize the projects
    that add the most value for your teams and your customers, which could very well
    include language-only models.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The other reason I’m optimistic about multimodal projects generally, as opposed
    to language-only projects, is that the visual domain carries so much information
    to humans. Humans learn to see before they learn to speak, and so many of our
    experiences and knowledge are gathered visually. For this reason, I believe foundation
    models will continue to converge around joint vision and language tasks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Finally, data augmentation is a simple and easy step to improve the accuracy
    of your models without adding a ton of extra work to get it. The core idea is
    that you’re adding some degree of variety in your dataset and slight changes in
    the provided samples, which will help your model learn the difference between
    signal and noise. Both text and vision have well-tested methods for augmentation.
    With vision, this is frequently as simple as pixel manipulations, light color
    manipulations, or image rotations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: With text, this can be substituting synonyms, sentence-level reconstruction,
    or lightweight punctuation modifications. The trick is that you don’t want to
    change the basic mechanism you are trying to learn. If you’re training an image
    detection model, don’t modify any of the images so that you can’t detect the images.
    If you’re training a text classifier, don’t alter the text so much that it moves
    into a different class.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation is usually less of an issue in large-scale pretraining, where most
    datasets are so large, as they already include more than enough noise and variation.
    It does, however, seem like a promising avenue for bias reduction especially.
    Another key technique for pretraining is reducing duplicate text. This is especially
    key in web data, where memes, comments, and threads can easily render the same
    text many hundreds of times across platforms and users.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve learned all about the early stages of preparing your data, let’s
    do a quick recap of what you just learned before we move on to preparing your
    model!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a wide variety of use cases for foundation modeling,
    encompassing scenarios where you can fine-tune an existing foundation model and
    where pretraining itself is competitive. We provided a simple economics framework
    to help you make the case for your pretraining project, notably by tying it to
    how much you expect your business to increase based on a more accurate model.
    After that, we talked about evaluating your dataset, comparing it to research
    datasets, and learning how to think critically about its sampling mechanism. We
    set up some basic ideas to use this critical thinking for framing experiments,
    which we’ll continue in the next chapter. We learned about the scaling laws and
    presented an open source notebook you can use to find which dataset size will
    help you hit performance levels, given fixed model and compute budgets. We talked
    about detecting and mitigating bias in your datasets, along with enhancing these
    with augmentation, modalities, and languages.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Next up is model preparation!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please go through the following content for more information on a few of the
    topics covered in this chapter:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '*Papers With* *Code*: [https://paperswithcode.com/datasets](https://paperswithcode.com/datasets).'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Hugging Face* *Hub*: [https://huggingface.co/datasets](https://huggingface.co/datasets)'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Hugging* *Face*: [https://huggingface.co/datasets?task_ids=task_ids:language-modeling&sort=downloads](https://huggingface.co/datasets?task_ids=task_ids:language-modeling&sort=downloads)'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT* *SCALE*:
    [https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scaling Laws for Neural Language Models: [https://arxiv.org/pdf/2001.08361.pdf](https://arxiv.org/pdf/2001.08361.pdf)'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Training Compute-Optimal Large Language Models: [https://arxiv.org/pdf/2203.15556.pdf](https://arxiv.org/pdf/2203.15556.pdf)'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*BigScience Episode #5 – Challenges & Perspectives in Creating Large Language*
    *Models*: [https://bigscience.huggingface.co/acl-2022](https://bigscience.huggingface.co/acl-2022)'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
