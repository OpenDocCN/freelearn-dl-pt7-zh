- en: Reinforcement Learning in Game Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Games have always been a phenomenon of human culture where people manifest intelligence,
    interaction, and competition. But games are also an important theoretical paradigm
    in logic, **artificial intelligence** (**AI**), computer science, linguistics,
    biology, and lately, increasingly in the social sciences and in psychology. Games,
    especially strategy games, offer reinforcement learning algorithms an ideal and
    privileged environment for testing, as they can act as models for real problems.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to use reinforcement learning algorithms
    to address a problem in game theory. By the end of the chapter, we will have learned
    the fundamental concepts of game theory. We will also learn how to install and
    configure the OpenAI Gym library, understand how the OpenAI Gym library works,
    and learn how to use Q-learning to solve game problems. Apart from that, we will
    understand how to make a learning and testing phase, and learn how to develop
    OpenAI Gym applications using R.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding game theory essentials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring game theory applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing the tic-tac-toe game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the OpenAI Gym library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robot control system using the FrozenLake environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2tffzMD](http://bit.ly/2tffzMD)'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding game theory essentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Game theory is a mathematical science that studies and analyzes the individual
    decisions of a subject in situations of conflict, or strategic interaction, with
    other rival subjects aimed at the maximum profit of each subject. In such situations,
    the decisions of one can influence the results achieved by the other(*s*), and
    vice versa, according to a feedback mechanism, by seeking competitive and or cooperative
    solutions through models.
  prefs: []
  type: TYPE_NORMAL
- en: The theory of games has its distant origins in 1654, following correspondence
    between Blaise Pascal and Pierre de Fermat on the calculation of probabilities
    for gambling. The expression **game theory** was first used by Emil Borel in the
    1920s. Borel developed the **Théorie des jeux**, a zero-sum game with two players,
    and tried to find a solution known as Von Neumann's concept of solving a zero-sum
    game. It is generally acknowledged that the release of the book *Theory of Games
    and Economic Behavior*, by John von Neumann and Oskar Morgenstern in 1944, marked
    the birth of modern game theory, although other authors (such as Ernst Zermelo
    and Armand Borel) had written about game theory.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of these two scholars can be described informally as an attempt to
    describe human behavior mathematically in the cases in which the interaction between
    men involves the winning, or dividing, of some kind of resource. The most famous
    scholar to have subsequently dealt with the theory of games, in particular with
    regard to **non-cooperative games**, is the mathematician John Forbes Nash jr.,
    to whom the Ron Howard film *A Beautiful Mind *is dedicated.
  prefs: []
  type: TYPE_NORMAL
- en: Eight Nobel prizes in economics were awarded to scholars who dealt with game
    theory. A Crafoord Prize has also been awarded to John Maynard Smith, a long-time
    distinguished biologist, geneticist and professor at the University of Sussex,
    for his contribution to this field.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will introduce the concepts underlying game theory
    and then analyze the main types of games faced by researchers.
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts of game theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Game theory has victory as its main objective. Everyone must be aware of the
    rules of the game and be aware of the consequences of every single move. The set
    of moves that an individual intends to do is called a strategy. Depending on the
    strategies adopted by all the players, each one receives a pay-off according to
    an adequate unit of measurement. The reward can be positive, negative, or null.
    A game is called a **constant sum** if, for each player's payout, there is a corresponding
    loss for others. A **zero-sum** game between two players represents a situation
    in which the reward is paid from one player to another. The strategy to follow
    is satisfactory for all players; otherwise, it is necessary to calculate and maximize
    the player's mathematical hope or expected value, which is the weighted average
    of the possible rewards, each weighed for the respective probabilities of the
    event.
  prefs: []
  type: TYPE_NORMAL
- en: In a game, there are one or more contenders who try to win the game, that is,
    to maximize their winnings. The winnings are defined by a rule that establishes
    quantitatively what the contenders win according to their behavior. This function
    is called a function of payments. Each player can undertake a finite-infinite
    number of actions or decisions that determine a strategy. Each strategy is characterized
    by a consequence for the player who has adopted it and which can be a reward (positive/negative).
    The result of the game is completely determined by the sequence of their strategies
    and the strategies adopted by the other players.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do you characterize the result of the game for each player? If you measure
    the consequence of a strategy in terms of reward, each strategy can be matched
    with a value: a negative value will indicate a payment to the opponent, such as
    a penalty; while a positive value will indicate winnings, that is, the collection
    of a prize. The gain or loss due to the generic player associated with his or
    her strategy and the strategies taken at a given moment by all the remaining players
    is expressed by the monetary value indicated by the payment function.'
  prefs: []
  type: TYPE_NORMAL
- en: The decisions taken by a player naturally collide, or are in accordance with
    the decisions made by the other players, and from such situations derive various
    types of games.
  prefs: []
  type: TYPE_NORMAL
- en: A useful tool to represent the interactions between two players, two companies,
    or two individuals is a double-entry decision matrix or table. This decision table
    shows the strategies and winnings of a game conducted by two players. The decision
    matrix is therefore a representation through which we catalog all the possible
    results of the interactions between players, and we assign the value of the reward
    that in each situation competes to each player. Another form of representation
    concerns the sequence with which each decision is taken, or the actions are conducted.
    This characteristic of each game can be described by means of a tree graph, representing
    every possible combination of how the contenders play from the initial state to
    the final states where the winnings are distributed.
  prefs: []
  type: TYPE_NORMAL
- en: 'To describe a strategic situation, four basic elements are required:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Players**: The decision makers in the game (who is involved?)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions**: The possible actions, or moves, that players can choose from (what
    can they do?)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Strategies**: The action plans of the players (what are they going to do?)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Winnings**: The possible gains that players get (what do they earn?)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A strategy is therefore a complete and contingent plan, or decision-making decision,
    that specifies how the player must act in any possible circumstances in which
    he may be called upon to decide. Being a complete contingent plan, a strategy
    often defines which action a player must choose in circumstances that may not
    be achieved during the game. In the following section, the games will be classified
    and a short description of each topic will be proposed.
  prefs: []
  type: TYPE_NORMAL
- en: Game types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Games can be classified according to different paradigms, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Cooperation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Symmetry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequencing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will see a short description of these topics.
  prefs: []
  type: TYPE_NORMAL
- en: Cooperative game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A cooperative game is presented when the interests of the players are not in
    direct opposition to each other, but there is a commonality of interests. Players
    pursue a common goal, at least for the duration of the game; some of them may
    tend to associate to improve their **pay-off**. The guarantee is given by the
    binding agreements. What is the mathematical representation of a shared interest?
    The concept of the union of individual interests in a coalition or alliance is
    expressed by the definition of essential play; while the value of a generic coalition
    is measured by a function called a characteristic function.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, in non-cooperative games, also called **competitive games**, players
    cannot enter into binding agreements (even by regulation), regardless of their
    objectives. The solution given by John Nash, with his *Equilibrium of Nash*, applies
    to this category, and it is probably the most famous notion of the whole theory,
    thanks to its vast field of applicability. The criterion of rational behavior
    adopted in non-cooperative games is individual and is called the maximum strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a symmetrical game, the profits deriving from the adoption of a strategy
    depend only on the other strategies employed, not by those who are playing them.
    If players' identities can be changed without changing the payoff, then a game
    is symmetrical.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, in asymmetric games there are no identical series of strategies
    for both players. It is possible, however, that a game has identical strategies
    for both players, but that it is asymmetric.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-sum game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Zero-sum games are a special case of constant-sum games in which the constant
    is zero. Zero-sum games model all the conflicting situations in which the contrast
    of the two players is total: the winning of a player coincides exactly with the
    loss of the other. In other words, the sum of the winnings of the two contenders
    according to the strategies used is always zero. In chess, for example, this means
    that the only three possible results are: victory, defeat, and draw (reward: +1,
    -1, and 0).'
  prefs: []
  type: TYPE_NORMAL
- en: Sequential games
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In sequential games, subsequent players retain some knowledge of previous actions.
    This does not mean that they know every action of previous players. For example,
    a player may know that a previous player has not performed a certain action, while
    he does not know which other available actions the first player performed.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have learned to classify games based on some paradigms. Why is analyzing
    games so important? This is due to the fact that many real-life problems can be
    tackled by deriving the solutions obtained from game theory. In the next section,
    we will see examples.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring game theory applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Game theory has always interested many researchers because of its usefulness
    in the practical field and in all fields of human work, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Philosophy**: This has always analyzed game theory because it provides a
    way to clarify the logical difficulty of some philosophers, such as Kant, Rousseau,
    Hobbes, and other social and political theorists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Economy**: Many of the speculations in the business world can be modeled
    using the methodology of game theory. A famous example is that of the similarity
    between the setting of oligopoly prices and the prisoner''s dilemma.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Biology**: Although nature is often considered brutal, there is cooperation
    between many different species. The reason for this coexistence can be modeled
    using game theory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI**: The human being can make decisions based on the environmental stimuli
    he receives. Instead, machines can plan only if programmed with decision lists
    based on several conditions. This limit can be overcome by artificial intelligence
    that can give the machine the ability to make new unplanned decisions from their
    creators. To do this, programs must generate new payoff matrices based on observed
    stimuli and experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we will analyze a widespread game and see how to deal
    with reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Playing the tic-tac-toe game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The game of tic-tac-toe is perfect as a first example of a game solved with
    the use of reinforcement learning: In fact, compared to other strategy games,
    it has a few simple rules. Furthermore, it is relatively easy to program and,
    since a game can last up to nine moves, the training of an evaluation function
    is extremely rapid. The tic-tac-toe is a game with perfect information for two
    players, where each player is assigned a symbol to play with. The symbols usually
    used are the cross and the circle. The game is started by the player who uses
    the cross.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The game grid has a 3x3 structure and presents nine initially empty cells as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3cf2eff-9c0b-47a8-a384-34b51a504344.png)'
  prefs: []
  type: TYPE_IMG
- en: In turn, players choose an empty cell and draw their own symbol. The player
    who manages to place three of his symbols in a horizontal, vertical, or diagonal
    line wins. If the game grid is filled without any of the players having succeeded
    in completing a straight line of three symbols, the game ends in a draw. So, if
    played correctly, tic-tac-toe will end in a draw, making the game useless.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will introduce the `tictactoe` package to play
    the game using the Q-learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The tictactoe package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To address the tic-tac-toe game, we will use the `tictactoe` package available
    on the CRAN website. This package implements the tic-tac-toe game to play on a
    console, either with human or AI players. Various levels of AI players are trained
    through the Q-learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table gives some information about this package:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Package | `tictactoe` |'
  prefs: []
  type: TYPE_TB
- en: '| Date | 2017-05-26 |'
  prefs: []
  type: TYPE_TB
- en: '| Version | 0.2.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Title | Tic-Tac-Toe Game |'
  prefs: []
  type: TYPE_TB
- en: '| Author | Kota Mori |'
  prefs: []
  type: TYPE_TB
- en: With the help of this package, we will play a first part with the computer to
    highlight the features of the game, and then we will train an artificial agent
    to play a game by following the best policy to get the maximum number of wins.
  prefs: []
  type: TYPE_NORMAL
- en: Playing a tic-tac-toe game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To begin, we will see how to set the game environment, and we will start a
    first game:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First we have to import the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can start the tic-tac-toe game on the R console using the `ttt()` function
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ttt_human()` creates a human tic-tac-toe player; if we wish we can also set
    a name using the name attribute (for example `name = "GIUSEPPE"`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ttt_random()` sets a random player, which merely places the opposite symbol
    (the circle) in one of the places available in a completely random way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The console returns the following grid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As anticipated the game is based on a simple 3 x 3 grid; to facilitate the identification
    of a cell, the columns are named with the letters A, B, C, while the rows with
    the numbers 1,2,3\. This means that the first cell on the top left will be identified
    with the symbol A1\. The last line printed on the console invites player 1 (`GIUSEPPE`)
    to make his move.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by placing the X in cell A1, the following result is returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As we can see in the preceding code block, the X has been positioned correctly
    in the upper-left cell, then the player 2 (computer) has placed his symbol (O)
    in a free position in a random way. Once again the last row invites player 1 to
    make the next move. We can proceed this way until the game is completed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three results are available—0, 1, and 2, which indicate the draw, the victory
    of player 1, and the victory of player 2, respectively, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this case, I won, but only because the computer put its symbols at random
    without following a strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Training the agent using Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can train the agent to follow a strategy. Let''s see how:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start we can simulate games in order to check the results obtained by two
    artificial agents when they play with each other:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the following functions have been used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ttt_ai()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ttt_simulate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `ttt_ai()` function creates an artificial tic-tac-toe game player. We have
    not used any arguments, in reality it is possible to use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: Player name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`level`: The AI strength must be an integer from 0 (weakest) to 5 (strongest).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `level` argument defines the effectiveness of the agent we are creating;
    from 0 to 5, its skill in game management increase, making the opponent's game
    more difficult. In the previous instructions, we already created an artificial
    agent using the `ttt_random ()` function that represents an alias of the  `ttt_ai
    ()` function in which the level of the agent is set by default equal to 0.
  prefs: []
  type: TYPE_NORMAL
- en: The `ttt_ai ()` function returns an object to which the `getmove ()` function
    is associated; this function accepts an object of the `ttt_game` type, and returns
    an optimal move using the political function.
  prefs: []
  type: TYPE_NORMAL
- en: A `ttt_ai` object has the value and policy functions. The value function associates
    a game state with the evaluation from the point of view of the first player. The
    political function associates a game state with a set of optimal actions obtained
    from the evaluation of the value function. These functions are trained through
    an algorithm based on Q-learning that we discussed extensively in [Chapter 7](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml),
    *Temporal Difference Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second function used, I refer to the `ttt_simulate()` function, simulates
    a tic-tac-toe game between two artificial agents. The following arguments are
    passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`player1`, `player2`: Artificial players to simulate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`N`: Number of simulation games'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to these, the following additional arguments are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose`: If true, shows a progress report.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`showboard`: If true, a game transition is displayed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pauseif`: Pauses the simulation when specified results occur. This can be
    useful for exploratory purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function returns a vector of integers with the results of the simulations
    carried out. In practice, each simulation will return a value between 0,1, and
    2 that, as we have already said, means a draw, a victory for player 1, or a victory
    for player 2, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify what we have said using the `str()` function, which returns a
    compact view of the internal structure of an object R:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can do more; for example, we can verify the occurrences of the three game
    results in the whole simulation using the `prop.table()` function as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This function accepts a table as an argument and calculates the proportions
    of the data it contains. The following results are returned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we can see that in the performed simulation player 1 is the one
    who wins more (51%) than player 2 (37%), and a draw is repeated a decidedly lower
    number of times (12%).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we repeat the experiment, but this time we will try to improve one of
    the two players through training based on the Q learning algorithm. As already
    done, we first create the two agents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After doing this, we focus on player 4 trying to improve his performance through
    a training phase in which he will learn to follow the best strategy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ttt_qlearn()` function trains a tic-tac-toe agent through Q-learning.
    The following arguments are available:'
  prefs: []
  type: TYPE_NORMAL
- en: '`player`: Artificial player to train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`N`: Number of episodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon`: The fraction of a random exploration move'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alpha`: Learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma`: Discount factor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`simulate`: If true, conduct simulation during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sim_every`: Conduct simulation after this many training games.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`N_sim`: Number of simulation games.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose`: If true, a progress report is shown.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the Q-learning-based training process, the agent plays against himself to
    update the value function and its policies. The algorithm used is Q-learning with
    epsilon greedy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each state s, the player updates the value function according to the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99a91547-fe4d-4dd7-a5a8-21469f1dc818.png)'
  prefs: []
  type: TYPE_IMG
- en: This happens on the first player's turn. When the second player's turn arrives,
    the formula to be used will always be the same, provided you replace the maximum
    with the minimum. In a similar way, we proceed to update the policy; that is,
    we look for the set of actions that allows us to reach the next state in order
    to maximize the value function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters that govern the process are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate that determines how often new information is acquired and
    will replace old information. A factor of 0 would prevent the agent from learning;
    however, a factor of 1 would cause the agent to be interested only in recent information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The discount factor that determines the importance of future rewards. A factor
    of 0 will cause the agent to use only the current rewards, while a factor tending
    to 1 will make the agent also attentive to the rewards he will receive in the
    long-term future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strategy used in the algorithm causes the player to choose the next action
    with the e-greedy method. This means that the agent will follow his policy with
    the probability 1-e, and will choose random actions with the probability e.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of a game, the player sets the final status as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 100 if player 1 wins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -100 if player 2 wins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 if a draw
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The learning process is repeated N times, the value set by the user:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After training player 4, we can simulate the game:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, we verify the number of occurrences of the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this case, it is the trained agent (player 4) who wins the games with the
    most occurrences (48%), then there are the draws (31%), and finally the games
    won by player 3 (21%). Thus, the player's training reversed the results by creating
    an agent who was able to identify a policy that allowed him to get more wins.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, the OpenAI Gym library will be introduced; this library
    contains many environments which will allow us to train agents using reinforcement
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing OpenAI Gym library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI Gym is a library that helps us to implement algorithms based on reinforcement
    learning. It focuses on the episodic setting of reinforcement learning. In other
    words, the agent's experience is divided into a series of episodes. The initial
    state of the agent is randomly sampled by a distribution, and the interaction
    proceeds until the environment reaches a terminal state. This procedure is repeated
    for each episode, with the aim of maximizing the total reward expectation per
    episode and achieving a high level of performance in the fewest possible episodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gym is a toolkit for developing and comparing reinforcement learning algorithms.
    It supports the ability to teach agents everything from walking to playing games
    such as Pong or pinball. The library is available at the following link: [https://gym.openai.com/](https://gym.openai.com).'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym is part of a much more ambitious project, known as the OpenAI project.
    OpenAI is an **artificial intelligence** (**AI**) research company founded by
    Elon Musk and Sam Altman. It is a non-profit project that aims to promote and
    develop friendly AI in such a way as to benefit humanity as a whole. The organization
    aims to collaborate freely with other institutions and researchers by making their
    patents and research open to the public. The founders decided to undertake this
    project as they were concerned by the existential risk deriving from the indiscriminate
    use of AI.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym is a library of programs that allows you to develop AIs, measure
    intellectual abilities, and enhance learning abilities. In short, a *gym* in the
    form of algorithms that trains the present digital brains to the OpenAI Gym and
    projects them into the future. But there is also another goal. OpenAI wants to
    stimulate research in the AI sector by funding projects that make humanity progress
    even in those fields where there is no economic return. With Gym, on the other
    hand, it intends to standardize the measurement of AI so that researchers can
    compete on equal terms and know where their colleagues have come from, but above
    all it focuses on results that are really useful for everyone.
  prefs: []
  type: TYPE_NORMAL
- en: The tools available are many. From the ability to play old video games like
    Pong to that of fighting in  GO to control a robot, we just enter our algorithm
    in this digital place to see how it works. The second step is to compare the benchmarks
    obtained with the other ones to see where we stand compared to others, and maybe
    we can collaborate with them to get mutual benefits.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list shows some environments available in the library; the environments
    are grouped by category to simplify the search:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithms**: Perform computations such as adding multi-digit numbers and
    reversing sequences. You might object that these tasks are easy for a computer,
    but the challenge is to learn these algorithms purely from examples. These tasks
    have the nice property that it''s easy to vary the difficulty by varying the sequence
    length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Atari**: Plays classic Atari games. We''ve integrated the Arcade learning
    Environment (which has had a big impact on reinforcement learning research) in
    an easy-to-install form.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Box2D**: Continuous control tasks in the Box2D simulator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classic control**: Complete small-scale tasks, mostly from the RL literature.
    They''re here to get you started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MuJoCo**: Continuous control tasks, running in a fast physics simulator.
    This task uses the MuJoCo physics engine, which was designed for fast and accurate
    robot simulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robotics**: Simulated goal-based tasks for the Fetch and ShadowHand robots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Toy text**: Simple text environments to get you started.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In particular, the classic control category offers very useful environments
    to reproduce the scenarios of important physical experiments.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym makes no assumptions about the structure of our agent and is compatible
    with any numerical computation library. The Gym library is a collection of test
    problems—environments—that we can use to work out our reinforcement learning algorithms.
    The environments have a shared interface, allowing you to write general algorithms. To
    begin, let's see how to install the library.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As already mentioned, OpenAI Gym is a library written in a Python environment.
    To be able to integrate it into the R environment, a version of Python must be
    installed on the computer. First we need to install the OpenAI `gym-http-api`
    API; these are APIs that allow access to an ever-increasing variety of environments.
  prefs: []
  type: TYPE_NORMAL
- en: The **APIs**, that is an abbreviation for **application programming interfaces**,
    are sets of definitions and protocols with which application software is created
    and integrated. They allow your products or services to communicate with other
    products or services without knowing how they are implemented, thus simplifying
    the development of the app and allowing a net saving of time and money. When creating
    new tools and products or managing existing ones, the APIs offer flexibility,
    guarantee opportunities for innovation, and simplify design, administration, and
    use.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download and install the OpenAI `gym-http-api` API, you can run the following
    shell commands. To execute these commands, it is necessary to use a command window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The first line of code in the preceding code block uses the Git software to
    download the bees to install from the github repository. The Git software allows
    you to manage a project while maintaining control of the source code and its history,
    and allows more developers to collaborate on it; it is essentially a version control
    system. Git is the de facto standard in the open source community, created in
    2005 by Linus Torvald to work on the Linux kernel, and has been maintained by
    Junio Hamano (a Google developer) two months after its creation; Git is constantly
    evolving and completely free and open source. The second line of code moves the
    command line to the folder where we copied the repository. Finally the third line
    uses the pip software to install the API.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code is intended to be run locally by a single user. A Python
    client is included, to demonstrate how to interact with the server.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the server from the command line, go to the folder where we installed
    the API, and run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the server is ready to interact with our scripts. We can install the `gym`
    package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table gives some information about the `gym` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Package | `gym` |'
  prefs: []
  type: TYPE_TB
- en: '| Date | 2016-10-25 |'
  prefs: []
  type: TYPE_TB
- en: '| Version | 0.1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Title | Provides Access to the OpenAI Gym API |'
  prefs: []
  type: TYPE_TB
- en: '| Author | Paul Hendricks |'
  prefs: []
  type: TYPE_TB
- en: To verify the functioning of the package, we can execute the example script
    supplied with the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by importing the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To start, we will establish the connection with the client to interact with
    the server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `create_GymClient()` function instantiates a `GymClient` instance to integrate
    with an OpenAI Gym server. The following result is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The connection has been established, and now we can create the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can list all the environments created with the bees available to us
    in the current work session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In this way, we have confirmation that the simulation environment has been
    correctly created, now we can request the information needed to interact with
    it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The `env_action_space_info()` function evaluates whether an action is a member
    of an environment action space.  The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Only two actions are available in this environment. Let''s create the agent
    now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `random_discrete_agent()` function simply creates a sample random discrete
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will set the folder in which to save the results, and open a window
    to monitor changes in the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s initialize some variables that we will need in the simulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will implement two loops to interact with the environment using random
    actions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/016ee645-08c2-4be8-a4a2-d24226453de5.png)'
  prefs: []
  type: TYPE_IMG
- en: In the screenshot, you can see the cart-pole that will move according to the
    indications provided by the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will close the window, using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This example helped us to start interacting with the environments available
    in OpenAI Gym. If you have not understood some passages, don't worry, in the following
    section we can learn more about them.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI Gym provides the `env` class, which encapsulates the environment and
    its possible internal dynamics. The class has different methods and attributes
    to implement to create a new environment. The most important methods are called
    reset, step, and render, let''s look at them briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: The **reset method** has the task of resetting the environment, initializing
    it to the initial state. Within the reset method, the definitions of the elements
    that make up the environment must be contained, in this case the definition of
    the mechanical arm, of the object to be grasped and its support.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **step method** has the task of moving the environment forward by one step.
    It requires the action to be performed as input, and returns the new observation
    to the agent. Within the method, the management of the dynamics of the movements,
    the calculation of the status and of the reward, and the controls for completing
    the episode must be defined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third and last method is that we must define which interior to **render**
    to, as the elements at each step must be represented. The method involves different
    types of rendering, such as human, `rgb_array`, or `ansi`. With the human type,
    the rendering is done on the screen or command-line interface, and the method
    does not return anything; with the `rgb_array` type, invoking the method returns
    an n-dimensional array representing the RGB pixels of the screen; choosing the
    third type, the return method returns a string containing a textual representation.
    To render, OpenAI Gym provides the viewer class, through which you can draw the
    elements of the environment as a set of polygons and circles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regarding the attributes of the environment, the `env` class provides the definition
    of action space, observation space, and reward range. The action space attribute
    represents the action space, which is the set of possible actions that the agent
    can perform within the environment. Using the observation space attribute, the
    number of parameters that makes up the state is defined, and for each of them
    the range of values that can be assumed. The reward range attribute contains the
    minimum and maximum rewards obtainable in the environment, by default set to (-∞,
    + ∞).
  prefs: []
  type: TYPE_NORMAL
- en: Using the `env` class proposed by the framework as a basis for new environments,
    the common interface provided by the toolkit is adopted. In this way, the environments
    created can be integrated into the toolkit library, and their dynamics can be
    learned from algorithms already implemented by users of the OpenAI Gym community.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will implement a robot control system using an
    OpenAI Gym environment.
  prefs: []
  type: TYPE_NORMAL
- en: Robot control system using the FrozenLake environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technically speaking, a robot can be seen as a particular type of automatic
    control, that is, an automaton physically located in an environment of which it
    can perceive certain characteristics through components called sensors, and on
    which it can perform actions with the aim of making changes to it. These actions
    are performed by so-called actuators.
  prefs: []
  type: TYPE_NORMAL
- en: 'All that is interposed between the measurements made by the sensors and the
    commands given to the actuators can be defined as the control program or the controller
    of the robot. This is the component in which the intelligence of the robot is
    encoded, and in a certain sense it constitutes the brain that must guide its actions
    in order to obtain the desired behavior. A controller can be implemented in various
    ways: usually it is software running on one or more microcontrollers physically
    integrated into the system (onboard), but it can also be obtained through electronic
    circuits (analog or digital) directly wired into the hardware of the robot. Let''s
    start by taking a look at the environment'
  prefs: []
  type: TYPE_NORMAL
- en: The FrozenLake environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The FrozenLake environment ([https://gym.openai.com/envs/FrozenLake-v0/](https://gym.openai.com/envs/FrozenLake-v0/)[)](https://gym.openai.com/envs/FrozenLake-v0/)
    is a 4 x 4 grid that contains four possible areas: **Safe** (**S**), **Frozen**
    (**F**), **Hole** (**H**), and **Goal** (**G**). The agent controls the movement
    of a character in a grid world, and moves around the grid until it reaches the
    goal or the hole. Some tiles of the grid are walkable, and others lead to the
    agent falling into the water. If it falls into the hole, it must start from the
    beginning and is rewarded the value 0\. The agent moves following an uncertain
    path that depends only in part on the chosen direction. The agent is rewarded
    when he finds a possible path to the set goal. The agent has four possible moves:
    up, down, left, and right. The process continues until it learns from every mistake
    and reaches the goal eventually.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The surface is described using a grid like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SFFF (**S**: starting point, safe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FHFH (**F**: frozen surface, safe)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FFFH (**H**: hole, fall to your doom)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HFFG (**G**: goal, where the frisbee is located)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see the FrozenLake grid (4 x 4):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f63117d0-0eed-4b9c-951a-9918920fdb17.png)'
  prefs: []
  type: TYPE_IMG
- en: The episode ends when you reach the goal or fall in a hole. You receive a reward
    of one if you reach the goal, and zero otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: The Q-learning solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we said in [Chapter 7](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml), *Temporal
    Difference Learning*, Q-learning is one of the most used reinforcement learning
    algorithms. This is due to its ability to compare the expected utility of the
    available actions without requiring an environment model. Thanks to this technique
    it is possible to find an optimal action for every given state in a finished **Markov
    decision process** (**MDP**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will use a Q-learning approach to find the right
    path from the start cell to goal cell in a 4 x 4 grid environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, we will analyze the code line by line. Let''s start by importing
    the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To start, we will establish the connection with the client to interact with
    the server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The `create_GymClient()` function instantiates a `GymClient` instance to integrate
    with an OpenAI Gym server. The following result is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The connection has been established; now we can create the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can list all the environments created with the bees available to us
    in the current work session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we have confirmation that the simulation environment has been correctly
    created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will retrieve some information from the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `env_action_space_info()` function evaluates whether an action is a member
    of an environment action space.  The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Four actions are available in this environment, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '0: Move left'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1: Move down'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2: Move right'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3: Move up'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let''s take a look at the states that an agent that moves in this environment
    can take:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `env_observation_space_info()` function gets information (name and dimensions/bounds)
    of the environment observation space. The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'From the following diagram, we can see that sixteen states are available in
    this environment (0 – 15) covering a 4x4 grid, counting each position from left
    to right, top to bottom as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de441345-9210-4c14-bf3c-b9e38f7935c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will then extract these values in order to reuse them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s initialize the parameters starting with `QTable`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`QTable` has a number of rows equal to the size of the observation space (`observation_space_info$n`),
    while the columns are equal to the size of the action space (`action_space_info$n`).
    As we said, the FrozenLake environment provides a state for each cell in the 4
    x 4 grid, and four actions, returning a 16 x 4 table.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This table is initialized with all zeros using the `matrix()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define some parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Basically, `alpha` is the learning rate, and `gamma` is the discount factor.
    The learning rate handles the updating of the information acquired, in the sense
    that it establishes when it is time to replace the old acquisitions with the new
    ones. Setting the learning rate with a value of 0 means that the agent does not
    learn anything. This will only exploit previous knowledge. With a value of 1 the
    agent considers only the most recent information and ignores previous knowledge.
    This is the exploration-exploitation dilemma. Ideally, the agent must explore
    all possible actions for each state, finding the one that is actually most rewarded
    for exploiting it in achieving its goal. The discount factor determines the importance
    of future rewards. A factor of 0 will consider only current rewards, while a factor
    approaching 1 will make it strive for a long-term high reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we set the number of episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The `NumEpisodes` parameter has the following meaning. The agent learns through
    experience, without a tutor to guide him; this way represents a learning without
    supervision. The agent will explore until he reaches the goal, moving from one
    state to the next. Every exploration is called an episode. Each episode consists
    of the movement of the agent that moves from the initial state to the objective
    state. Each time the agent arrives at the target state, we move on to the next
    episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will create a list to contain total rewards:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Then two more parameters are initialized; we will need them to recover the
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, after setting the parameters, it is possible to start the Q-learning
    cycle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we initialize the system using the `env_reset()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, a reward-counter and cycle-counter are initialized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'From this, the Q-learning table algorithm is implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We increase the cycle counter at each new step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to choose an action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: An action is chosen by greedily method-picking from `Qtable`. A noise is added
    because the environment is unknown, so it has to be explored in some way—your
    agent will do so using the power of randomness. Two functions are used, `which.max()`
    and `runif()`. The `which.max()` function returns the indices of the maximum values
    along an axis. The `runif()` function returns a sample (or samples) from the standard
    normal distribution. In the second line of code of the block just analyzed, we
    reduce the action index by one unit. This is necessary because as already mentioned
    the actions available in the environment range from 0 to 3 while in r the indexes
    of the tables start at 1 (unlike Python which instead starts from 0). Then to
    make the results compatible with the environment r it is necessary to make this
    correction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will use the `env_step()` method to return the new states in response
    to the actions with which we call it. Obviously, the action we pass to the method
    is the one we have just decided:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The `env_step()` method returns a list consisting of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`action`: An action to take in the environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`observation`: An agent''s observation of the current environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward`: The amount of reward returned after previous action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`done`: Whether the episode has ended'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info`: A list containing auxiliary diagnostic information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For now, we are only interested in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then update the `Qtable` field with new knowledge:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The formula used for the Q-function update was the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2d791ea-9828-439f-a82e-5367a8efd8c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Also, in this case, we had to introduce a correction for the state and the action
    (state + 1, action + 1) that the FrozenLake environment expects to assume the
    following values 0-15 for the state and 0-3 for the actions, but as we know the
    row and column index in r starts from 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will update the sum of the rewards with the one just obtained and the
    state of the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we insert a check to see if we have reached the end of the episode.
    We remember in this regard that the episode is considered finished when we reach
    the goal or end up in a hole:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: The `if` loop used for the end-of-episode check contains a series of instructions
    that guide us in finding the solution. In fact, I inserted a control that discriminates
    between reaching the objective and falling into a hole and then a series of prints
    that allow us to verify the success of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before leaving the cycle that runs through the episodes definitively, we will
    try to update the sum of the rewards and the index of the rewards list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we print the results, first the score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we print the `Qtable` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: The table containing the value function is now ready, and we can use it to extract
    the paths that will take us from the starting cell to the goal cell without falling
    into the holes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we limit our search to five possible paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'To do this, we used a cycle and we included the print of each episode for a
    check. Now, as we did in the training phase, let''s start: reset the environment
    using the `env_reset()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: We recall in this regard that the `env_reset()` function returns the environment
    to the initial state, that is, the state 0 which corresponds to the first cell.
    As we have already highlighted previously to make the OpenAI Gym environment designed
    for Python compatible with the environment, it is necessary to make a correction
    by adding a unit. In this way, the state 0 will correspond to the first line of
    the Qtable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next `for` loop will allow us to move within the grid to reach the goal
    following the directions provided by `Qtable`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now initialize a rewards counter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will extract the actions that have the maximum expected future reward
    given the actual state, that is, related to the current step. The Qtable returns
    this value as the index of the maximum value in the row corresponding to the current
    state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we reduce the action index by one unit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will use the `env_step()` method to return the new states in response
    to the actions extracted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Of the values returned by the `env_step()` function, we are currently only
    interested in the status and reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will update the reward counter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will check whether we have reached the end of the episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we have obtained five of the best routes to reach the goal without
    falling into the holes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned the general concepts of game theory. We learned
    about the essential characteristics of the games and how the solutions adopted
    can help us solve real-life problems. We analyzed a series of real applications
    in which the theories are used to obtain solutions. Following that, we analyzed
    in detail the OpenAI Gym library and how to interact with the available environments
    to simulate real problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we tackled the tic-tac-toe game using reinforcement learning. We used
    the `tictactoe` package to set the environment and to train an agent using Q-learning
    to play the game. Finally, we looked at the FrozenLake environment. This is a
    4 × 4 grid that contains four possible areas: Safe (S), Frozen (F), Hole (H),
    and Goal (G). The agent controls the movement of a character in a grid world,
    and moves around the grid until it reaches the goal or the hole. This environment
    is particularly suitable for simulating problems related to the mobility of a
    robot in an environment full of obstacles. After defining the environment, we
    created an agent that is able to move within the environment and find the goal
    using an algorithm based on Q-learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn the fundamental concepts of finance problems;
    how to forecast stock market prices, and how to optimize equity portfolios. Then,
    we will look at how to implement fraud detection techniques.
  prefs: []
  type: TYPE_NORMAL
