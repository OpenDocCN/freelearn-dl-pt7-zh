- en: '*Chapter 4*: Automating Document Processing Workflows'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we were introduced to **Amazon Comprehend** and **Amazon
    Comprehend Medical**, and we covered how to use these services to derive insights
    from text. We also spent some time understanding how Natural Language Processing
    algorithms work, the different types of insights you can uncover, and we also
    ran code samples trying out the Amazon Comprehend APIs.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will walk through our first real-world use case of automating
    a document management workflow that many organizations struggle with today. We
    put together this solution based on our collective experience and the usage trends
    we have observed in our careers. Fasten your seat belts and get ready to experience
    architecting an end-to-end AI solution one building block at a time and watch
    it taking shape in front of you. We expect to be hands-on throughout the course
    of this chapter, but we have all the code samples we need to get going.
  prefs: []
  type: TYPE_NORMAL
- en: We will dive deep into how you can automate document processing with **Amazon
    Textract** and then we will cover how you can set up compliance and control in
    the documents using **Amazon Comprehend**. Lastly, we will talk about architecture
    best practices while designing **real-time document processing** workflows versus
    **batch processing.** We will provide detailed code samples, designs, and development
    approaches, and a step-by-step guide on how to set up and run these examples along
    with access to GitHub repositories.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Automating document processing workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up compliance and control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing real-time document workflows versus batch document workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, you will need access to an AWS account. Please make sure to
    follow the instructions specified in the *Technical requirements* section in [*Chapter
    2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027), *Introducing Amazon Textract*,
    to create your AWS account, and log in to the AWS Management Console before trying
    the steps in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code and sample datasets for a walk-through of this chapter''s code
    are provided at the following link: [https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2004](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2004).
    Please use the instructions in the following sections along with the code in the
    repository to build the solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action at [https://bit.ly/3GlcCet](https://bit.ly/3GlcCet).
  prefs: []
  type: TYPE_NORMAL
- en: Automating document processing workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have discussed in the previous chapter how Amazon Textract can help us digitize
    scanned documents such as PDF and images by extracting text from any document.
    We also covered how Amazon Comprehend can help us extract insights from these
    documents, including entities, **Personal Identifiable Information** (**PII**),
    and sentiments.
  prefs: []
  type: TYPE_NORMAL
- en: Now, these services can be used together in an architecture to automate the
    document processing workflows for most organizations, be it a financial organization
    or healthcare, which we will cover in [*Chapter 12*](B17528_12_Final_SB_ePub.xhtml#_idTextAnchor141),
    *AI and NLP in Healthcare*.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with a fictitious bank, *LiveRight Pvt Ltd.*, whose customers are
    applying for home loans. We all know this loan origination process involves more
    than 400 documents to be submitted and reviewed by the bank before approval is
    forthcoming for your home loan. Automating this process will make it easier for
    banks as well as customers to get loans. The challenge with automating these workflows
    is that there are more than 1,000 templates for the loan origination process and
    going with any **Optical Character Recognition** (**OCR**) system will require
    managing these templates. Moreover, these OCR template-based approaches are not
    scalable and break with format changes. That's why we have Amazon Textract to
    extract text from any documents, enabling these documents to be automated and
    processed in hours rather than months or weeks.
  prefs: []
  type: TYPE_NORMAL
- en: You have extracted the data from these forms or semi-structured documents. You
    will now want to set up compliance and control on the data extracted from these
    documents; for example, making sure that if the data is PII, you can mask it for
    further processing. You will also want to extract the entities if you want to
    focus on the loan approval process, for example, the loan amount or the details
    of the requester. This is where Amazon Comprehend can help. In fact, you can perform
    custom classification of the documents submitted and the custom entities based
    on your requirements with Amazon Comprehend; for example, documents extracted
    by Textract and sent to Amazon Comprehend for custom classification to classify
    whether the document submitted is a driving license or W2 form.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the architecture of how you can use Amazon Textract and Amazon
    Comprehend together to automate your existing document flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Automating document processing workflows'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – Automating document processing workflows
  prefs: []
  type: TYPE_NORMAL
- en: In this architecture, you have documents coming in, and these documents may
    be financial documents, legal documents, mortgage applications, and so on. You
    send these documents to Amazon Textract to extract text from these documents.
    Once you have extracted text from these documents, you can send this text to Amazon
    Comprehend to extract insights. These insights can classify these documents based
    on document type, it can identify PII from these documents, or it can be **named
    entity recognition** (**NER**) using custom entity recognition. We cover custom
    entities in [*Chapter 14*](B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162), *Auditing
    Named Entity Recognition Workflows*, and document classification in [*Chapter
    15*](B17528_15_Final_SB_ePub.xhtml#_idTextAnchor178), *Classifying Documents and
    Setting up Human in the Loop for Active Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered how you can easily and quickly set up an automated
    document processing workflow with Amazon Textract and Amazon Comprehend by using
    these services together. In the next section, we will talk about how you can use
    these services together to set up compliance and control for LiveRight Pvt Ltd.,
    especially by means of masking or redacting the PII data in their forms.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up compliance and control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will talk about how LiveRight Pvt Ltd. can set up compliance
    and control as well as automate their loan origination process using Amazon Textract
    and Amazon Comprehend. We will walk you through the following architecture using
    code samples in a Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Setting up compliance and control'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Setting up compliance and control
  prefs: []
  type: TYPE_NORMAL
- en: 'We will walk you through this architecture using a single document and sample
    code. However, this architecture can be automated to process a large number of
    documents using the **step function** and **lambda functions** in a serverless
    manner. In this architecture, we will show you the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How you can upload a sample document and extract the text using `.txt` or `.csv`
    files back to an **Amazon S3 bucket**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we will show you how you can use **Amazon Comprehend's** real-time or
    sync API to detect PII.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will then cover how you can use the Amazon Comprehend PII detection job to
    mask and redact the PII in the extracted text/CSV file in **Amazon S3**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How you can find the redacted document text in Amazon S3 as an output of the
    Comprehend PII detection job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, let's get started with setting up the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up to solve the use case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have not done so in the previous chapters, you will first have to create
    an Amazon SageMaker Jupyter notebook and set up **Identity and Access Management**
    (**IAM**) permissions for that notebook role to access the AWS services we will
    use in this notebook. After that, you will need to clone the GitHub repository
    ([https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services)).
    Please perform the following steps to complete these tasks before we can execute
    the cells from our notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: Follow the instructions documented in the *Create an Amazon SageMaker Jupyter
    notebook instance* section within the *Setting up your AWS environment* section
    in [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027), *Introducing
    Amazon Textract*, to create your Jupyter notebook instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: IAM Role Permission while Creating Amazon SageMaker Jupyter Notebooks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Accept the default option for the IAM role at notebook creation time to allow
    access to any S3 bucket.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once you have created the notebook instance and its status is **InService**,
    click on **Open Jupyter** in the **Actions** menu heading for the notebook instance.![Figure
    4.3 – Opening the Jupyter notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17528_04_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.3 – Opening the Jupyter notebook
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This will take you to the home folder of your notebook instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on **New** and then select **Terminal**, as shown in the following screenshot:![Figure
    4.4 – Opening Terminal in a Jupyter notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17528_04_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.4 – Opening Terminal in a Jupyter notebook
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the terminal window, first, type `cd SageMaker` and then type `git clone
    https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services`,
    as shown in the following screenshot:![Figure 4.5 – git clone command
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17528_04_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.5 – git clone command
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, exit the terminal window, go back to the home folder, and you will see
    a folder called `Chapter 04`. Click the folder and you should see a notebook called
    `Chapter 4 Compliance and control.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open this notebook by clicking it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will cover the additional IAM prerequisites.
  prefs: []
  type: TYPE_NORMAL
- en: Additional IAM prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train the Comprehend custom entity recognizer and to set up real-time endpoints,
    we have to enable additional policies and update the trust relationships for our
    SageMaker notebook role. To do this, attach `AmazonS3FullAccess`, `TextractFullAccess`,
    and `ComprehendFullAccess` policies to your Amazon SageMaker Notebook IAM Role.
    To execute this step, please refer to *Changing IAM permissions and trust relationships
    for the Amazon SageMaker notebook execution role* in the *Setting up your AWS
    environment* section in [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027),
    *Introducing Amazon Textract*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the necessary IAM roles and notebook set up in the Amazon SageMaker
    notebook instance, let's jump to the code walk-through.
  prefs: []
  type: TYPE_NORMAL
- en: Automating documents for control and compliance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will give a code walk-through of the architecture we discussed
    for automating documents using Amazon Textract and setting compliance and control
    with PII masking using Amazon Comprehend in *Figure 14.2* using this notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: Execute the cell under *Step 1 – Setup and install libraries* in the Jupyter
    notebook you just set up at the following link, [https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2004/Chapter%204%20Compliance%20and%20control.ipynb](https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2004/Chapter%204%20Compliance%20and%20control.ipynb),
    to ensure that you have the libraries needed for the notebook. Note that in this
    cell, you are getting the Amazon SageMaker execution role for the notebook along
    with the SageMaker session. You are setting up boto3 libraries to call Amazon
    Textract, Amazon Comprehend, and Amazon S3 APIs. You are also using the SageMaker
    session to access the default SageMaker S3 bucket where you will be storing the
    data for this lab using a prefix or folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we will start with the sample bank statement. Execute the cells under
    *Step 2*, *Extracting text from a sample document* in the Jupyter notebook, to
    display the sample document to extract text and redact the PII:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following response:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Sample bank statement](img/B17528_04_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 4.6 – Sample bank statement
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we will invoke Amazon Textract''s Detect Document Text Sync API, [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html#Textract.Client.detect_document_text](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html#Textract.Client.detect_document_text),
    which extracts only text from documents in near-real time to extract data from
    the sample bank statement using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You get a JSON response from Amazon Textract using the Detect Document Text
    Sync API.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we will extract text from this JSON response using the Amazon Textract
    parser library we installed in *Step 1*. Run the following code to parse the Textract
    JSON response to text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have the extracted text from the Textract JSON response, let's move
    on to the next step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this step, we will save the extracted text from the bank statement to a text/CSV
    file and upload it to Amazon S3 for processing with the Amazon Comprehend batch
    job. Run the notebook cell *Step 3*, *Save the extracted text to a text/CSV file
    and upload it to an Amazon S3 bucket*, to save the data in a text file and then
    upload it to Amazon S3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we have extracted the text from bank statements, converted it into
    a text file, and uploaded it to Amazon S3, in this step, we will detect PII from
    the text using Amazon Comprehend Detect PII Sync APIs. Run the notebook cell *Step
    4*, *Check for PII using the Amazon Comprehend Detect PII Sync API*, to call the
    Comprehend APIs by passing the extracted text from Amazon Textract:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) First, initialize the boto3 handle for Amazon Comprehend:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'b) Then, call Amazon Comprehend and pass it the aggregated text from our sample
    bank statement image to Comprehend detect PII entities: [https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.detect_pii_entities](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.detect_pii_entities):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will get a response with identifying PII from the text, which will be redacted
    in the next step using the Amazon Comprehend PII analysis job.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.7 – PII detection using Amazon Comprehend in a bank statement](img/B17528_04_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 4.7 – PII detection using Amazon Comprehend in a bank statement
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will mask/redact these 15 PII entities we found in the sample bank statement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will call the `StartPiiEntitiesDetectionJob` API to start an asynchronous
    PII entity detection job for a collection of documents. For this example, we are
    just using one document sample. You can redact a large number of documents using
    this job. Run the notebook cell *Step 5*, *Mask PII using the Amazon Comprehend
    PII Analysis Job*, to set up and start the PII redaction analysis job with Amazon
    Comprehend:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) Then job requires the S3 location of documents to be redacted and the S3
    location of where you want the redacted output. Run the following cell to specify
    the location of the S3 text file we want to be redacted:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'b) Now we will call `comprehend.start_pii_entities_detection_job` by setting
    parameters for redaction and passing the input S3 location where data is stored
    by running the following notebook cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Refer to the API documentation for more details: [https://docs.aws.amazon.com/comprehend/latest/dg/API_StartPiiEntitiesDetectionJob.html](https://docs.aws.amazon.com/comprehend/latest/dg/API_StartPiiEntitiesDetectionJob.html).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) The job will take roughly 6-7 minutes. The following code is to check the
    status of the job. The cell execution will be completed once the job is complete:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will get a JSON response, and this job will take 5-6 minutes. You can go
    and grab a coffee until the notebook cell is running and you have a response.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the job is successful, we will now show you the extracted, redacted document
    output in this step. Run the notebook cell *Step 6*, *View the redacted/masked
    output in the Amazon S3 bucket*, to extract the output from the Amazon S3 bucket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following redacted bank statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Redacted bank statement using the Amazon Comprehend PII Redaction
    job'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – Redacted bank statement using the Amazon Comprehend PII Redaction
    job
  prefs: []
  type: TYPE_NORMAL
- en: In the output, you can see that the Amazon Comprehend PII job has masked the
    PII data, such as an address, name, SSN, and bank account number identified using
    the Amazon Comprehend Detect PII entity.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we walked you through an end-to-end conceptual architecture
    for automating documents for compliance and control. In the next section, we will
    talk about best practices for real-time document processing workflows versus batch
    processing workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Processing real-time document workflows versus batch document workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will talk about some best practices while architecting solutions
    using Amazon Textract for real-time workflows versus batch processing document
    workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compare the Textract real-time APIs against the batch APIs we discussed
    in [*Chapter 2*](B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027), *Introducing
    Amazon Textract*, with the help of the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Textract sync APIs versus batch APIs](img/B17528_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Textract sync APIs versus batch APIs
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The pricing of Textract is based on which of the three different APIs you are
    going to use out of Analyze Document (forms, table), Detect Text (text extraction),
    and Analyze Expense (invoices and receipts). You will not be charged irrespective
    of whether you use the sync or async (batch) implementation of these, so, feel
    free to design your architecture based on your need for real-time processing versus
    batch processing as pricing is based on the number of documents processed with
    one of the three APIs, irrespective of batch or real-time mode. Check prices here:
    [https://aws.amazon.com/textract/pricing/](https://aws.amazon.com/textract/pricing/).'
  prefs: []
  type: TYPE_NORMAL
- en: For example, LiveRight pvt Ltd*.* can use the batch or real-time implementation
    of the detect text API to detect text from their bank statements to process millions
    of documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'We covered architecture in *Figure 14.2*. This architecture implemented the
    Amazon Textract Detect Text Sync API in the code walk-through. Now, let''s see
    how we can automate the architecture through Lambda functions for scale to process
    multiple documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Synchronous document processing workflow](img/B17528_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Synchronous document processing workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding architecture, we walked you through how you can process scanned
    images using the proposed synchronous document processing workflow using the sync
    APIs of Amazon Textract. Here are the steps for this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Documents uploaded to Amazon S3 will send a message to an Amazon SQS queue to
    analyze a document. Amazon SQS is a serverless managed queuing service that polls
    the documents into the queue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Lambda function is invoked synchronously with an event that contains a queue
    message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Lambda function then calls Amazon Textract sync APIs and stores the Textract
    output or response in either Amazon S3 or response metadata in the Amazon DynamoDB
    table. Amazon DynamoDB is a NoSQL database managed by AWS that is like a key/value
    store.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You control the throughput of your pipeline by controlling the batch size and
    Lambda concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will walk you through the following architecture best practices for
    scaling multi-page scanned documents, which can be PDF or images using batch APIs
    of Amazon Textract:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Batch document processing workflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17528_04_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Batch document processing workflow
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, we have an architecture to walk through how batch
    processing workflow works with Amazon Textract batch jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: Multipage PDFs and images are uploaded in Amazon S3\. These documents are sent
    to the **Amazon Simple Queue Service** (**SQS**) queue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A job scheduler Lambda function runs at a certain frequency, for example, every
    5 minutes, and polls for messages in the SQS queue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each message in the queue, it submits an Amazon Textract job to process
    the document and continues submitting these jobs until it reaches the maximum
    limit of concurrent jobs in your AWS account.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As Amazon Textract finishes processing a document, it sends a completion notification
    to an **Amazon Simple Notification Service** (**SNS**) topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SNS then triggers the job scheduler Lambda function to start the next set of
    Amazon Textract jobs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SNS also sends a message to an SQS queue, which is then processed by a Lambda
    function to get results from Amazon Textract. The results are then stored in a
    relevant dataset, for example, DynamoDB or Amazon S3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This GitHub link, [https://github.com/aws-samples/amazon-textract-serverless-large-scale-document-processing](https://github.com/aws-samples/amazon-textract-serverless-large-scale-document-processing),
    has code samples to implement both the suggested architecture and it also has
    some additional components to backfill in case the documents already exist in
    the Amazon S3 bucket. Please feel free to set up and use this if you have large
    documents to experiment with.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use the following GitHub solution, [https://github.com/aws-samples/amazon-textract-textractor](https://github.com/aws-samples/amazon-textract-textractor),
    to implement large-scale document processing with Amazon Comprehend insights.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered architecture best practices for using real-time
    processing or batch processing with Amazon Textract. We also presented some already-existing
    GitHub implementations for large-scale document processing with Amazon Textract.
    Now, let's summarize what we have covered in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered how you can use Amazon Textract to automate your
    existing documents. We introduced a fictional bank use case with the help of *LiveRight
    Pvt Ltd*. We showed you how using an architecture can help banks automate their
    loan origination process and set up compliance and control with Amazon Comprehend.
    We also covered code samples using a sample bank statement, and how you can extract
    data from the scanned bank statement and save it into a `CSV.text` file in Amazon
    S3 for further analysis. Then, we showed you how you can use Amazon Comprehend
    to detect PII using a sync API and how you can redact that sample bank data text/CSV
    in Amazon S3 using an Amazon Comprehend batch PII redaction job.
  prefs: []
  type: TYPE_NORMAL
- en: We then covered some architecture patterns for using real-time processing document
    workflows versus batch processing workflows. We also provided some GitHub implementations
    that can be used to process large-scale documents.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned the differences between when to use and how to
    use real-time APIs versus batch APIs for document automation. You also learned
    how you can set up PII redaction with Amazon Comprehend PII jobs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at a different use case, but one that's equally
    popular among enterprises looking to leverage NLP to maximize their business value
    by building smart search indexes. We will cover how you can use Amazon Textract
    and Amazon Comprehend along with Amazon Elasticsearch and Amazon Kendra to create
    a quick NLP-based search. We will introduce the use case, discuss how to design
    the architecture, establish the prerequisites, and walk through in detail the
    various steps required to build the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Building a serverless document scanner using Amazon Textract and AWS Amplify*,
    by Moheeb Zara ([https://aws.amazon.com/blogs/compute/building-a-serverless-document-scanner-using-amazon-textract-and-aws-amplify/](https://aws.amazon.com/blogs/compute/building-a-serverless-document-scanner-using-amazon-textract-and-aws-amplify/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Automatically extract text and structured data from documents with Amazon
    Textract*, by Kashif Imran and Martin Schade ([https://aws.amazon.com/blogs/machine-learning/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract/](https://aws.amazon.com/blogs/machine-learning/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
