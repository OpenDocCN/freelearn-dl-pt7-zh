- en: Data Access and Distributed Processing for IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data is everywhere: images, speech, text, weather information, the speed of
    your car, your last EMI, changing stock prices. With the integration of **Internet
    of Things** (**IoT**) systems, the amount of data produced has increased many-fold;
    an example is sensor readings, which could be taken for room temperature, soil
    alkalinity, and more. This data is stored and made available in various formats. In
    this chapter, we will learn how to read, save, and process data in some popular
    formats. Specifically, you will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Access data in TXT format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read and write csv-formatted data via the CSV, pandas, and NumPy modules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access JSON data using JSON and pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn to work with the HDF5 format using PyTables, pandas, and h5py
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handle SQL databases using SQLite and MySQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handle NoSQL using MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with Hadoop's Distributed File System
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TXT format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the simplest and common formats for storing data is the TXT format; many
    IoT sensors log sensor readings with different timestamps in the simple `.txt`
    file format. Python provides built-in functions for creating, reading, and writing
    into TXT files.
  prefs: []
  type: TYPE_NORMAL
- en: We can access TXT files in Python itself without using any module; the data,
    in this case, is of the string type, and you will need to transform it to other
    types to use it. Alternatively, we can use NumPy or pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Using TXT files in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python has built-in functions that read and write into TXT files. The complete
    functionality is provided using four sets of functions: `open()`, `read()`, `write()`,
    and `close()`. As the names suggest, they are used to open a file, read from a
    file, write into a file, and finally close it. If you are dealing with string
    data (text), this is the best choice. In this section, we will use `Shakespeare` plays
    in TXT form; the file can be downloaded from the MIT site: [https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt](https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt).
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the following variables to access the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first step here is to open the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we read the whole file; we can use the `read `function, which will read
    the whole file as one single string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This reads the whole file (consisting of 4,583,798 characters) into the `contents`
    variable. Let''s explore the contents of the `contents` variable; the following
    command will print the first `1000` characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If the TXT files contain numeric data, it is better to use NumPy; if data is
    mixed, pandas is the best choice.
  prefs: []
  type: TYPE_NORMAL
- en: CSV format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**C****omma-separated value** (**CSV**) files are the most popular formats
    for storing tabular data generated by IoT systems. In a `.csv `file, the values
    of the records are stored in plain-text rows, with each row containing the values
    of the fields separated by a separator. The separator is a comma by default but
    can be configured to be any other character. In this section, we will learn how
    to use data from CSV files with Python''s `csv`, `numpy`, and `pandas` modules.
    We will use the `household_power_consumption` data file. The file can be downloaded
    from the following GitHub link: [https://github.com/ahanse/machlearning/blob/master/household_power_consumption.csv](https://github.com/ahanse/machlearning/blob/master/household_power_consumption.csv).
    To access the data files, we define the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Generally, to quickly read the data from CSV files, use the Python `csv` module;
    however, if the data needs to be interpreted as a mix of date, and numeric data
    fields, it's better to use the pandas package. If the data is only numeric, NumPy
    is the most appropriate package.
  prefs: []
  type: TYPE_NORMAL
- en: Working with CSV files with the csv module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Python, the `csv` module provides classes and methods for reading and writing
    CSV files. The `csv.reader` method creates a reader object from which rows can
    be read iteratively. Each time a row is read from the file, the reader object
    returns a list of fields. For example, the following code demonstrates reading
    the data file and printing rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The rows are printed as a list of field values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `csv.writer` method returns an object that can be used to write rows to
    a file. As an example, the following code writes the first 10 rows of the file
    to a temporary file and then prints it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `delimiter` field and the `quoting `field characters are important attributes
    that you can set while creating `reader` and `writer` objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the `delimiter` field is `,` and the other delimiters are specified
    with the `delimiter` argument to the `reader` or `writer` functions. For example,
    the following code saves the file with `|` as `delimiter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you do not specify a `delimiter` character when the file is read, the rows
    will be read as one field and printed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '`quotechar` specifies a character with which to surround fields. The `quoting`
    argument specifies what kind of fields can be surrounded with `quotechar`. The
    `quoting` argument can have one of the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`csv.QUOTE_ALL`: All the fields are quoted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`csv.QUOTE_MINIMAL`: Only fields containing special characters are quoted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`csv.QUOTE_NONNUMERIC`: All non-numeric fields are quoted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`csv.QUOTE_NONE`: None of the fields are quoted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example, let''s print the temp file first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s save it with all fields quoted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The file gets saved with the specified quote character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember to read the file with the same arguments; otherwise, the `*` quote
    character will be treated as part of the field values and printed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the correct arguments with the `reader` object prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now let's see how we can read CSV files with pandas, another popular Python
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Working with CSV files with the pandas module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In pandas, the `read_csv()` function returns a DataFrame after reading the
    CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The DataFrame is printed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We see in the preceding output that pandas automatically interpreted the `date`
    and `time` columns as their respective data types. The pandas DataFrame can be
    saved to a CSV file with the `to_csv()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'pandas, when it comes to reading and writing CSV files, offers plenty of arguments.
    Some of these are as follows, complete with how they''re used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`header`: Defines the row number to be used as a header, or none if the file
    does not contain any headers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sep`: Defines the character that separates fields in rows. By default, the
    value of `sep` is set to `,`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`names`: Defines column names for each column in the file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`usecols`: Defines columns that need to be extracted from the CSV file. Columns
    that are not mentioned in this argument are not read.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dtype`: Defines the data types for columns in the DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many other available options are documented at the following links: [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) and [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html).
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see how to read data from CSV files with the NumPy module.
  prefs: []
  type: TYPE_NORMAL
- en: Working with CSV files with the NumPy module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The NumPy module provides two functions for reading values from CSV files:
    `np.loadtxt()` and `np.genfromtxt()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of `np.loadtxt` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code reads columns `3` and `4` from the file that we created
    earlier, and saves them in a 9 × 2 array as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `np.loadtxt()` function cannot handle CSV files with missing data. For
    instances where data is missing, `np.genfromtxt()` can be used. Both of these
    functions offer many more arguments; details can be found in the NumPy documentation.
    The preceding code can be written using `np.genfromtxt()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy arrays produced as a result of applying AI to IoT data can be saved with
    `np.savetxt()`. For example, the array we loaded previously can be saved as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `np.savetxt()` function also accepts various other useful arguments, such
    as the format for saved fields and headers. Check the NumPy documentation for
    more details on this function.
  prefs: []
  type: TYPE_NORMAL
- en: CSV is the most popular data format on IoT platforms and devices. In this section,
    we learned how to read CSV data using three different packages in Python. Let's
    learn about XLSX, another popular format, in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: XLSX format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Excel, a component of the Microsoft Office pack, is one of the popular formats
    in which data is stored and visualized. Since 2010, Office has supported the `.xlsx` format.
    We can read XLSX files using the OpenPyXl and pandas functions.
  prefs: []
  type: TYPE_NORMAL
- en: Using OpenPyXl for XLSX files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenPyXl is a Python library for reading and writing Excel files. It is an
    open source project. A new `workbook` is created using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can access the currently `active` sheet by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'To change the sheet name, use the `title` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'A single row can be added to the sheet using the `append` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'A new sheet can be created using the `create_sheet()` method. An individual
    cell in the active sheet can be created using the `column` and `row` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: A workbook can be saved using the `save` method. To load an existing workbook,
    we can use the `load_workbook` method. The names of the different sheets in an
    Excel workbook can be accessed using `get_sheet_names()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code creates an Excel workbook with three sheets and saves it;
    later, it loads the sheet and accesses a cell. The code can be accessed from GitHub
    at `OpenPyXl_example.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You can learn more about OpenPyXL from its documentation, available at [https://openpyxl.readthedocs.io/en/stable/](https://openpyxl.readthedocs.io/en/stable/).
  prefs: []
  type: TYPE_NORMAL
- en: Using pandas with XLSX files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can load existing `.xlsx` files with the help of pandas. The `read_excel`
    method is used to read Excel files as a DataFrame. This method uses an argument, `sheet_name`,
    which is used to specify the sheet we want to load. The sheet name can be specified
    either as a string or number starting from 0\. The `to_excel` method can be used
    to write into an Excel file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code reads an Excel file, manipulates it, and saves it. The code
    can be accessed from GitHub at `Pandas_xlsx_example.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Working with the JSON format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**JavaScript** **Object** **Notation **(**JSON**) is another popular data format
    in IoT systems. In this section, we will learn how to read JSON data with Python''s
    JSON, NumPy, and pandas packages.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this section, we will use the `zips.json` file, which contains US ZIP codes
    with city codes, geolocation details, and state codes. The file has JSON objects recorded
    in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Using JSON files with the JSON module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To load and decode JSON data, use the `json.load()` or `json.loads() `functions.
    As an example, the following code reads the first 10 lines from the `zips.json`
    file and prints them nicely:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The objects are printed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `json.loads()` function takes string objects as input while the `json.load()` function takes
    file objects as input. Both functions decode the JSON object and load it in the
    `json_data` file as a Python dictionary object.
  prefs: []
  type: TYPE_NORMAL
- en: The `json.dumps()` function takes an object and produces a JSON string, and
    the `json.dump()` function takes an object and writes the JSON string to the file.
    Thus, both these function do the opposite of the `json.loads()` and `json.load()`
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: JSON files with the pandas module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'JSON strings or files can be read with the `pandas.read_json()` function, which returns
    a DataFrame or series object. For example, the following code reads the `zips.json`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We set `lines=True` because each line contains a separate object in JSON format.
    Without this argument being set to `True`, pandas will raise `ValueError`. The
    DataFrame is printed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: To save the pandas DataFrame or series object to a JSON file or string, use
    the `Dataframe.to_json() `function.
  prefs: []
  type: TYPE_NORMAL
- en: More information for both of these functions can be found at these links: [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html) and [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'While CSV and JSON remain the most popular data formats for IoT data, due to
    its large size, it is often necessary to distribute data. There are two popular distributed
    mechanisms for data storage and access: HDF5 and HDFS. Let''s first learn about
    the HDF5 format.'
  prefs: []
  type: TYPE_NORMAL
- en: HDF5 format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hierarchical Data Format** (**HDF**) is a specification put together by the
    HDF Group, a consortium of academic and industry organizations ([https://support.hdfgroup.org/HDF5/](https://support.hdfgroup.org/HDF5/)).
    In HDF5 files, data is organized into groups and datasets. A group is a collection
    of **groups** or **datasets**. A dataset is a multidimensional homogeneous array.'
  prefs: []
  type: TYPE_NORMAL
- en: In Python, PyTables and h5py are two major libraries for handling HDF5 files.
    Both these libraries require HDF5 to be installed. For the parallel version of
    HDF5, a version of MPI is also required to be installed. Installation of HDF5
    and MPI is beyond the scope of this book. Installation instructions for parallel
    HDF5 can be found at the following link: [https://support.hdfgroup.org/ftp/HDF5/current/src/unpacked/release_docs/INSTALL_parallel](https://support.hdfgroup.org/ftp/HDF5/current/src/unpacked/release_docs/INSTALL_parallel).
  prefs: []
  type: TYPE_NORMAL
- en: Using HDF5 with PyTables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s first create an HDF5 file from the numeric data we have in the `temp.csv` file
    with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the numeric data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Open the HDF5 file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the `root` node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a group with `create_group()` or a dataset with `create_array()`, and
    repeat this until all the data is stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Close the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s read the file and print the dataset to make sure it is properly written:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We get the NumPy array back.
  prefs: []
  type: TYPE_NORMAL
- en: Using HDF5 with pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also read and write HDF5 files with pandas. To read HDF5 files with
    pandas, they must first be created with it. For example, let''s use pandas to
    create a HDF5 file containing global power values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s read the HDF5 file that we created and print the array back:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The values of the DataFrame can be read in three different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '`store[''global_power'']`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`store.get(''global_power'')`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`store.global_power`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas also provides the high-level `read_hdf()` function and the `to_hdf()`
    DataFrame method for reading and writing HDF5 files.
  prefs: []
  type: TYPE_NORMAL
- en: More documentation on HDF5 in pandas is available at the following link: [http://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5](http://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5).
  prefs: []
  type: TYPE_NORMAL
- en: Using HDF5 with h5py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `h5py` module is the most popular way to handle HDF5 files in Python. A
    new or existing HDF5 file can be opened with the `h5py.File()` function. After
    the file is open, its groups can simply be accessed by subscripting the file object
    as if it was a dictionary object. For example, the following code opens an HDF5
    file with `h5py` and then prints the array stored in the `/global_power `group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The `arr` variable prints an `HDF5 dataset` type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'For a new `hdf5file`, datasets and groups can be created by using the `hdf5file.create_dataset()`
    function, returning the dataset object, and the `hdf5file.create_group()` function,
    returning the folder object. The `hdf5file` file object is also a folder object
    representing `/`, the root folder. Dataset objects support array style slicing
    and dicing to set or read values from them. For example, the following code creates
    an HDF5 file and stores one dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`h5py` provides an `attrs` proxy object with a dictionary-like interface to
    store and retrieve metadata about the file, folders, and datasets. For example,
    the following code sets and then prints the dataset and file attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'For more information about the `h5py` library, refer to the documentation at
    the following link: [http://docs.h5py.org/en/latest/index.html](http://docs.h5py.org/en/latest/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned about different data formats. Often, large data is stored
    commercially in databases, therefore we will explore how to access both SQL and
    NoSQL databases next.
  prefs: []
  type: TYPE_NORMAL
- en: SQL data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most databases are organized using relational models. A relational database
    consists of one or more related tables of information, and the relationship between
    information in different tables is described using keys. Conventionally, these
    databases are managed using the **Database Management System** (**DBMS**), software
    which interacts with end users, different applications, and the database itself
    to capture and analyze data. Commercially available DBMSes use **Structured Query
    Language** (**SQL**) to access and manipulate databases. We can also use Python
    to access relational databases. In this section, we will explore SQLite and MySQL,
    two very popular database engines that work with Python.
  prefs: []
  type: TYPE_NORMAL
- en: The SQLite database engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to the SQLite home page ([https://sqlite.org/index.html](https://sqlite.org/index.html)),
    *SQLite is a self-contained, high-reliability, embedded, full-featured, public-domain
    SQL database engine*.
  prefs: []
  type: TYPE_NORMAL
- en: SQLite is optimized for use in embedded applications. It is simple to use and
    quite fast. We need to use the `sqlite3` Python module to integrate SQLite with
    Python. The `sqlite3` module is bundled with Python 3, so there is no need to
    install it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the data from the European Soccer Database ([https://github.com/hugomathien/football-data-collection](https://github.com/hugomathien/football-data-collection))
    for demonstrative purposes. We assume that you already have a SQL server installed
    and started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step after importing `sqlite3` is to create a connection to the database
    using the `connect` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The European Soccer Database consists of eight tables. We can use `read_sql` to
    read the database table or SQL query into the DataFrame. This prints a list of
    all the tables in the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/94e3fff5-08d7-4fe5-ae06-5be6fdd7816a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s read data from the `Country` table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/5836ebd8-3242-4d4e-9db5-f0ac1f3b2a3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can use SQL queries on tables. In the following example, we select players
    whose height is greater than or equal to `180` and whose weight is greater than
    or equal to `170`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/4f0a7a4b-6b6f-4a6a-b613-aa60f23e8ca8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, do not forget to close the connection using the `close` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: If you made any changes in the database, you will need to use the `commit()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: The MySQL database engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though we can use SQLite for large databases, MySQL is generally preferred.
    In addition to being scalable for large databases, MySQL is also useful where
    data security is paramount. Before using MySQL, you will need to install the Python
    MySQL connector. There are many possible Python MySQL connectors such as, MySQLdb,
    PyMySQL, and MySQL; we will use `mysql-connector-python`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In all three, after making a connection using the `connect` method, we define
    the `cursor` element and use the `execute` method to run different SQL queries.
    To install MySQL, we use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the Python MySQL connector is installed, we can start a connection
    with the SQL server. Replace the `host`, `user`, and `password` configurations
    with your SQL server configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check existing databases in the server and list them. To do this, we
    use the `cursor` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/303fb99e-29d7-48b9-9ef8-00eecf6b76de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can access one of the existing databases. Let''s list the tables in one
    of the databases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: NoSQL data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Not Only Structured Query Language** (**NoSQL**) database is not a relational
    database; instead, data can be stored in key-value, JSON, document, columnar,
    or graph formats. They are frequently used in big data and real-time applications.
    We will learn here how to access NoSQL data using MongoDB, and we assume you have
    the MongoDB server configured properly and on:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will need to establish a connection with the Mongo daemon using the `MongoClient`
    object. The following code establishes the connection to the default host, `localhost` ,
    and port (`27017`). And it gives us access to the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we try to load the `cancer` dataset available in scikit-learn
    to the Mongo database. So, we first get the breast cancer dataset and convert
    it to a pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we convert this into the JSON format, use the `json.loads()` function
    to decode it, and insert the decoded data into the open database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create a collection named `cancer_data` that contains the data. We
    can query the document we just created, using the `cursor` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/cc5f0782-9f58-4952-b32a-fb0a42de7824.png)'
  prefs: []
  type: TYPE_IMG
- en: When it comes to distributed data on the IoT, **Hadoop Distributed File System **(**HDFS**)
    is another popular method for providing distributed data storage and access in
    IoT systems. In the next section, we study how to access and store data in HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HDFS is a popular storage and access method for storing and retrieving data
    files for IoT solutions. The HDFS format can hold large amounts of data in a reliable
    and scalable manner. Its design is based on the **Google File System** ([https://ai.google/research/pubs/pub51](https://ai.google/research/pubs/pub51)).
    HDFS splits individual files into fixed-size blocks that are stored on machines
    across the cluster. To ensure reliability, it replicates the file blocks and distributes
    them across the cluster; by default, the replication factor is 3\. HDFS has two
    main architecture components:'
  prefs: []
  type: TYPE_NORMAL
- en: The first, **NodeName**, stores the metadata for the entire filesystem, such
    as filenames, their permissions, and the location of each block of each file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second, **DataNode** (one or more), is where file blocks are stored. It
    performs **Remote Procedure Calls** (**RPCs**) using protobufs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RPC** is a protocol that one program can use to request a service from a
    program located on another computer on a network without having to know the network''s
    details. A procedure call is also sometimes known as a **function call** or a
    **subroutine call**.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many options for programmatically accessing HDFS in Python, such as `snakebite`,
    `pyarrow`, `hdfs3`, `pywebhdfs`, `hdfscli`, and so on. In this section, we will
    focus mainly on libraries that provide native RPC client interfaces and work with
    Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: Snakebite is a pure Python module and CLI that allows you to access HDFS from
    Python programs. At present, it only works with Python 2; Python 3 is not supported.
    Moreover, it does not yet support write operations, and so we are not including
    it in the book. However, if you are interested in knowing more about this, you
    can refer to Spotify's GitHub: [https://github.com/spotify/snakebite](https://github.com/spotify/snakebite).
  prefs: []
  type: TYPE_NORMAL
- en: Using hdfs3 with HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`hdfs3` is a lightweight Python wrapper around the C/C++ `libhdfs3` library.
    It allows us to use HDFS natively from Python. To start, we first need to connect
    with the HDFS NameNode; this is done using the `HDFileSystem` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This automatically establishes a connection with the NameNode. Now, we can
    access a directory listing using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This will list all the files and directories in the `tmp` folder. You can use
    functions such as `mkdir` to make a directory and `cp` to copy a file from one
    location to another. To write into a file, we open it first using the `open` method
    and use `write`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Data can be read from the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: You can learn more about `hdfs3` from its documentation: [https://media.readthedocs.org/pdf/hdfs3/latest/hdfs3.pdf](https://media.readthedocs.org/pdf/hdfs3/latest/hdfs3.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Using PyArrow's filesystem interface for HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyArrow has a C++-based interface for HDFS. By default, it uses `libhdfs`,
    a JNI-based interface, for the Java Hadoop client. Alternatively, we can also
    use `libhdfs3`, a C++ library for HDFS. We connect to the NameNode using `hdfs.connect`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: If we change the driver to `libhdfs3`, we will be using the C++ library for
    HDFS from Pivotal Labs. Once the connection to the NameNode is made, the filesystem
    is accessed using the same methods as for hdfs3.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS is preferred when the data is extremely large. It allows us to read and
    write data in chunks; this is helpful for accessing and processing streaming data.
    A nice comparison of the three native RPC client interfaces is presented in the
    following blog post: [http://wesmckinney.com/blog/python-hdfs-interfaces/](http://wesmckinney.com/blog/python-hdfs-interfaces/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter dealt with many different data formats, and, in the process, many
    different datasets. We started with the simplest TXT data and accessed the `Shakespeare`
    play data. We learned how to read data from CSV files using the `csv`, `numpy`,
    and `pandas` modules. We moved on to the JSON format; we used Python's JSON and
    pandas modules to access JSON data. From data formats, we progressed to accessing
    databases and covered both SQL and NoSQL databases. Next, we learned how to work
    with the Hadoop File System in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing data is the first step. In the next chapter, we will learn about machine
    learning tools that will help us to design, model, and make informed predictions
    on data.
  prefs: []
  type: TYPE_NORMAL
