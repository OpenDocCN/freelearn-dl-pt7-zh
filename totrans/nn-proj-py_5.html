<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Removing Noise from Images Using Autoencoders</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we will study a class of neural networks known as autoencoders, which have gained traction in recent years. In particular, the ability of autoencoders to remove noise from images has been greatly studied. In this chapter, we will build and train an autoencoder that is able to denoise and restore corrupted images. </p>
<p class="calibre2">In this chapter, we'll cover the following topics:</p>
<ul class="calibre11">
<li class="calibre12">What are autoencoders?</li>
<li class="calibre12">Unsupervised learning</li>
<li class="calibre12">Types of autoencoders—basic autoencoders, deep autoencoders and convolutional autoencoders</li>
<li class="calibre12">Autoencoders for image compression</li>
<li class="calibre12">Autoencoders for image denoising</li>
<li class="calibre12">Step-by-step guide to build and train an autoencoder in Keras</li>
<li class="calibre12">Analysis of our results</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="calibre2">The Python libraries required for this chapter are:</p>
<ul class="calibre11">
<li class="calibre12">matplotlib 3.0.2</li>
<li class="calibre12">Keras 2.2.4</li>
<li class="calibre12">Numpy 1.15.2</li>
<li class="calibre12">PIL 5.4.1</li>
</ul>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">The code and dataset for this chapter can be found in the GitHub repository for the book at <a href="https://github.com/PacktPublishing/Neural-Network-Projects-with-Python" target="_blank" class="calibre10">https://github.com/PacktPublishing/Neural-Network-Projects-with-Python</a>:</p>
<p class="calibre2">To download the code into your computer, you may run the following <kbd class="calibre13">git clone</kbd> command:</p>
<pre class="calibre17"><strong class="calibre1"><span>$ git clone https://github.com/PacktPublishing/Neural-Network-Projects-with-Python.git</span>   </strong> </pre>
<p class="calibre2">After the process is complete, there will be a folder titled <kbd class="calibre13">Neural-Network-Projects-with-Python</kbd><span class="calibre5">. Enter the folder by running:</span></p>
<pre class="calibre17"><strong class="calibre1">$ cd Neural-Network-Projects-with-Python</strong></pre>
<p class="calibre2">To install the required Python libraries in a virtual environment, run the following command:</p>
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre17"><strong class="calibre1"><span>$ conda</span> <span>env</span> <span>create</span> <span>-</span><span>f</span> <span>environment</span><span>.</span><span>yml</span></strong></pre></div>
</div>
<p class="calibre2">Note that you should have installed Anaconda in your computer first, before running this command. To enter the virtual environment, run the following command:</p>
<pre class="calibre17"><strong class="calibre1">$ conda activate neural-network-projects-python</strong></pre>
<p class="calibre2">Navigate to the <kbd class="calibre13">Chapter05</kbd><span class="calibre5"> folder </span>by running the following command:</p>
<pre class="calibre17"><strong class="calibre1">$ cd Chapter05</strong></pre>
<p class="calibre2">The following files are located in the folder:</p>
<ul class="calibre11">
<li class="calibre12"><kbd class="calibre13">autoencoder_image_compression.py</kbd>: This is the code for the <em class="calibre18">Building a simple autoencoder</em> section in this chapter</li>
<li class="calibre12"><kbd class="calibre13">basic_autoencoder_denoise_MNIST.py</kbd> and <kbd class="calibre13">conv_autoencoder_denoise_MNIST.py</kbd>: These are the code for the <em class="calibre18">Denoising autoencoder</em> section in this chapter</li>
<li class="calibre12"><kbd class="calibre13">basic_autoencoder_denoise_documents.py</kbd> and <kbd class="calibre13">deep_conv_autoencoder_denoise_documents.py</kbd>: These are the code for the <em class="calibre18">Denoising documents with autoencoders</em> section in this chapter</li>
</ul>
<p class="calibre2">To run the code in each file, simply execute each Python file, as follows:</p>
<pre class="calibre17"><strong class="calibre1">$ python <span>autoencoder_image_compression.py</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What are autoencoders?</h1>
                </header>
            
            <article>
                
<p class="calibre2">So far in this book, we have looked at the applications of neural networks for supervised learning. Specifically, in each project, we have a labeled dataset (that is, features <strong class="calibre4">x</strong> and label <strong class="calibre4">y</strong><em class="calibre8">) </em>and our goal is to train a neural network using this dataset, so that the neural network is able to predict label <strong class="calibre4">y</strong> from any new instance <strong class="calibre4">x</strong>. </p>
<p class="calibre2">A typical feedforward neural network is shown in the following diagram:</p>
<p class="mce-root"><img class="alignnone70" src="assets/802d112e-32ad-4fe9-9966-c86bb791ad3b.png"/></p>
<p class="calibre2">In this chapter, we will study a different class of neural networks, known as autoencoders. Autoencoders represent a paradigm shift from the conventional neural networks we have seen so far. The goal of autoencoders is to learn a <strong class="calibre4">Latent</strong> <strong class="calibre4">Representation</strong> of the input. This representation is usually a compressed representation of the original input.</p>
<p class="calibre2">All autoencoders have an <strong class="calibre4">Encoder </strong>and a <strong class="calibre4">Decoder.</strong> The role of the encoder is to encode the input to a learned, compressed representation, and the role of the decoder is to reconstruct the original input using the compressed representation. </p>
<p class="calibre2">The following diagram illustrates the architecture of a typical autoencoder:</p>
<p class="mce-root"><img class="alignnone71" src="assets/e2ccadaf-b45a-494f-8193-48b3fcd8914c.png"/><br class="calibre6"/></p>
<p class="calibre2">Notice that, in the preceding diagram, we do not require a label <em class="calibre8">y</em>, unlike in CNNs. This distinction means that autoencoders are a form of unsupervised learning, while CNNs fall within the realm of supervised learning. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Latent representation</h1>
                </header>
            
            <article>
                
<p class="calibre2">At this point, you might wonder what is the purpose of autoencoders. Why do we bother learning a representation of the original input, only to reconstruct a similar output? The answer lies in the learned representation of the input. By forcing the learned representation to be compressed (that is, having smaller dimensions compared to the input), we essentially force the neural network to learn the most salient representation of the input. This ensures that the learned representation only captures the most relevant characteristics of the input, known as the <strong class="calibre4">latent representation</strong>.</p>
<p class="calibre2">As a concrete example of latent representations, take, for example, an autoencoder trained on the cats and dogs dataset, as shown in the following diagram:</p>
<p class="mce-root"><img src="assets/f25ef79d-d1de-4ad3-866d-e7ee135cdfa7.png" class="calibre49"/></p>
<p class="calibre2">An autoencoder trained on this dataset will eventually learn that the salient characteristics of cats and dogs are the the shape of the ears, the length of whiskers, the snout size, and the length of the tongue visible. These salient characteristics are captured by the latent representation. </p>
<p class="calibre2">With this latent<span class="calibre5"> r</span>epresentation learned by the autoencoder, we can then do the following:</p>
<ul class="calibre11">
<li class="calibre12">Reduce the dimensionality of the input data. The latent<span> r</span>epresentation is a natural reduced representation of the input data.</li>
<li class="calibre12">Remove any noise from the input data (known as denoising). Noise is not a salient characteristic and therefore should be easily identifiable by using the latent representation.</li>
</ul>
<p class="calibre2">In the sections to follow, we shall create and train autoencoders for each of the preceding purposes.</p>
<div class="packttip">Note that, in the previous example, we have used examples such as shape of ears and snout size as descriptions for the latent representation. In reality, latent representations are simply a matrix of numbers and it is impossible to assign meaningful labels for them (nor do we need to). The descriptions that we used here simply provide an intuitive explanation for latent representations.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoencoders for data compression</h1>
                </header>
            
            <article>
                
<p class="calibre2">So far, we have seen how autoencoders are able to learn a reduced representation of the input data. It is natural to think that autoencoders can do a good job at generalized data compression. However, that is not the case. Autoencoders are poor at generalized data compression, such as image compression (that is, JPEG) and audio compression (that is, MP3), because the learned latent representation only represents the data on which it was trained. In other words, autoencoders only work well for images similar to those on which it was trained. </p>
<p class="calibre2">Furthermore, autoencoders are a "lossy" form of data compression, which means that the output from autoencoders will have less information when compared to the original input. These characteristics mean that autoencoders are poor at being generalized data compression techniques. Other forms of data compression, such as JPEG and MP3, are superior when compared to autoencoders.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The MNIST handwritten digits dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">One of the datasets that we'll use for this chapter is the MNIST handwritten digits dataset. The MNIST dataset contains 70,000 samples of handwritten digits, each of size 28 x 28 pixels. Each sample contains only one digit within the image, and all samples are labeled. </p>
<p class="calibre2">The MNIST dataset is provided directly in Keras, and we can import it by simply running the following code:</p>
<pre class="calibre17">from keras.datasets import mnist<br class="title-page-name"/><br class="title-page-name"/>training_set, testing_set = mnist.load_data()<br class="title-page-name"/>X_train, y_train = training_set<br class="title-page-name"/>X_test, y_test = testing_set</pre>
<p class="calibre2">Let's plot out each of the digits to better visualize our data. The following code snippet uses <kbd class="calibre13">matplotlib</kbd> to plot the data:</p>
<pre class="calibre17">from matplotlib import pyplot as plt<br class="title-page-name"/>fig, ((ax1, ax2, ax3, ax4, ax5), (ax6, ax7, ax8, ax9, ax10)) = plt.subplots(2, 5, figsize=(10,5))<br class="title-page-name"/><br class="title-page-name"/>for idx, ax in enumerate([ax1,ax2,ax3,ax4,ax5, ax6,ax7,ax8,ax9,ax10]):<br class="title-page-name"/>    for i in range(1000):<br class="title-page-name"/>        if y_test[i] == idx:<br class="title-page-name"/>            ax.imshow(X_test[i], cmap='gray')<br class="title-page-name"/>            ax.grid(False)<br class="title-page-name"/>            ax.set_xticks([])<br class="title-page-name"/>            ax.set_yticks([])<br class="title-page-name"/>            break<br class="title-page-name"/>plt.tight_layout()<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img src="assets/d22c2165-a1ac-4a11-953d-c2c4e12aef71.png" class="calibre50"/></p>
<p class="calibre2">We can see that the digits are definitely handwritten, and each 28 x 28 image captures only one digit. The autoencoder should be able to learn the compressed representation of these digits (smaller than 28 x 28), and to reproduce the images using this compressed representation.</p>
<p class="calibre2">The following diagram illustrates this:</p>
<p class="mce-root"><img class="alignnone72" src="assets/49774e97-83b3-4391-9d3c-cfe3f4776937.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a simple autoencoder</h1>
                </header>
            
            <article>
                
<p class="calibre2">To cement our understanding, let's start off by building the most basic autoencoder, as shown in the following diagram:</p>
<p class="mce-root"><img class="alignnone73" src="assets/10feca38-7f7b-4816-9733-322fe50ab617.png"/></p>
<p class="calibre2">So far, we have emphasized that the hidden layer (<strong class="calibre4">Latent</strong> <strong class="calibre4">Representation</strong>) should be of a smaller dimension than the input data. This ensures that the latent representation is a compressed representation of the salient features of the input. But how small should it be?</p>
<p class="calibre2">Ideally, the size of the hidden layer should balance between being:</p>
<ul class="calibre11">
<li class="calibre12">Sufficiently <em class="calibre18">small</em> enough to represent a compressed representation of the input features</li>
<li class="calibre12">Sufficiently <em class="calibre18">large</em> enough for the decoder to reconstruct the original input without too much loss</li>
</ul>
<p class="calibre2">In other words, the size of the hidden layer is a hyperparameter that we need to select carefully to obtain the best results. We shall see how we can define the size of the hidden layer in Keras.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building autoencoders in Keras</h1>
                </header>
            
            <article>
                
<p class="calibre2">First, let's start building our basic autoencoder in Keras. As always, we'll use the <kbd class="calibre13">Sequential</kbd> class in Keras to build our model.</p>
<p class="calibre2">We'll start by importing and defining a new <kbd class="calibre13">Sequential</kbd> class in Keras:</p>
<pre class="calibre17">from keras.models import Sequential<br class="title-page-name"/><br class="title-page-name"/>model = Sequential()</pre>
<p class="calibre2">Next, we'll add the hidden layer to our model. From the previous diagram, we can clearly see that the hidden layer is a fully connected layer (that is, a <kbd class="calibre13">Dense</kbd> layer). From the <kbd class="calibre13">Dense</kbd> class in Keras, we can define the size of the hidden layer through the <kbd class="calibre13">units</kbd> parameter. The number of units is a hyperparameter that we will be experimenting with. For now, let's use a single node (units=1) as the hidden layer. The <kbd class="calibre13">input_shape</kbd> to the <kbd class="calibre13">Dense</kbd> layer is a vector of size <kbd class="calibre13">784</kbd> (since we are using 28 x 28 images) and the <kbd class="calibre13">activation</kbd> function is the <kbd class="calibre13">relu</kbd> activation function.</p>
<p class="calibre2">The following code adds a <kbd class="calibre13">Dense</kbd> layer with a single node to our model:</p>
<pre class="calibre17">from keras.layers import Dense<br class="title-page-name"/><br class="title-page-name"/>hidden_layer_size = 1<br class="title-page-name"/>model.add(Dense(units=hidden_layer_size, input_shape=(784,), <br class="title-page-name"/>                activation='relu'))</pre>
<p class="calibre2">Lastly, we'll add the output layer. The output layer is also a fully connected layer (that is, a <kbd class="calibre13">Dense</kbd> layer), and the size of the output layer should naturally be <kbd class="calibre13">784</kbd>, since we are trying to output the original 28 x 28 image. We use a <kbd class="calibre13">Sigmoid</kbd> activation function for the output to constrain the output values (value per pixel) between 0 and 1.</p>
<p class="calibre2">The following code adds an output <kbd class="calibre13">Dense</kbd> layer with <kbd class="calibre13">784</kbd> units to our model:</p>
<pre class="calibre17">model.add(Dense(units=784, activation='sigmoid'))</pre>
<p class="calibre2">Before we train our model, let's check the structure of our model and make sure that it is consistent with our diagram.</p>
<p class="calibre2">We can do this by calling the <kbd class="calibre13">summary()</kbd> function:</p>
<pre class="calibre17">model.summary()</pre>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img src="assets/ad6c0a37-f66e-4c5d-a474-a9a77f619831.png" class="calibre51"/></p>
<p class="calibre2">Before we move on to the next step, let's create a function that encapsulates the model creation process that we just went through. Having such a function is useful, as it allows us to easily create different models with different hidden layer sizes.</p>
<p class="calibre2">The following code defines a function that creates a basic autoencoder with a <kbd class="calibre13">hidden_layer_size</kbd> <span class="calibre5">variable</span>:</p>
<pre class="calibre17">def create_basic_autoencoder(hidden_layer_size):<br class="title-page-name"/>    model = Sequential() <br class="title-page-name"/>    model.add(Dense(units=hidden_layer_size, input_shape=(784,), <br class="title-page-name"/>                    activation='relu'))<br class="title-page-name"/>    model.add(Dense(units=784, activation='sigmoid'))<br class="title-page-name"/>    return model<br class="title-page-name"/><br class="title-page-name"/>model = create_basic_autoencoder(hidden_layer_size=1)</pre>
<p class="calibre2">The next step is to preprocess our data. There are two preprocessing steps required:</p>
<ol class="calibre14">
<li class="calibre12">Reshape the images from a 28 x 28 vector to a 784 x 1 vector.</li>
<li class="calibre12">Normalize the values of the vector between 0 and 1 from the current 0 to 255. This smaller range of values makes it easier to train our neural network using the data.</li>
</ol>
<p class="calibre2">To reshape the images from 28 x 28 to 784 x 1, we simply run the following code:</p>
<pre class="calibre17">X_train_reshaped = X_train.reshape((X_train.shape[0],<br class="title-page-name"/>                                    X_train.shape[1]*X_train.shape[2]))<br class="title-page-name"/>X_test_reshaped = X_test.reshape((X_test.shape[0],<br class="title-page-name"/>                                  X_test.shape[1]*X_test.shape[2]))</pre>
<div class="packttip">Note that first dimension, <kbd class="calibre19">X_train.shape[0]</kbd>, refers to the number of samples.</div>
<p class="calibre2">To normalize the values of the vector between 0 and 1 (from the original range of 0 to 255), we run the following code:</p>
<pre class="calibre17">X_train_reshaped = X_train_reshaped/255.<br class="title-page-name"/>X_test_reshaped = X_test_reshaped/255.</pre>
<p class="calibre2">With that done, we can start to train our model. We'll first compile our model using the <kbd class="calibre13">adam</kbd> optimizer and <kbd class="calibre13">mean_squared_error</kbd> as the <kbd class="calibre13">loss</kbd> function. The <kbd class="calibre13">mean_squared_error</kbd> is useful in this case because we need a <kbd class="calibre13">loss</kbd> function that quantifies the pixel-wise discrepancy between the input and the output. </p>
<p class="calibre2">The following code compiles our model using the aforementioned parameters:</p>
<pre class="calibre17">model.compile(optimizer='adam', loss='mean_squared_error')</pre>
<p class="calibre2">Finally, let's train our model for <kbd class="calibre13">10</kbd> epochs. Note that we use <kbd class="calibre13">X_train_reshaped</kbd> as both the input (<em class="calibre8">x</em>) and output (<em class="calibre8">y</em>). This makes sense because we are trying to train the autoencoder to produce output that is identical to the input. </p>
<p class="calibre2">We train our autoencoder with the following code:</p>
<pre class="calibre17">model.fit(X_train_reshaped, X_train_reshaped, epochs=10)</pre>
<p class="calibre2"/>
<p class="calibre2">We'll see the following output:</p>
<p class="mce-root"><img src="assets/2cc71cc5-daf2-4546-9267-0f0b77e61671.png" class="calibre52"/></p>
<p class="calibre2">With our model trained, let's apply it on our testing set:</p>
<pre class="calibre17">output = model.predict(X_test_reshaped)</pre>
<p class="calibre2">We would like to plot the output, and see how closely it matches with the original input. Remember, the autoencoder should produce output images that are close to the original input images.</p>
<p class="calibre2">The following code selects five random images from the testing set and plots them on the top row. It then plots the output images for these five randomly selected inputs on the bottom row:</p>
<pre class="calibre17">import random<br class="title-page-name"/>fig, ((ax1, ax2, ax3, ax4, ax5),<br class="title-page-name"/>      (ax6, ax7, ax8, ax9, ax10)) = plt.subplots(2, 5, figsize=(20,7))<br class="title-page-name"/><br class="title-page-name"/># randomly select 5 images<br class="title-page-name"/>randomly_selected_imgs = random.sample(range(output.shape[0]),5)<br class="title-page-name"/><br class="title-page-name"/># plot original images (input) on top row<br class="title-page-name"/>for i, ax in enumerate([ax1,ax2,ax3,ax4,ax5]):<br class="title-page-name"/>    ax.imshow(X_test[randomly_selected_imgs[i]], cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_ylabel("INPUT",size=40)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/># plot output images from our autoencoder on the bottom row<br class="title-page-name"/>for i, ax in enumerate([ax6,ax7,ax8,ax9,ax10]):<br class="title-page-name"/>    ax.imshow(output[randomly_selected_imgs[i]].reshape(28,28), <br class="title-page-name"/>              cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_ylabel("OUTPUT",size=40)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>plt.tight_layout()<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We'll see the following output:</p>
<p class="mce-root"><img src="assets/445943f4-d849-4f17-99f4-59ca4d2b5694.png" class="calibre53"/></p>
<div class="packtfigref">Top: Original images provided to the autoencoder as input; bottom: images output from the autoencoder</div>
<p class="calibre2">Wait a minute: the output images look terrible! They look like a blurry white scribble and they look nothing like our original input images. Clearly, an autoencoder with a hidden layer size of one node is insufficient to encode this dataset. This latent representation is too small for our autoencoder to sufficiently capture the salient features of our data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Effect of hidden layer size on autoencoder performance</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's try training more autoencoders with different hidden layer sizes and see how they fare.</p>
<p class="calibre2"/>
<p class="calibre2">The following code creates and trains five different models with <kbd class="calibre13">2</kbd>, <kbd class="calibre13">4</kbd>, <kbd class="calibre13">8</kbd>, <kbd class="calibre13">16</kbd>, and <kbd class="calibre13">32</kbd> nodes in the hidden layer:</p>
<pre class="calibre17">hiddenLayerSize_2_model = create_basic_autoencoder(hidden_layer_size=2)<br class="title-page-name"/>hiddenLayerSize_4_model = create_basic_autoencoder(hidden_layer_size=4)<br class="title-page-name"/>hiddenLayerSize_8_model = create_basic_autoencoder(hidden_layer_size=8)<br class="title-page-name"/>hiddenLayerSize_16_model = create_basic_autoencoder(hidden_layer_size=16)<br class="title-page-name"/>hiddenLayerSize_32_model = create_basic_autoencoder(hidden_layer_size=32)</pre>
<p class="calibre2">Notice how each successive model has twice the number of nodes in the hidden layer as the preceding model.</p>
<p class="calibre2">Now, let's train all five of our models together. We use the <kbd class="calibre13">verbose=0</kbd> argument in the <kbd class="calibre13">fit()</kbd> function to hide the output, as shown in the following code snippet:</p>
<pre class="calibre17">hiddenLayerSize_2_model.compile(optimizer='adam',<br class="title-page-name"/>                                loss='mean_squared_error')<br class="title-page-name"/>hiddenLayerSize_2_model.fit(X_train_reshaped, X_train_reshaped, <br class="title-page-name"/>                            epochs=10, verbose=0)<br class="title-page-name"/><br class="title-page-name"/>hiddenLayerSize_4_model.compile(optimizer='adam',<br class="title-page-name"/>                                loss='mean_squared_error')<br class="title-page-name"/>hiddenLayerSize_4_model.fit(X_train_reshaped, X_train_reshaped,<br class="title-page-name"/>                            epochs=10, verbose=0)<br class="title-page-name"/><br class="title-page-name"/>hiddenLayerSize_8_model.compile(optimizer='adam',<br class="title-page-name"/>                                loss='mean_squared_error')<br class="title-page-name"/>hiddenLayerSize_8_model.fit(X_train_reshaped, X_train_reshaped,<br class="title-page-name"/>                            epochs=10, verbose=0)<br class="title-page-name"/><br class="title-page-name"/>hiddenLayerSize_16_model.compile(optimizer='adam',<br class="title-page-name"/>                                 loss='mean_squared_error')<br class="title-page-name"/>hiddenLayerSize_16_model.fit(X_train_reshaped, X_train_reshaped, <br class="title-page-name"/>                             epochs=10, verbose=0)<br class="title-page-name"/><br class="title-page-name"/>hiddenLayerSize_32_model.compile(optimizer='adam',<br class="title-page-name"/>                                 loss='mean_squared_error')<br class="title-page-name"/>hiddenLayerSize_32_model.fit(X_train_reshaped, X_train_reshaped,<br class="title-page-name"/>                             epochs=10, verbose=0)</pre>
<p class="calibre2"/>
<p class="calibre2">Once training is complete, we apply the trained models on the testing set:</p>
<pre class="calibre17">output_2_model = hiddenLayerSize_2_model.predict(X_test_reshaped)<br class="title-page-name"/>output_4_model = hiddenLayerSize_4_model.predict(X_test_reshaped)<br class="title-page-name"/>output_8_model = hiddenLayerSize_8_model.predict(X_test_reshaped)<br class="title-page-name"/>output_16_model = hiddenLayerSize_16_model.predict(X_test_reshaped)<br class="title-page-name"/>output_32_model = hiddenLayerSize_32_model.predict(X_test_reshaped)</pre>
<p class="calibre2">Now, let's plot five randomly selected outputs from each model and see how they compare to the original input image:</p>
<pre class="calibre17">fig, axes = plt.subplots(7, 5, figsize=(15,15))<br class="title-page-name"/><br class="title-page-name"/>randomly_selected_imgs = random.sample(range(output.shape[0]),5)<br class="title-page-name"/>outputs = [X_test, output, output_2_model, output_4_model, output_8_model,<br class="title-page-name"/>           output_16_model, output_32_model]<br class="title-page-name"/><br class="title-page-name"/># Iterate through each subplot and plot accordingly<br class="title-page-name"/>for row_num, row in enumerate(axes):<br class="title-page-name"/>    for col_num, ax in enumerate(row):<br class="title-page-name"/>        ax.imshow(outputs[row_num][randomly_selected_imgs[col_num]]. \<br class="title-page-name"/>                      reshape(28,28), cmap='gray')<br class="title-page-name"/>        ax.grid(False)<br class="title-page-name"/>        ax.set_xticks([])<br class="title-page-name"/>        ax.set_yticks([])<br class="title-page-name"/>plt.tight_layout()<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img class="alignnone74" src="assets/32751ee1-662f-4725-8b4f-603a5fecce27.png"/></p>
<p class="calibre2"/>
<p class="calibre2">Isn't it beautiful? We can clearly see a nice transition as we double the number of nodes in the hidden layer. Gradually, we see that the output images become clearer and closer to the original input as we increase the number of nodes in the hidden layer.</p>
<p class="calibre2">At 32 nodes in the hidden layer, the output becomes very close (though not perfect) to the original input. Interestingly, we have shrunk the original input by 24.5 times (784÷32) and still managed to produce a satisfactory output. That's a pretty impressive compression ratio!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Denoising autoencoders</h1>
                </header>
            
            <article>
                
<p class="calibre2">Another interesting application of autoencoders is image denoising. Image noise is defined as a random variations of brightness in an image. Image noise may originate from the sensors of digital cameras. Although digital cameras these days are capable of capturing high quality images, image noise may still occur, especially in low light conditions. </p>
<p class="calibre2">Denoising images has been a challenge for researchers for many years. Early methods include applying some sort of image filter (that is, mean averaging filter, where the pixel value is replaced with the average pixel value of its neighbors) over the image. However, such methods can sometimes fall short and the effects can be less than ideal.</p>
<p class="calibre2">A few years ago, researchers discovered that we can train autoencoders for image denoising. The idea is simple. Instead of using the same input and output when training conventional autoencoders (as described in the previous section), we use a noisy image as the input and a clean reference image for the autoencoder to compare its output against. This is illustrated in the following diagram:</p>
<p class="mce-root"><img class="alignnone75" src="assets/1eab68ab-bf04-407d-af27-ff529c41dc0f.png"/></p>
<p class="calibre2">During the training process, the autoencoder will learn that the noises in the image should not be part of the output, and will learn to output a clean image. Essentially, we are training our autoencoder to remove noise from images!</p>
<p class="calibre2">Let's start by introducing noise to the MNIST dataset. We'll add a random value between <kbd class="calibre13">-0.5</kbd> and <kbd class="calibre13">0.5</kbd> to each pixel in the original images. This has the effect of increasing and decreasing the intensity of pixels at random. The following code does this using <kbd class="calibre13">numpy</kbd>:</p>
<pre class="calibre17">import numpy as np<br class="title-page-name"/><br class="title-page-name"/>X_train_noisy = X_train_reshaped + np.random.normal(0, 0.5,<br class="title-page-name"/>                                    size=X_train_reshaped.shape)<br class="title-page-name"/>X_test_noisy = X_test_reshaped + np.random.normal(0, 0.5,<br class="title-page-name"/>                                    size=X_test_reshaped.shape)</pre>
<p class="calibre2">Finally, we clip the noisy images between <kbd class="calibre13">0</kbd> and <kbd class="calibre13">1</kbd> to normalize the images:</p>
<pre class="calibre17">X_train_noisy = np.clip(X_train_noisy, a_min=0, a_max=1)<br class="title-page-name"/>X_test_noisy = np.clip(X_test_noisy, a_min=0, a_max=1)</pre>
<p class="calibre2">Let's define a basic autoencoder just like we did in the previous section. This basic autoencoder has a single hidden layer with <kbd class="calibre13">16</kbd> nodes.</p>
<p class="calibre2">The following code creates this autoencoder using the function that we defined in the previous section:</p>
<pre class="calibre17">basic_denoise_autoencoder = create_basic_autoencoder(hidden_layer_size=16)</pre>
<p class="calibre2">Next, we train our denoising autoencoder. Remember, the input to the denoising autoencoder is a noisy image and the output is a clean image. The following code trains our basic denoising autoencoder:</p>
<pre class="calibre17">basic_denoise_autoencoder.compile(optimizer='adam', <br class="title-page-name"/>                                  loss='mean_squared_error')<br class="title-page-name"/>basic_denoise_autoencoder.fit(X_train_noisy, X_train_reshaped, epochs=10)</pre>
<p class="calibre2">Once training is done, we apply our denoising autoencoder on the test images:</p>
<pre class="calibre17">output = basic_denoise_autoencoder.predict(X_test_noisy)</pre>
<p class="calibre2">We plot the output and compare it with the original image and the noisy image:</p>
<pre class="calibre17">fig, ((ax1, ax2, ax3, ax4, ax5), (ax6, ax7, ax8, ax9, ax10), (ax11,ax12,ax13,ax14,ax15)) = plt.subplots(3, 5, figsize=(20,13))<br class="title-page-name"/>randomly_selected_imgs = random.sample(range(output.shape[0]),5)<br class="title-page-name"/><br class="title-page-name"/># 1st row for original images<br class="title-page-name"/>for i, ax in enumerate([ax1,ax2,ax3,ax4,ax5]):<br class="title-page-name"/>    ax.imshow(X_test_reshaped[randomly_selected_imgs[i]].reshape(28,28), <br class="title-page-name"/>              cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_ylabel("Original \n Images", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/># 2nd row for input with noise added<br class="title-page-name"/>for i, ax in enumerate([ax6,ax7,ax8,ax9,ax10]):<br class="title-page-name"/>    ax.imshow(X_test_noisy[randomly_selected_imgs[i]].reshape(28,28),<br class="title-page-name"/>              cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_ylabel("Input With \n Noise Added", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/># 3rd row for output images from our autoencoder<br class="title-page-name"/>for i, ax in enumerate([ax11,ax12,ax13,ax14,ax15]):<br class="title-page-name"/>    ax.imshow(output[randomly_selected_imgs[i]].reshape(28,28), <br class="title-page-name"/>              cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_ylabel("Denoised \n Output", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>plt.tight_layout()<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img src="assets/595e6fac-c11b-4147-bda1-fb09ccbca711.png" class="calibre54"/></p>
<p class="calibre2">How does it do? Well, it could definitely be better! This basic denoising autoencoder is perfectly capable of removing noise, but it doesn't do a very good job at reconstructing the original image. We can see that this basic denoising autoencoder sometimes fails to separate noise from the digits, especially near the center of the image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep convolutional denoising autoencoder</h1>
                </header>
            
            <article>
                
<p class="calibre2">Can we do better than the basic, one-hidden layer autoencoder? We saw in the previous chapter, <a href="48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml" target="_blank" class="calibre10">Chapter 4</a>, <em class="calibre8">Cats Versus Dogs – Image Classification Using CNNs</em>, that deep CNNs perform well for image classification tasks. Naturally, we can apply the same concept for autoencoders too. Instead of using only one hidden layer, we use multiple layers (that is, a deep network) and instead of a fully connected dense layer, we use convolutional layers.</p>
<p class="calibre2">The following diagram illustrates the architecture of a deep convolutional autoencoder:</p>
<p class="mce-root"><img class="alignnone76" src="assets/d03d2b8c-5e5d-486f-81f8-f5a6c1b8213d.png"/></p>
<p class="calibre2">Constructing a deep convolutional autoencoder in Keras is simple. Once again, we'll use the <kbd class="calibre13">Sequential</kbd> class in Keras to construct our model.</p>
<p class="calibre2">First, we define a new <kbd class="calibre13">Sequential</kbd> class:</p>
<pre class="calibre17">conv_autoencoder = Sequential()</pre>
<p class="calibre2">Next, let's add the first two convolutional layers, which act as the encoder in our model. There are several parameters we need to define while using the <kbd class="calibre13">Conv2D</kbd> class in Keras:</p>
<ul class="calibre11">
<li class="calibre12"><strong class="calibre1">Number of filters</strong>: Typically, we use a decreasing number of filters for each layer in the encoder. Conversely, we use an increasing number of filters for each layer in the decoder. Let's use 16 filters for the first convolutional layer in the encoder and eight filters for the second convolutional layer in the encoder. Conversely, let's use eight filters for the first convolutional layer in the decoder and 16 filters for the second convolutional layer in the decoder.</li>
<li class="calibre12"><strong class="calibre1">Filter size: </strong>As shown in the previous chapter, <a href="48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml" target="_blank" class="calibre10">Chapter 4</a>, <em class="calibre18">Cats Versus Dogs – Image Classification Using CNNs</em>, a filter size of 3 x 3 is typical for convolutional layers.</li>
<li class="calibre12"><strong class="calibre1">Padding: </strong>For autoencoders, we use a same padding. This ensures that the height and width of successive layers remains the same. This is useful because we need to ensure that the dimensions of the final output is the same as the input.</li>
</ul>
<p class="calibre2">The following code snippet adds the first two convolutional layers with the aforementioned parameters to our model:</p>
<pre class="calibre17">from keras.layers import Conv2D<br class="title-page-name"/>conv_autoencoder.add(Conv2D(filters=16, kernel_size=(3,3),<br class="title-page-name"/>                            activation='relu', padding='same', <br class="title-page-name"/>                            input_shape=(28,28,1)))<br class="title-page-name"/>conv_autoencoder.add(Conv2D(filters=8, kernel_size=(3,3),<br class="title-page-name"/>                            activation='relu', padding='same'))</pre>
<p class="calibre2">Next, we'll add the decoder layers onto our model. Just like the encoder layers, the decoder layers are also convolutional layers. The only difference is that, in the decoder layers, we use an increasing number of filters after each successive layer.</p>
<p class="calibre2">The following code snippet adds the next two convolutional layers as the decoder:</p>
<pre class="calibre17">conv_autoencoder.add(Conv2D(filters=8, kernel_size=(3,3),<br class="title-page-name"/>                            activation='relu', padding='same'))<br class="title-page-name"/>conv_autoencoder.add(Conv2D(filters=16, kernel_size=(3,3),<br class="title-page-name"/>                            activation='relu', padding='same'))</pre>
<p class="calibre2">Finally, we add the output layer to our model. The output layer should be a convolutional layer with only one filter, as we are trying to output a 28 x 28 x 1 image. The <kbd class="calibre13">Sigmoid</kbd> function is used as the activation function for the output layer.</p>
<p class="calibre2">The following code adds the final output layer:</p>
<pre class="calibre17">conv_autoencoder.add(Conv2D(filters=1, kernel_size=(3,3),<br class="title-page-name"/>                            activation='sigmoid', padding='same'))</pre>
<p class="calibre2">Let's take a look at the structure of the model to make sure that it is consistent with what was shown in the diagram earlier. We can do so by calling the <kbd class="calibre13">summary()</kbd> function:</p>
<pre class="calibre17">conv_autoencoder.summary()</pre>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img src="assets/dbaf5d0f-67fe-4d7b-a471-6805b614af25.png" class="calibre55"/></p>
<p class="calibre2">We are now ready to train our deep convolutional autoencoder. As usual, we define the training process under the <kbd class="calibre13">compile</kbd> function and call the <kbd class="calibre13">fit</kbd> function, as shown in the following code:</p>
<pre class="calibre17">conv_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')<br class="title-page-name"/>conv_autoencoder.fit(X_train_noisy.reshape(60000,28,28,1),<br class="title-page-name"/>                     X_train_reshaped.reshape(60000,28,28,1),<br class="title-page-name"/>                     epochs=10)</pre>
<p class="calibre2">Once training is done, we'll get the following output:</p>
<p class="mce-root"><img src="assets/12f069db-0d87-4491-8924-82ef829e91e8.png" class="calibre56"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Let's use the trained model on the testing set:</p>
<pre class="calibre17">output = conv_autoencoder.predict(X_test_noisy.reshape(10000,28,28,1))</pre>
<p class="calibre2">It will be interesting to see how this deep convolutional autoencoder performs on the testing set. Remember, the testing set represents images that the model has never seen before. </p>
<p class="calibre2">We plot the output and compare it with the original image and the noisy image:</p>
<pre class="calibre17">fig, ((ax1, ax2, ax3, ax4, ax5), (ax6, ax7, ax8, ax9, ax10), (ax11,ax12,ax13,ax14,ax15)) = plt.subplots(3, 5, figsize=(20,13))<br class="title-page-name"/>randomly_selected_imgs = random.sample(range(output.shape[0]),5)<br class="title-page-name"/><br class="title-page-name"/># 1st row for original images<br class="title-page-name"/>for i, ax in enumerate([ax1,ax2,ax3,ax4,ax5]):<br class="title-page-name"/>    ax.imshow(X_test_reshaped[randomly_selected_imgs[i]].reshape(28,28), <br class="title-page-name"/>              cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_ylabel("Original \n Images", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/># 2nd row for input with noise added<br class="title-page-name"/>for i, ax in enumerate([ax6,ax7,ax8,ax9,ax10]):<br class="title-page-name"/>    ax.imshow(X_test_noisy[randomly_selected_imgs[i]].reshape(28,28), <br class="title-page-name"/>              cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_ylabel("Input With \n Noise Added", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/># 3rd row for output images from our autoencoder<br class="title-page-name"/>for i, ax in enumerate([ax11,ax12,ax13,ax14,ax15]):<br class="title-page-name"/>    ax.imshow(output[randomly_selected_imgs[i]].reshape(28,28), <br class="title-page-name"/>              cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_ylabel("Denoised \n Output", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>plt.tight_layout()<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img src="assets/c61e9229-97c3-4945-afd2-a897ad928f41.png" class="calibre57"/></p>
<p class="calibre2">Isn't that amazing? The denoised output from our deep convolutional autoencoder is so good that we can barely differentiate the original images and the denoised output.</p>
<p class="calibre2">Despite the impressive results, it is important to keep in mind that the convolutional model that we used is pretty simple. The advantage of deep neural networks is that we can always increase the complexity of the model (that is, more layers and more filters per layer) and use it on more complex datasets. This ability to scale is one of the main advantages of deep neural networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Denoising documents with autoencoders</h1>
                </header>
            
            <article>
                
<p class="calibre2">So far, we have applied our denoising autoencoder on the MNIST dataset, which is a pretty simple dataset. Let's take a look now at a more complicated dataset, which better represents the challenges of denoising documents in real life.</p>
<p class="calibre2">The dataset that we will be using is provided for free by the <strong class="calibre4">University of California Irvine</strong> (<strong class="calibre4">UCI</strong>). For more information on the dataset, you can visit UCI's website at <a href="https://archive.ics.uci.edu/ml/datasets/NoisyOffice" target="_blank" class="calibre10">https://archive.ics.uci.edu/ml/datasets/NoisyOffice</a>.</p>
<p class="calibre2"/>
<p class="calibre2">The dataset can be found in the accompanying GitHub repository for this book. For more information on downloading the code and dataset for this chapter from the GitHub repository, please refer to the <em class="calibre8">Technical requirements</em> section earlier in the chapter.</p>
<p class="calibre2">The dataset consists of 216 different noisy images. The noisy images are scanned office documents that are tainted by coffee stains, wrinkled marks, and other sorts of defects that are typical in office documents. For every noisy image, a corresponding reference clean image is provided, which represents the office document in an ideal noiseless state.</p>
<p class="calibre2">Let's take a look at the dataset to have a better idea of what we are working with. The dataset is located at the following folder:</p>
<pre class="calibre17">noisy_imgs_path = 'Noisy_Documents/noisy/'<br class="title-page-name"/>clean_imgs_path = 'Noisy_Documents/clean/'</pre>
<p class="calibre2">The <kbd class="calibre13">Noisy_Documents</kbd> folder contains two subfolders (<kbd class="calibre13">noisy</kbd> and <kbd class="calibre13">clean</kbd>), which contains the noisy and clean images, respectively.</p>
<p class="calibre2">To load the <kbd class="calibre13">.png</kbd> images into Python, we can use the <kbd class="calibre13">load_img</kbd> function provided by Keras. To convert the loaded images into a <kbd class="calibre13">numpy</kbd> array, we use the <kbd class="calibre13">img_to_array</kbd> function in Keras.</p>
<p class="calibre2">The following code imports the noisy <kbd class="calibre13">.png</kbd> images in the <kbd class="calibre13">/Noisy_Documents/noisy/</kbd> folder into a <kbd class="calibre13">numpy</kbd> array:</p>
<pre class="calibre17">import os<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>from keras.preprocessing.image import load_img, img_to_array<br class="title-page-name"/><br class="title-page-name"/>X_train_noisy = []<br class="title-page-name"/><br class="title-page-name"/>for file in sorted(os.listdir(noisy_imgs_path)):<br class="title-page-name"/>    img = load_img(noisy_imgs_path+file, color_mode='grayscale', <br class="title-page-name"/>                   target_size=(420,540))<br class="title-page-name"/>    img = img_to_array(img).astype('float32')/255<br class="title-page-name"/>    X_train_noisy.append(img)<br class="title-page-name"/><br class="title-page-name"/># convert to numpy array<br class="title-page-name"/>X_train_noisy = np.array(X_train_noisy) </pre>
<p class="calibre2">To verify that our images are loaded properly into the <kbd class="calibre13">numpy</kbd> array, let's print the dimensions of the array:</p>
<pre class="calibre17">print(X_train_noisy.shape)</pre>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img src="assets/72d59ff5-918d-4594-90f7-acc0d445a78a.png" class="calibre58"/></p>
<p class="calibre2">We can see that there are <span class="calibre5">216</span> images in the array, each with dimensions 420 x 540 x 1 (width x height x number of channels for each image).</p>
<p class="calibre2">Let's do the same for the clean images. The following code imports the clean<span class="calibre5"> </span><kbd class="calibre13">.png</kbd><span class="calibre5"> </span>images in the<span class="calibre5"> </span><kbd class="calibre13">/Noisy_Documents/clean/</kbd><span class="calibre5"> </span>folder into a<span class="calibre5"> </span><kbd class="calibre13">numpy</kbd><span class="calibre5"> </span>array:</p>
<pre class="calibre17">X_train_clean = []<br class="title-page-name"/><br class="title-page-name"/>for file in sorted(os.listdir(clean_imgs_path)):<br class="title-page-name"/>    img = load_img(clean_imgs_path+file, color_mode='grayscale', <br class="title-page-name"/>                   target_size=(420,540))<br class="title-page-name"/>    img = img_to_array(img).astype('float32')/255<br class="title-page-name"/>    X_train_clean.append(img) <br class="title-page-name"/><br class="title-page-name"/># convert to numpy array<br class="title-page-name"/>X_train_clean = np.array(X_train_clean)</pre>
<p class="calibre2">Let's display the loaded images to have a better idea of the kind of images we are working with. The following code randomly selects <kbd class="calibre13">3</kbd> images and plots them, as shown in the following code:</p>
<pre class="calibre17">import random<br class="title-page-name"/>fig, ((ax1,ax2), (ax3,ax4), <br class="title-page-name"/>      (ax5,ax6)) = plt.subplots(3, 2, figsize=(10,12))<br class="title-page-name"/><br class="title-page-name"/>randomly_selected_imgs = random.sample(range(X_train_noisy.shape[0]),3)<br class="title-page-name"/><br class="title-page-name"/># plot noisy images on the left<br class="title-page-name"/>for i, ax in enumerate([ax1,ax3,ax5]):<br class="title-page-name"/>    ax.imshow(X_train_noisy[i].reshape(420,540), cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_title("Noisy Images", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/># plot clean images on the right<br class="title-page-name"/>for i, ax in enumerate([ax2,ax4,ax6]):<br class="title-page-name"/>    ax.imshow(X_train_clean[i].reshape(420,540), cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_title("Clean Images", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>plt.tight_layout()<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We get the output as shown in the following screenshot:</p>
<p class="mce-root"><img src="assets/b9cc19ed-e4e3-4d64-b155-b223743a8158.png" class="calibre59"/></p>
<p class="calibre2"/>
<p class="calibre2">We can see that the kind of noise in this dataset is markedly different from what we saw in the MNIST dataset. The noise in this dataset are random artifacts that appear throughout the image. Our autoencoder model needs to have a strong understanding of signal versus noise in order to successfully denoise this dataset.</p>
<p class="calibre2">Before we proceed to train our model, let's split our dataset into a training and testing set, as shown in the following code:</p>
<pre class="calibre17"># use the first 20 noisy images as testing images<br class="title-page-name"/>X_test_noisy = X_train_noisy[0:20,]<br class="title-page-name"/>X_train_noisy = X_train_noisy[21:,]<br class="title-page-name"/><br class="title-page-name"/># use the first 20 clean images as testing images<br class="title-page-name"/>X_test_clean = X_train_clean[0:20,]<br class="title-page-name"/>X_train_clean = X_train_clean[21:,]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic convolutional autoencoder</h1>
                </header>
            
            <article>
                
<p class="calibre2">We're now ready to tackle the problem. Let's start with a basic model to see how far we can go with it.</p>
<p class="calibre2">As always, we define a new <kbd class="calibre13">Sequential</kbd> class:</p>
<pre class="calibre17">basic_conv_autoencoder = Sequential()</pre>
<p class="calibre2">Next, we add a single convolutional layer as our encoder layer:</p>
<pre class="calibre17">basic_conv_autoencoder.add(Conv2D(filters=8, kernel_size=(3,3),<br class="title-page-name"/>                                  activation='relu', padding='same', <br class="title-page-name"/>                                  input_shape=(420,540,1)))</pre>
<p class="calibre2">We add a single convolutional layer as our decoder layer:</p>
<pre class="calibre17">basic_conv_autoencoder.add(Conv2D(filters=8, kernel_size=(3,3), <br class="title-page-name"/>                                  activation='relu', padding='same'))</pre>
<p class="calibre2">Finally, we add an output layer:</p>
<pre class="calibre17">basic_conv_autoencoder.add(Conv2D(filters=1, kernel_size=(3,3), <br class="title-page-name"/>                                  activation='sigmoid', padding='same'))</pre>
<p class="calibre2">Let's check the structure of the model:</p>
<pre class="calibre17">basic_conv_autoencoder.summary()</pre>
<p class="calibre2">We get the output as shown in the following screenshot:</p>
<p class="mce-root"><img src="assets/77fcef33-83cf-442d-8617-b55c2142bb42.png" class="calibre60"/></p>
<p class="calibre2">Here's the code to train our basic convolutional autoencoder:</p>
<pre class="calibre17">basic_conv_autoencoder.compile(optimizer='adam', <br class="title-page-name"/>                               loss='binary_crossentropy')<br class="title-page-name"/>basic_conv_autoencoder.fit(X_train_noisy, X_train_clean, epochs=10)</pre>
<p class="calibre2">Once the training is done, we apply our model on the testing set:</p>
<pre class="calibre17">output = basic_conv_autoencoder.predict(X_test_noisy)</pre>
<p class="calibre2">Let's plot the output and see what kind of results we got. The following code plots the original noisy images in the left column, the original clean images in the middle column, and the denoised image output from our model in the right column:</p>
<pre class="calibre17">fig, ((ax1,ax2,ax3),(ax4,ax5,ax6)) = plt.subplots(2,3, figsize=(20,10))<br class="title-page-name"/><br class="title-page-name"/>randomly_selected_imgs = random.sample(range(X_test_noisy.shape[0]),2)<br class="title-page-name"/><br class="title-page-name"/>for i, ax in enumerate([ax1, ax4]):<br class="title-page-name"/>    idx = randomly_selected_imgs[i]<br class="title-page-name"/>    ax.imshow(X_test_noisy[idx].reshape(420,540), cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_title("Noisy Images", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>for i, ax in enumerate([ax2, ax5]):<br class="title-page-name"/>    idx = randomly_selected_imgs[i]<br class="title-page-name"/>    ax.imshow(X_test_clean[idx].reshape(420,540), cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_title("Clean Images", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>for i, ax in enumerate([ax3, ax6]):<br class="title-page-name"/>    idx = randomly_selected_imgs[i]<br class="title-page-name"/>    ax.imshow(output[idx].reshape(420,540), cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_title("Output Denoised Images", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>plt.tight_layout()<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We get the output as shown in the following screenshot:</p>
<p class="mce-root"><img src="assets/a3bd8afd-8513-4ca9-88ed-9f890c713566.png" class="calibre61"/></p>
<p class="calibre2">Well, our model can certainly do a better job. The denoised images tend to have a gray background rather than a white background in the true <kbd class="calibre13">Clean Images</kbd>. The model also does a poor job at removing the coffee stains from the <kbd class="calibre13">Noisy Images</kbd>. Furthermore, the words in the denoised images are faint, showing that the model struggles at this task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep convolutional autoencoder</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's try denoising the images with a deeper model and more filters in each convolutional layer. </p>
<p class="calibre2">We start by defining a new <kbd class="calibre13">Sequential</kbd> class:</p>
<pre class="calibre17">conv_autoencoder = Sequential()</pre>
<p class="calibre2">Next, we add three convolutional layers as our encoder, with <kbd class="calibre13">32</kbd>, <kbd class="calibre13">16</kbd>, and <kbd class="calibre13">8</kbd> filters:</p>
<pre class="calibre17">conv_autoencoder.add(Conv2D(filters=32, kernel_size=(3,3),<br class="title-page-name"/>                            input_shape=(420,540,1), <br class="title-page-name"/>                            activation='relu', padding='same'))<br class="title-page-name"/>conv_autoencoder.add(Conv2D(filters=16, kernel_size=(3,3),<br class="title-page-name"/>                            activation='relu', padding='same'))<br class="title-page-name"/>conv_autoencoder.add(Conv2D(filters=8, kernel_size=(3,3),<br class="title-page-name"/>                            activation='relu', padding='same'))</pre>
<p class="calibre2">Similarly for the decoder, we add three convolutional layers with <kbd class="calibre13">8</kbd>, <kbd class="calibre13">16</kbd>, and <kbd class="calibre13">32</kbd> filters:</p>
<pre class="calibre17">conv_autoencoder.add(Conv2D(filters=8, kernel_size=(3,3), <br class="title-page-name"/>                            activation='relu', padding='same'))<br class="title-page-name"/>conv_autoencoder.add(Conv2D(filters=16, kernel_size=(3,3), <br class="title-page-name"/>                            activation='relu', padding='same'))<br class="title-page-name"/>conv_autoencoder.add(Conv2D(filters=32, kernel_size=(3,3), <br class="title-page-name"/>                            activation='relu', padding='same'))</pre>
<p class="calibre2">Finally, we add an output layer:</p>
<pre class="calibre17">conv_autoencoder.add(Conv2D(filters=1, kernel_size=(3,3), <br class="title-page-name"/>                            activation='sigmoid', padding='same'))</pre>
<p class="calibre2">Let's check the structure of our model:</p>
<pre class="calibre17">conv_autoencoder.summary()</pre>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img src="assets/4451482c-aee1-4535-ba4c-85239ccd27fb.png" class="calibre62"/></p>
<p class="calibre2">From the preceding output, we can see that there are <span class="calibre5">12,785</span> parameters in our model, which is approximately 17 times more than the basic model we used in the previous section.</p>
<p class="calibre2">Let's train the model and apply it on the testing images:</p>
<div class="packtinfobox"><span>Caution</span><br class="title-page-name"/>
The following code may take some time to run if you are not using Keras with a GPU. If the model is taking too long to train, you may reduce the number of filters in each convolutional layer in the model.</div>
<pre class="calibre17">conv_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')<br class="title-page-name"/>conv_autoencoder.fit(X_train_noisy, X_train_clean, epochs=10)<br class="title-page-name"/><br class="title-page-name"/>output = conv_autoencoder.predict(X_test_noisy)</pre>
<p class="calibre2">Finally, we <span class="calibre5">plot the output to see the kind of results we get. The following code plots the original noisy images in the left column, the original clean images in the middle column, and the denoised image output from our model in the right column:</span></p>
<pre class="calibre17">fig, ((ax1,ax2,ax3),(ax4,ax5,ax6)) = plt.subplots(2,3, figsize=(20,10))<br class="title-page-name"/><br class="title-page-name"/>randomly_selected_imgs = random.sample(range(X_test_noisy.shape[0]),2)<br class="title-page-name"/><br class="title-page-name"/>for i, ax in enumerate([ax1, ax4]):<br class="title-page-name"/>    idx = randomly_selected_imgs[i]<br class="title-page-name"/>    ax.imshow(X_test_noisy[idx].reshape(420,540), cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_title("Noisy Images", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>for i, ax in enumerate([ax2, ax5]):<br class="title-page-name"/>    idx = randomly_selected_imgs[i]<br class="title-page-name"/>    ax.imshow(X_test_clean[idx].reshape(420,540), cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_title("Clean Images", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>for i, ax in enumerate([ax3, ax6]):<br class="title-page-name"/>    idx = randomly_selected_imgs[i]<br class="title-page-name"/>    ax.imshow(output[idx].reshape(420,540), cmap='gray')<br class="title-page-name"/>    if i == 0:<br class="title-page-name"/>        ax.set_title("Output Denoised Images", size=30)<br class="title-page-name"/>    ax.grid(False)<br class="title-page-name"/>    ax.set_xticks([])<br class="title-page-name"/>    ax.set_yticks([])<br class="title-page-name"/><br class="title-page-name"/>plt.tight_layout()<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">We get the following output:</p>
<p class="mce-root"><img src="assets/03364997-57db-4feb-b88c-3ce300cf7b0d.png" class="calibre63"/></p>
<p class="calibre2">The result looks amazing! In fact, the output denoised images look so good that we can barely differentiate them from the true clean images. We can see that the coffee stain has been almost entirely removed and the noise from the crumpled paper is non-existent in the denoised image. Furthermore, the words in the denoised images look sharp and clear, and we can easily read the words in the denoised images.</p>
<p class="calibre2">This dataset truly demonstrates the power of autoencoders. By adding on additional complexity in the form of deeper convolutional layers and more filters, the model is able to differentiate the signal from the noise, allowing it to successfully denoise images that are heavily corrupted.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we looked at autoencoders, a class of neural networks that learn the latent representation of input images. We saw that all autoencoders have an encoder and decoder component. The role of the encoder is to encode the input to a learned, compressed representation and the role of the decoder is to reconstruct the original input using the compressed representation.</p>
<p class="calibre2">We first looked at autoencoders for image compression. By training an autoencoder with identical input and output, the autoencoder learns the most salient features of the input. Using MNIST images, we constructed an autoencoder with a 24.5 times compression rate. Using this learned 24.5x compressed representation, the autoencoder is able to <span class="calibre5">successfully </span>reconstruct the original input.</p>
<p class="calibre2">Next, we looked at denoising autoencoders. By training an autoencoder with noisy images as input and clean images as output, the autoencoder is able to pick out the signal from the noise, and is able to successfully denoise the noisy image. We trained a deep convolutional autoencoder, and the autoencoder was able to successfully denoise documents with coffee stains and other sorts of image corruptions. The results were impressive, with the autoencoder removing almost all of the noise in the noisy documents, producing an output that is almost identical to the true clean images.</p>
<p class="calibre2">In the next chapter, <a href="21ef7df7-5976-4e0d-bec5-d736ec571d94.xhtml" target="_blank" class="calibre10">Chapter 6</a>, <em class="calibre8">Sentiment Analysis of Movie Reviews Using LSTM</em> we'll use a <strong class="calibre4">long short-term memory</strong> (<strong class="calibre4">LSTM</strong>) neural network to predict the sentiment of movie reviews.</p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol class="calibre14">
<li class="calibre12">How are autoencoders different from a conventional feed forward neural network?</li>
</ol>
<p class="calibre26">Autoencoders are neural networks that learn a compressed representation of the input, known as the latent representation. They are different from conventional feed forward neural networks because their structure consists of an encoder and a decoder component, which is not present in CNNs. </p>
<ol start="2" class="calibre14">
<li class="calibre12">What happens when the latent representation of the autoencoder is too small? </li>
</ol>
<p class="calibre26">The size of the latent representation should be sufficiently <em class="calibre8">small</em> enough to represent a compressed representation of the input, and also be sufficiently <em class="calibre8">large</em> enough for the decoder to reconstruct the original image without too much loss.</p>
<ol start="3" class="calibre14">
<li class="calibre12">What are the input and output when training a denoising autoencoder?</li>
</ol>
<p class="calibre26">The input to a denoising autoencoder should be a noisy image and the output should be a reference clean image. During the training process, the autoencoder learns that the output should not contain any noise (through the <kbd class="calibre13">loss</kbd> function), and the latent representation of the autoencoder should only contain the signals (that is, non-noise elements)</p>
<ol start="4" class="calibre14">
<li class="calibre12">What are some of the ways we can improve the complexity of denoising autoencoders?</li>
</ol>
<p class="calibre26">For denoising autoencoders, convolutional layers always work better than dense layers, just as CNNs work better than conventional feed forward neural networks for image classification tasks. We can also improve the complexity of our model by building a deeper network with more layers, and by using more filters in each convolutional layer.</p>


            </article>

            
        </section>
    </body></html>