["```py\npip install tox\n```", "```py\nset MARTA_API_KEY=<your_api_key_here>\n```", "```py\nexport MARTA_API_KEY=<your_api_key_here>\n```", "```py\nfrom marta.api import get_buses, get_trains\n\n# To obtain list of all buses\nall_buses = get_buses()\n\n# To obtain a list of buses by route\nbuses_route = get_buses(route=1)\n\n# To obtain list of all trains\ntrains = get_trains()\n\n# To obtain list of trains specified by line\ntrains_red = get_trains(line='red')\n\n# To obtain list of trains by station\ntrains_station = get_trains(station='Midtown Station')\n\n# To obtain list of trains by destination\ntrains_doraville = get_trains(station='Doraville')\n\n# To obtain list of trains by line, station, and destination\ntrains_all = get_trains(line='blue', \n            station='Five Points Station', \n            destination='Indian Creek')\n```", "```py\nfrom pyspark.ml.classification import LogisticRegression as LR\nfrom pyspark.ml.feature import RegexTokenizer as RT\nfrom pyspark.ml.feature import StopWordsRemover as SWR\nfrom pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import col\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n        .appName(\"Crime Category Prediction\") \\\n        .config(\"spark.executor.memory\", \"70g\") \\\n        .config(\"spark.driver.memory\", \"50g\") \\\n        .config(\"spark.memory.offHeap.enabled\",True) \\\n        .config(\"spark.memory.offHeap.size\",\"16g\") \\\n        .getOrCreate()\n```", "```py\ndata = spark.read.format(\"csv\"). \\\n        options(header=\"true\", inferschema=\"true\"). \\\n        load(\"sf_crime_dataset.csv\")\n\ndata.columns\n```", "```py\ndrop_data = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\ndata = data.select([column for column in data.columns if column not in drop_data])\n\ndata.show(5)\n```", "```py\n# regular expression tokenizer\nre_Tokenizer = RT(inputCol=\"Descript\", \n            outputCol=\"words\", pattern=\"\\\\W\")    \n\n# stop words\nstop_words = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \nstop_words_remover = SWR(inputCol=\"words\", \n            outputCol=\"filtered\").setStopWords(stop_words)\n\n# bag of words count\ncount_vectors = CountVectorizer(inputCol=\"filtered\",\n         outputCol=\"features\", vocabSize=10000, minDF=5)\n\n#One hot encoding the label\nlabel_string_Idx = StringIndexer(inputCol = \"Category\", \n                outputCol = \"label\")\n\n# Create the pipeline\npipeline = Pipeline(stages=[re_Tokenizer, stop_words_remover,\n             count_vectors, label_string_Idx])\n\n# Fit the pipeline to data.\npipeline_fit = pipeline.fit(data)\ndataset = pipeline_fit.transform(data)\n\ndataset.show(5)\n```", "```py\n# Split the data randomly into training and test data sets.\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nprint(\"Training Dataset Size: \" + str(trainingData.count()))\nprint(\"Test Dataset Size: \" + str(testData.count()))\n```", "```py\n# Build the model\nlogistic_regrssor = LR(maxIter=20, \n                regParam=0.3, elasticNetParam=0)\n# Train model with Training Data\nmodel = logistic_regrssor.fit(trainingData)\n\n# Make predictions on Test Data\npredictions = model.transform(testData)\n\n# evaluate the model on test data set\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)\n```"]