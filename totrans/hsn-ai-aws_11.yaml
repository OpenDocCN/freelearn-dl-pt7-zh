- en: Creating Machine Learning Inference Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data transformation logic that is used to process data for model training
    is the same as the logic that's used to prepare data for obtaining inferences.
    It is redundant to repeat the same logic twice.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this chapter is to walk you through how SageMaker and other AWS
    services can be employed to create **machine learning** (**ML**) pipelines that
    can process big data, train algorithms, deploy trained models, and run inferences,
    all while using the same data processing logic for model training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the architecture of the inference pipeline in SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating features using Amazon Glue and SparkML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying topics by training NTM in SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running online as opposed to batch inference in SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at the technical requirements for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To illustrate the concepts that we will cover in this chapter, we will use
    the [ABC Millions Headlines](https://www.kaggle.com/therohk/million-headlines)
    dataset. This dataset contains approximately a million news headlines. In the
    [github](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/tree/master/Ch10_InferencePipeline_SageMaker/million-headlines-data)
    repository associated with this chapter, you should find the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[abcnews-date-text.zip](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/blob/master/Ch10_InferencePipeline_SageMaker/million-headlines-data/abcnews-date-text.zip):
    The input dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[libraries-mleap](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/tree/master/Ch10_InferencePipeline_SageMaker/million-headlines-data/libraries-mleap):
    MLeap libraries (includes a `.jar` file and a Python wrapper for the `.jar`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin by looking at the architecture of an inference pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the architecture of the inference pipeline in SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three major components of the inference pipeline we are building:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing (from *Step 1*) and inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the architectural diagram—the steps we are going to walk through
    are applicable to big data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b5a0f5c-f99a-4686-a9bb-f038157b4512.png)'
  prefs: []
  type: TYPE_IMG
- en: In the first step of the pipeline, we execute data processing logic on Apache
    Spark via AWS Glue. The Glue service is called from a SageMaker Notebook instance.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Glue is a fully managed, serverless **Extract, Transform, and Load**
    (**ETL**) service that's used to wrangle big data. ETL jobs are run on an Apache
    Spark environment where Glue provisions, the configuration and scale the resources
    that are required to run the jobs.
  prefs: []
  type: TYPE_NORMAL
- en: The data processing logic, in our case, includes creating tokens/words from
    each of the news headlines, removing stop words, and counting the frequency of
    each of the words in a given headline. The ETL logic is serialized into an MLeap
    bundle, which can be used at the time of inference for data processing. Both the
    serialized SparkML model and processed input data are stored in an S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: MLeap is an open source Spark package that's designed to serialize Spark-trained
    transformers. Serialized models are used to transform data into the desired format.
  prefs: []
  type: TYPE_NORMAL
- en: In the second step, the **neural topic model** (**NTM**) algorithm is trained
    on the processed data to discover topics.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, both the SparkML and trained NTM models are used to create a pipeline
    model, which is used to execute the models in the specified sequence. SparkML
    serves a docker container, and the NTM docker container is provisioned as an endpoint
    for real-time model predictions. The same pipeline model can be used to run inferences
    in batch mode, that is, score multiple news headlines in one go, discovering topics
    for each of them.
  prefs: []
  type: TYPE_NORMAL
- en: It is now time to delve globally into *Step 1—*how to invoke Amazon Glue from
    a SageMaker Notebook instance for big data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Creating features using Amazon Glue and SparkML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To create features in a big data environment, we will use PySpark to write data
    preprocessing logic. This logic will be part of the Python `abcheadlines_processing.py`
    file. Before we review the logic, we need to walk through some prerequisites.
  prefs: []
  type: TYPE_NORMAL
- en: Walking through the prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Provide SageMaker Execution Role access to the Amazon Glue service, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b28c721a-2aed-4dfe-9639-7147a6e1b999.png)'
  prefs: []
  type: TYPE_IMG
- en: Obtaining a SageMaker Execution Role by running the get_execution_role() method
    of the SageMaker session object
  prefs: []
  type: TYPE_NORMAL
- en: 'On the IAM Dashboard, click on Roles on the left navigation pane and search
    for this role. Click on the Target Role to navigate to its Summary page. Click
    on the Trust Relationships tab to add `AWS Glue` as an additional trusted entity.
    Click on Edit trust relationship to add the following entry to `"Service" key:
    "glue.amazonaws.com"`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upload MLeap binaries to the appropriate location on the S3 bucket, as follows.
    The binaries can be found in the source code for this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will use the `upload_data()` method of the SageMaker Session object to upload
    MLeap binaries to the appropriate location on the S3 bucket. We will need the
    `MLeap` Java package and the Python wrapper, MLeap, to serialize SparkML models.
    Similarly, we will upload the input data, that is, [abcnews-date-text.zip](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services/blob/master/Ch10_InferencePipeline_SageMaker/million-headlines-data/abcnews-date-text.zip),
    to the relevant location on the S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we'll review the data preprocessing logic in `abcheadlines_processing.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing data using PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following data preprocessing logic is executed on a Spark cluster. Let''s
    go through the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by gathering arguments sent by the SageMaker Notebook instance,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will use the `getResolvedOptions()` utility function from the AWS Glue library
    to read all the arguments that were sent by the SageMaker notebook instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will read the news headlines, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We use `spark`, which is the active SparkSession, to read the `.csv` file that
    contains the relevant news headlines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we retrieve 10% of the headlines and define the data transformations.
    We can process all 1,000,000 headlines using distributed computing from Apache
    Spark. We will, however, illustrate the concepts behind using AWS Glue from a
    SageMaker notebook instance by using a sample of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`hdl_fil_cnt` is 10% of the total number of headlines. `abcnewsdf` contains
    around 100,000 headlines. We use `Tokenizer`, `StopWordsRemover`, `CountVectorizer`,
    and the **inverse document frequency** (**IDF**) transformer and estimator objects
    from `pyspark.ml.feature` to transform the headline text, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, `Tokenizer` transforms the headline text into a list of words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, `StopWordsTokenizer` removes stop words from the list of words produced
    by `Tokenizer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Third, `CountVectorizer` takes the output from the previous step to calculate
    word frequency.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, IDF, an estimator, computes the inverse document frequency factor for
    each of the words (IDF is given by ![](img/f6c66ff6-087f-4596-b312-1ebda4c02ce7.png),
    where ![](img/cf4cbbb2-b784-479f-83eb-0dfe0164b3eb.png) is the term frequency
    of term i in headline j, N is the total number of headlines, and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f6f4c888-b901-412d-9772-29ec0561f6cf.png) is the number of headlines
    containing term i). Words that are unique to a headline are much more important
    than those that appear frequently in other headlines.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For more information on `Estimator` and `Transformer` objects in Spark ML, please
    refer to Spark's documentation at [https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will stitch all the transformer and estimator stages together into
    a pipeline and transform the headlines into feature vectors. The width of a feature
    vector is 200, as defined by `CountVectorizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we use the `Pipeline` object from `pyspark.ml` to tie
    data transformations together. We also call the `fit()` method on the `Pipeline`
    object, `news_pl`, to create `PipelineModel`. `news_pl_fit` will have learned
    the IDF factor for each of the words in the news headlines. When the `transform()`
    method is invoked on `news_pl_fit`, the input headlines are transformed into feature
    vectors. Each headline will be represented by a vector that's 200 in length. `CountVectorizer`
    picks the top 200 words ordered by word frequency across all the headlines. Note
    that the processed headlines will be stored in the `features` column, as indicated
    by the `outputCol` parameter of the IDF Estimator stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we save the resulting feature vectors in `.csv` format, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To save the processed headlines in `.csv` format, the `features` column needs
    to be in a simple string format. The CSV file format does not support storing
    arrays or lists in a column. We will define a user-defined function, `get_str`,
    to convert a feature vector into a string of comma-separated tf-idf numbers. Please
    look at the source code associated with this chapter for additional details. The
    resulting `news_save` DataFrame will be saved to a designated location on the
    S3 bucket as a `.csv` file. The following screenshot shows the format of the `.csv`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c3a22483-4c2e-4efc-b3f4-a1c75d0c4a6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, we will also save the vocabulary into a separate text file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now it''s time to serialize `news_pl_fit` and push it to an S3 bucket, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we use the `serializetoBundle()` method of the
    `SimpleSparkSerializer` object from the MLeap `pyspark` library to serialize `news_pl_fit`.
    We will convert the format of the serialized model from a `.zip` into a `tar.gz`
    before uploading it to the S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's walk through the process of running `abcheadlines_processing.py` through
    an AWS Glue job.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AWS Glue job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will create a Glue job using `Boto3`, which is the AWS SDK for Python.
    This SDK allows Python developers to create, configure, and manage AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a Glue job by providing the following specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we call the `create_job()` method of the AWS Glue
    client by passing in the job name, description, and role. We also specify how
    many concurrent we want to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at the command that''s sent by Glue to the Spark cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we define the command name and location of the Python
    script containing the data preprocessing logic, that is, `abcheadlines_processing.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s look at which binaries need to be configured in order to serialize
    SparkML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we define a default language so that we can preprocess
    big data, the locations of the MLeap `.jar` file, and the Python wrapper of MLeap.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have created the Glue job, let''s run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We invoke the `start_job_run()` method of the AWS Glue client by passing the
    name of the Glue job we created earlier, along with the arguments that define
    the input and location locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get the status of the Glue job as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3183a01-8ff2-4ff4-af3b-6179cebff7d9.png)'
  prefs: []
  type: TYPE_IMG
- en: We invoke the `get_job_run()` method of the AWS Glue client and pass in the
    name of the Glue job whose status we want to check.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the status of the AWS Glue job, you can also navigate to the AWS Glue
    service from the Services menu. Under the ETL section in the left-hand navigation
    menu, click on Jobs. Select a job name to look at the details of that Glue job:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95836dc1-f89a-4488-a734-4475aeef0e0e.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we will uncover topics that are in the ABC News Headlines dataset by fitting
    NTM to it.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying topics by training NTM in SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perform the following steps to train the NTM model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the processed ABC News Headlines dataset from the output folder on the
    designated S3 bucket, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We use the `read_csv()` function from the pandas library to read the processed
    news headlines into a DataFrame. The DataFrame contains 110,365 headlines and
    200 words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we split the dataset into three parts—train, validation, and test—as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we take 80% of the data for training, 10% for validation,
    and the remaining 10% for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Upload the train, validation, and test datasets to the appropriate location
    on the S3 bucket. We also need to upload the vocabulary text file that was created
    by the AWS Glue job to the auxiliary path. SageMaker's built-in algorithm uses
    the auxiliary path to provide additional information while training. In this case,
    our vocabulary contains 200 words. However, the feature vector from the previous
    section does not know the word name; it does, however, know the word index. Therefore,
    after the NTM is trained, so that SageMaker can output significant words that
    correspond to a topic, it needs a vocabulary text file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to define the NTM Estimator object from SageMaker by passing
    the number and type of compute instances and the Docker NTM image to a SageMaker
    session. Estimators are learning models that are suitable for the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now we are ready to train the NTM algorithm, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: To train the NTM algorithm, we use the `fit()` method of the `ntm` Estimator
    object by passing the location of the train, test, and auxiliary datasets. Since
    we have a whole new chapter, [Chapter 9](0537c904-c763-496c-bfd2-f18042dcd0a2.xhtml),
    *Discovering Topics in Text Collection*, dedicated to understanding how the NTM
    algorithm works, we will save the model training details for later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the model''s output—we''ve configured the model so that it
    retrieves five topics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There are two numbers at the beginning of each topic—kld and recons. We will
    go into each of these losses in the next chapter. But for now, understand that
    the first fraction reflects the loss in creating embedded news headlines, while
    the second fraction reflects the reconstruction loss (that is, creating headlines
    from embeddings). The smaller the losses, the better the topic clusters.
  prefs: []
  type: TYPE_NORMAL
- en: For each of the topics we've discovered, we manually label the topics based
    on the word groupings.
  prefs: []
  type: TYPE_NORMAL
- en: Now we are ready to look at inference patterns. Inferences can be obtained both
    in real-time and batch mode.
  prefs: []
  type: TYPE_NORMAL
- en: Running online versus batch inferences in SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In real-world production scenarios, we typically come across two situations:'
  prefs: []
  type: TYPE_NORMAL
- en: Running inferences in real-time or in online mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running inferences in batch or in offline mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To illustrate this, in the case of using a recommender system as part of a web/mobile
    app, real-time inferences can be used when you want to personalize item suggestions
    based on in-app activity. The in-app activity, such as items you browsed, items
    left in your shopping cart and not checked out, and so on, can be sent as input
    to an online recommender system.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you want to present item suggestions to your customers
    even before they engage with your web/mobile app, then you can send data related
    to their historical consumption behavior to an offline recommender system so that
    you can obtain item suggestions for your entire customer base in one shot.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at how real-time predictions are run.
  prefs: []
  type: TYPE_NORMAL
- en: Creating real-time predictions through an inference pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build a pipeline where we reuse the serialized SparkML
    model for data preprocessing and employ a trained NTM model to derive topics from
    preprocessed headlines. SageMaker's Python SDK provides classes such as `Model`,
    `SparkMLModel`, and `PipelineModel` to create an inference pipeline that can be
    used to conduct feature processing and then score processed data using the trained
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s walk through the steps for creating an endpoint that can be used for
    real-time predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `Model` from the NTM training job (the one we created in the previous
    section), as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create the `Model` object that's present in the `sagemaker.model` module.
    We pass in the location of the trained NTM model and the Docker registry path
    of the NTM inference image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a SparkML `Model` representing the learned data preprocessing logic,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We define `sparkml_data` as the location of the serialized `PipelineModel` from
    the `pyspark.ml` package. Remember that `PipelineModel` contains three transformers
    (`Tokenizer`, `StopWordsRemover`, and `CountVectorizer`) and one estimator (IDF)
    from the data preprocessing we did in the previous section. Then, we create a
    `SparkMLModel` object, `sparkml_model`, by passing the location of the trained
    Spark `PipelineModel` and schema of input data for inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `PipelineModel`, encompassing and sequencing the `sparkml_model` (data
    preprocessing) and `ntm_model` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We create a `PipelineModel` object from the `sagemaker.pipeline` module by passing
    in the model name, the `sagemaker` execution role, and the sequence of models
    we want to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to deploy the `PipelineModel`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We will invoke the `deploy()` method on `sm_model` to deploy the model as an
    endpoint. We pass the number and type of instances we need to host the endpoint,
    along with the endpoint's name, to the deployed model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to pass a sample headline from the test dataset to the newly
    created endpoint. Let''s walk through the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a `RealTimePredictor` object from the `sagemaker.predictor`
    module, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We define the `RealTimePredictor` object by passing the name of the endpoint
    created previously, the current SageMaker session, the serializer (this defines
    how the input data is encoded when transmitting it to an endpoint), and the request
    and response content types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we invoke the `predict()` method of the `RealTimePredictor` object, `predictor`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We call the `predict()` method of `predictor`, initialized as the `RealTimePredictor`
    object, by passing a sample headline from the test dataset as part of the `json`
    payload, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The payload variable contains two keys, `schema` and `data`. The `schema` key
    contains the input and output structure of `SparkMLModel`, while the `data` key
    contains a sample headline whose topics we want to discover. If we choose to override
    the SageMaker `sparkml` schema we specified while initializing `SparkMLModel`,
    we can pass the new schema. The following is the output from scoring a news headline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the headline has three prominent topics: International Politics
    and Conflict, followed by Funding/Expenses related challenges and Law Enforcement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A little bit of context—Lisa Scaffidi was the Lord Mayor of Perth, Western
    Australia. She was charged with inappropriate use of her position—failure to declare
    gifts and travel worth tens of thousands of dollars. Therefore, this headline
    aptly has a mixture of topics: International Politics and Conflict (51%), followed
    by Funding/Expenses-related challenges (22%) and then by Law Enforcement (17%).'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at inferring topics for a batch of headlines.
  prefs: []
  type: TYPE_NORMAL
- en: Creating batch predictions through an inference pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will turn our attention from real-time predictions to batch
    predictions. To address the need to deploy trained models in offline mode, SageMaker
    offers Batch Transform. Batch Transform is a newly released high-performance and
    throughput feature where inferences can be obtained for the entire dataset. Both
    the input and output data is stored in an S3 bucket. The *Batch Transform* service
    manages the necessary compute resources to score the input data, given the trained
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This following diagram shows how the *Batch Transform* service works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8f0276a-427e-4ad3-b79b-f3baa6febd93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding diagram, we can see the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The Batch Transform service ingests large volumes of input data (from the S3
    bucket) through an agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The role of the Batch Transform agent is to orchestrate communication between
    the trained model and the S3 bucket, where input and output data is stored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the request data is available to the agent, it sends it to the trained
    model, which transforms news headlines and generates topics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The inferences or topics that are produced are deposited back in the designated
    S3 bucket by the intermediate agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s go through the steps for running a Batch Transform job:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the path to the S3 bucket where the input and output data is stored,
    along with the name of the `PipelineModel` we created in the previous section.
    The name of the `PipelineModel` can be obtained either programmatically or through
    the AWS console (navigate to the SageMaker service on the left navigation pane;
    then, under Inference, click on Models).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a `Transformer` object from the `sagemaker.transformer` module, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, we define the compute resources that are required to run the pipeline
    model, for example, the EC2 instance type and number. Then, we define and assemble
    a strategy, that is, how to batch records (single or multiple records) and how
    to assemble the output. The Current SageMaker session and output content type
    defined by `accept` are also provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Invoke the `transform()` method of the transformer object we created in the
    previous step, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the path to the input data, the name of the Batch Transform job,
    the input content type, and how the input records are separated (news headlines
    are separated by line, in this case). Next, we wait for the batch inference to
    be run on all the input data. The following is an excerpt from the output that
    was produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d2d5be4-9609-4e0d-9862-b21a87aff883.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Remember that we have uncovered five topics: International Politics and Conflict,
    Sports and Crime, Natural Disasters and Funding, Protest and Law Enforcement,
    and Crime. For each news headline, the NTM algorithm predicts the probability
    that the headline contains topics 1 through 5\. Thus, each headline will be represented
    by a mixture of five topics.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the *Indonesian police say gunfire killed azahari* headline,
    crime-related topics are predominant. The topics are very relevant since the headline
    has to do with the murder of Azahari, the mastermind behind the 2002 Bali bombing.
  prefs: []
  type: TYPE_NORMAL
- en: By completing this section, we have successfully looked at two different patterns
    for running inferences in SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to reuse data preprocessing logic for training
    and inference and how to run online as opposed to offline inferences. We started
    by understanding the architecture of the machine learning inference pipeline.
    Then, we used the ABC News Headlines dataset to illustrate big data processing
    through AWS Glue and SparkML. Then, we discovered topics from the news headlines
    by fitting the NTM algorithm to processed headlines. Finally, we walked through
    real-time as opposed to batch inferences by utilizing the same data preprocessing
    logic for inference. Through the inference pipeline, data scientists and machine
    learning engineers can increase speed with which ML solutions are marketed.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll do a deep dive into **Neural Topic Models** (**NTMs**).
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following reading material is intended to enhance your understanding of
    what was covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline models in Spark**:[https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42](https://blog.insightdatascience.com/spark-pipelines-elegant-yet-powerful-7be93afcdd42)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch Transform**:[https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic modeling**:[https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df](https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformers in Spark**:[https://spark.apache.org/docs/1.6.0/ml-guide.html#transformers](https://spark.apache.org/docs/1.6.0/ml-guide.html#transformers)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
