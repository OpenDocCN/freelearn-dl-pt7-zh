- en: Platforms and Other Essentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll discuss important libraries and frameworks that one
    needs to get started in **Artificial Intelligence** (**AI**). We''ll cover the
    basic functions of the three most popular deep learning frameworks—TensorFlow,
    PyTorch, and Keras—show you how to get up and running in each of these frameworks,
    as we will be utilizing them in the following chapters. We''ll touch upon computing
    for AI, and discuss how GPUs and other advanced memory units can improve it. Lastly,
    we''ll discuss the fundamentals of two popular cloud computing frameworks for
    deep learning: AWS and Google Cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Essential libraries for deep learning in Python: TensorFlow, PyTorch, and Keras'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPUs, GPUs, and compute frameworks that are used for AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fundamentals of AWS and Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be working with TensorFlow, PyTorch, and Keras in Python 3. It is recommended
    that you have an NVIDIA GPU on your computer. The following models are recommended:'
  prefs: []
  type: TYPE_NORMAL
- en: GTX 1080 Ti
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GTX 1070
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GTX 1060
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you do not have an NVIDIA GPU, please follow the prompts in the *Cloud Computing*
    section to utilize a GPU instance on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: You must also have an AWS and Google Cloud account; both are free, and you can
    sign up at their respective websites.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow, PyTorch, and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll introduce three of the most popular deep learning frameworks:
    TensorFlow, PyTorch, and Keras. While we''ll look at the basic functionality of
    each of the packages, we''ll learn about specific deep learning functions for
    each of the frameworks in later chapters as part of our hands-on approach to learning
    AI.'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow** is the most popular, and most contributed to, deep learning
    library. Originally developed by Google Brain for use on Google''s own AI products,
    it was open sourced in 2015 and has since become the standard for deep learning.
    TensorFlow underlies all of Google''s own deep learning based products such as
    Google Translate and the Google Cloud Platform''s machine learning APIs. Google
    has setup TensorFlow specifically to be parallelized, and as such it performs
    really well in distributed environments.'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow provides APIs for Python, C++, Java, and others; however, in this
    book we are going to stick to utilizing Python. TensorFlow can be installed from
    PyPy with the simple: `pip install tensorflow`.
  prefs: []
  type: TYPE_NORMAL
- en: Basic building blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may have guessed from the name, TensorFlow relies on the algebraic concept
    of tensors that we learned about in the previous chapter. Everything, from input
    data to parameters, is stored in a tensor in TensorFlow. As such, TensorFlow has
    its own functions for many of the basic operations normally handled by NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: 'When writing tensors in TensorFlow, we''re really writing everything in an
    array structure. Remember how an array can be a rank 1 tensor? That is exactly
    what we are passing in the preceding example. If we wanted to pass a rank 3 tensor,
    we''d simply write `x = tf.constant([1,2,3,4],[5,6,7,8],[9,10,11,12])`. You''ll
    notice that we defined constants in the following code; these are just one of
    three types of data structure we can use in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Constants**: Defined values that cannot change'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Placeholders**: Objects that will be assigned a value during a TensorFlow
    **session**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variables**: Like constants, only the values can change'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alright, back to the code. If we had run the following code block, we would
    have been left with a **TensorFlow object**, which looks something like tensor
    (`"Mul:0"`, `shape=(4,), dtype=int32`).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Why? because TensorFlow runs on the concept of **sessions**. The underlying
    code of TensorFlow is written in C++, and a session allows a high-level TensorFlow
    package to communicate with the low-level C++ runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Before we run a TensorFlow session, we need to tell it to initialize all of
    the variables we declared, and then run the initialization## In Tensorflow, we
    must first initialize a session object
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'One last important concept in TensorFlow is that of **scopes**. Scopes help
    us control various operational blocks within our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: That's it! We've successfully performed out first operation in TensorFlow. We'll
    also be learning more about the in-depth operations of TensorFlow in the next
    chapter on building **Artificial Neural Networks **(**ANNs**).
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the more important and powerful features of TensorFlow is its graph.
    When you define one of the three types of TensorFlow data structures previously
    described, you automatically add a **node** and an **edge** to your graph. Nodes
    represent operations and edges represent tensors, so if we were to do basic multiplication
    such as the preceding example, `const1` and `const2` would represent edges in
    the graph, `tf.multiply` would represent a node, and `product` would represent
    an outgoing edge from that node. TensorFlow's graph is **static**, which means
    we cannot change it at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, an ANN performs hundreds of computations; computing and interpreting
    at each step would be extremely compute-intensive. The TensorFlow graph ...
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**PyTorch** is a newer, but growing deep learning library that is based on
    the Torch framework used for Facebook''s deep learning algorithms. Unlike TensorFlow,
    PyTorch is not a wrapper that compiles to an underlying language, but is written
    to mimic native Python. If you have had any experience with Python programming,
    PyTorch will feel extremely familiar to you.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch can be easily installed with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Currently, PyTorch does not have a Windows distribution, which may make it out
    of reach for some users.
  prefs: []
  type: TYPE_NORMAL
- en: Basic building blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Such as TensorFlow, PyTorch represents data in tensor form. Torch tensors are
    defined as standard data types, such as `torch.FloatTensor()` , `torch.charTensor()`,
    and `torch.intTensor()`. As mentioned, operations in PyTorch are highly Pythonic.
    To repeat the exact same multiplication operation that we performed in preceding
    TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result of it native Python feel, PyTorch allows for easy interaction between
    standard numpy arrays and PyTorch tensors. It''s easy to switch back and forth
    between the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The PyTorch graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch seems more Pythonic because of its **dynamic graph compute structure**.
    Since Python is an interpreted language, meaning that operations are executed
    at runtime, PyTorch's graphing feature seeks to replicate this by allowing us
    to alter variables in the graph at runtime. In simpler words, PyTorch's graphs
    are created at the time you actually execute the code, not defined statically
    beforehand like in TensorFlow. Architecturally, this means that you can actually
    change your network architecture during training, which means PyTorch can accommodate
    a lot more cutting edge, dynamic architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Keras** is the most high-level deep learning library available, and is often
    where people begin on their AI journey. While we will focus on applications with
    TensorFlow in this book, it is important to introduce Keras because of its ubiquity
    and ease of use.'
  prefs: []
  type: TYPE_NORMAL
- en: Written by François Chollet at Google, Keras is a wrapper that can run on top
    of TensorFlow or other libraries such as Apache, MXNet, or Theano. Like the other
    libraries, it is available through PyPy by running `pip install keras` in your
    terminal or command line. Functionally, it's very similar to the way the scikit-learn
    works, and hence is a popular library for those who wish to get their hands dirty
    with deep learning as quickly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Like PyTorch, Keras was designed to ...
  prefs: []
  type: TYPE_NORMAL
- en: Basic building blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As Keras is designed as a model-level library, it does not contain methods
    for doing basic operations as PyTorch of base TensorFlow does. Instead, it utilizes
    TensorFlow as a backend. As such, its basic operations are the same as basic TensorFlow
    operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Keras also uses the same graph structure as Tensorflow. We'll learn more about
    Keras model building methods in the next chapter on *Your First Artificial Neural
    Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, what is the best library to use? As you can see in the following screenshot,
    one benchmark places PyTorch firmly in the lead when compared with other deep
    learning libraries when running an ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60eaf104-5f98-440f-af9d-4444555f98d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Ultimately, your choice of library mostly comes down to personal preference;
    however, in general:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keras**: Best for beginners or those looking to do *quick and dirty* work
    on ANNs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow**: Widely used, there are great code bases and tutorials available
    online and it is widely integrated into cloud machine images and all types of
    computing frameworks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyTorch**: Provides exceptional speed and ease of use, but is largely still
    underdeveloped, ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud computing essentials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, on-premise GPU clusters are not always available or practical. More often
    than not, many businesses are migrating their AI applications to the cloud, utilizing
    popular cloud provider services such as **Amazon Web Services** (**AWS**) or the
    **Google Cloud Platform** (**GCP**). When we talk about the cloud, we are really
    talking about database and compute resources, offered as a service. Cloud solution
    providers such as AWS and GCP have data centers across the world that store data
    and run computing jobs for people remotely. When your data is in the cloud, or
    when you are running a program in the cloud, you are really running or storing
    in one of these data centers. In cloud terminology, we call these data centers
    or cluster of data centers **regions**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloud services are divided into three different offering structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Infrastructure as a Service** (**IaaS**): Raw computing and network resources
    that you can use to build infrastructure, just as you would locally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Platform as a Service** (**PaaS**): Managed services that obfuscate away
    infrastructure components'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software as a Service** (**SaaS**): Fully managed solutions, such as online
    email'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we'll cover both IaaS solutions, as well as PaaS solutions.
    While cloud providers do offer SaaS solutions for AI, they are a bit too high
    level for our needs. In this section, we'll discuss the basic tools that you'll
    need to utilize the compute power of the cloud. Towards the end of this chapter,
    we'll discuss cloud computing in more detail in the *Maintaining AI applications* section.
  prefs: []
  type: TYPE_NORMAL
- en: AWS basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**AWS** is the most popular cloud computing provider on the market. In this
    section, we''ll explore the basics for getting set up in the cloud, including
    creating and connecting to EC2 instances (Amazon''s main cloud computing framework),
    as well as how to set up virtual machines in the cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: We'll also touch upon how to utilize Amazon's bulk storage component, S3. While
    AWS offers several machine learning services, we're going to focus solely on the
    basic need to utilize AWS cloud computing architectures to power your AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: EC2 and virtual machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The building block for AWS systems is the **Elastic Cloud Compute **(**EC2**)
    instance; it is a virtual server that allows you to run applications in the cloud.
    In this chapter, EC2 will be the basis for our cloud computing work. For developers
    and data scientists, Amazon has a suite of virtual machines called **Amazon Machine
    Images** (**AMI**) that come preloaded with everything you need to get up and
    running with deep learning in the cloud. For our purposes, Amazon has both an
    Ubuntu AMI as well as an Amazon Linux distribution AMI, which are preloaded with
    Python 3 and TensorFlow, PyTorch, and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started with utilizing EC2 for deep learning, we''ll just have to follow
    a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to your Amazon Web Services Account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search for EC2 in the Search bar and select the service to open a new console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the Launch Instance button and search for the AWS deep learning AMI in
    the AWS Marketplace. You can select either the Ubuntu version or Amazon Linux.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a GPU instance to run your image on. We suggest either a G2 or P2 instance.
    Choose Next on each page until you reach Configure Security Group. Under Source,
    choose My IP to allow access using only your IP address.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click Launch Instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new Private Key and store this somewhere locally; this will help you
    connect to your instance later on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, you should have your AMI set up and ready to utilize. If you already have
    an EC2 instance up and running 0n your AWS account, select the instance and right-click
    on Image, Create Image under the dropdown for that instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/59fb62db-96e8-42cf-b8a7-fb7a62dc4784.png)'
  prefs: []
  type: TYPE_IMG
- en: Follow the prompts and select Create Image. Afterwards, you can find that AMI
    by selecting EC2 -> AMIs under the main Explorer toolbar. If you still can't see
    your AMI, you can find more detailed instructions on AWS website [https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/tkv-create-ami-from-instance.html](https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/tkv-create-ami-from-instance.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'To utilize your new virtual machine, first launch the instance on AWS. Here `ssh`
    is initialized by utilizing the following command (make sure you are in the same
    directory as the `pem` key file you just downloaded):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Once you've connected with your terminal or command line, you can utilize the
    interface just as you would the command line on your local computer.
  prefs: []
  type: TYPE_NORMAL
- en: S3 Storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Amazon Simple Storage Service** (**Amazon S3**), is AWS''s bulk cloud storage
    solution. S3 is designed to be simple, cheap, and efficient - it works just like
    a local directory on your computer would. These storage locations are known as
    **Buckets**, and can store up to 5 Terabytes of data.'
  prefs: []
  type: TYPE_NORMAL
- en: To setup an S3, log onto your AWS console, find the S3 service, and click Create
    bucket
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5003b9fd-4865-4250-9f01-dc2f33954891.png)'
  prefs: []
  type: TYPE_IMG
- en: You can set permissions for who can and cannot access the data in an S3 bucket,
    should you need to restrict access.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Sagemaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SageMaker is Amazon''s fully managed cloud machine learning offering. As a
    **Platform as a Service** (**PaaS**) product, SageMaker is one of the simplest
    ways in which you can deploy a machine learning model. Unlike it''s competitors,
    Amazon SageMaker only runs Python 2.7. SageMaker has two options for handling
    machine learning services in the cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating and training your model in a hosted Jupyter notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training from a dockerized version of the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll be diving into how to train and deploy models with SageMaker in the coming
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Platform basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While AWS is the major player in the cloud marketplace and has been for some
    time now, over the past few years the **Google Cloud Platform** (**GCP**) has
    been gaining popularity, especially in the field of machine learning. You can
    sign up for GCP for free by navigating to [https://cloud.google.com/free/](https://cloud.google.com/free/),
    and entering the console. Keep in mind that you do need a Google user account,
    such as a gmail account, to sign up for Google services. While many small tasks
    can be completed within the platform's free tier, GCP offers a $300.00 credit
    to new users to get started.
  prefs: []
  type: TYPE_NORMAL
- en: All services in GCP run under the umbrella of a project. Projects are tools
    for organizing computing tools, users and access rights, as well as billing. ...
  prefs: []
  type: TYPE_NORMAL
- en: GCP cloud storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cloud storage is a simple, bucket-structured storage option that is similar
    to AWS S3\. Like AWS, GCP cloud storage holds up to 5 Terabytes of data. As opposed
    to competitors such as AWS, or even Microsoft Azure, GCP's cloud storage has upload
    and download speeds for large files that are about three times faster than it's
    competitors. Cloud storage also has some of the fastest **throughput** on the
    market. Throughput, a cloud concept that measures how much data is processed at
    a given time - in simpler words, how fast can data be processed. When creating
    certain applications that rely on streaming data, this can be critical. Cloud
    storage also has the option to create buckets that span across service regions,
    which helps with fault tolerance and availability of your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To setup a Cloud storage bucket, log-on to the GCP console, search for storage,
    and click CREATE BUCKET:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a568ade9-aab8-439a-bc9a-15ef47c75f14.png)'
  prefs: []
  type: TYPE_IMG
- en: In addition to their standard compute and storage services, the GCP has another
    tool, ML engine, which provides seamless training and deployment operations for
    machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: GCP Cloud ML Engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Cloud Platform's Cloud ML Engine is Google's equivalent to AWS SageMaker.
    As a managed PaaS, Cloud ML handles the training and deployment processes for
    machine learning algorithms. If you're thinking - what about a basic compute service
    like EC2 on AWS? GCP has that as well. Compute Engine is GCP's answer to Amazon
    EC2; it provides basic, scalable cloud compute services. While we could use Compute
    Engine to setup AI platforms, GCP has made it extremely simple for us to build
    with Cloud ML Engine and as such, we will note be covering the basic Compute Engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s dive into the details. Cloud ML engine allows you to:'
  prefs: []
  type: TYPE_NORMAL
- en: Train scikit-learn and TensorFlow models both locally for testing and in the
    cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create retrainable ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CPUs, GPUs, and other compute frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Progress in AI has always been tied to our compute abilities. In this section,
    we will discuss CPUs and GPUs for powering AI applications, and how to set up
    your system to work with accelerated GPU processing.
  prefs: []
  type: TYPE_NORMAL
- en: The main computational hardware in your computer is known as the **central processing
    unit **(**CPUs**); CPUs are designed for general computing workloads. While your
    local CPU can be used to train a deep learning model, you might find your computer
    hanging up on the training process for hours. When training AI applications on
    hardware, it's smarter to use the CPU's cousin, the **Graphics Processing Unit**
    (**GPU**). GPUs are designed to process in parallel, just as an ANN process in
    parallel. As we learned in the last chapter, AI applications require many linear
    algebra operations, the exact same type of operations that are required for video
    games. GPUs, originally designed for the gaming industry, provide us with thousands
    of cores to process these operations as well as parallelize them. In this manner,
    they lend themselves naturally to constructing deep learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'When selecting a GPU to utilize in deep learning applications, we''re looking
    at three main characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Processing power**: How fast the GPU can compute; defined as *cores* x *speed*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory**: The ability of the GPU to handle various sizes of data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RAM**: The amount of data you can have on your GPU at any given time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we'll be focusing on utilizing the most popular GPU brand for
    deep learning, **NVIDIA**, whose CUDA toolkit makes out-of-the-box deep learning
    an easy task. As the major competitor to NVIDIA, Radeon AMD GPUs utilize a toolkit
    called **OpenCL**, which does not have direct compatibility with most deep learning
    libraries out of the box. While AMD GPUs provide great hardware at a reasonable
    price, it is best to go with an NVIDIA product to make getting up to speed easy.
  prefs: []
  type: TYPE_NORMAL
- en: Should you have another GPU on your computer or no GPU at all, it is recommended
    that you utilize a GPU instance on AWS to follow the steps.
  prefs: []
  type: TYPE_NORMAL
- en: Installing GPU libraries and drivers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's get our computers set up for building AI applications. If you wish
    to do this task on your local computer, it's advised that you have either Windows
    or a Linux distribution installed. Unfortunately, most macOS are not built to
    accommodate GPUs, and therefore we will not be touching on macOS in this section.
    If you do not have an NVIDIA GPU on your computer, please perform the following
    steps. If you do have an NVIDIA GPU, you may choose to follow along with the AWS-based
    section, or skip to the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re not sure whether you have an NVIDIA GPU, you can check for its existence
    with the following terminal command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With Linux (Ubuntu)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linux and AI go together such as natural complements. When you talk to Google
    Assistant on your Android phone, talk to Cortana on your Windows device, or wonder
    how Watson won a round at Jeopardy on TV, it's all based on Linux.
  prefs: []
  type: TYPE_NORMAL
- en: When we talk about Linux in this chapter, we'll be talking about a particular
    distribution called **Ubuntu**,one of the most popular distributions of the Linux
    operating system. It's recommended that you stick with an older, more stable version
    of Ubuntu (Version 14.04), as although it will do wonders for your AI applications;
    it's certainly not as stable as your standard Windows OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''d like to use Ubuntu on your local machine, check out Ubuntu''s tutorials
    for installing on a Windows machine ([https://tutorials.ubuntu.com/tutorial/tutorial-ubuntu-on-windows#0](https://tutorials.ubuntu.com/tutorial/tutorial-ubuntu-on-windows#0)).
    If you don''t have a PC or you''d like to set up a virtual instance, AWS has a
    great tutorial ([https://aws.amazon.com/getting-started/tutorials/launch-a-virtual-machine/](https://aws.amazon.com/getting-started/tutorials/launch-a-virtual-machine/))
    to walk you through the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started with deep learning for GPUs on Ubuntu, we first have to install
    the GPU''s driver. In this example, we''re going to utilize `wget` and `chmod`
    to retrieve and set up read/write access:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Once the installation finishes, you can check if it was intalled correctly with
    a simple `nvidia-smi` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, let''s install NVIDIA CUDA. CUDA is a NVIDIA package that allows us to
    run TensorFlow models on our GPUs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s add the library to our system path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we need to install a higher-level package called cuNN, which is a specific
    library that sits on top of CUDA and provides highly-tuned procedures for typical
    deep learning operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'One last step to move the files to the correct place:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And there you are, we''re set up on Ubuntu for GPU acceleration. Our last step
    is to simply install the GPU-enabled version of TensorFlow with Python 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To set up your GPU for deep learning on Windows, you must be running Windows
    7 or higher. You can verify that you have a CUDA-capable GPU through the **display
    adapters** section in the **Windows Device Manager**. Here, you will find the
    vendor name and model of your graphics card.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see what type of GPU you have, open a command line prompt and run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If you do not have the driver installed for your NVIDIA GPU, or would like to
    update the drivers, you can find the correct driver based on your device on the
    NVIDIA website: [http://www.nvidia.com/Download/index.aspx?lang=en-us](http://www.nvidia.com/Download/index.aspx?lang=en-us).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, go ahead and grab the CUDA toolkit ([https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads))
    form NVIDIA. Select **CUDA Version ...**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Basic GPU operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have our GPUs set up for deep learning, let''s learn how to utilize
    them. In TensorFlow, GPUs are represented as strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/cpu:0`: The CPU of your machine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/device:GPU:0`: The GPU of your machine, if you have one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/device:GPU:1`: The second GPU of your machine, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed training** is the practice of training a network across several
    GPUs, and it''s becoming an increasingly common way to train models in the AI
    field. TensorFlow, Keras, and PyTorch all support distributed training.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logging** is an operation in TensorFlow to assign a particular set of commands
    to a particular GPU or CPU on your system. With logging, we can also **parallelize**
    our operations, meaning that we can distribute training across several GPUs at
    the same time. To do this, we utilize a simple loop structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We first create an empty set, and use an iterator to assign the matrix multiplication
    procedure across two GPUs: GPU1 and GPU2\. Another procedure, a simple sum, gets
    assigned to our CPU. We then run both through a TensorFlow session to execute
    the operations as we did before.'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that this type of device management is for local devices, and that
    management is different for cloud architectures.
  prefs: []
  type: TYPE_NORMAL
- en: The future – TPUs and more
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google has recently released a new piece of hardware known as the **Tensor Processing
    Unit **(**TPU**), which is specifically designed for AI applications. These TPUs
    deliver 15-30x higher performance and 30-80x higher performance-per-watt than
    a CPU or GPU can deliver. Weights in large scale, production-level AI applications
    can number from five million to 100 million, and these TPUs excel in performing
    these operations.
  prefs: []
  type: TYPE_NORMAL
- en: TPUs are specifically tailored for TensorFlow processing, and are currently
    available on Google Cloud for use. If you're interested in exploring their functionality,
    GCP has a great tutorial ([https://cloud.google.com/tpu/docs/quickstart](https://cloud.google.com/tpu/docs/quickstart)) on
    how to get started.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The landscape for AI platforms and methods is diverse; in this chapter, we've
    outlined some of the most promising and well regarded technologies in the AI arena.
    For deep learning packages, we learned about TensorFlow, PyTorch, and Keras. TensorFlow,
    released by Google, is the most popular and robust of the deep learning libraries.
    It utilizes sessions and static graphs to compile to its underlying C++ code. PyTorch,
    on the other hand, is a newer library that features a dynamic graph for runtime
    execution, which allows it to feel like native Python. Lastly, Keras is a high-level
    library that runs on top of TensorFlow, and can be useful for creating straightforward
    networks where customization is not needed.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed cloud computing, utilizing AWS as the most popular cloud computing
    service, with its primary workhorses being the EC2 instance and the s3 Bucket.
    EC2 consists of virtual servers that can be used for scaling AI applications where
    hardware doesn't exist/is not needed. S3, Amazon's Simple Storage Service gives
    us the ability to store data and other resources that are necessary for running
    our applications. Lastly, we walked through enabling your computer and languages
    for GPU-accelerated deep learning. In the next chapter, we'll put this to use
    by creating our first ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll put together the fundamental knowledge that learned
    in [Chapter 2](c72aa49d-41f1-4a15-bee5-9efc9190f282.xhtml), *Machine Learning
    Basics*, with the platform knowledge from this chapter to create our first ANNs.
  prefs: []
  type: TYPE_NORMAL
