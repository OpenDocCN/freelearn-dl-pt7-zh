<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer080">
			<h1 id="_idParaDest-61"><em class="italic"><a id="_idTextAnchor063"/>Chapter 4</em>: Automating Document Processing Workflows </h1>
			<p>In the previous chapter, we were introduced to <strong class="bold">Amazon Comprehend</strong> and <strong class="bold">Amazon Comprehend Medical</strong>, and we covered how to use these services to derive insights from text. We also spent some time understanding how Natural Language Processing algorithms work, the different types of insights you can uncover, and we also ran code samples trying out the Amazon Comprehend APIs.</p>
			<p>In this chapter, we will walk through our first real-world use case of automating a document management workflow that many organizations struggle with today. We put together this solution based on our collective experience and the usage trends we have observed in our careers. Fasten your seat belts and get ready to experience architecting an end-to-end AI solution one building block at a time and watch it taking shape in front of you. We expect to be hands-on throughout the course of this chapter, but we have all the code samples we need to get going.</p>
			<p>We will dive deep into how you can automate document processing with <strong class="bold">Amazon Textract</strong> and then we will cover how you can set up compliance and control in the documents using <strong class="bold">Amazon Comprehend</strong>. Lastly, we will talk about architecture best practices while designing <strong class="bold">real-time document processing</strong> workflows versus <strong class="bold">batch processing.</strong> We will provide detailed code samples, designs, and development approaches, and a step-by-step guide on how to set up and run these examples along with access to GitHub repositories.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Automating document processing workflows</li>
				<li>Setting up compliance and control</li>
				<li>Processing real-time document workflows versus batch document workflows</li>
			</ul>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor064"/>Technical requirements</h1>
			<p>For this chapter, you will need access to an AWS account. Please make sure to follow the instructions specified in the <em class="italic">Technical requirements</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>,<em class="italic"> Introducing Amazon Textract</em>, to create your AWS account, and log in to the AWS Management Console before trying the steps in this chapter.</p>
			<p>The Python code and sample datasets for a walk-through of this chapter's code are provided at the following link: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2004">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2004</a>. Please use the instructions in the following sections along with the code in the repository to build the solution.</p>
			<p>Check out the following video to see the Code in Action at <a href="https://bit.ly/3GlcCet">https://bit.ly/3GlcCet</a>.</p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor065"/>Automating document processing workflows</h1>
			<p>We have <a id="_idIndexMarker279"/>discussed in the previous chapter how Amazon Textract can help us digitize scanned documents such as PDF and images by extracting text from any document. We also covered how Amazon Comprehend can help us extract insights from these documents, including<a id="_idIndexMarker280"/> entities, <strong class="bold">Personal Identifiable Information</strong> (<strong class="bold">PII</strong>), and sentiments.</p>
			<p>Now, these services can be used together in an architecture to automate the document processing workflows for most organizations, be it a financial organization or healthcare, which we will cover in <a href="B17528_12_Final_SB_ePub.xhtml#_idTextAnchor141"><em class="italic">Chapter 12</em></a>, <em class="italic">AI and NLP in Healthcare</em>.</p>
			<p>Let's start with a fictitious bank, <em class="italic">LiveRight Pvt Ltd.</em>, whose customers are applying for home loans. We all know this loan origination process involves more than 400 documents to be submitted and reviewed by the bank before approval is forthcoming for your home loan. Automating this process will make it easier for banks as well as customers to get loans. The challenge with automating these workflows is that there are more <a id="_idIndexMarker281"/>than 1,000  templates for the loan origination process and going with any <strong class="bold">Optical Character Recognition</strong> (<strong class="bold">OCR</strong>) system will require managing these templates. Moreover, these OCR template-based approaches are not scalable and break with format changes. That's why we have Amazon Textract to extract text from any documents, enabling these documents to be automated and processed in hours rather than months or weeks.</p>
			<p>You have extracted the data from these forms or semi-structured documents. You will now want to set up compliance and control on the data extracted from these documents; for example, making sure that if the data is PII, you can mask it for further processing. You will also want to extract the entities if you want to focus on the loan approval process, for example, the loan amount or the details of the requester. This is where Amazon Comprehend can help. In fact, you can perform custom classification of the documents submitted and the custom entities based on your requirements with Amazon Comprehend; for example, documents extracted by Textract and sent to Amazon Comprehend for custom classification to classify whether the document submitted is a driving license or W2 form.</p>
			<p>The following is the architecture of how you can use Amazon Textract and Amazon Comprehend<a id="_idIndexMarker282"/> together to automate your existing document flow:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="Images/B17528_04_01.jpg" alt="Figure 4.1 – Automating document processing workflows&#13;&#10;" width="1100" height="212"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Automating document processing workflows</p>
			<p>In this architecture, you have documents coming in, and these documents may be financial documents, legal documents, mortgage applications, and so on. You send these documents to Amazon Textract to extract text from these documents. Once you have extracted text from these documents, you can send this text to Amazon Comprehend to extract insights. These insights can classify these documents based on document type, it can identify PII from these<a id="_idIndexMarker283"/> documents, or it can be <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>) using custom entity recognition. We cover custom entities in <a href="B17528_14_Final_SB_ePub.xhtml#_idTextAnchor162"><em class="italic">Chapter 14</em></a>, <em class="italic">Auditing Named Entity Recognition Workflows</em>, and document classification in <a href="B17528_15_Final_SB_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 15</em></a>, <em class="italic">Classifying Documents and Setting up Human in the Loop for Active Learning</em>.</p>
			<p>In this section, we covered how you can easily and quickly set up an automated document processing workflow with Amazon Textract and Amazon Comprehend by using these services together. In the next section, we will talk about how you can use these services together to set up compliance and control for LiveRight Pvt Ltd., especially<a id="_idIndexMarker284"/> by means of masking or redacting the PII data in their forms.</p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor066"/>Setting up compliance and control</h1>
			<p>In this <a id="_idIndexMarker285"/>section, we will talk about how LiveRight <a id="_idIndexMarker286"/>Pvt Ltd. can set up compliance and control as well as automate their loan origination process using Amazon Textract and Amazon Comprehend. We will walk you through the following architecture using code samples in a Jupyter notebook:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="Images/B17528_04_02.jpg" alt="Figure 4.2 – Setting up compliance and control&#13;&#10;" width="1021" height="222"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – Setting up compliance and control</p>
			<p>We will walk you through this architecture using a single document and sample code. However, this<a id="_idIndexMarker287"/> architecture<a id="_idIndexMarker288"/> can be automated to process a large number of documents using the <strong class="bold">step function</strong> and <strong class="bold">lambda functions</strong> in a serverless manner. In this architecture, we will show you the following: </p>
			<ol>
				<li>How you can upload a sample document and extract the text using <strong class="bold">Amazon Textract</strong> and save the extracted data as <strong class="source-inline">.txt</strong> or <strong class="source-inline">.csv</strong> files back to an <strong class="bold">Amazon  S3 bucket</strong>. </li>
				<li>Then, we will show you how you can use <strong class="bold">Amazon Comprehend's</strong> real-time or sync API to detect PII. </li>
				<li>We will then cover how you can use the Amazon Comprehend PII detection job to mask and redact the PII in the extracted text/CSV file in <strong class="bold">Amazon S3</strong>.</li>
				<li>How you can find the redacted document text in Amazon S3 as an output of the Comprehend PII detection job.</li>
			</ol>
			<p>So, let's <a id="_idIndexMarker289"/>get <a id="_idIndexMarker290"/>started with setting up the notebook.</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor067"/>Setting up to solve the use case</h2>
			<p>If you have <a id="_idIndexMarker291"/>not done so in the previous chapters, you will first have to create an Amazon SageMaker Jupyter notebook and <a id="_idIndexMarker292"/>set up <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) permissions for that notebook role to access the AWS services we will use in this notebook. After that, you will need to clone the GitHub repository (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services</a>). Please perform the following steps to complete these tasks before we can execute the cells from our notebook:</p>
			<ol>
				<li value="1">Follow the instructions documented in the <em class="italic">Create an Amazon SageMaker Jupyter notebook instance</em> section within the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, to create your Jupyter notebook instance.<p class="callout-heading">IAM Role Permission while Creating Amazon SageMaker Jupyter Notebooks</p><p class="callout">Accept the default option for the IAM role at notebook creation time to allow access to any S3 bucket. </p></li>
				<li>Once you have created the notebook instance and its status is <strong class="bold">InService</strong>, click on <strong class="bold">Open Jupyter</strong> in the <strong class="bold">Actions</strong> menu heading for the notebook instance. <div id="_idContainer071" class="IMG---Figure"><img src="Images/B17528_04_03.jpg" alt="Figure 4.3 – Opening the Jupyter notebook&#13;&#10;" width="1140" height="362"/></div><p class="figure-caption">Figure 4.3 – Opening the Jupyter notebook</p><p>This will take you to the home folder of your notebook instance. </p></li>
				<li>Click on <strong class="bold">New</strong> and<a id="_idIndexMarker293"/> then select <strong class="bold">Terminal</strong>, as shown in the following screenshot:<div id="_idContainer072" class="IMG---Figure"><img src="Images/B17528_04_04.jpg" alt="Figure 4.4 – Opening Terminal in a Jupyter notebook&#13;&#10;" width="535" height="314"/></div><p class="figure-caption">Figure 4.4 – Opening Terminal in a Jupyter notebook</p></li>
				<li>In the terminal window, first, type <strong class="source-inline">cd SageMaker</strong> and then type <strong class="source-inline">git clone https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services</strong>, as shown in the following screenshot:<div id="_idContainer073" class="IMG---Figure"><img src="Images/B17528_04_05.jpg" alt="Figure 4.5 – git clone command&#13;&#10;" width="946" height="148"/></div><p class="figure-caption">Figure 4.5 – git clone command</p></li>
				<li>Now, exit the terminal window, go back to the home folder, and you will see a folder called <strong class="source-inline">Chapter 04</strong>. Click the folder and you should see a notebook called <strong class="source-inline">Chapter 4 Compliance and control.ipynb</strong>.</li>
				<li>Open<a id="_idIndexMarker294"/> this notebook by clicking it. </li>
			</ol>
			<p>Next, we will cover the additional IAM prerequisites.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor068"/>Additional IAM prerequisites</h2>
			<p>To train the<a id="_idIndexMarker295"/> Comprehend custom entity recognizer and to set up real-time endpoints, we have to enable additional policies and update the trust relationships for our SageMaker notebook role. To do this, attach <strong class="source-inline">AmazonS3FullAccess</strong>, <strong class="source-inline">TextractFullAccess</strong>, and <strong class="source-inline">ComprehendFullAccess</strong> policies to your Amazon SageMaker Notebook IAM Role. To execute this step, please refer to <em class="italic">Changing IAM permissions and trust relationships for the Amazon SageMaker notebook execution role</em> in the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>. </p>
			<p>Now that we have the necessary IAM roles and notebook set up in the Amazon SageMaker notebook instance, let's jump to the code walk-through.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor069"/>Automating documents for control and compliance</h2>
			<p>In this<a id="_idIndexMarker296"/> section, we will give a code walk-through of the architecture we discussed for automating documents using Amazon Textract and setting compliance and control with PII masking using Amazon Comprehend in <em class="italic">Figure 14.2</em> using this notebook:</p>
			<ol>
				<li value="1">Execute the cell under <em class="italic">Step 1 – Setup and install libraries</em> in the Jupyter notebook you just set up at the following link, <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2004/Chapter%204%20Compliance%20and%20control.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2004/Chapter%204%20Compliance%20and%20control.ipynb</a>, to ensure that you have the libraries needed for the notebook. Note that in this cell, you are getting the Amazon SageMaker execution role for the notebook along with the SageMaker session. You are setting up boto3 libraries to call Amazon Textract, Amazon Comprehend, and Amazon S3 APIs. You are also using the SageMaker session to access the default SageMaker S3 bucket where you will be storing the data for this lab using a prefix or folder.</li>
				<li>Now, we will start with the sample bank statement. Execute the cells under <em class="italic">Step 2</em>, <em class="italic">Extracting text from a sample document</em> in the Jupyter notebook, to display the sample document to extract text and redact the PII:<p class="source-code">documentName = "bankstatement.png"</p><p class="source-code">display(Image(filename=documentName))</p><p>You will get the following response:</p><div id="_idContainer074" class="IMG---Figure"><img src="Images/B17528_04_06.jpg" alt="Figure 4.6 – Sample bank statement" width="637" height="775"/></div><p class="figure-caption">Figure 4.6 – Sample bank statement</p></li>
				<li>Now we will invoke Amazon Textract's Detect Document Text Sync API, <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html#Textract.Client.detect_document_text">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/textract.html#Textract.Client.detect_document_text</a>, which extracts only text from documents in near-real time to extract data from the sample bank statement <a id="_idIndexMarker297"/>using the following code:<p class="source-code">client = boto3.client(service_name='textract',</p><p class="source-code">         region_name= 'us-east-1',</p><p class="source-code">         endpoint_url='https://textract.us-east-1.amazonaws.com')</p><p class="source-code">with open(documentName, 'rb') as file:</p><p class="source-code">            img_test = file.read()</p><p class="source-code">            bytes_test = bytearray(img_test)</p><p class="source-code">            print('Image loaded', documentName)</p><p class="source-code">response = client.detect_document_text(Document={'Bytes': bytes_test})</p><p class="source-code">print(response)</p><p>You get a JSON response from Amazon Textract using the Detect Document Text Sync API.</p></li>
				<li>Now we will extract text from this JSON response using the Amazon Textract parser library we installed in <em class="italic">Step 1</em>. Run the following code to parse the Textract JSON response to text:<p class="source-code">doc = Document(response)</p><p class="source-code">page_string = ''</p><p class="source-code">for page in doc.pages:</p><p class="source-code">        for line in page.lines:</p><p class="source-code">            page_string += str(line.text)</p><p class="source-code">print(page_string)</p><p>Now that we have the extracted text from the Textract JSON response, let's move on to the next step.</p></li>
				<li>In this step, we will save the extracted text from the bank statement to a text/CSV file and upload it to Amazon S3 for processing with the Amazon Comprehend batch job. Run the notebook cell <em class="italic">Step 3</em>, <em class="italic">Save the extracted text to a text/CSV file and upload it to an Amazon S3 bucket</em>, to save the data in a text file and then upload it to Amazon S3.</li>
				<li>Now that<a id="_idIndexMarker298"/> we have extracted the text from bank statements, converted it into a text file, and uploaded it to Amazon S3, in this step, we will detect PII from the text using Amazon Comprehend Detect PII Sync APIs. Run the notebook cell <em class="italic">Step 4</em>, <em class="italic">Check for PII using the Amazon Comprehend Detect PII Sync API</em>, to call the Comprehend APIs by passing the extracted text from Amazon Textract:<p>a) First, initialize the boto3 handle for Amazon<a id="_idTextAnchor070"/> Comprehend:</p><p class="source-code">`comprehend = boto3.client('comprehend')</p><p>b) Then, call Amazon Comprehend and pass it the aggregated text from our sample bank statement image to Comprehend detect PII entities: <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.detect_pii_entities">https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html#Comprehend.Client.detect_pii_entities</a>:</p><p class="source-code">piilist=comprehend.detect_pii_entities(Text = page_string, LanguageCode='en')</p><p class="source-code">redacted_box_color='red'</p><p class="source-code">dpi = 72</p><p class="source-code">pii_detection_threshold = 0.00</p><p class="source-code">print ('Finding PII text...')</p><p class="source-code">not_redacted=0</p><p class="source-code">redacted=0</p><p class="source-code">for pii in piilist['Entities']:</p><p class="source-code">    print(pii['Type'])</p><p class="source-code">    if pii['Score'] &gt; pii_detection_threshold:</p><p class="source-code">                    print ("detected as type '"+pii['Type']+"' and will be redacted.")</p><p class="source-code">                    redacted+=1</p><p class="source-code">                </p><p class="source-code">    else:</p><p class="source-code">        print (" was detected as type '"+pii['Type']+"', but did not meet the confidence score threshold and will not be redacted.")</p><p class="source-code">        not_redacted+=1</p><p class="source-code">print ("Found", redacted, "text boxes to redact.")</p><p class="source-code">print (not_redacted, "additional text boxes were detected, but did not meet the confidence score threshold.")s3_entity_key = prefix + "/train/entitylist.csv"</p><p>You will get a response with identifying PII from the text, which will be redacted in the next step using the Amazon Comprehend PII analysis job.</p><div id="_idContainer075" class="IMG---Figure"><img src="Images/B17528_04_07.jpg" alt="Figure 4.7 – PII detection using Amazon Comprehend in a bank statement" width="610" height="536"/></div><p class="figure-caption"> </p><p class="figure-caption">Figure 4.7 – PII detection using Amazon Comprehend in a bank statement </p><p>We will mask/redact these 15 PII entities we found in the sample bank statement.</p></li>
				<li>Next, we will call the <strong class="source-inline">StartPiiEntitiesDetectionJob</strong> API to start an asynchronous <a id="_idIndexMarker299"/>PII entity detection job for a collection of documents. For this example, we are just using one document sample. You can redact a large number of documents using this job. Run the notebook cell <em class="italic">Step 5</em>, <em class="italic">Mask PII using the Amazon Comprehend PII Analysis Job</em>, to set up and start the PII redaction analysis job with Amazon Comprehend: <p>a) Then job requires the S3 location of documents to be redacted and the S3 location of where you want the redacted output. Run the following cell to specify the location of the S3 text file we want to be redacted:</p><p class="source-code">import uuid</p><p class="source-code">InputS3URI= "s3://"+bucket+ "/pii-detection-redaction/pii_data.txt"</p><p class="source-code">print(InputS3URI)</p><p class="source-code">OutputS3URI="s3://"+bucket+"/pii-detection-redaction"</p><p class="source-code">print(OutputS3URI)</p><p>b) Now we will call <strong class="source-inline">comprehend.start_pii_entities_detection_job</strong> by setting parameters for redaction and passing the input S3 location where data is stored by<a id="_idIndexMarker300"/> running the following notebook cell:</p><p class="source-code">response = comprehend.start_pii_entities_detection_job(</p><p class="source-code">    InputDataConfig={</p><p class="source-code">        'S3Uri': InputS3URI,</p><p class="source-code">        'InputFormat': 'ONE_DOC_PER_FILE'</p><p class="source-code">    },</p><p class="source-code">    OutputDataConfig={</p><p class="source-code">        'S3Uri': OutputS3URI</p><p class="source-code">       </p><p class="source-code">    },</p><p class="source-code">    Mode='ONLY_REDACTION',</p><p class="source-code">    RedactionConfig={</p><p class="source-code">        'PiiEntityTypes': [</p><p class="source-code">           'ALL',</p><p class="source-code">        ],</p><p class="source-code">        'MaskMode': 'MASK',</p><p class="source-code">        'MaskCharacter': '*'</p><p class="source-code">    },</p><p class="source-code">    DataAccessRoleArn = role,</p><p class="source-code">    JobName=job_name,</p><p class="source-code">    LanguageCode='en',   </p><p class="source-code">)</p><p class="callout-heading">Note</p><p class="callout">Using this API or batch job, you have the choice to specify the mode, redaction config, and language.</p><p>Here are the parameters that can be modified as shown in the following code block:</p><p class="source-code">Mode='ONLY_REDACTION'|'ONLY_OFFSETS',</p><p class="source-code">    RedactionConfig={</p><p class="source-code">        'PiiEntityTypes': [</p><p class="source-code">            'BANK_ACCOUNT_NUMBER'|'BANK_ROUTING'|'CREDIT_DEBIT_NUMBER'|'CREDIT_DEBIT_CVV'|'CREDIT_DEBIT_EXPIRY'|'PIN'|'EMAIL'|'ADDRESS'|'NAME'|'PHONE'|'SSN'|'DATE_TIME'|'PASSPORT_NUMBER'|'DRIVER_ID'|'URL'|'AGE'|'USERNAME'|'PASSWORD'|'AWS_ACCESS_KEY'|'AWS_SECRET_KEY'|'IP_ADDRESS'|'MAC_ADDRESS'|'ALL',</p><p class="source-code">        ],</p><p class="source-code">        'MaskMode': 'MASK'|'REPLACE_WITH_PII_ENTITY_TYPE',</p><p class="source-code">        'MaskCharacter': 'string'</p><p>Refer to<a id="_idIndexMarker301"/> the API documentation for more details: <a href="https://docs.aws.amazon.com/comprehend/latest/dg/API_StartPiiEntitiesDetectionJob.html">https://docs.aws.amazon.com/comprehend/latest/dg/API_StartPiiEntitiesDetectionJob.html</a>.<strong class="bold"> </strong></p><p>c) The job will take roughly 6-7 minutes. The following code is to check the status of the job. The cell execution will be completed once the job is complete: </p><p class="source-code">from time import sleep</p><p class="source-code">job = comprehend.describe_pii_entities_detection_job(JobId=events_job_id)</p><p class="source-code">print(job)</p><p class="source-code">waited = 0</p><p class="source-code">timeout_minutes = 10</p><p class="source-code">while job['PiiEntitiesDetectionJobProperties']['JobStatus'] != 'COMPLETED':</p><p class="source-code">    sleep(60)</p><p class="source-code">    waited += 60</p><p class="source-code">    assert waited//60 &lt; timeout_minutes, "Job timed out after %d seconds." % waited</p><p class="source-code">    job = comprehend.describe_pii_entities_detection_job(JobId=events_job_id)</p><p>You will get a JSON response, and this job will take 5-6 minutes. You can go and grab a <a id="_idIndexMarker302"/>coffee until the notebook cell is running and you have a response.</p></li>
				<li>Once the job is successful, we will now show you the extracted, redacted document output in this step. Run the notebook cell <em class="italic">Step 6</em>, <em class="italic">View the redacted/masked output in the Amazon S3 bucket</em>, to extract the output from the Amazon S3 bucket: <p class="source-code">filename="pii_data.txt"</p><p class="source-code">s3_client = boto3.client(service_name='s3')</p><p class="source-code">output_data_s3_file = job['PiiEntitiesDetectionJobProperties']['OutputDataConfig']['S3Uri'] + filename + '.out'</p><p class="source-code">print(output_data_s3_file)</p><p class="source-code">output_data_s3_filepath=output_data_s3_file.split("//")[1].split("/")[1]+"/"+output_data_s3_file.split("//")[1].split("/")[2]+"/"+output_data_s3_file.split("//")[1].split("/")[3]+"/"+output_data_s3_file.split("//")[1].split("/")[4]</p><p class="source-code">print(output_data_s3_filepath)</p><p class="source-code">f = BytesIO()</p><p class="source-code">s3_client.download_fileobj(bucket, output_data_s3_filepath, f)</p><p class="source-code">f.seek(0)</p><p class="source-code">print(f.getvalue())</p><p>You will <a id="_idIndexMarker303"/>get the following redacted bank statement:</p></li>
			</ol>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="Images/B17528_04_08.jpg" alt="Figure 4.8 – Redacted bank statement using the Amazon Comprehend PII Redaction job&#13;&#10;" width="1385" height="471"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – Redacted bank statement using the Amazon Comprehend PII Redaction job</p>
			<p>In the output, you can see that the Amazon Comprehend PII job has masked the PII data, such as an address, name, SSN, and bank account number identified using the Amazon Comprehend Detect PII entity.</p>
			<p>In this section, we walked you through an end-to-end conceptual architecture for automating documents for compliance and control. In the next section, we will talk about best practices for real-time document processing workflows versus batch processing <a id="_idIndexMarker304"/>workflows.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor071"/>Processing real-time document workflows versus batch document workflows</h1>
			<p>In this <a id="_idIndexMarker305"/>section, we <a id="_idIndexMarker306"/>will talk about some best practices while architecting solutions using Amazon Textract for real-time workflows versus batch processing document workflows. </p>
			<p>Let's compare the Textract real-time APIs against the batch APIs we discussed in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, with the help of the following table:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="Images/B17528_04_09.jpg" alt="Figure 4.9 – Textract sync APIs versus batch APIs" width="1645" height="797"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – Textract sync APIs versus batch APIs</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The pricing of Textract is based on which of the three different APIs you are going to use out of Analyze Document (forms, table), Detect Text (text extraction), and Analyze Expense (invoices and receipts). You will not be charged irrespective of whether you use the sync<a id="_idIndexMarker307"/> or<a id="_idIndexMarker308"/> async (batch) implementation of these, so, feel free to design your architecture based on your need for real-time processing versus batch processing as pricing is based on the number of documents processed with one of the three APIs, irrespective of batch or real-time mode. Check prices here: <a href="https://aws.amazon.com/textract/pricing/">https://aws.amazon.com/textract/pricing/</a>.</p>
			<p>For example, LiveRight pvt Ltd<em class="italic">.</em> can use the batch or real-time implementation of the detect text API to detect text from their bank statements to process millions of documents. </p>
			<p>We covered architecture in <em class="italic">Figure 14.2</em>. This architecture implemented the Amazon Textract Detect Text Sync API in the code walk-through. Now, let's see how we can automate the architecture through Lambda functions for scale to process multiple documents:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="Images/B17528_04_10.jpg" alt="Figure 4.10 – Synchronous document processing workflow" width="1017" height="426"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – Synchronous document processing workflow</p>
			<p>In the preceding architecture, we walked you through how you can process scanned images using<a id="_idIndexMarker309"/> the <a id="_idIndexMarker310"/>proposed synchronous document processing workflow using the sync APIs of Amazon Textract. Here are the steps for this architecture:</p>
			<ul>
				<li>Documents uploaded to Amazon S3 will send a message to an Amazon SQS queue to analyze a document. Amazon SQS is a serverless managed queuing service that polls the documents into the queue.</li>
				<li>A Lambda function is invoked synchronously with an event that contains a queue message.</li>
				<li>The Lambda function then calls Amazon Textract sync APIs and stores the Textract output or response in either Amazon S3 or response metadata in the Amazon DynamoDB table. Amazon DynamoDB is a NoSQL database managed by AWS that is like a key/value store.</li>
			</ul>
			<p>You control the throughput of your pipeline by controlling the batch size and Lambda concurrency.</p>
			<p>Now we will walk you through the following architecture best practices for scaling multi-page scanned documents, which can be PDF or images using batch APIs of Amazon Textract:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="Images/B17528_04_11.jpg" alt="Figure 4.11 – Batch document processing workflow&#13;&#10;" width="1345" height="557"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 4.11 – Batch document processing workflow</p>
			<p>In the <a id="_idIndexMarker311"/>preceding<a id="_idIndexMarker312"/> diagram, we have an architecture to walk through how batch processing workflow works with Amazon Textract batch jobs:</p>
			<ul>
				<li>Multipage PDFs and images are uploaded in Amazon S3. These documents are sent to<a id="_idIndexMarker313"/> the <strong class="bold">Amazon Simple Queue Service</strong> (<strong class="bold">SQS</strong>) queue.</li>
				<li>A job scheduler Lambda function runs at a certain frequency, for example, every 5 minutes, and polls for messages in the SQS queue.</li>
				<li>For each message in the queue, it submits an Amazon Textract job to process the document and continues submitting these jobs until it reaches the maximum limit of concurrent jobs in your AWS account.</li>
				<li>As Amazon Textract finishes processing a document, it sends a completion notification<a id="_idIndexMarker314"/> to an <strong class="bold">Amazon Simple Notification Service</strong> (<strong class="bold">SNS</strong>) topic.</li>
				<li>SNS then triggers the job scheduler Lambda function to start the next set of Amazon Textract jobs.</li>
				<li>SNS also sends a message to an SQS queue, which is then processed by a Lambda function to get results from Amazon Textract. The results are then stored in a relevant dataset, for example, DynamoDB or Amazon S3.</li>
			</ul>
			<p>This GitHub link, <a href="https://github.com/aws-samples/amazon-textract-serverless-large-scale-document-processing">https://github.com/aws-samples/amazon-textract-serverless-large-scale-document-processing</a>, has code samples to implement both the suggested architecture and it also has some additional components to backfill in case the documents already exist in the Amazon S3 bucket. Please feel free to set up and use this if you have large documents to experiment with.</p>
			<p>You can also use the following GitHub solution, <a href="https://github.com/aws-samples/amazon-textract-textractor">https://github.com/aws-samples/amazon-textract-textractor</a>, to implement large-scale document processing with Amazon Comprehend insights.</p>
			<p>In this section, we covered architecture best practices for using real-time processing or batch processing with Amazon Textract. We also presented some already-existing GitHub<a id="_idIndexMarker315"/> implementations<a id="_idIndexMarker316"/> for large-scale document processing with Amazon Textract. Now, let's summarize what we have covered in this chapter.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor072"/>Summary</h1>
			<p>In this chapter, we covered how you can use Amazon Textract to automate your existing documents. We introduced a fictional bank use case with the help of <em class="italic">LiveRight Pvt Ltd</em>. We showed you how using an architecture can help banks automate their loan origination process and set up compliance and control with Amazon Comprehend. We also covered code samples using a sample bank statement, and how you can extract data from the scanned bank statement and save it into a <strong class="source-inline">CSV.text</strong> file in Amazon S3 for further analysis. Then, we showed you how you can use Amazon Comprehend to detect PII using a sync API and how you can redact that sample bank data text/CSV in Amazon S3 using an Amazon Comprehend batch PII redaction job.</p>
			<p>We then covered some architecture patterns for using real-time processing document workflows versus batch processing workflows. We also provided some GitHub implementations that can be used to process large-scale documents.</p>
			<p>In this chapter, you learned the differences between when to use and how to use real-time APIs versus batch APIs for document automation. You also learned how you can set up PII redaction with Amazon Comprehend PII jobs.</p>
			<p>In the next chapter, we will look at a different use case, but one that's equally popular among enterprises looking to leverage NLP to maximize their business value by building smart search indexes. We will cover how you can use Amazon Textract and Amazon Comprehend along with Amazon Elasticsearch and Amazon Kendra to create a quick NLP-based search. We will introduce the use case, discuss how to design the architecture, establish the prerequisites, and walk through in detail the various steps required to build the solution.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor073"/>Further reading</h1>
			<ul>
				<li><em class="italic">Building a serverless document scanner using Amazon Textract and AWS Amplify</em>, by Moheeb Zara (<a href="https://aws.amazon.com/blogs/compute/building-a-serverless-document-scanner-using-amazon-textract-and-aws-amplify/">https://aws.amazon.com/blogs/compute/building-a-serverless-document-scanner-using-amazon-textract-and-aws-amplify/</a>)</li>
				<li><em class="italic">Automatically extract text and structured data from documents with Amazon Textract</em>, by Kashif Imran and Martin Schade (<a href="https://aws.amazon.com/blogs/machine-learning/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract/">https://aws.amazon.com/blogs/machine-learning/automatically-extract-text-and-structured-data-from-documents-with-amazon-textract/</a>)</li>
			</ul>
		</div>
	</div></body></html>