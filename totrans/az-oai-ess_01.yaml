- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of technology, **Chat Generative Pre-trained Transformer** (**ChatGPT**)
    is a **large language model** (**LLM**)-based chatbot that was launched by OpenAI
    on November 30, 2022\. Just in ChatGPT’s first week, over a million people started
    using the technology. This is an important moment because it shows how regular
    people are now using generative **artificial intelligence** (**AI**) in their
    daily lives. By January 2023, ChatGPT had over 100 million users, making it the
    fastest-growing application in history and making OpenAI, the company behind it,
    worth $29 billion.
  prefs: []
  type: TYPE_NORMAL
- en: In this introductory chapter, we’ll establish the basic concepts behind LLMs,
    look at some examples, understand the concept of foundation models, and provide
    various business use cases where LLMs can be applied to solve complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are LLMs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of foundation models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are LLMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are a modern breakthrough in deep learning that focuses on human languages.
    They’ve shown themselves to be useful in many ways, such as content creation,
    customer support, coding assistance, education and tutoring, medical diagnosis,
    sentiment analysis, legal assistance, and more. Simply put, an LLM is a kind of
    smart computer program that can understand and create text like humans can by
    using large transformer models under the hood. The Transformer architecture enables
    models to understand context and relationships within data more effectively, making
    it particularly powerful for tasks involving human language and sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: For humans, text is a bunch of words put together. We read sentences, sentences
    make up paragraphs, and paragraphs make up chapters in a document. But for computers,
    text is just a series of letters and symbols. To make computers understand text,
    we can create a model using something called recurrent neural networks. This model
    goes through the text one word or character at a time and gives an answer when
    it finishes reading everything. This model is good, but sometimes, when it gets
    to the end of a block of text, it has trouble recalling the text from the beginning
    of that block. This is where the Transformer architecture shines. The key innovation
    of the Transformer architecture was its use of the self-attention mechanism, which
    allowed it to capture relationships between different parts of a sequence more
    effectively than previous models.
  prefs: []
  type: TYPE_NORMAL
- en: Back in 2017, Ashish Vaswani and their team wrote a paper called *Attention
    is All You Need* ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf))
    to introduce a new model called the Transformer. This model uses something called
    attention. Unlike the old way in which recurrent neural networks process text,
    attention lets you look at a whole sentence or even a whole paragraph all at once,
    instead of just one word at a time. This helps the transformer to better “understand”
    words as a result of the added context. Nowadays, many of the best LLMs are built
    on transformers.
  prefs: []
  type: TYPE_NORMAL
- en: When you want a Transformer model to understand a piece of text, you must break
    it down into separate words or parts called tokens. These tokens are then turned
    into numbers and mapped to special codes called embeddings, which are like special
    maps that store the semantic meaning of the tokens. Finally, the transformer’s
    encoder takes these embeddings and turns them into a representation. This “representation”
    is a vector that captures the contextual meaning of the input tokens, allowing
    the model to understand and process the input more effectively. In simple terms,
    you can think of it as putting all the pieces together to understand the whole
    story.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an example of a text string, its tokenization, and its vector embedding.
    Note that tokenization can turn words into subwords. For example, the word “generative”
    can be tokenized into “gener” and “ative.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the input text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the tokenized text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s look at the embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Think of the context vector as the heart of the input information. It enables
    the transformer’s decoder to determine what to say next. For example, by providing
    the decoder with a starting sentence as a hint, it can suggest the next word that
    makes sense. This process repeats, with each new suggestion becoming part of the
    hint, allowing the decoder to generate a naturally flowing paragraph from an initial
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder-based content generation is like a game, where each move is based on
    the previous one, and you end up with a complete story. This method of content
    generation is called “autoregressive generation.” Broadly speaking, this is how
    LLMs work. Autoregressive generation-based models can handle long input texts
    while also maintaining a context vector big enough to deal with complicated ideas.
    In addition to this, it has many layers in its decoder, making it highly sophisticated.
    It’s so big that it typically can’t run on just one computer and instead must
    run on a cluster of nodes working together that are accessed often. That’s why
    it’s offered as a service through an **application programming interface** (**API**).
    As you might have guessed, this enormous model is trained using a massive amount
    of text until it understands how language works, including all the patterns and
    structures of sentences. Now, let’s understand the main structure of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'An LLM’s structure (see *Figure 1**.1*) is mainly made up of different layers
    of neural networks, such as recurrent layers, feedforward layers, embedding layers,
    and attention layers. These layers collaborate to handle input text and make predictions
    about the output. Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The embedding layer** changes each word in the input text into a special
    kind of detailed description, kind of like a unique fingerprint. These descriptions
    hold crucial details about the words and their meanings, which helps the model
    understand the bigger picture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The feedforward layers** in LLMs consist of many connected layers that process
    the detailed descriptions created in the embedding layer. These layers perform
    complex transformations on these embeddings, which helps the model understand
    the more important ideas in the input text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The recurrent layers** in LLMs are designed to read the input text one step
    at a time. These layers have hidden memory that gets updated as they read each
    part of the text. This helps the model remember how the words are related to each
    other in a sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The attention mechanism** is another important part of LLMs. It’s like a
    spotlight where the model shines on different parts of the input text. This helps
    the model focus on the most important parts of the text and make better predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, when you read, you don’t pay equal attention to every word; instead,
    you focus more on keywords and important phrases to grasp the main idea. For instance,
    in the sentence “The cat sat on the mat,” you might emphasize “cat” and “mat”
    to understand what’s happening. Additionally, you use context from previous sentences
    to make sense of the current one – if you read about a cat playing earlier, you
    understand why the cat is now sitting on the mat. As you continue reading, you
    adjust your focus based on what’s important for comprehension, revisiting or paying
    more attention to crucial sections that help you understand the overall plot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In essence, just as humans read text by focusing on important words and using
    context to understand meaning, the attention mechanism in transformers focuses
    on key parts of the input and adjusts dynamically to capture the context and relationships
    between words:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.1: Transformer architecture (source: https://arxiv.org/pdf/1706.03762.pdf)](img/B21019_01_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Transformer architecture (source: [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned the basic concepts behind LLMs, let’s focus on some of
    the top industry examples.
  prefs: []
  type: TYPE_NORMAL
- en: LLM examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cutting-edge LLMs have been developed by many companies, including OpenAI (GPT-4),
    Meta (Llama 3.1), Anthropic (Claude), and Google (Gemini), to name a few. OpenAI
    has consistently maintained a dominant role in the field of LLMs. Let’s look at
    the top models that are used at the time of writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative Pre-trained Transformer (GPT)**: OpenAI has created various GPT
    models, including GPT1 (117 million parameters), GPT2 (1.5 billion parameters),
    GPT-3 (175 billion parameters), GPT 3.5, GPT4-Turbo, GPT4-o, and GPT4-o mini.
    GPT4-o is one of the most advanced LLMs globally. These models learn from a huge
    amount of text and can provide human-like answers to many subjects and questions.
    They also remember various parts of conversations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anthropic**: Anthropic’s Claude models are a family of advanced LLMs that
    are designed to handle complex tasks with high efficiency. The latest iteration,
    Claude 3, includes models such as Opus, Sonnet, and Haiku, each tailored for different
    performance needs. Opus is the most powerful, excelling in complex analysis and
    higher-order tasks, while Sonnet balances speed and intelligence, and Haiku offers
    the fastest response times for lightweight actions. These models are built with
    a focus on security, reliability, and ethical AI practices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Llama 3.1**: Llama 3.1 is a cutting-edge LLM that represents a significant
    milestone in AI research. With its advanced architecture and massive scale, Llama
    3.1 is capable of understanding and generating human-like text with unprecedented
    accuracy and nuance. This powerful tool has far-reaching implications for various
    applications, including **natural language processing** (**NLP**), text generation,
    and conversational AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Llama 2**: Llama 2 is a second-generation LLM developed by Meta. It’s open
    source and can be used to create chatbots such as ChatGPT or Google Bard. Llama
    2 was trained on 40% more data than Llama1 to make logical and natural-sounding
    responses. Llama 2 is available for anyone to use for research or business. Meta
    says that Llama 2 understands twice as much context as Llama 1\. This makes it
    a smarter language model that can give answers that sound just like what a human
    would provide.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gemini**: Google Gemini is a family of advanced multimodal LLMs developed
    by Google DeepMind. Announced on December 6, 2023, Gemini includes variants such
    as Gemini Ultra, Gemini Pro, Gemini Flash, and Gemini Nano. It’s designed to understand
    and operate across different types of information seamlessly, including text,
    images, audio, video, and code. Positioned as a competitor to OpenAI’s GPT-4,
    Gemini powers Google’s AI chatbot and aims to boost creativity and productivity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PaLM 2**: Finally, PaLM 2 is Google’s updated LLM. It’s skilled at handling
    complex tasks such as working with code and math, categorizing and answering questions,
    translating languages, being proficient in multiple languages, and creating human-like
    sentences. It outperforms the previously mentioned models, including the original
    PaLM. Google is careful about how it creates and uses AI, and PaLM 2 is a part
    of this approach. It went through thorough evaluations so that it could be checked
    for potential problems and biases. PaLM 2 is not just used in isolation but is
    also used in other advanced models, such as Med-PaLM 2 and Sec-PaLM. It’s also
    responsible for powering AI features and tools at Google, such as Bard and the
    PaLM API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The evolutionary tree (see *Figure 1**.2*) of modern LLMs illustrates how these
    models have evolved in recent years and highlights some of the most famous ones.
    Models that are closely related are shown on the same branches. Models that use
    the Transformer architecture are shown in different colors: those that only decode
    are on the blue branch, ones that only encode are on the pink branch, and models
    that do both encoding and decoding are on the green branch. The position of the
    models on the timeline shows when they were released. Open source models are represented
    by filled squares, while models that are not open source are represented by empty
    squares. The bar chart at the bottom right displays the number of models from
    different companies and organizations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2: The evolutionary tree of modern LLMs (source: https://arxiv.org/abs/2304.13712)](img/B21019_01_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: The evolutionary tree of modern LLMs (source: [https://arxiv.org/abs/2304.13712](https://arxiv.org/abs/2304.13712))'
  prefs: []
  type: TYPE_NORMAL
- en: Having explored some exemplary LLM instances, let’s discuss the concept of foundation
    models and their advantages and disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of foundation models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, there has been a huge buzz around LLMs such as ChatGPT sweeping
    across the world. LLMs are a subset of a broader category of models known as foundation
    models. Interestingly, the term “foundation models” was initially introduced by
    a team from Stanford. They observed a shift in the AI landscape, leading to the
    emergence of a new paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the past, AI applications were constructed by training individual AI models,
    each tailored to a specific task using specialized data. This approach often involved
    assembling a library of various AI models in a mostly supervised training manner.
    Its foundational capability, known as a foundation model (see *Figure 1**.3*),
    would become the driving force behind various applications and use cases. Essentially,
    this single model could cater to the very same applications that were once powered
    by distinct AI models in the traditional approach. This meant that a single model
    could fuel an array of diverse applications. The key here is that this model possesses
    the incredible ability to adapt to a multitude of tasks. What empowers this model
    to achieve such versatility is the fact that it has undergone extensive training
    on an immense volume of unstructured data through an unsupervised approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3: Foundational model (source: https://arxiv.org/pdf/2108.07258.pdf)](img/B21019_01_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Foundational model (source: [https://arxiv.org/pdf/2108.07258.pdf](https://arxiv.org/pdf/2108.07258.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a scenario where I start a sentence with “Don’t count your chickens
    before they’re.” Now, my goal is to guide the model in predicting the last word,
    which could be “hatched,” “grown,” or even “gone.” This process involves training
    the model to anticipate the appropriate word by analyzing the context provided
    by the words that come before it in the sentence. The impressive ability to generate
    predictions for the next word, while drawing on the context of preceding words
    it has encountered, positions foundation models within the realm of generative
    AI. In essence, these models fall under the category of generative AI because
    they’re capable of crafting something novel – in this case, predicting the upcoming
    word in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Although these models are primarily designed to generate predictions, particularly
    anticipating the next word in a sentence, they offer immense capabilities. With
    the addition of a modest amount of labeled data, we can adjust these models to
    perform exceptionally well on more traditional NLP tasks. These tasks include
    activities such as classification or named-entity recognition, which are typically
    not associated with generative capabilities. This transformation is achieved through
    a process known as fine-tuning. When you fine-tune your foundation model with
    a modest dataset, you adjust its parameters so that it can excel at a specific
    natural language task. This way, the model evolves from being primarily generative
    to being a powerful tool for targeted NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Even with limited data, foundation models can prove highly effective, especially
    in domains where data is scarce. Through a process known as prompting, or prompt
    engineering, techniques such as in-context learning, zero-shot, one-shot, and
    few-shot learning can be used to tackle complex downstream tasks. Let’s break
    down how you could prompt a model for a classification task. Imagine that you
    provide the model with a sentence and follow it up with the question, “Does this
    sentence carry a positive or negative sentiment?” The model would then work its
    magic, completing the sentence with generated words. The very next word it generates
    would serve as the answer to your classification question. Depending on where
    it perceives the sentiment of the sentence to lie, the model would respond with
    either “positive” or “negative.” This method leverages the model’s inherent ability
    to generate contextually relevant text to solve a specific classification challenge.
    We’ll cover different prompting techniques and advanced prompt engineering later
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s talk about some of the key advantages of foundation models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance**: These models have been trained on an immense amount of content
    with data volumes regularly in the terabyte range. When employed for smaller tasks,
    these models exhibit remarkable performance that far surpasses models trained
    on only a handful of data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Productivity gain**: LLMs can boost productivity in a big way. They’re like
    a super-efficient human for tasks that usually take a lot of time and effort.
    For instance, in customer service, LLMs can quickly answer common questions, freeing
    up human workers to handle more complex issues. In businesses, they can process
    and organize data way faster than people can. Using LLMs, companies can save time
    and money. This lets them focus on important tasks and thus acts like a turbocharger
    for productivity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, these foundation models also have key challenges and limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost**: These models tend to be quite costly to train because of the huge
    data volumes needed. This often poses challenges for smaller businesses attempting
    to train their foundation models. Additionally, as these models grow in size,
    reaching a scale of several billion parameters, they can become pricey to use
    for inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud providers such as Microsoft offer a service called **Azure OpenAI**. This
    service lets businesses use these models on-demand and pay only for what they
    use. This is similar to renting a powerful computer for a short time instead of
    buying one outright. Leveraging this service-based capability allows companies
    to save money on both model training and model use, especially considering the
    powerful, GPU-based hardware required.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To summarize, using services such as Azure OpenAI, businesses can take advantage
    of these advanced models without spending a ton on resources and infrastructure.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Trustworthiness**: Just as data serves as a massive advantage for these models,
    there’s a flip side to consider: LLMs are trained on vast amounts of internet-scraped
    language data that may contain biases, hate speech, or toxic content, compromising
    their reliability. This would be a monumental task. Furthermore, there’s the challenge
    of not even fully knowing what the data comprises. For many open source models,
    the exact datasets used for training many LLMs are unclear, making it difficult
    to assess their raising concerns about the models’ trustworthiness and potential
    biases. The sheer scale of LLM training data makes it nearly impossible for human
    annotators to thoroughly vet each data point, increasing the risk of unintended
    consequences such as perpetuating harmful biases or generating toxic content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big organizations are fully aware of the immense possibilities that these technologies
    hold. To solve the foundational model trustworthiness issue, OpenAI, Microsoft,
    Google, and Anthropic are jointly unveiling the creation of the Frontier Model
    Forum ([https://blogs.microsoft.com/on-the-issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-model-forum](https://blogs.microsoft.com/on-the-issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-model-forum)),
    a fresh industry initiative aimed at ensuring the safe and responsible advancement
    of frontier AI models. This new collaborative entity will tap into the collective
    technical and operational prowess of its member companies to foster progress across
    the broader AI landscape. One of its core objectives involves driving technical
    evaluations and benchmarks forward. Additionally, the forum will strive to construct
    a publicly accessible repository of solutions, bolstering the adoption of industry
    best practices and standards throughout the AI domain.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Hallucination**: Sometimes, LLMs can come up with information or answers
    that might not be entirely accurate. This is like when you have a dream that seems
    real, but it’s not based on what’s happening. LLMs might generate text that sounds
    right but isn’t completely true or accurate. So, while LLMs are highly intelligent,
    they can sometimes make mistakes or come up with content that isn’t real.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The applications of LLMs often require human oversight to make sure the outputs
    are trustworthy. However, there is a promising technique called **grounding the
    model** that aims to improve this situation. Grounding means connecting the LLM’s
    understanding with real-world information and context. This is like making sure
    the model is firmly rooted in reality. Later in this book, we’ll talk more about
    how to use this technique to stop the model from making things up and only give
    answers based on the given context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Limited context window**: LLMs have a limited context window or token size.
    A context window or token size can be seen as the amount of memory a model can
    process at a time. LLMs can only understand a certain number of pieces of information
    at once. For example, ChatGPT (GPT4-o) can handle 128K input tokens. This means
    that if you give it too much to read, it won’t be able to handle that and will
    throw errors. Therefore, it’s important to keep the input within this limit for
    the model to work well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following our understanding of the foundation model concept, let’s delve into
    some practical use cases of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring LLM use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LLMs have a wide range of use cases across various fields and industries due
    to their ability to understand and generate human-like text. Let’s take a look
    at some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Content generation**: LLMs can generate written content for blogs, articles,
    marketing materials, and social media posts. They can be used to automate content
    creation and come up with creative ideas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer support**: LLMs can provide automated responses to customer queries
    and support tickets, thus handling common questions and issues, freeing up human
    agents to handle more complex cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language translation**: LLMs can be employed to translate text between languages,
    making communication easier and more accessible on a global scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text summarization**: LLMs can quickly summarize lengthy texts, making it
    easier to grasp the main points of articles, reports, and other written materials.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chatbots and virtual assistants**: LLMs can power chatbots and virtual assistants
    that engage in natural language conversations, helping users with tasks, inquiries,
    and information retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content personalization**: LLMs can analyze user preferences and behavior
    to personalize recommendations, advertisements, and content delivery on platforms
    such as social media and streaming services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data entry and extraction**: LLMs can extract relevant information from unstructured
    text, such as documents or emails, and enter it into structured databases or spreadsheets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creative writing**: LLMs can assist writers by generating story ideas, dialogs,
    character descriptions, and even entire narratives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Healthcare chatbots**: LLM-powered chatbots can answer health-related questions,
    provide first-aid advice, and offer information about common medical conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical diagnostics**: LLMs can aid in diagnosing medical conditions by analyzing
    patient symptoms and medical records to provide potential diagnoses and treatment
    options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mental health support**: LLMs can provide empathetic responses and resources
    to individuals seeking support for mental health concerns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tourism and travel planning**: LLMs can assist travelers by suggesting itineraries,
    recommending attractions, and providing information about local customs and cuisines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recipe creation**: LLMs can devise creative recipes based on ingredients
    and dietary preferences, offering new culinary experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cybersecurity analysis**: LLMs can analyze cybersecurity threats and suggest
    strategies for protecting digital systems and data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fashion recommendations**: LLMs can suggest clothing and accessory combinations
    based on personal style preferences and current fashion trends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legal document review**: LLMs can review legal documents, contracts, and
    case histories to identify relevant information, anomalies, and potential issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Academic research**: LLMs can assist researchers by providing summaries of
    academic papers, helping with literature reviews, and generating ideas for further
    study.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Financial analysis**: LLMs can process and analyze financial data, generate
    reports, and provide insights into market trends and investment opportunities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language learning**: LLMs can help learners practice and improve their language
    skills by engaging in conversations, providing explanations, and offering exercises.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accessibility tools**: LLMs can be used to create audio descriptions for
    visually impaired individuals, generate subtitles for videos, and convert text
    into speech for people with reading difficulties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While these are just a few example use cases, the limitless flexibility of LLMs
    allows them to also be applied to even more situations as technology progresses.
  prefs: []
  type: TYPE_NORMAL
- en: People who want to create generative AI applications without the burden of training
    an LLM themselves or spending money on costly hardware can use the Azure OpenAI
    API. This lets them use advanced LLMs made by OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve covered the basics of LLMs. Moving forward, the next chapter will
    focus on the Azure OpenAI service in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started by introducing LLMs and how they’re influenced by
    Transformer networks. Then, we explored the various parts that make up LLMs. Next,
    we dove into some of the top LLM models created by OpenAI, Meta, and Google, discussing
    how these models have evolved. We also covered the concept of foundation models,
    including their advantages and limitations. Lastly, we looked at various business
    applications where LLMs have shown great potential.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward to the next chapter, our focus will be on Azure OpenAI service.
    We’ll learn how to access this service, including models such as GPT 3.5, GPT-4,
    Embeddings, and DALL.E 2\. We’ll also explain how the pricing works, discussing
    options such as pay-as-you-go and reserved capacity.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*AI Explainer*: *Foundation models and the next era of* *AI* ([https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/](https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Orca:* *Progressive Learning from Complex Explanation Traces of* *GPT-4* ([https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/](https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Florence:* *A New Foundation Model for Computer* *Vision* ([https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/](https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Accelerating Foundation Models* *Research* ([https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/](https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attention Is All You* *Need* ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*On the Opportunities and Risks of Foundation* *Models* ([https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
