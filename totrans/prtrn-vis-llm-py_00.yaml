- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, you want to work with foundation models? That is an excellent place to begin!
    Many of us in the machine learning community have followed these curious creatures
    for years, from their earliest onset in the first days of the Transformer models,
    to their expansion in computer vision, to the near ubiquitous presence of text
    generation and interactive dialogue we see in the world today.
  prefs: []
  type: TYPE_NORMAL
- en: But where do foundation models come from? How do they work? What makes them
    tick, and when should you pretrain and fine-tune them? How can you eke out performance
    gains on your datasets and applications? How many accelerators do you need? What
    does an end-to-end application look like, and how can you use foundation models
    to master this new surge of interest in generative AI?
  prefs: []
  type: TYPE_NORMAL
- en: These pages hope to provide answers to these very important questions. As you
    are no doubt aware, the pace of innovation in this space is truly breathtaking,
    with more foundation models coming online every day from both open-source and
    proprietary model vendors. To grapple with this reality, I’ve tried to focus on
    the most important conceptual fundamentals throughout the book. This means your
    careful study here should pay off for at least a few more years ahead.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of practical applications and guidance, I’ve overwhelmingly focused
    on cloud computing options available through AWS and especially Amazon SageMaker.
    I’ve spent more than the last five years very happily at AWS and enjoy sharing
    all of my knowledge and experience with you! Please do note that all thoughts
    and opinions shared in this book are my own, and do not represent those of Amazon’s.
  prefs: []
  type: TYPE_NORMAL
- en: The following chapters focus on concepts, not code. This is because software
    changes rapidly, while fundamentals change very slowly. You’ll find in the repository
    with the book links to my go-to resources for all of the key topics mentioned
    throughout these fifteen chapters, which you can use right away to get hands-on
    with everything you’re learning here. Starting July 1, 2023, you’ll also find
    in the repository a set of new pretraining and fine-tuning examples from yours
    truly to complete all of the topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might find this hard to believe, but in my early twenties I wasn’t actually
    coding: I was exploring the life of a Buddhist monastic. I spent five years living
    at a meditation retreat center in Arizona, the Garchen Institute. During this
    time, I learned how to meditate, focus my mind, watch my emotions and develop
    virtuous habits. After my master’s degree at the University of Chicago years later,
    and now at Amazon, I can see that these traits are extremely useful in today’s
    world as well!'
  prefs: []
  type: TYPE_NORMAL
- en: I mention this so that you can take heart. Machine learning, artificial intelligence,
    cloud computing, economics, application development, none of these topics are
    straightforward. But if you apply yourself, if you really stretch your mind to
    consider the core foundations of the topics at hand, if you keep yourself coming
    back to the challenge again and again, there’s truly nothing you can’t do. That
    is the beauty of humanity! And if a meditating yogi straight from the deep silence
    of a retreat hut can eventually learn what it takes to pretrain and fine-tune
    foundation models, then so can you!
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, let’s learn more about the book itself!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Most of the concepts mentioned here will be accompanied by scripting examples
    in the repository starting July 1, 2023\. However, to get you started even earlier,
    you can find a list of resources in the repository today with links to useful
    hands-on examples elsewhere for demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: Who is this book for?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re a machine learning researcher or enthusiast who wants to start a foundation
    modelling project, this book is for you. Applied scientists, data scientists,
    machine learning engineers, solution architects, product managers, and students
    will all benefit from this book. Intermediate Python is a must, along with introductory
    concepts of cloud computing. A strong understanding of deep learning fundamentals
    is needed, while advanced topics will be explained. The content covers advanced
    machine learning and cloud techniques, explaining them in an actionable, easy-to-understand
    way.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 1*](B18942_01.xhtml#_idTextAnchor016)*, An Introduction to Pretraining
    Foundation Models* In this chapter you’ll be introduced to foundation models,
    the backbone of many artificial intelligence and machine learning systems today.
    We will dive into their creation process, also called pretraining, and understand
    where it’s competitive to improve the accuracy of your models. We will discuss
    the core transformer architecture underpinning state of the art models like Stable
    Diffusion, BERT, Vision Transformers, CLIP, Flan-T5 and more. You will learn about
    the encoder and decoder frameworks that work to solve a variety of use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B18942_02.xhtml#_idTextAnchor034)*, Dataset Preparation: Part
    One* In this chapter, we begin to discuss what you’ll need in your dataset to
    start a meaningful pretraining project. This is the first of two parts on dataset
    preparation. It opens with some business guidance on finding a good use case for
    foundation modeling, where the data become instrumental. Then, focusing on the
    content of your dataset, we use qualitative and quantitative measures to compare
    it with datasets used in pretraining other top models. You’ll learn how to use
    the scaling laws to determine if your datasets are “large enough” and “good enough”
    to boost accuracy while pretraining. We discuss bias identification and mitigation,
    along with multilingual and multimodal solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B18942_03.xhtml#_idTextAnchor050)*, Model Preparation* In this
    chapter you’ll learn how to pick which model will be most useful to serve as a
    basis for your pretraining regime. You’ll learn how to think about the size of
    the model in parameters, along with the key loss functions and how they determine
    performance in production. You’ll combine the scaling laws with your expected
    dataset size to select ceiling and basement model sizes, which you’ll use to guide
    your experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B18942_04.xhtml#_idTextAnchor066)*, Containers and Accelerators
    on the Cloud* In this chapter, you’ll learn how to containerize your scripts and
    optimize them for accelerators on the cloud. We’ll learn about a range of accelerators
    for foundation models, including tradeoffs around cost and performance across
    the entire machine learning lifecycle. You’ll learn key aspects of Amazon SageMaker
    and AWS to train models on accelerators, optimize performance, and troubleshoot
    common issues. If you’re already familiar with using accelerators on AWS, feel
    free to skip this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B18942_05.xhtml#_idTextAnchor085)*, Distribution Fundamentals*
    In this chapter, you’ll learn conceptual fundamentals for the distribution techniques
    you need to employ for large scale pretraining and fine-tuning. First, you’ll
    master top distribution concepts for machine learning, notably model and data
    parallel. Then, you’ll learn how Amazon SageMaker integrates with distribution
    software to run your job on as many GPUs as you need. You’ll learn how to optimize
    model and data parallel for large-scale training especially with techniques like
    sharded data parallelism. Then, you’ll learn how to reduce your memory consumption
    with advanced techniques like optimizer state sharding, activation checkpointing,
    compilation, and more. Lastly, we’ll look at a few examples across language, vision,
    and more to bring all of these concepts together.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18942_06.xhtml#_idTextAnchor106)*, Dataset Preparation: Part
    Two, the Data Loader* In this chapter, you’ll learn how to prepare your dataset
    to immediately use with your chosen models. You’ll master the concept of a data
    loader, knowing why it’s a common source of error in training large models. You’ll
    learn about creating embeddings, using tokenizers and other methods to featurize
    your raw data for your preferred neural network. Following these steps, you’ll
    be able to prepare your entire dataset, using methods for both vision and language.
    Finally, you’ll learn about data optimizations on AWS and Amazon SageMaker to
    efficiently send datasets large and small to your training cluster. Throughout
    this chapter we’ll work backwards from the training loop; giving you incrementally
    all the steps you need to have functional deep neural networks training at scale.
    You’ll also follow a case study from how I trained on 10TB for Stable Diffusion
    on SageMaker!'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B18942_07.xhtml#_idTextAnchor116)*, Finding the Right Hyperparameters*
    In this chapter, you’ll dive into the key hyperparameters that govern performance
    for top vision and language models, such as batch size, learning rate, and more.
    First, we’ll start with a quick overview of hyperparameter tuning for those who
    are new or new a light refresh, including key examples in vision and language.
    Then we’ll explore hyperparameter tuning in foundation models, both what is possible
    today and where trends might emerge. Finally, we’ll learn how to do this on Amazon
    SageMaker, taking incremental steps in a cluster size and changing each hyperparameter
    as we do.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18942_08.xhtml#_idTextAnchor127)*, Large-Scale Training on SageMaker*
    In this chapter, we cover key features and functionality available with Amazon
    SageMaker for running highly optimized distributed training. You’ll learn how
    to optimize your script for SageMaker training, along with key usability features.
    You’ll also learn about backend optimizations for distributed training with SageMaker,
    like GPU health checks, resilient training, checkpointing, script mode, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18942_09.xhtml#_idTextAnchor138)*, Advanced Training Concepts*
    In this chapter, we will cover advanced training concepts at scale, like evaluating
    throughput, calculating model TFLOPS per device, compilation, and using the scaling
    laws to determine the right length of training time. In the last chapter you learned
    about how to do large-scale training on SageMaker generally speaking. In this
    chapter you’ll learn about particularly complex and sophisticated techniques you
    can use to drive down the overall cost of your job. This lower cost directly translates
    to higher model performance, because it means you can train for longer on the
    same budget.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18942_10.xhtml#_idTextAnchor152)*, Fine-Tuning and Evaluating*
    In this chapter, you’ll learn how to fine-tune your model on use case-specific
    datasets, comparing its performance to that of off-the-shelf public models. You
    should be able to see quantitative and qualitative boost from your pretraining
    regime. You’ll dive into some examples from language, text, and everything in-between.
    You’ll also learn how to think about and design a human-in-the-loop evaluation
    system, including the same RLHF that makes ChatGPT tick! This chapter focuses
    on *updating the trainable weights of the model*. For techniques that mimic learning
    but don’t update the weights, such as prompt tuning and standard retrieval augmented
    generation, see [*Chapter 13*](B18942_13.xhtml#_idTextAnchor198) on prompt engineering
    or [*Chapter 15*](B18942_15.xhtml#_idTextAnchor229) on future trends.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B18942_11.xhtml#_idTextAnchor167)*, Detecting, Mitigating, and
    Monitoring Bias* In this chapter, we’ll analyze leading bias identification and
    mitigation strategies for large vision, language, and multimodal models. You’ll
    learn about the concept of bias, both in a statistical sense and how it impacts
    human beings in critical ways. You’ll understand key ways to quantify and remedy
    this in vision and language models, eventually landing on monitoring strategies
    that enable you to reduce any and all forms of harm when applying your foundation
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B18942_12.xhtml#_idTextAnchor178)*, How to Deploy Your Model*
    In this chapter, we’ll introduce you to a variety of techniques for deploying
    your model, including real-time endpoints, serverless, batch options and more.
    These concepts apply to many compute environments, but we’ll focus on capabilities
    available on AWS within Amazon SageMaker. We’ll talk about why you should try
    to shrink the size of your model before deploying, along with techniques for this
    across vision and language. We’ll also cover distributed hosting techniques, for
    scenarios when you can’t or don’t need to shrink your model. Lastly, we’ll explore
    model serving techniques and concepts that can help you optimize the end-to-end
    performance of your model.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B18942_13.xhtml#_idTextAnchor198)*, Prompt Engineering* In this
    chapter, we’ll dive into a special set of techniques called prompt engineering.
    You’ll learn about this technique at a high level, including how it is similar
    to and different from other learning-based topics throughout this book. We’ll
    explore examples across vision and language, and dive into key terms and success
    metrics. In particular this chapter covers all of the tips and tricks for improving
    performance without updating the model weights. This means we’ll be mimicking
    the learning process, without necessarily changing any of the model parameters.
    This includes some advanced techniques like prompt and prefix tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B18942_14.xhtml#_idTextAnchor217)*, MLOps for Vision and Language*
    In this chapter, we’ll introduce core concepts of operations and orchestration
    for machine learning, also known as MLOps. This includes building pipelines, continuous
    integration and deployment, promotion through environments, and more. We’ll explore
    options for monitoring and human-in-the-loop auditing of model predictions. We’ll
    also identify unique ways to support large vision and language models in your
    MLOps pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B18942_15.xhtml#_idTextAnchor229)*, Future Trends in Pretraining
    Foundation Models* In this chapter, we’ll close out the book by pointing to where
    trends are headed for all relevant topics presented in this book. We’ll explore
    trends in foundation model application development, like using LangChain to build
    interactive dialogue applications, along with techniques like retrieval augmented
    generation to reduce LLM hallucination. We’ll explore ways to use generative models
    to solve classification tasks, human-centered design, and other generative modalities
    like code, music, product documentation, PowerPoints, and more! We’ll talk through
    AWS offerings like SageMaker JumpStart Foundation Models, Amazon Bedrock, Amazon
    Titan, and Amazon Code Whisperer, and top trends in the future of foundation models
    and pretraining itself.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, you want to be very happy in Python development to absolutely
    maximize your time in this book. The pages don’t spend a lot of time focusing
    on the software, but again, everything in the GitHub repository is Python. If
    you’re already using a few key AWS services, like Amazon SageMaker, S3 buckets,
    ECR images, and FSx for Lustre, that will speed you up tremendously in applying
    what you’ve learned here. If you’re new to these, that’s ok, we’ll include introductions
    to each of these.
  prefs: []
  type: TYPE_NORMAL
- en: '| **AWS Service or Open-source** **software framework** | **What we’re using**
    **it for** |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon SageMaker | Studio, notebook instances, training jobs, endpoints,
    pipelines |'
  prefs: []
  type: TYPE_TB
- en: '| S3 buckets | Storing objects and retrieving metadata |'
  prefs: []
  type: TYPE_TB
- en: '| Elastic Container Registry | Storing Docker images |'
  prefs: []
  type: TYPE_TB
- en: '| FSx for Lustre | Storing large-scale data for model training loops |'
  prefs: []
  type: TYPE_TB
- en: '| Python | General scripting: including managing and interacting with services,
    importing other packages, cleaning your data, defining your model training and
    evaluation loops, etc |'
  prefs: []
  type: TYPE_TB
- en: '| PyTorch and TensorFlow | Deep learning frameworks to define your neural networks
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hugging Face | Hub with more than 100,000 open-source pretrained models and
    countless extremely useful and reliable methods for NLP and increasingly CV |'
  prefs: []
  type: TYPE_TB
- en: '| Pandas | Go-to library for data analysis |'
  prefs: []
  type: TYPE_TB
- en: '| Docker | Open-source framework for building and managing containers |'
  prefs: []
  type: TYPE_TB
- en: '**If you are using the digital version of this book, we advise you to access
    the code from the book’s GitHub repository (a link is available in the next section),
    step through the examples, and type the code yourself. Doing so will help you
    avoid any potential errors related to the copying and pasting** **of code.**'
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Pretrain-Vision-and-Large-Language-Models-in-Python](https://github.com/PacktPublishing/Pretrain-Vision-and-Large-Language-Models-in-Python).
    If there’s an update to the code, it will be updated in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: “Mount the downloaded `WebStorm-10*.dmg` disk image
    file as another disk in your system.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold**. Here is an example:
    “Select **System info** from the **Administration** panel.”'
  prefs: []
  type: TYPE_NORMAL
- en: Tips or important notes
  prefs: []
  type: TYPE_NORMAL
- en: Appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback**: If you have questions about any aspect of this book,
    email us at [customercare@packtpub.com](http://customercare@packtpub.com) and
    mention the book title in the subject of your message.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    and fill in the form.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at [copyright@packt.com](http://copyright@packt.com)
    with a link to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Share Your Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you’ve read Pretrain Vision and Large Language Models in Python, we’d love
    to hear your thoughts! [Please click here to go straight to the Amazon review
    page for this book](https://packt.link/r/1-804-61825-X) and share your feedback.
  prefs: []
  type: TYPE_NORMAL
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  prefs: []
  type: TYPE_NORMAL
- en: Download a free PDF copy of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks for purchasing this book!
  prefs: []
  type: TYPE_NORMAL
- en: Do you like to read on the go but are unable to carry your print books everywhere?
  prefs: []
  type: TYPE_NORMAL
- en: Is your eBook purchase not compatible with the device of your choice?
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry, now with every Packt book you get a DRM-free PDF version of that
    book at no cost.
  prefs: []
  type: TYPE_NORMAL
- en: Read anywhere, any place, on any device. Search, copy, and paste code from your
    favorite technical books directly into your application.
  prefs: []
  type: TYPE_NORMAL
- en: The perks don’t stop there, you can get exclusive access to discounts, newsletters,
    and great free content in your inbox daily
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these simple steps to get the benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Scan the QR code or visit the link below
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18942_QR_Free_PDF.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[https://packt.link/free-ebook/9781804618257](https://packt.link/free-ebook/9781804618257)'
  prefs: []
  type: TYPE_NORMAL
- en: Submit your proof of purchase
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s it! We’ll send your free PDF and other benefits to your email directly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Part 1: Before Pretraining'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In part 1, you’ll learn how to get ready to pretrain a large vision and/or language
    model, including dataset and model preparation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 1*](B18942_01.xhtml#_idTextAnchor016), *An Introduction to Pretraining
    Foundation Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B18942_02.xhtml#_idTextAnchor034), *Dataset Preparation: Part
    One*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B18942_03.xhtml#_idTextAnchor050), *Model Preparation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
