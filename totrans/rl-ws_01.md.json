["```py\n    from typing import Tuple\n    ```", "```py\n    class Environment:\n        def __init__(self):\n            \"\"\"\n            Constructor of the Environment class.\n            \"\"\"\n            self._initial_state = 1\n            self._allowed_actions = [0, 1]  # 0: A, 1: B\n            self._states = [1, 2, 3]\n            self._current_state = self._initial_state\n    current_state variable to be equal to the initial state (state 1).\n    ```", "```py\n        def step(self, action: int) -> Tuple[int, int]:\n            \"\"\"\n            Step function: compute the one-step dynamic from the \\\n            given action.\n            Args:\n                action (int): the action taken by the agent.\n            Returns:\n                The tuple current_state, reward.\n            \"\"\"\n            # check if the action is allowed\n            if action not in self._allowed_actions:\n                raise ValueError(\"Action is not allowed\")\n            reward = 0\n            if action == 0 and self._current_state == 1:\n                self._current_state = 2\n                reward = 1\n            elif action == 1 and self._current_state == 1:\n                self._current_state = 3\n                reward = 10\n            elif action == 0 and self._current_state == 2:\n                self._current_state = 1\n                reward = 0\n            elif action == 1 and self._current_state == 2:\n                self._current_state = 3\n                reward = 1\n            elif action == 0 and self._current_state == 3:\n                self._current_state = 2\n                reward = 0\n            elif action == 1 and self._current_state == 3:\n                self._current_state = 3\n                reward = 10\n            return self._current_state, reward\n    ```", "```py\n        def reset(self) -> int:\n            \"\"\"\n            Reset the environment starting from the initial state.\n            Returns:\n                The environment state after reset (initial state).\n            \"\"\"\n            self._current_state = self._initial_state\n            return self._current_state\n    ```", "```py\n    env = Environment()\n    state = env.reset()\n    actions = [0, 0, 1, 1, 0, 1]\n    print(f\"Initial state is {state}\")\n    for action in actions:\n        next_state, reward = env.step(action)\n        print(f\"From state {state} to state {next_state} \\\n    with action {action}, reward: {reward}\")\n        state = next_state\n    Initial state is 1\n    From state 1 to state 2 with action 0, reward: 1\n    From state 2 to state 1 with action 0, reward: 0\n    From state 1 to state 3 with action 1, reward: 10\n    From state 3 to state 3 with action 1, reward: 10\n    From state 3 to state 2 with action 0, reward: 0\n    From state 2 to state 3 with action 1, reward: 1\n    ```", "```py\n    from typing import Callable, List\n    import matplotlib\n    from matplotlib import pyplot as plt\n    import numpy as np\n    import scipy.stats\n    ```", "```py\n    class LinearPolicy:\n        def __init__(\n            self, parameters: np.ndarray, \\\n            features: Callable[[np.ndarray], np.ndarray]):\n            \"\"\"\n            Linear Policy Constructor.\n            Args:\n                parameters (np.ndarray): policy parameters \n                as np.ndarray.\n                features (Callable[[np.ndarray], np.ndarray]): \n                function used to extract features from the \n                state representation.\n            \"\"\"\n            self._parameters = parameters\n            self._features = features\n    ```", "```py\n        def __call__(self, state: np.ndarray) -> np.ndarray:\n            \"\"\"\n            Call method of the Policy.\n            Args:\n                state (np.ndarray): environment state.\n            Returns:\n                The resulting action.\n            \"\"\"\n            # calculate state features\n            state_features = self._features(state)\n            \"\"\"\n            the parameters shape [0] should be the same as the \n            state features as they must be multiplied\n            \"\"\"\n            assert state_features.shape[0] == self._parameters.shape[0]\n            # dot product between parameters and state features\n            return np.dot(self._parameters.T, state_features)\n    ```", "```py\n    # sample a random set of parameters\n    parameters = np.random.rand(5, 1)\n    # define the state features as identity function\n    features = lambda x: x\n    # define the policy\n    pi: LinearPolicy = LinearPolicy(parameters, features)\n    # sample a state\n    state = np.random.rand(5, 1)\n    # Call the policy obtaining the action\n    action = pi(state)\n    print(action)\n    ```", "```py\n    [[1.33244481]]\n    ```", "```py\n# Import the gym Library\nimport gym\n# Create the environment using gym.make(env_name)\nenv = gym.make('CartPole-v1')\n\"\"\"\nAnalyze the action space of cart pole using the property action_space\n\"\"\"\nprint(\"Action Space:\", env.action_space)\n\"\"\"\nAnalyze the observation space of cartpole using the property observation_space\n\"\"\"\nprint(\"Observation Space:\", env.observation_space)\n```", "```py\nAction Space: Discrete(2)\nObservation Space: Box(4,)\n```", "```py\n# Analyze the bounds of the observation space\nprint(\"Lower bound of the Observation Space:\", \\\n      env.observation_space.low)\nprint(\"Upper bound of the Observation Space:\", \\\n      env.observation_space.high)\n```", "```py\nLower bound of the Observation Space: [-4.8000002e+00 -3.4028235e+38 \n-4.1887903e-01 -3.4028235e+38]\nUpper bound of the Observation Space: [4.8000002e+00 3.4028235e+38 \n4.1887903e-01 3.4028235e+38]\n```", "```py\n# Type hinting\nfrom typing import Tuple\nimport gym\n# Import the spaces module\nfrom gym import spaces\n# Create a discrete space composed by N-elements (5)\nn: int = 5\ndiscrete_space = spaces.Discrete(n=n)\n# Sample from the space using .sample method\nprint(\"Discrete Space Sample:\", discrete_space.sample())\n\"\"\"\nCreate a Box space with a shape of (4, 4)\nUpper and lower Bound are 0 and 1\n\"\"\"\nbox_shape: Tuple[int, int] = (4, 4)\nbox_space = spaces.Box(low=0, high=1, shape=box_shape)\n# Sample from the space using .sample method\nprint(\"Box Space Sample:\", box_space.sample())\n```", "```py\nDiscrete Space Sample: 4\nBox Space Sample: [[0.09071387 0.4223234  0.09272052 0.15551752]\n [0.8507258  0.28962377 0.98583364 0.55963445]\n [0.4308358  0.8658449  0.6882108  0.9076272 ]\n [0.9877584  0.7523759  0.96407163 0.630859  ]]\n```", "```py\n# Seed spaces to obtain reproducible samples\ndiscrete_space.seed(0)\nbox_space.seed(0)\n# Sample from the seeded space\nprint(\"Discrete Space (seed=0) Sample:\", discrete_space.sample())\n# Sample from the seeded space\nprint(\"Box Space (seed=0) Sample:\", box_space.sample())\n```", "```py\nDiscrete Space (seed=0) Sample: 0\nBox Space (seed=0) Sample: [[0.05436005 0.9653909  \n0.63269097 0.29001734]\n [0.10248426 0.67307633 0.39257675 0.66984606]\n [0.05983897 0.52698725 0.04029069 0.9779441 ]\n 0.46293673 0.6296479  0.9470484  0.6992778 ]]\n```", "```py\n    import gym\n    from gym import spaces\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    import numpy as np # used for the dtype of the space\n    ```", "```py\n    \"\"\"\n    since the Space is RGB images with shape 256x256 the final shape is (256, 256, 3)\n    \"\"\"\n    shape = (256, 256, 3)\n    # If we consider uint8 images the bounds are 0-255\n    low = 0\n    high = 255\n    # Space type: unsigned int\n    dtype = np.uint8\n    ```", "```py\n    # create the space\n    space = spaces.Box(low=low, high=high, shape=shape, dtype=dtype)\n    # Print space representation\n    print(\"Space\", space)\n    ```", "```py\n    Space Box(256, 256, 3)\n    ```", "```py\n    # Sample from the space\n    sample = space.sample()\n    print(\"Space Sample\", sample)\n    ```", "```py\n    Space Sample [[[ 37 254 243]\n      [134 179  12]\n      [238  32   0]\n      ...\n      [100  61  73]\n      [103 164 131]\n      [166  31  68]]\n     [[218 109 213]\n      [190  22 130]\n      [ 56 235 167]\n    ```", "```py\n    plt.imshow(sample)\n    ```", "```py\n    # we want a space representing the last n=4 frames\n    n_frames = 4  # number of frames\n    width = 256  # image width\n    height = 256  # image height\n    channels = 3  # number of channels (RGB)\n    shape_temporal = (n_frames, width, height, channels)\n    # create a new instance of space\n    space_temporal = spaces.Box(low=low, high=high, \\\n                                shape=shape_temporal, dtype=dtype)\n    print(\"Space with temporal component\", space_temporal)\n    ```", "```py\n    Space with temporal component Box(4, 256, 256, 3)\n    ```", "```py\n# Create the environment using gym.make(env_name)\nenv = gym.make(\"CartPole-v1\")\n# reset the environment (mandatory)\nenv.reset()\n# render the environment for 100 steps\nn_steps = 100\nfor i in range(n_steps):\n    action = env.action_space.sample()\n    env.step(action)\n    env.render()\n# close the environment correctly\nenv.close()\n```", "```py\n    import gym\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    env = gym.make(\"CartPole-v1\")\n    # each episode is composed by 100 timesteps\n    # define 10 episodes\n    n_episodes = 10\n    n_timesteps = 100\n    ```", "```py\n    # loop for the episodes\n    for episode_number in range(n_episodes):\n        # here we are inside an episode\n    ```", "```py\n        \"\"\"\n        the reset function resets the environment and returns\n        the first environment observation\n        \"\"\"\n        observation = env.reset()\n    ```", "```py\n        \"\"\"\n        loop for the given number of timesteps or\n        until the episode is terminated\n        \"\"\"\n        for timestep_number in range(n_timesteps):\n    ```", "```py\n            # render the environment\n            env.render(mode=\"rgb-array\")\n            # select the action\n            action = env.action_space.sample()\n            # apply the selected action by calling env.step\n            observation, reward, done, info = env.step(action)\n    ```", "```py\n            \"\"\"if done the episode is terminated, we have to reset\n            the environment\n            \"\"\"\n            if done:\n                print(f\"Episode Number: {episode_number}, \\\n    Timesteps: {timestep_number}\")\n                # break from the timestep loop\n                break\n    ```", "```py\n    # close the environment\n    env.close()\n    ```", "```py\n    Episode Number: 0, Timesteps: 34\n    Episode Number: 1, Timesteps: 10\n    Episode Number: 2, Timesteps: 12\n    Episode Number: 3, Timesteps: 21\n    Episode Number: 4, Timesteps: 16\n    Episode Number: 5, Timesteps: 17\n    Episode Number: 6, Timesteps: 12\n    Episode Number: 7, Timesteps: 15\n    Episode Number: 8, Timesteps: 16\n    Episode Number: 9, Timesteps: 16\n    ```", "```py\n    Episode Number: 0, Timesteps: 27, Return: 28.0\n    Episode Number: 1, Timesteps: 9, Return: 10.0\n    Episode Number: 2, Timesteps: 13, Return: 14.0\n    Episode Number: 3, Timesteps: 16, Return: 17.0\n    Episode Number: 4, Timesteps: 31, Return: 32.0\n    Episode Number: 5, Timesteps: 10, Return: 11.0\n    Episode Number: 6, Timesteps: 14, Return: 15.0\n    Episode Number: 7, Timesteps: 11, Return: 12.0\n    Episode Number: 8, Timesteps: 10, Return: 11.0\n    Episode Number: 9, Timesteps: 30, Return: 31.0\n    Statistics on Return: Average: 18.1, Variance: 68.89000000000001\n    ```", "```py\n# Train model and save the results to cartpole_model.pkl\npython -m baselines.run –alg=deepq –env=CartPole-v0 –save_path=./cartpole_model.pkl –num_timesteps=1e5\n```", "```py\n# Load the model saved in cartpole_model.pkl \n# and visualize the learned policy\npython -m baselines.run --alg=deepq --env=CartPole-v0 --load_path=./cartpole_model.pkl --num_timesteps=0 --play\n```", "```py\n    import gym\n    # Import the desired algorithm from baselines\n    from baselines import deepq\n    ```", "```py\n    def callback(locals, globals):\n        \"\"\"\n        function called at every step with state of the algorithm.\n        If callback returns true training stops.\n        stop training if average reward exceeds 199\n        time should be greater than 100 and the average of \n        last 100 returns should be >= 199\n        \"\"\"\n        is_solved = (locals[\"t\"] > 100 and \\\n                     sum(locals[\"episode_rewards\"]\\\n                               [-101:-1]) / 100 >= 199)\n        return is_solved\n    ```", "```py\n    # create the environment\n    env = gym.make(\"CartPole-v0\")\n    \"\"\"\n    Prepare learning parameters: network and learning rate\n    the policy is a multi-layer perceptron\n    \"\"\"\n    network = \"mlp\"\n    # set learning rate of the algorithm\n    learning_rate = 1e-3\n    ```", "```py\n    \"\"\"\n    launch learning on this environment using DQN\n    ignore the exploration parameter for now\n    \"\"\"\n    actor = deepq.learn(env, network=network, lr=learning_rate, \\\n                        total_timesteps=100000, buffer_size=50000, \\\n                        exploration_fraction=0.1, \\\n                        exploration_final_eps=0.02, print_freq=10, \\\n                        callback=callback,)\n    ```", "```py\n--------------------------------------\n| % time spent exploring  | 2        |\n| episodes                | 770      |\n| mean 100 episode reward | 145      |\n| steps                   | 6.49e+04 |\n```", "```py\nprint(\"Saving model to cartpole_model.pkl\")\nactor.save(\"cartpole_model.pkl\")\n```", "```py\n# Visualize the policy\nn_episodes = 5\nn_timesteps = 1000\nfor episode in range(n_episodes):\n    observation = env.reset()\n    episode_return = 0\n    for timestep in range(n_timesteps):\n        # render the environment\n        env.render()\n        # select the action according to the actor\n        action = actor(observation[None])[0]\n        # call env.step function\n        observation, reward, done, _ = env.step(action)\n        \"\"\"\n        since the reward is undiscounted we can simply add \n        the reward to the cumulated return\n        \"\"\"\n        episode_return += reward\n        if done:\n            break\n    # here an episode is terminated, print the return\n    print(\"Episode return\", episode_return) \n        \"\"\"\n        here an episode is terminated, print the return \n        and the number of steps\n        \"\"\"\n    print(f\"Episode return {episode_return}, \\\nNumber of steps: {timestep}\")\n```", "```py\nEpisode return 200.0, Number of steps: 199\nEpisode return 200.0, Number of steps: 199\nEpisode return 200.0, Number of steps: 199\nEpisode return 200.0, Number of steps: 199\nEpisode return 200.0, Number of steps: 199 \n```"]