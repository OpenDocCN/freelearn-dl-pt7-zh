<html><head></head><body>
		<div id="_idContainer100">
			<h1 id="_idParaDest-157" class="chapter-number"><a id="_idTextAnchor178"/>12</h1>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor179"/>How to Deploy Your Model</h1>
			<p>In this chapter, we’ll introduce you to a variety of techniques for deploying your model, including real-time endpoints, serverless, batch options, and more. These concepts apply to many compute environments, but we’ll focus on the capabilities available on AWS within Amazon SageMaker. We’ll talk about why you should try to shrink the size of your model before deploying, along with techniques for this across vision and language. We’ll also cover distributed hosting techniques for scenarios when you can’t or don’t need to shrink your model. Lastly, we’ll explore model-serving techniques and concepts that can help you optimize the end-to-end performance of <span class="No-Break">your model.</span></p>
			<p>We will cover the following topics in <span class="No-Break">the chapter:</span></p>
			<ul>
				<li>What is <span class="No-Break">model deployment?</span></li>
				<li>What is the best way to host <span class="No-Break">my model?</span></li>
				<li>Model deployment options on AWS <span class="No-Break">with SageMaker</span></li>
				<li>Techniques for reducing your <span class="No-Break">model size</span></li>
				<li>Hosting distributed models <span class="No-Break">on SageMaker</span></li>
				<li>Model servers and end-to-end <span class="No-Break">hosting optimizations</span></li>
			</ul>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor180"/>What is model deployment?</h1>
			<p>After you’ve spent weeks<a id="_idIndexMarker609"/> to months working on your custom model, from optimizing the datasets to the distributed training environment, evaluating it, and reducing bias, you must be hungry to finally release it to your customers! In this entire section of the book, we’ll focus on all the key topics related to model deployment. But first, let’s try to explain the <span class="No-Break">term itself.</span></p>
			<p><strong class="bold">Model deployment</strong> refers to <em class="italic">integrating your model into an application</em>. It means that beyond using your model for local analysis in a notebook, or for running reports, you connect it to other software applications. Most commonly, you’re integrating that model into an application. This application could be simply an analytics dashboard. It might be a fraud detection system, a natural language chat, a general search, an autonomous vehicle, or even a video game. In the next chapter, we’ll provide even more ideas for use cases across organizations, especially those that are supercharged by large pre-trained vision and <span class="No-Break">language models.</span></p>
			<p>To me, one of the biggest differentiators in data science teams is whether or not they deploy models. If they do, it usually means their model interacts with customers in an automated fashion and driving business value. This is typically a signal their team builds products as a primary output. Alternatively, you might see data science teams building knowledge as their primary output. This is common in some financial services, health care, and public sector organizations. They might focus on answering analytical questions for business stakeholders, with a smaller focus on delivering products and a higher focus on understanding their vast and complex datasets. Most of this book is dedicated to data science teams with a stronger focus on products, but many of the tools and concepts relate. Much of this chapter will be overwhelmingly relevant to building products. Why? Because the model becomes part of the product. Deployment is the step where <span class="No-Break">this happens.</span></p>
			<p>It is common for data science<a id="_idIndexMarker610"/> teams to offload model deployment to engineering. This is usually so that the data scientists and applied scientists can focus on the core research and development, while engineering can focus on optimizing the application end to end. Some teams include both data science and platform engineering, and some people are just insatiably curious about the entire flow! In <a href="B18942_14.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 14</em></span></a>, we’ll dive into the operations questions, today known as MLOps, that will help you develop<a id="_idIndexMarker611"/> people, processes, and technology to streamline deployments. This usually includes model monitoring, auditing, automatic retraining, tuning, <span class="No-Break">and more.</span></p>
			<p>Most of the deployment patterns we’ll focus on in this book explicitly keep the model in the cloud. This is to streamline your end-to-end operations. However, some applications can’t afford the extra latency of a round-trip hop to the cloud, no matter how low we drive this down. These include autonomous vehicles, video game execution, mobile phone deployments, low-internet connectivity scenarios, robotics, and more. These applications usually integrate the model artifact and inference scripts into the SDK builds directly. This is only possible, however, if the model is small enough to fit on the target deployment device. This is relevant for Meta’s smaller LLaMA models <em class="italic">(1)</em>, Stable Diffusion, and other single-GPU models. This means that the same model reduction techniques we’ll cover later in this chapter are relevant for both cloud and <span class="No-Break">on-device deployments.</span></p>
			<p>In 2021, I led a team at AWS<a id="_idIndexMarker612"/> to deliver a 35-page whitepaper on <strong class="bold">Hybrid Machine Learning</strong>. This is available online for free right here: <a href="https://docs.aws.amazon.com/pdfs/whitepapers/latest/hybrid-machine-learning/hybrid-machine-learning.pdf">https://docs.aws.amazon.com/pdfs/whitepapers/latest/hybrid-machine-learning/hybrid-machine-learning.pdf</a> <em class="italic">(2)</em>. It includes prescriptive guidance for and the advantages and disadvantages of each architecture. Similar to this book, many of the concepts apply to a variety of compute environments but offer deep technical information<a id="_idIndexMarker613"/> for working <span class="No-Break">on AWS.</span></p>
			<p>Now that you have a better idea of the concept of model deployment, let’s explore your <span class="No-Break">available options!</span></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor181"/>What is the best way to host my model?</h1>
			<p>As you probably expected, the answer<a id="_idIndexMarker614"/> to this question completely depends on the application you’re building. To begin, most customers start with one big question: do you need responses from your model in a real-time or synchronous manner? This would be the case for searches, recommendations, chat, and other applications. Most real-time model deployments use a <em class="italic">hosted endpoint</em>, which is an instance that stays<a id="_idIndexMarker615"/> on in the cloud to interact with requests. This is usually contrasted with its opposite: <em class="italic">batch</em>. Batch jobs<a id="_idIndexMarker616"/> take your model and inference data, spin up compute clusters to execute the inference script on all of the requested data, and spin back down. The key difference between real-time deployments and batch jobs is the amount of waiting time between new data and model inference<a id="_idIndexMarker617"/> requests. With real-time deployments, you’re getting the fastest possible model responses and paying more for the premium. With batch jobs, you won’t get a model response until the job has been completed. You’ll wait several minutes for the response but pay <span class="No-Break">much less.</span></p>
			<p>Let’s explore the real-time endpoints <a id="_idIndexMarker618"/>in more detail first, then we’ll unpack batch and even more options. For those of you who are already familiar with hosting on SageMaker and would like to jump straight to questions about how to host foundation models, please feel free to go ahead straight to the <span class="No-Break">following sections.</span></p>
			<p>One of our earliest features on Amazon SageMaker was<a id="_idIndexMarker619"/> our real-time endpoints. These are fully managed APIs that host your model and your scripts. As you can see in the following figure, when specified, they run on multiple instances across availability zones. They can be auto-scaled by SageMaker, spinning up and down based on customer traffic. SageMaker manages a load balancer to send traffic to them, all front-ended by the endpoint itself, which interacts with the <span class="No-Break">request traffic.</span></p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B18942_Figure_12_01.jpg" alt="Figure 12.1 – Example architecture pointing to a SageMaker endpoint"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – Example architecture pointing to a SageMaker endpoint</p>
			<p>The endpoint then interacts with an interface, for example, a Lambda function or simply an API gateway. The gateway then interacts with the client application directly. For example, you might be hosting a web application on premises, such as searching for airline flights. Based on the customer’s preferences and flight history, you’d want to use a recommendation algorithm. Your data science team might analyze that data in another account, training models and optimizing the ROI of that model. Once they’ve found a reasonably performant artifact, they can load it onto a SageMaker endpoint using their own scripts, packages, and objects. You might then promote that artifact into your production account, running penetration and security tests. After deployment, this new endpoint can interact with API requests. The website hosting team can then simply point to your new API hosted in the cloud, while your data science team updates and monitors the <span class="No-Break">model independently.</span></p>
			<p>We’ll cover many more of these architectural best practices<a id="_idIndexMarker620"/> in the upcoming chapters, but for now, let’s look at some of the model deployment options already available in your AWS accounts. </p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor182"/>Model deployment options on AWS with SageMaker</h2>
			<p>The following terms are some of the model deployment options already available in your <span class="No-Break">AWS accounts:</span></p>
			<ul>
				<li><strong class="bold">Real-time endpoints</strong>: As mentioned earlier, real-time endpoints<a id="_idIndexMarker621"/> are always-on f<a id="_idTextAnchor183"/>ully managed<a id="_idIndexMarker622"/> compute resources available through SageMaker. You bring your model and inference scripts; we bring the entire RESTful API for you. This includes the ability to spin up with increasing traffic and spin down with decreasing <a id="_idTextAnchor184"/>traffic. This impacts your costs because you are paying per instance per minute. Real-time endpoints come with many more features, such as the ability to run on GPUs, distributed hosting, multi-model endpoints, asynchronous endpoints, and more. Currently, they have a max payload size of 6 megabytes and a max request runtime of <span class="No-Break">60 seconds.</span></li>
				<li><strong class="bold">Batch transform and scheduled notebooks</strong>: There are two big alternatives to <span class="No-Break">real-time endpoints:</span><ul><li><strong class="bold">Batch </strong><span class="No-Break"><strong class="bold">transform jobs</strong></span></li><li><strong class="bold">Scheduled </strong><span class="No-Break"><strong class="bold">notebook jobs</strong></span></li></ul></li>
			</ul>
			<p>With <strong class="bold">batch transform</strong> on SageMaker, you start<a id="_idIndexMarker623"/> in a similar place<a id="_idIndexMarker624"/> as the real-time endpoint, with a trained model<a id="_idIndexMarker625"/> and an inference<a id="_idIndexMarker626"/> script, but you also point to a dataset known at runtime. This means you will start a batch transform job pointing to a known dataset. You will <a id="_idTextAnchor185"/>also identify the compute resources you need for this job. SageMaker will spin up these resources, invoke your model against your data on them, store the inference responses in<a id="_idTextAnchor186"/> S3, and spin the compute resources down.</p>
			<p>A similar service is a <em class="italic">notebook job</em>. Instead of taking a pre-trained model artifact, this takes a whole notebook as the starting point. You might use notebook jobs when you want to run a set of Python functions or data analysis steps, creating multiple graphs and charts as the result of your analysis. You can write your notebook in SageMaker Studio and simply create a scheduled notebook job without <a id="_idIndexMarker627"/>writing any code!</p>
			<ul>
				<li><strong class="bold">Asynchronous endpoints</strong>: If you expect to host large models, or if you plan to have a lot of computations<a id="_idIndexMarker628"/> in your inference<a id="_idIndexMarker629"/> script, then it is likely the inference request will not be complete within 60 seconds. When this is the case, you may want to consider asynchronous endpoints. These can give you up to 15 minutes of runtime and come with managed queues to handle all your requests. You will have a max payload size of 1 GB, giving you a significant uplift from the payload limit of 6 MB on real-time endpoints. Asynchronous endpoints are great for document processing, such as entity recognition <span class="No-Break">and extraction.</span></li>
				<li><strong class="bold">Multi-model endpoints</strong>: When using real-time endpoints, you<a id="_idIndexMarker630"/> have the extra option of hosting <em class="italic">more than one model</em> on your endpoint. This itself comes<a id="_idIndexMarker631"/> in three varieties. First, you can use one container hosted on the endpoint with limitless models in S3. This is great for solving use cases involving thousands of models, such as training small linear models for every customer in your database. You store as many models as you want in S3, so long as they use the same hosting image, and you send the name of the model to the SageMaker multi-model endpoint. We will load that model from S3 for you and move it into RAM, responding to the<a id="_idTextAnchor187"/> request. This is then cached for future traffic and sent back to S3 when no <span class="No-Break">longer needed.</span></li>
			</ul>
			<p>Another simpler option is <em class="italic">storing multiple containers on one endpoint</em>. In this pattern, you will create multiple containers, such as one using XGBoost, another using PyTorch, another using <strong class="source-inline">pandas</strong>, and so on. Your endpoint can host all of these, so long as it is large enough, and you can determine which container to use on request.</p>
			<p>Finally, you can also use what is called a <strong class="bold">serial inference pipeline</strong>. This also uses multiple containers, but each is invoked<a id="_idIndexMarker632"/> one after the other, similar<a id="_idIndexMarker633"/> to a pipeline. You might use this for feature preprocessing, such as running<a id="_idIndexMarker634"/> an LDA or a VAE, and then invoke it against your model.</p>
			<ul>
				<li><strong class="bold">Serverless endpoints</strong>: Another option for hosting your models<a id="_idIndexMarker635"/> on SageMaker is a serverless endpoint. This is great for CPU-based<a id="_idIndexMarker636"/> models, such as KNNs or logistic regressions, when you are expecting intermittent traffic. This might include long periods without any inference requests, with a sudden burst of traffic. Serverless options are very cost-effective, so if you are able to meet your latency goals on serverless, then this tends to be a great choice. Given that Lambda functions can now hold up to 10 GB of memory (<em class="italic">3</em>), you might be able to shrink an already small foundation model down to those runtime requirements. The CPU-based runtime will be challenging, but if a slower response time doesn’t block you, serverless may be <span class="No-Break">an option.</span></li>
			</ul>
			<p>There are so many other aspects of hosting on SageMaker. You can monitor your models, enable auto-scaling, explain them, validate models safely, apply shadow tests, catalog models in a registry, enable A/B testing, audit them, and more. We’ll dive into these topics and more in <a href="B18942_14.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 14</em></span></a>. For now, let’s learn about methods for reducing the size of our models <span class="No-Break">for inference.</span></p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor188"/>Why should I shrink my model, and how?</h1>
			<p>After learning all about how the power of large models<a id="_idIndexMarker637"/> can boost your accuracy, you may be wondering, why would I ever consider shrinking my model? The reality is that large models can be very slow to respond to inference requests and expensive to deploy. This is especially true for language and vision applications, including everything from visual searching to dialogue, image-to-music generation, open-domain question-answering, and more. While this isn’t necessarily an issue for training, because the only person waiting for your model to finish is you, it becomes a massive bottleneck in hosting when you are trying to keep your customers happy. As has been well studied, in digital experiences, every millisecond counts. Customers very strictly prefer fast, simple, and efficient interfaces online. This is why we have a variety of techniques in the industry to speed up your model inference without introducing drops in accuracy. Here, we’ll cover three key techniques for this: compilation, knowledge<a id="_idIndexMarker638"/> distillation, <span class="No-Break">and quantization.</span></p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor189"/>Model compilation</h2>
			<p>As we learned earlier, <strong class="bold">compilation</strong> is a technique you can use<a id="_idIndexMarker639"/> for GPU-based deep learning models. Depending<a id="_idIndexMarker640"/> on the operator support in your compiler, you may be able to compile a pre-trained model for your preferred target devices. AWS has a managed feature for this, SageMaker Neo, which runs a compilation job to transform your artifact for a specified environment. This works for deployments both in the cloud and on-device. While Neo can decrease the size of your model by up to 10 times, there’s no guarantee it will work for any arbitrary neural network, so proceed <span class="No-Break">with caution.</span></p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor190"/>Knowledge distillation</h2>
			<p><strong class="bold">Knowledge distillation</strong> is a fascinating technique that uses a larger model, called a teacher model, to impact the performance<a id="_idIndexMarker641"/> of a smaller model, called a student model. Through<a id="_idIndexMarker642"/> gradient descent, specifically<a id="_idIndexMarker643"/> a KL divergence<a id="_idIndexMarker644"/> that computes the difference between two distributions, we can teach the student model to mimic the behavior of the teacher model. A very logical use for this is after large-scale pretraining! Scaling up the size of the model to match the size of your data, for example, with the scaling laws, helps you maximize all your potential for accuracy and computational intelligence. After this, however, you can use knowledge distillation to optimize that model for performance in production. Depending on the gap in model sizes between your teacher and student, you could easily boost inference runtime by 10 times or more, while losing only a few points on accuracy. Here’s a visual rendition of knowledge distillation, as presented by Jianping Gou (<em class="italic">3</em>) et al. in their 2021 survey on <span class="No-Break">the domain.</span></p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B18942_Figure_12_02.jpg" alt="Figure 12.2 – Knowledge transfer through distillation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Knowledge transfer through distillation</p>
			<p>While the teacher model and the student model receive the same datasets, we <em class="italic">transfer knowledge</em> to the student by comparing the probabilities that both of them generate. Then, we simply update the student to minimize the difference <span class="No-Break">between them!</span></p>
			<p>Knowledge distillation is also useful<a id="_idIndexMarker645"/> in other applications, including machine translation and <strong class="bold">reinforcement learning from human feedback</strong> (<strong class="bold">RLHF</strong>). Pro tip: RLHF is one key underlying technology behind ChatGPT! We learned more about that in <a href="B18942_10.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>. Distillation<a id="_idIndexMarker646"/> is also responsible for DistiliBert <em class="italic">(4)</em>, a model presented<a id="_idIndexMarker647"/> by the Hugging Face team <span class="No-Break">in 2019.</span></p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor191"/>Quantization</h2>
			<p><strong class="bold">Quantization</strong> is another technique to reduce the runtime<a id="_idIndexMarker648"/> of your model. In this case, rather than strictly<a id="_idIndexMarker649"/> reducing the memory footprint of your model, which both compilation and distillation do, we refactor your network to use a lower precision<a id="_idIndexMarker650"/> data type. Here, data type refers to bit representations, usually ranging from a high of FP32 and dropping down to FP16 or even INT8. Integers are easier to represent computationally, so the literal storage required to hold them is smaller. However, floats are obviously more expressive, given that they can point to quite literally an infinite range of numbers between integers. Converting data representation, as you do with quantization, is useful, because when you convert your data types from floats in training into integers in hosting, the overall memory consumption drops. Instructions for how to do this vary across frameworks, with details about doing so in PyTorch right here (<em class="italic">5</em>) and NVIDIA’s TensorRT here <em class="italic">(6)</em>. Quantization does have trade-offs. Make sure you test a quantized model<a id="_idIndexMarker651"/> robustly before deploying it, so you know how it impacts<a id="_idIndexMarker652"/> both speed <span class="No-Break">and accuracy.</span></p>
			<p>Now that you’ve learned a few ways to reduce the footprint of your model, let’s cover techniques you can use when this isn’t an option for you: distributed <span class="No-Break">model hosting!</span></p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor192"/>Hosting distributed models on SageMaker</h1>
			<p>In <a href="B18942_05.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, we covered distribution<a id="_idIndexMarker653"/> fundamentals, where you learned<a id="_idIndexMarker654"/> how to think about splitting up your model and datasets across multiple GPUs. The good news is that you can use this same logic to host the model. In this case, you’ll be more interested in model parallel, placing layers and tensors on multiple GPU partitions. You won’t actually need a data parallel framework, because we’re not using backpropagation. We’re only running a forward pass through the network and getting inference results. There’s no gradient descent or weight <span class="No-Break">updating involved.</span></p>
			<p>When would you use distributed model hosting? To integrate extremely large models into your applications! Generally, this is scoped to large language models. It’s rare to see vision models stretch beyond single GPUs. Remember, in <a href="B18942_04.xhtml#_idTextAnchor066"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Containers and Accelerators on the Cloud</em>, we learned about different sizes of GPU memory. This is just as relevant for hosting as it is for training. One simple way of estimating the GB size of your model is to just read the footprint when it’s stored on disk. While the size will vary slightly as the object moves from disk to memory, the overall disk footprint is still a <span class="No-Break">good estimate.</span></p>
			<p>For extremely large models in the GPT-3 range of 175B parameters, it’s not uncommon for the model to require at least 350 GB of storage! In this case study <em class="italic">(7)</em> for hosting large models on SageMaker, we show hosting a model of this size on one p4d instance, using only 8 A100s. That’s one <strong class="source-inline">ml.p4d.24xlarge</strong> instance, which on public SageMaker pricing, is about $37 per hour! Granted, while this is a fraction of the cost of training, which can easily be 10 times or more for extremely large foundation models, it’s still painful to see this on <span class="No-Break">your bill.</span></p>
			<p>On top of the massive cost of this cluster, you’re also introducing the extra latency cost to your customer. Imagine running any process across 8 GPUs. Even with pipeline and tensor parallelism, that is still not going<a id="_idIndexMarker655"/> to be <span class="No-Break">particularly</span><span class="No-Break"><a id="_idIndexMarker656"/></span><span class="No-Break"> fast.</span></p>
			<p>Now, let’s learn about a few key underlying technologies that bring all of this together. Then, we’ll look at a few examples of hosting models at the 6B and <span class="No-Break">175B scales.</span></p>
			<h3>Large model hosting containers on SageMaker</h3>
			<p>Just as we learned about in training foundation<a id="_idIndexMarker657"/> models, it all comes down to the base container and the relevant packages you’re using to accomplish your goal. For hosting large models on SageMaker, we provide dedicated deep learning containers for this explicit purpose. These are open sourced on GitHub <em class="italic">(8)</em>, so you can easily view and build on top <span class="No-Break">of them.</span></p>
			<p>The large model inference containers package<a id="_idIndexMarker658"/> and provide two key technologies for you: DJLServing and DeepSpeed. The <strong class="bold">Deep Java Library</strong> (<strong class="bold">DJL</strong>) <em class="italic">(9)</em> was originally built for Java developers to build ML models and applications. They built a universal model-serving solution that is programming language-agnostic, providing a single common denominator to serve models across frameworks such as TensorFlow, ONNX, TensorRT, and Python. They also natively support multi-GPU hosting, through MPI and socket connections. This makes it an attractive proposition for <span class="No-Break">distributed hosting!</span></p>
			<p>The second key technology provided in the AWS large model hosting container is D<a id="_idTextAnchor193"/>eepSpeed. Notably, DeepSpeed is helpful because<a id="_idIndexMarker659"/> it shards your tensors across multiple GPUs, and it finds the best partitioning strategies for this automatically. As my colleagues discuss in this blog post <em class="italic">(10)</em>, DeepSpeed evaluates both inference latency and cost in determining the optimal <span class="No-Break">sharding regime.</span></p>
			<p>For hands-on examples of this in detail, feel free to look at our 6B GPT-J <span class="No-Break">notebook: </span><a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/deepspeed/GPT-J-6B_DJLServing_with_PySDK.ipynb"><span class="No-Break">https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/deepspeed/GPT-J-6B_DJLServing_with_PySDK.ipynb</span></a><span class="No-Break">.</span></p>
			<p>The smaller example is a good starting point because it gives you very simple, practical, and less expensive content for hosting models across multiple GPUs. Once you’ve tested this, then you can upgrade to a much larger example of hosting 175B parameters with this BLOOM <span class="No-Break">notebook: </span><a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/nlp/realtime/llm/bloom_176b/djl_deepspeed_deploy.ipynb"><span class="No-Break">https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/nlp/realtime/llm/bloom_176b/djl_deepspeed_deploy.ipynb</span></a><span class="No-Break">.</span></p>
			<p>Now that we’ve walked through a few options for distributed hosting, let’s close out the chapter with a quick discussion on model servers and optimizing the end-to-end <span class="No-Break">hosting experience.</span></p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor194"/>Model servers and end-to-end hosting optimizations</h1>
			<p>You might be wondering: if SageMaker is hosting<a id="_idIndexMarker660"/> my model artifact and my inference script, how do I convert<a id="_idIndexMarker661"/> that into a real-time service that can respond to live traffic? The answer is model servers! For those of you who aren’t particularly interested in learning how to convert your model inference response into a RESTful interface, you’ll be happy to know this is largely abstracted on SageMaker for easy and fast prototyping. However, if you’d like to optimize your inference stack to deliver state-of-the-art model responses, <span class="No-Break">read on.</span></p>
			<p>There are five key types of latency to trim down as you are improving your model hosting response. Here’s how we can <span class="No-Break">summarize them:</span></p>
			<ul>
				<li><strong class="bold">Container latency</strong>: This refers to the time overhead<a id="_idIndexMarker662"/> involved in entering and exiting one of your containers. As we learned earlier, on SageMaker, you might host a variety of containers in a <em class="italic">serial inference pipeline</em>. This is pictured here. Container latency is the time to invoke and exit one of <span class="No-Break">your containers.</span></li>
				<li><strong class="bold">Model latency</strong>: This includes the invocation and exit time<a id="_idIndexMarker663"/> of all containers on the endpoint. As you can see below in <span class="No-Break"><em class="italic">Figure 12</em></span><em class="italic">.13</em>, the latency for an individual container may be much smaller than the entire latency for <span class="No-Break">the model.</span></li>
				<li><strong class="bold">Overhead latency</strong>: This refers to the time for SageMaker to route<a id="_idIndexMarker664"/> your request, receive the request from the client, and return it, minus the <span class="No-Break">model's latency.</span></li>
				<li><strong class="bold">End-to-end latency</strong>. This is primarily calculated from the perspective<a id="_idIndexMarker665"/> of the client. It is impacted by the client’s requesting bandwidth, the connection to the cloud, any processing in front of SageMaker, the overhead latency, and the <span class="No-Break">model's latency.</span></li>
			</ul>
			<p>Let’s look at all these <span class="No-Break">pieces together:</span></p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B18942_Figure_12_03.jpg" alt="Figure 12.3 – End-to-end model latency on SageMaker"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – End-to-end model latency on SageMaker</p>
			<p>As a consumer of this service, you have a few optimization techniques you can deploy. First, and this is true for any application on AWS, <em class="italic">push the application to where your customers are!</em> A major reason to use AWS is that we have the single largest global infrastructure of any CSP. We have more regions, at higher availability designs, than any other cloud on the planet. Make this your asset when you push your application to a geographic region or point of presence that is closest to your customers. This will immediately reduce the amount of time it takes for their request to hit the cloud because it will have fewer miles to travel on <span class="No-Break">the network.</span></p>
			<p>My colleagues at AWS delivered a spectacular blog post on <em class="italic">optimizing the container</em> for your SageMaker hosting workloads. In particular, they explored NVIDIA’s Triton, an open source project that delivers ultra-low latency model inference results, as in, <span class="No-Break">single-digit milliseconds.</span></p>
			<p>For more details about<a id="_idIndexMarker666"/> Triton, in addition to end-to-end optimizations for SageMaker hosting, See their blog<a id="_idIndexMarker667"/> post on the topic here: <a href="https://aws.amazon.com/blogs/machine-learning/achieve-hyperscale-performance-for-model-serving-using-nvidia-triton-inference-server-on-amazon-sagemaker/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/achieve-hyperscale-performance-for-model-serving-using-nvidia-triton-inference-server-on-amazon-sagemaker/</span></a><span class="No-Break"> </span><span class="No-Break"><em class="italic">(11)</em></span><span class="No-Break">.</span></p>
			<p>Lastly, I’d like to also call out SageMaker’s <strong class="bold">inference recommender</strong> <em class="italic">(12)</em>, which you can use to help<a id="_idIndexMarker668"/> you pick the right instance<a id="_idIndexMarker669"/> type, count, and configurations<a id="_idIndexMarker670"/> based on your expected traffic. In fact, my<a id="_idIndexMarker671"/> team used the inference recommender <a id="_idIndexMarker672"/>to run their tests <span class="No-Break">on Triton!</span></p>
			<p>Now that you have a much better understanding of what model servers are and how you can use them to optimize your end-to-end hosting performance, let’s close out the chapter with an <span class="No-Break">overall recap.</span></p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor195"/>Summary</h1>
			<p>We defined model deployment as integrating your model into a client application. We talked about the characteristics of data science teams that may commonly deploy their own models, versus those who may specialize in more general analysis. We introduced a variety of use cases where model deployment is a critical part of the entire application. While noting a variety of hybrid architectures, we focused explicitly on deployments in the cloud. We learned about some of the best ways to host your models, including options on SageMaker such as real-time endpoints, batch transform and notebook jobs, asynchronous endpoints, multi-model endpoints, serverless endpoints, and more. We learned about options for reducing the size of your model, from compilation to distillation and quantization. We covered distributed model hosting and closed out with a review of model servers and end-to-end hosting optimization tips <span class="No-Break">on SageMaker.</span></p>
			<p>Next up, we’ll dive into a set of techniques you can use to interact with foundation models to eke out the best performance: <span class="No-Break">prompt engineering!</span></p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor196"/>References</h1>
			<ol>
				<li><em class="italic">LLaMA: Open and Efficient Foundation Language </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2302.13971.pdf "><span class="No-Break">https://arxiv.org/pdf/2302.13971.pdf</span></a></li>
				<li><em class="italic">Lambda </em><span class="No-Break"><em class="italic">quotas</em></span><span class="No-Break">: </span><span class="No-Break">https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html</span></li>
				<li><em class="italic">Knowledge Distillation: A </em><span class="No-Break"><em class="italic">Survey</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2006.05525.pdf"><span class="No-Break">https://arxiv.org/pdf/2006.05525.pdf</span></a></li>
				<li><em class="italic">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and </em><span class="No-Break"><em class="italic">lighter</em></span><span class="No-Break">: </span><span class="No-Break">https://arxiv.org/pdf/1910.01108.pdf</span></li>
				<li><span class="No-Break"><em class="italic">QUANTIZATION</em></span><span class="No-Break">: </span><a href="https://pytorch.org/docs/stable/quantization.html"><span class="No-Break">https://pytorch.org/docs/stable/quantization.html</span></a></li>
				<li><em class="italic">Achieve hyperscale performance for model serving using NVIDIA Triton Inference Server on Amazon </em><span class="No-Break"><em class="italic">SageMaker</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/blogs/machine-learning/achieve-hyperscale-performance-for-model-serving-using-nvidia-triton-inference-server-on-amazon-sagemaker/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/achieve-hyperscale-performance-for-model-serving-using-nvidia-triton-inference-server-on-amazon-sagemaker/</span></a></li>
				<li><em class="italic">Deploy BLOOM-176B and OPT-30B on Amazon SageMaker with large model inference Deep Learning Containers and </em><span class="No-Break"><em class="italic">DeepSpeed</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/</span></a></li>
				<li><em class="italic">Large Model Inference </em><span class="No-Break"><em class="italic">Containers</em></span><span class="No-Break">: </span><a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers"><span class="No-Break">https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers</span></a></li>
				<li><em class="italic">Deep Java </em><span class="No-Break"><em class="italic">Library</em></span><span class="No-Break">: </span><a href="https://djl.ai/"><span class="No-Break">https://djl.ai/</span></a></li>
				<li><em class="italic">Deploy large models on Amazon SageMaker using DJLServing and DeepSpeed model parallel </em><span class="No-Break"><em class="italic">inference</em></span><span class="No-Break">: </span><a href="https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/</span></a></li>
				<li><em class="italic">Achieve hyperscale performance for model serving using NVIDIA Triton Inference Server on Amazon </em><span class="No-Break"><em class="italic">SageMaker</em></span><span class="No-Break">: </span><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/achieve-hyperscale-performance-for-model-serving-using-nvidia-triton-inference-server-on-amazon-sagemaker/</span></li>
				<li><em class="italic">Amazon SageMaker Inference </em><span class="No-Break"><em class="italic">Recommender</em></span><span class="No-Break">: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html</span></a></li>
			</ol>
		</div>
	

		<div id="_idContainer101" class="Content">
			<h1 id="_idParaDest-170"><a id="_idTextAnchor197"/>Part 5: Deploy Your Model</h1>
			<p>In part 5, you’ll learn how to deploy your model. You’ll use techniques such as distillation, quantization, and compilation to reduce your model’s overall footprint. You’ll identify top use cases to scale your model across organizations, and learn about ongoing operations, monitoring, <span class="No-Break">and maintenance.</span></p>
			<p>This section has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B18942_13.xhtml#_idTextAnchor198"><em class="italic">Chapter 13</em></a><em class="italic">, Prompt Engineering</em></li>
				<li><a href="B18942_14.xhtml#_idTextAnchor217"><em class="italic">Chapter 14</em></a>, <em class="italic">MLOps for Vision and Language</em></li>
				<li><a href="B18942_15.xhtml#_idTextAnchor229"><em class="italic">Chapter 15</em></a>, <em class="italic">Future Trends in Pretraining Foundation Models</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer102" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer103">
			</div>
		</div>
	</body></html>