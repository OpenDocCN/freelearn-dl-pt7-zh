["```py\n    !pip install ctranslate2\n    !pip install transformers[torch]>=4.23\n    !pip install faster-whisper\n    ```", "```py\n    !wget -nv https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.mp3\n    librosa:\n\n    ```", "```py\n\n    This step is crucial for ensuring that the audio data is in the correct format for processing by the Whisper model.\n    ```", "```py\n    ct2-transformers-converter command converts models to the CTranslate2 format, optimized for fast inference. The core CTranslate2 implementation is framework-agnostic. The framework-specific logic is moved to a conversion step that loads supported models into a unified representation. The weights are then optionally quantized and saved into an optimized binary format for efficient storage and processing.When converting models using the CTranslate2 tool, the output directory typically contains several key files for the CTranslate2 engine to load and run the model. The command streamlines the process of preparing models for deployment in environments where computational efficiency is crucial and a preparatory step for quantization. While the exact output files can vary depending on the specific model being converted and the options used during conversion, standard files include the following:*   `config.json`: This JSON file contains configuration information about the model, such as its architecture, the size of its layers, and other hyperparameters. This information is crucial for the CTranslate2 engine to interpret the model’s binary weights and perform inference correctly.*   `model.bin`: This is the binary file containing the quantized weights of the model. Quantization reduces the precision of the model’s weights, which can significantly decrease the model size and improve inference speed, often with minimal impact on accuracy.*   `vocabulary.json` or similar vocabulary files (for example, `source.spm` and `target.spm` for models using `SentencePiece` tokenization): These files contain the mapping between tokens (words or subwords) and their corresponding indices in the model’s vocabulary. This mapping is essential for converting input text into a format that the model can process (tokenization) and converting the model’s output back into human-readable text (detokenization).These files represent the converted and optimized model and are ready for use with CTranslate2\\. The conversion process might also include copying additional files necessary for the model’s operation, such as tokenization configuration (`tokenizer_config.json`), special tokens mapping (`special_tokens_map.json`), and others, depending on the model’s requirements and the conversion options you use.\n    ```", "```py\n    !ct2-transformers-converter --force --model openai/whisper-tiny --output_dir whisper-tiny-ct2-int8 \\\n    INT8)\n    ```", "```py\n    # Detect the language.\n    results = model.detect_language(features)\n    language, probability = results[0][0]\n    print(\"Detected language %s with probability %f\" % (language, probability))\n    ```", "```py\n    prompt = processor.tokenizer.convert_tokens_to_ids(\n        [\n            \"<|startoftranscript|>\",\n            language,\n            \"<|transcribe|>\",\n            \"<|notimestamps|>\",  # Remove this token to generate timestamps.\n        ]\n    )\n    # Load the model on device\n    model = ctranslate2.models.Whisper(\"whisper-tiny-ct2-int8\", device=this_device)\n    # Run generation for the 30-second window.\n    results = model.generate(features, [prompt])\n    transcription = processor.decode(results[0].sequences_ids[0])\n    print(transcription))\n    ```", "```py\n    # Print the end time and the delta in seconds and fractions of a second.\n    end = time.time()\n    print('start: ', start)\n    print('end: ', end)\n    print('delta: ', end - start)\n    print('delta: ', datetime.timedelta(seconds=end - start))\n    ```", "```py\n%pip install -q \"transformers>=4.35\" onnx \"git+https://github.com/huggingface/optimum-intel.git\" \"peft==0.6.2\" --extra-index-url https://download.pytorch.org/whl/cpu\n%pip install -q \"openvino>=2023.2.0\" datasets  \"gradio>=4.0\" \"librosa\" \"soundfile\"\n%pip install -q \"nncf>=2.6.0\" \"jiwer\"\n```", "```py\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\nprocessor = AutoProcessor.from_pretrained(model_id.value)\npt_model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id.value)\npt_model.eval();\n```", "```py\n# Using HF transformers models\nfrom transformers import AutoModelForSpeechSeq2Seq\nfrom transformers import AutoTokenizer, pipeline\nmodel_id = \"distil-whisper/distil-large-v2\"\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\n# Using Optimum Inference models\nfrom optimum.intel.openvino import OVModelForSpeechSeq2Seq\nfrom transformers import AutoTokenizer, pipeline\nmodel_id = \"distil-whisper/distil-large-v2\"\nmodel = OVModelForSpeechSeq2Seq.from_pretrained(model_id, export=True)\n```", "```py\nfrom pathlib import Path\nfrom optimum.intel.openvino import OVModelForSpeechSeq2Seq\nmodel_path = Path(model_id.value.replace('/', '_'))\nov_config = {\"CACHE_DIR\": \"\"}\nif not model_path.exists():\n    ov_model = OVModelForSpeechSeq2Seq.from_pretrained(\n        model_id.value, ov_config=ov_config, export=True, compile=False, load_in_8bit=False\n    )\n    ov_model.half()\n    ov_model.save_pretrained(model_path)\nelse:\n    ov_model = OVModelForSpeechSeq2Seq.from_pretrained(\n        model_path, ov_config=ov_config, compile=False\n    )\n```", "```py\n-from transformers import AutoModelForSpeechSeq2Seq\n+from optimum.intel.openvino import OVModelForSpeechSeq2Seq\nfrom transformers import AutoTokenizer, pipeline\nmodel_id = \"distil-whisper/distil-large-v2\"\n-model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\n+model = OVModelForSpeechSeq2Seq.from_pretrained(model_id, export=True)\n```", "```py\nfrom transformers import pipeline\nov_model.generation_config = pt_model.generation_config\npipe = pipeline(\n    \"automatic-speech-recognition\",\n    model=ov_model,\n    tokenizer=processor.tokenizer,\n    feature_extractor=processor.feature_extractor,\n    max_new_tokens=128,\n    chunk_length_s=15,\n    batch_size=16,\n)\n```", "```py\n%%skip not $to_quantize.value\nfrom itertools import islice\nfrom optimum.intel.openvino.quantization import InferRequestWrapper\ndef collect_calibration_dataset(ov_model: OVModelForSpeechSeq2Seq, calibration_dataset_size: int):\n    # Overwrite model request properties, saving the original ones for restoring later\n    original_encoder_request = ov_model.encoder.request\n    original_decoder_with_past_request = ov_model.decoder_with_past.request\n    encoder_calibration_data = []\n    decoder_calibration_data = []\n    ov_model.encoder.request = InferRequestWrapper(original_encoder_request, encoder_calibration_data)\n    ov_model.decoder_with_past.request = InferRequestWrapper(original_decoder_with_past_request,\n                                                             decoder_calibration_data)\n    calibration_dataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", streaming=True)\n    for sample in tqdm(islice(calibration_dataset, calibration_dataset_size), desc=\"Collecting calibration data\",\n                       total=calibration_dataset_size):\n        input_features = extract_input_features(sample)\n        ov_model.generate(input_features)\n    ov_model.encoder.request = original_encoder_request\n    ov_model.decoder_with_past.request = original_decoder_with_past_request\n    return encoder_calibration_data, decoder_calibration_data\n```", "```py\n%%skip not $to_quantize.value\nimport gc\nimport shutil\nimport nncf\nCALIBRATION_DATASET_SIZE = 50\nquantized_model_path = Path(f\"{model_path}_quantized\")\ndef quantize(ov_model: OVModelForSpeechSeq2Seq, calibration_dataset_size: int):\n    if not quantized_model_path.exists():\n        encoder_calibration_data, decoder_calibration_data = collect_calibration_dataset(\n            ov_model, calibration_dataset_size\n        )\n        print(\"Quantizing encoder\")\n        quantized_encoder = nncf.quantize(\n            ov_model.encoder.model,\n            nncf.Dataset(encoder_calibration_data),\n            subset_size=len(encoder_calibration_data),\n            model_type=nncf.ModelType.TRANSFORMER,\n            # Smooth Quant algorithm reduces activation quantization error; optimal alpha value was obtained through grid search\n            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=0.50)\n        )\n        ov.save_model(quantized_encoder, quantized_model_path / \"openvino_encoder_model.xml\")\n        del quantized_encoder\n        del encoder_calibration_data\n        gc.collect()\n        print(\"Quantizing decoder with past\")\n        quantized_decoder_with_past = nncf.quantize(\n            ov_model.decoder_with_past.model,\n            nncf.Dataset(decoder_calibration_data),\n            subset_size=len(decoder_calibration_data),\n            model_type=nncf.ModelType.TRANSFORMER,\n            # Smooth Quant algorithm reduces activation quantization error; optimal alpha value was obtained through grid search\n            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=0.95)\n        )\n        ov.save_model(quantized_decoder_with_past, quantized_model_path / \"openvino_decoder_with_past_model.xml\")\n        del quantized_decoder_with_past\n        del decoder_calibration_data\n        gc.collect()\n        # Copy the config file and the first-step-decoder manually\n        shutil.copy(model_path / \"config.json\", quantized_model_path / \"config.json\")\n        shutil.copy(model_path / \"openvino_decoder_model.xml\", quantized_model_path / \"openvino_decoder_model.xml\")\n        shutil.copy(model_path / \"openvino_decoder_model.bin\", quantized_model_path / \"openvino_decoder_model.bin\")\n    quantized_ov_model = OVModelForSpeechSeq2Seq.from_pretrained(quantized_model_path, ov_config=ov_config, compile=False)\n    quantized_ov_model.to(device.value)\n    quantized_ov_model.compile()\n    return quantized_ov_model\nov_quantized_model = quantize(ov_model, CALIBRATION_DATASET_SIZE)\n```", "```py\n    %%skip not $to_quantize.value\n    dataset = load_dataset(\n        \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n    )\n    sample = dataset[0]\n    ```", "```py\n    input_features = extract_input_features(sample)\n    predicted_ids = ov_model.generate(input_features)\n    ```", "```py\n    transcription_original = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n    ```", "```py\n    predicted_ids = ov_quantized_model.generate(input_features)\n    transcription_quantized = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n    ```", "```py\n    display(ipd.Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"]))\n    ```", "```py\n    print(f\"Original : {transcription_original[0]}\")\n    print(f\"Quantized: {transcription_quantized[0]}\")\n    ```", "```py\n    word_accuracy = (1 - wer(ground_truths, predictions, reference_transform=wer_standardize,                         hypothesis_transform=wer_standardize)) * 100\n    ```", "```py\n    mean_whole_infer_time = sum(whole_infer_times)\n    mean_encoder_infer_time = sum(encoder_infer_times)\n    mean_decoder_with_time_infer_time = sum(decoder_with_past_infer_times)\n    ```", "```py\n    print(f\"Encoder performance speedup: {times_original[1] / times_quantized[1]:.3f}\")\n    print(f\"Decoder with past performance speedup: {times_original[2] / times_quantized[2]:.3f}\")\n    print(f\"Whole pipeline performance speedup: {times_original[0] / times_quantized[0]:.3f}\")\n    print(f\"Whisper transcription word accuracy. Original model: {accuracy_original:.2f}%. Quantized model: {accuracy_quantized:.2f}%.\")\n    print(f\"Accuracy drop: {accuracy_original - accuracy_quantized:.2f}%.\")\n    ```", "```py\n%%capture\n!pip -q install gradio\n```", "```py\nfrom huggingface_hub import notebook_login\nnotebook_login()\nfrom huggingface_hub import whoami\nwhoami()\n```", "```py\nfrom transformers import pipeline\np = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n```", "```py\nimport gradio as gr\nfrom transformers import pipeline\nimport numpy as np\ntranscriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\ndef transcribe(audio):\n    sr, y = audio\n    y = y.astype(np.float32)\n    y /= np.max(np.abs(y))\n    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\ndemo = gr.Interface(\n    transcribe,\n    gr.Audio(sources=[\"microphone\"]),\n    \"text\",\n)\ndemo.launch(debug=True)\n```", "```py\nimport gradio as gr\nfrom transformers import pipeline\nimport numpy as np\ntranscriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\ndef transcribe(state, new_chunk):\n    if state is None:\n        stream = np.array([], dtype=np.float32)\n        previous_text = \"\"\n    else:\n        stream, previous_text = state\n    sr, y = new_chunk\n    duration = len(y) / sr\n    y = y.astype(np.float32)\n    y /= np.max(np.abs(y))\n    overlap = int(sr * 0.5)  # Half a second overlap\n    if len(stream) > 0:\n        stream = np.concatenate([stream[-overlap:], y])\n    else:\n        stream = y\n    # Transcribe the current chunk\n    new_text = transcriber({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n    # Update the previous text based on the overlap\n    if len(previous_text) > 0:\n        overlap_text = previous_text[-int(len(previous_text) * 0.1):]  # Last 10% of previous text\n        combined_text = previous_text[:-len(overlap_text)] + new_text\n    else:\n        combined_text = new_text\n    return (stream, combined_text), combined_text\ndemo = gr.Interface(\n    transcribe,\n    [\"state\", gr.Audio(sources=[\"microphone\"], streaming=True)],\n    [\"state\", \"text\"],\n    live=True,\n)\ndemo.launch(debug=True)\n```"]