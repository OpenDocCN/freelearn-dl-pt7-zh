- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Future Trends in Pretraining Foundation Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll close out the book by pointing to where trends are headed
    for all relevant topics presented in this book. We’ll explore trends in foundation
    model application development, like using LangChain to build interactive dialogue
    applications, along with techniques like retrieval augmented generation to reduce
    LLM hallucination. We’ll explore ways to use generative models to solve classification
    tasks, human-centered design, and other generative modalities like code, music,
    product documentation, powerpoints, and more! We’ll talk through AWS offerings
    like SageMaker JumpStart Foundation Models, Amazon Bedrock, Amazon Titan, and
    Amazon Code Whisperer, and top trends in the future of foundation models and pretraining
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we’ll dive into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for building applications for LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative modalities outside of vision and language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS offerings in foundation models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The future of foundation models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The future of pretraining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Techniques for building applications for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you’ve learned about foundation models, and especially large language
    models, let’s talk through a few key ways you can use them to build applications.
    One of the most significant takeaways of the ChatGPT moment in December 2022 is
    that customers clearly love for their chat to be knowledgeable about every moment
    in the conversation, remember topics mentioned earlier and encompassing all the
    twists and turns of dialogue. Said another way, beyond generic question answering,
    there’s a clear consumer preference for chat to be *chained*. Let’s take a look
    at an example in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.1 – Chaining questions for chat applications](img/B18942_Figure_15_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.1 – Chaining questions for chat applications
  prefs: []
  type: TYPE_NORMAL
- en: The key difference between the left- and the right-hand side of *Figure 15**.1*
    is that on the left-hand side, the answers are **discontinuous**. That means the
    model simply sees each question as a single entity before providing its response.
    On the right-hand side, however, the answers are **continuous**. That means the
    entire dialogue is provided to the model, with the newest question at the bottom.
    This helps to ensure the continuity of responses, with the model more capable
    of maintaining the context.
  prefs: []
  type: TYPE_NORMAL
- en: How can you set this up yourself? Well, on the one hand, what I’ve just described
    isn’t terribly difficult. Imagine just reading from your HTML page, packing in
    all of that call and response data into the prompt, and siphoning out the response
    to return it to your end user. If you don’t want to build it yourself, however,
    you can just use a few great open source options!
  prefs: []
  type: TYPE_NORMAL
- en: Building interactive dialogue apps with open-source stacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you haven’t seen it before, let me quickly introduce you to LangChain. Available
    for free on GitHub here: [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain),
    LangChain is an open source toolkit built by Harrison Chase and more than 600
    other contributors. It provides functionality similar to the famous ChatGPT by
    pointing to OpenAI’s API, or any other foundation model, but letting you as the
    developer and data scientist create your own frontend and customer experience.'
  prefs: []
  type: TYPE_NORMAL
- en: Decoupling the application from the model is a smart move; in the last few months
    alone the world has seen nothing short of hundreds of new large language models
    come online, with teams around the world actively developing more. When your application
    interacts with the model via a single API call, then you can more easily move
    from one model to the next as the licensing, pricing, and capabilities upgrade
    over time. This is a big plus for you!
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting open-source technology here is Haystack (*26*). Developed
    by the German start-up, Deepset, Haystack is a useful tool for, well, finding
    a needle in a haystack. Specifically they operate like a interface for you to
    bring your own LLMs into expansive question/answering scenarios. This was their
    original area of expertise, and since then have expanded quite a bit!
  prefs: []
  type: TYPE_NORMAL
- en: 'At AWS, we have an open source template for building applications with LangChain
    on AWS. It’s available on GitHub here: [https://github.com/3coins/langchain-aws-template](https://github.com/3coins/langchain-aws-template).
    In the following diagram, you can see a quick representation of the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 15.2 – \uFEFFHosting LangChain on AWS](img/B18942_Figure_15_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 15.2 – Hosting LangChain on AWS
  prefs: []
  type: TYPE_NORMAL
- en: 'While this can point to any frontend, we provide an example template you can
    use to get off the ground for your app. You can also easily point to *any* custom
    model, whether it’s on a SageMaker endpoint or in the new AWS service, Bedrock!
    More on that a bit later in this chapter. As you can see in the previous image,
    in this template you can easily run a UI anywhere that interacts with the cloud.
    Let’s take a look at all of the steps.:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the UI hits the API gateway.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, credentials are retrieved via IAM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Third, the service is invoked via Lambda.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fourth, the model credentials are retrieved via Secrets Manager.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fifth, your model is invoked through either an API call to a serverless model
    SDK, or a custom model you’ve trained that is hosted on a SageMaker endpoint is
    invoked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sixth, look up the relevant conversation history in DynamoDB to ensure your
    answer is accurate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does this chat interface ensure it’s not hallucinating answers? How does
    it point to a set of data stored in a database? Through **retrieval augmented
    generation** (**RAG**), which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Using RAG to ensure high accuracy in LLM applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As explained in the original 2020 *(1)* paper, RAG is a way to retrieve documents
    relevant to a given query. Imagine your chat application takes in a question about
    a specific item in your database, such as one of your products. Rather than having
    the model make up the answer, you’d be better off retrieving the right document
    from your database and simply using the LLM to *stylize* the response. That’s
    where RAG is so powerful; you can use it to ensure the accuracy of your generated
    answers stays high, while keeping the customer experience consistent in both style
    and tone. Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.3 – RAG](img/B18942_Figure_15_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.3 – RAG
  prefs: []
  type: TYPE_NORMAL
- en: 'First, a question comes in from the left-hand side. In the top left, you can
    see a simple question, **Define “middle ear”**. This is processed by a query encoder,
    which is simply a language model producing an embedding of the query. This embedding
    is then applied to the index of a database, with many candidate algorithms in
    use here: **K** **Nearest Neighbors**, **Maximum Inner Product Search** (**MIPS**),
    and others. Once you’ve retrieved a set of similar documents, you can feed the
    best ones into the generator, the final model on the right-hand side. This takes
    the input documents and returns a simple answer to the question. Here, the answer
    is **The middle ear includes the tympanic cavity and the** **three ossicles.**'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, however, the LLM here doesn’t really define what the middle ear
    is. It’s actually answering the question, “what objects are contained within the
    middle ear?” Arguably, any definition of the middle ear would include its purpose,
    notably serving as a buffer between your ear canal and your inner ear, which helps
    you keep your balance and lets you hear. So, this would be a good candidate for
    expert **reinforcement learning with human feedback**, or **RLHF**, optimization.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 15**.3*, this entire RAG system is tunable. That means you
    can and should fine-tune the encoder and decoder aspects of the architecture to
    dial in model performance based on your datasets and query types. Another way
    to classify documents, as we’ll see, is generation!
  prefs: []
  type: TYPE_NORMAL
- en: Is generation the new classification?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we learned in [*Chapter 13*](B18942_13.xhtml#_idTextAnchor198), *Prompt
    Engineering*, there are many ways you can push your language model to output the
    type of response you are looking for. One of these ways is actually to have it
    classify what it sees in the text! Here is a simple diagram to illustrate this
    concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.4 – Using generation in place of classification](img/B18942_Figure_15_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15.4 – Using generation in place of classification
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the diagram, with traditional classification you train the
    model ahead of time to perform one task: **classification**. This model may do
    well on classification, but it won’t be able to handle new tasks at all. This
    key drawback is one of the main reasons why foundation models, and especially
    large language models, are now so popular: they are extremely flexible and can
    handle many different tasks without needing to be retrained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the right-hand side of *Figure 15**.4*, you can see we’re using the same
    text as the starting point, but instead of passing it to an encoder-based text
    model, we’re passing it to a decoder-based model and simply adding the instruction
    **classify this sentence into positive or negative sentiment**. You could just
    as easily say, “tell me more about how this customer really feels,” or “how optimistic
    is this home buyer?” or “help this homebuyer find a different house that meets
    their needs.” Arguably each of those three instructions is slightly different,
    veering away from pure classification and into more general application development
    or customer experience. Expect to see more of this over time! Let’s look at one
    more key technique for building applications with LLMs: keeping humans in the
    loop.'
  prefs: []
  type: TYPE_NORMAL
- en: Human-centered design for building applications with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We touched on this topic previously, in [*Chapter 2*](B18942_02.xhtml#_idTextAnchor034)*,*
    *Dataset Preparation: Part One*, [*Chapter 10*](B18942_10.xhtml#_idTextAnchor152)*,*
    *Fine-Tuning and Evaluating*, [*Chapter 11*](B18942_11.xhtml#_idTextAnchor167)*,*
    *Detecting, Mitigating, and Monitoring Bias*, and [*Chapter 14*](B18942_14.xhtml#_idTextAnchor217)*,*
    *MLOps for Vision and Language*. Let me say this yet again; I believe that human
    labeling will become even more of a competitive advantage that companies can provide.
    Why? Building LLMs is now incredibly competitive; you have both the open source
    and proprietary sides actively competing for your business. Open source options
    are from the likes of Hugging Face and Stability, while proprietary offerings
    are from AI21, Anthropic, and OpenAI. The differences between these options are
    questionable; you can look up the latest models at the top of the leaderboard
    from *Stanford’s HELM* *(2)*, which incidentally falls under their human-centered
    AI initiative. With enough fine-tuning and customization, you should generally
    be able to meet performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What then determines the best LLM applications, if it’s not the foundation
    model? Obviously, the end-to-end customer experience is critical, and will always
    remain so. Consumer preferences wax and wane over time, but a few tenets remain
    for general technology: speed, simplicity, flexibility, and low cost. With foundation
    models we can clearly see that customers prefer explainability and models they
    can trust. This means that application designers and developers should grapple
    with these long-term consumer preferences, picking solutions and systems that
    maximize them. As you may have guessed, that alone is no small task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond the core skill of designing and building successful applications, what
    else can we do to stay competitive in this brave new world of LLMs? I would argue
    that amounts to customizing your data. Focus on making your data and your datasets
    unique: singular in purpose, breadth, depth, and completeness. Lean into labeling
    your data with the best resources you can, and keep that a core part of your entire
    application workflow. This brings you to **continuous learning**, or the ability
    of the model to constantly get better and better based on signals from your end
    users.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s take a look at upcoming generative modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Other generative modalities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the 2022 ChatGPT moment, most of the technical world has been fascinated
    by the proposition of *generating novel content*. While this was always somewhat
    interesting, the meeting of high-performance foundation models with an abundance
    of media euphoria over the capabilities, combined with a post-pandemic community
    with an extremely intense fear of missing out, has led us to the perfect storm
    of a global fixation on generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: s this a good thing? Honestly, I’m happy to finally see the shift; I’ve been
    working on generating content with AI/ML models in some fashion since at least
    2019, and as a writer and creative person myself, I’ve always thought this was
    the most interesting part of machine learning. I was very impressed by David Foster’s
    book *(3)* on the topic. He’s just published an updated version of this to include
    the latest foundation models and methods! Let’s quickly recap some other types
    of modalities that are common in generative AI applications today.
  prefs: []
  type: TYPE_NORMAL
- en: Key modalities for generation outside of vision and language
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is my shortlist for the most interesting type of content generation outside
    of what we’ve seen throughout the book:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating music
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating PowerPoint slides, ads, and visuals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating product documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating architectural designs, and then building the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating movies, TV shows, and entertainment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating websites, games, and mobile apps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generating code should be no surprise to most of you; its core similarities
    to language generation make it a perfect candidate! Fine-tuning an LLM to spit
    out code in your language of choice is pretty easy; here’s my 2019 project *(4)*
    doing exactly that with the SageMaker example notebooks! Is the code great? Absolutely
    not, but fortunately, LLMs have come a long way since then. Many modern code-generating
    models are excellent, and thanks to a collaboration between Hugging Face and ServiceNow
    we have an open source model to use! This is called *StarCoder* and is available
    for free on Hugging Face right here: [https://huggingface.co/bigcode/starcoder](https://huggingface.co/bigcode/starcoder).'
  prefs: []
  type: TYPE_NORMAL
- en: What I love about using an open source LLM for code generation is that you can
    *customize it*! This means you can point to your own private code repositories,
    tokenize the data, update the model, and immediately train this LLM to generate
    code in the style of your organization! At the organizational level you might
    even do some continued pretraining on a open-source LLM for code generation on
    your own repositories to speed up all of your developers. We’ll take a look at
    more ways you can use LLMs to write your own code faster in the next section when
    we focus on AWS offerings, especially **Amazon Code** **Whisperer**. (*27*)
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the preceding content can all be great candidates for your own generative
    AI projects. Truly, just as we saw general machine learning moving from the science
    lab into the foundation of most businesses and projects, it’s likely that generative
    capabilities in some fashion will do the same.
  prefs: []
  type: TYPE_NORMAL
- en: Does that mean engineering roles will be eliminated? Honestly, I doubt it. Just
    as the rise of great search engines didn’t eliminate software engineering roles
    but made them more fun and doable for a lot of people, I’m expecting generative
    capabilities to do the same. They are great at searching many possibilities and
    quickly finding great options, but it’s still up to you to know the ins and outs
    of your consumers, your product, and your design. Models aren’t great at critical
    thinking, but they are good at coming up with ideas and finding shortcomings,
    at least in words.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve looked at other generative modalities at a very high level, let’s
    learn about AWS offerings for foundation models!
  prefs: []
  type: TYPE_NORMAL
- en: AWS offerings in foundation models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On AWS, as you’ve seen throughout the book, you have literally hundreds of
    ways to optimize your foundation model development and operationalization. Let’s
    now look at a few ways AWS is explicitly investing to improve the customer experience
    in this domain:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SageMaker JumpStart Foundation Model Hub**: Announced in preview at re:Invent
    2022, this is an option for pointing to foundation models nicely packaged in the
    SageMaker environment. This includes both open source models such as BLOOM and
    Flan-T5 from Hugging Face, and proprietary models such as AI21 Jurassic. A list
    of all the foundation models is available here *(5)*. To date, we have nearly
    20 foundation models, all available for hosting in your own secure environments.
    Any data you use to interact with or fine-tune models on the Foundation Model
    Hub is not shared with providers. You can also optimize costs by selecting the
    instances yourself. We have tens of example notebooks pointing to these models
    for training and hosting across a wide variety of use cases available here *(6)*
    and elsewhere. For more information about the data the models were trained on,
    you can read about that in the playground directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Bedrock**: If you have been watching AWS news closely in early 2023,
    you may have noticed a new service we announced for foundation models: Amazon
    Bedrock! As discussed in this blog post *(7)* by Swami Sivasubramanian, Bedrock
    is a service that lets you interact with a variety of foundation models through
    a serverless interface that stays secure. Said another way, Bedrock provides a
    point of entry for multiple foundation models, letting you get the best of all
    possible providers. This includes AI start-ups such as AI21, Anthropic, and Stability.
    Interacting with Bedrock means invoking a serverless experience, saving you from
    dealing with the lower-level infrastructure. You can also fine-tune your models
    with Bedrock!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Titan**: Another model that will be available through Bedrock is *Titan*,
    a new large language model that’s fully trained and managed by Amazon! This means
    we handle the training data, optimizations, tuning, debiasing, and all enhancements
    for getting you results with large language models. Titan will also be available
    for fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amazon Code Whisperer: As you may have seen, Code Whisperer is an AWS service
    announced in 2022 and made generally available in 2023\. Interestingly it seems
    to tightly couple with a given development environment, taking the entire context
    of the script you are writing and generating recommendations based on this. You
    can write pseudo-code, markdown, or other function starts, and using keyboard
    shortcuts invoke the model. This will send you a variety of options based on the
    context of your script, letting you ultimately select the script that makes the
    most sense for you! Happily, this is now supported for both Jupyter notebooks
    and SageMaker Studio; you can read more about these initiatives from AWS Sr Principal
    Technologist Brain Granger, co-founder of Project Jupyter. Here’s Brian’s blog
    post on the topic: [https://aws.amazon.com/blogs/machine-learning/announcing-new-jupyter-contributions-by-aws-to-democratize-generative-ai-and-scale-ml-workloads/](https://aws.amazon.com/blogs/machine-learning/announcing-new-jupyter-contributions-by-aws-to-democratize-generative-ai-and-scale-ml-workloads/)
    Pro tip: Code Whisperer is free to individuals! Close readers of Swami’s blog
    post above will also notice updates to our latest ML infrastructure, like the
    second edition of the *inferentia chip*, *inf2*, and a trainium instance with
    more bandwidth, *trn1n*.'
  prefs: []
  type: TYPE_NORMAL
- en: Close readers of Swami’s blog post will also notice updates to our latest ML
    infrastructure, such as the second edition of the *inferentia chip*, *inf2*, and
    a Trainium instance with more bandwidth, *trn1n*. We also released our code generation
    service, *CodeWhisperer*, at no cost to you!
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned about some of the AWS offerings in this space, let’s
    hypothesize about the future of foundation models.
  prefs: []
  type: TYPE_NORMAL
- en: The future of foundation models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To me, a few key points seem incredibly obvious for where foundation models
    are trending:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Intense competition will continue between open source and proprietary model
    providers*. As mentioned previously, right now we are in a perfect storm of hyper-focus
    on foundation models from most of the technology industry worldwide. A key axis
    here is proprietary versus open source. As suggested by this leaked Google document
    on May 4 *(8)*, the capabilities of the open source world are advancing and in
    many cases, open source options are better than proprietary ones. They actually
    describe open source models as “pound-for-pound more capable.” This means that
    for the size of the model itself, the smaller ones produced by the open source
    world are better in a per-byte-size comparison.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model consumers will get more options at a lower cost if they are flexible
    about model providers*. To me, the result of this intense competition is clear;
    you’ll get more and more options at a lower price point as time goes by! This
    means the clear right choice here, as a model consumer, is not getting locked
    into a single model. As the brave new world of foundation models swims on, put
    you and your teams in the best possible scenario and stay flexible to new models
    as they emerge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Models are getting smaller and stronger*. In no small part thanks to the drive
    enthusiasm in the open-source community coming from Stable Diffusion, foundation
    models are now decreasing in size but increasing in accuracy. This means today
    you can accomplish with a 13B-parameter model, like *StableVicuna* (*28*) what
    a few years ago took a 175B-parameter model to do. Keep this in mind as you’re
    designing apps!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Make proprietary data and human feedback your strength*. To best take advantage
    of all this competition, I’d suggest leaning into your data. As long as you’re
    open to the latest and greatest model backbone, using your own proprietary data
    and human feedback as a key investment area lets you differentiate from the rest
    of the market. Make sure you and your teams are labeling data early and often,
    using as much of your unique perspectives and expertise as possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Security is critical for interacting with foundation models*. This signal
    shows up so clearly across the market, consumers very strongly prefer environments
    that secure their data and do not allow sharing with model providers, including
    both model queries and fine-tuning assets. This is in some sense a derivative
    of making proprietary data and human feedback your strength; in tomorrow’s world,
    your data becomes your selling point. Protecting this in the form of your own
    foundation model, in terms of keeping your model unbiased, safe from jailbreak
    attacks, and not able to generate hate speech will continue to be important. It
    seems like an entirely new portfolio of security measures is necessary to ensure
    foundation model applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Foundation models are becoming the new database, and natural language is the
    new programming language*. I am expecting most applications in the next twelve
    months to include foundation models as a new type of database. Rather than just
    storing your records on disk, now you can crunch through them with neural networks,
    learn mappings and relationships at scales that single humans can’t compete with,
    and interact with humans in natural language. Optimizing, scaling, debiasing,
    ensuring accuracy, and taking costs out of this equation will be the work of the
    next few years.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I’m also expecting many more foundation models, across modalities, user bases,
    languages, technologies, goals, and domains. As the cost of training and producing
    them goes down, thanks to intense competition, we might see more and more entry-level
    developers entering this market. With that in mind, let’s close out the book with
    parting thoughts on the future of pretraining.
  prefs: []
  type: TYPE_NORMAL
- en: The future of pretraining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a full roundabout, let’s take a look at some skeptical trends that provide
    a sense of caution and critical evaluation of the recent goldrush in foundation
    models. One of the most prominent examples is Geoffrey Hinton quitting Google
    (*29*) to warn about the dangers of AI development, calling for a 6-month moratorium
    (*30*) on all foundation model research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Personally, I think this pause fails to recognize what drove the hype in the
    first place: the invisible hand of AI economics. Landing high-paying tech jobs
    is not easy; getting and maintaining visas for the US and similar countries is
    non-trivial. Growing your career in an industry with some of the most intelligent
    and driven people, with ground-shattering fast-paced changes is anything but simple.
    Foundation models and all improvements in technology evolve because humans need
    to prove themselves, grow their careers, and provide for their families. Asking
    to pause experiments on large-scale training is almost the same as asking massive
    swaths of young people to stop putting all of their passion, skills, and time
    into some of the best possible options for their own career development. Obviously,
    that’s the opposite of what''s in their best interest to do!'
  prefs: []
  type: TYPE_NORMAL
- en: However, even within the camp of those who do support continued research on
    foundation models, not everyone is overly optimistic about the continued value
    of pretraining Transformers. Yann LeCun, the same scientist we mentioned in [*Chapter
    1*](B18942_01.xhtml#_idTextAnchor016) who developed MNIST, considers self-supervised
    learning to be “*the dark matter of intelligence*. (*9*)” Since the viral ChatGPT
    global moment LeCun has been very critical about the bounded performance of large
    language and auto-regressive models, given their core strength is simply predicting
    the next most likely token in a sequence and not actually understanding the world
    in any reliable way. Instead, he suggests we build AI models that learn how to
    reason, including developing hierarchical representations of action plans.
  prefs: []
  type: TYPE_NORMAL
- en: He’s not alone in this cautionary note. MacArthur Fellow at the University of
    Washington and the Allen Institute for Artificial Intelligence Yejin Choi *(10)*
    recently shared her thoughts in a TED talk about why “AI is incredibly smart –
    and shockingly stupid.” *(11)* Choi’s multiple decades of work on common sense
    reasoning shows that while NLP models may solve some limited tasks well, they
    still struggle with extremely basic human tasks, such as understanding the difference
    between reality and fiction or hypothesizing and planning, the value of simplicity,
    and basic logical mapping of the world around them.
  prefs: []
  type: TYPE_NORMAL
- en: To me, the divide here is clear. On the one hand, we have a decades-long push
    for the best and most impressive artificial representations of intelligence. On
    the other hand, we have massive industries of business, technology, and academia
    with millions of human beings actively trying to build applications and provide
    value that serve as the foundation for their careers and their team’s longevity.
    Will this economy of technological development create the most intelligent machines?
    Possibly. Will it build applications and businesses that provide value to consumers?
    Certainly. Will these two related and yet distinct vectors continue to overlap
    for decades to come? Without a doubt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we close things out, let me mention a few interesting technical trends
    of note to pretraining in particular:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous pretraining**: If your foundation model benefits from the latest
    data, and this is available in a constant stream of updates, why not build a continuous
    loop of ingestion and training to keep your application performant? That’s the
    core proposal in this 2022 paper *(12)*. I’d imagine that some applications benefit
    from this constant stream of training, especially when parameter-efficient fine-tuning
    (*31*) makes the cost of this more appealing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieval pretraining**: The demand for accurate generated text will continue
    increasing as generative AI applications expand. This approach, suggested by DeepMind,
    applies the retrieval process *(14)* to pretraining and gets similar performance
    to GPT-3 while using 25x fewer parameters, making it much more efficient and appealing
    for both training and hosting. I’m expecting this basic concept of retrieving
    tokens during pretraining to evolve with RAG *(15)* to create LLMs that provide
    much higher accuracy guarantees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More universal pretraining regimes**: As you are probably aware, much of
    the media and open source attention seems to focus only on the models and new
    state-of-the-art performance in low-level tasks. I think this is a problem because
    it misses the foundation of where these models come from and how they are built:
    pretraining itself. This indicates that if pretraining can become more accessible
    and more universal, we can move from a fixation on the model to broad support
    for pretraining more generally. A few steps exist already in this direction, such
    as Data2Vec *(16)*, which proposes a general framework for self-supervised learning
    across vision, speech, and language. Another attempt is **UL2**, **Unifying Language
    Learning Paradigms**. This Google Brain team suggests combining diverse pretraining
    paradigms and then switching to different paradigms during fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**More languages, please**! One of the first conferences I attended in person
    after the pandemic was the Association of Computational Linguists in 2022 *(18)*.
    I was happily surprised by their focus on being multi-lingual, strongly and admirably
    pushing for capabilities that bridge languages and extend NLP capabilities across
    endangered languages and communities worldwide. Admirably the UN declared 2022-2023
    the International Decade of Indigenous Languages, estimating that at least 50%
    of spoken languages today will be extinct or seriously endangered by 2100\. (*32*).
    This will continue to be an important topic in foundation models because it serves
    as a bottleneck for technical adoption and innovation. One step in this direction
    is Tsinghua’s GLM-130B *(19)*, a model explicitly pretrained with Chinese and
    English. Another notable bilingual model is Hugging Face’s BLOOM *(20), which
    was trained on 46 natural and 13 programming languages.* Other similar projects
    provide capabilities in singular non-English languages, such as LightOn’s French
    model PAGnol *(21)*, a Japanese masked-language model *(22)*, a German LLM *(23)*,
    and more. There are even calls for a BritGPT *(24)*, to bring generative capabilities
    into British styles of speaking and conversing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On that note, let’s close out the book with the final conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What a journey! To those of you who made it to the end with me, thank you so
    much for the time, creativity, and energy you’ve put into studying my words and
    thoughts. I hope at least some of the insights were worth your time, and that
    the mistakes weren’t too glaring.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we walked through the entire process of pretraining foundation
    models, looking at key use cases and examples from vision and language, and understanding
    core capabilities on AWS to build your applications and projects. I love hearing
    from my audience, so please, reach out to me and stay in touch! I’m active on
    LinkedIn; you can always ping me with questions or comments. I run a weekly Twitch
    show on Generative AI, so you can always find there me and hop in with feedback
    or comments. *(25)*
  prefs: []
  type: TYPE_NORMAL
- en: And of course, you can always talk to your teams at AWS to reach out to me directly!
    I love meeting customers, thinking through architectural choices, communicating
    your needs to our service teams, and thinking big with you about how we can build
    a better tomorrow. Let me know what you’re itching to build!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please go through the following content for more information on the topics
    covered in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: [https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'HELM: [https://crfm.stanford.edu/helm/latest/](https://crfm.stanford.edu/helm/latest/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play
    1st Edition: [https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1492041947](https://www.amazon.com/Generative-Deep-Learning-Teaching-Machines/dp/1492041947)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'aws-samples/amazon-sagemaker-architecting-for-ml: [https://github.com/aws-samples/amazon-sagemaker-architectingfor-ml/tree/master/Example-Project](https://github.com/aws-samples/amazon-sagemaker-architectingfor-ml/tree/master/Example-Project)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Getting started with Amazon SageMaker JumpStart: [https://aws.amazon.com/sagemaker/jumpstart/gettingstarted/?sagemaker-jumpstart-cards.sort-by=item.additionalFields.priority&sagemaker-jumpstart-cards.sort-order=asc&awsf.sagemakerjumpstart-filter-product-type=*all&awsf.sagemaker-jumpstartfilter-text=*all&awsf.sagemaker-jumpstart-filter-vision=*all&awsf.sagemaker-jumpstart-filter-tabular=*all&awsf.sagemaker-jumpstartfilter-audio-tasks=*all&awsf.sagemaker-jumpstart-filtermultimodal=*all&awsf.sagemaker-jumpstart-filter-RL=*all](https://aws.amazon.com/sagemaker/jumpstart/gettingstarted/?sagemaker-jumpstart-cards.sort-by=item.additionalFields.priority&sagemaker-jumpstart-cards.sort-order=asc&awsf.sagemakerjumpstart-filter-product-type=*all&awsf.sagemaker-jumpstartfilter-text=*all&awsf.sagemaker-jumpstart-filter-vision=*all&awsf.sagemaker-jumpstart-filter-tabular=*all&awsf.sagemaker-jumpstartfilter-audio-tasks=*all&awsf.sagemaker-jumpstart-filtermultimodal=*all&awsf.sagemaker-jumpstart-filter-RL=*all)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'aws/amazon-sagemaker-examples: [https://github.com/aws/amazon-sagemaker-examples/tree/main/](https://github.com/aws/amazon-sagemaker-examples/tree/main/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Announcing New Tools for Building with Generative AI on AWS: [https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/](https://aws.amazon.com/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Google "We Have No Moat, And Neither Does OpenAI": [https://www.semianalysis.com/p/google-we-have-no-moat-and-neither](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Self-supervised learning: The dark matter of intelligence: [https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Yejin Choi: [https://homes.cs.washington.edu/~yejin/](https://homes.cs.washington.edu/~yejin/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Why AI is incredibly smart and shockingly stupid: [https://www.ted.com/talks/yejin_choi_why_ai_is_incredibly_smart_and_shockingly_stupid/c?language=en](https://www.ted.com/talks/yejin_choi_why_ai_is_incredibly_smart_and_shockingly_stupid/c?language=en)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Continual PreTraining Mitigates Forgetting in Language and Vision: [https://arxiv.org/pdf/2205.09357.pdf](https://arxiv.org/pdf/2205.09357.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LoRA: Low-Rank Adaptation of Large Language Models: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Improving language models by retrieving from trillions of tokens: [https://arxiv.org/pdf/2112.04426.pdf](https://arxiv.org/pdf/2112.04426.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: [https://arxiv.org/pdf/2005.11401.pdf](https://arxiv.org/pdf/2005.11401.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'data2vec: A General Framework for Self-supervised Learning in Speech, Vision
    and Language: [https://arxiv.org/pdf/2202.03555.pdf](https://arxiv.org/pdf/2202.03555.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'UL2: Unifying Language Learning Paradigms: [https://arxiv.org/pdf/2205.05131.pdf](https://arxiv.org/pdf/2205.05131.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ACL 2022: [https://www.2022.aclweb.org/](https://www.2022.aclweb.org/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL: [https://openreview.net/pdf?id=-Aw0rrrPUF](https://openreview.net/pdf?id=-Aw0rrrPUF)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'BLOOM: A 176B-Parameter Open-Access Multilingual Language Model: [https://arxiv.org/pdf/2211.05100.pdf](https://arxiv.org/pdf/2211.05100.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LightOn releases PAGnol, the largest French Language Model: [https://medium.com/@LightOnIO/lighton-releases-pagnol-the-largest-french-language-model-f50b719352ab](https://medium.com/@LightOnIO/lighton-releases-pagnol-the-largest-french-language-model-f50b719352ab)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A Japanese Masked Language Model for Academic Domain: [https://aclanthology.org/2022.sdp-1.16.pdf](https://aclanthology.org/2022.sdp-1.16.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cedille.ai launches the largest language model in German for text generation:
    [https://cedille.ai/blog/cedille-ai-launches-the-largest-language-model-in-german-for-text-generation](https://cedille.ai/blog/cedille-ai-launches-the-largest-language-model-in-german-for-text-generation)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'UK needs its own ‘BritGPT’ or will face an uncertain future, MPs hear: [https://www.theguardian.com/business/2023/feb/22/uk-needs-its-own-britgpt-or-will-face-an-uncertain-future-mps-hear](https://www.theguardian.com/business/2023/feb/22/uk-needs-its-own-britgpt-or-will-face-an-uncertain-future-mps-hear)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'AWS Schedule: [https://www.twitch.tv/aws/schedule?seriesID=340be301-27dc-42c6-890a-302cd13899af](https://www.twitch.tv/aws/schedule?seriesID=340be301-27dc-42c6-890a-302cd13899af)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'deepset-ai/haystack: [https://github.com/deepset-ai/haystack](https://github.com/deepset-ai/haystack)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'AWS- Overview: [https://aws.amazon.com/codewhisperer/](https://aws.amazon.com/codewhisperer/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hugging Face: https://huggingface.co/CarperAI/stable-vicuna-13b-delta'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Godfather of AI: [https://www.theguardian.com/technology/2023/may/02/geoffrey-hinton-godfather-of-ai-quits-google-warns-dangers-of-machine-learning](https://www.theguardian.com/technology/2023/may/02/geoffrey-hinton-godfather-of-ai-quits-google-warns-dangers-of-machine-learning)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pause Giant AI Experiments: An Open Letter: [https://futureoflife.org/open-letter/pause-giant-ai-experiments/](https://futureoflife.org/open-letter/pause-giant-ai-experiments/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'huggingface/peft: [https://github.com/huggingface/peft](https://github.com/huggingface/peft)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Department of Economic and Social AffairsIndigenous Peoples: [https://www.un.org/development/desa/indigenouspeoples/indigenous-languages.html](https://www.un.org/development/desa/indigenouspeoples/indigenous-languages.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
