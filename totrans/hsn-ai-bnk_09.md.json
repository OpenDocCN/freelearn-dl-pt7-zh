["```py\n'''*************************************\n#1\\. Import libraries and key variable values\n\n'''\nfrom searchtweets import ResultStream, gen_rule_payload, load_credentials\nfrom searchtweets import collect_results\nimport json\nimport os\n\nscript_dir = os.path.dirname(__file__)\n#Twitter search commerical accounts credential\npremium_search_args = load_credentials(\"~/.twitter_keys.yaml\",\n                                      env_overwrite=False)\nMAX_RESULTS=500 #maximum at 500\n\n#list of companies in the same industry\n...\n\n'''*************************************\n#2\\. download tweets of each company\n\n'''\nfor comp in comp_list:\n   ...\n```", "```py\n'''*************************************\n#1\\. Import libraries and key variable values\n\n'''\nimport json\nimport os\nimport re\nimport sqlite3\nimport 7A_lib_cnt_sentiment as sentiment\n\n#db file\ndb_path = 'parsed_tweets.db'\ndb_name = 'tweet_db'\n\n#sql db\n...\n#load tweet json\n...\n#loop through the tweets\n    ...\n    for tweet in data:\n        ...\n        tweet_txt_pos,tweet_txt_neg = sentiment.cnt_sentiment(tweet_txt)\n        keywords,sentences_list,words_list = \\\n                                           sentiment.NER_topics(tweet_txt)\n        ...\n        if len(url_link)>0:\n            ...\n            url_txt = sentiment.url_to_string(url)\n            temp_tweet_link_txt_pos, temp_tweet_link_txt_neg = \\\n                                           sentiment.cnt_sentiment(url_txt)\n            link_keywords,link_sentences_list,link_words_list = \\\n                                           sentiment.NER_topics(tweet_txt)\n            ...\n```", "```py\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\n...\n#cal the positive and negative sentiment words given the text\ndef cnt_sentiment(text_to_be_parsed):\n    ...\n\ndef noun_phrase(sentence,item_list,lower):\n   ...\n\n#NER\nimport spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport math\n\n#text has to be less than 1000000\ndef NER_topics(text_to_be_parsed):\n    ...\n    MAX_SIZE =100000\n    ...\n    for nlp_cnt in range(number_nlp):\n        start_pos = nlp_cnt*MAX_SIZE\n        end_pos = min(MAX_SIZE,txt_len-start_pos)+start_pos-1\n        txt_selected = text_to_be_parsed[start_pos:end_pos]\n        ...\n        sentences_list = [x for x in article.sents]\n        full_sentences_list+=sentences_list\n        for sent in sentences_list:\n            phrases_list =[]\n            phases_list,items_list = noun_phrase(sent, items_list, \\\n                                                 lower=True)\n     ...\n\n#convert the URL's content into string\ndef url_to_string(url):\n    ...\n```", "```py\n'''*************************************\n#1\\. Import libraries and key variable values\n\n'''\nimport sqlite3\nimport pandas as pd\nimport plotly\nimport plotly.graph_objs as go\nimport quandl\nimport json\n\n# Create your connection.\ndb_path = 'parsed_tweets.db'\ncnx = sqlite3.connect(db_path)\ndb_name = 'tweet_db'\n\n'''*************************************\n#2\\. Gauge the sentiment of each security\n\n'''\n...\nsql_str = ...\n...\nprint('Sentiment across securities')\nfield_list = ['positive','negative']\nfor sec in sec_list:\n    ...\n```", "```py\n#run it on different companies\nprint('Retrieve data')\ndf_comp = pd.read_csv('ticker_companyname.csv')\ncorr_results={}\n\nfor index, row in df_comp.iterrows():\n    tkr = row['ticker']\n    name = row['name']\n\n    target_sec = '\"'+name +'\"data.json'\n\n    corr_result = price_sentiment(tkr,target_sec,date_range)\n    try:\n        corr_results[name]=corr_result['close'][0]\n    except Exception:\n        continue\n\nf_corr = open('corr_results.json','w')\njson.dump(corr_results,f_corr)\nf_corr.close()\n```", "```py\n'''*************************************\n#1\\. Import relevant libraries and variables\n\n'''\n#custom made function\nimport 7B_lib_entitiesExtraction as entitiesExtraction\nimport 7B_lib_parser_pdf as pdf_parser\nimport json\nimport sqlite3\n\npdf_path = 'annualrpt/NYSE_DUK_2017.pdf'\n...\n```", "```py\n'''*************************************\n#2\\. NLP\n\n'''\n#Named Entity Extraction\nprint('ner')\n#see if we need to convert everything to lower case words - we keep the original format for this case\nlower=False\ncommon_words, sentences, words_list,verbs_list = entitiesExtraction.NER_topics(text,lower)\nentities_in_sentences = entitiesExtraction.org_extraction(text)\n...\n#create this list to export the list of ent and cleanse them\n...\nprint('looping sentences')\nfor sentence in entities_in_sentences:\n    ents_dict[sentence_cnt] = {}\n    for entity in sentence:\n        ...\n        if ent_type in( 'ORG','PERSON','FAC','NORP','GPE','LOC','PRODUCT'):\n        ...\n    #handle other type\n    ...\n```", "```py\n'''*************************************\n#1\\. Import relevant libraries and variables\n\n'''\n#generate network\nimport sqlite3\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n#db file\ndb_path = 'parsed_network.db'\ndb_name = 'network_db'\n\n#sql db\nconn = sqlite3.connect(db_path)\nc = conn.cursor()\n\n...\n\nnetwork_dict={}\nedge_list=[]\ncurr_source =''\ncurr_entity = ''\norg_list = []\nperson_list = []\n\n'''*************************************\n#2\\. generate the network with all entities connected to Duke Energy - whose annual report is parsed\n\n'''\ntarget_name = 'Duke Energy'\n#loop through the database to generate the network format data\nfor index, row in df_org.iterrows():\n    ...\n\n#Generate the output in networkX\nprint('networkx')\n\n#output the network\nG = nx.from_edgelist(edge_list)\npos = nx.spring_layout(G)\nnx.draw(G, with_labels=False, nodecolor='r',pos=pos, edge_color='b')\nplt.savefig('network.png')\n```", "```py\n#Generate output for Neo4j\nprint('prep data for Neo4j')\nf_org_node=open('node.csv','w+')\nf_org_node.write('nodename\\n')\n\nf_person_node=open('node_person.csv','w+')\nf_person_node.write('nodename\\n')\n\nf_vertex=open('edge.csv','w+')\nf_vertex.write('nodename1,nodename2,weight\\n')\n...\n```", "```py\nsudo cp '[path]/edge.csv' /var/lib/Neo4j/import/edge.csv\nsudo cp '[path]/node.csv' /var/lib/Neo4j/import/node.csv\n\nsudo service Neo4j restart\n```", "```py\nhttp://localhost:7474/browser/\n```", "```py\nMATCH (n) DETACH DELETE n;\n\nUSING PERIODIC COMMIT\nLOAD CSV WITH HEADERS FROM \"file:///node.csv\" AS row\nCREATE (:ENTITY {node: row.nodename});\n\nCREATE INDEX ON :ENTITY(node);\n\nUSING PERIODIC COMMIT\nLOAD CSV WITH HEADERS FROM \"file:///edge.csv\" AS row\nMATCH (vertex1:ENTITY {node: row.nodename1})\nMATCH (vertex2:ENTITY {node: row.nodename2})\nMERGE (vertex1)-[:LINK]->(vertex2);\n\nMATCH (n:ENTITY)-[:LINK]->(ENTITY) RETURN n;\n```"]