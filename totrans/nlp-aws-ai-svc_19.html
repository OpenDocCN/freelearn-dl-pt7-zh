<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer292">
			<h1 id="_idParaDest-184"><em class="italic"><a id="_idTextAnchor189"/>Chapter 16</em>: Improving the Accuracy of PDF Batch Processing</h1>
			<p>Congratulations on getting so far in this book! At this point, you are an advanced builder of real-world applications that harness the power of NLP and AI to deliver tangible business benefits. You may not have realized it but the topics we've covered so far – and will continue to cover – address some of the most popular, in-demand business challenges that we have helped our customers with. <strong class="bold">Intelligent Document Processing</strong> (<strong class="bold">IDP</strong>) is a very hot requirement today and is something prevalent across almost every industry type. We started reading about advanced concepts from <a href="B17528_13_Final_SB_ePub.xhtml#_idTextAnchor151"><em class="italic">Chapter 13</em></a>, <em class="italic">Improving the Accuracy of Document Processing Workflows</em> onward, and we saw how <strong class="bold">Amazon A2I</strong> (<a href="https://aws.amazon.com/augmented-ai/">https://aws.amazon.com/augmented-ai/</a>) plays a key role in making human reviews of your ML workflows easier and more accurate, enabling active learning in the process.</p>
			<p>In this chapter, we will tackle an operational need that has been around for a while, is ubiquitous, and yet organizations struggle to address it efficiently. This is known as <strong class="bold">PDF batch processing</strong>. Think of this as setting up an automated document processing workflow (similar to what we built in the previous chapters) but with the added flexibility of bulk processing PDF documents, combined with the intelligence to automatically route specific text passages in the document for human reviews due to low - confidence detection caused by illegible or erroneous text.</p>
			<p>By now, due to your diligent efforts in implementing advanced AI solutions, <strong class="bold">LiveRight Holdings</strong> has seen its profitability go through the roof. This growth has resulted in LiveRight spinning up a couple of subsidiaries as independent organizations in their own right, and the board has decided that all three companies will go public in the mid-term. You have been promoted to Chief Architect of Operations at LiveRight, and the CIO has tasked you with building the necessary components to automate the registration process for the three companies with the <strong class="bold">Securities and Exchanges Commission</strong> (<strong class="bold">SEC</strong>) as publicly traded companies.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Introducing the PDF batch processing use case </li>
				<li>Building the solution</li>
			</ul>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor190"/>Technical requirements</h1>
			<p>For this chapter, you will need access to an AWS account, which you can get by going to <a href="https://aws.amazon.com/console/">https://aws.amazon.com/console/</a>. Please refer to the <em class="italic">Signing up for an AWS account</em> subsection within the <em class="italic">Setting up your AWS environment</em> section of <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a><em class="italic">, Introducing Amazon Textract</em>, for detailed instructions on how to sign up for an AWS account and sign into the <strong class="bold">AWS Management Console</strong>.</p>
			<p>The Python code and sample datasets for the solution discussed in this chapter can be found in this book's GitHub repository: <a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2016">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/tree/main/Chapter%2016</a>.</p>
			<p>Check out the following video to see the Code in Action at <a href="https://bit.ly/3nobrCo">https://bit.ly/3nobrCo</a>.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor191"/>Introducing the PDF batch processing use case </h1>
			<p>To determine what the architecture will look like, you talk to your accounting department <a id="_idIndexMarker897"/>to understand the process for registering companies with the SEC. As per the process, the accounting department will generate PDF documents using the SEC's template for registration, also known as <em class="italic">Form S20</em> (<a href="https://www.sec.gov/files/forms-20.pdf">https://www.sec.gov/files/forms-20.pdf</a>). The process also involves creating all the supporting documentation, along with the registration, which will be sent together to the SEC <a id="_idIndexMarker898"/>using an API call. LiveRight's <strong class="bold">Partner Integration</strong> team has the handshake with SEC in place, and they need the form data to be available in an <strong class="bold">Amazon DynamoDB</strong> (<a href="https://aws.amazon.com/dynamodb/">https://aws.amazon.com/dynamodb/</a>) table that they <a id="_idIndexMarker899"/>will consume to create the message call to the SEC API. </p>
			<p>However, before making the data available to the Partner Integration team, the accounting team mentioned that they need to review a collection of text lines that have been detected in the PDF document, specifically the ones that may not have been interpreted correctly due to document quality issues.</p>
			<p>With this input, you realize that you need to add a batch component to your document processing solution. This will enable bulk detection of text from PDF documents and routing of those text lines that fall below a confidence threshold to a human review loop comprised of the accounting team members. You decide to use the asynchronous document text detection API from <strong class="bold">Amazon Textract</strong> to leverage its pre-trained ML model for <a id="_idIndexMarker900"/>text extraction from PDF documents, <strong class="bold">Amazon A2I</strong>, to set up a human <a id="_idIndexMarker901"/>workflow to review and modify text detected with a confidence of less than 95%, and Amazon DynamoDB to store the original detected text, along with the corrections for consumption by the Partner Integration team. </p>
			<p>We will be <a id="_idIndexMarker902"/>building our solution using an Amazon SageMaker Jupyter notebook that will allow us to review the code and results as we execute it step by step. We will be performing the following tasks:</p>
			<ol>
				<li>As a first step, we will create a private labeling workforce for human review using the Amazon SageMaker Console. For more details, please refer to <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-private.html">https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-private.html</a>.</li>
				<li>We will start the solution workflow by inspecting the sample registration forms available to us when we clone the GitHub repository for this chapter. We will use Amazon Textract to start an asynchronous text detection job.</li>
				<li>Then, we will get the results for the text detection job, select specific lines from the document, and inspect the detection confidence scores.</li>
				<li>We will set up an Amazon A2I human review loop using the tabular task UI template and send the text lines from each document for all the documents to the human loop.</li>
				<li>Logging in as a private worker, we will work on the allocated review task, making changes to the text lines with low - confidence detection scores for all the documents.</li>
				<li>We will upload the detected and corrected text lines to a DynamoDB table for downstream processing.</li>
			</ol>
			<p>Now that we've got the context for the exercise and gone over our intended process, let's start building the solution.</p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor192"/>Building the solution</h1>
			<p>In the previous section, we introduced our use case, which is to submit company registrations <a id="_idIndexMarker903"/>for public trading to the SEC, covered the architecture of the solution we will be building, and briefly walked through the solution components and workflow steps. In this section, we will get right down to business and start executing the tasks that will build our solution. But first, there are some prerequisites we have to take care of. </p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor193"/>Setting up for the solution build</h2>
			<p>If you have not done so in the previous chapters, you will have to create an Amazon SageMaker <a id="_idIndexMarker904"/>Jupyter notebook, as well as setting up <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) permissions for that notebook <a id="_idIndexMarker905"/>role to access the AWS services we will use in this notebook. After that, you will need to clone this book's GitHub repository (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services</a>), create an Amazon S3 (<a href="https://aws.amazon.com/s3/">https://aws.amazon.com/s3/</a>) bucket, and provide the bucket's name in the notebook to start execution.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Please ensure that you have completed the tasks mentioned in the <em class="italic">Technical requirements</em> section.</p>
			<p>Follow these steps to complete these tasks before we execute the cells from our notebook:</p>
			<ol>
				<li value="1">Follow the instructions documented in the <em class="italic">Creating an Amazon SageMaker Jupyter notebook instance</em> subsection in the <em class="italic">Setting up your AWS environment</em> section of <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, to create your Jupyter notebook instance.<p class="callout-heading">IAM role Permissions While Creating Amazon SageMaker Jupyter Notebooks</p><p class="callout">Accept the default for the IAM role at notebook creation time to allow access for any S3 bucket. </p></li>
				<li>Once you have created the notebook instance and its status is <strong class="bold">InService</strong>, click on <strong class="bold">Open Jupyter</strong> in the <strong class="bold">Actions</strong> menu heading for the notebook instance. </li>
				<li>This will take you to the <strong class="bold">home</strong> folder of your notebook instance. </li>
				<li>Click on <strong class="bold">New</strong> and select <strong class="bold">Terminal</strong>.</li>
				<li>In the terminal window, type <strong class="source-inline">cd SageMaker</strong> and then <strong class="source-inline">git clone https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services</strong>.</li>
				<li>Now, exit <a id="_idIndexMarker906"/>the terminal window, go back to the home folder, and you will see a folder called <strong class="source-inline">Natural-Language-Processing-with-AWS-AI-Services</strong>. Click this folder to bring up the chapter folders and click <strong class="source-inline">Chapter 16</strong>.</li>
				<li>Open this folder by clicking on it. You should see a notebook called <strong class="source-inline">Improve-accuracy-of-pdf-processing-with-Amazon-Textract-and-Amazon-A2I-forGitHub.ipynb</strong>.</li>
				<li>Open this notebook by clicking on it. </li>
				<li>Follow the steps in this notebook that correspond to the next few subheadings in this section by executing one cell at a time. Please read the descriptions that were added to each notebook cell.</li>
			</ol>
			<p>Next, we'll cover some additional IAM prerequisites. </p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor194"/>Additional IAM prerequisites</h2>
			<p>We have <a id="_idIndexMarker907"/>to enable additional policies for our SageMaker Notebook role. Please refer to the <em class="italic">Changing IAM permissions and trust relationships for the Amazon SageMaker Notebook execution role</em> subsection in the <em class="italic">Setting up your AWS environment</em> section in <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, for detailed instructions on how to execute the following steps:</p>
			<ol>
				<li value="1">Please attach the <strong class="source-inline">TextractFullAccess</strong> and<strong class="source-inline"> AmazonAugmentedAIFullAccess</strong> policies to your Amazon SageMaker Notebook IAM role if you've not already done so.</li>
				<li>Add an <strong class="source-inline">iam:PassRole</strong> permission as an inline policy to your SageMaker Notebook execution role:<p class="source-code">{ "Version": "2012-10-17", "Statement": [ {</p><p class="source-code">  "Action": [</p><p class="source-code">      "iam:PassRole"</p><p class="source-code">  ],</p><p class="source-code">  "Effect": "Allow",</p><p class="source-code">  "Resource": "&lt;your sagemaker notebook execution role ARN"&gt;</p><p class="source-code">  }</p><p class="source-code"> ]</p><p class="source-code">}</p></li>
			</ol>
			<p>Now that <a id="_idIndexMarker908"/>we have set up our notebook and set up an IAM role to run the walkthrough notebook, we will create the private labeling workforce.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor195"/>Creating a private team for the human loop</h2>
			<p>Refer to <em class="italic">Step 0</em> in the notebook (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2016/Improve-accuracy-of-pdf-processing-with-Amazon-Textract-and-Amazon-A2I-forGitHub.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2016/Improve-accuracy-of-pdf-processing-with-Amazon-Textract-and-Amazon-A2I-forGitHub.ipynb</a>) for the instructions we will execute now. In this section, we will <a id="_idIndexMarker909"/>create a private team using the Amazon SageMaker labeling workforce console, and we will add ourselves to the private team as a worker. This is required so that we can log in to the labeling task UI when we reach the Amazon A2I step in this solution. Please execute the following steps:</p>
			<ol>
				<li value="1">Log in to the AWS Management Console if you've not already done so (please refer to the <em class="italic">Technical requirements</em> section at the beginning of this chapter for more details), type Amazon SageMaker in the <strong class="bold">Services</strong> search bar, and go to the Amazon SageMaker console. Once there, on the left of the UI, click on <strong class="bold">Ground Truth</strong> and then <strong class="bold">Labelling workforces</strong>. On the screen, select the <strong class="bold">Private</strong> tab at the top and click on <strong class="bold">Create private team</strong>.</li>
				<li>Enter a name for your private team in the <strong class="bold">Team Name</strong> field and leave the default selection of <strong class="bold">Create a new Amazon Cognito user group</strong> as-is in the <strong class="bold">Add Workers</strong> section. Scroll down and click <strong class="bold">Create private team</strong>.</li>
				<li>You will be returned to the <strong class="bold">Labelling workforces</strong> screen. The private <strong class="source-inline">nlp-doc-team</strong> team should be visible under <strong class="bold">Private teams</strong>. Next to that, you will see an ARN, which is a long string that looks like <strong class="source-inline">arn:aws:sagemaker:region-name-123456:workteam/private-crowd/team-name</strong>. Please copy this ARN and provide it in the notebook in <strong class="bold">Step 1 – Cell 1</strong>:<p class="source-code">WORKTEAM_ARN= '&lt;your-private-workteam-arn&gt;'</p></li>
				<li>Next, scroll <a id="_idIndexMarker910"/>down the previous screen, go to the <strong class="bold">Workers</strong> section, and click on <strong class="bold">Invite new workers</strong>. Provide your email address and click <strong class="bold">Invite new workers</strong>. You will receive an email from <strong class="source-inline">no-reply@verificationemail.com</strong>. Follow the instructions provided to complete the signup process.</li>
				<li>Now, add yourself to the private team by clicking on <strong class="bold">nlp-doc-team</strong> and then <strong class="bold">Add workers to team</strong>. Select your email address from the list and click on <strong class="bold">Add workers to team</strong>.</li>
			</ol>
			<p>Now that we have added the private team, let's create an Amazon S3 bucket. </p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor196"/>Creating an Amazon S3 bucket</h2>
			<p>Follow the <a id="_idIndexMarker911"/>instructions documented in the <em class="italic">Creating an Amazon S3 bucket, a folder, and uploading objects</em> subsection in the <em class="italic">Setting up your AWS environment</em> section of <a href="B17528_02_Final_SB_ePub.xhtml#_idTextAnchor027"><em class="italic">Chapter 2</em></a>, <em class="italic">Introducing Amazon Textract</em>, to create your Amazon S3 bucket. If you created an S3 bucket in the previous sections, please reuse that bucket. For this chapter, you just need to create the S3 bucket. We will create the folders and upload the objects directly from the notebook: </p>
			<ol>
				<li value="1">Once you have the bucket's name, please type it in <strong class="bold">Step 1 – Cell 1</strong> of the notebook:<p class="source-code">bucket = "&lt;S3-bucket-name&gt;"</p></li>
				<li>Execute <strong class="bold">Step 1 – Cell 1</strong> of the notebook by clicking the <strong class="bold">Run</strong> button in the top menu of the notebook UI. This will import the libraries we need, initialize the variables, and get our kernel ready for the next set of steps.</li>
				<li>Finally, execute <strong class="bold">Step 1 – Cell 2</strong> in the notebook to upload the registration documents to our S3 bucket:<p class="source-code">s3_client = boto3.client('s3')</p><p class="source-code">for secfile in os.listdir():</p><p class="source-code">    if secfile.endswith('pdf'):</p><p class="source-code">        response = s3_client.upload_file(secfile, bucket, prefix+'/'+secfile)</p><p class="source-code">        print("Uploaded {} to S3 bucket {} in folder {}".format(secfile, bucket, prefix))</p></li>
			</ol>
			<p>Now that <a id="_idIndexMarker912"/>we have created the S3 bucket, imported the libraries we need, and uploaded the documents to our S3 bucket, let's extract the contents using <strong class="bold">Amazon Textract</strong>.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor197"/>Extracting the registration document's contents using Amazon Textract</h2>
			<p>This <a id="_idIndexMarker913"/>section corresponds to <em class="italic">Steps 2</em> and <em class="italic">3</em> in the notebook (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2016/Improve-accuracy-of-pdf-processing-with-Amazon-Textract-and-Amazon-A2I-forGitHub.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2016/Improve-accuracy-of-pdf-processing-with-Amazon-Textract-and-Amazon-A2I-forGitHub.ipynb</a>). In this section, we will submit an asynchronous text detection job using Amazon Textract. Once the job completes, we will get the results of the text detection and load them into a <strong class="bold">pandas DataFrame</strong> (<a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html</a>), select the text lines <a id="_idIndexMarker914"/>we need, and review the results. Follow these steps using the aforementioned notebook and execute the cells to perform the tasks required:</p>
			<ol>
				<li value="1">Execute <strong class="bold">Step 2 – Cell 1</strong> to define the bucket handle and declare a dictionary for <a id="_idIndexMarker915"/>storing Textract Job IDs for each of our documents:<p class="source-code">input_bucket = s3.Bucket(bucket)</p><p class="source-code">jobids = {}</p></li>
				<li>Execute <strong class="bold">Step 2 – Cell 2</strong> to submit the three text detection jobs, one for each registration document:<p class="source-code">for doc in input_bucket.objects.all():</p><p class="source-code">    if doc.key.startswith(prefix) and doc.key.endswith('pdf'): </p><p class="source-code">        tres = <strong class="bold">textract.start_document_text_detection</strong>(</p><p class="source-code">            DocumentLocation={</p><p class="source-code">                "S3Object": {</p><p class="source-code">                    "Bucket": bucket,</p><p class="source-code">                    "Name": doc.key</p><p class="source-code">                }</p><p class="source-code">            }</p><p class="source-code">        )</p><p class="source-code">        jobids[doc.key.split('/')[2]] = tres['JobId']</p><p class="callout-heading">Note</p><p class="callout">When you build this solution in an event-driven architecture using AWS Lambda (<a href="https://aws.amazon.com/lambda/">https://aws.amazon.com/lambda/</a>), an event-driven serverless compute service, you can pass the <strong class="source-inline">NotificationChannel</strong> attribute as input to the Textract <strong class="source-inline">StartDocumentTextDetection</strong> API (<a href="https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentTextDetection.html">https://docs.aws.amazon.com/textract/latest/dg/API_StartDocumentTextDetection.html</a>) to indicate the Amazon SNS (<a href="https://aws.amazon.com/sns">https://aws.amazon.com/sns</a>) topic that the message will be sent to when the job completes. You can set up AWS Lambda to subscribe to the topic, and on receipt of the message, you can call the Textract <strong class="source-inline">GetDocumentTextDetection</strong> API (<a href="https://docs.aws.amazon.com/textract/latest/dg/API_GetDocumentTextDetection.html">https://docs.aws.amazon.com/textract/latest/dg/API_GetDocumentTextDetection.html</a>) to retrieve the extracted text. We will execute this API in the notebook in <strong class="bold">Step 3 – Cell 1</strong> here.</p></li>
				<li>Finally, execute <strong class="bold">Step 2 – Cell 3</strong> to print the Job IDs for each of the documents:<p class="source-code">for j in jobids:</p><p class="source-code">    print("Textract detection Job ID for {} is {}".format(j,str(jobids[j])))</p></li>
				<li>Now, we must go to <em class="italic">Step 3</em> in the notebook. Here, we will define the helper classes <a id="_idIndexMarker916"/>to parse the JSON response from Textract. Then, we will load the text lines we need into a dictionary that we will use in the subsequent steps. Click <strong class="bold">Run</strong> in the notebook to execute <strong class="bold">Step 3 – Cell 1</strong>.</li>
				<li>Execute <strong class="bold">Step 3 – Cell 2</strong> to call the helper class we defined in the previous section and extract the text for each of our registration documents. The extracted text will be loaded into a DataFrame called <strong class="source-inline">df_indoc</strong>:<p class="source-code">text_extractor = TextExtractor()</p><p class="source-code">indoc = {}</p><p class="source-code">df_indoc = pd.DataFrame(columns = ['DocName','LineNr','DetectedText','Confidence', 'CorrectedText', 'Comments'])</p><p class="source-code">for x in jobids:</p><p class="source-code">    pages = text_extractor.extract_text(jobids[x])</p><p class="source-code">    contdict =pages[1]['Content']</p><p class="source-code">    for row in range(1,(int(len(contdict)/2))+1):</p><p class="source-code">        df_indoc.loc[len(df_indoc.index)] = [x, row, contdict['Text'+str(row)], round(contdict['Confidence'+str(row)],1),'','']</p></li>
				<li>Execute <strong class="bold">Step 3 – Cell 3</strong> in the notebook to define the filter criteria for what text <a id="_idIndexMarker917"/>lines are important to us when reviewing the registration documents. Finally, execute <strong class="bold">Step 3 – Cell 4</strong> to create a new DataFrame that only contains the text lines we are interested in:<p class="source-code">df_newdoc = pd.DataFrame(columns = ['DocName','LineNr','DetectedText','Confidence','CorrectedText','Comments'])</p><p class="source-code">for idx, row in df_indoc.iterrows():</p><p class="source-code">    if str(row['LineNr']) in bounding_dict['lines'].split(':'):</p><p class="source-code">        df_newdoc.loc[len(df_newdoc.index)] = [row['DocName'],row['LineNr'], row['DetectedText'], row['Confidence'], row['CorrectedText'],row['Comments']]</p><p class="source-code">df_newdoc</p></li>
				<li>The DataFrame's results are shown in the following screenshot. Some of the low - confidence entries are highlighted here:</li>
			</ol>
			<div>
				<div id="_idContainer287" class="IMG---Figure">
					<img src="Images/B17528_16_01.jpg" alt="Figure 16.1 – Text lines from the SEC registration documents&#13;&#10;" width="1201" height="728"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.1 – Text lines from the SEC registration documents</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The text entries appear garbled because these were intentionally introduced in the PDF documents for our use case to trigger low - confidence predictions. </p>
			<p>Now <a id="_idIndexMarker918"/>that we have digitized the text we need from the registration documents, let's cover setting up our human review workflow using Amazon A2I.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor198"/>Setting up an Amazon A2I human workflow loop</h2>
			<p>For <a id="_idIndexMarker919"/>the code blocks discussed here, please refer to <em class="italic">Steps 4</em>, <em class="italic">5</em>, and <em class="italic">6</em> in the notebook (<a href="https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2016/Improve-accuracy-of-pdf-processing-with-Amazon-Textract-and-Amazon-A2I-forGitHub.ipynb">https://github.com/PacktPublishing/Natural-Language-Processing-with-AWS-AI-Services/blob/main/Chapter%2016/Improve-accuracy-of-pdf-processing-with-Amazon-Textract-and-Amazon-A2I-forGitHub.ipynb</a>). It is time to set up a human workflow using the Private Team we created in <em class="italic">Step 0</em> and send the results to the <strong class="bold">Amazon A2I</strong> human loop for review and modifications, as required: </p>
			<ol>
				<li value="1">Let's start by initializing some variables we will need for the next few tasks. Please execute <strong class="bold">Step 4 – Cell 1</strong> in the notebook.</li>
				<li>Execute <strong class="bold">Step 4 – Cell 2</strong> in the notebook to define the human task UI template that we will use for the human review activity. We selected the task template for tabular data from the Amazon A2I Sample task UI GitHub repository (<a href="https://github.com/aws-samples/amazon-a2i-sample-task-uis">https://github.com/aws-samples/amazon-a2i-sample-task-uis</a>) and customized it for our needs.</li>
				<li>Execute <strong class="bold">Step 4 – Cell 3</strong> to create the task UI based on the template:<p class="source-code">def create_task_ui():</p><p class="source-code">        response = sagemaker.create_human_task_ui(</p><p class="source-code">        HumanTaskUiName=taskUIName,</p><p class="source-code">        UiTemplate={'Content': template})</p><p class="source-code">    return response</p><p class="source-code"># Create task UI</p><p class="source-code">humanTaskUiResponse = create_task_ui()</p><p class="source-code">humanTaskUiArn = humanTaskUiResponse['HumanTaskUiArn']</p><p class="source-code">print(humanTaskUiArn)</p></li>
				<li>We <a id="_idIndexMarker920"/>will get the following output:<p class="source-code">arn:aws:sagemaker:us-east-1:&lt;aws-account-nr&gt;:human-task-ui /ui-pdf-docs-&lt;timestamp&gt;</p></li>
				<li>Now, execute <strong class="bold">Step 5 – Cell 1</strong> in the notebook to create an <strong class="bold">Amazon A2I Flow Definition</strong> that orchestrates tasks to workforces and collects output data:<p class="source-code">create_workflow_definition_response = <strong class="bold">sagemaker_client.create_flow_definition</strong>(</p><p class="source-code">        FlowDefinitionName=flowDefinitionName,</p><p class="source-code">        RoleArn=role,</p><p class="source-code">        HumanLoopConfig= {</p><p class="source-code">            "WorkteamArn": WORKTEAM_ARN,</p><p class="source-code">            "HumanTaskUiArn": humanTaskUiArn,</p><p class="source-code">            "TaskCount": 1,</p><p class="source-code">            "TaskDescription": "Review the contents and correct values as indicated",</p><p class="source-code">            "TaskTitle": "SEC Registration Form Review"</p><p class="source-code">        },</p><p class="source-code">        OutputConfig={</p><p class="source-code">            "S3OutputPath" : OUTPUT_PATH</p><p class="source-code">        }</p><p class="source-code">    )</p><p class="source-code">flowDefinitionArn = create_workflow_definition_response['FlowDefinitionArn'] # let's save this ARN for future use</p></li>
				<li>Execute <strong class="bold">Step 5 – Cell 2</strong> to <a id="_idIndexMarker921"/>start the human workflow loop:<p class="source-code">for x in range(60):</p><p class="source-code">    describeFlowDefinitionResponse = sagemaker_client.describe_flow_definition(FlowDefinitionName=flowDefinitionName)</p><p class="source-code">    print(describeFlowDefinitionResponse ['FlowDefinitionStatus'])</p><p class="source-code">    if (describeFlowDefinitionResponse ['FlowDefinitionStatus'] == 'Active'):</p><p class="source-code">        print("Flow Definition is active")</p><p class="source-code">        break</p><p class="source-code">    time.sleep(2)</p></li>
				<li>We will get the following results:<p class="source-code">Initializing</p><p class="source-code">Active</p><p class="source-code">Flow Definition is active</p></li>
				<li>Execute <strong class="bold">Step 6 – Cell 1</strong> to upload the scanned images for the first page of the registration documents to our S3 bucket. We will refer to these images from within the Amazon A2I task UI:<p class="source-code">reg_images = {}</p><p class="source-code">for image in os.listdir():</p><p class="source-code">    if image.endswith('png'):</p><p class="source-code">        reg_images[image.split('_')[0]] = S3Uploader.upload(image, 's3://{}/{}'.format(bucket, prefix))</p></li>
				<li>Execute <strong class="bold">Step 6 – Cell 2</strong> to start the human loop for all three registration documents <a id="_idIndexMarker922"/>in our use case. In this cell, we will create a random name for each human loop, select specific lines from each document that fall below the confidence threshold of 95%, and send those inputs to an Amazon A2I <strong class="bold">StartHumanLoop</strong> API call (<a href="https://docs.aws.amazon.com/augmented-ai/2019-11-07/APIReference/API_StartHumanLoop.html">https://docs.aws.amazon.com/augmented-ai/2019-11-07/APIReference/API_StartHumanLoop.html</a>):<p class="source-code">humanLoopName = {}</p><p class="source-code">docs = df_newdoc.DocName.unique()</p><p class="source-code"># confidence threshold</p><p class="source-code">confidence_threshold = 95</p><p class="source-code">for doc in docs:</p><p class="source-code">    doc_list = []</p><p class="source-code">    humanLoopName[doc] = str(uuid.uuid4())</p><p class="source-code">    for idx, line in df_newdoc.iterrows():</p><p class="source-code">        # Send only those lines whose confidence score is less than threshold</p><p class="source-code">        if line['DocName'] == doc and line['Confidence'] &lt;= confidence_threshold:</p><p class="source-code">            doc_list.append({'linenr': line['LineNr'], 'detectedtext': line['DetectedText'], 'confidence':line['Confidence']})</p><p class="source-code">    ip_content = {"document": doc_list,</p><p class="source-code">              'image': reg_images[doc.split('.')[0]]</p><p class="source-code">             }                </p><p class="source-code">    <strong class="bold">start_loop_response = a2i.start_human_loop</strong>(</p><p class="source-code">            HumanLoopName=humanLoopName[doc],</p><p class="source-code">            FlowDefinitionArn=flowDefinitionArn,</p><p class="source-code">            HumanLoopInput={</p><p class="source-code">                "InputContent": json.dumps(ip_content)</p><p class="source-code">            }</p><p class="source-code">        )</p></li>
				<li>Execute <strong class="bold">Step 6 – Cell 3</strong> to <a id="_idIndexMarker923"/>check the status of our human loops; the status should be <strong class="bold">InProgress</strong>:<p class="source-code">completed_human_loops = []</p><p class="source-code">for doc in humanLoopName:</p><p class="source-code">    resp = a2i.describe_human_loop(HumanLoopName=humanLoopName[doc])</p><p class="source-code">    print(f'HumanLoop Name: {humanLoopName[doc]}')</p><p class="source-code">    print(f'HumanLoop Status: {resp["HumanLoopStatus"]}')</p><p class="source-code">    print(f'HumanLoop Output Destination: {resp["HumanLoopOutput"]}')</p><p class="source-code">    print('\n')</p></li>
				<li>Now, we will log in to the Amazon A2I task UI to review and modify the text lines. Let's log in to the worker portal to review the predictions and modify them as required. Execute <strong class="bold">Step 6 – Cell 4</strong> to get the URL to our Task UI:<p class="source-code">workteamName = WORKTEAM_ARN[WORKTEAM_ARN.rfind('/') + 1:]</p><p class="source-code">print("Navigate to the private worker portal and do the tasks. Make sure you've invited yourself to your workteam!")</p><p class="source-code">print('https://' + sagemaker.describe_workteam(WorkteamName=workteamName)['Workteam']['SubDomain'])</p></li>
				<li>Use the <a id="_idIndexMarker924"/>credentials you set up in <em class="italic">Step 0</em> when creating the labeling workforce to log in to the task UI. You will see a task called <strong class="bold">SEC Registration Form Review</strong>. Select it and click on <strong class="bold">Start working</strong>.</li>
				<li>The first page of the original registration form will be displayed:<div id="_idContainer288" class="IMG---Figure"><img src="Images/B17528_16_02.jpg" alt="Figure 16.2 – Task UI displaying an image of the registration form with illegible text&#13;&#10;" width="1071" height="792"/></div><p class="figure-caption">Figure 16.2 – Task UI displaying an image of the registration form with illegible text</p></li>
				<li>Scroll <a id="_idIndexMarker925"/>down the page to find a table that displays what Textract detected, the confidence score of the text line, a radio button to check if we think the detected text is correct or incorrect, an input area for us to modify the detected text, and a comments field. Make changes to the table and click the <strong class="bold">Submit</strong> button at the lower left of the page:<p class="figure-caption"> </p><div id="_idContainer289" class="IMG---Figure"><img src="Images/B17528_16_03.jpg" alt="Figure 16.3 – The document modifications page in Amazon A2I&#13;&#10;" width="1187" height="597"/></div><p class="figure-caption">Figure 16.3 – The document modifications page in Amazon A2I</p></li>
				<li>Now, the task UI will be refreshed to show the next document from the three we <a id="_idIndexMarker926"/>sent to Amazon A2I for human review. Repeat the preceding two steps to review the image, scroll down to make changes in the table, and click <strong class="bold">Submit</strong>. You will have to repeat this for the last document as well. </li>
				<li>Once you have made your changes and submitted the task for all three documents, go back to the notebook and execute <strong class="bold">Step 6 – Cell 5</strong> to check the status of the human loops. All three human loops will have a status of <strong class="bold">Completed</strong>.</li>
				<li>Finally, execute <strong class="bold">Step 6 – Cell 7</strong> in the notebook to retrieve the changes that were made by the human reviewers and add this to our DataFrame. When inspecting the DataFrame, we will see the following result:</li>
			</ol>
			<div>
				<div id="_idContainer290" class="IMG---Figure">
					<img src="Images/B17528_16_04.jpg" alt="Figure 16.4 – A2I human review results updated&#13;&#10;" width="999" height="478"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.4 – A2I human review results updated</p>
			<p>In <a id="_idIndexMarker927"/>this section, we covered the majority of the processing needs for this solution by using Amazon Textract asynchronous APIs to extract text from multiple PDF documents. After that, we used Amazon A2I to set up a human loop to review and correct low - confidence text detections. As the final step in our solution, we will persist the results of our activity.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor199"/>Storing results for downstream processing</h2>
			<p>Now that <a id="_idIndexMarker928"/>we understand how to set up a review workflow, let's persist the results for consumption by downstream applications. We will be executing the cells in <em class="italic">Step 7</em> of the notebook for this section:</p>
			<ol>
				<li value="1">Execute <strong class="bold">Step 7 – Cell 1</strong> to create an <strong class="bold">Amazon DynamoDB</strong> table, a managed database service for storing and accessing key-value pairs with very low latency.</li>
				<li>Execute <strong class="bold">Step 7 – Cell 2</strong> to upload the contents of our DataFrame to the DynamoDB table:<p class="source-code">for idx, row in df_newdoc.iterrows():</p><p class="source-code">    table.put_item(</p><p class="source-code">       Item={</p><p class="source-code">        'row_nr': idx,</p><p class="source-code">        'doc_name': str(row['DocName']) ,</p><p class="source-code">        'line_nr': str(row['LineNr']),</p><p class="source-code">        'detected_line': str(row['DetectedText']),</p><p class="source-code">        'confidence': str(row['Confidence']),   </p><p class="source-code">        'corrected_line': str(row['CorrectedText']),</p><p class="source-code">        'change_comments': str(row['Comments'])   </p><p class="source-code">        }</p><p class="source-code">    )</p><p class="source-code">print("Items were successfully created in DynamoDB table")</p></li>
				<li>The <a id="_idIndexMarker929"/>values will be inserted into the DynamoDB table, as follows:</li>
			</ol>
			<div>
				<div id="_idContainer291" class="IMG---Figure">
					<img src="Images/B17528_16_05.jpg" alt="Figure 16.5 – Corrected registration document entries in DynamoDB&#13;&#10;" width="1332" height="579"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.5 – Corrected registration document entries in DynamoDB</p>
			<p>That concludes the solution build. Please refer to the <em class="italic">Further reading</em> section for a code sample for building a similar solution using AWS Lambda and CloudFormation.</p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor200"/>Summary</h1>
			<p>In this chapter, we continued building advanced NLP solutions to address real-world requirements. We focused on asynchronously processing PDF documents and improving their accuracy by reviewing and modifying low - confidence detections using Amazon Textract and Amazon A2I. </p>
			<p>We learned how to register companies to the SEC use case with a need to extract text, and then validate and modify specific text lines in the documents before they could be passed to the Partner Integration team for submission to SEC. We considered an architecture built for scale and ease of setup. We assumed that you are the chief architect overseeing this project, and we then proceeded to provide an overview of the solution components in the <em class="italic">Introducing the PDF batch processing use case</em> section.</p>
			<p>We then went through the prerequisites for the solution build, set up an Amazon SageMaker Notebook instance, cloned our GitHub repository, and started executing the code in the notebook based on the instructions provided in this chapter. We covered setting up our private work team using Amazon SageMaker labeling workforces, extracting the relevant content from the PDF documents in batch mode using Amazon Textract, forwarding the detection results to an Amazon A2I human review loop, completing the human task steps using the UI, reviewing the results, and storing the document's contents, along with the corrections, in an Amazon DynamoDB table for downstream processing.</p>
			<p>In the next chapter, we will be addressing one more interesting feature in Amazon Textract, namely handwriting detection, and how to set up a solution to detect handwritten content for review, modification, and consumption. </p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor201"/>Further reading</h1>
			<p>Please refer to the following resources for more information:</p>
			<ul>
				<li><em class="italic">Deriving conversational insights from invoices with Amazon Textract, Amazon Comprehend and Amazon Lex</em>, by Mona Mona, Prem Ranga, and Saida Chanda (<a href="https://aws.amazon.com/blogs/machine-learning/deriving-conversational-insights-from-invoices-with-amazon-textract-amazon-comprehend-and-amazon-lex/">https://aws.amazon.com/blogs/machine-learning/deriving-conversational-insights-from-invoices-with-amazon-textract-amazon-comprehend-and-amazon-lex/</a>).</li>
				<li><em class="italic">Amazon Textract documentation for asynchronous operations </em>(<a href="https://docs.aws.amazon.com/textract/latest/dg/async.html">https://docs.aws.amazon.com/textract/latest/dg/async.html</a>).</li>
			</ul>
		</div>
	</div></body></html>