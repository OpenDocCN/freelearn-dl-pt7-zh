["```py\ndef weight_variable(shape):\n    shape = tf.TensorShape(shape)\n    initial_values = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial_values)\ndef bias_variable(shape):\n    initial_values = tf.zeros(tf.TensorShape(shape))\n    return tf.Variable(initial_values)\n```", "```py\n# Define placeholders\nX = tf.placeholder(tf.float32, shape=[None, 100])\ny = tf.placeholder(tf.int32, shape=[None, 10])\n```", "```py\n# Define variables\nw1 = weight_variable([X_input.shape[1], 64])\nb1 = bias_variable([64])\nw2 = weight_variable([64, 10])\nb2 = bias_variable([10])\n```", "```py\n# Define network\n# Hidden layer\nz1 = tf.add(tf.matmul(X, w1), b1)\na1 = tf.nn.relu(z1)\n# Output layer\nz2 = tf.add(tf.matmul(a1, w2), b2)\ny_pred = tf.nn.softmax(z2)\ny_one_hot = tf.one_hot(y, 10)\n```", "```py\n# Define loss function\nloss = tf.losses.softmax_cross_entropy(y, y_pred, \\\n       reduction=tf.losses.Reduction.MEAN)\n# Define optimizer\noptimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n# Metric\naccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, axis=1), \\\n           tf.argmax(y_pred, axis=1)), tf.float32))\nfor _ in range(n_epochs):\n    sess.run(optimizer, feed_dict={X: X_train, y: y_train})\n```", "```py\nExercise 3.01, Building a Sequential Model with the Keras High-Level API, you will see how much more straightforward it is to do the same job using a Keras high-level API.\n```", "```py\n    import tensorflow as tf\n    from __future__ import absolute_import, division, \\\n    print_function, unicode_literals\n    import tensorflow as tf\n    print(\"TensorFlow version: {}\".format(tf.__version__))\n    ```", "```py\n    TensorFlow version: 2.1.0\n    ```", "```py\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(units=64, \\\n                                    activation='relu', input_dim=100))\n    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))\n    ```", "```py\n    model.summary()\n    ```", "```py\n    Model: \"sequential_1\"\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    dense_2 (Dense)              (None, 64)                6464      \n    _________________________________________________________________\n    dense_3 (Dense)              (None, 10)                650       \n    =================================================================\n    Total params: 7,114\n    Trainable params: 7,114\n    Non-trainable params: 0\n    _________________________________________________________________\n    ```", "```py\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', \\\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=5, batch_size=32)\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\nclasses = model.predict(x_test, batch_size=128)\n```", "```py\nimport tensorflow as tf\ninputs = tf.keras.layers.Input(shape=(784,))\n```", "```py\nx = tf.keras.layers.Dense(64, activation='relu')(inputs)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\n```", "```py\npredictions = tf.keras.layers.Dense(10, activation='softmax')(x)\n```", "```py\nmodel = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n```", "```py\nmodel.compile(optimizer='rmsprop', \\\n              loss='categorical_crossentropy', \\\n              metrics=['accuracy'])\nmodel.fit(data, labels)  # starts training\n```", "```py\ninputs = tf.keras.layers.Input(shape=(784,))\nx = tf.keras.layers.Dense(64, activation='relu')(inputs)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\npredictions = tf.keras.layers.Dense(10, activation='softmax')(x)\nmodel = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n```", "```py\nmodel.compile(optimizer='rmsprop', \\\n              loss='categorical_crossentropy', \\\n              metrics=['accuracy'])\nmodel.fit(data, labels)  # starts training\n```", "```py\nloss_CatCrossEntropy = tf.keras.losses\\\n                       .SparseCategoricalCrossentropy()\n```", "```py\nloss_CatCrossEntropy(y_true=groundTruth, y_pred=predictions)\n```", "```py\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\noptimizer = tf.keras.optimizers.Adadelta(learning_rate=0.01)\noptimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01)\noptimizer = tf.keras.optimizers.Adamax(learning_rate=0.01)\noptimizer = tf.keras.optimizers.Ftrl(learning_rate=0.01)\noptimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n```", "```py\nlr_schedule = tf.keras.optimizers.schedules\\\n              .InverseTimeDecay(0.001,\\\n                                decay_steps=STEPS_PER_EPOCH*1000,\\\n                                decay_rate=1, staircase=False)\n```", "```py\ntf.keras.optimizers.Adam(lr_schedule)\n```", "```py\ntrain_stats = train_dataset.describe()\ntrain_stats = train_stats.transpose()\ndef norm(x):\n    return (x - train_stats['mean']) / train_stats['std']\nnormed_train_data = norm(train_dataset)\n```", "```py\nmodel.fit(normed_train_data, train_labels, epochs=epochs, \\\n          validation_split = 0.2, verbose=2)\n```", "```py\ntf.keras.layers.Dense(512, activation='relu', \\\n                      kernel_regularizer=tf.keras\\\n                                         .regularizers.l2(0.001))\n```", "```py\ndropout_model = tf.keras.Sequential([\n    #[...]\n    tf.keras.layers.Dense(512, activation='relu'), \\\n    tf.keras.layers.Dropout(0.5), \\\n    tf.keras.layers.Dense(256, activation='relu'), \\\n    #[...]\n    ])\n```", "```py\n    model = tf.keras.Sequential([tf.keras.layers.Dense\\\n            (64, activation='relu',\\\n             input_shape=[len(train_dataset.keys())]),\\\n             tf.keras.layers.Dense(64, activation='relu'),\\\n             tf.keras.layers.Dense(1)])\n```", "```py\n    from __future__ import absolute_import, division, \\\n    print_function, unicode_literals\n    import tensorflow as tf\n    print(\"TensorFlow version: {}\".format(tf.__version__))\n    ```", "```py\n    TensorFlow version: 2.1.0\n    ```", "```py\n    INPUT_DIM = 100\n    OUTPUT_DIM = 10\n\n    model = tf.keras.Sequential([tf.keras.layers.Dense\\\n            (128, activation='relu', \\\n            input_shape=[INPUT_DIM]), \\\n            tf.keras.layers.Dense(256, activation='relu'), \\\n            tf.keras.layers.Dense(OUTPUT_DIM, activation='softmax')])\n    ```", "```py\n    model.summary()\n    ```", "```py\n    Model: \"sequential\"\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    dense (Dense)                (None, 128)               12928     \n    _________________________________________________________________\n    dense_1 (Dense)              (None, 256)               33024     \n    _________________________________________________________________\n    dense_2 (Dense)              (None, 10)                2570      \n    =================================================================\n    Total params: 48,522\n    Trainable params: 48,522\n    Non-trainable params: 0\n    _________________________________________________________________\n    ```", "```py\n    from __future__ import absolute_import, division, \\\n    print_function, unicode_literals\n    import tensorflow as tf\n    print(\"TensorFlow version: {}\".format(tf.__version__))\n    ```", "```py\n    TensorFlow version: 2.1.0\n    ```", "```py\n    IMG_HEIGHT = 480\n    IMG_WIDTH = 680\n    model = tf.keras.Sequential([tf.keras.layers.Conv2D\\\n            (16, 3, padding='same',\\\n             activation='relu',\\\n             input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\\\n             tf.keras.layers.MaxPooling2D(),\\\n             tf.keras.layers.Conv2D(32, 3, padding='same',\\\n             activation='relu'),\\\n             tf.keras.layers.MaxPooling2D(),\\\n             tf.keras.layers.Conv2D(64, 3, padding='same',\\\n             activation='relu'),\\\n             tf.keras.layers.MaxPooling2D(),\\\n             tf.keras.layers.Flatten(),\\\n             tf.keras.layers.Dense(512, activation='relu'),\\\n             tf.keras.layers.Dense(1)])\n    model.summary()\n    ```", "```py\n    Model: \"sequential\"\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    conv2d (Conv2D)              (None, 480, 680, 16)      448       \n    _________________________________________________________________\n    max_pooling2d (MaxPooling2D) (None, 240, 340, 16)      0         \n    _________________________________________________________________\n    conv2d_1 (Conv2D)            (None, 240, 340, 32)      4640      \n    _________________________________________________________________\n    max_pooling2d_1 (MaxPooling2 (None, 120, 170, 32)      0         \n    _________________________________________________________________\n    conv2d_2 (Conv2D)            (None, 120, 170, 64)      18496     \n    _________________________________________________________________\n    max_pooling2d_2 (MaxPooling2 (None, 60, 85, 64)        0         \n    _________________________________________________________________\n    flatten (Flatten)            (None, 326400)            0         \n    _________________________________________________________________\n    dense (Dense)                (None, 512)               167117312 \n    _________________________________________________________________\n    dense_1 (Dense)              (None, 1)                 513       \n    =================================================================\n    Total params: 167,141,409\n    Trainable params: 167,141,409\n    Non-trainable params: 0\n    ```", "```py\n    from __future__ import absolute_import, division, \\\n    print_function, unicode_literals\n    import tensorflow as tf\n    print(\"TensorFlow version: {}\".format(tf.__version__))\n    ```", "```py\n    TensorFlow version: 2.1.0\n    ```", "```py\n    EMBEDDING_SIZE = 8000\n    model = tf.keras.Sequential([\\\n            tf.keras.layers.Embedding(EMBEDDING_SIZE, 64),\\\n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\\\n            tf.keras.layers.Dense(64, activation='relu'),\\\n            tf.keras.layers.Dense(1)])\n    model.summary()\n    ```", "```py\n    Model: \"sequential\"\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    embedding (Embedding)        (None, None, 64)          512000    \n    _________________________________________________________________\n    bidirectional (Bidirectional (None, 128)               66048     \n    _________________________________________________________________\n    dense (Dense)                (None, 64)                8256      \n    _________________________________________________________________\n    dense_1 (Dense)              (None, 1)                 65        \n    =================================================================\n    Total params: 586,369\n    Trainable params: 586,369\n    Non-trainable params: 0\n    _________________________________________________________________\n    ```", "```py\n    from __future__ import absolute_import, division, \\\n    print_function, unicode_literals\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import pandas as pd\n    import seaborn as sns\n    import tensorflow as tf\n    print(\"TensorFlow version: {}\".format(tf.__version__))\n    ```", "```py\n    TensorFlow version: 2.1.0\n    ```", "```py\n    dataset_path = tf.keras.utils.get_file(\"auto-mpg.data\", \\\n                   \"https://raw.githubusercontent.com/\"\\\n                   \"PacktWorkshops/\"\\\n                   \"The-Reinforcement-Learning-Workshop/master/\"\\\n                   \"Chapter03/Dataset/auto-mpg.data\")\n    column_names = ['MPG','Cylinders','Displacement','Horsepower',\\\n                    'Weight', 'Acceleration', 'Model Year', 'Origin']\n    raw_dataset = pd.read_csv(dataset_path, names=column_names,\\\n                              na_values = \"?\", comment='\\t',\\\n                              sep=\" \", skipinitialspace=True)\n    dataset = raw_dataset.copy()\n    dataset.tail()\n    ```", "```py\n    dataset.isna().sum()\n    ```", "```py\n    MPG             0\n    Cylinders       0\n    Displacement    0\n    Horsepower      6\n    Weight          0\n    Acceleration    0\n    Model Year      0\n    Origin          0\n    dtype: int64\n    ```", "```py\n    dataset = dataset.dropna()\n    ```", "```py\n    dataset['Origin'] = dataset['Origin']\\\n                        .map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n    dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n    dataset.tail()\n    ```", "```py\n    train_dataset = dataset.sample(frac=0.8,random_state=0)\n    test_dataset = dataset.drop(train_dataset.index)\n    ```", "```py\n    sns.pairplot(train_dataset[[\"MPG\", \"Cylinders\", \"Displacement\", \\\n                                \"Weight\"]], diag_kind=\"kde\")\n    ```", "```py\n    train_stats = train_dataset.describe()\n    train_stats.pop(\"MPG\")\n    train_stats = train_stats.transpose()\n    train_stats\n    ```", "```py\n    train_labels = train_dataset.pop('MPG')\n    test_labels = test_dataset.pop('MPG')\n    def norm(x):\n        return (x - train_stats['mean']) / train_stats['std']\n    normed_train_data = norm(train_dataset)\n    normed_test_data = norm(test_dataset)\n    ```", "```py\n    def build_model():\n        model = tf.keras.Sequential([\n                tf.keras.layers.Dense(64, activation='relu',\\\n                                      input_shape=[len\\\n                                      (train_dataset.keys())]),\\\n                tf.keras.layers.Dense(64, activation='relu'),\\\n                tf.keras.layers.Dense(1)])\n        optimizer = tf.keras.optimizers.RMSprop(0.001)\n        model.compile(loss='mse', optimizer=optimizer,\\\n                      metrics=['mae', 'mse'])\n        return model\n    model = build_model()\n    model.summary()\n    ```", "```py\n    epochs = 1000\n    history = model.fit(normed_train_data, train_labels,\\\n                        epochs=epochs, validation_split = 0.2, \\\n                        verbose=2)\n    ```", "```py\n    Epoch 999/1000251/251 - 0s - loss: 2.8630 - mae: 1.0763 \n    - mse: 2.8630 - val_loss: 10.2443 - val_mae: 2.3926 \n    - val_mse: 10.2443\n    Epoch 1000/1000251/251 - 0s - loss: 2.7697 - mae: 0.9985 \n    - mse: 2.7697 - val_loss: 9.9689 - val_mae: 2.3709 - val_mse: 9.9689\n    ```", "```py\n    hist = pd.DataFrame(history.history)\n    hist['epoch'] = history.epoch\n    plt.plot(hist['epoch'],hist['mae'])\n    plt.plot(hist['epoch'],hist['val_mae'])\n    plt.ylim([0, 10])\n    plt.ylabel('MAE [MPG]')\n    plt.legend([\"Training\", \"Validation\"])\n    ```", "```py\n    plt.plot(hist['epoch'],hist['mse'])\n    plt.plot(hist['epoch'],hist['val_mse'])\n    plt.ylim([0, 20])\n    plt.ylabel('MSE [MPG^2]')\n    plt.legend([\"Training\", \"Validation\"])\n    ```", "```py\n    model = build_model()\n    ```", "```py\n    early_stop = tf.keras.callbacks\\\n                 .EarlyStopping(monitor='val_loss', patience=10)\n    ```", "```py\n    early_history = model.fit(normed_train_data, train_labels,\\\n                              epochs=epochs, validation_split=0.2,\\\n                              verbose=2, callbacks=[early_stop])\n    ```", "```py\n    Epoch 42/1000251/251 - 0s - loss: 7.1298 - mae: 1.9014 \n    - mse: 7.1298 - val_loss: 8.1151 - val_mae: 2.1885 \n    - val_mse: 8.1151\n    Epoch 43/1000251/251 - 0s - loss: 7.0575 - mae: 1.8513 \n    - mse: 7.0575 - val_loss: 8.4124 - val_mae: 2.2669 \n    - val_mse: 8.4124\n    ```", "```py\n    early_hist = pd.DataFrame(early_history.history)\n    early_hist['epoch'] = early_history.epoch\n    ```", "```py\n    plt.plot(early_hist['epoch'],early_hist['mae'])\n    plt.plot(early_hist['epoch'],early_hist['val_mae'])\n    plt.ylim([0, 10])\n    plt.ylabel('MAE [MPG]')\n    plt.legend([\"Training\", \"Validation\"])\n    ```", "```py\n    loss, mae, mse = model.evaluate(normed_test_data, \\\n                                    test_labels, verbose=2)\n    print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))\n    ```", "```py\n    78/78 - 0s - loss: 6.3067 - mae: 1.8750 - mse: 6.3067 \n    Testing set Mean Abs Error:  1.87 MPG\n    ```", "```py\n    test_predictions = model.predict(normed_test_data).flatten()\n    a = plt.axes(aspect='equal')\n    plt.scatter(test_labels, test_predictions)\n    plt.xlabel('True Values [MPG]')\n    plt.ylabel('Predictions [MPG]')\n    lims = [0, 50]\n    plt.xlim(lims)\n    plt.ylim(lims)\n    _ = plt.plot(lims, lims)\n    ```", "```py\n    from __future__ import absolute_import, division, \\\n    print_function, unicode_literals\n    from  IPython import display\n    from matplotlib import pyplot as plt\n    from scipy.ndimage.filters import gaussian_filter1d\n    import pandas as pd\n    import numpy as np\n    import tensorflow as tf\n    print(\"TensorFlow version: {}\".format(tf.__version__))\n    ```", "```py\n    TensorFlow version: 2.1.0\n    ```", "```py\n    higgs_path = tf.keras.utils.get_file('HIGGSSmall.csv.gz', \\\n                 'https://github.com/PacktWorkshops/'\\\n                 'The-Reinforcement-Learning-Workshop/blob/'\\\n                 'master/Chapter03/Dataset/HIGGSSmall.csv.gz?raw=true')\n    ```", "```py\n    N_TEST = int(1e3)\n    N_VALIDATION = int(1e3)\n    N_TRAIN = int(1e4)\n    BUFFER_SIZE = int(N_TRAIN)\n    BATCH_SIZE = 500\n    STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\n    N_FEATURES = 28\n    ds = tf.data.experimental\\\n         .CsvDataset(higgs_path,[float(),]*(N_FEATURES+1), \\\n                     compression_type=\"GZIP\")\n    def pack_row(*row):\n        label = row[0]\n        features = tf.stack(row[1:],1)\n        return features, label\n    packed_ds = ds.batch(N_TRAIN).map(pack_row).unbatch()\n    ```", "```py\n    for features,label in packed_ds.batch(1000).take(1):\n        print(features[0])\n        plt.hist(features.numpy().flatten(), bins = 101)\n    ```", "```py\n    tf.Tensor(\n    [ 0.8692932 -0.6350818  0.22569026  0.32747006 -0.6899932\n      0.7542022 -0.2485731 -1.0920639   0\\.          1.3749921\n     -0.6536742  0.9303491  1.1074361   1.1389043  -1.5781983\n     -1.0469854  0\\.         0.65792954 -0.01045457 -0.04576717\n      3.1019614  1.35376    0.9795631   0.97807616  0.92000484\n      0.72165745 0.98875093 0.87667835], shape=(28,), dtype=float32)\n    ```", "```py\n    validate_ds = packed_ds.take(N_VALIDATION).cache()\n    test_ds = packed_ds.skip(N_VALIDATION).take(N_TEST).cache()\n    train_ds = packed_ds.skip(N_VALIDATION+N_TEST)\\\n               .take(N_TRAIN).cache()\n    ```", "```py\n    feature_names = [\"lepton pT\", \"lepton eta\", \"lepton phi\",\\\n                     \"missing energy magnitude\", \\\n                     \"missing energy phi\",\\\n                     \"jet 1 pt\", \"jet 1 eta\", \"jet 1 phi\",\\\n                     \"jet 1 b-tag\",\\\n                     \"jet 2 pt\", \"jet 2 eta\", \"jet 2 phi\",\\\n                     \"jet 2 b-tag\",\\\n                     \"jet 3 pt\", \"jet 3 eta\", \"jet 3 phi\",\\\n                     \"jet 3 b-tag\",\\\n                     \"jet 4 pt\", \"jet 4 eta\", \"jet 4 phi\",\\\n                     \"jet 4 b-tag\",\\\n                     \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\",\\\n                     \"m_wbb\", \"m_wwbb\"]\n    label_name = ['Measure']\n    class_names = ['Signal', 'Background']\n    print(\"Features: {}\".format(feature_names))\n    print(\"Label: {}\".format(label_name))\n    print(\"Class names: {}\".format(class_names))\n    ```", "```py\n    Features: ['lepton pT', 'lepton eta', 'lepton phi', \n    'missing energy magnitude', 'missing energy phi', \n    'jet 1 pt', 'jet 1 eta', 'jet 1 phi', 'jet 1 b-tag', \n    'jet 2 pt', 'jet 2 eta', 'jet 2 phi', 'jet 2 b-tag', \n    'jet 3 pt', 'jet 3 eta', 'jet 3 phi', 'jet 3 b-tag', \n    'jet 4 pt', 'jet 4 eta', 'jet 4 phi', 'jet 4 b-tag', \n    'm_jj', 'm_jjj', 'm_lv', 'm_jlv', 'm_bb', 'm_wbb', 'm_wwbb']\n    Label: ['Measure']\n    Class names: ['Signal', 'Background']\n    ```", "```py\n    features, labels = next(iter(train_ds))\n    print(\"Features =\")\n    print(features.numpy())\n    print(\"Labels =\")\n    print(labels.numpy())\n    ```", "```py\n    Features =\n    [ 0.3923715   1.3781117   1.5673449   0.17123567  1.6574531  \n    0.86394763    0.88821083  1.4797885   2.1730762   1.2008675   \n    0.9490923 -0.30092147    2.2148721   1.277294    0.4025028  \n    0.50748837  0\\.         0.50555664\n     -0.55428815 -0.7055601   0\\.          0.94152564  0.9448251  \n    0.9839765    0.7801499   1.4989641   0.91668195  0.8027126 ]\n    Labels = 0.0\n    ```", "```py\n    test_ds = test_ds.batch(BATCH_SIZE)\n    validate_ds = validate_ds.batch(BATCH_SIZE)\n    train_ds = train_ds.shuffle(BUFFER_SIZE).repeat()\\\n               .batch(BATCH_SIZE)\n    ```", "```py\n    lr_schedule = tf.keras.optimizers.schedules\\\n                  .InverseTimeDecay(0.001,\\\n                                    decay_steps=STEPS_PER_EPOCH*1000, \\\n                                    decay_rate=1,  staircase=False)\n    ```", "```py\n    def compile_and_fit(model, name, max_epochs=3000):\n        optimizer = tf.keras.optimizers.Adam(lr_schedule)\n        model.compile(optimizer=optimizer,\\\n        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\\\n        metrics=[tf.keras.losses.BinaryCrossentropy(from_logits=True,\\\n                 name='binary_crossentropy'),'accuracy'])\n    ```", "```py\n        model.summary()\n    ```", "```py\n        history = model.fit(train_ds, \\\n                  steps_per_epoch = STEPS_PER_EPOCH,\\\n                  epochs=max_epochs, validation_data=validate_ds, \\\n                  callbacks=[tf.keras.callbacks\\\n                             .EarlyStopping\\\n                             (monitor='val_binary_crossentropy',\\\n                             patience=200)],verbose=2)\n        return history\n    ```", "```py\n    small_model = tf.keras.Sequential([\\\n                  tf.keras.layers.Dense(16, activation='elu',\\\n                                        input_shape=(N_FEATURES,)),\\\n                  tf.keras.layers.Dense(1)])\n    size_histories = {}\n    size_histories['small'] = compile_and_fit(small_model, 'sizes/small')\n    ```", "```py\n    Epoch 1522/3000\n    20/20 - 0s - loss: 0.5693 - binary_crossentropy: 0.5693 \n    - accuracy: 0.6846 - val_loss: 0.5841 \n    - val_binary_crossentropy: 0.5841 - val_accuracy: 0.6640\n    Epoch 1523/3000\n    20/20 - 0s - loss: 0.5695 - binary_crossentropy: 0.5695 \n    - accuracy: 0.6822 - val_loss: 0.5845 \n    - val_binary_crossentropy: 0.5845 - val_accuracy: 0.6600\n    ```", "```py\n    test_accuracy = tf.keras.metrics.Accuracy()\n    for (features, labels) in test_ds:\n        logits = small_model(features)\n        probabilities = tf.keras.activations.sigmoid(logits)\n        predictions = 1*(probabilities.numpy() > 0.5)\n        test_accuracy(predictions, labels)\n        small_model_accuracy = test_accuracy.result()\n    print(\"Test set accuracy:{:.3%}\".format(test_accuracy.result()))\n    ```", "```py\n    Test set accuracy: 68.200%\n    ```", "```py\n    large_model = tf.keras.Sequential([\\\n                  tf.keras.layers.Dense(512, activation='elu',\\\n                                        input_shape=(N_FEATURES,)),\\\n                  tf.keras.layers.Dense(512, activation='elu'),\\\n                  tf.keras.layers.Dense(512, activation='elu'),\\\n                  tf.keras.layers.Dense(512, activation='elu'),\\\n                  tf.keras.layers.Dense(1)]) \n    size_histories['large'] = compile_and_fit(large_model, \"sizes/large\")\n    ```", "```py\n    Epoch 221/3000\n    20/20 - 0s - loss: 1.0285e-04 - binary_crossentropy: 1.0285e-04 \n    - accuracy: 1.0000 - val_loss: 2.5506 \n    - val_binary_crossentropy: 2.5506 - val_accuracy: 0.6660\n    Epoch 222/3000\n    20/20 - 0s - loss: 1.0099e-04 - binary_crossentropy: 1.0099e-04 \n    - accuracy: 1.0000 - val_loss: 2.5586 \n    - val_binary_crossentropy: 2.5586 - val_accuracy: 0.6650\n    ```", "```py\n    test_accuracy = tf.keras.metrics.Accuracy()\n    for (features, labels) in test_ds:\n        logits = large_model(features)\n        probabilities = tf.keras.activations.sigmoid(logits)\n        predictions = 1*(probabilities.numpy() > 0.5)\n        test_accuracy(predictions, labels)\n        large_model_accuracy = test_accuracy.result()\n        regularization_model_accuracy = test_accuracy.result()\n    print(\"Test set accuracy: {:.3%}\"\\\n          . format(regularization_model_accuracy))\n    ```", "```py\n    Test set accuracy: 65.200%\n    ```", "```py\n    regularization_model = tf.keras.Sequential([\\\n                           tf.keras.layers.Dense(512,\\\n                           kernel_regularizer=tf.keras.regularizers\\\n                                              .l2(0.0001),\\\n                           activation='elu', \\\n                           input_shape=(N_FEATURES,)),\\\n                           tf.keras.layers.Dropout(0.5),\\\n                           tf.keras.layers.Dense(512,\\\n                           kernel_regularizer=tf.keras.regularizers\\\n                                              .l2(0.0001),\\\n                           activation='elu'),\\\n                           tf.keras.layers.Dropout(0.5),\\\n                           tf.keras.layers.Dense(512,\\\n                           kernel_regularizer=tf.keras.regularizers\\\n                                              .l2(0.0001),\\\n                           activation='elu'),\\\n                           tf.keras.layers.Dropout(0.5),\\\n                           tf.keras.layers.Dense(512,\\\n                           kernel_regularizer=tf.keras.regularizers\\\n                                              .l2(0.0001),\\\n                           activation='elu'),\\\n                           tf.keras.layers.Dropout(0.5),\\\n                           tf.keras.layers.Dense(1)])\n    size_histories['regularization'] = compile_and_fit\\\n                                       (regularization_model,\\\n                                        \"regularizers/regularization\",\\\n                                        max_epochs=9000)\n    ```", "```py\n    Epoch 1264/9000\n    20/20 - 0s - loss: 0.5873 - binary_crossentropy: 0.5469 \n    - accuracy: 0.6978 - val_loss: 0.5819 \n    - val_binary_crossentropy: 0.5416 - val_accuracy: 0.7030\n    Epoch 1265/9000\n    20/20 - 0s - loss: 0.5868 - binary_crossentropy: 0.5465 \n    - accuracy: 0.7024 - val_loss: 0.5759 \n    - val_binary_crossentropy: 0.5356 - val_accuracy: 0.7100\n    ```", "```py\n    test_accuracy = tf.keras.metrics.Accuracy()\n    for (features, labels) in test_ds:\n        logits = regularization_model (features)\n        probabilities = tf.keras.activations.sigmoid(logits)\n        predictions = 1*(probabilities.numpy() > 0.5)\n        test_accuracy(predictions, labels)\n    print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\n    ```", "```py\n    Test set accuracy: 69.300%\n    ```", "```py\n    histSmall = pd.DataFrame(size_histories[\"small\"].history)\n    histSmall['epoch'] = size_histories[\"small\"].epoch\n    histLarge = pd.DataFrame(size_histories[\"large\"].history)\n    histLarge['epoch'] = size_histories[\"large\"].epoch\n    histReg = pd.DataFrame(size_histories[\"regularization\"].history)\n    histReg['epoch'] = size_histories[\"regularization\"].epoch\n    trainSmoothSmall = gaussian_filter1d\\\n                       (histSmall['binary_crossentropy'], sigma=3)\n    testSmoothSmall = gaussian_filter1d\\\n                      (histSmall['val_binary_crossentropy'], sigma=3)\n    trainSmoothLarge = gaussian_filter1d\\\n                       (histLarge['binary_crossentropy'], sigma=3)\n    testSmoothLarge = gaussian_filter1d\\\n                      (histLarge['val_binary_crossentropy'], sigma=3)\n    trainSmoothReg = gaussian_filter1d\\\n                     (histReg['binary_crossentropy'], sigma=3)\n    testSmoothReg = gaussian_filter1d\\\n                    (histReg['val_binary_crossentropy'], sigma=3)\n    plt.plot(histSmall['epoch'], trainSmoothSmall, '-', \\\n             histSmall['epoch'], testSmoothSmall, '--')\n    plt.plot(histLarge['epoch'], trainSmoothLarge, '-', \\\n             histLarge['epoch'], testSmoothLarge, '--')\n    plt.plot(histReg['epoch'], trainSmoothReg, '-', \\\n             histReg['epoch'], testSmoothReg, '--',)\n    plt.ylim([0.5, 0.7])\n    plt.ylabel('Binary Crossentropy')\n    plt.legend([\"Small Training\", \"Small Validation\", \\\n                \"Large Training\", \"Large Validation\", \\\n                \"Regularization Training\", \\\n                \"Regularization Validation\"])\n    ```", "```py\n    trainSmoothSmall = gaussian_filter1d\\\n                       (histSmall['accuracy'], sigma=6)\n    testSmoothSmall = gaussian_filter1d\\\n                      (histSmall['val_accuracy'], sigma=6)\n    trainSmoothLarge = gaussian_filter1d\\\n                       (histLarge['accuracy'], sigma=6)\n    testSmoothLarge = gaussian_filter1d\\\n                      (histLarge['val_accuracy'], sigma=6)\n    trainSmoothReg = gaussian_filter1d\\\n                     (histReg['accuracy'], sigma=6)\n    testSmoothReg = gaussian_filter1d\\\n                    (histReg['val_accuracy'], sigma=6)\n    plt.plot(histSmall['epoch'], trainSmoothSmall, '-', \\\n             histSmall['epoch'], testSmoothSmall, '--')\n    plt.plot(histLarge['epoch'], trainSmoothLarge, '-', \\\n             histLarge['epoch'], testSmoothLarge, '--')\n    plt.plot(histReg['epoch'], trainSmoothReg, '-', \\\n             histReg['epoch'], testSmoothReg, '--',)\n    plt.ylim([0.5, 0.75])\n    plt.ylabel('Accuracy')\n    plt.legend([\"Small Training\", \"Small Validation\", \\\n                \"Large Training\", \"Large Validation\",\\\n                \"Regularization Training\", \\\n                \"Regularization Validation\",])\n    ```", "```py\n    from __future__ import absolute_import, division, \\\n    print_function, unicode_literals\n    from  IPython import display\n    from matplotlib import pyplot as plt\n    from scipy.ndimage.filters import gaussian_filter1d\n    import pandas as pd\n    import numpy as np\n    import datetime\n\n    import tensorflow as tf\n\n    !rm -rf ./logs/ \n\n    # Load the TensorBoard notebook extension\n    %load_ext tensorboard\n    ```", "```py\n    higgs_path = tf.keras.utils.get_file('HIGGSSmall.csv.gz', \\\n                 'https://github.com/PacktWorkshops/'\\\n                 'The-Reinforcement-Learning-Workshop/blob/master/'\\\n                 'Chapter03/Dataset/HIGGSSmall.csv.gz?raw=true')\n    ```", "```py\n    N_TEST = int(1e3)\n    N_VALIDATION = int(1e3)\n    N_TRAIN = int(1e4)\n    BUFFER_SIZE = int(N_TRAIN)\n    BATCH_SIZE = 500\n    STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\n\n    N_FEATURES = 28\n\n    ds = tf.data.experimental.CsvDataset\\\n         (higgs_path,[float(),]*(N_FEATURES+1), \\\n          compression_type=\"GZIP\")\n    def pack_row(*row):\n        label = row[0]\n        features = tf.stack(row[1:],1)\n        return features, label\n    packed_ds = ds.batch(N_TRAIN).map(pack_row).unbatch()\n    ```", "```py\n    validate_ds = packed_ds.take(N_VALIDATION).cache()\n    test_ds = packed_ds.skip(N_VALIDATION).take(N_TEST).cache()\n    train_ds = packed_ds.skip(N_VALIDATION+N_TEST)\\\n               .take(N_TRAIN).cache()\n\n    test_ds = test_ds.batch(BATCH_SIZE)\n    validate_ds = validate_ds.batch(BATCH_SIZE)\n    train_ds = train_ds.shuffle(BUFFER_SIZE)\\\n               .repeat().batch(BATCH_SIZE)\n    ```", "```py\n    lr_schedule = tf.keras.optimizers.schedules\\\n                  .InverseTimeDecay(0.001, \\\n                                    decay_steps=STEPS_PER_EPOCH*1000,\\\n                                    decay_rate=1, staircase=False)\n    ```", "```py\n    log_dir = \"logs/fit/\" + datetime.datetime.now()\\\n              .strftime(\"%Y%m%d-%H%M%S\")\n    def compile_and_fit(model, name, max_epochs=3000):\n        optimizer = tf.keras.optimizers.Adam(lr_schedule)\n        model.compile(optimizer=optimizer,\\\n        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\\\n        metrics=[tf.keras.losses.BinaryCrossentropy\\\n                (from_logits=True, name='binary_crossentropy'),\\\n                 'accuracy'])\n        model.summary()\n        tensorboard_callback = tf.keras.callbacks.TensorBoard\\\n                               (log_dir=log_dir,\\\n                                histogram_freq=1,\\\n                                profile_batch=0)\n        history = model.fit\\\n                  (train_ds,\\\n                   steps_per_epoch = STEPS_PER_EPOCH,\\\n                   epochs=max_epochs,\\\n                   validation_data=validate_ds,\\\n                   callbacks=[tf.keras.callbacks.EarlyStopping\\\n                             (monitor='val_binary_crossentropy',\\\n                              patience=200),\\\n                              tensorboard_callback],               verbose=2)\n        return history\n    ```", "```py\n    regularization_model = tf.keras.Sequential([\\\n                           tf.keras.layers.Dense(512,\\\n                           kernel_regularizer=tf.keras.regularizers\\\n                                              .l2(0.0001),\\\n                           activation='elu', \\\n                           input_shape=(N_FEATURES,)),\\\n                           tf.keras.layers.Dropout(0.5),\\\n                           tf.keras.layers.Dense(512,\\\n                           kernel_regularizer=tf.keras.regularizers\\\n                                              .l2(0.0001),\\\n                           activation='elu'),\\\n                           tf.keras.layers.Dropout(0.5),\\\n                           tf.keras.layers.Dense(512,\\\n                           kernel_regularizer=tf.keras.regularizers\\\n                                              .l2(0.0001),\\\n                           activation='elu'),\\\n                           tf.keras.layers.Dropout(0.5),\\\n                           tf.keras.layers.Dense(512,\\\n                           kernel_regularizer=tf.keras.regularizers\\\n                                              .l2(0.0001),\\\n                           activation='elu'),\\\n                           tf.keras.layers.Dropout(0.5),\\\n                           tf.keras.layers.Dense(1)])\n    compile_and_fit(regularization_model,\\\n                    \"regularizers/regularization\", max_epochs=9000)\n    ```", "```py\n    Epoch 1112/9000\n    20/20 - 1s - loss: 0.5887 - binary_crossentropy: 0.5515 \n    - accuracy: 0.6949 - val_loss: 0.5831 \n    - val_binary_crossentropy: 0.5459 - val_accuracy: 0.6960\n    ```", "```py\n    test_accuracy = tf.keras.metrics.Accuracy()\n    for (features, labels) in test_ds:\n        logits = regularization_model(features)\n        probabilities = tf.keras.activations.sigmoid(logits)\n        predictions = 1*(probabilities.numpy() > 0.5)\n        test_accuracy(predictions, labels)\n    print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))\n    ```", "```py\n    Test set accuracy: 69.300%\n    ```", "```py\n    %tensorboard --logdir logs/fit\n    ```"]