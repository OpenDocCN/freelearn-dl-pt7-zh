- en: Removing Noise from Images Using Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will study a class of neural networks known as autoencoders,
    which have gained traction in recent years. In particular, the ability of autoencoders
    to remove noise from images has been greatly studied. In this chapter, we will
    build and train an autoencoder that is able to denoise and restore corrupted images.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What are autoencoders?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of autoencoders—basic autoencoders, deep autoencoders and convolutional
    autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders for image compression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders for image denoising
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step-by-step guide to build and train an autoencoder in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of our results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Python libraries required for this chapter are:'
  prefs: []
  type: TYPE_NORMAL
- en: matplotlib 3.0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras 2.2.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numpy 1.15.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PIL 5.4.1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code and dataset for this chapter can be found in the GitHub repository
    for the book at [https://github.com/PacktPublishing/Neural-Network-Projects-with-Python](https://github.com/PacktPublishing/Neural-Network-Projects-with-Python):'
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the code into your computer, you may run the following `git clone`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After the process is complete, there will be a folder titled `Neural-Network-Projects-with-Python`.
    Enter the folder by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To install the required Python libraries in a virtual environment, run the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that you should have installed Anaconda in your computer first, before
    running this command. To enter the virtual environment, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Navigate to the `Chapter05` folder by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following files are located in the folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '`autoencoder_image_compression.py`: This is the code for the *Building a simple
    autoencoder* section in this chapter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`basic_autoencoder_denoise_MNIST.py` and `conv_autoencoder_denoise_MNIST.py`:
    These are the code for the *Denoising autoencoder* section in this chapter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`basic_autoencoder_denoise_documents.py` and `deep_conv_autoencoder_denoise_documents.py`:
    These are the code for the *Denoising documents with autoencoders* section in
    this chapter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To run the code in each file, simply execute each Python file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: What are autoencoders?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, we have looked at the applications of neural networks for
    supervised learning. Specifically, in each project, we have a labeled dataset
    (that is, features **x** and label **y***) *and our goal is to train a neural
    network using this dataset, so that the neural network is able to predict label **y**
    from any new instance **x**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical feedforward neural network is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/802d112e-32ad-4fe9-9966-c86bb791ad3b.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter, we will study a different class of neural networks, known as
    autoencoders. Autoencoders represent a paradigm shift from the conventional neural
    networks we have seen so far. The goal of autoencoders is to learn a **Latent** **Representation**
    of the input. This representation is usually a compressed representation of the
    original input.
  prefs: []
  type: TYPE_NORMAL
- en: All autoencoders have an **Encoder **and a **Decoder.** The role of the encoder
    is to encode the input to a learned, compressed representation, and the role of
    the decoder is to reconstruct the original input using the compressed representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the architecture of a typical autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2ccadaf-b45a-494f-8193-48b3fcd8914c.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that, in the preceding diagram, we do not require a label *y*, unlike
    in CNNs. This distinction means that autoencoders are a form of unsupervised learning,
    while CNNs fall within the realm of supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Latent representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, you might wonder what is the purpose of autoencoders. Why do
    we bother learning a representation of the original input, only to reconstruct
    a similar output? The answer lies in the learned representation of the input.
    By forcing the learned representation to be compressed (that is, having smaller
    dimensions compared to the input), we essentially force the neural network to
    learn the most salient representation of the input. This ensures that the learned
    representation only captures the most relevant characteristics of the input, known
    as the **latent representation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a concrete example of latent representations, take, for example, an autoencoder
    trained on the cats and dogs dataset, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f25ef79d-d1de-4ad3-866d-e7ee135cdfa7.png)'
  prefs: []
  type: TYPE_IMG
- en: An autoencoder trained on this dataset will eventually learn that the salient
    characteristics of cats and dogs are the the shape of the ears, the length of
    whiskers, the snout size, and the length of the tongue visible. These salient
    characteristics are captured by the latent representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this latent representation learned by the autoencoder, we can then do
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the dimensionality of the input data. The latent representation is a
    natural reduced representation of the input data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove any noise from the input data (known as denoising). Noise is not a salient
    characteristic and therefore should be easily identifiable by using the latent
    representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the sections to follow, we shall create and train autoencoders for each of
    the preceding purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in the previous example, we have used examples such as shape of ears
    and snout size as descriptions for the latent representation. In reality, latent
    representations are simply a matrix of numbers and it is impossible to assign
    meaningful labels for them (nor do we need to). The descriptions that we used
    here simply provide an intuitive explanation for latent representations.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders for data compression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how autoencoders are able to learn a reduced representation
    of the input data. It is natural to think that autoencoders can do a good job
    at generalized data compression. However, that is not the case. Autoencoders are
    poor at generalized data compression, such as image compression (that is, JPEG)
    and audio compression (that is, MP3), because the learned latent representation
    only represents the data on which it was trained. In other words, autoencoders
    only work well for images similar to those on which it was trained.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, autoencoders are a "lossy" form of data compression, which means
    that the output from autoencoders will have less information when compared to
    the original input. These characteristics mean that autoencoders are poor at being
    generalized data compression techniques. Other forms of data compression, such
    as JPEG and MP3, are superior when compared to autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST handwritten digits dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the datasets that we'll use for this chapter is the MNIST handwritten
    digits dataset. The MNIST dataset contains 70,000 samples of handwritten digits,
    each of size 28 x 28 pixels. Each sample contains only one digit within the image,
    and all samples are labeled.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MNIST dataset is provided directly in Keras, and we can import it by simply
    running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d22c2165-a1ac-4a11-953d-c2c4e12aef71.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the digits are definitely handwritten, and each 28 x 28 image
    captures only one digit. The autoencoder should be able to learn the compressed
    representation of these digits (smaller than 28 x 28), and to reproduce the images
    using this compressed representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/49774e97-83b3-4391-9d3c-cfe3f4776937.png)'
  prefs: []
  type: TYPE_IMG
- en: Building a simple autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To cement our understanding, let''s start off by building the most basic autoencoder,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10feca38-7f7b-4816-9733-322fe50ab617.png)'
  prefs: []
  type: TYPE_IMG
- en: So far, we have emphasized that the hidden layer (**Latent** **Representation**)
    should be of a smaller dimension than the input data. This ensures that the latent
    representation is a compressed representation of the salient features of the input.
    But how small should it be?
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, the size of the hidden layer should balance between being:'
  prefs: []
  type: TYPE_NORMAL
- en: Sufficiently *small* enough to represent a compressed representation of the
    input features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sufficiently *large* enough for the decoder to reconstruct the original input
    without too much loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In other words, the size of the hidden layer is a hyperparameter that we need
    to select carefully to obtain the best results. We shall see how we can define
    the size of the hidden layer in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Building autoencoders in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let's start building our basic autoencoder in Keras. As always, we'll
    use the `Sequential` class in Keras to build our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by importing and defining a new `Sequential` class in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll add the hidden layer to our model. From the previous diagram, we
    can clearly see that the hidden layer is a fully connected layer (that is, a `Dense`
    layer). From the `Dense` class in Keras, we can define the size of the hidden
    layer through the `units` parameter. The number of units is a hyperparameter that
    we will be experimenting with. For now, let's use a single node (units=1) as the
    hidden layer. The `input_shape` to the `Dense` layer is a vector of size `784`
    (since we are using 28 x 28 images) and the `activation` function is the `relu`
    activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code adds a `Dense` layer with a single node to our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we'll add the output layer. The output layer is also a fully connected
    layer (that is, a `Dense` layer), and the size of the output layer should naturally
    be `784`, since we are trying to output the original 28 x 28 image. We use a `Sigmoid`
    activation function for the output to constrain the output values (value per pixel)
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code adds an output `Dense` layer with `784` units to our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Before we train our model, let's check the structure of our model and make sure
    that it is consistent with our diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this by calling the `summary()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad6c0a37-f66e-4c5d-a474-a9a77f619831.png)'
  prefs: []
  type: TYPE_IMG
- en: Before we move on to the next step, let's create a function that encapsulates
    the model creation process that we just went through. Having such a function is
    useful, as it allows us to easily create different models with different hidden
    layer sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code defines a function that creates a basic autoencoder with
    a `hidden_layer_size` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to preprocess our data. There are two preprocessing steps
    required:'
  prefs: []
  type: TYPE_NORMAL
- en: Reshape the images from a 28 x 28 vector to a 784 x 1 vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the values of the vector between 0 and 1 from the current 0 to 255\.
    This smaller range of values makes it easier to train our neural network using
    the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To reshape the images from 28 x 28 to 784 x 1, we simply run the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Note that first dimension, `X_train.shape[0]`, refers to the number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'To normalize the values of the vector between 0 and 1 (from the original range
    of 0 to 255), we run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With that done, we can start to train our model. We'll first compile our model
    using the `adam` optimizer and `mean_squared_error` as the `loss` function. The
    `mean_squared_error` is useful in this case because we need a `loss` function
    that quantifies the pixel-wise discrepancy between the input and the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code compiles our model using the aforementioned parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Finally, let's train our model for `10` epochs. Note that we use `X_train_reshaped`
    as both the input (*x*) and output (*y*). This makes sense because we are trying
    to train the autoencoder to produce output that is identical to the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'We train our autoencoder with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cc71cc5-daf2-4546-9267-0f0b77e61671.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With our model trained, let''s apply it on our testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We would like to plot the output, and see how closely it matches with the original
    input. Remember, the autoencoder should produce output images that are close to
    the original input images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code selects five random images from the testing set and plots
    them on the top row. It then plots the output images for these five randomly selected
    inputs on the bottom row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/445943f4-d849-4f17-99f4-59ca4d2b5694.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Top: Original images provided to the autoencoder as input; bottom: images output
    from the autoencoder'
  prefs: []
  type: TYPE_NORMAL
- en: 'Wait a minute: the output images look terrible! They look like a blurry white
    scribble and they look nothing like our original input images. Clearly, an autoencoder
    with a hidden layer size of one node is insufficient to encode this dataset. This
    latent representation is too small for our autoencoder to sufficiently capture
    the salient features of our data.'
  prefs: []
  type: TYPE_NORMAL
- en: Effect of hidden layer size on autoencoder performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's try training more autoencoders with different hidden layer sizes and see
    how they fare.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code creates and trains five different models with `2`, `4`,
    `8`, `16`, and `32` nodes in the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Notice how each successive model has twice the number of nodes in the hidden
    layer as the preceding model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s train all five of our models together. We use the `verbose=0` argument
    in the `fit()` function to hide the output, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Once training is complete, we apply the trained models on the testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s plot five randomly selected outputs from each model and see how
    they compare to the original input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32751ee1-662f-4725-8b4f-603a5fecce27.png)'
  prefs: []
  type: TYPE_IMG
- en: Isn't it beautiful? We can clearly see a nice transition as we double the number
    of nodes in the hidden layer. Gradually, we see that the output images become
    clearer and closer to the original input as we increase the number of nodes in
    the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: At 32 nodes in the hidden layer, the output becomes very close (though not perfect)
    to the original input. Interestingly, we have shrunk the original input by 24.5
    times (784÷32) and still managed to produce a satisfactory output. That's a pretty
    impressive compression ratio!
  prefs: []
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another interesting application of autoencoders is image denoising. Image noise
    is defined as a random variations of brightness in an image. Image noise may originate
    from the sensors of digital cameras. Although digital cameras these days are capable
    of capturing high quality images, image noise may still occur, especially in low
    light conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising images has been a challenge for researchers for many years. Early
    methods include applying some sort of image filter (that is, mean averaging filter,
    where the pixel value is replaced with the average pixel value of its neighbors)
    over the image. However, such methods can sometimes fall short and the effects
    can be less than ideal.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few years ago, researchers discovered that we can train autoencoders for
    image denoising. The idea is simple. Instead of using the same input and output
    when training conventional autoencoders (as described in the previous section),
    we use a noisy image as the input and a clean reference image for the autoencoder
    to compare its output against. This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1eab68ab-bf04-407d-af27-ff529c41dc0f.png)'
  prefs: []
  type: TYPE_IMG
- en: During the training process, the autoencoder will learn that the noises in the
    image should not be part of the output, and will learn to output a clean image.
    Essentially, we are training our autoencoder to remove noise from images!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by introducing noise to the MNIST dataset. We''ll add a random
    value between `-0.5` and `0.5` to each pixel in the original images. This has
    the effect of increasing and decreasing the intensity of pixels at random. The
    following code does this using `numpy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we clip the noisy images between `0` and `1` to normalize the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Let's define a basic autoencoder just like we did in the previous section. This
    basic autoencoder has a single hidden layer with `16` nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code creates this autoencoder using the function that we defined
    in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we train our denoising autoencoder. Remember, the input to the denoising
    autoencoder is a noisy image and the output is a clean image. The following code
    trains our basic denoising autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Once training is done, we apply our denoising autoencoder on the test images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We plot the output and compare it with the original image and the noisy image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/595e6fac-c11b-4147-bda1-fb09ccbca711.png)'
  prefs: []
  type: TYPE_IMG
- en: How does it do? Well, it could definitely be better! This basic denoising autoencoder
    is perfectly capable of removing noise, but it doesn't do a very good job at reconstructing
    the original image. We can see that this basic denoising autoencoder sometimes
    fails to separate noise from the digits, especially near the center of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Deep convolutional denoising autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Can we do better than the basic, one-hidden layer autoencoder? We saw in the
    previous chapter, [Chapter 4](48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml), *Cats
    Versus Dogs – Image Classification Using CNNs*, that deep CNNs perform well for
    image classification tasks. Naturally, we can apply the same concept for autoencoders
    too. Instead of using only one hidden layer, we use multiple layers (that is,
    a deep network) and instead of a fully connected dense layer, we use convolutional
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the architecture of a deep convolutional
    autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d03d2b8c-5e5d-486f-81f8-f5a6c1b8213d.png)'
  prefs: []
  type: TYPE_IMG
- en: Constructing a deep convolutional autoencoder in Keras is simple. Once again,
    we'll use the `Sequential` class in Keras to construct our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a new `Sequential` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s add the first two convolutional layers, which act as the encoder
    in our model. There are several parameters we need to define while using the `Conv2D`
    class in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of filters**: Typically, we use a decreasing number of filters for
    each layer in the encoder. Conversely, we use an increasing number of filters
    for each layer in the decoder. Let''s use 16 filters for the first convolutional
    layer in the encoder and eight filters for the second convolutional layer in the
    encoder. Conversely, let''s use eight filters for the first convolutional layer
    in the decoder and 16 filters for the second convolutional layer in the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filter size: **As shown in the previous chapter, [Chapter 4](48f7db0c-7c74-42a2-82b7-a17c9f220423.xhtml),
    *Cats Versus Dogs – Image Classification Using CNNs*, a filter size of 3 x 3 is
    typical for convolutional layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Padding: **For autoencoders, we use a same padding. This ensures that the
    height and width of successive layers remains the same. This is useful because
    we need to ensure that the dimensions of the final output is the same as the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code snippet adds the first two convolutional layers with the
    aforementioned parameters to our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll add the decoder layers onto our model. Just like the encoder layers,
    the decoder layers are also convolutional layers. The only difference is that,
    in the decoder layers, we use an increasing number of filters after each successive
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet adds the next two convolutional layers as the decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we add the output layer to our model. The output layer should be a
    convolutional layer with only one filter, as we are trying to output a 28 x 28
    x 1 image. The `Sigmoid` function is used as the activation function for the output
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code adds the final output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the structure of the model to make sure that it is consistent
    with what was shown in the diagram earlier. We can do so by calling the `summary()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dbaf5d0f-67fe-4d7b-a471-6805b614af25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are now ready to train our deep convolutional autoencoder. As usual, we
    define the training process under the `compile` function and call the `fit` function,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Once training is done, we''ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12f069db-0d87-4491-8924-82ef829e91e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s use the trained model on the testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: It will be interesting to see how this deep convolutional autoencoder performs
    on the testing set. Remember, the testing set represents images that the model
    has never seen before.
  prefs: []
  type: TYPE_NORMAL
- en: 'We plot the output and compare it with the original image and the noisy image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c61e9229-97c3-4945-afd2-a897ad928f41.png)'
  prefs: []
  type: TYPE_IMG
- en: Isn't that amazing? The denoised output from our deep convolutional autoencoder
    is so good that we can barely differentiate the original images and the denoised
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the impressive results, it is important to keep in mind that the convolutional
    model that we used is pretty simple. The advantage of deep neural networks is
    that we can always increase the complexity of the model (that is, more layers
    and more filters per layer) and use it on more complex datasets. This ability
    to scale is one of the main advantages of deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising documents with autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have applied our denoising autoencoder on the MNIST dataset, which
    is a pretty simple dataset. Let's take a look now at a more complicated dataset,
    which better represents the challenges of denoising documents in real life.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset that we will be using is provided for free by the **University of
    California Irvine** (**UCI**). For more information on the dataset, you can visit
    UCI's website at [https://archive.ics.uci.edu/ml/datasets/NoisyOffice](https://archive.ics.uci.edu/ml/datasets/NoisyOffice).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset can be found in the accompanying GitHub repository for this book.
    For more information on downloading the code and dataset for this chapter from
    the GitHub repository, please refer to the *Technical requirements* section earlier
    in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset consists of 216 different noisy images. The noisy images are scanned
    office documents that are tainted by coffee stains, wrinkled marks, and other
    sorts of defects that are typical in office documents. For every noisy image,
    a corresponding reference clean image is provided, which represents the office
    document in an ideal noiseless state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the dataset to have a better idea of what we are working
    with. The dataset is located at the following folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The `Noisy_Documents` folder contains two subfolders (`noisy` and `clean`),
    which contains the noisy and clean images, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: To load the `.png` images into Python, we can use the `load_img` function provided
    by Keras. To convert the loaded images into a `numpy` array, we use the `img_to_array`
    function in Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code imports the noisy `.png` images in the `/Noisy_Documents/noisy/`
    folder into a `numpy` array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify that our images are loaded properly into the `numpy` array, let''s
    print the dimensions of the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72d59ff5-918d-4594-90f7-acc0d445a78a.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that there are 216 images in the array, each with dimensions 420
    x 540 x 1 (width x height x number of channels for each image).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s do the same for the clean images. The following code imports the clean `.png` images
    in the `/Noisy_Documents/clean/` folder into a `numpy` array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s display the loaded images to have a better idea of the kind of images
    we are working with. The following code randomly selects `3` images and plots
    them, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the output as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9cc19ed-e4e3-4d64-b155-b223743a8158.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the kind of noise in this dataset is markedly different from
    what we saw in the MNIST dataset. The noise in this dataset are random artifacts
    that appear throughout the image. Our autoencoder model needs to have a strong
    understanding of signal versus noise in order to successfully denoise this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed to train our model, let''s split our dataset into a training
    and testing set, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Basic convolutional autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're now ready to tackle the problem. Let's start with a basic model to see
    how far we can go with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, we define a new `Sequential` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add a single convolutional layer as our encoder layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We add a single convolutional layer as our decoder layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we add an output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the structure of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the output as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77fcef33-83cf-442d-8617-b55c2142bb42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here''s the code to train our basic convolutional autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training is done, we apply our model on the testing set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s plot the output and see what kind of results we got. The following code
    plots the original noisy images in the left column, the original clean images
    in the middle column, and the denoised image output from our model in the right
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the output as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3bd8afd-8513-4ca9-88ed-9f890c713566.png)'
  prefs: []
  type: TYPE_IMG
- en: Well, our model can certainly do a better job. The denoised images tend to have
    a gray background rather than a white background in the true `Clean Images`. The
    model also does a poor job at removing the coffee stains from the `Noisy Images`.
    Furthermore, the words in the denoised images are faint, showing that the model
    struggles at this task.
  prefs: []
  type: TYPE_NORMAL
- en: Deep convolutional autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's try denoising the images with a deeper model and more filters in each
    convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining a new `Sequential` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add three convolutional layers as our encoder, with `32`, `16`, and
    `8` filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly for the decoder, we add three convolutional layers with `8`, `16`,
    and `32` filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we add an output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the structure of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4451482c-aee1-4535-ba4c-85239ccd27fb.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding output, we can see that there are 12,785 parameters in our
    model, which is approximately 17 times more than the basic model we used in the
    previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s train the model and apply it on the testing images:'
  prefs: []
  type: TYPE_NORMAL
- en: Caution
  prefs: []
  type: TYPE_NORMAL
- en: The following code may take some time to run if you are not using Keras with
    a GPU. If the model is taking too long to train, you may reduce the number of
    filters in each convolutional layer in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we plot the output to see the kind of results we get. The following
    code plots the original noisy images in the left column, the original clean images
    in the middle column, and the denoised image output from our model in the right
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/03364997-57db-4feb-b88c-3ce300cf7b0d.png)'
  prefs: []
  type: TYPE_IMG
- en: The result looks amazing! In fact, the output denoised images look so good that
    we can barely differentiate them from the true clean images. We can see that the
    coffee stain has been almost entirely removed and the noise from the crumpled
    paper is non-existent in the denoised image. Furthermore, the words in the denoised
    images look sharp and clear, and we can easily read the words in the denoised
    images.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset truly demonstrates the power of autoencoders. By adding on additional
    complexity in the form of deeper convolutional layers and more filters, the model
    is able to differentiate the signal from the noise, allowing it to successfully
    denoise images that are heavily corrupted.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at autoencoders, a class of neural networks that
    learn the latent representation of input images. We saw that all autoencoders
    have an encoder and decoder component. The role of the encoder is to encode the
    input to a learned, compressed representation and the role of the decoder is to
    reconstruct the original input using the compressed representation.
  prefs: []
  type: TYPE_NORMAL
- en: We first looked at autoencoders for image compression. By training an autoencoder
    with identical input and output, the autoencoder learns the most salient features
    of the input. Using MNIST images, we constructed an autoencoder with a 24.5 times
    compression rate. Using this learned 24.5x compressed representation, the autoencoder
    is able to successfully reconstruct the original input.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we looked at denoising autoencoders. By training an autoencoder with noisy
    images as input and clean images as output, the autoencoder is able to pick out
    the signal from the noise, and is able to successfully denoise the noisy image.
    We trained a deep convolutional autoencoder, and the autoencoder was able to successfully
    denoise documents with coffee stains and other sorts of image corruptions. The
    results were impressive, with the autoencoder removing almost all of the noise
    in the noisy documents, producing an output that is almost identical to the true
    clean images.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 6](21ef7df7-5976-4e0d-bec5-d736ec571d94.xhtml), *Sentiment
    Analysis of Movie Reviews Using LSTM* we'll use a **long short-term memory** (**LSTM**)
    neural network to predict the sentiment of movie reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How are autoencoders different from a conventional feed forward neural network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Autoencoders are neural networks that learn a compressed representation of the
    input, known as the latent representation. They are different from conventional
    feed forward neural networks because their structure consists of an encoder and
    a decoder component, which is not present in CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: What happens when the latent representation of the autoencoder is too small?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The size of the latent representation should be sufficiently *small* enough
    to represent a compressed representation of the input, and also be sufficiently *large*
    enough for the decoder to reconstruct the original image without too much loss.
  prefs: []
  type: TYPE_NORMAL
- en: What are the input and output when training a denoising autoencoder?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input to a denoising autoencoder should be a noisy image and the output
    should be a reference clean image. During the training process, the autoencoder
    learns that the output should not contain any noise (through the `loss` function),
    and the latent representation of the autoencoder should only contain the signals
    (that is, non-noise elements)
  prefs: []
  type: TYPE_NORMAL
- en: What are some of the ways we can improve the complexity of denoising autoencoders?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For denoising autoencoders, convolutional layers always work better than dense
    layers, just as CNNs work better than conventional feed forward neural networks
    for image classification tasks. We can also improve the complexity of our model
    by building a deeper network with more layers, and by using more filters in each
    convolutional layer.
  prefs: []
  type: TYPE_NORMAL
