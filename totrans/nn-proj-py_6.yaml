- en: Sentiment Analysis of Movie Reviews Using LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we looked at neural network architectures, such as the
    basic MLP and feedforward neural networks, for classification and regression tasks.
    We then looked at CNNs, and we saw how they are used for image recognition tasks.
    In this chapter, we will turn our attention to **recurrent neural networks** (**RNNs**)
    (in particular, to **long short-term memory** (**LSTM**) networks) and how they
    can be used in sequential problems, such as **Natural Language Processing** (**NLP**).
    We will develop and train a LSTM network to predict the sentiment of movie reviews
    on IMDb.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Sequential problems in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP and sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to RNNs and LSTM networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of the IMDb movie reviews dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A step-by-step guide to building and training an LSTM network in Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of our results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Python libraries required for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: matplotlib 3.0.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras 2.2.4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: seaborn 0.9.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn 0.20.2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter can be found in the GitHub repository for the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'To download the code onto your computer, you may run the following `git clone`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After the process is complete, there will be a folder entitled `Neural-Network-Projects-with-Python`.
    Enter the folder by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To install the required Python libraries in a virtual environment, run the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that you should have installed Anaconda on your computer first, before
    running this command. To enter the virtual environment, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Navigate to the `Chapter06` folder by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following file is located in the folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lstm.py`: This is the main code for this chapter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To run the code, simply execute the `lstm.py` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Sequential problems in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Sequential problems** are a class of problem in machine learning in which
    the order of the features presented to the model is important for making predictions.
    Sequential problems are commonly encountered in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: NLP, including sentiment analysis, language translation, and text prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, let''s consider the text prediction problem, as shown in the following
    screenshot, which falls under NLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a095e669-07db-4705-b0df-32802fff8f36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Human beings have an innate ability for this, and it is trivial for us to know
    that the word in the blank is probably the word *Japanese*. The reason for this
    is that as we read the sentence, we process the words as a sequence. The sequence
    of the words captures the information required to make the prediction. By contrast,
    if we discard the sequential information and only consider the words individually,
    we get a *bag of words,* as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/342284d6-22cd-487b-8f5e-e4adf78c8dca.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that our ability to predict the word in the blank is now severely
    impacted. Without knowing the sequence of words, it is impossible to predict the
    word in the blank.
  prefs: []
  type: TYPE_NORMAL
- en: Besides text predictions, sentiment analysis and language translation are also
    sequential problems. In fact, many NLP problems are sequential problems, because
    the languages that we speak are sequential in nature, and the sequence conveys
    context and other subtle nuances.
  prefs: []
  type: TYPE_NORMAL
- en: Sequential problems also occur naturally in time series problems. Time series
    problems are common in stock markets. Often, we wish to know whether a particular
    stock will rise or fall on a certain day. This problem is accurately defined as
    a time series problem, because knowing the movement of the stocks in the preceding
    hours or minutes is often crucial to predicting whether the stock will rise or
    fall. Today, machine learning methods are being heavily applied in this domain,
    with algorithmic trading strategies driving the buying and selling of stocks.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on NLP problems. In particular, we will create
    a neural network for sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: NLP and sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP is a subfield in **artificial intelligence** (**AI**) that is concerned
    with the interaction of computers and human languages. As early as the 1950s,
    scientists were interested in designing intelligent machines that could understand
    human languages. Early efforts to create a language translator focused on the
    rule-based approach, where a group of linguistic experts handcrafted a set of
    rules to be encoded in machines. However, this rule-based approach produced results
    that were sub-optimal, and, often, it was impossible to convert these rules from
    one language to another, which meant that scaling up was difficult. For many decades,
    not much progress was made in NLP, and human language was a goal that AI couldn't
    reach—until the resurgence of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: With the proliferation of deep learning and neural networks in the image classification
    domain, scientists began to wonder whether the powers of neural networks could
    be applied to NLP. In the late '00s, tech giants, including Apple, Amazon, and
    Google, applied LSTM networks to NLP problems, and the results were astonishing.
    The ability of AI assistants, such as Siri and Alexa, to understand multiple languages
    spoken in different accents was the result of deep learning and LSTM networks.
    In recent years, we have also seen a massive improvement in the abilities of text
    translation software, such as Google Translate, which is capable of producing
    translations as good as human language experts.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment analysis** is also an area of NLP that benefited from the resurgence
    of deep learning. Sentiment analysis is defined as the prediction of the positivity
    of a text. Most sentiment analysis problems are classification problems (positive/neutral/negative)
    and not regression problems.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many practical applications of sentiment analysis. For example, modern
    customer service centers use sentiment analysis to predict the satisfaction of
    customers through the reviews they provide on platforms such as Yelp or Facebook.
    This allows businesses to step in immediately whenever customers are dissatisfied,
    allowing the problem to be addressed as soon as possible, and preventing customer
    churn.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis has also been applied in the domain of stocks trading. In
    2010, scientists showed that by sampling the sentiment in Twitter (positive versus
    negative tweets), we can predict whether the stock market will rise. Similarly,
    high-frequency trading firms use sentiment analysis to sample the sentiment of
    news related to certain companies, and execute trades automatically, based on
    the positivity of the news.
  prefs: []
  type: TYPE_NORMAL
- en: Why sentiment analysis is difficult
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Early efforts in sentiment analysis faced many hurdles, due to the presence
    of subtle nuances in human languages. The same word can often covey a different
    meaning, depending on the context. Take for example the following two sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5060c2e1-acc7-4763-a93f-5a2630eba114.png)'
  prefs: []
  type: TYPE_IMG
- en: We know that the sentiment of the first sentence is negative, as it probably
    means that the building is literally on fire. On the other hand, we know that
    the sentiment of the second sentence is positive, since it is unlikely that the
    person is literally on fire. Instead, it probably means that the person is on
    a *hot streak*, and this is positive. The rule-based approach toward sentiment
    analysis suffers because of these subtle nuances, and it is incredibly complex
    to encode this knowledge in a rule-based manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another reason sentiment analysis is difficult is because of sarcasm. Sarcasm
    is commonly used in many cultures, especially in an online medium. Sarcasm is
    difficult for computers to understand. In fact, even humans fail to detect sarcasm
    at times. Take for example the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa2c5812-d49e-49ee-8dd2-c9cf0b647c02.png)'
  prefs: []
  type: TYPE_IMG
- en: You can probably detect sarcasm in the preceding sentence, and come to the conclusion
    that the sentiment is negative. However, it is not easy for a program to understand
    that.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at RNNs and LSTM nets, and how they have been
    used to tackle sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up until now, we have used neural networks such as the MLP, feedforward neural
    network, and CNN in our projects. The constraint faced by these neural networks
    is that they only accept a fixed input vector such as an image, and output another
    vector. The high-level architecture of these neural networks can be summarized
    by the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f373841-27d5-4ef7-aee5-75cd6eca7761.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This restrictive architecture makes it difficult for CNNs to work with sequential
    data. To work with sequential data, the neural network needs to take in specific
    bits of the data at each time step, in the sequence that it appears. This provides
    the idea for an RNN. An RNN has high-level architecture, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/afa90262-19e2-491e-8c06-6076e52cce9b.png)'
  prefs: []
  type: TYPE_IMG
- en: From the previous diagram, we can see that an RNN is a multi-layered neural
    network. We can break up the raw input, splitting it into time steps. For example,
    if the raw input is a sentence, we can break up the sentence into individual words
    (in this case, every word represents a time step). Each word will then be provided
    in the corresponding layer in the RNN as **Input**. More importantly, each layer
    in an RNN passes its output to the next layer. The intermediate output passed
    from layer to layer is known as the hidden state. Essentially, the hidden state
    allows an RNN to maintain a memory of the intermediate states from the sequential
    data.
  prefs: []
  type: TYPE_NORMAL
- en: What's inside an RNN?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now take a closer look at what goes on inside each layer of an RNN.
    The following diagram depicts the mathematical function inside each layer of an
    RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94651e9b-3a1d-4ffa-b914-830c5b8e82d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The mathematical function of an RNN is simple. Each layer *t *within an RNN
    has two inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The input from the time step *t*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hidden state passed from the previous layer *t-1*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each layer in an RNN simply sums up the two inputs and applies a *tanh* function
    to the sum. It then outputs the result, to be passed as a hidden state to the
    next layer. It''s that simple! More formally, the output hidden state of layer
    *t* is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1077d130-7e09-41e7-8909-2feb6bae8f02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But what exactly is the *tanh* function? The *tanh *function is the hyperbolic
    tangent function, and it simply squashes a value between **1** and **-1**. The
    following graph illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97058633-3829-4297-b6e8-c11d927ad0ab.png)'
  prefs: []
  type: TYPE_IMG
- en: The tanh function is a good choice as a non-linear transformation of the combination
    of the current input and the previous hidden state, because it ensures that the
    weights don't diverge too rapidly. It has also other nice mathematical properties,
    such as being easily differentiable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to get the final output from the last layer in the RNN, we simply
    apply a *sigmoid* function to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/278c8e5e-8dcd-450e-b2eb-bf90e5ff7aa4.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous equation, *n *is the index of the last layer in the RNN. Recall
    from previous chapters that the *sigmoid* function produces an output between
    0 and 1, hence providing the probabilities for each class as a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that if we stack these layers together, the final output from an
    RNN depends on the non-linear combination of the inputs at different time steps.
  prefs: []
  type: TYPE_NORMAL
- en: Long- and short-term dependencies in RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The architecture of an RNN makes it ideal for handling sequential data. Let's
    take a look at some concrete examples, to understand how an RNN handles different
    lengths of sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first take a look at a short piece of text as our sequential data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c28bc829-1bc6-4cc4-891b-2822035d7491.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can treat this short sentence as sequential data by breaking it down into
    five different inputs, with each word at each time step. This is illustrated in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/080f8b94-57ff-458f-bc19-468b7a905241.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, suppose that we are building a simple RNN to predict whether is it snowing
    based on this sequential data. The RNN would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1694093-ef42-428a-aa8d-bab48e18164b.png)'
  prefs: []
  type: TYPE_IMG
- en: The critical piece of information in the sequence is the word **HOT**, at time
    step 4 (**t[4]**[,]circled in red). With this piece of information, the RNN is
    able to easily predict that it is not snowing today. Notice that the critical
    piece of information came just shortly before the final output. In other words,
    we would say that there is a short-term dependency in this sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, RNNs have no problems with short-term dependencies. But what about
    long-term dependencies? Let''s take a look now at a longer sequence of text. Let''s
    use the following paragraph as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/febffd6f-8a0e-41b8-bece-f47008b67a48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our goal is to predict whether the customer liked the movie. Clearly, the customer
    liked the movie but not the cinema, which was the main complaint in the paragraph.
    Let''s break up the paragraph into a sequence of inputs, with each word at each
    time step (32 time steps for 32 words in the paragraph). The RNN would look this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2e3ea44-bff6-448e-9913-13426c6cb967.png)'
  prefs: []
  type: TYPE_IMG
- en: The critical words **liked the movie** appeared between time steps 3 and 5\.
    Notice that there is a significant gap between the critical time steps and the
    output time step, as the rest of the text was largely irrelevant to the prediction
    problem (whether the customer liked the movie). In other words, we say that there
    is a long-term dependency in this sequence. Unfortunately, RNNs do not work well
    with long-term dependency sequences. RNNs have a good short-term memory, but a
    bad long-term memory. To understand why this is so, we need to understand the
    **vanishing gradient problem** when training neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: The vanishing gradient problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The vanishing gradient problem is a problem when training deep neural networks
    using gradient-based methods such as backpropagation. Recall in previous chapters,
    we discussed the backpropagation algorithm in training neural networks. In particular,
    the `loss` function provides information on the accuracy of our predictions, and
    allows us to adjust the weights in each layer, to reduce the loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have assumed that backpropagation works perfectly. Unfortunately,
    that is not true. When the loss is propagated backward, the loss tends to decrease
    with each successive layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4f16081-a20b-45b3-bd17-1f0491d6404e.png)'
  prefs: []
  type: TYPE_IMG
- en: As a result, by the time the loss is propagated back toward the first few layers,
    the loss has already diminished so much that the weights do not change much at
    all. With such a small loss being propagated backward, it is impossible to adjust
    and train the weights of the first few layers. This phenomenon is known as the
    vanishing gradient problem in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the vanishing gradient problem does not affect CNNs in computer
    vision problems. However, when it comes to sequential data and RNNs, the vanishing
    gradient can have a significant impact. The vanishing gradient problem means that
    RNNs are unable to learn from early layers (early time steps), which causes it
    to have poor long-term memory.
  prefs: []
  type: TYPE_NORMAL
- en: To address this problem, Hochreiter and others proposed a clever variation of
    the RNN, known as the **long short-term memory** (**LSTM**) network.
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LSTMs are a variation of RNNs, and they solve the long-term dependency problem
    faced by conventional RNNs. Before we dive into the technicalities of LSTMs, it
    is useful to understand the intuition behind them.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs – the intuition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we explained in the previous section, LSTMs were designed to overcome the
    problem with long-term dependencies. Let''s assume we have this movie review:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5025b4a0-5ce9-46b6-b1b8-2cfaa5cfe2c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our task is to predict whether the reviewer liked the movie. As we read this
    review, we immediately understand that this review is positive. In particular,
    the following words (highlighted) are the most important:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc8f9769-2b2a-425c-b41b-421a64229f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: If we think about it, only the highlighted words are important, and we can ignore
    the rest of the words. This is an important strategy. By selectively remembering
    certain words, we can ensure that our neural network does not get bogged down
    by too many unnecessary words that do not provide much predictive power. This
    is an important distinction of LSTMs over conventional RNNs. Conventional RNNs
    have a tendency to remember everything (even unnecessary inputs) that results
    in the inability to learn from long sequences. By contrast, LSTMs selectively
    remember important inputs (such as the preceding highlighted text), and this allows
    them to handle both short- and long-term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: The ability of LSTMs to learn from both short- and long-term dependencies gives
    it its name, **long short-term memory** (**LSTM**).
  prefs: []
  type: TYPE_NORMAL
- en: What's inside an LSTM network?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LSTMs have the same repeating structure of RNNs that we have seen previously.
    However, LSTMs differ in their internal structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a high-level overview of the repeating unit of
    an LSTM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c496ae83-c098-434f-984e-5019fb236cc2.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram might look complicated to you now, but, don't worry, as
    we'll go through everything step by step. As we mentioned in the previous section,
    LSTMs have the ability to selectively remember important inputs and to forget
    the rest. The internal structure of an LSTM allows it to do that.
  prefs: []
  type: TYPE_NORMAL
- en: An LSTM differs from a conventional RNN in that it has a cell state, in addition
    to the hidden state. You can think of the cell state as the current memory of
    the LSTM. It flows from one repeating structure to the next, conveying important
    information that has to be retained at the moment. In contrast, the hidden state
    is the overall memory of the entire LSTM. It contains everything that we have
    seen so far, both important and unimportant information.
  prefs: []
  type: TYPE_NORMAL
- en: 'How does the LSTM release information between the hidden state and the cell
    state? It does so via three important gates:'
  prefs: []
  type: TYPE_NORMAL
- en: Forget gate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input gate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output gate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like physical gates, the three gates restrict the flow of information from
    the hidden state to the cell state.
  prefs: []
  type: TYPE_NORMAL
- en: Forget gate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Forget gate (f)** of an LSTM is highlighted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4818dad7-3ddd-49ec-b302-1e64e9f340f0.png)'
  prefs: []
  type: TYPE_IMG
- en: The **Forget gate (f)** forms the first part of the LSTM repeating unit, and
    its role is to decide how much data we should forget or remember from the previous
    cell state. It does so by first concatenating the **Previous Hidden State** **(****h[t−1]****)**
    and the current **Input** **(x[t]****)**, then passing the concatenated vector
    through a sigmoid function. Recall that the sigmoid function outputs a vector
    with values between 0 and 1\. A value of 0 means to stop the information from
    passing through (forget), and a value of 1 means to pass the information through
    (remember).
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the forget gate, *f,* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70b53f38-c533-4e23-9ab4-3daa49b7117b.png)'
  prefs: []
  type: TYPE_IMG
- en: Input gate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next gate is the **Input gate (i)**. The **Input gate (i)** controls how
    much information to pass to the current cell state. The input gate of an LSTM
    is highlighted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99ef8fca-77fd-4ab4-b03e-bdb4a516ee90.png)'
  prefs: []
  type: TYPE_IMG
- en: Just like the forget gate, the **Input gate (i)** takes as input the concatenation
    of the **Previous Hidden State (h[t-1]****)** and the current **Input (x[t]****)**.
    It then passes two copies of the concatenated vector through a sigmoid function
    and a tanh function, before multiplying them together.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the input gate, *i, *is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c50b499-d1cd-4f90-ada2-8ce9a399d8dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we have what is required to compute the current cell state (**C[t]**)
    to be output. This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a735add6-042e-447c-a3e8-eec2751d1ff3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The current cell state *C[t]* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/924d4067-4019-49cf-bf2c-df5f5320dfd0.png)'
  prefs: []
  type: TYPE_IMG
- en: Output gate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, the output gate controls how much information is to be retained in
    the hidden state. The output gate is highlighted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4ab5c42-4c8c-41e6-bd1d-9079314aafa3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we concatenate the **Previous Hidden State (h[t−1]****)** and the current
    **Input (x[t])**, and pass it through a sigmoid function. Then, we take the current
    cell state (*C[t]*) and pass it through a tanh function. Finally, we take the
    multiplication of the two, which is passed to the next repeating unit as the hidden
    state (*h[t]*). This process is summarized by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92ecd107-228b-434f-85ee-54dd723b56af.png)'
  prefs: []
  type: TYPE_IMG
- en: Making sense of this
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many beginners to LSTMs often get intimidated by the mathematical formulas involved.
    Although it is useful to understand the mathematical functions behind LSTMs, it
    is often difficult (and not very useful) to try to relate the intuition behind
    LSTMs and the mathematical formulas. Instead, it is more useful to understand
    LSTMs at a high level, and then to apply a black box algorithm, as we shall see
    in the later sections.
  prefs: []
  type: TYPE_NORMAL
- en: The IMDb movie reviews dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, let's take a quick look at the IMDb movie reviews dataset before
    we start building our model. It is always a good practice to understand our data
    before we build our model.
  prefs: []
  type: TYPE_NORMAL
- en: The IMDb movie reviews dataset is a corpus of movie reviews posted on the popular
    movie reviews website [https://www.imdb.com/](https://www.imdb.com/). Each movie
    review has a label indicating whether the review is positive (1) or negative (0).
  prefs: []
  type: TYPE_NORMAL
- en: 'The IMDb movie reviews dataset is provided in Keras, and we can import it by
    simply calling the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print out the first movie review as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We see a sequence of numbers, because Keras has already encoded the words as
    numbers as part of the preprocessing. We can convert the review back to words,
    using the built-in word-to-index dictionary provided by Keras as part of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can show the original review in words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Clearly, the sentiment of this review is negative! Let''s make sure by printing
    the `y` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'A `y` value of `0` refers to a negative review and a `y` value of `1` refers
    to a positive review. Let''s take a look at an example of a positive review:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To check the sentiment of the review, try this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Representing words as vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have looked at what RNNs and LSTM networks represent. There remains
    an important question we need to address: how do we represent words as input data
    for our neural network? In the case of CNNs, we saw how images are essentially
    three-dimensional vectors/matrixes, with dimensions represented by the image width,
    height, and the number of channels (three channels for color images). The values
    in the vectors represent the intensity of each individual pixel.'
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do we create a similar vector/matrix for words so that they can be used
    as input to our neural network? In earlier chapters, we saw how categorical variables
    such as the day of week can be one-hot encoded to numerical variables by creating
    a new feature for each variable. It may be tempting to think that we can also
    one-hot encode our sentences in this manner, but such a method has significant
    disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider phrases such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Happy, excited
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Happy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Excited
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows a one-hot encoded two-dimensional representation
    of these phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d146e4e6-ee6c-4319-9fe7-73f6885b8b6b.png)'
  prefs: []
  type: TYPE_IMG
- en: In this vector representation, the phrase **"Happy*,*** **excited"** has a value
    of **1** for both axes, because both the words **"Happy"** and **"Excited"** are
    present in the phrase. Similarly, the phrase **Happy** has a value of **1** for
    the **Happy** axis and a value of **0** for the **Excited** axis, because it only
    contains the word **Happy**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full two-dimensional vector representation is shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Happy** | **Excited** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: There are several problems with this one-hot encoded representation. Firstly,
    the number of axes depends on the number of unique words in our dataset. As we
    can imagine, there are tens of thousands of unique words in the English dictionary.
    If we were to create an axis for each word, then the size of our vector would
    quickly grow out of hand. Secondly, such a vector representation would be extremely
    sparse (full of zeros). This is because most words appear only once in each sentence/paragraph.
    It is difficult to train a neural network on such a sparse vector.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, and perhaps most importantly, such a vector representation does not
    take into consideration the similarity of words. In our preceding example, **Happy**
    and **Excited** are both words that convey positive emotions. However, this one-hot
    encoded representation does not take this similarity into consideration. Thus,
    important information is lost when words are represented in this form.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, there are significant disadvantages associated with one-hot encoded
    vectors. In the next section, we'll look at **word embeddings**, which overcome
    these disadvantages.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word embeddings are a learned form of vector representation for words. The main
    advantage of word embeddings is that they have fewer dimensions than the one-hot
    encoded representation, and they place similar words close to one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an example of a word embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66dd6536-a7c8-4cc8-8852-d88aac6101d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the learned word embedding knows that the words **"Elated"**, **"Happy"**,
    and **"Excited"** are similar words, and hence should be placed near each other.
    Similarly, the words **"Sad"**, **"Disappointed"**, **"Angry"**, and **"Furious"** are
    on the opposite ends of the spectrum, and should be placed far away.
  prefs: []
  type: TYPE_NORMAL
- en: We won't go into detail regarding the creation of the word embeddings, but essentially
    they are trained using supervised learning algorithms. Keras also provides a convenient
    API for training our own word embeddings. In this project, we will train our word
    embeddings on the IMDb movie reviews dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at the model architecture of our IMDb movie review sentiment
    analyzer, shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6ceb135-9662-4be0-9145-2b4411613192.png)'
  prefs: []
  type: TYPE_IMG
- en: This should be fairly familiar to you by now! Let's go through each component
    briefly.
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The input to our neural network shall be IMDb movie reviews. The reviews will
    be in the form of English sentences. As we've seen, the dataset provided in Keras
    has already encoded the English words into numbers, as neural networks require
    numerical inputs. However, there remains a problem we need to address. As we know,
    movie reviews have different lengths. If we were to represent the reviews as a
    vector, then different reviews would have different vector lengths, which is not
    acceptable for a neural network. Let's keep this in mind for now, and we'll see
    how we can address this issue as we build our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first layer in our neural network is the word embedding layer. As we've
    seen earlier, word embeddings are a learned form of vector representation for
    words. The word embedding layer takes in words as input, and then outputs a vector
    representation of these words. The vector representation should place similar
    words close to one another, and dissimilar words distant from one another. The
    word embedding layer learns this vector representation during training.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LSTM layer takes as input the vector representation of the words from the
    word embedding layer, and learns how to classify the vector representation as
    positive or negative. As we've seen earlier, LSTMs are a variation of RNNs, which
    we can think of as multiple neural networks stacked on top of one another.
  prefs: []
  type: TYPE_NORMAL
- en: Dense layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next layer is the dense layer (fully connected layer). The dense layer takes
    as input the output from the LSTM layer, and transforms it into a fully connected
    manner. Then, we apply a sigmoid activation on the dense layer, so that the final
    output is between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The output is a probability between 0 and 1, representing the probability that
    the movie review is positive or negative. A probability near to 1 means that the
    movie review is positive, while a probability near to 0 means that the movie review
    is negative.
  prefs: []
  type: TYPE_NORMAL
- en: Model building in Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're finally ready to start building our model in Keras. As a reminder, the
    model architecture that we're going to use is shown in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Importing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s import the dataset. The IMDb movie reviews dataset is already
    provided in Keras, so we can import it directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `imdb` class has a `load_data` main function, which takes in the following
    important argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '`num_words`: This is defined as the maximum number of unique words to be loaded.
    Only the *n *most common unique words (as they appear in the dataset) will be
    loaded. If *n *is small, the training time will be faster at the expense of accuracy.
    Let''s set `num_words = 10000`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `load_data` function returns two tuples as the output. The first tuple holds
    the training set, while the second tuple holds the testing set. Note that the
    `load_data` function splits the data equally and randomly into training and testing
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code imports the data, with the previously mentioned parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s do a quick check to see the amount of data we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6e43246-75a5-45ca-a83c-7928975691e4.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that we have `25000` training and testing samples each.
  prefs: []
  type: TYPE_NORMAL
- en: Zero padding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can use the data as input to our neural network, we need to address
    an issue. Recall that in the previous section, we mentioned that movie reviews
    have different lengths, and therefore the input vectors have different sizes.
    This is an issue, as neural networks only accept fixed-size vectors.
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, we are going to define a `maxlen` parameter. The `maxlen` parameter
    shall be the maximum length of each movie review. Reviews that are longer than
    `maxlen` will be truncated, and reviews that are shorter than `maxlen` will be
    padded with zeros.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the zero padding process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd494e9a-92be-4cfc-9c9b-5558270af72a.png)'
  prefs: []
  type: TYPE_IMG
- en: Using zero padding, we ensure that the input will have a fixed vector length.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, Keras provides a handy function to perform zero padding. Under the
    Keras `preprocessing` module, there''s a `sequence` class that allows us to perform
    preprocessing for sequential data. Let''s import the `sequence` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sequence` class has a `pad_sequences` function that allows us to perform
    zero padding on our sequential data. Let''s truncate and pad our training and
    testing data using a `maxlen` of `100`. The following code shows how we can do
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s verify the vector length after zero padding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd7a4488-6528-492b-88d7-2c21a2bdb401.png)'
  prefs: []
  type: TYPE_IMG
- en: Word embedding and LSTM layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our input preprocessed, we can now turn our attention to model building.
    As always, we will use the `Sequential` class in Keras to build our model. Recall
    that the `Sequential` class allows us to stack layers on top of one another, making
    it really easy to build complex models layer by layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, let''s define a new `Sequential` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now add the word embedding layer to our model. The word embedding layer
    can be constructed directly from the `keras.layers` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Embedding` class takes the following important arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`input_dim`: **The input dimensions of the word embedding layer. This should
    be the same as the `num_words` parameter that we used when we loaded in our data.
    Essentially, this is the maximum number of unique words in our dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_dim`: The output dimensions of the word embedding layer. This should
    be a hyperparameter to be fine-tuned. For now, let''s use a value of `128`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can add an embedding layer with the previously mentioned parameters to our
    sequential model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we can add a `LSTM` layer directly from `keras.layers` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `LSTM` class takes the following important arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`units`: This refers to the number of recurring units in the `LSTM` layer.
    A larger number of units results in a more complex model, at the expense of training
    time and overfitting. For now, let''s use a typical value of `128` for the number
    of units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation`: This refers to the type of activation function applied to the
    cell state and the hidden state. The default value is the tanh function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`recurrent_activation`: This refers to the type of activation function applied
    to the forget, input, and output gates. The default value is the `sigmoid` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might notice that the kind of activation function is rather limited in Keras.
    Instead of selecting individual activations for the forget, input, and output
    gates, we are limited to choosing a single activation function for all three gates.
    This is unfortunately a limitation that we need to work with. However, the good
    news is that this deviation from theory does not significantly affect our results.
    The LSTM that we build in Keras is perfectly able to learn from the sequential
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can add an `LSTM` layer with the previously mentioned parameters to our
    sequential model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we add a `Dense` layer with `sigmoid` as the `activation` function.
    Recall that the purpose of this layer is to ensure that the output of our model
    has a value between `0` and `1`, representing the probability that the movie review
    is positive. We can add a `Dense` layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Dense` layer is the final layer in our neural network. Let''s verify the
    structure of our model by calling the `summary()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46b595da-5c3a-43ee-8d10-1d99999d405c.png)'
  prefs: []
  type: TYPE_IMG
- en: Nice! We can see that the structure of our Keras model matches the model architecture
    in the diagram that we introduced at the start of the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling and training models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the model building complete, we''re ready to compile and train our model.
    By now, you should be familiar with the model compilation in Keras. As always,
    there are certain parameters we need to decide when we compile our model. They
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loss function**: We use a `binary_crossentropy` loss function when the target
    output is binary and a `categorical_crossentropy` loss function when the target
    output is multi-class. Since the sentiment of movie reviews in this project is
    **binary** (that is, positive or negative), we will use a `binary_crossentropy`
    loss function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer**: The choice of optimizer is an interesting problem in LSTMs.
    Without getting into the technicalities, certain optimizers may not work for certain
    datasets, due to the vanishing gradient and the **exploding gradient problem**
    (the opposite of the vanishing gradient problem). It is often impossible to know
    beforehand which optimizer works better for the dataset. Therefore, the best way
    to know is to train different models using different optimizers, and to use the
    optimizer that gives the best results. Let''s try the `SGD`, `RMSprop`, and the
    `adam` optimizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can compile our model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s train our model for `10` epochs, using the testing set as the validation
    data. We can do so as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `scores` object returned is a Python dictionary that provides the training
    and validation accuracy and the loss per epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go on to analyze our results, let's put all our code into a single
    function. This allows us to easily test and compare the performance of different
    optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a `train_model()` function that takes in an `Optimizer` as an argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this function, let''s train three different models using three different
    optimizers, the `SGD`, `RMSprop`, and the `adam` optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Analyzing the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s plot the validation accuracy per epoch for the three different models.
    First, we plot for the model trained using the `sgd` optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f1825b8-fcce-41c9-9eb9-b9f9abd20cac.png)'
  prefs: []
  type: TYPE_IMG
- en: Did you notice anything wrong? The training and validation accuracy is stuck
    at 50%! Essentially, this shows that the training has failed and our neural network
    performs no better than a random coin toss for this binary classification task.
    Clearly, the `sgd` optimizer is not suitable for this dataset and this LSTM network.
    Can we do better if we use another optimizer? Let's try the `RMSprop` optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We plot the training and validation accuracy for the model trained using the
    `RMSprop` optimizer, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cc437e5-ae37-4787-a4ce-02b21068f538.png)'
  prefs: []
  type: TYPE_IMG
- en: That's much better! Within 10 epochs, our model is able to achieve a training
    accuracy of more than 95% and a validation accuracy of around 85%. That's not
    bad at all. Clearly, the `RMSprop` optimizer performs better than the `sgd` optimizer
    for this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s try the `adam` optimizer and see how it performs. We plot the
    training and validation accuracy for the model trained using the `adam` optimizer,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/342e8c75-01db-4fa2-ac2a-737051be217f.png)'
  prefs: []
  type: TYPE_IMG
- en: The `adam` optimizer does pretty well. From the preceding graph, we can see
    that the `Training Accuracy` is almost 100% after `10` epochs, while the `Validation
    Accuracy` is around 80%. This gap of 20% suggests that overfitting is happening
    when the `adam` optimizer is used.
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, the gap between training and validation accuracy is smaller for
    the `RMSprop` optimizer. Hence, we conclude that the `RMSprop` optimizer is the
    most optimal for this dataset and the LSTM network, and we shall use the model
    built using the `RMSprop` optimizer from this point onward.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml), *Diabetes Prediction
    with Multilayer Perceptrons*, we saw how the confusion matrix is a useful visualization
    tool to evaluate the performance of our model. Let's also use the confusion matrix
    to evaluate the performance of our model in this project.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, these are the definitions of the terms in the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True negative**: The actual class is negative (negative sentiment), and the
    model also predicted negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False positive**: The actual class is negative (negative sentiment), but
    the model predicted positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False negative**: The actual class is positive (positive sentiment), but
    the model predicted negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True positive**: The actual class is positive (positive sentiment), and the
    model predicted positive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want our false positive and false negative numbers to be as low as possible,
    and for the true negative and true positive numbers to be as high as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can construct a confusion matrix using the `confusion_matrix` class from
    `sklearn`, using `seaborn` for visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f968623a-9280-45d6-b958-9c3851e9f407.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding confusion matrix, we can see that most of the testing data
    was classified correctly, with the number of true negatives and true positives
    at around 85%. In other words, our model is 85% accurate at predicting sentiment
    for movie reviews. That's pretty impressive!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at some of the wrongly classified samples, and see where
    the model got it wrong. The following code captures the index of the wrongly classified
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Let's first take a look at the false positives. As a reminder, false positives
    refer to movie reviews that were negative but that our model wrongly classified
    as positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have selected an interesting false positive; this is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Even as a human, it is hard to predict the sentiment of this movie review! The
    first sentence of the movie probably sets the tone of the reviewer. However, it
    is written in a really subtle manner, and it is difficult for our model to pick
    out the intention of the sentence. Furthermore, the middle of the review praises
    the movie, before ending with the conclusion that the `movie gets very twisted
    at points and is hard to really understand`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at some false negatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This review is definitely on the fence, and it looked pretty neutral, with the
    reviewer presenting the good and bad of the movie. Another point to note is that,
    at the start of the review, the reviewer quoted another reviewer (`I hate reading
    reviews that say something like 'don't waste your time this film stinks on ice'`).
    Our model probably didn't understand that this quote is not the opinion of this
    reviewer. Quoted text is definitely a challenge for most NLP models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at another false negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This movie review can be considered a *rant* against other movie reviews, similar
    to the previous review that we showed. The presence of multiple negative words
    in the movie probably misled our model, and our model did not understand that
    the review was ranting against all the other negative reviews. Statistically speaking,
    such reviews are relatively rare, and it is difficult for our model to learn the
    true sentiment of such reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered a lot in this chapter. Let''s consolidate all our code here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we created an LSTM-based neural network that can predict the
    sentiment of movie reviews with 85% accuracy. We first looked at the theory behind
    recurrent neural networks and LSTMs, and we understood that they are a special
    class of neural network designed to handle sequential data, where the order of
    the data matters.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at how we can convert sequential data such as a paragraph of
    text into a numerical vector, as input for neural networks. We saw how word embeddings
    can reduce the dimensionality of such a numerical vector into something more manageable
    for training neural networks, without necessarily losing information. A word embedding
    layer does this by learning which words are similar to one another, and it places
    such words in a cluster, in the transformed vector.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at how we can easily construct a LSTM neural network in Keras,
    using the `Sequential` model. We also investigated the effect of different optimizers
    on the LSTM, and we saw how the LSTM is unable to learn from the data when certain
    optimizers are used. More importantly, we saw that tuning and experimenting is
    an essential part of the machine learning process, in order to maximize our results.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we analyzed our results, and we saw how LSTM-based neural networks fail
    to detect sarcasm and other subtleties in our language. NLP is an extremely challenging
    subfield of machine learning that researchers are still working on today.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 7](9223bc03-bd68-42df-93ff-32c5d0f7e246.xhtml),
    *Implementing a Facial Recognition System with Neural Networks,* we'll look at
    **Siamese neural networks**, and how they can be used to create a face recognition
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are sequential problems in machine learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sequential problems are a class of problem in machine learning in which the
    order of the features presented to the model is important for making predictions.
    Examples of sequential problems include NLP problems (for example, speech and
    text) and time series problems.
  prefs: []
  type: TYPE_NORMAL
- en: What are some reasons that make it challenging for AI to solve sentiment analysis
    problems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Human languages often contain words that have different meanings, depending
    on the context. It is therefore important for a machine learning model to fully
    understand the context before making a prediction. Furthermore, sarcasm is common
    in human languages, which is difficult for an AI-based model to comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: How is an RNN different than a CNN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RNNs can be thought of as multiple, recursive copies of a single neural network.
    Each layer in an RNN passes its output to the next layer as input. This allows
    an RNN to use sequential data as input.
  prefs: []
  type: TYPE_NORMAL
- en: What is the hidden state of an RNN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The intermediate output passed from layer to layer in an RNN is known as the
    hidden state. The hidden state allows an RNN to maintain a memory of the intermediate
    states from the sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: What are the disadvantages of using an RNN for sequential problems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RNNs suffer from the vanishing gradient problem, which results in features early
    on in the sequence being "forgotten" due to the small weights assigned to them.
    Therefore, we say that RNNs have a long-term dependency problem.
  prefs: []
  type: TYPE_NORMAL
- en: How is an LSTM network different than a conventional RNN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LSTM networks are designed to overcome the long-term dependency problem in conventional
    RNNs. An LSTM network contains three gates (input, output, and forget gates),
    which allows it to place emphasis on certain features (that is, words), regardless
    of when the feature appears in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: What is the disadvantage of one-hot encoding words to transform them to numerical
    inputs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dimensionality of a one-hot encoded word vector tends to be huge (due to
    the amount of different words in a language), which makes it difficult for the
    neural network to learn from the vector. Furthermore, a one-hot encoded vector
    does not take into consideration the relationships between similar words in a
    language.
  prefs: []
  type: TYPE_NORMAL
- en: What are word embeddings?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Word embeddings are a learned formed of vector representation for words. The
    main advantage of word embeddings is that they have smaller dimensions than the
    one-hot encoded representation, and they place similar words close to one another.
    Word embeddings are usually the first layer in an LSTM-based neural network.
  prefs: []
  type: TYPE_NORMAL
- en: What important preprocessing step is required when working with textual data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Textual data often has uneven lengths, which results in vectors of different
    sizes. Neural networks are unable to accept vectors of different sizes as input.
    Therefore, we apply zero padding as a preprocessing step, to truncate and pad
    vectors evenly.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning and experimenting is often an essential part of the machine learning
    process. What experimenting have we done in this project?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this project, we experimented with different optimizers (the `SGD`, `RMSprop`,
    and `adam` optimizers) for training our neural network. We found that the `SGD`
    optimizer was unable to train the LSTM network, while the `RMSprop` optimizer
    had the best accuracy.
  prefs: []
  type: TYPE_NORMAL
