["```py\n    pip install eli5\n    ```", "```py\n    pip install lime\n    ```", "```py\n    pip install shap\n    ```", "```py\n    pip install dowhy\n    ```", "```py\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import make_scorer\n    from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score\n    import pickle\n    import eli5\n    from eli5.sklearn import PermutationImportance\n    import lime\n    from lime import lime_tabular\n    import shap\n    ```", "```py\n    dataset = pd.read_csv('https://raw.githubusercontent.com/krishnaik06/Lime-Model-Interpretation/main/Churn_Modelling.csv')\n    X = dataset.iloc[:, 3:13]\n    y = dataset.iloc[:, 13]\n    geography = pd.get_dummies(X[\"Geography\"],\n            drop_first=False)\n    gender=pd.get_dummies(X['Gender'],drop_first=False)\n    ```", "```py\n    X=pd.concat([X,geography,gender],axis=1)\n    X=X.drop(['Geography','Gender'],axis=1)\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(X,\n            y, test_size = 0.2, random_state = 0)\n    ```", "```py\n    Classifier=RandomForestClassifier()\n    classifier.fit(X_train,y_train)\n    ```", "```py\n    y_predict = classifier.predict(X_test)\n    y_prob = [probs[1] for probs in\n            classifier.predict_proba(X_test)]\n    ```", "```py\n    # Compute area under the curve\n    fpr, tpr, _ = roc_curve(y_test, y_prob)\n    roc_auc = auc(fpr, tpr)\n    # Plot ROC curve\n    plt.figure()\n    plt.plot(fpr, tpr, color='orange',\n            lw=2, label='ROC curve (area = %0.2f)' %\n            roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2,\n            linestyle='—')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(\"Churn Modeling\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    ```", "```py\n    # Feature importance in a dataframe\n    imp_df = pd.DataFrame({\n            'feature': X_train.columns.values,\n            'importance':classifier.feature_importances_})\n     # Reorder by importance\n    ordered_df = imp_df.sort_values(by='importance')\n    imp_range = range(1,len(imp_df.index)+1)\n    ## Bar chart with confidence intervals\n    height = ordered_df['importance']\n    bars = ordered_df['feature']\n    y_pos = np.arange(len(bars))\n    plt.barh(y_pos, height)\n    plt.yticks(y_pos, bars)\n    plt.xlabel(\"Mean reduction in tree impurity in random forest\")\n    plt.tight_layout()\n    plt.show()\n    ```", "```py\n    perm_test = PermutationImportance(classifier,\n            scoring=make_scorer(roc_auc_score),\n            n_iter=50, random_state=123, cv=\"prefit\")\n    ```", "```py\n    perm_test.fit(X_train, y_train)\n    ```", "```py\n    imp_df = eli5.explain_weights_df(perm_test)\n    ```", "```py\n    label_df = pd.DataFrame({\n            'feature': [ \"x\" + str(i) for i in range(\n            len(X_test.columns))],\n            'feature_name': X_test.columns.values})\n    imp_df = pd.merge(label_df, imp_df, on='feature',\n            how='inner', validate=\"one_to_one\")\n    # Reorder by importance\n    ordered_df = imp_df.sort_values(by='weight')\n    imp_range=range(1,len(imp_df.index)+1)\n    ## Bar chart with confidence intervals\n    height = ordered_df['weight']\n    bars = ordered_df['feature_name']\n    y_pos = np.arange(len(bars))\n    plt.barh(y_pos, height)\n    plt.yticks(y_pos, bars)\n    plt.xlabel(\"Permutation feature importance training\n            set (decrease in AUC)\")\n    plt.tight_layout()\n    plt.show()\n    ```", "```py\ninterpretor = lime_tabular.LimeTabularExplainer(\n    training_data=np.array(X_train),\n    feature_names=X_train.columns,\n    mode='classification')\n```", "```py\n    X_test.iloc[3]:\n    CreditScore           788.00\n    Age                    32.00\n    Tenure                  4.00\n    Balance            112079.58\n    NumOfProducts           1.00\n    HasCrCard               0.00\n    IsActiveMember          0.00\n    EstimatedSalary     89368.59\n    France                  1.00\n    Germany                 0.00\n    Spain                   0.00\n    Female                  0.00\n    Male                    1.00\n    Name: 5906, dtype: float64\n    ```", "```py\n    exp = interpretor.explain_instance(\n        data_row=X_test.iloc[3], #new data\n        predict_fn=classifier.predict_proba\n    )\n    exp.show_in_notebook(show_table=True)\n    ```", "```py\n    explainer = shap.TreeExplainer(classifier)\n    shap_values = explainer.shap_values(X_test)\n    ```", "```py\n    shap.initjs() #initialize javascript in cell\n    shap.force_plot(explainer.expected_value[0],\n            shap_values[0][3,:], X_test.iloc[3,:])\n    ```", "```py\n    shap.dependence_plot(\"Age\", shap_values[0], X_test)\n    ```", "```py\n    shap.summary_plot(shap_values[1], X_test, plot_type=\"violin\")\n    ```", "```py\n    import dowhy\n    import dowhy.datasets\n    from dowhy import CausalModel\n    ```", "```py\n    # Generate data\n    data = dowhy.datasets.linear_dataset(beta=10,\n            num_common_causes=5,\n            num_instruments = 2,\n            num_effect_modifiers=2,\n            num_samples=6000,\n            treatment_is_binary=True,\n            stddev_treatment_noise=9,\n            num_discrete_common_causes=1)\n    df = data[\"df\"]\n    ```", "```py\n# Input a causal graph in GML format\nmodel=CausalModel(\n        data = df,\n        treatment=data[\"treatment_name\"],\n        outcome=data[\"outcome_name\"],\n        graph=data[\"gml_graph\"]\n        )\n```", "```py\n    # Identification\n    identified_estimand =\n            model.identify_effect(\n            proceed_when_unidentifiable=True)\n    print(identified_estimand)\n    >>>\n     Estimand type: EstimandType.NONPARAMETRIC_ATE\n    ### Estimand : 1\n    Estimand name: backdoor\n    Estimand expression:\n      d\n    ─────(E[y|W0,W1,W3,W2,W4])\n    d[v₀]\n    Estimand assumption 1, Unconfoundedness: If U→{v0} and U→y then P(y|v0,W0,W1,W3,W2,W4,U) = P(y|v0,W0,W1,W3,W2,W4)\n    ### Estimand : 2\n    Estimand name: iv\n    Estimand expression:\n     ⎡                              -1 ⎤\n     ⎢    d        ⎛    d          ⎞  ⎥\n    E⎢─────────(y)⋅ ─────────([v₀])    ⎥\n     ⎣d[Z₁  Z₀]     ⎝d[Z₁  Z₀]       ⎠  ⎦\n    Estimand assumption 1, As-if-random: If U→→y then ¬(U →→{Z1,Z0})\n    Estimand assumption 2, Exclusion: If we remove {Z1,Z0}→{v0}, then ¬({Z1,Z0}→y)\n    ### Estimand : 3\n    Estimand name: frontdoor\n    No such variable(s) found!\n    ```", "```py\n    # Estimation\n    causal_estimate =\n            model.estimate_effect(identified_estimand,\n            method_name=\"backdoor.propensity_score_stratification\")\n    print(causal_estimate)\n    >>>\n     *** Causal Estimate ***\n    ## Identified estimand\n    Estimand type: EstimandType.NONPARAMETRIC_ATE\n    ### Estimand : 1\n    Estimand name: backdoor\n    Estimand expression:\n      d\n    ─────(E[y|W0,W1,W3,W2,W4])\n    d[v₀]\n    Estimand assumption 1, Unconfoundedness: If U→{v0} and U→y then P(y|v0,W0,W1,W3,W2,W4,U) = P(y|v0,W0,W1,W3,W2,W4)\n    ## Realized estimand\n    b: y~v0+W0+W1+W3+W2+W4\n    Target units: ate\n    ## Estimate\n    Mean value: 7.151535367146138\n    ```"]