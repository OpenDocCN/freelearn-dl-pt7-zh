<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep Learning for Finance</h1>
                </header>
            
            <article>
                
<p class="mce-root">Deep learning is one of the most exciting new technologies being used in the financial services industry, and when used correctly, can improve investment returns. While tasks such as computer vision and <strong>natural language processing</strong> (<strong>NLP</strong>) are well-researched areas, the use of <strong>Artificial Intelligence</strong> (<strong>AI</strong>) techniques in financial services is still growing. It's important to note that some of the most advanced, lucrative deep learning techniques in AI are not published, nor will they ever be. The lucrative nature of the financial services space necessitates guarding advanced algorithms and measures, and so in this chapter we will focus on principles. </p>
<p class="mce-root"><span>The application of AI in the financial services industry is nuanced; ...</span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Requirements</h1>
                </header>
            
            <article>
                
<p>As usual, we will be utilizing Python 3 for our analysis. Python is an excellent choice for quantitative trading applications that have a frequency that's greater than a few seconds. For high-frequency applications, it is recommended that you use a mid-level language such as Java or C++.</p>
<p>In this chapter, we will be using finance-specific Python libraries on top of our standard deep learning stack: </p>
<p>Zipline—An algorithmic trading library in Python. It is currently used as the backtesting package for the quantitative trading website Quantopian (<a href="https://www.quantopian.com">https://www.quantopian.com</a>).</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to AI in finance</h1>
                </header>
            
            <article>
                
<p>Despite being one of the most computationally intensive fields, financial services is full of heuristics. The application of advanced AI techniques is tenuous at best; many firms simply don't engage in strategies that allow for easy adoption of AI. Talent wars for top quantitative talent with Silicon Valley has also made the problem worse. You may be saying to yourself <em>don't I need to have a finance background to work with this data? </em>It's worth noting that two of the world's top hedge funds were founded by teams that participated in the famous Netflix Machine Learning challenge. While there is incredible benefit in studying the techniques of algorithmic trading, you can get started with your knowledge of ANNs ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep learning in trading</h1>
                </header>
            
            <article>
                
<p><strong>Trading</strong> is the buying and selling of items in the financial market; in financial parlance, we call these items <strong>derivatives</strong>. Trades can be short-term (inter-day), medium-term (several days), or long-term (several weeks or more). <span>According to experts at JP Morgan Chase, one of the largest banks in the world, AI applications are proven to be better suited than humans at short and medium-term trading strategies. In this section, we'll explore some fundamental strategies for developing intelligent trading algorithms for short and medium- term trades. But first, let's cover some basic concepts.</span></p>
<p>Trading strategies seek to exploit<span> </span>market inefficiencies<strong> </strong>in order to make profit. <span>One of the core policies in algorithmic training is called</span> <strong>alpha</strong>,<strong> </strong>which is a measure of performance. Alpha measures the active return on a specific investment by matching a stock against an index. The difference between the performance of an individual investment and its matched index is the investment's alpha. In building networks for trading strategies, we want our networks to spot market inefficiencies that generate the most alpha for us.</p>
<p>We can generally break traditional stock analysis down into two categories:</p>
<ul>
<li><strong>Fundamental analysis</strong> looks at the underlying factors that could influence a financial derivative, such as the general financial health of a company</li>
<li><strong>Technical analysis</strong> looks at the actual performance of the financial derivative in a more mathematical sense, attempting to predict price movements based on patterns in the asset's price movements</li>
</ul>
<p>In both of these cases, analysis is typically done with human reasoning, whereas deep learning comes into the world of <strong>quantitative analysis</strong>, specifically in what is known as <strong>algorithmic trading</strong>. Broadly defined, algorithmic trading is just as it sounds: trading that is conducted by a coded algorithm and not a physical human. Algorithmic trading strategies are validated by a process called <strong>backtesting</strong>, which runs the algorithm on historical data to determine whether it will perform well in the market.</p>
<p>Algorithmic trading is used in several different types of areas:</p>
<ul>
<li><strong>Buy</strong>-<strong>side firms</strong>: Firms utilize algorithmic trading to manage their mid-to long-term portfolio investments</li>
<li><strong>Sell</strong>-<strong>side firms</strong>: Firms use high-frequency algorithmic trading to take advantage of market opportunities and move markets themselves</li>
<li><strong>Systematic traders</strong>: These individuals and firms try to match a long-term investment with a short-term investment of highly correlated financial derivatives</li>
</ul>
<p class="mce-root"/>
<p>What's shared among all three of these market entities is that algorithmic trading provides a more stable and systematic approach to active investing, which is something that a human instinct could not provide.</p>
<p>Another strategy relies on technical indicators, which are mathematical calculations based on the historical analysis of data. <span>Most trading algorithms are used in what is known as </span><strong>high</strong>-<strong>frequency trading</strong> (<strong>HFT</strong>)<span>, which attempts to exploit market inefficiencies by conducting large numbers of extremely fast trades across markets. Unless you have access to some seriously fast computer hardware, it's unlikely for an individual to compete in this arena. Instead, we're going to build some fundamental algorithms in TensorFlow for non-HFT algorithmic trading.</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a trading platform</h1>
                </header>
            
            <article>
                
<p>Before we dive into any particular strategies, let's get started with building the basis for our trading platform. In this section, we'll build out the code that will handle data ingestion and trading, and then we'll dive into two specific strategies.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic trading functions</h1>
                </header>
            
            <article>
                
<p>Let's start with the fundamental actions our platform could take on the market; we need it to be able to buy, sell, or hold stock:</p>
<ol>
<li>First, let's start with some imports:</li>
</ol>
<pre style="padding-left: 60px">import math<br/>from time import time<br/>from enum import Enum</pre>
<ol start="2">
<li>To make things easier on us down the road, we're going to wrap these functions inside a single class that we'll call <kbd>TradingPosition</kbd>:</li>
</ol>
<pre style="padding-left: 60px">class TradingPosition(object):<br/>   ''' Class that manages the trading position of our platform'''<br/><br/>    def __init__(self, code, buy_price, amount, next_price):<br/>        self.code = code ## Status code for what action our algorithm is taking<br/>        self.amount = amount ## The amount of the trade<br/>        self.buy_price = buy_price ## The purchase price of a trade<br/>        self.current_price = buy_price ## Buy price of the trade<br/>        self.current_value = self.current_price * self.amount<br/>        self.pro_value = next_price * self.amount</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>Let's break the input variables down. The first variable that we are initializing is <kbd>code</kbd>, which we'll be using later as a status code for the action we are taking to buy, sell, or hold. We then create the variables for the price of the security, the amount of the security (that is, how much stock), and the current value of the security.</li>
<li>Now that we have our variables, we can start coding our trading actions. We'll want to create a <kbd>status</kbd> function, which tracks the movement of prices in the market. For simplicity, we'll call this function <kbd>TradeStatus</kbd>:</li>
</ol>
<pre style="padding-left: 60px">def TradeStatus(self, current_price, next_price, amount):<br/>        ''' Manages the status of a trade that is in action '''<br/>        self.current_price = current_price ## updates the current price variable that is maintained within the class<br/>        self.current_value = self.current_price * amount<br/>        pro_value = next_price * amount</pre>
<ol start="5"/>
<ol start="5">
<li>Next, let's create a function to <kbd>buy a stock</kbd>:</li>
</ol>
<pre style="padding-left: 60px">def BuyStock(self, buy_price, amount, next_price):<br/> ''' Function to buy a stock '''<br/>     self.buy_price = ((self.amount * self.buy_price) + (amount * buy_price)) / (self.amount + amount)<br/> self.amount += amount<br/>     self.TradeStatus(buy_price, next_price)</pre>
<ol start="6">
<li>Here, our function takes a <kbd>buy price</kbd>, the <kbd>amount</kbd>, and the <kbd>next price</kbd> in the series. We calculate the <kbd>buy price</kbd>, update our trading volume, and return a <kbd>trade status</kbd>. Next, let's move on to <kbd>sell a stock</kbd>: </li>
</ol>
<pre style="padding-left: 60px">def SellStock(self, sell_price, amount, next_price):<br/>''' Function to sell a stock '''<br/>     self.current_price = sell_price<br/>     self.amount -= amount<br/>     self.TradeStatus(sell_price, next_price)</pre>
<ol start="7">
<li>In terms of the buy function, we feed in the <kbd>sell price</kbd> and the volume, update the class's internal variables, and return a status. Lastly, we'll just create a simple function to hold a stock, which more or less gives us a status of what the current price of that stock is:</li>
</ol>
<pre style="padding-left: 60px"> def HoldStock(self, current_price, next_price):<br/> ''' Function to hold a stock '''<br/>     self.TradeStatus(current_price, next_price)</pre>
<ol start="8">
<li>Now, let's move on to creating a class that will represent our artificial trader. </li>
</ol>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Creating an artificial trader</h1>
                </header>
            
            <article>
                
<p>While utilizing algorithms to inform trading decisions is the definition of algorithmic trading, this is not necessarily automated trading. For that, we need to create an artificial trading agent that will execute our strategies for us:</p>
<ol>
<li>We'll call this class <kbd>Trader</kbd> and initialize all of the variables that we'll need for the trader algorithm:</li>
</ol>
<pre style="padding-left: 60px">class Trader(object): ''' An Artificial Trading Agent '''     def __init__(self, market, cash=100000000.0):         ## Initialize all the variables we need for our trader         self.cash = cash ## Our Cash Variable         self.market = market ##         self.codes = market.codes         self.reward = 0         self.positions = []         self.action_times = 0         self.initial_cash = cash         self.max_cash = cash * 3 self.total_rewards ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Managing market data</h1>
                </header>
            
            <article>
                
<p><span>As with any machine learning algorithm, selecting features for market prediction algorithms is critical, and can lead to the success or failure of the algorithm's strategy. </span><span>To reduce price curve data into its most fundamental parts, we can use a dimensionality reduction algorithm such as PCA, or even embed stock information to try to capture the most salient latent features. </span><span>As we have learned, deep learning can help us overcome some of these selection issues, as ANNs implicitly conduct feature selection during the training process:</span></p>
<ol>
<li>We'll call our new class <kbd>MarketHandler</kbd> and initialize all of the parameters and data that will be needed to handle our different trading strategies:</li>
</ol>
<pre style="padding-left: 60px">class MarketHandler(object):<br/> ''' Class for handling our platform's interaction with market data'''<br/>     Running = 0<br/>     Done = -1<br/><br/>     def __init__(self, codes, start_date="2008-01-01", end_date="2018-05-31", **options):<br/>         self.codes = codes<br/>         self.index_codes = []<br/>         self.state_codes = []<br/>         self.dates = []<br/>         self.t_dates = []<br/>         self.e_dates = []<br/>         self.origin_frames = dict()<br/>         self.scaled_frames = dict()<br/>         self.data_x = None<br/>         self.data_y = None<br/>         self.seq_data_x = None<br/>         self.seq_data_y = None<br/>         self.next_date = None<br/>         self.iter_dates = None<br/>         self.current_date = None<br/><br/>         ## Initialize the stock data that will be fed in <br/>         self._init_data(start_date, end_date)<br/><br/>         self.state_codes = self.codes + self.index_codes<br/>         self.scaler = [scaler() for _ in self.state_codes]<br/>         self.trader = Trader(self, cash=self.init_cash)<br/>         self.doc_class = Stock if self.m_type == 'stock' else Future</pre>
<ol start="2">
<li>We'll also need to initialize a great deal of data handling processes to correctly manipulate our data for analysis:</li>
</ol>
<pre style="padding-left: 60px">def _init_data_frames(self, start_date, end_date):<br/>     self._validate_codes()<br/>     columns, dates_set = ['open', 'high', 'low', 'close', 'volume'], set()<br/>     ## Load the actual data<br/>     for index, code in enumerate(self.state_codes):<br/>         instrument_docs = self.doc_class.get_k_data(code, start_date, end_date)<br/>         instrument_dicts = [instrument.to_dic() for instrument in instrument_docs]<br/>         dates = [instrument[1] for instrument in instrument_dicts]<br/>         instruments = [instrument[2:] for instrument in instrument_dicts]<br/>         dates_set = dates_set.union(dates)<br/>         scaler = self.scaler[index]<br/>         scaler.fit(instruments)<br/>         instruments_scaled = scaler.transform(instruments)<br/>         origin_frame = pd.DataFrame(data=instruments, index=dates, columns=columns)<br/>         scaled_frame = pd.DataFrame(data=instruments_scaled, index=dates, columns=columns)<br/>         self.origin_frames[code] = origin_frame<br/> self.scaled_frames[code] = scaled_frame<br/>         self.dates = sorted(list(dates_set))<br/>    for code in self.state_codes:<br/>         origin_frame = self.origin_frames[code]<br/>         scaled_frame = self.scaled_frames[code]<br/>         self.origin_frames[code] = origin_frame.reindex(self.dates, method='bfill')<br/>         self.scaled_frames[code] = scaled_frame.reindex(self.dates, method='bfill')</pre>
<ol start="3">
<li>Now we initialize the <kbd>env_data()</kbd> method and call the <kbd>self</kbd> class:</li>
</ol>
<pre style="padding-left: 60px">def _init_env_data(self):<br/>     if not self.use_sequence:<br/>         self._init_series_data()<br/>     else:<br/>         self._init_sequence_data()</pre>
<ol start="4">
<li>Lastly, let's initialize the data handling function that we just created: </li>
</ol>
<pre style="padding-left: 60px">self._init_data_frames(start_date, end_date)</pre>
<p>Next, let's dive into building the models for our platform. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Price prediction utilizing LSTMs</h1>
                </header>
            
            <article>
                
<p>Let's start by walking through a supervised learning example utilizing an LSTM to predict where the price of a given stock is going, based on its past performance. As we learned in previous chapters, LSTMs and <strong>Recurrent Neural Networks </strong>(<strong>RNN</strong>) in general are superior at modeling and prediction for series data. This model will utilize the trading platform structure that we created previously:</p>
<ol>
<li>Let's start with our imports:</li>
</ol>
<pre style="padding-left: 60px">import tensorflow as tffrom sklearn.preprocessing import MinMaxScalerimport loggingimport os</pre>
<ol start="2">
<li>Let's create a class that will contain all of the code needed to run the RNN, which we'll call <kbd>TradingRNN</kbd>. We'll also initialize the necessary variables:</li>
</ol>
<pre style="padding-left: 60px">class TradingRNN(): ''' An RNN Model for ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Backtesting your algorithm</h1>
                </header>
            
            <article>
                
<p>Backtesting is the process of testing your trading algorithm on bits of historical data in order to simulate its performance. While it's no guarantee that the algorithm will perform well in the wild, it gives us a good idea of how it will perform. </p>
<p>In Python, we can backtest our algorithms using a library called <strong>Zipline</strong>. Zipline was created by the online trading algorithms platform Quantopian as their backtesting platform, and it's been open sourced to the public on GitHub. It provides ten years of historical stock data and a realistic trading environment in which you can test algorithms, including transaction costs, order delays, and<span> </span>slippage<strong>.</strong> Slippage is the price difference that can occur between the expected price at which a trade happens and the actual price it's executed at. <span>To get started with Zipline in Python, we simply need to</span> run <kbd>pip install zipline</kbd> <span>on the command line.</span></p>
<p><span>Any time we use Zipline, we must define two functions:</span></p>
<ul>
<li><kbd>initialize(context)</kbd>: This is called by Zipline before it starts running your algorithm. The context variable contains all of the global variables needed in your algorithm. Initialize is very similar to how we initialize variables in TensorFlow before running them through a session. </li>
<li><kbd>handle_data(context, data)</kbd>: This function does exactly what it says: it passes the open, high, low, and close stock market data to your algorithm, along with the necessary context variables. </li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Event-driven trading platforms</h1>
                </header>
            
            <article>
                
<p>Event-driven investing is an investing strategy that focuses on socioeconomic factors that might influence the stock market's movements, particularly right before a financial event such as an earnings call or merger. This strategy is typically used by larger funds, as they frequently have access to information not entirely open to the public and because it requires a large amount of expertise in analyzing these events correctly. </p>
<p>To do this, we'll extract events from raw text into tuples that describe the event. For instance, if we said that <em>Google</em> buys <em>Facebook</em>, the tuple would be (<em>Actor = Google, Action = buys, Object = Facebook, Time = January 1 2018</em>). These tuples can help us boil down events into their ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Gathering stock price data</h1>
                </header>
            
            <article>
                
<p>The majority of real-time market data comes though paid services; think Bloomberg terminals or a brokerage firm's website. Currently, the only non-paid real-time data API for financial markets is Alpha Vantage, which is maintained by a conglomerate of business and academic interests. You can install it by running <kbd>pip install alpha_vantage</kbd> on your command line. You can sign up for a free API key on Alpha Vantage's website.</p>
<p>Once you have your key, you can easily query the <kbd>api</kbd> with the following:</p>
<pre>ts = TimeSeries(key='YOUR_API_KEY', output_format='pandas')<br/>data, meta_data = ts.get_intraday(symbol='TICKER',interval='1min', outputsize='full')</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Generating word embeddings</h1>
                </header>
            
            <article>
                
<p>For our embedding scheme, we are going to be using the implementation of GloVe from the previous chapter:</p>
<pre>from collections import Counter, defaultdictimport osfrom random import shuffleimport tensorflow as tfimport nltk</pre>
<pre>class GloVeModel(): def __init__(self, embedding_size, window_size, max_vocab_size=100000, min_occurrences=1, scaling_factor=3/4, cooccurrence_cap=100, batch_size=512, learning_rate=0.05): self.embedding_size = embedding_size#First we define the hyper-parameters of our model if isinstance(context_size, tuple): self.left_context, self.right_context = context_size elif isinstance(context_size, int): self.left_context = self.right_context = context_size   self.max_vocab_size = max_vocab_size self.min_occurrences ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Neural Tensor Networks for event embeddings</h1>
                </header>
            
            <article>
                
<p>A <strong>Neural Tensor Network</strong> (<strong>NTN</strong>) is a new form of neural network that works like a standard feed-forward network, only it contains something known as a <strong>tensor layer</strong> instead of standard hidden layers. The network was originally developed as a means of completing knowledge bases by connecting unconnected entities. For instance, if we had the entities Google and YouTube, the network would help connect the two entities so that Google -&gt; Owns -&gt; YouTube. It passes different relationship pairs through the network instead of through a singular vector, and it does this by passing them as a tensor. Each slice of that tensor represents a different variation of a relationship that two entities can have. </p>
<p>In the realm of event-driven trading, we're interested in NTNs because of their ability to relate entities to each other. For us, that means learning the entity event pairs that we created in the first part of this section: </p>
<ol>
<li>Let's start by building our NTN with our core network, which we'll contain in a function called <kbd>NTN</kbd>:</li>
</ol>
<pre style="padding-left: 60px">def NTN(batch_placeholders, corrupt_placeholder, init_word_embeds,     entity_to_wordvec,\<br/> num_entities, num_relations, slice_size, batch_size, is_eval, label_placeholders):<br/>     d = 100 <br/>     k = slice_size<br/>     ten_k = tf.constant([k])<br/>     num_words = len(init_word_embeds)<br/>     E = tf.Variable(init_word_embeds) <br/>     W = [tf.Variable(tf.truncated_normal([d,d,k])) for r in range(num_relations)]<br/>     V = [tf.Variable(tf.zeros([k, 2*d])) for r in range(num_relations)]<br/>     b = [tf.Variable(tf.zeros([k, 1])) for r in range(num_relations)]<br/>     U = [tf.Variable(tf.ones([1, k])) for r in range(num_relations)]<br/><br/>     ent2word = [tf.constant(entity_i)-1 for entity_i in entity_to_wordvec]<br/>     entEmbed = tf.pack([tf.reduce_mean(tf.gather(E, entword), 0) for entword in ent2word])</pre>
<ol start="2">
<li>Still within the <kbd>NTN</kbd> function, we'll loop over our embeddings and start to generate relationship embeddings from them:</li>
</ol>
<pre style="padding-left: 60px">predictions = list()<br/>for r in range(num_relations):<br/>     e1, e2, e3 = tf.split(1, 3, tf.cast(batch_placeholders[r], tf.int32)) #TODO: should the split dimension be 0 or 1?<br/>     e1v = tf.transpose(tf.squeeze(tf.gather(entEmbed, e1, name='e1v'+str(r)),[1]))<br/>     e2v = tf.transpose(tf.squeeze(tf.gather(entEmbed, e2, name='e2v'+str(r)),[1]))<br/>     e3v = tf.transpose(tf.squeeze(tf.gather(entEmbed, e3, name='e3v'+str(r)),[1]))<br/>     e1v_pos = e1v<br/>     e2v_pos = e2v<br/>     e1v_neg = e1v<br/>     e2v_neg = e3v<br/>     num_rel_r = tf.expand_dims(tf.shape(e1v_pos)[1], 0)<br/>     preactivation_pos = list()<br/>     preactivation_neg = list()</pre>
<ol start="3">
<li>Lastly, we'll run the relationship through a nonlinearity and output them:</li>
</ol>
<pre style="padding-left: 60px">for slice in range(k):<br/>     preactivation_pos.append(tf.reduce_sum(e1v_pos*tf.matmul(W[r][:,:,slice], e2v_pos), 0))<br/>     preactivation_neg.append(tf.reduce_sum(e1v_neg*tf.matmul( W[r][:,:,slice], e2v_neg), 0))<br/><br/>preactivation_pos = tf.pack(preactivation_pos)<br/>preactivation_neg = tf.pack(preactivation_neg)<br/><br/>temp2_pos = tf.matmul(V[r], tf.concat(0, [e1v_pos, e2v_pos]))<br/>temp2_neg = tf.matmul(V[r], tf.concat(0, [e1v_neg, e2v_neg]))<br/><br/>preactivation_pos = preactivation_pos+temp2_pos+b[r]<br/>preactivation_neg = preactivation_neg+temp2_neg+b[r]<br/><br/>activation_pos = tf.tanh(preactivation_pos)<br/>activation_neg = tf.tanh(preactivation_neg)<br/><br/>score_pos = tf.reshape(tf.matmul(U[r], activation_pos), num_rel_r)<br/>score_neg = tf.reshape(tf.matmul(U[r], activation_neg), num_rel_r)<br/>if not is_eval:<br/>    predictions.append(tf.pack([score_pos, score_neg]))<br/>else:<br/>    predictions.append(tf.pack([score_pos,             tf.reshape(label_placeholders[r], num_rel_r)]))</pre>
<ol start="4">
<li>Lastly, let's return all of our relationships that are embedding <kbd>predictions</kbd>: </li>
</ol>
<pre style="padding-left: 60px">predictions = tf.concat(1, predictions)<br/><br/>return predictions</pre>
<ol start="5">
<li>Next, let's define our <kbd>loss</kbd> function for the network. We'll manually build out our <kbd>loss</kbd> function from TensorFlow's native operations:</li>
</ol>
<pre style="padding-left: 60px">def loss(predictions, regularization):<br/>     temp1 = tf.maximum(tf.sub(predictions[1, :], predictions[0, :]) + 1, 0)<br/>     temp1 = tf.reduce_sum(temp1)<br/>     temp2 = tf.sqrt(sum([tf.reduce_sum(tf.square(var)) for var in     tf.trainable_variables()]))<br/>     temp = temp1 + (regularization * temp2)<br/>     return temp</pre>
<ol start="6">
<li>We'll define a training algorithm that simply returns the minimized <kbd>loss</kbd> function utilizing TensorFlow's built-in functions: </li>
</ol>
<pre style="padding-left: 60px">def training(loss, learningRate):<br/>    return tf.train.AdagradOptimizer(learningRate).minimize(loss)</pre>
<ol start="7">
<li>Finally, we'll create a short function to evaluate the performance of the network: </li>
</ol>
<pre style="padding-left: 60px">def eval(predictions):<br/>     print("predictions "+str(predictions.get_shape()))<br/>     inference, labels = tf.split(0, 2, predictions)<br/>     return inference, labels</pre>
<p>Next, we'll finish up our model by predicting price movements with a CNN.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Predicting events with a convolutional neural network</h1>
                </header>
            
            <article>
                
<p>Now that we have our embedding structure, it's time to predict off of that structure with a CNN. When you typically think of a CNN, and the work that we have completed on them, you're probably thinking of computer vision tasks such as recognizing an object in an image. Although this is what they were designed for, CNNs can also be great at detecting features in text.</p>
<p>When we use CNNs in NLP, we replace the standard input of pixels with word embeddings. While in typical computer vision tasks you utilize the CNNs filters over small patches of the image, for NLP tasks, we use the same sliding window over the rows of a matrix of embeddings. The width of the sliding window, therefore, becomes ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep learning in asset management</h1>
                </header>
            
            <article>
                
<p>In financial services, a portfolio is a range of investments that are held by a person or organization. To achieve the best return possible (as anyone would want to!), portfolios are optimized by deciding how much capital should be invested into certain financial assets. In portfolio optimization theory, the objective is to have an allocation of assets that minimize risk and maximize reward. We would therefore need to create an algorithm that predicts the expected risks and rewards for each asset so that we may find the best optimization. <span>Traditionally, this work is done by a financial advisor, however, AI has been shown to outperform many traditional advisor-built portfolios.</span></p>
<p>Lately, there have been several attempts to develop deep learning models for asset allocations. Giving credence to the fact that many of these techniques are not published publicly, we are going to take a look at some fundamental methods that we as AI scientists may use for accomplishing this task.</p>
<p>Our goal will be to train a model on an index of stocks, and see if we can outperform that index by at least 1%. We are going to effectively build an autoencoder to encode latent market information, and then use the decoder to construct an optimal portfolio. As we are dealing with series information, we'll use an RNN for both our encoder and decoder. Once we have an autoencoder trained on the data, we'll use it as the input for a simple feed-forward network that will predict our optimal portfolio allocations.</p>
<p>Let's walk through how we would do this in TensorFlow.</p>
<ol>
<li>As usual, let's start with our imports:</li>
</ol>
<pre style="padding-left: 60px"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np<br/></span>import tensorflow as tf from tensorflow.contrib.rnn import LSTMCell</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li> Let's load up our stock data:</li>
</ol>
<pre style="padding-left: 60px"><span class="n">ibb</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">defaultdict</span><span class="p">)</span>
<span class="n">ibb_full</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'data/ibb.csv'</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">'float32'</span><span class="p">)</span>

<span class="n">ibb_lp</span> <span class="o">=</span> <span class="n">ibb_full</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> 
<span class="n">ibb</span><span class="p">[</span><span class="s1">'calibrate'</span><span class="p">][</span><span class="s1">'lp'</span><span class="p">]</span> <span class="o">=</span> <span class="n">ibb_lp</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">104</span><span class="p">]</span>
<span class="n">ibb</span><span class="p">[</span><span class="s1">'validate'</span><span class="p">][</span><span class="s1">'lp'</span><span class="p">]</span> <span class="o">=</span> <span class="n">ibb_lp</span><span class="p">[</span><span class="mi">104</span><span class="p">:]</span>

<span class="n">ibb_net</span> <span class="o">=</span> <span class="n">ibb_full</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> 
<span class="n">ibb</span><span class="p">[</span><span class="s1">'calibrate'</span><span class="p">][</span><span class="s1">'net'</span><span class="p">]</span> <span class="o">=</span> <span class="n">ibb_net</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">104</span><span class="p">]</span>
<span class="n">ibb</span><span class="p">[</span><span class="s1">'validate'</span><span class="p">][</span><span class="s1">'net'</span><span class="p">]</span> <span class="o">=</span> <span class="n">ibb_net</span><span class="p">[</span><span class="mi">104</span><span class="p">:]</span>

<span class="n">ibb_percentage</span> <span class="o">=</span> <span class="n">ibb_full</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span> 
<span class="n">ibb</span><span class="p">[</span><span class="s1">'calibrate'</span><span class="p">][</span><span class="s1">'percentage'</span><span class="p">]</span> <span class="o">=</span> <span class="n">ibb_percentage</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">104</span><span class="p">]</span>
<span class="n">ibb</span><span class="p">[</span><span class="s1">'validate'</span><span class="p">][</span><span class="s1">'percentage'</span><span class="p">]</span> <span class="o">=</span> <span class="n">ibb_percentage</span><span class="p">[</span><span class="mi">104</span><span class="p">:]</span></pre>
<ol start="3">
<li>Let's begin our modeling process by creating our <kbd>AutoEncoder</kbd>, which we will contain in an <kbd>AutoEncoder</kbd> class. We'll start by initializing the primary network variables, like we did previously:</li>
</ol>
<pre style="padding-left: 60px">class AutoEncoder():<br/>    ''' AutoEncoder for Data Drive Portfolio Allocation '''<br/>    def __init__(self, config):<br/>        """First, let's set up our hyperparameters"""<br/>        num_layers = tf.placeholder('int')<br/>        hidden_size = tf.placeholder('int')<br/>        max_grad_norm = tf.placeholder('int')<br/>        batch_size = tf.placeholder('int')<br/>        crd = tf.placeholder('int')<br/>        num_l = tf.placeholder('int')<br/>        learning_rate = tf.placeholder('float')<br/>        self.batch_size = batch_size<br/>        <br/>        ## sl will represent the length of an input sequence, which we would like to eb dynamic based on the data <br/>        sl = tf.placeholder("int")<br/>        self.sl = sl</pre>
<ol start="4">
<li>Next, we'll create the <kbd>placeholders</kbd> for the input data, <em>x:</em></li>
</ol>
<pre style="padding-left: 60px">self.x = tf.placeholder("float", shape=[batch_size, sl], name='Input_data')<br/>self.x_exp = tf.expand_dims(self.x, 1)<br/>self.keep_prob = tf.placeholder("float")</pre>
<ol start="5">
<li class="mce-root">Next, let's create our encoder. We'll create a series of LSTM cells to encode the series data, but we will do it in a way we haven't seen yet: by using a handy function from TensorFlow called <kbd>MultiRNNCell</kbd>. This function acts as a larger placeholder of RNNs that we can iterate inside of so that we may dynamically create the amount of layers based on whatever we decide the <kbd>num_layers</kbd> parameter is:</li>
</ol>
<pre style="padding-left: 60px">## Create the Encoder as a TensorFlow Scope<br/>with tf.variable_scope("Encoder") as scope:<br/>     ## For the encoder, we will use an LSTM cell with Dropout<br/>     EncoderCell = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])<br/>     EncoderCell = tf.contrib.rnn.DropoutWrapper(EncoderCell, output_keep_prob=self.keep_prob)<br/><br/>     ## Set the initial hidden state of the encoder<br/>     EncInitialState = EncoderCell.zero_state(batch_size, tf.float32)<br/><br/>     ## Weights Factor<br/>     W_mu = tf.get_variable('W_mu', [hidden_size, num_l])<br/><br/>     ## Outputs of the Encoder Layer<br/>     outputs_enc, _ = tf.contrib.rnn.static_rnn(cell_enc,<br/>     inputs=tf.unstack(self.x_exp, axis=2),<br/>     initial_state=initial_state_enc)<br/>     cell_output = outputs_enc[-1]<br/><br/>     ## Bias Factor<br/>     b_mu = tf.get_variable('b_mu', [num_l])<br/> <br/>     ## Mean of the latent space variables<br/>     self.z_mu = tf.nn.xw_plus_b(cell_output, W_mu, b_mu, name='z_mu') <br/><br/>     lat_mean, lat_var = tf.nn.moments(self.z_mu, axes=[1])<br/>     self.loss_lat_batch = tf.reduce_mean(tf.square(lat_mean) + lat_var - tf.log(lat_var) - 1)</pre>
<ol start="6">
<li>Next, we'll create a layer to handle the hidden states that are generated by the encoder: </li>
</ol>
<pre style="padding-left: 60px">## Layer to Generate the Initial Hidden State from the Encoder<br/> with tf.name_scope("Initial_State") as scope:<br/> ## Weights Parameter State<br/> W_state = tf.get_variable('W_state', [num_l, hidden_size])<br/><br/> ## Bias Paramter State<br/> b_state = tf.get_variable('b_state', [hidden_size])<br/> <br/> ## Hidden State<br/> z_state = tf.nn.xw_plus_b(self.z_mu, W_state, b_state, name='hidden_state')</pre>
<ol start="7">
<li>We can then create the <kbd>decoder</kbd> layer in the same fashion that we did with the encoder layer: </li>
</ol>
<pre style="padding-left: 60px">## Decoder Layer <br/> with tf.variable_scope("Decoder") as scope:<br/> <br/>     DecoderCell = tf.contrib.rnn.MultiRNNCell([LSTMCell(hidden_size) for _ in range(num_layers)])<br/><br/>     ## Set an initial state for the decoder layer<br/>     DecState = tuple([(z_state, z_state)] * num_layers)<br/>     dec_inputs = [tf.zeros([batch_size, 1])] * sl<br/> <br/>     ## Run the decoder layer<br/>     outputs_dec, _ = tf.contrib.rnn.static_rnn(cell_dec, inputs=dec_inputs, initial_state=DecState)</pre>
<ol start="8">
<li>Lastly, we'll create the output layer for the network: </li>
</ol>
<pre style="padding-left: 60px">## Output Layer<br/> with tf.name_scope("Output") as scope:<br/>     params_o = 2 * crd <br/>     W_o = tf.get_variable('W_o', [hidden_size, params_o])<br/>     b_o = tf.get_variable('b_o', [params_o])<br/>     outputs = tf.concat(outputs_dec, axis=0) <br/>     h_out = tf.nn.xw_plus_b(outputs, W_o, b_o)<br/>     h_mu, h_sigma_log = tf.unstack(tf.reshape(h_out, [sl, batch_size, params_o]), axis=2)<br/>     h_sigma = tf.exp(h_sigma_log)<br/>     dist = tf.contrib.distributions.Normal(h_mu, h_sigma)<br/>     px = dist.log_prob(tf.transpose(self.x))<br/> loss_seq = -px<br/> self.loss_seq = tf.reduce_mean(loss_seq)</pre>
<ol start="9">
<li>Now that we have the actual model constructed, we can go ahead and set up the training process. We'll use exponential decay for the learning rate, which helps stabilize the training process by slowly decreasing the value of the learning rate:</li>
</ol>
<pre style="padding-left: 60px">## Train the AutoEncoder<br/> with tf.name_scope("Training") as scope:<br/> <br/>     ## Global Step Function for Training<br/>     global_step = tf.Variable(0, trainable=False)<br/> <br/>     ## Exponential Decay for the larning rate<br/>     lr = tf.train.exponential_decay(learning_rate, global_step, 1000, 0.1, staircase=False)<br/><br/>     ## Loss Function for the Network<br/>     self.loss = self.loss_seq + self.loss_lat_batch<br/> <br/>     ## Utilize gradient clipping to prevent exploding gradients<br/>     grads = tf.gradients(self.loss, tvars)<br/> grads, _ = tf.clip_by_global_norm(grads, max_grad_norm)<br/>     self.numel = tf.constant([[0]])<br/><br/>     ## Lastly, apply the optimization process<br/>     optimizer = tf.train.AdamOptimizer(lr)<br/>     gradients = zip(grads, tvars)<br/>     self.train_step = optimizer.apply_gradients(gradients, global_step=global_step)<br/>     self.numel = tf.constant([[0]])<br/>  </pre>
<ol start="10">
<li>Now, we can run the training process:</li>
</ol>
<pre style="padding-left: 60px">if True:<br/>     sess.run(model.init_op)<br/>     writer = tf.summary.FileWriter(LOG_DIR, sess.graph) # writer for Tensorboard<br/><br/> step = 0 # Step is a counter for filling the numpy array perf_collect<br/> for i in range(max_iterations):<br/>     batch_ind = np.random.choice(N, batch_size, replace=False)<br/>     result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.train_step],<br/> feed_dict={model.x: X_train[batch_ind], model.keep_prob: dropout})<br/><br/> if i % plot_every == 0:<br/>     perf_collect[0, step] = loss_train = result[0]<br/>     loss_train_seq, lost_train_lat = result[1], result[2]<br/><br/> batch_ind_val = np.random.choice(Nval, batch_size, replace=False)<br/><br/> result = sess.run([model.loss, model.loss_seq, model.loss_lat_batch, model.merged],<br/> feed_dict={model.x: X_val[batch_ind_val], model.keep_prob: 1.0})<br/> perf_collect[1, step] = loss_val = result[0]<br/> loss_val_seq, lost_val_lat = result[1], result[2]<br/> summary_str = result[3]<br/> writer.add_summary(summary_str, i)<br/> writer.flush()<br/><br/> print("At %6s / %6s train (%5.3f, %5.3f, %5.3f), val (%5.3f, %5.3f,%5.3f) in order (total, seq, lat)" % (<br/> i, max_iterations, loss_train, loss_train_seq, lost_train_lat, loss_val, loss_val_seq, lost_val_lat))<br/> step += 1<br/>if False:<br/><br/> start = 0<br/> label = [] # The label to save to visualize the latent space<br/> z_run = []<br/><br/> while start + batch_size &lt; Nval:<br/> run_ind = range(start, start + batch_size)<br/> z_mu_fetch = sess.run(model.z_mu, feed_dict={model.x: X_val[run_ind], model.keep_prob: 1.0})<br/> z_run.append(z_mu_fetch)<br/> start += batch_size<br/><br/> z_run = np.concatenate(z_run, axis=0)<br/> label = y_val[:start]<br/><br/> plot_z_run(z_run, label)<br/><br/>saver = tf.train.Saver()<br/>saver.save(sess, os.path.join(LOG_DIR, "model.ckpt"), step)<br/>config = projector.ProjectorConfig()<br/><br/>embedding = config.embeddings.add()<br/>embedding.tensor_name = model.z_mu.name</pre>
<p>After we autoencode our stock index, we'll look at the difference between each different stock and its corresponding autoencoder version. We'll then rank the stocks by how well they have been autoencoded. As the algorithm learns the most important information about each of the stocks, the <span>proximity of a stock to its version that has been run through the autoencoder</span><span> provides a measure for that stock against the entire potential portfolio.</span></p>
<p><span>As there is no benefit in having multiple stocks contributing to the latent information, we will limit the selected stocks to the top ten of these stocks that are close to their autoencoded version:</span></p>
<pre><span class="n">communal_information</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">83</span><span class="p">):</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">((</span><span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">reconstruct</span><span class="p">[:,</span><span class="n">i</span><span class="p">]))</span> <span class="c1"># 2 norm difference</span>
    <span class="n">communal_information</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">diff</span><span class="p">))</span>
 
<span class="nb">print</span><span class="p">(</span><span class="s2">"stock #, 2-norm, stock name"</span><span class="p">)</span>
<span class="n">ranking</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">communal_information</span><span class="p">)</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
<span class="k">for</span> <span class="n">stock_index</span> <span class="ow">in</span> <span class="n">ranking</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">stock_index</span><span class="p">,</span> <span class="n">communal_information</span><span class="p">[</span><span class="n">stock_index</span><span class="p">],</span> <span class="n">stock</span><span class="p">[</span><span class="s1">'calibrate'</span><span class="p">][</span><span class="s1">'net'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">stock_index</span><span class="p">]</span><span class="o">.</span><span class="n">name</span><span class="p">)</span> <span class="c1"># print stock name from lowest different to highest</span></pre>
<p class="mce-root"><span>We can take a look at how the autoencoder is working as follows: </span></p>
<pre><span class="n">which_stock</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">stock_autoencoder</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">reconstruct</span><span class="p">[:,</span> <span class="n">which_stock</span><span class="p">])</span>
<span class="n">stock_autoencoder</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">stock_autoencoder</span> <span class="o">=</span> <span class="n">stock_autoencoder</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>
<span class="n">stock_autoencoder</span> <span class="o">+=</span> <span class="p">(</span><span class="n">stock</span><span class="p">[</span><span class="s1">'calibrate'</span><span class="p">][</span><span class="s1">'lp'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">which_stock</span><span class="p">])</span>

<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">stock</span><span class="p">[</span><span class="s1">'calibrate'</span><span class="p">][</span><span class="s1">'lp'</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">which_stock</span><span class="p">]</span><span class="o">.</span><span class="n">as_matrix</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s1">'01/06/2012'</span><span class="p">,</span> <span class="n">periods</span><span class="o">=</span><span class="mi">104</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s1">'W'</span><span class="p">))</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">'stock original'</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">stock_autoencoder</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s1">'01/06/2012'</span><span class="p">,</span> <span class="n">periods</span> <span class="o">=</span> <span class="mi">104</span><span class="p">,</span><span class="n">freq</span><span class="o">=</span><span class="s1">'W'</span><span class="p">))</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">'stock autoencoded'</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></pre>
<p>While we still have to choose between the available stock, our picking decisions are now based on the out of sample performance of these stocks, making our market autoencoder a novel, data-driven approach.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to apply our deep learning knowledge to the financial services sector. We learned the principles of trading systems, and then designed a trading system of our own in TensorFlow. We then looked at how we can create a different type of trading system, one that utilizes events surrounding a company to predict its stock prices. Lastly, we explored a novel technique for embedding the stock market and utilizing those embeddings to predict price movement. </p>
<p>Financial markets can be tricky to model due to their properties, but the techniques that we have covered in this chapter will give you the basis to build further models. Remember to always backtest your algorithm before deploying it in a real-time environment! ...</p></article></section></div>



  </body></html>