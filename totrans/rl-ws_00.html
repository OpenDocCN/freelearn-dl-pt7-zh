<html><head></head><body>
		<div id="_idContainer005" class="Content">
			<h1 id="_idParaDest-2"><a id="_idTextAnchor001"/>Preface</h1>
		</div>
		<div id="_idContainer006" class="Content">
			<h1 id="_idParaDest-3">About the Book<a id="_idTextAnchor002"/></h1>
			<p>Various intelligent applications such as video games, inventory management software, warehouse robots, and translation tools use <strong class="bold">Reinforcement Learning</strong> (<strong class="bold">RL</strong>) to make decisions and perform actions that maximize the probability of the desired outcome. This book will help you to get to grips with the techniques and the algorithms for implementing RL in your machine learning models. </p>
			<p>Starting with an introduction to RL, you'll be guided through different RL environments and frameworks. You'll learn how to implement your own custom environments and use OpenAI baselines to run RL algorithms. Once you've explored classic RL techniques such as Dynamic Programming, Monte Carlo, and TD Learning, you'll understand when to apply the different deep learning methods in RL and advance to deep Q-learning. The book will even help you understand the different stages of machine-based problem-solving by using DARQN on a popular video game Breakout. Finally, you'll find out when to use a policy-based method to tackle an RL problem. </p>
			<p>By the end of <em class="italic">The Reinforcement Learning Workshop</em>, you'll be equipped with the knowledge and skills needed to solve challenging machine learning problems using reinforcement learning. </p>
			<h2 id="_idParaDest-4"><a id="_idTextAnchor003"/>Audience</h2>
			<p>If you are a data scientist, machine learning enthusiast, or a Python developer who wants to learn basic to advanced deep reinforcement learning algorithms, this workshop is for you. A basic understanding of the Python language is necessary.</p>
			<h2 id="_idParaDest-5"><a id="_idTextAnchor004"/>About the Chapters</h2>
			<p><em class="italic">Chapter 1</em>, <em class="italic">Introduction to Reinforcement Learning</em>, introduces you to RL, which is one of the most exciting fields in machine learning and artificial intelligence.</p>
			<p><em class="italic">Chapter 2</em>, <em class="italic">Markov Decision Processes and Bellman Equations</em>, teaches you about Markov chains, Markov reward processes, and Markov decision processes. You will learn about state values and action values, as well as using the Bellman equation to calculate these quantities.</p>
			<p><em class="italic">Chapter 3</em>, <em class="italic">Deep Learning in Practice with TensorFlow 2</em>, introduces you to TensorFlow and Keras, giving you an overview of their key features and applications and how they work in synergy.</p>
			<p><em class="italic">Chapter 4</em>, <em class="italic">Getting Started with OpenAI and TensorFlow for Reinforcement Learning</em>, sees you working with two popular OpenAI tools, Gym and Universe. You will learn how to formalize the interfaces of these environments, how to interact with them, and how to create a custom environment for a specific problem.</p>
			<p><em class="italic">Chapter 5</em>, <em class="italic">Dynamic Programming</em>, teaches you how to use dynamic programming to solve problems in RL. You will learn about the concepts of policy evaluation, policy iteration, and value iteration, and see how to implement them.</p>
			<p><em class="italic">Chapter 6</em>, <em class="italic">Monte Carlo Methods,</em> teaches you how to implement the various types of Monte Carlo methods, including the "first visit" and "every visit" techniques. You will see how to use these Monte Carlo methods to solve the frozen lake problem.</p>
			<p><em class="italic">Chapter 7</em>, <em class="italic">Temporal Difference Learning</em>, prepares you to implement TD(0), SARSA, and TD(λ) Q-learning algorithms in both stochastic and deterministic environments.</p>
			<p><em class="italic">Chapter 8</em>, <em class="italic">The Multi-Armed Bandit Problem</em>, introduces you to the popular multi-armed bandit problem and shows you some of the most commonly used algorithms to solve the problem.</p>
			<p><em class="italic">Chapter 9</em>, <em class="italic">What Is Deep Q-Learning?</em>, educates you on deep Q-learning and covers some hands-on implementations of advanced variants of deep Q-learning, such as double deep Q-learning, with PyTorch.</p>
			<p><em class="italic">Chapter 10</em>, <em class="italic">Playing an Atari Game with Deep Recurrent Q-Networks</em>, introduces you to <strong class="bold">Deep Recurrent Q-Networks</strong> and its variants. You will get hands-on experience in training RL agents to play an Atari game.</p>
			<p><em class="italic">Chapter 11</em>, <em class="italic">Policy-Based Methods for Reinforcement Learning</em>, teaches you how to implement different policy-based methods of RL, such as policy gradients, deep deterministic policy gradients, trust region policy optimization, and proximal policy optimization.</p>
			<p><em class="italic">Chapter 12</em>, <em class="italic">Evolutionary Strategies for RL</em>, combines evolutionary strategies with traditional machine learning methods, specifically in the selection of neural network hyperparameters. You will also identify the limitations of these evolutionary methods.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The interactive version of <em class="italic">The Reinforcement Learning Workshop</em> contains a bonus chapter, <em class="italic">Recent Advancements</em> and <em class="italic">Next Steps</em>. This chapter teaches you novel methods of implementing reinforcement learning algorithms with an emphasis on areas of further exploration such as one-shot learning and transferable domain priors. You can find the interactive version here: <a href="http://courses.packtpub.com">courses.packtpub.com</a>.</p>
			<h2 id="_idParaDest-6"><a id="_idTextAnchor005"/>Conventions</h2>
			<p>Code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles are shown as follows: "Recall that an algorithm class' implementation needs two specific methods to interact with the bandit API, <strong class="source-inline">decide()</strong> and <strong class="source-inline">update()</strong>, the latter of which is simpler and is implemented."</p>
			<p>Words that you see onscreen (for example, in menus or dialog boxes) also appear in the text like this: "The <strong class="source-inline">DISTRIBUTIONS</strong> tab provides an overview of how the model parameters are distributed across epochs."</p>
			<p>A block of code is set as follows:</p>
			<p class="source-code">class Greedy:</p>
			<p class="source-code">    def __init__(self, n_arms=2):</p>
			<p class="source-code">        self.n_arms = n_arms</p>
			<p class="source-code">        self.reward_history = [[] for _ in range(n_arms)]</p>
			<p>New terms and important words are shown like this: "Its architecture allows users to run it on a wide variety of hardware, from CPUs to <strong class="bold">Tensor Processing Units</strong> (<strong class="bold">TPUs</strong>), including GPUs as well as mobile and embedded platforms."</p>
			<h2 id="_idParaDest-7"><a id="_idTextAnchor006"/>Code Presentation</h2>
			<p>Lines of code that span multiple lines are split using a backslash ( <strong class="source-inline">\</strong> ). When the code is executed, Python will ignore the backslash, and treat the code on the next line as a direct continuation of the current line.</p>
			<p>For example:</p>
			<p class="source-code">history = model.fit(X, y, epochs=100, batch_size=5, verbose=1, \</p>
			<p class="source-code">                    validation_split=0.2, shuffle=False)</p>
			<p>Comments are added into code to help explain specific bits of logic. Single-line comments are denoted using the <strong class="source-inline">#</strong> symbol, as follows:</p>
			<p class="source-code"># Print the sizes of the dataset</p>
			<p class="source-code">print("Number of Examples in the Dataset = ", X.shape[0])</p>
			<p class="source-code">print("Number of Features for each example = ", X.shape[1])</p>
			<p>Multi-line comments are enclosed by triple quotes, as shown below:</p>
			<p class="source-code">"""</p>
			<p class="source-code">Define a seed for the random number generator to ensure the </p>
			<p class="source-code">result will be reproducible</p>
			<p class="source-code">"""</p>
			<p class="source-code">seed = 1</p>
			<p class="source-code">np.random.seed(seed)</p>
			<p class="source-code">random.set_seed(seed)</p>
			<h2 id="_idParaDest-8"><a id="_idTextAnchor007"/>Setting up Your Environment</h2>
			<p>Before we explore the book in detail, we need to set up specific software and tools. In the following section, we shall see how to do that.</p>
			<h2 id="_idParaDest-9"><a id="_idTextAnchor008"/>Installing Anaconda for Jupyter Notebook</h2>
			<p>Jupyter notebooks are available once you install Anaconda on your system. Anaconda can be installed on Windows systems using the steps available at <a href="https://docs.anaconda.com/anaconda/install/windows/">https://docs.anaconda.com/anaconda/install/windows/</a>.</p>
			<p>For other systems, navigate to the respective installation guide from <a href="https://docs.anaconda.com/anaconda/install/">https://docs.anaconda.com/anaconda/install/</a>.</p>
			<h2 id="_idParaDest-10"><a id="_idTextAnchor009"/>Installing a Virtual Environment</h2>
			<p>In general, it is good practice to use separate virtual environments when installing Python modules, to be sure that the dependencies of different projects do not conflict with one another. So, it is recommended that you adopt this approach before executing these instructions.</p>
			<p>Since we are using Anaconda here, it is highly recommended that you use conda-based environment management. Run the following commands in Anaconda Prompt to create an environment and activate it: </p>
			<p class="source-code">conda create --name [insert environment name here]</p>
			<p class="source-code">conda activate [insert environment name here]</p>
			<h2 id="_idParaDest-11"><a id="_idTextAnchor010"/>Installing Gym</h2>
			<p>To install Gym, please make sure you have Python 3.5+ installed on your system. You can simply install Gym using <strong class="source-inline">pip</strong>. Run the code in Anaconda Prompt, as shown in the following code snippet:</p>
			<p class="source-code">pip install gym</p>
			<p>You can also build the Gym installation from source, by cloning the Gym Git repository directly. This type of installation proves useful when modifying Gym or adding environments if required. Use the following code to install Gym from source:</p>
			<p class="source-code">git clone https://github.com/openai/gym</p>
			<p class="source-code">cd gym</p>
			<p class="source-code">pip install -e .</p>
			<p>Run the following code to perform a full installation of Gym. This installation may need you to install other dependencies, which include <strong class="source-inline">cmake</strong> and a recent version of <strong class="source-inline">pip</strong>:</p>
			<p class="source-code">pip install -e .[all]</p>
			<p>In <em class="italic">Chapter 11, Policy-Based Methods for Reinforcement Learning</em>, you will be working in the <strong class="source-inline">Box2D</strong> environment available in Gym. You can install the <strong class="source-inline">Box2D</strong> environment by using the following command:</p>
			<p class="source-code">pip install gym "gym[box2d]"</p>
			<h2 id="_idParaDest-12"><a id="_idTextAnchor011"/>Installing TensorFlow 2</h2>
			<p>To install TensorFlow 2, run the following command in Anaconda Prompt:</p>
			<p class="source-code">pip install tensorflow</p>
			<p>If you are using a GPU, you can use the following command:</p>
			<p class="source-code">pip install tensorflow-gpu</p>
			<h2 id="_idParaDest-13"><a id="_idTextAnchor012"/>Installing PyTorch</h2>
			<p>PyTorch can be installed on Windows using the steps available at <a href="https://pytorch.org/">https://pytorch.org/</a>.</p>
			<p>In the case of non-availability of a GPU on your system, you can install the CPU version of PyTorch by running the following code in Anaconda Prompt:</p>
			<p class="source-code">conda install pytorch-cpu torchvision-cpu -c pytorch</p>
			<h2 id="_idParaDest-14"><a id="_idTextAnchor013"/>Installing OpenAI Baselines</h2>
			<p>OpenAI Baselines can be installed using the instructions at <a href="https://github.com/openai/baselines">https://github.com/openai/baselines</a>.</p>
			<p>Download the OpenAI Baselines repository, check out the TensorFlow 2 branch, and install it as follows:</p>
			<p class="source-code">git clone <a href="https://github.com/openai/baselines.git ">https://github.com/openai/baselines.git</a></p>
			<p class="source-code">cd baselines</p>
			<p class="source-code">git checkout tf2</p>
			<p class="source-code">pip install -e .</p>
			<p>We use OpenAI Baselines in <em class="italic">Chapter 1, Introduction to Reinforcement Learning</em>, and <em class="italic">Chapter 4, Getting Started with OpenAI and TensorFlow</em> for Reinforcement Learning. As OpenAI Baselines uses a version of Gym that is not the latest version, <strong class="source-inline">0.14</strong>, you might get an error as follows:</p>
			<p class="source-code">AttributeError: 'EnvSpec' object has no attribute '_entry_point'</p>
			<p>The solution to this bug is to change the two <strong class="source-inline">env.entry_point</strong> attributes in <strong class="source-inline">baselines/run.py</strong> back to <strong class="source-inline">env._entry_point</strong>. </p>
			<p>The detailed solution is available at <a href="https://github.com/openai/baselines/issues/977#issuecomment-518569750">https://github.com/openai/baselines/issues/977#issuecomment-518569750</a>.</p>
			<p>Alternatively, you can also use the following command to upgrade the Gym installation in that environment:</p>
			<p class="source-code">pip install --upgrade gym</p>
			<h2 id="_idParaDest-15"><a id="_idTextAnchor014"/>Installing Pillow</h2>
			<p>Use the following command in Anaconda Prompt to install Pillow:</p>
			<p class="source-code">conda install -c anaconda pillow</p>
			<p>Alternatively, you can also run the following command using <strong class="source-inline">pip</strong>:</p>
			<p class="source-code">pip install pillow</p>
			<p>You can read more about Pillow at <a href="https://pypi.org/project/Pillow/2.2.1/">https://pypi.org/project/Pillow/2.2.1/</a>.</p>
			<h2 id="_idParaDest-16"><a id="_idTextAnchor015"/>Installing Torch</h2>
			<p>Use the following command to install <strong class="source-inline">torch</strong> using <strong class="source-inline">pip</strong>:</p>
			<p class="source-code">pip install torch==0.4.1 -f <a href="https://download.pytorch.org/whl/torch_stable.html ">https://download.pytorch.org/whl/torch_stable.html</a></p>
			<p>Note that you will be using version <strong class="source-inline">0.4.1</strong> of <strong class="source-inline">torch</strong> only in <em class="italic">Chapter 11, Policy-Based Methods for Reinforcement Learning</em>. You can revert to the updated version of PyTorch by using the command under the <em class="italic">Installing PyTorch</em> section for the other chapters.</p>
			<h2 id="_idParaDest-17"><a id="_idTextAnchor016"/>Installing Other Libraries</h2>
			<p><strong class="source-inline">pip</strong> comes pre-installed with Anaconda. Once Anaconda is installed on your machine, all the required libraries can be installed using <strong class="source-inline">pip</strong>, for example, <strong class="source-inline">pip install numpy</strong>. Alternatively, you can install all the required libraries using <strong class="source-inline">pip install –r requirements.txt</strong>. You can find the <strong class="source-inline">requirements.txt</strong> file at <a href="https://packt.live/311jlIu">https://packt.live/311jlIu</a>.</p>
			<p>The exercises and activities will be executed in Jupyter Notebooks. Jupyter is a Python library and can be installed in the same way as the other Python libraries – that is, with <strong class="source-inline">pip install jupyter</strong>, but fortunately, it comes pre-installed with Anaconda. To open a notebook, simply run the command <strong class="source-inline">jupyter notebook</strong> in the Terminal or Command Prompt.</p>
			<h2 id="_idParaDest-18"><a id="_idTextAnchor017"/>Accessing the Code Files</h2>
			<p>You can find the complete code files of this book at <a href="https://packt.live/2V1MwHi">https://packt.live/2V1MwHi</a>. </p>
			<p>We've tried to support interactive versions of all activities and exercises, but we recommend a local installation as well for instances where this support isn't available.</p>
			<p>If you have any issues or questions about installation, please email us at <strong class="source-inline">workshops@packt.com</strong>.</p>
		</div>
		<div>
			<div id="_idContainer007" class="Content">
			</div>
		</div>
	</body></html>