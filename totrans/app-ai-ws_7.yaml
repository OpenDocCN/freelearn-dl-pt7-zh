- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Introduction to Artificial Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 1.01: Generating All Possible Sequences of Steps in a Tic-Tac-Toe
    Game'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reuse the function codes of *Steps 2–9* from the previous, *Exercise 1.02*,
    *Creating an AI with Random Behavior for the Tic-Tac-Toe Game*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a function that maps the `all_moves_from_board_list` function to each
    element of a list of boards. This way, we will have all of the nodes of a decision
    tree in each depth:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code snippet, we have defined the `all_moves_from_board` function,
    which will enumerate all the possible moves from the board and add the move to
    a list called `move_list`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called board that contains the `EMPTY_SIGN * 9` decision
    tree and calls the `all_moves_from_board_list` function with the board and `AI_SIGN`.
    Save its output in a variable called `all_moves` and print its content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `filter_wins` function that takes the ended games out from the list
    of moves and appends them in an array containing the board states won by the AI
    player and the opponent player:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code snippet, we have defined a `filter_wins` function, which
    will add the winning state of the board for each player to a list.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `count_possibilities` function, which prints and returns the number
    of decision tree leaves that ended with a draw, that were won by the first player,
    and that were won by the second player, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have up to `9` steps in each state. In the 0th, 2nd, 4th, 6th, and 8th iterations,
    the AI player moves. In all the other iterations, the opponent moves. We create
    all possible moves in all steps and take out the completed games from the move list.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Execute the number of possibilities to experience the combinatorial explosion
    and save the results in four variables called `first_player`, `second_player`,
    `draw`, and `total`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the tree of the board states consists of a total of `266073`
    leaves. The `count_possibilities` function essentially implements a BFS algorithm
    to traverse all the possible states of the game. Notice that we count these states
    multiple times because placing an `X` in the top-right corner in *Step 1* and
    placing an `X` in the top-left corner in *Step 3* leads to similar possible states
    as starting with the top-left corner and then placing an `X` in the top-right
    corner. If we implemented the detection of duplicate states, we would have to
    check fewer nodes. However, at this stage, due to the limited depth of the game,
    we will omit this step.
  prefs: []
  type: TYPE_NORMAL
- en: A decision tree, however, is identical to the data structure examined by `count_possibilities`.
    In a decision tree, we explore the utility of each move by investigating all possible
    future steps up to a certain extent. In our example, we could calculate the utility
    of the initial moves by observing the number of wins and losses after fixing the
    first few moves.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The root of the tree is the initial state. An internal state of the tree is
    a state in which a game has not been ended and moves are still possible. A leaf
    of the tree contains a state where a game has ended.
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3doxPog](https://packt.live/3doxPog).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3dpnuIz](https://packt.live/3dpnuIz).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 1.02: Teaching the Agent to Realize Situations When It Defends Against
    Losses'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reuse all the code from *Steps 2–6* from the previous, *Exercise 1.03*, *Teaching
    the Agent to Win*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a function called `player_can_win` that takes all the moves from the
    board using the `all_moves_from_board` function and iterates over them using the
    `next_move` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each iteration, it checks whether the game can be won by the player.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extend the AI move so that it prefers making safe moves. A move is safe if
    the opponent cannot win the game in the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code snippet, we have defined the `ai_move` function, which
    tells the AI how to move by looking at the list of all the possibilities and choosing
    one where the player cannot win in the next move. If you test our new application,
    you will find that the AI has made the correct move.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, place this logic in the state space generator and check how well the computer
    player is doing by generating all the possible games:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code snippet, we have defined a function that generates all
    possible moves. As soon as we find the next move that can make the player win,
    we return a move to counter it. We do not care whether the player has multiple
    options to win the game in one move – we just return the first possibility. If
    the AI cannot stop the player from winning, we return all possible moves.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's see what this means in terms of counting all of the possibilities at each
    step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Count the options that are possible:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are doing better than before. We not only got rid of almost 2/3 of possible
    games again, but, most of the time, the AI player either wins or settles for a
    draw.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2B0G9xf](https://packt.live/2B0G9xf).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2V7qLpO](https://packt.live/2V7qLpO).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 1.03: Fixing the First and Second Moves of the AI to Make It Invincible'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reuse the code from *Steps 2–4* of the previous, *Activity 1.02*, *Teaching
    the Agent to Realize Situations When It Defends Against Losses*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, count the number of empty fields on the board and make a hardcoded move
    in case there are 9 or 7 empty fields. You can experiment with different hardcoded
    moves. We found that occupying any corner, and then occupying the opposite corner,
    leads to no loss. If the opponent occupies the opposite corner, making a move
    in the middle results in no losses:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, verify the state space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After fixing the first two steps, we only need to deal with 8 possibilities
    instead of 504\. We also guided the AI into a state where the hardcoded rules
    were sufficient enough for it to never lose a game. Fixing the steps is not important
    because we would give the AI hardcoded steps to start with, but it is important
    because it is a tool that is used to evaluate and compare each step. After fixing
    the first two steps, we only need to deal with 8 possibilities instead of 504\.
    We also guided the AI into a state, where the hardcoded rules were sufficient
    for never losing a game. As you can see, the AI is now nearly invincible and will
    only win or make a draw.
  prefs: []
  type: TYPE_NORMAL
- en: The best that a player can hope to get against this AI is a draw.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2YnUcpA](https://packt.live/2YnUcpA).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/318TBtq](https://packt.live/318TBtq).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 1.04: Connect Four'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's set up the `TwoPlayersGame` framework by writing the `init` method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the board as a one-dimensional list, like the tic-tac-toe example. We
    could use a two-dimensional list, too, but modeling will not get much easier or
    harder. Beyond making initialization like we did in the tic-tac-toe game, we will
    work a bit further ahead. We will generate all of the possible winning combinations
    in the game and save them for future use, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, handle the `possible_moves` function, which is a simple enumeration.
    Notice that we are using column indices from `1` to `7` in the move names because
    it is more convenient to start a column indexing with `1` in the human player
    interface than with zero. For each column, we check whether there is an unoccupied
    field. If there is one, we will make the column a possible move:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Making a move is like the `possible_moves` function. We check the column of
    the move and find the first empty cell starting from the bottom. Once we find
    it, we occupy it. You can also read the implementation of the both the `make_move`
    function: `unmake_move`. In the `unmake_move` function, we check the column from
    top to down, and we remove the move at the first non-empty cell. Notice that we
    rely on the internal representation of `easyAI` so that it does not undo moves
    that it hasn''t made. Otherwise, this function would remove a token of the other
    player without checking whose token was removed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since we already have the tuples that we must check, we can mostly reuse the
    `lose` function from the tic-tac-toe example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our final task is to implement the `show` method, which prints the board. We
    will reuse the tic-tac-toe implementation and just change the `show` and `scoring` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that all the functions are complete, you can try out the example. Feel free
    to play a round or two against your opponent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.30: Expected output for the Connect Four game'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_01_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.30: Expected output for the Connect Four game'
  prefs: []
  type: TYPE_NORMAL
- en: By completing this activity, you have seen that the opponent is not perfect,
    but that it plays reasonably well. If you have a strong computer, you can increase
    the parameter of the `Negamax` algorithm. We encourage you to come up with a better
    heuristic.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3esk2hI](https://packt.live/3esk2hI).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3dnkfS5](https://packt.live/3dnkfS5).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. An Introduction to Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 2.01: Boston House Price Prediction with Polynomial Regression of
    Degrees 1, 2, and 3 on Multiple Variables'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the required packages and load the Boston House Prices data from `sklearn`
    into a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `df` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.28: Output displaying the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.28: Output displaying the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Earlier in this chapter, you learned that most of the required packages to perform
    linear regression come from `sklearn`. We need to import the `preprocessing` module
    to scale the data, the `linear_model` module to train linear regression, the `PolynomialFeatures`
    module to transform the inputs for the polynomial regression, and the `model_selection`
    module to evaluate the performance of each model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Prepare the dataset for prediction by converting the label and features into
    NumPy arrays and scaling the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `features` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.29: Labels and features converted to NumPy arrays'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, our features have been properly scaled.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we don't have any missing values and we are not trying to predict a future
    value as we did in *Exercise 2.03*, *Preparing the Quandl Data for Prediction*,
    we can directly convert the label (`'MEDV'`) and features into NumPy arrays. Then,
    we can scale the arrays of features using the `preprocessing.scale()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create three different set of features by transforming the scaled features
    into a suitable format for each of the polynomial regressions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `poly_1_scaled_features` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our `scaled_features` variable has been properly transformed for the polynomial
    regression of degree `1`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output for `poly_2_scaled_features` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.31: Output showing poly_2_scaled_features'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our `scaled_features` variable has been properly transformed for the polynomial
    regression of degree `3`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We had to transform the scaled features in three different ways as each degree
    of polynomial regression required a different input transformation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the data into a training set and a testing set with `random state = 8`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we have three different sets of scaled transformed features but the same
    set of labels, we had to perform three different splits. By using the same set
    of labels and `random_state` in each splitting, we ensure that we obtain the same
    `poly_label_train` and `poly_label_test` for every split.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform a polynomial regression of degree 1 and evaluate whether the model
    is overfitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `model_1_score_train` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `model_1_score_test` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To estimate whether a model is overfitting or not, we need to compare the scores
    of the model applied to the training set and testing set. If the score for the
    training set is much higher than the test set, we are overfitting. This is the
    case here where the polynomial regression of degree 1 achieved a score of `0.74`
    for the training set compared to `0.68` for the testing set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform a polynomial regression of degree 2 and evaluate whether the model
    is overfitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `model_2_score_train` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `model_2_score_test` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Like with the polynomial regression of degree 1, our polynomial regression of
    degree 2 is overfitting even more than degree 1, but has managed to achieve better
    results at the end.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform a polynomial regression of degree 3 and evaluate whether the model
    is overfitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `model_3_score_train` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `model_3_score_test` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These results are very interesting because the polynomial regression of degree
    3 managed to achieve a near-perfect score with `0.99` (1 is the maximum). This
    is a warning sign that our model is overfitting too much. We have the confirmation
    of this warning when the model is applied to the testing set and achieves a very
    low negative score of `-8430`. As a reminder, a score of 0 can be achieved by
    using the mean of the data as a prediction. This means that our third model managed
    to make worse predictions than just using the mean.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compare the predictions of the 3 models against the label on the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `df_prediction` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.32: Output showing the expected predicted values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.32: Output showing the expected predicted values'
  prefs: []
  type: TYPE_NORMAL
- en: After applying the `predict` function for each model on their respective testing
    set, in order to get the predicted values, we convert them into a single `df_prediction`
    DataFrame with the label values. Increasing the number of degrees in polynomial
    regressions does not necessarily mean that the model will perform better compared
    to one with a lower degree. In fact, increasing the degree will lead to more overfitting
    on the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3eD8gAY](https://packt.live/3eD8gAY).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3etadjp](https://packt.live/3etadjp).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we learned how to perform polynomial regressions of degrees
    1 to 3 with multiple variables on the Boston House Price dataset and saw how increasing
    the degrees led to overfitted models.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. An Introduction to Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 3.01: Increasing the Accuracy of Credit Scoring'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file and execute all the steps from the previous
    exercise, *Exercise 3.04*, *K-Nearest Neighbors Classification in Scikit-Learn*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `neighbors` from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function called `fit_knn` that takes the following parameters: `k`,
    `p`, `features_train`, `label_train`, `features_test`, and `label_test`. This
    function will fit `KNeighborsClassifier` with the training set and print the accuracy
    score for the training and testing sets, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `fit_knn()` function with `k=5` and `p=2`, save the results in `2`
    variables, and print them. These variables are `acc_train_1` and `acc_test_1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With `k=5` and `p=2`, KNN achieved a good accuracy score close to `0.78`. But
    the score is quite different from the training and testing sets, which means the
    model is overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `k=10` and `p=2`, save the results in `2`
    variables, and print them. These variables are `acc_train_2` and `acc_test_2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Increasing the number of neighbors to 10 has decreased the accuracy score of
    the training set, but now it is very close to the testing set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `k=15` and `p=2`, save the results in `2`
    variables, and print them. These variables are `acc_train_3` and `acc_test_3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With `k=15` and `p=2`, the difference between the training and testing sets
    has  increased.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `k=25` and `p=2`, save the results in `2`
    variables, and print them. These variables are `acc_train_4` and `acc_test_4`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Increasing the number of neighbors to `25` has a significant impact on the training
    set. However, the model is still overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `k=50` and `p=2`, save the results in `2`
    variables, and print them. These variables are `acc_train_5` and `acc_test_5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Bringing the number of neighbors to `50` neither improved the model's performance
    or the overfitting issue.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `k=5` and `p=1`, save the results in `2`
    variables, and print them. These variables are `acc_train_6` and `acc_test_6`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Changing to the Manhattan distance has helped increase the accuracy of the training
    set, but the model is still overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `k=10` and `p=1`, save the results in `2`
    variables, and print them. These variables are `acc_train_7` and `acc_test_7`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With `k=10`, the accuracy score for the training and testing sets are quite
    close to each other: around `0.78`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `k=15` and `p=1`, save the results in `2`
    variables, and print them. These variables are `acc_train_8` and `acc_test_8`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Bumping `k` to `15`, the model achieved a better accuracy score and is not overfitting
    very much.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `k=25` and `p=1`, save the results in `2`
    variables, and print them. These variables are `acc_train_9` and `acc_test_9`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With `k=25`, the difference between the training and testing sets' accuracy
    is increasing, so the model is overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `k=50` and `p=1`, save the results in `2`
    variables, and print them. These variables are `acc_train_10` and `acc_test_10`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With `k=50`, the model's performance on the training set dropped significantly
    and the model is definitely overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this activity, we tried multiple combinations of hyperparameters for `n_neighbors`
    and `p`. The best one we found was for `n_neighbors=10` and `p=2`. With these
    hyperparameters, the model is not overfitting much and it achieved an accuracy
    score of around `78%` for both the training and testing sets.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2V5TOtG](https://packt.live/2V5TOtG).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Bx0yd8](https://packt.live/2Bx0yd8).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.02: Support Vector Machine Optimization in scikit-learn'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file and execute all the steps mentioned in the
    previous, *Exercise 3.04*, *K-Nearest Neighbor Classification in scikit-learn*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `svm` from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function called `fit_knn` that takes the following parameters: `features_train`,
    `label_train`, `features_test`, `label_test`, `kernel="linear"`, `C=1`, `degree=3`,
    and `gamma=''scale''`. This function will fit an SVC with the training set and
    print the accuracy score for both the training and testing sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the `fit_knn()` function with the default hyperparameter values, save
    the results in `2` variables, and print them. These variables are `acc_train_1`
    and `acc_test_1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the default hyperparameter values (linear model), the performance of the
    model is quite different between the training and the testing set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `kernel="poly"`, `C=1`, `degree=4`, and
    `gamma=0.05`, save the results in `2` variables, and print them. These variables
    are `acc_train_2` and `acc_test_2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With a fourth-degree polynomial, the model is not performing well on the training
    set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `kernel="poly"`, `C=2`, `degree=4`, and
    `gamma=0.05`, save the results in `2` variables, and print them. These variables
    are `acc_train_3` and `acc_test_3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Increasing the regularization parameter, `C`, didn't impact the model's performance
    at all.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `kernel="poly"`, `C=1`, `degree=4`, and
    `gamma=0.25`, save the results in `2` variables, and print them. These variables
    are `acc_train_4` and `acc_test_4`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Increasing the value of gamma to `0.25` has significantly improved the model's
    performance on the training set. However, the accuracy on the testing set is much
    lower, so the model is overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `kernel="poly"`, `C=1`, `degree=4`, and
    `gamma=0.5`, save the results in `2` variables, and print them. These variables
    are `acc_train_5` and `acc_test_5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Increasing the value of gamma to `0.5` has drastically improved the model's
    performance on the training set, but it is definitely overfitting as the accuracy
    score on the testing set is much lower.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `kernel="poly"`, `C=1`, `degree=4`, and
    `gamma=0.16`, save the results in `2` variables, and print them. These variables
    are `acc_train_6` and `acc_test_6`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With `gamma=0.16`, the model achieved a better accuracy score than it did for
    the best KNN model. Both the training and testing sets have a score of around `0.77`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `kernel="sigmoid"`, save the results in
    `2` variables, and print them. These variables are `acc_train_7` and `acc_test_7`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The sigmoid kernel achieved a low accuracy score.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `kernel="rbf"` and `gamma=0.15`, save the
    results in `2` variables, and print them. These variables are `acc_train_8` and
    `acc_test_8`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `rbf` kernel achieved a good score with `gamma=0.15`. The model is overfitting
    a bit, though.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `kernel="rbf"` and `gamma=0.25`, save the
    results in `2` variables, and print them. These variables are `acc_train_9` and
    `acc_test_9`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model performance got better with `gamma=0.25`, but it is still overfitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Call the `fit_knn()` function with `kernel="rbf"` and `gamma=0.35`, save the
    results in `2` variables, and print them. These variables are `acc_train_10` and
    `acc_test_10`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the `rbf` kernel and `gamma=0.35`, we got very similar results for the
    training and testing sets and the model's performance is higher than the best
    KNN we trained in the previous activity. This is our best model for the German
    credit dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3fPZlMQ](https://packt.live/3fPZlMQ).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3hVlEm3](https://packt.live/3hVlEm3).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this activity, we tried different values for the main hyperparameters of
    the SVM classifier: `kernel`, `gamma`, `C`, and `degrees`. We saw how they affected
    the model''s performance and their tendency to overfit. With trial and error,
    we finally found the best hyperparameter combination and achieved an accuracy
    score close to 0.78\. This process is called **hyperparameter tuning** and is
    an important step for any data science project.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. An Introduction to Decision Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 4.01: Car Data Classification'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `pandas` package as `pd`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new variable called `file_url` that will contain the URL to the raw dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data using the `pd.read_csv()` method.:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the first five rows of `df`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.13: The first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_04_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.13: The first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import the `preprocessing` module from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a function called `encode()` that takes a DataFrame and column name
    as parameters. This function will instantiate `LabelEncoder()`, fit it with the
    unique value of the column, and transform its data. It will return the transformed column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `for` loop that will iterate through each column of `df` and will
    encode them with the `encode()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, print the first five rows of `df`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14: The updated first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_04_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.14: The updated first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract the class column using `.pop()` from pandas and save it in a variable
    called `label`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `model_selection` from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the dataset into training and testing sets with `test_size=0.1` and `random_state=88`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `DecisionTreeClassifier` from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `DecisionTreeClassifier()` and save it in a variable called `decision_tree`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the decision tree with the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.15: Decision tree fit with the training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_04_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.15: Decision tree fit with the training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the score of the decision tree on the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The decision tree is achieving an accuracy score of `0.95` for our first try.
    This is remarkable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import `classification_report` from `sklearn.metrics`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the classification report of the test labels and predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.16: Output showing the expected classification report'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_04_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.16: Output showing the expected classification report'
  prefs: []
  type: TYPE_NORMAL
- en: From this classification report, we can see that our model is performing quite
    well for the precision scores for all four classes. Regarding the recall score,
    we can see that it didn't perform as well for the last class.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3hQDLtr](https://packt.live/3hQDLtr).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2NkEEML](https://packt.live/2NkEEML).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: By completing this activity, you have prepared the car dataset and trained a
    decision tree model. You have learned how to get its accuracy score and a classification
    report so that you can analyze its precision and recall scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.02: Random Forest Classification for Your Car Rental Company'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reuse the code mentioned in *Steps 1 - 4* of *Activity 1*, *Car Data Classification*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `RandomForestClassifier` from `sklearn.ensemble`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a random forest classifier with `n_estimators=100`, `max_depth=6`,
    and `random_state=168`. Save it to a variable called `random_forest_classifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the random forest classifier with the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.17: Logs of the RandomForest classifier with its hyperparameter
    values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_04_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.17: Logs of the RandomForest classifier with its hyperparameter values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These are the logs of the `RandomForest` classifier with its hyperparameter values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make predictions on the testing set using the random forest classifier and
    save them in a variable called `rf_preds_test`. Print its content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.18: Output showing the predictions on the testing set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_04_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.18: Output showing the predictions on the testing set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import `classification_report` from `sklearn.metrics`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the classification report with the labels and predictions from the test
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.19: Output showing the classification report with the labels and
    predictions from the test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_04_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.19: Output showing the classification report with the labels and predictions
    from the test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The F1 score in the preceding report shows us that the random forest is performing
    well on class `2` but not as good for classes `0` and `3`. The model is unable
    to predict accurately for class `1`, but there were only 9 observations in the
    testing set. The accuracy score is `0.84`, while the F1 score is `0.82`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import `confusion_matrix` from `sklearn.metrics`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Display the confusion matrix on the true and predicted labels of the testing
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From this confusion matrix, we can see that the `RandomForest` model is having
    difficulties accurately predicting the first class. It incorrectly predicted 16
    cases (8 + 5 + 3) for this class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the feature importance score of the test set using `.feature_importance_`
    and save the results in a variable called `rf_varimp`. Print its contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding output shows us that the most important features are the fourth
    and sixth ones, which correspond to `persons` and `safety`, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import `ExtraTreesClassifier` from `sklearn.ensemble`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `ExtraTreestClassifier` with `n_estimators=100`, `max_depth=6`,
    and `random_state=168`. Save it to a variable called `random_forest_classifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the `extratrees` classifier with the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.20: Output with the extratrees classifier with the training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_04_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.20: Output with the extratrees classifier with the training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These are the logs of the `extratrees` classifier with its hyperparameter values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make predictions on the testing set using the `extratrees` classifier and save
    them in a variable called `et_preds_test`. Print its content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.21: Predictions on the testing set using extratrees'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_04_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.21: Predictions on the testing set using extratrees'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the classification report with the labels and predictions from the test
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.22: Classification report with the labels and predictions from the
    test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_04_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.22: Classification report with the labels and predictions from the
    test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The F1 score shown in the preceding report shows us that the random forest is
    performing well on class `2` but not as good for class `0`. The model is unable
    to predict accurately for classes `1` and `3`, but there were only `9` and `8`
    observations in the testing set, respectively. The accuracy score is `0.82`, while
    the F1 score is `0.78`. So, our `RandomForest` classifier performed better with
    `extratrees`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Display the confusion matrix of the true and predicted labels of the testing
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From this confusion matrix, we can see that the `extratrees` model is having
    difficulties accurately predicting the first and third classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the feature importance score on the test set using `.feature_importance_`
    and save the results in a variable called `et_varimp`. Print its content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding output shows us that the most important features are the sixth
    and fourth ones, which correspond to `safety` and `persons`, respectively. It
    is interesting to see that `RandomForest` has the same two most important features
    but in a different order.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2YoUY5t](https://packt.live/2YoUY5t).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3eswBcW](https://packt.live/3eswBcW).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Artificial Intelligence: Clustering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 5.01: Clustering Sales Data Using K-Means'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the dataset as a DataFrame and inspect the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `df` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.18: Output showing the contents of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_05_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.18: Output showing the contents of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you look at the output, you will notice that our dataset contains `811` rows,
    with each row representing a product. It also contains `107` columns, with the
    first column being the product code, then `52` columns starting with `W` representing
    the sale quantity for each week, and finally, the normalized version of the `52`
    columns, starting with the `Normalized` columns. The normalized columns will be
    a better choice to work with rather than the absolute sales columns, `W`, as they
    will help our k-means algorithms to find the center of each cluster faster. Since
    we are going to work on the normalized columns, we can remove every `W` column
    plus the `Product_Code` column. We can also remove the `MIN` and `MAX` columns
    as they do not bring any value to our clustering. Also notice that the weeks run
    from `0` to `51` and not `1` to `52`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, create a new DataFrame without the unnecessary columns, as shown in the
    following code snippet (the first `55` columns of the dataset). You should use
    the `inplace` parameter to help you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `df2` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.19: Modified DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_05_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a k-means clustering model with `8` clusters and with `random state
    = 8`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We build a k-means model with the default value for every parameter except for
    `n_clusters=8` with `random_state=8` in order to obtain `8` clusters and reproducible
    results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Retrieve the labels from the clustering algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `labels` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.20: Output array of labels'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_05_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.20: Output array of labels'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is very hard to make sense out of this output, but each index of `labels`
    represents the cluster that the product has been assigned, based on similar weekly
    sales trends. We can now use these cluster labels to group products together.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, from the first DataFrame, `df`, keep only the `W` columns and add the
    labels as a new column, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code snippet, we removed all the unneeded columns and added
    `labels` as a new column in the DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of `df` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.21: Updated DataFrame with the new labels as a new column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_05_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.21: Updated DataFrame with the new labels as a new column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have the label, we can perform aggregation on the `label` column
    in order to calculate the yearly average sales of each cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform the aggregation (use the `groupby` function from pandas) in order to
    obtain the yearly average sale of each cluster, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code snippet, we first used the `groupby` function with the
    `sum()` method of the DataFrame to calculate the sum of every product's sales
    for each `W` column and cluster, and stored the results in `df_agg`. We then used
    the `groupby` function with the `count()` method on a single column (an arbitrary
    choice) of `df` to obtain the total number of products per cluster (note that
    we also had to rename the `W0` column after the aggregation). The next step was
    to sum all the sales columns of `df_agg` in order to obtain the total sales for
    each cluster. Finally, we calculated the `yearly_average_sales` for each cluster
    by dividing `total_sales` by `count_product`. We also included a final step to
    sort out the cluster by the highest `yearly_average_sales`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of `df_final` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.22: Expected output on the sales transaction dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_05_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.22: Expected output on the sales transaction dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Now, with this output, we see that our k-means model has managed to put similarly
    performing products together. We can easily see that the `115` products in cluster
    `3` are the best-selling products, whereas the `123` products of cluster `1` are
    performing very badly. This is very valuable for any business, as it helps them
    automatically identify and group together a number of similarly performing products
    without having any bias in the product name or description.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3fVpSbT](https://packt.live/3fVpSbT).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3hW24Gk](https://packt.live/3hW24Gk).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: By completing this activity, you have learned how to perform k-means clustering
    on multiple columns for many products. You have also learned how useful clustering
    can be for a business, even without label data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5.02: Clustering Red Wine Data Using the Mean Shift Algorithm and
    Agglomerative Hierarchical Clustering'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the dataset as a DataFrame with `sep = ";"` and inspect the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `df` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.23: df showing the dataset as the output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_05_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.23: df showing the dataset as the output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The output from the preceding screenshot is truncated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Our dataset contains `1599` rows, with each row representing a red wine. It
    also contains `12` columns, with the last column being the quality of the wine.
    We can see that the remaining 11 columns will be our features, and we need to
    scale them in order to help the accuracy and speed of our models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create `features`, `label`, and `scaled_features` variables from the initial
    DataFrame, `df`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code snippet, we separated the label (`quality`) from the features.
    Then we used `preprocessing.scale` function from `sklearn` in order to scale our
    features, as this will improve our models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, create a mean shift clustering model, then retrieve the model''s predicted
    labels and the number of clusters created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `n_cluster_mean_shift` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our mean shift model has created `10` clusters, which is already more than the
    number of groups that we have in our `quality` label. This will probably affect
    our extrinsic scores and might be an early indicator that wines sharing similar
    physicochemical properties don't belong in the same quality group.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of `label_mean_shift` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.24: Output array of label_mean_shift'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_05_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.24: Output array of label_mean_shift'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is a very interesting output because it clearly shows that most wines in
    our dataset are very similar; there are a lot more wines in cluster `0` than in
    the other clusters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now create an agglomerative hierarchical clustering model after creating a
    dendrogram and selecting the optimal number of clusters for it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `dendrogram` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.25: Output showing the dendrogram for the clusters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_05_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.25: Output showing the dendrogram for the clusters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From this output, we can see that seven clusters seems to be the optimal number
    for our model. We get this number by searching for the highest difference on the
    *y* axis between the lowest branch and the highest branch. In our case, for seven
    clusters, the lowest branch has a value of `29` and the highest branch has a value
    of `41`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of `label_agglomerative` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.26: Array showing label_agglomerative'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_05_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.26: Array showing label_agglomerative'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can see that we have a predominant cluster, `1`, but not as much as was the
    case in the mean shift model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, compute the following extrinsic approach scores for both models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a. Begin with the adjusted Rand index:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `ARI_mean` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, enter `ARI_agg` to get the expected values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `ARI_agg` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our agglomerative model has a much higher `adjusted_rand_score` than the mean
    shift model, but both scores are very close to `0`, which means that neither model
    is performing very well with regard to the true labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b. Next, calculate the adjusted mutual information:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `AMI_mean` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, enter `AMI_agg` to get the expected values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `AMI_agg` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our agglomerative model has a much higher `adjusted_mutual_info_score` than
    the mean shift model, but both scores are very close to `V_mean` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, enter `V_agg` to get the expected values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `V_agg` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our agglomerative model has a higher V-Measure than the mean shift model, but
    both scores are very close to `FM_mean` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, enter `FM_agg` to get the expected values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `FM_agg` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This time, our mean shift model has a higher Fowlkes-Mallows score than the
    agglomerative model, but both scores are still on the lower range of the score,
    which means that neither model is performing very well with regard to the true labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In conclusion, with the extrinsic approach evaluation, neither of our models
    were able to find clusters containing wines of a similar quality based on their
    physicochemical properties. We will confirm this by using the intrinsic approach
    evaluation to ensure that our models' clusters are well defined and are properly
    grouping similar wines together.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, compute the following intrinsic approach scores for both models:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a. Begin with the Silhouette Coefficient:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `Sil_mean` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, enter `Sil_agg` to get the expected values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `Sil_agg` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our mean shift model has a higher Silhouette Coefficient than the agglomerative
    model, but both scores are very close to `CH_mean` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, enter `CH_agg` to get the expected values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `CH_agg` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our agglomerative model has a much higher Calinski-Harabasz index than the mean
    shift model, which means that the agglomerative model has much more dense and
    well-defined clusters than the mean shift model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c. Finally, find the Davies-Bouldin index:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `DB_mean` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, enter `DB_agg` to get the expected values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of `DB_agg` will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our agglomerative model has a higher David-Bouldin index than the mean shift
    model, but both scores are close to **0**, which means that both models are performing
    well with regard to the definition of their clusters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2YXMl0U](https://packt.live/2YXMl0U).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Bs7sAp](https://packt.live/2Bs7sAp).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In conclusion, with the intrinsic approach evaluation, both our models were
    well defined and confirm our intuition on the red wine dataset, that is, similar
    physicochemical properties are not associated with similar quality. We were also
    able to see that in most of our scores, the agglomerative hierarchical model performs
    better than the mean shift model.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Neural Networks and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 6.01: Finding the Best Accuracy Score for the Digits Dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `tensorflow.keras.datasets.mnist` as `mnist`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `mnist` dataset using `mnist.load_data()` and save the results into
    `(features_train, label_train), (features_test, label_test)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the content of `label_train`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `label` column contains numeric values that correspond to the `10` handwritten
    digits: `0` to `9`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the shape of the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The training set is composed of `60,000` observations of shape `28` by `28`.
    We will need to flatten the input for our neural network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the shape of the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The testing set is composed of `10,000` observations of shape `28` by `28`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Standardize `features_train` and `features_test` by dividing them by `255`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set `8` as the seed for NumPy and TensorFlow using `np.random_seed()` and `tf.random.set_seed()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `tf.keras.Sequential()` class and save it into a variable called
    `model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `layers.Flatten()` with `input_shape=(28,28)` and save it into
    a variable called `input_layer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `layers.Dense()` class with `128` neurons and `activation=''relu''`,
    then save it into a variable called `layer1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a second `layers.Dense()` class with `1` neuron and `activation=''softmax''`,
    then save it into a variable called `final_layer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the three layers you just defined to the model using `.add()` and add a
    `layers.Dropout(0.25)` layer in between each of them (except for the flatten layer):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `tf.keras.optimizers.Adam()` class with `0.001` as learning rate
    and save it into a variable called `optimizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the neural network using `.compile()` with `loss=''sparse_categorical_crossentropy'',
    optimizer=optimizer, metrics=[''accuracy'']`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print a summary of the model using `.summary()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.29: Summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_06_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.29: Summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This output summarizes the architecture of our neural networks. We can see it
    is composed of four layers with one flatten layer, two dense layers, and one dropout
    layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Instantiate the `tf.keras.callbacks.EarlyStopping()` class with `monitor=''val_loss''`
    and `patience=5` as the learning rate and save it into a variable called `callback`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the neural networks with the training set and specify `epochs=10`, `validation_split=0.2`,
    `callbacks=[callback]`, and `verbose=2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.30: Fitting the neural network with the training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_06_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.30: Fitting the neural network with the training set'
  prefs: []
  type: TYPE_NORMAL
- en: We achieved an accuracy score of `0.9825` for the training set and `0.9779`
    for the validation set for recognizing hand-written digits after just `10` epochs.
    These are amazing results. In this section, you learned how to build and train
    a neural network from scratch using TensorFlow to classify digits.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/37UWf7E](https://packt.live/37UWf7E).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/317R2b3](https://packt.live/317R2b3).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6.02: Evaluating a Fashion Image Recognition Model Using CNNs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `tensorflow.keras.datasets.fashion_mnist` as `fashion_mnist`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the Fashion MNIST dataset using `fashion_mnist.load_data()` and save the
    results into `(features_train, label_train), (features_test, label_test)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the shape of the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The training set is composed of `60,000` images of size `28`*`28`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the shape of the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The testing set is composed of `10,000` images of size `28`*`28`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Reshape the training and testing sets with the dimensions (`number_rows`, `28`,
    `28`, `1`), as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Standardize `features_train` and `features_test` by dividing them by `255`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set `8` as the seed for `numpy` and `tensorflow` using `np.random_seed()` and
    `tf.random.set_seed()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `tf.keras.Sequential()` class and save it into a variable called
    `model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `layers.Conv2D()` with `64` kernels of shape `(3,3), activation=''relu''
    and input_shape=(28,28)` and save it into a variable called `conv_layer1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `layers.Conv2D()` with `64` kernels of shape `(3,3), activation=''relu''`
    and save it into a variable called `conv_layer2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `layers.Flatten()` with `128` neurons and `activation=''relu''`,
    then save it into a variable called `fc_layer1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `layers.Flatten()` with `10` neurons and `activation=''softmax''`,
    then save it into a variable called `fc_layer2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the four layers you just defined to the model using `.add()` and add a
    `MaxPooling2D()` layer of size `(2,2)` in between each of the convolutional layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `tf.keras.optimizers.Adam()` class with `0.001` as the learning
    rate and save it into a variable called `optimizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the neural network using `.compile()` with `loss=''sparse_categorical_crossentropy'',
    optimizer=optimizer, metrics=[''accuracy'']`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print a summary of the model using `.summary()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.31: Summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_06_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.31: Summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The summary shows us that there are more than `240,000` parameters to be optimized
    with this model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the neural network with the training set and specify `epochs=5`, `validation_split=0.2`,
    and `verbose=2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.32: Fitting the neural network with the training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_06_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.32: Fitting the neural network with the training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After training for `5` epochs, we achieved an accuracy score of `0.925` for
    the training set and `0.9042` for the validation set. Our model is overfitting
    a bit.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Evaluate the performance of the model on the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We achieved an accuracy score of `0.8976` on the testing set for predicting
    images of clothing from the Fashion MNIST dataset. You can try on your own to
    improve this score and reduce the overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Nzt6pn](https://packt.live/2Nzt6pn).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2NlM5nd](https://packt.live/2NlM5nd).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we designed and trained a CNN architecture for recognizing
    images of clothing from the Fashion MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6.03: Evaluating a Yahoo Stock Model with an RNN'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `pandas` as `pd` and `numpy` as `np`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `file_url` containing a link to the raw dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset using `pd.read_csv()` into a new variable called `df`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the values of the second column using `.iloc` and `.values` and save
    the results in a variable called `stock_data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `MinMaxScaler` from `sklearn.preprocessing`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `MinMaxScaler()` and save it to a variable called `sc`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Standardize the data with `.fit_transform()` and save the results in a variable
    called `stock_data_scaled`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create two empty arrays called `X_data` and `y_data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `window` that will contain the value `30`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `for` loop starting from the `window` value and iterate through the
    length of the dataset. For each iteration, append to `X_data` the previous rows
    of `stock_data_scaled` using `window` and append the current value of `stock_data_scaled`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`y_data` will contain the opening stock price for each day and `X_data` will
    contain the last 30 days'' stock prices.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Convert `X_data` and `y_data` into NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshape `X_data` as (number of rows, number of columns, 1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the first `1,000` rows as the training data and save them into two variables
    called `features_train` and `label_train`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the rows after row `1,000` as the testing data and save them into two variables
    called `features_test` and `label_test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set `8` as `seed` for NumPy and TensorFlow using `np.random_seed()` and `tf.random.set_seed()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `tf.keras.Sequential()` class and save it into a variable called
    `model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `layers.LSTM()` with `50` units, `return_sequences=''True''`, and
    `input_shape=(X_train.shape[1], 1)`, then save it into a variable called `lstm_layer1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `layers.LSTM()` with `50` units and `return_sequences=''True''`,
    then save it into a variable called `lstm_layer2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `layers.LSTM()` with `50` units and `return_sequences=''True''`,
    then save it into a variable called `lstm_layer3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `layers.LSTM()` with `50` units and save it into a variable called
    `lstm_layer4`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `layers.Dense()` with `1` neuron and save it into a variable called `fc_layer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the five layers you just defined to the model using `.add()` and add a
    `Dropout(0.2)` layer in between each of the LSTM layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `tf.keras.optimizers.Adam()` class with `0.001` as the learning
    rate and save it into a variable called `optimizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the neural network using `.compile()` with `loss=''mean_squared_error'',
    optimizer=optimizer, metrics=[mse]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print a summary of the model using `.summary()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.33: Summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_06_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.33: Summary of the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The summary shows us that there are more than `71,051` parameters to be optimized
    with this model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fit the neural network with the training set and specify `epochs=10, validation_split=0.2,
    verbose=2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE225]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.34: Fitting the neural network with the training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_06_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.34: Fitting the neural network with the training set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After training for `10` epochs, we achieved a mean squared error score of `0.0025`
    for the training set and `0.0033` for the validation set. Our model is overfitting
    a little bit.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, evaluate the performance of the model on the testing set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We achieved a mean squared error score of `0.0017` on the testing set, which
    means we can quite accurately predict the stock price of Yahoo using the last
    30 days' stock price data as features.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3804U8P](https://packt.live/3804U8P).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3hWtU5l](https://packt.live/3hWtU5l).
  prefs: []
  type: TYPE_NORMAL
- en: You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we designed and trained an RNN model to predict the Yahoo
    stock price from the previous 30 days of data.
  prefs: []
  type: TYPE_NORMAL
