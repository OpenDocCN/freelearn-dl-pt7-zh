- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Emergence of Risk-Averse Methodologies and Frameworks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter gives a detailed overview of defining and architecting ML defense
    frameworks that can protect data, ML models, and other necessary artifacts at
    different stages of ML training and evaluation pipelines. In this chapter, you
    will learn about different anonymization, encryption, and application-level privacy
    techniques, as well as hybrid security measures, that serve as the basis of ML
    model development for both centralized and distributed learning. In addition,
    you will also discover scenario-based defense techniques that can be applied to
    safeguard data and models to solve practical industry-grade ML use cases. The
    primary objective of this chapter is to explain the application of commonly used
    defense tools, libraries, and metrics available for large-scale ML SaaS platforms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, these topics will be covered in the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Threat matrix and defense techniques
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anonymization and data encryption
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Differential** **Privacy** (**DP**)'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hybrid privacy methods and models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial risk mitigation frameworks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further, with the use of `pysft`, `Pyhfel`, `secml` , `ml_privacy_meter`, `tensorflow_privacy`,
    `mia`, `diffprivlib`, and `foolbox`, we will see how to test model robustness
    against adversarial attacks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter requires you to have Python 3.8 installed along with the Python
    packages listed here (with their installation commands), as well as Keras 2.7.0
    and TensorFlow 2.7.0:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '`pip` `install adversarial-robustness-toolbox`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip` `install syft==0.2.9`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip` `install Pyfhel`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip` `install secml`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`git` `clone https://github.com/privacytrustlab/ml_privacy_meter`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip install -r requirements.txt`, `pip` `install -e`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip` `install diffprivlib`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip` `install tensorflow-privacy`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip` `install mia`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip` `install foolbox`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing the threat matrix and defense techniques
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, let''s look at different defense techniques essential for
    enterprises to proactively manage threats related to adversarial attacks during
    the following stages:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Initial research, planning, and system and model design/architecture phase
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML model training and deployment
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML model live in production
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will also get learn additional capabilities, expertise, and infrastructure
    that organizations need to invest in to have a foolproof defense system.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Researching and planning during the system and model design/architecture phase
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This phase (*Figure 2**.1*) is related to all actions taken during model design,
    architectural planning, and conceptualization in which the adversary carries out
    preliminary investigations, searching to gain knowledge of the victim’s infrastructure,
    datasets, and models that will enable them to set up their own capabilities for
    initiating attacks on ML SaaS platforms.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Relevant attack stages during ML model design and development](img/Figure_2.01_B18681.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Relevant attack stages during ML model design and development
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – 在机器学习模型设计和开发过程中的相关攻击阶段
- en: We see here the large scope of the initial phase, where adversarial actions
    can be detrimental to our model and architecture conceptualization. Now, let's
    discuss the different steps adversaries take when trying to perform an attack.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，初期阶段的范围非常广泛，对手的行动可能对我们的模型和架构设计构成不利影响。现在，让我们讨论对手在尝试进行攻击时所采取的不同步骤。
- en: Reconnaissance
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 侦察
- en: '**Reconnaissance** is one of the early stages where an adversary actively or
    passively gathers information to use in later adversarial stages to enable **resource
    development**, execute **initial access**, or lead to the execution of continuous
    reconnaissance attempts. Some of the associated risks and mitigations of this
    stage are described in the following list. The best way for the victim to mitigate
    reconnaissance attempts is to minimize the availability of sensitive information
    to external entities and employ network content, network flow, file creation,
    and application log monitoring agents to detect and raise alarms if suspicious
    activity (such as bots or web crawling) is detected from a single IP source.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**侦察**是对手主动或被动地收集信息的早期阶段，收集的信息将在后续的对抗阶段中使用，以促进**资源开发**、执行**初步访问**或导致持续的侦察尝试。一些与此阶段相关的风险和缓解措施如下所述。受害者减轻侦察尝试的最佳方法是将敏感信息对外部实体的可用性降到最低，并采用网络内容、网络流量、文件创建和应用程序日志监控代理，以便在从单一IP源检测到可疑活动（如机器人或网站抓取）时发出警报。'
- en: 'Let''s now describe how reconnaissance can take place:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来描述侦察是如何进行的：
- en: '**Active scanning**: This step involves scanning operations by adversaries
    to gather information for targeting. Scanning and search operations (on websites/domains
    or open technical databases) may be carried out on victim infrastructure via network
    traffic (with network protocols such as ICMP) by probing mechanisms, or by collecting
    information through external remote services or public-facing applications.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动扫描**：此步骤涉及对手通过扫描操作收集信息以进行目标定位。扫描和搜索操作（在网站/域名或开放技术数据库上）可能通过网络流量（如ICMP协议）在受害者基础设施上执行，或者通过外部远程服务或面向公众的应用程序收集信息。'
- en: '**Gather victim host/identity/organization information**: This step involves
    adversarial activity to gain information related to victims’ administrative data
    (e.g., name, assigned IP, functionality, IP ranges, domain names, etc.), configuration
    (e.g., operating system, language, etc.), names of divisions/departments, business
    operations, and the roles and responsibilities of major employees.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集受害者主机/身份/组织信息**：此步骤涉及对手活动，旨在获取与受害者的管理数据相关的信息（例如，姓名、分配的IP、功能、IP范围、域名等）、配置（例如，操作系统、语言等）、部门/部门名称、业务操作以及主要员工的角色和职责。'
- en: '`User-Agent` string HTTP/S fields) to automatically remove malicious links
    and attachments.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`User-Agent` 字符串 HTTP/S 字段）自动删除恶意链接和附件。'
- en: Using anti-spoofing mechanisms, providing restricted access to websites that
    have attachments (`.pdf`, `.docx`, `.exe`, `.pif`, `.cpl`, and so on), and enabling
    email authentication can enable protection against phishing activities.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用反欺骗机制、提供对附件（`.pdf`、`.docx`、`.exe`、`.pif`、`.cpl`等）的限制访问权限，并启用电子邮件身份验证，可以有效防止钓鱼活动。
- en: '**Search closed sources, open technical databases, websites and domains, and
    victim-owned websites**: These search operations by the adversary can help to
    retrieve confidential information from reputable private sources (such as databases,
    repositories, or paid subscriptions to feeds of technical/threat intelligence
    data). In addition, registrations of domains/certificates; network data/artifacts
    gathered from traffic and/or scans; business-, department-, and employee-related
    information from online sites; and social media can all help the attacker gather
    the information necessary for targeting.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索封闭源、开放技术数据库、网站和域名以及受害者拥有的网站**：对手进行的这些搜索操作有助于从信誉良好的私人源（如数据库、仓库或订阅技术/威胁情报数据的付费信息源）中检索机密信息。此外，域名/证书注册；通过流量和/或扫描收集的网络数据/痕迹；来自在线网站的业务、部门和员工相关信息；以及社交媒体等都可以帮助攻击者收集目标所需的信息。'
- en: Resource development
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 资源开发
- en: '**Resource development** is another early phase of adversarial action, where
    adversaries engage themselves in creating resources to use in subsequent attack
    stages. Resources may be created, purchased, or stolen to target victims. Let''s
    examine this in more detail:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源开发**是对抗性行动的另一个早期阶段，在此阶段，攻击者致力于创建用于后续攻击阶段的资源。资源可以通过创建、购买或盗取来针对受害者。我们来更详细地分析这一点：'
- en: '**Public ML artifact retrieval**: This is an important action taken by adversaries
    to retrieve ML artifacts from public sources, cloud storage, public-facing services,
    and data repositories. These artifacts can reveal information related to the software
    stacks, libraries, algorithms, hyperparameters, and model architectures used to
    train, deploy, test, and evaluate ML models. Adversaries can use either the victim’s
    representative datasets or models to modify and craft the datasets and models
    and accordingly train proxy ML models tailored to offline attacks, without directly
    accessing the target model. The best control measures against this that can be
    adopted by organizations are the following:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公共机器学习模型获取**：这是对手从公共来源、云存储、面向公众的服务和数据仓库中获取机器学习模型（ML）工件的重要行为。这些工件可以揭示与用于训练、部署、测试和评估机器学习模型的软件堆栈、库、算法、超参数和模型架构相关的信息。对手可以利用受害者的代表性数据集或模型，修改并构建数据集和模型，从而训练适用于离线攻击的代理机器学习模型，而无需直接访问目标模型。组织可以采取的最佳控制措施包括以下几点：'
- en: Enabling multi-level security rules for the full protection of datasets, models,
    and artifacts by employing built-in multi-factor authentication schemes
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过采用内建的多因素认证方案，启用多层次安全规则以全面保护数据集、模型和工件
- en: Using cloud security rules and ACLs to provide restricted access to ML data
    and artifacts
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用云安全规则和访问控制列表（ACL）提供对机器学习数据和工件的受限访问
- en: '**Gathering adversarial ML attack implementation information**: Open source
    implementations of ML algorithms and adversarial attack code (such as CleverHans
    or ART ([https://researchain.net/archives/pdf/Technical-Report-On-The-Cleverhans-V2-1-0-Adversarial-Examples-Library-2906240](https://researchain.net/archives/pdf/Technical-Report-On-The-Cleverhans-V2-1-0-Adversarial-Examples-Library-2906240))
    and Foolbox ([https://arxiv.org/pdf/1707.04131.pdf](https://arxiv.org/pdf/1707.04131.pdf)))
    can be misused by attackers. As well as facilitating research, these open source
    tools can be used to carry out attacks against victims’ infrastructures. In this
    chapter, we give examples to demonstrate how ART and Foolbox can be used.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集对抗性机器学习攻击实现信息**：机器学习算法和对抗性攻击代码的开源实现（如CleverHans或ART（[https://researchain.net/archives/pdf/Technical-Report-On-The-Cleverhans-V2-1-0-Adversarial-Examples-Library-2906240](https://researchain.net/archives/pdf/Technical-Report-On-The-Cleverhans-V2-1-0-Adversarial-Examples-Library-2906240)）和Foolbox（[https://arxiv.org/pdf/1707.04131.pdf](https://arxiv.org/pdf/1707.04131.pdf)））可能被攻击者滥用。这些开源工具不仅促进研究，也可以被用来对受害者的基础设施发起攻击。在本章中，我们通过示例展示如何使用ART和Foolbox。'
- en: '**Gaining adversarial ML attack implementation expertise and capabilities**:
    After gaining information on open source attack tools, adversaries can deep dive
    into research papers and use their own ideas to craft their own attack models
    and start using them.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获得对抗性机器学习攻击实现专业知识和能力**：在获得开源攻击工具信息后，攻击者可以深入研究论文，运用自己的思路制作攻击模型并开始使用它们。'
- en: '**Acquiring infrastructure – attack development and staging workspaces**: In
    this phase, adversaries rely on the free compute resources available from major
    cloud providers (such as AWS, Google Cloud, Google Colaboratory, and Azure) to
    initiate attacks. The use of multiple workspaces can help them avoid detection.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获取基础设施——攻击开发和部署工作空间**：在这一阶段，攻击者依赖于主要云服务提供商（如AWS、Google Cloud、Google Colaboratory和Azure）提供的免费计算资源来启动攻击。使用多个工作空间可以帮助他们避免被检测。'
- en: '**Publishing poisoned datasets and triggering poisoned data training**: This
    step involves creating poisoned datasets (by modifying source datasets, data,
    or its labels) and publishing these to compromise victims’ ML supply chains. The
    vulnerabilities embedded in these ML models using poisoned data are activated
    later and cannot easily be detected.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发布毒化数据集并触发毒化数据训练**：此步骤涉及创建毒化数据集（通过修改源数据集、数据或其标签），并将其发布，以破坏受害者的机器学习供应链。这些使用毒化数据的机器学习模型中所嵌入的漏洞在之后会被激活，且不易被检测到。'
- en: 'Strategies that can be employed to protect against poison attacks include leveraging
    De-Pois (De-Pois: An Attack-Agnostic Defense against Data Poisoning Attacks :
    [https://arxiv.org/pdf/2105.03592.pdf](https://arxiv.org/pdf/2105.03592.pdf)),
    an attack-agnostic defense framework used to construct mimic models. This framework
    uses **Generative Adversarial Networks** (**GANs**) to enable the training of
    data with augmentations and the creation of models that behave similarly, in terms
    of outcome, to the original model. This model can detect the poisoned samples
    by evaluating prediction differences between the target model and the mimic model.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '防御中毒攻击的策略之一是利用De-Pois（De-Pois: 一种针对数据中毒攻击的攻击无关防御：[https://arxiv.org/pdf/2105.03592.pdf](https://arxiv.org/pdf/2105.03592.pdf)），这是一个用于构建模仿模型的攻击无关防御框架。该框架使用**生成对抗网络**（**GANs**）来训练带有增强数据的模型，并创建在结果上与原始模型行为相似的模型。通过评估目标模型和模仿模型之间的预测差异，该模型可以检测中毒样本。'
- en: 'In addition to generating defensive awareness of the aforementioned possible
    intrusions, enterprise-grade defense frameworks should take into consideration
    some of the following aspects of security bottlenecks and take appropriate remedial
    measures:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了增强对上述可能入侵的防御意识外，企业级防御框架应考虑一些安全瓶颈的方面，并采取适当的补救措施：
- en: '**Establishing accounts**: In this phase, external adversaries engage themselves
    in creating accounts to build a persona across different social media platforms,
    such as LinkedIn, as well as on GitHub, to impersonate real people. These personas
    can be used to accumulate public information, set up email accounts, and strengthen
    public profiles, which will aid in stealing information over the course of time.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建立账户**：在这个阶段，外部对手通过在不同的社交媒体平台上（如LinkedIn）以及GitHub上创建账户来建立虚假身份，冒充真实人物。这些虚假身份可以用于积累公开信息、设置电子邮件账户和强化公开个人资料，进而在一段时间内帮助窃取信息。'
- en: The best tactic to protect against such actions is to identify any suspicious
    activity of individuals who claim to work for the organization or have made connection
    requests to different organizational accounts.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 防止此类行为的最佳策略是识别任何声称为组织工作或已向不同组织账户发送连接请求的可疑活动。
- en: '**Obtaining capabilities**: Here, adversaries rely on stealing, purchasing,
    or freely downloading malware, licensed software, exploits, certificates, and
    information related to vulnerabilities. Mitigation actions include the following:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获取能力**：在此阶段，对手依赖于窃取、购买或免费下载恶意软件、许可软件、漏洞利用、证书和与漏洞相关的信息。缓解措施包括以下几点：'
- en: Carefully analyze and detect features and services that are easy to embed and
    can be associated with malware providers (such as compilers, debugging artifacts,
    code extracts, or any other offerings related to **Malware as a** **Service**
    (**MaaS**)).
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细分析和检测易于嵌入且与恶意软件提供者（如编译器、调试文档、代码片段或任何其他与**恶意软件即服务**（**MaaS**）相关的产品）相关的特征和服务。
- en: Malware repository scanning and feature identification can help to blacklist
    adversaries.
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶意软件仓库扫描和特征识别有助于将对手列入黑名单。
- en: Now, let's discuss a defense strategy involving the use of the open source secml
    library ([https://secml.github.io/class6/](https://secml.github.io/class6/), a
    security evaluation framework) to build, explain, attack, and evaluate security
    using algorithms such as **Support Vector Machine** (**SVM**) and ClassifierRidge
    (a custom ML Ridge classifier). These types of classification algorithms can be
    used to detect malware in Android applications and explain ML classifier model's
    predicted outcomes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一种防御策略，该策略使用开源secml库（[https://secml.github.io/class6/](https://secml.github.io/class6/)，一个安全评估框架）来构建、解释、攻击和评估安全性，使用的算法包括**支持向量机**（**SVM**）和ClassifierRidge（一个自定义的ML
    Ridge分类器）。这些类型的分类算法可用于检测安卓应用中的恶意软件，并解释ML分类器模型的预测结果。
- en: 'In the following code snippet, we have loaded a toy dataset of Android applications,
    named `DrebinRed`. The loaded dataset consists of 12,000 benign and 550 malicious
    samples extracted from Drebin. On training (using a 0.5:0.5 train-test split)
    the dataset with SVM or the Ridge classifier, we observe the model has a 2% **False
    Positive Rate** (**FPR**) in correctly identifying the benign and malicious samples:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，我们加载了一个名为`DrebinRed`的Android应用程序玩具数据集。加载的数据集包含12,000个良性样本和550个恶意样本，这些样本来自Drebin。通过使用SVM或岭回归分类器对数据集进行训练（采用0.5:0.5的训练-测试划分），我们观察到模型在正确识别良性和恶意样本时的**假阳性率**（**FPR**）为2%：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following output snippet further illustrates the most significant components
    of the Android malware detector application. secml uses a `Gradient * Input` gradient-based
    explanation technique to explain the attributions of different points during the
    classification phase. The most important features (the top 5) and their relevance
    (in terms of percentage) help to explain each correct (not a part of the malware
    component) and corrupted sample, and even this approach/technique to explain attributions
    on sparse datasets:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出片段进一步说明了Android恶意软件检测器应用程序中最重要的组件。secml使用`Gradient * Input`基于梯度的解释技术来解释分类阶段不同点的归因。最重要的特征（前五名）及其相关性（以百分比表示）有助于解释每个正确（非恶意软件组件）和被污染的样本，甚至这种方法/技术可用于解释稀疏数据集上的归因。
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As ~25% of the relevance is attributed to five features, these features have
    a larger impact on the classifier being susceptible to adversarial evasion attacks.
    Leveraging this behavior of the malware detector, a gradient-based maximum-confidence
    evasion attack can be employed to generate adversarial samples against the classifier.
    This can trigger an L1-order sparse attack by changing one feature at a time to
    misclassify outputs as 1 instead of 0 and vice versa. We can trigger attacks such
    as the one demonstrated in the following code snippet, where feature addition
    works better to fool the malware classifier than feature removal. Removing features
    may remove other important components of the model, making it more difficult to
    misclassify. On the contrary, feature addition is an easy way to fool the model
    into classifying correct (benign components of the loaded dataset) samples as
    corrupted.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大约25%的相关性归因于五个特征，这些特征对分类器易受对抗性规避攻击的影响更大。利用恶意软件检测器的这一行为，可以采用基于梯度的最大置信度规避攻击来生成对抗样本，攻击该分类器。这可以通过一次改变一个特征来触发L1阶稀疏攻击，将输出错误分类为1而不是0，反之亦然。我们可以触发类似于以下代码片段中展示的攻击，其中特征添加比特征移除更能欺骗恶意软件分类器。移除特征可能会删除模型的其他重要组件，使得错误分类更难实现。相反，特征添加是一个轻松的方式，可以让模型将正确的（数据集中的良性部分）样本错误分类为被污染样本。
- en: 'After adding the adversarial samples, we can trigger the evasion attack with
    `classifier`, `distance`, and other parameters, as shown here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 添加对抗样本后，我们可以使用`classifier`、`distance`以及其他参数触发规避攻击，如下所示：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: secml determines the model robustness using a `epsvalue` varying between 0 and
    28 with a step size of `4`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: secml通过使用`epsvalue`（在0到28之间变化，步长为`4`）来确定模型的鲁棒性。
- en: 'To test the Android malware detector against a greater number of added features,
    we can run the evasion attack on the security evaluation method, as detailed in
    the following code snippet:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试Android恶意软件检测器在更多新增特征下的表现，我们可以对安全评估方法进行规避攻击，具体代码如下：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, let''s plot the SEC:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制SEC：
- en: 'The following code begins the process of getting the SEC:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码开始获取SEC的过程：
- en: '[PRE4]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, get the ROC threshold at which the detection rate should be computed:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，获取应计算检测率的ROC阈值：
- en: '[PRE5]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, use the convenience function to plot the SEC:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用方便的函数来绘制SEC：
- en: '[PRE6]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can see how the SVM classifier is highly vulnerable to adversarial attacks,
    and particularly sensitive to attacks against the most impactful features. An
    attack can evade this classifier with a perturbation as small as eps (ε) = 0.1\.
    When we change it to have fewer than 10 features (which are the most important
    ones), half of the corrupted samples are misclassified as correct ones.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，SVM分类器对对抗性攻击非常脆弱，尤其是对攻击最具影响力的特征特别敏感。攻击可以通过一个像eps（ε）= 0.1这么小的扰动来规避该分类器。当我们将特征数减少到少于10个（这些是最重要的特征）时，半数的被污染样本被错误分类为正确样本。
- en: In the following figure, *Figure 2**.2*, the chart labeled **A** shows a detection
    rate of 97% with an FPRof 20\. While the detection rate falls with increasing
    epsilon (ε), we observe that the fall is very steep for the Ridge classifier (**C**),
    while it happens in a step fashion for SVM (**B**). As the fall is steeper for
    the Ridge classifier, it is not a better option than SVM, which will exhibit a
    lower FPR. Make sure to examine the **SECs** in the following graphs, which provide
    estimations of the detection rate (%) with ε. The SEC plots help us to conclude
    that the malware detector ceases to perform with increasing levels of adversarial
    perturbations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，*图 2.2*，标记为**A**的图表显示了97%的检测率和20的FPR。当ε增大时，检测率会下降；然而，我们观察到，Ridge分类器（**C**）的下降非常陡峭，而SVM（**B**）则呈阶梯式下降。由于Ridge分类器的下降更陡峭，它并不是比SVM更好的选择，后者将表现出更低的FPR。请确保查看下图中的**SECs**，它们提供了随ε变化的检测率（%）的估算值。SEC图帮助我们得出结论，即随着对抗性扰动的增加，恶意软件检测器的表现停止。
- en: "![Figure 2.2 – Malware detection rate and SEC on SVM and \uFEFFthe Ridge classifier](img/Figure_2.02_B18681.jpg)"
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – SVM和Ridge分类器上的恶意软件检测率和SEC](img/Figure_2.02_B18681.jpg)'
- en: Figure 2.2 – Malware detection rate and SEC on SVM and the Ridge classifier
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – SVM和Ridge分类器上的恶意软件检测率和SEC
- en: '**Staging** refers to actions taken by adversaries to upload, install, and
    set up capabilities on infrastructures that were previously compromised or rented
    by them, to target victim networks. Such activities might include setting up web
    resources to exploit the victim’s browsing website (to steal confidential information)
    or uploading malware tools to initiate attacks on the victim’s network. There’s
    no prompt detection technique to avoid this; however, internet scanning tools
    may reveal the date and time of such attacks.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**阶段化**是指对手在之前被攻陷或租用的基础设施上进行上传、安装和设置功能，以便针对受害者网络进行攻击。这些活动可能包括设置网页资源以利用受害者的浏览网站（窃取机密信息）或上传恶意软件工具以启动对受害者网络的攻击。没有有效的检测技术可以避免这种情况；然而，互联网扫描工具可能会揭示这些攻击的日期和时间。'
- en: Initial access
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始访问
- en: Initial access helps an adversary to leverage security weaknesses on public-facing
    web servers and gain access to a network. This can occur in one of the early stages
    of development when the model design and the system architecture are still in
    the development phase. The primary steps to mitigate initial adversarial access
    include controlling the abuse of credentials via proper management of user account
    control, issuing valid accounts, enforcing privileged account management practices,
    defining organization password policies (such as the frequency of password changes),
    and having in place a systematic user training process and application developer
    guidance to restrict any illegitimate access to systems.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 初始访问帮助对手利用公共面向的网络服务器上的安全漏洞并获得对网络的访问权限。这可能发生在开发的早期阶段，当模型设计和系统架构仍处于开发阶段时。减轻初始对抗访问的主要步骤包括通过适当的用户账户管理控制凭证滥用，发放有效账户，执行特权账户管理实践，定义组织密码策略（例如密码更改的频率），并建立系统化的用户培训流程和应用开发者指导，以限制任何不正当的系统访问。
- en: 'Let''s now explore the different actions that can be taken by adversaries if
    they are successful in acquiring initial access:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来探讨一下，如果对手成功获得初始访问，他们可能采取的不同行动：
- en: '**Supply chain compromise**: In this step, adversaries compromise different
    components of a victim’s system (such as GPU hardware, data and its annotations,
    parts of the ML software stack, or the model) to carry out an attack. The attacker
    manipulates development tools, environments, code repositories, open source dependencies,
    and software update/distribution mechanisms; compromises system images; and replaces
    legitimate software (using different versions) to successfully compromise the
    victim’s systems. Organizations should mitigate tampering activities by employing
    techniques to verify distributed binaries (hash checking), along with using tools
    to scan malicious signatures and engaging in physical hardware inspection. Even
    using patch management processes and vulnerability scanning tools to scan dependencies,
    unnecessary features, components, and files can help prevent adversarial access
    by enforcing strong testing rules prior to deployment.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**供应链妥协**：在这一步骤中，攻击者会妥协受害者系统的不同组件（如GPU硬件、数据及其注释、机器学习软件栈的部分内容或模型），以进行攻击。攻击者操控开发工具、环境、代码库、开源依赖项和软件更新/分发机制；妥协系统镜像；并通过替换合法软件（使用不同版本）来成功妥协受害者的系统。组织应通过使用技术验证分发的二进制文件（哈希检查），结合使用扫描恶意签名的工具，并进行硬件实物检查来减少篡改活动。即使使用补丁管理流程和漏洞扫描工具扫描依赖项、不必要的功能、组件和文件，也有助于通过在部署前执行严格的测试规则来防止攻击者的访问。'
- en: '**Drive-by compromise**: This involves the exploitation of the victim’s browser
    (where the adversary may inject malicious code with JavaScript, iFrames, and cross-site
    scripting, or help to serve malicious ads) and application access tokens, and
    can be mitigated by doing the following:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**驱动式妥协**：这涉及到攻击受害者的浏览器（攻击者可能通过JavaScript、iFrames和跨站脚本注入恶意代码，或者协助投放恶意广告）和应用程序访问令牌，这可以通过以下方式减轻：'
- en: Using browser sandboxes, deploying virtualization measures, and applying micro-segmentation
    logic. We can limit attacks by isolating applications and web browsers by creating
    and defining zones in data centers and cloud environments (to isolate workloads).
    This is one of the strongest ways to limit network traffic and client-side exploitation.
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用浏览器沙箱、部署虚拟化措施以及应用微分段逻辑。我们可以通过在数据中心和云环境中创建和定义区域来隔离应用程序和网页浏览器（以隔离工作负载），从而限制攻击。这是限制网络流量和客户端利用的最有效方法之一。
- en: Employing defense tools such as **Enhanced Mitigation Experience Toolkit** (**EMET**),
    network intrusion detectors with SSL/TLS inspection, firewalls, proxies, ad blockers,
    and script-blocking extensions can help to control exploitation behavior, block
    bad domains and ads, and prevent the execution of JavaScript.
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**增强型缓解体验工具**（**EMET**）、具有SSL/TLS检测功能的网络入侵检测器、防火墙、代理服务器、广告拦截器以及脚本阻止扩展可以帮助控制利用行为，阻止恶意域名和广告，并防止JavaScript的执行。
- en: '**Exploit public-facing applications**: As this technique involves adversaries
    accessing, exploiting, and bringing down public-facing services such as databases,
    **Server Message Block** (**SMB**), and other applications with open sockets,
    the main remediation tasks lie with the security architects in designing and deploying
    application in isolation and sandboxing (limiting exploited targets’ access to
    other processes), web application firewalls, network segmentation (segmenting
    public interfaces on a demilitarized zone or a separate hosting infrastructure),
    and privileged account management (adhering to the principle of least privilege
    for accessing services) to limit attack traffic. In addition, regular software
    updates, patch management, vulnerability scanning tools, and application log and
    network flow monitoring tools (using deep packet inspection to discover artifacts
    of malicious traffic, such as SQL injection) can be used to detect improper input
    traffic and raise alerts.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用面向公众的应用程序**：由于这种技术涉及攻击者访问、利用并破坏面向公众的服务，如数据库、**服务器消息块**（**SMB**）以及其他具有开放套接字的应用程序，主要的修复任务在于安全架构师在设计和部署应用程序时进行隔离和沙箱化（限制被利用目标对其他进程的访问）、应用程序防火墙、网络分段（将公共接口分段到非军事区或单独的托管基础设施中）以及特权账户管理（遵循最小特权原则来访问服务），以限制攻击流量。此外，定期进行软件更新、补丁管理、漏洞扫描工具、应用程序日志和网络流量监控工具（使用深度数据包检查来发现恶意流量的痕迹，如SQL注入）可以用于检测不当的输入流量并发出警报。'
- en: '**External remote services**: This method involves adversaries discovering
    external-facing remote services, such as VPNs or Citrix, and finding routes to
    connect to internal enterprise network resources from these external locations.
    To alleviate such risks, security teams should be extra cautious:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disable or block unnecessary remotely available services, limit access to resources
    over the network (by prompting the use of managed remote access systems such as
    VPNs), enable multi-factor authentication, and allow network segmentation (through
    the use of network proxies, gateways, and firewalls).
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Facilitate log monitoring related to applications, session logons, and network
    traffic to detect authenticated sessions, discover unusual access patterns and
    times of operation, and assist in detecting adversarial behavior.
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware additions**: Introducing additional computer accessories or hardware
    components in the network can permit adversaries to undertake passive network
    tapping, network traffic modification through adversary/man-in-the-middle attacks,
    keystroke injection, or kernel memory reading via **Direct Memory Access** (**DMA**).
    To avoid this, asset management systems should be used to do the following:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limit access to resources over the network, limit installation of hardware,
    and employ hardware detectors or endpoint sensors to discover additions of USB,
    Thunderbolt, and other external device communication ports in the network.
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Further, to safeguard adversarial copying operations on removable media, organization
    policies should forbid or restrict removable media.
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ML model access
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Adversaries may gain access to an ML model legitimately through four different
    techniques that we’ll examine in this section. The best mitigation strategy is
    to include sufficient security rules (cloud- and token-based authorization schemes)
    to enable authentic access to model APIs, ML services, physical environments,
    and ML models, as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '**Model inference API access**: This involves restricting adversary access
    through the use of APIs to discover the ML model''s ontology or family. The corresponding
    defense action is to limit the introduction of test data into the target systems
    by single agents to prevent issues related to evading ML models and eroding ML
    model integrity. As we saw in [*Chapter 1*](B18681_01.xhtml#_idTextAnchor014),
    there is a possibility of an evasion attack where attackers try to evade detection
    by hiding the content of spam and malware code. The same kind of attack is possible
    by using model inference APIs to misclassify examples (individual data samples)
    as legitimate.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML-enabled product or service limit**: This method limits indirect access
    to ML models to hide information related to the model’s inference from its logs
    and metadata. Indirect access can originate from any product or service built
    by adversaries to gain access to the victim’s ML model.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Physical environment access**: To eliminate the scope of adversarial attacks
    in data engineering pipelines, enable data validation checks across multiple layers
    of input data ingestion, preprocessing, and feature engineering.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Full ML model access**: To prevent the adversary from gaining full access
    to the model, the best possible defense strategy is to incorporate privacy-preserving
    ML techniques for data aggregation and training to enable protection from adversarial
    white-box attacks. Otherwise, these attacks allow the adversary to gain complete
    information on the model''s architecture, parameters, and class ontology and exfiltrate
    the model to execute offline attacks once the model is running live with production
    data.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One of the preferred mechanisms for defending against white-box (model parameters)
    and black-box (output predictions) attacks is to train the model and evaluate
    the accuracy of the attacks. If we use **ML Privacy Meter** (a Python library
    that helps to quantify risk in ML models) prior to releasing models, we can test
    the models by initiating attacks and determine the model’s tendency to leak information.
    This helps us to act as adversaries and detect whether each data instance actually
    belongs to the required dataset. Training the model against such attacks can be
    accomplished in two ways:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '**White box**: By observing the model’s parameters when the model is deployed
    in an untrusted cloud or takes part as one of the participating models in a **Federated
    Learning** (**FL**) setup'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Black box**: By fetching the model’s predictions from the output'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During model training and evaluation, the attack accuracy is evaluated on a
    validation/test set. Moreover, the accuracy is only considered on the best-performing
    attack model of all attack models. In *Figure 2**.3*, we can see three plots that
    illustrate the probabilities (in the range of 0 to 1) of responses that actually
    respond to an attack based on membership status.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Overall privacy risk (left) and privacy risk for classes 24
    and 35 (center and right)](img/Figure_2.03_B18681.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Overall privacy risk (left) and privacy risk for classes 24 and
    35 (center and right)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'With the following code, we are able to detect the trade-off between the model’s
    achieved accuracy (correct identification of members in the training dataset)
    and error (incorrect identification or false positives). The following code snippets
    show how to invoke attack models and verify the probability of each member getting
    discovered through an adversarial attack:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The method for starting the attack is shown as follows, where the first two
    parameters specify the target training model and target attack model, the third
    and fourth parameters denote the training and attack datasets, while the remaining
    parameters are used to specify layers, gradients, model name, and more:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In addition, we are also able to see the privacy risk histograms for each output
    class. While the first histogram shows that there is an increase in risk at every
    step for training data members, the privacy risk for class 24 is more uniformly
    distributed between 0.4 and 1.0\. On the other hand, the privacy risk for class
    35 is more skewed between 0.85 and 1.0 for most of the training members. The overall
    privacy risk histogram is an average aggregation of all privacy risk classes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Model training and development
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This phase, as shown in *Figure 2**.4*, pertains to all actions during model
    training and deployment where the adversary has started to extract model and system
    parameters and constraints to their advantage, evading defense frameworks in the
    target environment and preparing the ground for continued attacks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Different attack stages during model training and deployment](img/Figure_2.04_B18681.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Different attack stages during model training and deployment
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Execution
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different command and script interpreters can be used by adversaries to execute
    commands, scripts, or binaries by embedding them as payloads to mislead and lure
    victims. Container administration commands and container deployments (with or
    without remote execution) can help adversaries to execute commands within a container
    and facilitate container deployment in an environment to evade defenses. Adversaries
    can also be prompted to schedule jobs for the recurrent execution of malicious
    code or force users to undertake specific actions (for example, opening a malicious
    file) to execute malicious code.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Execution can be accomplished in the following ways.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '**User execution – unsafe ML artifacts**: Adversaries may develop unsafe ML
    artifacts (without adhering to serialization principles) that can enable them
    to gain access and execute harmful artifacts.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploitation for client execution**: Adversaries may exploit vulnerabilities
    in client software by leveraging browser-based exploitations, inter-process communication,
    system services, and native APIs (and their hierarchy of interfaces) in their
    favor to enforce the execution of malicious content.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software deployment tools**: After gaining access to an enterprise’s third-party
    software, it becomes easier for adversaries to gain access to and wipe information
    from hard drives at all endpoints.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to the commonly used defense mechanisms that we examined in the
    first phase, defense strategies should focus on enforcing limits on harmful operations
    such as the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Limiting access to resources over the network (enabling authenticated local
    and secure port access to aid communication with APIs over TLS), privileged account
    management (not allowing containers or services to run as root), behavior prevention
    on endpoints, execution prevention (by using application control logic and tools
    such as Windows Defender Application Control and AppLocker, or software restriction
    policies), code signing, application isolation, and sandboxing.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When adopting system-level security measures, DevOps and security teams should
    wisely use and manage the operating system's configuration management (forcing
    scheduled tasks to run under authenticated accounts instead of allowing them to
    run under system services).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Active Directory configuration to reinforce Group Policy enforcement to isolate
    and limit access to critical network elements.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To further curb execution operations practiced by adversaries, the following
    persistence actions should be enforced by system administrators to prevent unwanted
    intrusion.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Persistence
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here is a list of the actions to prevent intrusion:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Preventing the execution of code that has not been downloaded from legitimate
    repositories (which means ensuring only non-vulnerable applications are allowed
    to have `setuid` and `setgid` bits set)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Privileged account management (don't allow users to be unnecessarily added to
    the admin group)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restricting file and directory permissions
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restricting library loading through permissions and audits
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User account control (using the highest enforcement level, leaving no room for
    bypassing access control)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defense evasion
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Defense evasion comprises all operations used by adversaries that enable them
    to evade detection and the existing security controls. Adversaries are powerful
    enough to break through the victim’s systems with the untrusted activities listed
    in *the* *following table:*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '| **Item** **No.** | **Mode of** **Defense Evasion** |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| 1 | Uninstall/disable security software; elevate privilege rights; evade
    virtualizations/sandboxes; hide the presence of programs, files, network connections,
    services, and drivers; execute malicious code; practice reflective code loading
    into a process to conceal the execution of malicious payloads; obfuscate and encrypt
    code/data (use XSL files to embed scripts). |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| 2 | Trusted processes can work in the adversaries’ favor to help them hide,
    conceal their malware, and manipulate feature artifacts in such a manner that
    they appear to be legitimate actions. Bypass and impair existing signature-based
    defenses by either running proxying execution of malicious code or deploying a
    new container image that has malware without any security, firewalls, network
    rules, access controls, or user limitations. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| 3 | Carry out process or template injection; execute scripts to hijack code
    flow; modify authentication processes, cloud compute infrastructure, registries,
    system images, file and directory permissions, network boundary bridging (taking
    control of network boundary devices and allowing the passage of prohibited traffic),
    and Active Directory data (including credentials and keys) in the target environment.
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – Different modes of ML model defense evasion
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'As defense evasion relies on the abuse of system failures, stringent security
    measures should be put in place to close all loopholes. Most of the defensive
    tactics described previously apply here. In addition, prevention techniques that
    need greater attention are the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Deploying network monitoring tools to filter network traffic.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying antivirus and antimalware detectors for monitoring.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employing endpoint behavioral anomaly detection techniques to stop the retrieval
    and execution of malicious payloads.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating systems should be configured such that administrator accounts are
    not enumerated and do not reveal account names.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When not in use, active macros and content should be removed from programs to
    mitigate risks arising from the execution of malicious payloads.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unnecessary scripts should be blocked, passwords should be encrypted, and boot
    images of network devices should always be cryptographically signed.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovery
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The discovery phase helps adversaries gain knowledge of the victim’s account,
    operating system, and configuration (as listed in *Table 2.2*) for systematic
    planning prior to invading the victim’s systems.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '| **Item No.** | **Discovery Mechanisms** |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| 1 | Browser data (for information related to banking sites, interests, and
    social media), a list of open application windows, network information (configuration
    settings, such as IP and/or MAC addresses), programs, and services (peripheral
    devices, remote programs, and file folders) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| 2 | System (location, time, and owner), cloud infrastructure (instances,
    virtual machines, and snapshots, as well as storage and database services), dashboards,
    orchestration/container services, domain trust relationships, Group Policy settings
    (identifying paths for privilege escalation), and other information related to
    connection entry points |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| 3 | Quickly altering the malware and disengaging from the victim’s system
    to hide the core functions of the implant |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: Table 2.2 – Different discovery mechanisms
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to adversarial pre-planning, the foremost defense steps include the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Enable monitoring of all events together, without viewing any suspicious action
    in isolation. Sequential information discovery and collection are part of a larger
    attack plan, such as lateral data movement or data corruption.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovery and proof collection (using screenshots and keyboard inputs), which
    could help in the process of reconciliation to justify acts of data stealing.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collection
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After the discovery of data sources, an adversary will be enthusiastic to collect
    and steal (exfiltrate) confidential sensitive information either manually or through
    automated means. Common target sources include various drive types, removable
    media, browser sessions, audio, video, emails, cloud storage, and configuration.
    Other sources of information include repositories, local systems, network shared
    drives, screenshots, audio/video captures, and keyboard input.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: ML model live in production
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This phase shown in *Figure 2**.5* relates to attacks performed on ML models
    and ML SaaS platforms at scale. Here, the adversary is fully equipped with full
    system-level information, data, and proxy models that are essential for them to
    execute attacks and impact the victim’s business operations.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF5 – Different attack stages when ML models are live in production](img/Figure_2.05_B18681.jpg)"
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Different attack stages when ML models are live in production
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: This phase involves the manipulation, interruption, or destruction of data by
    adversaries to compromise system integrity and disrupt business operations. Attacks
    can range from data tampering activities to techniques involving crafting adversarial
    data, to restrict ML models from yielding the right predicted results.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Staging ML model attacks
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the discovery and collection phases are over, the adversary leverages
    their new knowledge to plan and attack the system intelligently (online or offline)
    by training proxy models and triggering poisoned attacks by injecting adversarial
    inputs into target models. Target models act as important resources in staging
    attack operations:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '**Collecting ML artifacts**: Once the adversary has successfully gathered information
    on the ML artifacts that exist on the network, they may exfiltrate them for immediate
    use in staging an ML attack. To mitigate risks involving the collection of model
    artifacts, note the following:'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ML model training methodology should encompass all privacy-preserving techniques.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, all ACL rules (of related microservices) and encryption logic should
    frequently be audited and revisited.
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training proxy ML models**: Adversaries often train ML models to create proxy
    models and trigger attacks on the target models in a simulated manner. This offline
    simulation helps them to gain information from target models and validate and
    initiate attacks without any need for higher-level access rights or privileges.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replicating ML models**: Here, an adversary replicates the target model as
    a separate private model, where the target model’s inferences are recorded as
    labels for training the offline private version of the model. This kind of operation
    involves repeated queries to the victim’s model inference APIs. To throttle repeated
    requests from the same IP, defenders can use rate limiting and blacklist source
    IPs to limit such queries and consequently the number of inferences extracted.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Poisoning ML models**: Adversaries can generate poisoned models from the
    previous steps by injecting poisoned data for training or carefully retrieving
    the model inferences. In fact, poisoned models are a persistent artifact at the
    victim’s end that the adversary can use to their advantage to insert and trigger
    backdoor triggers with completely random patterns and locations to evade backdoor
    defense mechanisms, making it difficult for monitoring tools to detect issues
    and raise alerts.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One mechanism of defense against poison attacks is to use spectral signatures
    ([https://proceedings.neurips.cc/paper/2018/file/280cf18baf4311c92aa5a042336587d3-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/280cf18baf4311c92aa5a042336587d3-Paper.pdf)),
    which should detect the deviations in average value created by the minority sub-population
    of poisoned inputs. This algorithm depends on the fact that the means of two sub-populations
    are fairly different in comparison to the overall variance of the populations,
    owing to the fact these two sub-populations show either the presence or absence
    of correctly labeled samples or corrupted samples. In such a scenario, one population
    sub-group containing mislabeled corrupted inputs can be identified using `SpectralSignature`
    defense employed on a Keras classifier, which returns a report (a dictionary containing
    an index of keys and values as the outlier score of suspected poisons) and `is_clean_lst`,
    denoting whether each data point in the training data is clean or poisoned:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Verifying attack**: This action helps an adversary to plan, prepare, and
    verify planned attacks based on the suitability of the time chosen and the availability
    of the victim’s physical or virtual environments. Mitigation strategies against
    this type of attack are difficult to implement as adversaries can leverage inference
    APIs with limited queries or create offline versions of the victim’s target model.
    This leads to increased API billing costs for the victim, as API costs are directly
    borne by them.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Crafting adversarial data**: Adversarial data that serves as input to ML
    models can be misclassified, increase energy consumption, or make the model prone
    to failure. White-box optimization, black-box optimization, black-box transfer,
    and manual modification are population algorithms that can help adversaries to
    generate input data samples to evade ML architecture. The key purpose of adversaries
    is to disrupt the ML models by challenging their integrity.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Command-and-control requests
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These operations help adversaries move one step closer to extracting useful
    information from the victim’s network, as listed in the following table.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '| **Item No.** | **Modes of** **Control Operations** |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| 1 | Use removable media (for example, by initiating communication between
    the host and other compromised services on the target network), utilize uncommonly
    used port-protocol pairs, or deploy authenticated web services. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| 2 | Mix the commands with existing traffic, encode/obfuscate the requests
    over encrypted/fallback (when the primary channel is inaccessible)/multi-stage
    obfuscation channels to trigger commands, and dynamically establish connections
    with the target infrastructures. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: Table 2.3 – Different modes of control operations
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the adversary is clever enough to do this in such a way that the existing
    defense strategies on the target network will not raise an alarm. Therefore, some
    appropriate defense techniques are as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the adoption of specially crafted adversarial protocol tunnels, hiding
    open ports through traffic signaling, and making use of proxies can help to avoid
    direct communication between a command-and-control server and the victim’s network.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of different application- and network-level authentication and application
    sandboxing mechanisms, discussed previously, is highly recommended.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, network segmentation by properly configuring firewalls for existing
    microservices, databases, and proxies to limit outgoing traffic is essential.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only authorized ports and network gateways should be kept open for hosts to
    establish communication over these authorized interfaces.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exfiltration
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data exfiltration can be carried out by adversaries (as listed in *Table 2.4*)
    over the network, after data collection, encryption, compression, and packaging.
    Data can be packaged and compressed to different-sized blocks before being transmitted
    out of the network using a command-and-control channel or alternative channel
    strategies.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '| **Item No.** | **Modes** **of Exfiltration** |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| 1 | Automated exfiltration (unauthorized transfer of information collection),
    exfiltration over alternate protocols (relying on different protocols, such as
    FTP, SMTP, HTTP/S, DNS, or SMB instead of the existing command-and-control channel),
    and exfiltration over an existing command-and-control channel (over time for defense
    evasion) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| 2 | Network medium (Wi-Fi connection, modem, cellular data connection, Bluetooth
    or another **Radio Frequency** (**RF**) channel, etc.) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| 3 | Physical medium (removable drive) or web service (SSL/TLS encryption)
    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| 4 | Scheduled transfers that move data to cloud accounts |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: Table 2.4 – Different modes of exfiltration
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common and easiest ways to carry out exfiltration attacks are by doing
    the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '**Inferencing ML model APIs for exfiltration**: ML model inference API access
    is the primary means for adversaries to look for ways to exfiltrate/steal private
    information from the model’s inference APIs.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To mitigate exfiltration risks, the following defense actions are mandatory:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Private data should be trained using application-level privacy techniques or
    by making the best use of hybrid security measures (application- and transport-level
    security) to protect against leakage of **Personally Identifiable** **Information**
    (**PII**).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Loss Prevention** (**DLP**) APIs can be used to detect and block the
    transfer of sensitive data over unencrypted protocols.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network intrusion detection and prevention systems can be used with network
    signatures to monitor and block malware traffic.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restricting web content access by using web proxies can help to minimize unauthorized
    external access.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evading ML models**: Adversaries can use traditional cyberattacks where adversarial
    actions can evade ML-based virus/malware detection.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Denial of Service (DDoS)**: Here, adversaries are driven by the objective
    to bring down ML systems in production by issuing a flood of requests. The requests
    may be computationally intensive, requiring large amounts of memory, GPU resources,
    and processing cycles, and can overload the productionized systems, which may
    become too slow to respond.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spamming ML systems**:Here, the adversaries increase the number of predictions
    in the output by spamming the ML system with false and arbitrary data. This impacts
    the ML team at the victim’s organization, who end up spending extra time deducing
    the correct inferences from the data.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eroding ML model integrity**: Adversaries may degrade the target model’s
    performance with adversarial data inputs to erode confidence in the system over
    time. This can lead to the victim organization wasting time and money attempting
    to fix the system.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Harvesting cost**: This is similar to a DDoS attack, where adversaries engage
    themselves in targeting the victim’s ML services to increase the compute and running
    costs by bombarding the system with false and specially crafted queries. Sponge
    examples are specially crafted adversarial inputs, designed to increase processing
    speed and energy consumption, which can degrade the overall performance of the
    victim’s systems.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML IP theft**: Here, adversaries steal intellectual property from ML models,
    training and evaluation datasets, and their related artifacts with the objective
    of causing economic harm to the victim organization. This act enables adversaries
    to have unlimited access to their victim''s service free of cost, avoiding the
    MLaaS provider’s API charges.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System breakdowns**: Other than the commonly used mechanisms, impact strategies
    (the third attack strategy shown in *Figure 2**.5)* are mainly targeted at systems
    in production and include a variety of irrecoverable data destruction mechanisms
    such as overwriting files and directories with random data, manipulating data,
    defacement, wiping data, corrupting firmware, large-scale data encryption on target
    systems to disrupt the availability of system and network resources, commands
    to stop the service, system shutdown/reboot, and resource hijacking with the objective
    of bringing down the victim’s system resources.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ideal way to mitigate risks related to impacts is to follow these best
    practices:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Have a data backup process to protect against any data loss/modification attempts.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have model robustness test strategies in place by thoroughly testing ML models
    against sponge attacks.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Worst-case or low threshold boundaries to validate model robustness can also
    aid in detecting adversarial attacks, where system-level degradations in performance
    are a symptom of inputs from external sources not designed for the system.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Up to now, we have discussed the attack threat matrix, which we first saw in
    [*Chapter 1*](B18681_01.xhtml#_idTextAnchor014), *Figure 1**.13*, and different
    defense mechanisms available for different types of adversarial attacks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look into the data anonymization and encryption techniques available
    to protect sensitive data.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Anonymization and data encryption
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the possibility of different attacks and threats, organizations have
    become more responsible about safeguarding the data rights of their employees.
    The Data Breach Survey of 2019 revealed that 79% of CIOs were convinced that company
    data was put at risk in the previous year because of actions by their employees
    ([https://www.grcelearning.com/blog/cios-increasingly-concerned-about-insider-threats](https://www.grcelearning.com/blog/cios-increasingly-concerned-about-insider-threats)).
    The data security practices of as many as 61% of employees put the company at
    risk, which led organizations to adopt best practices related to data anonymization.
    Some of the practices that organizations should follow to comply with GDPR and
    other regulations will be discussed in this section.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Data anonymization or pseudo-anonymization needs to be carried out on PII, which
    mainly includes names, ages, **Social Security Numbers** (**SSNs**), credit card
    details, bank account numbers, salaries, mobile numbers, passwords, and security
    questions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: In addition to this, company policy and database administrators can define extra
    processes before the application of anonymization techniques. Now, let's look
    at some of the most commonly used techniques for data anonymization.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Data masking
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This technique hides and protects the original data by generating mirrored
    versions of it at random and then shuffling it with the original version of the
    data. There are five primary types of masking measures that make it difficult
    for the attacker to decipher the original data:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '**Deterministic data masking**: This process allows the replacement of any
    columnar value with a specific value in any location of the table – be it the
    same row, the same database/schema, or between instances/servers/database types.
    It takes into consideration similar settings to generate replacement global salt
    keys (these are cryptographic elements that hash the data for security; for example,
    a website’s cookies). For example, XYZ can be replaced with ABC.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic Data Masking** (**DDM**): The objective of this masking technique
    is to mask real-time production-grade data such that live data streams are modified
    without the data generator/requestor having access to the sensitive data. It can
    be used by setting the central data masking policy to mask sensitive fields with
    full or partial masking functions, along with random masking for numeric data.
    It also finds heavy usage in simple transact SQL commands (one or more SQL commands
    grouped together, that can be committed to a database as a single logical unit
    or rollback) SQL commands (for example, on SQL Server 2016 (13.x) and Azure SQL
    Database). Now, let’s look at an example of how masking is done in Azure:'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here, the `Email` method uses masking to expose only the first letter of an
    email address and the constant suffix `.com`, producing the following: [aXXX@XXXX.com](mailto:aXXX@XXXX.com).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: However, dynamic masking cannot be applied to encrypted columns, file streams,
    `COLUMN_SET`, or computed columns that have no dependency on any other columns
    with a mask.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '**On-the-fly data masking**: This process is common when data from development
    environments is masked without the use of a staging environment due to factors
    including insufficient extra space, or under the constraint that the data must
    be migrated to the target environment. This masking technique is used in Agile
    development processes where **Extract, Transform, Load** (**ETL**) is directly
    able to load the data into the target environment without creating backups and
    copies. However, the general recommendation is to refrain from using this technique
    widely (other than in the initial stages of the project) to avoid risks related
    to compliance and security issues.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Julia Gee` to `NULL Fhjoweeww` and `andwb@yahoo.com` to `yjjfd@yahoo.com`
    respectively.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Synthetic data**: Synthetic data is a data anonymization technique employed
    to preserve the statistical properties of the original dataset, with a considerable
    margin of variable privacy gain and unpredictable utility loss. The increased
    privacy provided by this method offers protection against privacy-related attacks
    and prevents the re-identification of individuals. The synthetic data generated
    from generative models (for example, using deep learning techniques to generate
    deep fakes, where synthetic data recreates fake images resembling the originals)
    ensures high utility by enabling similar inferences as the original data.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data swapping
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This procedure shuffles and rearranges data to completely break the similarity
    between the original and the resultant datasets. There are three popular data-swapping
    techniques:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: K-anonymity
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L-diversity
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T-closeness
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The techniques can all be used to make the deanonymization of data difficult
    for any intruder.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Data perturbation
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This data anonymization principle adds noise to numerical data in databases
    to ensure its confidentiality. The process of adding or multiplying random noise
    additive, multiplicative, or random noise (used in Gaussian or Laplace distribution)
    helps to distort data, protecting it from being parsed by an attacker.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Data generalization
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This method makes the data less identifiable by allowing you to remove certain
    ranges or portions of data (for example, outliers) from the database. One example
    is replacing `age 45 years` with `<= 45`, where the value is replaced by a wider
    range that is still semantically consistent. Data portioning of this type or attribute
    assignment to specific categories serves as a measure to generalize the data.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Broadly, generalization can be applied at the full domain level or to individual
    sub-domains. In the former, data transformation takes place for a generic domain
    or level of the hierarchy, while in the latter the level of generalization occurs
    on different subsets of the same domain.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Generalization-based techniques can be further applied to categorical or discrete
    numeric attributes to keep them private. There are two primary means by which
    generalization can be applied: a *hierarchy-based approach* or a *partition-based
    approach* (where a structure or order of partition needs to be established on
    the data items, before running the partition scheme on continuous numerical attributes).
    The partition-based approach partitions the data items into ranges, while hierarchy-based
    generalization requires the existence of generalization hierarchies and can be
    used for categorical and discrete numeric attributes. Some of these generalization
    techniques are described in the following sections.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: K-anonymity
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sweeney first proposed the K-anonymity principle, which can be used as a framework
    to evaluate algorithms that carry sensitive information ([https://epic.org/wp-content/uploads/privacy/reidentification/Sweeney_Article.pdf](https://epic.org/wp-content/uploads/privacy/reidentification/Sweeney_Article.pdf)).
    In the absence of K-anonymity, sensitive attributes can leak and reveal boundary
    limits from the information elements. The application of K-anonymity is aimed
    at transforming the protected elements (either by generalization or suppression)
    with the objective of safeguarding the dataset from the hands of an intruder.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: K-anonymity ensures any generalized block consisting of an individual record
    cannot be differentiated from K-1 other individuals in terms of any set of quasi-identifiers
    (such as zip code, age, or gender). This anonymization principle helps protect
    against linkage attacks, as discussed in [*Chapter 1*](B18681_01.xhtml#_idTextAnchor014).
    We see that in a linkage attack, an attacker is able to reveal the identity of
    a victim (unique identification number, credit card number, or others) and combine
    this with other background information, such as a user's travel details like source,
    destination, and means of travel. This combined information can assist an adversary
    in tracking the user's entire whereabouts. For example, a value of 10 for K on
    the protected attributes of age, **SSN**, nationality, salary, and bank account
    details will produce a minimum of 10 different records for each combination of
    the defined attributes.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: As *Figure 2**.6* illustrates, with K = 2, there are two records listed as Asian
    and two as Hispanic. We can see that an equivalent number of records are distributed
    with respect to sensitive attributes such as age and salary across the two races.
    *Figure 2**.6* provides a complete demonstration of an end-to- end pipeline, showing
    risk score assessments, information recognition, pseudonymization, and anonymization,
    to audit and store sensitive data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF6 – Different \uFEFFanonymity models – K-anonymity, L-diversity,\
    \ and T-closeness\uFEFF](img/Figure_2.06_B18681.jpg)"
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – Different anonymity models – K-anonymity, L-diversity, and T-closeness
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: The limitation of this system is that if an adversary has any information about
    the victims beforehand (say, their age), then it would be much easier for them
    to identify other attributes, such as salary- and disease-related information.
    For example, a friend of Nancy's may know the age group and race to which she
    belongs, which would enable her friend to guess and retrieve her salary. The presence
    of homogeneous salary or age groups makes it possible for the adversary to derive
    sensitive information by the process of elimination or negative disclosure. It
    becomes easier for an adversary to initiate their attacks successfully and with
    high confidence. As a result, improvements for K-anonymity have been suggested
    to prevent background knowledge and homogeneity-based attacks.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: L-diversity
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The L-diversity principle (defined by Machanavajjhala et al.) was designed to
    handle the drawbacks of the K-anonymity algorithm to reduce the probability of
    homogeneity and background knowledge attacks. The L-diversity principle provides
    privacy where the data publisher is not aware of the knowledge that is possessed
    by the adversary. The fundamental idea behind L-diversity is the requirement that
    each group sees an equivalent of the sensitive values. The known information can
    be modeled as a probability distribution by applying Bayesian inferencing to reduce
    the granularity of the base data representation. The primary objective of generalization
    is to obtain different values for sensitive data that is evenly distributed.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: The major limitation of this algorithm becomes visible when the distribution
    is skewed and fails to achieve an entropy of uniformly distributed L distinct
    sensitive values for each equivalence class. In such a case, the overall entropy
    level of the table drops, and the algorithm becomes non-functional and cannot
    offer sufficient protection.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Even though this algorithm takes into consideration the diversity of sensitive
    values within each group, it fails to constrain the value ranges and boundaries
    of the diverse groups. Given features with very close boundaries (say, salary
    > US $20,000 versus salary > US $30,000), the attacker can retrieve the salary
    information from the two equivalence classes (similar classes with close boundaries)
    that have different age groups.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: T-closeness
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As K-anonymity and L-diversity together cannot safeguard against skewness and
    similarity attacks, T-closeness came into being to offer extra robustness to the
    anonymization framework. This principle states that the two distributions – one
    of which is the sensitive value distributions of any group (say, racial groups;
    Asian and Hispanic in our example in *Figure 2**.6*) and the other is the overall
    sensitive value distribution – cannot differ by more than a threshold of `t`.
    The distance metric used to evaluate the difference between the two distributions
    is the **Earth-Mover Distance** (**EMD**). It is computed using the possible set
    of sensitive attributes by evaluating the maximum distance between them. The metric
    gives a measurement between 0 and 1, in the space normalized to 1, with the intention
    of including the semantic closeness of attribute values in addition to the generalization
    of quasi-identifiers and the suppression of sensitive values.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Though T-closeness can resolve the shortcomings of K-anonymity and L-diversity,
    by offering protection from homogeneity, background knowledge, skewness, and similarity
    attacks, it remains vulnerable to minimality and composition attacks. In the former,
    the attacker deduces the boundary condition or the minimality criteria beyond
    which the anonymized models (K-anonymity, L-diversity, and T-closeness) cannot
    provide sufficient protection against external threats. Using this information,
    the intruder can decipher sensitive information for some of the equivalence classes.
    In the second category of attack, the attacker exploits the availability of all
    different releases of anonymized datasets to integrate them into one single unit,
    where the unified dataset is used to breach individual privacy.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Among other, less popular anonymized models is the p-sensitive K-anonymity model,
    which ensures K-anonymity conditions on `p`. In addition, it also stops learning
    sensitive associations at the same time. We can denote an anonymity model as (∊,
    `m`) (with the frequency of sensitive attribute values ∊ remaining within the
    user-defined threshold in the equivalence classes). This was designed to protect
    sensitive numerical attributes from proximity breaches, where data boundaries
    are too close. This allows an attacker to gather information with high confidence
    that a sensitive attribute value falls in a specified interval.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Encryption
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Encryption of personal data is a methodology used to protect data from external
    sources and limit access to users who are not authorized to access it. Three major
    data encryption schemes of symmetric encryption, asymmetric encryption, and hybrid
    encryption can be employed for the purpose of key generation, registration, usage,
    storage, monitoring, rotation, and deletion.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '**Symmetric encryption** techniques such as **Advanced Encryption Standard**
    (**AES**) are fast and can be accelerated by processors for bulk encryption purposes.
    This procedure is dependent on the pre-sharing/exchange of a single key between
    the client and the server to be used for encryption and decryption purposes. To
    provide support for integrity and authentication, a message authentication code
    can be added on top of this.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '**Asymmetric encryption** schemes such as **Rivest**, **Shamir**, **Adleman**
    (**RSA**), **Digital Signature Algorithm** (**DSA**), and **Elliptic Curve Cryptograph**
    (**ECC**) use two keys, one public and the other private, and offer strong protection
    against adversaries. However, the procedure is slow and has limited applications
    for protecting data in ML systems. The communicating parties are responsible for
    secretly storing their private keys, while the public keys are shared. This happens
    over the following steps:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Using **Transport Layer Security** (**TLS**), the sender and receiver finalize
    the symmetric key (session key) that is used to encrypt data sent by the sender.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The sender is responsible for encrypting the symmetric key with the receiver’s
    public key.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The symmetric key is decrypted by the receiver with their own private key.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the symmetric key, the data is decrypted by the receiver.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we’ve had an overview of some encryption techniques, let's look at
    another level of abstraction process with pseudonymization, which further increases
    the difficulty for adversaries to break the encryption key.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Pseudonymization
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This method helps to anonymize data while preserving accuracy in statistics
    and value. Different types of encryption and hashing techniques can be used for
    this process. PII information (quasi-identifiers) is encoded with pseudonyms and
    preserved separately, which allows the easy re-identification of the original
    data through the use of cross-references or identifiers. In contrast to the anonymization
    technique, this procedure prevents permanent data replacement using a substitution
    principle. Pseudonymization can also be used for the encryption and decryption
    of PII, where the original data is translated into ciphertext, which can be reversed
    with the relevant decryption key.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: The main disadvantage of this process is that when datasets contain billions
    or trillions of records, human review and re-assessment or re-identification becomes
    impossible. Moreover, when the most sensitive fields require precise values, we
    may miss potential sources of re-identification attacks due to the privacy-utility
    trade-off.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: This process further allows vertical or horizontal distribution of PII (by storing
    it in some protected storage units) and maintaining a link between the identifiers.
    In a vertical data distribution system, pseudonymity is guaranteed by compartmentalizing
    individual data corresponding to different subsets of sensitive attributes in
    different sites. In a horizontal data distribution system, sites near user locations
    are responsible for storing data with the same sets of user attributes. One primary
    example of horizontal data distribution is health-related information, where data
    integration from different sources is a preliminary step to infer an individual’s
    health-related information.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common types of pseudonymization techniques are as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '**Encryption with secret key**: PII can be encrypted and then decrypted by
    the owner using any of the symmetric or asymmetric encryption methods discussed
    in previous sections.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hash function**: This function is applied to transform a dataset with variable
    feature attributes and data size to yield a fixed-size output. This method often
    runs the risk of revealing sensitive PII if the range of input values applied
    to the hash function is known beforehand. A better method of adding higher-order
    protection is to use a salted-hash function, which adds randomization and reduces
    the probability of an attacker retrieving the actual values.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hash function with secret stored key**: The hash function used for transformation
    is supplied with a secret key to the input, which further reduces the probability
    of an attacker being able to retrieve the actual data by replay attacks, where
    an attacker intercepts the network. The data owner will still be able to retrieve
    the data with the secret key, but for the attacker it becomes increasingly difficult
    to compute all sorts of permutations and generate the actual key used for encryption.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hash function with deletion of secret key**: This process involves the selection
    of a random number corresponding to every feature in the dataset to act as the
    pseudonym. Further, the correspondence table is deleted to reduce the probability
    of linkage attacks where an attacker can link the personal data of individuals
    from the dataset in question to other available datasets with other pseudonyms.
    It makes it even more difficult for an attacker to use all permutations for a
    non-existent secret key to replay the function and retrieve the original data.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenization**: This method of data encryption enables the conversion of
    sensitive data with a randomly generated token. It is widely prevalent in the
    payment card industry in credit cards, wallets, and other applications involving
    unique identifiers (PAN cards, driving licenses, SSNs, etc.). Organizations dealing
    with such secret identifiers often employ a tokenization service that is responsible
    for payment authorization by generating and validating a token based on users’
    identities. The generated token can then be stored and mapped in the database
    of the third-party service provider. The storage service reduces the risk of data
    loss by providing unlimited data retention capacity. The involvement of third-party
    providers helps to reduce issues related to **Payment Card Industry Data Security
    Standard** (**PCI** **DSS**) compliance.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Homomorphic encryption
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Homomorphic encryption** (**HE**) has gained a lot of prominence in the data
    security industry, especially in the design of high-grade cloud security applications,
    owing to the amount of protection guaranteed by this technique. It relies on a
    probabilistic asymmetric algorithm and adds an extra protective layer where parties
    who do not own the encryption and decryption keys are unable to decipher the encrypted
    data. The primary advantage of this method over traditional encryption methodologies
    is that it allows cloud providers to process the already encrypted data without
    having to decrypt it first. The processed results are available in encrypted form
    to the owner, who can retrieve the processed results with a decryption key.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: HE follows multiplicative laws of encryption and computation, where the order
    of encryption and computation can be interchanged. Any computation, when applied
    on datasets *a* and *b* after they have been encrypted individually as *E*(*a*)
    and *E*(*b*), yields the same result as it would have when encryption is applied
    to the computed result *E*(*a ** *b*).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Hence, mathematically, *E*(*a * b*) = *E*(*a*) * *E*(*b*), where *E*(*a*) and
    *E*(*b*) refer to the encryptions applied to datasets *a* and *b,* respectively,
    and *E*(*a * b*) refers to the encryption applied on the resultant computations
    of *a* and *b*.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: As they are equal, the decrypted values of *E*(*a * b*) and *E*(*a*) * *E*(*b*)
    are also equal.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.7* illustrates a use case of HE that helps to achieve anonymous
    data processing for (a) single users and (b) multiple users seeking to process
    their encrypted (using their public key) sensitive data on a cloud server. In
    the first case (a), the individuals can directly obtain decrypted results as computed,
    evaluated, and returned by the server. In the second case (b), the server can
    aggregate the individual data by stripping off the identity information, processing
    their encrypted data, inferring statistical information, and sending the inferred
    data back to a third-party source that is responsible for collecting the final
    derived outcome. The data received by the third-party source can then decrypt
    the data and obtain the inferred results.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF7 – Single\uFEFF- or multi-party \uFEFFHE](img/Figure_2.07_B18681.jpg)"
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – Single- or multi-party HE
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Such multi-party agents using HE can be used to efficiently run electronic voting
    systems. Individual voters can use their public keys to encrypt their ballots
    and send them to the voting server. The cloud server makes use of homomorphic
    evaluation to run extra validity checks on encrypted ballots and compute an aggregated
    encrypted result. The computed encrypted result can be sent to the organizers,
    who can then decrypt and deduce the overall voting results without having any
    knowledge of how specific individuals cast their votes.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.8* demonstrates the change in approach to privacy using HE in comparison
    with traditional privacy approaches. In the traditional privacy mechanism, users
    were dependent on relevant cloud storage and its computation facility for data
    transmission, processing, and storage functionalities. The customers needed to
    establish added trust with the service provider, where the providers are not allowed
    to share private information with third parties without the customer’s consent.
    In the present day, HE-based privacy systems (for example, Microsoft’s SEAL platform)
    guarantee confidentiality with full protection of customer data in addition to
    encrypted storage and computation capabilities.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF8 – A diagram showing traditional encryption versus \uFEFF\
    HE](img/Figure_2.08_B18681.jpg)"
  id: totrans-288
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – A diagram showing traditional encryption versus HE
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: 'This methodology can also be efficiently used in the following areas of security
    involving cloud services:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Private storage and computation
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Private prediction services
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosted private training
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Private set intersection
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure collaborative computation
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code demonstrates instantiating a `Pyfhel` object (with public
    and private keys) at the client end, with the generation and saving of public
    and private key pairs within the context. The `contextGen` function is used to
    generate an HE context based on input parameters. We save the public key along
    with the context:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Then we try to decrypt the encrypted values (`a` and `b`) in the cloud using
    `PyCtxt`. But the decryption process fails in the cloud, as demonstrated in the
    output. The cloud server then applies the mean of the encrypted values and sends
    it back to the client. But the client successfully decrypts the results and can
    obtain the mean value (the mean of 1.5 and 2.5 is 2).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'The cloud server tries to decrypt and then apply a mathematical mean operation
    on the encrypted results sent by the client:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The cloud computes the mean value of the ciphertexts obtained in the previous
    steps:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The client loads and decrypts the result:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in the following output:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let’s see when this might be a useful technique.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Applications of HE
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'HE finds widespread usage in scenarios where there is a demand for huge computational
    capacity for processing high volumes of sensitive data not available to users.
    Some useful practical applications are predictive and analytics use cases in genomics
    research, finance, healthcare, pharmaceuticals, government, insurance, manufacturing,
    and the oil and gas sector, as illustrated in the following list:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging cloud services for backtesting stock market trading strategies in
    such a manner that the data remains secure and private from external systems,
    including attackers and cloud operators
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backtesting
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Backtesting is a strategy that allows a trader to simulate a trading strategy
    using historical data to generate results and accordingly identify and analyze
    the risk and profitability before risking any actual capital.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: ML applications related to fraud detection, automated claims processing, and
    threat intelligence
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML-based SaaS platforms analyzing DNA with DNA sequencing classifiers to provide
    predictive insights to medical institutions and hospitals
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML-based SaaS platforms providing medical diagnosis, medical support systems
    such as healthcare bots, and preventive care
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML-based complex design and architectural patterns to innovate and run novel
    algorithms; they can also be used to run a mechanical structural analysis for
    the aerospace or construction industry
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive platforms for running secure auctions
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, there are certain limitations on the use of HE, such as encrypted search
    (returning search results to the query without learning about the response) on
    a database or spam-filtering encrypted emails. In the former scenario, it is not
    feasible for the server to determine the search query content, which would entail
    huge processing loads and costs on the server to perform a homomorphic evaluation
    of the entire search operation. In the case of spam filtering, the procedure can
    produce a list of encrypted messages as spam, which cannot be deleted unless the
    client is aware of the filtering criteria (for example, keywords) that qualify
    a message as spam.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Secure Multi-Party Computation (MPC/SMPC)
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a security technique used in ML to train private sensitive data and
    create risk-proof ML models. Here, the participating candidates are allowed to
    perform computations on private data and evaluate the private models of one participant
    with another participant’s private data.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: This protocol works on the principle of secret sharing, where a dealer can share
    a secret *s* among *n* parties. This scheme can protect the secret from *t* or
    fewer participating candidates, and, at the same time, a subset of *t + 1* candidates
    can reconstruct the secret. The participating candidates obtain their shares in
    the output, which helps them to reconstruct the actual outputs through interpolation.
    If only a few selected candidates are configured to obtain their shares, participating
    candidates are also allowed to send shares to only relevant individuals. Let's
    now investigate some practical use cases where MPC was used to protect sensitive
    data.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Google uses MPC to evaluate advertisement conversion rates by computing the
    privacy-preserving set intersection between those users viewing an ad and those
    who go on to purchase the product. Some organizations rely on threshold cryptography
    instead of legacy hardware for protecting cryptographic keys. Here, organizations
    depend on MPC for key generation, computation, and storage, instead of allowing
    individuals to hold private information.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'This process helps organizations to protect keys from adversaries as the key
    shares are placed in different environments. What makes MPC most useful for big
    data and large-scale predictive systems is the following:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Easy adaptability to cross-platform deployment models.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A multi-tenant MPC can run as a cloud-native **Key Management** **Service**
    (**KMS**).
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be functional across multiple clouds (for example, AWS, Google, and Azure)
    simultaneously to maximize security and availability.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports hybrid cloud multi-site enterprise deployments.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy scalability.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sustained secure operations.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF9 – SMPC – A\uFEFF. Shamir’s secret sharing and B\uFEFF. Threshold\
    \ cryptography](img/Figure_2.09_B18681.jpg)"
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – SMPC – A. Shamir’s secret sharing and B. Threshold cryptography
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet demonstrates how to use the `mpyc` library to determine
    an aircraft’s location in a private manner using five sensors. Each sensor communicates
    using SMPC to share secrets for encrypting confidential information such as the
    location and time of arrival of the aircraft:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Having understood different encryption methodologies used in multi-party communications
    and in the process of ML model training, let's try to understand how we can employ
    application-level privacy techniques during the training phase of a model.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Differential Privacy (DP)
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DP is a popular application-level privacy-enabling framework used to protect
    private or sensitive data on large datasets. This method guarantees an almost
    identical output when a statistical query is executed on two nearly identical
    datasets that differ only by the presence or absence of one record.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: DP provides security against record linkage attacks by hiding the influence
    of any single record (for example, individual PII) or records of small groups
    of users in the predicted outcomes. The process of anonymization and protecting
    the availability of information related to the presence or absence of individual
    records in the data-training process is closely associated with the privacy of
    data against linkage attacks. The cumulative loss is defined as the *privacy budget*
    and is called **epsilon** (**ε**), which represents the quantifiable amount of
    privacy provided, where a low value signifies a high level of privacy. The loss
    is also associated with a decrease in utility (accuracy), and it is the task of
    the data scientist to arrive at an acceptable trade-off between the two.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.10* illustrates a **Stochastic Gradient Descent** (**SGD**)-enabled
    DP training process, the computation of the loss function, the addition of random
    noise, and the clipping of gradients in successive iterations. When examining
    the diagram, note the following:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: A randomly sampled set of data points has been used for training (from two different
    datasets, *D*1 and *D*2, both ∈ to *S*, and differing in only one record).
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training error (or training loss) is computed from the model’s predicted
    output and the training labels in successive steps are then differentiated with
    respect to the model’s parameters.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process continues iteratively where the computed gradients (using SGD) are
    applied to the model’s parameters, taking into consideration the impact left by
    every point in the resultant gradient.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The impact of every gradient is controlled by clipping (or bounding) the gradients.
    The certainty of inferring a point’s inclusion in the dataset is diminished through
    the process of randomization where noise is added to every data point.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the model converges, the final gradient is computed to derive the privacy
    estimate, such that *O*1 – *O*2 < ε.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF0 – Training models with \uFEFFDP on two input datasets\uFEFF\
    , D1 and D2](img/Figure_2.10_B18681.jpg)"
  id: totrans-344
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Training models with DP on two input datasets, D1 and D2
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet illustrates the necessary imports, including IBM’s
    `Diffprivlib`, used for training DP-based models. The first step involves having
    the necessary imports of libraries and fetching the `adult_income` dataset from
    the web, where for `X_train` we only use columns 0, 4, 10, 11, and 12, and for
    `y_train` we use the column specified as income <=50K or >50K:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The next code snippet illustrates how we can incorporate DP in different components
    of a pipeline such as `StandardScaler`, `PCA`, and `LogisticRegression`. The initial
    step involves scaling the feature attributes followed by dimensionality reduction
    and ML model classification.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'The `data_norm` parameter quantifies the Laplace-distributed random noise that
    we added in order to yield a differentially private ML model. The `bounds` parameter
    defines the bounds of the data and takes in a tuple value as *min, max,* where
    these two entries represent scalars after being aggregated at a feature level.
    From *Figure 2**.11*, it is further evident that accuracy is oscillating, with
    ε ranging from 10-3 to 10-1.5, after which accuracy becomes stable. This observation
    reinforces our discovery that selecting an acceptable trade-off value between
    accuracy and utility is needed to add a higher privacy margin without compromising
    accuracy:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, when we plot the results, we get the following graph demonstrating the
    variation of accuracy with ε:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: "![ Figure 2.1\uFEFF1 – A DP-enabled pipeline exhibiting \uFEFFthe accuracy\
    \ and privacy-budget (ε) trade-off](img/Figure_2.11_B18681.jpg)"
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – A DP-enabled pipeline exhibiting the accuracy and privacy-budget
    (ε) trade-off
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: By now, we have understood how ε serves as an important metric to measure the
    expected privacy of a DP model. Now, let's try to understand the second most important
    metric, sensitivity.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sensitivity is deﬁned as the maximum inﬂuence exerted by a single data record
    on the differential private result in response to a numeric query. For any arbitrary
    function *f*, the sensitivity *∆f* of *f*, on *x* and *y*, two neighboring datasets,
    can be given as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Δ*f* = *max* { || *f*(*x*) – *f*(*y*) ||1 }, where ||.||1 represents the L1
    norm of a vector
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Properties of DP
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DP solutions possess essential properties of postprocessing:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Invariance/robustness
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantifiability
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Composition
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The invariance/robustness of DP ensures additional computations executed on
    ε-DP solutions are also ε-DP. Additionally, the quantifiability property of DP
    allows the flexibility of being transparent (to the data scientist). Transparency
    reveals the exact quantity of noise/perturbation caused by the randomization process.
    This characteristic feature of DP algorithms gives them an extra edge over other
    traditional de-identification algorithms. The traditional algorithms hide the
    process by which the data was transformed, so data scientists cannot interpret
    and analyze the accuracy of such models. Further, with the composition property,
    it is possible to derive the amount of degradation in privacy by executing different
    DP algorithms on overlapping datasets. For example, two DP algorithms executed
    on ε1-DP and ε2-DP that overlap are DP private and given by (ε1+ε2)-DP.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: We have now become familiar with different privacy- and security-enabled measures
    that can be used during model training and inference. However, each of these methods
    has its own specific advantages, and the entire system can only get the full benefit
    from a hybrid security-enabled system.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid privacy methods and models
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure 2**.12* illustrates the integration of varying levels of the privacy
    components discussed in the previous sections to create a fully proofed privacy-preserving
    AI system. There are two different labels associated with **2** and **5**, where
    **2** and **5** denote access by model owners, while **2’** and **5’** denote
    access by adversaries where they craft adversarial data to steal important model
    information.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: The system can ingest data from multiple heterogeneous devices (**1** and **2**)
    before triggering different algorithmic training (**3**, **6**, and **8**). Such
    a hybrid system can ensure data and algorithm sovereignty, in addition to adhering
    to ethics, compliance, transparency, and the trustworthiness of applications.
    The main objective is to have a robust ethical defense framework in place that
    can protect a single data record from identity or **Membership Inference Attacks
    (MIAs)** by identifying its presence in the dataset (**2’**).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: This is addressed by having a private AI unit that encompasses the task of adding
    application-level privacy through DP (during model training or post-model convergence),
    anonymization, and pseudonymization (*Figure 2**.12*) to protect the data. Here,
    the random noise and regularization added by DP algorithms (trained with SGD or
    private aggregation of teacher ensembles) can increase resilience against inversion
    attacks.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: As we studied previously, anonymization and pseudonymization still leave room
    for de-identification processes through feature re-derivation and re-identification
    (**2’**) where an attacker is able to break into look-up tables. Therefore, additional
    security measures need to be adopted to safeguard insecure storage.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF2 – Application of different privacy measures in a hybrid\
    \ privacy framework](img/Figure_2.12_B18681.jpg)"
  id: totrans-370
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Application of different privacy measures in a hybrid privacy
    framework
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: To safeguard lookup-table-related risks, AI research has concentrated on decentralization
    where remote execution becomes the central mechanism to train a global model.
    Locally trained ML models with their weights, parameters, and data (from mobile,
    **Internet of Things** (**IoT**), and **Internet of Medical Things** (**IoMT**)
    devices) are updated to a central repository to aggregate the model at a global
    level. However, the local model’s weights still run the risk of being corrupted
    by adversaries either by the modification of transmitted parameters through poisoning
    or model-inversion/reconstruction attacks (**5’**) where SMPC and HE can be employed
    to best tackle the existing threats.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: The decentralized approach to data training in a federation topology is revolutionizing
    the privacy landscape in the AI industry as devices are able to retain their sovereignty
    while participating in the gossip strategy. The flexibility of joining in the
    training process (by limiting their continuous availability) to share model parameters
    with peer nodes helps devices to sustain battery life for longer durations. It
    gives a broader scope of application to this federated mode of training in addition
    to data and model governance capabilities, where the tracking/auditing of the
    times that each device sends model parameters, convergence, and performance metrics
    can be monitored.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '**FL** carries the underlying risk of model parameters and PII being stolen
    or reconstructed by adversaries in the absence of encryption techniques, from
    the nodes and communicating interfaces. Hence, local algorithms need to be encrypted
    and securely aggregated where HE can be employed (with or without DP) to securely
    aggregate encrypted algorithms. Another viable risk arising from neural networks
    is their compressed representation. Such compressed formats are achieved by applying
    either one of the following mechanisms: pruning the convolutional layers, quantization,
    tensor decomposition, knowledge distillation, or a combination of all the stated
    methods.Without encryption, attackers will find it easy to execute model inversion
    or reconstruction attacks and retrieve confidential model parameters with high
    accuracy.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more on knowledge distillation at *Knowledge Distillation: Principles,
    Algorithms, Applications*: [https://neptune.ai/blog/knowledge-distillation](https://neptune.ai/blog/knowledge-distillation)
    and about quantization at *Pruning and Quantization for Deep Neural Network Acceleration*:
    A Survey, [https://arxiv.org/pdf/2101.09671.pdf](https://arxiv.org/pdf/2101.09671.pdf)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: The solution is further extended to include secure multi-party computation to
    multiple participating entities where each of them receives a split of encrypted
    data to proceed with further processing. This decentralization approach removes
    the risk of complete data exposure or leakage to participating candidates, allowing
    data recovery only through the method of mutual consensus. SMC also works in semi-trusted
    and low-trust environments, but one of the requirements of this method is continuous
    data transfer between parties and the continuous online availability of devices,
    leading to additional communication overheads. This acts as a limitation on the
    reliability, redundancy, and scalability of the system, but can be overcome by
    designing appropriate sleep and wake cycles for devices, discussed more in [*Chapter
    11*](B18681_11.xhtml#_idTextAnchor232). Data scientists should employ a holistic
    approach, incorporating all the privacy measures discussed to validate the integrity
    and quality of predicted ML results.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial risk mitigation frameworks
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let's walk through some of the newly evolving risk mitigation
    frameworks concerning specific scenarios of input data distribution or model architecture
    when the model is used for training and serving. These frameworks are highly successful
    in curbing real-world attacks. One example is the identification of diseases where
    clinical datasets have been used to train the associated model. In such cases,
    an attacker can infer from a clinical record that a given patient has a specific
    disease with a high probability of success.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Let's now discuss how we can evaluate model risk for **MIAs**, where adversaries
    are able to copy the principal model functionality and trigger adversarial attacks.
    MIAs can be either black box or white box. In black-box attacks, the attacker
    knows only the model inputs and can only query the model’s predicted output label,
    whereas in white-box attacks, the attacker has knowledge of the model inputs,
    architecture, and model internals such as weights, biases, and other coefficient
    values.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: The goal of MIAs is to infer whether a given data record is in the target dataset.
    To construct the MIA model, a shadow training technique is applied to generate
    the ground truth for membership inference. *Figure 2**.13* shows an overview of
    MIAs.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: MIAs enable an attacker to determine the presence of a specific data point *z*
    in the training set of a target model *a*. When this attack takes place on a model
    trained with sensitive data, evaluating an individual’s presence in the dataset
    will expose confidential information to an attacker. MIAs achieve high performance
    on **Independent and** **Identically Distributed** data(**IID**), thereby completely
    ignoring the fact that data dependencies in training samples underestimate the
    attack performance. This suggests a new direction of research to evaluate vulnerabilities
    and devise defense techniques for correlated data to protect sensitive information.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Hence, there is a need for a risk mitigation technique that can evaluate and
    label ML models where such data dependencies exist (such as, for example, the
    effect of all members being from a specific health region (or hospital) and non-members
    from all other regions, or when both originate from the same source). To evaluate
    such associated risks, this framework could be used to access/test the model’s
    behavior under MIAs when data dependencies exist, and models are prone to correct
    membership inference outcomes. For example, with MIAs, the model is able predict
    whether people of the same racial background are likely to suffer from a disease.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: This risk assessment framework first uses public data to train a set of shadow
    models that can emulate/mimic the target model’s functionality. In the next step,
    an attack model is trained to reveal the membership status of a sample using outputs
    from the shadow models. Before the identification phase begins, the dataset can
    be split between members and non-members (for example, through a clustering algorithm).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: In the following code snippet, we have used the `adult_income` dataset to train
    a shadow model (80% training and 20% test data split) with a CNN. The shadow and
    attack models are trained using the same dataset after dividing the dataset equally
    between the two halves. The purpose of this framework is to measure the effectiveness
    of DP against MIAs. We can conduct MIAs on the best target models and evaluate
    the protection offered by DP for different values of privacy budget, noise multiple,
    and regularization.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.13\uFEFF – Measuring model robustness against MIAs with DP-enabled\
    \ training](img/Figure_2.13_B18681.jpg)"
  id: totrans-385
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Measuring model robustness against MIAs with DP-enabled training
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in *Figure 2**.13*, the shadow model we employed here depends
    on the target model’s architecture and weights. Hence, having a white-box model
    attack in place makes our job easy, as the same architecture and hyperparameters
    can be reused from the target model. The shadow model is trained on the shadow
    dataset to follow the target model and generate the ground truth data required
    to train the attack model. The shadow model’s probability output is aggregated
    with the true labels to generate the attack dataset, where input to the attack
    dataset is labeled as `in` or `out` based on the condition of whether it is used
    to train the shadow model.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'We get approximately 50% attack accuracy since the data source of the train
    and test sets remains the same. However, the attack accuracy would decrease if
    the private training dataset for the target model were not overlapping with the
    public dataset that trains the shadow model. This MIA attack model has been trained
    using `RandomForestClassifier`. Attack models could be executed to perform attacks
    *n* times:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first step is to create an instance of a shadow model using the shadow
    dataset:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the next step, we train the shadow models with the same parameters as the
    target model and generate the attack data:'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After training the shadow models, we train the attack model using `RandomForestClassifer`:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The next step involves evaluating the success of the attack. To do so, we segregate
    the data points used in the training and those that were not present during training
    for both independent and target variables:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The final step involves computing the attack accuracy of the attack test data
    by comparing the predicted outcomes of the attack model with the membership labels.
    The attack data is prepared in the expected format for `AttackModelBundle`:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Both the models are trained with DP, with `DPKerasSGDOptimizer`, which is an
    optimizer built over `SGDOptimizer` for DP. `noise_multiplier` is a parameter
    supplied to the optimizer to control how much noise is sampled and added to gradients.
    The `steps` parameter represents the number of steps/epochs the optimizer takes
    over the training data, whereas the `l2_norm_clip` parameter provides a mechanism
    to tune the optimizer’s sensitivity to individual training points, by considering
    the maximum Euclidean norm of each individual gradient from the mini-batch:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The first step is to set the optimizer as shown in the previous code snippet.
    The next step is to compute a vector of per-example loss rather than its mean
    over a mini-batch. As demonstrated in the next code snippet, we compile the model
    loss with the model classifier using Keras:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The computation of epsilon is given in the next code snippet. The sampling probability
    is computed using `batch_size` and the inverse of the input data size (which is
    the delta) and is approximately 50,000 in the input dataset. The probability metric
    represents the probability of an individual training point being included in a
    mini-batch, which is then used with the noise multiplier to evaluate `rdp` and
    finally in the computation of epsilon. This is the final model metric that can
    be used to judge the privacy guarantee by considering how much the probability
    of a particular model output can vary by including (or removing) a single training
    record sample. The concept of micro-batches was introduced in `tensorflow_privacy`
    ([https://github.com/tensorflow/privacy](https://github.com/tensorflow/privacy))
    to facilitate faster processing by providing a degree of parallelism where gradients
    no longer remain to be clipped on a per-sample basis, but rather clipped at a
    micro-batch granularity.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'This process can clip 32 gradients averaged over micro-batches, with each micro-batch
    having 8 data samples:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '*Table 2.5* represents a study on epsilon, noise multipliers, and metrics from
    attack models. When the value of the noise multiplier increases, epsilon decreases,
    which means there is an increase in the privacy budget. Though most of the attack
    model metrics remain constant, a decrease of 1% in attack accuracy is noticed
    when epsilon decreases from 1.2203 to 0.3283, which reinforces the fact that higher
    privacy increases the robustness of the model to attacks (at least to some extent).
    The attack metrics also exhibit low precision and high recall, signifying the
    presence of more false positives (records that are identified to be coming from
    the training dataset, but they are not) and correct identification of the relevant
    record’s presence in the training dataset. However, models trained with DP are
    not strong enough to provide protection from MIAs, particularly when the data
    is correlated.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '| **Epsilon** | **Noise Multiplier** | **Attack Metrics** |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| 1.2203 | 0.8 | Attack accuracy: 0.5023300438596491Precision: 0.5012701733413031Recall:
    0.9195449561403509 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| 0.6952 | 1.0 | Attack accuracy: 0.5006167763157895Precision: 0.5004499550044995Recall:
    0.6859923245614035 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| 1.2203 | 1.2 | Attack accuracy: 0.5023300438596491Precision: 0.5012701733413031Recall:
    0.9195449561403509 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| 0.3283 | 1.4 | Attack accuracy: 0.49828673245614036Precision: 0.4988314480695522Recall:
    0.731359649122807 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| 0.2523 | 1.6 | Attack accuracy: 0.49828673245614036Precision: 0.49885352655232507Recall:
    0.7454769736842105 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| 0.20230 | 1.8 | Attack accuracy: 0.49897203947368424Precision: 0.49929158401813545Recall:
    0.7245065789473685 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '| 0.16715 | 2.0 | Attack accuracy: 0.49780701754385964Precision: 0.49852643212377973Recall:
    0.7419133771929824 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: Table 2.5 – A table showing variation of epsilon and noise multiplier
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Model robustness
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model robustness is a measure of the model performance taking into account indistinguishable
    changes in the model inputs. Different perturbation techniques help us to compare
    and benchmark the ML models against their robustness metrics. The Python package
    **Foolbox** ([https://arxiv.org/pdf/1907.06291.pdf](https://arxiv.org/pdf/1907.06291.pdf))
    helps to determine model robustness by generating adversarial perturbations. This
    is built on the fact that the minimal perturbation that generates an adversarial
    sample when applied to any model input (such as an image) helps to quantify a
    model’s robustness to the pre-fed adversarial samples, and demonstrates a model’s
    susceptibility to adversarial attacks. The flexibility provided by the framework
    to apply hyperparameter tuning helps to evaluate the minimal adversarial perturbation,
    resulting in misclassification in the predicted class probabilities in the output.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'To run different attacks using this toolbox, we need an input, its label, a
    model, the adversarial criterion, and a distance parameter that measures the length
    of a perturbation, called the *L*1 norm. We can also mix and match using the composite
    model feature, where the predictions of one model can be combined with the gradient
    of another model, allowing us to initiate non-differentiable models by leveraging
    gradient-based attacks. The tool provides several criteria outlined in the following
    list to initiate attacks where a given input and label can be considered adversarial:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '**Misclassification**: Wrong predicted class at model output'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TopKMisclassification**: Modification of adversarial inputs in such a way
    as to alter the original class so it is different from one of the top-k predicted
    classes'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OriginalClassProbability**: Modification of adversarial inputs to alter the
    probability of the original class being below a specified threshold'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TargetedMisclassification**: Modification of adversarial inputs to make the
    predicted class appear as the target class'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TargetClassProbability**: Modification of adversarial inputs to increase
    the probability of a target class beyond a threshold value'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let''s study, with the following code snippet, the necessary imports that
    can trigger an adversarial attack on a PyTorch reset model:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After doing the necessary imports, let''s trigger the attack as follows:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, in the preceding example, we observe the following:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: We demonstrate a few attacks including **Fast Gradient Sign Method** (**FGSM**)
    (added noise for perturbation is on the same side as the gradient of the cost
    function)
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linf projected gradient descent** (of the white-box variety with the attacker
    having access to the model gradient and being able to alter the code to evade
    ML-based detection systems)'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L-infinity basic iterative method** (where adversarial samples are generated
    by evaluating the absolute value difference between two images, returning the
    maximum distance over all pixels)'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AdditiveUniformNoiseAttack** and **DeepFoolAttack** (a fast gradient-based
    adversarial attack that considers the minimum distance to arrive at the class
    boundary by modifying the model classifier with a linear classifier) on a pre-trained
    ResNet model by varying the number of steps to perform the attack (as denoted
    by epsilon)'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, the preceding attacks are executed on a pre-trained ResNet model by varying
    the number of steps to perform the attack:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The robust accuracy of the model can be evaluated as follows. This metric signifies
    the model''s accuracy when the attack is triggered on the best sample of the model:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The results are printed as follows:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.1\uFEFF4 – Variation of the robust accuracy metric with different\
    \ attacks](img/Figure_2.14_B18681.jpg)"
  id: totrans-438
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – Variation of the robust accuracy metric with different attacks
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Model robustness with constraints
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model robustness can be increased by training the model with adversarial perturbations
    and constraints. Adding domain constraints (just as in designing AI solutions
    of the network, such as intrusion detection systems) imposes extra restrictions
    and challenges on the adversary to maintain complex relationships between input
    features in order to trigger and realize an attack. Even though domain constraints
    limit adversarial capabilities to trigger an attack by generating perturbed samples,
    creating realistic (constraint-compliant) examples is often possible for adversaries.
    Research results suggest models gain robustness on being enforced with constraints
    (where the set of constrained variables is solved for optimization with a tractable
    linear program) that can cause the model accuracy to increase by 34%. Threat models
    designed with constraints properly assess realistic attack vectors and succeed
    in optimizing defensive performance.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: One such relevant example is the defensive performance of AdvGAN- and FGSM-based
    mitigation ([https://www.hindawi.com/journals/scn/2021/9924684/](https://www.hindawi.com/journals/scn/2021/9924684/)),
    as shown in *Figure 2**.15*, where **A** has been trained without constraints
    and **B** has been trained with constraints.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF15 – Model accuracy when trained with adversarial perturbations\
    \ A) without constraints and B) with constraints](img/Figure_2.15_B18681.jpg)"
  id: totrans-443
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – Model accuracy when trained with adversarial perturbations A)
    without constraints and B) with constraints
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Model robustness metric
  id: totrans-445
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: FGSM, JSMA, DeepFool, and **Carlini and Wagner** (**CW**) attacks have been
    useful in generating adversarial examples and triggering adversarial attacks,
    causing the misclassification of predicted outputs. There has been rigorous research
    in the field of devising metrics to evaluate model robustness by feeding in adversarial
    inputs to train the neural network.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: This research led to suggestions for improved robustness metrics, and one metric
    called **Cross Lipschitz Extreme Value for nEtwork Robustness** (**CLEVER**) was
    proposed, which defines an approximate lower bound on the minimum distortion needed
    for an attack to succeed. This robustness metric is attack-agnostic (successful
    against powerful attacks on different types of classifiers and neural networks
    such as ResNet, Inceptionv3, and MobileNet). This was also found to work on continuously
    differentiable functions to a special class of non-differentiable functions –
    neural networks with ReLU activations. The CLEVER metric serves as a comparative
    technique for comparing different network designs and training procedures.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm at first generates *N* samples in a sphere around a given sample
    in an independent and uniform manner in each batch, out of a fixed total batch
    size. Then the gradient norm of each sample is computed and the maximum value
    of the gradient over those *N* samples is evaluated. The minimum value is used
    to determine the maximum likelihood, which is in turn used to retrieve the distributional
    parameters (reverse Weibull distribution) and maximizing the probability of these
    gradients.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: The average CLEVER scores are obtained for different target classes. A high
    CLEVER score means networks have better network robustness, in which minimal adversarial
    perturbation increases the *L*p norm to a higher value. This framework from IBM
    lays the foundation to build reliable systems without invoking specific adversarial
    attacks.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: This CLEVER score can be used to evaluate the effectiveness of CNNs and help
    us to certify a neural network’s attack-resistance level. For example, in mission-critical
    applications (such as autonomous vehicles), the evaluation of classification robustness
    could increase human confidence and would serve as an important metric for compliance
    and ethics.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: If introducing adversarial perturbation impacted the accuracy of the recognition
    of traffic signs and led to a speed limit being misclassified, it would have a
    disastrous impact on humans. Hence, it becomes mandatory to evaluate the ML model
    against the right robustness metric before launching the model at scale. This
    metric introduced by IBM considers the network architectures of CNNs, including
    convolutional layers, max-pooling layers, batch normalization layers, and residual
    blocks, as well as general activation functions, and limits the perturbation of
    each pixel within a threshold margin to guarantee the network classification is
    not changed by any external attack. IBM research further assures that this metric
    derived from the input and output relations of each layer generates a matrix that
    is efficient to compute. *Figure 2**.16* illustrates the trade-offs associated
    with model robustness (as determined by the CLEVER score) and accuracy for 18
    different ImageNet models.Recent research produced a new defense framework named
    TRADES ([http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf](http://proceedings.mlr.press/v97/zhang19p/zhang19p.pdf))
    that optimizes the adversarial robustness to achieve a trade-off between accuracy
    and robustness, providing strong resilience against both black-box and white-box
    attacks.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: 'It has been found that deep neural networks when trained with regularized input
    gradients become more robust, and interpretable. By gradient regularization, we
    mean how the addition of constraints controls the change in the gradient of the
    input features with respect to the loss function.Hence in addition to looking
    for the right trade-off between model accuracy and CLEVER score, we should also
    look for the right trade-off between model accuracy and interpretability. We should
    also be aware that the accuracy or predictive power of deep learning models is
    high, whereas interpretability orders of linear and generalized additive models
    are higher. In descending orders of magnitude, we can say this is the order of
    interpretability of models: linear models, generalized additive models, decision
    trees, SVMs, random forests, and neural networks.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 2.\uFEFF16 – Trade-off between a model’s CLEVER score (robustness)\
    \ and accuracy](img/Figure_2.16_B18681.jpg)"
  id: totrans-453
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Trade-off between a model’s CLEVER score (robustness) and accuracy
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-455
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned about different defense practices for mitigating
    attacks in different stages of data and model life cycle management. We have talked
    about different cloud components, techniques, and measures that can be adopted
    for data anonymization, deanonymization, training ML algorithms with DP, and encrypted
    transfer methodologies. In reference to this, we took a deep dive into adversarial
    risk mitigation frameworks (especially open source deep learning-based frameworks)
    that can be used to test the robustness of ML models before deployment and exposing
    the ML model to public APIs. Leveraging the use of existing frameworks and designing
    new ones can offer resilience against semi-honest or dishonest participants/adversaries
    attempting to undermine AI models, systems, and services. In addition, we have
    also seen how decentralized data storage, FL, and efficient cryptographic and
    privacy measures serve as design choices for next-generation, high-potential,
    privacy-enabled systems.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters, we will explore some of the defense pipeline creation
    methodologies, optimization strategies, and metrics, along with their ability
    to deduce trade-offs between accuracy, interpretability, fairness, bias, and privacy
    (the privacy-utility trade-off). All these important parameters serve as prerequisites
    to productionizing secure, private, auditable, and objectively designed trustworthy
    AI systems, enabling universal acceptance by both consumers and policy-makers.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will understand the different laws and policies put
    in place that enforce the correct standards and best practices to build a fully
    ethics-compliant system.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Data masking*: *what it is, how it works, types, and best practices*, Cem
    Dilmegani:[https://research.aimultiple.com/data-masking/](https://research.aimultiple.com/data-masking/)'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Defense Framework for Privacy Risks in Remote Machine Learning Service,*
    Yang Bai, Yu Li, Mingchuang Xie, and Mingyu Fan:[https://www.hindawi.com/journals/scn/2021/9924684/](https://www.hindawi.com/journals/scn/2021/9924684/)'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Study on k-anonymity, l-diversity, and t-closeness Techniques focusing Medical
    Data. 17,* Rajendran, Keerthana, Jayabalan, Manoj, and Rana, Muhammad Ehsan. (2017):[https://www.researchgate.net/publication/322330948_A_Study_on_k-anonymity_l-diversity_and_t-closeness_Techniques_focusing_Medical_Data](https://www.researchgate.net/publication/322330948_A_Study_on_k-anonymity_l-diversity_and_t-closeness_Techniques_focusing_Medical_Data)'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dataprof - deterministic data* *masking*:[https://www.datprof.com/solutions/deterministic-data-masking/](https://www.datprof.com/solutions/deterministic-data-masking/.)'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Study Of The Use Of Anonymity Models,* Carmen Marcano:[https://education.dellemc.com/content/dam/dell-emc/documents/en-us/2020KS_Marcano-Study_of_the_Use_of_Anonymity_Models.pdf](https://education.dellemc.com/content/dam/dell-emc/documents/en-us/2020KS_Marcano-Study_of_the_Use_of_Anonymity_Models.pdf)'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Privacy Protection*: *p-Sensitive k-Anonymity Property. IEEE Computer Society.
    2006\. 94 - 94\. 10.1109/ICDEW.2006.116\.* Truta, T. M. & Vinay, Bindu. (2006):[https://www.researchgate.net/publication/4238176_Privacy_Protection_p-Sensitive_k-Anonymity_Property](https://www.researchgate.net/publication/4238176_Privacy_Protection_p-Sensitive_k-Anonymity_Property)'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MITRE ATT&CK: [https://attack.mitre.org/#](https://attack.mitre.org/#)'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*De-Pois*: *An Attack-Agnostic Defense against Data Poisoning Attacks. C*hen,
    J., Zhang, X., Zhang, R., Wang, C., & Liu, L. (2021):[https://arxiv.org/pdf/2105.03592.pdf](https://arxiv.org/pdf/2105.03592.pdf)'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Designing Access with Differential Privacy*, Wood, Alexandra, Micah Altman,
    Kobbi Nissim, and Salil Vadhan. (2020): [https://admindatahandbook.mit.edu/book/v1.0/diffpriv.html](https://admindatahandbook.mit.edu/book/v1.0/diffpriv.html)'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Machine Learning with Differential Privacy in* *TensorFlow*:[http://www.cleverhans.io/privacy/2019/03/26/machine-learning-with-differential-privacy-in-tensorflow.html](http://www.cleverhans.io/privacy/2019/03/26/machine-learning-with-differential-privacy-in-tensorflow.html)'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Guidelines for Anonymization &* *Pseudonymization*:[https://ispo.newschool.edu/guidelines/anonymization-pseudonymization/](https://ispo.newschool.edu/guidelines/anonymization-pseudonymization/)'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Python Library for Secure and Explainable ML,* Melis, M. Demontis, A., Pintor,
    M. Sotgiu, A., Biggio, B.*secml*: [https://arxiv.org/pdf/1912.10013.pdf]( https://arxiv.org/pdf/1912.10013.pdf)'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Evaluating the Robustness of Neural Networks*: *An Extreme Value Theory Approach,*
    Weng, Tsui-Wei et al.: [https://openreview.net/pdf?id=BkUHlMZ0b](https://openreview.net/pdf?id=BkUHlMZ0b)'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Foolbox*: *A Python toolbox to benchmark the robustness of machine learning
    models. Reliable ML in the Wild Workshop, 34th International Conference on ML,*
    J. Rauber, W. Brendel, and M. Bethge:[https://arxiv.org/pdf/1707.04131.pdf](https://arxiv.org/pdf/1707.04131.pdf)'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*On the Robustness of Domain Constraints. CCS ‘21*: *Proceedings of the 2021
    ACM SIGSAC Conference on Computer and Communications Security,* Ryan Sheatsley,
    Blaine Hoak, Eric Pauley, Yohan Beugin, Michael J. Weisman, and Patrick McDaniel:[https://arxiv.org/pdf/2105.08619.pdf](https://arxiv.org/pdf/2105.08619.pdf)'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L-Diversity: Privacy Beyond k-Anonymity, ASHWIN MACHANAVAJJHALA DANIEL KIFER
    JOHANNES GEHRKE, https://www.cs.rochester.edu/u/muthuv/ldiversity-TKDD.pdf'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
