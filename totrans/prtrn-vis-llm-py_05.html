<html><head></head><body>
		<div id="_idContainer036">
			<h1 id="_idParaDest-67" class="chapter-number"><a id="_idTextAnchor085"/>5</h1>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor086"/>Distribution Fundamentals</h1>
			<p>In this chapter, you’ll learn conceptual fundamentals for the distribution techniques you need to employ for large-scale pretraining and fine-tuning. First, you’ll master top distribution concepts for <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), notably model and data parallel. Then, you’ll learn how <a id="_idIndexMarker249"/>Amazon SageMaker integrates with distribution software to run your job on as many GPUs as you need. You’ll learn how to optimize model and data parallel for large-scale training, especially with techniques such as sharded data parallelism. Then, you’ll learn how to reduce your memory consumption with advanced techniques such as optimizer state sharding, activation checkpointing, compilation, and more. Lastly, we’ll look at a few examples across language, vision, and more to bring all of these <span class="No-Break">concepts together.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Understanding key concepts—data and <span class="No-Break">model parallelism</span></li>
				<li>Combining model and <span class="No-Break">data parallel</span></li>
				<li>Distributed training on <span class="No-Break">Amazon SageMaker</span></li>
				<li>Advanced techniques to reduce <span class="No-Break">GPU memory</span></li>
				<li>Bringing it all home with examples from <span class="No-Break">models today</span></li>
			</ul>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor087"/>Understanding key concepts – data and model parallelism</h1>
			<p>Some of my most extreme memories of working with ML infrastructure came from graduate school. I’ll <a id="_idIndexMarker250"/>always remember the stress of a new homework assignment, usually <a id="_idIndexMarker251"/>some large dataset I needed to analyze. However, more often than not, the dataset wouldn’t fit on my laptop! I’d have to clear out all of my previous assignments just to start the download. Then, the download would take a long time, and it was often interrupted by my spotty café network. Once I managed to download, I realized to my dismay that it was too large to fit into memory! On a good day, the Python library <em class="italic">pandas</em>, which you were introduced to in <a href="B18942_02.xhtml#_idTextAnchor034"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, had a function built to read that file type, which could limit the read to just a few objects. On a bad day, I needed to build a streaming reader myself. After I managed to run some analysis, I would pick a handful of models I thought would be relevant and well suited. However, they seemed to take forever to train! I would sit in front of my laptop for hours, making sure the connection didn’t fail and that my Jupyter kernel stayed alive, reading my debug statements, and hoping the loop would finish in time for my report due the <span class="No-Break">following morning.</span></p>
			<p>Fortunately for ML developers today, many of these issues have excellent solutions now—as we covered in the last chapter, Amazon SageMaker and the AWS cloud generally being one of them. Now, let’s unpack one aspect of these in great detail: the <em class="italic">training runtime</em>. As it turns out, distribution is a concept you can master to train extremely large models and datasets. In this chapter, we’ll explore two key concepts that, when used properly, will help you scale your training up to as large as your dreams. The two concepts are set <span class="No-Break">out here:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Data parallelism</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Model parallelism</strong></span></li>
			</ul>
			<p>Data parallelism <em class="italic">makes copies of your model per GPUs</em>, breaking up your dataset to help you train faster, and model parallelism <em class="italic">splits your model across GPUs</em>, helping you train even larger <a id="_idIndexMarker252"/>models. Put another way, data parallelism splits the data across <a id="_idIndexMarker253"/>accelerators in both single and multi-node settings. It applies different splits of the data to exactly the same model, copied <em class="italic">N</em> times. Model parallelism, on the other hand, splits this same model across multiple accelerators and nodes, using the same data for every <span class="No-Break">model split.</span></p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor088"/>What data parallel is all about</h2>
			<p>Data parallel is useful when <a id="_idIndexMarker254"/>you are working with extremely large datasets. In the simplest case, you might have an instance with two GPUs. Assuming you’re working with a model that is small enough to fit only a single GPU—say, something south of 1 billion parameters—your data parallel software framework might make two copies of your model, one <a id="_idIndexMarker255"/>per GPU. This same framework will also need a <strong class="bold">distributed data loader</strong>. This data loader will need to point to a single source—for example, your train and test files—but <em class="italic">split each batch by the number of model copies</em>. For example, if your global batch size were 32, your batch size per GPU would then become 16. Your data loader manages this for you, ensuring that each global batch is split and allocated correctly across your entire <em class="italic">world size</em>, or the total number of GPUs you are using to train across <span class="No-Break">all machines.</span></p>
			<p>How do you get one model—you ask—instead of two? By a simple average! The forward pass is simple to understand: each copy of the model executes a single forward pass through the network using the per-GPU batch. However, for the backward pass, the gradients <em class="italic">are averaged across all model copies</em>. After the forward pass, each model copy sends its output <a id="_idIndexMarker256"/>to the centralized control plane. This control plane computes a weighted average of the outputs of all the copies, compares this to the ground truth, and runs the gradient descent algorithm via the optimizer. The optimizer then sends new <a id="_idIndexMarker257"/>model weights out to each of the model copies. Each batch completion is called a <strong class="bold">step</strong>, and a full <a id="_idIndexMarker258"/>pass through your dataset is called <span class="No-Break">an </span><span class="No-Break"><strong class="bold">epoch</strong></span><span class="No-Break">.</span></p>
			<p>This basic concept will scale as long as you keep adding GPUs. This means that good data parallel software, such as <strong class="bold">SageMaker Distributed Data Parallel</strong> (<strong class="bold">SM DDP</strong>), will help you run <a id="_idIndexMarker259"/>on multiple GPUs in both single- and multi-instance cases. We’ll learn more about SageMaker-managed libraries for distributed training later in <span class="No-Break">this chapter.</span></p>
			<p>But first, now that you have a working understanding of data parallelism, let’s unpack the second dimension of distribution: <span class="No-Break">model parallel.</span></p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor089"/>What model parallel is all about</h2>
			<p>As we’ve come to find out, many of the state-of-the-art models in today’s world are extremely large. Commonly, these <a id="_idIndexMarker260"/>range from anything as small as a few billion parameters to hundreds of billions of parameters, and occasionally trillions. Remember—<strong class="bold">parameters</strong> are the weights in your neural network. They are what is inside all of <a id="_idIndexMarker261"/>your layers. As data passes through your network, each step is a mathematical function that transforms the input data itself, using some formula defined by the type of layer, frequently applying some activation function, and sending the results into the next layer. Computationally, the layer is fundamentally a list. The list is composed of parameters! When set to <strong class="bold">trainable</strong>, these parameters will change during the stochastic gradient descent of the backward pass. When set to <strong class="bold">not trainable</strong>, these parameters will not change, allowing you to either deploy your model or fine-tune it with <span class="No-Break">downstream layers.</span></p>
			<p>But how do we deal with large models? Model parallelism is <span class="No-Break">the answer!</span></p>
			<p>Model parallelism encompasses a variety of methods that help you split your model across multiple GPUs. The simplest of these is called <strong class="bold">pipeline parallel</strong>. In pipeline parallel, your software <a id="_idIndexMarker262"/>framework will simply take layers of your neural network and place them on different GPUs. If your neural network had two extremely large layers, and you wanted to train this on an instance with two GPUs, you might place one layer on <span class="No-Break">each GPU.</span></p>
			<p>Similarly, with the preceding data parallel example, you’ll still need a distributed data loader. This distributed data <a id="_idIndexMarker263"/>loader will still break up each global batch size into per-GPU <strong class="bold">microbatches</strong>. Each part of the model—in this case, each layer of the <a id="_idIndexMarker264"/>model—can then receive one microbatch at a time for the forward pass. The centralized control plane will then <em class="italic">execute each layer asynchronously</em>, passing the microbatches to relevant layers at the right points in time. Every layer will still see every item from the dataset, so mathematically it’s the same as if all layers were packed into some massive single GPU. Thank you, <span class="No-Break">commutative property!</span></p>
			<p>You can see an illustration of the process in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B18942_Figure_5.1.jpg" alt="Figure 5.1 – Model parallelism"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Model parallelism</p>
			<p>Another key aspect of model parallel is working with layers that are <em class="italic">too large to fit on a single GPU</em>. This is especially common in large language models, where the transformer attention head we learned about in the first chapter can easily surpass the memory limitations <a id="_idIndexMarker265"/>of modern GPUs. How do we do this? <strong class="bold">Tensor parallelism</strong>, which is the third dimension of distributed training. In a tensor parallel framework, you might have part of one tensor on one GPU, while the other part of that same tensor is placed onto a different GPU. The centralized distributed software still passes microbatches to them so that logically there is no difference in the operations. Tensor parallelism is strictly necessary for training the GPT-3-sized models of the world today, with 175 billion parameters <span class="No-Break">and more.</span></p>
			<p>To learn more about <a id="_idIndexMarker266"/>model parallelism, and especially the distributed library available through SageMaker model parallelism, take a look at our paper on the topic here (<em class="italic">4</em>). Now that you’ve learned about two foundational topics in distributed training, notably data and model parallelism, let’s learn how to <span class="No-Break">combine them!</span></p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor090"/>Combining model and data parallel</h1>
			<p>As you may have suspected previously, and as is empirically evidenced by scaling laws, large models <em class="italic">are only effective when combined with large datasets</em>. That is to say, if you use an extremely large model with a small or moderately sized dataset, you are extremely likely <a id="_idIndexMarker267"/>to overfit your model. This means <a id="_idIndexMarker268"/>it may eventually learn how to replicate the core examples you’ve provided, but it is very unlikely to handle new <span class="No-Break">challenges well.</span></p>
			<p>Surprisingly, the reverse is not necessarily true. As a general rule of thumb, it is helpful to increase the model size with the dataset size. However, in most computer vision cases, model sizes rarely surpass the memory sizes of single GPUs. I can say the majority of vision customers I work with, from autonomous vehicles to manufacturing, financial services to health care, tend to work with models that can fit quite nicely onto single GPUs. In these cases, data parallel alone is a strong candidate to improve the throughput of the training loop, because with every copy of the model on additional GPUs, your ability to train <span class="No-Break">faster increases.</span></p>
			<p>However, this is usually not the case in <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), where the most <a id="_idIndexMarker269"/>performant models regularly require at least a few GPUs, and sometimes hundreds or even thousands. In these cases, <em class="italic">you can expect to use a combination of model and data parallelism</em>, as demonstrated by an early example from Alpa (<em class="italic">5</em>). Model parallel enables you to simply hold the model in active memory on GPUs, and data parallel improves your overall speed by copying your model and increasing the overall amount of data your model can process per step. When holding even one copy of the model requires multiple GPUs, it just means that each additional copy will require the same amount. So, if your model needs 4 GPUs, and based on your data size you used scaling laws to determine that your total compute budget includes 64 GPUs (8 instances with 8 GPUs each), then you’d have 16 copies of the model! This is because each 8-GPU instance could hold 2 copies of the model each. Let’s break it down with <span class="No-Break">a visual:</span></p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B18942_Figure_5.2.jpg" alt="Figure 5.2 – Model and data parallelism"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Model and data parallelism</p>
			<p>Remember—each copy of the model is handled through model parallelism. Then, merging all copies <a id="_idIndexMarker270"/>of the model is handled <a id="_idIndexMarker271"/>through data parallelism. Seems complex, right? But hopefully, these concepts are starting to sink in—once they do, suddenly all these terms and ideas will start making sense. Remember—you aren’t alone in this journey. We have a fully managed service to help you train models at incredible scales! In the next section, I’d like to share with you some of what Amazon SageMaker automates, manages, and brings to the table to help you get to your <span class="No-Break">science, faster.</span></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor091"/>Distributed training on Amazon SageMaker</h1>
			<p>In the last chapter, we learned about SageMaker generally. Now, I’d like to dive into distributed <a id="_idIndexMarker272"/>training capabilities. We can break these up <a id="_idIndexMarker273"/>into four different categories: containers, orchestration, usability, and performance <span class="No-Break">at scale.</span></p>
			<p>As we learned in an earlier chapter, AWS offers <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) containers that you can easily point to <a id="_idIndexMarker274"/>for your own scripts and code. These are strongly recommended as the first starting point for your project because all of the frameworks, versions, and libraries have been tested and integrated for you. This means that you can simply pick a container based on whichever DL framework you are using—for example, PyTorch or TensorFlow—<em class="italic">and this container has already been tested on AWS and SageMaker</em>. You can also select the GPU version of this container, and it will already have all of the NVIDIA libraries compiled and installed to run nicely on your GPUs. If you have your own container, however, you can simply push that to Amazon’s <strong class="bold">Elastic Container Registry</strong> (<strong class="bold">ECR</strong>) and use it for training. You’ll want to add the training toolkit to <a id="_idIndexMarker275"/>enable all of the training features for custom containers, such as the entrypoint script, log emissions, warm pools, and <span class="No-Break">so on.</span></p>
			<p>Once you have your image selected, you’re ready to format your script! On SageMaker we use the <strong class="bold">estimator</strong> concept heavily. This is a Python wrapper around the API, <strong class="source-inline">CreateTrainingJob</strong>, and it’s <a id="_idIndexMarker276"/>a concept you’ll <a id="_idIndexMarker277"/>want to get very familiar with. The core idea is that you use your estimator to point to basic objects, such as your DL container, your scripts, and all of your job parameters. Then, you simply call <strong class="source-inline">estimator.fit()</strong>. This submits your call to the <strong class="source-inline">CreateTrainingJob</strong> API, which then executes the command to create <span class="No-Break">the job!</span></p>
			<p>Remember that SageMaker training <em class="italic">initializes remote instances for you during training</em>. This means that once you’ve executed <strong class="source-inline">estimator.fit()</strong>, then in the console under <strong class="bold">Training Jobs</strong>, you’ll see new instances initialized. These are managed by the service. Once the instances are initialized, they’ll copy your data onto them, download your image, and run your training script on your data. All the logs are sent to CloudWatch, and all the metadata for your job is maintained. This means your experiments are <a id="_idIndexMarker278"/>reproducible by default! Once your job is finished, the trained model artifact is sent to Amazon <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>) on <span class="No-Break">your behalf.</span></p>
			<p>Now, you must be thinking: This seems doable for a script that uses just one GPU. But how do I use multiple GPUs? The answer is <span class="No-Break">easy: software!</span></p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor092"/>Distributed training software</h2>
			<p>The critical step in scaling your code from one to multiple GPUs is using the right software for this. And fortunately <a id="_idIndexMarker279"/>for you, there are many options available today. On SageMaker, you can use any open source software you want; it’s easy to bring extra packages, scripts, and frameworks onto the <strong class="source-inline">training</strong> API. This means you can certainly implement any one of the top distributed training frameworks in our Training API. Some of these <a id="_idIndexMarker280"/>include DeepSpeed ZeRO-3D, Megatron-LM, PyTorch <strong class="bold">DistributedDataParallel</strong> (<strong class="bold">DDP</strong>), Horovod, and more. If you have code running in one of these frameworks already, your first step in scaling will likely be to just move this onto AWS and SageMaker especially. However, if you only use open source distributed frameworks, <em class="italic">you are leaving efficiency gains on </em><span class="No-Break"><em class="italic">the table</em></span><span class="No-Break">.</span></p>
			<p>The reason for these efficiency gains comes fundamentally from a concept we use a lot at Amazon: <em class="italic">there’s no compression algorithm for experience</em>. What this literally means is that Amazon constantly makes a lot of improvements in optimizing DL on the cloud, especially DL at scale. In particular, we have a software solution, SageMaker distributed training libraries, that helps you achieve state-of-the-art performance <span class="No-Break">on AWS.</span></p>
			<p>We’ll dive more into the nuances of this different distributed training software, including key design decisions such as the difference between parameter servers and ring-based approaches, in <a href="B18942_08.xhtml#_idTextAnchor127"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>. For now, let’s explore the libraries available at a <span class="No-Break">high level.</span></p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor093"/>SM DDP</h2>
			<p>Remember in <a href="B18942_04.xhtml#_idTextAnchor066"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, we learned about a concept called <strong class="bold">communication collectives</strong>. These are <a id="_idIndexMarker281"/>core algorithms designed to facilitate distributed <a id="_idIndexMarker282"/>gradient descent on multiple GPUs. However, the <strong class="bold">NVIDIA Collective Communication Library</strong> (<strong class="bold">NCCL</strong>) is <a id="_idIndexMarker283"/>actually designed with a target infrastructure in mind: InfiniBand. This is an extremely generous networking solution that enables more than 1 TB of communication transfer. Operationalizing this is quite expensive and requires a large upfront investment to acquire and utilize <span class="No-Break">this on-premises.</span></p>
			<p>At AWS, we design our <a id="_idIndexMarker284"/>own custom communication collectives that are purpose-built for the <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) network topology. These enable the best performance on AWS, scaling to thousands of GPUs and beyond without the overhead requirement of massive networking. The primary way you can interact with these custom collectives is through SM DDP (<em class="italic">3</em>). SM DDP is a fully managed data parallel software that integrates with your training script via the backend. This means that you can bring your own data parallel neural network software—notably, PyTorch DDP, Hugging Face’s Accelerate, or TensorFlow’s Horovod—and simply set SM DDP as <span class="No-Break">your backend.</span></p>
			<p>The primary reason for setting SM DDP as your backend is to <strong class="bold">increase scaling efficiency</strong>. Without <a id="_idIndexMarker285"/>SM DDP, you are likely to use communication collective algorithms that aren’t designed explicitly for the AWS EC2 instance topology. As a result, when you add more instances to your overall cluster size, you will experience <em class="italic">decreasing returns</em>. From a theoretical perspective, in a perfect world, you should be able to exactly halve your training time by moving from one to two nodes. Moving from one to three should cut down your train time by 3 times. Moving from one to four <a id="_idIndexMarker286"/>should cut down your train time by 4 times. This theoretical frontier is called <strong class="bold">linear </strong><span class="No-Break"><strong class="bold">scaling efficiency</strong></span><span class="No-Break">.</span></p>
			<p>However, we don’t live in a perfect world. Computationally, this linear scaling efficiency is effectively impossible to reach. What you will see are attempts at approaching better and better scaling efficiency, such as with better communication collective algorithms as provided by SM DDP. The gains from SM DDP are especially notable at larger scales. For example, if you <a id="_idIndexMarker287"/>compare an 8-node cluster using PyTorch DDP with the same one using SM DDP, the SM DDP job can be as much as 40% better. Those gains are on a massive scale. It means not only do your experiments come back faster, giving you more time to try new ideas and get your solution to market faster, but the actual compute cost of training is that <span class="No-Break">much lower!</span></p>
			<p>Now that we’ve learned about the SM DDP library, let’s explore another option for clocking efficiency gains at scale on AWS: the <strong class="bold">SageMaker Model Parallel</strong> (<span class="No-Break"><strong class="bold">SMP</strong></span><span class="No-Break">) library.</span></p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor094"/>SMP library</h2>
			<p>Remember that <a id="_idIndexMarker288"/>earlier in this chapter we introduced large-scale training on Amazon SageMaker. We also clarified that this enables you to run any open source library on as many GPUs as you like, without limitations around distribution software. In terms of model parallelism, this includes DeepSpeed, Megatron-LM, and others. However, to truly take advantage of all the performance enhancements SageMaker offers, I’d strongly recommend that you evaluate the SMP library. This is what we will use in detail throughout <span class="No-Break">the book.</span></p>
			<p>SMP is a Python SDK built and managed by AWS that helps you easily scale your neural network models across multiple GPUs. SMP integrates nicely with PyTorch and offers advanced features to help you scale models to anywhere from only a few to a few hundred or even thousands of GPUs. These include pipeline parallelism, tensor parallelism, optimizer state sharding, activation offloading and checkpointing, sharded data parallelism, and more. Later in this chapter, we’ll explore the advanced features, but first, let’s understand simply how to configure and use the library for basic <span class="No-Break">model distribution.</span></p>
			<p>Generally speaking, once you have a working PyTorch model that can train on a single GPU, it’s time to evaluate scaling that up. First, ensure that the base container you are working with is compatible with SMP. If you are using an AWS-managed DL container with support for GPUs, SageMaker, and training, then you are ready to move to the next step. If not, follow the documented steps (<em class="italic">1</em>) to extend a prebuilt Docker container from any arbitrary <span class="No-Break">base image.</span></p>
			<p>Once you’ve ensured your container supports SMP, it’s time to integrate the library into your training script. This centers on three key aspects, <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Import the library into your script. As of November 2022, this is as simple as executing the following command: <strong class="source-inline">import smdistributed.modelparallel.torch </strong><span class="No-Break"><strong class="source-inline">as smp</strong></span><span class="No-Break">.</span></li>
				<li>Wrap your model and optimizer with the relevant <strong class="source-inline">smp</strong> objects. This is actually pretty straightforward. After you’ve finished using PyTorch to define your neural network, or simply after you’ve loaded a PyTorch model from Hugging Face, simply pass it as an argument to the <strong class="source-inline">smp.DistributedModel()</strong> object. Then, continue passing your model throughout the rest of your script as you normally would. The optimizer follows a similar <span class="No-Break">syntactic structure.</span></li>
				<li>Refactor your training loop <a id="_idIndexMarker289"/>to include two distinct functions, a <strong class="bold">training step</strong> and a <strong class="bold">test step</strong>. Both of these should take your model, optimizer, and <a id="_idIndexMarker290"/>other relevant parameters. The train step should pass your data forward through the network, compute the loss, propagate it backward through the network via the optimizer, and return the loss. The test step should simply compute the loss, also <span class="No-Break">returning it.</span></li>
			</ol>
			<p>For both the train and test step functions, you’ll need to <em class="italic">add a Python decorator</em> declaring them as <strong class="source-inline">@smp.step</strong>. This decorator is critical because <em class="italic">everything included in this function will be sharded onto multiple GPUs</em>. The SMP library will explicitly evaluate the activity in <a id="_idIndexMarker291"/>these functions, notably your model and how data passes through it, to optimally place <a id="_idIndexMarker292"/>your model across <span class="No-Break">multiple GPUs.</span></p>
			<p>Once your script has these and a few other relevant changes, you’ll need to make one last configuration. In your SageMaker training job estimator, add another parameter called <strong class="source-inline">distribution</strong>. As we’ll see throughout the book, this parameter will allow us to configure many aspects of SageMaker training's backend, including both SMP and SM DDP. Pass a flag to enable SMP, along with any other relevant parameters. You’ll also need to enable <strong class="bold">Message Passing Interface</strong> (<strong class="bold">MPI</strong>), which we learned about earlier in the <a id="_idIndexMarker293"/>book. MPI is an open source framework that enables your nodes to communicate with each other while training. SMP uses MPI to communicate <span class="No-Break">across nodes.</span></p>
			<p>Finally, test out your script! An easy way to test model parallelism within SageMaker Training is called <strong class="bold">local mode</strong>. Local mode is an extremely useful technique that lets you develop the SageMaker Training API, including containers, data pointers, scripts, and job parameters, without <a id="_idIndexMarker294"/>waiting for the overhead of your <a id="_idIndexMarker295"/>cluster to spin up. You can use SageMaker local mode from anywhere that runs Docker, such as a SageMaker notebook instance or even your local laptop. As of this writing, Studio does not support local mode. Local mode helps you take steps quickly and easily as you write your code, ensuring that everything is designed and <span class="No-Break">working nicely.</span></p>
			<p>Once you have increased the size of your model, either through importing a larger version from Hugging Face or through increasing the number of parameters manually in your PyTorch definition, and you have proof that this works nicely on at least two GPUs, it’s time to explore advanced techniques in the SMP library to reduce your overall GPU <span class="No-Break">memory footprint.</span></p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor095"/>Advanced techniques to reduce GPU memory</h1>
			<p>Now, let’s imagine that you are pretty far into your project. You’ve already identified your dataset, your use case, and your base model. You’ve tested this at a small scale on SageMaker, such <a id="_idIndexMarker296"/>as by using 1% of your data on the smallest version of your model, and this seems to be working well. You’ve used scaling laws, or have simply seen through another example that a large model would help you increase your accuracy, and you’re confident that you have enough data to justify that larger model. You’ve increased your model enough to run on at least two GPUs and have successfully tested this <span class="No-Break">on AWS.</span></p>
			<p>If you haven’t hit all of those stages, then frankly I’d recommend you simply skip to the next section. Here, we’re about to seriously dive into very complex, detailed, and niche topics in the cutting-edge space of model parallelism. If you aren’t ready for them, such as through having hit all of the preceding stages in your project I just listed previously, then you’d be better off skipping this topic altogether for now. You can always come back and reference this material later on. Particularly if you are truly a beginner in this space, the topics we’re about to discuss may overwhelm you, making it harder for you to continue in distributed training. You can still train state-of-the-art models, such as Stable Diffusion, without using extreme-scale model parallelism. However, for those of you who truly are ready to dive completely into the world of model parallelism, let’s <span class="No-Break">get started!</span></p>
			<p>You may have noticed that splitting a model across multiple accelerators has the natural effect of <em class="italic">reducing the model’s GPU memory footprint</em>. Put another way, when a model is too big to fit on a single GPU, it is bottlenecked by the available memory of that GPU, so we need a way to reduce its memory footprint. Splitting the model across GPUs is one way of doing this, but it isn’t the only way. Let’s introduce a <span class="No-Break">few more.</span></p>
			<p>Once you’ve integrated your training script with the SMP library, using the rest of the features is as simple as adding and removing hyperparameters. While coding for them is quite simple, understanding them and designing for them successfully is quite challenging. First off, let’s recap the basics. The <strong class="source-inline">pipeline_parallel_degree</strong> parameter indicates how you’ll be splitting your model across multiple GPUs. For example, if you have 8 GPUs on your box, and you set a <strong class="source-inline">pipeline_parallel_degree</strong> value of <strong class="source-inline">2</strong>, then depending on how you allocate parameters in your model you will possibly have split your model in half. If each half of the model were using 4 GPUs, then the entire model could consume 8 GPUs, with each half consuming 4 GPUs. If you wanted to add a data parallel degree to this, you’d need <span class="No-Break">another instance.</span></p>
			<p>You’ll also need to consider how large your batch size is per GPU. In the SMP library, we call this <strong class="source-inline">microbatches</strong>. All of <a href="B18942_07.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> is about finding the right hyperparameters, but here you need to understand that <em class="italic">increasing the batch size directly increases your GPU utilization</em>. The central goal in model parallelism is finding efficient ways to decrease the GPU <a id="_idIndexMarker297"/>memory footprint of your model, allowing you to increase batch size—and hence GPU utilization—which reduces the overall runtime of your job, and hence its price tag. However, as you’ll learn in <a href="B18942_07.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, accurate models generally need lower batch sizes. Yann LeCunn is famous for stating on Twitter that “<em class="italic">friends don’t let friends use batch sizes </em><span class="No-Break"><em class="italic">over 32</em></span><span class="No-Break">”.</span></p>
			<p>Apart from pipeline parallelism and microbatches, a few other key terms to understand in model parallelism include tensor parallelism, optimizer state sharding, activation checkpointing, and sharded <span class="No-Break">data parallelism.</span></p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor096"/>Tensor parallelism</h2>
			<p>While pipeline parallelism <a id="_idIndexMarker298"/>allowed us to place <a id="_idIndexMarker299"/>different layers in our neural network on different devices, in tensor parallelism we take this another step further to <em class="italic">break up the layers themselves</em>. Typically, this is common in cases of extreme model parallelism, such as with GPT-3-type models with more than 100 billion parameters. In SMP, you can enable this simply with <strong class="source-inline">tensor_parallel_degree</strong>. Try to make sure that you’re keeping all aspects of a single layer within a single node, as this is critical in maximizing the bandwidth in your <span class="No-Break">training cluster.</span></p>
			<p>If you’re especially curious, for scaling up to 1 trillion parameters another useful technique is a spare network. This <a id="_idIndexMarker300"/>was originally proposed in 2017 (<em class="italic">6</em>) as a <strong class="bold">mixture-of-experts</strong> (<strong class="bold">MoE</strong>) technique to activate only part of a neural network during training, enabling more efficient scaling to massive parameter sizes. A distributed train<a id="_idTextAnchor097"/>ing team out of Huawei proposed an update to this focused on transformers, implementing what they call <strong class="bold">random routed experts</strong>. Impressively, they claim <a id="_idIndexMarker301"/>this was trained on only 512 accelerators <a id="_idIndexMarker302"/>in over 100 days, improving the state <a id="_idIndexMarker303"/>of the art for Chinese NLP tasks (<em class="italic">7</em>). For the rest of this book, however, we will mostly focus on dense networks that are applicable for models up to a few hundred <span class="No-Break">billion parameters.</span></p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor098"/>Optimizer state sharding</h2>
			<p>When the number of trainable weights, or parameters, in your neural network is large, you can expect the <a id="_idIndexMarker304"/>optimizer to be <a id="_idIndexMarker305"/>equally large. If you have multiple copies of your model in your training cluster, such as by using data parallelism in concert with model parallelism, then consider splitting the optimizer by setting <strong class="source-inline">shard_optimizer_state : True</strong>. Interestingly, this scopes the <strong class="source-inline">DistributedOptimizer</strong> object to only the parameters held in that <a id="_idIndexMarker306"/>data parallel rank. These are then called <strong class="bold">virtual parameters</strong>, and they share the underlying storage with the <span class="No-Break">original parameters.</span></p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor099"/>Activation checkpointing</h2>
			<p>Activation checkpointing is actually <a id="_idIndexMarker307"/>a technique that <em class="italic">trades extra computation time for reduced memory overhead</em>. Put <a id="_idIndexMarker308"/>another way, when you have activation checkpointing enabled, you’ll be able to load more objects into your cleared GPU memory, but at the price of each step taking a bit longer. This works through clearing activations of some layers and recomputing these while backpropagating <span class="No-Break">the network.</span></p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor100"/>Sharded data parallelism</h2>
			<p>One of our teams at <a id="_idIndexMarker309"/>Amazon in 2022 <a id="_idIndexMarker310"/>developed a novel strategy to optimize for large-scale distributed training on AWS. In particular, they realized that <em class="italic">not all GPUs should be treated equally</em>. This is relevant when you are trying to optimize communication collectives <em class="italic">assuming some combination of model and data parallel</em>. They designed a hierarchical approach to <strong class="bold">certificates of completion</strong> (<strong class="bold">CCLs</strong>) for distributed training, one that <a id="_idIndexMarker311"/>looks first <em class="italic">within</em> a data parallel group (or shard, as it’s called in the documentation), and then <em class="italic">across</em> data parallel groups. This minimizes the amount of overall communication required to synchronize the gradients <a id="_idIndexMarker312"/>during backpropagation and increases the overall speed of this job. Hence their name: <strong class="bold">minimize communication scale</strong>, or <strong class="bold">MiCS</strong> (<em class="italic">8</em>). This MiCS technique <a id="_idIndexMarker313"/>is available on SageMaker within the SMP library; it’s known as <strong class="bold">sharded </strong><span class="No-Break"><strong class="bold">data parallelism</strong></span><span class="No-Break">.</span></p>
			<p>Now that you’ve learned about some advanced ways to reduce your overall GPU consumption and speed up your job, let’s explore examples from interesting models that will help you bring all of these <span class="No-Break">concepts together.</span></p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor101"/>Bringing it all home with examples from models today</h1>
			<p>Remember we learned earlier in the book that <em class="italic">truly every state-of-the-art model requires some amount of distribution</em>. This is because good models come from good datasets, and good datasets are large. These take time to process, so you need to distribute your processes in order to complete them in a timely manner. Some of them have models that are too big to fit on a single GPU, so they’ll require some amount of model parallelism. But others have models that are quite small, meaning they will only require data parallelism. Let’s step through two examples from top models today: Stable Diffusion <span class="No-Break">and GPT-2.</span></p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor102"/>Stable Diffusion – data parallelism at scale</h2>
			<p>Stable Diffusion is a fascinating <a id="_idIndexMarker314"/>model that enables you to <em class="italic">create images from text</em>. Once trained, you can simply provide textual input to Stable Diffusion, and it will generate a new picture for you! While researchers have been attempting this since at least 2017, Stable Diffusion achieves performance that begins to approach human-level creativity. Models with similar performance, but not shared publicly, include Imagen (<em class="italic">9</em>) and DALL-E (<em class="italic">10</em>). The quality of the images it generates is almost immediately usable. There are still certainly issues around bias, control of the images, resolution, and common-sense reasoning, but the jump up since the state-of-the-art performance in 2017 is <span class="No-Break">truly exciting.</span></p>
			<p>Fortunately for the average Python developer, Stable Diffusion is a small model! It can fit on a single GPU, by design, which means with just a tiny bit of scripting and a moderate GPU, you can easily stand up your own demo. The drivers behind Stable Diffusion’s success are fewfold and are set out <span class="No-Break">as follows:</span></p>
			<ol>
				<li>They use four models <a id="_idIndexMarker315"/>during training: a CLIP tokenizer, a CLIP text encoder, a <strong class="bold">variational autoencoder</strong> (<strong class="bold">VAE</strong>), and a 2D convolutional U-Net. The tokenizer and encoder use intelligent language models to process the textual data, while the VAE encodes the images and converts them to a smaller <span class="No-Break">latent space.</span></li>
				<li>They add noise to those same latent images during the “diffusion”, or learning process. Using the text encoded by a language model, they try to predict the noise residuals, computing the loss and propagating this back through <span class="No-Break">the UNet.</span></li>
				<li>They use multiple billions of images! Their training data and code are both available publicly. Personally, I’ve written scripts to download 50 million images onto an optimized distributed filesystem on AWS, FSx for Lustre, and used almost 200 GPUs on Amazon SageMaker to take a few steps through this massive dataset. Their original dataset is <strong class="bold">LAION-5B</strong>, with the “<strong class="bold">5B</strong>” standing for 5 billion. These billions <a id="_idIndexMarker316"/>of images come straight from the internet, and the dataset includes textual captions for each image. Then, their model combines the images with the captions <span class="No-Break">during training.</span></li>
			</ol>
			<p>What this means is that having read this chapter, if you now have a solid understanding of data parallelism, you <a id="_idIndexMarker317"/>have everything you need to embark on training your own Stable Diffusion model! This will be the case both for pretraining and fine-tuning. In the next chapter, we’ll dive into the data loader, and you’ll learn how to prepare a new dataset for pretraining or fine-tuning at scale. But first, let’s walk through a complex model parallel case <span class="No-Break">study: GPT-3.</span></p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor103"/>GPT-3 – model and data parallelism at scale</h2>
			<p>As we learned in <a href="B18942_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, GPT-3 is a really important model. When the team at OpenAI 10xed their model size <a id="_idIndexMarker318"/>and tripled their accuracy, moving from GPT-2 to GPT-3, they unleashed a worldwide movement that is now practically synonymous with <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>). Core to <a id="_idIndexMarker319"/>this step in scaling, as we’ve learned throughout this chapter, is model parallelism. Let’s unpack how this works for models with more than 100 <span class="No-Break">billion parameters!</span></p>
			<p>Let’s start by estimating the memory size of the model itself. First, let’s count this in parameters—say, 100 billion parameters. Then, for a job that uses <strong class="source-inline">FP16</strong> data types and Adam optimizers, you can assume a single FP16 parameter takes about 2 bytes, and an FP16 gradient takes about the same. So, for a model with 10 billion parameters, you’ll need at least 200 GB of GPU memory. A model with 100 billion would then need about 2 TB of <span class="No-Break">GPU memory!</span></p>
			<p>Assuming you have 40 GB of GPU memory available per device, as is the case for the <strong class="source-inline">p4d.24xlarge</strong> instances, that’s 50 GPUs just to hold one full copy of the model in memory. Each <strong class="source-inline">p4d.24xlarge</strong> instance has 8 GPUs, so you’re looking at just over 6 <strong class="source-inline">p4d</strong> instances per model copy. Assuming you both want an accurate model and not have to wait years for the <a id="_idIndexMarker320"/>job to finish training, you’ll want many, many copies of this model. I’ve helped customers train on 128 p4d instances on SageMaker for large language models of this size, which in this calculation would give them about 20 copies of the model across all of those <span class="No-Break">1,024 GPUs.</span></p>
			<p>For scripting examples to wrap your head around what this would look like in action, consider our notebooks at the <em class="italic">SageMaker Examples</em> repository on GitHub. You should find it if you search for <strong class="source-inline">GPT-2</strong>, or even <strong class="source-inline">model parallel</strong> in the repository. Currently, the link is <span class="No-Break">here: (</span><span class="No-Break"><em class="italic">2</em></span><span class="No-Break">).</span></p>
			<p>If you inspect the notebook, you’ll notice a <span class="No-Break">few things:</span></p>
			<ul>
				<li>It offers different sizes for training the model, starting from the very smallest and going up to a few tens of billions <span class="No-Break">of parameters</span></li>
				<li>Each of these different settings requires slightly <span class="No-Break">different hyperparameters</span></li>
			</ul>
			<p>All of <a href="B18942_07.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> is about selecting these. And now, you’ve just learned why this is so challenging and important! When you configure the job, you need to determine your distribution strategy, integrating <a id="_idIndexMarker321"/>the model and data parallel, your overall world size, any extra GPU memory reduction techniques, the model size and relevant parameters, and so <span class="No-Break">much more.</span></p>
			<p>But don’t lose heart—we still have a lot to learn. For now, let’s close out the chapter with a <span class="No-Break">quick summary.</span></p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor104"/>Summary</h1>
			<p>In this chapter, you learned about the basics of distributed training. You learned about data parallelism and model parallelism as key concepts that will enable you to scale your training up to sizes that approach state of the art. You learned how to combine them, and especially how managed orchestration platforms such as Amazon SageMaker help you seamlessly work with hundreds to thousands of GPUs with optimized distributed training libraries. You then learned about advanced GPU memory reduction techniques and brought this to life with real-world examples such as Stable Diffusion <span class="No-Break">and GPT-3.</span></p>
			<p>In the next chapter, we’ll dive into the engineering fundamentals and concepts you need to build your own <span class="No-Break">data loader!</span></p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor105"/>References</h1>
			<p>Please go through the following content for more information on a few topics covered in <span class="No-Break">the chapter:</span></p>
			<ol>
				<li><em class="italic">SMP </em><span class="No-Break"><em class="italic">library</em></span><span class="No-Break">: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-sm-sdk.html#model-parallel-customize-container"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-sm-sdk.html#model-parallel-customize-container</span></a></li>
				<li><em class="italic">Amazon SageMaker </em><span class="No-Break"><em class="italic">example</em></span><span class="No-Break">: </span><a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt2/smp-train-gpt-simple.ipynb"><span class="No-Break">https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt2/smp-train-gpt-simple.ipynb</span></a></li>
				<li><em class="italic">SM DDP </em><span class="No-Break"><em class="italic">library</em></span><span class="No-Break">: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.htmll"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html</span></a></li>
				<li><em class="italic">Amazon SageMaker Model Parallism: A General and Flexible Framework for Large Model </em><span class="No-Break"><em class="italic">Training</em></span><span class="No-Break"> </span><a href="https://arxiv.org/pdf/2111.05972.pdf"><span class="No-Break">https://arxiv.org/pdf/2111.05972.pdf</span></a></li>
				<li><em class="italic">Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep </em><span class="No-Break"><em class="italic">Learning</em></span><span class="No-Break">: </span><span class="No-Break">https://www.usenix.org/conference/osdi22/presentation/zheng-lianmin</span></li>
				<li><em class="italic">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts </em><span class="No-Break"><em class="italic">Layer</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/1701.06538.pdf"><span class="No-Break">https://arxiv.org/pdf/1701.06538.pdf</span></a></li>
				<li><em class="italic">Pangu-Σ: Towards Trillion Parameter Language Model With Sparse Heterogenous </em><span class="No-Break"><em class="italic">Computing</em></span><span class="No-Break">: </span><span class="No-Break">https://arxiv.org/pdf/2303.10845.pdf</span></li>
				<li><em class="italic">MiCS: Near-linear Scaling for Training Gigantic Model on Public </em><span class="No-Break"><em class="italic">Cloud</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><span class="No-Break">https://arxiv.org/pdf/2205.00119.pdf</span></li>
				<li><em class="italic">Photorealistic Text-to-Image Diffusion Models with Deep Language </em><span class="No-Break"><em class="italic">Understanding</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2205.11487.pdf"><span class="No-Break">https://arxiv.org/pdf/2205.11487.pdf</span></a></li>
				<li><em class="italic">Zero-Shot Text-to-Image </em><span class="No-Break"><em class="italic">Generation</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2102.12092.pdf"><span class="No-Break">https://arxiv.org/pdf/2102.12092.pdf</span></a></li>
			</ol>
		</div>
	</body></html>