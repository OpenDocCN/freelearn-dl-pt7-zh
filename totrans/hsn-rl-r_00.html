<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Preface</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Reinforcement learning is an exciting part of machine learning. It has a variety of uses in technology, ranging from autonomous cars to game playing. Reinforcement learning creates algorithms that can learn and adapt to environmental changes.</span></p>
<p class="mce-root"><span>This book provides a hands-on approach to the implementation of reinforcement learning. It will explore interesting practical examples, such as using tabular Q-learning to control robots. It also explores associated methodologies that will have you up and running in no time.</span></p>
<p class="mce-root"><span>We'll introduce the underlying concepts of reinforcement learning. We will cover the agent-environment interface, Markov decision process, and policy gradient methods. </span><span>We will then use the R libraries available to develop a model based on Markovian chains. We will examine the multi-armed bandit problem. Applying dynamic programming and Monte Carlo methods, we will find the best policy and make predictions. Finally, we will use temporal difference learning for vehicle routing problem applications. Later, we</span><span> will apply the concepts learned so far to real-life problems. We will start from the world of games, and then move on to financial problems such as fraud detection. From the world of health care, we will see how you can use TD learning to detect cancers.</span></p>
<p class="mce-root"><span>And finally, we will explore deep reinforcement learning. This involves using algorithms based on neural networks to increase the potential of reinforcement learning. We will approach reinforcement learning by using the Keras model. Finally, we will see what the next challenges of reinforcement learning are.</span></p>
<p>By the end of this book, you will be able to implement various deep learning algorithms in R for a variety of use cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Who this book is for</h1>
                </header>
            
            <article>
                
<p>This book is for anyone who wants to learn about reinforcement learning from scratch. This book will extend your knowledge about the major role of machine learning in different domains. It covers important concepts of RL and the problems associated with it, while using R 3.x.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What this book covers</h1>
                </header>
            
            <article>
                
<p><a href="2362715d-f8f2-435a-9c00-975ed61986a8.xhtml">Chapter 1</a>, <em>Overview of Reinforcement Learning with R</em><span>, helps you get up and running with reinforcement learning by introducing you to reinforcement learning and working with the <kbd>MDPtoolbox</kbd> package.</span></p>
<p><a href="aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml">Chapter 2</a>, <em>Building Blocks of Reinforcement Learning</em><span>, gets you ready to enjoy reinforcement learning using R, from concepts through to building models. By the end of this chapter, you will be ready to dive into working on real-world projects.</span></p>
<p><a href="7ee860fd-cd4c-4034-8dd6-9c803e129418.xhtml">Chapter 3</a>, <em>Markov Decision Processes in Action</em>, explains the Markov decision process in detail. The chapter further illustrates how the process can be implemented in real-world applications.</p>
<p><a href="80162fc2-33f6-4f5a-9f70-6d063b32d9c9.xhtml">Chapter 4</a>, <em>Multi-Armed Bandit Models</em>, explains the multi-armed bandit model, and the various use cases for it.</p>
<p><a href="63830c6d-50dd-4f5c-bb26-b499d1ecd48a.xhtml">Chapter 5</a>, <em>Dynamic Programming for Optimal Policies</em>, covers the various features of dynamic programming (DP). It explains the top-down approach to DP, and several optimization techniques where it can be applied. Finally, the chapter shows us how to build a DP application.</p>
<p><a href="1d216456-f579-400b-89d5-6366983ac2b8.xhtml">Chapter 6</a>, <em>Monte Carlo Methods for Predictions</em>, teaches you how to use Monte Carlo methods for forecasting stock market prices.</p>
<p class="mce-root"><a href="9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml">Chapter 7</a>, <em>Temporal Difference Learning</em>, covers how to use <strong>temporal difference</strong> (<strong>TD</strong>) learning algorithms to resolve the vehicle routing problem.</p>
<p><a href="47b30864-c93f-4e61-aa44-fa46b70508dd.xhtml">Chapter 8</a>, <em>Reinforcement Learning in Game Applications</em>, shows how to use reinforcement learning algorithms to address a problem in game theory.</p>
<p><a href="a77d7712-6b3e-4d10-8e12-5f6115fec41f.xhtml">Chapter 9</a>, <em>MAB for Financial Engineering</em>, addresses financial engineering issues with <strong>reinforcement learning</strong> (<strong>RL</strong>) to learn optimization and anomaly identification techniques.</p>
<p><a href="1149d32b-6985-4114-ab08-57da8680fa0f.xhtml">Chapter 10</a>, <em>TD Learning in Health Care</em>, addresses health care problems with the help of reinforcement learning.</p>
<p><a href="91935d6b-70d6-4d61-b1b8-86d84470caf4.xhtml">Chapter 11</a>, <em>Exploring Deep Reinforcement Learning Methods</em>, teaches you the fundamentals of artificial neural networks. Here, you will learn how to implement a deep recurrent Q network using R.</p>
<p><a href="c220bf9a-c913-4b8c-9b66-f85bf45a8c9d.xhtml">Chapter 12</a>, <em>Deep Q-Learning Using Keras</em>, explores the Keras model using TensorFlow as the backend engine, and how to use Keras to set up a <strong>multilayer perceptron</strong> (<strong>MLP</strong>) model. Then, you will learn how to use deep reinforcement learning to balance a cart pole system. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><a href="f77770d7-ca30-4b10-ab18-751fc8ed368b.xhtml">Chapter 13</a>, <em>Whats Next?</em>, explores a quick summary of RL concepts, and we will explore the main RL applications in real life. In so doing, we will discover the next steps for RL and upcoming real-life challenges in terms of the construction and implementation of machine learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">To get the most out of this book</h1>
                </header>
            
            <article>
                
<p>Basic knowledge of R is required for this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Download the example code files</h1>
                </header>
            
            <article>
                
<p>You can download the example code files for this book from your account at <a href="http://www.packt.com" target="_blank">www.packt.com</a>. If you purchased this book elsewhere, you can visit <a href="https://www.packtpub.com/support">www.packtpub.com/support</a> and register to have the files emailed directly to you.</p>
<p>You can download the code files by following these steps:</p>
<ol>
<li>Log in or register at <a href="http://www.packt.com" target="_blank">www.packt.com</a>.</li>
<li>Select the <span class="packt_screen">Support</span> tab.</li>
<li>Click on <span class="packt_screen">Code Downloads</span>.</li>
<li>Enter the name of the book in the <span class="packt_screen">Search</span> box and follow the onscreen instructions.</li>
</ol>
<p>Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
<ul>
<li>WinRAR/7-Zip for Windows</li>
<li>Zipeg/iZip/UnRarX for Mac</li>
<li>7-Zip/PeaZip for Linux</li>
</ul>
<p><span>The code bundle for the book is also hosted on GitHub at</span> <a href="https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-with-R"><span>https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-with-R</span></a><span>. </span><span>In case there's an update to the code, it will be updated on the existing GitHub repository.</span></p>
<p><span>We also have other code bundles from our rich catalog of books and videos available at</span><span> </span><strong><span class="Object"><a href="https://github.com/PacktPublishing/" target="_blank">https://github.com/PacktPublishing/</a></span></strong><span>. Check them out!</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Download the color images</h1>
                </header>
            
            <article>
                
<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="http://www.packtpub.com/sites/default/files/downloads/9781789616712_ColorImages.pdf">http://www.packtpub.com/sites/default/files/downloads/9781789616712_ColorImages.pdf</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conventions used</h1>
                </header>
            
            <article>
                
<p>There are a number of text conventions used throughout this book.</p>
<p><kbd>CodeInText</kbd>: <span>Indicates c</span>ode words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. <span>Here is an example:</span> "<span>The </span><kbd>keras</kbd><span> library bases its technology on the levels that are used to manage input and output.</span>"</p>
<p>A block of code is set as follows:</p>
<pre>Xtrain &lt;- MnistData$train$x<br/>Ytrain &lt;- MnistData$train$y<br/>Xtest &lt;- MnistData$test$x<br/>Ytest &lt;- MnistData$test$y</pre>
<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
<pre>&gt; dim(Xtrain)<br/><strong>[1] 60000    28    28</strong><br/>&gt; dim(Xtest)<br/><strong>[1] 10000    28    28</strong></pre>
<p><strong>Bold</strong>: Indicates a new term, an important word, or w<span>ords that you see on screen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: "<strong>Rectified linear unit</strong> (<strong>ReLU</strong>) is the most widely used activation function since 2015.</span><span>"</span></p>
<div class="packt_infobox">Warnings or important notes appear like this.</div>
<div class="packt_tip">Tips and tricks appear like this.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Code in Action</h1>
                </header>
            
            <article>
                
<p>Visit the following link to check out videos of the code being run:<br/>
<a href="http://bit.ly/2LOXtY6">http://bit.ly/2LOXtY6</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Get in touch</h1>
                </header>
            
            <article>
                
<p>Feedback from our readers is always welcome.</p>
<p class="mce-root"><strong>General feedback</strong>: If you have questions about any aspect of this book, <span>mention the book title in the subject of your message and</span> email us at <kbd><span>customercare@packtpub.com</span></kbd>.</p>
<p><strong>Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="https://www.packtpub.com/support/errata" target="_blank">www.packtpub.com/support/errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
<p><strong>Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <kbd>copyright@packt.com</kbd> with a link to the material.</p>
<p class="mce-root"><strong>If you are interested in becoming an author</strong>: If there is a topic that you have expertise in, and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com/" target="_blank">authors.packtpub.com</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reviews</h1>
                </header>
            
            <article>
                
<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
<p>For more information about Packt, please visit <a href="http://www.packt.com/" target="_blank">packt.com</a>.</p>


            </article>

            
        </section>
    </body></html>