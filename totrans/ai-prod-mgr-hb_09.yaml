- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Macro and Micro AI for Your Product
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term *AI* is often used as an umbrella term that encapsulates the idea that
    a machine, whether physical or not, is mimicking the way humans either think,
    work, speak, express, or understand. This is a pretty big concept to wrap up in
    one term. It’s hard to embody not just the diversity of models and use cases but
    the implementation of these models and use cases themselves. This chapter will
    serve as a handy recap of the various types of AI that products can absorb as
    you begin to explore the various ways you can leverage AI, as well as some of
    the most successful examples and common mistakes that can arise as PMs build AI
    products.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the application of AI has seen many iterations and
    will continue to do so as we build into the next decade, but it’s hard to understate
    the importance of what this technological wave can offer PMs and companies. This
    is a substantial wave of innovation that’s showing no signs of going away, and
    having a clear picture of the options that are ahead of you will help in building
    something that will have self-evident value to offer the market.
  prefs: []
  type: TYPE_NORMAL
- en: Though we’ve touched on the complexities and types of engagement with various
    ML and DL models, we will use this chapter to cover our bases with regard to the
    greater umbrella of AI to offer a comprehensive macro example of what AI is and
    how it can most bring value to product creators and users alike. In the *Macro
    AI – Foundations and umbrellas* section, we will understand how AI can be used
    in ways that help a product perform more optimally and evolve over time at a high
    level. In the *Micro AI – Feature level* section, we will go over common, accessible
    uses of AI at the more detailed feature level. Then, we will use the successes
    and challenges sections to point out a few examples of applications of AI that
    embody the concepts in the previous two sections.
  prefs: []
  type: TYPE_NORMAL
- en: Any PM will know that, as we discussed in [*Chapter 8*](B18935_08.xhtml#_idTextAnchor246),
    understanding what their product needs to be successful will require their own
    domain knowledge, as well as knowledge of what the competitors in their space
    are offering their markets. There is a third piece that we hope to bridge in this
    chapter, and that’s in understanding what the baseline for AI adoption looks like
    so that you can tailor this baseline to your own domain and find success in the
    design, promotion, and performance of your product. This is where the fault line
    lies between domain knowledge and the foundational understanding of what AI can
    offer your audience. We want to empower you to be able to apply these general
    concepts to your particular domains in a way that feels accessible and exciting.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve the goals we set here, we will cover the following topics in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Macro AI – Foundations and umbrellas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micro AI – Feature level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Successes – Examples that inspire
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges – Common pitfalls
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Macro AI – Foundations and umbrellas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve talked a great deal about ML and DL models in the previous chapters.
    This was intentional because most of the time when we see AI advertised to us
    through various products, this is what the underlying technology employed is—for
    the most part. It’s either a DL or an ML algorithm that’s powering the products
    we’ve discussed. But as you’ve seen in the previous chapters, AI is a great umbrella
    term that can actually mean more than just an ML or a DL model is being used.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of major domains in AI that don’t involve ML and DL. We’ve
    minimally touched on the other areas but haven’t given them their due in their
    impact and contributions to the AI landscape. Focusing only on ML and DL makes
    sense from a practical perspective to offer AI technologists, entrepreneurs, and
    PMs the best chance to pitch their AI products to investors and users, but it
    also leaves holes in our greater tapestry of AI options that are out there. It’s
    also important to understand that as we carry on and evolve through AI adoption
    in the coming years and decades, new domains within AI could emerge that may perform
    better than ML or DL, and there are a number of prominent voices out there that
    offer this perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, the following is an inclusive macro screenshot of the various categories
    of AI. Since we’ve focused on ML and DL for most of this book, we wanted to offer
    a holistic view of all current concentrations of AI. As this section goes on,
    we will be addressing each area of AI in an effort to broaden your understanding
    of the other options you have when building AI products:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Categories of AI](img/B18935_9_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Categories of AI
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s dive into these various domains of AI and offer a thorough overview
    of what each of these domains is, what they offer their products and users, and
    how they’ve impacted their greater markets! We will end this section with what
    new, further domains in AI might look like to catch a glimpse of future manifestations
    of AI before we start to discuss the use and implementation of these domains at
    the feature level in more applied terms in the following *Micro AI – Feature*
    *level* section.
  prefs: []
  type: TYPE_NORMAL
- en: ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**ML** is a general concept that gives us the basic thinking and organizing
    power that gives machines the ability to reason with data in a way a human might.
    You can think of this as a way for a machine to mimic how a human can think, understand,
    and work. It dominates the market, and it serves as its own umbrella term of sorts
    because within it we can break down ML into the models we associate with traditional
    ML, along with their specializations such as **computer vision**, **natural language
    processing** (**NLP**), and DL as further subsets of ML.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve gone over the specific models and algorithms that are used in ML in *Chapters
    2* and *3* of this book, so we won’t go into those in detail here, but suffice
    it to say this is where there are major categories in the kind of learning these
    machines do. To jog your memory, those learning types are **supervised learning**,
    **unsupervised learning**, **reinforcement learning**, and **deep learning** (**neural
    networks**). All these types of learning can be applied in generalized ways, either
    together or separately, to make up an ML network in what’s considered ensemble
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Generally speaking, you can see the term *AI* used so often because of the confusion
    and ambiguity that plagues the market now. In many cases, companies opt for it
    because their audiences may have the familiarity to appreciate more specificity,
    or because they don’t want you to know what methods they actually employ in their
    products. This will start to change but for now, this is likely due to AI still
    being new in its market adoption and to end users still being unclear on what
    to call the underlying tech. As we see more and more adoption of AI, we will also
    start to see more clarity in what kind of AI a product is using.
  prefs: []
  type: TYPE_NORMAL
- en: Even ML can perhaps be too broad of a descriptor. We may even see the term *AI*
    or *ML* fall off entirely, giving way to more exact descriptions of the tech.
    If you aren’t sure what’s considered ML and what isn’t, it might be convenient
    to break down the terms themselves. ML presupposes that the machines, or models,
    are learning by training from past examples. This learning and adaptation from
    the past is not an explicitly programmed action. Let’s get into the various learning
    methods of ML further.
  prefs: []
  type: TYPE_NORMAL
- en: In supervised learning, we humans are giving machines a clue as to what the
    data points mean. To put it simply, we label the data for the machines to understand.
    But the pattern recognition and optimization that goes on to predict a future
    value, let’s say, is not something we tell the machine to do. In unsupervised
    learning, we’re not giving the machines any indication of what the data means
    or whether it’s correct in its prediction. So, in unsupervised learning, the machines
    are not just coming up with their own conclusions, but they’re assembling the
    patterns they see based on no input from us at all. They are *learning* and deriving
    insights based on their own ability to see patterns. In reinforcement learning,
    we are doing a combination of the two, so when the machines do perform well, we
    offer them a *cheat code* to be able to recreate that good performance again to
    reinforce their learning. Finally, in neural networks, we let the machine’s hidden
    layers derive meaning; it is left to its own devices.
  prefs: []
  type: TYPE_NORMAL
- en: The last major aspect of ML to keep in mind is that data is the major input
    that impacts its success. Unlike other domains of AI mentioned next, ML relies
    on the quantity and quality of the data it’s fed on for its success and performance.
    Though there are innovations being worked on now to move ML to be less data-reliant,
    we still haven’t gotten there. ML still requires massive swaths of data that needs
    to be stored, used, and learned in order for us to see its value. The foundational
    intelligence that powers how it does this at the model level is based on underlying
    mathematical foundations, so it’s trying to understand and correct how wrong it
    was from the correct answer even incrementally so that it guesses or predicts
    more correctly with each wave of *learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Computer vision** is the underlying tech that powers everything from agriculture
    and climate change tracking to autonomous vehicles, facial recognition and surveillance,
    and medical imaging and manufacturing. Basically, it’s ML for images and dynamic
    video. Much like what we discussed in the previous section, it’s learning by example,
    and its models are optimized in much the same way but for visual content. Whereas
    the ML umbrella term is for recognizing, understanding, and predicting words and
    numbers, computer vision does the same but for visual data. We still consider
    this ML because the basic principle of ML from previous examples and optimizing
    for correcting its own mistakes is still present with computer vision models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps what’s most interesting about computer vision is that it’s in essence
    using mathematical foundations to basically split up an image into what it can
    understand: a matrix of numbers. Then, in some cases, it’s learning from that
    breakdown in order to reassemble it back into a new image. Computer vision models
    look for ways to recognize objects and images and translate an image based on
    various factors such as edges, frames, textures, and 3D shapes to distinguish
    between what’s a static or moving object. They try to understand motion as it
    progresses through time.'
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision is a translation of how a machine might see, but a more exact
    description is that it’s processing data in a way that mimics how we see. It’s
    translating data from something a human eye can understand to something a machine
    can understand, and it’s able to do so much more quickly than we humans can. If
    you doubt that, you should see the human versus self-driving car crash records.
  prefs: []
  type: TYPE_NORMAL
- en: NLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If computer vision is how a machine might see, **NLP** is how a machine might
    hear and speak. It’s a machine processing human language and all the specifics
    that come along with that. NLP is used heavily in translation, speech-to-text,
    and speech recognition. It dominates the market in terms of products that work
    with human speech, personal assistants, conversational AI, and chatbots, and it’s
    a huge domain within AI as these markets continue to grow.
  prefs: []
  type: TYPE_NORMAL
- en: NLP is made up of two components, **natural language understanding** (**NLU**)
    and **natural language generation** (**NLG**), so the entire system together is
    called natural language processing. But do keep in mind these two major components
    that have their own expertise. A machine needs to fully understand what’s being
    asked of it before it can create a response, and given the popularity of massive
    language models such as OpenAI’s GPT-3 and GPT-4, Google’s BERT, and IBM Watson,
    NLP is only going to grow in adoption and reliance.
  prefs: []
  type: TYPE_NORMAL
- en: NLP is truly a tremendous innovation when you consider all the world’s languages
    that exist and even the disparity in how much variety exists in the way people
    express ideas and speak. It’s not just about the words people will choose to use
    and the inherent variety within that but also the speed, inflections, and accents
    in the words they choose as well—the mood of the person speaking, how much they
    enunciate, how lazy or fast they are with delivering their words. There’s so much
    nuance in the world of language, and advancements in NLP have proven to triumph
    against these nuances. They aren’t perfect, and we sometimes do have to repeat
    something when we're communicating with a personal assistant, but those cases
    are increasingly rare because the more we interact with them, the more they can
    anticipate and understand our inquiries and offer thoughtful, correct responses.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, **DL** is a subset we’ve discussed at length throughout this book,
    and it relies on neural networks to arrive at insights and predictions. You’ve
    likely seen the term *black-box models* thrown around often throughout the book,
    and this is your reminder that this likely means DL models are involved because
    hidden layers are opaque structures that the humans that program DL neural networks
    can’t see into. The reason for this is that within each hidden layer there are
    nodes that make computational decisions, and these decisions are happening essentially
    all at once, so it’s not a matter of not wanting to offer transparency into the
    models—it’s a matter of really not being able to in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Considering that a lack of transparency is one of the major issues with using
    DL models, particularly in applications that require explainability, seeing advancements
    in how DL models can be applied in ways that are more transparent would be a big
    win for DL. Current market applications of DL thrive when there isn’t a huge need
    for explainability because there are minimized negative downstream effects or
    because good performance is reason enough to use them. So, if your product uses
    DL and this lack of transparency isn’t hurting any of your customers, users, or
    your own reputation, you can carry on. For those that want the performance of
    DL but can’t move forward until there is less opacity, you’ll sadly need to wait
    for the models to catch up to themselves. Perhaps a day will come when they will
    explain themselves!
  prefs: []
  type: TYPE_NORMAL
- en: Despite the naming conventions, explainability, and neural aspect of the models,
    DL is still ML in the sense that the models are still learning from prior examples,
    they’re still optimizing and predicting for future values and checking against
    that performance to see how *correct* they were so that they can mimic the performance
    later, and they still follow the learning types we discussed prior. There are
    still supervised, unsupervised, semi-supervised, and reinforcement neural networks.
    They are still finding patterns within the data you feed them and learning from
    past examples with every new training they receive.
  prefs: []
  type: TYPE_NORMAL
- en: The major difference between ML and DL is in the data needs DL has. With traditional
    ML, you might be OK with—say—hundreds, thousands, or hundreds of thousands of
    data points, but with DL, even those scales might not be enough. We don’t want
    to offer specific numbers because the data required is always going to depend
    on the models you’re using, what you’re doing with them, and how diverse the datasets
    are in terms of the variety of examples they offer. The more diverse your data,
    the better chance you’re offering your models to understand more and more so that
    they’re optimized for success and performance. This data hunger for DL is another
    barrier to entry for a lot of folks that want to be working with neural networks
    in their products. They may not have the data luxury to offer them, even if the
    performance is high, and currently, investments are being made in researching
    ways to make DL models less reliant on data.
  prefs: []
  type: TYPE_NORMAL
- en: Robotics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If AI is considered an umbrella term for machines mimicking how humans might
    work and reach conclusions, this is perhaps most visibly obvious with robotics,
    which is physically trying to reproduce the work of a human. The term *robot*
    is in and of itself a generic term that encompasses a lot of nuances, much like
    the term *AI*. ML is considered as powerful as it is because of the ability of
    machines to learn from past actions and behaviors. In that sense, perhaps they
    are considered more advanced than the underlying tech that powers robotics, but
    we felt it wasn’t fair to exclude robotics from AI because if a robot can make
    a meal, make a part for a car, or assist with surgery, it’s intelligent enough
    to be considered in the realm of AI.
  prefs: []
  type: TYPE_NORMAL
- en: The heart of innovations in this space will also come from ML being used in
    combination with robotics so that it can learn from the past. In [*Chapter 3*](B18935_03.xhtml#_idTextAnchor101),
    we briefly touched on the idea of Boston Dynamics robot dogs not using ML. Even
    iRobot’s Roomba only recently got an AI upgrade in 2021 with the launch of its
    Roomba j7+. This is showing that robotics may thrive on its own without the need
    for perhaps more *advanced* areas of AI such as ML and DL to be involved in its
    development as a product. Though robotics can stand on its own, we’re already
    seeing ways in which robotics is coming to the forefront in integrating AI with
    products on the market with the rising popularity of robots such as Sophia, sexual
    companions, and surgical robots that have come on the scene.
  prefs: []
  type: TYPE_NORMAL
- en: A big part of their popularity lies in not just the precision and trust that
    can come from a robot doing repetitive tasks that might be laborious for a person,
    but also in their inability to experience fatigue. Robots can malfunction, but
    they certainly won’t want a vacation or get sick. This gives rise to the other
    aspect of robotics adoption, which is more ethical and human-centric. Just because
    we can *hire* a robotic worker in an assembly line, restaurant, or hospital doesn’t
    mean we should. People, at least in most cases, need to work to make a living.
    Disruptions to embracing AI from the general public often involve distrust and
    contempt over AI taking jobs away from the humans that need them. The most successful
    and least controversial areas to be using robots to replace workers would be for
    tasks that are considered too unpleasant for humans or tasks that are repeatedly
    putting humans in danger where there is a shortage of willing participants in
    the first place.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a number of specializations of robots we can go over that have varying
    degrees of intelligence and autonomy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Autonomous robots**: Autonomous robots, for instance, can perceive and maneuver
    through their environment and make decisions based on those perceptions without
    the intervention or control of a human. The best example of this might be a Roomba,
    which automatically picks up debris to clean our floors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Teleoperated robots**: Teleoperated robots such as daVinci and NeuroArm ([https://mind.ilstu.edu/curriculum/medical_robotics/teleo.html](https://mind.ilstu.edu/curriculum/medical_robotics/teleo.html))
    are often referred to as **automated guided vehicles** (**AGVs**), which are used
    with a human intervening to perform tasks such as medical procedures, moving materials
    in a factory, or drones for disaster response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augmenting robots**: Taking this a step further, we also see a number of
    augmenting robots such as Raytheon XOS, HAL, eLegs, and the DEKA arm, which are
    made for targeted muscle reinnervation through electronic prosthetic devices ([https://mind.ilstu.edu/curriculum/medical_robotics/augmenting.html#:~:text=Augmenting%20robots%20generally%20enhance%20capabilities,that%20a%20person%20has%20lost](https://mind.ilstu.edu/curriculum/medical_robotics/augmenting.html#:~:text=Augmenting%20robots%20generally%20enhance%20capabilities,that%20a%20person%20has%20lost)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Humanoid robots**: Then, there are humanoid robots such as Sophia, Tesla’s
    Optimus, and Ameca, which are often using some kind of ML to express themselves
    in ways we can understand ([https://builtin.com/robotics/humanoid-robots](https://builtin.com/robotics/humanoid-robots)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: "**Pre-programmed robots**: People have come to rely on these robots because\
    \ of their heavy use in the automobile and manufacturing industries ([https://mind.ilstu.edu/curriculum/medical_robotics/prepro.html](https://mind.ilstu.edu/curriculum/medic\uFEFF\
    al_robotics/prepro.html))."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expert systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An `If Then` statements. A **rule-based engine** means there are a set of pre-programmed
    instructions and algorithms that have been programmed into the backbone of how
    a product or system functions and there is an absence of self-learning. This means
    that ML models are not used and the system is not learning over time. Though this
    might sound like a *dumb* system, it’s still considered AI because it still might
    be functioning in a way that mimics how a human might apply intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Just because it’s pre-programmed doesn’t mean it’s not working optimally, accurately,
    or in a way that’s intelligent. There are a number of applications that don’t
    need ML because what they do is already optimal enough for what’s expected of
    them. If it’s simulating the knowledge or judgment of its human counterparts,
    then it’s intelligent. Also, many of these expert systems have been optimized
    over time, so there still might be a huge amount of complexity. These expert systems
    have the ability to emulate the decision-making or the reasoning of a person based
    on a set of instructions, and there could be manifestations of it today that can
    be bolstered by ML.
  prefs: []
  type: TYPE_NORMAL
- en: Fuzzy logic/fuzzy matching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Fuzzy matching**, also referred to as approximate string matching, uses some
    logic to find terms or phrases that are similar to each other. Perhaps you’re
    looking through your database to isolate anyone with a first name *John* but some
    entries are *Jonathan* or *Johnny*. Fuzzy matching would be an intelligent way
    of finding those alternative names. Fuzzy matching was used ubiquitously in translation
    software before machine translation came into the picture. Whether you’re looking
    for alternative naming conventions or mistakes, fuzzy logic and matching are able
    to offer us intelligent ways for machines to find the things we’re looking for.'
  prefs: []
  type: TYPE_NORMAL
- en: As with robotics and other areas of AI, we can see an ensemble with fuzzy matching
    as well. We’re seeing ML applied to fuzzy matching in an effort to improve accuracy.
    But even without ML, fuzzy logic and fuzzy matching can stand on their own as
    a subset of AI that’s been relied on heavily before ML and continues to be prominent
    to this day for things such as translation or data/database management.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve gone over all the major areas of AI to understand what they are
    and how they offer value, we can now get onto the feature level to see how these
    major areas can translate into various features you can integrate into your product.
    The point of this isn’t to give you ideas on what kinds of products to build but
    rather to understand the variety of how these innovations can be used and work
    together.
  prefs: []
  type: TYPE_NORMAL
- en: Micro AI – Feature level
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It can be daunting to understand how various categories of AI fit together,
    and the reality is that in real-world AI product applications, many of these are
    working in concert. Seeing various examples of how that happens, particularly
    when we get to the later sections in this chapter, will offer us a way to see
    how much opportunity and potential AI really offers us!
  prefs: []
  type: TYPE_NORMAL
- en: We will consolidate ML, DL, computer vision, and NLP into their own section
    because these models are often used collaboratively as well. That collaboration
    can then bleed into the other subsets of AI. Robotics, expert systems, and fuzzy
    logic can remain in their own sections because their applications are so specialized
    in and of themselves. Seeing how subsets of AI can work together further results
    in the greater complexity and growth that powers innovation for our markets and
    brings to market products that serve, delight, and capture our hearts.
  prefs: []
  type: TYPE_NORMAL
- en: ML (traditional/DL/computer vision/NLP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which type of model you’re using will depend on your use case and goals for
    your product. As we’ve covered in varying chapters, the exact model you go with
    will depend on the data you have, how you’re able to tune your hyperparameters,
    and what level of explainability and transparency you’ll need for your use case.
    We’re focusing on AI/ML native products in this section of the book and, as such,
    identifying which ML model(s) you will use for the foundation of your product
    will be an important decision, and all features you add onto your core product
    will also be an act of doing a cost-benefit analysis of the models you’re adding
    to power those features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most products that are out there right now are not AI/ML native in that they
    are existing software programs and packages that are incrementally adding new
    AI features and then rebranding their products as AI products. This isn’t exactly
    true, but it does create a marketing sensation around their product that they
    can then capitalize on through their customer outreach and ads. But from one PM
    to another, we do want to make it very clear: these are not products foundationally
    rooted in AI/ML because if they were, models would be at the heart of the underlying
    logic their product is built on. Rather, these are products that improve certain
    features with the use of AI/ML, but the true impact of leveraging this tech stays
    superficially at the feature level and doesn’t substantially impact the product
    in a meaningful way. Apart from productized ML services, personal assistants,
    and autonomous cars, there are very few true-blue AI products out there. But with
    the passage of time and with adequate funding from the private sector, more will
    continue to enter the market.'
  prefs: []
  type: TYPE_NORMAL
- en: NLP models will be highly useful if your product is heavily working with textual
    data. Things such as email filters, smart assistants and conversational AIs, search
    result analyzers, voice-to-text, spellcheck, autocomplete, sentiment analysis,
    predictive text, translation, and data and text analysis are all potential features
    you can incorporate into your product to make use of NLP. NLP is also particularly
    useful when you’re trying to optimize your customer communications so that you
    can make sense of large quantities of feedback that can then be incorporated to
    identify additional future features of your product as you’re building your product
    roadmap.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision models account for edges, corners, interest points, blobs, regions
    of interest points, and ridges as they make sense of an image and try to turn
    it into a matrix of numbers. This conversion is how they’re able to understand
    images, so you’ll use these models if your product is trying to observe and identify
    objects or patterns in an environment. For instance, analyzing images of areas
    to create potential future projections of possible environmental decay and damage
    would also be a great use for computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, the use of ML will be highly impactful for a few different use cases.
    The following are the most common use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rank/recommendation systems**: If you’re looking for a ranked list of a grouping
    of similar products or documents, recommendation systems will get you there. If
    your product is heavily focused on offering a selection of options or the discovery
    of new options to your customers, AI-/ML-powered rankings will be a great use
    of AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction**: Using historical data points toward the goal of predicting
    future values being central to your product is one of the most applicable ways
    to use ML models, and you’ll be able to choose the model that best supports prediction
    for your product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: Similar to the logic in recommendation systems, classification
    use cases of ML will help to group people, customers, objects, choices, and subjects
    into certain categories. You’re taking generalized data and forming classes from
    that general data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation**: Maybe your product is generating content for your users, whether
    that’s visual or textual. You’re able to feed these models examples to learn from
    so that they can create new samples altogether based on the examples you’ve given.
    Whatever inputs you give generative models, they will create outputs you can sell
    or pass on to your users for other purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: These are other models that can group information together.
    The only difference is with clustering, the models come up with their own groupings,
    which is a way for ML to help create distinctions between data points that your
    engineers or users couldn’t.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we’ve covered some of the major use cases for applying ML at
    the feature level. Many of these use cases should be familiar to you by now. In
    the following section, we will cover the area of robotics.
  prefs: []
  type: TYPE_NORMAL
- en: Robotics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With robotics, you’re creating physical structures that operate optimally for
    certain use cases. As we saw in the *Robotics* section (see under *Macro AI –
    Foundations and umbrellas*), some robotics do relate to each other, particularly
    when we think about teleoperated robots and augmented robots where there is some
    overlap. But by and large, these are created specifically for the use case they
    have. We can think of robotics as the hardware and the incorporation of ML as
    the software upgrade package.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the example of an autonomous robot that is scanning its environment
    to find the optimal route or chain of events by which to operate. It might be
    able to derive insights on its own without using ML algorithms to optimize its
    route, but if it does want to get better over time, maybe its logs can be used
    to teach it to maneuver more effectively. With teleoperated robots, DL can be
    used to optimize the speed, strength, or depth at which it’s handling its subjects
    and quickly build on the best practices surgeons spend decades mastering (to take
    the example of robotics in surgery). Because DL’s performance is more important
    than its explainability in surgical contexts, it can be a useful feature for medical
    devices. Even augmenting robots ([https://arpost.co/2021/08/26/augmented-reality-in-robotics-enhances-robots/](https://arpost.co/2021/08/26/augmented-reality-in-robotics-enhances-robots/))
    can benefit from collaborations with other AI domains to test and simulate their
    uses before they’re opened up to testing with real people.
  prefs: []
  type: TYPE_NORMAL
- en: A huge area of collaboration that gets a lot of press, however, is with humanoid
    robots, which need a fair amount of overlap with other AI domains to come to market.
    After all, you can’t call something a humanoid robot if it can’t listen, speak,
    or see like a human can. Humanoid robots rely on computer vision to see their
    surroundings, identify people and objects, and move throughout their environments.
    They also rely on NLP to understand what people are saying to them and to offer
    thoughtful responses. They might also be programmed with ML or DL models to optimize
    for varying situations they might encounter or to predict what people might ask
    of them and how to respond.
  prefs: []
  type: TYPE_NORMAL
- en: Expert systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Expert systems essentially run on `if-else` logic, so they come with a set of
    instructions that rarely change, and when they do, those changes are hardcoded.
    The limitations they experience are a lack of responsiveness to the data they
    process and a huge amount of complexity within the rules; they don’t quite capture
    the full nuance that a human might and they don’t do well when novelty is introduced.
    These limitations are ripe for incorporating ML so that as new scenarios, new
    data, and new experiences are introduced, some of the logic can also be updated.
  prefs: []
  type: TYPE_NORMAL
- en: This is a good example because it shows that ML need not replace an expert system
    but it can collaboratively work alongside the expert so that the expert becomes
    more of an expert without the necessity of a human to make changes to its decision-making
    process. You might have a need, as an expert system creator, to have one machine
    in place that offers objective, structured static instructions for your use case.
    Building an end-to-end ML product to solve that might be unnecessarily complex,
    innovative, or expensive to run, and you might get a lot more out of your product
    by just using ML to optimize the steps in your system rather than replace the
    whole thing altogether. There aren’t many present-day examples of this to draw
    from, but it is theoretically possible to combine expert systems with ML.
  prefs: []
  type: TYPE_NORMAL
- en: Fuzzy logic/fuzzy matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of features that can incorporate fuzzy logic or fuzzy matching
    if you’re building a product that needs to account for certain similarities in
    entries you have. Perhaps you’re building a scraper that’s looking to find matching
    hotel listings or a rental that has a certain number of amenities, and you’re
    trying to compare them. Or maybe you’re looking for a review consolidator that
    will find reviews related to a business or product, and you’re matching the reviews
    based on the description.
  prefs: []
  type: TYPE_NORMAL
- en: Another common example might be searching your database to consolidate your
    users that come from mobile versus desktop or users that interact with varying
    **business units** (**BUs**) within your company to create a single customer view.
    Your internal customer data might be in multiple areas. Perhaps you have multiple
    databases or you have your customer data in HubSpot and your purchase data in
    Stripe and you’re looking to make associations. If you don’t have a concrete unique
    identifier that matches them, you might have to use something such as fuzzy matching
    to make those associations and get rid of duplicates in your database as well.
  prefs: []
  type: TYPE_NORMAL
- en: Fuzzy matching will allow you to select certain keywords that your fuzzy engine
    will optimize for so that you can then arrange them by the percentage of the match
    and group them together. Then, once you have these examples and you’re able to
    create some groupings, you might want to then incorporate some ML to take the
    outputs of that fuzzy matching so that the matches get better over time. ML models
    will optimize how well that fuzzy matching works over time and as your business
    grows. You may want to take this a step further and use NLP for fuzzy matching
    things such as translations or phrases that belong together as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve had a chance to give an overview of the applied use cases and
    features from the major areas of AI in the preceding sections, let’s turn our
    attention to a few positive examples from the real world.
  prefs: []
  type: TYPE_NORMAL
- en: Successes – Examples that inspire
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be looking at examples of complex, collaborative AI
    products that use a number of models and build a product intuition of how they
    can be inspired by examples where companies had a considerable amount of commercial
    success. The purpose of this section is to show real-life examples where a product
    used an assortment of AI/ML specializations to deliver a product that gave value
    to its end users and market.
  prefs: []
  type: TYPE_NORMAL
- en: The product examples we will be covering in the following section include **Lensa**
    (a generative AI selfie app) and **PeriWatch Vigilance** (a health app for mothers
    and babies made by PeriGen).
  prefs: []
  type: TYPE_NORMAL
- en: Lensa
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given the current sensation of the Lensa app, we thought this would serve as
    a great first example. Lensa took the internet by storm with its fantasy AI selfie-generation
    app. The idea is you feed it between 10 and 20 images for the neural networks
    to learn from, and based on that training, it will generate 50 or more new selfies
    of your own image in new fantastical ways. Part of its success lies in the novelty
    of the app: there aren’t many apps out there that can do this.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another part of its success lies in our own vanity: it’s an app that’s promising
    to deliver on giving us new manifestations of our likeness in ways that are hard
    to recreate in the real world. Could we sit down with a canvas and try to render
    our own likeness as a forest nymph? Sure, we could. But the decision to do so,
    along with the time and skill commitment, would make it much harder than the five
    seconds it takes for us to upload selfies we''ve previously taken and send them
    to Lensa.'
  prefs: []
  type: TYPE_NORMAL
- en: “*“Our goal is to move forward mobile photography and video creation to the
    next level using neural networks, deep learning and computer vision technics.
    We aim to create new ways for people to express their emotions through the camera,”*
    *Prisma Labs wrote on its LinkedIn* *profile.*” ([https://builtin.com/artificial-intelligence/prisma-labs-lensa-ai-selfies](https://builtin.com/artificial-intelligence/prisma-labs-lensa-ai-selfies))
  prefs: []
  type: TYPE_NORMAL
- en: The combination of DL and computer vision here is a powerful one because the
    neural networks are looking at the examples we’re feeding them, generating new
    manifestations of ourselves with this training, and delivering new images that
    aim to improve on our likeness while also adding a fantastical element. It’s also
    interesting to note that its popularity does not just stem from the novelty of
    a neural net-powered app but also from the public’s willingness to publicly share
    the kind of flattering images of themselves that are hard to come by with traditional
    filters and selfies. Lensa soared in popularity, making the generative AI app
    a viral sensation in a very short time.
  prefs: []
  type: TYPE_NORMAL
- en: PeriGen
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PeriGen, Inc. had already built a successful reputation on computerized early
    warning systems for mothers and infants, and on April 6, 2022, it received an
    award from CogX in London for its PeriWatch Vigilance product, which uses pattern
    recognition to identify and flag potentially harmful trends in childbirth. The
    award was given for its ability to harness AI for new challenges in the health
    sector precisely because its AI product was significantly more successful at recognizing
    trends than its human counterparts have observed.
  prefs: []
  type: TYPE_NORMAL
- en: Because the company already had a weak baseline to improve on, and because it’s
    been invested in the growth and success of PeriWatch Vigilance for over six years
    now, its good work did not go unnoticed. PeriWatch Vigilance is an automated app
    that serves as an early warning system and clinical decision-support tool for
    OBGYNs. It’s used for a variety of reasons, including early intervention, standardizing
    care, and helping the clinics that use it with other efficiencies. It has been
    able to track patients across multiple sites and notify clinicians of abnormalities,
    as well as analyze all the content it has for fetal heart-rate anomalies, contractions,
    and labor progression, in addition to keeping track of maternal vital signs.
  prefs: []
  type: TYPE_NORMAL
- en: Its commitment to consistently improving its product, continuously offering
    its community of partners and patients quality service and value, and the evolution
    of its product to adjust to the needs of the market in an ethical way has given
    the company a solid reputation to continue to bring something of value into the
    world. It’s a testament to how AI can be celebrated and honored when it truly
    focuses on the needs and preferences of the humans it is trying to help.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen examples of AI products that had a positive reception, let’s
    take a look at a few of the common pitfalls we commonly see with AI products and
    a few product examples to accompany those pitfalls in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges – Common pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve spent a considerable amount of time talking about how to build AI/ML products
    and use models in a way that empowers your products. We’ve also discussed the
    hype and commercial excitement about AI. In this section, we’ll temper this hype
    by understanding why certain AI/ML products fail. We’ll be looking at a few real-world
    examples that highlight some of the common reasons why AI deployments have received
    controversy. We will also look into some of the underlying themes within that
    controversy for new AI products and their creators to try to avoid.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will focus on challenges associated with ethics,
    performance, and safety and their accompanying examples.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Companies have long struggled with maintaining the quality and ethics of consumer-facing
    conversational AIs. If you recall back in 2016 when Microsoft unleashed its AI
    named Tay onto the Twittersphere, it took less than 24 hours for Tay to spew racist,
    sexist, homophobic rhetoric against Twitter users ([https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)).
  prefs: []
  type: TYPE_NORMAL
- en: This phenomenon seems to be happening again with the latest craze ChatGPT, which
    is made by OpenAI. ChatGPT is derived from OpenAI’s massive language model GPT-3
    and, according to Sam Biddle, ([https://theintercept.com/2022/12/08/openai-chatgpt-ai-bias-ethics/](https://theintercept.com/2022/12/08/openai-chatgpt-ai-bias-ethics/))
    it is “*staggeringly impressive, uncannily impersonating an intelligent person
    (or at least someone trying their hardest to sound intelligent) using generative
    AI, software that studies massive sets of inputs to generate new outputs in response
    to* *user prompts.*”
  prefs: []
  type: TYPE_NORMAL
- en: One of the things that makes ChatGPT so good at mimicking the way humans speak
    is that it’s trained on billions of texts and human coaching. While it’s great
    at being able to convince someone they’re interacting with a highly intelligent
    entity and in many ways passing the Turing test, it seems to be following in the
    same footsteps as Tay did back in 2016\. Though its biases aren’t exactly coming
    from the feedback from the people it interacts with, there are still many biases
    that are found in the texts it’s been trained on.
  prefs: []
  type: TYPE_NORMAL
- en: 'One tester asked ChatGPT to write a Python program outlining whether someone
    is a good scientist based on their race and gender, as outlined in the tweet shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – ChatGPT bias tweet](img/B18935_9_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – ChatGPT bias tweet
  prefs: []
  type: TYPE_NORMAL
- en: This alarmingly highlights the importance of companies remaining extremely vigilant
    and committed to analyzing their products for bias. Though not all products will
    have the same viral appeal as ChatGPT, all AI products are going to be sensitive
    to bias that’s inherent in the data they’re being trained on. This example also
    serves as a stark reminder that all PMs need to make sure their products are thoroughly
    tested for bias before they’re released to market and as an ongoing best practice.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the clear reasons to rein AI in and avoid harm to those that interact
    with it, there are still prominent voices that believe de-biasing AI systems is
    a way of *censoring* them. This is an unethical viewpoint to have, placing AIs
    on an equal level as the humans that interact with them. It’s not censoring. AIs
    are not sentient, and their biases can cause real harm to those that actually
    do possess consciousness.
  prefs: []
  type: TYPE_NORMAL
- en: Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a highly vigilant testing process that encompasses all use cases and
    enough potential edge cases is crucial for AI to be well received. Some of the
    greatest AI failures have come from the negligence of companies that don’t do
    their due diligence when they’re testing their products for market. It might be
    impossible to account for all potential edge cases where AI is concerned. But
    when we’re working with people in the real world, it’s better to be over-prepared
    than underprepared.
  prefs: []
  type: TYPE_NORMAL
- en: During a live stream in 2020, well into the global pandemic, with folks at home
    wanting to watch a soccer match, an AI camera operator made by Pixellot was deployed
    by Inverness Caledonian Thistle FC, a Scottish soccer team ([https://www.theverge.com/tldr/2020/11/3/21547392/ai-camera-operator-football-bald-head-soccer-mistakes](https://www.theverge.com/tldr/2020/11/3/21547392/ai-camera-operator-football-bald-head-soccer-mistakes)).
    The built-in ball-tracking technology that was embedded in the Pixellot cameras
    started to hone in on one of the linesmen because it confused his bald head for
    the soccer ball. Rather than seeing the best view of the ball and, therefore,
    the action in the game, viewers were instead consistently seeing the back of someone’s
    head.
  prefs: []
  type: TYPE_NORMAL
- en: Though this was relatively harmless, it became a source of scandal and ridicule
    for the company. This incident highlights the importance of testing software thoroughly
    before bringing it to market. Surely there could have been less high-profile games
    the software could have been tested on before a live game. Having a multi-staged
    approach to testing is vital for companies that are bringing products to market
    that work on real-time use cases such as live sporting events. A good compromise
    would have been analyzing performance during the first phase of testing, then
    testing with a beta user team with a live game and not recordings, and once there
    was a consistent level of performance, finally unleashing the cameras on a live
    game.
  prefs: []
  type: TYPE_NORMAL
- en: Safety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve discussed the amazing ability of AI to help in healthcare, particularly
    with looking for treatments for certain diseases and helping with diagnostics.
    While this area of AI adoption looks incredibly promising over time, we do have
    a couple of public examples of failure. The first is IBM Watson’s lofty 2013 goal
    of curing cancer. Watson for Oncology partnered with the University of Texas MD
    Anderson Cancer Center on the *Oncology Expert Advisor* system, which was a repository
    of the cancer center’s patient data and research database ([https://www.lexalytics.com/blog/stories-ai-failure-avoid-ai-fails-2020/](https://www.lexalytics.com/blog/stories-ai-failure-avoid-ai-fails-2020/)).
    What resulted were startling claims that Watson for Oncology was giving its users
    dangerous advice for cancer treatments.
  prefs: []
  type: TYPE_NORMAL
- en: Because the AI system was trained on simulated data of theoretical cancer patients
    rather than Anderson Cancer Center’s real patient data, meaning the training data
    was highly flawed, the recommendations it was giving were unethical and negligent.
    According to the *Lexalytics* blog ([https://www.lexalytics.com/blog/stories-ai-failure-avoid-ai-fails-2020/](https://www.lexalytics.com/blog/stories-ai-failure-avoid-ai-fails-2020/)),
    “*Medical specialists and customers identified “multiple examples of unsafe and
    incorrect treatment recommendations,” including one case where Watson suggested
    that doctors give a cancer patient with severe bleeding a drug that could worsen
    the bleeding.*” Several years and $62 million later, MD Anderson ended its partnership
    with IBM Watson.
  prefs: []
  type: TYPE_NORMAL
- en: Another example comes from Google’s AI research team, which unveiled a product
    to help with a diabetic eye disease called **diabetic retinopathy** (**DR**).
    A neural network was used in this product to try and help interpret signs of DR
    in retinal photographs to get potential patients with this disease screened more
    quickly. Google partnered with Thailand’s Ministry of Public Health on the first
    deployment but it didn’t provide accurate diagnoses after being tested on 4.5
    million patients and after years of tweaking the models. Most of the backlash
    came from the patients themselves, and part of the issue with this is the AI made
    them question its safety and competency, but the episode also contributed to general
    distrust of AI ([https://www.forbes.com/sites/forbestechcouncil/2020/06/09/three-insights-from-googles-failed-field-test-to-use-ai-for-medical-diagnosis/?sh=859fc65bac42](https://www.forbes.com/sites/forbestechcouncil/2020/06/09/three-insights-from-googles-failed-field-test-to-use-ai-for-medical-diagnosis/?sh=859fc65bac42)).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many examples of AI failures, but the aforementioned themes are the
    major categories of issues the failures have in common: they’re either unethical
    because of bias, they just haven’t been tested properly and experience embarrassing
    lapses in performance, or they are downright harmful from a safety perspective.
    Either way, all these failures stem from either incompetence or negligence on
    the part of these products'' creators, because they missed giving their products
    the best chance they have for success, and that’s rigorous testing. Because we’re
    at an inflection point with AI, maintaining good AI stewardship and being transparent
    about a desire to bring a product to market that won’t just perform well but will
    be ethical and safe is crucial for future technologists and PMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve covered a lot of ground. We’ve discussed the various
    areas of AI at a high level, giving us a macro landscape of the variety of options
    we can take when building AI products. We’ve also brought those options down to
    the feature level, giving us a micro view of applied AI features. We were then
    able to look at a few examples of collaborative AI products that have received
    positive feedback and acclaim, along with a few examples that highlight the challenges
    of AI products. Building AI products is still new. We’re still building out new
    use cases, and with every new AI product that comes to market, we’re able to discover
    new pathways to use these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: This means that every newly applied use case has the potential to show the world
    what AI can do, and that’s what makes the current phase we’re in so exciting.
    In order to uncover innovative new uses for AI/ML we must be willing to make mistakes
    and learn from those mistakes. Building a successful product comes with its own
    trials and tribulations. It isn’t until we go through the process of balancing
    the technical requirements of our product with the expectations of our customers
    and market that we find success. In the next chapter, we will be discussing some
    common markers for success. This will include benchmarking your product for performance
    and establishing KPIs, considerations of cost and pricing, and, ultimately, their
    impact on growth hacking.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://www.hansonrobotics.com/sophia/](https://www.hansonrobotics.com/sophia/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://theintercept.com/2022/12/08/openai-chatgpt-ai-bias-ethics/](https://theintercept.com/2022/12/08/openai-chatgpt-ai-bias-ethics/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.thedailybeast.com/openais-impressive-chatgpt-chatbot-is-not-immune-to-racism](https://www.thedailybeast.com/openais-impressive-chatgpt-chatbot-is-not-immune-to-racism)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.theverge.com/tldr/2020/11/3/21547392/ai-camera-operator-football-bald-head-soccer-mistakes](https://www.theverge.com/tldr/2020/11/3/21547392/ai-camera-operator-football-bald-head-soccer-mistakes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stories of AI Failure and How to Avoid Similar AI* *Fails*: [https://www.lexalytics.com/blog/stories-ai-failure-avoid-ai-fails-2020/](https://www.lexalytics.com/blog/stories-ai-failure-avoid-ai-fails-2020/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Three Insights From Google’s ‘Failed’ Field Test To Use AI For Medical* *Diagnosis*:
    [https://www.forbes.com/sites/forbestechcouncil/2020/06/09/three-insights-from-googles-failed-field-test-to-use-ai-for-medical-diagnosis/?sh=2b4233c4bac4](https://www.forbes.com/sites/forbestechcouncil/2020/06/09/three-insights-from-googles-failed-field-test-to-use-ai-for-medical-diagnosis/?sh=2b4233c4bac4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.theverge.com/2021/9/9/22660467/irobot-roomba-ai-dog-poop-avoidance-j7-specs-price](https://www.theverge.com/2021/9/9/22660467/irobot-roomba-ai-dog-poop-avoidance-j7-specs-price)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
