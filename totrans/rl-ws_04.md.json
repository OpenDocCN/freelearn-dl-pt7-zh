["```py\nfrom gym import envs\nprint(envs.registry.all())\n```", "```py\n[EnvSpec(DoubleDunk-v0), EnvSpec(InvertedDoublePendulum-v0), \nEnvSpec(BeamRider-v0), EnvSpec(Phoenix-ram-v0), EnvSpec(Asterix-v0), \nEnvSpec(TimePilot-v0), EnvSpec(Alien-v0), EnvSpec(Robotank-ram-v0), \nEnvSpec(CartPole-v0), EnvSpec(Berzerk-v0), EnvSpec(Berzerk-ram-v0), \nEnvSpec(Gopher-ram-v0), ...\n```", "```py\n    import gym\n    ```", "```py\n    env = gym.make('CartPole-v0')\n    env.reset()\n    ```", "```py\n    array([ 0.03972635,  0.00449595,  0.04198141, -0.01267544])\n    ```", "```py\n    for _ in range(1000):\n        env.render()\n        # take a random action\n        _, _, done, _ = env.step(env.action_space.sample())\n        if done:\n            env.reset()\n    env.close()\n    ```", "```py\nimport gym\nenv = gym.make('CartPole-v0')\nprint(\"Action space =\", env.action_space)\nprint(\"Observation space =\", env.observation_space)\n```", "```py\nAction space = Discrete(2)\nObservation space = Box(4,)\n```", "```py\nprint(\"Observations superior limit =\", env.observation_space.high)\nprint(\"Observations inferior limit =\", env.observation_space.low)\n```", "```py\nObservations superior limit = array([ 2.4, inf, 0.20943951, inf])\nObservations inferior limit = array([-2.4, -inf,-0.20943951, -inf])\n```", "```py\nimport gym\nenv = gym.make('CartPole-v0')\nfor i_episode in range(20):\n    observation = env.reset()\n    for t in range(100):\n        env.render()\n        print(observation)\n        action = env.action_space.sample()\n        observation, reward, done, info = env.step(action)\n        if done:\n            print(\"Episode finished after {} timesteps\".format(t+1))\n            break\nenv.close()\n```", "```py\n[-0.061586   -0.75893141  0.05793238  1.15547541]\n[-0.07676463 -0.95475889  0.08104189  1.46574644]\n[-0.0958598  -1.15077434  0.11035682  1.78260485]\n[-0.11887529 -0.95705275  0.14600892  1.5261692 ]\n[-0.13801635 -0.7639636   0.1765323   1.28239155]\n[-0.15329562 -0.57147373  0.20218013  1.04977545]\nEpisode finished after 14 timesteps\n[-0.02786724  0.00361763 -0.03938967 -0.01611184]\n[-0.02779488 -0.19091794 -0.03971191  0.26388759]\n[-0.03161324  0.00474768 -0.03443415 -0.04105167]\n```", "```py\n    import gym\n    env = gym.make('CartPole-v0')\n    ```", "```py\n    for i_episode in range(20):\n        observation = env.reset()\n        for t in range(100):\n    ```", "```py\n            env.render()\n            print(observation)\n    ```", "```py\n            action = RL_agent.select_action(observation)\n    ```", "```py\n            observation, reward, done, info = env.step(action)\n    ```", "```py\n            if done:\n                print(\"Episode finished after {} timesteps\"\\\n                      .format(t+1))\n                break\n    env.close()\n    ```", "```py\nimport gym\nenv = gym.make('CartPole-v0')\nfor i_episode in range(20):\n    observation = env.reset()\n    for t in range(100):\n        env.render()\n        print(observation)\n        action = RL_agent.select_action(observation)\n        new_observation, reward, done, info = env.step(action)\n        RL_agent.train(observation, action, reward, \\\n                       new_observation)\n        observation = new_observation\n        if done:\n            print(\"Episode finished after {} timesteps\"\\\n                  .format(t+1))\n            break\nenv.close()\n```", "```py\n        RL_agent.train(observation, action, reward, new_observation)\n```", "```py\nimport gym\nfrom gym import spaces\nclass CustomEnv(gym.Env):\n    \"\"\"Custom Environment that follows gym interface\"\"\"\n    metadata = {'render.modes': ['human']}\n    def __init__(self, arg1, arg2, ...):\n      super(CustomEnv, self).__init__()\n      # Define action and observation space\n      # They must be gym.spaces objects\n      # Example when using discrete actions:\n      self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)\n      # Example for using image as input:\n      self.observation_space = spaces.Box\\\n                               (low=0, high=255, \\\n                                shape=(HEIGHT, WIDTH, \\\n                                       N_CHANNELS), \\\n                                dtype=np.uint8)\n    def step(self, action):\n      # Execute one time step within the environment\n      ...\n      # Compute reward\n      ...\n      # Check if in final state\n      ...\n      return observation, reward, done, info\n    def reset(self):\n      # Reset the state of the environment to an initial state\n      ...\n      return observation\n    def render(self, mode='human', close=False):\n      # Render the environment to the screen\n      ...\n      return\n```", "```py\n    import gym\n    # register Universe environments into Gym\n    import universe\n    ```", "```py\n    # Universe env ID here\n    env = gym.make('flashgames.DuskDrive-v0')\n    observation_n = env.reset()\n    ```", "```py\n    while True:\n        # agent which presses the Up arrow 60 times per second\n        action_n = [[('KeyEvent', 'ArrowUp', True)] \\\n                    for _ in observation_n]\n        observation_n, reward_n, done_n, info = env.step(action_n)\n        env.render()\n    ```", "```py\n    git clone https://github.com/openai/universe && pip install -e universe\n    ```", "```py\n    # -p 5900:5900 and -p 15900:15900 \n    # expose the VNC and WebSocket ports\n    # --privileged/--cap-add/--ipc=host \n    # needed to make Selenium work\n    $ docker run --privileged --cap-add=SYS_ADMIN --ipc=host \\\n        -p 5900:5900 -p 15900:15900 quay.io/openai/universe.flashgames\n    ```", "```py\nimport gym\nimport universe # register Universe environments into Gym\n# Universe [environment ID]\nenv = gym.make('flashgames.DuskDrive-v0')\n\"\"\"\nIf using docker-machine, replace \"localhost\" with specific Docker IP\n\"\"\"\nenv.configure(remotes=\"vnc://localhost:5900+15900\")\nobservation_n = env.reset()\nwhile True:\n    # agent which presses the Up arrow 60 times per second\n    action_n = [[('KeyEvent', 'ArrowUp', True)] \\\n                for _ in observation_n]\n    observation_n, reward_n, done_n, info = env.step(action_n)\n    env.render()\n```", "```py\n    import numpy as np\n    import gym\n    import tensorflow as tf\n    ```", "```py\n    env = gym.make('CartPole-v0')\n    ```", "```py\n    print(\"Action space =\", env.action_space)\n    print(\"Observation space =\", env.observation_space)\n    ```", "```py\n    Action space = Discrete(2)\n    Observation space = Box(4,)\n    ```", "```py\n    print(\"Action space dimension =\", env.action_space.n)\n    print(\"Observation space dimension =\", \\\n          env.observation_space.shape[0])\n    ```", "```py\n    Action space dimension = 2\n    Observation space dimension = 4\n    ```", "```py\n    model = tf.keras.Sequential\\\n            ([tf.keras.layers.Dense(64, activation='relu', \\\n              input_shape=[env.observation_space.shape[0]]), \\\n              tf.keras.layers.Dense(64, activation='relu'), \\\n              tf.keras.layers.Dense(env.action_space.n, \\\n              activation=\"softmax\")])\n    model.summary()\n    ```", "```py\n    Model: \"sequential_2\"\n    _________________________________________________________________\n    Layer (type)               Output Shape                  Param #   =================================================================\n    dense (Dense)             (None, 64)                      320       _________________________________________________________________\n    dense_1 (Dense)             (None, 64)                     4160      _________________________________________________________________\n    dense_2 (Dense)             (None, 2)                       130       =================================================================\n    Total params: 4,610\n    Trainable params: 4,610\n    Non-trainable params: 0\n    ```", "```py\n    t = 1\n    observation = env.reset()\n    ```", "```py\n    while True:\n        env.render()\n        # Print the observation\n        print(\"Observation = \", observation)\n    ```", "```py\n        action_probabilities =model.predict\\\n                              (np.expand_dims(observation, axis=0))\n        action = np.argmax(action_probabilities)\n        print(\"Action = \", action)\n    ```", "```py\n        observation, reward, done, info = env.step(action)\n        # Print received reward\n        print(\"Reward = \", reward)\n        # If terminal state reached, close the environment\n        if done:\n            print(\"Episode finished after {} timesteps\".format(t+1))\n            break\n        t += 1\n    env.close()\n    ```", "```py\n    Observation =  [-0.00324467 -1.02182257  0.01504633  1.38740738]\n    Action =  0\n    Reward =  1.0\n    Observation =  [-0.02368112 -1.21712879  0.04279448  1.684757  ]\n    Action =  0\n    Reward =  1.0\n    Observation =  [-0.0480237  -1.41271906  0.07648962  1.99045154]\n    Action =  0\n    Reward =  1.0\n    Observation =  [-0.07627808 -1.60855467  0.11629865  2.30581208]\n    Action =  0\n    Reward =  1.0\n    Observation =  [-0.10844917 -1.80453455  0.16241489  2.63191088]\n    Action =  0\n    Reward =  1.0\n    Episode finished after 11 timesteps\n    ```", "```py\nsave_dir = \"./\"\nmodel_name = \"modelName\"\nprint(\"Saving best model to {}\".format(save_dir))\nmodel.save_weights(os.path.join(save_dir,\\\n                                'model_{}.h5'.format(model_name)))\n```", "```py\nSaving best model to ./\n```", "```py\npython -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4\n    --num_timesteps=2e7 --save_path=./models/pong_20M_ppo2\n    --log_path=./logs/Pong/\n```", "```py\npython -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4\n    --num_timesteps=0 --load_path=./models/pong_20M_ppo2 --play\n```", "```py\n    from baselines.ppo2.ppo2 import learn\n    from baselines.ppo2 import defaults\n    from baselines.common.vec_env import VecEnv, VecFrameStack\n    from baselines.common.cmd_util import make_vec_env, make_env\n    from baselines.common.models import register\n    import tensorflow as tf\n    ```", "```py\n    @register(\"custom_mlp\")\n    def custom_mlp(num_layers=2, num_hidden=64, activation=tf.tanh):\n        \"\"\"\n        Stack of fully-connected layers to be used in a policy /\n        q-function approximator\n        Parameters:\n        ----------\n        num_layers: int   number of fully-connected layers (default: 2)\n        num_hidden: int   size of fully-connected layers (default: 64)\n        activation:       activation function (default: tf.tanh)\n        Returns:\n        -------\n        function that builds fully connected network with a \n        given input tensor / placeholder\n        \"\"\"\n        def network_fn(input_shape):\n            print('input shape is {}'.format(input_shape))\n            x_input = tf.keras.Input(shape=input_shape)\n            h = x_input\n            for i in range(num_layers):\n                h = tf.keras.layers.Dense\\\n                    (units=num_hidden, \\\n                     name='custom_mlp_fc{}'.format(i),\\\n                     activation=activation)(h)\n            network = tf.keras.Model(inputs=[x_input], outputs=[h])\n            network.summary()\n            return network\n        return network_fn\n    ```", "```py\n    def build_env(env_id, env_type):\n        if env_type in {'atari', 'retro'}:\n            env = make_vec_env\\\n                  (env_id, env_type, 1, None, gamestate=None,\\\n                   reward_scale=1.0)\n            env = VecFrameStack(env, 4)\n        else:\n            env = make_vec_env\\\n                  (env_id, env_type, 1, None,\\\n                   reward_scale=1.0, flatten_dict_observations=True)\n        return env\n    ```", "```py\n    env_id = 'CartPole-v0'\n    env_type = 'classic_control'\n    print(\"Env type = \", env_type)\n    env = build_env(env_id, env_type)\n    hidden_nodes = 64\n    hidden_layers = 2\n    model = learn(network=\"custom_mlp\", env=env, \\\n                  total_timesteps=1e4, num_hidden=hidden_nodes, \\\n                  num_layers=hidden_layers)\n    ```", "```py\n    Env type =  classic_control\n    Logging to /tmp/openai-2020-05-11-16-00-34-432546\n    input shape is (4,)\n    Model: \"model\"\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #  \n    =================================================================\n    input_1 (InputLayer)         [(None, 4)]               0        \n    _________________________________________________________________\n    custom_mlp_fc0 (Dense)       (None, 64)                320      \n    _________________________________________________________________\n    custom_mlp_fc1 (Dense)       (None, 64)                4160     \n    =================================================================\n    Total params: 4,480\n    Trainable params: 4,480\n    Non-trainable params: 0\n    _________________________________________________________________\n    -------------------------------------------\n    | eplenmean               | 22.3          |\n    | eprewmean               | 22.3          |\n    | fps                     | 696           |\n    | loss/approxkl           | 0.00013790815 |\n    | loss/clipfrac           | 0.0           |\n    | loss/policy_entropy     | 0.6929994     |\n    | loss/policy_loss        | -0.0029695872 |\n    | loss/value_loss         | 44.237858     |\n    | misc/explained_variance | 0.0143        |\n    | misc/nupdates           | 1             |\n    | misc/serial_timesteps   | 2048          |\n    | misc/time_elapsed       | 2.94          |\n    | misc/total_timesteps    | 2048          |\n    ```", "```py\n    obs = env.reset()\n    if not isinstance(env, VecEnv):\n        obs = np.expand_dims(np.array(obs), axis=0)\n    episode_rew = 0\n    while True:\n        actions, _, state, _ = model.step(obs)\n        obs, reward, done, info = env.step(actions.numpy())\n        if not isinstance(env, VecEnv):\n            obs = np.expand_dims(np.array(obs), axis=0)\n        env.render()\n        print(\"Reward = \", reward)\n        episode_rew += reward\n        if done:\n            print('Episode Reward = {}'.format(episode_rew))\n            break\n    env.close()\n    ```", "```py\n    #[...]\n    Reward =  [1.]\n    Reward =  [1.]\n    Reward =  [1.]\n    Reward =  [1.]\n    Reward =  [1.]\n    Reward =  [1.]\n    Reward =  [1.]\n    Reward =  [1.]\n    Reward =  [1.]\n    Reward =  [1.]\n    Episode Reward = [28.]\n    ```", "```py\n    !python -m baselines.run --alg=ppo2 --env=CartPole-v0 \n    --num_timesteps=1e4 --save_path=./models/CartPole_2M_ppo2 \n    --log_path=./logs/CartPole/\n    ```", "```py\n    -------------------------------------------\n    | eplenmean               | 20.8          |\n    | eprewmean               | 20.8          |\n    | fps                     | 675           |\n    | loss/approxkl           | 0.00041882397 |\n    | loss/clipfrac           | 0.0           |\n    | loss/policy_entropy     | 0.692711      |\n    | loss/policy_loss        | -0.004152138  |\n    | loss/value_loss         | 42.336742     |\n    | misc/explained_variance | -0.0112       |\n    | misc/nupdates           | 1             |\n    | misc/serial_timesteps   | 2048          |\n    | misc/time_elapsed       | 3.03          |\n    | misc/total_timesteps    | 2048          |\n    -------------------------------------------\n    ```", "```py\n    !python -m baselines.run --alg=ppo2 --env=CartPole-v0 \n    --num_timesteps=0\n        --load_path=./models/CartPole_2M_ppo2 --play\n    ```", "```py\n    episode_rew=27.0\n    episode_rew=27.0\n    episode_rew=11.0\n    episode_rew=11.0\n    episode_rew=13.0\n    episode_rew=29.0\n    episode_rew=28.0\n    episode_rew=14.0\n    episode_rew=18.0\n    episode_rew=25.0\n    episode_rew=49.0\n    episode_rew=26.0\n    episode_rew=59.0\n    ```", "```py\n    !wget -O cartpole_1M_ppo2.tar.gz \\\n    https://github.com/PacktWorkshops/The-Reinforcement-Learning-\\\n    Workshop/blob/master/Chapter04/cartpole_1M_ppo2.tar.gz?raw=true\n    ```", "```py\n    Saving to: 'cartpole_1M_ppo2.tar.gz'\n    cartpole_1M_ppo2.ta 100%[===================>]  53,35K  --.-KB/s    in 0,05s  \n    2020-05-11 15:57:07 (1,10 MB/s) - 'cartpole_1M_ppo2.tar.gz' saved [54633/54633]\n    ```", "```py\n    !tar xvzf cartpole_1M_ppo2.tar.gz\n    ```", "```py\n    cartpole_1M_ppo2/ckpt-1.index\n    cartpole_1M_ppo2/ckpt-1.data-00000-of-00001\n    cartpole_1M_ppo2/\n    cartpole_1M_ppo2/checkpoint\n    ```", "```py\n    !python -m baselines.run --alg=ppo2 --env=CartPole-v0 \n    --num_timesteps=0 --load_path=./cartpole_1M_ppo2 –play\n    ```", "```py\n    episode_rew=16.0\n    episode_rew=200.0\n    episode_rew=200.0\n    episode_rew=200.0\n    episode_rew=26.0\n    episode_rew=176.0\n    ```"]