<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Working with Amazon SageMaker</h1>
                </header>
            
            <article>
                
<p>In the last few chapters, you have learned about readily-available <strong>Machine Learning</strong> (<strong>ML</strong>) APIs that solve business challenges. In this chapter, we will deep dive into AWS SageMaker—the service that is used to <span>build, train, and deploy models seamlessly when the ML APIs do not completely meet your requirements. SageMaker </span>increases the productivity of data scientists and machine learning engineers by abstracting away the complexity involved in provisioning compute and storage.</p>
<p>This is what will we cover in this chapter:</p>
<ul>
<li>Processing big data through Spark EMR</li>
<li>Conducting training in Amazon SageMaker</li>
<li>Deploying trained models and running inference</li>
<li>Runninghyperparameter optimization</li>
<li>Understanding SageMaker experimentation service</li>
<li>Bring your own model – SageMaker, MXNet, and Gluon</li>
<li>Bring your own container – R Model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>For the following sections, we will employ the book rating dataset known as <kbd>goodbooks-10k</kbd> to illustrate all of the topics outlined previously. The dataset consists of 6 million ratings <span>on 10,000 books </span>from 53,424 users. More details on the goodbooks-10k dataset can be found <a href="https://www.kaggle.com/zygmunt/goodbooks-10k#books.csv">https://www.kaggle.com/zygmunt/goodbooks-10k#books.csv</a>. </p>
<p>In the <a href="https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-on-Amazon-Web-Services">folder</a> associated with this chapter, you will find two CSV files:</p>
<ul>
<li><kbd>ratings.csv</kbd>: Contains book ratings, user IDs, book IDs, and rating</li>
<li><kbd>books.csv</kbd>: Contains book attributes, including title</li>
</ul>
<p>It is now time to wrangle big data to create a dataset for modeling.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing big data through Spark EMR</h1>
                </header>
            
            <article>
                
<p class="mce-root">The design pattern to execute models in SageMaker is to read the data placed in S3. The data may not be readily consumable most of the time. If the datasets required are large, then wrangling the data in the Jupyter notebook may not be practical. In such cases, Spark EMR clusters can be employed to conduct operations on big data.</p>
<p class="mce-root">Wrangling a big dataset in Jupyter notebooks results in out-of-memory errors. Our solution is to employ AWS EMR (Elastic MapReduce) clusters to conduct distributed data processing. Hadoop will be used as the underlying distributed filesystem while Spark will be used as the distributed computing framework.</p>
<p class="mce-root">Now, to run commands against the EMR cluster to process big data, AWS offers EMR notebooks. EMR notebooks provide a managed notebook environment, based on Jupyter Notebook. These notebooks can be used to interactively wrangle large data, visualize the same, and prepare analytics-ready datasets. Data engineers and data scientists can employ a variety of languages, Python, SQL, R, and Scala, to process large volumes of data. These EMR notebooks can also be saved periodically to a persistent data store, S3, so the saved work can be retrieved later. One of the critical components of Amazon EMR architecture is the <span>Livy service. It is an open source REST interface for interacting with Spark clusters without the need for Spark client. The Livy service enables communication between the EMR notebook and EMR cluster, where the service is installed.</span></p>
<p class="CDPAlignLeft CDPAlign"><span>The following architecture diagram details how EMR notebooks communicate with Spark EMR clusters to process large data:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a8460360-5965-4e0f-9beb-afc46b6cffb1.png" style=""/></div>
<p>Now that we've looked at how EMR clusters interact with EMR notebooks to process big data interactively, let's begin by creating an EMR notebook and cluster, as shown in the following:</p>
<ol>
<li>Navigate to<span> </span><span class="packt_screen">Amazon EMR</span><span> </span>under<span> </span><span class="packt_screen">Services</span><span> </span>and click on<span> </span><span class="packt_screen">Notebooks</span>.</li>
<li>In the<span> </span><span class="packt_screen">Create notebook</span><span> </span>page, enter <span class="packt_screen">Notebook name</span> and <span class="packt_screen">Description</span>, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/aa257df7-8459-46e0-85cb-cdf5d2e72172.png" style=""/></div>
<ol start="3">
<li>Next, select the option <span class="packt_screen">Create a cluster</span>,<span class="packt_screen"> </span>enter <span class="packt_screen">Cluster name</span>, and select <span class="packt_screen">Instance</span> type and number. As you can see in the preceding screenshot, the EMR cluster comes with <span class="packt_screen">Hadoop</span>, <span class="packt_screen">Spark</span>, <span class="packt_screen">Livy</span>, and <span class="packt_screen">Hive</span> applications.</li>
<li>Now, let's review the policies of EMR role and EC2 instance profile and enter the S3 location where EMR notebooks will be saved, as in the following:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b9185ba5-2ac3-40e9-84cc-b48651fdf7f4.png" style=""/></div>
<p style="padding-left: 60px"><span>From the preceding visual, we can see the following:</span></p>
<ul>
<li style="padding-left: 30px">The EMR role is used to give the EMR service access to other AWS services (for example, EC2).</li>
<li style="padding-left: 30px">The EMR EC2 instance profile further enables EC2 instances launched by EMR to have access to other AWS services (for example, S3).</li>
<li style="padding-left: 30px">We<span><span> configured appropriate security groups around the EMR cluster to allow communication between the EMR notebook and master node of the EMR cluster.</span></span></li>
<li style="padding-left: 30px"><span>We also assigned a service role to the EMR cluster, so </span>it can in<span>teract with other AWS services.</span></li>
<li style="padding-left: 30px">Also, EMR notebooks are saved to the designated S3 location when you click on <span class="packt_screen">Save</span> in EMR notebooks.</li>
</ul>
<ol start="5">
<li>Now, click on <span class="packt_screen">Create notebook</span> to launch a new EMR notebook. The notebook and cluster will start provisioning, as shown in the following:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/48c2f140-2144-48d3-b56c-8790b6c073e1.png" style=""/></div>
<ol start="6">
<li>Once the EMR notebook and cluster are provisioned, click on <span class="packt_screen">Open </span>to open the notebook. We will use the EMR notebook to create a dataset that will be used to recommend books to users via the <span class="packt_screen">object2vec</span> algorithm, <span>which is a built-in SageMaker algorithm used to predict the affinity of a user toward a book.</span></li>
</ol>
<p>In the EMR notebook, we do five things:</p>
<ol>
<li>Read the ratings and books CSV files.</li>
<li>Analyze the ratings dataset to understand the number of ratings by user and book.</li>
<li>Filter the original ratings dataset to only include ratings, <span>where it contains users who have rated more than 1% of books and books that have been rated by at least 2% of the users.</span></li>
<li>Create indexes (starting with zero) for both users and books in the ratings dataset—this is required to train the <kbd>object2vec</kbd> algorithm.</li>
<li>Write (in parquet format) the resulting ratings dataset, which also includes the book title, to relevant S3 bucket. The ratings dataset will then have a rich history of user preferences, along with the popularity of books. </li>
</ol>
<p style="padding-left: 60px">In the following code block, we will whittle down 6 million ratings to ~1 million:</p>
<pre style="padding-left: 60px"># Filter ratings by selecting books that have been rated by at least 1200 users and users who have rated at least 130 books<br/>fil_users = users.filter(F.col("count") &gt;= 130)<br/>fil_books = books.filter(F.col("count") &gt;= 1200)</pre>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px"><span>In the preceding code, we filtered ratings to include users who have rated at least 130 books and books that have been rated by at least 1,200 users.</span></p>
<ol start="6">
<li class="CDPAlignLeft CDPAlign">Once the ratings dataset is prepared, we'll persist it to S3 bucket, as shown in the following:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bcbad147-df09-42ca-a85c-ca24d5dd54a0.png" style=""/></div>
<p>From the preceding screenshot, the following is understood:</p>
<ul>
<li>Since the data is parallel processed on the EMR cluster, the output contains several <kbd>parquet</kbd> files.</li>
<li>Apache Parquet is an open source compressed columnar storage format in the Apache Hadoop ecosystem.</li>
<li>Compared to the traditional approach where data is stored in a row-oriented approach, Parquet allows us to be more efficient in terms of storage and performance.</li>
<li>Stop the notebook and terminate the cluster after you are done storing the processed dataset in S3 to avoid unnecessary costs.</li>
</ul>
<p><span>Now, we are ready to understand the built-in <kbd>object2vec</kbd> algorithm and start training the model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conducting training in Amazon SageMaker</h1>
                </header>
            
            <article>
                
<p>Let's begin by spending a few minutes understanding how the <kbd>object2vec</kbd> algorithm works. It is a multi-purpose algorithm that can create lower dimensional embeddings of higher dimensional objects. This process is known as dimensionality reduction, most commonly implemented through a statistical procedure called <strong>Principal Component Analysis</strong> (<strong>PCA</strong>). However, Object2Vec uses neural networks to learn these embeddings.</p>
<p>Some of the common applications of these embeddings include customer segmentation and product search. In the case of customer segmentation, similar customers appear closer in the lower dimensional space. A customer can be defined through multiple attributes such as name, age, home address, and email address. With regards to product search, because product embeddings capture the semantics of the underlying data, any combination of search terms can be used to retrieve the target product. The embedding of these search terms (semantics) should just match that of the product.</p>
<p>Let's look at how Object2Vec works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning how Object2Vec Works</h1>
                </header>
            
            <article>
                
<p>Object2vec can learn embeddings of pairs of objects. In our case, the higher the rating of the book, the stronger the relationship between the user and the book. The idea is that users with similar tastes are likely to rate similar books higher. Object2vec approximates the book rating by using embeddings of users and books. The closer a user is to some books, the higher the rating given by that user to the books. We provide the algorithm with <kbd>(user_ind</kbd> and <kbd>book_ind)</kbd> pairs; for each such pair, we also provide a <strong>label</strong> that tells the algorithm whether the user and book are similar or not. The <strong>label</strong> in our case is the book rating. Therefore, the trained model can be used to predict the rating of a book for a given user such as the book; in this case, the one which has never been rated by the user.</p>
<p>Following is the conceptual diagram of how <kbd>object2vec</kbd> works:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c119c8a2-2164-40f7-8851-9110ce8d48f1.png" style=""/></div>
<p>From the preceding visual, we can see the following:</p>
<ul>
<li>We can see that the user and item or book embeddings are concatenated, which are then passed to the <strong>Multiple Layer Perceptron</strong> (<strong>MLP</strong>).</li>
<li>User and book embeddings are created from a one-hot encoded representation of user and book indexes respectively.</li>
<li>Through supervised learning, MLP can learn the weights of the network and these weights can be used to predict score or rating of user-book pair.</li>
</ul>
<p>To further understand the inner workings of <kbd>object2vec</kbd>, see the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ed402b5e-223e-4a8e-b70d-094957f3f948.png" style=""/></div>
<p><span>From the preceding visual, we can see the following:</span></p>
<ul>
<li>Object2vec starts with representing user and book with one-hot encoding. To explain, in our case, a user can be represented with an array of the size 12,347, which means that there are a total of <span>12,347</span> unique users in the dataset.</li>
<li>User #1 can be represented by denoting 1 at position 1, while all of the other positions in the array have zeros.</li>
<li>Books can also be represented in a comparable manner.</li>
<li>It is time to now reduce the dimensionality of these representations. Therefore, the algorithm uses an embedding layer with 1,024 neurons, each for a user and a book.</li>
<li>Object2vec further extracts additional features by conducting element-wise multiplication and subtraction between 1,024 user embedding neurons and 1,024 item embedding neurons.</li>
</ul>
<p>In other words, the user and book embeddings are compared in different ways. Overall, we will then have 4,096 neurons when all of the neurons from the previous layers are merged. The algorithm then uses a single perceptron layer with 256 neurons. This perceptron layer is then fully connected to the output layer with one neuron. This one neuron will then predict the rating of a book given by a user.</p>
<p>It is now time to train the Object2Vec algorithm</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the Object2Vec algorithm</h1>
                </header>
            
            <article>
                
<p>Now that we have an understanding of how the algorithm works, let's dive into the training process:</p>
<ol>
<li><strong>Data processing</strong>: Feed data in the form of JSON lines; random shuffle the data for optimal performance. As you will see later, we send data in the format of <kbd>user index</kbd>, <kbd>book index</kbd>, <kbd>label=rating</kbd>.</li>
<li><strong>Model training</strong>: We pass both training and validation data to the algorithm. There are multiple hyperparameters that we can configure to fine-tune the model's performance. We will review them in the upcomings sections. The objective function, in our case, is to minimize the <span><strong>Mean Squared Error</strong> </span>(<strong><span>MSE</span></strong>). The error is the difference between the label (actual value) and the predicted rating.</li>
</ol>
<p>Once the model has been trained, we will deploy it as an endpoint for inference. </p>
<p class="CDPAlignLeft CDPAlign">In data processing, we will do the following:</p>
<ol>
<li>First, we will read the ratings dataset stored in <kbd>parquet</kbd> format <span>on the S3 bucket, as shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px">s3 = s3fs.S3FileSystem()<br/><br/>s3_bucket = 's3://ai-in-aws1/'<br/>input_prefix = 'Chapter7/object2vec/bookratings.parquet'<br/>dataset_name = s3_bucket + input_prefix<br/><br/>df_bkRatngs = pq.ParquetDataset(dataset_name, filesystem=s3).read_pandas().to_pandas()</pre>
<p style="padding-left: 60px">In the preceding code, we can see the following:</p>
<ul>
<li style="padding-left: 30px"><kbd>s3fs</kbd> is a Python library that is based on boto3, an AWS SDK for Python. <kbd>s3fs</kbd> provides a filesystem interface for S3.</li>
<li style="padding-left: 30px">We use the <kbd>pyarrow</kbd> Python library to read partitioned <kbd>parquet</kbd> files from a designated s3 bucket.</li>
<li style="padding-left: 30px">Specifically, we call the <kbd>ParquetDataset()</kbd> function by passing in the dataset name and filesystem.</li>
</ul>
<ol start="2">
<li>After reading the dataset, we display it to ensure that the data is read correctly, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d9f71163-02e1-4eb9-a117-76d0368b087a.png" style=""/></div>
<p>Then, we load the dataframe in a format required by the <kbd>Object2Vec</kbd> algorithm. For each user-book pair and rating label, we create an entry in a data list by calling the <kbd>load_df_data()</kbd> function. Please refer to the source code attached to this chapter for details.</p>
<p>In model training, we start by partitioning the dataset into training, validation, and test sets. For each of the sets, we call the <kbd>write_data_list_to_jsonl()</kbd> function to create <kbd>.jsonl</kbd> (JSON lines) files, the format required by <kbd>object2vec</kbd>. <span>A sample <kbd>jsonl</kbd> file is shown in the following screenshot:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d714d5cd-cbc6-46e5-9e6f-54f76a075332.png" style=""/></div>
<ol>
<li>Then, we upload the prepared datasets to the designated S3 bucket.</li>
</ol>
<ol start="2">
<li>We <span>obtain a Docker image of the Object2Vec algorithm, as follows:</span></li>
</ol>
<pre style="padding-left: 60px">container = get_image_uri(boto3.Session().region_name, 'object2vec') </pre>
<p>In the preceding code, we can see the following:</p>
<ul>
<li>To get the <span><strong>Uniform Resource Identifier</strong> </span>(<strong><span>URI</span></strong>) of the <kbd>object2vec</kbd> Docker image, we called the <kbd>get_image_uri()</kbd> function by passing the region name of the local SageMaker session and the name of the algorithm as input.</li>
<li>The <kbd>get_image_uri()</kbd> function is part of the SageMaker Python SDK.</li>
</ul>
<p> After obtaining the <kbd>uri</kbd> of the <kbd>object2vec</kbd> algorithm, we define the hyperparameters, as follows:</p>
<ul>
<li><strong>Encoder network</strong>: This includes the following:
<ul>
<li><kbd>enc0_layers</kbd>: This is the number of layers in the encoder network.</li>
<li><kbd>enc0_max_seq_len</kbd>: This is the maximum number of sequences sent to the encoder network (in this case, only one user sequence is sent to the network).</li>
<li><kbd>enc0_network</kbd>: This defines how embeddings are handled. In this case, since we address one user embedding at a time, no aggregation is necessary.</li>
<li><kbd>enc0_vocab_size</kbd>: This defines the first encoder vocabulary size. It represents the number of users in the dataset.</li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">Since there are two encoders in the network, the same hyperparameters apply for encoder 1. For encoder 1, the vocabulary size needs to be defined appropriately, which is the number of books in the dataset—<kbd>enc1_vocab_size: 985</kbd>.</p>
<ul>
<li><strong>MLP</strong>: This includes the following:
<ul>
<li><kbd>mlp_dim</kbd>: This is the number of neurons in the MLP layers. In our experiment, we set it to 256.</li>
<li><kbd>mlp_layers</kbd>: This is the number of layers in the MLP network. We use a single layer in our experiment.</li>
<li><kbd>mlp_activation</kbd>: This is the activation function for MLP layers. In our experiment, we use the <strong><span>Rectified Linear Unit</span></strong> (<strong><span>ReLU</span></strong>) activation function for faster convergence and to avoid vanishing gradient issues. Note that the ReLU activation function is given by <img class="fm-editor-equation" src="assets/d77d47c4-5a7c-4fa7-8ee1-ac2b6e4dd1d8.png" style="width:7.08em;height:1.42em;"/>.</li>
</ul>
</li>
<li><strong>The following instances control how</strong> <kbd>object2vec</kbd> <strong>is trained:</strong>
<ul>
<li><kbd>epochs</kbd>: This is the number of backward and forward passes. We use 10 in our case.</li>
<li><kbd>mini_batch_size</kbd>: This is the number of training examples to process before updating weights. We use 64.</li>
<li><kbd>early_stopping_patience</kbd>: This is the maximum number of bad epochs (epochs where loss does not improve) that are executed before stopping. We use 2.</li>
<li><kbd>early_stopping_tolerance</kbd>: This is the improvement in loss function required between two consecutive epochs for training to continue. This is after the number of patience epochs conclude. We use 0.01 for this parameter.</li>
</ul>
</li>
<li><strong>Others</strong> includes the following:
<ul>
<li><kbd>optimizer</kbd>: This is the optimization algorithm to arrive at optimal network parameters. In this experiment, we use adaptive moment estimation, also known as Adam. It computes the individual learning rate for each parameter. Parameters pertaining to features or inputs with sparse data go through large updates relative to the ones with dense data. Also, Adam computes individual momentum changes for each of the parameters. Remember that, during backpropagation, it is important to navigate in the right direction for faster convergence. Momentum changes help to navigate in the correct direction.</li>
<li><kbd>output_layer</kbd>: This defines whether the network is a classifier or a regressor. In this case, since the network is trying to learn to rate, we define the output layer as a mean squared error (linear).</li>
</ul>
</li>
</ul>
<p>After the hyperparameters have been defined, we fit the <kbd>object2vec</kbd> estimator to the prepared datasets (train and validation), as shown in the following code:</p>
<pre># create object2vec estimator<br/>regressor = sagemaker.estimator.Estimator(container, role, train_instance_count=1, <br/> train_instance_type='ml.m5.4xlarge', output_path=output_path, sagemaker_session=sess)<br/><br/># set hyperparameters<br/>regressor.set_hyperparameters(**static_hyperparameters)<br/><br/># train and tune the model<br/>regressor.fit(input_paths)</pre>
<p>In the preceding code, we are doing the following:</p>
<ol>
<li>We begin by creating an <kbd>object2vec</kbd> estimator by passing the Docker image, current execution role, number, and type of training instances, and current <kbd>sagemaker</kbd> session.</li>
<li>We then set hyperparameters for the newly created <kbd>object2vec</kbd> estimator using the <kbd>set_hyperparameters()</kbd> function.</li>
<li>Then, we fit the model to the training and validation datasets using the <kbd>fit()</kbd> function of the <kbd>Estimator</kbd> object.</li>
<li>The duration of training depends on the training instance type and the number of instances. For one <kbd>m5.4xlarge</kbd> machine learning instance, it took 2 hours to complete 10 epochs. </li>
</ol>
<p>To monitor the training job in progress, navigate to the <span class="packt_screen">Training</span> section on the left-hand side of the SageMaker service. Click on <span class="packt_screen">Training Jobs</span> and then on the job name of your current job. After, navigate to the <span class="packt_screen">monitor</span> section to see the training job's progress, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6dd6d633-0114-44bf-8d98-be745ae67bfb.png" style=""/></div>
<p>As you can see in the preceding screenshot, <span>as the training MSE decreases, the validation MSE also decreases—although, in the validation dataset, the decrease in error is not as steep as the decrease in the training dataset. The training throughput can also be monitored through this dashboard. </span></p>
<p>Now that the training is done, let's deploy the trained model as an endpoint for inference.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying the trained Object2Vec and running inference</h1>
                </header>
            
            <article>
                
<p>Now, let's deploy the trained <kbd>object2vec</kbd> model. The SageMaker SDK offers methods so that we can seamlessly deploy trained models:</p>
<ol>
<li>First, we will create a model from the training job using the <kbd>create_model()</kbd> method of the SageMaker <kbd>Estimator</kbd> object, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">from sagemaker.predictor import json_serializer, json_deserializer<br/><br/># create a model using the trained algorithm<br/>regression_model = regressor.create_model(serializer=json_serializer,<br/> deserializer=json_deserializer,content_type='application/json')</pre>
<p style="padding-left: 60px">To the <kbd>create_model()</kbd> method, we passed the type of serializers and deserializers to be used for the payload at the time of inference. </p>
<ol start="2">
<li>Once the model has been created, it can be deployed as an endpoint via the <kbd>deploy()</kbd> method of the SageMaker <kbd>Model</kbd> object, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px"># deploy the model<br/>predictor = regression_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')</pre>
<p style="padding-left: 60px"><span> To the <kbd>deploy()</kbd> method, we have specified the number and type of instances that you have to launch to host the endpoint.</span></p>
<ol start="3">
<li>Once the <kbd>object2vec</kbd> model has been deployed as an endpoint, we can navigate to <span>the </span><span><span class="packt_screen">Endpoints </span>section under the</span> <span class="packt_screen">Inference </span>grouping (present on the left navigation menu under the SageMaker service). The status of the deployed endpoint can be viewed here, as shown in the following screenshot:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/15cbd4d2-7734-4865-8f38-d9f8d352ef52.png" style=""/></div>
<p style="padding-left: 60px">Now that we have the <kbd>object2vec</kbd> endpoint available, let's run inference.</p>
<ol start="4">
<li>We will create the <kbd>RealTimePredictor</kbd> object (the SageMaker Python SDK) by passing the endpoint name, along with the type of serialization and deserialization for the input and output, respectively. See the following code on how to initialize the <kbd>RealTimePredictor</kbd> object:</li>
</ol>
<pre style="padding-left: 60px">from sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer<br/><br/>predictor = RealTimePredictor(endpoint='object2vec-2019-08-23-21-59-03-344', sagemaker_session=sess, serializer=json_serializer, deserializer=json_deserializer, content_type='application/json')</pre>
<p style="padding-left: 60px">You can change the endpoint name to reflect your current endpoint (the first argument of the <kbd>RealTimePredictor</kbd> object).</p>
<ol start="5">
<li>We then invoke the <kbd>predict()</kbd> method of <kbd>RealTimePredictor</kbd>, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px"># Send data to the endpoint to get predictions<br/><br/>prediction = predictor.predict(test_data)<br/>print("The mean squared error on test set is %.3f" %get_mse_loss(prediction, test_label))</pre>
<p style="padding-left: 60px">In the preceding code, <span>remember that <kbd>test_data</kbd> should be in a format that's consumable by <kbd>object2vec.</kbd> </span><span>We use the <kbd>data_list_to_inference_format()</kbd> function to transform the test data into two components: instances and label. For details on this function, please see the source code associated with this chapter. Check out the following screenshot to see how the test data should be structured:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/87ccadde-beb8-4ac8-8ad7-e07f9b28168d.png" style=""/></div>
<p style="padding-left: 60px">As shown in the preceding screenshot, the inputs for 0 (<kbd>in0</kbd>) and 1 (<kbd>in1</kbd>) should have the indexes of the user and book, respectively. As for the test label, we produce a data list of ratings for each of the associated user-book pairs, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/21b95547-7e80-4592-9686-68aec3f766cf.png" style=""/></div>
<p style="padding-left: 60px">As shown in the preceding screenshot, we pass the first 100 user-book pairs from the test dataset to the <kbd>predict()</kbd> method of <kbd>RealTimePredictor</kbd>. The result is an MSE of 0.110.</p>
<ol start="6">
<li>Now, let's compare this MSE with the MSE from the naive options of computing book ratings:</li>
</ol>
<ul>
<li style="padding-left: 30px"><strong>Baseline 1</strong>: For each user-book pair in the test dataset, compute the rating, which is the average book ratings across all of the users, as shown in the following code:</li>
</ul>
<pre style="padding-left: 60px">train_label = [row['label'] for row in copy.deepcopy(train_list)]<br/>bs1_prediction = round(np.mean(train_label), 2)<br/>print("The validation mse loss of the Baseline 1 is {}".format(get_mse_loss(len(test_label)*[bs1_prediction], test_label)))</pre>
<p style="padding-left: 60px">To compute the average rating across all users, we do the following:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>We iterate through all of the ratings in the training dataset to create a labels list, <kbd>train_label</kbd>.</li>
<li><kbd>train_label</kbd> is then used to compute the mean. To calculate the MSE, in the <kbd>get_mse_loss()</kbd> function, the <span>average rating across all of the users is subtracted from </span>each of the ratings in <kbd>test_label</kbd>.</li>
<li>The error is then squared and averaged across all of the test users. Please see the attached source code for details. The MSE from this option is 1.13.</li>
</ul>
</li>
<li style="padding-left: 30px"><strong>Baseline 2</strong>: <span>For each user-book pair in the test dataset, we compute the rating, which is the average book rating for that user (that is, the average rating across all books rated by the user), as shown in the following code:</span></li>
</ul>
<pre style="padding-left: 60px">def bs2_predictor(test_data, user_dict):<br/><span>  test_data = copy.deepcopy(test_data['instances'])<br/></span><span>  predictions = list()<br/></span><span>  for row in test_data:<br/></span><span>    userID = int(row["in0"][0])<br/></span></pre>
<p class="mce-root" style="padding-left: 60px">In the <kbd>bs2_predictor()</kbd> function, we passed the test data and user dictionary from the training dataset as inputs. For each user in the test data, if they exist in the training dataset, we computed the average book rating across all of the books rated by them. If they do not exist in the training dataset, we just get the average rating across all of the users, as shown in the following code:</p>
<pre class="mce-root">    if userID in user_dict:<br/><span>      local_books, local_ratings = zip(*user_dict[userID])<br/></span><span>      local_ratings = [float(score) for score in local_ratings]<br/></span><span>      predictions.append(np.mean(local_ratings))<br/></span><span>    else:<br/></span><span>      predictions.append(bs1_prediction)<br/></span><span><br/>   return predictions<br/></span></pre>
<p class="mce-root">In the preceding <kbd>bs2_predictor()</kbd> function, the <kbd>zip(*)</kbd> function is used to return lists of books and ratings for each user. <kbd>bs1_prediction</kbd> is the average rating across all of the users in the training dataset. <span>The MSE from this option is 0.82.</span></p>
<p>As we can see, an MSE of 0.110 from <kbd>object2vec</kbd> is better than the baselines:</p>
<ul>
<li><strong>Baseline 1 MSE</strong>: 1.13, where the predicted book rating is the global average book rating across all users</li>
<li><strong>Baseline 2 MSE</strong>: 0.82, where the predicted book rating is the average book rating by user</li>
</ul>
<p class="mce-root">Now that we have trained and evaluated the built-in SageMaker algorithm, <kbd>object2vec</kbd>, it is time to understand the features that SageMaker offers so that we can automate hyperparameter tuning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running hyperparameter optimization (HPO)</h1>
                </header>
            
            <article>
                
<p>It takes data scientists numerous hours and experiments to arrive at an optimal set of hyperparameters that are required for best model performance. This process is mostly based on trial and error.</p>
<p>Although <kbd>GridSearch</kbd> is one of the techniques that is traditionally used by data scientists, it suffers from the curse of dimensionality. For example, if we have two hyperparameters, with each taking five possible values, we're looking at calculating objective function 25 times (5 x 5). As the number of hyperparameters grows, the number of times that the objective function is computed blows out of proportion.</p>
<p>Random Search addresses this issue by randomly selecting values of hyperparameters, without doing an exhaustive search of every single combination of hyperparameters. This <a href="http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf">paper</a> by Bergstra et al. claims that a random search of the parameter space is guaranteed to be more effective than a grid search.</p>
<p>The idea is that some parameters have much less effect than others on the objective function. This is reflected by the number of values that are picked for each parameter in the grid search. Random Search enables the exploration of more values for each parameter, given several trials. The following is a diagram that illustrates the difference between grid search and random search:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5af628ab-0cee-4750-8500-fdc7c9768e62.png" style=""/></div>
<p>As you can see from the preceding screenshot, in a random search, we can test more values for important parameters, resulting in increased performance from training a model.</p>
<p>Neither of these techniques automate the process of hyperparameter optimization. <strong>Hyperparameter Optimization</strong> (<strong>HPO</strong>), from SageMaker, automates the process of selecting the optimal combination of hyperparameters. Here is how the tool works:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/ab83634c-2e45-4199-ae95-39f64bd9c640.png" style=""/></div>
<p>Take a look at the following points:</p>
<ul>
<li>HPO uses a Bayesian technique to iteratively select a combination of hyperparameters to train the algorithm.</li>
<li>HPO picks the next set of hyperparameters, given the performance of the model and the configuration of hyperparameters in all of the historical steps.</li>
<li>Also, it employs an <em>acquisition function</em> to determine where the next best opportunity is to lower the cost function.</li>
<li>After a specified number of iterations, you will arrive at an optimal configuration of hyperparameters producing the best model.</li>
</ul>
<p><strong> </strong>For the <kbd>object2vec</kbd> algorithm, let's select the hyperparameters that we want to tune:</p>
<ul>
<li><kbd>learning_rate</kbd>: Controls the speed with which weights in the neural network are optimized</li>
<li><kbd>dropout</kbd>: The percent of the neurons in a layer that are ignored in forward and backward passes</li>
<li><kbd>enc_dim</kbd>: The number of neurons to generate user/item embedding</li>
<li><kbd>mlp_dim</kbd>: The number of neurons in the MLP layer</li>
<li><kbd>weight_decay</kbd>: A factor to prevent overfitting (L2 regularization—causes the weight to decay in proportion to the factor specified)</li>
</ul>
<p>We will use the <span><kbd>HyperparameterTuner</kbd> class from the <kbd>sagemaker</kbd> Python SDK to create tuning jobs. The goal of the tuning jobs is to reduce the MSE for the validation dataset</span>. Depending on your budget and time, you can choose the number of training jobs you want to run. In this case, I chose to run 10 jobs, with only one job running at a given moment. You can choose to run multiple jobs in parallel.</p>
<p>To instantiate hyperparameter tuning jobs, we will need to do the following:</p>
<ul>
<li>Define the hyperparameters to tune and specify the objective function, as shown in the following code:</li>
</ul>
<pre class="mce-root" style="padding-left: 60px">tuning_job_name = "object2vec-job-{}".format(strftime("%d-%H-%M-%S", gmtime())) <br/><br/>hyperparameters_ranges = { <br/>"learning_rate": ContinuousParameter(0.0004, 0.02),<br/>"dropout": ContinuousParameter(0.0, 0.4),<br/>"enc_dim": IntegerParameter(1000, 2000),<br/>"mlp_dim": IntegerParameter(256, 500), <br/>"weight_decay": ContinuousParameter(0, 300) }<br/><br/>objective_metric_name = 'validation:mean_squared_error'</pre>
<p>As shown in the preceding code, we defined the ranges for each of the hyperparameters. For the objective function, we specified it as the mean squared error in the validation dataset:</p>
<ul>
<li>Define an estimator to train the <kbd>object2vec</kbd> model.</li>
<li>Define the <kbd>HyperparameterTuner</kbd> job by passing the estimator, the objective function and type, and the maximum number of jobs to run, as shown here:</li>
</ul>
<pre style="padding-left: 60px">tuner = HyperparameterTuner(regressor, objective_metric_name, hyperparameters_ranges, objective_type='Minimize', max_jobs=5, max_parallel_jobs=1)</pre>
<p style="padding-left: 60px">The <kbd>HyperparameterTuner</kbd> object takes the estimator (named <kbd>regressor</kbd>) as one of the inputs. The estimator should be initialized with hyperparameters, along with the number and type of instances to be launched. Please see the associated source code for this chapter.</p>
<ul>
<li>Fit the tuner to the training and validation datasets, as shown in the following code:</li>
</ul>
<pre style="padding-left: 60px">tuner.fit({'train': input_paths['train'], 'validation': input_paths['validation']}, job_name=tuning_job_name, include_cls_metadata=False)<br/>tuner.wait()</pre>
<p class="CDPAlignLeft CDPAlign">To the <kbd>fit</kbd> method of <kbd>hyperparameterTuner</kbd>, we pass the location of training and validation datasets. We wait for the tuner to finish running all of the jobs.</p>
<p>The following screenshot shows a few training jobs with a different set of hyperparameters that have been executed by <kbd>HyperparameterTuner</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/69bfb043-79b5-4a23-a772-90d01bb38b2d.png" style=""/></div>
<p>With each job, you can look at the hyperparameters that were used and the value of the objective function.</p>
<p>To look at the best job with the lowest MSE, navigate to the <span class="packt_screen">Best job</span> tab, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/267713ac-752b-4961-a157-0a8a1989602d.png" style=""/></div>
<p>After the jobs are executed, you can run analytics on the results from hyperparameter optimization to answer questions, such as how does the MSE vary as the tuning jobs are being executed? You can also look at whether there is a correlation between the MSE and hyperparameters being tuned, such as the learning rate, dropout, weight decay, and the number of dimensions for both the encoder and <kbd>mlp</kbd>.</p>
<p>In the following code, we plot how the MSE changes as the training jobs are being executed:</p>
<pre>objTunerAnltcs = tuner.analytics()<br/>dfTuning = objTunerAnltcs.dataframe(force_refresh=False)<br/>p = figure(plot_width=500, plot_height=500, x_axis_type = 'datetime') <br/>p.circle(source=dfTuning, x='TrainingStartTime', y='FinalObjectiveValue')<br/>show(p)</pre>
<p class="mce-root">In the preceding code, we create an analytics object from <kbd>HyperparameterTuner</kbd>, which we created earlier. We then obtain a <span>DataFrame</span> from the analytics object—the DataFrame contains the metadata of all of the training jobs that were executed by the tuner. We then plot the MSE against time.</p>
<p class="mce-root">In the following diagram, we track how the MSE varies with the training time:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ab528e01-b0cc-4fb3-a4db-39795733b17d.png" style=""/></div>
<p class="mce-root">As you can see, the plot is very bumpy. If you increase the number of training jobs, perhaps the hyperparameter tuning job will converge.</p>
<p class="mce-root">It is time to look at another important feature of SageMaker, that is, the experiment service or search.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the SageMaker experimentation service</h1>
                </header>
            
            <article>
                
<p>The goal of experiment management with SageMaker Search is to accelerate the model's development and experimentation phase, improving the productivity of data scientists and developers, while also reducing the overall time to market machine learning solutions.</p>
<p>The machine learning life cycle (continuous experimentation and tuning) states that when you initiate the training of a new learning algorithm, to improve model performance, you conduct hyperparameter tuning. With each iteration of the tuning, you will need to check how the model's performance is improving.</p>
<p>This leads to hundreds and thousands of experiments and model versions. The whole process slows down the selection of a final optimized model. Additionally, it is critical to monitor the performance of a production model. If the predictive performance of the model is degrading, it is important to know how the real-life data is different from the data that's used during training and validation.</p>
<p>SageMaker's Search tackles all of the challenges we highlighted previously by providing the following features:</p>
<ul>
<li><strong>Organizing, tracking, and evaluating model training experiments</strong>: Creating leaderboards for winning models, cataloging model training runs, and comparing models by performance metrics such as training loss and validation accuracy</li>
<li><strong>Seamlessly searching and retrieving the most relevant training runs</strong>: Runs that can be searched by key attributes, which can be the training job name, status, start time, last modified time, and failure reason, among other things</li>
<li><strong>Tracking the lineage of a deployed model in a live environment</strong>: Tracking the training data used, values of the hyperparameters specified, resulting model performance, and version of the model deployed</li>
</ul>
<p>Let's illustrate the features of SageMaker Search:</p>
<ol>
<li>Navigate to <span class="packt_screen">Search</span><em> </em>on the left navigation pane of the Amazon SageMaker service.</li>
<li>Search for experiments that have been conducted using the <kbd>object2vec</kbd> algorithm:
<ol>
<li>In the <span class="packt_screen">Search</span><em> </em>pane, under <span class="packt_screen">Property</span>, select <span class="packt_screen">AlgorithmSpecification.TrainingImage</span>.</li>
<li>Under <span class="packt_screen">Operator</span>, select <span class="packt_screen">Contains</span>.</li>
<li>Under <span class="packt_screen">Value</span>, select <span class="packt_screen">object2vec</span>, as shown in the following code:</li>
</ol>
</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7a43c9f2-3732-4716-82cd-db0d870b2552.png" style=""/></div>
<p style="padding-left: 60px">You can also search for experiments programmatically using <kbd>boto3</kbd>, the AWS SDK for Python, as shown here:</p>
<pre style="padding-left: 60px">sgmclient = boto3.client(service_name='sagemaker')<br/><span>results = sgmclient.search(**search_params)</span></pre>
<p style="padding-left: 60px">In the preceding code, <span>we instantiated the <kbd>sagemaker</kbd> client by passing the service name.</span></p>
<ol start="3">
<li>We will then call the search function of the SageMaker client by passing search parameters, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">search_params={ "MaxResults": 10, "Resource": "TrainingJob",<br/>"SearchExpression": {<br/> "Filters": [{"Name": "AlgorithmSpecification.TrainingImage","Operator": "Equals","Value": "Object2Vec"}]},<br/> "SortBy": "Metrics.validation:mean_squared_error",<br/> "SortOrder": "Descending"}</pre>
<p>In the preceding code block, we defined search parameters such as the type of resource to search for the maximum number of results to show, search expression, and sort by and order. We pass the search parameters that were defined to the search function of the <span>SageMaker</span> client to retrieve results:</p>
<p><span><strong>To find the winning training job</strong>, do the following:</span></p>
<ol>
<li><span>Search for the experiments, as we discussed earlier. We can search based on several attributes, such as fields related to <kbd>TrainingJob</kbd>, <kbd>TuningJob</kbd>, <kbd>AlgorithmSpecification</kbd>, <kbd>InputDataConfiguration</kbd>, and <kbd>ResourceConfiguration</kbd></span>.</li>
<li><span>Once the relevant experiments have been retrieved, </span>we can sort them by objective metrics to find the winning training job.</li>
</ol>
<p><strong>To</strong> <strong>deploy the best mode</strong>, follow these steps:</p>
<ol>
<li>Click on the winning training job and click on the <span class="packt_screen">Create Model</span> button at the top.</li>
<li>Specify the location of model artifacts and registry path of the inference image, among other details, to create a model. Once the model have been created, navigate to <span class="packt_screen">Models</span> under the <span class="packt_screen">Inference</span> section (the left navigation menu) of the SageMaker service.</li>
<li>You will find two options: <span class="packt_screen">Create batch transform job</span> and <span class="packt_screen">create endpoint</span>. For real-time inference, click on <span class="packt_screen">create endpoint</span> and provide configuration details.</li>
</ol>
<p><span>To track the lineage of a deployed model, do the following:</span></p>
<ol>
<li class="mce-root"><span>Choose </span><span class="packt_screen">Endpoints</span><span> in the left navigation pane and select the endpoint of the winning model.</span></li>
<li><span>Scroll to the <span class="packt_screen">Endpoint Configuration Settings </span></span><span>to locate the hyperlink to the <span class="packt_screen">Training Job</span> that was used to create the endpoint.</span></li>
<li><span>Once the hyperlink has been clicked, you should see details on the model and training job, as shown in the following screenshot:</span></li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d43f5251-49a1-459a-84f3-19cf3053a07d.png" style=""/></div>
<p>You can also programmatically track the lineage of a deployed model:</p>
<ol>
<li>Use <kbd>boto3</kbd> to get the endpoint configuration by calling the <kbd>describe_endpoint_config()</kbd> function of the SageMaker client.</li>
<li>From the configuration, select the model name to retrieve the Model Data URL.</li>
<li>R<span>etrieve a training job </span>from the Model Data URL. By doing this, from a deployed endpoint, we can trace back to the training job.</li>
</ol>
<p>Let's now turn our attention to how SageMaker allows data scientists to bring their own machine learning and deep learning libraries to AWS. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bring your own model – SageMaker, MXNet, and Gluon</h1>
                </header>
            
            <article>
                
<p>This section focuses on how SageMaker allows you to bring your own deep learning libraries to the Amazon Cloud and still utilize the productivity features of SageMaker to automate training and deployment at scale.</p>
<p>The deep learning library we will bring in here is Gluon:</p>
<ul>
<li>Gluon is an open source deep learning library jointly created by AWS and Microsoft.</li>
<li>The primary goal of the library is to allow developers to build, train, and deploy machine learning models in the cloud.</li>
</ul>
<p>In the past, a tremendous amount of research has been conducted on recommender systems. In particular, Deep Structured Semantic models attempt to capture information from attributes, such as product image, title, and description. Extracting semantic information from these additional characteristics will solve the cold start problem in the space of recommender systems. In other words, when there is not much consumption history for a given user, a recommender system can propose products similar to the minimal products that are purchased by the user.</p>
<p>Let's see how pretrained word embeddings, available via the <kbd>gluonnlp</kbd> library, can be used in SageMaker to find books similar to the books that a user likes, that is, recommended books whose titles are semantically similar to titles of books that a user likes.</p>
<p>To do this, we will look at the same book ratings dataset we used in the previous sections of this chapter:</p>
<ol>
<li><span>Let's begin by installing the prerequisites:</span>
<ul>
<li><span><kbd>mxnet</kbd>: This is a deep learning framework.</span></li>
<li><kbd>gluonnlp</kbd>: This builds on top of MXNet. It is an open source deep learning library for <strong>natural language processing</strong> (<strong>NLP</strong>).</li>
<li><kbd>nltk</kbd>: This is a Python natural language toolkit.</li>
</ul>
</li>
<li>Next, we will read the filtered book ratings dataset that we created in the <em>Conduct Training in Amazon SageMaker </em>section. Then, we will obtain unique book titles from the dataset.</li>
<li>From each of the book titles, remove words with punctuation marks, numbers, and other special characters and only retain words that contain alphabets, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">words = []<br/><br/>for i in df_bktitles['BookTitle']:<br/>    tokens = word_tokenize(i)<br/>    words.append([word.lower() for word in tokens if word.isalpha()])</pre>
<p style="padding-left: 60px">In the preceding code block, we can see the following:</p>
<ul>
<li style="padding-left: 30px">We iterate through each of the book titles and create tokens by calling the <kbd>word_tokenize()</kbd> function from <kbd>nltk.tokenize</kbd>.</li>
<li style="padding-left: 30px">For each title, we only retain words containing alphabets by calling the <kbd>isapha()</kbd> method on word strings. In the end, we have a list of lists called <kbd>words</kbd>.</li>
</ul>
<ol start="4">
<li>Next, we will count the frequency of tokens across all of the book titles, as shown in the following:</li>
</ol>
<pre style="padding-left: 60px">counter = nlp.data.count_tokens(itertools.chain.from_iterable(words))</pre>
<p style="padding-left: 60px">In the preceding code, we can see the following:</p>
<ul>
<li style="padding-left: 30px">To compute the frequency of tokens, we called the <kbd>count_tokens()</kbd> function from <kbd>gluonnlp.data</kbd> by passing the words list to it.</li>
<li style="padding-left: 30px"><kbd>counter</kbd><em> </em>is a dictionary containing tokens (keys) and associated frequencies (values).</li>
</ul>
<ol start="5">
<li>Load the pre-trained word embedding vectors that were trained using fastText—a library from the Facebook AI Research lab that's used to learn word embeddings. Then, tie the word embeddings to each of the words in a book title, as shown here:</li>
</ol>
<pre style="padding-left: 60px">vocab = nlp.Vocab(counter)<br/>fasttext_simple = nlp.embedding.create('fasttext', source='wiki.simple')<br/>vocab.set_embedding(fasttext_simple)</pre>
<p style="padding-left: 60px"><span> In the preceding code block, we can see the following:</span></p>
<ul>
<li style="padding-left: 30px"><span>We created the indexes of tokens that can be attached to token embeddings by instantiating the <kbd>Vocab</kbd> class.</span></li>
<li style="padding-left: 30px"><span>We then instantiated word/token embeddings by passing embedding type as <kbd>fasttext</kbd></span>.</li>
<li style="padding-left: 30px"><span>We called the <kbd>set_embedding()</kbd> method of the <kbd>Vocab</kbd> object to attach pre-trained word embedding to each of the tokens.</span></li>
</ul>
<ol start="6">
<li>Now, we create the embedding of a book title by averaging across individual word embeddings, as shown here:</li>
</ol>
<pre style="padding-left: 60px">for title in words:<br/>title_arr = ndarray.mean(vocab.embedding[title], axis=0, keepdims=True)<br/>title_arr_list = np.append(title_arr_list, title_arr.asnumpy(), axis=0)</pre>
<p style="padding-left: 60px">In the preceding code, we can see the following:</p>
<ul>
<li style="padding-left: 30px">We iterated through each of the book titles and computed its embedding by averaging across all of the embeddings of the words in the title. This is done by calling the <kbd>mean()</kbd> method of the <kbd>ndarray</kbd> object, an <em>n-</em>dimensional array.</li>
<li style="padding-left: 30px">We then created an array, <kbd>title_arr_list</kbd>, of title embeddings by using the <span><kbd>append()</kbd> method of the </span><kbd>numpy</kbd> module.</li>
</ul>
<ol start="7">
<li>It is now time to plot book titles—first, we will reduce the dimensions of the embeddings from 300 dimensions to 2. Note that the shape of <kbd>title_arr_list</kbd> is 978 x 300. This means that the array has 978 unique book titles and each title is represented by a vector that's 300 in size. We will use the <strong>T-distributed Stochastic Neighbor Embedding</strong> (<strong><span>TSNE</span></strong>) algorithm to reduce the dimensionality but still retain its original meaning—that is, the distance between titles in a higher dimensional space is going to be the same as the distance between titles in a lower dimensional space. To go to a lower dimensional space for the title, we instantiate the <kbd>TSNE</kbd> class from the <kbd>sklearn</kbd> library, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">tsne = TSNE(n_components=2, random_state=0)<br/>Y = tsne.fit_transform(title_arr_list)</pre>
<p>In the preceding code block, we called the <kbd>fit_transform()</kbd> method of the <kbd>TSNE</kbd> object to return the transformed version of embedding. </p>
<p>After we get the transformed embedding, we will do a scatter plot with one dimension on the <em>x</em>-a<span>xis and another dimension on the <em>y</em>-axis, as shown</span><span> in the </span>following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9684b30c-d85a-40d1-a9e1-9f315f2caa23.png" style=""/></div>
<p><span>The proximity of book titles implies that they are semantically similar. </span>For example, titles such as <em>Room</em> and <em>A Room with a View</em> seem to talk about the same subject room. These titles are located together in the lower dimensional space.</p>
<p>In this section, you learned how to bring pretrained word embeddings from fastText via the MXNet deep learning library to SageMaker. It is also possible to also train neural networks that have been built using the MXNet deep learning library from scratch. The same capabilities of SageMaker, such as training and deployment, are equally available for both built-in and custom algorithms.</p>
<p>Now that we have walked through how to bring your machine and/or deep learning library to SageMaker, it is time to look at how to bring your own container.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bring your own container – R model</h1>
                </header>
            
            <article>
                
<p>In this section, we will illustrate the process of bringing your own Docker container to Amazon SageMaker. Particularly, we will focus on training and hosting R models seamlessly in Amazon SageMaker. Rather than reinventing the wheel in terms of building ML models using SageMaker's built-in algorithms, data scientists and machine learning engineers can reuse the work that they've done in R in SageMaker.</p>
<p>The following is the architecture regarding how different AWS components interact to train and host R models:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/894122ec-03e5-4497-9581-3b1126170431.png" style=""/></div>
<p>To follow the preceding architectural diagram, we start with Amazon <strong>Elastic Container Registry</strong> (<strong>ECR</strong>):</p>
<ol>
<li>We create a Docker image containing an underlying operating system, prerequisites to train a recommender algorithm in R, and R code for training and scoring the <strong>User-Based Collaborative Filtering</strong> (<strong><span>UBCF</span></strong>) recommender algorithm.</li>
<li>The created Docker image is then published to Amazon ECR. Remember that the training data for both SageMaker built-in and custom algorithms sits in the S3 bucket.</li>
<li>To start a training job in SageMaker, you designate the location of the training data and Docker registry path (in ECR) of the training image.</li>
<li>During training, the appropriate R functions are triggered to train the UBCF algorithm. The training happens on SageMaker's machine learning compute instances.</li>
<li>The resulting trained models known as model artifacts, are saved to the designated location on the S3 bucket.</li>
</ol>
<p>As for hosting the trained model, SageMaker requires two things:</p>
<ul>
<li>Model artifacts</li>
<li>The Docker registry path of the inference image</li>
</ul>
<p>To create an inference endpoint, the following takes place:</p>
<ol>
<li>SageMaker will create a model by passing the Docker registry path of the inference image of the R model and model artifacts.</li>
<li>Once the SageMaker model has been created, SageMaker launches machine learning to compute instances by instantiating the Docker inference image.</li>
<li>The compute instances will have R code for inference available as a RESTful API.</li>
</ol>
<p>In this section, we will look at the same book ratings dataset we used in the previous sections of this chapter, goodbooks-10k. Our goal is to suggest the top five books to users who are not part of the training dataset. We will use the <kbd>recommenderlab</kbd> R package to measure the cosine distance between users (<span>UBCF</span>). For our target user, we will pick 10 users/neighbors from the training set based on cosine similarity.</p>
<p>To estimate the top five book recommendations for the target user, the UBCF algorithm uses two things:</p>
<ul>
<li>Target user preferences for some books in the collection</li>
<li>The trained model</li>
</ul>
<p>With the help of a trained model, we will compute ratings for books that the target user has never rated before. The top five books (among all of the books in the dataset) with the highest ratings are proposed to a given user. The trained model fills in ratings for all the books and for all of the users in the training dataset, as shown in the following:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d1670572-6bc3-4aa8-b9d0-45d59653ed10.png" style=""/></div>
<p>During the training process, UBCF computes the missing ratings. Let's assume that we want to fill in missing ratings for user A. User A has only rated <strong>book #1</strong> (<strong>BK1</strong>) and <strong>book #3</strong> (<strong>BK3</strong>). To compute ratings for books 2, 4, and 5, the UBCF algorithm does the following:</p>
<ul>
<li>It computes the cosine similarity between user A and the rest of the users in the training dataset. To compute the similarity between user A and B, we do the following:</li>
</ul>
<ol>
<li style="padding-left: 30px">If users A and B have common books that they've rated, multiply the ratings by the book.</li>
<li style="padding-left: 30px">Add these ratings across all of the common books.</li>
<li style="padding-left: 30px">Then, divide the result by the norm of vectors represented by users A and B.</li>
</ol>
<ul>
<li>Given a similarity score for users B through E relative to A, compute the rating for a given new book by taking the weighted average of ratings given by users B through E for that book:</li>
</ul>
<ol>
<li style="padding-left: 30px">For example, to compute the rating for book #2 for user A, we multiply a rating of 3 given by user B for book #2 by a similarity score of 0.29 and <span>multiply a rating of 4 given by user C for book #2 by a similarity score of 0.73. We add these two factors together.</span></li>
<li style="padding-left: 30px"><span>We then add the two similarity score of 0.29 and 0.73.</span></li>
<li style="padding-left: 30px">Finally, we divide the results from 1 with 2.</li>
</ol>
<p>Now that we have looked at the training and hosting architecture for custom containers in SageMaker and discussed the use case, let's begin the implementation:</p>
<ol>
<li>The first step is to define the Dockerfile by highlighting the requirements to run the R code. The requirements are an underlying operating system, the R version, the R packages, and the location of the R logic for training and inference. Create and publish a Docker image to the <strong>EC2 Container Registry</strong> (<strong>ECR</strong>).</li>
</ol>
<p style="padding-left: 60px"><span>The following Dockerfile defines the specifications for training and hosting R model:</span></p>
<pre style="padding-left: 60px"><strong>FROM ubuntu:16.04</strong><br/><br/><strong>RUN apt-get -y update --allow-unauthenticated &amp;&amp; apt-get install -y --no-install-recommends \</strong><br/><strong> wget \</strong><br/><strong> r-base \</strong><br/><strong> r-base-dev \</strong><br/><strong> ca-certificates</strong></pre>
<p style="padding-left: 60px">In the <span>preceding </span>code block, we can see the following:</p>
<ul>
<li style="padding-left: 30px">We defined the version of the Ubuntu operating system to install.</li>
<li style="padding-left: 30px">We also specified that we need R installed.</li>
<li style="padding-left: 30px">Additionally, we have specified the R packages that need to be in place for the <kbd>Recommender</kbd> algorithm to work, as shown in the following code:</li>
</ul>
<pre style="padding-left: 60px">RUN R -e "install.packages(c('reshape2', 'recommenderlab', 'plumber', 'dplyr', 'jsonlite'), quiet = TRUE)"<br/><br/>COPY Recommender.R /opt/ml/Recommender.R<br/>COPY plumber.R /opt/ml/plumber.R<br/><br/>ENTRYPOINT ["/usr/bin/Rscript", "/opt/ml/Recommender.R", "--no-save"]</pre>
<p style="padding-left: 60px">In the preceding code, we can see the following:</p>
<ul>
<li style="padding-left: 30px">We copied the training (<kbd>Recommender.R</kbd>) and inference (<kbd>plumber.R</kbd>) code to the appropriate location.</li>
<li style="padding-left: 30px">Later, we specified an entry point (code to run) after the Docker image is instantiated.</li>
</ul>
<p style="padding-left: 60px">Now that the Dockerfile has been compiled, it is time to create a Docker image and push it to ECR, as shown here:</p>
<pre style="padding-left: 60px">Docker build -t ${algorithm_name}.<br/>Docker tag ${algorithm_name} ${fullname}<br/>Docker push ${fullname}</pre>
<p style="padding-left: 60px">In the preceding code, we can see the following:</p>
<ul>
<li style="padding-left: 30px">To build the Docker image locally, we run the <kbd>Docker build</kbd> command by passing the image name to the local SageMaker instance.</li>
<li style="padding-left: 30px">The Dockerfile from the local directory (<kbd>"."</kbd>) is leveraged.</li>
<li style="padding-left: 30px">After tagging the Docker image, we then push it to ECR with the <kbd>Docker push</kbd> command.</li>
</ul>
<ol start="2">
<li>The next step is to create a SageMaker training job, listing training dataset, the latest Docker image for training, and infrastructure specifications. The model artifacts from the training job are stored in the relevant S3 bucket. This is very similar to running any training job on SageMaker.</li>
</ol>
<p style="padding-left: 60px">Let's understand the R functions that are triggered during training:</p>
<ul>
<li style="padding-left: 30px">Remember that the <kbd>Recommender.R</kbd> code gets executed when ML compute instances are launched as part of the training.</li>
<li style="padding-left: 30px">Depending on the command-line arguments that are passed, either the <kbd>train()</kbd> function or <kbd>serve()</kbd> function is executed, as shown in the following code:</li>
</ul>
<pre style="padding-left: 60px">args &lt;- commandArgs()<br/>if (any(grepl('train', args))) {<br/> train()}<br/>if (any(grepl('serve', args))) {<br/> serve()}</pre>
<ul>
<li style="padding-left: 30px">If the command-line argument contains the <kbd>train</kbd> <span>keyword, the</span> <kbd>train()</kbd> function gets executed. The same logic holds true for the <kbd>serve</kbd> keyword.</li>
</ul>
<p style="padding-left: 60px">During training, SageMaker copies the training dataset from the S3 bucket to ML compute instances. After we prepare training data for model fitting, we call the <kbd>Recommender</kbd> method (the <kbd>recommenderlab</kbd> R package) by specifying the number of users in the training set, the type of recommender algorithm, and the type of output (top N book recommendations), as shown in the following:</p>
<pre style="padding-left: 60px">rec_model = Recommender(ratings_mat[1:n_users], method = "UBCF", param=list(method=method, nn=nn))</pre>
<p style="padding-left: 60px">In the preceding code block, we can see the following:</p>
<ul>
<li style="padding-left: 30px">We train the model on 270 users and 973 books.</li>
<li style="padding-left: 30px">The entire dataset contains 275 users.</li>
</ul>
<p style="padding-left: 60px">Please refer to the source code attached to this chapter. Once the UBCF algorithm has been trained, the resulting model is saved in the designated location on the ML compute instance, which is then pushed to the specified location on the S3 bucket (model output path).</p>
<ol start="3">
<li>The third step is to host the trained model as an endpoint (RESTful API). SageMaker will need to create a model before provisioning an endpoint. <span>Model artifacts and Docker images from training are required to define a SageMaker model. Note that the Docker image that was used for training is also used for inference. The SageMaker endpoint takes infrastructure specifications for ML compute instances as input, along with the SageMaker model. Again, this process of creating an endpoint in SageMaker for custom containers is the same as that for built-in algorithms.</span></li>
</ol>
<p style="padding-left: 60px"><span>Let's understand the R functions that are triggered during inference. </span></p>
<p style="padding-left: 60px">The following R function is run when SageMaker sends the <kbd>serve</kbd> command at the time of inference, as shown in the following code:</p>
<pre style="padding-left: 60px"># Define scoring function<br/>serve &lt;- function() {<br/> app &lt;- plumb(paste(prefix, 'plumber.R', sep='/'))<br/> app$run(host='0.0.0.0', port=8080)}</pre>
<p style="padding-left: 60px">In the preceding code, we can see the following:</p>
<ul>
<li style="padding-left: 30px">We have used the plumber R package to turn R functions into REST endpoints.</li>
<li style="padding-left: 30px">R functions that will need to be converted in to REST APIs are decorated with appropriate comments.</li>
<li style="padding-left: 30px">We used the <kbd>plumb()</kbd> method to host the <kbd>plumber.R</kbd> code as an endpoint.</li>
</ul>
<p style="padding-left: 60px">For each of the HTTP requests that's sent to the endpoint, the appropriate function is called, as shown here:</p>
<pre style="padding-left: 60px">load(paste(model_path, 'rec_model.RData', sep='/'), verbose = TRUE)<br/>pred_bkratings &lt;- predict(rec_model, ratings_mat[ind], n=5)</pre>
<p style="padding-left: 60px">In the preceding code, we can see the following:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li>At the time of inference, we load the trained model by calling the <kbd>load()</kbd> method and passing the path to the model artifacts.</li>
<li>We then call the <kbd>predict()</kbd> method by specifying the name of the trained model, the new user vector or book preferences, and the number of books to recommend.</li>
<li>Note that the ratings matrix, <kbd>ratings_mat</kbd>, contains all 275 users and their ratings, where present, for books. In this case, we are interested in user #272. Remember that, in the dataset for this section, we have a total of 275 users and 973 books. </li>
</ul>
</li>
</ul>
<ol start="4">
<li>The fourth step is to run model inference, as shown here:</li>
</ol>
<pre style="padding-left: 60px">payload = ratings.to_csv(index=False) <br/><br/>response = runtime.invoke_endpoint(EndpointName='BYOC-r-endpoint-&lt;timestamp&gt;', ContentType='text/csv', Body=payload)<br/><br/>result = json.loads(response['Body'].read().decode())<br/><br/></pre>
<p style="padding-left: 60px">In the preceding code, we can see the following:</p>
<ul>
<li style="padding-left: 30px">We captured the entire dataset of 275 users in a CSV file called <strong>payload</strong>.</li>
<li style="padding-left: 30px">We then pass the payload file as input to the <kbd>invoke_endpoint()</kbd> method of the SageMaker runtime, along with the endpoint name and content type.</li>
</ul>
<p style="padding-left: 60px">The endpoint responds with results, as shown in the following:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5275b7b3-6cc1-408f-aebd-d80134ae0e43.png" style=""/></div>
<p class="mce-root">By doing this, we have seen how seamless it is to bring your own container to SageMaker to train and host models, reusing training and scoring (inference) logic that's been written in other languages.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you've learned how to process big data to create an analytics-ready dataset. You've also seen how SageMaker automates most of the steps of the machine learning life cycle, enabling you to build, train, and deploy models seamlessly. Additionally, we've illustrated some of the productivity features, such as hyperparameter optimization and experimentation service, which enable data scientists to run multiple experiments and deploy the winning model. Finally, we have looked at bringing our own models and containers <span>to the SageMaker ecosystem. Through bringing our own models </span>based on open source machine learning libraries, we can readily build solutions based on open source frameworks, while still leveraging all of the capabilities of the platform. Similarly, by bringing our own container, we can readily port solutions, written in other programming languages besides Python, to SageMaker.</p>
<p>Learning all of the aforementioned aspects of Amazon SageMaker enables data scientists and machine learning engineers to decrease speed-to-market machine learning solutions.</p>
<p>In the next chapter, we will cover how to create training and inference pipelines so that models can be trained and deployed for efficiently running inferences (by creating reusable components).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>For extended examples and details on working with SageMaker, please refer to the following AWS blogs:</p>
<ul>
<li><span class="MsoHyperlink"><a href="https://github.com/awslabs/amazon-sagemaker-examples">https://github.com/awslabs/amazon-sagemaker-examples</a></span></li>
<li><span class="MsoHyperlink"><a href="https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/">https://aws.amazon.com/blogs/machine-learning/introduction-to-amazon-sagemaker-object2vec/</a><br/></span></li>
<li><span class="MsoHyperlink"><a href="https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-now-comes-with-new-capabilities-for-accelerating-machine-learning-experimentation/">https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-now-comes-with-new-capabilities-for-accelerating-machine-learning-experimentation/</a><br/></span></li>
</ul>


            </article>

            
        </section>
    </body></html>