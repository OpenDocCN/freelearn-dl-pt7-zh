["```py\n    import gym\n    import numpy as np\n    from collections import defaultdict\n    from functools import partial\n    ```", "```py\n    #set the environment as blackjack\n    env = gym.make('Blackjack-v0')\n    ```", "```py\n    #number of observation space value\n    print(env.observation_space)\n    #number of action space value\n    print(env.action_space)    \n    ```", "```py\n    Tuple(Discrete(32), Discrete(11), Discrete(2))\n    Discrete(2)\n    ```", "```py\n    def play_game(state):\n        player_score, dealer_score, usable_ace = state \n        #if player_score is greater than 17, stick\n        if (player_score >= 17):\n            return 0 # don't take any cards, stick\n        else:\n            return 1 # take additional cards, hit\n    ```", "```py\n    for game_num in range(100):\n        print('***Start of Game:', game_num)\n        state = env.reset()\n        action_text = {1:'Hit, Take more cards!!', \\\n                       0:'Stick, Dont take any cards' }\n        player_score, dealer_score, usable_ace = state\n        print('Player Score=', player_score,', \\\n              Dealer Score=', dealer_score, ', \\\n              Usable Ace=', usable_ace)\n    ```", "```py\n        for i in range(100):\n            action = play_game(state)\n            state, reward, done, info = env.step(action)\n            player_score, dealer_score, usable_ace = state\n            print('Action is', action_text[action])\n            print('Player Score=', player_score,', \\\n                  Dealer Score=', dealer_score, ', \\\n                  Usable Ace=', usable_ace, ', Reward=', reward)\n            if done:\n                if (reward == 1):\n                    print('***End of Game:', game_num, \\\n                          ' You have won Black Jack!\\n')\n                elif (reward == -1):\n                    print('***End of Game:', game_num, \\\n                          ' You have lost Black Jack!\\n')\n                elif (reward ==0):\n                    print('***End of Game:', game_num, \\\n                          ' The game is a Draw!\\n') \n                break\n    ```", "```py\nif current_state not in states[:i]:\n```", "```py\n\"\"\"\nonly include the rewards of the states that have not been visited before\n\"\"\"\n            if current_state not in states[:i]:\n                #increasing the count of states by 1\n                num_states[current_state] += 1\n\n                #finding the value_function by incremental method\n                value_function[current_state] \\\n                += (total_rewards - value_function[current_state]) \\\n                / (num_states[current_state])\n      return value_function\n```", "```py\n    import gym\n    import numpy as np\n    from collections import defaultdict\n    from functools import partial\n    ```", "```py\n    env = gym.make('Blackjack-v0')\n    ```", "```py\n    def policy_blackjack_game(state):\n        player_score, dealer_score, usable_ace = state\n        if (player_score >= 17):\n            return 0 # don't take any cards, stick\n        else:\n            return 1 # take additional cards, hit\n    ```", "```py\n    def generate_blackjack_episode():\n        #initializing the value of episode, states, actions, rewards\n        episode = []\n        states = []\n        actions = []\n        rewards = []\n    ```", "```py\n       #starting the environment\n        state = env.reset()\n\n        \"\"\"\n        setting the state value to player_score, \n        dealer_score and usable_ace\n        \"\"\"\n        player_score, dealer_score, usable_ace = state\n    ```", "```py\n        while (True):\n            #finding the action by passing on the state\n            action = policy_blackjack_game(state)\n            next_state, reward, done, info = env.step(action)\n    ```", "```py\n            #creating a list of episodes, states, actions, rewards\n            episode.append((state, action, reward))\n            states.append(state)\n            actions.append(action)\n            rewards.append(reward)\n    ```", "```py\n            if done:\n                break\n            state = next_state\n    ```", "```py\n        return episode, states, actions, rewards\n    ```", "```py\n    def black_jack_first_visit_prediction(policy, env, num_episodes):\n        \"\"\"\n        initializing the value of total_rewards, \n        number of states, and value_function\n        \"\"\"\n        total_rewards = 0\n        num_states = defaultdict(float)\n        value_function = defaultdict(float)\n    ```", "```py\n        for k in range (0, num_episodes):\n            episode, states, actions, rewards = \\\n            generate_blackjack_episode()\n            total_rewards = 0\n            for i in range(len(states)-1, -1,-1):\n                current_state = states[i]\n                #finding the sum of rewards\n                total_rewards += rewards[i]\n    ```", "```py\n                \"\"\"\n                only include the rewards of the states that \n                have not been visited before\n                \"\"\"\n                if current_state not in states[:i]:\n                    #increasing the count of states by 1\n                    num_states[current_state] += 1\n\n                    #finding the value_function by incremental method\n                    value_function[current_state] \\\n                    += (total_rewards \\\n                    - value_function[current_state]) \\\n                    / (num_states[current_state])\n        return value_function\n    ```", "```py\n    black_jack_first_visit_prediction(policy_blackjack_game, env, 10000)\n    ```", "```py\nif current_state not in states[:i]:\n```", "```py\n            #all the state values of every visit are considered\n            #increasing the count of states by 1\n            num_states[current_state] += 1\n\n            #finding the value_function by incremental method\n            value_function[current_state] \\\n            += (total_rewards - value_function[current_state]) \\\n            / (num_states[current_state])\n    return value_function\n```", "```py\n    import gym\n    import numpy as np\n    from collections import defaultdict \n    from functools import partial\n    ```", "```py\n    env = gym.make('Blackjack-v0')\n    ```", "```py\n    def policy_blackjack_game(state):\n        player_score, dealer_score, usable_ace = state \n        if (player_score >= 17):\n            return 0 # don't take any cards, stick\n        else:\n            return 1 # take additional cards, hit\n    ```", "```py\n    def generate_blackjack_episode():\n        #initializing the value of episode, states, actions, rewards\n        episode = []\n        states = []\n        actions = []\n        rewards = []\n    ```", "```py\n        #starting the environment\n        state = env.reset()\n        \"\"\"\n        setting the state value to player_score, dealer_score and \n        usable_ace\n        \"\"\"\n        player_score, dealer_score, usable_ace = state\n    ```", "```py\n        while (True):\n            #finding the action by passing on the state\n            action = policy_blackjack_game(state)       \n            next_state, reward, done, info = env.step(action)\n    ```", "```py\n            #creating a list of episodes, states, actions, rewards\n            episode.append((state, action, reward))\n            states.append(state)\n            actions.append(action)\n            rewards.append(reward)\n    ```", "```py\n            if done:\n                break\n            state = next_state\n    ```", "```py\n        return episode, states, actions, rewards\n    ```", "```py\n    def black_jack_every_visit_prediction\\\n    (policy, env, num_episodes):\n        \"\"\"\n        initializing the value of total_rewards, number of states, \n        and value_function\n        \"\"\"\n        total_rewards = 0\n        num_states = defaultdict(float)\n        value_function = defaultdict(float)\n    ```", "```py\n        for k in range (0, num_episodes):\n            episode, states, actions, rewards = \\\n            generate_blackjack_episode() \n            total_rewards = 0\n            for i in range(len(states)-1, -1,-1):\n                current_state = states[i]\n                #finding the sum of rewards\n                total_rewards += rewards[i]\n    ```", "```py\n                #all the state values of every visit are considered\n                #increasing the count of states by 1\n                num_states[current_state] += 1\n                #finding the value_function by incremental method\n                value_function[current_state] \\\n                += (total_rewards - value_function[current_state]) \\\n                / (num_states[current_state])\n        return value_function\n    ```", "```py\n    black_jack_every_visit_prediction(policy_blackjack_game, \\\n                                      env, 10000)\n    ```", "```py\nfor step in range(len(episode))[::-1]:\n            state, action, reward = episode[step]\n            #G <- gamma * G + Rt+1\n            G = discount_factor * G + reward    \n            # C(St, At) = C(St, At) + W\n            C[state][action] += W\n            #Q (St, At) <- Q (St, At) + W / C (St, At)\n            Q[state][action] += (W / C[state][action]) \\\n            * (G - Q[state][action])\n            \"\"\"\n            If action not equal to argmax of target policy \n            proceed to next episode\n            \"\"\"\n            if action != np.argmax(target_policy(state)):\n                break\n            # W <- W * Pi(At/St) / b(At/St)\n            W = W * 1./behaviour_policy(state)[action]\n```", "```py\n    import gym\n    import numpy as np\n    from collections import defaultdict\n    from functools import partial\n    ```", "```py\n    env = gym.make('Blackjack-v0')\n    ```", "```py\n    \"\"\"\n    creates a random policy which is a linear probability distribution\n    num_Action is the number of Actions supported by the environment\n    \"\"\"\n    def create_random_policy(num_Actions): \n    #Creates a list of size num_Actions, with a fraction 1/num_Actions.\n    #If 2 is numActions, the array value would [1/2, 1/2]\n        Action = np.ones(num_Actions, dtype=float)/num_Actions\n        def policy_function(observation):\n            return Action\n        return policy_function\n    ```", "```py\n    #creates a greedy policy,\n    \"\"\"\n    sets the value of the Action at the best_possible_action, \n    that maximizes the Q, value to be 1, rest to be 0\n    \"\"\"\n    def create_greedy_policy(Q):\n        def policy_function(state):\n            #Initializing with zero the Q\n            Action = np.zeros_like(Q[state], dtype = float)\n            #find the index of the max Q value \n            best_possible_action = np.argmax(Q[state])\n            #Assigning 1 to the best possible action\n            Action[best_possible_action] = 1.0\n            return Action\n        return policy_function\n    ```", "```py\n    def black_jack_importance_sampling\\\n    (env, num_episodes, behaviour_policy, discount_factor=1.0):\n            #Initialize the value of Q\n            Q = defaultdict(lambda: np.zeros(env.action_space.n))\n            #Initialize the value of C\n            C = defaultdict(lambda: np.zeros(env.action_space.n))\n            #target policy is the greedy policy\n            target_policy = create_greedy_policy(Q)\n    ```", "```py\n            for i_episode in range(1, num_episodes + 1):\n                episode = []\n                state = env.reset()\n    ```", "```py\n                for i in range(100):\n                    probability = behaviour_policy(state)\n                    action = np.random.choice\\\n                             (np.arange(len(probability)), p=probability)\n                    next_state, reward, done, info = env.step(action)\n                    episode.append((state, action, reward))\n    ```", "```py\n                    if done:\n                        break\n                    state = next_state \n    ```", "```py\n                   # G <- 0\n                         G = 0.0\n                         # W <- 0\n                         W = 1.0  \n    ```", "```py\n                \"\"\"\n                Loop for each step of episode t=T-1, T-2,...,0 \n                while W != 0\n                \"\"\"\n                for step in range(len(episode))[::-1]:\n                    state, action, reward = episode[step]\n                    #G <- gamma * G + Rt+1\n                    G = discount_factor * G + reward\n                    # C(St, At) = C(St, At) + W\n                    C[state][action] += W\n                    #Q (St, At) <- Q (St, At) + W / C (St, At)\n                    Q[state][action] += (W / C[state][action]) \\\n                    * (G - Q[state][action])\n                    \"\"\"\n                    If action not equal to argmax of target policy \n                    proceed to next episode\n                    \"\"\"\n                    if action != np.argmax(target_policy(state)):\n                        break\n                    # W <- W * Pi(At/St) / b(At/St)\n                    W = W * 1./behaviour_policy(state)[action]\n    ```", "```py\n            return Q, target_policy \n    ```", "```py\n    #create random policy\n    random_policy = create_random_policy(env.action_space.n)\n    \"\"\"\n    using importance sampling evaluates the target policy \n    by learning from the behaviour policy\n    \"\"\"\n    Q, policy = black_jack_importance_sampling\\\n                (env, 50000, random_policy)\n    ```", "```py\n    valuefunction = defaultdict(float)\n    for state, action_values in Q.items():\n        action_value = np.max(action_values)\n        valuefunction[state] = action_value\n        print(\"state is\", state, \"value is\", valuefunction[state])\n    ```", "```py\nwhile not done:\n\n        #random action less than epsilon\n        if np.random.rand() < epsilon:\n            #we go with the random action\n            action = env.action_space.sample()\n        else:\n            \"\"\"\n            1 - epsilon probability, we go with the greedy algorithm\n            \"\"\"\n            action = np.argmax(Q[state, :])\n```", "```py\nimport gym\nenv = gym.make(\"FrozenLake-v0\", is_slippery=False)\n```"]