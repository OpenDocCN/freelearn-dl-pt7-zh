- en: 3\. An Introduction to Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces you to classification. You will implement various techniques,
    such as k-nearest neighbors and SVMs. You will use the Euclidean and Manhattan
    distances to work with k-nearest neighbors. You will apply these concepts to solve
    intriguing problems such as predicting whether a credit card applicant has a risk
    of defaulting and determining whether an employee would stay with a company for
    more than two years. By the end of this chapter, you will be confident enough
    to work with any data using classification and come to a certain conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you were introduced to regression models and learned
    how to fit a linear regression model with single or multiple variables, as well
    as with a higher-degree polynomial.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike regression models, which focus on learning how to predict continuous
    numerical values (which can have an infinite number of values), classification,
    which will be introduced in this chapter, is all about splitting data into separate
    groups, also called classes.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, a model can be trained to analyze emails and predict whether they
    are spam or not. In this case, the data is categorized into two possible groups
    (or classes). This type of classification is also called **binary classification**,
    which we will see a few examples of in this chapter. However, if there are more
    than two groups (or classes), you will be working on a **multi-class classification**
    (you will come across some examples of this in *Chapter 4*, *An Introduction to
    Decision Trees*).
  prefs: []
  type: TYPE_NORMAL
- en: 'But what is a real-world classification problem? Consider a model that tries
    to predict a given user''s rating for a movie where this score can only take values:
    *like*, *neutral*, or *dislike*. This is a classification problem.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to classify data using the k-nearest neighbors
    classifier and SVM algorithms. Just as we did for regression in the previous chapter,
    we will build a classifier based on cleaned and prepared training data and test
    the performance of our classifier using testing data.
  prefs: []
  type: TYPE_NORMAL
- en: We'll begin by looking at the fundamentals of classification.
  prefs: []
  type: TYPE_NORMAL
- en: The Fundamentals of Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated earlier, the goal of any classification problem is to separate the
    data into relevant groups accurately using a training set. There are a lot of
    applications of such projects in different industries, such as education, where
    a model can predict whether a student will pass or fail an exam, or healthcare,
    where a model can assess the level of severity of a given disease for each patient.
  prefs: []
  type: TYPE_NORMAL
- en: A classifier is a model that determines the label (output) or value (class)
    of any data point that it belongs to. For instance, suppose you have a set of
    observations that contains credit-worthy individuals, and another one that contains
    individuals that are risky in terms of their credit repayment tendencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s call the first group P and the second one Q. Here is an example of such
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Sample dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.1: Sample dataset'
  prefs: []
  type: TYPE_NORMAL
- en: With this data, you will train a classification model that will be able to correctly
    classify a new observation into one of these two groups (this is binary classification).
    The model can find patterns such as a person with a salary above $60,000 being
    less risky or that having a mortgage/income ratio above ratio 10 makes an individual
    more at risk of not repaying their debts. This will be a **multi-class classification**
    exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification models can be grouped into different families of algorithms.
    The most famous ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Distance-based, such as **k-nearest neighbors**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear models, such as **logistic regression** or **SVMs**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree-based, such as **random forest**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, you will be introduced to two algorithms from the first two
    types of family: k-nearest neighbors (distance-based) and SVMs (linear models).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We'll walk you through tree-based algorithms such as random forest in *Chapter
    4*, *An Introduction to Decision Trees*.
  prefs: []
  type: TYPE_NORMAL
- en: But before diving into the models, we need to clean and prepare the dataset
    that we will be using in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will work on a German credit approvals dataset
    and perform all the data preparation required for the modeling stage. Let's start
    by loading the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.01: Predicting Risk of Credit Card Default (Loading the Dataset)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will be loading a dataset into a pandas DataFrame and exploring
    its contents. We will use the dataset of German credit approvals to determine
    whether an individual presents a risk of defaulting.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The CSV version of this dataset can be found on our GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.live/3eriWTr](https://packt.live/3eriWTr).'
  prefs: []
  type: TYPE_NORMAL
- en: The original dataset and information regarding the dataset can be found at [https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29).
  prefs: []
  type: TYPE_NORMAL
- en: The data files are located at [https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/).
  prefs: []
  type: TYPE_NORMAL
- en: Citation - *Dua, D., & Graff, C.. (2017). UCI Machine Learning Repository*.
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `pandas` package as `pd`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new variable called `file_url`, which will contain the URL to the
    raw dataset file, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the data using the `pd.read_csv()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `df.head()` to print the first five rows of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.2: The first five rows of the dataset](img/B16060_03_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.2: The first five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, the output in the preceding screenshot shows us the features
    of the dataset, which can be either numerical or categorical (text).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, use `df.tail()` to print the last five rows of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.3: The last five rows of the dataset](img/B16060_03_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.3: The last five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The last rows of the DataFrame are very similar to the first ones we saw earlier,
    so we can assume the structure is consistent across the rows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, use `df.dtypes` to print the list of columns and their data types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.4: The list of columns and their data types'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_03_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.4: The list of columns and their data types'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3hQXJEs](https://packt.live/3hQXJEs).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3fN0DrT](https://packt.live/3fN0DrT).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding output, we can see that this DataFrame has some numerical
    features ( `int64`) but also text (`object`). We can also see that most of these
    features are either personal details for an individual, such as their age, or
    financial information such as credit history or credit amount.
  prefs: []
  type: TYPE_NORMAL
- en: By completing this exercise, we have successfully loaded the data into the DataFrame
    and had a first glimpse of the features and information it contains.
  prefs: []
  type: TYPE_NORMAL
- en: In the topics ahead, we will be looking at preprocessing this data.
  prefs: []
  type: TYPE_NORMAL
- en: Data Preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before building a classifier, we need to format our data so that we can keep
    relevant data in the most suitable format for classification and remove all the
    data that we are not interested in.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following points are the best ways to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`N/A` (or `NA`) values in the dataset, we may be better off substituting these
    values with a numeric value we can handle. Recall from the previous chapter that
    `NA` stands for `NA` values or replace them with an outlier value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `fillna()` method changes all `NA` values into numeric values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This numeric value should be far from any reasonable value in the DataFrame.
    Minus one million is recognized by the classifier as an exception, assuming that
    only positive values are there, as mentioned in the preceding note.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`0`) specifies that we drop rows, not columns. The second argument (`inplace=True`)
    specifies that we perform the drop operation without cloning the DataFrame, and
    will save the result in the same DataFrame. This DataFrame doesn''t have any missing
    values, so the `dropna()` method didn''t alter the DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The second argument (value `1`) indicates that we are dropping columns, instead
    of rows. The first argument is an enumeration of the columns we would like to
    drop (here, this is `['telephone']`). The `inplace` argument is used so that the
    call modifies the original DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`MinMaxScaler` method of the scikit-learn `preprocessing` utility, as shown
    in the following code snippet:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Binarization transforms data into ones and zeros based on a condition, as shown
    in the following code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding example, we transformed the original data `([19, 65],[4, 52],[2,
    33])` into a binary form based on the condition of whether each value is greater
    than `10` or not (as defined by the `threshold=10` parameter). For instance, the
    first value, `19`, is above `10`, so it is replaced by `1` in the results.
  prefs: []
  type: TYPE_NORMAL
- en: Label encoding is important for preparing your features (inputs) for the modeling
    stage. While some of your features are string labels, scikit-learn algorithms
    expect this data to be transformed into numbers.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the `preprocessing` library of scikit-learn comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed that in the credit scoring example, there were two data
    files. One contained labels in string form, while the other contained labels in
    integer form. We loaded the data with string labels so that you got some experience
    of how to preprocess data properly with the label encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Label encoding is not rocket science. It creates a mapping between string labels
    and numeric values so that we can supply numbers to scikit-learn, as shown in
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s enumerate the encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding result shows us that scikit-learn has created a mapping for each
    day of the week to a respective number; for example, `Friday` will be `0` and
    `Tuesday` will be `3`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: By default, scikit-learn assigned the mapping number by sorting the original
    values alphabetically. This is why `Friday` is mapped to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can use this mapping (also called an encoder) to transform data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try this out on two examples, `Wednesday` and `Friday`, using the `transform()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As expected, we got the results `4` and `0`, which are the mapping values for
    `Wednesday` and `Friday`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use this encoder to perform the inverse transformation with the
    `inverse_transform` function. Let''s try this with the values `0` and `4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As expected, we got back the values `Friday` and `Wednesday`. Now, let's practice
    what we've learned here on the German dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.02: Applying Label Encoding to Transform Categorical Variables into
    Numerical Variables'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will use one of the preprocessing techniques we just learned,
    label encoding, to transform all categorical variables into numerical ones. This
    step is necessary before training any machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using the same dataset that we used in the previous exercise: the
    German credit approval dataset: [https://packt.live/3eriWTr](https://packt.live/3eriWTr).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `pandas` package as `pd`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new variable called `file_url`, which will contain the URL to the
    raw dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data using the `pd.read_csv()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `preprocessing` from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function called `fit_encoder()` that takes a DataFrame and a column
    name as parameters and will fit a label encoder on the values of the column. You
    will use `.LabelEncoder()` and `.fit()` from `preprocessing` and `.unique()` from
    `pandas` (this will extract all the possible values of a DataFrame column):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function called `encode()` that takes a DataFrame, a column name,
    and a label encoder as parameters and will transform the values of the column
    using the label encoder. You will use the `.transform()` method to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new DataFrame called `cat_df` that contains only non-numeric columns
    and print its first five rows. You will use the `.select_dtypes()` method from
    pandas and specify `exclude=''number''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output (not all columns are shown) is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.5: First five rows of the DataFrame containing only non-numeric
    columns'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_03_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.5: First five rows of the DataFrame containing only non-numeric columns'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a list called `cat_cols` that contains the column name of `cat_df` and
    print its content. You will use `.columns` from pandas to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `for` loop that will iterate through each column from `cat_cols`,
    fit a label encoder using `fit_encoder()`, and transform the column with the `encode()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the first five rows of `df`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.6: First five rows of the encoded DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.6: First five rows of the encoded DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Njh57h](https://packt.live/2Njh57h).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2YZhtx5](https://packt.live/2YZhtx5).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully encoded non-numeric columns. Now, our DataFrame contains
    only numeric values.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying Features and Labels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before training our model, we still have to perform two final steps. The first
    one is to separate our features from the label (also known as a response variable
    or dependent variable). The `label` column is the one we want our model to predict.
    For the German credit dataset, in our case, it will be the column called `default`,
    which tells us whether an individual will present a risk of defaulting or not.
  prefs: []
  type: TYPE_NORMAL
- en: The features are all the other columns present in the dataset. The model will
    use the information contained in those columns and find the relevant patterns
    in order to accurately predict the corresponding label.
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn package requires the labels and features to be stored in two
    different variables. Luckily, the pandas package provides a method to extract
    a column from a DataFrame called `.pop()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will extract the `default` column and store it in a variable called `label`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we look at the content of `df`, we will see that the `default` column
    is not present anymore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our features and labels ready, we need to split our dataset
    into training and testing sets.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting Data into Training and Testing Using Scikit-Learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The final step that''s required before training a classifier is to split our
    data into training and testing sets. We already saw how to do this in *Chapter
    2*, *An Introduction to Regression*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `train_test_split` method shuffles and then splits our features and labels
    into a training dataset and a testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We can specify the size of the testing dataset as a number between `0` and `1`.
    A `test_size` of `0.1` means that `10%` of the data will go into the testing dataset.
    You can also specify a `random_state` so that you get the exact same split if
    you run this code again.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the training set to train our classifier and use the testing set
    to evaluate its predictive performance. By doing so, we can assess whether our
    model is overfitting and has learned patterns that are only relevant to the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce you to the famous k-nearest neighbors
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The K-Nearest Neighbors Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our training and testing data, it is time to prepare our classifier
    to perform k-nearest neighbor classification. After being introduced to the k-nearest
    neighbor algorithm, we will use scikit-learn to perform classification.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the K-Nearest Neighbors Algorithm (KNN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of classification algorithms is to divide data so that we can determine
    which data points belong to which group.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that a set of classified points is given to us. Our task is to determine
    which class a new data point belongs to.
  prefs: []
  type: TYPE_NORMAL
- en: In order to train a k-nearest neighbor classifier (also referred to as KNN),
    we need to provide the corresponding class for each observation on the training
    set, that is, which group it belongs to. The goal of the algorithm is to find
    the relevant relationship or patterns between the features that will lead to this
    class. The k-nearest neighbors algorithm is based on a proximity measure that
    calculates the distance between data points.
  prefs: []
  type: TYPE_NORMAL
- en: The two most famous proximity (or distance) measures are the Euclidean and the
    Manhattan distance. We will go through more details in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: For any new given point, KNN will find its k nearest neighbor, see which class
    is the most frequent between those k neighbors, and assign it to this new observation.
    But what is k, you may ask? Determining the value of k is totally arbitrary. You
    will have to set this value upfront. This is not a parameter that can be learned
    by the algorithm; it needs to be set by data scientists. This kind of parameter
    is called a **hyperparameter**. Theoretically, you can set the value of k to between
    1 and positive infinity.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main best practices to take into consideration:'
  prefs: []
  type: TYPE_NORMAL
- en: k should always be an odd number. The reason behind this is that we want to
    avoid a situation that ends in a tie. For instance, if you set *k=4* and it so
    happens that two of the neighbors of a point are from class A and the other two
    are from class B, then KNN doesn't know which class to choose. To avoid this situation,
    it is better to choose *k=3* or *k=5*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The greater k is, the more accurate KNN will be. For example, if we compare
    the cases between *k=1* and *k=15*, the second one will give you more confidence
    that KNN will choose the right class as it will need to look at more neighbors
    before making a decision. On the other hand, with *k=1*, it only looks at the
    closest neighbor and assigns the same class to an observation. But how can we
    be sure it is not an outlier or a special case? Asking more neighbors will lower
    the risk of making the wrong decision. But there is a drawback to this: the higher
    k is, the longer it will take KNN to make a prediction. This is because it will
    have to perform more calculations to get the distance between all the neighbors
    of an observation. Due to this, you have to find the sweet spot that will give
    correct predictions without compromising too much on the time it takes to make
    a prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance Metrics With K-Nearest Neighbors Classifier in Scikit-Learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many distance metrics could work with the k-nearest neighbors algorithm. We
    will present the two most frequently used ones: the Euclidean distance and the
    Manhattan distance of two data points.'
  prefs: []
  type: TYPE_NORMAL
- en: The Euclidean Distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The distance between two points, `A` and `B`, with the coordinates `A=(a1,
    a2, …, an)` and `B=(b1, b2, …, bn)`, respectively, is the length of the line connecting
    these two points. For example, if A and B are two-dimensional data points, the
    Euclidean distance, `d`, will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: Visual representation of the Euclidean distance between A and
    B'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.7: Visual representation of the Euclidean distance between A and B'
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula to calculate the Euclidean distance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: Distance between points A and B'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.8: Distance between points A and B'
  prefs: []
  type: TYPE_NORMAL
- en: As we will be using the Euclidean distance in this book, let's see how we can
    use scikit-learn to calculate the distance of multiple points.
  prefs: []
  type: TYPE_NORMAL
- en: We have to import `euclidean_distances` from `sklearn.metrics.pairwise`. This
    function accepts two sets of points and returns a matrix that contains the pairwise
    distance of each point from the first and second sets of points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the example of an observation, Z, with coordinates (`4, 4`). Here,
    we want to calculate the Euclidean distance with 3 others points, A, B, and C,
    with the coordinates (`2, 3`), (`3, 7`), and (`1, 6`), respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Here, the distance of Z=(`4,4`) and B=(`3,7`) is approximately `3.162`, which
    is what we got in the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also calculate the Euclidean distances between points in the same set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The diagonal that contains value `0` corresponds to the Euclidean distance between
    each data point and itself. This matrix is symmetric from this diagonal as it
    calculates the distance of two points and its reverse. For example, the value
    `4.12310563` on the first row is the distance between A and B, while the same
    value on the second row corresponds to the distance between B and A.
  prefs: []
  type: TYPE_NORMAL
- en: The Manhattan/Hamming Distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The formula of the Manhattan (or Hamming) distance is very similar to the Euclidean
    distance, but rather than using the square root, it relies on calculating the
    absolute value of the difference of the coordinates of the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9: The Manhattan and Hamming distance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_03_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.9: The Manhattan and Hamming distance'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can think of the Manhattan distance as if we''re using a grid to calculate
    the distance rather than using a straight line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10: Visual representation of the Manhattan distance between A and
    B'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_03_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.10: Visual representation of the Manhattan distance between A and
    B'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding plot, the Manhattan distance will follow the path
    defined by the grid to point B from A.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting property is that there can be multiple shortest paths between
    A and B, but their Manhattan distances will all be equal to each other. In the
    preceding example, if each cell of the grid equals a unit of 1, then all three
    of the shortest paths highlighted will have a Manhattan distance of 9.
  prefs: []
  type: TYPE_NORMAL
- en: The Euclidean distance is a more accurate generalization of distance, while
    the Manhattan distance is slightly easier to calculate as you only need to find
    the difference between the absolute value rather than calculating the difference
    between squares and then taking the root.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.03: Illustrating the K-Nearest Neighbors Classifier Algorithm in
    Matplotlib'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we have a list of employee data. Our features are the number of hours
    worked per week and the yearly salary. Our label indicates whether an employee
    has stayed with our company for more than 2 years. The length of stay is represented
    by zero if it is less than 2 years and one if it is greater than or equal to 2
    years.
  prefs: []
  type: TYPE_NORMAL
- en: We want to create a three-nearest neighbors classifier that determines whether
    an employee will stay with our company for at least 2 years.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we would like to use this classifier to predict whether an employee with
    a request to work 32 hours a week and earning 52,000 dollars per year is going
    to stay with the company for 2 years or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned dataset is available on GitHub at [https://packt.live/2V5VaV9](https://packt.live/2V5VaV9).
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `pandas` package as `pd`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new variable called `file_url()`, which will contain the URL to the
    raw dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data using the `pd.read_csv()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the rows of the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11: DataFrame of the employees dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_03_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.11: DataFrame of the employees dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import `preprocessing` from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `MinMaxScaler` with `feature_range=(0,1)` and save it to a variable
    called `scaler`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Scale the DataFrame using `.fit_transform()`, save the results in a new variable
    called `scaled_employees`, and print its content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code snippet, we have scaled our original dataset so that all
    the values range between 0 and 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From the scaled data, extract each of the three columns and save them into
    three variables called `hours_worked`, `salary`, and `over_two_years`, as shown
    in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `matplotlib.pyplot` package as `plt`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create two scatter plots with `plt.scatter` using `hours_worked` as the *x*-axis
    and `salary` as the *y*-axis, and then create different markers according to the
    value of `over_two_years`. You can add the labels for the *x* and *y* axes with
    `plt.xlabel` and `plt.ylabel`. Display the scatter plots with `plt.show()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12: Scatter plot of the scaled data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_03_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's say we got a new observation and we want to calculate the Euclidean
    distance with the data from the scaled dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a new variable called `observation` with the coordinates `[0.5, 0.26]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `euclidean_distances` function from `sklearn.metrics.pairwise`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new variable called `features`, which will extract the first two columns
    of the scaled dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the Euclidean distance between `observation` and `features` using
    `euclidean_distances`, save it into a variable called `dist`, and print its value,
    as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3djY1jO](https://packt.live/3djY1jO).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3esx7HF](https://packt.live/3esx7HF).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From the preceding output, we can see that the three nearest neighbors are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0.1564897` for point `[0.6, 0.37037037, 1.]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0.17114358` for point `[0.6, 0.11111111, 0.]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0.32150303` for point `[0.6, 0.55555556, 1.]`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we choose `k=3`, KNN will look at the classes for these three nearest neighbors
    and since two of them have a label of `1`, it will assign this class to our new
    observation, `[0.5, 0.26]`. This means that our three-nearest neighbors classifier
    will classify this new employee as being more likely to stay for at least 2 years.
  prefs: []
  type: TYPE_NORMAL
- en: By completing this exercise, we saw how a KNN classifier will classify a new
    observation by finding its three closest neighbors using the Euclidean distance
    and then assign the most frequent class to it.
  prefs: []
  type: TYPE_NORMAL
- en: Parameterization of the K-Nearest Neighbors Classifier in scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The parameterization of the classifier is where you fine-tune the accuracy
    of your classifier. Since we haven''t learned all of the possible variations of
    k-nearest neighbors, we will concentrate on the parameters that you will understand
    based on this topic:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can access the documentation of the k-nearest neighbors classifier here:
    [http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_neighbors`: This is the k value of the k-nearest neighbors algorithm. The
    default value is `5`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric`: When creating the classifier, you will see a name – `Minkowski`.
    Don''t worry about this name – you have learned about the first- and second-order
    Minkowski metrics already. This metric has a `power` parameter. For `p=1`, the
    Minkowski metric is the same as the Manhattan metric. For `p=2`, the Minkowski
    metric is the same as the Euclidean metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p`: This is the power of the Minkowski metric. The default value is `2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You have to specify these parameters once you create the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you will have to fit the KNN classifier with your training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The `predict()` method can be used to predict the label for any new data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: In the next exercise, we will be using the KNN implementation from scikit-learn
    to automatically find the nearest neighbors and assign corresponding classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.04: K-Nearest Neighbors Classification in scikit-learn'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will use scikit-learn to automatically train a KNN classifier
    on the German credit approval dataset and try out different values for the `n_neighbors`
    and `p` hyperparameters to get the optimal output values. We will need to scale
    the data before fitting KNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This exercise is a follow up from *Exercise 3.02*, *Applying Label Encoding
    to Transform Categorical Variables into Numerical*. We already saved the resulting
    dataset from *Exercise 3.02*, *Applying Label Encoding to Transform Categorical
    Variables into Numerical* in the GitHub repository at [https://packt.live/2Yqdb2Q](https://packt.live/2Yqdb2Q).
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `pandas` package as `pd`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new variable called `file_url`, which will contain the URL to the
    raw dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data using the `pd.read_csv()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `preprocessing` from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `MinMaxScaler` with `feature_range=(0,1)` and save it to a variable
    called `scaler`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the scaler and apply the corresponding transformation to the DataFrame
    using `.fit_transform()` and save the results to a variable called `scaled_credit`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the `response` variable (the first column) to a new variable called
    `label`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the features (all the columns except for the first one) to a new variable
    called `features`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `model_selection.train_test_split` from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the scaled dataset into training and testing sets with `test_size=0.2`
    and `random_state=7` using `train_test_split`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `neighbors` from `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `KNeighborsClassifier` and save it to a variable called `classifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the k-nearest neighbors classifier on the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since we have not mentioned the value of k, the default is `5`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the accuracy score for the training set with `.score()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With this, we''ve achieved an accuracy score of `0.78625` on the training set
    with the default hyperparameter values: *k=5* and the Euclidean distance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's have a look at the score for the testing set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the accuracy score for the testing set with `.score()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The accuracy score dropped to `0.75` on the testing set. This means our model
    is overfitting and doesn't generalize well to unseen data. In the next activity,
    we will try different hyperparameter values and see if we can improve this.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2ATeluO](https://packt.live/2ATeluO).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2VbDTKx](https://packt.live/2VbDTKx).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we learned how to split a dataset into training and testing
    sets and fit a KNN algorithm. Our final model can accurately predict whether an
    individual is more likely to default or not 75% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.01: Increasing the Accuracy of Credit Scoring'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, you will be implementing the parameterization of the k-nearest
    neighbors classifier and observing the end result. The accuracy of credit scoring
    is currently 75%. You need to find a way to increase it by a few percentage points.
  prefs: []
  type: TYPE_NORMAL
- en: You can try different values for k (`5`, `10`, `15`, `25`, and `50`) with the
    Euclidean and Manhattan distances.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This activity requires you to complete *Exercise 3.04*, *K-Nearest Neighbors
    Classification in scikit-learn* first as we will be using the previously prepared
    data here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `neighbors` from `sklearn`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a function to instantiate `KNeighborsClassifier` with hyperparameters
    specified, fit it with the training data, and return the accuracy score for the
    training and testing sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the function you created, assess the accuracy score for k = (`5`, `10`,
    `15`, `25`, `50`) for both the Euclidean and Manhattan distances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the best combination of hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 343.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we will introduce you to another machine learning classifier:
    a **Support Vector Machine** (**SVM**).'
  prefs: []
  type: TYPE_NORMAL
- en: Classification with Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We first used SVMs for regression in *Chapter 2*, *An Introduction to Regression*.
    In this topic, you will find out how to use SVMs for classification. As always,
    we will use scikit-learn to run our examples in practice.
  prefs: []
  type: TYPE_NORMAL
- en: What Are Support Vector Machine Classifiers?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of an SVM is to find a surface in an n-dimensional space that separates
    the data points in that space into multiple classes.
  prefs: []
  type: TYPE_NORMAL
- en: In two dimensions, this surface is often a straight line. However, in three
    dimensions, the SVM often finds a plane. These surfaces are optimal in the sense
    that they are based on the information available to the machine so that it can
    optimize the separation of the n-dimensional spaces.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal separator found by the SVM is called the best separating hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: An SVM is used to find one surface that separates two sets of data points. In
    other words, SVMs are **binary classifiers**. This does not mean that SVMs can
    only be used for binary classification. Although we were only talking about one
    plane, SVMs can be used to partition a space into any number of classes by generalizing
    the task itself.
  prefs: []
  type: TYPE_NORMAL
- en: The separator surface is optimal in the sense that it maximizes the distance
    of each data point from the separator surface.
  prefs: []
  type: TYPE_NORMAL
- en: A vector is a mathematical structure defined on an n-dimensional space that
    has a magnitude (length) and a direction. In two dimensions, you draw the vector
    (*x, y*) from the origin to the point (x, y). Based on geometry, you can calculate
    the length of the vector using the Pythagorean theorem and the direction of the
    vector by calculating the angle between the horizontal axis and the vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in two dimensions, the vector (3, -4) has the following magnitude:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'It has the following direction (in degrees):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Understanding Support Vector Machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose that two sets of points with two different classes, 0 and 1, are given.
    For simplicity, we can imagine a two-dimensional plane with two features: one
    mapped on the horizontal axis and one mapped on the vertical axis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective of the SVM is to find the best separating line that separates
    points `A`, `D`, `C`, `B`, and `H`, which all belong to class 0, from points `E`,
    `F`, and `G`, which are of class 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13: Line separating red and blue members'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.13: Line separating red and blue members'
  prefs: []
  type: TYPE_NORMAL
- en: 'But separation is not always that obvious. For instance, if there is a new
    point of class 0 in-between `E`, `F`, and `G`, there is no line that could separate
    all the points without causing errors. If the points from class 0 form a full
    circle around the class 1 points, there is no straight line that could separate
    the two sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14: Graph with two outlier points'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_03_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.14: Graph with two outlier points'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the preceding graph, we tolerate two outlier points, `O` and
    `P`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following solution, we do not tolerate outliers, and instead of a line,
    we create the best separating path consisting of two half-lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15: Graph removing the separation of the two outliers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_03_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.15: Graph removing the separation of the two outliers'
  prefs: []
  type: TYPE_NORMAL
- en: The perfect separation of all data points is rarely worth the resources. Therefore,
    the SVM can be regularized to simplify and restrict the definition of the best
    separating shape and allow outliers.
  prefs: []
  type: TYPE_NORMAL
- en: The regularization parameter of an SVM determines the rate of errors to allow
    or forbid misclassifications.
  prefs: []
  type: TYPE_NORMAL
- en: An SVM has a kernel parameter. A linear kernel strictly uses a linear equation
    to describe the best separating hyperplane. A polynomial kernel uses a polynomial,
    while an exponential kernel uses an exponential expression to describe the hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: A margin is an area centered around the separator and is bounded by the points
    closest to the separator. A balanced margin has points from each class that are
    equidistant from the line.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to defining the allowed error rate of the best separating hyperplane,
    a gamma parameter decides whether only the points near the separator count in
    determining the position of the separator, or whether the points farthest from
    the line count, too. The higher the gamma, the lower the number of points that
    influence the location of the separator.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines in scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our entry point is the end result of *Activity 3.02*, *Support Vector Machine
    Optimization in scikit-learn*. Once we have split the training and test data,
    we are ready to set up the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of using the k-nearest neighbors classifier, we will use the `svm.SVC()` classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: It seems that the default SVM classifier of scikit-learn does a slightly better
    job than the k-nearest neighbors classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Parameters of the scikit-learn SVM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the parameters of the scikit-learn SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kernel`: This is a string or callable parameter specifying the kernel that''s
    being used in the algorithm. The predefined kernels are `linear`, `poly`, `rbf`,
    `sigmoid`, and `precomputed`. The default value is `rbf`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`degree`: When using a polynomial, you can specify the degree of the polynomial.
    The default value is `3`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma`: This is the kernel coefficient for `rbf`, `poly`, and `sigmoid`. The
    default value is `auto`, which is computed as *1/number_of_features*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`C`: This is a floating-point number with a default of `1.0` that describes
    the penalty parameter of the error term.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can read about the parameters in the reference documentation at [http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here is an example of an SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Activity 3.02: Support Vector Machine Optimization in scikit-learn'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, you will be using, comparing, and contrasting the different
    SVMs' classifier parameters. With this, you will find a set of parameters resulting
    in the highest classification data on the training and testing data that we loaded
    and prepared in *Activity 3.01,* *Increasing the Accuracy of Credit Scoring*.
  prefs: []
  type: TYPE_NORMAL
- en: 'You must different combinations of hyperparameters for SVM:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kernel="linear"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel="poly", C=1, degree=4, gamma=0.05`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel="poly", C=1, degree=4, gamma=0.05`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel="poly", C=1, degree=4, gamma=0.25`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel="poly", C=1, degree=4, gamma=0.5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel="poly", C=1, degree=4, gamma=0.16`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel="sigmoid"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel="rbf", gamma=0.15`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel="rbf", gamma=0.25`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel="rbf", gamma=0.5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel="rbf", gamma=0.35`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file and execute all the steps mentioned in the
    previous, *Exercise 3.04*, *K-Nearest Neighbor Classification in scikit-learn*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import `svm` from `sklearn`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a function to instantiate an SVC with the hyperparameters specified,
    fit with the training data, and return the accuracy score for the training and
    testing sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the function you created, assess the accuracy scores for the different
    hyperparameter combinations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the best combination of hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 347.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the basics of classification and the difference
    between regression problems. Classification is about predicting a response variable
    with limited possible values. As for any data science project, data scientists
    need to prepare the data before training a model. In this chapter, we learned
    how to standardize numerical values and replace missing values. Then, you were
    introduced to the famous k-nearest neighbors algorithm and discovered how it uses
    distance metrics to find the closest neighbors to a data point and then assigns
    the most frequent class among them. We also learned how to apply an SVM to a classification
    problem and tune some of its hyperparameters to improve the performance of the
    model and reduce overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will walk you through a different type of algorithm,
    called decision trees.
  prefs: []
  type: TYPE_NORMAL
