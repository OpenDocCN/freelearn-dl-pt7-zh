<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Markov Decision Processes in Action</h1>
                </header>
            
            <article>
                
<p>Stochastic processes involve systems that evolve over time (but also more generally in space) according to probabilistic laws. Such systems or models describe the complex phenomena of the real world that have the possibility of being random. These phenomena are more frequent than we believe them to be. We encounter these phenomena when the quantities we are interested in aren't predictable with absolute certainty. However, when such phenomena show a variety of possible outcomes that can be somehow explained or described, then we can introduce a probabilistic model of the phenomenon.</p>
<p>A Markov chain is a stochastic process whereby the evolution of a system depends only on its present state and not on its past state. A Markov chain is characterized by a set of states and by the probability of a transition occurring between states. Think of a point that can move randomly forward or backward along a line at discrete intervals of time, covering a certain distance at each interval.</p>
<p><span>In this chapter, we will get to grips with the concepts of the Markov process. </span><span>Stochastic Markov processes </span><span>will be analyzed in detail. We will be introduced to the Markov chain and then we will learn how to use these algorithms to make weather forecasts. Finally, we will learn how to evaluate the optimal policy for the solution of a Markov reward problem.</span></p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li class="mce-root"><span>An overview of the Markov process</span></li>
<li class="mce-root"><span>Introduction to Markov chains</span></li>
<li class="mce-root">Markov chains applications <span>–</span> weather forecasting</li>
<li class="mce-root">Markov reward model</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/36xyVLj">http://bit.ly/36xyVLj</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview of the Markov process</h1>
                </header>
            
            <article>
                
<p>As we mentioned in <a href="aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml">Chapter 2</a>, <em>Building Blocks of Reinforcement Learning,</em> a stochastic process is called <strong>Markovian</strong> when a certain instant <em>t</em> of observation is chosen. The evolution of the process starting with <em>t</em> depends only on <em>t</em>, while it does not depend on the previous instants in any way. Thus, a process is Markovian when, given the moment of observation, only that instant determines the future evolution of the process, while this evolution does not depend on the past. In the next section, we will explore the concept of stochastic processes and we will see how it is related to probability theory.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the stochastic process</h1>
                </header>
            
            <article>
                
<p>In order to provide a formal definition of a Markov process, it is necessary to specify what is meant by a set of random variables having a temporal ordering. Such a set of random variables can best be represented by a stochastic process.</p>
<p>The theory of stochastic processes concerns the study of systems that evolve over time (but also more generally in space) according to probabilistic laws. Such systems or models describe complex phenomena of the real world that have the possibility of being random. These phenomena are more frequent than we believe them to be, and we face these situations when the quantities we are interested in can't be predicted with certainty. We define a stochastic process in discrete time and discrete states in a sequence that contains the following random variables:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1a976054-188b-468d-b04a-5c477c331844.png" style="width:13.92em;height:1.25em;"/></p>
<p>In the preceding sequence, each <em>X<sub>n</sub></em> is a discrete random variable with values in a set <em>S = s<sub>1</sub>, s<sub>2</sub>,…, s<sub>n</sub>,</em> called the <strong>space of the states</strong>. Without losing generality, suppose that <em>S</em> is a subset of the relative integers <em>Z</em>. Each value of <em>X<sub>n</sub></em> as the index <em>n</em> changes will represent the state of the system over time. This process we will analyze starts in any of the states represented by <em>X<sub>n</sub></em> and will move to the next state <em>X<sub>n + 1</sub></em>. Each transition is called a <strong>step</strong>.</p>
<p>As time passes, the process can jump from one state to another. If a step <em>n</em> is in a state <em>i</em>, and at the next step, <em>n + 1</em> is in a state <em>j ≠ i</em>, we can say that there has been a transition.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating the probability</h1>
                </header>
            
            <article>
                
<p>Given a stochastic process (<em>X<sub>n</sub></em>), we are interested in calculating the probabilities associated with it. Now, let's explore the basic concepts of probability. If you already know about these concepts, you can skip this section. Either way, this section will allow you to explore the basics of probability theory.</p>
<p>The <strong>probability</strong> (<strong>a priori</strong>) that a given event (<em>E</em>) occurs is the ratio between the number (<em>s</em>) of favorable cases of the event itself and the total number (<em>n</em>) of the possible cases, provided all the considered cases are equally probable:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5ec2e412-6aaa-429d-9d86-edb35b873d4d.png" style="width:18.17em;height:2.00em;"/></p>
<p>Let's look at a simple example:</p>
<ul>
<li>While throwing a dice, what is the probability that 3 shows up? The number of possible cases is 6, {1, 2, 3, 4, 5, 6}, while the number of favorable cases is 1, that is, {3}. So, P(3) =1/6 =0.166 =16.6 %.</li>
</ul>
<p>The probability of an event <em>P(E)</em> always being number between 0 and 1 can be formulated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c2978ca2-9cda-4ab2-a605-3b5bf4ac782b.png" style="width:6.33em;height:1.25em;"/></p>
<p>The extreme values are defined as follows:</p>
<ul>
<li>An event that has a probability of 0 is called an impossible event. Suppose we have six red balls in a bag; what is the probability of picking a black ball? The number of possible cases is 6; the number of favorable cases is 0 because there are no black balls in the bag. Hence, <em>P(E) = 0/6 = 0</em>.</li>
<li>An event that has a probability of 1 is called a certain event. Suppose we have six red balls in a bag; what is the probability of picking a red ball? The number of possible cases is 6; the number of favorable cases is 6 because there are only red balls in the bag. Therefore, <em>P(E) = 6/6 = 1</em>.</li>
</ul>
<p>So far, we've talked about the likelihood of an event, but what happens when there's more than one possible event? Two random events, A and B, are independent if the probability of the occurrence of event A is not dependent on whether event B has occurred, and vice versa.</p>
<p>For example, let's say we have two 52 decks of French playing cards. When extracting a card from each deck, the following two events are independent:</p>
<ul>
<li><strong>E1</strong>: The card that's extracted from the first deck is an ace.</li>
<li><strong>E2</strong>: The card that's extracted from the second deck is a clubs card.</li>
</ul>
<p>Each can happen with the same probability, independent of the other's occurrence.</p>
<p>Conversely, a random event, A, is dependent on another event, B, if the probability of event A depends on whether event B has occurred or not. Suppose we have a deck of 52 cards; by extracting two cards in succession without putting the first card back in the deck, the following two events are dependent:</p>
<ul>
<li><strong>E1</strong>: The first extracted card is an ace.</li>
<li><strong>E2</strong>: The second extracted card is an ace.</li>
</ul>
<p>To be precise, the probability of E2 depends on whether or not E1 occurs, as follows:</p>
<ul>
<li>The probability of E1 is 4/52.</li>
<li>The probability of E2 if the first card was an ace is 3/51.</li>
<li>The probability of E2 if the first card was not an ace is 4/51.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding joint probability</h1>
                </header>
            
            <article>
                
<p>Now, let's deal with the case of <strong>joint probability</strong>, both independent and dependent. Given two events, A and B, if the two events are independent (I mean the occurrence of one doesn't affect the probability of the other), the joint probability of the event is equal to the product of the probabilities of A and B:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/35371c3c-59bf-46c3-88b9-39e0a1ffff97.png" style="width:9.50em;height:1.00em;"/></p>
<p>Let's look at an example. We have two decks of 52 cards. By extracting a card from each deck, let's consider the two independent events:</p>
<ul>
<li>A: The card that's extracted from the first deck is an ace.</li>
<li>B: The card that's extracted from the second deck is a clubs card.</li>
</ul>
<p>What is the probability that both of them occur?</p>
<ul>
<li>P(A) = 4/52</li>
<li>P(B) = 13/52</li>
<li>P(A ∩ B) = 4/52 * 13/52 = 52 /(52 * 52) = 1/52</li>
</ul>
<p class="mce-root"/>
<p>If the two events are dependent (that is, the occurrence of one affects the probability of the other), then the same rule may apply, provided that P(B|A) is the probability of event B given that event A has occurred. This condition introduces conditional probability, which we are going to dive into:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6885c500-059a-47c2-8a2b-f6124b7053a5.png" style="width:13.08em;height:1.25em;"/></p>
<p>A bag contains two white balls and three red balls. Two balls are pulled out from the bag in two successive extractions without reintroducing the first ball that was pulled out of the bag.</p>
<p>Let's calculate the probability that the two balls that were extracted were both white:</p>
<ul>
<li>The probability that the first ball is white is 2/5.</li>
<li>The probability that the second ball is white, provided that the first ball is white, is 1/4.</li>
</ul>
<p>The probability of having two white balls is as follows:</p>
<ul>
<li>P(two whites) = 2/5 * 1/4 = 2/20 = 1/10</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding conditional probability</h1>
                </header>
            
            <article>
                
<p>Now, it's time to introduce you to the concept of conditional probability. The probability that event B occurs, calculated by the condition that event A occurred, is called conditional probability and is indicated by the symbol P(B | A). It is calculated using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3e116f0e-c9dc-431e-98bf-a68615d412aa.png" style="width:7.67em;height:2.17em;"/></p>
<p>Now that we are able to understand the different kinds of probabilities, let's apply them to the stochastic processes. Let's start with the simplest type of probability, which is written in the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/799e0042-af2a-4233-8709-e63c34c3410c.png" style="width:5.08em;height:1.33em;"/></p>
<p>This represents the probability of observing the system in the state i <span>at step n</span>. In addition to these simple probabilities, we should be interested in the calculation of more complex probabilities involving multiple steps at the same time.</p>
<p class="mce-root"/>
<p>For example, it may be interesting to calculate the probability of being in state j at step n + 1, knowing that it is in state i at step n (as we can see, this is the conditional probability we defined previously):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d1d2a758-1e1a-48a3-8413-3dcd5c52aa4d.png" style="width:9.33em;height:1.25em;"/></p>
<p>This is called the transition probability from i to j at step n. Using the conditional probability definition, this rewrites itself, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/40ae8605-8803-478d-b437-aad895b0a122.png" style="width:20.92em;height:2.67em;"/></p>
<p>Therefore, for this calculation, it is sufficient to know the a priori probability and the joint probability. To calculate more complex expressions, it is necessary to know the generic joint probabilities given by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/82c1f1ed-7411-4545-823c-5ab13248bb9b.png" style="width:12.58em;height:1.33em;"/></p>
<p>All of this occurs due to variations of all the i<sub>0</sub>,...,i<sub>n</sub> in the set of integers Z. In a certain sense, these probabilities exhaust all possible information: the stochastic process is statistically determined when all the combined (discrete) densities are known, that is, the densities of all the multiple discrete variables (X<sub>1</sub>, ..., X<sub>n</sub>) to the variation of all the i<sub>0</sub>,...,i<sub>n</sub> Z. The calculation of these joint probabilities is a very difficult problem in general.</p>
<p>In the next section, we will delve deeper into the concepts behind the Markov process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to Markov chains</h1>
                </header>
            
            <article>
                
<p>A Markov chain is a mathematical model of a random phenomenon that evolves over time in such a way that the past influences the future only through the present. The time can be discrete (whole variable), continuous (real variable), or a totally ordered whole. In this section, we will only consider discrete chains. Markov chains were introduced in 1906 by Andrei Andreyevich Markov (1856–1922), which is where the name is derived.</p>
<p>A Markov chain is a stochastic model that represents a sequence of possible cases in which the probability that each case occurs depends only on the state relative to the previous case. So, Markov chains have the <strong>memorylessness</strong> property.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's consider a random process described by the sequence of random variables X = X<sub>0</sub>, ..., X<sub>n</sub> which can assume values in a set, that is, j<sub>0</sub>, j<sub>1</sub>,…, jn. Let's say that the process we are analyzing has the property of Markov if the evolution of the process in the future depends only on the value of the present state and not on the past history. In formulas, using the conditional probability, we will have the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/90728e8f-b785-4c4e-8df7-d5d489b3b59e.png" style="width:30.33em;height:1.33em;"/></p>
<p>This relationship must apply to all the parameters if they are well-defined conditional probabilities. A discrete-time stochastic process X that has the Markov property is said to be a Markov chain. A Markov chain is said to be homogeneous if the transition probabilities are as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fc6a5e1b-a2bf-4cc4-b07a-fb4439356daa.png" style="width:9.33em;height:1.25em;"/></p>
<p>This does not depend on n, but only on i and j. When this happens, we get the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8808b23b-c001-44e5-a120-6d720fcbf9ee.png" style="width:11.92em;height:1.25em;"/></p>
<p>We can calculate all the joint probabilities by knowing the numbers p<sub>ij</sub> along with the following initial distribution:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/71bf319c-a4bb-4ed5-b61a-42d21032f74c.png" style="width:7.50em;height:1.42em;"/></p>
<p>This probability is called the <strong>distribution of the process over time zero</strong>. The p<sub>ij</sub> probabilities are called transition probabilities, while p<sub>ij</sub> is the probability of a transition occurring from i to j in a time step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the transition matrix</h1>
                </header>
            
            <article>
                
<p>The study of homogeneous Markov chains becomes particularly simple and effective using matrix representation. In particular, the formula expressed by the previous proposition becomes much more readable. Due to this, the structure of a Markov chain can be completely represented by the following transition matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/954dc61a-be80-46e1-a782-232c21fb7278.png" style="width:11.17em;height:4.92em;"/></p>
<p class="mce-root"/>
<p>The properties of the transition probability matrices derive directly from the nature of the elements that compose them. In fact, by observing that the elements of the matrix are probabilities, they must have a value between 0 and 1. So, this is a positive matrix in which the sum of the elements of each row is unitary. In fact, the elements of the i-th row are the probabilities that the chain, being in the state Si at the instant t, transits in S1 or in S2,... or in Sn at the next step. Such transitions are mutually exclusive and exhaustive of all possibilities. Such a matrix (positive with unit sum rows) is called <strong>stochastic</strong>. Therefore, we will need to define each positive row vector as stochastic, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/41f49b00-7d14-4652-96e4-f1d28d06375d.png" style="width:13.25em;height:1.67em;"/></p>
<p>In this vector, the sum of the elements takes a unit value, as shown in the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e2a878d0-f75e-4bac-a8e2-0637ae862b12.png" style="width:4.75em;height:3.17em;"/></p>
<p>Now, we will see that a particular form assumes this matrix in the case of a one-dimensional random walk. As shown in the following diagram, in a one-dimensional random walk, we study the motion of a point-like particle that's constrained to move along a straight line in the two allowed directions (right and left). At each movement, it moves (randomly) one step to the right with a fixed probability p or to the left with a probability <span>q, in such a way that</span> p+q=1. Each step is of equal length and independent of the others:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-471 image-border" src="assets/05a5cbc3-baa1-42bf-ba6a-e233ae3cec81.png" style="width:28.00em;height:5.50em;"/></p>
<p>Suppose that the random variables Z<sub>n</sub> with n = 1,2, .. are independent and all have the same distribution. Due to this, the position of the particle instant n is given by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/abc6350b-0da2-4de5-b433-6dae2265a17d.png" style="width:18.08em;height:1.17em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Here, X<sub>0</sub> = 0. The state space is S = (0, ±1, ±2,…). The X<sub>n</sub> process is a Markov chain because, to determine the probability that the particle is in a certain position <span>the next moment</span>, we just need to know where it is at the current moment, even if we are aware of where it was in all the moments before the current one. This concept can be expressed through the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3e9936e0-7708-4d00-b3d9-c394ce87ce4a.png" style="width:23.50em;height:4.33em;"/></p>
<p>Here, the Zn variables are independent. The transition matrix is a matrix with finite rows and as many columns, where 0 is on the main diagonal, p is on the diagonal above the main, q is on the diagonal below the main, and 0 is everywhere else, as shown in the following formula:</p>
<p>                                                      <img class="size-full wp-image-626 image-border" src="assets/e0dd7625-cb5b-4d05-8567-10c17c0a0a5d.png" style="width:13.50em;height:7.50em;"/></p>
<p>Here, we can see that this generalization greatly simplifies the problem at hand.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the transition diagram</h1>
                </header>
            
            <article>
                
<p>A very intuitive alternative to the description of a Markov chain through the transition matrix is that of associating a Markov chain with an oriented graph (transition diagram). Here, the following occurs:</p>
<ul>
<li>Vertices are labeled by the S1,S2,…, Sn states (or, briefly, from the indices 1, 2, …, n of the states).</li>
<li>There is a directed edge that connects the vertex Si to the vertex Sj if, and only if, the probability of transition from Si to Sj is positive (this is the probability, which is, in turn, used as a label of the edge itself).</li>
</ul>
<p class="mce-root"/>
<p>It is clear that the transition matrix and transition diagram provide the same information about the same Markov chain. To understand this duality, we need to look at a simple example <span>– </span>consider a Markov chain with three possible states, that is, 1, 2, and 3, and the following transition matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b5e49b40-837c-41ca-8d4d-2f85fdcdc1d6.png" style="width:9.83em;height:3.92em;"/></p>
<p>The transition diagram for the newly introduced Markov chain can be seen in the following diagram. We can identify three possible states: 1, 2, and 3. The two-state border contains the transition probabilities p<sub>ij</sub>. When there is no border between the two states, this means that the probability of transition is zero:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-472 image-border" src="assets/ae10ccc2-02e9-40b9-8c06-f71fd00d925e.png" style="width:30.50em;height:18.75em;"/></p>
<p>In the preceding diagram, we can see that the arrows that come out of a state always sum up exactly at 1, just like what happens for every row in the transition matrix whose values must be added exactly to 1 <span>–</span> which represents the probability distribution. By comparing the transition matrix and the transition diagram, we can understand the duality between the two resources. As always, a diagram is much more explanatory.</p>
<p>In the next section, we will put what we've learned into practice by addressing a prediction problem with Markov chains. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Markov chain application – weather forecasting</h1>
                </header>
            
            <article>
                
<p>To apply what we've learned so far, we will look at a weather forecasting model based on Markov chains. To simplify this model, we will assume that there are only three states <span>– </span>rainy, cloudy, and sunny. Let's also assume that we have made some calculations and discovered that tomorrow's time is somehow based on today's time, according to the following transition matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0ecf6447-7b56-4345-81c2-92b7d03ba387.png" style="width:10.58em;height:3.92em;"/></p>
<p>Each row must contain non-negative numbers and the sum of them must be equal to 1. Recall that this matrix contains the conditional probabilities of the type expressed as <em>P (A | B)</em><em>,</em>, that is, the probability of <em>A</em> given <em>B</em>. So, this matrix contains the following conditional probabilities:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ad894ba3-27ff-43c8-8c3c-f0016f338828.png" style="width:19.50em;height:4.25em;"/></p>
<p>Here, we have the following properties:</p>
<ul>
<li><em>Ra</em>: Rainy</li>
<li><em>Cl</em>: Cloudy</li>
<li><em>Su</em>: Sunny</li>
</ul>
<p>The weather conditions between two days are not necessarily correlated, so the process is Markovian.</p>
<p>At this point, the following questions come to mind:</p>
<ul>
<li>If today is sunny, how can we calculate the probability that it is rainy in the next few days?</li>
<li>After a certain number of days, what will be the proportion of sunny and rainy days?</li>
</ul>
<p>Both questions, as well as many others that may come to mind, can be answered through the tools that make Markov chains available to us.</p>
<p class="mce-root"/>
<p>The following is the R code that allows us to alternate between sunny, cloudy, and rainy days, starting from a specific initial condition:</p>
<pre>library(markovchain)<br/>set.seed(1)<br/>States &lt;- c("Rainy","Cloudy","Sunny")<br/>TransMat &lt;- matrix(c(0.30,0.50,0.20,0.25,0.4,0.35,0.1,0.2,0.70),<br/>                 nrow = 3, byrow = TRUE,dimnames = list(States,States))<br/><br/>MarkovChainModel &lt;- new("markovchain",transitionMatrix=TransMat, states=States,<br/>                    byrow = TRUE, name="MarkovChainModel")<br/>MarkovChainModel<br/><br/>states(MarkovChainModel)<br/>dim(MarkovChainModel)<br/>str(MarkovChainModel)<br/>MarkovChainModel@transitionMatrix<br/><br/>library(diagram)<br/>plot(MarkovChainModel,package="diagram")<br/><br/>transitionProbability(MarkovChainModel, "Sunny", "Rainy")<br/><br/>StartState&lt;-c(0,0,1)<br/>After3Days &lt;- StartState * (MarkovChainModel ^ 3)<br/>print (round(After3Days, 3))<br/>After1Week &lt;- StartState * (MarkovChainModel ^ 7)<br/>print (round(After1Week, 3))<br/><br/>steadyStates(MarkovChainModel)<br/><br/>YearWeatherState &lt;- rmarkovchain(n = 365, object = MarkovChainModel, t0 = "Sunny")<br/>YearWeatherState[1:40]</pre>
<p><span>Let's analyze this code line by line:</span></p>
<ol start="1">
<li>The first line loads the library:</li>
</ol>
<pre style="padding-left: 60px">library(markovchain)</pre>
<div class="packt_tip">
<p>Keep in mind that if you need to install a library that isn't present in the initial distribution of R, you must use the <kbd>install.packages()</kbd> function. This function should be used just once and not every time the code is run.</p>
</div>
<ol start="2">
<li>For example, to install the <kbd>markovchain</kbd> package, we should write the following:</li>
</ol>
<pre style="padding-left: 60px">install.packages("markovchain")</pre>
<p>This function downloads and installs packages from CRAN-like repositories or from local files. Instead, the load command must be used whenever the script is executed in a new session of R.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importing the markovchain package</h1>
                </header>
            
            <article>
                
<p>The <kbd>markovchain</kbd> package contains functions and S4 methods that we can use to create and manage discrete-time Markov chains. In addition to this, functions that we can use to perform statistical (fitting and drawing random variates) and probabilistic (analysis of their structural proprieties) analysis are provided. A brief description of the <kbd>markovchain</kbd> package, which can be extracted from the official documentation, is shown in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Version</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">0.6.9.14</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>      Date</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">2019-01-20</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>Maintainer</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Giorgio Alfredo Spedicato</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>   License</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">GPL-2</p>
</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign">
<p>   Authors</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Giorgio Alfredo Spedicato, Tae Seung Kang, Sai Bhargav Yalamanchi, Mildenberger Thoralf, Deepak Yadav, Ignacio Cordón, Vandit Jain, Toni Giorgino</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>We will use this package in a variety of chapters in this book to demonstrate the usefulness of the features it provides. Let's get started:</p>
<ol>
<li>Let's continue analyzing the code:</li>
</ol>
<pre style="padding-left: 60px">set.seed(1)</pre>
<p style="padding-left: 60px">The <kbd>set.seed()</kbd> command sets the seed of the R random number generator. This is necessary whenever we want to make the example reproducible. When <kbd>set.seed()</kbd> is used, the random numbers that are used in the algorithm will always be the same, so that a subsequent reproduction of the algorithm will provide the same results. Each seed value will correspond to a sequence of values that are generated for a given random number generator.</p>
<ol start="2">
<li>In the following line, we define the states of the weather condition:</li>
</ol>
<pre style="padding-left: 60px">States &lt;- c("Rainy","Cloudy","Sunny")</pre>
<p style="padding-left: 60px">As shown here, only three states are provided: <kbd>Rainy</kbd>, <kbd>Cloudy</kbd>, and <kbd>Sunny</kbd>. At this point, we have to define the possible transitions of weather conditions.</p>
<ol start="3">
<li>Let's move on and define the transition matrix according to what was established at the beginning of this section:</li>
</ol>
<pre style="padding-left: 60px">TransMat &lt;- matrix(c(0.30,0.50,0.20,0.25,0.4,0.35,0.1,0.2,0.70),nrow = 3, byrow = TRUE,dimnames = list(States,States))</pre>
<p style="padding-left: 60px">Remember that this matrix contains the conditional probabilities of the type expressed as <em>P(A | B)</em>, that is, the probability of <em>A</em> given <em>B</em>. As we mentioned previously, the rows of this matrix add up to 1.</p>
<ol start="4">
<li>Now, we can create the <kbd>markovchain</kbd> object:</li>
</ol>
<pre style="padding-left: 60px">MarkovChainModel &lt;- new("markovchain",transitionMatrix=TransMat, states=States, byrow = TRUE, name="MarkovChainModel")</pre>
<p style="padding-left: 60px">The <kbd>markovchain</kbd> class has been designed to handle homogeneous Markov chain processes. The following slots are passed:</p>
<ul>
<li style="padding-left: 60px"><kbd>transitionMatrix</kbd>: Square transition matrix containing the probabilities of the transition matrix.</li>
<li style="padding-left: 60px"><kbd>states</kbd>: Name of the states. It must be the same as the colnames and rownames of the transition matrix. This is a character vector listing the states for which transition probabilities are defined.</li>
<li style="padding-left: 60px"><kbd>byrow</kbd>: Binary flag. A logical element indicating whether transition probabilities are shown by row or by column.</li>
<li style="padding-left: 60px"><kbd>name</kbd>: Optional character element to name the discrete-time Markov chains.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">To provide a summary of the model we've just created, use the following command:</p>
<pre style="padding-left: 60px">MarkovChainModel</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px"><strong>MarkovChainModel</strong><br/><strong> A  3 - dimensional discrete Markov Chain defined by the following states:</strong><br/><strong> Rainy, Cloudy, Sunny</strong><br/><strong> The transition matrix  (by rows)  is defined as follows:</strong><br/><strong>       Rainy Cloudy Sunny</strong><br/><strong>Rainy   0.30    0.5  0.20</strong><br/><strong>Cloudy  0.25    0.4  0.35</strong><br/><strong>Sunny   0.10    0.2  0.70</strong></pre>
<p>As you can see, the dimensions of the object, the states, and the transition matrix are printed. To obtain this information individually, we can use some methods associated with the <kbd>markovchain</kbd> object:</p>
<ol>
<li>For example, to get the states of the <kbd>markovchain</kbd> object, we can use the <kbd>states</kbd> method, as follows:</li>
</ol>
<pre style="padding-left: 60px">states(MarkovChainModel)</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px">[1] "Rainy" "Cloudy" "Sunny"</pre>
<ol start="2">
<li>To get the dimension of the <kbd>markovchain</kbd> object, we can use the <kbd>dim</kbd> method, as follows:</li>
</ol>
<pre style="padding-left: 60px">dim(MarkovChainModel)</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>[1] 3</strong></pre>
<ol start="3">
<li>To see which elements are contained in the object we have created, we can use the <kbd>str()</kbd> function, which shows a compact view of the internal structure of an R object:</li>
</ol>
<pre style="padding-left: 60px">str(MarkovChainModel)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The following results are printed:</p>
<pre style="padding-left: 60px"><strong>Formal class 'markovchain' [package "markovchain"] with 4 slots</strong><br/><strong>  ..@ states          : chr [1:3] "Rainy" "Cloudy" "Sunny"</strong><br/><strong>  ..@ byrow           : logi TRUE</strong><br/><strong>  ..@ transitionMatrix: num [1:3, 1:3] 0.3 0.25 0.1 0.5 0.4 0.2 0.2 0.35 0.7</strong><br/><strong>  .. ..- attr(*, "dimnames")=List of 2</strong><br/><strong>  .. .. ..$ : chr [1:3] "Rainy" "Cloudy" "Sunny"</strong><br/><strong>  .. .. ..$ : chr [1:3] "Rainy" "Cloudy" "Sunny"</strong><br/><strong>  ..@ name            : chr "MarkovChainModel"</strong></pre>
<p style="padding-left: 60px">As we can see, four slots are listed: <kbd>states</kbd>, <kbd>byrow</kbd>, <kbd>transitionMatrix</kbd>, and <kbd>name</kbd>. To retrieve the elements contained in each one, we can use the name of the object (<kbd>MarkovChainModel</kbd>), followed by the name of the slot, separated by the <kbd>@</kbd> symbol.</p>
<ol start="4">
<li>For example, to print the transition matrix, we will write the following:</li>
</ol>
<pre style="padding-left: 60px">MarkovChainModel@transitionMatrix</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 90px"><strong>       Rainy Cloudy Sunny</strong><br/><strong>Rainy   0.30    0.5  0.20</strong><br/><strong>Cloudy  0.25    0.4  0.35</strong><br/><strong>Sunny   0.10    0.2  0.70</strong></pre>
<p>As we mentioned in the <em>Transition diagram</em> section, a very intuitive alternative to describing a Markov chain through the transition matrix is that of associating a Markov chain with an oriented graph (transition diagram).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Importing the diagram package</h1>
                </header>
            
            <article>
                
<p>To plot a transition diagram, we can use the <kbd>diagram</kbd> package. This package contains several functions for visualizing simple graphs (networks) and plotting flow diagrams.</p>
<p>We can see a short description of the <kbd>diagram</kbd> package, which has been extracted from the official documentation, in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Version</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">1.6.4</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Date</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">2017-08-16</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Maintainer</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Karline Soetaert</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">License</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">GPL-2</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Authors</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Karline Soetaert</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now, let's learn how to use the functions that are available in the package to create a diagram:</p>
<ol>
<li>Let's start by importing the library:</li>
</ol>
<pre style="padding-left: 60px">library(diagram)</pre>
<ol start="2">
<li>Now, we can create the <kbd>markovchain</kbd> object called <kbd>diagram</kbd>:</li>
</ol>
<pre style="padding-left: 60px">plot(MarkovChainModel,package="diagram")</pre>
<p style="padding-left: 60px">The following diagram is printed:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-473 image-border" src="assets/e5690e1e-7f36-4277-8385-7fa14b1e1fd3.png" style="width:33.08em;height:21.58em;"/></p>
<p>In the preceding diagram, we can see that the arrows that come out of the states always sum up exactly to 1, just like what happens for every row in the transition matrix, whose values must add up to exactly 1. This represents the probability distribution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Obtaining transition probability</h1>
                </header>
            
            <article>
                
<p>Something else that we can extract from the newly developed model is the transition probability, which represents the probability of passing from one state to another. Recall that a Markov chain is said to be homogeneous in time if the probabilities of the transition from one state to another are independent of the time index. To obtain this information, we will use the <kbd>transitionProbability()</kbd> function, which allows us to get the transition probabilities from initial to subsequent states. Let's get started:</p>
<ol>
<li>Use the following command to get this information:</li>
</ol>
<pre style="padding-left: 60px">transitionProbability(MarkovChainModel, "Sunny", "Rainy")</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>[1] 0.1</strong></pre>
<p style="padding-left: 60px">We can confirm this result by analyzing the transition matrix and the transition diagram. In the transition matrix, the transition from the <kbd>Sunny</kbd> state to the <kbd>Rainy</kbd> state is given by the element <kbd>p31</kbd>, which is equal to 0.1. In the same way, in the transition diagram, the branch that leaves the <kbd>Sunny</kbd> state to arrive at the <kbd>Rainy</kbd> state has a value of 0.1.</p>
<p style="padding-left: 60px">After correctly setting up our Markov chain-based model, it's time to use it to make predictions. But first, we need to set the initial state. Let's say we're starting from the sunny (<kbd>Sunny</kbd>) condition.</p>
<ol start="2">
<li>Based on the vector containing the three states of our model, this condition is represented by the vector (0,0,1). We can set this value like so:</li>
</ol>
<pre style="padding-left: 60px">StartState&lt;-c(0,0,1)</pre>
<p style="padding-left: 60px">For example, to calculate the state of time in 3 days, we can use a property of Markov chains. If X<sub>n</sub> is a homogeneous Markov chain with transition probability p<sub>ij</sub> and initial distribution p<sup><sub>0</sub></sup>, then the following formula holds:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/85823075-8c9a-49d5-bb41-a5fd1f0d3f9f.png" style="width:11.67em;height:2.92em;"/></p>
<p style="padding-left: 60px">This formula in vector terms becomes as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/db0b4f6a-900f-4cc0-a011-2cd0f71b561b.png" style="width:6.25em;height:1.33em;"/></p>
<p style="padding-left: 60px">In the preceding formula, p<sup><sub>n</sub></sup> and p<sup><sub>0</sub></sup> are row vectors and p<sup><sub>0</sub></sup> x P<sup><sub>n</sub></sup> represents a product between a row vector and a matrix (row by column product).</p>
<ol start="3">
<li>Let's write this product using our model:</li>
</ol>
<pre style="padding-left: 60px">Pred3Days &lt;- StartState * (MarkovChainModel ^ 3)<br/>print (round(Pred3Days, 3))</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>     Rainy Cloudy Sunny</strong><br/><strong>[1,]  0.17  0.299  0.53</strong></pre>
<ol start="4">
<li>In this way, we get a three-day forecast. To get a forecast for one week, we will write the following:</li>
</ol>
<pre style="padding-left: 60px">Pred1Week &lt;- StartState * (MarkovChainModel ^ 7)<br/>print (round(Pred1Week, 3))</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>     Rainy Cloudy Sunny</strong><br/><strong>[1,] 0.184  0.319 0.497</strong></pre>
<p>Something else that we can get from the model we developed is the stationary distribution. The stationary distribution of a Markov chain with transition matrix P is a vector, π, so that π⋅P = π (in other words, π is invariant by the matrix P.). π is a row vector whose entries are probabilities summing to 1. This is a probability distribution that remains constant as the Markov chain evolves over time.</p>
<p>The <kbd>markovchain</kbd> package has a specific function, called <span>the <kbd>steadyStates()</kbd> function, </span>to obtain the stationary distribution of the Markov chain. Let's call it:</p>
<pre>steadyStates(MarkovChainModel)</pre>
<p>The following result is returned:</p>
<pre><strong>         Rainy    Cloudy     Sunny</strong><br/><strong>[1,] 0.1848739 0.3193277 0.4957983</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Finally, let's learn how to generate a forecast of the state of time for a whole year, day after day, starting from a specific state. To do this, we can use the <kbd>rmarkovchain()</kbd> function, which returns a sequence of states from homogeneous or nonhomogeneous Markov chains. Let's do this:</p>
<pre>YearWeatherState &lt;- rmarkovchain(n = 365, object = MarkovChainModel, t0 = "Sunny")</pre>
<p>The following arguments are passed:</p>
<ul>
<li><kbd>n</kbd>: Sample size</li>
<li><kbd>object</kbd>: Either a <kbd>markovchain</kbd> or a <kbd>markovchainList</kbd> object</li>
<li><kbd>t0</kbd>: The initial state</li>
</ul>
<p>At this point, we can extract forecasts for each day of next year. Let's print the forecasts for the next 40 days:</p>
<pre>YearWeatherState[1:40]</pre>
<p>The following results are returned:</p>
<pre><strong>[1] "Sunny"  "Sunny"  "Sunny"  "Rainy"  "Cloudy" "Rainy"  "Sunny"  "Sunny"  "Sunny"  "Sunny"  "Sunny"</strong><br/><strong>[12] "Sunny"  "Sunny"  "Sunny"  "Cloudy" "Sunny"  "Cloudy" "Rainy"  "Cloudy" "Rainy"  "Sunny"  "Sunny"</strong><br/><strong>[23] "Sunny"  "Sunny"  "Sunny"  "Sunny"  "Sunny"  "Sunny"  "Cloudy" "Cloudy" "Sunny"  "Sunny"  "Sunny"</strong><br/><strong>[34] "Sunny"  "Cloudy" "Sunny"  "Cloudy" "Cloudy" "Sunny"  "Sunny"</strong></pre>
<p> The prediction sequence is well-defined, starting from the initial state.</p>
<p>In the next section, we will develop models that use a reward to extend the characteristics of a Markov chain.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Markov reward model</h1>
                </header>
            
            <article>
                
<p>So far, we have dealt with Markov processes, random processes without memory, a sequence of random states that satisfy the Markov property, and much more. This process is defined through the space of the state S, and the transition function P, which determines its dynamics. In these models, there is no value associated with a specific state that allows us to reach a goal.</p>
<p>If we add a reward rate to each state, we get a Markov reward model, which represents a stochastic process that extends the characteristics of a Markov chain or a continuous-time Markov chain. The reward accumulated (R) over time is recorded in an additional variable. These concepts were introduced in <a href="aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml">Chapter 2</a>, <em>Building Blocks of Reinforcement Learning.</em> Now, let's try to apply these concepts to a practical case of forest management. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tiny forest management problem</h1>
                </header>
            
            <article>
                
<p>To understand these newly revised concepts, we will use an example from the <kbd>MDPToolbox</kbd> package (<kbd>mdp_example_forest</kbd>). This example deals with the management problem of a forest stand and has two main objectives:</p>
<ul>
<li>The first objective is to maintain an old forest for wildlife.</li>
<li>The second objective is to earn money by selling the cut wood.</li>
</ul>
<p>To achieve these objectives, two actions are available: Wait or Cut. An action is decided for each 20-year period and applied at the beginning of the period.</p>
<p>Three states are defined according to three tree age classes:</p>
<ul>
<li><strong>state 1</strong>: Age group 0-20 years</li>
<li><strong>state 2</strong>: Age group 21-40 years</li>
<li><strong>state 3</strong>: Age group over 40 years</li>
</ul>
<p>State 3 corresponds to the oldest age class. At the end of a period t, if the state is s and the Wait action is chosen, the state at the next period will be given by the minimum of the two following values ​​(s + 1, 3)  if a fire does not occur. This is because, if there are no fires, then the trees age, but can never take on a state higher than 3. There is a chance that a fire will burn the forest down after the application of the action, bringing the whole population back into position in the range of a younger age (state 1).</p>
<p>Let's say p = 0.1 is the probability that a wildfire occurs during a period of time. The problem is how to manage this in the long term to maximize the reward. This problem can be treated as a Markov decision process.</p>
<p>First, we define the transition matrix P (s, s', a). Remember that it tells us what the probabilities are of passing from one state to another. Since the available actions are (Wait, Cut), we will define two matrices of transitions. If we denote the probability of a fire with p, then we will have the following transition matrix relating to the choice of action 1 (Wait):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/047548c1-221a-47a9-ae90-20af3187227c.png" style="width:10.67em;height:3.25em;"/></p>
<p>This is because, if we are in state 1, then we will have a probability of p remaining in that state (if a fire occurs) and the remaining 1-p probability of moving to the next state (if no fire occurs). While the probability of passing to state 3 is equal to 0, it isn't possible to pass from state 1 to 3 directly. On the other hand, if<span> we are in state 2, we will have a probability of p passing into state 1 (if a fire occurs) and the remaining 1-p probability of passing to the next state, that is, 3 (if no fire occurs).</span></p>
<p><span class="">Here, the probability of remaining in state 2 is equal to 0. Finally, if we are in state 3, we will have a probability equal to p to go into state 1 (if a fire occurs) and the remaining 1-p probability to remain in state 3 (if no fire occurs) since this is the last to be possible over time. The probability of passing to state 2 is equal to 0.</span></p>
<p>Now, let's define the transition matrix in terms of the choice of action 2 (Cut):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/22f260f3-5622-4d46-9fc2-b1f377a4ff75.png" style="width:7.75em;height:3.25em;"/></p>
<p>In this case, its meaning is more intuitive when choosing to cut the wood. Here, the transition leads to state 1 in all three cases with a unit probability. </p>
<p>Now, we can define the two vectors of the rewards R (s, a):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7ab5efeb-3f8d-48d7-823d-fda0b0c67128.png" style="width:4.67em;height:3.08em;"/></p>
<p>If the chosen action is to wait for the growth of the forest, then we will have 0 for the reward for the first two states and the maximum reward for state 3. In this case, we have chosen 4 as a reward, which represents the value that's provided by the system by default. If the chosen action is to cut the wood instead, we will have the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fd13cb68-1400-4a3e-b40c-47c0f438bbae.png" style="width:4.17em;height:2.75em;"/></p>
<p>Here, if the chosen action is to cut the wood, then we will have the following rewards: 0 for state 1, 1 for state 2, and 2 for state 3.</p>
<p>Our goal is to calculate a policy that allows us to get the maximum reward based on the settings we just developed:</p>
<ol>
<li>Let's look at the code that allows us to do this:</li>
</ol>
<pre style="padding-left: 60px">library(MDPtoolbox)<br/>data = mdp_example_forest()<br/><br/>print(data$P[,,1])<br/>print(data$P[,,2])<br/><br/>print(data$R[,1])<br/>print(data$R[,2])<br/><br/>mdp_check(data$P, data$R)<br/><br/>solver=mdp_policy_iteration(P=data$P, R=data$R, discount = 0.95)<br/><br/>print(solver$V)<br/>print(solver$policy)<br/>print(solver$iter)<br/>print(solver$time)</pre>
<ol start="2">
<li>Let's analyze the code line by line to understand the meaning of each command. Let's start by importing the library:</li>
</ol>
<pre style="padding-left: 60px">library(MDPtoolbox)</pre>
<p>The <kbd>MDPtoolbox</kbd> package provides functions related to the resolution of discrete-time Markov decision processes, that is, finite horizon, value iteration, policy iteration, linear programming algorithms with some variants, and some functions related to reinforcement learning.  We can see a short description of the <kbd>MDPtoolbox</kbd> package, which can be extracted from the official documentation, in the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Version</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">4.0.3</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Date</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">2017-03-02</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Maintainer</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Guillaume Chapron</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">License</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">BSD_3_clause + file LICENSE</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Authors</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Iadine Chades, Guillaume Chapron, Marie-Josee Cros, Frederick Garcia, Regis Sabbadin</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>As we anticipated, the first thing we need to do is define the matrices P for the transition function and R for the reward function. First, we will use the data contained in the example supplied with the package:</p>
<ol>
<li>To do this, simply invoke the example, as follows:</li>
</ol>
<pre style="padding-left: 60px">data = mdp_example_forest()</pre>
<ol start="2">
<li>The object that we've created (a list, in this case) contains both the transition matrix P and the reward vectors. Let's look at its content:</li>
</ol>
<pre style="padding-left: 60px">str(data)</pre>
<p style="padding-left: 60px">The following results are returned:</p>
<pre style="padding-left: 60px"><strong>List of 2</strong><br/><strong> $ P: num [1:3, 1:3, 1:2] 0.1 0.1 0.1 0.9 0 0 0 0.9 0.9 1 ...</strong><br/><strong> $ R: num [1:3, 1:2] 0 0 4 0 1 2</strong><br/><strong> ..- attr(*, "dimnames")=List of 2</strong><br/><strong> .. ..$ : NULL</strong><br/><strong> .. ..$ : chr [1:2] "R1" "R2"</strong></pre>
<ol start="3">
<li>To extract the transition matrices, we can write the following:</li>
</ol>
<pre style="padding-left: 60px">print(data$P[,,1])<br/>print(data$P[,,2])</pre>
<p style="padding-left: 60px">By doing this, we can see the two transition matrices:</p>
<pre style="padding-left: 60px">&gt; print(data$P[,,1])<br/><strong>   [,1] [,2] [,3]</strong><br/><strong>[1,] 0.1 0.9 0.0</strong><br/><strong>[2,] 0.1 0.0 0.9</strong><br/><strong>[3,] 0.1 0.0 0.9</strong><br/><br/>&gt; print(data$P[,,2])<br/><strong> [,1] [,2] [,3]</strong><br/><strong>[1,] 1 0 0</strong><br/><strong>[2,] 1 0 0</strong><br/><strong>[3,] 1 0 0</strong></pre>
<ol start="4">
<li>By default, <kbd>p = 0.1</kbd>. Here,  p represents the probability of a fire developing. Using this value, we can confirm the shape of the transition matrix. Let's move on and view the reward vectors:</li>
</ol>
<pre style="padding-left: 60px">print(data$R[,1])<br/>print(data$R[,2])</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px"><span> The following results are returned:</span></p>
<pre style="padding-left: 60px">&gt; print(data$R[,1])<br/><strong>[1] 0 0 4</strong><br/><br/>&gt; print(data$R[,2])<br/><strong>[1] 0 1 2</strong></pre>
<ol start="5">
<li>Before developing the model, it is necessary to verify that P and R satisfy the criteria that are necessary for the problem to be of the MDP type. To do this, we'll use the <kbd>mdp_check()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">mdp_check(data$P, data$R)</pre>
<p>This function checks whether the MDP that's defined by the transition probability array (P) and the reward array (R) is valid. If P and R are correct, the function returns an empty error message. If they aren't correct, the function returns an error message describing the problem. <span>The following result is returned:</span></p>
<pre style="padding-left: 60px">&gt; mdp_check(data$P, data$R)<br/><strong>[1] ""</strong></pre>
<p><span>Here, we can see that the problem has been set. Now, we can search for the best policy for forest management.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy iteration algorithm</h1>
                </header>
            
            <article>
                
<p>As we mentioned in <a href="aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml">Chapter 2</a>, <em>Building Blocks of Reinforcement Learning</em>, <span>policy iteration is a dynamic programming algorithm that uses a value function to model the expected return for each pair of action-state. We will apply this method to the case in question.</span></p>
<p>At this point, we will try to solve the problem at hand using the <kbd>mdp_policy_iteration()</kbd> function:</p>
<pre>solver=mdp_policy_iteration(P=data$P, R=data$R, discount = 0.95)</pre>
<p>This function solves discounted MDP with the policy iteration algorithm. As we mentioned in <a href="aed130c4-9d8b-42d1-826a-e26a4162ebcf.xhtml">Chapter 2</a>, <em>Building Blocks of Reinforcement Learning</em>, <kbd>policy iteration</kbd> is a dynamic programming algorithm that uses a value function to model the expected return for each pair of action-state. These techniques update the value functions using the immediate reward and the (discounted) value of the next state in a process called bootstrapping. Therefore, they imply the storage of Q (s, a) in tables or with approximate function techniques. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Starting from an initial P0 policy, the iteration of the policy alternates between the following two phases:</p>
<ul>
<li><kbd>Policy evaluation</kbd>: Given the current policy P, estimate the action-value function QP.</li>
<li><kbd>Policy Improvement</kbd>: If we calculate a better policy P ' based on QP, then set P' as the new policy and return to the previous step.</li>
</ul>
<p>When the QP value function can be calculated exactly for each action-state pair, the policy iteration with the greedy policy improvement leads to convergence by returning the optimal policy. Essentially, repeatedly executing these two processes converges the general process toward the optimal solution.</p>
<p>The following arguments are passed:</p>
<ul>
<li><strong>P</strong>: Transition probability array. P can be a three-dimensional array [S,S,A] or a list [[A]], with each element containing a sparse matrix [S,S].</li>
<li><strong>R</strong>: Reward array. R can be a three-dimensional array [S,S,A] or a list [[A]], with each element containing a sparse matrix [S,S] or a two-dimensional matrix [S,A] that's possibly sparse.</li>
<li><strong>discount</strong>: Discount factor. The discount is a real that belongs to ]0; 1[.</li>
</ul>
<p>The <span>policy iteration </span>algorithm <span>improves the policy iteratively using</span> the evaluation of the current policy. Iterating is stopped when two successive policies are identical or when a specified number (<kbd>max_iter</kbd>) of iterations have been performed.</p>
<p>The following results are returned:</p>
<ul>
<li><strong>V:</strong> Optimal value function. V is an S length vector.</li>
<li><strong>policy</strong>: Optimal policy. The policy is an S length vector. Each element is an integer corresponding to an action that maximizes the value function.</li>
<li><strong>iter</strong>: Number of iterations.</li>
<li><strong>cpu_time</strong>: CPU time used to run the program.</li>
</ul>
<p>Now that the model is ready, we just have to evaluate the results by checking the obtained policy:</p>
<ol>
<li>Let's learn how to extract these results from our model:</li>
</ol>
<pre style="padding-left: 60px">print(solver$V)</pre>
<p style="padding-left: 60px">The first element we have visualized is the optimal value function. </p>
<p class="mce-root"/>
<div class="packt_tip">Recall that a value function represents how good a state is for an agent. It is equal to the total reward expected for an agent from the status s. The value function depends on the policy that the agent selects for the actions to be performed on.</div>
<p style="padding-left: 60px"><span>The following results are returned:</span></p>
<pre style="padding-left: 60px"><strong><span>[1] 58.482 61.902 65.902</span></strong></pre>
<ol start="2">
<li>Let's look at the policy that's returned by the model:</li>
</ol>
<pre style="padding-left: 60px">print(solver$policy)</pre>
<div class="packt_tip">Recall that a policy defines the behavior of the learning agent at a given time. It maps the detected states of the environment and the actions to take when they are in those states. This corresponds to what in psychology would be called a set of rules or associations of stimulus-response. The policy is the fundamental part of a reinforcing learning agent in the sense that it alone is enough to determine behavior.</div>
<p style="padding-left: 60px"><span>The following results are returned:</span></p>
<pre style="padding-left: 60px"><strong><span>[1] 1 1 1</span></strong></pre>
<p style="padding-left: 60px">Here, the optimal policy is to not cut the forest in all three states. This is due to the low probability of developing a fire that causes the wait to be the best action to perform. In this way, the forest has time to grow and we can achieve both goals: <span>maintain an old forest for wildlife and earn money by selling the cut wood.</span></p>
<ol start="3">
<li>Now, we can look at how many iterations the model has taken to converge:</li>
</ol>
<pre style="padding-left: 60px">print(solver$iter)</pre>
<p style="padding-left: 60px"><span>The following result is printed:</span></p>
<pre style="padding-left: 60px"><strong><span>[1] 2</span></strong></pre>
<ol start="4">
<li>As we can see, it only takes two iterations to get the result. Finally, we want to see how much CPU time it took to process the program:</li>
</ol>
<pre style="padding-left: 60px">print(solver$time)</pre>
<p style="padding-left: 60px"><span>The following result is printed:</span></p>
<pre style="padding-left: 60px"><strong>Time difference of 0.4140239 secs</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This first example allowed us to understand how easy it is to derive an optimal policy from a well-posed problem. Now, let's look at what happens when we modify the starting conditions of the system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">New state transition matrix</h1>
                </header>
            
            <article>
                
<p>So far, we have seen that, when the probability of a fire developing is low, the optimal policy advises us to wait and not cut the forest. But what happens if the probability of developing a fire is higher? Here, we just need to change the problem settings by changing the probability value p. Let's get started:</p>
<ol>
<li>The following code allows us to do this:</li>
</ol>
<pre style="padding-left: 60px">library(MDPtoolbox)<br/>data = mdp_example_forest(3,4,2,0.8)<br/><br/>print(data$P[,,1])<br/>print(data$P[,,2])<br/><br/>print(data$R[,1])<br/>print(data$R[,2])<br/><br/>mdp_check(data$P, data$R)<br/><br/>solver=mdp_policy_iteration(P=data$P, R=data$R, discount = 0.95)<br/><br/>print(solver$V)<br/>print(solver$policy)<br/>print(solver$iter)<br/>print(solver$time)</pre>
<ol start="2">
<li>Let's analyze the code line by line, focusing on the changes that are made to the initial code:</li>
</ol>
<pre style="padding-left: 60px">data = mdp_example_forest(3,4,2,0.8)</pre>
<p style="padding-left: 60px">This is the modified code. We haven't used the values that are provided by the problem; instead, we have set new values. Remember that the syntax of the <kbd>mdp_example_forest()</kbd> function is as follows:</p>
<pre style="padding-left: 60px">mdp_example_forest(S, r1, r2, p)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The following arguments are passed:</p>
<ul>
<li><strong>S</strong> (optional): Number of states. S is an integer greater than 0. By default, S is set to 3.</li>
<li><strong>r1</strong> (optional): The reward when the forest is in the oldest state and the Wait action is performed. r1 is a real greater than 0. By default, r1 is set to 4.</li>
<li><strong>r2</strong> (optional): The reward when the forest is in the oldest state and the Cut action is performed. r2 is a real greater than 0. By default, r2 is set to 2.</li>
<li><strong>p</strong> (optional): The probability of a wildfire occurring. p is a real in ]0, 1[. By default, p is set to 0.1.</li>
</ul>
<p style="padding-left: 60px">Here, we have confirmed the three states, legally modified the rewards, and increased the probability that a fire will develop, bringing it from the initial value of 0.1 to the new value of 0.8.</p>
<ol>
<li>Let's see what results we get by making this change:</li>
</ol>
<pre style="padding-left: 60px">print(data$P[,,1])<br/>print(data$P[,,2])</pre>
<p style="padding-left: 60px">Here, we can see the two transition matrices:</p>
<pre style="padding-left: 60px">&gt; print(data$P[,,1])<br/><strong>     [,1] [,2] [,3]</strong><br/><strong>[1,] 0.8 0.2 0.0</strong><br/><strong>[2,] 0.8 0.0 0.2</strong><br/><strong>[3,] 0.8 0.0 0.2</strong><br/><br/>&gt; print(data$P[,,2])<br/><strong> [,1] [,2] [,3]</strong><br/><strong>[1,] 1 0 0</strong><br/><strong>[2,] 1 0 0</strong><br/><strong>[3,] 1 0 0</strong></pre>
<ol start="2">
<li> Let's move on and view the reward arrays:</li>
</ol>
<pre style="padding-left: 60px">print(data$R[,1])<br/>print(data$R[,2])</pre>
<p style="padding-left: 60px"><span> The following results are returned:</span></p>
<pre style="padding-left: 60px">&gt; print(data$R[,1])<br/><strong>[1] 0 0 3</strong><br/><br/>&gt; print(data$R[,2])<br/><strong>[1] 0 1 2</strong></pre>
<ol start="3">
<li>Before developing the model, it is necessary to verify that P and R satisfy the criteria for the problem so that they're of the MDP type. To do this, we'll use the <kbd>mdp_check()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">mdp_check(data$P, data$R)</pre>
<p style="padding-left: 60px"><span> </span><span>The following result is returned:</span></p>
<pre style="padding-left: 60px">&gt; mdp_check(data$P, data$R)<br/><strong>[1] ""</strong></pre>
<ol start="4">
<li>Now that the problem has been set, we can try to solve the problem using the<span> </span><kbd>mdp_policy_iteration</kbd><span> </span>function:</li>
</ol>
<pre style="padding-left: 60px">solver=mdp_policy_iteration(P=data$P, R=data$R, discount = 0.95)</pre>
<p style="padding-left: 60px">Let's extract the results from the model:</p>
<pre style="padding-left: 60px">print(solver$V)</pre>
<p style="padding-left: 60px">The first element we have visualized is the optimal value function. </p>
<div class="packt_tip">Recall that a value function represents how good a state is for an agent. It is equal to the total reward that's expected for an agent from the status s. The value function depends on the policy that the agent selects for the actions to be performed on.</div>
<p style="padding-left: 60px"><span>The following results are returned:</span></p>
<pre style="padding-left: 60px"><strong><span>[1] 3.193277 4.033613 6.699865</span></strong></pre>
<ol start="5">
<li>By comparing this with the results we obtained in the initial model, we can see that the total rewards have decreased considerably. Let's look at the policy that's returned by the model:</li>
</ol>
<pre style="padding-left: 60px">print(solver$policy)</pre>
<p style="padding-left: 60px"><span>The following results are returned:</span></p>
<pre style="padding-left: 60px"><strong><span>[1] 1 2 1</span></strong></pre>
<p style="padding-left: 60px">Here, we can see that the optimal policy has changed. In this case, in states 1 and 3, the advice to choose the wait action remains. However, in state 2, it is recommended to cut, to avoid losing the wood that's been obtained so far.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="6">
<li>Now, let's look at how many iterations the model has to go through in order to converge:</li>
</ol>
<pre style="padding-left: 60px">print(solver$iter)</pre>
<p style="padding-left: 60px"><span>The following result is printed:</span></p>
<pre style="padding-left: 60px"><strong><span>[1] 1</span></strong></pre>
<ol start="7">
<li>Finally, we can see how much CPU time it took to process the program:</li>
</ol>
<pre style="padding-left: 60px">print(solver$time)</pre>
<p style="padding-left: 60px"><span>The following result is printed:</span></p>
<pre style="padding-left: 60px"><strong>Time difference of 0.009001017 secs</strong></pre>
<p>This example has shown us how to modify the parameters of the problem. Here, we can see that by increasing the probability of developing a fire, the optimal policy that's developed by the model changes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at stochastic processes and their applications. The theory of stochastic processes concerns the study of systems that evolve over time according to probabilistic laws. Due to this, we are interested in calculating the probabilities associated with it. For this reason, we learned about the basic concepts of probability. The a priori probability, joint probability, and conditional probability were all defined, followed by examples of how to calculate them.</p>
<p>Then, we were introduced to Markov chains. A Markov chain is a mathematical model of a random phenomenon that evolves over time in such a way that the past influences the future through the present. In other words, it represents the stochastic description of a sequence of possible events. The probability of each event depends on the state that was reached in the previous event. Here, we learned how to define and read a transition matrix and a transition diagram. We used Markov chains for forecasting the weather conditions for 365 consecutive days. Finally, we saw how to use the <kbd>MDPtoolbox</kbd> package to calculate the optimal policy for managing a tiny forest.</p>
<p>In the next chapter, we will explore the basic concepts of the multi-armed bandit model. We will discover the different techniques that we can use and the meaning of the action-value implementation. We will learn how to address a problem using a contextual approach and learn how to implement asynchronous actor-critic agents.</p>


            </article>

            
        </section>
    </body></html>