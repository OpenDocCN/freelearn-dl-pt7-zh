<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer057">
			<h1 id="_idParaDest-183" class="chapter-number"><a id="_idTextAnchor207"/>9</h1>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor208"/>Harnessing Whisper for Personalized Voice Synthesis</h1>
			<p>Welcome to <a href="B21020_09.xhtml#_idTextAnchor207"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, where we’ll delve into <strong class="bold">personalized voice synthesis</strong> (<strong class="bold">PVS</strong>). This field encompasses various applications and technologies that create synthetic voices tailored to individual preferences or needs. PVS is a versatile process that can be customized for various purposes, including assistive technologies, virtual assistant development, and digital content creation. In this context, OpenAI’s Whisper tool enables voice synthesis by providing accurate speech data transcriptions during preprocessing <span class="No-Break">and registration.</span></p>
			<p>As we begin, it’s crucial to distinguish between voice cloning and PVS. Voice cloning involves creating a digital replica of a natural person’s voice. While this technology has valid applications, it also raises significant ethical concerns. PVS, however, focuses on creating unique voices inspired by specific characteristics without directly copying an individual’s voice. This distinction is vital in discussions about the ethical use of voice synthesis technologies. In this chapter, we will guide you on harnessing Whisper’s power to create PVS models, ensuring you have the knowledge to use this <span class="No-Break">technology responsibly.</span></p>
			<p>We’ll begin by exploring <strong class="bold">speech synthesis</strong> and <strong class="bold">text-to-speech </strong>(<strong class="bold">TTS</strong>) fundamentals. You will gain insights into the role of neural networks, audio processing, and voice synthesis in this domain. Building on this foundation, we will guide you through converting audio files to the <strong class="bold">LJSpeech</strong> format, a standardized dataset structure commonly used in <span class="No-Break">TTS tasks.</span></p>
			<p>Next, we will introduce you to the <strong class="bold">Deep Learning Art School</strong> (<strong class="bold">DLAS</strong>) toolkit, a robust framework for fine-tuning PVS models. This is where your learning journey will truly begin. You will discover how to set up the training environment, prepare the dataset, and configure the model architecture. By leveraging the power of Whisper’s accurate transcriptions, you can align audio segments with their corresponding text, creating a dataset suitable for training PVS models. This tutorial is not just a guide but your gateway to mastering the art of PVS with Whisper. Get ready to be inspired <span class="No-Break">and motivated!</span></p>
			<p>Hands-on examples and code snippets will give you practical experience fine-tuning a pre-trained PVS model using your LJSpeech dataset. You will discover how to customize the training process, select appropriate hyperparameters, and evaluate the <span class="No-Break">model’s performance.</span></p>
			<p>Finally, we will test your fine-tuned PVS model by synthesizing realistic and expressive speech. You will learn how to generate natural-sounding speech by providing text input to the model, bringing the PVS voice <span class="No-Break">to life.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Understanding TTS <span class="No-Break">in PVS</span></li>
				<li>Converting audio files into <span class="No-Break">LJSpeech format</span></li>
				<li>Fine-tuning a PVS model using the <span class="No-Break">DLAS toolkit</span></li>
				<li>Synthesizing speech using a fine-tuned <span class="No-Break">PVS model</span></li>
			</ul>
			<p>By the end of this chapter, you will have a comprehensive understanding of how to utilize Whisper for PVS. You will possess the knowledge and skills to preprocess audio data, fine-tune voice models, and generate realistic speech using PVS frameworks. Whether you are a researcher, developer, or enthusiast in speech technology, this chapter will equip you with valuable insights and practical techniques to unlock the potential of PVS using <span class="No-Break">OpenAI’s Whisper.</span></p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor209"/>Technical requirements</h1>
			<p>To harness the capabilities of OpenAI’s Whisper for advanced applications, this chapter leverages Python and Google Colab for ease of use and accessibility. The Python environment setup includes the Whisper library for <span class="No-Break">transcription tasks.</span></p>
			<p><span class="No-Break"><strong class="bold">Key requirements</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="bold">Google Colab notebooks</strong>: The notebooks are set to run our Python code with the minimum required memory and capacity. If the <strong class="bold">T4 GPU</strong> runtime type is available, select it for <span class="No-Break">better performance.</span></li>
				<li><strong class="bold">Python environment</strong>: Each notebook contains directives to load the required <span class="No-Break">Python libraries.</span></li>
				<li><strong class="bold">Hugging Face account</strong>: Some notebooks require a Hugging Face account and login API key. The Colab notebooks include information about <span class="No-Break">this topic.</span></li>
				<li><strong class="bold">Audacity</strong>: Audacity is a free and open source digital audio editor and recording application available for Windows, macOS, Linux, and other Unix-like operating systems. It is an excellent choice if you want to synthesize <span class="No-Break">your voice.</span></li>
				<li><strong class="bold">Microphone and speakers</strong>: Some notebooks implement audio with voice recording and audio playback. A microphone and speakers connected to your computer might help you experience the interactive <span class="No-Break">voice features.</span></li>
				<li><strong class="bold">GitHub repository access</strong>: All Python code, including examples, is available in this chapter’s GitHub repository (<a href="https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter09">https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter09</a>). These Colab notebooks are ready to run, providing a practical and hands-on approach <span class="No-Break">to learning.</span></li>
			</ul>
			<p>By meeting these technical requirements, you will be prepared to explore Whisper in different contexts while enjoying the streamlined experience of Google Colab and the comprehensive resources available <span class="No-Break">on GitHub.</span></p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor210"/>Understanding text-to-speech in voice synthesis</h1>
			<p>TTS is a crucial component in the voice<a id="_idIndexMarker879"/> synthesis process, enabling speech to be generated from written text using the synthesized voice. Understanding the <a id="_idIndexMarker880"/>fundamentals of TTS is essential to grasp how voice synthesizing works and how it can be applied in various scenarios. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.1</em> illustrates a high-level overview of how TTS works in the context of voice synthesis without delving too deeply into <span class="No-Break">technical specifics:</span></p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/Figure_9.1_B21020.jpg" alt="Figure 9.1 – The TTS voice synthesis pipeline" width="1209" height="433"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – The TTS voice synthesis pipeline</p>
			<p>There are five components in the TTS voice <span class="No-Break">synthesis pipeline:</span></p>
			<ol>
				<li><span class="No-Break"><strong class="bold">Text preprocessing</strong></span><span class="No-Break">:</span><ol><li class="upper-roman">The input text is first normalized <span class="No-Break">and</span><span class="No-Break"><a id="_idIndexMarker881"/></span><span class="No-Break"> preprocessed.</span></li><li class="upper-roman">Numbers, abbreviations, and special characters are expanded into <span class="No-Break">full words.</span></li><li class="upper-roman">The text is divided into individual sentences, words, and phonemes (distinct <span class="No-Break">sound units).</span></li></ol></li>
				<li><span class="No-Break"><strong class="bold">Text-to-spectrogram</strong></span><span class="No-Break">:</span><ol><li class="upper-roman">The normalized text is converted into a <a id="_idIndexMarker882"/>sequence of linguistic features and encoded into a <span class="No-Break">vector representation.</span></li><li class="upper-roman">A spectrogram generator model, usually a deep learning model, takes this encoded text and generates <span class="No-Break">a spectrogram.</span></li><li class="upper-roman">The spectrogram visually represents the frequencies and intensities of the speech sounds <span class="No-Break">over time.</span></li></ol></li>
				<li><span class="No-Break"><strong class="bold">Spectrogram-to-waveform</strong></span><span class="No-Break">:</span><p class="list-inset">The spectrogram is then fed into a vocoder model. The vocoder is a generative model trained to convert spectrograms into audible waveforms. It reconstructs the speech signal from the <a id="_idIndexMarker883"/>frequency information in <span class="No-Break">the spectrogram.</span></p></li>
				<li><span class="No-Break"><strong class="bold">Voice synthesis</strong></span><span class="No-Break">:</span><p class="list-inset">To synthesize a specific person’s <a id="_idIndexMarker884"/>voice, the TTS models are fine-tuned on a dataset of that person’s speech. This allows the models to learn their voices’ unique characteristics, tone, and prosody. With sufficient training data, the generated speech will mimic the <span class="No-Break">target voice.</span></p></li>
				<li><span class="No-Break"><strong class="bold">Synthesis</strong></span><span class="No-Break">:</span><p class="list-inset">Finally, the generated waveform is<a id="_idIndexMarker885"/> output as audible synthetic speech. The result is a synthesized voice that speaks the original <span class="No-Break">input text.</span></p></li>
			</ol>
			<p>Modern TTS systems can produce highly natural-sounding speech with appropriate intonation and expressiveness. The TTS pipeline, with its complex interplay of text processing, acoustic modeling, and speech synthesis, forms the foundation of the PVS transformative technology. As we explore the intricacies of voice synthesis, it is essential to understand how TTS systems can be leveraged to create <span class="No-Break">personalized voices.</span></p>
			<p>One such robust TTS<a id="_idIndexMarker886"/> implementation is <strong class="bold">TorToiSe-TTS-Fast</strong>, a high-performance TTS system that harnesses the power of neural networks to generate realistic and expressive speech. The following sections will delve into TorToiSe-TTS-Fast’s capabilities and demonstrate how it can synthesize voices with remarkable accuracy <span class="No-Break">and naturalness.</span></p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor211"/>Introducing TorToiSe-TTS-Fast</h2>
			<p>In <a href="B21020_05.xhtml#_idTextAnchor142"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, we used the gTTS Python<a id="_idIndexMarker887"/> library, an interface to Google Translate’s TTS API. gTTS lets you generate spoken audio from text using Google’s TTS engine. This time, we will explore the TorToiSe-TTS-Fast project, a high-performance TTS system that leverages neural networks to synthesize realistic speech without fine-tuning. Next, we will learn how to initialize the <strong class="source-inline">TextToSpeech</strong> model, which is the core component of the TTS system. We will explore the <strong class="source-inline">TextToSpeech</strong> class and understand its role in converting text <span class="No-Break">into speech.</span></p>
			<p>One of the exciting features of the TorToiSe-TTS-Fast project is its ability to generate speech using different audio clip samples of a given voice. The project provides a collection of pre-packaged voices as audio clips organized in separate folders. These audio clips are used to determine many properties of the voice synthesized output, such as the pitch and tone of the voice, speaking speed, and even speaking defects, such as a lisp or stuttering. We will <a id="_idIndexMarker888"/>delve into selecting a voice from that collection of pre-existing voice samples. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em> shows the TorToiSe-TTS-Fast <span class="No-Break">voice processing:</span></p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/Figure_9.2_B21020.jpg" alt="Figure 9.2 – TorToiSe-TTS-Fast voice processing pipeline" width="1160" height="750"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – TorToiSe-TTS-Fast voice processing pipeline</p>
			<p>By following the steps in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.2</em>, you can incorporate additional voices into TorToiSe and enhance <span class="No-Break">its versatility:</span></p>
			<ol>
				<li>Collect audio samples featuring the desired voice(s). Interviews on YouTube (which can be downloaded using <strong class="source-inline">youtube-dl</strong> or the <strong class="source-inline">pytube</strong> Python library, as we did in <a href="B21020_06.xhtml#_idTextAnchor160"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>), audiobooks, and podcasts are excellent sources. I recommend the <strong class="bold">Audacity</strong> tool as a viable option for recording your voice and processing <span class="No-Break">audio files.</span></li>
				<li>Divide the collected audio into segments of approximately 10 seconds each. A minimum of 3 clips is required, but more clips are recommended for better results. During testing, I experimented with up to <span class="No-Break">5 clips.</span></li>
				<li>Convert the audio segments into WAV format with floating-point encoding and a sample rate of <span class="No-Break">22,050 Hz.</span></li>
				<li>Once you run the <strong class="source-inline">LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb</strong> notebook later in this chapter, you will see a directory structure called <strong class="source-inline">/tortoise/voices/</strong> with audio clip samples in it. This is the default folder TorToiSe uses to store and retrieve audio samples. If you create your samples, create a<a id="_idIndexMarker889"/> folder in that <strong class="source-inline">/tortoise/voices/</strong> directory and save your files there. For example, I made the <strong class="source-inline">/tortoise/voices/josue</strong> folder to store my <span class="No-Break">audio files.</span></li>
				<li>Transfer the processed audio segments into the newly <span class="No-Break">created subdirectory.</span></li>
				<li>To utilize the new voice, execute the <strong class="source-inline">tortoise</strong> utilities with the <strong class="source-inline">--voice</strong> flag, followed by the name of <span class="No-Break">your subdirectory.</span></li>
			</ol>
			<p>After exploring the TorToiSe-TTS-Fast pipeline, it should be clear that high-quality audio data is foundational to creating convincing, natural-sounding synthesized voices. Preparing this audio data involves creating new recordings or manipulating existing audio files to ensure they are suitable for voice synthesis. This is where Audacity comes into play as a powerful tool for audio creation, editing, and refinement. Of course, I encourage you to use other tools you are already using for audio processing; Audacity and creating an audio file <span class="No-Break">is optional.</span></p>
			<p>Audacity is a versatile tool for creating, editing, and manipulating audio files, an essential step in the voice synthesis pipeline. It allows you to record your voice samples, trim and split audio clips, adjust audio properties such as pitch and speed, and export files in various formats compatible with voice synthesis tools. By leveraging Audacity’s capabilities, you can prepare high-quality audio data tailored to your voice <span class="No-Break">synthesis requirements.</span></p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor212"/>Using Audacity for audio processing</h2>
			<p>At its core, Audacity is<a id="_idIndexMarker890"/> a multitrack audio editor and recorder that supports many operating systems, including Windows, macOS, GNU/Linux, and other Unix-like systems. Its open source nature ensures it remains free for all users and fosters a vibrant community of developers and audio enthusiasts who continuously contribute to its development and enhancement. This collaborative effort has equipped Audacity with various capabilities, from basic recording and editing to more advanced features such as noise reduction, spectral analysis, and support for different audio formats. If you prefer another audio editor, go for it. The use of Audacity is optional. If you want to install it, here is a <span class="No-Break">step-by-step guide.</span></p>
			<p>The installation process of Audacity is straightforward, regardless of your operating system. Detailed instructions are available on the Audacity website (<a href="https://support.audacityteam.org/basics/downloading-and-installing-audacity">https://support.audacityteam.org/basics/downloading-and-installing-audacity</a>). Here, we’ll cover the basic steps to get Audacity up and running on <span class="No-Break">your machine.</span></p>
			<h4>Installing Audacity for Windows</h4>
			<p>Follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li><strong class="bold">Download the installer</strong>: Navigate to the official<a id="_idIndexMarker891"/> Audacity website (<a href="https://www.audacityteam.org/">https://www.audacityteam.org/</a>) and click on the download link for the Windows<a id="_idIndexMarker892"/> version. The site will automatically detect your operating system, but you can manually select the version <span class="No-Break">if needed.</span></li>
				<li><strong class="bold">Run the installer</strong>: Once the download is complete, locate the installer file (usually in your <strong class="source-inline">Downloads</strong> folder) and double-click to initiate installation. You might encounter a security prompt asking for permission to allow the installer to change your system; click <strong class="bold">Yes</strong> <span class="No-Break">to proceed.</span></li>
				<li><strong class="bold">Follow the installation wizard</strong>: The installer will guide you through several steps. You’ll select your preferred language, agree to the license terms, choose the installation directory, and decide on additional tasks, such as creating a <span class="No-Break">desktop shortcut.</span></li>
				<li><strong class="bold">Complete the installation</strong>: After configuring your preferences, click <strong class="bold">Install</strong> to begin the installation. Once <a id="_idIndexMarker893"/>completed, you can<a id="_idIndexMarker894"/> launch Audacity directly from the installer or find it in your <span class="No-Break">Start menu.</span></li>
			</ol>
			<h4>Installing Audacity for macOS</h4>
			<p>Follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li><strong class="bold">Download the DMG file</strong>: Visit the <a id="_idIndexMarker895"/>Audacity website and download the macOS version. The site should automatically provide the correct <a id="_idIndexMarker896"/>version of <span class="No-Break">your system.</span></li>
				<li><strong class="bold">Install Audacity</strong>: Open the downloaded DMG file and drag the Audacity icon to your <strong class="source-inline">Applications</strong> folder to install the software. You might need to authenticate using your <span class="No-Break">administrator password.</span></li>
				<li><strong class="bold">Launch Audacity</strong>: Open it in your <strong class="source-inline">Applications</strong> folder. macOS might prompt you to confirm that you trust the application, especially if you’re running it for the <span class="No-Break">first time.</span></li>
			</ol>
			<h4>Installing Audacity for Linux</h4>
			<p>Linux users can download AppImage<a id="_idIndexMarker897"/> from the Audacity website or install <a id="_idIndexMarker898"/>Audacity using their distribution’s package manager. For AppImage, follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li><strong class="bold">Make the AppImage executable</strong>: After downloading, right-click the file, navigate to <strong class="bold">Properties</strong> | <strong class="bold">Permissions</strong>, and check the option to make the <span class="No-Break">file executable.</span></li>
				<li><strong class="bold">Run Audacity</strong>: Double-click AppImage to <span class="No-Break">launch Audacity.</span></li>
			</ol>
			<p>Alternatively, use commands such as <strong class="source-inline">sudo apt install audacity</strong> for Debian-based distributions <a id="_idIndexMarker899"/>or <strong class="source-inline">sudo yum install audacity</strong> for Fedora/RHEL to install Audacity through <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker900"/></span><span class="No-Break"> Terminal.</span></p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor213"/>Running the notebook with TorToiSe-TTS-Fast</h2>
			<p>With a more detailed understanding<a id="_idIndexMarker901"/> of Audacity as an audio <a id="_idIndexMarker902"/>creation, manipulation, and management tool, let’s do some hands-on work with TorToiSe-TTS-Fast. Please find and open the Colab notebook called <strong class="source-inline">LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb</strong> (<a href="https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb">https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb</a>). This notebook is based on the TorToiSe-TTS-Fast (<a href="https://github.com/152334H/tortoise-tts-fast">https://github.com/152334H/tortoise-tts-fast</a>) TTS project, which drastically boosts the performance of <strong class="source-inline">TorToiSe</strong> (<a href="https://github.com/neonbjb/tortoise-tts">https://github.com/neonbjb/tortoise-tts</a>), without modifying the <span class="No-Break">base models.</span></p>
			<p>Using the notebook, we will develop speech from a given text with the <strong class="source-inline">TextToSpeech</strong> model initialized and a chosen voice. Furthermore, we will investigate the flexibility of the TorToiSe-TTS-Fast project by generating speech using random and even custom voices. We can create personalized voices for speech synthesis by uploading and preprocessing our <span class="No-Break">WAV files.</span></p>
			<p>Lastly, we will explore the fascinating capability of combining multiple voices to generate speech with blended characteristics. We can create unique and intriguing voice combinations by loading voice samples and conditioning latents from <span class="No-Break">different voices.</span></p>
			<p>By the end of this section, you will have a solid understanding of TTS in the context of voice synthesis. You will have the knowledge and practical skills to set up the environment, initialize the <strong class="source-inline">TextToSpeech</strong> model, select voices, generate speech, and create custom and combined voices using the TorToiSe-TTS-Fast project. This understanding will serve as a foundation for further exploring the potential of voice synthesis and its applications in <span class="No-Break">various domains.</span></p>
			<p>Let’s open the notebook and run the cells to better understand the voice synthesis pipeline in the <span class="No-Break">TorToiSe-TTS-Fast project:</span></p>
			<ol>
				<li><strong class="bold">Setting up the environment</strong>: Here, we will install and instantiate several libraries, each serving a distinct <a id="_idIndexMarker903"/>purpose in the <span class="No-Break">project setup:</span><pre class="source-code">
!git clone <a href="https://github.com/152334H/tortoise-tts-fast">https://github.com/152334H/tortoise-tts-fast</a>
%cd tortoise-tts-fast
!pip3 install -r requirements.txt --no-deps
!pip3 install -e .
!pip3 install git+<a href="https://github.com/152334H/BigVGAN.git">https://github.com/152334H/BigVGAN.git</a>
!pip install transformers==4.29.2
!pip install voicefixer==0.1.2
%cd tortoise-tts-fast
from huggingface_hub import notebook_login
notebook_login()
from huggingface_hub import whoami
whoami()</pre><p class="list-inset">Let’s briefly review <span class="No-Break">each library:</span></p><ul><li><strong class="source-inline">torch</strong>: This is the PyTorch <a id="_idIndexMarker904"/>library, a popular open source machine learning library for computer vision and natural language processing applications. In the context of this project, PyTorch provides the foundational framework for building and training the neural networks that underpin the voice synthesis capabilities <span class="No-Break">of TorToiSe-TTS-Fast.</span></li><li><strong class="source-inline">torchaudio</strong>: As an extension to PyTorch, <strong class="source-inline">torchaudio</strong> offers easy access to audio processing tools within the PyTorch framework. It is used for loading and saving audio files and performing transformations and augmentations on audio data, which are essential tasks in <span class="No-Break">voice synthesis.</span></li><li><strong class="source-inline">huggingface_hub</strong>: This library from Hugging Face allows users to easily download and upload models and other files to the Hugging Face Hub, which may include the pre-trained <a id="_idIndexMarker905"/>models or components required by the <strong class="source-inline">TextToSpeech</strong> class <a id="_idIndexMarker906"/>for voice synthesis. The <strong class="source-inline">huggingface_hub</strong> library also provides a function for authenticating with the Hugging Face Hub via <strong class="source-inline">notebook_login()</strong> and managing user information via <strong class="source-inline">whoami()</strong>, facilitating access to the models and resources stored on the Hub required for <span class="No-Break">voice synthesis.</span></li><li><strong class="source-inline">transformers (version 4.29.2)</strong>: The <strong class="source-inline">transformers</strong> library, also from Hugging Face, provides thousands of pre-trained models for various natural language processing tasks, including TTS. This library supports the underlying NLP and TTS functionalities of the TorToiSe-TTS-Fast project by providing access to state-of-the-art models <span class="No-Break">and utilities.</span></li><li><strong class="source-inline">voicefixer (version 0.1.2)</strong>: This tool is designed for repairing and enhancing human voice recordings. <strong class="source-inline">voicefixer</strong> improves the quality of voice samples before they are processed by the voice synthesis system, ensuring higher fidelity in the <span class="No-Break">synthesized voices.</span></li><li><strong class="source-inline">BigVGAN</strong>: The TTS model uses a <strong class="bold">voice generative adversarial network</strong> (<strong class="bold">VGAN</strong>) to generate or modify voice data. The <strong class="source-inline">BigVGAN</strong> library plays a role in the voice synthesis process, enhancing the realism or quality of the <span class="No-Break">generated voices.</span></li></ul><p class="list-inset">Each of these libraries contributes to the overall functionality of the TorToiSe-TTS-Fast project by providing essential tools and frameworks for machine learning, audio processing, model management, and voice enhancement. These enable the efficient and practical synthesis of <span class="No-Break">human voices.</span></p></li>				<li><strong class="bold">Initializing the TextToSpeech model</strong>: Here, we’ll set up the <strong class="source-inline">TextToSpeech</strong> model that will be used for synthesizing voices. The steps involved in this section are <a id="_idIndexMarker907"/>designed to initialize the TTS model so that it is ready to process text input and generate corresponding speech output using the <span class="No-Break">selected voice:</span><pre class="source-code">
from tortoise.api import TextToSpeech
from tortoise.utils.audio import load_audio, load_voice, load_voices
# This will download all the models Tortoise uses from the HuggingFace hub.
tts = TextToSpeech()</pre><p class="list-inset">Here are the steps that are outlined in the <span class="No-Break">preceding code:</span></p><ul><li><strong class="bold">Importing the </strong><strong class="source-inline">TextToSpeech</strong><strong class="bold"> class</strong>: The first step is to import the <strong class="source-inline">TextToSpeech</strong> class from the <strong class="source-inline">tortoise.api</strong> module. This class is the primary interface for the TTS functionality provided by the <span class="No-Break">TorToiSe-TTS-Fast project.</span></li><li><strong class="bold">Creating an instance of </strong><strong class="source-inline">TextToSpeech</strong>: After importing the class, an instance of <strong class="source-inline">TextToSpeech</strong> is created<a id="_idIndexMarker908"/> by simply calling the class constructor without any arguments. This instance is assigned to the <span class="No-Break"><strong class="source-inline">tts</strong></span><span class="No-Break"> variable.</span></li><li><strong class="bold">Downloading the required models</strong>: Upon creating an instance of <strong class="source-inline">TextToSpeech</strong>, the necessary models are automatically downloaded from the Hugging Face Hub. This step ensures that all the required components for voice synthesis are <span class="No-Break">available locally.</span></li><li>The <strong class="source-inline">TextToSpeech</strong> class encapsulates the functionality needed to convert text into speech. Initializing it is critical in preparing the system for voice synthesizing tasks. Once the model has been initialized, it can be used in subsequent steps to generate speech from text using <span class="No-Break">various voices.</span></li></ul></li>				<li><strong class="bold">Selecting a voice</strong>: This section is crucial for personalizing the voice synthesis process by allowing you to choose <a id="_idIndexMarker909"/>from various<a id="_idIndexMarker910"/> pre-existing voice samples or uploaded <span class="No-Break">voice clips:</span><pre class="source-code">
import os
from ipywidgets import Dropdown
voices_dir = "tortoise/voices"
# Get a list of all directories in the voices directory
voice_names = os.listdir(voices_dir)
voice_folder = Dropdown(
    options=sorted(voice_names),
    description='Select a voice:',
    value='tom',
    disabled=False,
    style={'description_width': 'initial'},
)
voice_folder
import os
from ipywidgets import Dropdown
voices_dir = f"tortoise/voices/{voice_folder.value}"
# Get a list of all directories in the voices directory
voice_files = os.listdir(voices_dir)
voice = Dropdown(
    options=sorted(voice_files),
    description='Select a voice:',
    # value='tom',
    disabled=False,
    style={'description_width': 'initial'},
)
Voice
#Pick one of the voices from the output above
IPython.display.Audio(filename=f'tortoise/voices/{voice_folder.value}/{voice.value}')</pre><p class="list-inset">The steps involved in guiding <a id="_idIndexMarker911"/>you through the process <a id="_idIndexMarker912"/>of selecting a specific voice to synthesize are <span class="No-Break">as follows:</span></p><ul><li><strong class="bold">Listing the available voice folders</strong>: The code utilizes the <strong class="source-inline">os</strong> module to interact with the operating system and lists all the available voice folders in the <strong class="source-inline">tortoise/voices</strong> directory. This step is essential for identifying which voices are available <span class="No-Break">for synthesis.</span></li><li><strong class="bold">Creatinge a dropdown widget for voice folder selection</strong>: A dropdown widget is created using the <strong class="source-inline">Dropdown</strong> class from the <strong class="source-inline">ipywidgets</strong> library. This widget allows you to select a voice folder from the list of available folders. The <strong class="source-inline">Dropdown</strong> widget is configured with options populated from the list of voice folders, a description prompt (<strong class="source-inline">"Select a voice:"</strong>), and other settings to <span class="No-Break">ensure usability.</span></li><li><strong class="bold">Selecting a specific voice file</strong>: After selecting a voice folder, another dropdown widget is created to select a particular voice file within the chosen folder. This step is similar to the previous one but focuses on the individual voice files within the selected folder. The list of voice files is obtained using the <strong class="source-inline">os.listdir</strong> function, and the <strong class="source-inline">Dropdown</strong> widget is again used to present these options <span class="No-Break">to you.</span></li><li><strong class="bold">Playing the selected voice</strong>: Once a specific voice file is selected, the <strong class="source-inline">IPython.display.Audio</strong> class <a id="_idIndexMarker913"/>plays the voice file chosen directly in the Colab notebook. This<a id="_idIndexMarker914"/> feature provides immediate auditory feedback, enabling you to confirm that the selected voice is the desired one <span class="No-Break">for synthesis.</span></li></ul><p class="list-inset">These steps collectively enable a user-friendly and interactive approach to selecting a voice for synthesis. They ensure that you can easily navigate the available options and make an informed choice based on your pre<a id="_idTextAnchor214"/>ferences or <span class="No-Break">project requirements.</span></p></li>				<li><strong class="bold">Generating speech with a selected voice</strong>: This section of the notebook generates speech audio from text using the voice previously specified <span class="No-Break">by you:</span><pre class="source-code">
text = " Words, once silent, now dance on digital breath, speaking volumes through the magic of text-to-speech."
preset = "ultra_fast"
voice = voice_folder.value
voice_samples, conditioning_latents = load_voice(voice)
gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents, preset=preset)
torchaudio.save(generated_filename, gen.squeeze(0).cpu(), 24000)
IPython.display.Audio(generated_filename)</pre><p class="list-inset">The steps in this section are designed to convert your input text into spoken words in the chosen voice’s style. Here <a id="_idIndexMarker915"/>are the steps outlined in <span class="No-Break">the code:</span></p><ul><li><strong class="bold">Defining the text</strong>: Here, you define the text that they want to be spoken by assigning it to a variable named <strong class="source-inline">text</strong>. This text will be synthesized <span class="No-Break">into speech.</span></li><li><strong class="bold">Setting the quality preset</strong>: The <a id="_idIndexMarker916"/>quality of the generated speech is set by assigning a value to the preset variable. The options for the preset include <strong class="source-inline">"ultra_fast"</strong>, <strong class="source-inline">"fast"</strong>, <strong class="source-inline">"standard"</strong>, and <strong class="source-inline">"high_quality"</strong>. This determines the trade-off between generation speed and <span class="No-Break">audio quality.</span></li><li><strong class="bold">Loading the selected voice</strong>: The <strong class="source-inline">load_voice</strong> function from <strong class="source-inline">tortoise.utils.audio</strong> is used to load the selected voice. This function returns two items: <strong class="source-inline">voice_samples</strong> and <strong class="source-inline">conditioning_latents</strong>. These condition the TTS model to generate speech in the selected <span class="No-Break">voice’s style.</span></li><li><strong class="bold">Generating the speech</strong>: The <strong class="source-inline">tts_with_preset</strong> method of the <strong class="source-inline">tts</strong> object (an instance of the <strong class="source-inline">TextToSpeech</strong> class) is called with the text, voice samples, conditioning latents, and preset. This method synthesizes the speech based on the <span class="No-Break">given parameters.</span></li><li><strong class="bold">Saving and playing the speech</strong>: The generated speech is saved as a WAV file using the <strong class="source-inline">torchaudio.save</strong> function. The file is then played using <strong class="source-inline">IPython.display.Audio</strong> to allow you to hear the <span class="No-Break">synthesized speech.</span></li></ul><p class="list-inset">These steps enable you to create a spoken version of their text using the specific characteristics of the chosen voice, effectively using the PVS model for <span class="No-Break">speech synthesis.</span></p></li>				<li><strong class="bold">Generating speech with a random voice</strong>: This is similar to the previous step, <em class="italic">Generating speech with a selected voice</em>, but with <strong class="source-inline">voice_samples</strong> and <strong class="source-inline">conditioning_latents</strong> set to <strong class="source-inline">None</strong>, which<a id="_idIndexMarker917"/> generates speech using a <span class="No-Break">random voice:</span><pre class="source-code">
gen = tts.tts_with_preset(text, voice_samples=None, conditioning_latents=None, preset=preset)
torchaudio.save(' synthetized_voice_sample.wav', gen.squeeze(0).cpu(), 24000)
IPython.display.Audio('synthetized_voice_sample.wav')</pre></li>				<li><strong class="bold">Using a custom voice</strong>: The<a id="_idIndexMarker918"/> following code allows users to upload their WAV files (6-10 seconds long) to create a <span class="No-Break">custom voice:</span><pre class="source-code">
CUSTOM_VOICE_NAME = "custom"
import os
from google.colab import files
custom_voice_folder = f"tortoise/voices/{CUSTOM_VOICE_NAME}"
os.makedirs(custom_voice_folder)
for i, file_data in enumerate(files.upload().values()):
  with open(os.path.join(custom_voice_folder, f'{i}.wav'), 'wb') as f:
    f.write(file_data)
# Generate speech with the custom voice.
voice_samples, conditioning_latents = load_voice(CUSTOM_VOICE_NAME)
gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents,
                          preset=preset)
torchaudio.save(f'generated-{CUSTOM_VOICE_NAME}.wav', gen.squeeze(0).cpu(), 24000)
IPython.display.Audio(f'generated-{CUSTOM_VOICE_NAME}.wav')</pre><p class="list-inset">It creates a custom voice folder using <strong class="source-inline">os.makedirs</strong> and saves the uploaded files in that folder. The custom voice is then<a id="_idIndexMarker919"/> loaded and used to generate speech, similar to <em class="italic">steps 4</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">5</em></span><span class="No-Break">.</span></p></li>				<li><strong class="bold">Combining voices</strong>: The <strong class="source-inline">load_voices</strong> function loads multiple voices (in this case, <strong class="source-inline">'pat'</strong> and <strong class="source-inline">'william'</strong>). The <strong class="source-inline">tts_with_preset</strong> method combines voice samples and <a id="_idIndexMarker920"/>conditioning latents to generate speech with traits from <span class="No-Break">both voices:</span><pre class="source-code">
voice_samples, conditioning_latents = load_voices(['freeman', 'deniro'])
gen = tts.tts_with_preset("Words, once silent, now dance on digital breath, speaking volumes through the magic of text-to-speech.",
                          voice_samples=voice_samples, conditioning_latents=conditioning_latents,
                          preset=preset)
torchaudio.save('freeman_deniro.wav', gen.squeeze(0).cpu(), 24000)
IPython.display.Audio('freeman_deniro.wav')</pre></li>			</ol>
			<p>Having gained a foundational understanding of TTS in voice synthesis and explored the powerful capabilities of the TorToiSe-TTS-Fast <a id="_idIndexMarker921"/>project, we’ll turn our attention to a crucial step in preparing our data <a id="_idIndexMarker922"/>for the voice synthesizing process: converting audio files into the <span class="No-Break">LJSpeech format.</span></p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor215"/>PVS step 1 – Converting audio files into LJSpeech format</h1>
			<p>This section and the accompanying notebook, <strong class="source-inline">LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb</strong>, represent the initial step in the three-step PVS process outlined in this chapter. This step takes an audio sample of the<a id="_idIndexMarker923"/> target voice as input and processes it into the LJSpeech dataset format. The notebook demonstrates using the OZEN Toolkit and OpenAI’s Whisper to extract speech, transcribe it, and organize the data according to the LJSpeech structure. The resulting LJSpeech-formatted dataset, consisting of segmented audio files and corresponding transcriptions, serves as the input for the second step, <em class="italic">PVS step 2 – Fine-tuning a discrete variational autoencoder using the DLAS toolkit</em>, where a PVS model will be fine-tuned using <span class="No-Break">this dataset.</span></p>
			<p>An LJSpeech-formatted dataset is crucial in TTS models as it provides a standardized structure for organizing audio files and their corresponding transcriptions. By following the LJSpeech format, researchers and developers can ensure compatibility with various TTS tools and <span class="No-Break">facilitate training.</span></p>
			<p>An LJSpeech-formatted dataset refers to a specific structure and organization of audio files and their corresponding transcriptions modeled after the <strong class="bold">LJSpeech dataset</strong> (<a href="https://keithito.com/LJ-Speech-Dataset/">https://keithito.com/LJ-Speech-Dataset/</a>). The LJSpeech dataset is a public domain speech dataset that includes 13,100 short audio clips of a single speaker reading passages from seven non-fiction books, with a transcription of each clip. The audio clips vary in length and have a total duration of approximately 24 hours. When formatting a dataset for training a TTS model in the style of LJSpeech, the following structure <span class="No-Break">is recommended:</span></p>
			<ul>
				<li>Audio clips should be divided into separate files with a <span class="No-Break">corresponding transcription.</span></li>
				<li>The WAV file format should be used for the audio to avoid <span class="No-Break">compression artifacts.</span></li>
				<li>The audio clips and their transcriptions are collected in a folder <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">wavs</strong></span><span class="No-Break">.</span></li>
				<li>A metadata text file maps each audio clip to its transcription. This file should have columns delimited by a special character, typically a pipe (<strong class="source-inline">|</strong>), to separate the audio filename, the transcription, and the <span class="No-Break">normalized transcription.</span></li>
				<li>The delimiter used in the <a id="_idIndexMarker924"/>metadata file should not appear in the transcription <span class="No-Break">text itself.</span></li>
				<li>If normalized transcriptions are unavailable, the same transcription can be used for both columns, with normalization applied later in <span class="No-Break">the pipeline.</span></li>
			</ul>
			<p>The folder structure for an LJSpeech-formatted dataset would look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
/MyDataset
---├── train.txt
---├── valid.txt
---├── /wavs
---------├── 0.wav
---------├── 1.wav
        ...</pre>			<p>In the text files, entries would be formatted <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
wavs/0.wav|This is Josue Batista.
wavs/1.wav|I am the author of the book Learn OpenAI Whisper, Transform Your Understanding of Generative AI
wavs/2.wav|through robust and accurate speech processing solutions.
...</pre>			<p>The LJSpeech format is widely used because it is supported by various TTS tools, such as TorToiSe, which provides tooling for the LJSpeech dataset. Formatting a dataset this way allows for immediate model training without additional <span class="No-Break">formatting steps.</span></p>
			<p>Now that we understand the LJSpeech format and why it’s used, let’s convert our audio files into this format. By doing so, we’ll ensure that our dataset is compatible with various TTS tools and ready for training our <span class="No-Break">PVS models.</span></p>
			<p>Once you have a recording of the voice you would like to synthesize, the next step is to preprocess the audio files using the OZEN Toolkit. This toolkit simplifies extracting speech, transcribing it using Whisper, and saving the results in the LJSpeech format. It can handle both single audio files and entire folders of <span class="No-Break">audio files.</span></p>
			<p>Leveraging the OZEN Toolkit and<a id="_idIndexMarker925"/> Whisper allows us to efficiently convert our audio data into the LJSpeech format. The toolkit automates segmenting audio files, generating corresponding WAV files, and creating the necessary metadata files (<strong class="source-inline">train.txt</strong> and <strong class="source-inline">valid.txt</strong>) that map the audio files to <span class="No-Break">their transcriptions.</span></p>
			<p>Converting audio files into the LJSpeech format is a crucial skill in the voice synthesis pipeline as it ensures data compatibility and facilitates the training process. Mastering this technique will prepare you to tackle the subsequent steps, such as fine-tuning PVS models and <span class="No-Break">synthesizing speech.</span></p>
			<p>Please find and open the Colab notebook called <strong class="source-inline">LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb</strong> (<a href="https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb">https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb</a>). This notebook is based on the OZEN Toolkit project (<a href="https://github.com/devilismyfriend/ozen-toolkit">https://github.com/devilismyfriend/ozen-toolkit</a>). Given a folder of files or a single audio file, it will extract the speech, transcribe using Whisper, and save it in LJ format (segmented audio files in WAV format go in the <strong class="source-inline">wavs</strong> folder, while transcriptions go in the <strong class="source-inline">train</strong> and <strong class="source-inline">valid</strong> folders). Let’s walk through the code while explaining the steps and providing <span class="No-Break">code samples:</span></p>
			<ol>
				<li><strong class="bold">Cloning the OZEN Toolkit repository</strong>: The following command clones the OZEN Toolkit repository from GitHub, which contains the necessary scripts and utilities for processing <span class="No-Break">audio files:</span><pre class="source-code">
<strong class="bold">!git clone </strong><a href="https://github.com/devilismyfriend/ozen-toolkit">https://github.com/devilismyfriend/ozen-toolkit</a></pre></li>				<li><strong class="bold">Installing the required libraries</strong>: The following commands install the necessary libraries for audio processing, speech recognition, and text formatting. After installing<a id="_idIndexMarker926"/> the dependencies, restarting the session is recommended to ensure the installed packages are <span class="No-Break">initialized adequately:</span><pre class="source-code">
<strong class="bold">!pip install transformers</strong>
<strong class="bold">!pip install huggingface</strong>
<strong class="bold">!pip install pydub</strong>
<strong class="bold">!pip install yt-dlp</strong>
<strong class="bold">!pip install pyannote.audio</strong>
<strong class="bold">!pip install colorama</strong>
<strong class="bold">!pip install termcolor</strong>
<strong class="bold">!pip install pyfiglet</strong></pre></li>				<li><strong class="bold">Changinge the working directory</strong>: The following command changes the working directory to the cloned <span class="No-Break"><strong class="source-inline">ozen-toolkit</strong></span><span class="No-Break"> directory:</span><pre class="source-code">
<strong class="bold">%cd ozen-toolkit</strong></pre></li>				<li><strong class="bold">Downloading a sample audio file</strong>: If you do not have an audio file for synthesis, this command downloads a sample audio file from the specified URL for <span class="No-Break">demonstration purposes:</span><pre class="source-code">
<strong class="bold">!wget -nv </strong><a href="https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.mp3">https://github.com/PacktPublishing/Learn-OpenAI-Whisper/raw/main/Chapter01/Learn_OAI_Whisper_Sample_Audio01.mp3</a></pre></li>				<li><strong class="bold">Uploading custom audio files</strong>: If you have your audio file, this code block allows users to upload their audio files to the Colab environment. It creates a directory in <strong class="source-inline">/content/ozen-toolkit</strong> to store the uploaded files and saves them in <span class="No-Break">that directory:</span><pre class="source-code">
import os
from google.colab import files
custom_voice_folder = "./myaudiofile"
os.makedirs(custom_voice_folder, exist_ok=True)  # Create the directory if it doesn't exist
for filename, file_data in files.upload().items():
    with open(os.path.join(custom_voice_folder, filename), 'wb') as f:
        f.write(file_data)
%ls -l "$PWD"/{*,.*}</pre></li>				<li><strong class="bold">Creating a configuration file</strong>: The following code section creates a configuration file named <strong class="source-inline">"config.ini"</strong> using the <strong class="source-inline">configparser</strong> library. It defines various settings, such<a id="_idIndexMarker927"/> as the Hugging Face API key, Whisper model, device, diarization and segmentation models, validation ratio, and <span class="No-Break">segmentation parameters:</span><pre class="source-code">
import configparser
config = configparser.ConfigParser()
config['DEFAULT'] = {
    'hf_token': '&lt;Your HF API key&gt;',
    'whisper_model': 'openai/whisper-medium',
    'device': 'cuda',
    'diaization_model': 'pyannote/speaker-diarization',
    'segmentation_model': 'pyannote/segmentation',
    'valid_ratio': '0.2',
    'seg_onset': '0.7',
    'seg_offset': '0.55',
    'seg_min_duration': '2.0',
    'seg_min_duration_off': '0.0'
}
with open('config.ini', 'w') as configfile:
    config.write(configfile)</pre></li>				<li><strong class="bold">Running the OZEN script</strong>: This command runs the <strong class="source-inline">ozen.py</strong> script with the sample audio file as an <a id="_idIndexMarker928"/>argument (or the file <span class="No-Break">you uploaded):</span><pre class="source-code">
<strong class="bold">!python ozen.py Learn_OAI_Whisper_Sample_Audio01.mp3</strong></pre><p class="list-inset"><strong class="bold">IMPORTANT</strong>: <strong class="source-inline">ozen.py</strong> requires Hugging Face’s <strong class="source-inline">pyannote/segmentation</strong> model. This is a gated model; you MUST request access before attempting to run the next cell. Thankfully, getting access is relatively straightforward and fast. Here are <span class="No-Break">the steps:</span></p><ol><li class="upper-roman">You must already have a Hugging Face account; if you do not have one, see the instructions in the notebook for <a href="B21020_03.xhtml#_idTextAnchor088"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>: <span class="No-Break"><strong class="source-inline">LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb</strong></span><span class="No-Break"> (</span><a href="https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter03/LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb"><span class="No-Break">https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter03/LOAIW_ch03_working_with_audio_data_via_Hugging_Face.ipynb</span></a><span class="No-Break">)</span></li><li class="upper-roman">Visit <a href="https://hf.co/pyannote/segmentation">https://hf.co/pyannote/segmentation</a> to accept the <span class="No-Break">user conditions:</span></li></ol></li>			</ol>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/Figure_9.3_B21020.jpg" alt="Figure 9.3 – The pyannote/segmentation gated model on Hugging Face" width="986" height="678"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – The pyannote/segmentation gated model on Hugging Face</p>
			<ol>
				<li class="upper-roman" value="3">Run the cell after ensuring you have access to the pyannote/segmentation model. The <strong class="source-inline">ozen.py</strong> script <a id="_idIndexMarker929"/>processes the audio file, extracts speech, transcribes it using Whisper, and saves the output in the LJSpeech format. The script saves the DJ format files in a folder called <strong class="source-inline">ozen-toolkit/output/&lt;audio file name + timestamp&gt;/</strong>. Her<a id="_idTextAnchor216"/>e is an example of the expected <span class="No-Break">file structure:</span></li>
			</ol>
			<pre class="source-code">
ozen-toolkit/output/
---├── Learn_OAI_Whisper_Sample_Audio01.mp3_2024_03_16-16_36/
------------------├── valid.txt
------------------├── train.txt
------------------├── wavs/
--------------------------├── 0.wav
--------------------------├── 1.wav
--------------------------├── 2.wav</pre>			<ol>
				<li value="8"><strong class="bold">Mounting Google Drive</strong>: The following lines mount your Google Drive to the Colab environment, allowing access to<a id="_idIndexMarker930"/> the drive for saving checkpoints and <span class="No-Break">loading datasets:</span><pre class="source-code">
from google.colab import drive
drive.mount('/content/gdrive')</pre></li>				<li><strong class="bold">Copying the output to Google Drive</strong>: The following command copies the processed output files from the <strong class="source-inline">ozen-toolkit/output</strong> directory to your Google Drive. After running the cell, go to your Google Drive using a web browser, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.4</em>; you will see a directory called <strong class="source-inline">output</strong> with the DJ format dataset files <span class="No-Break">in it:</span><pre class="source-code">
<strong class="bold">%cp -r /content/ozen-toolkit/output/ /content/gdrive/MyDrive/ozen-toolkit/output/</strong></pre><p class="list-inset">Here’s <span class="No-Break">the output:</span></p></li>			</ol>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/Figure_9.4_B21020.jpg" alt="Figure 9.4 – Identifying the location of the DJ format files from the ozen.py script" width="737" height="532"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Identifying the location of the DJ format files from the ozen.py script</p>
			<p class="list-inset">After running the cell, go to your Google Drive using a web browser; you will see a directory called <strong class="source-inline">output</strong> with <a id="_idIndexMarker931"/>the DJ format dataset files <span class="No-Break">in it:</span></p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/Figure_9.5_B21020.jpg" alt="Figure 9.5 – Example of the DJ format output folder in Google Colab" width="756" height="300"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Example of the DJ format output folder in Google Colab</p>
			<p>The Python code in the <strong class="source-inline">LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb</strong> notebook demonstrates setting up the environment, installing dependencies, configuring the OZEN Toolkit, processing audio files using Whisper, and saving the output in the LJSpeech format. It provides a streamlined workflow for preparing audio data for further analysis or use in <span class="No-Break">downstream tasks.</span></p>
			<p>With our audio data now converted into the LJSpeech format, we are well-prepared to embark on the following critical stage of the voice synthesis journey: fine-tuning a PVS model using the powerful DLAS toolkit. The <strong class="source-inline">LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb</strong> notebook will cover this process in detail in the next section. By leveraging the DLAS toolkit’s comprehensive features and the structured LJSpeech dataset, we can create a personalized voice model that captures the unique characteristics of our target voice with remarkable accuracy <span class="No-Break">and naturalness.</span></p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor217"/>PVS step 2 – Fine-tuning a PVS model with the DLAS toolkit</h1>
			<p>Fine-tuning a PVS model is<a id="_idIndexMarker932"/> a critical step in creating personalized voices that<a id="_idIndexMarker933"/> capture the unique characteristics of a voice. To achieve high-quality results, utilizing a robust framework that leverages state-of-the-art techniques and provides flexibility in customizing the training process is essential. The DLAS toolkit emerges as a comprehensive solution for fine-tuning PVS models, offering a range of features <span class="No-Break">and capabilities.</span></p>
			<p>Before starting the fine-tuning process, ensuring that the necessary components and resources are in place is crucial. This includes setting up a suitable training environment, such as Google Colab, which provides access to powerful GPUs and sufficient RAM to handle the computational demands of PVS models. Checking the availability and compatibility of NVIDIA GPUs is vital to <a id="_idIndexMarker934"/>ensuring optimal performance <span class="No-Break">during training.</span></p>
			<p>The dataset preparation phase is another essential aspect of fine-tuning a PVS model. The DLAS toolkit requires a specific<a id="_idIndexMarker935"/> repository structure and dependencies, which must be cloned and installed before proceeding. Additionally, pre-trained model <a id="_idIndexMarker936"/>checkpoints, such as the <strong class="bold">discrete variational autoencoder</strong> (<strong class="bold">dVAE</strong>), play a crucial role in learning a discrete latent representation of the speech data. Verifying the integrity of these checkpoints is necessary to accelerate the fine-tuning process and achieve <span class="No-Break">better results.</span></p>
			<p>Selecting appropriate hyperparameters based on the dataset’s size is critical in fine-tuning a PVS model. The DLAS toolkit offers intelligent suggestions for hyperparameters, such as batch sizes, learning rate decay steps, and validation frequencies, all of which consider the specific characteristics of the dataset. Understanding how these hyperparameters are calculated and their impact on the training process is essential for achieving <span class="No-Break">optimal results.</span></p>
			<p>Customization is another critical aspect of fine-tuning a PVS model using the DLAS toolkit. Researchers and developers often have specific requirements and preferences for training settings, such as experiment names, dataset names, and turning certain features on or off. The DLAS toolkit provides flexibility in modifying these settings, allowing for tailored fine-tuning processes that align with specific needs <span class="No-Break">and goals.</span></p>
			<p>The DLAS toolkit utilizes a YAML configuration file to ensure the fine-tuning process is configured according to the desired specifications. This file serves as a blueprint for the training process, specifying various parameters and settings. The toolkit applies the customized training settings to the YAML file using sophisticated <strong class="source-inline">sed</strong> commands, ensuring that the fine-tuning process is tailored to the specific requirements and enables the reproducibility of the experiments (<strong class="source-inline">sed</strong> stands for Stream Editor, a powerful command-line utility that’s used for parsing and transforming text using a simple, compact <span class="No-Break">programming language).</span></p>
			<p>With the configuration file ready, the training process can be initiated by running the <strong class="source-inline">train.py</strong> script provided by the DLAS toolkit. This script leverages the power of GPUs to efficiently fine-tune the PVS model, utilizing optimization algorithms and loss functions to guide the learning process. Monitoring the training progress and evaluating the model’s performance using appropriate metrics is crucial for ensuring the quality of the fine-tuned <span class="No-Break">PVS model.</span></p>
			<p>Finally, saving and exporting the <a id="_idIndexMarker937"/>fine-tuned PVS model is essential for future use and deployment. The DLAS toolkit provides convenient methods to store the trained model checkpoints and experiment files, ensuring data persistence and facilitating research collaboration. Proper management and organization of the fine-tuned models are critical for seamless integration into various applications, such as virtual assistants, audiobook narration, and personalized <span class="No-Break">voice interfaces.</span></p>
			<p>Researchers and developers<a id="_idIndexMarker938"/> can create personalized voices that capture the nuances and characteristics by understanding the components, processes, and considerations involved in fine-tuning a PVS model using the DLAS toolkit. The ability to customize the training process, select appropriate hyperparameters, and leverage pre-trained checkpoints empowers users to achieve high-quality results <a id="_idTextAnchor218"/>and explore exciting possibilities in <span class="No-Break">voice synthesis.</span></p>
			<p>Please find and open the Colab notebook called <span class="No-Break"><strong class="source-inline">LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb</strong></span><span class="No-Break"> (</span><a href="https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb"><span class="No-Break">https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb</span></a><span class="No-Break">).</span></p>
			<p>This notebook fine-tunes a PVS model using the DLAS toolkit. It is based on the <em class="italic">TorToiSe fine-tuning with DLAS</em> project by James Betker (<a href="https://github.com/152334H/DL-Art-School">https://github.com/152334H/DL-Art-School</a>). I cloned and modified the code to run on Google Colab and leveraged an NVIDIA GPU <span class="No-Break">for training.</span></p>
			<p>Let’s walk through the steps in the <span class="No-Break"><strong class="source-inline">LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb</strong></span><span class="No-Break"> notebook:</span></p>
			<ol>
				<li><strong class="bold">Checking the GPU</strong>: The code first checks whether an NVIDIA GPU is available using the <strong class="source-inline">nvidia-smi</strong> command. It prints out the GPU’s information if <span class="No-Break">it’s connected:</span><pre class="source-code">
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') &gt;= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)</pre></li>				<li><strong class="bold">Checking virtual memory</strong>: It then <a id="_idIndexMarker939"/>checks the available RAM on the runtime using the <strong class="source-inline">psutil</strong> library. It prints<a id="_idIndexMarker940"/> a message if it’s using a <span class="No-Break">high-RAM runtime:</span><pre class="source-code">
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))
if ram_gb &lt; 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')</pre></li>				<li><strong class="bold">Mounting Google Drive</strong>: The following code mounts your Google Drive to save trained checkpoints and load <span class="No-Break">the dataset:</span><pre class="source-code">
from google.colab import drive
drive.mount('/content/gdrive')</pre></li>				<li><strong class="bold">Installing the requirements</strong>: It clones the DLAS repository, downloads pre-trained model checkpoints, and installs the <span class="No-Break">required dependencies:</span><pre class="source-code">
<strong class="bold">!git clone </strong><a href="https://github.com/josuebatista/DL-Art-School.git">https://github.com/josuebatista/DL-Art-School.git</a>
<strong class="bold">%cd DL-Art-School</strong>
<strong class="bold">!wget </strong><a href="https://huggingface.co/Gatozu35/tortoise-tts/resolve/main/dvae.pth">https://huggingface.co/Gatozu35/tortoise-tts/resolve/main/dvae.pth</a><strong class="bold"> -O experiments/dvae.pth</strong>
<strong class="bold">!wget </strong><a href="https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth">https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth</a><strong class="bold"> -O experiments/autoregressive.pth</strong>
<strong class="bold">!pip install -r codes/requirements.laxed.txt</strong></pre><p class="list-inset">After installing the dependencies, restarting the session is recommended to ensure the installed packages are properly initialized. We must also verify the integrity of the dVAE <a id="_idIndexMarker941"/>checkpoint. A dVAE is a component of the PVS model that helps learn a discrete latent representation of the data. The <strong class="source-inline">sha256sum</strong> command calculates the checksum. If the <strong class="source-inline">grep</strong> command does not find a match to the expected checksum value, the integrity <a id="_idIndexMarker942"/>check fails and reports an error message. Thus, the entire command line ensures that the checkpoint file has not been corrupted <span class="No-Break">or altered:</span></p><pre class="source-code"><strong class="bold">!sha256sum /content/DL-Art-School/experiments/dvae.pth | grep a990825371506c16bcf0e8167bf24ccf82f65bb6aldbcbfcf058d76f9b197e35 || echo "SOMETHING IS WRONG WITH THE CHECKPOINT; REPORT THIS AS A GITHUB ISSUE AND DO NOT PROCEED"</strong></pre><p class="list-inset">The preceding command calculates the SHA-256 checksum of the checkpoint file and compares it with the expected checksum. If the checksums match, the integrity of the file is confirmed. If they do not match, the user is advised to report the issue on GitHub and not <span class="No-Break">proceed further.</span></p></li>				<li><strong class="bold">Calculating hyperparameters</strong>: This section automatically calculates suggested hyperparameters for training based on the provided dataset sizes. It adjusts the batch sizes to minimize leftover samples in each epoch, calculates the number of steps per epoch, and determines the frequencies for learning rate decay, validation, and <a id="_idIndexMarker943"/>checkpoint saving. Hyperparameters are crucial as they directly control the training algorithm’s behavior and significantly impact the <span class="No-Break">model’s performance.</span><p class="list-inset">To find the path to <strong class="source-inline">Dataset_Training_Path</strong> and <strong class="source-inline">ValidationDataset_Training_Path</strong>, click on Google Colab’s <strong class="bold">Files</strong> option and search Google Drive for the directory where the DJ-format datasets were stored in the <a id="_idIndexMarker944"/>previous notebook. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.6</em> shows an example of where the DJ-format dataset is found. Keep in mind that <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.6</em> is just an example. Do not search for that literal name. Instead, you must search for the directory name you set while creating the <span class="No-Break">DJ-formatted files:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/Figure_9.6_B21020.jpg" alt="Figure 9.6 – Example of searching for the DJ format dataset in the output directory created in the previous notebook" width="640" height="335"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Example of searching for the DJ format dataset in the output directory created in the previous notebook</p>
			<p class="list-inset">The following is the entire script that performs the hyperparameter calculation. An explanation of how it works is <a id="_idIndexMarker945"/>provided after the <span class="No-Break">code listing:</span></p>
			<pre class="source-code">
from pathlib import Path
from math import ceil
DEFAULT_TRAIN_BS = 64
DEFAULT_VAL_BS = 32
Dataset_Training_Path = "/content/gdrive/MyDrive/Generative_AI/Deep_Fakes_Voice/output/Learn_OAI_Whisper_Sample_Audio01.mp3_2024_03_16-16_36/train.txt" #@param {type:"string"}
ValidationDataset_Training_Path = "/content/gdrive/MyDrive/Generative_AI/Deep_Fakes_Voice/output/Learn_OAI_Whisper_Sample_Audio01.mp3_2024_03_16-
if Dataset_Training_Path == ValidationDataset_Training_Path:
  print("WARNING: training dataset path == validation dataset path!!!")
  print("\tThis is technically okay but will make all of the validation metrics useless. ")
  print("it will also SUBSTANTIALLY slow down the rate of training, because validation datasets are supposed to be much smaller than training ones.")
def txt_file_lines(p: str) -&gt; int:
  return len(Path(p).read_text().strip().split('\n'))
training_samples = txt_file_lines(Dataset_Training_Path)
val_samples = txt_file_lines(ValidationDataset_Training_Path)
if training_samples &lt; 128: print("WARNING: very small dataset! the smallest dataset tested thus far had ~200 samples.")
if val_samples &lt; 20: print("WARNING: very small validation dataset! val batch size will be scaled down to account")
def div_spillover(n: int, bs: int) -&gt; int: # returns new batch size
  epoch_steps,remain = divmod(n,bs)
  if epoch_steps*2 &gt; bs: return bs # don't bother optimising this stuff if epoch_steps are high
  if not remain: return bs # unlikely but still
  if remain*2 &lt; bs: # "easier" to get rid of remainder -- should increase bs
    target_bs = n//epoch_steps
  else: # easier to increase epoch_steps by 1 -- decrease bs
    target_bs = n//(epoch_steps+1)
  assert n%target_bs &lt; epoch_steps+2 # should be very few extra
  return target_bs
if training_samples &lt; DEFAULT_TRAIN_BS:
  print("WARNING: dataset is smaller than a single batch. This will almost certainly perform poorly. Trying anyway")
  train_bs = training_samples
else:
  train_bs = div_spillover(training_samples, DEFAULT_TRAIN_BS)
if val_samples &lt; DEFAULT_VAL_BS:
  val_bs = val_samples
else:
  val_bs = div_spillover(val_samples, DEFAULT_VAL_BS)
steps_per_epoch = training_samples//train_bs
lr_decay_epochs = [20, 40, 56, 72]
lr_decay_steps = [steps_per_epoch * e for e in lr_decay_epochs]
print_freq = min(100, max(20, steps_per_epoch))
val_freq = save_checkpoint_freq = print_freq * 3
print("===CALCULATED SETTINGS===")
print(f'{train_bs=} {val_bs=}')
print(f'{val_freq=} {lr_decay_steps=}')
print(f'{print_freq=} {save_checkpoint_freq=}')</pre>			<p class="list-inset">Let’s break down the purpose and steps in <span class="No-Break">this section:</span></p>
			<ul>
				<li>The code imports the <a id="_idIndexMarker946"/>necessary libraries: <strong class="source-inline">Path</strong> from <strong class="source-inline">pathlib</strong> to work with files and<a id="_idIndexMarker947"/> directories, and <strong class="source-inline">ceil</strong>, a function in the built-in <strong class="source-inline">math</strong> module for rounding numbers up to the <span class="No-Break">nearest integer.</span></li>
				<li>It defines default training <a id="_idIndexMarker948"/>and validation batch size values: <strong class="source-inline">DEFAULT_TRAIN_BS = 64 and DEFAULT_VAL_BS = </strong><span class="No-Break"><strong class="source-inline">32</strong></span><span class="No-Break">.</span></li>
				<li>You are prompted to provide the paths for the training and validation datasets: <strong class="source-inline">Dataset_Training_Path</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">ValidationDataset_Training_Path</strong></span><span class="No-Break">.</span></li>
				<li>The code checks whether the training and validation dataset paths are identical. If they are, it prints a warning message indicating that validation metrics will be useless and the training rate will be <span class="No-Break">substantially slowed.</span></li>
				<li>The code defines a helper function, <strong class="source-inline">txt_file_lines</strong>, that takes a file path as input and returns the number of lines in <span class="No-Break">the file.</span></li>
				<li>It calculates the training and validation samples by calling <strong class="source-inline">txt_file_lines</strong> with the respective <span class="No-Break">dataset paths.</span></li>
				<li>The code prints warning messages if the dataset sizes are small: less than 128 for training and less than 20 <span class="No-Break">for validation.</span></li>
				<li>It defines a helper function called <strong class="source-inline">div_spillover</strong> that takes the number of samples (<strong class="source-inline">n</strong>) and batch size (<strong class="source-inline">bs</strong>) as input and returns an adjusted batch size to minimize the number of leftover samples in <span class="No-Break">each epoch.</span></li>
				<li>The code calculates the<a id="_idIndexMarker949"/> training batch size (<strong class="source-inline">train_bs</strong>) based on the number of training samples. If the number of training samples is smaller than <strong class="source-inline">DEFAULT_TRAIN_BS</strong>, it sets <strong class="source-inline">train_bs</strong> to the number of training samples and prints a warning message. Otherwise, it calls <strong class="source-inline">div_spillover</strong> with the number of training samples and <strong class="source-inline">DEFAULT_TRAIN_BS</strong> to calculate an adjusted <span class="No-Break">batch size.</span></li>
				<li>Similarly, it calculates the validation batch size (<strong class="source-inline">val_bs</strong>) based on the number of validation samples. If the number of validation samples is smaller than <strong class="source-inline">DEFAULT_VAL_BS</strong>, it sets <strong class="source-inline">val_bs</strong> to the number of validation samples. Otherwise, it calls <strong class="source-inline">div_spillover</strong> with the number of validation samples and <strong class="source-inline">DEFAULT_VAL_BS</strong> to calculate an adjusted <span class="No-Break">batch size.</span></li>
				<li>The code calculates the number of steps per epoch (<strong class="source-inline">steps_per_epoch</strong>) by dividing the number <a id="_idIndexMarker950"/>of training samples by the training <span class="No-Break">batch size.</span></li>
				<li>It defines the epochs at which learning rate decay should occur via <strong class="source-inline">lr_decay_epochs = [20, 40, </strong><span class="No-Break"><strong class="source-inline">56, 72]</strong></span><span class="No-Break">.</span></li>
				<li>The code calculates the corresponding steps for learning rate decay (<strong class="source-inline">lr_decay_steps</strong>) by multiplying <strong class="source-inline">steps_per_epoch</strong> with each value <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">lr_decay_epochs</strong></span><span class="No-Break">.</span></li>
				<li>It calculates the frequency for printing training progress (<strong class="source-inline">print_freq</strong>) based on the number of steps per epoch, with a minimum of 20 and a maximum <span class="No-Break">of 100.</span></li>
				<li>The code sets the frequency for validation and saving checkpoints (<strong class="source-inline">val_freq</strong> and <strong class="source-inline">save_checkpoint_freq</strong>) to three times the <span class="No-Break"><strong class="source-inline">print_freq</strong></span><span class="No-Break"> value.</span></li>
				<li>Finally, it prints the calculated settings: <strong class="source-inline">train_bs</strong>, <strong class="source-inline">val_bs</strong>, <strong class="source-inline">val_freq</strong>, <strong class="source-inline">lr_decay_steps</strong>, <strong class="source-inline">print_freq</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">save_checkpoint_freq</strong></span><span class="No-Break">.</span></li>
				<li>From the creator of the<a id="_idIndexMarker951"/> DLAS trainer, the values of <strong class="source-inline">print_freq</strong>, <strong class="source-inline">val_freq</strong>, and <strong class="source-inline">save_checkpoint_freq</strong> should all be adjusted to the dataset’s size. The Python code states the recommended values: <strong class="source-inline">val_freq == save_checkpoint_freq == print_freq*3</strong>; <strong class="source-inline">print_freq == min(epoch_steps,100)</strong>. Again, these are recommendations; I encourage you to experiment with different ones and compare results for optimal <span class="No-Break">hyperparameter settings.</span></li>
			</ul>
			<p class="list-inset">By calculating these hyperparameters, the code aims to provide reasonable default values that can be used to train the PVS model. However, we can override these calculated values in the subsequent sections <span class="No-Break">if needed.</span></p>
			<ol>
				<li value="6"><strong class="bold">Training settings</strong>: This section allows us to customize the training settings according to their requirements and available resources. It provides flexibility in naming the experiment, specifying <a id="_idIndexMarker952"/>dataset names, turning certain features on or off, and overriding calculated settings. The code also includes notes and warnings to guide you in making appropriate choices based on the system’s storage and <span class="No-Break">computational capabilities:</span><pre class="source-code">
Experiment_Name = "Learn_OAI_Whisper_20240316"
Dataset_Training_Name= "TestDataset"
ValidationDataset_Name = "TestValidation"
SaveTrainingStates = False
Keep_Last_N_Checkpoints = 0
Fp16 = False
Use8bit = True
TrainingRate = "1e-5"
TortoiseCompat = False
TrainBS = ""
ValBS = ""
ValFreq = ""
LRDecaySteps = ""
PrintFreq = ""
SaveCheckpointFreq = ""
def take(orig, override):
  if override == "": return orig
  return type(orig)(override)
train_bs = take(train_bs, TrainBS)
val_bs = take(val_bs, ValBS)
val_freq = take(val_freq, ValFreq)
lr_decay_steps = eval(LRDecaySteps) if LRDecaySteps else lr_decay_steps
print_freq = take(print_freq, PrintFreq)
save_checkpoint_freq = take(save_checkpoint_freq, SaveCheckpointFreq)
assert len(lr_decay_steps) == 4
gen_lr_steps = ', '.join(str(v) for v in lr_decay_steps)</pre><p class="list-inset">Let’s break down the purpose and steps in <span class="No-Break">this section:</span></p><ul><li>You can specify the <a id="_idIndexMarker953"/>following <span class="No-Break">training settings:</span><ul><li><strong class="source-inline">Experiment_Name</strong>: A string to <a id="_idIndexMarker954"/>name <span class="No-Break">the experiment.</span></li><li><strong class="source-inline">Dataset_Training_Name</strong>: A string to name the <span class="No-Break">training dataset.</span></li><li><strong class="source-inline">ValidationDataset_Name</strong>: A string to name the <span class="No-Break">validation dataset.</span></li><li><strong class="source-inline">SaveTrainingStates</strong>: A Boolean to indicate whether to save <span class="No-Break">training states.</span></li><li><strong class="source-inline">Keep_Last_N_Checkpoints</strong>: An integer slider to specify the number of checkpoints to keep. Setting it to 0 means keeping all <span class="No-Break">saved models.</span></li></ul></li><li>The code provides notes <span class="No-Break">and warnings:</span><ul><li>It mentions that keeping all saved models (setting <strong class="source-inline">Keep_Last_N_Checkpoints</strong> to 0) could potentially cause <span class="No-Break">out-of-storage issues.</span></li><li>Without training states, <strong class="bold">each model takes up approximately 1.6 GB</strong> in Google Drive, giving you around 50 GB of <span class="No-Break">free space.</span></li><li>With training states, each<a id="_idIndexMarker955"/> model (including the state) takes up approximately 4.9 GB, and Colab may crash if there are around 10 <span class="No-Break">undeleted checkpoints.</span></li></ul></li><li>Other <span class="No-Break">training parameters:</span><ul><li><strong class="source-inline">Fp16</strong>: A Boolean to turn 16-bit floating-point precision on <span class="No-Break">or off.</span></li><li><strong class="source-inline">Use8bit</strong>: A Boolean to turn 8-bit precision on <span class="No-Break">or off.</span></li><li><strong class="source-inline">TrainingRate</strong>: A string to specify the <span class="No-Break">learning rate.</span></li><li><strong class="source-inline">TortoiseCompat</strong>: A Boolean to turn compatibility with the TorToiSe model on or off. Enabling it to introduce breaking changes to the training process and then disabling it is recommended to reproduce <span class="No-Break">older models.</span></li></ul></li><li>Calculated <span class="No-Break">settings override:</span><ul><li>You can manually override the calculated settings from the previous cell by specifying values for <strong class="source-inline">TrainBS</strong>, <strong class="source-inline">ValBS</strong>, <strong class="source-inline">ValFreq</strong>, <strong class="source-inline">LRDecaySteps</strong>, <strong class="source-inline">PrintFreq</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">SaveCheckpointFreq</strong></span></li><li>If left blank, the calculated<a id="_idIndexMarker956"/> defaults from the previous cell will <span class="No-Break">be used</span></li></ul></li><li>The code defines a <strong class="source-inline">take</strong> function to override the calculated settings. If the override is an empty string, it returns the original value; otherwise, it returns the <span class="No-Break">overridden value.</span></li><li>The code assigns the overridden or default values to the corresponding variables: <strong class="source-inline">train_bs</strong>, <strong class="source-inline">val_bs</strong>, <strong class="source-inline">val_freq</strong>, <strong class="source-inline">lr_decay_steps</strong>, <strong class="source-inline">print_freq</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">save_checkpoint_freq</strong></span><span class="No-Break">.</span></li><li>Finally, the code prompts you to run the cell after editing <span class="No-Break">the settings.</span></li></ul></li>				<li><strong class="bold">Applying the the settings</strong>: This <a id="_idIndexMarker957"/>section applies the user-defined settings from <em class="italic">step 6</em> to a fresh YAML configuration file using <span class="No-Break"><strong class="source-inline">sed</strong></span><span class="No-Break"> commands:</span><pre class="source-code">
%cd /content/DL-Art-School
# !wget <a href="https://raw.githubusercontent.com/152334H/DL-Art-School/master/experiments/EXAMPLE_gpt.yml">https://raw.githubusercontent.com/152334H/DL-Art-School/master/experiments/EXAMPLE_gpt.yml</a> -O experiments/EXAMPLE_gpt.yml
!wget <a href="https://raw.githubusercontent.com/josuebatista/DL-Art-School/master/experiments/EXAMPLE_gpt.yml">https://raw.githubusercontent.com/josuebatista/DL-Art-School/master/experiments/EXAMPLE_gpt.yml</a> -O experiments/EXAMPLE_gpt.yml
import os
%cd /content/DL-Art-School
!sed -i 's/batch_size: 128/batch_size: '"$train_bs"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's/batch_size: 64/batch_size: '"$val_bs"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's/val_freq: 500/val_freq: '"$val_freq"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's/500, 1000, 1400, 1800/'"$gen_lr_steps"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's/print_freq: 100/print_freq: '"$print_freq"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's/save_checkpoint_freq: 500/save_checkpoint_freq: '"$save_checkpoint_freq"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's+CHANGEME_validation_dataset_name+'"$ValidationDataset_Name"'+g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's+CHANGEME_path_to_validation_dataset+'"$ValidationDataset_Training_Path"'+g' ./experiments/EXAMPLE_gpt.yml
if(Fp16==True):
  os.system("sed -i 's+fp16: false+fp16: true+g' ./experiments/EXAMPLE_gpt.yml")
!sed -i 's/use_8bit: true/use_8bit: '"$Use8bit"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's/disable_state_saving: true/disable_state_saving: '"$SaveTrainingStates"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's/tortoise_compat: True/tortoise_compat: '"$TortoiseCompat"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's/number_of_checkpoints_to_save: 0/number_of_checkpoints_to_save: '"$Keep_Last_N_Checkpoints"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's/CHANGEME_training_dataset_name/'"$Dataset_Training_Name"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's/CHANGEME_your_experiment_name/'"$Experiment_Name"'/g' ./experiments/EXAMPLE_gpt.yml
!sed -i 's+CHANGEME_path_to_training_dataset+'"$Dataset_Training_Path"'+g' ./experiments/EXAMPLE_gpt.yml
if (not TrainingRate=="1e-5"):
  os.system("sed -i 's+!!float 1e-5 # CHANGEME:+!!float '" + TrainingRate + "' #+g' ./experiments/EXAMPLE_gpt.yml")</pre><p class="list-inset">Let’s break down the <a id="_idIndexMarker958"/>purpose and steps in <span class="No-Break">this section:</span></p><ul><li>The code changes the current directory to <strong class="source-inline">/content/DL-Art-School</strong> using the <strong class="source-inline">%cd</strong> <span class="No-Break">magic command</span></li><li>It downloads a<a id="_idIndexMarker959"/> fresh YAML configuration file named <strong class="source-inline">EXAMPLE_gpt.yml</strong> from the GitHub repository, <strong class="source-inline">152334H/DL-Art-School</strong>, using the <strong class="source-inline">wget</strong> command and saves it in the <span class="No-Break"><strong class="source-inline">experiments</strong></span><span class="No-Break"> directory</span></li><li>The code then uses a series of <strong class="source-inline">sed</strong> commands to modify the values in the <strong class="source-inline">EXAMPLE_gpt.yml</strong> file based on the <span class="No-Break">user-defined settings:</span><ul><li>It replaces the <strong class="source-inline">batch_size</strong> values for training and validation with the values stored in the <strong class="source-inline">$train_bs</strong> and <strong class="source-inline">$val_bs</strong> <span class="No-Break">variables, respectively</span></li><li>It updates the <strong class="source-inline">val_freq</strong> value with the value stored <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">$val_freq</strong></span></li><li>It replaces the learning rate decay steps with the values stored <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">$gen_lr_steps</strong></span></li><li>It updates the <strong class="source-inline">print_freq</strong> and <strong class="source-inline">save_checkpoint_freq</strong> values with the corresponding values stored in the <strong class="source-inline">$print_freq</strong> and <strong class="source-inline">$</strong><span class="No-Break"><strong class="source-inline">save_checkpoint_freq</strong></span><span class="No-Break"> variables</span></li></ul></li><li>The code replaces placeholders in the YAML file with the <span class="No-Break">user-defined values</span></li><li>Finally, if <strong class="source-inline">TrainingRate</strong> is not equal to the default value of <strong class="source-inline">1e-5</strong>, the code uses <strong class="source-inline">sed</strong> to replace the placeholder of <strong class="source-inline">CHANGEME:</strong> with the user-defined <strong class="source-inline">TrainingRate</strong> value in the <span class="No-Break">YAML file</span></li></ul><p class="list-inset">The training process can be customized according to your requirements by modifying the YAML file with the user-specified values. This ensures the training process is configured based on <a id="_idIndexMarker960"/>your preferences and <span class="No-Break">dataset specifications.</span></p></li>				<li><strong class="bold">Training</strong>: The code starts <a id="_idIndexMarker961"/>training by running the <strong class="source-inline">train.py</strong> script with the configured YAML file. Press the stop button for this cell when you are satisfied with the results and have seen the <span class="No-Break">following output:</span><pre class="source-code">
INFO:base:Saving models and training states</pre><p class="list-inset"><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.7</em> shows an example of the output and the <strong class="source-inline">60_gtp.pht</strong> checkpoint as it looks in Google Colab’s <span class="No-Break"><strong class="bold">Files</strong></span><span class="No-Break"> interface:</span></p></li>			</ol>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/Figure_9.7_B21020.jpg" alt="Figure 9.7 – Example of the checkpoint file at the 60-epoch mark" width="1210" height="351"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Example of the checkpoint file at the 60-epoch mark</p>
			<p class="list-inset">If your training run saves many models, you might exceed the storage limits on the Google Colab runtime. To <a id="_idIndexMarker962"/>prevent this, try to delete old checkpoints in <strong class="source-inline">/content/DL-Art-School/experiments/$Experiment_Name/(models|training_state)/</strong> via the file explorer panel as the training runs. Resuming training after a crash requires config editing, so try not to let <span class="No-Break">that happen:</span></p>
			<pre class="source-code">
<strong class="bold">%cd /content/DL-Art-School/codes</strong>
<strong class="bold">!python3 train.py -opt ../experiments/EXAMPLE_gpt.yml</strong></pre>			<ol>
				<li value="9"><strong class="bold">Exporting to Google Drive</strong>: After training, the code<a id="_idIndexMarker963"/> allows you to copy the <strong class="source-inline">experiments</strong> folder to Google Drive <span class="No-Break">for persistence:</span><pre class="source-code">
<strong class="bold">!cp -r /content/DL-Art-School/experiments/$Experiment_Name /content/gdrive/MyDrive/</strong></pre><p class="list-inset">After running the cell, go to your Google Drive using a web browser; you will see a directory with the same name as the value of the <span class="No-Break"><strong class="source-inline">Experiment_Name</strong></span><span class="No-Break"> variable:</span></p></li>			</ol>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/Figure_9.8_B21020.jpg" alt="Figure 9.8 – Example of the folder in Google Drive containing DLAS checkpoint files" width="625" height="264"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.8 – Example of the folder in Google Drive containing DLAS checkpoint files</p>
			<p class="list-inset">You will find the<a id="_idIndexMarker964"/> model checkpoints in the <strong class="source-inline">&lt;Experiment_Name&gt;/models</strong> folder – that is, the files with the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">pth</strong></span><span class="No-Break"> extension:</span></p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/Figure_9.9_B21020.jpg" alt="Figure 9.9 – Example of the DLAS checkpoint file. In the next step, you will need a checkpoint file to synthesize a fine-tuned PVS model" width="999" height="292"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.9 – Example of the DLAS checkpoint file. In the next step, you will need a checkpoint file to synthesize a fine-tuned PVS model</p>
			<p>This concludes our overview of the PVS fine-tuning process using the DLAS toolkit. That <strong class="source-inline">.pth</strong> file is the fine-tuned PVS model we<a id="_idIndexMarker965"/> just created with DLAS. In the next step, we will use that file to synthesize the voice <span class="No-Break">using TorToiSe-TTS-Fast.</span></p>
			<p class="callout-heading">Fine-tuning PVS models – Hyperparameters versus dataset size</p>
			<p class="callout">When fine-tuning PVS models, it’s essential to consider the relationship between hyperparameters and dataset size. The Google Colab training notebook automatically suggests appropriate hyperparameters based on the provided dataset size to ensure optimal performance <span class="No-Break">and results.</span></p>
			<p class="callout">One key aspect to remember is that the number of steps per epoch (<strong class="source-inline">epoch_steps</strong>) is calculated as <strong class="source-inline">dataset_size // batch_size</strong>. The trainer discards partial batches, so selecting a batch size that evenly divides the dataset <span class="No-Break">is crucial.</span></p>
			<p class="callout">If your dataset is relatively small (50-500 samples), consider making the following adjustments to <span class="No-Break">the hyperparameters:</span></p>
			<p class="callout"><strong class="bold">- Reduce the batch size</strong>: To minimize discarded samples, choose a batch size that is a clean divisor of your <span class="No-Break">dataset size.</span></p>
			<p class="callout"><strong class="bold">- Set the learning rate decay (</strong><strong class="source-inline">gen_lr_steps</strong><strong class="bold">)</strong>: Decay the learning rate faster for smaller datasets. Aim for no more than 10 epochs before the first decay – that is, the first value in <strong class="source-inline">gen_lr_steps</strong> should be less than <strong class="source-inline">epoch_steps * 10</strong>. Experiments show that the loss may increase if there is <a id="_idIndexMarker966"/>no decay by <span class="No-Break">epoch 20.</span></p>
			<p class="callout"><strong class="bold">- Adjust the validation and checkpoint frequencies</strong>: Adjust <strong class="source-inline">print_freq</strong>, <strong class="source-inline">val_freq</strong>, and <strong class="source-inline">save_checkpoint_freq</strong> based on the dataset’s size. A recommended setting is <strong class="source-inline">val_freq == save_checkpoint_freq == print_freq*3</strong>; <strong class="source-inline">print_freq == </strong><span class="No-Break"><strong class="source-inline">min(epoch_steps,100)</strong></span><span class="No-Break">.</span></p>
			<p class="callout">By carefully tuning these hyperparameters according to your dataset’s size, you can optimize the fine-tuning process and achieve better results with your <span class="No-Break">PVS models.</span></p>
			<p>Having successfully fine-tuned our PVS model using the DLAS toolkit, we are now ready to test our personalized voice <a id="_idIndexMarker967"/>by synthesizing speech that captures the essence of our target voice. In the next section, we will explore generating realistic and expressive speech using our fine-tuned model, bringing the synthesized voice to life and unlocking the potential for a wide range of <span class="No-Break">exciting applications.</span></p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor219"/>PVS step 3 – Synthesizing speech using a fine-tuned PVS model</h1>
			<p>Synthesizing speech using a fine-tuned PVS model is the culmination of the voice synthesizing process, where the<a id="_idIndexMarker968"/> personalized voice is brought to life. It is the stage where the fine-tuned model is tested, generating realistic and natural-sounding speech. The ability to synthesize speech using a fine-tuned PVS model opens up various applications, from creating virtual assistants and audiobook narration to personalized <span class="No-Break">voice interfaces.</span></p>
			<p>Several key components and considerations come into play when embarking on the journey of speech synthesis. Firstly, it is essential to have a suitable computing environment that can handle the computational demands of speech synthesis. This often involves leveraging the power of GPUs, particularly NVIDIA GPUs, which can significantly accelerate the synthesis process. Checking the availability and compatibility of the GPU is crucial to ensure smooth and efficient <span class="No-Break">speech generation.</span></p>
			<p>In addition to the hardware requirements, the speech synthesis process relies on a robust software stack. The TorToiSe-TTS-Fast project, a high-performance TTS system, emerges as a powerful tool. To utilize TorToiSe-TTS-Fast, it is necessary to clone the project repository and install the required dependencies, ensuring that all the necessary libraries and packages <span class="No-Break">are available.</span></p>
			<p>Loading the fine-tuned PVS model is a critical step in speech synthesis. During the fine-tuning phase, we stored the model in <strong class="bold">Google Drive</strong> as a checkpoint file or a serialized model object. The location and format of the fine-tuned model may vary depending on the specific locations you used during the <span class="No-Break">fine-tuning process.</span></p>
			<p>With the fine-tuned PVS model loaded, the next step is to prepare the text input that will be synthesized into speech. Depending on the desired output, this text input can be a single sentence, a paragraph, or a script. It is essential to ensure that the text input is formatted correctly and free of any errors or inconsistencies that could impact the quality of the <span class="No-Break">synthesized speech.</span></p>
			<p>The speech synthesis process involves feeding the text input into the fine-tuned PVS model and generating the corresponding audio output. This is where the magic happens as the model applies its learned knowledge to convert the text into speech that mimics the voice we used as the foundation. The synthesis process may involve various techniques, such as neural vocoding, to generate high-quality <span class="No-Break">audio waveforms.</span></p>
			<p>Depending on the specific requirements and preferences, various parameters and settings can be adjusted during speech synthesis. These can include factors such as the speaking rate, pitch, volume, and emotional tone of the generated speech. Fine-tuning these parameters allows greater control over the final output and can help achieve the desired expressiveness and naturalness in the <span class="No-Break">synthesized speech.</span></p>
			<p>Once the speech synthesis process is complete, the generated audio can be saved to a file format suitable for <a id="_idIndexMarker969"/>playback, such as WAV or MP3. This allows us to integrate synthesized speech into various applications and platforms easily. Consider the desired audio quality and compatibility when choosing the output file format – that is, <strong class="source-inline">"ultra_fast"</strong>, <strong class="source-inline">"fast" (default</strong>), <strong class="source-inline">"standard"</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">"high_quality"</strong></span><span class="No-Break">.</span></p>
			<p>Finally, evaluating the quality and naturalness of the synthesized speech is a crucial step in the process. This can involve subjective assessments, such as listening tests conducted by human evaluators, and objective metrics that measure various aspects of the generated speech, such as intelligibility, naturalness, and similarity to the target voice. Iterative refinement and fine-tuning based on the evaluation results can help improve the overall quality of the <span class="No-Break">synthesized speech.</span></p>
			<p>Researchers and developers can unlock the potential of personalized speech generation by understanding the components, considerations, and steps involved in synthesizing speech using a fine-tuned PVS model. The ability to create realistic and expressive speech that captures the essence of a voice opens exciting possibilities in various domains, from entertainment and education to accessibility and beyond. With the right tools, techniques, and attention to detail, the speech synthesis process using fine-tuned PVS models can be a powerful and transformative technology in speech and <span class="No-Break">audio processing.</span></p>
			<p>Please find and open the Colab notebook called <span class="No-Break"><strong class="source-inline">LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb</strong></span><span class="No-Break"> (</span><a href="https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb"><span class="No-Break">https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb</span></a><span class="No-Break">).</span></p>
			<p>This notebook demonstrates how to check the GPU and RAM, install the necessary libraries, load a fine-tuned PVS model, synthesize speech using the model, and play the generated audio. The code relies on the tortoise-TTS-Fast project to achieve high-performance voice synthesis. Let’s walk through the code in the <strong class="source-inline">LOAIW_ch09_4_Synthetizing_speech_using_fine-tuned_PVS_models.ipynb</strong> file, with explanations and <span class="No-Break">code samples:</span></p>
			<ol>
				<li><strong class="bold">Checking the NVIDIA GPU</strong>: The notebook starts by checking if an NVIDIA GPU is available using the <strong class="source-inline">nvidia-smi</strong> command. It prints the GPU information if connected. Otherwise, it indicates that no GPU <span class="No-Break">is connected:</span><pre class="source-code">
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') &gt;= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)</pre></li>				<li><strong class="bold">Checking the virtual memory</strong>: Next, we check the available RAM using the <strong class="source-inline">psutil</strong> library. It<a id="_idIndexMarker970"/> prints the amount of available RAM in GB and indicates if a high-RAM runtime is <span class="No-Break">being used:</span><pre class="source-code">
from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))
if ram_gb &lt; 20:
  print('Not using a high-RAM runtime')
else:
  print('You are using a high-RAM runtime!')</pre></li>				<li><strong class="bold">Cloning and installing tortoise-tts-fast</strong>: This section of the notebook clones the <strong class="source-inline">tortoise-tts-fast</strong> repository from GitHub and installs the required dependencies <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">pip3</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">!git clone </strong><a href="https://github.com/152334H/tortoise-tts-fast"><strong class="bold">https://github.com/152334H/tortoise-tts-fast</strong></a>
<strong class="bold">%cd tortoise-tts-fast</strong>
<strong class="bold">!pip3 install -r requirements.txt --no-deps</strong>
<strong class="bold">!pip3 install -e .</strong></pre></li>				<li><strong class="bold">Installing additional supporting libraries</strong>: Next, we install additional libraries, such as <strong class="source-inline">transformers</strong>, <strong class="source-inline">voicefixer</strong>, and <strong class="source-inline">BigVGAN</strong>, <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">pip3</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">!pip3 install transformers==4.29.2</strong>
<strong class="bold">!pip3 uninstall voicefixer</strong>
<strong class="bold">!pip3 install voicefixer==0.1.2</strong>
<strong class="bold">!pip3 install git+https://github.com/152334H/BigVGAN.git</strong></pre></li>				<li><strong class="bold">Mounting Google Drive</strong>: We must mount Google Drive to load the fine-tuned PVS model we <span class="No-Break">created earlier:</span><pre class="source-code">
from google.colab import drive
drive.mount('/content/gdrive')</pre></li>				<li><strong class="bold">Loading a fine-tuned PVS voice model</strong>: Here, we set the path to the fine-tuned PVS model (<strong class="source-inline">gpt_path</strong>) and the text to be <span class="No-Break">synthesized (</span><span class="No-Break"><strong class="source-inline">text</strong></span><span class="No-Break">):</span><pre class="source-code">
<strong class="bold">gpt_path = '/content/gdrive/MyDrive/&lt;filepath/ filename_gpt.pth'</strong>
<strong class="bold">text = "Benny, bring me everyone. EVERYONE!"</strong></pre><p class="list-inset">In Google Colab, use the <strong class="bold">Files</strong> interface to navigate and find the checkpoint you created in the previous notebook<a id="_idIndexMarker971"/> using DLAS. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.10</em> shows an example of a <strong class="source-inline">.pth</strong> checkpoint file in Google Colab’s <strong class="bold">Files</strong> interface. Right-click on that checkpoint file; you will be presented with a menu option to copy the location of such a file using <span class="No-Break"><strong class="source-inline">Copy Path</strong></span><span class="No-Break">:</span></p></li>			</ol>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/Figure_9.10_B21020.jpg" alt="Figure 9.10 – Example of a checkpoint file created by DLAS in Google Colab" width="567" height="283"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.10 – Example of a checkpoint file created by DLAS in Google Colab</p>
			<ol>
				<li value="7"><strong class="bold">Running tortoise_tts.py</strong>: We are now ready to run the <strong class="source-inline">tortoise_tts.py</strong> script with<a id="_idIndexMarker972"/> the specified arguments, including the <strong class="source-inline">--preset</strong> option for inference speed, the <strong class="source-inline">--ar_checkpoint</strong> option for the fine-tuned model path, the <strong class="source-inline">-o</strong> option for an output filename, and the text to <span class="No-Break">be synthesized:</span><pre class="source-code">
<strong class="bold">!python tortoise_tts.py --preset fast --ar_checkpoint $gpt_path -o "152.wav" $text</strong></pre></li>				<li><strong class="bold">Playing the synthesized audio</strong>: Finally, we search in Google Colab’s <strong class="bold">Files</strong> interface for the <strong class="source-inline">tortoise-tts-fast/scripts/results/</strong> directory. You will find the generated audio from the voice synthesis model in that directory. We use <strong class="source-inline">IPython</strong> to display and play the synthesized <span class="No-Break">audio file.</span><pre class="source-code">
import IPython
IPython.display.Audio('/content/tortoise-tts-fast/scripts/results/random_00_00.wav')</pre><p class="list-inset"><span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.11</em> shows an example of the directory structure and files created <span class="No-Break">by TorToiSe-TTS-Fast:</span></p></li>			</ol>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/Figure_9.11_B21020.jpg" alt="Figure 9.11 – Example of audio files created with TorToiSe-TTS-Fast using a fine-tuned PVS model’s checkpoint .pth file" width="387" height="402"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.11 – Example of audio files created with TorToiSe-TTS-Fast using a fine-tuned PVS model’s checkpoint .pth file</p>
			<p>I encourage you to examine and run all<a id="_idIndexMarker973"/> the notebooks in this chapter. They provide an end-to-end practical understanding of leveraging fine-tuned PVS models for high-quality, efficient TTS synthesis. This knowledge will enable you to create PVS applications and explore the potential of personalized speech generation using the TorToiSe-TTS-Fast project in conjunction with the techniques you’ve learned throughout <span class="No-Break">this book.</span></p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor220"/>Summary</h1>
			<p>In this chapter, we explored PVS using OpenAI’s Whisper. We discovered how to harness its power to create customized voice models that capture the unique characteristics of a voice or entirely new voices, opening a wide range of <span class="No-Break">exciting applications.</span></p>
			<p>We began by exploring the fundamentals of TTS in voice synthesis, gaining insights into the role of neural networks, audio processing, and voice synthesis. We learned how to convert audio files into the LJSpeech format, a standardized dataset structure commonly used in TTS tasks, using the OZEN Toolkit and Whisper. This hands-on experience provided a solid foundation for the subsequent steps in the voice <span class="No-Break">synthesizing process.</span></p>
			<p>Next, we delved into the DLAS toolkit, a robust framework for fine-tuning PVS models. We learned how to set up the training environment, prepare the dataset, and configure the model architecture. By leveraging Whisper’s accurate transcriptions, we aligned audio segments with their corresponding text, creating a dataset suitable for training personalized <span class="No-Break">PVS models.</span></p>
			<p>Through practical examples and code snippets, we gained hands-on experience fine-tuning a pre-trained PVS model using our LJSpeech dataset. We discovered how to customize the training process, select appropriate hyperparameters, and evaluate the model’s performance. This experience gave us the knowledge and skills to create high-quality personalized <span class="No-Break">PVS models.</span></p>
			<p>Finally, we tested our fine-tuned PVS model by synthesizing realistic and expressive speech. We learned how to generate natural-sounding speech by providing text input to the model, bringing our synthesized voice to life. The ability to create personalized speech opened a wide range of applications, from virtual assistants and audiobook narration to personalized <span class="No-Break">voice interfaces.</span></p>
			<p>As we conclude this chapter, we look ahead to <a href="B21020_10.xhtml#_idTextAnchor221"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Shaping the Future with Whisper</em>. In this final chapter, we will explore the evolving landscape of ASR and Whisper’s role in shaping its future. We will delve into upcoming trends, anticipated features, ethical considerations, and the general direction of voice technologies, including advanced voice TTS fine-tuning techniques. This forward-looking perspective will provide us with the knowledge and foresight to prepare for and adapt to the future of ASR and <span class="No-Break">voice technology.</span></p>
		</div>
	</div>
</div>
</body></html>