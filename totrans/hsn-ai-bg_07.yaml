- en: Generative Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative models are the most promising push toward enabling computers to
    have an understanding of the world. They are true unsupervised models, and are
    able to perform those tasks that many today consider to be at the cutting edge
    of **artificial intelligence **(**AI**). Generative models are different for precisely
    the reason as it sounds: they generate data. Centered mostly around computer vision
    tasks, this class of network has the power to create new faces, new handwriting,
    or even paintings.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll introduce generative models and their foundations, focusing
    specifically on the two most popular types of model, the **variational autoencoder **(**VAE**),
    and the **generative adversarial network** (**GAN**), where you'll learn ...
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be utilizing Python 3\. You''ll need the following
    packages to be installed:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It will be helpful if your machine is GPU enabled, as discussed in [Chapter
    3](69346214-320e-487f-b4cf-bd5c469dc75e.xhtml), *Platforms and Other Essentials*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to AI – generative models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative models are a class of neural networks that are wholly different from
    what we have discussed thus far. The networks that we've discussed hitherto are
    feedforward networks. CNNs and RNNs are all discriminatory networks, in that they
    try to classify data. Given a specific input, they can predict classes or other
    labels. Generative models, on the other hand, try to predict features given a
    certain label. They do this by having a parameter set that is much smaller than
    the amount of data they are learning, which forces them to comprehend the general
    essence of the data in an efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main types of generative model, VAE and GAN. First, we'll start
    with the motivations for generative ...
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders, and their encoder/decoder frameworks, are the inspiration behind
    generative models. They are a self-supervised technique for representation learning,
    where our network learns about its input so that it may generate new data just
    as input. In this section, we'll learn about their architecture and uses as an
    introduction to the generative networks that they inspire.
  prefs: []
  type: TYPE_NORMAL
- en: Network architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders work by taking an input and generating a smaller vector representation
    for later *reconstructing its own input*. They do this by using an encoder to
    impose an information bottleneck on incoming data, and then utilizing a decoder
    to recreate the input data based on that representation. This is based on the
    idea that there are *structures* within data (that is, correlations, and so on)
    that exist, but that are not readily apparent. Autoencoders are a means of automatically
    learning these relationships without explicitly doing so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Structurally, autoencoders consist of an **input layer**, a **hidden layer**,
    and an **output** **layer**, as demonstrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder learns to preserve as much of the relevant ...
  prefs: []
  type: TYPE_NORMAL
- en: Building an autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're thinking that the task of reconstructing an output doesn't appear
    that useful, you're not alone. What exactly do we use these networks for? Autoencoders
    help to extract features when there are no known labeled features at hand. To
    illustrate how this works, let's walk through an example using TensorFlow. We're
    going to reconstruct the MNIST dataset here, and, later on, we will compare the
    performance of the standard autoencoder against the variational autoencoder in
    relation to the same task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started with our imports and data. MNIST is contained natively within
    TensorFlow, so we can easily import it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For ease, we can build the auto-encoder with the `tf.layers` library. We'll
    want our Autoencoder architecture to follow the convolutional/de-convolutional
    pattern, where the input layer of the decoder matches the size of the input and
    the subsequent layer squash the data into a smaller and smaller representation.
    The decoder will be the same architecture reversed, starting with the small representation
    and working larger.
  prefs: []
  type: TYPE_NORMAL
- en: 'All together, we want it to look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba95c19c-4ed2-4962-a28b-b99d4faef25e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s start with the encoder; we''ll define an initializer for the the weight
    and bias factors first, and then define the encoder as a function that takes and
    input, x. we''ll then use the `tf.layers.dense` function to create standard, fully
    connected neural network layers. The encoder will have three layers, with the
    first layer size matching the input dimensions of the input data (`784`), with
    the subsequent layers getting continually smaller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's let's build our decoder; it will be using the same layer type and
    initializer as the encoder, only now we invert the layers, so that the first layer
    of the decoder is the smallest and the last is the largest.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Before we get to training, let's define some hyper-parameters that will be needed
    during the training cycle. We'll define the size of our input, the learning rate,
    number of training steps, the batch size for the training cycle, as well as how
    often we want to display information about our training progress.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll then define the placeholder for our input data so that we can compile
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And subsequently, we compile the model and the optimizer as you''ve seen before
    in previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we''ll code up the training cycle. By this point, most of this should
    be fairly familiar to you; start a TensorFlow session, and iterate over the epochs/batches,
    computing the loss and accuracy at each point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For this particular example, we''ll add in a little something more to this
    process; a way to plot the reconstructed images alongside their original versions.
    Keep in mind that this code is still contained within the training session, just
    outside of the training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After training, you should end up with a result along the lines of the following,
    with the actual digits on the left, and the reconstructed digits on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a2dfa95-fd49-4543-90a7-ed79f9de4aab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So what have we done here? By training the autoencoder on unlabeled digits,
    we''ve done the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Learned the latent features of the dataset without having explicit labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Successfully learned the distribution of the data and reconstructed the image
    from scratch, from that distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s say that we wanted to take this further and generate or classify
    new digits that we haven''t seen yet. To do this, we could remove the decoder
    and attach a classifier or generator network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58fbd830-412a-4784-b3b7-08eb64a140ee.png)'
  prefs: []
  type: TYPE_IMG
- en: The encoder therefore becomes a means of initializing a supervised training
    model. Standard autoencoders have been used in a variety of tasks. In the supplementary
    code for this chapter, we'll walk through an example where we utilize autoencoders
    for visual anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Variational autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Variational autoencoders** (**VAEs**) are built on the idea of the standard
    autoencoder, and are powerful generative models and one of the most popular means
    of learning a complicated distribution in an unsupervised fashion. VAEs are **probabilistic
    models **rooted in Bayesian inference. A probabilistic model is exactly as it
    sounds:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Probabilistic models incorporate random variables and probability distributions
    into the model of an event or phenomenon.*'
  prefs: []
  type: TYPE_NORMAL
- en: VAEs, and other generative models, are probabilistic in that they seek to learn
    a distribution that they utilize for subsequent sampling. While all generative
    models are probabilistic models, not all probabilistic models are generative models.
  prefs: []
  type: TYPE_NORMAL
- en: The probabilistic structure of ...
  prefs: []
  type: TYPE_NORMAL
- en: Structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like standard autoencoders, VAEs utilize the same encoder/decoder framework,
    but, that aside, they are mathematically different from their namesake. VAEs take
    a probabilistic perspective in terms of guiding the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/763b5004-ecd3-4102-99ab-f55907db8e60.png)'
  prefs: []
  type: TYPE_IMG
- en: Both our **encoder** and **decoder** networks are generating distributions from
    their input data. The encoder generates a distribution from its training data,
    **Z**, which then becomes the input distribution for the decoder. The decoder
    takes this distribution, **Z**, and tries to replicate the original distribution, **X**,
    from it.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The encoder generates its distribution by first defining its prior as a standard
    normal distribution. Then, during training, this distribution becomes updated,
    and the decoder can easily sample from this distribution later on. Both the encoder
    and the decoder are unique in terms of VAEs in that they output two vectors instead
    of one: a vector of means, *μ*, and another vector of standard deviation, *σ*.
    These help to define the limits for our generated distributions. Intuitively,
    the mean vector controls where the encoding of an input should be centered, while
    the standard deviation controls the extent to which the encoding may vary from
    the mean. This constraint on the encoder forces the network to learn a distribution,
    thereby taking ...'
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like the standard autoencoder, the decoder in the VAE is a backward convolutional
    network, or a deconvolutional network. In processing the decoding, data is sampled
    from the generation stochastically (randomly), making the VAE one of the few models
    that can directly sample a probability distribution without a Markov chain Monte
    Carlo method. As a result of the stochastic generation process, the encoding that
    we generate from each pass will be a different representation of the data, all
    while maintaining the same mean and standard deviation. This helps with the decoder's
    sampling technique; because all encodings are generated from the same distribution,
    the decoder learns that a latent data point and its surrounding points are all
    members of the same class. This allows the decoder to learn how to generate from
    similar, but slightly varying, encodings.
  prefs: []
  type: TYPE_NORMAL
- en: Training and optimizing VAEs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VAEs utilize a negative log-likelihood loss as their reconstruction loss to
    measure how much information is lost during the reconstruction phase of the decoder.
    If the decoder does not reconstruct the input satisfactorily, it will incur a
    large reconstruction loss. VAEs also introduce something called **Kullback**–**Leibler** (**KL**)
    divergence into their loss functions. KL divergence simply measures how much two
    probability distributions diverge; in other words, how different they are from
    one another. We want to minimize the KL distance between the mean and standard
    deviation of the target distribution and that of a standard normal. It is properly
    minimized when the mean is zero and the standard deviation is one. The log-likelihood
    ...
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing a VAE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can construct a variational autoencoder in TensorFlow to see how it compares
    to it's simpler, standard autoencoder cousin. In this section, we'll be using
    the same MNIST dataset so that we can standardize our comparison across methods. Let's
    walk through how to construct a VAE by utilizing it to generate handwriting based
    on the `MNIST` dataset. Think of *x* as being the individual written characters
    and *z* as the latent features in each of the individual characters that we are
    trying to learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s start with our imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we can import the `''MNIST_data''` directly from the TensorFlow
    library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can start to build the encoder. We''re going to be utilizing the same
    `tf.layers` package as we did before. Here, our encoder will look fairly similar
    to how it did in the previous example, our layers will take in an input and gradually
    compress that input until we generate a latent distribution, *z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s where we start to diverge from the standard autoencoder, however. While
    the last layer in the encoder will give us the potential z-distribution that represents
    our data, we''ll need to calculate the values of ![](img/f91f9b49-ee69-4ac5-b432-07f3b201f54f.png) and ![](img/0a07d995-2583-4465-b9a0-78b667741e97.png) that
    will help define that distribution. We can do that by creating two new layers
    that take in the potential distribution z, and output out values of `mu` and `sigma`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll use these values to go ahead and calculate the KL divergence for
    the encoder, which will eventually go into constructing our final loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go ahead and create the decoder portion of the variational autoencoder
    now; we''ll create a deconvolutional pattern that reverses the dimensions of the
    encoder. All of this will be contained under a function called `decoder(z)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Also under the decoder function, we''ll use the decoder output to calculate
    the reconstruction loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, we''ll prepare our training parameters before we start initializing
    the model. We''ll define a learning rate, batch size for our training, the number
    of training epochs, dimension of the input, and the size of our total training
    sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll also define the placeholder for our input data, `x`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we start training, we''ll initialize the model, loss, and `optimizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can run the actual training process. This we be similar to the
    training processes that we''ve already built and experienced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we can use the bit of code following code to generate new samples from
    our newly trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Ultimately, you should end up with an image such as the following, with the
    original digits on the left and the generated digits on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0587722-c356-4abe-afbd-5d0f2a9e727c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Observe how much clearer the digits are compared to the original autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3867cf5a-5047-4f9a-8f3f-ca2c310363ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let's see how we can take this further with GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative adversarial networks (**GANs**) are a class of networks that were
    introduced by Ian Goodfellow in 2014\. In GANs, two neural networks play off against
    one another as adversaries in an **actor**-**critic model**, where one is the
    creator and the other is the scrutinizer. The creator, referred to as the **generator
    network**, tries to create samples that will fool the scrutinizer, the discriminator
    network. These two increasingly play off against one another, with the generator
    network creating increasingly believable samples and the discriminator network
    getting increasingly good at spotting the samples. In summary:'
  prefs: []
  type: TYPE_NORMAL
- en: The generator tries to maximize the probability of the discriminator passing
    its outputs as real, ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The discriminator network in image-related GANs is a standard convolutional
    neural network. It takes in an image and outputs a single number that tells us
    whether the image is *real* or *fake*. The discriminator takes in an image, and
    learns the attributes of that image so that it may be a good *judge* vis-à-vis
    the outputs of the generator. In TensorFlow, we can create the `discriminator`
    as a function that we will then run in a TensorFlow session later on. This framework
    is more or less the same as you''ve seen in the previous sections with autoencoder
    and variational autoencoders; we''ll use the higher level `tf.layers` api to create
    three main network layers and an output layer. After each of the main network
    layers, we''ll add a dropout layer for regularization. The last layer will be
    slightly different, as we''ll want to squash the output. For this, we''ll use
    a sigmoid activation function that will give us a final output saying if an image
    is believed to be fake or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have this discriminator defined, let's go ahead and move on to the
    generator.
  prefs: []
  type: TYPE_NORMAL
- en: Generator network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can think of the `generator` portion of the GAN as a reverse convolutional
    neural network. Like a VAE, it uses generic normal distribution, the only difference
    being that it up samples the distribution to form an image. This distribution
    represents our prior, and is updated during training as the GAN improves at producing
    images that the discriminator is unable to determine whether they are fake.
  prefs: []
  type: TYPE_NORMAL
- en: In between each layer, we utilize a `ReLu` activation function and `batch _normalization`
    to stabilize each layer's outputs. As the discriminator starts inspecting the
    outputs of `generator`, `generator` will continually adjust the distribution from
    which it's drawing to closely match the target distribution. The code will look
    fairly ...
  prefs: []
  type: TYPE_NORMAL
- en: Training GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs are easy to train, but difficult to optimize due to a number of unstable
    dynamics in their training processes. To train a GAN, we train the generator on
    sub samples of a high-dimensional training distribution; since this does not innately
    exist, we initially sample from a standard normal (Gaussian) distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the generator and the discriminator are trained jointly in a minimax game
    using an objective function, also referred to as the `minimax` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d9f4c89-a4aa-4edb-81d3-d11a43fa60c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s break this down a bit. The function is telling us what happens where.
    Let''s look at the initial bit of the first expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08265ab7-a501-4899-b033-57c9e012c274.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The ![](img/c45f3372-9119-4fb7-a267-37a3a3ba0936.png) notation means expectation,
    so we are saying that the expected output, *x*, of the discriminator for real
    images drawn from the actual distribution of will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2652248-ec02-47e7-9085-72afbd140ce5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Likewise, here''s the second expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc93119c-59d1-4f43-902c-1d724c5f3745.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s telling us that the expected value of the output of the discriminator
    for fake images drawn from the generated distribution will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db7c7441-db5a-49a5-ac30-b18d81adc99d.png)'
  prefs: []
  type: TYPE_IMG
- en: The discriminator wants to maximize (![](img/92648cd3-1ae5-41ef-92ba-1dc9222594bd.png))
    the objective so that its output for real data *D(x)* is as close to one as possible,
    while its output for fake data *D(G(z))* is as close to zero as possible. Meanwhile,
    the generator seeks the opposite, to minimize (![](img/6f3a5e8a-ee08-497c-b37c-5fd1625f0352.png))
    the objective function so that *D(x)* is as close to zero as possible, while *D(G(z))*
    is as close to one as possible. Mathematically, this is how the generator and
    the discriminator play off against one another.
  prefs: []
  type: TYPE_NORMAL
- en: 'When training GANs, we train to minimize the objective function so that the
    generator can win. We want the generator to be able to create examples that are
    realistic enough to fool the discriminator. To do this, we train and optimize
    the discriminator and the generator in parallel using gradient ascent. For each
    iteration of training, we are going to train the discriminator network in small
    batches, and then train the generator network in small batches, alternating between
    the two paradigms. Gradient ascent for the discriminator computes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8919266-0d1b-42b1-ba3d-135b54ea748f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Training both the discriminator and the generator jointly can be challenging.
    If we tried to actually minimize the loss function for the generator, as follows,
    we would run into some issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/958a783f-cfb4-4b21-9153-0e600f597d17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we look at a plot of the `minimax` function, we can see why this is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c65fc91-c973-4cdf-bb15-e4ba4455e415.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Optimization procedures look for gradient signals, which more or less tell
    gradient descent which way to go. In the `minimax` function, the biggest signals
    for gradient descent are to the right, but we actually want to to learn values
    to the left of the function, where it''s minimized to zero and the generator is
    fooling the discriminator. However, as the generator optimizes, it will move away
    from its optimal point, taking us away from where it should be. To solve this,
    we can flip the paradigm of the generator. Instead of focusing on what it did
    right, we can make it focus on what it did wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efaa5ce7-54a4-4f9d-a6c6-0a45966a7db8.png)'
  prefs: []
  type: TYPE_IMG
- en: By taking the maximum of the generator's objective, we're maximizing the likelihood
    of being wrong. This parallelized training process can still be unstable, however,
    and stabilizing GANs is a very active area of research at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get back to the TensorFlow process. We''ll start by defining our network''s
    training parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We then need to define our placeholders, both for the input `x`, as well as
    the `z` distribution which the generator will generate from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Like before, we''ll create a Glorot `Initializer` that will initialize our
    weight and bias values for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have all of this, we can go ahead and actually define our network pieces.
    You''ll notice that for the discriminator, we''re using something called a scope.
    Scopes allow us to reuse items from the TensorFlow graph without generating an
    error - in this case, we want to use the variables from the discriminator function
    twice in a row, so we use the `tf.variable_scope` function that TensorFlow provides
    us. Between the two, we simply use the `scope.reuse_variables()` function to tell
    TensorFlow what we''re doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we''ll define the loss functions for both the generator and discriminator,
    and set the optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can the run the training cycle just as we have in the previous two examples.
    The only two differences you''ll see here is that we run two optimization processes,
    one for the generator and one for the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: GANs are fairly computational expensive, so training this network may take a
    while unless you scale with a web services platform.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, all of the models that we've run thus far have built upon each
    other. Even with advanced generative models like GANs, we can use certain recipes
    to create powerful neural networks, and larger AI applications, quickly and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Other forms of generative models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While we''ve only covered two types of generative model, there are many different
    types that you may encounter in the literature. The following chart is not exhaustive,
    but does provide a general overview of the types of generative models out there:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37d3c00a-b73f-4f97-a532-4a468b6f1171.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s break this down:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Explicit density models**: Model our data directly from a probability distribution.
    We explicitly define the probability and solve for it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implicit density models**: Learn to sample from a probability distribution
    without defining what that distribution is'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Within explicit density models, we have **tractable density** models and **approximate
    density ...**
  prefs: []
  type: TYPE_NORMAL
- en: Fully visible belief nets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fully visible belief networks are a class of explicit density models and a
    form of deep belief network. They use the chain rule to decompose a probability
    distribution ![](img/f74fdadd-39f8-4b3b-a292-aea743773a22.png) over a vector,
    into a product over each of the members of the vector, represented between by
    ![](img/1b1c3bcf-cf33-4ca8-88e2-8be9f01139e0.png). All together, it''s formula
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44925bb5-891f-4ab8-afb4-8f1a68efef10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The most popular model in this family is PixelCNN, an **autoregressive** generative
    model. Pixels approach image generation problems by turning them into a sequence
    modeling problem, where the next pixel value is determined by all the previously
    generated pixel values. The network scans an image one pixel at a time, and predicts
    conditional distributions over the possible pixel values. We want to assign a
    probability to every pixel image based on the last pixels that the network saw.
    For instance, if we''re looking at the same horse images as in the previous example,
    we would be consistently predicting what the next anticipated pixel looks such
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea5df82a-188b-4d1d-8600-342f9995cee1.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on the features that we've seen, will the next pixel still contain the
    horse's ear, or will it be background? While their training cycles are more stable
    than GANs, the biggest issue with the networks is that they generate new samples
    extremely slowly; the model must be run again fully in order to generate a new
    sample. They also block the execution, meaning that their processes cannot be
    run in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Markov models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A hidden Markov model is a type of **Markov model**, which is itself a subclass
    of **Dynamic Bayesian Networks**.Markov models are used to model randomly changing
    systems called **Markov processes** also called **Markov chains**. Simply put,
    a Markov process is a sequence of events where the probability of an event happening
    solely depends on the previous event.
  prefs: []
  type: TYPE_NORMAL
- en: 'Markov chains appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/747b092b-e6c7-4ffd-8b43-bcb1d522d7fc.png)'
  prefs: []
  type: TYPE_IMG
- en: In this simple chain, there are three states, represented by the circles. We
    then have probabilities for transitioning to another state, as well as probabilities
    of staying in a current state. The classic example of a Markov chain is that of
    the ...
  prefs: []
  type: TYPE_NORMAL
- en: Boltzmann machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Boltzmann machines are a general class of models that contain take binary vectors
    as input and units that assign a probability distribution to each of those binary
    vectors. As you can see in the following diagram, each unit is dependent on every
    other unit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7692c3f7-5955-49d3-afe4-bc5d0595b3cf.png)'
  prefs: []
  type: TYPE_IMG
- en: A Boltzmann machine uses something called an **energy function**, which is similar
    to a loss function. For any given vector, the probability of a particular state
    is proportional to each of the energy function values. To convert this to an actual
    probability distribution, it's necessary to renormalize the distribution, but
    this problem becomes another intractable problem. Monte Carlo methods are again
    used here for sampling as a workaround, hence making Boltzmann machines a Monte
    Carlo-based method.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have documents that are represented by binary features. A Boltzmann
    machine can help us determine whether a particular word or phrase came from a
    particular document. We can also use Boltzmann machines for anomaly detection
    in large, complex systems. They work well up to a point, although this method
    does not work well in high dimensional spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about some of the most exciting networks in AI,
    variational autoencoders and GANs. Each of these relies on the same fundamental
    concepts of condensing data, and then generating from again from that condensed
    form of data. You will recall that both of these networks are probabilistic models,
    meaning that they rely on inference from probability distributions in order to
    generate data. We worked through examples of both of these networks, and showed
    how we can use them to generate new images.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to learning about these exciting new techniques, most importantly
    you learned that the building blocks of advanced networks can be broken down into
    smaller, simpler, and repetitive parts. When you think about ...
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[statisticshowto.com](http://www.statisticshowto.com/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure adapted from Ian Goodfellow, Tutorial on GANs, 2017
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
