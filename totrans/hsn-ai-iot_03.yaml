- en: Machine Learning for IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **machine learning** (**ML**) refers to computer programs that can
    automatically detect meaningful patterns in data and improve with experience.
    Though it isn't a new field, it's presently at the peak of its hype cycle. This
    chapter introduces the reader to standard ML algorithms and their applications
    in the field of IoT.
  prefs: []
  type: TYPE_NORMAL
- en: 'After reading this chapter, you will know about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What ML is and the role it plays in the IoT pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised and unsupervised learning paradigms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression and how to perform linear regression using TensorFlow and Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popular ML classifiers and implementing them in TensorFlow and Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees, random forests, and techniques to perform boosting and how to
    write code for them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tips and tricks to improve the system performance and model limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML and IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML, a subset of artificial intelligence, aims to build computer programs with
    an ability to automatically learn and improve from experience without being explicitly
    programmed. In this age of big data, with data being generated at break-neck speed,
    it isn't humanly possible to go through all of the data and understand it manually.
    According to an estimate by Cisco, a leading company in the field of IT and networking,
    IoT will generate 400 zettabytes of data a year by 2018\. This suggests that we
    need to look into automatic means of understanding this enormous data, and this
    is where ML comes in.
  prefs: []
  type: TYPE_NORMAL
- en: The complete Cisco report, released on February 1, 2018, can be accessed at [https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html).
    It forecasts data traffic and cloud service trends in light of the amalgamation
    of IoT, robotics, AI, and telecommunication.
  prefs: []
  type: TYPE_NORMAL
- en: Every year, Gartner, a research and advisory firm, releases a graphical representation
    providing a visual and conceptual presentation of the maturity of emerging technologies
    through five phases.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the image of *Gartner Hype Cycle for Emerging Technologies* in
    the year 2018 at [https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/](https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/).
  prefs: []
  type: TYPE_NORMAL
- en: We can see that both IoT platforms and ML are at the Peak of Inflated Expectations.
    What does it mean? The Peak of Inflated Expectations is the stage in the lifetime
    of technology when there's over enthusiasm about the technology. A large number
    of vendors and startups invest in the technology present at the peak crest. A
    growing number of business establishments explore how the new technology may fit
    within their business strategies. In short, it's the time to jump in to the technology.
    You can hear investors joking at venture fund events that *if you just include
    machine learning in your pitch, you can add a zero on to the end of your valuation*.
  prefs: []
  type: TYPE_NORMAL
- en: So, fasten your seat belts and let's dive deeper into ML technology.
  prefs: []
  type: TYPE_NORMAL
- en: Learning paradigms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ML algorithms can be classified based on the method they use as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic versus non-probabilistic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling versus optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised versus unsupervised
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this book, we classify our ML algorithms as supervised versus unsupervised.
    The distinction between these two depends on how the model learns and the type
    of data that''s provided to the model to learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: Let''s say I give you a series and ask you to predict
    the next element:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*(1, 4, 9, 16, 25*,...)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You guessed right: the next number will be 36, followed by 49 and so on. This
    is supervised learning, also called **learning by example**; you weren''t told
    that the series represents the square of positive integers—you were able to guess
    it from the five examples provided.'
  prefs: []
  type: TYPE_NORMAL
- en: In a similar manner, in supervised learning, the machine learns from example.
    It's provided with a training data consisting of a set of pairs (*X*, *Y*) where
    *X* is the input (it can be a single number or an input value with a large number
    of features) and *Y* is the expected output for the given input. Once trained
    on the example data, the model should be able to reach an accurate conclusion
    when presented with a new data.
  prefs: []
  type: TYPE_NORMAL
- en: The supervised learning is used to predict, given set of inputs, either a real-valued
    output (regression) or a discrete label (classification). We'll explore both regression
    and classification algorithms in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: Let''s say you''re given with eight circular blocks
    of different radii and colors, and you are asked to arrange or group them in an
    order. What will you do?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some may arrange them in increasing or decreasing order of radii, some may group
    them according to color. There are so many ways, and for each one of us, it will
    be dependent on what internal representation of the data we had while grouping.
    This is unsupervised learning, and a majority of human learning lies in this category.
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, the model is just given the data (*X*) but isn't told
    anything about it; the model learns by itself the underlying patterns and relationships
    in the data. Unsupervised learning is normally used for clustering and dimensionality
    reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Though we use TensorFlow for most of the algorithms in this book, in this chapter,
    due to the efficiently built scikit library for ML algorithms, we'll use the functions
    and methods provided by scikit wherever they provide more flexibility and features.
    The aim is to provide you, the reader, with to use AI/ML techniques on the data
    generated by IoT, not to reinvent the wheel.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction using linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aaron, a friend of mine, is a little sloppy with money and is never able to
    estimate how much his monthly credit card bill will be. Can we do something to
    help him? Well, yes, linear regression can help us to predict a monthly credit
    card bill if we have sufficient data. Thanks to the digital economy, all of his
    monetary transactions for the last five years are available online. We extracted
    his monthly expenditure on groceries, stationery, and travel and his monthly income.
    Linear regression helped not only in predicting his monthly credit card bill,
    it also gave an insight into which factor was most responsible for his spending.
  prefs: []
  type: TYPE_NORMAL
- en: This was just one example; linear regression can be used in many similar tasks.
    In this section, we'll learn how we can perform linear regression on our data.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is a supervised learning task. It's one of the most basic,
    simple, and extensively used ML techniques for prediction. The goal of regression
    is to find a function *F*(*x, W*), for a given input-output pair (*x*, *y*), so
    that *y* = *F*(*x, W*). In the (*x*, *y*) pair, *x* is the independent variable
    and *y* the dependent variable, and both of them are continuous variables. It
    helps us to find the relationship between the dependent variable *y* and the independent
    variable(s) *x*.
  prefs: []
  type: TYPE_NORMAL
- en: The input *x* can be a single input variable or many input variables. When *F*(*x,
    W*) maps a single input variable *x*, it's called **simple linear regression**;
    for multiple input variables, it's called **multiple linear regression**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function *F*(*x, W*) is approximated using the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b866dc12-794c-4ae7-804a-5e1d8ef703e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this expression, *d* is the dimensions of *x* (number of independent variables),
    and *W* is the weight associated with each component of *x*. To find the function
    *F*(*x, W*), we need to determine the weights. The natural choice is to find the
    weights that reduce the squared error, hence our objective function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2da2354-6889-4776-96d6-062c18f9b2cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding function, *N* is the total number of the input-output pair
    presented. To find the weights, we differentiate the objective function with respect
    to weight and equate it to *0*. In matrix notation, we can write the solution
    for the column vector *W* = (*W*[0], *W*[1], *W*[2], ..., *W*[d])^T as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d0fdad2-b84a-4896-868c-3718dc47f9af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On differentiating and simplifying, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d758faaa-14ab-4347-8655-6c3ed714d269.png)'
  prefs: []
  type: TYPE_IMG
- en: '*X* is the input vector of size [*N*, *d*] and *Y* the output vector of size
    [*N*, 1]. The weights can be found if (*X^TX*)^(-1) exists, that''s if all of
    the rows and columns of *X* are linearly independent. To ensure this, the number
    of input-output samples (*N*) should be much greater than the number of input
    features (*d*).'
  prefs: []
  type: TYPE_NORMAL
- en: An important thing to remember is that *Y*, the dependent variable, isn't linear
    with respect to the dependent variable X; instead, it's linear with respect to
    the model parameter *W*, the weights. And so we can model relationships such as
    exponential or even sinusoidal (between *Y* and *X*) using linear regression.
    In this case, we generalize the problem to finding weights *W*, so that *y* =
    *F*(*g*(*x*), *W*), where *g*(*x*) is a non-linear function of *X.*
  prefs: []
  type: TYPE_NORMAL
- en: Electrical power output prediction using regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you've understood the basics of linear regression, let's use it to
    predict the electrical power output of a combined cycle power plant. We described
    this dataset in [Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml), *Principles
    and Foundations of AI and IoT*; here, we'll use TensorFlow and its automatic gradient
    to find the solution. The dataset can be downloaded from the UCI ML archive ([http://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant](http://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant)).
    The complete code is available on GitHub ([https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-for-IoT](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-for-IoT))
    under the filename `ElectricalPowerOutputPredictionUsingRegression.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the execution of code in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import `tensorflow`, `numpy`, `pandas`, `matplotlib`, and some useful functions
    of scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The data file is loaded and analyzed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the data isn''t normalized, before using it, we need to normalize it
    using the `MinMaxScaler` of `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define a class, `LinearRegressor`; this is the class where all of the
    real work happens. The class initialization defines the computational graph and
    initializes all of the `Variables` (weights and bias). The class has the `function`
    method, which models the function *y* = *F*(*X*,*W*); the `fit` method performs
    the auto gradient and updates the weights and bias, the `predict` method is used
    to get the output *y* for a given input *X*, and the `get_weights` method returns
    the learned weights and bias:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the previous class to create our linear regression model and train it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the performance of our trained linear regressor. A plot of mean
    square error with **Epochs** shows that the network tried to reach a minimum value
    of mean square error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08d71db2-d508-4a8a-a136-ca9176a05f18.png)'
  prefs: []
  type: TYPE_IMG
- en: On the test dataset, we achieved an *R²* value of *0.768* and mean square error
    of *0.011*.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we learned how to predict. There''s another common
    task in ML: the task of classification. Separating dogs from cats and spam from
    not spam, or even identifying the different objects in a room or scene—all of
    these are classification tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression is an old classification technique. It provides the probability
    of an event taking place, given an input value. The events are represented as
    categorical dependent variables, and the probability of a particular dependent variable
    being *1* is given using the logit function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13aab929-f33c-4e8b-8bdc-33726bf818d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Before going into the details of how we can use logistic regression for classification,
    let''s examine the logit function (also called the **sigmoid** function because
    of its S-shaped curve). The following diagram shows the logit function and its
    derivative varies with respect to the input *X,* the Sigmoidal function (blue)
    and its derivative (orange):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae962533-66ec-4952-997b-524596c97b5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A few important things to note from this diagram are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The value of sigmoid (and hence *Y[pred]*) lies between (*0*, *1*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The derivative of the sigmoid is highest when *W^TX + b = 0.0* and the highest
    value of the derivative is just *0.25* (the sigmoid at same place has a value
    *0.5*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The slope by which the sigmoid varies depends on the weights, and the position
    where we'll have the peak of derivative depends on the bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I would suggest you play around with the `Sigmoid_function.ipynb` program available
    at this book's GitHub repository, to get a feel of how the sigmoid function changes
    as the weight and bias changes.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression aims to find weights *W* and bias *b*, so that each input
    vector, *X[i]*, in the input feature space is classified correctly to its class,
    *y[i]*. In other words, *y[i]* and *![](img/e1e5c9a3-577d-4910-997f-dbdb5cd0cd59.png)*
    should have a similar distribution for the given *x[i]*. We first consider a binary
    classification problem; in this case, the data point *y[i]* can have value *1*
    or *0*. Since logistic regression is a supervised learning algorithm, we give
    as input the training data pair (*X[i]*, *Y[i]*) and let *![](img/62f0973e-bd28-4c44-9a70-93a322422a4a.png)*
    be the probability that *P*(*y*=*1*|*X*=*X[i]*); then, for *p* training data points,
    the total average loss is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f45cc770-d660-465d-bf48-ed5d7d8eef35.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, for every data pair, for *Y[i]* = *1*, the first term will contribute
    to the loss term, with the contribution changing from infinity to *0* as ![](img/05c8ae04-6675-4cc7-b35a-5caa58800101.png)varies
    from *0* to *1*, respectively. Similarly, for *Y[i]* = *0*, the second term will
    contribute to the loss term, with the contribution changing from infinity to zero
    as ![](img/4295ffc5-6a17-4ece-b58d-ea27a6688761.png)varies from *1* to *0*, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'For multiclass classification, the loss term is generalized to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f18ab66-73bb-4fd7-ac3d-3015a548ef93.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding, *K* is the number of classes. An important thing to note is
    that, while for binary classification the output *Y[i]* and *Y[pred]* were single
    values, for multiclass problems, both *Y[i]* and *Y*[*pred*] are now vectors of
    *K* dimensions, with one component for each category.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying wine using logistic regressor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now use what we''ve learned to classify wine quality. I can hear you
    thinking: *What wine quality? No way!* Let''s see how our logistic regressor fares
    as compared to professional wine tasters. We''ll be using the wine quality dataset
    ([https://archive.ics.uci.edu/ml/datasets/wine+quality](https://archive.ics.uci.edu/ml/datasets/wine+quality));
    details about the dataset are given in [Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml),
    *Principles and Foundation of AI and IoT*. The full code is in the file named `Wine_quality_using_logistic_regressor.ipynb`
    at the GitHub repository. Let''s understand the code step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is loading all of the modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We read the data; in the present code, we are analyzing only the red wine,
    so we read data from the `winequality-red.csv` file. The file contains the data
    values separated not by commas, but instead by semicolons, so we need to specify
    the separator argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We separate from the data file input features and target quality. In the file,
    the target, wine quality is given on a scale from 0—10\. Here, for simplicity,
    we divide it into three classes, so if the initial quality is less than five,
    we make it the third class (signifying bad); between five and eight, we consider
    it `ok` (second class); and above eight, we consider it `good` (the first class).
    We also normalize the input features and split the data into training and test
    datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The main part of the code is the `LogisticRegressor` class; at first glance,
    you''ll think that it''s similar to the `LinearRegressor` class we made earlier.
    The class is defined in the Python file, `LogisticRegressor.py`. It is indeed,
    but there are a few important differences: the `Y` `output` is replaced by `Y[pred]`,
    which instead of having a single value, now is a three-dimensional categorical
    value, each dimension specifying the probability of three categories. The weights
    here have dimensions of *d × n*, where `d` is the number of input features and
    `n` the number of output categories. The bias too now is three-dimensional. Another
    important change is the change in the loss function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now we simply train our model and predict the output. The learned model gives
    us an accuracy of ~85% on the test dataset. Pretty impressive!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using ML, we can also identify what ingredients make wine good quality. A company
    called IntelligentX recently started brewing beer based on user feedback; it uses
    AI to get the recipe for the tastiest beer. You can read about the work in this
    *Forbes* article: [https://www.forbes.com/sites/emmasandler/2016/07/07/you-can-now-drink-beer-brewed-by-artificial-intelligence/#21fd11cc74c3](https://www.forbes.com/sites/emmasandler/2016/07/07/you-can-now-drink-beer-brewed-by-artificial-intelligence/#21fd11cc74c3).'
  prefs: []
  type: TYPE_NORMAL
- en: Classification using support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Support Vector Machines** (**SVMs**) is arguably the most used ML technique
    for classification. The main idea behind SVM is that we find an optimal hyperplane
    with maximum margin separating the two classes. If the data is linearly separable,
    the process of finding the hyperplane is straightforward, but if it isn''t linearly
    separable, then kernel trick is used to make the data linearly separable in some
    transformed high-dimensional feature space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SVM is considered a non-parametric supervised learning algorithm. The main
    idea of SVM is to find a **maximal margin separator**: a separating hyperplane
    that is farthest from the training samples presented.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following diagram; the red dots represent class 1 for which the
    output should be 1, and the blue dots represent the class 2 for which the output
    should be -1\. There can be many lines which can separate the red dots from the
    blue ones; the diagram demonstrates three such lines: **A**, **B**, and **C**
    respectively. Which of the three lines do you think will be the best choice? Intuitively,
    the best choice is line B, because it''s farthest from the examples of both classes,
    and hence ensures the least error in classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/900315da-37fc-4a5b-a32f-78220b239342.png)'
  prefs: []
  type: TYPE_IMG
- en: In the following section, we'll learn the basic maths behind finding the maximal-separator
    hyperplane. Though the maths here is mostly basic, if you don't like maths you
    can simply skip to the implementation section where we use SVM to classify wine
    again! Cheers!
  prefs: []
  type: TYPE_NORMAL
- en: Maximum margin hyperplane
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From our knowledge of linear algebra, we know that the equation of a plane
    is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f9a231d-0111-405f-9996-da15704d42da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In SVM, this plane should separate the positive classes (*y*= *1*) from the
    negative classes (*y*=*-1*), and there''s an additional constrain: the distance
    (margin) of this hyperplane from the closest positive and negative training vectors
    (*X[pos]* and *X[neg]* respectively) should be maximum. Hence, the plane is called
    the maximum margin separator.'
  prefs: []
  type: TYPE_NORMAL
- en: The vectors *X*[*pos* ]and *X*[*neg* ]are called **support vectors,** and they
    play an important role in defining the SVM model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, this means that the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/297a2f4c-fa15-4849-b82f-6966b1dd6eb9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And, so is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1203bf35-5600-4a16-8e6c-1a320d09f32a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From these two equations, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a6caae8-8666-4d86-a2fe-409fdb573500.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Dividing by the weight vector length into both sides, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fee17cdc-d92b-4cab-a6bd-bb1b5093899d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So we need to find a separator so that the margin between positive and negative
    support vectors is maximum, that is: ![](img/825f71a2-a026-4d06-9122-5a60ef4e70ee.png)
    is maximum, while at the same time all the points are classified correctly, such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a4af9f3-b099-47f9-867e-d1003b8823ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using a little maths, which we''ll not go into in this book, the preceding
    condition can be represented as finding an optimal solution to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bef553ea-90e1-4f0b-a4e8-3bfae275d27a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Subject to the constraints that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fe746d0-2a30-48cd-9ed6-783275dd1486.png)![](img/f7738f34-ca7f-4970-80ff-2439f0bf2464.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the values of alpha, we can get weights *W* from *α*, the vector of coefficients,
    using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4098e849-68a2-46d8-8c8a-6ce87f78a05b.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a standard quadratic programming optimization problem. Most ML libraries
    have built-in functions to solve it, so you need not worry about how to do so.
  prefs: []
  type: TYPE_NORMAL
- en: For the reader interested in knowing more about SVMs and the math behind it,
    the book *The Nature of Statistical Learning Theory* by Vladimir Vapnik, published
    by *Springer Science+Business Media*, 2013, is an excellent reference.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel trick
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous method works fine when the input feature space is linearly separable.
    What should we do when it isn''t? One simple way is to transform the data (*X*)
    into a higher dimensional space where it''s linearly separable and find a maximal
    margin hyperplane in that high-dimensional space. Let''s see how; our hyperplane
    in terms of *α* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/1cfc0edb-71e9-44d3-9b6d-500bfccdbac2.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: Let *φ* be the transform, then we can replace *X* by *φ*(*X*) and hence its
    dot product *X^(T )X^((i))* with a function K(*X^T*, *X*^((*i*))) = *φ*(*X*)*^T*
    *φ*(*X*^((*i*))) called **kernel**. So we now just preprocess the data by applying
    the transform *φ* and then find a linear separator in the transformed space as
    before.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most commonly used kernel function is the **Gaussian kernel**, also called
    **radial basis function**, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c95c021-9f0e-4d3f-8f22-7f0425e2e684.png)'
  prefs: []
  type: TYPE_IMG
- en: Classifying wine using SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll use the `svm.SVC` function provided by the scikit library for the task.
    The reason to do so is that the TensorFlow library provides us, as of the time
    of writing, with only a linear implementation of SVM, and it works only for binary
    classification. We can make our own SVM using the maths we learned in previously
    in TensorFlow, and `SVM_TensorFlow.ipynb` in the GitHub repository contains the
    implementation in TensorFlow. The following code can be found in the `Wine_quality_using_SVM.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SVC classifier of scikit is a support vector classifier. It can also handle
    multiclass support using a one-versus-one scheme. Some of the optional parameters
    of the method are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`C`: It''s a parameter specifying the penalty term (default value is `1.0`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel`: It specifies the kernel to be used (default is `rbf`). The possible
    choices are `linear`, `poly`, `rbf`, `sigmoid`, `precomputed`, and `callable`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma`: It specifies the kernel coefficient for `rbf`, `poly`, and `sigmoid` and
    the default value (the default is `auto`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state`: It sets the seed of the pseudo-random number generator to use
    when shuffling the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Follow the given steps to create our SVM model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load all of the modules we''ll need for the code. Note that we aren''t
    importing TensorFlow here and instead have imported certain modules from the `scikit`
    library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We read the data file, preprocess it, and separate it into test and training
    datasets. This time, for simplicity, we''re dividing into two classes, `good`
    and `bad`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we use the `SVC` classifier and train it on our training dataset with the
    `fit` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now predict the output for the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The model gave an accuracy of `67.5%` and the confusion matrix is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a5309c9f-56b6-401b-9897-c3259f565a76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding code uses the binary classification; we can change the code to
    work for more than two classes as well. For example, in the second step, we can
    replace the code with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we have three categories just as our previous logistic classifier, and
    the accuracy is 65.9%. And the confusion matrix is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the three-class case, the training data distribution is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`good` `855`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ok` `734`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bad` `10`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the number of samples in the `bad` class (corresponding to `0` in the
    confusion matrix) is only `10`, the model isn't able to learn what parameters
    contribute to bad wine quality. Hence, data should be uniformly distributed among
    all classes of the classifiers that we explore in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Naive Bayes is one of the simplest and fastest ML algorithms. This too belongs
    to the class of supervised learning algorithms. It''s based on the Bayes probability
    theorem. One important assumption that we make in the case of the Naive Bayes
    classifier is that all of the features of the input vector are **independent and
    identically distributed** (**iid**). The goal is to learn a conditional probability
    model for each class *C*[*k*] in the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9796dc8e-c201-401f-b6a8-4427c915c783.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Under the iid assumption, and using the Bayes theorem, this can be expressed
    in terms of the joint probability distribution *p*(*C[k]*, *X*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11e80a0f-f697-4acb-bc36-98b77039a6bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We pick the class that maximizes this term ***Maximum A Posteriori* **(**MAP**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4981d980-6d92-4ae7-8acf-a96f36da3805.png)'
  prefs: []
  type: TYPE_IMG
- en: There can be different Naive Bayes algorithms, depending upon the distribution
    of *p*(*x[i]*|*C[k]*). The common choices are Gaussian in the case of real-valued
    data, Bernoulli for binary data, and MultiNomial when the data contains the frequency
    of a certain event (such as document classification).
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see whether we can classify the wine using Naive Bayes. For the sake
    of simplicity and efficiency, we'll use the scikit built-in Naive Bayes distributions.
    Since the features values we have in our data are continuous-valued—we'll assume
    that they have a Gaussian distribution, and we'll use `GaussianNB` of scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Naive Bayes for wine quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The scikit-learn Naive Bayes module supports three Naive Bayes distributions.
    We can choose either of them depending on our input feature data type. The three
    Naive Bayes available in scikit-learn are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`GaussianNB`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultinomialNB`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BernoulliNB`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The wine data, as we have already seen, is a continuous data type. Hence, it
    will be good if we use Gaussian distribution for *p*(*x[i]*|*C[k]*)—that is, the
    `GaussianNB` module, and so we'll add `from sklearn.naive_bayes import GaussianNB` in
    the import cell of the Notebook. You can read more details about the `GaussianNB`
    module from the is scikit-learn link: [http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first two steps will remain the same as in the SVM case. But now, instead
    of declaring an `SVM` classifier, we''ll declare a `GaussianNB` classifier and
    we''ll use its `fit` method to learn the training examples. The result from the
    learned model is obtained using the `predict` method. So follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules. Note that now we''re importing `GaussianNB` from
    the `scikit` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the data file and preprocess it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we declare a Gaussian Naive Bayes, train it on the training dataset, and
    use the trained model to predict the wine quality on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s all, folks; our model is ready and kicking. The accuracy of this model
    is 71.25% for the binary classification case. In the following screenshot, you
    can a the heatmap of the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/556bf535-7318-4f9b-9ee5-78f824c7035e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Before you conclude that Naive Bayes is best, let''s be aware of some of its
    pitfalls:'
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes makes the prediction based on the frequency-based probability; therefore,
    it's strongly dependent on the data we use for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another issue is that we made the iid assumption about input feature space;
    this isn't always true.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you''ll learn about another ML algorithm that''s very popular
    and fast—decision trees. In decision trees, we build a tree-like structure of
    decisions; we start with the root, choose a feature and split into branches, and
    continue till we reach the leaves, which represent the predicted class or value.
    The algorithm of decision trees involves two main steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Decide which features to choose and what conditions to use for splitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Know when to stop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s understand it with an example. Consider a sample of 40 students; we
    have three variables: the gender (boy or girl; discrete), class (XI or XII; discrete),
    and height (5 to 6 feet; continuous). Eighteen students prefer to go to the library
    in their spare time and rest prefer to play. We can build a decision tree to predict
    who will be going to the library and who will be going to the playground in their
    leisure time. To build the decision tree, we''ll need to separate the students
    who go to library/playground based on the highly significant input variable among
    the three input variables. The following diagram gives the split based on each
    input variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bebf484-66d9-4936-83c1-79dd982f6e57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We consider all of the features and choose the one that gives us the maximum
    information. In the previous example, we can see that a split over the feature
    height generates the most homogeneous groups, with the group **Height > 5.5 ft**
    containing 80% students who play and 20% who go to the library in the leisure
    time and the group **Height < 5.5 ft** containing 13% students who play and 86%
    who go to the library in their spare time. Hence, we''ll make our first split
    on the feature height. We''ll continue the split in this manner and finally reach
    the decision (leaf node) telling us whether the student will play or go to the
    library in their spare time. The following diagram shows the decision tree structure;
    the black circle is the **Root** **Node**, the blue circles are the **Decision**
    **Nodes**, and the green circles are the **Leaf** **Nodes**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc3007bb-01fb-4b84-ac7c-9a850c58f8bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The decision trees belong to the family of greedy algorithms. To find the most
    homogeneous split, we define our cost function so that it tries to maximize the
    same class input values in a particular group. For regression, we generally use
    the mean square error cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89ebe80b-22b8-431f-a110-be4f8f7ce105.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* and *y[pred]* represent the given and predicted output values for
    the input values (*i*); we find the split that minimizes this loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'For classification, we use either the *gini* impurity or cross-entropy as the
    loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51381f73-fa84-40ff-bf3f-6c81da1bea53.png)![](img/104b5b6f-3a29-4cbd-a8af-358bc8d9e867.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding, *c[k]* defines the proportion of same class input values present
    in a particular group.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some good resources to learn more about decision trees are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: L. Breiman, J. Friedman, R. Olshen, and C. Stone: *Classification and Regression
    Trees,* Wadsworth, Belmont, CA, 1984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'J.R. Quinlan: *C4\. 5: programs for ML,* Morgan Kaufmann, 1993'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T. Hastie, R. Tibshirani and J. Friedman: *Elements of Statistical Learning*,
    Springer, 2009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees in scikit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `scikit` library provides `DecisionTreeRegressor` and `DecisionTreeClassifier`
    to implement regression and classification. Both can be imported from `sklearn.tree`.
    `DecisionTreeRegressor` is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The different arguments are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`criterion`: It defines which loss function to use to determine the split.
    The default value is mean square error (`mse`). The library supports the use of `friedman_mse` and
    mean absolute error (`mae`) as loss functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`splitter`: We use this to decide whether to use the greedy strategy and go
    for the best split (default) or we can use random `splitter` to choose the best
    random split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: It defines the maximum depth of the tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: It defines the minimum number of samples required to split
    an internal node. It can be integer or float (in this case it defines the percentage
    of minimum samples needed for the split).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DecisionTreeClassifier` is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The different arguments are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`criterion`: It tells which loss function to use to determine the split. The
    default value for the classifier is the `gini`. The library supports the use of `entropy`
    as loss functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`splitter`: We use this to decide how to choose the split (default value is
    the best split) or we can use random `splitter` to choose the best random split.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: It defines the maximum depth of the tree. When the input feature
    space is large, we use this to restrict the maximum depth and take care of overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: It defines the minimum number of samples required to split
    an internal node. It can be integer or float (in this case it tells the percentage
    of minimum samples needed for the split).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ve listed only the commonly used preceding arguments; details regarding
    the remaining parameters of the two can be read on the scikit-learn website: [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) and [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll use a decision tree regressor to predict electrical power output first.
    The dataset and its description have already been introduced in [Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml),
    *Principles and Foundations of IoT and AI*. The code is available at the GitHub repository
    in the file named `ElectricalPowerOutputPredictionUsingDecisionTrees.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We get an R-square value of 0.90 and mean square error of 0.0047 on the test
    data; it''s a significant improvement over the prediction results obtained using
    linear regressor (R-square: `0.77`;mse: `0.012`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also see the performance of decision trees in the classification task;
    we use it for the wine quality classification as before. The code is available
    in the `Wine_quality_using_DecisionTrees.ipynb` file in the GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The decision tree generates a classification accuracy of around 70%. We can
    see that, for small data size, we can use both decision trees and Naive Bayes
    with almost equal success. Decision trees suffer from overfitting, which can be
    taken care of by restricting the maximum depth or setting a minimum number of
    training inputs. They, like Naive Bayes, are unstable—a little variation in the
    data can result in a completely different tree; this can be resolved by making
    use of bagging and boosting techniques. Last, but not least, since it's a greedy
    algorithm, there's no guarantee that it returns a globally optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our daily life, when we have to make a decision, we take guidance not from
    one person, but from many individuals whose wisdom we trust. The same can be applied
    in ML; instead of depending upon one single model, we can use a group of models
    (ensemble) to make a prediction or classification decision. This form of learning
    is called **ensemble learning**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conventionally, ensemble learning is used as the last step in many ML projects.
    It works best when the models are as independent of one another as possible. The
    following diagram gives a graphical representation of ensemble learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73af0f0f-00be-4082-a4a5-7d4540b67e24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The training of different models can take place either sequentially or in parallel.
    There are various ways to implement ensemble learning: voting, bagging and pasting,
    and random forest. Let''s see what each of these techniques and how we can implement
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: Voting classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The voting classifier follows the majority; it aggregates the prediction of
    all the classifiers and chooses the class with maximum votes. For example, in
    the following screenshot, the voting classifier will predict the input instance
    to belong to class **1**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f60022e-2236-4291-a411-268ed5152367.png)'
  prefs: []
  type: TYPE_IMG
- en: 'scikit has the `VotingClassifier` class to implement this. Using ensemble learning
    on wine quality classification, we reach an accuracy score of 74%, higher than
    any of the models considered alone. The complete code is in the `Wine_quality_using_Ensemble_learning.ipynb`
    file. The following is the main code to perform ensemble learning using voting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Bagging and pasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In voting, we used different algorithms for training on the same dataset. We
    can also achieve ensemble learning by using different models with the same learning
    algorithm, but we train them on different training data subsets. The training
    subset is sampled randomly. The sampling can be done with replacement (bagging)
    or without replacement (pasting):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging**: In it, additional data for training is generated from the original
    dataset using combinations with repetitions. This helps in decreasing the variance
    of different models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pasting**: Since pasting is without replacement, each subset of the training
    data can be used at most once. It''s more suitable if the original dataset is
    large.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `scikit` library has a method for performing bagging and pasting; from
    `sklearn.ensemble`, we can import `BaggingClassifier` and use it. The following
    code estimates `500` decision tree classifiers, each with `1000` training samples
    using bagging (for pasting, keep `bootstrap=False`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This results in an accuracy of 77% for wine quality classification. The last
    argument to `BaggingClassifier`, `n_jobs`, defines how many CPU cores to use (that's
    the number of jobs to run in parallel); when its value is set to `-1`, then it
    uses all of the available CPU cores.
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble of only decision trees is called **random forest**. And so what
    we've implemented previously is a random forest. We can directly implement random
    forest in scikit using the `RandomForestClassifier` class. The advantage of using
    the class is that it introduces extra randomness while building the tree. While
    splitting, it searches for the best feature to split among a random subset of
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Improving your model – tips and tricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned a large number of ML algorithms, each with its
    own pros and cons. In this section, we'll look into some common problems and ways
    to resolve them.
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling to resolve uneven data scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data that''s collected normally doesn''t have the same scale; for example,
    one feature may be varying in the range 10–100 and another one may be only distributed
    in range 2–5\. This uneven data scale can have an adverse effect on learning.
    To resolve this, we use the method of feature scaling (normalization). The choice
    of normalization has been found to drastically affect the performance of certain
    algorithms. Two common normalization methods (also called standardization in some
    books) are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Z-score normalization**: In z-score normalization, each individual feature
    is scaled so that it has the properties of a standard normal distribution, that
    is, a mean of *0* and variance of *1*. If *μ* is the mean and *σ* the variance,
    we can compute Z-score normalization by making the following linear transformation
    on each feature as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/40dddd5e-f8af-4ab1-8b59-5a708d51b61b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Min-max normalization**: The min-max normalization rescales the input features
    so that they lie in the range between *0* and *1*. It results in reducing the
    standard deviation in the data and hence suppresses the effect of outliers. To
    achieve min-max normalization, we find the maximum and minimum value of the feature
    (*x[max]* and *x[min]* respectively), and perform the following linear transformation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/724c7e3c-3d4d-4643-a5b5-bf3c29c82b30.png)'
  prefs: []
  type: TYPE_IMG
- en: We can use the `scikit` library `StandardScaler` or `MinMaxscaler` methods to
    normalize the data. In all of the examples in this chapter, we've used `MinMaxScaler`;
    you can try changing it to `StandardScalar` and observe if the performance changes.
    In the next chapter, we'll also learn how to perform these normalizations in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes the model tries to overfit the training dataset; in doing so, it
    loses its ability to generalize and hence performs badly on the validation dataset;
    this in turn will affect its performance on unseen data values. There are two
    standard ways to take care of overfitting: regularization and cross-validation.'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regularization adds a term in the loss function to ensure that the cost increases
    as the model increases the number of features. Hence, we force the model to stay
    simpler. If *L(X*, *Y)* was the loss function earlier, we replace it with the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c6cd641-a45d-4cb1-abc4-902e2e23e1ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding, *N* can be *L*[1] norm, *L*[2] norm, or a combination of
    the two, and *λ* is the regularization coefficient. Regularization helps in reducing
    the model variance, without losing any important properties of the data distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lasso regularization**: In this case, the *N* is *L*[1] norm. It uses the
    modulus of weight as the penalty term *N:*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/76d82420-39b8-4956-bc07-a4fc8d6c1000.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Ridge regularization**: In this case, the *N* is *L2* norm, given by the
    following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c03bc8ca-e933-4a96-bd00-4bfc0c492b73.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using cross-validation can also help in reducing the problem of overfitting.
    In *k*-fold cross-validation, data is divided into *k*-subsets, called **folds**.
    Then it trains and evaluates the model *k*-times; each time, it picks one of the
    folds for validation and the rest for training the model. We can perform the cross-validation
    when the data is less and training time is small. scikit provides a `cross_val_score`
    method to implement the k-folds. Let `classifier` be the model we want to cross-validate,
    then we can use the following code to perform cross-validation on `10` folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The result of this is an average mean and variance value. A good model should
    have a high average and low variance.
  prefs: []
  type: TYPE_NORMAL
- en: No Free Lunch theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With so many models, one always wonders which one to use. Wolpert, in his famous
    paper *The Lack of A Priori Distinctions Between Learning*, explored this issue
    and showed that if we make no prior assumption about the input data, then there's
    no reason to prefer one model over any other. This is known as the **No Free Lunch**
    **theorem**.
  prefs: []
  type: TYPE_NORMAL
- en: This means that there's no model hat can be *a* priori guaranteed to work better.
    The only way we can ascertain which model is best is by evaluating them all. But,
    practically, it isn't possible to evaluate all of the models and so, in practice,
    we make reasonable assumptions about the data and evaluate a few relevant models.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning and grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Different models have different hyperparameters; for example, in linear regressor,
    the learning rate was a hyperparameter; if we''re using regularization, then the
    regularizing parameter λ is a hyperparameter. What should be their value? While
    there''s a rule of thumb for some hyperparameters, most of the time we make either
    a guess or use grid search to perform a sequential search for the best hyperparameters.
    In the following, we present the code to perform hyperparameter search in the
    case of SVM using the `scikit` library; in the next chapter, we''ll see how we
    can use TensorFlow to perform hyperparameter tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`GridSearchCV` will provide us with the hyperparameters that produce the best
    results for the SVM classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this chapter was to provide you with  intuitive understanding of
    different standard ML algorithms so that you can make an informed choice. We covered
    the popular ML algorithms used for classification and regression.We also learnt
    how supervised and unsupervised learning are different from each other. Linear
    regression, logistic regression, SVM, Naive Bayes, and decision trees were introduced
    along with the fundamental principles involved in each. We used the regression
    methods to predict electrical power production of a thermal station and classification
    methods to classify wine as good or bad. Lastly, we covered the common problems
    with different ML algorithms and some tips and tricks to solve them.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll study different deep learning models and learn how
    to use them to analyze our data and make predictions.
  prefs: []
  type: TYPE_NORMAL
