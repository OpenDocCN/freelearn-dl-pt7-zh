- en: Machine Learning for IoT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网的机器学习
- en: The term **machine learning** (**ML**) refers to computer programs that can
    automatically detect meaningful patterns in data and improve with experience.
    Though it isn't a new field, it's presently at the peak of its hype cycle. This
    chapter introduces the reader to standard ML algorithms and their applications
    in the field of IoT.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）指的是能够自动检测数据中有意义的模式并通过经验不断改进的计算机程序。尽管这不是一个新兴领域，但目前正处于其兴奋周期的顶峰。本章将向读者介绍标准的机器学习算法及其在物联网领域的应用。'
- en: 'After reading this chapter, you will know about the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，您将了解以下内容：
- en: What ML is and the role it plays in the IoT pipeline
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习（ML）是什么，它在物联网（IoT）管道中扮演的角色
- en: Supervised and unsupervised learning paradigms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习的范式
- en: Regression and how to perform linear regression using TensorFlow and Keras
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归分析以及如何使用TensorFlow和Keras进行线性回归
- en: Popular ML classifiers and implementing them in TensorFlow and Keras
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的机器学习分类器，并在TensorFlow和Keras中实现它们
- en: Decision trees, random forests, and techniques to perform boosting and how to
    write code for them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树、随机森林以及执行提升的技术和如何为它们编写代码
- en: Tips and tricks to improve the system performance and model limitations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升系统性能和模型局限性的技巧与方法
- en: ML and IoT
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与物联网（IoT）
- en: ML, a subset of artificial intelligence, aims to build computer programs with
    an ability to automatically learn and improve from experience without being explicitly
    programmed. In this age of big data, with data being generated at break-neck speed,
    it isn't humanly possible to go through all of the data and understand it manually.
    According to an estimate by Cisco, a leading company in the field of IT and networking,
    IoT will generate 400 zettabytes of data a year by 2018\. This suggests that we
    need to look into automatic means of understanding this enormous data, and this
    is where ML comes in.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是人工智能的一个子集，旨在构建能够通过经验自动学习和改进的计算机程序，而无需明确编程。在大数据时代，随着数据生成速度惊人地加快，人类无法逐一处理和理解所有数据。根据思科（Cisco）公司的估算，这家公司是IT和网络领域的领先企业，到2018年，物联网（IoT）将每年产生400泽字节的数据。这表明我们需要寻找自动化的手段来理解这些庞大的数据，这正是机器学习的作用所在。
- en: The complete Cisco report, released on February 1, 2018, can be accessed at [https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html).
    It forecasts data traffic and cloud service trends in light of the amalgamation
    of IoT, robotics, AI, and telecommunication.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的思科报告，发布于2018年2月1日，可以通过[https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html)访问。报告预测了物联网、机器人技术、人工智能和电信结合下的数据流量和云服务趋势。
- en: Every year, Gartner, a research and advisory firm, releases a graphical representation
    providing a visual and conceptual presentation of the maturity of emerging technologies
    through five phases.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每年，Gartner这家研究和咨询公司都会发布一份图示，提供新兴技术成熟度的视觉和概念性展示，分为五个阶段。
- en: You can find the image of *Gartner Hype Cycle for Emerging Technologies* in
    the year 2018 at [https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/](https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/](https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/)找到2018年*Gartner新兴技术兴奋周期*的图片。
- en: We can see that both IoT platforms and ML are at the Peak of Inflated Expectations.
    What does it mean? The Peak of Inflated Expectations is the stage in the lifetime
    of technology when there's over enthusiasm about the technology. A large number
    of vendors and startups invest in the technology present at the peak crest. A
    growing number of business establishments explore how the new technology may fit
    within their business strategies. In short, it's the time to jump in to the technology.
    You can hear investors joking at venture fund events that *if you just include
    machine learning in your pitch, you can add a zero on to the end of your valuation*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: So, fasten your seat belts and let's dive deeper into ML technology.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Learning paradigms
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ML algorithms can be classified based on the method they use as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic versus non-probabilistic
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling versus optimization
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised versus unsupervised
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this book, we classify our ML algorithms as supervised versus unsupervised.
    The distinction between these two depends on how the model learns and the type
    of data that''s provided to the model to learn:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: Let''s say I give you a series and ask you to predict
    the next element:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*(1, 4, 9, 16, 25*,...)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'You guessed right: the next number will be 36, followed by 49 and so on. This
    is supervised learning, also called **learning by example**; you weren''t told
    that the series represents the square of positive integers—you were able to guess
    it from the five examples provided.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: In a similar manner, in supervised learning, the machine learns from example.
    It's provided with a training data consisting of a set of pairs (*X*, *Y*) where
    *X* is the input (it can be a single number or an input value with a large number
    of features) and *Y* is the expected output for the given input. Once trained
    on the example data, the model should be able to reach an accurate conclusion
    when presented with a new data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The supervised learning is used to predict, given set of inputs, either a real-valued
    output (regression) or a discrete label (classification). We'll explore both regression
    and classification algorithms in the coming sections.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: Let''s say you''re given with eight circular blocks
    of different radii and colors, and you are asked to arrange or group them in an
    order. What will you do?'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some may arrange them in increasing or decreasing order of radii, some may group
    them according to color. There are so many ways, and for each one of us, it will
    be dependent on what internal representation of the data we had while grouping.
    This is unsupervised learning, and a majority of human learning lies in this category.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, the model is just given the data (*X*) but isn't told
    anything about it; the model learns by itself the underlying patterns and relationships
    in the data. Unsupervised learning is normally used for clustering and dimensionality
    reduction.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Though we use TensorFlow for most of the algorithms in this book, in this chapter,
    due to the efficiently built scikit library for ML algorithms, we'll use the functions
    and methods provided by scikit wherever they provide more flexibility and features.
    The aim is to provide you, the reader, with to use AI/ML techniques on the data
    generated by IoT, not to reinvent the wheel.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Prediction using linear regression
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aaron, a friend of mine, is a little sloppy with money and is never able to
    estimate how much his monthly credit card bill will be. Can we do something to
    help him? Well, yes, linear regression can help us to predict a monthly credit
    card bill if we have sufficient data. Thanks to the digital economy, all of his
    monetary transactions for the last five years are available online. We extracted
    his monthly expenditure on groceries, stationery, and travel and his monthly income.
    Linear regression helped not only in predicting his monthly credit card bill,
    it also gave an insight into which factor was most responsible for his spending.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: This was just one example; linear regression can be used in many similar tasks.
    In this section, we'll learn how we can perform linear regression on our data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is a supervised learning task. It's one of the most basic,
    simple, and extensively used ML techniques for prediction. The goal of regression
    is to find a function *F*(*x, W*), for a given input-output pair (*x*, *y*), so
    that *y* = *F*(*x, W*). In the (*x*, *y*) pair, *x* is the independent variable
    and *y* the dependent variable, and both of them are continuous variables. It
    helps us to find the relationship between the dependent variable *y* and the independent
    variable(s) *x*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: The input *x* can be a single input variable or many input variables. When *F*(*x,
    W*) maps a single input variable *x*, it's called **simple linear regression**;
    for multiple input variables, it's called **multiple linear regression**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'The function *F*(*x, W*) is approximated using the following expression:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b866dc12-794c-4ae7-804a-5e1d8ef703e5.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'In this expression, *d* is the dimensions of *x* (number of independent variables),
    and *W* is the weight associated with each component of *x*. To find the function
    *F*(*x, W*), we need to determine the weights. The natural choice is to find the
    weights that reduce the squared error, hence our objective function is as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2da2354-6889-4776-96d6-062c18f9b2cf.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding function, *N* is the total number of the input-output pair
    presented. To find the weights, we differentiate the objective function with respect
    to weight and equate it to *0*. In matrix notation, we can write the solution
    for the column vector *W* = (*W*[0], *W*[1], *W*[2], ..., *W*[d])^T as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d0fdad2-b84a-4896-868c-3718dc47f9af.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'On differentiating and simplifying, we get the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d758faaa-14ab-4347-8655-6c3ed714d269.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: '*X* is the input vector of size [*N*, *d*] and *Y* the output vector of size
    [*N*, 1]. The weights can be found if (*X^TX*)^(-1) exists, that''s if all of
    the rows and columns of *X* are linearly independent. To ensure this, the number
    of input-output samples (*N*) should be much greater than the number of input
    features (*d*).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: An important thing to remember is that *Y*, the dependent variable, isn't linear
    with respect to the dependent variable X; instead, it's linear with respect to
    the model parameter *W*, the weights. And so we can model relationships such as
    exponential or even sinusoidal (between *Y* and *X*) using linear regression.
    In this case, we generalize the problem to finding weights *W*, so that *y* =
    *F*(*g*(*x*), *W*), where *g*(*x*) is a non-linear function of *X.*
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Electrical power output prediction using regression
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you've understood the basics of linear regression, let's use it to
    predict the electrical power output of a combined cycle power plant. We described
    this dataset in [Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml), *Principles
    and Foundations of AI and IoT*; here, we'll use TensorFlow and its automatic gradient
    to find the solution. The dataset can be downloaded from the UCI ML archive ([http://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant](http://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant)).
    The complete code is available on GitHub ([https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-for-IoT](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-for-IoT))
    under the filename `ElectricalPowerOutputPredictionUsingRegression.ipynb`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the execution of code in the following steps:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'We import `tensorflow`, `numpy`, `pandas`, `matplotlib`, and some useful functions
    of scikit-learn:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data file is loaded and analyzed:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Since the data isn''t normalized, before using it, we need to normalize it
    using the `MinMaxScaler` of `sklearn`:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we define a class, `LinearRegressor`; this is the class where all of the
    real work happens. The class initialization defines the computational graph and
    initializes all of the `Variables` (weights and bias). The class has the `function`
    method, which models the function *y* = *F*(*X*,*W*); the `fit` method performs
    the auto gradient and updates the weights and bias, the `predict` method is used
    to get the output *y* for a given input *X*, and the `get_weights` method returns
    the learned weights and bias:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We use the previous class to create our linear regression model and train it:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s see the performance of our trained linear regressor. A plot of mean
    square error with **Epochs** shows that the network tried to reach a minimum value
    of mean square error:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08d71db2-d508-4a8a-a136-ca9176a05f18.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: On the test dataset, we achieved an *R²* value of *0.768* and mean square error
    of *0.011*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression for classification
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we learned how to predict. There''s another common
    task in ML: the task of classification. Separating dogs from cats and spam from
    not spam, or even identifying the different objects in a room or scene—all of
    these are classification tasks.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression is an old classification technique. It provides the probability
    of an event taking place, given an input value. The events are represented as
    categorical dependent variables, and the probability of a particular dependent variable
    being *1* is given using the logit function:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/13aab929-f33c-4e8b-8bdc-33726bf818d8.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'Before going into the details of how we can use logistic regression for classification,
    let''s examine the logit function (also called the **sigmoid** function because
    of its S-shaped curve). The following diagram shows the logit function and its
    derivative varies with respect to the input *X,* the Sigmoidal function (blue)
    and its derivative (orange):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae962533-66ec-4952-997b-524596c97b5f.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: 'A few important things to note from this diagram are the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: The value of sigmoid (and hence *Y[pred]*) lies between (*0*, *1*)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The derivative of the sigmoid is highest when *W^TX + b = 0.0* and the highest
    value of the derivative is just *0.25* (the sigmoid at same place has a value
    *0.5*)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The slope by which the sigmoid varies depends on the weights, and the position
    where we'll have the peak of derivative depends on the bias
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I would suggest you play around with the `Sigmoid_function.ipynb` program available
    at this book's GitHub repository, to get a feel of how the sigmoid function changes
    as the weight and bias changes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy loss function
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Logistic regression aims to find weights *W* and bias *b*, so that each input
    vector, *X[i]*, in the input feature space is classified correctly to its class,
    *y[i]*. In other words, *y[i]* and *![](img/e1e5c9a3-577d-4910-997f-dbdb5cd0cd59.png)*
    should have a similar distribution for the given *x[i]*. We first consider a binary
    classification problem; in this case, the data point *y[i]* can have value *1*
    or *0*. Since logistic regression is a supervised learning algorithm, we give
    as input the training data pair (*X[i]*, *Y[i]*) and let *![](img/62f0973e-bd28-4c44-9a70-93a322422a4a.png)*
    be the probability that *P*(*y*=*1*|*X*=*X[i]*); then, for *p* training data points,
    the total average loss is defined as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f45cc770-d660-465d-bf48-ed5d7d8eef35.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: Hence, for every data pair, for *Y[i]* = *1*, the first term will contribute
    to the loss term, with the contribution changing from infinity to *0* as ![](img/05c8ae04-6675-4cc7-b35a-5caa58800101.png)varies
    from *0* to *1*, respectively. Similarly, for *Y[i]* = *0*, the second term will
    contribute to the loss term, with the contribution changing from infinity to zero
    as ![](img/4295ffc5-6a17-4ece-b58d-ea27a6688761.png)varies from *1* to *0*, respectively.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'For multiclass classification, the loss term is generalized to the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f18ab66-73bb-4fd7-ac3d-3015a548ef93.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: In the preceding, *K* is the number of classes. An important thing to note is
    that, while for binary classification the output *Y[i]* and *Y[pred]* were single
    values, for multiclass problems, both *Y[i]* and *Y*[*pred*] are now vectors of
    *K* dimensions, with one component for each category.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Classifying wine using logistic regressor
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s now use what we''ve learned to classify wine quality. I can hear you
    thinking: *What wine quality? No way!* Let''s see how our logistic regressor fares
    as compared to professional wine tasters. We''ll be using the wine quality dataset
    ([https://archive.ics.uci.edu/ml/datasets/wine+quality](https://archive.ics.uci.edu/ml/datasets/wine+quality));
    details about the dataset are given in [Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml),
    *Principles and Foundation of AI and IoT*. The full code is in the file named `Wine_quality_using_logistic_regressor.ipynb`
    at the GitHub repository. Let''s understand the code step by step:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is loading all of the modules:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We read the data; in the present code, we are analyzing only the red wine,
    so we read data from the `winequality-red.csv` file. The file contains the data
    values separated not by commas, but instead by semicolons, so we need to specify
    the separator argument:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We separate from the data file input features and target quality. In the file,
    the target, wine quality is given on a scale from 0—10\. Here, for simplicity,
    we divide it into three classes, so if the initial quality is less than five,
    we make it the third class (signifying bad); between five and eight, we consider
    it `ok` (second class); and above eight, we consider it `good` (the first class).
    We also normalize the input features and split the data into training and test
    datasets:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The main part of the code is the `LogisticRegressor` class; at first glance,
    you''ll think that it''s similar to the `LinearRegressor` class we made earlier.
    The class is defined in the Python file, `LogisticRegressor.py`. It is indeed,
    but there are a few important differences: the `Y` `output` is replaced by `Y[pred]`,
    which instead of having a single value, now is a three-dimensional categorical
    value, each dimension specifying the probability of three categories. The weights
    here have dimensions of *d × n*, where `d` is the number of input features and
    `n` the number of output categories. The bias too now is three-dimensional. Another
    important change is the change in the loss function:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now we simply train our model and predict the output. The learned model gives
    us an accuracy of ~85% on the test dataset. Pretty impressive!
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using ML, we can also identify what ingredients make wine good quality. A company
    called IntelligentX recently started brewing beer based on user feedback; it uses
    AI to get the recipe for the tastiest beer. You can read about the work in this
    *Forbes* article: [https://www.forbes.com/sites/emmasandler/2016/07/07/you-can-now-drink-beer-brewed-by-artificial-intelligence/#21fd11cc74c3](https://www.forbes.com/sites/emmasandler/2016/07/07/you-can-now-drink-beer-brewed-by-artificial-intelligence/#21fd11cc74c3).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Classification using support vector machines
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Support Vector Machines** (**SVMs**) is arguably the most used ML technique
    for classification. The main idea behind SVM is that we find an optimal hyperplane
    with maximum margin separating the two classes. If the data is linearly separable,
    the process of finding the hyperplane is straightforward, but if it isn''t linearly
    separable, then kernel trick is used to make the data linearly separable in some
    transformed high-dimensional feature space.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'SVM is considered a non-parametric supervised learning algorithm. The main
    idea of SVM is to find a **maximal margin separator**: a separating hyperplane
    that is farthest from the training samples presented.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following diagram; the red dots represent class 1 for which the
    output should be 1, and the blue dots represent the class 2 for which the output
    should be -1\. There can be many lines which can separate the red dots from the
    blue ones; the diagram demonstrates three such lines: **A**, **B**, and **C**
    respectively. Which of the three lines do you think will be the best choice? Intuitively,
    the best choice is line B, because it''s farthest from the examples of both classes,
    and hence ensures the least error in classification:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/900315da-37fc-4a5b-a32f-78220b239342.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: In the following section, we'll learn the basic maths behind finding the maximal-separator
    hyperplane. Though the maths here is mostly basic, if you don't like maths you
    can simply skip to the implementation section where we use SVM to classify wine
    again! Cheers!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Maximum margin hyperplane
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From our knowledge of linear algebra, we know that the equation of a plane
    is given by the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f9a231d-0111-405f-9996-da15704d42da.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: 'In SVM, this plane should separate the positive classes (*y*= *1*) from the
    negative classes (*y*=*-1*), and there''s an additional constrain: the distance
    (margin) of this hyperplane from the closest positive and negative training vectors
    (*X[pos]* and *X[neg]* respectively) should be maximum. Hence, the plane is called
    the maximum margin separator.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The vectors *X*[*pos* ]and *X*[*neg* ]are called **support vectors,** and they
    play an important role in defining the SVM model.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, this means that the following is true:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/297a2f4c-fa15-4849-b82f-6966b1dd6eb9.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: 'And, so is this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1203bf35-5600-4a16-8e6c-1a320d09f32a.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'From these two equations, we get the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a6caae8-8666-4d86-a2fe-409fdb573500.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: 'Dividing by the weight vector length into both sides, we get the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fee17cdc-d92b-4cab-a6bd-bb1b5093899d.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'So we need to find a separator so that the margin between positive and negative
    support vectors is maximum, that is: ![](img/825f71a2-a026-4d06-9122-5a60ef4e70ee.png)
    is maximum, while at the same time all the points are classified correctly, such
    as the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a4af9f3-b099-47f9-867e-d1003b8823ae.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'Using a little maths, which we''ll not go into in this book, the preceding
    condition can be represented as finding an optimal solution to the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bef553ea-90e1-4f0b-a4e8-3bfae275d27a.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'Subject to the constraints that:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fe746d0-2a30-48cd-9ed6-783275dd1486.png)![](img/f7738f34-ca7f-4970-80ff-2439f0bf2464.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'From the values of alpha, we can get weights *W* from *α*, the vector of coefficients,
    using the following equation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4098e849-68a2-46d8-8c8a-6ce87f78a05b.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: This is a standard quadratic programming optimization problem. Most ML libraries
    have built-in functions to solve it, so you need not worry about how to do so.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: For the reader interested in knowing more about SVMs and the math behind it,
    the book *The Nature of Statistical Learning Theory* by Vladimir Vapnik, published
    by *Springer Science+Business Media*, 2013, is an excellent reference.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Kernel trick
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous method works fine when the input feature space is linearly separable.
    What should we do when it isn''t? One simple way is to transform the data (*X*)
    into a higher dimensional space where it''s linearly separable and find a maximal
    margin hyperplane in that high-dimensional space. Let''s see how; our hyperplane
    in terms of *α* is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/1cfc0edb-71e9-44d3-9b6d-500bfccdbac2.png)*'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Let *φ* be the transform, then we can replace *X* by *φ*(*X*) and hence its
    dot product *X^(T )X^((i))* with a function K(*X^T*, *X*^((*i*))) = *φ*(*X*)*^T*
    *φ*(*X*^((*i*))) called **kernel**. So we now just preprocess the data by applying
    the transform *φ* and then find a linear separator in the transformed space as
    before.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'The most commonly used kernel function is the **Gaussian kernel**, also called
    **radial basis function**, defined as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c95c021-9f0e-4d3f-8f22-7f0425e2e684.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: Classifying wine using SVM
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll use the `svm.SVC` function provided by the scikit library for the task.
    The reason to do so is that the TensorFlow library provides us, as of the time
    of writing, with only a linear implementation of SVM, and it works only for binary
    classification. We can make our own SVM using the maths we learned in previously
    in TensorFlow, and `SVM_TensorFlow.ipynb` in the GitHub repository contains the
    implementation in TensorFlow. The following code can be found in the `Wine_quality_using_SVM.ipynb`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'The SVC classifier of scikit is a support vector classifier. It can also handle
    multiclass support using a one-versus-one scheme. Some of the optional parameters
    of the method are as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '`C`: It''s a parameter specifying the penalty term (default value is `1.0`).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel`: It specifies the kernel to be used (default is `rbf`). The possible
    choices are `linear`, `poly`, `rbf`, `sigmoid`, `precomputed`, and `callable`.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma`: It specifies the kernel coefficient for `rbf`, `poly`, and `sigmoid` and
    the default value (the default is `auto`).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_state`: It sets the seed of the pseudo-random number generator to use
    when shuffling the data.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Follow the given steps to create our SVM model:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load all of the modules we''ll need for the code. Note that we aren''t
    importing TensorFlow here and instead have imported certain modules from the `scikit`
    library:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We read the data file, preprocess it, and separate it into test and training
    datasets. This time, for simplicity, we''re dividing into two classes, `good`
    and `bad`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we use the `SVC` classifier and train it on our training dataset with the
    `fit` method:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s now predict the output for the test dataset:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The model gave an accuracy of `67.5%` and the confusion matrix is as follows:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/a5309c9f-56b6-401b-9897-c3259f565a76.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'The preceding code uses the binary classification; we can change the code to
    work for more than two classes as well. For example, in the second step, we can
    replace the code with the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then we have three categories just as our previous logistic classifier, and
    the accuracy is 65.9%. And the confusion matrix is as follows:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the three-class case, the training data distribution is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '`good` `855`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ok` `734`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bad` `10`'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the number of samples in the `bad` class (corresponding to `0` in the
    confusion matrix) is only `10`, the model isn't able to learn what parameters
    contribute to bad wine quality. Hence, data should be uniformly distributed among
    all classes of the classifiers that we explore in this chapter.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Naive Bayes is one of the simplest and fastest ML algorithms. This too belongs
    to the class of supervised learning algorithms. It''s based on the Bayes probability
    theorem. One important assumption that we make in the case of the Naive Bayes
    classifier is that all of the features of the input vector are **independent and
    identically distributed** (**iid**). The goal is to learn a conditional probability
    model for each class *C*[*k*] in the training dataset:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9796dc8e-c201-401f-b6a8-4427c915c783.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'Under the iid assumption, and using the Bayes theorem, this can be expressed
    in terms of the joint probability distribution *p*(*C[k]*, *X*):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11e80a0f-f697-4acb-bc36-98b77039a6bf.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: 'We pick the class that maximizes this term ***Maximum A Posteriori* **(**MAP**):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4981d980-6d92-4ae7-8acf-a96f36da3805.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: There can be different Naive Bayes algorithms, depending upon the distribution
    of *p*(*x[i]*|*C[k]*). The common choices are Gaussian in the case of real-valued
    data, Bernoulli for binary data, and MultiNomial when the data contains the frequency
    of a certain event (such as document classification).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see whether we can classify the wine using Naive Bayes. For the sake
    of simplicity and efficiency, we'll use the scikit built-in Naive Bayes distributions.
    Since the features values we have in our data are continuous-valued—we'll assume
    that they have a Gaussian distribution, and we'll use `GaussianNB` of scikit-learn.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian Naive Bayes for wine quality
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The scikit-learn Naive Bayes module supports three Naive Bayes distributions.
    We can choose either of them depending on our input feature data type. The three
    Naive Bayes available in scikit-learn are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '`GaussianNB`'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MultinomialNB`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BernoulliNB`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The wine data, as we have already seen, is a continuous data type. Hence, it
    will be good if we use Gaussian distribution for *p*(*x[i]*|*C[k]*)—that is, the
    `GaussianNB` module, and so we'll add `from sklearn.naive_bayes import GaussianNB` in
    the import cell of the Notebook. You can read more details about the `GaussianNB`
    module from the is scikit-learn link: [http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'The first two steps will remain the same as in the SVM case. But now, instead
    of declaring an `SVM` classifier, we''ll declare a `GaussianNB` classifier and
    we''ll use its `fit` method to learn the training examples. The result from the
    learned model is obtained using the `predict` method. So follow these steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules. Note that now we''re importing `GaussianNB` from
    the `scikit` library:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Read the data file and preprocess it:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we declare a Gaussian Naive Bayes, train it on the training dataset, and
    use the trained model to predict the wine quality on the test dataset:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'That''s all, folks; our model is ready and kicking. The accuracy of this model
    is 71.25% for the binary classification case. In the following screenshot, you
    can a the heatmap of the confusion matrix:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/556bf535-7318-4f9b-9ee5-78f824c7035e.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'Before you conclude that Naive Bayes is best, let''s be aware of some of its
    pitfalls:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes makes the prediction based on the frequency-based probability; therefore,
    it's strongly dependent on the data we use for training.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another issue is that we made the iid assumption about input feature space;
    this isn't always true.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you''ll learn about another ML algorithm that''s very popular
    and fast—decision trees. In decision trees, we build a tree-like structure of
    decisions; we start with the root, choose a feature and split into branches, and
    continue till we reach the leaves, which represent the predicted class or value.
    The algorithm of decision trees involves two main steps:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Decide which features to choose and what conditions to use for splitting
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Know when to stop
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s understand it with an example. Consider a sample of 40 students; we
    have three variables: the gender (boy or girl; discrete), class (XI or XII; discrete),
    and height (5 to 6 feet; continuous). Eighteen students prefer to go to the library
    in their spare time and rest prefer to play. We can build a decision tree to predict
    who will be going to the library and who will be going to the playground in their
    leisure time. To build the decision tree, we''ll need to separate the students
    who go to library/playground based on the highly significant input variable among
    the three input variables. The following diagram gives the split based on each
    input variable:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2bebf484-66d9-4936-83c1-79dd982f6e57.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
- en: 'We consider all of the features and choose the one that gives us the maximum
    information. In the previous example, we can see that a split over the feature
    height generates the most homogeneous groups, with the group **Height > 5.5 ft**
    containing 80% students who play and 20% who go to the library in the leisure
    time and the group **Height < 5.5 ft** containing 13% students who play and 86%
    who go to the library in their spare time. Hence, we''ll make our first split
    on the feature height. We''ll continue the split in this manner and finally reach
    the decision (leaf node) telling us whether the student will play or go to the
    library in their spare time. The following diagram shows the decision tree structure;
    the black circle is the **Root** **Node**, the blue circles are the **Decision**
    **Nodes**, and the green circles are the **Leaf** **Nodes**:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc3007bb-01fb-4b84-ac7c-9a850c58f8bb.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: 'The decision trees belong to the family of greedy algorithms. To find the most
    homogeneous split, we define our cost function so that it tries to maximize the
    same class input values in a particular group. For regression, we generally use
    the mean square error cost function:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89ebe80b-22b8-431f-a110-be4f8f7ce105.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: Here, *y* and *y[pred]* represent the given and predicted output values for
    the input values (*i*); we find the split that minimizes this loss.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'For classification, we use either the *gini* impurity or cross-entropy as the
    loss function:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51381f73-fa84-40ff-bf3f-6c81da1bea53.png)![](img/104b5b6f-3a29-4cbd-a8af-358bc8d9e867.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: In the preceding, *c[k]* defines the proportion of same class input values present
    in a particular group.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Some good resources to learn more about decision trees are as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: L. Breiman, J. Friedman, R. Olshen, and C. Stone: *Classification and Regression
    Trees,* Wadsworth, Belmont, CA, 1984
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'J.R. Quinlan: *C4\. 5: programs for ML,* Morgan Kaufmann, 1993'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T. Hastie, R. Tibshirani and J. Friedman: *Elements of Statistical Learning*,
    Springer, 2009
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees in scikit
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `scikit` library provides `DecisionTreeRegressor` and `DecisionTreeClassifier`
    to implement regression and classification. Both can be imported from `sklearn.tree`.
    `DecisionTreeRegressor` is defined as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The different arguments are as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '`criterion`: It defines which loss function to use to determine the split.
    The default value is mean square error (`mse`). The library supports the use of `friedman_mse` and
    mean absolute error (`mae`) as loss functions.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`splitter`: We use this to decide whether to use the greedy strategy and go
    for the best split (default) or we can use random `splitter` to choose the best
    random split.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: It defines the maximum depth of the tree.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: It defines the minimum number of samples required to split
    an internal node. It can be integer or float (in this case it defines the percentage
    of minimum samples needed for the split).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DecisionTreeClassifier` is defined as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The different arguments are as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '`criterion`: It tells which loss function to use to determine the split. The
    default value for the classifier is the `gini`. The library supports the use of `entropy`
    as loss functions.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`splitter`: We use this to decide how to choose the split (default value is
    the best split) or we can use random `splitter` to choose the best random split.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: It defines the maximum depth of the tree. When the input feature
    space is large, we use this to restrict the maximum depth and take care of overfitting.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: It defines the minimum number of samples required to split
    an internal node. It can be integer or float (in this case it tells the percentage
    of minimum samples needed for the split).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ve listed only the commonly used preceding arguments; details regarding
    the remaining parameters of the two can be read on the scikit-learn website: [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) and [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees in action
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll use a decision tree regressor to predict electrical power output first.
    The dataset and its description have already been introduced in [Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml),
    *Principles and Foundations of IoT and AI*. The code is available at the GitHub repository
    in the file named `ElectricalPowerOutputPredictionUsingDecisionTrees.ipynb`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We get an R-square value of 0.90 and mean square error of 0.0047 on the test
    data; it''s a significant improvement over the prediction results obtained using
    linear regressor (R-square: `0.77`;mse: `0.012`).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also see the performance of decision trees in the classification task;
    we use it for the wine quality classification as before. The code is available
    in the `Wine_quality_using_DecisionTrees.ipynb` file in the GitHub repository:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The decision tree generates a classification accuracy of around 70%. We can
    see that, for small data size, we can use both decision trees and Naive Bayes
    with almost equal success. Decision trees suffer from overfitting, which can be
    taken care of by restricting the maximum depth or setting a minimum number of
    training inputs. They, like Naive Bayes, are unstable—a little variation in the
    data can result in a completely different tree; this can be resolved by making
    use of bagging and boosting techniques. Last, but not least, since it's a greedy
    algorithm, there's no guarantee that it returns a globally optimal solution.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our daily life, when we have to make a decision, we take guidance not from
    one person, but from many individuals whose wisdom we trust. The same can be applied
    in ML; instead of depending upon one single model, we can use a group of models
    (ensemble) to make a prediction or classification decision. This form of learning
    is called **ensemble learning**.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Conventionally, ensemble learning is used as the last step in many ML projects.
    It works best when the models are as independent of one another as possible. The
    following diagram gives a graphical representation of ensemble learning:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73af0f0f-00be-4082-a4a5-7d4540b67e24.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: 'The training of different models can take place either sequentially or in parallel.
    There are various ways to implement ensemble learning: voting, bagging and pasting,
    and random forest. Let''s see what each of these techniques and how we can implement
    them.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Voting classifier
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The voting classifier follows the majority; it aggregates the prediction of
    all the classifiers and chooses the class with maximum votes. For example, in
    the following screenshot, the voting classifier will predict the input instance
    to belong to class **1**:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f60022e-2236-4291-a411-268ed5152367.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: 'scikit has the `VotingClassifier` class to implement this. Using ensemble learning
    on wine quality classification, we reach an accuracy score of 74%, higher than
    any of the models considered alone. The complete code is in the `Wine_quality_using_Ensemble_learning.ipynb`
    file. The following is the main code to perform ensemble learning using voting:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Bagging and pasting
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In voting, we used different algorithms for training on the same dataset. We
    can also achieve ensemble learning by using different models with the same learning
    algorithm, but we train them on different training data subsets. The training
    subset is sampled randomly. The sampling can be done with replacement (bagging)
    or without replacement (pasting):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging**: In it, additional data for training is generated from the original
    dataset using combinations with repetitions. This helps in decreasing the variance
    of different models.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pasting**: Since pasting is without replacement, each subset of the training
    data can be used at most once. It''s more suitable if the original dataset is
    large.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `scikit` library has a method for performing bagging and pasting; from
    `sklearn.ensemble`, we can import `BaggingClassifier` and use it. The following
    code estimates `500` decision tree classifiers, each with `1000` training samples
    using bagging (for pasting, keep `bootstrap=False`):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This results in an accuracy of 77% for wine quality classification. The last
    argument to `BaggingClassifier`, `n_jobs`, defines how many CPU cores to use (that's
    the number of jobs to run in parallel); when its value is set to `-1`, then it
    uses all of the available CPU cores.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble of only decision trees is called **random forest**. And so what
    we've implemented previously is a random forest. We can directly implement random
    forest in scikit using the `RandomForestClassifier` class. The advantage of using
    the class is that it introduces extra randomness while building the tree. While
    splitting, it searches for the best feature to split among a random subset of
    features.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Improving your model – tips and tricks
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've learned a large number of ML algorithms, each with its
    own pros and cons. In this section, we'll look into some common problems and ways
    to resolve them.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Feature scaling to resolve uneven data scale
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data that''s collected normally doesn''t have the same scale; for example,
    one feature may be varying in the range 10–100 and another one may be only distributed
    in range 2–5\. This uneven data scale can have an adverse effect on learning.
    To resolve this, we use the method of feature scaling (normalization). The choice
    of normalization has been found to drastically affect the performance of certain
    algorithms. Two common normalization methods (also called standardization in some
    books) are as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '**Z-score normalization**: In z-score normalization, each individual feature
    is scaled so that it has the properties of a standard normal distribution, that
    is, a mean of *0* and variance of *1*. If *μ* is the mean and *σ* the variance,
    we can compute Z-score normalization by making the following linear transformation
    on each feature as follows:'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/40dddd5e-f8af-4ab1-8b59-5a708d51b61b.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: '**Min-max normalization**: The min-max normalization rescales the input features
    so that they lie in the range between *0* and *1*. It results in reducing the
    standard deviation in the data and hence suppresses the effect of outliers. To
    achieve min-max normalization, we find the maximum and minimum value of the feature
    (*x[max]* and *x[min]* respectively), and perform the following linear transformation:'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/724c7e3c-3d4d-4643-a5b5-bf3c29c82b30.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: We can use the `scikit` library `StandardScaler` or `MinMaxscaler` methods to
    normalize the data. In all of the examples in this chapter, we've used `MinMaxScaler`;
    you can try changing it to `StandardScalar` and observe if the performance changes.
    In the next chapter, we'll also learn how to perform these normalizations in TensorFlow.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes the model tries to overfit the training dataset; in doing so, it
    loses its ability to generalize and hence performs badly on the validation dataset;
    this in turn will affect its performance on unseen data values. There are two
    standard ways to take care of overfitting: regularization and cross-validation.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regularization adds a term in the loss function to ensure that the cost increases
    as the model increases the number of features. Hence, we force the model to stay
    simpler. If *L(X*, *Y)* was the loss function earlier, we replace it with the
    following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c6cd641-a45d-4cb1-abc4-902e2e23e1ac.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding, *N* can be *L*[1] norm, *L*[2] norm, or a combination of
    the two, and *λ* is the regularization coefficient. Regularization helps in reducing
    the model variance, without losing any important properties of the data distribution:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '**Lasso regularization**: In this case, the *N* is *L*[1] norm. It uses the
    modulus of weight as the penalty term *N:*'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/76d82420-39b8-4956-bc07-a4fc8d6c1000.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: '**Ridge regularization**: In this case, the *N* is *L2* norm, given by the
    following:'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c03bc8ca-e933-4a96-bd00-4bfc0c492b73.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: Cross-validation
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using cross-validation can also help in reducing the problem of overfitting.
    In *k*-fold cross-validation, data is divided into *k*-subsets, called **folds**.
    Then it trains and evaluates the model *k*-times; each time, it picks one of the
    folds for validation and the rest for training the model. We can perform the cross-validation
    when the data is less and training time is small. scikit provides a `cross_val_score`
    method to implement the k-folds. Let `classifier` be the model we want to cross-validate,
    then we can use the following code to perform cross-validation on `10` folds:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The result of this is an average mean and variance value. A good model should
    have a high average and low variance.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: No Free Lunch theorem
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With so many models, one always wonders which one to use. Wolpert, in his famous
    paper *The Lack of A Priori Distinctions Between Learning*, explored this issue
    and showed that if we make no prior assumption about the input data, then there's
    no reason to prefer one model over any other. This is known as the **No Free Lunch**
    **theorem**.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: This means that there's no model hat can be *a* priori guaranteed to work better.
    The only way we can ascertain which model is best is by evaluating them all. But,
    practically, it isn't possible to evaluate all of the models and so, in practice,
    we make reasonable assumptions about the data and evaluate a few relevant models.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning and grid search
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Different models have different hyperparameters; for example, in linear regressor,
    the learning rate was a hyperparameter; if we''re using regularization, then the
    regularizing parameter λ is a hyperparameter. What should be their value? While
    there''s a rule of thumb for some hyperparameters, most of the time we make either
    a guess or use grid search to perform a sequential search for the best hyperparameters.
    In the following, we present the code to perform hyperparameter search in the
    case of SVM using the `scikit` library; in the next chapter, we''ll see how we
    can use TensorFlow to perform hyperparameter tuning:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`GridSearchCV` will provide us with the hyperparameters that produce the best
    results for the SVM classifier.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this chapter was to provide you with  intuitive understanding of
    different standard ML algorithms so that you can make an informed choice. We covered
    the popular ML algorithms used for classification and regression.We also learnt
    how supervised and unsupervised learning are different from each other. Linear
    regression, logistic regression, SVM, Naive Bayes, and decision trees were introduced
    along with the fundamental principles involved in each. We used the regression
    methods to predict electrical power production of a thermal station and classification
    methods to classify wine as good or bad. Lastly, we covered the common problems
    with different ML algorithms and some tips and tricks to solve them.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll study different deep learning models and learn how
    to use them to analyze our data and make predictions.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
