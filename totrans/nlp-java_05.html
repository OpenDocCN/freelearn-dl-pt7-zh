<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Finding and Working with Words"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Finding and Working with Words</h1></div></div></div><p>In this chapter, we cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introduction to tokenizer factories – finding words in a character stream</li><li class="listitem" style="list-style-type: disc">Combining tokenizers – lowercase tokenizer</li><li class="listitem" style="list-style-type: disc">Combining tokenizers – stop word tokenizers</li><li class="listitem" style="list-style-type: disc">Using Lucene/Solr tokenizers</li><li class="listitem" style="list-style-type: disc">Using Lucene/Solr tokenizers with LingPipe</li><li class="listitem" style="list-style-type: disc">Evaluating tokenizers with unit tests</li><li class="listitem" style="list-style-type: disc">Modifying tokenizer factories</li><li class="listitem" style="list-style-type: disc">Finding words for languages without white spaces</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec21"/>Introduction</h1></div></div></div><p>An important part of building NLP systems is to work with the appropriate unit for processing. This chapter addresses the abstraction layer associated with the word level of processing. This is called tokenization, which amounts to grouping adjacent characters into meaningful chunks in support of classification, entity finding, and the rest of NLP.</p><p>LingPipe provides a broad range of tokenizer needs, which are not covered in this book. Look at the Javadoc for tokenizers that do stemming, Soundex (tokens based on what English words sound like), and more.</p></div></div>
<div class="section" title="Introduction to tokenizer factories &#x2013; finding words in a character stream"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec22"/>Introduction to tokenizer factories – finding words in a character stream</h1></div></div></div><p>LingPipe tokenizers are built on a common pattern of a base tokenizer that can be used on its own, or can be as the source for subsequent filtering tokenizers. Filtering tokenizers manipulate the tokens/white spaces provided by the base tokenizer. This recipe covers our most commonly used tokenizer, <code class="literal">IndoEuropeanTokenizerFactory</code>, which is good for languages that use the Indo-European style of punctuation and word separators—examples include English, Spanish, and French. As always, the Javadoc has useful information.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>
<code class="literal">IndoEuropeanTokenizerFactory</code> creates <a class="indexterm" id="id149"/>tokenizers with built-in support for alpha-numerics, numbers, and other common constructs in Indo-European languages.</p><p>The tokenization rules are roughly based on those used in MUC-6 but are necessarily more fine grained, because the MUC tokenizers are based on lexical and semantic information, such as whether a string is an abbreviation.</p></div></div><p>MUC-6<a class="indexterm" id="id150"/> refers to the Message Understanding Conference that originated the idea of government-sponsored <a class="indexterm" id="id151"/>competitions between contractors in 1995. The informal term was <span class="emphasis"><em>Bake off</em></span>, in reference to the Pillsbury Bake-Off that started in 1949, and one of the authors was a participant as postdoc in MUC-6. MUC drove much of the innovation in the evaluation of NLP systems.</p><p>LingPipe <a class="indexterm" id="id152"/>tokenizers are built using the LingPipe<a class="indexterm" id="id153"/> <code class="literal">TokenizerFactory</code> interface, which<a class="indexterm" id="id154"/> provides a way of invoking different types of tokenizers using the same interface. This is very useful in creating filtered tokenizers, which are constructed as a chain of tokenizers and modify their output in some way. A <code class="literal">TokenizerFactory</code> instance<a class="indexterm" id="id155"/> might be created either as a basic tokenizer, which takes simple parameters in its construction, or as a filtered tokenizer, which takes other tokenizer factory objects as parameters. In either case, an instance of <code class="literal">TokenizerFactory</code> has a<a class="indexterm" id="id156"/> single <code class="literal">tokenize()</code> method, which takes input as a character array, a start index, and the number of characters to process and outputs a <code class="literal">Tokenizer</code> object. The <code class="literal">Tokenizer</code> object<a class="indexterm" id="id157"/> represents the state of tokenizing a particular slice of string and provides a stream of tokens. While <code class="literal">TokenizerFactory</code> is thread safe and/or serializable, tokenizer instances are typically neither thread safe nor serializable. The <code class="literal">Tokenizer</code> object provides methods to iterate over the tokens in the string and to provide token positions of the tokens in the underlying text.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec44"/>Getting ready</h2></div></div></div><p>Download the JAR file and source for the book if you have not already done so.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec45"/>How to do it...</h2></div></div></div><p>It is all pretty simple. The <a class="indexterm" id="id158"/>following are the steps to get started with<a class="indexterm" id="id159"/> tokenization:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Go to the <code class="literal">cookbook</code> directory and invoke the following class:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp "lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar" com.lingpipe.cookbook.chapter2.RunBaseTokenizerFactory</strong></span>
</pre></div><p>This will lead us to a command prompt, which asks us to type in some text:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>type a sentence to see tokens and white spaces</strong></span>
</pre></div></li><li class="listitem">If we type a sentence such as: <code class="literal">It's no use growing older if you only learn new ways of misbehaving yourself</code>, we will get the following output:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>It's no use growing older if you only learn new ways of misbehaving yourself. </strong></span>
<span class="strong"><strong>Token:'It'</strong></span>
<span class="strong"><strong>WhiteSpace:''</strong></span>
<span class="strong"><strong>Token:'''</strong></span>
<span class="strong"><strong>WhiteSpace:''</strong></span>
<span class="strong"><strong>Token:'s'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'no'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'use'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'growing'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'older'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'if'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'you'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'only'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'learn'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'new'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'ways'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'of'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'misbehaving'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'yourself'</strong></span>
<span class="strong"><strong>WhiteSpace:''</strong></span>
<span class="strong"><strong>Token:'.'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
</pre></div></li><li class="listitem">Examine the output and note what the tokens and white spaces are. The text is from the short story, <span class="emphasis"><em>The Stampeding of Lady Bastable</em></span>, by Saki.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec46"/>How it works...</h2></div></div></div><p>The code is so <a class="indexterm" id="id160"/>simple that it can be included in its entirety as<a class="indexterm" id="id161"/> follows:</p><div class="informalexample"><pre class="programlisting">package com.lingpipe.cookbook.chapter2;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;

public class RunBaseTokenizerFactory {

  public static void main(String[] args) throws IOException {
    TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
    BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));

    while (true) {
      System.out.println("type a sentence to " + "see the tokens and white spaces");
      String input = reader.readLine();
      Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(), 0, input.length());
      String token = null;
      while ((token = tokenizer.nextToken()) != null) {
        System.out.println("Token:'" + token + "'");
        System.out.println("WhiteSpace:'" + tokenizer.nextWhitespace() + "'");
        
      }
    }
  }
}</pre></div><p>This <a class="indexterm" id="id162"/>recipe starts with the creation of <code class="literal">TokenizerFactory tokFactory</code> in the first statement of the <code class="literal">main()</code> method. Note that a singleton <code class="literal">IndoEuropeanTokenizerFactory.INSTANCE</code> is used. The factory will produce tokenizers for a given string, which is evident in the line, <code class="literal">Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(), 0, input.length())</code>. The<a class="indexterm" id="id163"/> entered string is converted to a character array with <code class="literal">input.toCharArray()</code> as the first argument to the <code class="literal">tokenizer</code> method and the start and finish offsets provided into the created character array.</p><p>The resulting <code class="literal">tokenizer</code> provides tokens for the provided slice of character array, and the white spaces and tokens are printed out in the <code class="literal">while</code> loop. Calling the <code class="literal">tokenizer.nextToken()</code> method<a class="indexterm" id="id164"/> does a few things:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The method returns the next token or null if there is no next token. The null then ends the loop; otherwise, the loop continues.</li><li class="listitem" style="list-style-type: disc">The method also increments the corresponding white space. There is always a white space with a token, but it might be the empty string.</li></ul></div><p>
<code class="literal">IndoEuropeanTokenizerFactory</code> assumes<a class="indexterm" id="id165"/> a fairly standard abstraction over characters that break down as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Characters from the beginning of the <code class="literal">char</code> array to the first token are ignored and not reported as white space</li><li class="listitem" style="list-style-type: disc">Characters from the end of the last token to the end of the <code class="literal">char</code> array are reported as the next white space</li><li class="listitem" style="list-style-type: disc">White spaces can be the empty string because of two adjoining tokens—note the apostrophe in the output and corresponding white spaces</li></ul></div><p>This means<a class="indexterm" id="id166"/> that it is not possible to reconstruct the <a class="indexterm" id="id167"/>original string necessarily if the input does not start with a token. Fortunately, tokenizers are easily modified for customized needs. We will see this later in the chapter.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec47"/>There's more…</h2></div></div></div><p>Tokenization<a class="indexterm" id="id168"/> can be arbitrarily complex. The LingPipe tokenizers are intended to cover most common uses, but you might need to create your own tokenizer to have fine-grained control, for example, Victoria's Secret with "Victoria's" as the token. Consult the source for <code class="literal">IndoEuropeanTokenizerFactory</code> if such customization is needed, to see how arbitrary tokenization is done here.</p></div></div>
<div class="section" title="Combining tokenizers &#x2013; lowercase tokenizer"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec23"/>Combining tokenizers – lowercase tokenizer</h1></div></div></div><p>We mentioned in the previous recipe<a class="indexterm" id="id169"/> that LingPipe tokenizers can be basic or filtered. Basic tokenizers, such as the Indo-European tokenizer, don't need much in terms of parameterization, none at all as a matter of fact. However, filtered tokenizers need a tokenizer as a parameter. What we're doing with filtered tokenizers is invoking multiple tokenizers where a base tokenizer is usually modified by a filter to produce a different tokenizer.</p><p>LingPipe provides <a class="indexterm" id="id170"/>several basic <a class="indexterm" id="id171"/>tokenizers, such as <code class="literal">IndoEuropeanTokenizerFactory</code> <a class="indexterm" id="id172"/>or <code class="literal">CharacterTokenizerFactory</code>. A complete list can be found in the Javadoc for LingPipe. In this section, we'll show you how to combine an Indo-European tokenizer with a lowercase tokenizer. This is a fairly common process that many search engines implement for Indo-European languages.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec48"/>Getting ready</h2></div></div></div><p>You will need to download the JAR file for the book and have Java and Eclipse set up so that you can run the example.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec49"/>How to do it...</h2></div></div></div><p>This works just the same<a class="indexterm" id="id173"/> way as the previous recipe. Perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Invoke the <code class="literal">RunLowerCaseTokenizerFactory</code> class from the command line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp "lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar" com.lingpipe.cookbook.chapter2.RunLowerCaseTokenizerFactory.</strong></span>
</pre></div></li><li class="listitem">Then, in the command prompt, let's use the following example:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>type a sentence below to see the tokens and white spaces are:</strong></span>
<span class="strong"><strong>This is an UPPERCASE word and these are numbers 1 2 3 4.5.</strong></span>
<span class="strong"><strong>Token:'this'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'is'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'an'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'uppercase'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'word'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'and'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'these'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'are'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'numbers'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'1'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'2'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'3'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'4.5'</strong></span>
<span class="strong"><strong>WhiteSpace:''</strong></span>
<span class="strong"><strong>Token:'.'</strong></span>
<span class="strong"><strong>WhiteSpace:''</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec50"/>How it works...</h2></div></div></div><p>You can see in the preceding<a class="indexterm" id="id174"/> output that all the tokens are converted to lowercase, including<a class="indexterm" id="id175"/> the word <code class="literal">UPPERCASE</code>, which was typed in uppercase. As this example uses an Indo-European tokenizer as its base tokenizer, you can see that the number 4.5 is retained as <code class="literal">4.5</code> instead of being broken up into 4 and 5.</p><p>The way we put tokenizers together is very simple:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>public static void</strong></span> main(String[] args) <span class="strong"><strong>throws</strong></span> IOException {
  
  TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
  tokFactory = <span class="strong"><strong>new</strong></span> LowerCaseTokenizerFactory(tokFactory);
  tokFactory = <span class="strong"><strong>new</strong></span> WhitespaceNormTokenizerFactory(tokFactory);
  
  BufferedReader reader = <span class="strong"><strong>new</strong></span> BufferedReader(<span class="strong"><strong>new</strong></span> InputStreamReader(System.in));
  
  <span class="strong"><strong>while (true)</strong></span> {
    System.out.println("type a sentence below to see the tokens and white spaces are:");
    String input = reader.readLine();
    Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(), 0, input.length());
    String token = <span class="strong"><strong>null</strong></span>;
    <span class="strong"><strong>while</strong></span> ((token = tokenizer.nextToken()) != <span class="strong"><strong>null</strong></span>) {
      System.out.println("Token:'" + token + "'");
      System.out.println("WhiteSpace:'" + tokenizer.nextWhitespace() + "'");
    }
  }
}</pre></div><p>Here, we created a tokenizer that returns case and white space normalized tokens produced using an Indo-European tokenizer. The tokenizer created from the tokenizer factory is a filtered tokenizer that starts with the Indo-European base tokenizer, which is then modified by <code class="literal">LowerCaseTokenizer</code> to produce the lowercase tokenizer. This is then once again modified by <code class="literal">WhiteSpaceNormTokenizerFactory</code> to produce a lowercase, white space-normalized Indo-European tokenizer.</p><p>Case normalization is applied <a class="indexterm" id="id176"/>where the case of words doesn't matter much; for example, search engines often store case-normalized words in their indexes. Now, we will use case-normalized tokens in the <a class="indexterm" id="id177"/>upcoming examples on classifiers.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec51"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For more details on how filtered tokenizers are built, see the Javadoc for the abstract class, <code class="literal">ModifiedTokenizerFactory.</code></li></ul></div></div></div>
<div class="section" title="Combining tokenizers &#x2013; stop word tokenizers"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec24"/>Combining tokenizers – stop word tokenizers</h1></div></div></div><p>Similarly to the way in<a class="indexterm" id="id178"/> which we put together a lowercase and white space normalized tokenizer, we can use a filtered tokenizer to create a tokenizer that filters out stop words. Once again, using search engines as our example, we can remove commonly occurring words from our input set so as to normalize the text. The stop words that are typically removed convey very little information by themselves, although they might convey information in context.</p><p>The input is <a class="indexterm" id="id179"/>tokenized using whatever base tokenizer is set up, and then, the resulting tokens are filtered out by the stop tokenizer to produce a token stream that is free of the stop words specified when the stop tokenizer is initialized.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec52"/>Getting ready</h2></div></div></div><p>You will need to download the JAR file for the book and have Java and Eclipse set up so that you can run the example.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec53"/>How to do it...</h2></div></div></div><p>As we did earlier, we will go through the steps of interacting with the tokenizer:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Invoke the <code class="literal">RunStopTokenizerFactory</code> class from the command line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp "lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar" com.lingpipe.cookbook.chapter2.RunStopTokenizerFactory</strong></span>
</pre></div></li><li class="listitem">Then, in the prompt, let's use the following example:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>type a sentence below to see the tokens and white spaces:</strong></span>
<span class="strong"><strong>the quick brown fox is jumping</strong></span>
<span class="strong"><strong>Token:'quick'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'brown'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'fox'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'jumping'</strong></span>
<span class="strong"><strong>WhiteSpace:''</strong></span>
</pre></div></li><li class="listitem">Note that we lose adjacency information. In the input, we have <code class="literal">fox is jumping</code>, but the tokens came out as <code class="literal">fox</code> followed by <code class="literal">jumping</code>, because <code class="literal">is</code> was filtered. This can be a problem for token-based processes that need accurate adjacency information. In the <span class="emphasis"><em>Foreground- or background-driven interesting phrase detection</em></span> recipe of <a class="link" href="ch04.html" title="Chapter 4. Tagging Words and Tokens">Chapter 4</a>, <span class="emphasis"><em>Tagging Words and Tokens</em></span>, we will show a length-based filtering tokenizer that preserves adjacency.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec54"/>How it works...</h2></div></div></div><p>The stop words <a class="indexterm" id="id180"/>used in this <code class="literal">StopTokenizerFactory</code> filter<a class="indexterm" id="id181"/> are just<a class="indexterm" id="id182"/> a very short list of words, <code class="literal">is</code>, <code class="literal">of</code>, <code class="literal">the</code>, and <code class="literal">to</code>. Obviously, this list can be much longer if required. As you saw in the preceding output, the words <code class="literal">the</code> and <code class="literal">is</code> have been removed from the tokenized output. This is done with a very simple step: we instantiate <code class="literal">StopTokenizerFactory</code> in <code class="literal">src/com/lingpipe/cookbook/chapter2/RunStopTokenizerFactory.java</code>. The relevant code is:</p><div class="informalexample"><pre class="programlisting">TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
tokFactory = new LowerCaseTokenizerFactory(tokFactory);
Set&lt;String&gt; stopWords = new HashSet&lt;String&gt;();
stopWords.add("the");
stopWords.add("of");
stopWords.add("to");
stopWords.add("is");

tokFactory = new StopTokenizerFactory(tokFactory, stopWords);</pre></div><p>As we're using <code class="literal">LowerCaseTokenizerFactory</code> as one <a class="indexterm" id="id183"/>of the filters in the tokenizer factory, we can get away with the stop words that contain only lowercase words. If we want to preserve the case of the input tokens and continue to remove the stop words, we will need to add uppercase or mixed-case <a class="indexterm" id="id184"/>versions as well.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec55"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The complete list of filtered tokenizers provided by LingPipe can be found on the<a class="indexterm" id="id185"/> Javadoc page at <a class="ulink" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html">http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html</a></li></ul></div></div></div>
<div class="section" title="Using Lucene/Solr tokenizers"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec25"/>Using Lucene/Solr tokenizers</h1></div></div></div><p>The very popular search engine, Lucene, includes many analysis modules, which provide general purpose tokenizers as well as language-specific tokenizers from Arabic to Thai. As of Lucene 4, most of these different analyzers can be found in separate JAR files. We will cover Lucene tokenizers, because they can be used as LingPipe tokenizers, as you will see in the next recipe.</p><p>Much like the LingPipe tokenizers, Lucene tokenizers<a class="indexterm" id="id186"/> also can be split into basic tokenizers and filtered <a class="indexterm" id="id187"/>tokenizers. Basic tokenizers take a reader as input, and filtered tokenizers take other tokenizers as input. We will look at an example of using a standard Lucene analyzer along with a lowercase-filtered tokenizer. A Lucene analyzer essentially maps a field to a token stream. So, if you have an existing Lucene index, you can use the analyzer with the field name instead of the raw tokenizer, as we will show in the later part of this chapter.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec56"/>Getting ready</h2></div></div></div><p>You will need to download the JAR file for the book and have Java and Eclipse set up so that you can run the example. Some of the Lucene analyzers used in the examples are part of the <code class="literal">lib</code> directory. However, if you'd like to experiment with other language analyzers, download them from the Apache Lucene<a class="indexterm" id="id188"/> website at <a class="ulink" href="https://lucene.apache.org">https://lucene.apache.org</a>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec57"/>How to do it...</h2></div></div></div><p>Remember that we are not using a LingPipe tokenizer in this recipe but introducing the Lucene tokenizer classes:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Invoke the <code class="literal">RunLuceneTokenizer</code> class from the command line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lucene-analyzers-common-4.6.0.jar:lib/lucene-core-4.6.0.jar com.lingpipe.cookbook.chapter2.RunLuceneTokenize</strong></span>
</pre></div></li><li class="listitem">Then, in the prompt, let's use the following example:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>the quick BROWN fox jumped</strong></span>
<span class="strong"><strong>type a sentence below to see the tokens and white spaces:</strong></span>
<span class="strong"><strong>The rain in Spain.</strong></span>
<span class="strong"><strong>Token:'the' Start: 0 End:3</strong></span>
<span class="strong"><strong>Token:'rain' Start: 4 End:8</strong></span>
<span class="strong"><strong>Token:'in' Start: 9 End:11</strong></span>
<span class="strong"><strong>Token:'spain' Start: 12 End:17</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec58"/>How it works...</h2></div></div></div><p>Let's review the <a class="indexterm" id="id189"/>following code to see how the Lucene tokenizers differ in<a class="indexterm" id="id190"/> invocation from the previous examples—the relevant part of the code from <code class="literal">src/com/lingpipe/cookbook/chapter2/RunLuceneTokenizer.java</code> is:</p><div class="informalexample"><pre class="programlisting">BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));

while (true) {</pre></div><p>The preceding snippet sets up <code class="literal">BufferedReader</code> from the command line and starts a perpetual <code class="literal">while()</code> loop. Next, the prompt is provided, the <code class="literal">input</code> is read, and it is used to construct a <code class="literal">Reader</code> object:</p><div class="informalexample"><pre class="programlisting">System.out.println("type a sentence below to see the tokens and white spaces:");
String input = reader.readLine();
Reader stringReader = new StringReader(input);</pre></div><p>All the input is now wrapped up, and it is time to construct the actual tokenizer:</p><div class="informalexample"><pre class="programlisting">TokenStream tokenStream = new StandardTokenizer(Version.LUCENE_46,stringReader);

tokenStream = new LowerCaseFilter(Version.LUCENE_46,tokenStream);</pre></div><p>The input text is used to construct <code class="literal">StandardTokenizer</code> with Lucene's versioning system supplied—this produces an instance of <code class="literal">TokenStream</code>. Then, we used <code class="literal">LowerCaseFilter</code> to create the final filtered <code class="literal">tokenStream</code> with the base <code class="literal">tokenStream</code> as an argument.</p><p>In Lucene, we need to attach the attributes we're interested in from the token stream; this is done by the <code class="literal">addAttribute</code> method:</p><div class="informalexample"><pre class="programlisting">CharTermAttribute terms = tokenStream.addAttribute(CharTermAttribute.class);
OffsetAttribute offset = tokenStream.addAttribute(OffsetAttribute.class);
tokenStream.reset();</pre></div><p>Note that in Lucene 4, once the tokenizer has been instantiated, the <code class="literal">reset()</code> method must be called before using the tokenizer:</p><div class="informalexample"><pre class="programlisting">while (tokenStream.incrementToken()) {
  String token = terms.toString();
  int start = offset.startOffset();
  int end = offset.endOffset();
  System.out.println("Token:'" + token + "'" + " Start: " + start + " End:" + end);
}</pre></div><p>The <code class="literal">tokenStream</code> is wrapped<a class="indexterm" id="id191"/> up <a class="indexterm" id="id192"/>with the following:</p><div class="informalexample"><pre class="programlisting">tokenStream.end();
tokenStream.close();</pre></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec59"/>See also</h2></div></div></div><p>An excellent introduction to Lucene is in <span class="emphasis"><em>Text Processing with Java</em></span>, <span class="emphasis"><em>Mitzi Morris</em></span>, <span class="emphasis"><em>Colloquial Media Corporation</em></span>, where the guts of what we explained earlier are made clearer than what we can provide in a recipe.</p></div></div>
<div class="section" title="Using Lucene/Solr tokenizers with LingPipe"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec26"/>Using Lucene/Solr tokenizers with LingPipe</h1></div></div></div><p>We can use these <a class="indexterm" id="id193"/>Lucene tokenizers with LingPipe; this is <a class="indexterm" id="id194"/>useful because Lucene has such a rich set of <a class="indexterm" id="id195"/>them. We are going to show how to <a class="indexterm" id="id196"/>wrap a Lucene <code class="literal">TokenStream</code> into a LingPipe <code class="literal">TokenizerFactory</code> by extending the <code class="literal">Tokenizer</code> abstract class.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec60"/>How to do it...</h2></div></div></div><p>We will shake things up a bit and have a recipe that is not interactive. Perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Invoke the <code class="literal">LuceneAnalyzerTokenizerFactory</code> class from the command line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lucene-analyzers-common-4.6.0.jar:lib/lucene-core-4.6.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter2.LuceneAnalyzerTokenizerFactory</strong></span>
</pre></div></li><li class="listitem">The <code class="literal">main()</code> method in the class specifies the input:<div class="informalexample"><pre class="programlisting">String text = "Hi how are you? " + "Are the numbers 1 2 3 4.5 all integers?";
Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_46);
TokenizerFactory tokFactory = new LuceneAnalyzerTokenizerFactory(analyzer, "DEFAULT");
Tokenizer tokenizer = tokFactory.tokenizer(text.toCharArray(), 0, text.length());

String token = null;
while ((token = tokenizer.nextToken()) != null) {
  String ws = tokenizer.nextWhitespace();
  System.out.println("Token:'" + token + "'");
  System.out.println("WhiteSpace:'" + ws + "'");
}</pre></div></li><li class="listitem">The preceding <a class="indexterm" id="id197"/>snippet creates a Lucene <code class="literal">StandardAnalyzer</code> and<a class="indexterm" id="id198"/> uses it to construct a<a class="indexterm" id="id199"/> LingPipe <code class="literal">TokenizerFactory</code>. The output is as follows—the <code class="literal">StandardAnalyzer</code> filters<a class="indexterm" id="id200"/> stop words, so the token <code class="literal">are</code> is filtered:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Token:'hi'</strong></span>
<span class="strong"><strong>WhiteSpace:'default'</strong></span>
<span class="strong"><strong>Token:'how'</strong></span>
<span class="strong"><strong>WhiteSpace:'default'</strong></span>
<span class="strong"><strong>Token:'you'</strong></span>
<span class="strong"><strong>WhiteSpace:'default'</strong></span>
<span class="strong"><strong>Token:'numbers'</strong></span>
<span class="strong"><strong>WhiteSpace:'default'</strong></span>
</pre></div></li><li class="listitem">The white spaces report as <code class="literal">default</code> because the implementation does not accurately provide white spaces but goes with a default. We will discuss this limitation in the <span class="emphasis"><em>How it works…</em></span> section.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec61"/>How it works...</h2></div></div></div><p>Let's take a look at the <code class="literal">LuceneAnalyzerTokenizerFactory</code> class. This class implements the LingPipe <code class="literal">TokenizerFactory</code> interface by wrapping a Lucene analyzer. We will start with the class definition from <code class="literal">src/com/lingpipe/cookbook/chapter2/LuceneAnalyzerTokenizerFactory.java</code>:</p><div class="informalexample"><pre class="programlisting">public class LuceneAnalyzerTokenizerFactory implements TokenizerFactory, Serializable {
  
  private static final long serialVersionUID = 8376017491713196935L;
  private Analyzer analyzer;
  private String field;
  public LuceneAnalyzerTokenizerFactory(Analyzer analyzer, String field) {
    super();
    this.analyzer = analyzer;
    this.field = field;
  }</pre></div><p>The<a class="indexterm" id="id201"/> constructor stores the analyzer and the name of the field <a class="indexterm" id="id202"/>as private variables. As this class implements <a class="indexterm" id="id203"/>the <code class="literal">TokenizerFactory</code> interface, we<a class="indexterm" id="id204"/> need to implement the <code class="literal">tokenizer()</code> method:</p><div class="informalexample"><pre class="programlisting">public Tokenizer tokenizer(char[] charSeq , int start, int length) {
  Reader reader = new CharArrayReader(charSeq,start,length);
  TokenStream tokenStream = analyzer.tokenStream(field,reader);
  return new LuceneTokenStreamTokenizer(tokenStream);
}</pre></div><p>The <code class="literal">tokenizer()</code> method<a class="indexterm" id="id205"/> creates a new character-array reader and passes it to the Lucene analyzer to convert it to a <code class="literal">TokenStream</code>. An instance of <code class="literal">LuceneTokenStreamTokenizer</code> is created based on the token stream. <code class="literal">LuceneTokenStreamTokenizer</code> is a nested static class that extends LingPipe's <code class="literal">Tokenizer</code> class:</p><div class="informalexample"><pre class="programlisting">static class LuceneTokenStreamTokenizer extends Tokenizer {
  private TokenStream tokenStream;
  private CharTermAttribute termAttribute;
  private OffsetAttribute offsetAttribute;
    
  private int lastTokenStartPosition = -1;
  private int lastTokenEndPosition = -1;
    
  public LuceneTokenStreamTokenizer(TokenStream ts) {
    tokenStream = ts;
    termAttribute = tokenStream.addAttribute(
      CharTermAttribute.class);
    offsetAttribute = tokenStream.addAttribute(OffsetAttribute.class);
  }</pre></div><p>The constructor stores <code class="literal">TokenStream</code> and attaches the term and the offset attributes. In the previous recipe, we saw that the term and the offset attributes contain the token string, and the token start and end offsets into the input text. The token offsets are also initialized to <code class="literal">-1</code> before any tokens are found:</p><div class="informalexample"><pre class="programlisting">@Override
public String nextToken() {
  try {
    if (tokenStream.incrementToken()){
      lastTokenStartPosition = offsetAttribute.startOffset();
      lastTokenEndPosition = offsetAttribute.endOffset();
      return termAttribute.toString();
    } else {
      endAndClose();
      return null;
    }
  } catch (IOException e) {
    endAndClose();
    return null;
  }
}</pre></div><p>We will<a class="indexterm" id="id206"/> implement the <code class="literal">nextToken()</code> method<a class="indexterm" id="id207"/> and use the <code class="literal">incrementToken()</code> method of the token stream to retrieve any tokens from the<a class="indexterm" id="id208"/> token stream. We will set the token start and end offsets<a class="indexterm" id="id209"/> using <code class="literal">OffsetAttribute</code>. If the token <a class="indexterm" id="id210"/>stream is finished or the <code class="literal">incrementToken()</code> method<a class="indexterm" id="id211"/> throws an I/O exception, we will end and close the <code class="literal">TokenStream</code>.</p><p>The <code class="literal">nextWhitespace()</code> method<a class="indexterm" id="id212"/> has some limitations, because <code class="literal">offsetAttribute</code> is focused on the current token where LingPipe tokenizers quantize the input into the next token and next offset. A general solution here will be quite challenging, because there might not be any well-defined white spaces between tokens—think character ngrams. So, the <code class="literal">default</code> string is supplied just to make it clear. The method is:</p><div class="informalexample"><pre class="programlisting">@Override
public String nextWhitespace() {   return "default";
}</pre></div><p>The code also covers how to serialize the tokenizer, but we will not cover this in the recipe.</p></div></div>
<div class="section" title="Evaluating tokenizers with unit tests"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec27"/>Evaluating tokenizers with unit tests</h1></div></div></div><p>We will not evaluate Indo-European <a class="indexterm" id="id213"/>tokenizers like the other components of <a class="indexterm" id="id214"/>LingPipe with measures such as precision and recall. Instead, we will develop them with unit tests, because our tokenizers are heuristically constructed and expected to perform perfectly on example data—if a tokenizer fails to tokenize a known case, then it is a bug, not a reduction in performance. Why is this? There are a few reasons:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Many tokenizers are very "mechanistic" and are amenable to the rigidity of the unit test framework. For example, the <code class="literal">RegExTokenizerFactory</code> is obviously a candidate to unit test rather than an evaluation harness.</li><li class="listitem" style="list-style-type: disc">The heuristic rules that drive most tokenizers are very general, and there is no issue of over-fitting training data at the expense of a deployed system. If you have a known bad case, you can just go and fix the tokenizer and add a unit test.</li><li class="listitem" style="list-style-type: disc">Tokens and white spaces are assumed to be semantically neutral, which means that tokens don't change depending on context. This is not totally true with our Indo-European tokenizer, because it treats <code class="literal">.</code> differently if it is part of a decimal or at the end of a sentence, for example, <code class="literal">3.14 is pi.</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Token:'3.14'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'is'</strong></span>
<span class="strong"><strong>WhiteSpace:' '</strong></span>
<span class="strong"><strong>Token:'pi'</strong></span>
<span class="strong"><strong>WhiteSpace:''</strong></span>
<span class="strong"><strong>Token:'.'</strong></span>
<span class="strong"><strong>WhiteSpace:''.</strong></span>
</pre></div></li></ul></div><p>It might be appropriate to use an evaluation metric for statistics-based tokenizers; this is discussed in the <span class="emphasis"><em>Finding words for languages without white spaces</em></span> recipe in this chapter. See the <span class="emphasis"><em>Evaluation of sentence detection</em></span> recipe in <a class="link" href="ch05.html" title="Chapter 5. Finding Spans in Text – Chunking">Chapter 5</a>, <span class="emphasis"><em>Finding Spans in Text – Chunking</em></span>, for appropriate span-based evaluation techniques.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec62"/>How to do it...</h2></div></div></div><p>We will forgo <a class="indexterm" id="id215"/>running the code step and just get right into the <a class="indexterm" id="id216"/>source to put together a tokenizer evaluator. The source is in <code class="literal">src/com/lingpipe/chapter2/TestTokenizerFactory.java</code>. Perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The following code sets up a base tokenizer factory with a regular expression—look at the Javadoc for the class if you are not clear about what is being constructed:<div class="informalexample"><pre class="programlisting">public static void main(String[] args) {
  String pattern = "[a-zA-Z]+|[0-9]+|\\S";
  TokenizerFactory tokFactory = new RegExTokenizerFactory(pattern);
  String[] tokens = {"Tokenizers","need","unit","tests","."};
  String text = "Tokenizers need unit tests.";
  checkTokens(tokFactory,text,tokens);
  String[] whiteSpaces = {" "," "," ","",""};
  checkTokensAndWhiteSpaces(tokFactory,text,tokens,whiteSpaces);
  System.out.println("All tests passed!");
}</pre></div></li><li class="listitem">The <code class="literal">checkTokens</code> method takes <code class="literal">TokenizerFactory</code>, an array of <code class="literal">String</code> that is the desired tokenization, and <code class="literal">String</code> that is to be tokenized. It follows:<div class="informalexample"><pre class="programlisting">static void checkTokens(TokenizerFactory tokFactory, String string, String[] correctTokens) {
  Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(),0,input.length());
  String[] tokens = tokenizer.tokenize();
  if (tokens.length != correctTokens.length) {
    System.out.println("Token list lengths do not match");
    System.exit(-1);
  }
  for (int i = 0; i &lt; tokens.length; ++i) {
    if (!correctTokens[i].equals(tokens[i])) {
      System.out.println("Token mismatch: got |" + tokens[i] + "|");
      System.out.println(" expected |" + correctTokens[i] + "|" );
      System.exit(-1);
    }
  }</pre></div></li><li class="listitem">The method<a class="indexterm" id="id217"/> is quite intolerant of errors, because<a class="indexterm" id="id218"/> it exits the program if the token arrays are not of the same length or if any of the tokens are not equal. A proper unit test framework such as JUnit will be a better framework, but that is beyond the scope of the book. You can look at the LingPipe unit tests in <code class="literal">lingpipe.4.1.0</code>/<code class="literal">src/com/aliasi/test</code> for how JUnit is used.</li><li class="listitem">The <code class="literal">checkTokensAndWhiteSpaces()</code> method<a class="indexterm" id="id219"/> checks white spaces as well as tokens. It follows the same basic ideas<a class="indexterm" id="id220"/> of <code class="literal">checkTokens()</code>, so we leave it unexplained.</li></ol></div></div></div>
<div class="section" title="Modifying tokenizer factories"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec28"/>Modifying tokenizer factories</h1></div></div></div><p>In this recipe, we will describe a <a class="indexterm" id="id221"/>tokenizer that modifies the tokens in the token stream. We will extend the <code class="literal">ModifyTokenTokenizerFactory</code> class to return text that is rotated by 13 places in the English alphabet, also known as rot-13. Rot-13 is a very simple substitution cipher, which replaces a letter with the letter that follows after 13 places. For example, the letter <code class="literal">a</code> will be replaced by the letter <code class="literal">n</code>, and the letter <code class="literal">z</code> will be replaced by the letter <code class="literal">m</code>. This is a reciprocal cypher, which means that applying the same cypher twice recovers the original text.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec63"/>How to do it...</h2></div></div></div><p>We will invoke the <code class="literal">Rot13TokenizerFactory</code> class from the command line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp "lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar" com.lingpipe.cookbook.chapter2.Rot13TokenizerFactory</strong></span>

<span class="strong"><strong>type a sentence below to see the tokens and white spaces:</strong></span>
<span class="strong"><strong>Move along, nothing to see here.</strong></span>
<span class="strong"><strong>Token:'zbir'</strong></span>
<span class="strong"><strong>Token:'nybat'</strong></span>
<span class="strong"><strong>Token:','</strong></span>
<span class="strong"><strong>Token:'abguvat'</strong></span>
<span class="strong"><strong>Token:'gb'</strong></span>
<span class="strong"><strong>Token:'frr'</strong></span>
<span class="strong"><strong>Token:'urer'</strong></span>
<span class="strong"><strong>Token:'.'</strong></span>
<span class="strong"><strong>Modified Output: zbir nybat, abguvat gb frr urer.</strong></span>
<span class="strong"><strong>type a sentence below to see the tokens and white spaces:</strong></span>
<span class="strong"><strong>zbir nybat, abguvat gb frr urer.</strong></span>
<span class="strong"><strong>Token:'move'</strong></span>
<span class="strong"><strong>Token:'along'</strong></span>
<span class="strong"><strong>Token:','</strong></span>
<span class="strong"><strong>Token:'nothing'</strong></span>
<span class="strong"><strong>Token:'to'</strong></span>
<span class="strong"><strong>Token:'see'</strong></span>
<span class="strong"><strong>Token:'here'</strong></span>
<span class="strong"><strong>Token:'.'</strong></span>
<span class="strong"><strong>Modified Output: move along, nothing to see here.</strong></span>
</pre></div><p>You can see that the input text, which<a class="indexterm" id="id222"/> was mixed case and in normal English, has been transformed into its Rot-13 equivalent. You can see that the second time around, we passed the Rot-13 modified text as input and got the original text back, except that it was all lowercase.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec64"/>How it works...</h2></div></div></div><p>
<code class="literal">Rot13TokenizerFactory</code> extends the <code class="literal">ModifyTokenTokenizerFactory</code> class. We will override the <code class="literal">modifyToken()</code> method, which operates a token at a time and, in this case, converts the token to its Rot-13 equivalent. There is a similar <code class="literal">modifyWhiteSpace</code> (String) method, which modifies the white spaces if required:</p><div class="informalexample"><pre class="programlisting">public class Rot13TokenizerFactory extends ModifyTokenTokenizerFactory{

  public Rot13TokenizerFactory(TokenizerFactory f) {
    super(f);
  }
  
  @Override
  public String modifyToken(String tok) {
    return rot13(tok);
  }

  public static void main(String[] args) throws IOException {

  TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
  tokFactory = new LowerCaseTokenizerFactory(tokFactory);
  tokFactory = new Rot13TokenizerFactory(tokFactory);</pre></div><p>The start and end offsets<a class="indexterm" id="id223"/> of the tokens themselves remain the same as that of the underlying tokenizer. Here, we will use an Indo-European tokenizer as our base tokenizer. Filter it once through <code class="literal">LowerCaseTokenizer</code> and then through <code class="literal">Rot13Tokenizer</code>.</p><p>The <code class="literal">rot13</code> method is:</p><div class="informalexample"><pre class="programlisting">public static String rot13(String input) {
  StringBuilder sb = new StringBuilder();
  for (int i = 0; i &lt; input.length(); i++) {
    char c = input.charAt(i);
    if       (c &gt;= 'a' &amp;&amp; c &lt;= 'm') c += 13;
    else if  (c &gt;= 'A' &amp;&amp; c &lt;= 'M') c += 13;
    else if  (c &gt;= 'n' &amp;&amp; c &lt;= 'z') c -= 13;
    else if  (c &gt;= 'N' &amp;&amp; c &lt;= 'Z') c -= 13;
    sb.append(c);
  }
  return sb.toString();
}</pre></div></div></div>
<div class="section" title="Finding words for languages without white spaces"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec29"/>Finding words for languages without white spaces</h1></div></div></div><p>Languages such as Chinese do not have word boundaries. For example, 木卫三是围绕木星运转的一颗卫星，公转周期约为7天  from Wikipedia is a sentence in Chinese that translates roughly into "Ganymede is running around Jupiter's moons, orbital period of about seven days" as<a class="indexterm" id="id224"/> done by the machine translation service at <a class="ulink" href="https://translate.google.com">https://translate.google.com</a>. Notice the absence of white spaces.</p><p>Finding tokens in this sort of data requires a very different approach that is based on character-language models and our spell-checking class. This recipe encodes finding words by treating untokenized text as <span class="emphasis"><em>misspelled</em></span> text, where the <span class="emphasis"><em>correction</em></span> inserts a space to delimit tokens. Of course, there is nothing misspelled about Chinese, Japanese, Vietnamese, and other non-word delimiting orthographies, but we have encoded it in our spelling-correction class.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec65"/>Getting ready</h2></div></div></div><p>We will approximate non-word delimiting orthographies with de-white spaced English. This is sufficient to understand the recipe and can be easily modified to the actual language when needed. Get a 100,000 or so words of English and get them to the disk in UTF-8 encoding. The reason for fixing the encoding is that the input is assumed to be UTF-8—you can change it by changing the encoding and recompiling the recipe.</p><p>We used <span class="emphasis"><em>A Connecticut Yankee in King Arthur's Court</em></span> by Mark Twain, downloaded from Project Gutenberg (<a class="ulink" href="http://www.gutenberg.org/">http://www.gutenberg.org/</a>). Project Gutenberg<a class="indexterm" id="id225"/> is an excellent source of texts that are in the public domain, and Mark Twain is fine writer—we highly recommend the book. Place your selected text in the cookbook directory or work with our default.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec66"/>How to do it...</h2></div></div></div><p>We will run a <a class="indexterm" id="id226"/>program, play with it a bit, and explain what it does and how it does it, using the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Type the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter2.TokenizeWithoutWhiteSpaces</strong></span>
<span class="strong"><strong>Type an Englese sentence (English without spaces like Chinese):</strong></span>
<span class="strong"><strong>TheraininSpainfallsmainlyontheplain</strong></span>
</pre></div></li><li class="listitem">The following is the output:<div class="informalexample"><pre class="programlisting">The rain in Spain falls mainly on the plain</pre></div></li><li class="listitem">You might not get the perfect output. How good is Mark Twain at recovering proper white space from the Java program that generated it? Let's find out:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>type an Englese sentence (English without spaces like Chinese)</strong></span>
<span class="strong"><strong>NGramProcessLMlm=newNGramProcessLM(nGram);</strong></span>
<span class="strong"><strong>NGram Process L Mlm=new NGram Process L M(n Gram);</strong></span>
</pre></div></li><li class="listitem">The preceding way was not very good, but we are not being very fair; let's use the concatenated source of LingPipe as training data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter2.TokenizeWithoutWhiteSpaces data/cookbookSource.txt</strong></span>
<span class="strong"><strong>Compiling Spell Checker</strong></span>
<span class="strong"><strong>type an Englese sentence (English without spaces like Chinese)</strong></span>
<span class="strong"><strong>NGramProcessLMlm=newNGramProcessLM(nGram);</strong></span>
<span class="strong"><strong>NGramProcessLM lm = new NGramProcessLM(nGram);</strong></span>
</pre></div></li><li class="listitem">This is the perfect space insertion.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec67"/>How it works...</h2></div></div></div><p>For all the fun <a class="indexterm" id="id227"/>and games, there is very little code involved. The cool thing is that we are building on the character-language models from <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>. The source is in <code class="literal">src/com/lingpipe/chapter2/TokenizeWithoutWhiteSpaces.java</code>:</p><div class="informalexample"><pre class="programlisting">public static void main (String[] args) throws IOException, ClassNotFoundException {
  int nGram = 5;
  NGramProcessLM lm = new NGramProcessLM(nGram);
  WeightedEditDistance spaceInsertingEditDistance
    = CompiledSpellChecker.TOKENIZING;
  TrainSpellChecker trainer = new TrainSpellChecker(lm, spaceInsertingEditDistance);</pre></div><p>The <code class="literal">main()</code> method starts up with the creation of <code class="literal">NgramProcessLM</code>. Next up, we will access a class for edit distance that is designed to only add spaces to a character stream. That's it. <code class="literal">Editdistance</code> is typically a fairly crude measure of string similarity that scores how many edits need to happen to to <code class="literal">string1</code> to make it the same as <code class="literal">string2</code>. A lot of information on this is Javadoc <code class="literal">com.aliasi.spell</code>. For example, <code class="literal">com.aliasi.spell.EditDistance</code> has an excellent discussion of the basics.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>The <code class="literal">EditDistance</code> class<a class="indexterm" id="id228"/> implements the standard notion of edit distance, with or without transposition. The distance without transposition is known as the Levenshtein distance, and with transposition, it is known as the Damerau-Levenstein distance.</p></div></div><p>Read the Javadoc with LingPipe; it has a lot of useful of information that we don't have space for in this book.</p><p>So far we configured and constructed a <code class="literal">TrainSpellChecker</code> class. The next step is to naturally train it:</p><div class="informalexample"><pre class="programlisting">File trainingFile = new File(args[0]);
String training = Files.readFromFile(trainingFile, Strings.UTF8);
training = training.replaceAll("\\s+", " ");
trainer.handle(training);</pre></div><p>We slurped up a text file, assuming it is UTF-8; if not, correct the character encoding and recompile. Then, we replaced all the multiple white spaces with a single one. This might not be the best move if multiple white spaces have meaning. This is followed by training, just like we trained language models in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>.</p><p>Next up, we will compile and configure the spell checker:</p><div class="informalexample"><pre class="programlisting">System.out.println("Compiling Spell Checker");
CompiledSpellChecker spellChecker = (CompiledSpellChecker)AbstractExternalizable.compile(trainer);

spellChecker.setAllowInsert(true);
spellChecker.setAllowMatch(true);
spellChecker.setAllowDelete(false);
spellChecker.setAllowSubstitute(false);
spellChecker.setAllowTranspose(false);
spellChecker.setNumConsecutiveInsertionsAllowed(1);</pre></div><p>The next <a class="indexterm" id="id229"/>interesting line compiles <code class="literal">spellChecker</code>, which translates all the counts in the underlying language model to precomputed probabilities, which is much faster. The compilation step can write to a disk, so it can be used later without training; however, visit the Javadoc for <code class="literal">AbstractExternalizable</code> on how to do this. The next lines configure <code class="literal">CompiledSpellChecker</code> to only consider the edits that insert characters and to check for the exact string matches, but it forbids deletions, substitutions, and transpositions. Finally, only one insert is allowed. It should be clear that we are using a very limited portion of the capabilities of <code class="literal">CompiledSpellChecker</code>, but this is exactly what is called for—insert a space or don't.</p><p>Last up is our standard I/O routine:</p><div class="informalexample"><pre class="programlisting">BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
while (true) {
  System.out.println("type an Englese sentence (English " + "without spaces like Chinese)"));
  String input = reader.readLine();
  String result = spellChecker.didYouMean(input);
  System.out.println(result);
}</pre></div><p>The mechanics <a class="indexterm" id="id230"/>of the <code class="literal">CompiledSpellChecker</code> and <code class="literal">WeightedEditDistance</code> classes are better described in either the Javadoc or the <span class="emphasis"><em>Using edit distance and language models for spelling correction</em></span> recipe in <a class="link" href="ch06.html" title="Chapter 6. String Comparison and Clustering">Chapter 6</a>, <span class="emphasis"><em>String Comparison and Clustering</em></span>. However, the basic idea is that the string entered is <a class="indexterm" id="id231"/>compared to the language model just trained, resulting in a score that shows how good a fit this string is to the model. This string is going to be one huge word without any white spaces—but note that there is no tokenizer at work here, so the spell checker starts inserting spaces and reassessing the score of the resulting sequence. It keeps these sequences where insertion of spaces increases the score of the sequence.</p><p>Remember that the language model was trained on text with white spaces. The spell checker tries to insert a space everywhere it can and keeps a set of "best so far" insertions of white spaces. In the end, it returns the best scoring series of edits.</p><p>Note that to complete the<a class="indexterm" id="id232"/> tokenizer, the appropriate <code class="literal">TokenizerFactory</code> needs to be applied to the white space-modified text, but this is left as an exercise for the reader.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec68"/>There's more...</h2></div></div></div><p>
<code class="literal">CompiledSpellChecker</code> allows for <a class="indexterm" id="id233"/>an <span class="emphasis"><em>n</em></span>-best output as well; this allows for multiple possible analyses of the text. In a high-coverage/recall situation such as a research search engine, it might serve to allow the application of multiple tokenizations. Also, the edit costs can be manipulated by extending the <code class="literal">WeightedEditDistance</code> class directly to tune the system.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec69"/>See also</h2></div></div></div><p>It will be unhelpful to not actually provide non-English resources for this recipe. We built and evaluated a Chinese tokenizer using resources available on the web for research use. Our tutorial on Chinese word segmentation covers this in detail. You can find the Chinese word segmentation tutorial<a class="indexterm" id="id234"/> at <a class="ulink" href="http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html</a>.</p></div></div></body></html>