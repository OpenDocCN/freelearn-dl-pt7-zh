<html><head></head><body>
		<div id="_idContainer240">
			<h1 id="_idParaDest-212" class="chapter-nu ber"><a id="_idTextAnchor232"/>11</h1>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor233"/>The Ethics of Model Adaptability</h1>
			<p>This chapter gives a detailed overview of detecting different types of model drift for model governance purposes in organizations. The primary objective of this chapter is to demonstrate variations of ML models, with multiple examples to give you an awareness of the importance of the different statistical measures available for detecting data changes and model metric variations. This will help data scientists and MLOps professionals to choose the right drift detection mechanisms and stick to the correct model metric performance thresholds to control risks arising due to incorrect predictions. You’ll learn how to quantify and explain model drift and answer questions related to the need for model calibration. This will also allow you to understand the scope of designing fairly <span class="No-Break">calibrated models.</span></p>
			<p>In this chapter, these topics will be covered in the <span class="No-Break">following sections:</span></p>
			<ul>
				<li>Adaptability framework for data and <span class="No-Break">model drift</span></li>
				<li>How we can explain ML models when subjected to drift <span class="No-Break">or calibration</span></li>
				<li>Understanding the need for <span class="No-Break">model calibration</span></li>
			</ul>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor234"/>Technical requirements</h1>
			<p>This chapter requires you to have Python 3.8 and to run the following commands <span class="No-Break">before starting:</span></p>
			<ul>
				<li><strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install</strong></span><span class="No-Break"> </span><span class="No-Break"><strong class="source-inline">alibi-detect</strong></span></li>
				<li><strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install</strong></span><span class="No-Break"> </span><span class="No-Break"><strong class="source-inline">river</strong></span></li>
				<li><strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install</strong></span><span class="No-Break"> </span><span class="No-Break"><strong class="source-inline">detecta</strong></span></li>
				<li><strong class="source-inline">pip install nannyml (</strong><span class="No-Break"><strong class="source-inline">dependency numpy==1.21.0)</strong></span></li>
				<li><strong class="source-inline">git </strong><span class="No-Break"><strong class="source-inline">clone</strong></span><span class="No-Break"> </span><a href="https://github.com/zelros/cinnamon.git"><span class="No-Break">https://github.com/zelros/cinnamon.git</span></a></li>
				<li><strong class="source-inline">python3 </strong><span class="No-Break"><strong class="source-inline">setup.py install</strong></span></li>
				<li><strong class="source-inline">git </strong><span class="No-Break"><strong class="source-inline">clone</strong></span><span class="No-Break"> </span><a href="https://github.com/Western-OC2-Lab/PWPAE-Concept-Drift-Detection-and-Adaptation.git"><span class="No-Break">https://github.com/Western-OC2-Lab/PWPAE-Concept-Drift-Detection-and-Adaptation.git</span></a></li>
			</ul>
			<p>The <strong class="source-inline">alibi-detect</strong> package mentioned in the installation step can be found on GitHub. For reference, you can check out more details of the project <span class="No-Break">at </span><a href="https://github.com/SeldonIO/alibi-detect"><span class="No-Break">https://github.com/SeldonIO/alibi-detect</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor235"/>Adaptability framework for data and model drift</h1>
			<p>Data processing depends on the way data<a id="_idIndexMarker1316"/> is accessed based on its<a id="_idIndexMarker1317"/> availability, whether it’s sequential or continuous. Along<a id="_idIndexMarker1318"/> with the data processing<a id="_idIndexMarker1319"/> and modeling techniques built on different modes of incoming data, various factors (internal and external) cause the data<a id="_idIndexMarker1320"/> distribution to change dynamically. This change is called <strong class="bold">concept drift</strong>, and it creates several threats to ML models in production. In concept drift terminology, and as far as the shift in data distributions is concerned, the term <strong class="bold">window</strong> is used to refer to the most<a id="_idIndexMarker1321"/> recently known concept that was used to train the current or most <span class="No-Break">recent predictor.</span></p>
			<p>Examples of concept drift can be seen in e-commerce systems where ML algorithms profile the shopping patterns of the user and provide personalized recommendations of relevant products. Factors that result in concept drift include events such as marriage and relocating to a different geographical region. The COVID-19 pandemic caused a drastic change in consumers' buying behavior, because people were forced to turn to e-commerce platforms for online shopping. This resulted in higher demand for products than expected, causing a high rate of prediction errors in forecasting demand in supply chain networks. The e-commerce, supply chain, and banking industries have experienced changes in incoming data patterns, leading to <span class="No-Break">model drift.</span></p>
			<div>
				<div id="_idContainer217" class="IMG---Figure">
					<img src="image/Figure_11.01_B18681.jpg" alt="Figure 11.1 – Four types of concept drift"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Four types of concept drift</p>
			<p>As illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.1</em>, there are four kinds of concept drift, caused by various internal or external factors, or even <span class="No-Break">adversarial activities:</span></p>
			<ul>
				<li><strong class="bold">Abrupt</strong>: Caused by <span class="No-Break">behavior</span><span class="No-Break"><a id="_idIndexMarker1322"/></span><span class="No-Break"> shifts</span></li>
				<li><strong class="bold">Incremental</strong>: A sudden change with <span class="No-Break">slower</span><span class="No-Break"><a id="_idIndexMarker1323"/></span><span class="No-Break"> decay</span></li>
				<li><strong class="bold">Reoccurring</strong>: Similar to <span class="No-Break">seasonal</span><span class="No-Break"><a id="_idIndexMarker1324"/></span><span class="No-Break"> trends</span></li>
				<li><strong class="bold">Gradual</strong>: Slow, <span class="No-Break">long-lasting</span><span class="No-Break"><a id="_idIndexMarker1325"/></span><span class="No-Break"> changes</span></li>
			</ul>
			<p>Other indirect factors, such as the speed<a id="_idIndexMarker1326"/> of learning, errors in reporting<a id="_idIndexMarker1327"/> the right<a id="_idIndexMarker1328"/> features (or units of measurement), and <a id="_idIndexMarker1329"/>large changes in the classification or prediction accuracy, can also result in concept drift. This is illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.2</em>. To address concept drift, we need to update the models causing the drift. This could be a blind update or training with weighted data, model ensembling, an incremental model update, or applying modes of <span class="No-Break">online learning.</span></p>
			<div>
				<div id="_idContainer218" class="IMG---Figure">
					<img src="image/Figure_11.02_B18681.jpg" alt="Figure 11.2 – Different types of drift-causing factors﻿ and remedial actions"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Different types of drift-causing factors and remedial actions</p>
			<p>We categorize concept drift detectors (illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.3</em>) primarily based on the batch or online data arrival mode. Batch detection techniques can further be classified into whole-batch and partial-batch detection techniques. The size and sample of the batch are two of the factors used to classify them as whole-batch or partial-batch detection. Online detectors are further classified based on their ability to manipulate the reference window in order to detect drift. The detection window is often a sliding window that moves with an incoming instance, often referred to as the current concept. However, fixed reference windows are also used to detect <span class="No-Break">concept drift.</span></p>
			<p>Online detectors function by evaluating the test statistics computed for the first <em class="italic">W</em> data points and then updating<a id="_idIndexMarker1330"/> the test statistics. The updates<a id="_idIndexMarker1331"/> can be done sequentially<a id="_idIndexMarker1332"/> at a lower cost, thereby <a id="_idIndexMarker1333"/>helping us to detect fluctuations in the test statistics beyond a threshold value. A value exceeding the threshold indicates that drift has <span class="No-Break">taken place.</span></p>
			<div>
				<div id="_idContainer219" class="IMG---Figure">
					<img src="image/Figure_11.03_B18681.jpg" alt="Figure 11.3 – Commonly used drift detectors"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Commonly used drift detectors</p>
			<p><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.3</em> illustrates the idea<a id="_idIndexMarker1334"/> of unsupervised<a id="_idIndexMarker1335"/> batch-based (with a fixed window) and<a id="_idIndexMarker1336"/> online-based (fixed and sliding windows) drift detection<a id="_idIndexMarker1337"/> techniques that, after necessary data distribution comparison and a significance test, detect whether there has been drift or not. Batch-based methods may require instance selection and statistical computations on a batch to confirm the testing of drift conditions, enabling us to infer whether drift has occurred or not. The numbers here signify the sequence of steps required to detect both online and <span class="No-Break">offline drift.</span></p>
			<div>
				<div id="_idContainer220" class="IMG---Figure">
					<img src="image/Figure_11.04_B18681.jpg" alt="Figure 11.4 – Online and batch drift detection methods"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Online and batch drift detection methods</p>
			<p>An example of an online<a id="_idIndexMarker1338"/> drift adaptive framework is the <strong class="bold">Performance Weighted Probability Averaging Ensemble</strong> (<strong class="bold">PWPAE</strong>) framework, which can be used effectively in IoT<a id="_idIndexMarker1339"/> anomaly detection<a id="_idIndexMarker1340"/> use cases. </p>
			<p>This framework can be deployed on IoT cloud servers<a id="_idIndexMarker1341"/> to process<a id="_idIndexMarker1342"/> big data streams using a wireless medium from IoT devices. This kind of ensemble adaptive drift detector is composed of four base learners that help with real-time<a id="_idIndexMarker1343"/> <span class="No-Break">drift</span><span class="No-Break"><a id="_idIndexMarker1344"/></span><span class="No-Break"> detection:</span></p>
			<ul>
				<li>An <strong class="bold">Adaptive Random Forest</strong> (<strong class="bold">ARF</strong>) model with an <strong class="bold">ADWIN drift detector </strong>(referred to <span class="No-Break">as </span><span class="No-Break"><strong class="bold">ARF-ADWIN</strong></span><span class="No-Break">)</span></li>
				<li>An ARF<a id="_idIndexMarker1345"/> model with a <strong class="bold">DDM drift detector</strong> (referred to <span class="No-Break">as </span><span class="No-Break"><strong class="bold">ARF-DDM</strong></span><span class="No-Break">)</span></li>
				<li>A <strong class="bold">Streaming Random Patches</strong> (<strong class="bold">SRP</strong>) model<a id="_idIndexMarker1346"/> with an <strong class="bold">ADWIN drift detector</strong> (referred to <span class="No-Break">as </span><span class="No-Break"><strong class="bold">SRP-ADWIN</strong></span><span class="No-Break">)</span></li>
				<li>An<a id="_idIndexMarker1347"/> SRP model with a <strong class="bold">DDM drift detector</strong> (referred to <span class="No-Break">as </span><span class="No-Break"><strong class="bold">SRP-DDM</strong></span><span class="No-Break">)</span></li>
			</ul>
			<p>The four base online learners are combined by weighting them based on their accuracy and classification probabilities. Let’s try the PWPAE framework on the CICIDS2017 (<a href="https://www.unb.ca/cic/datasets/ids-2017.html">https://www.unb.ca/cic/datasets/ids-2017.html</a>) simulated intrusion detection dataset, which contains benign and recent common attacks, resembling true <span class="No-Break">real-world examples:</span></p>
			<ol>
				<li>To run PWPAE, let's first<a id="_idIndexMarker1348"/> make the necessary imports<a id="_idIndexMarker1349"/> from the <span class="No-Break"><strong class="source-inline">river</strong></span><span class="No-Break"> package:</span><pre class="console">
from river import metrics
from river import stream
from river import tree,neighbors,naive_bayes,ensemble,linear_model
from river.drift import DDM, ADWIN</pre></li>
				<li>Then we set up the PWPAE model with <strong class="source-inline">X_train</strong> and <strong class="source-inline">y_train</strong> to train the model, and we test it on <strong class="source-inline">X_test</strong> and <strong class="source-inline">y_test</strong>. The following code snippet uses the <span class="No-Break">PWPAE model:</span><pre class="console">
name = "Proposed PWPAE model"
t, m = PWPAE(X_train, y_train, X_test, y_test)
acc_fig(t, m, name)</pre></li>
			</ol>
			<p>The trained results and the comparative outputs are visualized in <em class="italic">Figure 11.5</em>.</p>
			<div>
				<div id="_idContainer221" class="IMG---Figure">
					<img src="image/Figure_11.05_B18681.jpg" alt="Figure 11.5 – The performance results of PWPAE with ﻿the ﻿Hoeffding Tree (HT) and Leveraging Bagging (LB) models"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – The performance results of PWPAE with the Hoeffding Tree (HT) and Leveraging Bagging (LB) models</p>
			<p>The accuracy of PWPAE is 99.06% and it exceeds the accuracy of other models.</p>
			<p>Let us investigate some supervised<a id="_idIndexMarker1350"/> drift detection strategies where<a id="_idIndexMarker1351"/> actual feedback for prediction is available<a id="_idIndexMarker1352"/> and is used with the predicted outcomes<a id="_idIndexMarker1353"/> to yield <span class="No-Break">error metrics.</span></p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor236"/>Statistical methods</h2>
			<p>Statistical methods help us to compare and assess<a id="_idIndexMarker1354"/> two different distributions. A divergence factor or a distance metric can be used to measure the difference between two distributions at different points in time to understand their behavior. This helps with the timely detection of the model’s performance metrics and finding the features that are causing <span class="No-Break">the change.</span></p>
			<h3>Kullback–Leibler divergence</h3>
			<p><strong class="bold">Kullback–Leibler</strong> (<strong class="bold">KL</strong>) divergence, also popularly known as <strong class="bold">relative entropy</strong>, quantifies<a id="_idIndexMarker1355"/> how much one probability<a id="_idIndexMarker1356"/> distribution differs from another. Mathematically, it can be stated <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer222" class="IMG---Figure">
					<img src="image/Formula_11_001.jpg" alt=""/>
				</div>
			</div>
			<p><em class="italic">Q</em> is the distribution of the old data and <em class="italic">P</em> is the distribution of the new data for which we compute the divergence, and || represents the divergence. When <em class="italic">P</em>(<em class="italic">x</em>) is high and <em class="italic">Q</em>(<em class="italic">x</em>) is low, the divergence will be high. On the other hand, if <em class="italic">P</em>(<em class="italic">x</em>) is low and <em class="italic">Q</em>(<em class="italic">x</em>) is high, the divergence will be high but not too high. When <em class="italic">P</em>(<em class="italic">x</em>) and <em class="italic">Q</em>(<em class="italic">x</em>) are similar, then the divergence will be low. The following code creates a KL divergence plot for a (<em class="italic">P</em>, <em class="italic">Q</em>, <em class="italic">M</em>) distribution with a mean of 5 and a standard deviation <span class="No-Break">of 4:</span></p>
			<pre class="source-code">
x = np.arange(-10, 10, 0.001)
q = norm.pdf(x, 5, 4)
plt.title('KL(P||Q) = %1.3f' % kl_divergence(p, q))
plt.plot(x, p)
plt.plot(x, q, c='red')</pre>
			<p>The change in distribution patterns due to KL divergence is illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer223" class="IMG---Figure">
					<img src="image/Figure_11.06_B18681.jpg" alt="Figure 11.6 – KL divergence"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – KL divergence</p>
			<p>There<a id="_idIndexMarker1357"/> are more forms<a id="_idIndexMarker1358"/> of divergence, which we will look <span class="No-Break">at next.</span></p>
			<h3>Jensen–Shannon divergence</h3>
			<p><strong class="bold">Jensen–Shannon</strong> (<strong class="bold">JS</strong>) divergence uses KL divergence<a id="_idIndexMarker1359"/> and can be formulated mathematically <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer224" class="IMG---Figure">
					<img src="image/Formula_11_002.jpg" alt=""/>
				</div>
			</div>
			<p>One difference between KL and JS divergence<a id="_idIndexMarker1360"/> is that JS divergence is symmetrical with a mandatory finite value. The change in distribution patterns due to JS divergence is illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer225" class="IMG---Figure">
					<img src="image/Figure_11.07_B18681.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Figure"> </p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – JS divergence</p>
			<p>To compute JS and KL divergence, we run the following <span class="No-Break">code snippet:</span></p>
			<ol>
				<li>First, we create a normal distribution for <span class="No-Break">distribution 1:</span><pre class="console">
data1 = scipy.stats.norm.rvs(size=100000, loc=0, scale=1.5, random_state=123)
hist1 = np.histogram(data1, bins=100)
hist1_dist = scipy.stats.rv_histogram(hist1)
import matplotlib.pyplot as plt
X1 = np.linspace(-8.0, -2.0, 1000)
plt.title("PDF")
plt.hist(data1, density=True, bins=100,  color ='blue')
plt.plot(X1, hist1_dist.pdf(X1), label='PDF', color = 'blue')</pre></li>
				<li> Next, we create another<a id="_idIndexMarker1361"/> normal distribution, which is our <span class="No-Break">second distribution:</span><pre class="console">
data2 = scipy.stats.norm.rvs(size=100000, loc=0, scale=5.5, random_state=123)
hist2 = np.histogram(data2, bins=100)
hist2_dist = scipy.stats.rv_histogram(hist2)
X2 = np.linspace(4.0, 8.0, 1000)
plt.title("Probability Density Function")
plt.hist(data2, density=True, bins=100, color ='green')
plt.plot(X2, hist2_dist.pdf(X2), label='PDF', color = 'green')
plt.legend(['X1', 'X2'])</pre></li>
				<li>In the next step, we first<a id="_idIndexMarker1362"/> evaluate the KL divergence between<a id="_idIndexMarker1363"/> probability distribution 1 and the mean of the two <strong class="bold">Probability Density Functions </strong>(<strong class="bold">PDFs</strong>) of <strong class="source-inline">Y1</strong> and <strong class="source-inline">Y2</strong> and the same for probability <span class="No-Break">distribution 2:</span><pre class="console">
Y1 = hist1_dist.pdf(X1)
Y2 = hist2_dist.pdf(X2)
M = (Y1 + Y2) / 2
d1 = scipy.stats.entropy(Y1, M, base=2)
print("KL div Y1 and M", d1)
d2 = scipy.stats.entropy(Y2, M, base=2)
print("KL div Y2 and M", d2)</pre></li>
				<li>The previous step yields the <span class="No-Break">following output:</span><pre class="console">
KL div X1 and X2 0.21658815880427068
<strong class="bold">KL div Y1 and M 1.0684247605300703</strong>
<strong class="bold">KL div Y2 and M 0.1571132219534354</strong></pre></li>
				<li>In the next step, we first evaluate the JS divergence between the distributions and also within each <span class="No-Break">distribution individually:</span><pre class="console">
js_dv = (d1 + d2) / 2
js_distance = np.sqrt(js_dv)
print("JS Dist d1 and d2", js_distance)</pre></li>
			</ol>
			<p>We evaluate this against the SciPy calculation <a id="_idIndexMarker1364"/>of JS divergence between the distributions:</p>
			<pre class="console">
js_distance_scipy = scipy.spatial.distance.jensenshannon(Y1, Y2)
print("JS Dist d1 and d2 of Scipy", js_distance_scipy)
js_distance_scipy = scipy.spatial.distance.jensenshannon(X1, X2)
print("JS Dist X1 and X2 of Scipy", js_distance_scipy)
dx1 = scipy.stats.entropy(Y1, X1, base=2)
dx2 = scipy.stats.entropy(Y2, X2, base=2)
js_dv = (dx1 + dx2) / 2
print("JS Div X1 and X2", js_dv)</pre>
			<p>The preceding step yields the following output:</p>
			<pre class="console">
<strong class="bold">JS Dist d1 and d2 0.7827956254615587</strong>
<strong class="bold">JS Dist d1 and d2 of Scipy 0.6037262820103958</strong>
<strong class="bold">JS Dist X1 and X2 of Scipy 0.1941318696014193</strong>
<strong class="bold">JS Div X1 and X2 1.3749093686870903</strong></pre>
			<p>The first distance gives the symmetric JS divergence, while the second evaluated metric gives the JS distance, which is the square <a id="_idIndexMarker1365"/>root of the JS divergence. The third and fourth distance metrics evaluated give us the JS distance between <strong class="source-inline">X1</strong>, <strong class="source-inline">X2</strong> and <strong class="source-inline">dx1</strong>, <strong class="source-inline">dx2</strong>, respectively. <strong class="source-inline">dx1</strong> and <strong class="source-inline">dx2</strong> here signify the entropy of distributions created from <strong class="source-inline">Y1</strong>, <strong class="source-inline">X1</strong> and <strong class="source-inline">Y2</strong>, <span class="No-Break"><strong class="source-inline">X2</strong></span><span class="No-Break">, respectively.</span></p>
			<h3>The Kolmogorov-Smirnov test</h3>
			<p>The two-sample <strong class="bold">Kolmogorov-Smirnov</strong> (<strong class="bold">KS</strong>) test is a general nonparametric<a id="_idIndexMarker1366"/> method used to differentiate two samples. The data change pattern, best identified by the KS test, is illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.8</em>. The <strong class="bold">Cumulative Distribution Function</strong> (<strong class="bold">CDF</strong>) of (<em class="italic">sample, x</em>) quantifies the percentage of observations<a id="_idIndexMarker1367"/> below <em class="italic">x</em> on the sample. This can be obtained by doing <span class="No-Break">the following:</span></p>
			<ol>
				<li>Sorting <span class="No-Break">the sample</span></li>
				<li>Counting whether the number of observations within the sample is less than or equal <span class="No-Break">to </span><span class="No-Break"><em class="italic">x</em></span></li>
				<li>Dividing the numerator computed in step 2 by the total number of observations on <span class="No-Break">the sample</span></li>
			</ol>
			<p>The purpose of the drift detector is to detect drift patterns when two distribution functions observe a change, causing a shape change in <span class="No-Break">two samples:</span></p>
			<div>
				<div id="_idContainer226" class="IMG---Figure">
					<img src="image/Figure_11.08_B18681.jpg" alt="Figure 11.8 – KS test"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – KS test</p>
			<h3>Population stability index</h3>
			<p><strong class="bold">Population stability index</strong> (<strong class="bold">PSI</strong>) is a metric that monitors and measures shifts in population<a id="_idIndexMarker1368"/> behavior between two samples or over two periods of time. It serves as a risk-scorecard metric to give a probable risk estimation between an out-of-time validation sample and a modeling sample including both dependent and independent variables. The application of PSI can also be extended to compare the education, income, and health status of two or more populations in <span class="No-Break">social-demographic studies.</span></p>
			<p><strong class="bold">Model distillation</strong> is a technique that allows the transfer<a id="_idIndexMarker1369"/> of knowledge from a large network to a small network, which trains a second model with a simplified architecture on soft targets (the output distributions or the logits) retrieved from the original model. It paves the way to detect adversarial and malicious data and data drift by comparing the output distributions of both the original model and the <span class="No-Break">distilled model.</span></p>
			<p>Let us now see, with an example, how adversarial scores are detected by the model distillation detector in the context of drift detection. The KS test has been used as the scoring function to run a simple univariate test between the adversarial scores of the reference batch and the test data. A high adversarial score indicates a harmful drift, and a flag is raised for malicious data drift. Here, we can fetch the pretrained model distillation detector from a Google Cloud bucket or train one <span class="No-Break">from scratch:</span></p>
			<ol>
				<li>First, we import the necessary packages <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">alibi_detect</strong></span><span class="No-Break">:</span><pre class="console">
from alibi_detect.cd import KSDrift
from alibi_detect.ad import ModelDistillation
from alibi_detect.models.tensorflow.resnet import scale_by_instance
from alibi_detect.utils.fetching import fetch_tf_model, fetch_detector
from alibi_detect.utils.tensorflow.prediction import predict_batch
from alibi_detect.utils.saving import save_detector
from alibi_detect.datasets import fetch_cifar10c, corruption_types_cifar10c</pre></li>
				<li>Then, we define<a id="_idIndexMarker1370"/> and train the <span class="No-Break">distilled model:</span><pre class="console">
from tensorflow.keras.layers import Conv2D, Dense, Flatten, InputLayer
from tensorflow.keras.regularizers import l1
def distilled_model_cifar10(clf, nb_conv_layers=8, distl_model_filters1=256, nb_dense=40, kernel1=8, kernel2=8, kernel3=8, ae_arch=False):
    distl_model_filters1 = int(distl_model_filters1)
    distl_model_filters2 = int(distl_model_filters1 / 2)
    distl_model_filters3 = int(distl_model_filters1 / 4)
    layers = [InputLayer(input_shape=(64, 64, 3)),
              Conv2D(distl_model_filters1, kernel1, strides=2, padding='same')]
    if nb_conv_layers &gt; 2:
        layers.append(Conv2D(distl_model_filters2,
kernel2, strides=2, padding='same', activation=tf.nn.relu, kernel_regularizer=l1(1e-5)))
    if nb_conv_layers &gt; 2:
        layers.append(Conv2D(distl_model_filters3, kernel3, strides=2, padding='same',
                             activation=tf.nn.relu, kernel_regularizer=l1(1e-5)))
    layers.append(Flatten())
    layers.append(Dense(nb_dense))
    layers.append(Dense(clf.output_shape[1], activation='softmax'))
    distilled_model = tf.keras.Sequential(layers)
    return distilled_model</pre></li>
				<li>Next, based on our configuration, we can either<a id="_idIndexMarker1371"/> load a pretrained model or train a <span class="No-Break">new model:</span><pre class="console">
load_pretrained = True
detector_type = 'adversarial'
detector_name = 'model_distillation'
filepath = os.path.join(filepath, detector_name)
if load_pretrained:
    ad = fetch_detector(filepath, detector_type, dataset, detector_name, model=model)
else:
    distilled_model = distilled_model_cifar10(clf)
    print(distilled_model.summary())
    ad = ModelDistillation(distilled_model=distilled_model, model=clf)
    ad.fit(X_train, epochs=50, batch_size=128, verbose=True)
    save_detector(ad, filepath)</pre></li>
				<li>We now plot the mean scores and standard deviations per severity level. We define the model accuracy plot as the mean and standard deviation of the harmfulness and <span class="No-Break">no-harmfulness scores:</span><pre class="console">
def evaluate_plot_model_accuracy():
mu_noharm, std_noharm = [], []
mu_harm, std_harm = [], []
acc = [clf_accuracy['original']]
for k, v in score_drift.items():
   mu_noharm.append(v['noharm'].mean())
   std_noharm.append(v['noharm'].std())
   mu_harm.append(v['harm'].mean())
   std_harm.append(v['harm'].std())
   acc.append(v['acc'])</pre></li>
			</ol>
			<p>The plot (illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.9</em>) shows the mean harmfulness scores (the line plot starting from the left-hand side) and ResNet-32 accuracies (the bars displayed on the right-hand side) for increasing data corruption severity levels. Level 0 corresponds to the original test set. We have demonstrated the impact of varying levels of malicious (corrupted) data along with its <span class="No-Break">severity levels.</span></p>
			<p>Harmful scores signify instances<a id="_idIndexMarker1372"/> that gave an incorrect prediction because of corrupted data. Even not-harmful predictions are known to exist, which remains unchanged (as shown by the harmful index, marked in yellow along the <em class="italic">Y</em> axis) after the data corruption due to the injection of malicious adversarial samples. To summarize further, we see in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.9</em> that the corruption severity increases as the harmfulness score increases (shown by the cyan bars) and the accuracy decreases (shown by the <span class="No-Break">blue line).</span></p>
			<div>
				<div id="_idContainer227" class="IMG---Figure">
					<img src="image/Figure_11.09_B18681.jpg" alt="Figure 11.9 – Distilled drift detector detecting the corruption severity"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – Distilled drift detector detecting the corruption severity</p>
			<p>There are some other methods<a id="_idIndexMarker1373"/> that we can categorize as <span class="No-Break">contextual methods.</span></p>
			<h3>Contextual methods</h3>
			<p>The purpose of these methods is to compare and assess the difference between the train and test datasets and evaluate the drift when there’s a significant difference in the <span class="No-Break">predicted outcomes.</span></p>
			<h4>Tree features</h4>
			<p>This method enables<a id="_idIndexMarker1374"/> you to train a simple tree based on data<a id="_idIndexMarker1375"/> and prediction timestamps that are fed as independent input features, along with other features. Once the tree model is analyzed for feature importance, it is evident that the effect on data at different points in time helps to substantiate the differences arising due to concept drift. The tree splits, and the feature splits done on the timestamp, help to explain the changes due <span class="No-Break">to drift.</span></p>
			<h4>Shuffling and resampling (SR)</h4>
			<p>The data is split into train and test sets<a id="_idIndexMarker1376"/> at an assumed<a id="_idIndexMarker1377"/> drift point, and then the model is trained using the train dataset and evaluated against the test dataset to compute the error rates. The same mechanism of training and testing is repeated by shuffling the same dataset and recomputing the error metrics. Drift is said to be detected when the difference between the ordered data error rate and the average shuffled data error rate is above a specified threshold. This is also a computationally intensive mechanism as it involves training multiple<a id="_idIndexMarker1378"/> models during<a id="_idIndexMarker1379"/> occurrences <span class="No-Break">of drift.</span></p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor237"/>Statistical process control</h2>
			<p>This kind of drift detector control mechanism<a id="_idIndexMarker1380"/> ensures that when the model in production generates varying accuracy metrics over time, the errors in the model can be managed. Though this method is effective in detecting sudden, gradual, and incremental drift in a short span of time, the latency could be high when extracting labels from the samples. The requirement of having labeled data makes it more difficult to be <span class="No-Break">applied widely.</span></p>
			<h3>Drift detection method (DDM)</h3>
			<p>This method of drift<a id="_idIndexMarker1381"/> detection is one of the earliest devised. Incoming data is assumed to be in a sequence following a binomial distribution and a Bernoulli trial variable (or single data point) inferring the occurrence of drift based on the prediction <span class="No-Break">error rate.</span></p>
			<p>The algorithm<a id="_idIndexMarker1382"/> records the <strong class="bold">minimum probability of error</strong> (<em class="italic">p</em>) rate and the <strong class="bold">minimum standard deviation</strong> (<em class="italic">s</em>) of the binomial distribution when <em class="italic">p + s</em> reaches<a id="_idIndexMarker1383"/> its own minimum. Drift<a id="_idIndexMarker1384"/> is said to be present<a id="_idIndexMarker1385"/> when the <em class="italic">p + s</em> value exceeds the sum of the <strong class="bold">minimum probability of error</strong> (p<span class="subscript">min</span>) and a <strong class="bold">multiple of the minimum standard deviation</strong> (s<span class="subscript">min</span>). We can state that as (p + s) &gt; (p<span class="subscript">min</span> + 3 ✶ <span class="No-Break"><em class="italic">s</em></span><span class="No-Break"><span class="subscript">min</span></span><span class="No-Break">)</span><span class="No-Break">.</span></p>
			<p>The recommended multiplying factor is 3. This method has limitations when the change occurs slowly, where<a id="_idIndexMarker1386"/> the cache/memory <span class="No-Break">may overflow.</span></p>
			<h3>Early Drift Detection Method (EDDM)</h3>
			<p>This method, although<a id="_idIndexMarker1387"/> similar to DDM, focuses<a id="_idIndexMarker1388"/> on gradua<a id="_idIndexMarker1389"/>l drift by computing the <strong class="bold">mean </strong>(<em class="italic">m</em>) and <strong class="bold">standard deviation</strong> (<em class="italic">s</em>) of the distance between two errors. It records (<em class="italic">m</em> + 2 ✶ <em class="italic">s</em>) and when it reaches its maximum value, it saves both values as m<span class="subscript">max</span> and s<span class="subscript">max</span> , respectively. When the ratio, (<em class="italic">m</em> + 2 ✶ <em class="italic">s</em>)/(<em class="italic">m</em> + 2 ✶ <em class="italic">s</em><span class="subscript">max</span>), drops below a threshold (<em class="italic">β</em>; the recommended value is 0.9), drift is detected, and an alarm should <span class="No-Break">be raised.</span></p>
			<h3>CUSUM and Page-Hinkley</h3>
			<p><strong class="bold">Cumulative Sum</strong> (<strong class="bold">CUSUM</strong>) and its variant, <strong class="bold">Page-Hinkley</strong> (<strong class="bold">PH</strong>), both rely on a sequential analysis<a id="_idIndexMarker1390"/> technique, typically<a id="_idIndexMarker1391"/> from an average Gaussian signal. These methods detect the change and raise an alarm when they observe that the difference between the observed values and the mean is higher than a user-defined threshold. As the changes are sensitive to the parameter values, one disadvantage of this is the triggering of false alarms. These methods can be widely applied to <span class="No-Break">data streams.</span></p>
			<h4>CUSUM</h4>
			<p>This drift detection algorithm<a id="_idIndexMarker1392"/> detects small changes in the mean using CUSUM. When the probability distributions before and after the change are known, then the CUSUM procedure optimizes an objective function by considering the delays and frequency of false alarms. It has the additional advantage of being simple and intuitive to interpret in terms of maximum likelihood. It is memoryless, one-sided, and asymmetrical, with the ability to detect only an increase in the difference between the observed value and <span class="No-Break">the mean.</span></p>
			<p>The CUSUM detector<a id="_idIndexMarker1393"/> is a kernel-based technique that continuously compares samples from the database. The metric for drift determination is called the <strong class="bold">Maximum Mean Discrepancy</strong> (<strong class="bold">MMD</strong>). This procedure is well suited for large data volumes because it does not need to compare pre- and post-distributions, instead concentrating on the current data to identify drift. CUSUM has been enhanced to use a dual mean value on nested sliding windows, which is called the <strong class="bold">Double CUSUM Based on Data Stream</strong> (<strong class="bold">DCUSUM-DS</strong>). Another variant of CUSUM is DCUSUM-DS, which uses a dual mean value CUSUM. The DCUSUM-DS algorithm works on nested sliding windows and detects drift by calculating the average value of the data within the window twice. After detecting the average, it extracts new features and then generates accumulated and controlled graphs to avoid false inference. One major benefit of this method is that it can detect new features and rerun its analysis to ensure it detects the correct drift and does not rely only on the average <span class="No-Break">values detected.</span></p>
			<p>The kernel-based variant of CUSUM does not require the pre- and post-change distributions and instead depends on a database of samples from the pre-change distribution, with which it can continuously compare incoming observations with samples from the database. The kernel function chosen<a id="_idIndexMarker1394"/> by the user and the statistical metric for comparison is MMD. The <strong class="bold">Kernel Cumulative Sum</strong> (<strong class="bold">KCUSUM</strong>) algorithm works well in settings where there is a huge amount of background data available, and when it is necessary to detect deviations from the <span class="No-Break">background data.</span></p>
			<p>The algorithm can be configured with a threshold that sets the limit beyond which an alarm is triggered. The magnitude of the drift threshold (say, 80%, 50%, or 30%) helps us to obtain the right metric for identifying a drift. Accordingly, an alarm needs to be raised when any deviations in the data or model pattern are observed. For example, an algorithm can be set to a very large amplitude with an 80% threshold boundary, which will enable it to detect drift more frequently than when it is set to 30%. The detector returns the <span class="No-Break">following values:</span></p>
			<ul>
				<li><strong class="source-inline">ta</strong>: Change detection index – a return value that represents the alarm time (the index when the change <span class="No-Break">was detected)</span></li>
				<li><strong class="source-inline">tai</strong>: Starting index of change – shows the index when the <span class="No-Break">change started</span></li>
				<li><strong class="source-inline">taf</strong>: Ending index of change – denotes the index when the change ended (if <strong class="source-inline">ending</strong> <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">True</strong></span><span class="No-Break">)</span></li>
				<li><strong class="source-inline">amp</strong>: Denotes the amplitude of the changes (if <strong class="source-inline">ending</strong> <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">True</strong></span><span class="No-Break">)</span></li>
			</ul>
			<p>One way to configure the parameters<a id="_idIndexMarker1395"/> is to start with a very large <strong class="source-inline">threshold</strong> value and set <strong class="source-inline">drift</strong> to half of the expected change. We can also adjust <strong class="source-inline">drift</strong> so that <strong class="source-inline">g</strong> is <strong class="source-inline">0</strong> more than 50% of the time. We can then fine-tune <strong class="source-inline">threshold</strong> so the required number of false alarms or delays in detecting drift is obtained. For faster drift detection, we need to decrease <strong class="source-inline">drift</strong>, whereas to reduce false alarms and minimize the effect of small changes, we need to <span class="No-Break">increase </span><span class="No-Break"><strong class="source-inline">drift</strong></span><span class="No-Break">.</span></p>
			<p>The following code snippet demonstrates how to use the CUSUM <span class="No-Break">drift detector:</span></p>
			<pre class="source-code">
from detecta import detect_cusum
x = np.random.randn(500)/5
x[200:300] += np.arange(0, 4, 4/100)
ta, tai, taf, amp = detect_cusum(x, 4, .025, True, True)
x = np.random.randn(500)
x[200:300] += 6
detect_cusum(x, 3, 1.5, True, True)
x = 2*np.sin(2*np.pi*np.arange(0, 2.8, .01))
ta, tai, taf, amp = detect_cusum(x, 1.8, .05, True, True)</pre>
			<p>The preceding code is able to detect drift between a range of data, illustrated in the following figure, by drift percentage, threshold, and the number of instances <span class="No-Break">of change.</span></p>
			<div>
				<div id="_idContainer228" class="IMG---Figure">
					<img src="image/Figure_11.10_B18681.jpg" alt="Figure 11.10 – CUSUM drift detector change detections"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – CUSUM drift detector change detections</p>
			<p>The threshold-based drift detection<a id="_idIndexMarker1396"/> technique used here demonstrates (<span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.10</em>) the role of the CUSUM of positive and negative changes in <span class="No-Break">detecting drift.</span></p>
			<h4>Covariate and prior probability data drift</h4>
			<p>Covariate drift<a id="_idIndexMarker1397"/> occurs due to changes<a id="_idIndexMarker1398"/> in the distributions of one or more of the independent features due to internal or external factors, but the relationship between the input <em class="italic">X</em> and target <em class="italic">Y</em> remains the same. While the distribution of input feature <em class="italic">X</em> changes with covariate data drift, with prior probability shift, the distribution of the input variables remains the same but the distribution of the target variable changes. Changes in the target distribution result in prior probability data drift. To implement covariate drift, we apply a shift to the mean of one of the <span class="No-Break">normal distributions.</span></p>
			<p>The model is now being tested on a new region of the feature space, causing the model to misclassify new test observations. In the absence of true test labels, it is impossible to measure the model’s accuracy. Here, the drift detector helps by detecting whether covariate or prior probability drift is occurring. If it’s the latter, a proxy for prior drift can be monitored by initializing the detector on labels from the reference set, which is then fed into a model’s predicted labels to identify drift. </p>
			<p>The following steps illustrate how to detect data drift by comparing it with the original model trained on the <span class="No-Break">initial dataset:</span></p>
			<ol>
				<li>First, we take a multivariate<a id="_idIndexMarker1399"/> normal distribution and then specify the reference<a id="_idIndexMarker1400"/> data to initialize <span class="No-Break">the detectors:</span><pre class="console">
shift_norm_0 = multivariate_normal([2, -4], np.eye(2)*sigma**2)
X_0 = shift_norm_0.rvs(size=int(N_test*phi1),random_state=2)
X_1 = ref_norm_1.rvs(size=int(N_test*phi2),random_state=2)</pre></li>
				<li>We stack the reference distributions and try to estimate the drift by comparing it with <strong class="source-inline">true_slope</strong>, which has been set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">-1</strong></span><span class="No-Break">:</span><pre class="console">
phi1 = phi1*1.5
phi2 = phi2*2.5
true_slope = -1
shift_norm_0 = multivariate_normal([3, -8], np.eye(2)*sigma**2)
X_0 = shift_norm_0.rvs(size=int(N_test*phi1),random_state=2)
X_1 = ref_norm_1.rvs(size=int(N_test*phi2),random_state=2)
X_test = np.vstack([X_0, X_1])
y_test = true_model(X_test,true_slope)
plot(X_test,y_test,true_slope,clf=clf)
print('Mean test accuracy %.2f%%' %(100*clf.score(X_test,y_test)))
pred = detector.predict(X_test)
print('Is drift? %s!' %labels[pred['data']['is_drift']])</pre></li>
			</ol>
			<p>The code snippet generates<a id="_idIndexMarker1401"/> the following plots to demonstrate a use case<a id="_idIndexMarker1402"/> of no drift versus covariate drift, exhibiting lower mean accuracy where there is drift (right side) than where there is none (left side).</p>
			<div>
				<div id="_idContainer229" class="IMG---Figure">
					<img src="image/Figure_11.11_B18681.jpg" alt="Figure 11.11 – Covariate data drift"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Covariate data drift</p>
			<ol>
				<li value="3">While <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.11</em> shows covariate data drift, the following code demonstrates the use of the MMD method, in which an estimate of the expected squared difference between the kernel <strong class="bold">conditional</strong> mean embeddings of <em class="italic">X</em><span class="subscript">ref</span> | <em class="italic">C </em>and<em class="italic"> X</em><span class="subscript">test</span> | <em class="italic">C</em> are computed to evaluate the <span class="No-Break">test statistic:</span><pre class="console">
def detect_prior_drift():
    label_detector = MMDDrift(y_ref.reshape(-1,1), backend='tensorflow', p_val=.05)
    y_pred = clf.predict(X_test)
    label_detector.predict(y_pred.reshape(-1,1))
detect_prior_drift()</pre></li>
				<li>We get the <span class="No-Break">following output:</span><pre class="console">
<strong class="bold">{'data': {'is_drift': 1,</strong>
<strong class="bold">  'distance': 0.107620716,</strong>
<strong class="bold">  'p_val': 0.0,</strong>
<strong class="bold">  'threshold': 0.05,</strong>
<strong class="bold">  'distance_threshold': 0.013342142},</strong>
<strong class="bold"> 'meta': {'name': 'MMDDriftTF',</strong>
<strong class="bold">  'detector_type': 'offline',</strong>
<strong class="bold">  'data_type': None,</strong>
<strong class="bold">  'version': '0.9.0',</strong>
<strong class="bold">  'backend': 'tensorflow'}}</strong></pre></li>
			</ol>
			<p>The MMD detector<a id="_idIndexMarker1403"/> detects drift with a distance of <strong class="source-inline">0.1076</strong>, and the threshold<a id="_idIndexMarker1404"/> of drift detection is <strong class="source-inline">0.013342</strong>.</p>
			<h4>Least-squared density difference</h4>
			<p>The <strong class="bold">Least-squared density difference</strong> (<strong class="bold">LSDD</strong>) drift detection technique directly estimates the density<a id="_idIndexMarker1405"/> difference without separately estimating the densities of the prior and the current distribution. In the following sample code, the dataset is shuffled and normalized so that each feature takes a value in the range of <strong class="source-inline">[0,1]</strong> and predicts the same <span class="No-Break">binary outcomes.</span></p>
			<p>The LSDD online<a id="_idIndexMarker1406"/> drift detector from <strong class="source-inline">alibi_detect</strong> necessitates an <strong class="bold">Expected Runtime</strong> (<strong class="bold">ERT</strong>) (an inverted <strong class="bold">False Positive Rate</strong> (<strong class="bold">FPR</strong>)), allowing the detector to run<a id="_idIndexMarker1407"/> an average number of steps in the absence of drift before making a false detection. With a high ERT, detectors lose their sensitivity and become slow to respond, so the configuration adjusts the trade-off between the ERT and the expected detection delay to target desirable ERTs. The best way to simulate the desired configuration is to select training data that is an order of magnitude larger than the <span class="No-Break">desired ERT:</span></p>
			<ol>
				<li>In the following code snippet, the model is trained on white wine samples, which form the reference<a id="_idIndexMarker1408"/> distribution, and red wine samples are drawn from a drifted distribution. The steps following the upcoming code block illustrate how to run LSDD drift detection, and how it helps to compare no drift <span class="No-Break">versus drift:</span><pre class="console">
white, red = np.asarray(white, np.float32), np.asarray(red, np.float32)
n_white, n_red = white.shape[0], red.shape[0]
col_maxes = white.max(axis=0)
white, red = white / col_maxes, red / col_maxes
white, red = white[np.random.permutation(n_white)], red[np.random.permutation(n_red)]
X = white[:, :-1]
X_corr = red[:, :-1]
X_train = X[:(n_white//2)]
X_ref = X[(n_white//2) :(3*n_white//4)]
X_h0 = X[(3*n_white//4):]
X_ref = np.concatenate([X_train, X_ref], axis=0)</pre></li>
				<li>In the first run, without the detector set, we do not detect <span class="No-Break">any drift:</span><pre class="console">
n_runs = 550
times_h0 = [time_run(cd, X_h0, window_size) for _ in range(n_runs)]
print (f"Average run-time under no-drift: {np.mean(times_h0)}")
_ = scipy.stats.probplot(np.array(times_h0), dist=scipy.stats.geom, sparams=1/ert, plot=plt)</pre></li>
			</ol>
			<p>The preceding code yields the following output:</p>
			<pre class="console">
<strong class="bold">Average run-time under no-drift: 47.72</strong></pre>
			<ol>
				<li value="3">The following code<a id="_idIndexMarker1409"/> snippet imports <strong class="source-inline">LSDDDriftOnline</strong> from <strong class="source-inline">alibi_detect</strong> and sets it up with reference data, <strong class="source-inline">ert</strong>, <strong class="source-inline">window_size</strong>, the number of runs, and a TensorFlow backend both for the original and current distributions to detect drift. Then, the online drift detector is run with an <strong class="source-inline">ert</strong> value of <strong class="source-inline">50</strong> and <strong class="source-inline">window_size</strong> <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">10</strong></span><span class="No-Break">:</span><pre class="console">
X_new_ref = np.concatenate([X_train, X_ref], axis=0)
from alibi_detect.cd import LSDDDriftOnline
def lsdd_detector():
    cd = LSDDDriftOnline(
        X_new_ref, ert, window_size, backend='tensorflow', n_bootstraps=5500,
    )
 times_h0 = [time_run(cd, X_h0, window_size) for _ in range(n_runs)]
    print(f"Average run-time under no-drift: {np.mean(times_h0)}")
    _ = scipy.stats.probplot(np.array(times_h0), dist=scipy.stats.geom, sparams=1/ert, plot=plt)</pre></li>
			</ol>
			<p>This produces the following output:</p>
			<div>
				<div id="_idContainer230" class="IMG---Figure">
					<img src="image/Figure_11.12_B18681.jpg" alt="Figure 11.12 – LSDD drift detector"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – LSDD drift detector</p>
			<p><em class="italic">Figure 11.12</em> demonstrates online drift detection with LSDD. Here, we let the detector run a configurable number of times (50) or iterations and configure 5,500 bootstraps to detect the drift. The bootstraps are used to run the simulations and configure the thresholds. A greater magnitude helps to achieve better accuracy in terms of obtaining the ERT, and it is typically configured to be an order of magnitude larger than the ERT.</p>
			<p>We get the following output:</p>
			<pre class="console">
<strong class="bold">Average run-time under drift: 54.39</strong></pre>
			<p>Furthermore, we observe<a id="_idIndexMarker1410"/> that the detector on the held-out reference data in the first run follows a geometric distribution with mean ERT, without having any drift. However, as soon as drift is detected, the detector is very fast to respond, as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">.</span></p>
			<h4>Page-Hinkley</h4>
			<p>This method of drift detection functions<a id="_idIndexMarker1411"/> by detecting changes by computing the observed values and their mean up to the current moment. Without issuing any warning signals, it runs the PH test to detect concept drift if the observed mean is found to exceed a threshold lambda value. Mathematically, it can be formulated <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer231" class="IMG---Figure">
					<img src="image/Formula_11_003.jpg" alt=""/>
				</div>
			</div>
			<p>When g<span class="subscript">t</span>– G<span class="subscript">t</span> &gt; h, an alarm <span class="No-Break">is raised.</span></p>
			<p>Now, let us walk through the step-by-step<a id="_idIndexMarker1412"/> process of detecting drift using the <span class="No-Break">PH method:</span></p>
			<ol>
				<li>The following code sample demonstrates how we simulate <span class="No-Break">two distributions:</span><pre class="console">
import random
from river import drift
import matplotlib.pyplot as plt
import numpy as np
rng = random.Random(123456)
ph = drift.PageHinkley()</pre></li>
				<li>Now, we compose and plot a data stream composed of three <span class="No-Break">data distributions:</span><pre class="console">
data_stream = rng.choices([50, 100], k=1200) + rng.choices(range(600, 900), k=5000) + rng.choices(range(200, 300), k=5000)
plt.plot(data_stream)
plt.show()</pre></li>
				<li>Now, we update the drift detector and see whether a change has <span class="No-Break">been detected:</span><pre class="console">
for I, val in enumerate(data_stream):
     in_drift, in_warning = ph.update(val)
     if in_drift:
       print (""Change detected at index {i}, input value: {val"")</pre></li>
				<li>We get the following<a id="_idIndexMarker1413"/> output. We see that drift is detected for three different distributions of the graph at different points in time. The change detection points are printed on two sides of <span class="No-Break">the plot.</span></li>
			</ol>
			<div>
				<div id="_idContainer232" class="IMG---Figure">
					<img src="image/Figure_11.13_B18681.jpg" alt="Figure 11.13 – Drift detected at three ranges of a distribution with a PH detector"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – Drift detected at three ranges of a distribution with a PH detector</p>
			<h3>The Fast Hoeffding Drift Detection Method</h3>
			<p>The <strong class="bold">Fast Hoeffding Drift Detection Method</strong> (<strong class="bold">FHDDM</strong>) allows the constant tracking of a sliding window of values<a id="_idIndexMarker1414"/> of the probability of correct predictions along with the maximum observed probability values. A drift is said to have occurred when the correct prediction probability drops below the maximum configured value, along with the difference in probabilities exceeding <span class="No-Break">a threshold.</span></p>
			<h3>Paired learner</h3>
			<p>The <strong class="bold">paired learner</strong> (<strong class="bold">PL</strong>) mechanism includes two<a id="_idIndexMarker1415"/> learners, one of which is a stable learner that gets trained on all data, and the other learner is trained on recent data. A counter is incremented each time the stable learner makes an error in prediction but the recent learner does not. To account for mistakes, a counter is decremented each time the recent learner makes an error in prediction. Once the increment counter exceeds a specified threshold, drift is considered to have occurred. This mechanism involves heavy computation to train new models and to have two learners <span class="No-Break">in place.</span></p>
			<h3>Exponentially Weighted Moving Average Concept Drift Detection</h3>
			<p>In the <strong class="bold">Exponentially Weighted Moving Average Concept Drift Detection</strong> (<strong class="bold">ECDD</strong>) method, the <strong class="bold">exponentially weighted moving average</strong> (<strong class="bold">EWMA</strong>) forecast is used by calculating the forecast’s mean and standard deviation continuously. It is often used to monitor<a id="_idIndexMarker1416"/> and detect the misclassification<a id="_idIndexMarker1417"/> rate of a streaming classifier. Drift is detected when the forecast exceeds the sum of the mean plus a multiple factor/coefficient of the <span class="No-Break">standard deviation.</span></p>
			<h3>Feature distribution</h3>
			<p>This drift detection technique functions<a id="_idIndexMarker1418"/> without response feedback by identifying a change in <em class="italic">p(y|x)</em> due to a corresponding change in<em class="italic"> p(x)</em>. This change can be detected using any multivariate unsupervised drift <span class="No-Break">detection technique.</span></p>
			<h3>Drift in a regression model</h3>
			<p>To detect drift<a id="_idIndexMarker1419"/> in a regression model, you take the regression error (a real number) and apply<a id="_idIndexMarker1420"/> any unsupervised drift detection technique to the <span class="No-Break">error data.</span></p>
			<h3>Ensemble and hierarchy drift detectors</h3>
			<p>Ensemble detectors<a id="_idIndexMarker1421"/> work primarily on an agreed<a id="_idIndexMarker1422"/> consensus level, where consensus can be derived from only a few, all, or a majority of learners. Hierarchical detectors come into play only after drift is detected by a detector at the first level using any of the drift detection techniques discussed previously. Then, the consensus<a id="_idIndexMarker1423"/> approach can<a id="_idIndexMarker1424"/> be used to validate the result<a id="_idIndexMarker1425"/> at other<a id="_idIndexMarker1426"/> levels, starting<a id="_idIndexMarker1427"/> from the next level. Some ensemble and hierarchy drift detector algorithms include <strong class="bold">Linear Fore Rates</strong> (<strong class="bold">LFR</strong>), <strong class="bold">Selective Detector Ensemble</strong> (<strong class="bold">eDetector</strong>), <strong class="bold">Drift Detection Ensemble</strong> (<strong class="bold">DDE</strong>), and <strong class="bold">Hierarchical Hypothesis </strong><span class="No-Break"><strong class="bold">Testing</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">HLFR</strong></span><span class="No-Break">).</span></p>
			<p>Now that we have looked at different types of concept drift detection techniques, let us discuss model explainability whenever there <span class="No-Break">is drift/calibration.</span></p>
			<h3>Multivariate drift detection with PCA</h3>
			<p>To detect drift from multivariate<a id="_idIndexMarker1428"/> data distributions, we use <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) by compressing<a id="_idIndexMarker1429"/> the data to a lower-dimensional space and then decompressing the data to retrieve the original feature representation. As we preserve only the relevant information in the transformation process, the reconstruction errors (evaluated using the Euclidean distance between the original and transformed data) help us to identify a change in data relationships among one or multiple features. In the first step, we compute the PCA on the original reference dataset and store the reconstruction errors with allowable limits of upper and lower thresholds. The process is repeated with the new data, where we compress and decompress the data using PCA. When the reconstruction errors exceed the upper or lower threshold, it signifies a change in <span class="No-Break">data distribution.</span></p>
			<p>The following code demonstrates how we can detect multivariate <span class="No-Break">feature drift:</span></p>
			<ol>
				<li>In the first<a id="_idIndexMarker1430"/> step, we have<a id="_idIndexMarker1431"/> the <span class="No-Break">necessary imports:</span><pre class="console">
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nannyml as nml
from scipy.spatial.transform import Rotation</pre></li>
				<li>Next, we have a random data setup based on its <span class="No-Break">three features:</span><pre class="console">
# analyze with reference and analysis periods
# Days/week * Hours/day * events/hour
DPP = 7*24*24
np.random.seed(23)
s1 = np.random.randn(DPP*22)
x1 = s1 + np.random.randn(DPP*22)/8
x2 = s1 + np.random.randn(DPP*22)/8
x3 = np.random.randn(DPP*22)/8
xdat = np.array([x1, x2, x3]).T
rot = Rotation.from_euler('z', 90, degrees=True)
# following matrix multiplication implementation, we need a 3xN data matrix hence we transpose
ydat = np.matmul(rot.as_matrix(), xdat.T).T
# drift is sudden and affects last 5-7 weeks
dataar = np.concatenate(
(xdat[:-5*DPP], ydat[-5*DPP:]),
axis=0)
datadf = pd.DataFrame(dataar, columns=['feature1', 'feature2', 'feature3'])
datadf = datadf.assign(ordered = pd.date_range(start='1/3/2020', freq='5min', periods=22*DPP))
datadf['week'] = datadf.ordered.dt.isocalendar().week - 1
datadf['partition'] = 'reference'
datadf.loc[datadf.week &gt;= 8, ['partition']] = 'analysis'
datadf = datadf.assign(y_pred_proba = np.random.rand(DPP*22))
datadf = datadf.assign(y_true = np.random.randint(2, size=DPP*22))</pre></li>
				<li>Next, we can do a further<a id="_idIndexMarker1432"/> interpretation of the independent<a id="_idIndexMarker1433"/> feature, but the goal is to set up the drift detector <span class="No-Break">as follows:</span><pre class="console">
rcerror_calculator = nml.DataReconstructionDriftCalculator(
feature_column_names=feature_column_names,
timestamp_column_name='ordered',
chunk_size=DPP
).fit(reference_data=reference)
rcerror_results = rcerror_calculator.calculate(data=analysis)
figure = rcerror_results.plot(plot_reference=True)
figure.show()</pre></li>
				<li>This yields what is shown in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.14</em>, where we see<a id="_idIndexMarker1434"/> that the data has drifted<a id="_idIndexMarker1435"/> from 0.84 <span class="No-Break">to 0.80.</span></li>
			</ol>
			<div>
				<div id="_idContainer233" class="IMG---Figure">
					<img src="image/Figure_11.14_B18681.jpg" alt="Figure 11.14 – Multivariate drift detector using PCA"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – Multivariate drift detector using PCA</p>
			<h1 id="_idParaDest-218"><a id="_idTextAnchor238"/>Understanding model explainability during concept drift/calibration</h1>
			<p>In the preceding section, we learned <a id="_idIndexMarker1436"/>about different types<a id="_idIndexMarker1437"/> of concept drift. Now, let us study how we can explain them with <span class="No-Break">interpretable ML:</span></p>
			<ol>
				<li>First, we import the necessary packages for creating a regression model and the drift explainer library. The California Housing dataset has been used to explain <span class="No-Break">concept drift:</span><pre class="console">
from xgboost import XGBRegressor
from cinnamon.drift import ModelDriftExplainer, AdversarialDriftExplainer
from sklearn import datasets
from sklearn.datasets import fetch_california_housing
from sklearn.datasets import fetch_openml
california = fetch_openml(name="house_prices", as_frame=True)
california_df = pd.DataFrame(california.data, columns=california.feature_names)
RANDOM_SEED = 2021</pre></li>
				<li>Then, we train the XGBoost <span class="No-Break">regressor model:</span><pre class="console">
model = XGBRegressor(n_estimators=1000, booster="gbtree",objective="reg:squarederror", learning_rate=0.05,max_depth=6,seed=RANDOM_SEED, use_label_encoder=False)
model.fit(X=X_train, y=y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=20, verbose=10)</pre></li>
				<li>In the next step, we fit<a id="_idIndexMarker1438"/> our trained<a id="_idIndexMarker1439"/> model using <strong class="source-inline">ModelDriftExplainer</strong>, plot the prediction, and retrieve any drift, if it is observed by <span class="No-Break">the explainer:</span><pre class="console">
drift_explainer = ModelDriftExplainer(model)
drift_explainer.fit(X_train, X_test, y_train, y_test)
drift_explainer.plot_prediction_drift()
drift_explainer.get_prediction_drift()</pre></li>
			</ol>
			<p>The following figure illustrates differences in drift detection in two different datasets.</p>
			<div>
				<div id="_idContainer234" class="IMG---Figure">
					<img src="image/Figure_11.15_B18681.jpg" alt="Figure 11.15 – Drift from the input features or data distributions of two datasets"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – Drift from the input features or data distributions of two datasets</p>
			<p>In <em class="italic">Figure 11.15</em>, it is evident<a id="_idIndexMarker1440"/> that there isn’t any<a id="_idIndexMarker1441"/> apparent drift in the distributions of the predictions.</p>
			<ol>
				<li value="4">Then, we plot the target labels to evaluate any drift in the <span class="No-Break">predicted outcomes:</span><pre class="console">
drift_explainer.plot_target_drift()
drift_explainer.get_target_drift()</pre></li>
			</ol>
			<p>However, as shown in <em class="italic">Figure 11.16</em>, we do not observe any apparent drift in the target labels:</p>
			<div>
				<div id="_idContainer235" class="IMG---Figure">
					<img src="image/Figure_11.16_B18681.jpg" alt="Figure 11.16 – Drift from the target data distributions of the California Housing dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16 – Drift from the target data distributions of the California Housing dataset</p>
			<ol>
				<li value="5">In the next step, when we evaluate<a id="_idIndexMarker1442"/> the performance<a id="_idIndexMarker1443"/> metrics of the California Housing train and test datasets, we can see a data drift from the mean and the explained variance of the <span class="No-Break">performance metrics:</span><pre class="console">
drift_explainer.get_performance_metrics_drift()</pre></li>
			</ol>
			<p>On running the drift explainer on the California Housing dataset, we get the following output:</p>
			<pre class="console">
<strong class="bold">PerformanceMetricsDrift(dataset1=RegressionMetrics (mse=10567733.794917172, explained_variance=0.9983637108518892), dataset2=RegressionMetrics(mse=540758766.1766539, explained_variance=0.9088298308495063))</strong></pre>
			<ol>
				<li value="6">Our next task is to plot drift values computed with the tree-based approach, obtain the feature importances of the California Housing dataset, and use <strong class="source-inline">AdversarialDriftExplainer</strong> on the datasets. This is illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.17</em>, which clearly shows that the <strong class="source-inline">Neighborhood_OldTown</strong> and <strong class="source-inline">BsmtQual_Gd</strong> features are the features most impacted by <span class="No-Break">the drift:</span></li>
			</ol>
			<div>
				<div id="_idContainer236" class="IMG---Figure">
					<img src="image/Figure_11.17_B18681.jpg" alt="Figure 11.17 – Feature importance in the resultant drift"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.17 – Feature importance in the resultant drift</p>
			<ol>
				<li value="7">In the end, we can <a id="_idIndexMarker1444"/>plot each feature<a id="_idIndexMarker1445"/> and evaluate the drift for each of them, as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.18</em>. Here, <strong class="source-inline">Neighborhood_OldTown</strong>, the first feature, does not show any noticeable drift between the train and <span class="No-Break">test datasets:</span><pre class="console">
drift_explainer.plot_feature_drift('Neighborhood_Old_Town')
drift_explainer.get_feature_drift('Neighborhood_Old_Town')'</pre></li>
			</ol>
			<p>The preceding code snippet yields the following output, showing the difference between the two datasets is not significant, as <strong class="source-inline">p_value</strong> is 0.996 &gt; 0.05:</p>
			<pre class="console">
<strong class="bold">DriftMetricsNum(mean_difference=-0.022504892367906065, wasserstein=0.022504892367906093, ks_test=BaseStatisticalTestResult (statistic=0.022504892367906065, pvalue=0.996919812985332))</strong></pre>
			<div>
				<div id="_idContainer237" class="IMG---Figure">
					<img src="image/Figure_11.18_B18681.jpg" alt="Figure 11.18 – Distribution differences/drift due to the Neighborhood_Old_Town feature"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18 – Distribution differences/drift due to the Neighborhood_Old_Town feature</p>
			<p>After gaining an understanding<a id="_idIndexMarker1446"/> of drift, as data<a id="_idIndexMarker1447"/> scientists, we also need to understand when we need to calibrate our models in the event of <span class="No-Break">any change.</span></p>
			<p>Understanding the need for <span class="No-Break">model calibration</span></p>
			<p>Recommendation<a id="_idIndexMarker1448"/> systems (content-based filtering or hybrid systems) are used in almost all industry sectors, including retail, telecoms, and energy and utilities. Deep learning recommendation models using user-to-user or item-to-item embeddings with explainability features have been able to build trust and confidence and improve the user experience. Deep learning recommendation systems have often used attention distributions to explain the neural<a id="_idIndexMarker1449"/> network’s performance, but such explanations in the case of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) are limited by poor calibrations of deep <span class="No-Break">neural networks.</span></p>
			<p>It has been observed that models become less reliable due to over-confidence or under-confidence impacting models designed for healthcare (disease detection) and autonomous driving, among others. In such a scenario where model reliability comes into question, it is important to have a metric such as model calibration in place so that the degree of a model’s predicted probability is correlated with its true correctness likelihood, which determines the <span class="No-Break">model's performance.</span></p>
			<p>In other words, a calibrated model can be called authentic when it has a high confidence level (more than 80%, say) where more than 80% of the predictions are classified accurately. We can also use model calibration to plot reliability plots. This serves as the accuracy of the model by interpreting reliability as a function of its confidence in the predictions. An over-confident model’s reliability plot falls below the identity function, while an under-confident plot’s reliability goes above the identity function. We also see that an authentic calibrated<a id="_idIndexMarker1450"/> model provides the perfect classification boundary where the reliability plot can be benchmarked as the <span class="No-Break">identity function.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.19</em> contains a reliability plot for the <strong class="bold">Deep Item-Based Collaborative Filtering</strong> (<strong class="bold">DeepICF</strong>) model. DeepICF examines nonlinear and higher-order<a id="_idIndexMarker1451"/> relationships among all interacting item pairs by training them using nonlinear neural networks. It can help us to understand how we model the predictions in different groups and study the trade-off between accuracy and confidence through a <span class="No-Break">reliability plot.</span></p>
			<div>
				<div id="_idContainer238" class="IMG---Figure">
					<img src="image/Figure_11.19_B18681.jpg" alt=" Figure 11.19 – A reliability plot for the DeepICF model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 11.19 – A reliability plot for the DeepICF model</p>
			<p>We segmented the model predictions into different buckets based on their confidence and calculated the accuracy<a id="_idIndexMarker1452"/> for each of them. <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.19</em> demonstrates the DeepICF model (with attention networks: <a href="https://www.researchgate.net/publication/333866071_Model_Explanations_under_Calibration">https://www.researchgate.net/publication/333866071_Model_Explanations_under_Calibration</a>) becoming over-confident as the confidence increases for both positive and negative classes. The DeepICF model is a deep neural network that is produced after learning latent low-dimensional embeddings of users and items. The pair-wise user and item interactions are captured with a neural network layer by means of an element-wise dot product. Further, the model also uses attention based pooling to yield an output vector of fixed size. This leads to a drop in accuracy for imbalanced and negatively skewed datasets, demonstrating that model explanations generated from the attention distribution become<a id="_idIndexMarker1453"/> less reliable with <span class="No-Break">over-confident predictions.</span></p>
			<p>Now, let us discuss how explainability and model calibration can be brought together when we see drifts in a model’s <span class="No-Break">predicted outcomes.</span></p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor239"/>Explainability and calibration</h2>
			<p>Model explainability and proper model calibration<a id="_idIndexMarker1454"/> can be achieved by addressing<a id="_idIndexMarker1455"/> imbalance in the dataset and by adding stability to attention distributions. However, one more problem that needs to be addressed is the calibration drift resulting from the same factors as concept drift. One example evident in the healthcare industry is poorly calibrated risk predictions with changing patient characteristics and disease incidence or prevalence rates in different health centers, regions, and countries. When an algorithm is trained in a setting with a high disease incidence, it is dominated by the model inputs and yields overestimated risk estimates. When such calibration drift occurs due to the deployment of models in nonstationary environments, these models require retraining and recalibration. Recalibration helps to fix the model’s accuracy and confidence levels and, consequently, the <span class="No-Break">reliability plot.</span></p>
			<p>Now, let us see, with an example, why it is necessary to calibrate a recommendation model, which is most useful in the <span class="No-Break">following situations:</span></p>
			<ul>
				<li>When a change in user preferences is observed due to the addition of new customer segments in <span class="No-Break">the population</span></li>
				<li>When a change in user preferences is observed among <span class="No-Break">existing customers</span></li>
				<li>When there are promotions/campaigns or new products <span class="No-Break">are released</span></li>
			</ul>
			<p>In the following example, let us study how post-preprocessing logic can be embedded in an underlying recommendation algorithm to ensure the recommendation becomes more calibrated. To explain this problem, we will <span class="No-Break">use </span><span class="No-Break"><strong class="source-inline">movielens-20m-dataset</strong></span><span class="No-Break">:</span></p>
			<ol>
				<li>To compute the utility metrics of recommender systems, we must compute the KL divergence between the user-item interaction distribution and the recommendation distribution. Here, we have chosen an associated lambda term that controls the score and calibration trade-off. The higher the lambda, the higher the probability that the resulting recommendation will <span class="No-Break">be calibrated:</span><pre class="console">
def compute_recommendation_utility(reco_items, interacted_distr, lmbda=0.65):
    total_score = 0.0
    reco_distr = compute_likely_genres(items_to_recommend)
    kl_div = evaluate_kl_divergence(interacted_distr, reco_distr)
    for item in items_to_recommend:
        total_score += item.score
    rec_utility = (1 - lmbda) * total_score - lmbda * kl_div
    return rec_utility</pre></li>
				<li>The utility function defined <a id="_idIndexMarker1456"/>here is invoked at each iteration to update the list<a id="_idIndexMarker1457"/> with the item that maximizes the <span class="No-Break">utility function:</span><pre class="console">
def calibrate_recommendations(items, interacted_distr, topn, lmbda=0.65):
    calib_rec_items = []
    for _ in range(topn):
        max_utility = -np.inf
        for rec_item in items:
            if rec_item in calib_rec_items:
                continue
            rec_utility = compute_recommendation_utility(calib_rec_items + [rec_item], interacted_distr, lmbda)
            if rec_utility &gt; max_utility:
                max_utility = rec_utility
                best_item = rec_item
        calib_rec_items.append(best_item)
    return calib_rec_items</pre></li>
				<li>The lambda term allows<a id="_idIndexMarker1458"/> us to tweak the controller (lambda) to extremely elevated levels<a id="_idIndexMarker1459"/> to generate the modified calibrated recommendation list. Now, let us differentiate and evaluate the computed recommendation generated after calibrating it (to optimize the score, 𝑠), the original recommendation, and the user’s past relationship with the items. Here, 𝑠(𝑖) represents the score of the items, 𝑖∈𝐼, predicted by the recommender system, and s(I) = ∑i ∈ Is(i) denotes the sum of all the items’ scores in the newly <span class="No-Break">generated list:</span><pre class="console">
calib_rec_item_distr = compute_likely_genres(calib_items_to_recommend)
calib_reco_kl_div = evaluate_kl_divergence(interacted_distr, calib_rec_item_distr)
reco_kl_div = evaluate_kl_divergence(interacted_distr, reco_distr)
distr_comparison_plot(interacted_distr, calib_rec_item_distr)</pre></li>
			</ol>
			<p>Here, we observe that the calibrated recommendation has larger coverage of the genre, and its distribution looks like that of the distribution of the user’s past interactions and the calibration metric. KL divergence also ensures that the value generated from the calibrated recommendations is lower than the original recommendation’s score. Even though the precision of calibrated recommendation distribution (0.125) is lower than the original distribution’s precision (0.1875), we can further control the lambda to achieve an acceptable trade-off between precision <span class="No-Break">and calibration.</span></p>
			<div>
				<div id="_idContainer239" class="IMG---Figure">
					<img src="image/Figure_11.20_B18681.jpg" alt="Figure 11.20 – Comparing a user’s historical distribution and calibrated recommendation distribution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.20 – Comparing a user’s historical distribution and calibrated recommendation distribution</p>
			<p>In the preceding discussion, we saw<a id="_idIndexMarker1460"/> the importance of developing a calibrated model due<a id="_idIndexMarker1461"/> to changes in the input data and the model. Now, let us discuss, from the standpoint of ethical AI, how to incorporate fairness into models and build <span class="No-Break">calibrated models.</span></p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor240"/>Challenges with calibration and fairness</h2>
			<p>So far, we have learned what<a id="_idIndexMarker1462"/> it means to have a fair ML model across different population<a id="_idIndexMarker1463"/> subgroups so that the prediction results are unbiased across all races, ethnicities, genders, and other population categories. From the standpoint of AI ethics, we should also try to design fair and calibrated models, and in the process, try to understand the risks associated with them. To design a fair and calibrated model, it is essential that a group of people assigned a predicted probability of <em class="italic">p</em> of generic ML models sees a fair representation. To achieve this, we should have a <em class="italic">p</em> fraction of members of this set belonging to positive instances of the <span class="No-Break">classification problem.</span></p>
			<p>Thus, to justify fairness between two groups, G1 and G2 (such as African-American and white defendants), the best way to satisfy both groups is for the calibration condition to hold simultaneously for each individual within each of these groups <span class="No-Break">as well.</span></p>
			<p>However, calibration and error-rate constraints have mutually conflicting goals. Research studies demonstrate that calibration is tolerant only with a single error constraint (which is equal false negative rates across groups). It also becomes increasingly hard to minimize<a id="_idIndexMarker1464"/> error disparity across different population groups<a id="_idIndexMarker1465"/> with calibrated probability estimates. Even when the objective is satisfied, the resulting solution resembles a generic classifier, which only optimizes for a percentage of predictions. Thus, to summarize, a perfectly fair and calibrated model cannot <span class="No-Break">be designed.</span></p>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor241"/>Summary</h1>
			<p>In this chapter, we have learned about different ideas related to concept drift. These can be applied to both streaming (batch streams) and live data as well as trained ML models. We also learned how both statistical and contextual methods play an important role in estimating model metrics by determining model drift. The chapter also answered some important questions related to model drift and explainability and helped you to understand model calibration. In the context of calibration, we also learned about fairness and calibration and the limitations of achieving both at the <span class="No-Break">same time.</span></p>
			<p>In the next chapter, we will learn more about model evaluation techniques and handling uncertainties in <span class="No-Break">model-building pipelines.</span></p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor242"/>Further reading</h1>
			<ul>
				<li><em class="italic">8 Concept Drift Detection </em><span class="No-Break"><em class="italic">Methods</em></span><span class="No-Break">: </span><a href="https://www.aporia.com/blog/concept-drift-detection-methods/"><span class="No-Break">https://www.aporia.com/blog/concept-drift-detection-methods/</span></a></li>
				<li><em class="italic">On Fairness and </em><span class="No-Break"><em class="italic">Calibration</em></span><span class="No-Break">: </span><a href="https://proceedings.neurips.cc/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf"><span class="No-Break">https://proceedings.neurips.cc/paper/2017/file/b8b9c74ac526fffbeb2d39ab038d1cd7-Paper.pdf</span></a></li>
				<li><em class="italic">Calibrated </em><span class="No-Break"><em class="italic">Recommendations</em></span><span class="No-Break">: </span><a href="http://ethen8181.github.io/machine-learning/recsys/calibration/calibrated_reco.html"><span class="No-Break">http://ethen8181.github.io/machine-learning/recsys/calibration/calibrated_reco.html</span></a></li>
			</ul>
		</div>
	</body></html>