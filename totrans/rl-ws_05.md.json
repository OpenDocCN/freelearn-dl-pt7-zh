["```py\n2 x 2 x 2 x 2 x 2 = 25= 32\n```", "```py\nPRICES = [\"NA\", 9, 40, 50, 70, 80]\n```", "```py\ndef partition(cake_size):\n    \"\"\"\n    Partitions a cake into different sizes, and calculates the\n    most profitable cut configuration\n    Args:\n        cake_size: size of the cake\n    Returns:\n        the best profit possible\n    \"\"\"\n    if cake_size == 0:\n        return 0\n    best_profit = -1\n    for i in range(1, cake_size + 1):\n        best_profit = max(best_profit, PRICES[i] \\\n                         + partition(cake_size - i))\n    return best_profit\n```", "```py\nif __name__ == '__main__':\n    size = 5\n    best_profit_result = partition(size)\n    print(f\"Best profit: {best_profit_result}\")\n```", "```py\nBest profit: 90\n```", "```py\ndef partition(cake_size):\n    \"\"\"\n    Partitions a cake into different sizes, and calculates the\n    most profitable cut configuration\n    Args:\n        cake_size: size of the cake\n    Returns:\n        the best profit possible\n    \"\"\"\n    if cake_size == 0:\n        return 0\n    best_profit = -1\n    for i in range(1, cake_size + 1):\n        best_profit = max(best_profit, PRICES[i] \\\n                      + partition(cake_size - i))\n    print(f\"Best profit for size {cake_size} is {best_profit}\")\n    return best_profit\n```", "```py\nif __name__ == '__main__':\n    size = 5\n    best_profit_result = partition(size)\n    print(f\"Best profit: {best_profit_result}\")\n```", "```py\nBest profit for size 1 is 9\nBest profit for size 2 is 40\nBest profit for size 1 is 9\nBest profit for size 3 is 50\nBest profit for size 1 is 9\nBest profit for size 2 is 40\nBest profit for size 1 is 9\nBest profit for size 4 is 80\nBest profit for size 1 is 9\nBest profit for size 2 is 40\nBest profit for size 1 is 9\nBest profit for size 3 is 50\nBest profit for size 1 is 9\nBest profit for size 2 is 40\nBest profit for size 1 is 9\nBest profit for size 5 is 90\nBest profit: 90\n```", "```py\n    if cake_size == 0:\n        return 0\n    if cake_size in memo:\n        return memo[cake_size]\n```", "```py\ndef memoized_partition(cake_size, memo):\n    \"\"\"\n        Partitions a cake into different sizes, and calculates the\n        most profitable cut configuration using memoization.\n        Args:\n            cake_size: size of the cake\n            memo: a dictionary of 'best_profit' values indexed\n                by 'cake_size'\n        Returns:\n            the best profit possible\n        \"\"\"\n    if cake_size == 0:\n        return 0\n    if cake_size in memo:\n        return memo[cake_size]\n    else:\n        best_profit = -1\n        for i in range(1, cake_size + 1):\n            best_profit = max(best_profit, \\\n                              PRICES[i] + memoized_partition\\\n                                          (cake_size - i, memo))\n        print(f\"Best profit for size {cake_size} is {best_profit}\")\n        memo[cake_size] = best_profit\n        return best_profit\n```", "```py\nBest profit for size 1 is 9\nBest profit for size 2 is 40\nBest profit for size 3 is 50\nBest profit for size 4 is 80\nBest profit for size 5 is 90\nBest profit: 90\n```", "```py\ndef tabular_partition(cake_size):\n    \"\"\"\n    Partitions a cake into different sizes, and calculates the\n    most profitable cut configuration using tabular method.\n    Args:\n        cake_size: size of the cake\n    Returns:\n        the best profit possible\n    \"\"\"\n    profits = [0] * (cake_size + 1)\n    for i in range(1, cake_size + 1):\n        best_profit = -1\n        for current_size in range(1, i + 1):\n            best_profit = max(best_profit,\\\n                          PRICES[current_size] \\\n                          + profits[i - current_size])\n        profits[i] = best_profit\n    return profits[cake_size]\n```", "```py\nBest profit: 90\n```", "```py\nFibonacci(n) = Fibonacci(n – 1) + Fibonacci(n – 2)\nTribonacci(n) = Tribonacci(n – 1) \\\n                + Tribonacci(n – 2) + Tribonacci(n – 3)\n```", "```py\n    def tribonacci_recursive(n):\n        \"\"\"\n        Uses recursion to calculate the nth tribonacci number\n        Args:\n            n: the number\n        Returns:\n            nth tribonacci number\n        \"\"\"\n        if n <= 1:\n            return 0\n        elif n == 2:\n            return 1\n        else:\n            return tribonacci_recursive(n - 1) \\\n                   + tribonacci_recursive(n - 2) \\\n                   + tribonacci_recursive(n - 3)\n    ```", "```py\n    if __name__ == '__main__':\n        print(tribonacci_recursive(6))\n    ```", "```py\n    def tribonacci_memo(n, memo):\n        \"\"\"\n        Uses memoization to calculate the nth tribonacci number\n        Args:\n            n: the number\n            memo: the dictionary that stores intermediate results\n        Returns:\n            nth tribonacci number\n        \"\"\"\n        if n in memo:\n            return memo[n]\n        else:\n            ans1 = tribonacci_memo(n - 1, memo)\n            ans2 = tribonacci_memo(n - 2, memo)\n            ans3 = tribonacci_memo(n - 3, memo)\n            res = ans1 + ans2 + ans3\n            memo[n] = res\n            return res\n    ```", "```py\n    if __name__ == '__main__':\n        memo = {0: 0, 1: 0, 2: 1}\n        print(tribonacci_memo(6, memo))\n    ```", "```py\n    7\n    ```", "```py\n    import numpy as np\n    ```", "```py\n    def lcs_brute_force(first, second):\n        \"\"\"\n        Use brute force to calculate the longest common \n        substring of two strings\n        Args:\n            first: first string\n            second: second string\n        Returns:\n            the length of the longest common substring\n        \"\"\"\n        len_first = len(first)\n        len_second = len(second)\n        max_lcs = -1\n        lcs_start, lcs_end = -1, -1\n        # for every possible start in the first string\n        for i1 in range(len_first):\n            # for every possible end in the first string\n            for j1 in range(i1, len_first):\n                # for every possible start in the second string\n                for i2 in range(len_second):\n                    # for every possible end in the second string\n                    for j2 in range(i2, len_second):\n                        \"\"\"\n                        start and end position of the current\n                        candidates\n                        \"\"\"\n                        slice_first = slice(i1, j1)\n                        slice_second = slice(i2, j2)\n                        \"\"\"\n                        if the strings match and the length is the\n                        highest so far\n                        \"\"\"\n                        if first[slice_first] == second[slice_second] \\\n                           and j1 - i1 > max_lcs:\n                            # save the lengths\n                            max_lcs = j1 - i1\n                            lcs_start = i1\n                            lcs_end = j1\n        print(\"LCS: \", first[lcs_start: lcs_end])\n        return max_lcs\n    ```", "```py\n    if __name__ == '__main__':\n        a = \"BBBABDABAA\"\n        b = \"AAAABDABBAABB\"\n        lcs_brute_force(a, b)\n    ```", "```py\n     LCS:  ABDAB\n    ```", "```py\n    def lcs_tabular(first, second):\n        \"\"\"\n        Calculates the longest common substring using memoization.\n        Args:\n            first: the first string\n            second: the second string\n        Returns:\n            the length of the longest common substring.\n        \"\"\"\n        # initialize the table using numpy\n        table = np.zeros((len(first), len(second)), dtype=int)\n        for i in range(len(first)):\n            for j in range(len(second)):\n                if first[i] == second[j]:\n                    table[i][j] += 1 + table[i - 1][j - 1]\n        print(table)\n        return np.max(table)\n    ```", "```py\n    if __name__ == '__main__':\n        a = \"BBBABDABAA\"\n        b = \"AAAABDABBAABB\"\n        lcs_tabular(a, b)\n    ```", "```py\n    import numpy as np\n    import pandas as pd\n    ```", "```py\n    def count_changes(N, denominations):\n        \"\"\"\n        Counts the number of ways to add the coin denominations\n        to N.\n        Args:\n            N: number to sum up to\n            denominations: list of coins\n        Returns:\n        \"\"\"\n        print(f\"Counting number of ways to get to {N} using coins:\\\n    {denominations}\")\n    ```", "```py\n        table = np.ones((len(denominations), N + 1)).astype(int)\n        # run the loop from 1 since the first row will always 1s\n        for i in range(1, len(denominations)):\n            for j in range(N + 1):\n                if j < denominations[i]:\n                    \"\"\"\n                    If the index is less than the denomination\n                    then just copy the previous best\n                    \"\"\"\n                    table[i, j] = table[i - 1, j]\n                else:\n                    \"\"\"\n                    If not, the add two things:\n                    1\\. The number of ways to sum up to \n                       N *without* considering\n                       the existing denomination.\n                    2\\. And, the number of ways to sum up to N minus \n                       the value of the current denomination \n                       (by considering the current and the \n                       previous denominations)\n                    \"\"\"\n                    table[i, j] = table[i - 1, j] \\\n                                  + table[i, j - denominations[i]]\n    ```", "```py\n        # print the table\n        print_table(table, denominations)\n    ```", "```py\n    def print_table(table, denominations):\n        \"\"\"\n        Pretty print a numpy table\n        Args:\n            table: table to print\n            denominations: list of coins\n        Returns:\n        \"\"\"\n        df = pd.DataFrame(table)\n        df = df.set_index(np.array(denominations))\n        print(df)\n    ```", "```py\n    if __name__ == '__main__':\n        N = 5\n        denominations = [1, 2]\n        count_changes(N, denominations)\n    ```", "```py\n    Counting number of ways to get to 5 using coins: [1, 2]\n       0  1  2  3  4  5\n    1  1  1  1  1  1  1\n    2  1  1  2  2  3  3\n    ```", "```py\ndef initialize_environment():\n    \"\"\"initialize the OpenAI Gym environment\"\"\"\n    env = gym.make(\"Taxi-v3\")\n    print(\"Initializing environment\")\n    # reset the current environment\n    env.reset()\n    # show the size of the action space\n    action_size = env.action_space.n\n    print(f\"Action space: {action_size}\")\n    # Number of possible states\n    state_size = env.observation_space.n\n    print(f\"State space: {state_size}\")\n    return env\n```", "```py\ndef random_step(n_steps=5):\n    \"\"\"\n    Steps through the taxi v3 environment randomly\n    Args:\n        n_steps: Number of steps to step through\n    \"\"\"\n    # reset the environment\n    env = initialize_environment()\n    state = env.reset()\n    for i in range(n_steps):\n        # choose an action at random\n        action = env.action_space.sample()\n        env.render()\n        new_state, reward, done, info = env.step(action)\n        print(f\"New State: {new_state}\\n\"\\\n              f\"reward: {reward}\\n\"\\\n              f\"done: {done}\\n\"\\\n              f\"info: {info}\\n\")\\\n        print(\"*\" * 20)\n```", "```py\n    def policy_iteration(env):\n        \"\"\"\n        Find the most optimal policy for the Taxi-v3 environment \n        using Policy Iteration\n        Args:\n            env: Taxi=v3 environment\n        Returns:\n            policy: the most optimal policy\n        \"\"\"\n        V = dict()\n    ```", "```py\n    \"\"\"\n    initially the value function for all states\n    will be random values close to zero\n    \"\"\"\n    state_size = env.observation_space.n\n    for i in range(state_size):\n        V[i] = np.random.random()\n    # when the change is smaller than this, stop\n    small_change = 1e-20\n    # future reward coefficient\n    gamma = 0.9\n    episodes = 0\n    # train for this many episodes\n    max_episodes = 50000\n    # initially we will start with a random policy\n    current_policy = dict()\n    for s in range(state_size):\n        current_policy[s] = env.action_space.sample()\n    ```", "```py\n    while episodes < max_episodes:\n        episodes += 1\n        # policy evaluation\n        V = policy_evaluation(V, current_policy, \\\n                              env, gamma, small_change)\n        # policy improvement\n        current_policy, policy_changed = policy_improvement\\\n                                         (V, current_policy, \\\n                                          env, gamma)\n        # if the policy didn't change, it means we have converged\n        if not policy_changed:\n            break\n    print(f\"Number of episodes trained: {episodes}\")\n    return current_policy\n    ```", "```py\n    def policy_evaluation(V, current_policy, env, gamma, \\\n                          small_change):\n        \"\"\"\n        Perform policy evaluation iterations until the smallest \n        change is less than\n        'smallest_change'\n        Args:\n            V: the value function table\n            current_policy: current policy\n            env: the OpenAI Tax-v3 environment\n            gamma: future reward coefficient\n            small_change: how small should the change be for the \n              iterations to stop\n        Returns:\n            V: the value function after convergence of the evaluation\n        \"\"\"\n        state_size = env.observation_space.n\n    ```", "```py\n        while True:\n            biggest_change = 0\n            # loop through every state present\n            for state in range(state_size):\n                old_V = V[state]\n                # take the action according to the current policy\n                action = current_policy[state]\n                prob, new_state, reward, done = env.env.P[state]\\\n                                                [action][0]\n    ```", "```py\n                V[state] = reward + gamma * V[new_state]\n                \"\"\"\n                if the biggest change is small enough then it means\n                the policy has converged, so stop.\n                \"\"\"\n                biggest_change = max(biggest_change, \\\n                                     abs(V[state] – old_V))\n            if biggest_change < small_change:\n                break\n        return V\n    ```", "```py\n    def policy_improvement(V, current_policy, env, gamma):\n        \"\"\"\n        Perform policy improvement using the \n        Bellman Optimality Equation.\n        Args:\n            V: the value function table\n            current_policy: current policy\n            env: the OpenAI Tax-v3 environment\n            gamma: future reward coefficient\n        Returns:\n            current_policy: the updated policy\n            policy_changed: True, if the policy was changed, \n            else, False\n        \"\"\"\n    ```", "```py\n        state_size = env.observation_space.n\n        action_size = env.action_space.n\n        policy_changed = False\n        for state in range(state_size):\n            best_val = -np.inf\n            best_action = -1\n            # loop over all actions and select the best one\n            for action in range(action_size):\n                prob, new_state, reward, done = env.env.P[state]\\\n                                                [action][0]\n    ```", "```py\n                future_reward = reward + gamma * V[new_state]\n                if future_reward > best_val:\n                    best_val = future_reward\n                    best_action = action\n            \"\"\"\n            using assert statements we can avoid getting \n            into unwanted situations\n            \"\"\"\n            assert best_action != -1\n            if current_policy[state] != best_action:\n                policy_changed = True\n            # update the best action for this current state\n            current_policy[state] = best_action\n        # if the policy didn't change, it means we have converged\n        return current_policy, policy_changed\n    ```", "```py\n    if __name__ == '__main__':\n        env = initialize_environment()\n        policy = value_iteration(env)\n        play(policy, render=True)\n    ```", "```py\n    def play(policy, render=False):\n        \"\"\"\n        Perform a test pass on the Taxi-v3 environment\n        Args:\n            policy: the policy to use\n            render: if the result should be rendered at every step. \n                    False by default\n        \"\"\"\n        env = initialize_environment()\n        rewards = []\n    ```", "```py\n        max_steps = 25\n        test_episodes = 2\n        for episode in range(test_episodes):\n            # reset the environment every new episode\n            state = env.reset()\n            total_rewards = 0\n            print(\"*\" * 100)\n            print(\"Episode {}\".format(episode))\n            for step in range(max_steps):\n    ```", "```py\n                action = policy[state]\n                new_state, reward, done, info = env.step(action)\n                if render:\n                    env.render()\n                total_rewards += reward\n                if done:\n                    rewards.append(total_rewards)\n                    print(\"Score\", total_rewards)\n                    break\n                state = new_state\n        env.close()\n        print(\"Average Score\", sum(rewards) / test_episodes)\n    ```", "```py\ndef value_iteration(env):\n    \"\"\"\n    Performs Value Iteration to find the most optimal policy for the\n    Tax-v3 environment\n    Args:\n        env: Taxiv3 Gym environment\n    Returns:\n        policy: the most optimum policy\n    \"\"\"\n    V = dict()\n    gamma = 0.9\n    state_size = env.observation_space.n\n    action_size = env.action_space.n\n    policy = dict()\n    # initialize the value table randomly\n    # initialize the policy randomly\n    for x in range(state_size):\n        V[x] = 0\n        policy[x] = env.action_space.sample()\n```", "```py\n\"\"\"\nthis loop repeats until the change in value function\nis less than delta\n\"\"\"\nwhile True:\n    delta = 0\n    for state in reversed(range(state_size)):\n        old_v_s = V[state]\n        best_rewards = -np.inf\n        best_action = None\n        # for all the actions in current state\n        for action in range(action_size):\n            # check the reward obtained if we were to perform\n            # this action\n            prob, new_state, reward, done = \n              env.env.P[state][action][0]\n            potential_reward = reward + gamma * V[new_state]\n            # select the one that has the best reward\n            # and also save the action to the policy\n            if potential_reward > best_rewards:\n                best_rewards = potential_reward\n                best_action = action\n        policy[state] = best_action\n        V[state] = best_rewards\n        # terminate if the change is not high\n        delta = max(delta, abs(V[state] - old_v_s))\n    if delta < 1e-30:\n        break\nif __name__ == '__main__':\n    env = initialize_environment()\n    # policy = policy_iteration(env)\n    policy = value_iteration(env)\n    play(policy, render=True)\n```", "```py\nSFFF       (S: starting point, safe)\nFHFH       (F: frozen surface, safe)\nFFFH       (H: hole, fall to your doom)\nHFFG       (G: goal, where the frisbee is located)\n```"]