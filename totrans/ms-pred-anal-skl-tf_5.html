<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Predictive Analytics with TensorFlow and Deep Neural Networks</h1>
                </header>
            
            <article>
                
<p>TensorFlow is an open source library developed by <strong>Google Brain Team</strong>. It is used in large-scale machine learning applications, such as neural networks, and for making numerical computations. Developers are able to create dataflow graphs using TensorFlow. These graphs show the movement of data. TensorFlow can be used to train and run deep neural networks for various applications such as image recognition, machine language translation, and natural language processing.</p>
<p>We already know that predictive analytics is about providing predictions about unknown events. We are going to use it here with TensorFlow.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Predictions with TensorFlow</li>
<li>Regression with <strong>Deep Neural networks</strong> (<strong>DNNs</strong>)</li>
<li>Classification with DNNs</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Predictions with TensorFlow</h1>
                </header>
            
            <article>
                
<p>We will perform the <kbd>hello world</kbd> example of deep learning. This example is used to check and ensure that a model is working as intended. For this, we will use the MNIST dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to the MNIST dataset</h1>
                </header>
            
            <article>
                
<p>MNIST stands for <strong>Mixed National Institute of Standards and Technology</strong>, which has produced a handwritten digits dataset. This is one of the most researched datasets in machine learning, and is used to classify handwritten digits. This dataset is helpful <span><span>for predictive </span></span>analytics because of its sheer size, allowing deep learning to work its magic efficiently. This dataset contains 60,000 training images and 10,000 testing images, formatted as 28 x 28 pixel monochrome images. The following screenshot shows the images contained in this dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-612 image-border" src="assets/29266a1b-4e37-493c-b841-555f004034c3.png" style="width:29.00em;height:34.58em;"/></p>
<p>In the preceding screenshot, we can see that, for every handwritten digit, there is a corresponding true label; we can therefore use this dataset to build classification models. So we can use the image to classify each into one of the 10 digits from 0 to 9.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building classification models using MNIST dataset</h1>
                </header>
            
            <article>
                
<p>Let's take a look at the following steps and learn to build a classification model:</p>
<ol>
<li>We have to import the libraries that we will use in this dataset. Use t<span>he following lines of code to import the</span> <kbd>tensorflow</kbd>, <kbd>numpy</kbd>, and <kbd>matplotlib</kbd> libraries:</li>
</ol>
<pre style="padding-left: 60px">import tensorflow as tf<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>from tensorflow.contrib.layers import fully_connected<br/><br/>%matplotlib inline</pre>
<ol start="2">
<li>We will import the <kbd>fully_connected</kbd><span> function, which we will be used to build the layers of our network, from </span><kbd>tensorflow.contrib.layers</kbd>. </li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elements of the DNN model</h1>
                </header>
            
            <article>
                
<p>Before running the model, we first have to determine the elements that we will use in building a multilayer perceptron model. Following are the elements that we will use in this model:</p>
<ul>
<li><strong>Architecture</strong>: The model contains 728 neurons in the input layer. This is because we have 28 images and each image has 28 pixels. Here, each pixel is a feature in this case, so we have 728 pixels. We will have 10 elements in the output layer, and we will also use three hidden layers, although we could use any number of hidden layers. Here, we will use three hidden layers. The number of neurons we will use in each layer is 350 in the first layer, 200 in the second one, and 100 in the last layer.</li>
<li><strong>Activation function</strong>: We will use the ReLU activation function, as shown in the following code block:</li>
</ul>
<pre style="padding-left: 90px">vector = np.arange(-5,5,0.1)<br/>def relu(x) :<br/>return max(0.,x)<br/>relu = np.vectorize(relu)</pre>
<p style="padding-left: 60px">If the input is negative, the function outputs <kbd>0</kbd>, and if the input is positive the function just outputs the same value as the input. So, mathematically, the ReLU function looks similar to this. The following screenshot shows the lines of code used for generating the graphical representation of the ReLU activation function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-613 image-border" src="assets/e5bfc50a-2e28-44e0-8cf2-294a24807836.png" style="width:36.08em;height:29.42em;"/></p>
<p style="padding-left: 60px">It gains the maximum between <kbd>0</kbd> and the input. This activation function will be used in every neuron of the hidden layers.</p>
<ul>
<li><strong>Optimizing </strong><strong>algorithm</strong>: The optimizing algorithm used here is the gradient descent with a learning rate of 0.01.</li>
<li><strong>Loss function</strong>: For the <kbd>loss</kbd> function, we will use the <kbd>cross_entropy</kbd> function, but as with other loss functions that we have used in this book, this function measures the distance between the actual values and the predictions that the model makes.</li>
<li><strong>Weights initialization strategy</strong>: For this, we will use the Xavier initializer, <span>a method that actually comes with</span> the <kbd>fully_connected</kbd><span> function from TensorFlow as a default.</span></li>
<li><strong>Regularization strategy</strong>: We are not going to use any regularization strategy.</li>
</ul>
<ul>
<li><strong>Training strategy</strong>: We are going to use 20 epochs. The dataset will be presented to the network 20 times, and in every iteration, we will use a batch size of 80. So, we will present the data to the network 80 points at a time and the whole dataset 20 times.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the DNN</h1>
                </header>
            
            <article>
                
<p>Now we will import the dataset that we are going to use. The reason for using this dataset is that it is easily available. We are going to actually use this dataset and build a DNN model around it. In the next sections, we will see the steps involved in building a DNN model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading the data</h1>
                </header>
            
            <article>
                
<p>Here, we read data in the cell. The following screenshot shows the lines of code used to read the data:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-614 image-border" src="assets/d3aaf54f-010c-4f34-a41a-d7f11fef7522.png" style="width:43.58em;height:14.17em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the architecture</h1>
                </header>
            
            <article>
                
<p>We will use three hidden layers, with 256 neurons for the first layer, 128 for the second, and 64 for the third one. The following code snippet shows the architecture for the classification example:</p>
<pre>n_inputs = 28*28<br/>n_hidden1 = 350<br/>n_hidden2 = 200<br/>n_hidden3 = 100<br/>n_outputs = 10</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Placeholders for inputs and labels</h1>
                </header>
            
            <article>
                
<p>The values of different layers are the objects, and are also called placeholders for inputs and labels. These placeholders are used for feeding the data into the network. The following lines of code are used for showing placeholders for the inputs and labels:</p>
<pre>X = tf.placeholder(tf.float32, shape=[None, n_inputs])<br/>y = tf.placeholder(tf.int64)</pre>
<p>So we have a placeholder <kbd>X</kbd> for the features, which is the input layer, and we have a placeholder <kbd>y</kbd> for the target value. So this object will contain the actual true labels of the digits.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the neural network</h1>
                </header>
            
            <article>
                
<p>For building DNNs, we use the <kbd>fully_connected</kbd> function for the first hidden layer. The input for this hidden layer is <kbd>x</kbd>, which is the data from the placeholder. <kbd>n_hidden1</kbd> is the number of neurons that we have in this hidden layer, which you will remember is 350 neurons. Now, this hidden layer 1 becomes the input for the hidden layer 2, and <kbd>n_hidden2</kbd> is the number of neurons in this layer. Likewise, hidden layer 2 becomes the input for the third hidden layer and we will use this number of neurons in this layer. Finally, the output layer, which we will call <kbd>logits</kbd>, is the fully connected layer that we use as input, hidden layer 3. The following screenshot shows the lines of code used for building the neural network:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-615 image-border" src="assets/02a2f19c-c256-420c-9373-11069daa5387.png" style="width:56.58em;height:12.67em;"/></p>
<p>We enter the output as 10 because we have 10 categories in our classification problem and we know that in the output layer we don't use any activation function.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The loss function</h1>
                </header>
            
            <article>
                
<p>For our <kbd>loss</kbd> function, we use the cross-entropy function. TensorFlow provides us with many such functions. For example, in this case, we are using the <kbd>sparse_softmax_cross_entropy _with_logits</kbd> function because here we got <kbd>logits</kbd> from the network. So, in this function, we pass the actual labels. These are the true labels, which are <kbd>logits</kbd>—the results or the output of our network. The following screenshot shows the lines of code used for showing the use of the <kbd>reduce_mean</kbd> function with this cross-entropy for getting the loss:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-618 image-border" src="assets/73884877-d7ad-4b6c-90d8-2b054d336ead.png" style="width:52.83em;height:9.67em;"/></p>
<p>Now, using this cross-entropy, we can calculate the loss as the mean of the vector that we will get here. So this is the <kbd>loss</kbd> function and the mean of the cross-entropy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining optimizer and training operations</h1>
                </header>
            
            <article>
                
<p>The goal of the optimizer is to minimize the loss, and it does this by adjusting the different weights that we have in all the layers of our network. The optimizer used here is the gradient descent with a learning rate of <kbd>0.01</kbd>. The following screenshot shows the lines of code used for defining the optimizer and also shows the training operations.</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-619 image-border" src="assets/2861b9b5-3425-4cbc-917c-34c6bebfb8fc.png" style="width:50.58em;height:10.67em;"/></p>
<p>Each time we run the training operation <kbd>training_op</kbd>, the optimizer will change the values of these weights a little bit. In doing so, it minimizes the loss, and the predictions and the actual values are as close as possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training strategy and valuation of accuracy of the classification</h1>
                </header>
            
            <article>
                
<p>Here we set the training strategy. We will use 20 epochs with a batch size of 80. In all of these cells, we have build the computational graph that will be used in this program. The following screenshot shows the lines of code used for showing the training strategy and the couple of nodes for evaluating the accuracy of the classification:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-622 image-border" src="assets/c059d22e-2a90-4e34-94ba-d7f806e01d5a.png" style="width:58.17em;height:20.42em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the computational graph</h1>
                </header>
            
            <article>
                
<p>For actually running the computational graph, first we will initialize all the variables in our program. The following screenshot shows the lines of code used for running the computational graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-623 image-border" src="assets/dfff9daa-7bfb-4207-bb88-7419a4c8654a.png" style="width:73.75em;height:41.83em;"/></p>
<p>In line 3, we initialize all the variables in our program. Now, here, we don't have any variables explicitly. However, the variables that are inside are fully connected. The <kbd>fully_connected</kbd> function is where we have all the hidden layers that contain the weights. These are the variables which is why we must initialize the variables with the <kbd>global_ variables_initializer</kbd> object and run this node. For each epoch, we run this loop 20 times. Now, for each iteration that we have in the number of examples over the batch size, which is 80, we get the values for the features and the targets. So this will be 80 data points for each iteration. Then, we run the training operation and will pass as <kbd>x</kbd>; we will pass the feature values and here we will pass the target values. Remember, <kbd>x</kbd> and <kbd>y</kbd> are our placeholders. Then, we evaluate the accuracy of the training and then evaluate the accuracy in the testing dataset, and we get the testing dataset. We get from <kbd>mnist.test.images</kbd>, and so these are now the features and <kbd>test.labels</kbd> are the targets. Then, we print the two accuracies after these two loops are completed.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We then produce some individual predictions for the first 15 images in the testing dataset. After running this, we get the first epoch, with a training accuracy of 86 percent and a testing accuracy of 88-89 percent. The following screenshot shows the results of training and the testing results for different epochs:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/16acb53f-7d10-4c38-9d16-273f6bd7b386.png" style="width:98.92em;height:61.92em;"/></p>
<p>The programs takes a little bit of time to run, but after 20 epochs, the testing accuracy is almost 97 percent. The following screenshot shows the actual labels and the predicted labels. These are the predictions the network made:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/affdeb3b-3f28-4b4b-a801-43210ca51816.png" style="width:25.50em;height:20.00em;"/></p>
<p>So we have built our first DNN model and we were able to classify handwritten digits using this program with almost 97 percent accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regression with Deep Neural Networks (DNN)</h1>
                </header>
            
            <article>
                
<p>For regression with DNNs, we first have to import the libraries we will use here. We will import TensorFlow, pandas, NumPy, and matplotlib with the lines of code shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-626 image-border" src="assets/d2622fc4-ef72-4d49-ac2e-025f5c814142.png" style="width:27.25em;height:9.58em;"/></p>
<p>We will use the <kbd>fully_ connected</kbd> function from the <kbd>tensorflow.contrib.layers</kbd> model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elements of the DNN model</h1>
                </header>
            
            <article>
                
<p>Before running the model, we first have to determine the elements that we will use in building a multilayer perceptron model, shown as follows:</p>
<ul>
<li><strong>Architecture:</strong> The model contains 23 elements in the input layer, hence we have 25 features in this dataset. We have only one element in the output layer and we will use three hidden layers, although we could use any number of hidden layers. We will use 256 neurons for the first layer, 128 for the second, and 64 for the third one. These are the powers of two.</li>
<li><strong>Activation function:</strong> We will choose the ReLu activation function.</li>
<li><strong>Optimizing a</strong><strong>lgorithm</strong>: The optimization algorithm used here is the Adam optimizer. The Adam optimizer is one of the most popular optimizers as it is the best option for a lot of problems.</li>
<li><strong>Loss function</strong>: We will use the mean squared error because we are doing a regression problem here and this is one of the optimal choices for the <kbd>loss</kbd> function.</li>
<li><strong>Weights initialization strategy:</strong> For this, we will use the Xavier initializer, which comes as the default that with the <kbd>fully_connected</kbd> function from TensorFlow.</li>
<li><strong>Regularization strategy</strong>: We are not going to use any regularization strategy.</li>
<li><strong>Training strategy</strong>: We are going to use 40 epochs. We will present the dataset 40 times to the network and, in every iteration, we will use batches of 50 data points each time we run the training operation. So, we will use 50 elements of the dataset.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the DNN</h1>
                </header>
            
            <article>
                
<p>First, we import the dataset that we will use. The reason behind using this dataset is that, it is easily available. The following are the steps involved in building a DNN model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading the data</h1>
                </header>
            
            <article>
                
<p>We are going to read data in the cell and filter it to our preference. The following screenshot shows the lines of code used to read the data:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/60d20624-6098-4ad9-8d91-a1b138c925b6.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Objects for modeling</h1>
                </header>
            
            <article>
                
<p>After importing the datasets, we prepare the objects for modeling. So we have training and testing here for <kbd>x</kbd> and for <kbd>y</kbd>. The following screenshot shows the lines of code used to prepare the objects for modelling:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e9278a94-5327-4ee6-bd04-3a925170420e.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training strategy</h1>
                </header>
            
            <article>
                
<p>This is the training strategy with 40 epochs and a batch size of 50. This is created with the following lines of code:</p>
<pre>n_epochs = 40<br/>batch_size = 50</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Input pipeline for the DNN</h1>
                </header>
            
            <article>
                
<p>Since this is an external dataset, we have to use a data input pipeline, and TensorFlow provides different tools for getting data inside the deep learning model. Here, we create a dataset object and an iterator object with the lines of code shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/edc095ad-d63a-4b69-87c3-3d262b2e7345.png"/></p>
<p>First, we produce the dataset object. Then, we pass the whole training dataset to some placeholders that we will use. Then, we shuffle the data and divide or partition the training dataset into batches of 50. Hence, the dataset object is prepared, containing all of the training samples partitioned into batches of size 50. Next, we make an iterator object. Then, with the <kbd>get_next</kbd> method, we create a node called <kbd>next_element</kbd>, which provides the batches of 50 from the training examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the architecture</h1>
                </header>
            
            <article>
                
<p>We use three hidden layers with 256 neurons for the first layer, 128 for the second, and 64 for the third one. The following code snippet shows the architecture for this procedure:</p>
<pre>n_inputs = X_train.shape[1] #23<br/>n_hidden1 = 256<br/>n_hidden2 = 128<br/>n_hidden3 = 64<br/>n_outputs = 1</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Placeholders for input values and labels</h1>
                </header>
            
            <article>
                
<p>The values of different layers are the objects, also called the placeholders, for inputs and labels. These placeholders are used for feeding the data into the network. The following lines of code shows the placeholders for inputs and labels:</p>
<pre>X = tf.placeholder(X_train.dtype, shape=[None,n_inputs])<br/>y = tf.placeholder(y_train.dtype)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the DNN</h1>
                </header>
            
            <article>
                
<p>For building the following example, we first have to define the <kbd>DNN</kbd> function. This function will take <kbd>X_values</kbd> and output the predictions. For the first hidden layer, we use a <kbd>fully_ connected</kbd> function. The input for this hidden layer will be <kbd>X</kbd>, which is the data that comes from the placeholder, and <kbd>n_hidden1</kbd> is the number of neurons that we have in this hidden layer. Remember we have 350 neurons in the first hidden layer. Now, the first hidden layer becomes the input for the second hidden layer, and <kbd>n_hidden2</kbd> is the number of neurons that we use in this second hidden layer. Likewise, this second hidden layer becomes the input for the third hidden layer and we use this number of neurons in this layer. Finally, we have the output layer, let's call it <kbd>y_pred</kbd>, and this is a fully connected layer, with<span> the third hidden layer </span>as input. This is one output and this layer has no activation function. The following screenshot shows the lines of code used for building the neural network:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e509c688-e82a-44e0-a99b-1abce7126c60.png" style="width:40.08em;height:10.08em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The loss function</h1>
                </header>
            
            <article>
                
<p>We will use the <kbd>mean_squared _error</kbd> function—TensorFlow provides us with many such functions. We pass the observed values and the predicted values and this function calculates the mean squared error. The following screenshot shows the lines of code used for showing the <kbd>mean_squared _error</kbd> function:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cedfa97e-88ee-4564-9297-6299ce5e7652.png" style="width:37.50em;height:3.50em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining optimizer and training operations</h1>
                </header>
            
            <article>
                
<p>The goal of the optimizer is to minimize the loss and it does this by adjusting the different weights that we have in all of the layers of our network. The optimizer used here is the Adam optimizer with a learning rate of 0.001.</p>
<p>The following screenshot shows the lines of code used for defining the optimizer and also shows the training operations:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d34af37c-db85-4d8d-82ec-1274c6805ea2.png" style="width:24.08em;height:3.50em;"/></p>
<p>The following screenshot shows some of the NumPy arrays that we created and will use for evaluation purposes:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/88fcf3a8-6cf2-4475-b25e-d7ee0db78a4b.png" style="width:21.25em;height:3.83em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the computational graph</h1>
                </header>
            
            <article>
                
<p>For actually running the computational graph, first we will initialize all of the variables in our program. The following screenshot shows the lines of code used for running the computational graph:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ea17818a-74ea-4811-ad41-4040102b4673.png"/></p>
<p>The variables are the weights that are implicit in the <kbd>fully_connected</kbd> function. Then, for every epoch, we initialize the iterator object and pass the training dataset. Here, we have <kbd>batch_data</kbd>, we run this <kbd>next_ element</kbd> node, and we get batches of 50. We can get the feature values and the labels, we can get the labels, and then we can run the training operation. When the object runs out of data, we get an error. In this case, when we get one of these errors, it means that we have used all of the training datasets. We then break from this <kbd>while</kbd> loop and proceed to the next epoch. Later, we produce some individual predictions so you can take a look at concrete predictions that this neural network makes.</p>
<p>The following screenshot shows the behavior of the training and the testing MSE of all 40 epochs as we present the data to this network:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e9121538-21c3-43f6-8fe0-59ecaa8da16e.png" style="width:44.17em;height:40.33em;"/></p>
<p>In the last tested MSE (epoch 40) we get the final value of the training and the testing MSE.</p>
<p>We get the actual predictions from the network and the values are relatively close. Here, we can see the predicted prices. For cheap diamonds, the network produced values that are relatively close. For very expensive diamonds, the network produced high values. Also, the predicted values are pretty close to the observed values. The following screenshot shows the actual and the predicted values that we got from the network:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c37daf44-699d-4ec7-a142-2c8cfb057b90.png" style="width:37.92em;height:34.33em;"/></p>
<p>The following screenshot shows the graph of the training MSE with the testing MSE and the lines of code used to produce it:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d41fa0ad-74c9-4232-b9e7-e8788788c194.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification with DNNs</h1>
                </header>
            
            <article>
                
<p>For understanding classification with DNNs, we first have to understand the concept of exponential linear unit function and the elements of the model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exponential linear unit activation function</h1>
                </header>
            
            <article>
                
<p>The <strong>Exponential Linear Unit</strong> (<strong>ELU</strong>) function is a relatively recent modification to the ReLU function. It looks very similar to the ReLU function, but it has very different mathematical properties. The following screenshot shows the ELU function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-627 image-border" src="assets/1e4604f6-28a4-4da0-9faa-de706a4da045.png" style="width:42.25em;height:48.00em;"/></p>
<p>The preceding screenshot shows that, at <kbd>0</kbd>, we don't have a corner. In the case of the ReLU function, we have a corner. In this function, instead of a single value going to <kbd>0</kbd>, we have the ELU function slowly going to the negative alpha p<span>arameter</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification with DNNs</h1>
                </header>
            
            <article>
                
<p>For classification with DNNs, we first have to import the libraries that we will use. Use the lines of code in the <span>following screenshot to </span>import the <kbd>tensorflow</kbd>, <kbd>pandas</kbd>, <kbd>numpy</kbd>, and <kbd>matplotlib</kbd> libraries:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e70a38d6-3326-48a1-a4e2-2a8156ff1188.png" style="width:42.17em;height:14.33em;"/></p>
<p>We will also import the <kbd>train_test_split</kbd> function from <kbd>sklearn.model_selection</kbd>, <kbd>RobustScaler</kbd> from <kbd>sklearn.preprocessiong</kbd> , and <kbd>precision_score</kbd>, <kbd>recall_score</kbd>, and <kbd>accuracy_score</kbd> from <kbd>sklearn.metrics</kbd>. We also import the <kbd>fully_connected</kbd> function from <kbd>tensorflow.contrib.layers</kbd> to build the layers of our network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Elements of the DNN model</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">Before running the model, we first have to determine the elements that we will use in<br/>
building a multilayer perceptron model, shown as follows:</span></p>
<ul>
<li><strong>Architecture</strong>: <span class="fontstyle0">The model contains 25 elements in the input layer because we have<br/>
25 features in the dataset. We have two elements in the output layer and we will<br/>
also use three hidden layers, although we could use any number of hidden<br/>
layers. We will use the same number of neurons in each layer, 200. Here we use<br/>
the powers of 2, which is an arbitrary choice.</span></li>
</ul>
<ul>
<li><strong>Activation function</strong>: <span class="fontstyle0">We will choose the ELU activation function, which was<br/>
explained in the preceding chapter.</span></li>
<li><strong>Optimizing a</strong><strong>lgorithm</strong>: <span class="fontstyle0">The optimization algorithm used here is the Adam<br/>
optimizer with a learning rate of 0.001.</span></li>
<li><strong>Loss function</strong>: <span class="fontstyle0">For the <kbd>loss</kbd> function, we will use the cross-entropy function.</span></li>
<li><strong>Weights initialization strategy</strong>: <span class="fontstyle0">For this, we will use the Xavier initializer, a<br/>
method that comes as default with the</span> <kbd><span class="fontstyle2">fully_connected</span></kbd> <span class="fontstyle0">function from<br/>
TensorFlow.</span></li>
<li><strong>Regularization strategy</strong>: <span class="fontstyle0">We are not going to use any regularization strategy.</span></li>
<li><strong>Training strategy</strong>: <span class="fontstyle0">We are going to use 40 epochs. So, we will present the dataset<br/>
40 times to the network, and in every iteration, we will use a batch size of 100.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the DNN</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">Now, we import the dataset that we will use. The reason behind using this dataset is that it<br/>
is easily available. The following are the steps involved in building a DNN model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading the data</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">We are going to read the data in the cell. The following screenshot shows the lines of code used<br/>
to read the data:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dfee505c-4e6a-46e0-aafc-a1a73825fa69.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Producing the objects for modeling</h1>
                </header>
            
            <article>
                
<p>Now, we produce the objects used for modeling. We are going to use 10 percent for testing<br/>
and 90 percent for training. The following screenshot shows the lines of code used for<br/>
producing the objects for modeling:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/57d58fe3-2781-4cb0-b875-cbf1ef50ebcb.png"/><strong><br/></strong></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training strategy</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">This is the training strategy that we previously mentioned, 40 epochs and a batch size of<br/>
100. The following code block shows the parameters we set in this strategy</span>:</p>
<pre>n_epochs = 40<br/>batch_size = 100</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Input pipeline for DNN</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">Now, we perform the same thing that we did with the regression example. We create a<br/>
<kbd>dataset</kbd> object and an iterator object. In the end, we have</span> <kbd><span class="fontstyle2">next_element</span></kbd><span class="fontstyle0">. This will be a node in our computational graph that will give us 100 data points each time. Hence, we get<br/>
the batches. The following screenshot shows the lines of code used for producing an input<br/>
pipeline for the DNN:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b2c44778-8a38-4d4a-ae49-bac788b96ee4.png" style="width:55.92em;height:11.67em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the architecture</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">We will use three hidden layers and 200 neurons for all three. The following code snippet<br/>
shows the architecture we will use in this example:</span></p>
<pre>n_inputs = X_train.shape[1] #25<br/>n_hidden1 = 200<br/>n_hidden2 = 200<br/>n_hidden3 = 200<br/>n_outputs = 2</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Placeholders for inputs and labels</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">The values of different layers are the objects, also called the placeholders, for inputs and<br/>
labels. These placeholders are used for feeding the data into the network. The following<br/>
lines of code are used for showing placeholders for inputs and labels:</span></p>
<pre>X = tf.placeholder(X_train.dtype, shape=[None,n_inputs])<br/>y = tf.placeholder(y_train.dtype)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the neural network</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">For building deep neural networks, we will use the</span> <kbd><span class="fontstyle2">DNN</span></kbd> <span class="fontstyle0">function. We have three layers and we will use the ELU function as the activation function. You can get this function from<br/>
TensorFlow,</span> <kbd><span class="fontstyle2">tf.nn.elu</span></kbd><span class="fontstyle0">, from which you can get a lot of functions that will help you build your deep learning models. The following screenshot shows the lines of code used for<br/>
producing this function and for getting the output in the form of</span> <kbd><span class="fontstyle2">logits</span></kbd><span class="fontstyle0">:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e5e00979-4492-4f24-9bbd-666d6c417cc6.png" style="width:50.83em;height:9.08em;"/></p>
<p><span class="fontstyle0">The final layer is called the</span> <kbd>logits</kbd> laye<span class="fontstyle0">r. We won't be using any activation function in<br/>
this layer.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The loss function</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">For the <kbd>loss</kbd> function, again, we are going to get</span> <kbd><span class="fontstyle2">logits</span></kbd> <span class="fontstyle0">from the DNN and then pass this</span> <kbd><span class="fontstyle2">logits</span></kbd> t<span class="fontstyle0">o the</span> <kbd><span class="fontstyle2">softmax_cross_entropy_with_logits</span></kbd><span class="fontstyle0">function from TensorFlow. We pass the true labels and</span> <kbd><span class="fontstyle2">logits</span></kbd><span class="fontstyle0">, and then we can get the loss by using the</span> <span class="fontstyle2"><kbd>reduce_mean</kbd> </span><span class="fontstyle0">function with</span> <kbd><span class="fontstyle2">cross_entropy</span></kbd><span class="fontstyle0">. The following screenshot shows the lines of code used for showing the use of the</span> <kbd><span class="fontstyle2">reduce_mean</span></kbd><span class="fontstyle0">function with </span><kbd><span class="fontstyle2">cross_entropy</span></kbd> <span class="fontstyle0">for getting the loss:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2c3d959e-568f-4e50-bf45-e28bcaf89b8c.png" style="width:53.00em;height:6.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation nodes</h1>
                </header>
            
            <article>
                
<p>Now, for evaluation, we will calculate the probabilities of default and non-default variables; you can get the probabilities by applying a <kbd>softmax</kbd> function to <kbd>logits</kbd>. The following screenshot shows the <kbd>softmax</kbd> function:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c08b33df-ddba-4203-8a20-c7c8d56f40cc.png" style="width:46.08em;height:8.83em;"/></p>
<p><span class="fontstyle0">The <kbd>softmax</kbd> function is used for providing the probabilities for the different categories.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizer and the training operation</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">The goal of the optimizer is to minimize loss, and it does this by adjusting the different weights that we have in all of the layers of our network.</span></p>
<p class="mce-root"/>
<p><span class="fontstyle0">The following screenshot shows the lines of code used for defining the optimizer and shows the training operations:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/31bcbcf6-c2aa-4cd5-a759-3cf5b0a68f49.png" style="width:38.17em;height:5.00em;"/></p>
<p><span class="fontstyle0">In this case, the optimizer is, again, the Adam optimizer with a learning rate of <kbd>0.001</kbd>. The training operation is the operation in which the optimizer minimizes the loss.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Run the computational graph</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">To actually run the computational graph, first we initialize all of the variables in our<br/>
program. The variables are the weights that are implicit in the</span> <kbd><span class="fontstyle2">fully_connected</span></kbd> <span class="fontstyle0">function. We run four epochs and for each epoch, we initialize our iterator object. We pass training <kbd>x</kbd> and training <kbd>y</kbd>, and then we run this loop. This loop will run as long as we have data in</span> <kbd>next_elementelement</kbd><span class="fontstyle0">. So, we get the next 100 elements and then, in the next iteration, the next 100 elements, and so on. In every iteration, we run the training operation. Now, what this training operation does is ask the optimizer to adjust the parameters and the weights, a little bit in order to make better predictions.</span></p>
<p><span class="fontstyle0">The following screenshot shows the lines of code used for running the computational graph:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3d0faf9a-3035-4af5-9cd1-a133bc2502cc.png"/></p>
<p><span class="fontstyle0">In the end, we can get the probabilities and we can use these for evaluation purposes.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model with a set threshold</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">The <kbd>probabilities</kbd> object is produced to actually evaluate the model performance with<br/>
different classification thresholds. The classification threshold can be modified for a binary<br/>
classification problem and can be used for calculating the recall score, the precision, and the<br/>
accuracy. On using a classification threshold of <kbd>0.16</kbd>, these are the metrics that we get in the testing dataset:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f23f3ebc-019f-4be1-a50d-dd44eaec977b.png"/></p>
<p class="mce-root"><span class="fontstyle0">On calculating, we get a recall score of <kbd>82.53</kbd> percent, precision of <kbd>34.02</kbd> percent, and an accuracy of <kbd>60.7</kbd> percent.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned how to make predictions using TensorFlow. We studied the MNIST dataset and classification of models using this dataset. We came across the elements of DNN models and the process of building the DNN. Later, we progressed to study regression and classification with DNNs. We classified handwritten digits and learned more about building models in TensorFlow. This brings us to the end of this book! We learned how to use ensemble algorithms to produce accurate predictions. We applied various techniques to combine and build better models. We learned how to perform cross-validation efficiently. We also implemented various techniques to solve current issues in the domain of predictive analysis. And, the best part, we used the DNN models we built to solve classification and regression problems. This book has helped us implement various machine learning techniques to build advanced predictive models and apply them in the real world.</p>


            </article>

            
        </section>
    </body></html>