<html><head></head><body>
		<div id="_idContainer010">
			<h1 id="_idParaDest-16" class="chapter-number"><a id="_idTextAnchor015"/>1</h1>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Introduction to Large Language Models</h1>
			<p>In the world of technology, <strong class="bold">Chat Generative Pre-trained Transformer</strong> (<strong class="bold">ChatGPT</strong>) is a <strong class="bold">large language model</strong> (<strong class="bold">LLM</strong>)-based <a id="_idIndexMarker000"/>chatbot<a id="_idIndexMarker001"/> that was launched by OpenAI on November 30, 2022. Just in ChatGPT’s first week, over a million people started using the technology. This is an important moment because it shows how regular people are now using <a id="_idIndexMarker002"/>generative <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) in their daily lives. By January 2023, ChatGPT had over 100 million users, making it the fastest-growing application in history and making OpenAI, the company behind it, worth $<span class="No-Break">29 billion.</span></p>
			<p>In this introductory chapter, we’ll establish the basic concepts behind LLMs, look at some examples, understand the concept of foundation models, and provide various business use cases where LLMs can be applied to solve <span class="No-Break">complex problems.</span></p>
			<p>This chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>What <span class="No-Break">are LLMs?</span></li>
				<li><span class="No-Break">LLM examples</span></li>
				<li>The concept of <span class="No-Break">foundation models</span></li>
				<li>LLM <span class="No-Break">use cases</span></li>
			</ul>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>What are LLMs?</h1>
			<p>LLMs are a modern <a id="_idIndexMarker003"/>breakthrough in deep learning that focuses on human languages. They’ve shown themselves to be useful in many ways, such as content creation, customer support, coding assistance, education and tutoring, medical diagnosis, sentiment analysis, legal assistance, and more. Simply put, an LLM is a kind of smart computer program that can understand and create text like humans can by using large transformer models under the hood. The Transformer architecture enables models to understand context and relationships within data more effectively, making it particularly powerful for tasks involving human language and <span class="No-Break">sequential data.</span></p>
			<p>For humans, text is a bunch of words put together. We read sentences, sentences make up paragraphs, and paragraphs make up chapters in a document. But for computers, text is just a series of letters and symbols. To make computers understand text, we can create a model using something called recurrent neural networks. This model goes through the text one word or character at a time and gives an answer when it finishes reading everything. This model is good, but sometimes, when it gets to the end of a block of text, it has trouble recalling the text from the beginning of that block. This is where the Transformer architecture shines. The key innovation of the Transformer architecture was its use of the self-attention mechanism, which allowed it to capture relationships between different parts of a sequence more effectively than <span class="No-Break">previous models.</span></p>
			<p>Back in 2017, Ashish Vaswani and their team wrote a paper called <em class="italic">Attention is All You Need</em> (<a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a>) to <a id="_idIndexMarker004"/>introduce a new model called the Transformer. This model uses something called attention. Unlike the old way in which recurrent neural networks process text, attention lets you look at a whole sentence or even a whole paragraph all at once, instead of just one word at a time. This helps the transformer to better “understand” words as a result of the added context. Nowadays, many of the best LLMs are <a id="_idIndexMarker005"/>built <span class="No-Break">on transformers.</span></p>
			<p>When you want a Transformer model to understand a piece of text, you must break it down into separate words or parts called tokens. These tokens are then turned into numbers and mapped to special codes called embeddings, which are like special maps that store the semantic meaning of the tokens. Finally, the transformer’s encoder takes these embeddings and turns them into a representation. This “representation” is a vector that captures the contextual meaning of the input tokens, allowing the model to understand and process the input more effectively. In simple terms, you can think of it as putting all the pieces together to understand the <span class="No-Break">whole story.</span></p>
			<p>Here’s an example of a text string, its tokenization, and its vector embedding. Note that tokenization can turn words into subwords. For example, the word “generative” can be tokenized into “gener” <span class="No-Break">and “ative.”</span></p>
			<p>Let’s look at the <span class="No-Break">input text:</span></p>
			<pre class="source-code">
Generative AI, a groundbreaking technology fueled by intricate algorithms and machine learning, possesses the remarkable ability to independently craft content across diverse domains. By meticulously analyzing vast datasets and discerning intricate patterns, it generates textual compositions, artistic creations, and a myriad of other outputs that mirror human ingenuity. This innovative capability is reshaping industries far and wide, driving unprecedented advancements in fields such as language generation, creative arts, and data synthesis.</pre>			<p>Here’s the <span class="No-Break">tokenized text:</span></p>
			<pre class="source-code">
['Gener', 'ative', ' AI', ',', ' a', ' groundbreaking', ' technology', ' fueled', ' by', ' intricate', ' algorithms', ' and', ' machine', ' learning', ',', ' possesses', ' the', ' remarkable', ' ability', ' to', ' independently', ' craft', ' content', ' across', ' diverse', ' domains', '.', ' By', ' meticulously', ' analyzing', ' vast', ' datasets', ' and', ' discern', 'ing', ' intricate', ' patterns', ',', ' it', ' generates', ' textual', ' compositions', ',', ' artistic', ' creations', ',', ' and', ' a', ' myriad', ' of', ' other', ' outputs', ' that', ' mirror', ' human', ' ing', 'enuity', '.', ' This', ' innovative', ' capability', ' is', ' resh', 'aping', ' industries', ' far', ' and', ' wide', ',', ' driving', ' unprecedented', ' advancements', ' in', ' fields', ' such', ' as', ' language', ' generation', ',', ' creative', ' arts', ',', ' and', ' data', ' synthesis', '.']</pre>			<p>Now, let’s look at<a id="_idIndexMarker006"/> <span class="No-Break">the embeddings:</span></p>
			<pre class="source-code">
[-0.02477909065783024, -0.013280253857374191, 0.014264720492064953,0.002092828741297126, 0.008900381624698639, 0.017131058499217033, 0.04224500060081482, 0.012088178656995296, -0.028958052396774292, 0.04128062725067139, 0.020171519368886948, 0.034369271248579025, 0.005337550304830074, -0.011920752003788948, 0.0027072832453995943,0.008103433065116405, 0.035440798848867416, 0.015430007129907608,
…
…
-0.02812761813402176, -0.009549995884299278, 0.02203330025076866, 0.015215701423585415, 0.02339949831366539, -0.008967352099716663, 0.01867138035595417, -0.01762663945555687, 0.01278467196971178, 0.029922427609562874, -0.0002689284738153219, -0.010213003493845463]</pre>			<p>Think of the context vector as the heart of the input information. It enables the transformer’s decoder to determine what to say next. For example, by providing the decoder with a starting sentence as a hint, it can suggest the next word that makes sense. This process repeats, with each new suggestion becoming part of the hint, allowing the decoder to generate a naturally flowing paragraph from an <span class="No-Break">initial sentence.</span></p>
			<p>Decoder-based content generation is like a game, where each move is based on the previous one, and you end up with a complete story. This method of content generation is called “autoregressive generation.” Broadly speaking, this is how LLMs work. Autoregressive generation-based models can handle long input texts while also maintaining a context vector big enough to deal with complicated ideas. In addition to this, it has many layers in its decoder, making it highly sophisticated. It’s so big that it typically can’t run on just one computer and instead must run on a cluster of nodes working together that <a id="_idIndexMarker007"/>are accessed often. That’s why it’s offered as a service<a id="_idIndexMarker008"/> through an <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>). As you might have guessed, this enormous model is trained using a massive amount of text until it understands how language works, including all the patterns and structures of sentences. Now, let’s understand the main structure <span class="No-Break">of LLMs.</span></p>
			<p>An LLM’s structure (see <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.1</em>) is mainly made up of different layers of neural networks, such as recurrent layers, feedforward layers, embedding layers, and attention layers. These layers collaborate to handle input text and make predictions about the output. Let’s take a <span class="No-Break">closer look:</span></p>
			<ul>
				<li><strong class="bold">The embedding layer</strong> changes <a id="_idIndexMarker009"/>each word in the input text into a special kind of detailed description, kind of like a unique fingerprint. These descriptions hold crucial details about the words and their meanings, which helps the model understand the <span class="No-Break">bigger picture.</span></li>
				<li><strong class="bold">The feedforward layers</strong> in LLMs consist of many connected layers that process the detailed descriptions created in the embedding layer. These layers perform complex transformations on these embeddings, which helps the model understand the more important ideas in the <span class="No-Break">input text.</span></li>
				<li><strong class="bold">The recurrent layers</strong> in LLMs are designed to read the input text one step at a time. These layers have hidden memory that gets updated as they read each part of the text. This helps the model remember how the words are related to each other in <span class="No-Break">a sentence.</span></li>
				<li><strong class="bold">The attention mechanism</strong> is another important part of LLMs. It’s like a spotlight where the model shines on different parts of the input text. This helps the model focus on the<a id="_idIndexMarker010"/> most important parts of the text and make <span class="No-Break">better predictions.</span><p class="list-inset">For example, when <a id="_idIndexMarker011"/>you read, you don’t pay equal attention to every word; instead, you focus more on keywords and important phrases to grasp the main idea. For instance, in the sentence “The cat sat on the mat,” you might emphasize “cat” and “mat” to understand what’s happening. Additionally, you use context from previous sentences to make sense of the current one – if you read about a cat playing earlier, you understand why the cat is now sitting on the mat. As you continue reading, you adjust your focus based on what’s important for comprehension, revisiting or paying more attention to crucial sections that help you understand the <span class="No-Break">overall plot.</span></p><p class="list-inset">In essence, just as humans read text by focusing on important words and using context to understand meaning, the attention mechanism in transformers focuses on key parts of the input and adjusts dynamically to capture the context and relationships <span class="No-Break">between words:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="image/B21019_01_1.jpg" alt="Figure 1.1: Transformer architecture (source: https://arxiv.org/pdf/1706.03762.pdf)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1: Transformer architecture (source: <a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a>)</p>
			<p>Now that we’ve learned <a id="_idIndexMarker012"/>the basic concepts behind LLMs, let’s focus on some of the top <span class="No-Break">industry examples.</span></p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>LLM examples</h1>
			<p>Cutting-edge LLMs<a id="_idIndexMarker013"/> have been developed by many companies, including OpenAI (GPT-4), Meta (Llama 3.1), Anthropic (Claude), and Google (Gemini), to name a few. OpenAI has consistently maintained a dominant role in the field of LLMs. Let’s look at the top models that are used at the time <span class="No-Break">of writing:</span></p>
			<ul>
				<li><strong class="bold">Generative Pre-trained Transformer (GPT)</strong>: OpenAI has created various GPT models, including<a id="_idIndexMarker014"/> GPT1 (117 million<a id="_idIndexMarker015"/> parameters), GPT2 (1.5 billion parameters), GPT-3 (175 billion parameters), GPT 3.5, GPT4-Turbo, GPT4-o, and GPT4-o mini. GPT4-o is one of the most advanced LLMs globally. These models learn from a huge amount of text and can provide human-like answers to many subjects and questions. They also remember various parts <span class="No-Break">of conversations.</span></li>
				<li><strong class="bold">Anthropic</strong>: Anthropic’s Claude<a id="_idIndexMarker016"/> models are a family of advanced LLMs that are designed to handle complex tasks with high efficiency. The latest iteration, Claude 3, includes models such as Opus, Sonnet, and Haiku, each tailored for different performance needs. Opus is the most powerful, excelling in complex analysis and higher-order tasks, while Sonnet balances speed and intelligence, and Haiku offers the fastest response times for lightweight actions. These models are built with a focus on security, reliability, and ethical <span class="No-Break">AI practices.</span></li>
				<li><strong class="bold">Llama 3.1</strong>: Llama 3.1 is a <a id="_idIndexMarker017"/>cutting-edge LLM that represents a significant milestone in AI research. With its advanced architecture and massive scale, Llama 3.1 is capable of understanding and generating human-like text with unprecedented accuracy and nuance. This powerful tool has far-reaching implications for various applications, including <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), text <a id="_idIndexMarker018"/>generation, and <a id="_idIndexMarker019"/><span class="No-Break">conversational AI.</span></li>
				<li><strong class="bold">Llama 2</strong>: Llama 2 is a <a id="_idIndexMarker020"/>second-generation LLM developed by Meta. It’s open source and can be used to create chatbots such as ChatGPT or Google Bard. Llama 2 was trained on 40% more data than Llama1 to make logical and natural-sounding responses. Llama 2 is available for anyone to use for research or business. Meta says that Llama 2 understands twice as much context as Llama 1. This makes it a smarter language model that can give answers that sound just like what a human <span class="No-Break">would provide.</span></li>
				<li><strong class="bold">Gemini</strong>: Google Gemini is <a id="_idIndexMarker021"/>a family of advanced multimodal LLMs developed by Google DeepMind. Announced on December 6, 2023, Gemini includes variants such as Gemini Ultra, Gemini Pro, Gemini Flash, and Gemini Nano. It’s designed to understand and operate across different types of information seamlessly, including text, images, audio, video, and code. Positioned as a competitor to OpenAI’s GPT-4, Gemini powers Google’s AI chatbot and aims to boost creativity <span class="No-Break">and productivity.</span></li>
				<li><strong class="bold">PaLM 2</strong>: Finally, PaLM 2 is <a id="_idIndexMarker022"/>Google’s updated LLM. It’s skilled at handling complex tasks such as working with code and math, categorizing and answering questions, translating languages, being proficient in multiple languages, and creating human-like sentences. It outperforms the previously mentioned models, including the original PaLM. Google is careful about how it creates and uses AI, and PaLM 2 is a part of this approach. It went through thorough evaluations so that it <a id="_idIndexMarker023"/>could be checked for potential problems and biases. PaLM 2 is not just used in isolation but is also used in other advanced models, such as Med-PaLM 2 and Sec-PaLM. It’s also responsible for powering AI features and tools at Google, such as Bard and the <span class="No-Break">PaLM API.</span></li>
			</ul>
			<p>The evolutionary tree (see <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em>) of modern LLMs illustrates how these models have evolved in recent years and highlights some of the most famous ones. Models that are closely related are <a id="_idIndexMarker024"/>shown on the same branches. Models that use the Transformer architecture are shown in different colors: those that only decode are on the blue branch, ones that only encode are on the pink branch, and models that do both encoding and decoding are on the green branch. The position of the models on the timeline shows when they were released. Open source models are represented by filled squares, while models that are not open source are represented by empty squares. The bar chart at the bottom right displays the number of models from different companies <span class="No-Break">and organizations:</span></p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="image/B21019_01_2.jpg" alt="Figure 1.2: The evolutionary tree of modern LLMs (source: https://arxiv.org/abs/2304.13712)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2: The evolutionary tree of modern LLMs (source: <a href="https://arxiv.org/abs/2304.13712">https://arxiv.org/abs/2304.13712</a>)</p>
			<p>Having explored <a id="_idIndexMarker025"/>some exemplary LLM instances, let’s discuss the concept of foundation models and their advantages <span class="No-Break">and disadvantages.</span></p>
			<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/>The concept of foundation models</h1>
			<p>In recent years, there <a id="_idIndexMarker026"/>has been a huge buzz around LLMs such as ChatGPT sweeping across the world. LLMs are a subset of a broader category of models known as foundation models. Interestingly, the term “foundation models” was initially introduced by a team from Stanford. They observed a shift in the AI landscape, leading to the emergence of a <span class="No-Break">new paradigm.</span></p>
			<p>In the past, AI applications were constructed by training individual AI models, each tailored to a specific task using specialized data. This approach often involved assembling a library of various AI models in a mostly supervised training manner. Its foundational capability, known as a foundation model (see <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.3</em>), would become the driving force behind various applications and use cases. Essentially, this single model could cater to the very same applications that were once powered by distinct AI models in the traditional approach. This meant that a single model could fuel an array of diverse applications. The key here is that this model possesses the incredible ability to adapt to a multitude of tasks. What empowers this model to achieve such versatility is the fact that it has undergone extensive training on an immense volume of unstructured data through an <span class="No-Break">unsupervised approach:</span></p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/B21019_01_3.jpg" alt="Figure 1.3: Foundational model (source: https://arxiv.org/pdf/2108.07258.pdf)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3: Foundational model (source: <a href="https://arxiv.org/pdf/2108.07258.pdf">https://arxiv.org/pdf/2108.07258.pdf</a>)</p>
			<p>Consider a scenario where I start a sentence with “Don’t count your chickens before they’re.” Now, my goal is to guide the model in predicting the last word, which could be “hatched,” “grown,” or even “gone.” This <a id="_idIndexMarker027"/>process involves training the model to anticipate the appropriate word by analyzing the context provided by the words that come before it in the sentence. The impressive ability to generate predictions for the next word, while drawing on the context of preceding words it has encountered, positions foundation models within the realm of generative AI. In essence, these models fall under the category of generative AI because they’re capable of crafting something novel – in this case, predicting the upcoming word in <span class="No-Break">a sentence.</span></p>
			<p>Although these models are primarily designed to generate predictions, particularly anticipating the next word in a sentence, they offer immense capabilities. With the addition of a modest amount of labeled data, we can adjust these models to perform exceptionally well on more traditional NLP tasks. These tasks include activities such as classification or named-entity recognition, which are typically not associated with generative capabilities. This transformation is achieved through a process known as fine-tuning. When you fine-tune your foundation model with a modest dataset, you adjust its parameters so that it can excel at a specific natural language task. This way, the model evolves from being primarily generative to being a powerful tool for targeted <span class="No-Break">NLP tasks.</span></p>
			<p>Even with limited data, foundation models can prove highly effective, especially in domains where data is scarce. Through a process known as prompting, or prompt engineering, techniques such as in-context learning, zero-shot, one-shot, and few-shot learning can be used to tackle complex downstream tasks. Let’s break down how you could prompt a model for a classification task. Imagine that you provide the model with a sentence and follow it up with the question, “Does this sentence carry a positive or negative sentiment?” The model would then work its magic, completing the sentence with generated words. The very next word it generates would serve as the answer to your classification question. Depending on where it perceives the sentiment of the sentence to lie, the model would respond with either “positive” or “negative.” This method leverages the model’s inherent ability to generate contextually relevant text to solve a specific classification challenge. We’ll cover different prompting techniques and advanced prompt <a id="_idIndexMarker028"/>engineering later in <span class="No-Break">this book.</span></p>
			<p>Let’s talk about some of the key advantages of <span class="No-Break">foundation models:</span></p>
			<ul>
				<li><strong class="bold">Performance</strong>: These models <a id="_idIndexMarker029"/>have been trained on an immense amount of content with data volumes regularly in the terabyte range. When employed for smaller tasks, these models exhibit remarkable performance that far surpasses models trained on only a handful of <span class="No-Break">data points.</span></li>
				<li><strong class="bold">Productivity gain</strong>: LLMs can boost productivity in a big way. They’re like a super-efficient human for tasks that usually take a lot of time and effort. For instance, in customer service, LLMs can quickly answer common questions, freeing up human workers to handle more complex issues. In businesses, they can process and organize data way faster than people can. Using LLMs, companies can save time and money. This lets them focus on important tasks and thus acts like a turbocharger <span class="No-Break">for productivity.</span></li>
			</ul>
			<p>However, these foundation models also have key challenges <span class="No-Break">and limitations:</span></p>
			<ul>
				<li><strong class="bold">Cost</strong>: These models <a id="_idIndexMarker030"/>tend to be quite costly to train because of the huge data volumes needed. This often poses challenges for smaller businesses attempting to train their foundation models. Additionally, as these models grow in size, reaching a scale of several billion parameters, they can become pricey to use <span class="No-Break">for inference.</span><p class="list-inset">Cloud providers such as Microsoft offer a service<a id="_idIndexMarker031"/> called <strong class="bold">Azure OpenAI</strong>. This service lets businesses use these models on-demand and pay only for what they use. This is similar to renting a powerful computer for a short time instead of buying one outright. Leveraging this service-based capability allows companies to save money on both model training and model use, especially considering the powerful, GPU-based <span class="No-Break">hardware required.</span></p><p class="list-inset">To summarize, using services such as Azure OpenAI, businesses can take advantage of these advanced models without spending a ton on resources <span class="No-Break">and infrastructure.</span></p></li>
				<li><strong class="bold">Trustworthiness</strong>: Just as data serves as a massive advantage for these models, there’s a flip side to consider: LLMs are trained on vast amounts of internet-scraped language data that may contain biases, hate speech, or toxic content, compromising their reliability. This would be a monumental task. Furthermore, there’s the challenge of not even fully knowing what the data comprises. For many open source models, the exact datasets used for training many LLMs are unclear, making it difficult to assess their raising concerns about the models’ trustworthiness and potential biases. The sheer scale of LLM training data makes it nearly impossible for human annotators to thoroughly vet each data point, increasing the risk of unintended consequences such as perpetuating harmful biases or generating <span class="No-Break">toxic content.</span><p class="list-inset">Big organizations are fully aware of the immense possibilities that these technologies hold. To solve the foundational model trustworthiness issue, OpenAI, Microsoft, Google, and Anthropic are jointly unveiling the creation of the Frontier <a id="_idIndexMarker032"/>Model Forum (<a href="https://blogs.microsoft.com/on-the-issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-model-forum">https://blogs.microsoft.com/on-the-issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-model-forum</a>), a fresh industry initiative aimed at ensuring the safe and responsible advancement of frontier AI models. This new collaborative entity will tap into the collective technical and operational prowess of its member companies to foster progress across the broader AI landscape. One of its core objectives involves driving technical evaluations and benchmarks forward. Additionally, the forum will strive to construct a publicly accessible repository of solutions, bolstering the adoption of industry best practices and standards throughout the <span class="No-Break">AI domain.</span></p></li>
				<li><strong class="bold">Hallucination</strong>: Sometimes, LLMs can come up with information or answers that might not be entirely accurate. This is like when you have a dream that seems real, but it’s not based on what’s happening. LLMs might generate text that sounds right but isn’t completely true or accurate. So, while LLMs are highly intelligent, they can sometimes make mistakes or come up with content that <span class="No-Break">isn’t real.</span><p class="list-inset">The applications <a id="_idIndexMarker033"/>of LLMs often require human oversight to make sure the outputs are trustworthy. However, there is a promising technique<a id="_idIndexMarker034"/> called <strong class="bold">grounding the model</strong> that aims to improve this situation. Grounding means connecting the LLM’s understanding with real-world information and context. This is like making sure the model is firmly rooted in reality. Later in this book, we’ll talk more about how to use this technique to stop the model from making things up and only give answers based on the <span class="No-Break">given context.</span></p></li>
				<li><strong class="bold">Limited context window</strong>: LLMs have a limited context window or token size. A context window or token size can be seen as the amount of memory a model can process at a time. LLMs can only understand a certain number of pieces of information at once. For example, ChatGPT (GPT4-o) can handle 128K input tokens. This means that if you give it too much to read, it won’t be able to handle that and will throw errors. Therefore, it’s important to keep the input within this limit for the model to <span class="No-Break">work well.</span></li>
			</ul>
			<p>Following our <a id="_idIndexMarker035"/>understanding of the foundation model concept, let’s delve into some practical use cases <span class="No-Break">of LLMs.</span></p>
			<h1 id="_idParaDest-21"><a id="_idTextAnchor020"/>Exploring LLM use cases</h1>
			<p>LLMs have a wide range of use <a id="_idIndexMarker036"/>cases across various fields and industries due to their ability to understand and generate human-like text. Let’s take a look at some <span class="No-Break">of them:</span></p>
			<ul>
				<li><strong class="bold">Content generation</strong>: LLMs can generate written content for blogs, articles, marketing materials, and social media posts. They can be used to automate content creation and come up with <span class="No-Break">creative ideas.</span></li>
				<li><strong class="bold">Customer support</strong>: LLMs can provide automated responses to customer queries and support tickets, thus handling common questions and issues, freeing up human agents to handle more <span class="No-Break">complex cases.</span></li>
				<li><strong class="bold">Language translation</strong>: LLMs can be employed to translate text between languages, making communication easier and more accessible on a <span class="No-Break">global scale.</span></li>
				<li><strong class="bold">Text summarization</strong>: LLMs can quickly summarize lengthy texts, making it easier to grasp the main points of articles, reports, and other <span class="No-Break">written materials.</span></li>
				<li><strong class="bold">Chatbots and virtual assistants</strong>: LLMs can power chatbots and virtual assistants that engage in natural language conversations, helping users with tasks, inquiries, and <span class="No-Break">information retrieval.</span></li>
				<li><strong class="bold">Content personalization</strong>: LLMs can analyze user preferences and behavior to personalize recommendations, advertisements, and content delivery on platforms such as social media and <span class="No-Break">streaming services.</span></li>
				<li><strong class="bold">Data entry and extraction</strong>: LLMs can extract relevant information from unstructured text, such as documents or emails, and enter it into structured databases <span class="No-Break">or spreadsheets.</span></li>
				<li><strong class="bold">Creative writing</strong>: LLMs can assist writers by generating story ideas, dialogs, character descriptions, and even <span class="No-Break">entire narratives.</span></li>
				<li><strong class="bold">Healthcare chatbots</strong>: LLM-powered chatbots can answer health-related questions, provide first-aid advice, and offer information about common <span class="No-Break">medical conditions.</span></li>
				<li><strong class="bold">Medical diagnostics</strong>: LLMs can aid in diagnosing medical conditions by analyzing patient symptoms and medical records to provide potential diagnoses and <span class="No-Break">treatment options.</span></li>
				<li><strong class="bold">Mental health support</strong>: LLMs can provide empathetic responses and resources to individuals seeking support for mental <span class="No-Break">health concerns.</span></li>
				<li><strong class="bold">Tourism and travel planning</strong>: LLMs can assist travelers by suggesting itineraries, recommending attractions, and providing information about local customs <span class="No-Break">and cuisines.</span></li>
				<li><strong class="bold">Recipe creation</strong>: LLMs can devise creative recipes based on ingredients and dietary preferences, offering new <span class="No-Break">culinary experiences.</span></li>
				<li><strong class="bold">Cybersecurity analysis</strong>: LLMs can analyze cybersecurity threats and suggest strategies for protecting digital systems <span class="No-Break">and data.</span></li>
				<li><strong class="bold">Fashion recommendations</strong>: LLMs can suggest clothing and accessory combinations based on personal style preferences and current <span class="No-Break">fashion trends.</span></li>
				<li><strong class="bold">Legal document review</strong>: LLMs can review legal documents, contracts, and case histories to identify relevant information, anomalies, and <span class="No-Break">potential issues.</span></li>
				<li><strong class="bold">Academic research</strong>: LLMs can assist researchers by providing summaries of academic papers, helping <a id="_idIndexMarker037"/>with literature reviews, and generating ideas for <span class="No-Break">further study.</span></li>
				<li><strong class="bold">Financial analysis</strong>: LLMs can process and analyze financial data, generate reports, and provide insights into market trends and <span class="No-Break">investment opportunities.</span></li>
				<li><strong class="bold">Language learning</strong>: LLMs can help learners practice and improve their language skills by engaging in conversations, providing explanations, and <span class="No-Break">offering exercises.</span></li>
				<li><strong class="bold">Accessibility tools</strong>: LLMs can be used to create audio descriptions for visually impaired individuals, generate subtitles for videos, and convert text into speech for people with <span class="No-Break">reading difficulties.</span></li>
			</ul>
			<p>While these are just a few example use cases, the limitless flexibility of LLMs allows them to also be applied to even more situations as <span class="No-Break">technology progresses.</span></p>
			<p>People who want to create generative AI applications without the burden of training an LLM themselves or spending money on costly hardware can use the Azure OpenAI API. This lets them use <a id="_idIndexMarker038"/>advanced LLMs made <span class="No-Break">by OpenAI.</span></p>
			<p>So far, we’ve covered the basics of LLMs. Moving forward, the next chapter will focus on the Azure OpenAI service in <span class="No-Break">more detail.</span></p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>Summary</h1>
			<p>In this chapter, we started by introducing LLMs and how they’re influenced by Transformer networks. Then, we explored the various parts that make up LLMs. Next, we dove into some of the top LLM models created by OpenAI, Meta, and Google, discussing how these models have evolved. We also covered the concept of foundation models, including their advantages and limitations. Lastly, we looked at various business applications where LLMs have shown <span class="No-Break">great potential.</span></p>
			<p>Moving forward to the next chapter, our focus will be on Azure OpenAI service. We’ll learn how to access this service, including models such as GPT 3.5, GPT-4, Embeddings, and DALL.E 2. We’ll also explain how the pricing works, discussing options such as pay-as-you-go and <span class="No-Break">reserved capacity.</span></p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor022"/>Further reading</h1>
			<ul>
				<li><em class="italic">AI Explainer</em>: <em class="italic">Foundation models and the next era of </em><span class="No-Break"><em class="italic">AI</em></span><span class="No-Break"> (</span><a href="https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/"><span class="No-Break">https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Orca:</em> <em class="italic">Progressive Learning from Complex Explanation Traces of </em><span class="No-Break"><em class="italic">GPT-4</em></span><span class="No-Break"> (</span><a href="https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/"><span class="No-Break">https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Florence:</em> <em class="italic">A New Foundation Model for Computer </em><span class="No-Break"><em class="italic">Vision</em></span><span class="No-Break"> (</span><a href="https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/"><span class="No-Break">https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Accelerating Foundation Models </em><span class="No-Break"><em class="italic">Research</em></span><span class="No-Break"> (</span><a href="https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/"><span class="No-Break">https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Attention Is All You </em><span class="No-Break"><em class="italic">Need</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/1706.03762"><span class="No-Break">https://arxiv.org/abs/1706.03762</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">On the Opportunities and Risks of Foundation </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break"> (</span><a href="https://arxiv.org/abs/2108.07258"><span class="No-Break">https://arxiv.org/abs/2108.07258</span></a><span class="No-Break">)</span></li>
			</ul>
		</div>
	</body></html>