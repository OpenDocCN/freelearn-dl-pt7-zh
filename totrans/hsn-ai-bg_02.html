<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Machine Learning Basics</h1>
                </header>
            
            <article>
                
<p><strong>Artificial Int</strong><strong>elligence </strong>(<strong>AI</strong>) is rooted in mathematics and statistics. When creating an <strong>Artificial Neural Network</strong> (<strong>ANN</strong>), we're conducting mathematical operations on data represented in linear space; it is, <span>by nature, applied mathematics and statistics.</span> Machine learning algorithms are nothing but function approximations; they try and find a mapping between an input and a correct corresponding output. We use algebraic methods to create algorithms that learn these mappings.</p>
<p>Almost all machine learning can be expressed in a fairly straight-forward formula; bringing together a dataset and model, along with a loss function and optimization technique that are applicable to the dataset and model. This section is intended as a review of the basic mathematical tools and techniques that are essential to understanding <em><span><span>what's under the hood</span></span></em> in AI.</p>
<p>In this chapter, we'll review linear algebra and probability, and then move on to the construction of basic and fundamental machine learning algorithms and systems, before touching upon optimization techniques that can be used for all of your methods going forward. While we will utilize mathematical notation and expressions in this chapter and the following chapters, we will focus on translating each of these concepts into Python code. In general, Python is easier to read and comprehend than mathematical expressions, and allows readers to get off the ground quicker.</p>
<p>We will be covering the following topics in this chapter:</p>
<ul>
<li>Applied math basics</li>
<li>Probability theory</li>
<li>Constructing basic machine learning algorithms</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will be working in Python 3 with the scikit-learn scientific computing package. You can install the package, you can run <kbd>pip install sklearn</kbd> in your terminal or command line. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Applied math basics</h1>
                </header>
            
            <article>
                
<p>When we talk about mathematics as related to deep learning and AI, we're often talking about linear algebra. Linear algebra is a branch of continuous mathematics that involves the study of vector space and operations performed in vector space. If you remember back to grade-school algebra, algebra in general deals with unknown variables. With linear algebra, we're extending this study into linear systems that have an arbitrary number of dimensions, which is what makes this a form of continuous mathematics.</p>
<p>AI relies on the basic building block of the tensor. Within AI, these mathematical objects store information throughout ANNs that allow them to operate; they are data structures that are utilized throughout AI. As we will see, a tensor has a <strong>rank</strong>,<strong> </strong>which essentially tells us about the <strong>indices</strong> of the data (how many rows and columns the data has). </p>
<p>While many problems in deep learning are not formally linear problems<em>, </em>the basic building blocks of matrices and tensors are the primary data structures for solving, optimizing, and approximating within an ANN. </p>
<p>Want to see how linear algebra can help us from a programmatic standpoint? Take a look at the following code block: </p>
<pre class="mce-root">import numpy as np<br/>## Element-wise multiplication without utilizing linear algebra techniques<br/><br/>x = [1,2,3]<br/>y = [4,5,6]<br/><br/>product = []<br/>for i in range(len(x)):<br/>    product.append(x[i]*y[i])<br/><br/>## Element-wise multiplication utilizing linear algebra techniques<br/><br/>x = np.array([1,2,3])<br/>y = np.array([4,5,6])<br/>x * y</pre>
<p>We can eliminate strenuous loops by simply utilizing NumPy's built-in linear algebra functions. When you think of AI, and the thousands upon thousands of operations that have to be computed at the runtime of an application, the building blocks of linear algebra can also help us out programmatically. In the following sections, we'll be reviewing these fundamental concepts in both mathematical notation and Python. </p>
<div class="packt_infobox">Each of the following examples will use the Python package NumPy; <kbd>import numpy as np</kbd></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The building blocks – scalars, vectors, matrices, and tensors</h1>
                </header>
            
            <article>
                
<p>In the following section, we'll introduce the fundamental types of linear algebra objects that are used throughout AI applications; <strong>scalars</strong>, <strong>vectors</strong>, <strong>matrices</strong>, and <strong>tensors</strong>. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scalars</h1>
                </header>
            
            <article>
                
<p><strong>Scalars</strong> are nothing but singular, <strong>real numbers</strong> that can take the form of an integer or floating point. In Python, we create a scalar by simply assigning it:</p>
<pre>my_scalar = 5<br/>my_scalar = 5.098</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Vectors</h1>
                </header>
            
            <article>
                
<p><strong>Vectors</strong> are one-dimensional arrays of integers. Geometrically, they store the direction and magnitude of change from a point. We'll see how this works in machine learning algorithms when we discuss <strong>principal component analysis</strong> (<strong>PCA</strong>) in the next few pages. Vectors in Python are created as <kbd>numpy array</kbd> objects:</p>
<pre class="graf graf--pre graf-after--figure">my_vector = np.array([5,6])</pre>
<p>Vectors can be written in several ways:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/7f77e9e8-5267-45e7-9950-b6cb37c453ff.png" style="width:11.92em;height:3.00em;" width="1920" height="480"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Matrices</h1>
                </header>
            
            <article>
                
<p><strong>Matrices</strong> are two-dimensional lists of numbers that contain rows and columns. <span>Typically, rows in a matrix are denoted by <em>i</em>, while columns are denoted by <em>j</em>.</span></p>
<p><span>Matrices are represented as:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/73f016a2-d87e-481b-9229-394b7661d3b6.png" style="width:12.17em;height:3.00em;" width="1990" height="480"/></div>
<p class="mce-root">We can easily create matrices in Python as NumPy arrays, much like we can with vectors:</p>
<pre class="graf graf--pre graf-after--figure">matrix = np.array([[5,6], [6,9]])</pre>
<p>The only different is that we are adding an additional vector to the array to create the matrix. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tensors</h1>
                </header>
            
            <article>
                
<p><span>While you may have heard of vectors and matrices before, the name <strong>t</strong></span><strong>ensor</strong><span><strong> </strong>may be new. A tensor is a generalized matrix, and they have different sizes, or </span>ranks<em>,</em><span> which measure their dimensions.</span></p>
<p>Tensors are three (or more)-dimensional lists; you can think of them as a sort of multi-dimensional object of numbers, such as a cube. Tensors have a unique transitive property and form; if a tensor transforms another entity, it too must transform. Any rank 2 tensor can be represented as a matrix, but not all matrices are automatically rank 2 tensors. A tensor must have this transitive property. As we'll see, this will come into play with neural networks in the next chapter. We can create tensors in Python such as the following: </p>
<pre>tensor = [[[1,2,3,4]],[[2,5,6,3]],[[7,6,3,4]]] ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Matrix math</h1>
                </header>
            
            <article>
                
<p>The basic operations of an ANN are based on matrix math. In this section, we'll be reviewing the basic operations that you need to know to understand the mechanics of ANNs. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scalar operations</h1>
                </header>
            
            <article>
                
<p>Scalar operations involve a vector (or matrix) and a scalar. To perform an operation with a scalar on a matrix, simply apply to the scalar to every item in the matrix:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/034243e4-25c0-4081-b381-7a6c87ad45f2.png" style="width:10.67em;height:2.83em;" width="1820" height="480"/></div>
<p>In Python, we would simply do the following: </p>
<pre class="graf graf--pre graf-after--figure">vector = np.array([[1,2], [1,2]])new_vector = vector + 2</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Element–wise operations</h1>
                </header>
            
            <article>
                
<p><span>In element-wise operations, position matters. Values that correspond positionally are combined to create a new value. </span></p>
<p><span>To add to and/or subtract matrices or vectors:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/e07114f4-e679-4e3e-a997-747773c84017.png" style="width:14.92em;height:2.75em;" width="2580" height="480"/></div>
<div class="CDPAlignCenter CDPAlign"><span> <img class="fm-editor-equation" src="Images/5f919d62-e0c8-4c0a-9ffc-c19a2b915b6a.png" style="width:18.00em;height:2.92em;" width="3040" height="480"/></span></div>
<p>And in Python:</p>
<pre style="padding-left: 30px">vector_one = np.array([[1,2],[3,4]])<br/>vector_two = np.array([[5,6],[7,8]])<br/>    a + b<br/>    ## You should see:<br/>        array([[ 6, 8],[10, 12]])<br/>        array([[ 6, 8],[10, 12]])<br/>     a - b<br/>     ## You should see:<br/>         array([[-4, -4], [-4, -4]])</pre>
<p>There are two forms of multiplication that we may perform with vectors: the<strong><span> </span><span>D</span>ot product</strong><span>, </span>and the<span> </span><strong>Hadamard product</strong>.</p>
<p>The dot product is a special case of multiplication, and is rooted in larger theories of geometry that are used across the physical and computational sciences. It is a special case of a more general mathematical principle known as an <strong>inner product</strong>. <span>When utilizing the dot product of two vectors, the output is a scalar:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6d2dd8e6-671e-4644-93b3-1e8c056760ee.png" style="width:10.58em;height:3.00em;" width="1670" height="480"/></div>
<p>Dot products are a workhorse in machine learning. Think about a basic operation: let's say we're doing a simple classification problem where we want to know if an image contains a cat or a dog. If we did this with a neural network, it would look as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/3cc5fe55-f9ae-4e86-becb-f86437c3d3b3.png" style="width:9.25em;height:1.67em;" width="1270" height="220"/></div>
<p>Here, <em>y</em> is our classification cat or dog. We determine <em>y</em> by utilizing a network represented by <em>f</em>, where the input is <em>x</em>, while <em>w</em> and <em>b</em> represent a<span> weight and bias factor</span> (don't worry, we'll explain this in more detail in the coming chapter!). Our <em>x</em> and <em>w</em> are both matrices, and we need to output a scalar <img class="fm-editor-equation" src="Images/77024446-eb78-43c0-8166-b29644cd0086.png" style="width:0.58em;height:1.00em;" width="90" height="160"/>, which represents either cat or dog. We can only do this by taking the dot product of <em>w</em> and <img class="fm-editor-equation" src="Images/ea06997c-67a5-4c3f-96e1-ec7f15945c3a.png" style="width:0.75em;height:0.83em;" width="110" height="110"/>.</p>
<p> Relating back to our example, if this function were presented with an unknown image, taking the dot product will tell us how similar in direction the new vector is to the cat vector (<em>a</em>) or dog vector (<em>b</em>) by the measure of the angle (<img class="fm-editor-equation" src="Images/fe9988d5-fb5b-406c-bac6-3f9b490e57af.png" style="width:0.75em;height:1.42em;" width="90" height="170"/>) between them:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1399 image-border" src="Images/e2ac2c9f-cbe9-4b2e-a3f1-58daae581e1e.png" style="width:16.00em;height:12.00em;" width="552" height="413"/></div>
<p>If the vector is closer to the <span><span>direction</span></span> of the cat vector (a), we'll classify the image as containing a cat. If it's closer to the dog vector (b), we'll classify it as containing a dog. In deep learning, a more complex version of this scenario is performed over and over; it's the core of how ANNs work. </p>
<p>In Python, we can take the dot product of two vectors by using a built-in function from <kbd>numpy</kbd>, <span><span><kbd>np.dot()</kbd>:</span></span></p>
<pre class="graf graf--pre graf-after--figure">## Dot Product<br/>vector_one = np.array([1,2,3])<br/>vector_two = np.array([2,3,4])<br/>np.dot(vector_one,vector_two) ## This should give us 20</pre>
<p>The Hadamard product, on the other hand, outputs a vector:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/ae8c61da-3bff-49b9-bd6d-0853315777c5.png" style="width:12.00em;height:3.42em;" width="1700" height="480"/></div>
<p>The Hadamard product is element-wise, meaning that the individual numbers in the new matrix are the scalar multiples of the numbers from the previous matrices. Looking back to Python, we can easily perform this operation in Python with a simple <kbd>*</kbd> operator:</p>
<pre>vector_one = np.array([1,2,3])<br/>vector_two = np.array([2,3,4])<br/>vector_one * vector_two<br/>## You should see:<br/><span>array([ 2,  6, 12])</span></pre>
<div class="cell code_cell unrendered unselected">
<div class="input">
<p class="prompt input_prompt">Now that we've scratched the surface of basic matrix operations, let's take a look at how probability theory can aid us in the artificial intelligence field. </p>
</div>
</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic statistics and probability theory</h1>
                </header>
            
            <article>
                
<p><strong>Probability</strong>, the mathematical method for modeling uncertain scenarios, underpins the algorithms that make AI intelligent, helping to tell us how our systems should reason. So, what is probability? We'll define it as follows:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><em>Probability is a frequency expressed as a fraction of the sample size, n</em> [1].</div>
<p>Simply said, probability is the mathematical study of uncertainty. In this section, we'll cover the basics of probability space and probability distributions, as well as helpful tools for solving simple problems. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The probability space and general theory</h1>
                </header>
            
            <article>
                
<p>When probability is discussed, it's often referred to in terms of the probability of a certain <strong>event</strong> happening. Is it going to rain? Will the price of apples go up or down? In the context of machine learning, probabilities tell us the likelihood of events such as a comment being classified as positive vs. negative, or whether a fraudulent transaction will happen on a credit card. We measure probability by defining what we refer to as the <strong>probability space</strong>. A probability space is a measure of <em>how</em> and <em>why</em> of the probabilities of certain events. Probability spaces are defined by three characteristics: </p>
<ol>
<li>The sample space, which tells us the possible outcomes or a situation </li>
<li>A defined set of events; such as two fraudulent credit card transactions</li>
<li>The measure of probability of each of these events</li>
</ol>
<p>While probability spaces are a subject worthy of studying in their own right, for our own understanding, we'll stick to this basic definition. </p>
<p><span>In probability theory, the idea of </span><strong>independence</strong><span> is essential. Independence is a state where a random variable does not change based on the value of another random variable. This is an important assumption in deep learning, as non–independent features can often intertwine and affect the predictive power of our models.</span></p>
<p>In statistical terms, a collection of data about an event is a <strong>sample, </strong>which is drawn from a theoretical superset of data called a <strong>population</strong> that represents everything that is known about a grouping or event. For instance, if we were poll people on the street about whether they believe in Political View A or Political View B, we would be generating a <strong>random sample</strong> from the population, which would be entire population of the city, state, or country where we are polling.</p>
<p>Now let's say we wanted to use this sample to predict the likelihood of a person having one of the two political views, but we mostly polled people who were at an event supporting Political View A. In this case, we may have a <strong>biased sample</strong>. When sampling, it is important to take a random sample to decrease bias, otherwise any statistical analysis or modeling that we do with sample will be biased as well. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Probability distributions</h1>
                </header>
            
            <article>
                
<p>You've probably seen a chart such as the following one; it's showing us the values that appear in a dataset, and how many times those values appear. This is called a <strong>distribution</strong> of a variable. In this particular case, we're displaying the distribution with the help of a <strong>histogram</strong>, which shows the <strong>frequency</strong> of the variables: </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="Images/b4d6dc2a-a29b-4cbf-ad33-b1c538ea3618.png" style="width:20.00em;height:13.25em;" width="296" height="196"/></div>
<p>In this section, we're interested in a particular type of distribution, called a <strong>probability</strong> <strong>distribution</strong>. When we talk about probability distributions, we're talking about the likelihood of a random variable taking on a certain value, and we create one by dividing the frequencies in the preceding ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Probability mass functions </h1>
                </header>
            
            <article>
                
<p> <strong>Probability mass functions</strong> (<strong>PMFs</strong>)<strong> </strong>are discrete distributions. The random variables of the distribution can take on a finite number of values: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1400 image-border" src="Images/f44d2684-9c3f-40cb-b887-ef093f87eaba.png" style="width:19.17em;height:16.58em;" width="472" height="408"/></div>
<p>PMFs look a bit different from our typical view of a distribution, and that is because of their finite nature. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Probability density functions </h1>
                </header>
            
            <article>
                
<p><strong>Probability density functions</strong> (<strong>PDFs</strong>) are continuous distributions; values from the distribution can take on infinitely many values. For example, take the image as follows: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1401 image-border" src="Images/5c89482b-887c-40d8-b880-a7f612bca80a.png" style="width:34.17em;height:16.75em;" width="1024" height="502"/></div>
<p class="CDPAlignLeft CDPAlign"><span>You've probably seen something like this before; it's a probability density function of a <strong>standard normal</strong>, or <strong>Gaussian distribution</strong>. </span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Conditional and joint probability</h1>
                </header>
            
            <article>
                
<p><strong>Conditional probability</strong> is the probability that <em>x</em> happens, given that <em>y</em> happens. It's one of the key tools for reasoning about uncertainty in probability theory. Let's say we are talking about your winning the lottery, given that it's a sunny day. Maybe you're feeling lucky! How would we write that in a probability statement? It would be the probability of your lottery win, <em>A</em>, given the probability of it being sunny, <em>B</em>, so <em>P(A|B)</em>.</p>
<p><strong>Joint probability</strong> is the probability of two things happening simultaneously: what is the probability of you winning the lottery <em>and</em> it being a sunny day?</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chain rule for joint probability</h1>
                </header>
            
            <article>
                
<p>Joint probability is important in the AI space; it's what underlies the mechanics of <strong>generative models</strong>, which are able to replicate voice, pictures, and other unstructured information. These models learn the joint probability distribution of a phenomenon. They generate all possible values for a given object or event. A chain rule is a technique by which to evaluate the join probability of two variables. Formally, it is written as follows: </p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/a223b2fa-6759-4f86-b295-482060293233.png" style="width:19.33em;height:1.42em;" width="3170" height="220"/></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Bayes' rule for conditional probability</h1>
                </header>
            
            <article>
                
<p><strong>Bayes</strong>' <strong>rule</strong> is another essential tenet of probability theory in the machine learning sphere. It allows us to calculate the conditional probability of an event happening by inverting the conditions of the events. Bayes' rule is formally written as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/dcfd8314-ce15-4407-a1b0-468f855c74be.png" style="width:14.75em;height:3.25em;" width="2180" height="480"/></p>
<p>Let's use Bayes' rule to look at a simple conditional probability problem. In the following table, we see the likelihood of a patient contacting a disease:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td/>
<td>
<p><strong>Disease (1%)</strong></p>
</td>
<td>
<p><strong>No Disease (99%)</strong></p>
</td>
</tr>
<tr>
<td>
<p>Test Positive</p>
</td>
<td>
<p>80%</p>
</td>
<td>
<p>9.6%</p>
</td>
</tr>
<tr>
<td>
<p>Test Negative</p>
</td>
<td>
<p>20%</p>
</td>
<td>
<p>90.4%</p>
</td>
</tr>
</tbody>
</table>
<p>How do we interpret this table? The <em>x </em>axis tells us the percentage of the population who have the disease; if you have it, you are firmly in the Disease column. Based on that condition, the <em>y </em>axis is the likelihood of you testing positive or negative, based on whether you actually have the disease or not.</p>
<p>Now, let's say that we have a positive test result; what is the chance that we actually have the disease? We can use Bayes' formula to figure solve:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/b4469294-7f8e-4add-8f71-f3c50db0bfc2.png" style="width:32.25em;height:3.33em;" width="4640" height="480"/></p>
<p>Our answer comes out to 7.8%, the actual probability of having the disease given a positive test:</p>
<p>In the following code, we can see how Bayes' formula can model these conditional events based on likelihood. In machine learning and AI in general, this comes in handy when modeling situations or perhaps classifying objects. Conditional probability problems also play into discriminative models, which we will discuss in our section on <strong>Generative adversarial networks</strong>:</p>
<pre>.p_diseasePos = 0.8 ## Chance of having the disease given a positive result<br/>p_diseaseNeg = 0.2 ## Chance of having the disease given a negative result<br/>p_noPos = 0.096<br/>p_noNeg = 0.904<br/>p_FalsePos = (.80 * .01) + (.096 * .99)<br/>p_disease_given_pos = (.80 * .01) / p_FalsePos<br/>print(p_disease_given_pos)</pre>
<p><span>Remember: when conducting multiplication, the type of operation matters. We can use the Hadamard product to multiply two equally-sized vectors or matrices where the output will be another equally-sized vector or matrix. We use the dot product in situations where we need a single num</span>ber as an output. The dot produc<span>t is essential in machine learning and deep learning; with neural networks, inputs are passed to each layer as a matrix or vector, and these are then multiplied with another matrix of weights, which forms the core of basic network operations. </span></p>
<p><span>Probability distributions and the computations based on them rely on Bayesian thinking in the machine learning realm. As we'll see in later chapters, some of the most innovative networks in AI directly rely on these distributions and the core concepts of Baye's theorem. Recall that there are two primary forms of probability distribution: PMFs<strong> </strong>for discrete variables, and probability density functions PDFs for continuous variables; CDF, which apply to any random variables, also exist.</span></p>
<p>Baye's rule, in fact, has inspired an entire branch of statistics known as <strong>Bayesian statistics</strong>.<strong> </strong>Thus far, we have discussed frequent statistics<strong>,</strong> which measure probability based on an observed space of repeatable events<strong>. </strong>Bayesian probability, on the other hand, measures degrees of belief; how likely is an event to happen based on the information that is currently available? This will become important as we delve into ANNs in the following chapters.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Constructing basic machine learning algorithms</h1>
                </header>
            
            <article>
                
<p><strong> </strong>As mentioned in the last chapter, machine learning is a term that was developed as a reaction to the first AI winter. Today, we generally consider machine learning to be the overarching subject area for deep learning and ANNs in general.</p>
<p>Most machine learning solutions can be broken down into either a <strong>classification</strong> problem or a <strong>regression</strong> problem. A classification problem is when the output variables are categorical, such as fraud or not fraud. A regression problem is when the output is continuous, such as dollars or site visits. Problems with numerical output can be categorical, but are typically transformed to have a categorical output such as first class and second class.</p>
<p><span>Within machine ...</span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Supervised learning algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">Supervised algorithms rely on human knowledge to complete their tasks. Let's say we have a dataset related to loan repayment that contains several demographic indicators, as well as whether a loan was paid back or not:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Income</strong></p>
</td>
<td>
<p><strong>Age</strong></p>
</td>
<td>
<p><strong>Marital Status</strong></p>
</td>
<td>
<p><strong>Location</strong></p>
</td>
<td>
<p><strong>Savings</strong></p>
</td>
<td>
<p><strong>Paid</strong></p>
</td>
</tr>
<tr>
<td>
<p>$90,000</p>
</td>
<td>
<p>28</p>
</td>
<td>
<p>Married</p>
</td>
<td>
<p>Austin, Texas</p>
</td>
<td>
<p>$50,000</p>
</td>
<td>
<p>y</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>The Paid column, which tells us if a loan was paid back or not, is called the <strong>target</strong> - it's what we would like to predict. The data that contains information about the applicants background is known as the <strong>features</strong> of the datasets. In supervised learning, algorithms learn to predict the target based on the features, or in other words, what indicators give a high probability that an applicant will pay back a loan or not? Mathematically, this process looks as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/99f325bf-11d3-4441-b8ae-b87042a3fc7a.png" style="width:7.25em;height:1.67em;" width="1000" height="220"/></p>
<p>Here, we are saying that our label <img class="fm-editor-equation" src="Images/c855e39b-a2d0-41eb-b053-60907ec03d63.png" style="width:0.75em;height:1.17em;" width="90" height="160"/> <em>is a function of</em> the input features <img class="fm-editor-equation" src="Images/390b3c7c-cf71-47df-8996-42f6d8b2bc99.png" style="width:0.67em;height:0.75em;" width="110" height="110"/>, plus some amount of error <img class="fm-editor-equation" src="Images/9b202d7a-42f7-4dfe-933e-2a62c9b4deb6.png" style="width:0.67em;height:0.92em;" width="80" height="110"/> that it caused naturally by the dataset. We know that a certain set of features will likely produce a certain outcome. In supervised learning, we set up an algorithm to <em>learn</em> what function will produce the correct mapping of a set of features to an outcome. </p>
<p class="mce-root">To illustrate how supervised learning works, we are going to utilize a famous example toy dataset in the machine learning field, the Iris Dataset. It shows four features: Sepal Length, Sepal Width, Petal Length, and Petal Width. In this dataset, our target variable (sometimes called a <strong>label</strong>) is <em>Name. </em>The dataset is available in the GitHub repository that corresponds with this chapter:</p>
<pre>import pandas as pd<br/>data = pd.read_csv("iris.csv")<br/>data.head()</pre>
<p>The preceding code generates the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-854 image-border" src="Images/313467ac-81ee-4e6d-8953-97e7abc893ba.png" style="width:35.50em;height:13.75em;" width="914" height="354"/></p>
<p>Now that we have our data ready to go, let's jump into some supervised learning!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Random forests</h1>
                </header>
            
            <article>
                
<p class="mce-root"><strong>Random forests</strong> are one of the most commonly utilized supervised learning algorithms. While they can be used for both classification and regression tasks, we're going to focus on the former. Random forests are an example of an <strong>ensemble method</strong>, which works by aggregating the outputs of multiple models in order to construct a stronger performing model. Sometimes, you'll hear this being referred to as a grouping of <strong>weak learners</strong> to create a <strong>strong learner</strong>. </p>
<p class="mce-root">Setting up a random forest classifier in Python is quite simple with the help of scikit-learn. First, we import the modules and set up our data. We do not have to perform any data cleaning here, as the Iris dataset comes pre-cleaned.</p>
<p class="mce-root">Before training machine learning algorithms, ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Unsupervised learning algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">Unsupervised learning algorithms learn the properties of data on their own without explicit human intervention or labeling. Typically within the AI field,<span><span> unsupervised learning technique learn the probability distribution that generated a dataset.</span></span> These algorithms, such as <strong>autoencoders</strong> (we will visit these later in the book), are useful for a variety of tasks where we simply don't know important information about our data that would allow us to use traditional supervised techniques.</p>
<p class="mce-root"> PCA is an unsupervised method for feature extraction. It combines input variables in such a way that we can drop those variables that provide the least amount of information to us. Afterwards, we are left with new variables that are independent of each other, making them easy to utilize in a basic linear model. </p>
<p class="mce-root"><span>AI applications are fundamentally hampered by the </span><strong>curse of dimensionality</strong><span>. This phenomenon, which occurs when the number of </span><strong>dimensions</strong><span> in your data is high, makes it incredibly difficult for learning algorithms to perform well. PCA can help alleviate this problem for us. </span>PCA is one of the primary examples of what we call <strong>dimensionality reduction</strong>, which helps us take high-feature spaces (lots of data attributes) and transform them into lower-feature spaces (only the important features). </p>
<p class="mce-root">Dimensionality reduction can be conducted in two primary ways: <strong>feature elimination</strong> and <strong>feature extraction</strong>. Whereas feature elimination may involve the arbitrary cutting of features from the dataset, feature extraction (PCA is a form of) this gives us a more intuitive way to reduce our dimensionality. So, how does it work? In a nutshell:</p>
<ul>
<li>We create a matrix (correlation or covariance) that describes how all of our data relates to each other</li>
<li>We decompose this matrix into separate components, called the <strong>eigenvalues</strong> and the <strong>eigenvectors</strong>, which describe the direction and magnitude of our data</li>
<li>We then transform or project our original data onto these components</li>
</ul>
<p class="mce-root">Let's break this down in Python manually to illustrate the process. We'll use the same Iris dataset that we used for the supervised learning illustration. First, we'll create the correlation matrix:</p>
<pre>features = (features - features.mean()) / features.std()<br/><br/>corr_matrix = np.corrcoef(data.values.T)<br/>corr_matrix.corr()</pre>
<p>The output should look as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1403 image-border" src="Images/6b9f2573-871b-4295-abe0-f9e104b9e3d5.png" style="width:29.33em;height:9.83em;" width="887" height="298"/></p>
<p class="mce-root">Our correlation matrix contains information on how every element of the matrix relates to each other element. This record of association is essential in providing the algorithm with information. Lots of variability typically indicates a signal, whereas a lack of variability indicates noise. The more variability that is present in a particular direction, the more there is to detect. Next, we'll create our <kbd>eigen_values</kbd> and <kbd>eigen_vectors</kbd>: </p>
<pre>eigen_values, eigen_vectors = np.linalg.eig(corr_matrix)</pre>
<p>The output should look as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-860 image-border" src="Images/faf4a54f-4dc7-48a6-aef8-12465565c874.png" style="width:31.58em;height:6.58em;" width="978" height="206"/></p>
<p><span>Eigenvectors and Eigenvalues come in pairs ; each eigenvectors are directions in of data, and eigenvalues tell us how much variance exists within that direction. In PCA, we want to understand which inputs account for the most variance in the data (that is: how much do they explain the data). By calculating eigenvectors and their corresponding eigenvalues, we can begin to understand what is most important in our dataset. </span></p>
<p>We now want to sort the pairs of eigenvectors/eigenvalues from highest to lowest. </p>
<pre><br/>eigenpairs = [[eigen_values[i], eigen_vectors[:,i]] for i in range(len(eigen_vectors))]<br/><br/>eigenpairs.sort(reverse=True)</pre>
<p>Lastly, we need to project these pairs into a <em>lower dimensional spac</em>e. This is dimensionality reduction aspect of PCA:</p>
<pre>projection = np.hstack((eigenpairs[0][1].reshape(eig_vectors.shape[1],1),<br/>                                  eigenpairs[1][1].reshape(eig_vectors.shape[1],1)))</pre>
<p>We'll then conduct this transformation on the original data:</p>
<pre>transform = features.dot(projection)</pre>
<p>We can then plot the components against each other: </p>
<pre>fig = plt.figure(figsize=(8,8))<br/><br/>ax = fig.gca()<br/>ax = sns.regplot(transform.iloc[:,0], transform.iloc[:,1],fit_reg=False, scatter_kws={'s':70}, ax=ax)<br/><br/>ax.set_xlabel('principal component 1', fontsize=10)<br/>ax.set_ylabel('principal component 2', fontsize=10)<br/><br/><br/>for tick in ax.xaxis.get_major_ticks():<br/>    tick.label.set_fontsize(12) <br/>    <br/>for tick in ax.yaxis.get_major_ticks():<br/>    tick.label.set_fontsize(12) <br/>    <br/>ax.set_title('Pricipal Component 1 vs Principal Component 2\n', fontsize=15)<br/><br/>plt.show()</pre>
<p>You should see the plot as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1404 image-border" src="Images/0223443c-4129-4038-9661-00e32a31e296.png" style="width:45.83em;height:38.17em;" width="550" height="458"/></p>
<p class="mce-root">So when should we use PCA? Use PCA when the following are true:</p>
<ul>
<li>Do you have high dimensionality (too many variables) and want a logical way to reduce them?</li>
<li>Do you need to ensure that your variables are independent of each other?</li>
</ul>
<p class="mce-root">One of the downsides of PCA, however, is that it makes the underlying data more opaque, thus hurting it's interpretability. Besides PCA and the k–means clustering model that we precedingly described, other commonly seen non-deep learning unsupervised learning algorithms are: </p>
<ul>
<li>K-means clustering</li>
<li>Hierarchical clustering</li>
<li>Mixture models</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic tuning</h1>
                </header>
            
            <article>
                
<p>So you've built a model, now what? Can you call it a day? Chances are, you'll have some optimization to do on your model. A key part of the machine learning process is the optimization of our algorithms and methods. In this section, we'll be covering the basic concepts of optimization, and will be continuing our learning of tuning methods throughout the following chapters. </p>
<p><span>Sometimes, when our models do not perform well with new data it can be related to them</span> <strong>overfitting</strong> <span>or</span> <strong>underfitting</strong><span>. Let's cover some methods that we can use to prevent this from happening. First off, let's look at the random forest classifier that we trained earlier. In your notebook, call the <kbd>predict</kbd> method on it and pass the <kbd>x_test</kbd> data in to receive some ...</span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Overfitting and underfitting</h1>
                </header>
            
            <article>
                
<p><strong>Overfitting</strong> is a phenomenon that happens when an algorithm learns it's training data <em>too well</em> to the point where it cannot accurately predict on new data. Models that overfit learn the small, intricate details of their training set and don't generalize well. For analogy, think about it as if you were learning a new language. Instead of learning the general form of the language, say Spanish, you've learned to perfect a local version of it from a remote part of South America, including all of the local slang. If you went to Spain and tried to speak that version of Spanish, you would probably get some puzzled looks from the locals! Underfitting would be exact opposite of this; you didn't study enough Spanish, and so you do not have enough knowledge to communicate effectively. From a modeling standpoint, an underfit model is not complex enough to generalize to new data.</p>
<p>Overfitting and underfitting are tried to a machine learning phenomenon known as the<span> </span><strong>bias</strong>/<strong>variance</strong><span> </span>tradeoff:</p>
<ul>
<li><strong>Bias</strong><span> </span>is the error that your model learns as it tries to approximately predict things. Understanding that models are simplified versions of reality, bias in a model is the error that develops from trying to create this simplified version.</li>
<li><strong>Variance</strong><span> </span>is the degree to which your error changes based on variations in the input data. It measures your model's sensitivity to the intricacies of the input data. </li>
</ul>
<p>The way to mitigate bias is to increase the complexity of a model, although this will increase variance and will lead to overfitting. To mitigate variance on the other hand, we could make our model to generalize well by reducing complexing, although this would lead to higher bias. As you can see, we cannot have a both low bias and low variance at the same time! A good model will be balanced between it's bias and variance. There are two ways to combat overfitting; cross-validation and regularization. We will touch upon cross-validation methods now, and come back to regularization in <a href="8c724645-08d4-4a08-af9e-45bf607f8a88.xhtml" target="_blank">Chapter 4</a>, <em>Your First Artificial Neural Network</em> when we begin to build our first ANNs. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">K-fold cross-validation</h1>
                </header>
            
            <article>
                
<p>You've already seen a form of cross-validation before; holding out a portion of our data is the simplest form of cross- validation that we can have. While this is generally a good practice, it can sometimes leave important features out of the training set that can create poor performance when it comes time to test. To remedy this, we can take standard cross validation a step further with a technique called <strong>k</strong>-<strong>fold cross validation</strong>. </p>
<p>In k-fold cross validation, our dataset is evenly divided in <em>k</em> event parts, chosen by the user. As a rule of thumb, generally you should stick to k = 5 or k = 10 for best performance. <span>The model is then trained and tested <em>k</em> times over. During each training episode, one <em>k</em> segment of the data ...</span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hyperparameter optimization</h1>
                </header>
            
            <article>
                
<p>Aside from protecting against overfitting, we can optimize models by searching for the best combination of <strong>model hyperparameters</strong>. Hyperparameters are configuration variables that tell the model what methods to use, as opposed to <strong>model parameters</strong> which are learned during training - we'll learn more about these in upcoming chapter. They are programmatically added to a model, and are present in all modeling packages in Python. In the random forest model that we built precedingly, for instance, <kbd>n_estimators</kbd> is a hyperparameter that tells the model how many trees to build. The process of searching for the combination of hyperparameters that leads to the best model performance is called <strong>hyperparameter tuning</strong>. </p>
<p>In Python, we can tune hyperparameter with an exhaustive search over their potential values, called a <strong>Grid Search</strong>. Let's use our random forest model to see how we can do this in Python by import <kbd>GrisSearchCV</kbd>:</p>
<pre>from sklearn.model_selection import GridSearchCV<br/><br/>parameters = {<br/> 'n_estimators': [100, 500, 1000],<br/> 'max_features': [2, 3, 4],<br/> 'max_depth': [90, 100, 110, 120],<br/> 'min_samples_split': [6, 10, 14],<br/> 'min_samples_leaf': [2, 4, 6], <br/>}</pre>
<p>In this case, we are going to pass the Grid Search a few different hyperparameters to check; you can read about what they do in the documentation for the classifier (<a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" target="_blank">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>). </p>
<p>To create the search, we simply have to initialize it: </p>
<pre>search = GridSearchCV(estimator = rf_classifier, param_grid = parameters, cv = 3)</pre>
<p>We can then apply it to the data: </p>
<pre>search.fit(x_train, y_train)<br/>search.best_params_</pre>
<p>If we then want to check the performance of the best combination of parameters, we can easily do that in sklearn by evaluating it on the test data: </p>
<pre>best = search.best_estimator_<br/>accuracy = evaluate(best, x_test, y_test)</pre>
<p class="mce-root">Hyperparameter tuning searches can be applied to the neural network models that we'll be utilizing in the coming chapters. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">Machine learning, and by extension, deep learning, relies on the building blocks of linear algebra and statistics at its core. Vectors, matrices, and tensors provide the means by which we represent input data and parameters in machine learning algorithms, and the computations between these are the core operations of these algorithms. Likewise, distributions and probabilities help us model data and events in machine learning. </p>
<p>We also covered two classes of algorithms that will inform how we think about ANNs in further chapters: supervised learning methods and unsupervised learning methods. With supervised learning, we provide the algorithm with a set of features and labels, and it learns how to appropriately map certain feature combinations ...</p></article></section></div>



  </body></html>