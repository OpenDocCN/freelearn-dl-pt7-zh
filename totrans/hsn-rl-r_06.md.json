["```py\nlibrary(contextual)\nhorizon <- 500\nsimulations <- 1000\nProbClick <- c(0.1, 0.3, 0.7)\nbandit <- BasicBernoulliBandit$new(weights = ProbClick)\npolicy <- EpsilonFirstPolicy$new(time_steps = 100)\nagent <- Agent$new(policy, bandit)\nsimulator <- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)\nhistory <- simulator$run()\nplot(history, type = \"average\", regret = FALSE, lwd = 2, legend_position = \"bottomright\")\nplot(history, type = \"cumulative\", regret = FALSE, rate = TRUE, lwd = 2)\nplot(history, type = \"arms\")\n```", "```py\nlibrary(contextual)\n```", "```py\nhorizon <- 500\n```", "```py\nsimulations <- 1000\n```", "```py\nProbClick <- c(0.1, 0.3, 0.7)\n```", "```py\nbandit <- BasicBernoulliBandit$new(weights = ProbClick)\n```", "```py\npolicy <- EpsilonFirstPolicy$new(time_steps = 100)\n```", "```py\nagent <- Agent$new(policy, bandit)\n```", "```py\nsimulator <- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)\n```", "```py\nhistory <- simulator$run()\n```", "```py\nplot(history, type = \"average\", regret = FALSE, lwd = 2, legend_position = \"bottomright\")\n```", "```py\nplot(history, type = \"cumulative\", regret = FALSE, rate = TRUE, lwd = 2)\n```", "```py\nplot(history, type = \"arms\")\n```", "```py\nlibrary(contextual)\nhorizon <- 500\nsimulations <- 1000\nProbClick <- c(0.1, 0.3, 0.7)\nbandit <- BasicBernoulliBandit$new(weights = ProbClick)\npolicy <- EpsilonGreedyPolicy$new(epsilon = 0.1)\nagent <- Agent$new(policy,bandit)\nsimulator <- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)\nhistory <- simulator$run()\nplot(history, type = \"average\", regret = FALSE, lwd = 2, legend_position = \"bottomright\")\nplot(history, type = \"cumulative\", regret = FALSE, rate = TRUE, lwd = 2, legend_position = \"bottomright\")\nplot(history, type = \"arms\", legend_position = \"topright\")\n```", "```py\nlibrary(contextual)\n```", "```py\nhorizon <- 500\n```", "```py\nsimulations <- 1000\n```", "```py\nProbClick <- c(0.1, 0.3, 0.7)\n```", "```py\nbandit <- BasicBernoulliBandit$new(weights = ProbClick)\n```", "```py\npolicy <- EpsilonGreedyPolicy$new(epsilon = 0.1)\n```", "```py\nagent <- Agent$new(policy,bandit)\n```", "```py\nsimulator <- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)\n```", "```py\nSimulation horizon: 500\nNumber of simulations: 1000\nNumber of batches: 1\nStarting main loop.\ndata.table 1.12.0 Latest news: r-datatable.com\nFinished main loop.\nCompleted simulation in 0:01:25.191\nComputing statistics.\n```", "```py\nhistory <- simulator$run()\n```", "```py\nplot(history, type = \"average\", regret = TRUE, lwd = 2, legend_position = \"topright\")\n```", "```py\nplot(history, type = \"cumulative\", regret = TRUE, rate = TRUE, lwd = 2,legend_position = \"topright\")\n```", "```py\nplot(history, type = \"arms\")\n```", "```py\nlibrary(contextual)\nhorizon <- 500\nsimulations <- 1000\nProbClickContx <- matrix(c(0.1, 0.3, 0.7, 0.8, 0.4, 0.1),\n nrow = 2, ncol = 3, byrow = TRUE)\nbandit <- ContextualBinaryBandit$new(weights = ProbClickContx)\npolicy <- EpsilonGreedyPolicy$new(epsilon = 0.1)\nagent <- Agent$new(policy,bandit)\nsimulator <- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)\nhistory <- simulator$run()\nplot(history, type = \"arms\", legend_position = \"bottomright\")\n```", "```py\nlibrary(contextual)\n```", "```py\nhorizon <- 500\n```", "```py\nsimulations <- 1000\n```", "```py\nProbClickContx <- matrix(c(0.1, 0.3, 0.7, 0.8, 0.4, 0.1),\n                    nrow = 2, ncol = 3, byrow = TRUE)\n```", "```py\nbandit <- ContextualBinaryBandit$new(weights = ProbClickContx)\n```", "```py\npolicy <- EpsilonGreedyPolicy$new(epsilon = 0.1)\n```", "```py\nagent <- Agent$new(policy,bandit)\n```", "```py\nsimulator <- Simulator$new(agent, horizon, simulations, do_parallel = FALSE)\n```", "```py\nSimulation horizon: 500\nNumber of simulations: 1000\nNumber of batches: 1\nStarting main loop.\nFinished main loop.\nCompleted simulation in 0:01:43.277\nComputing statistics.\n```", "```py\nhistory <- simulator$run()\n```", "```py\nplot(history, type = \"arms\", legend_position = \"bottomright\")\n```", "```py\nlibrary(contextual)\nhorizon <- 500\nsimulations <- 1000\nProbClick <- c(0.8, 0.3, 0.1)\nbandit <- BasicBernoulliBandit$new(weights = ProbClick)\n\nagents <- list(Agent$new(OraclePolicy$new(), bandit),\n Agent$new(UCB1Policy$new(), bandit),\n Agent$new(ThompsonSamplingPolicy$new(1.0, 1.0), bandit),\n Agent$new(EpsilonGreedyPolicy$new(0.1), bandit),\n Agent$new(SoftmaxPolicy$new(tau = 0.1),bandit),\n Agent$new(Exp3Policy$new(0.1), bandit))\n\nsimulation <- Simulator$new(agents, horizon, simulations)\nhistory <- simulation$run()\n\nplot(history, type = \"cumulative\", regret = FALSE)\n```", "```py\nlibrary(contextual)\n```", "```py\nhorizon <- 500\nsimulations <- 1000\n```", "```py\nProbClick <- c(0.8, 0.3, 0.1)\n```", "```py\nbandit <- BasicBernoulliBandit$new(weights = ProbClick)\n```", "```py\nagents <- list(Agent$new(OraclePolicy$new(), bandit),\n               Agent$new(UCB1Policy$new(), bandit),\n               Agent$new(ThompsonSamplingPolicy$new(1.0, 1.0), bandit),\n               Agent$new(EpsilonGreedyPolicy$new(0.1), bandit),\n               Agent$new(SoftmaxPolicy$new(tau = 0.1),bandit),\n               Agent$new(Exp3Policy$new(0.1), bandit))\n```", "```py\nsimulation <- Simulator$new(agents, horizon, simulations)\nhistory <- simulation$run()\n```", "```py\nplot(history, type = \"cumulative\")\n```"]