- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure OpenAI Advanced Topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding chapters, we’ve covered the basics of **Azure OpenAI** (**AOAI**)
    service, including model deployment and various pricing structures. Now, our attention
    will turn to exploring more advanced topics within AOAI.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will delve into the following advanced AOAI topics:'
  prefs: []
  type: TYPE_NORMAL
- en: AOAI model context window
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOAI Embedding models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure vector databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOAI On Your Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOAI multimodal model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOAI function calling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOAI Assistants API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOAI Batch API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOAI fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOAI model context window
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of **Large Language Models** (**LLMs**), a context window defines
    the amount of text a model can process at once, impacting how it generates and
    comprehends language. This window is measured by the number of tokens (either
    whole words or fragments), directly affecting how much information the model uses
    to predict the next token. In simple terms, it dictates how much context the model
    takes into account when forming predictions or crafting responses.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the GPT-3.5-Turbo (0125) model context has 16,385 input tokens
    and 4,096 output tokens, while the GPT-4o and GPT-4o mini models have a much larger
    number at 128,000 input tokens and 16,384 output tokens. For information about
    the context window of AOAI models, visit [https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?#gpt-4-and-gpt-4-turbo-models](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?#gpt-4-and-gpt-4-turbo-models).
  prefs: []
  type: TYPE_NORMAL
- en: 'The trend in LLMs is moving toward bigger context windows, which enable more
    detailed and coherent outputs. However, this comes at a cost: larger context windows
    require more computational power and memory. In practical terms, the context window
    defines how much of the prior conversation the model can “remember” during an
    interaction. When the conversation surpasses the context window, the model loses
    the earliest parts of the dialogue, potentially affecting its consistency in lengthy
    interactions or intricate tasks. As a result, the context window size is a crucial
    factor to consider when building applications that utilize LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: While larger context windows enable LLMs to handle more data, they also come
    with substantial computational and financial costs. Processing extensive context
    lengths is extremely expensive and incredibly slow, which is only acceptable in
    limited cases. For instance, a context window of a million tokens could take nearly
    a minute to produce a single response when working with several million tokens.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, **Retrieval-Augmented Generation** (**RAG**) is more efficient
    because it fetches only the most relevant information for each query, reducing
    the number of tokens the model needs to process. This efficiency makes RAG a more
    cost-effective solution, particularly for applications requiring frequent or high-volume
    queries and data-intensive tasks. We’ll explain RAG in detail in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: After text is split into tokens, each token is converted into a numerical form
    called an embedding. Embeddings are dense vector representations designed to capture
    the meaning of tokens. These vectors exist in a high-dimensional space, where
    the distance and direction between vectors can represent semantic and syntactic
    relationships between the words they represent.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll discuss the embedding model that AOAI uses to convert
    tokens into embedding vectors.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI embedding models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AOAI has four different embedding models, and each model has specific limits
    for input tokens and output dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**text-embedding-ada-002 (****version 2)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max input** **tokens**: 8,191'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output** **dimensions**: 1,536'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text-embedding-ada-002 (****version 1)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max input** **tokens**: 2046'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output** **Dimensions**: 1536'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text-embedding-3-large**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max input** **tokens**: 8191'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output** **Dimensions**: 3072'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text-embedding-3-small**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max input** **tokens**: 8191'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output** **Dimensions**: 1536'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: text-embedding-3-small and text-embedding-3-large are the newest and most performant
    embedding models. They are now available in AOAI.
  prefs: []
  type: TYPE_NORMAL
- en: text-embedding-ada-002 (version 2)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The text-embedding-ada-002 model consolidates the functions of five different
    models used for searching text, comparing text similarity, and searching code.
    It performs better than our previous top model, Davinci, in most tasks and is
    99.8% less expensive. This model has a longer context length of 8,192 compared
    to the previous version of text-embedding-ada-002.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use this embedding model using just a few lines of code, as you did
    with the previous version, by using our OpenAI Python library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: text-embedding-3-small
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: text-embedding-3-small is a new, highly efficient embedding model, offering
    a significant upgrade over text-embedding-ada-002 (version 2), released in December
    2022.
  prefs: []
  type: TYPE_NORMAL
- en: 'It boasts improved performance:'
  prefs: []
  type: TYPE_NORMAL
- en: text-embedding-3-small outperforms text-embedding-ada-002 in **multilingual
    retrieval** (**MIRACL**), increasing the average score from 31.4% to 44.0%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For **English tasks** (**MTEB**), the average score has improved from 61.0%
    to 62.3%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also comes with lower costs. text-embedding-3-small is five times more cost-efficient
    than text-embedding-ada-002, reducing the price per 1,000 tokens from $0.0001
    to $0.00002.
  prefs: []
  type: TYPE_NORMAL
- en: text-embedding-3-large
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: text-embedding-3-large is another new, next-generation embedding model, producing
    embeddings with up to 3,072 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'It comes with improved performance:'
  prefs: []
  type: TYPE_NORMAL
- en: text-embedding-3-large is the best-performing model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On MIRACL, it improves the average score from 31.4% (text-embedding-ada-002)
    to 54.9%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On MTEB, the average score increases from 61.0% to 64.6%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As for the cost, text-embedding-3-large is slightly higher cost than text-embedding-3-small,
    with the price per 1,000 tokens set at $0.00013.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI is not discontinuing text-embedding-ada-002 (version 2), so customers can
    continue using it if they prefer. However, it’s recommended to switch to the newer
    model for better price performance.
  prefs: []
  type: TYPE_NORMAL
- en: Both new embedding models use a technique called `dimensions` API parameter,
    you can reduce the size of the embeddings without losing their meaning.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, on the MTEB benchmark, a text-embedding-3-large embedding can
    be shortened to 256 dimensions while still outperforming an unshortened text-embedding-ada-002
    embedding with 1,536 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, it’s best to use the `dimensions` parameter when creating the embedding.
    If you need to change the dimensions afterward, ensure that the embedding is normalized,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Changing dimensions dynamically allows for flexible usage. For example, if a
    vector DB only supports embeddings up to 1,536 dimensions, developers can still
    use the best model, text-embedding-3-large, by setting the `dimensions` API parameter
    to `1536`. This reduces the embedding from 3,072 dimensions, sacrificing some
    accuracy for a smaller vector size. This leads us perfectly into discussing the
    vector search feature in the Azure AI Search service in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Azure vector databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we explored various AOAI embedding models for generating
    vector embeddings. After creating these vectors, it’s essential to have a database
    optimized for storing and managing them effectively. The key distinction between
    a vector database and other types of databases is its capability to handle high-dimensional
    data. A vector database is specifically engineered to store data as high-dimensional
    vectors, which are mathematical representations of various features or attributes.
    Each vector comprises multiple dimensions, ranging from tens to thousands, depending
    on the data’s complexity and detail. These vectors are usually generated by applying
    transformation or embedding functions to raw data sources such as text, images,
    audio, video, and more. This type of database enables the indexing and querying
    of embeddings using vector search algorithms that assess vector distance or similarity.
    To ensure accurate retrieval of relevant information, a robust mechanism is necessary.
    Prominent vector search algorithms include **Hierarchical Navigable Small World**
    (**HNSW**), **Inverted File** (**IVF**), and DiskANN, among others.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary benefit of a vector database is its ability to conduct rapid and
    precise similarity searches and data retrieval based on vector distance or similarity.
    Unlike traditional databases that depend on exact matches or predefined criteria
    for queries, a vector database allows for the identification of the most similar
    or relevant data based on semantic or contextual meanings. Here are some practical
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Search engines**: Vector databases transform search engines by enabling efficient
    similarity-based searches. They help find similar items, improve search relevance,
    and enhance user experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image retrieval**: Identify images that are similar to a given image based
    on visual content and style.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document search**: Locate documents similar to a given document, considering
    factors such as topic and sentiment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product recommendations**: Discover products that are similar to a given
    product based on features and ratings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic search**: Vector databases enhance semantic search capabilities,
    allowing applications to find contextually related content. This makes them valuable
    for information retrieval, chatbots, and question-answering systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommender systems**: These systems benefit from vector databases by providing
    personalized product recommendations or content suggestions based on user preferences,
    thereby improving recommendation accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical and scientific research**: In fields such as genomics and chemistry,
    vector databases facilitate the analysis of genomic data, identification of chemical
    compound similarities, and acceleration of scientific discoveries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To perform similarity search and retrieval in a vector database, a query vector
    encapsulating your desired information or criteria is needed. This query vector
    can originate from the same type of data as the stored vectors (e.g., using an
    image to query an image database) or from different types of data (e.g., using
    text to query an image database). The next step involves using a similarity measure
    to determine the proximity or distance between two vectors within the vector space.
    Various metrics can be employed for this purpose, such as cosine similarity, Euclidean
    distance, Hamming distance, and Jaccard index. The result of the similarity search
    and retrieval process is typically a ranked list of vectors that exhibit the highest
    similarity scores to the query vector. Subsequently, you can retrieve the associated
    data for each vector from the original source or index.
  prefs: []
  type: TYPE_NORMAL
- en: 'Azure provides six varieties of vector database options tailored to diverse
    needs and use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Azure AI Search**: Azure AI Search is a powerful tool that features vector
    search capabilities. It utilizes the HNSW algorithm for vector searches and the
    **Best Match 25** (**BM25**) algorithm for full-text searches. Additionally, it
    offers a hybrid search option that merges the results from both full-text and
    vector queries, each using different ranking methodologies such as BM25 and HNSW.
    The **Reciprocal Rank Fusion** (**RRF**) algorithm is employed to consolidate
    these results, ensuring a single, cohesive result set that highlights the most
    relevant matches from the search index. In fact, OpenAI leverages Azure AI Search
    to enhance the capabilities of its ChatGPT application. By integrating Azure AI
    Search, OpenAI benefits from advanced search functionalities such as vector search,
    full-text search, and hybrid search, which combine the strengths of the BM25 and
    HNSW algorithms. This integration allows ChatGPT to deliver more accurate and
    relevant responses by efficiently retrieving and ranking information from a search
    index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, it is important to note that Azure AI Search does not generate vector
    embeddings for your content; you are required to provide these embeddings yourself.
    One viable option for generating these embeddings is through AOAI embedding models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From an architectural standpoint, the AI Search service functions as an intermediary
    between external data stores that house your un-indexed data and the client application
    that sends query requests to a search index and manages the responses.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.1: Azure AI Search application architecture](img/B21019_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Azure AI Search application architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Within your client application, the search experience is crafted using Azure
    AI Search APIs, which can include features such as relevance tuning, semantic
    ranking, autocomplete, synonym matching, fuzzy matching, pattern matching, filtering,
    and sorting.
  prefs: []
  type: TYPE_NORMAL
- en: Azure AI Search also offers seamless integration with other Azure services.
    This is facilitated through indexers that automate data ingestion and retrieval
    from various azure data sources, as well as skillsets that incorporate AI functionalities
    from Azure AI services. These skillsets can include image and natural language
    processing, or custom code encapsulated within Azure Functions.
  prefs: []
  type: TYPE_NORMAL
- en: '`WHERE` clauses. This enables your vector searches to yield the most relevant
    data to your applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This feature enhances the core capabilities of Azure Cosmos DB, making it more
    versatile for handling vector data and search requirements in AI applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Azure Cosmos DB for MongoDB**: Azure Cosmos DB for MongoDB also offers integrated
    vector database capabilities, allowing embeddings to be stored, indexed, and queried
    alongside other relational data. This feature enables efficient management and
    retrieval of vector data within MongoDB collections, enhancing the versatility
    of Azure Cosmos DB for diverse application needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MongoDB vCore provides two types of vector indexing methods:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**IVF**: IVF indexing is a method used in vector search to efficiently organize
    and manage large sets of vectors by clustering them into groups. Each cluster
    is represented by a centroid, or center point. During a search, the query vector
    is first compared to these centroids to identify the closest cluster. The search
    is then conducted within this specific cluster, significantly reducing the search
    space and improving retrieval times. This method balances speed and accuracy,
    making it ideal for applications requiring rapid and efficient vector searches.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HNSW**: This is an advanced algorithm for **approximate nearest neighbor**
    (**ANN**) search in high-dimensional spaces, utilizing a multi-layer graph structure
    to organize data points. Each layer represents a different level of proximity,
    with higher layers containing fewer, more broadly representative points and lower
    layers containing more detailed points. This navigable graph supports efficient
    search by starting from the top layer and moving downward to find closer neighbors,
    leveraging “small world” properties for quick access across the dataset. HNSW
    provides fast and accurate ANN searches, making it ideal for applications such
    as recommendation systems, image retrieval, and natural language processing.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pgvector` extension available on Azure PostgreSQL flexible servers, you can
    seamlessly integrate vector capabilities into your PostgreSQL environment. `pgvector`
    is an open source extension tailored for PostgreSQL, enabling the storage and
    retrieval of vectors from natural language processing or deep learning models
    directly within PostgreSQL. What makes `pgvector` particularly appealing is its
    familiar SQL-based interface, mirroring traditional PostgreSQL operations for
    tasks such as creating vector columns, defining tables with vector columns, and
    performing nearest neighbor searches using L2 distance. Whether you’re developing
    AI applications, creating recommendation systems, or handling high-dimensional
    data, `pgvector` simplifies vector management within a familiar database framework,
    eliminating the need for specialized storage solutions and extensive vector database
    expertise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure SQL**: Azure SQL Database now supports vector operations directly within
    the database, enabling efficient vector similarity searches. This capability,
    combined with full-text search and BM25 ranking, allows for the development of
    powerful search engines suitable for various applications. There are two methods
    to perform vector operations: the native option and the classic option:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Native option**: Utilize the newly introduced Vector Functions in Azure SQL
    Database. These functions are designed to perform vector operations directly within
    the database, providing a streamlined and efficient approach.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classic option**: Use traditional T-SQL for vector operations, leveraging
    columnstore indexes to achieve high performance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both options offer robust solutions for implementing vector searches, making
    Azure SQL Database a versatile tool for advanced search scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Microsoft’s recent announcement of *SQL Server 2025* marks a significant step
    in database evolution, introducing it as an enterprise-ready *vector database*.
    This release integrates *built-in security and compliance*, emphasizing its focus
    on supporting enterprise-grade *AI solutions*. A standout feature is the *native
    vector store and index*, which is powered by DiskANN and leverages disk storage
    to execute high-performance searches across vast datasets. This functionality
    underpins semantic searching, enabling efficient chunking and accurate data retrieval
    – a critical feature for AI-driven insights. This advancement enables efficient
    handling of high-dimensional data, making it well-suited for AI workloads such
    as recommendation systems, natural language processing, and image search.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`RediSearch` module offers comprehensive search capabilities, including various
    distance metrics such as Euclidean, cosine, and Inner Product, and supports both
    KNN with FLAT indexing and ANNs using HNSW indexing. It allows vector storage
    in hash or JSON formats and supports top-K and vector range queries to find items
    within a specific vector distance. Additionally, Redis enhances search functionalities
    with advanced features such as geospatial filtering, numeric and text filters,
    prefix and fuzzy matching, phonetic matching, and Boolean queries. Often considered
    a cost-effective solution, Redis is widely used for caching or session storage,
    enabling it to handle both traditional caching roles and vector search applications
    concurrently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most Azure vector databases, such as AI Search, CosmosDB, and Azure Managed
    Redis, are integrated with LLM frameworks such as Semantic Kernel, LangChain,
    and LlamaIndex to facilitate easy vector creation and ingestion into their respective
    services. These integrations streamline the process of embedding generation, storage,
    and retrieval, enabling efficient handling of vector data and enhancing the capabilities
    of GenAI applications.
  prefs: []
  type: TYPE_NORMAL
- en: Azure also offers a no-code solution for creating vector embeddings and automatically
    ingesting them into Azure AI Search or Cosmos DB as part of its native features.
    In the next section, we will explore this capability as part of *AOAI On* *Your
    Data*.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI On Your Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A common application in various industries is the creation of personalized chatbots
    utilizing their own enterprise data through generative AI. Traditionally, this
    required customers to manually code processes for extracting text from unstructured
    data, generate embeddings, and store them in a vector database, which was both
    time-consuming and labor-intensive for developers. However, the Azure OpenAI On
    Your Data feature significantly simplifies this workflow, allowing developers
    to achieve the same results with minimal or no coding. This means that building
    a chatbot application can now be accomplished with just a few clicks.
  prefs: []
  type: TYPE_NORMAL
- en: This functionality operates behind the scenes using a RAG technique, as illustrated
    in *Figure 3**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: AOAI On Your Data RAG architecture](img/B21019_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: AOAI On Your Data RAG architecture'
  prefs: []
  type: TYPE_NORMAL
- en: When you upload a collection of documents in various formats, those are then
    divided into smaller chunks. Each chunk is converted into an embedding, such as
    *text-embedding-ada-002*, using AOAI’s embedding model. These embeddings are stored
    in an AI search vector database, utilizing an HNSW index, which is configured
    for semantic search. Users can input queries through GPT4-o chat, which are also
    transformed into embeddings using the same embedding model. Then a similarity
    search is conducted to find the most relevant document vectors using ANN techniques
    on the HNSW index. Next, the retrieved context and the raw user query are provided
    to the GPT models to generate responses to user questions.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Azure RBAC ensures secure access and permissions throughout the
    system. The entire process is streamlined and user-friendly, allowing you to focus
    on utilizing the insights without dealing with the technical complexities.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that the Azure AI Search resource is set up before utilizing the On your
    Data feature. To set up Azure AI Search resource, refer to the instructions provided
    at [https://learn.microsoft.com/en-us/azure/search/search-create-service-portal](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s demonstrate how you can utilize this feature effortlessly without
    needing to write any code:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the Azure AI Foundry Portal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you haven’t set up a chat model and an embedding model yet, go to the `text-embedding-ada-002`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to **Chat** from the **Playgrounds** menu and select the **Add your**
    **data** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Add a** **data source**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have several choices for data sources. In this demonstration, we’re uploading
    the file manually. Alternatively, you can opt for a direct blob storage account
    if you have existing data there. If you have an AI search index where data is
    pre-indexed and vectorized, you can connect to that index as well.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select the appropriate subscriptions and the blob storage account where your
    uploaded data will be stored. Next, choose the **Azure AI Search resource** option
    you’ll use for storing the vector index, and finally, specify the index name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tick the checkbox to enable vector search for this AI search resource. Refer
    to *Figure 3**.3* for details.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.3: AOAI On Your Data settings](img/B21019_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: AOAI On Your Data settings'
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next screen, upload a PDF file. You also have the option to upload files
    in `.txt`, `.md`, `.html`, `.pdf`, `.docx`, or `.pptx` formats. In this instance,
    I’m uploading a hotel invoice PDF receipt. Refer to *Figure 3**.4* for a sample
    view of the receipt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.4: Sample data for AOAI On Your Data](img/B21019_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Sample data for AOAI On Your Data'
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the **Search type** option you’d like to use. You have three options
    to choose from:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Vector**: This finds documents similar to a query using vector embeddings.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid** (vector + keyword): This combines similarity search over vector
    fields with keyword-based full-text search.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid + semantic**: This utilizes vector embeddings, language understanding,
    and flexible query parsing for advanced search experiences. This option is highly
    recommended due to its superior search quality.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the right type enhances your search capabilities based on your specific
    needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Select the chunk size. Chunking involves dividing documents into smaller segments
    for efficient search and retrieval, with chunk size measured in tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Next**. On the following screen, choose the type of authentication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The time required for indexing may increase with document size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **Deployment** option to use and fill in the **System message** field
    as shown in *Figure 3**.5*. Then click **Apply changes**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.5: On Your Data settings](img/B21019_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: On Your Data settings'
  prefs: []
  type: TYPE_NORMAL
- en: Now you are all set to start the chat from **Chat playground**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refer to *Figure 3**.6* for a sample question. You can ask questions based on
    the document provided to receive accurate answers. You can click on the references
    to view the citations for the answers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.6: Sample questions and answers, with references](img/B21019_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Sample questions and answers, with references'
  prefs: []
  type: TYPE_NORMAL
- en: You’ve observed how the answer is grounded on your data using the AOAI On Your
    Data feature. If you’re looking to develop a web application and deploy it to
    production, simply click **Deploy as a web app** and fill in the app service details
    as shown in *Figure 3**.7*. Then click on **Deploy** to create a web application
    for users.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: Chat On Your Data web app settings](img/B21019_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Chat On Your Data web app settings'
  prefs: []
  type: TYPE_NORMAL
- en: 'After deployment, you’ll receive a public URL for the application, enabling
    end users to interact with the document they uploaded, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: Web application view](img/B21019_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Web application view'
  prefs: []
  type: TYPE_NORMAL
- en: This simplifies the process, allowing you to make your chat application available
    to end users with just a few clicks without needing to write any code. It’s an
    efficient way to deploy and share your application quickly. However, it’s important
    to note that while this feature works well for small **proofs of concept**, developing
    a robust production application requires careful consideration of various factors
    for improved accuracy. These include strategies for chunking, query rewriting,
    and designing custom prompt templates. These capabilities are somewhat limited
    in this regard, often necessitating a code-first approach to create a more sophisticated
    and tailored solution.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, we’ve utilized AOAI models for text-related scenarios. However,
    there are instances where image understanding is required. In the next section,
    we’ll explore AOAI multimodal capabilities, where the input is an image and the
    output is text. This approach broadens the applicability of AOAI by incorporating
    visual data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI multimodal model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AOAI multimodal capabilities can be applied in various real-world scenarios,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image captioning**: Automatically generating descriptive text for images,
    which is useful in organizing digital photo collections or assisting visually
    impaired users'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Audio processing**: Low-latency conversational use cases requiring real-time
    engagement between a user and a model, such as customer support chatbots, voice
    assistants, and live translation services'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual question answering**: Responding to questions about an image, which
    can enhance interactive educational tools or customer support systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content moderation**: Analyzing images to detect inappropriate or harmful
    content, improving safety on social media platforms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E-commerce**: Providing product descriptions from images, aiding in cataloging
    and improving user search experiences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Healthcare**: Assisting in medical diagnostics by interpreting medical images
    and providing preliminary reports'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These use cases demonstrate the versatility of integrating both visual and
    textual data processing. To enable such functionalities, AOAI provides a GPT-4
    category model with built-in multimodal features. Presently, AOAI offers three
    distinct models within the GPT-4 family that support these native multimodal capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT4-Turbo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT4-o
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT4-o-mini
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s test the multimodal capability of GPT4-o from **Chat playground**:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the Azure AI Foundry Portal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the **Chat from** **Playgrounds** menu.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload an image to the chat and ask a question about it. Alternatively, you
    can achieve this through the SDK by providing a base64-encoded image as input
    to the model. For illustration purposes, I’ve used *Figure 3**.9*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.9: GPT4-o sample image](img/B21019_03_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: GPT4-o sample image'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can ask questions such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"What is the dimension of the` `CB line?"`'
  prefs: []
  type: TYPE_NORMAL
- en: 'While AOAI GPT4-o’s vision capabilities are highly versatile and useful across
    various applications, it is crucial to recognize its limitations. Here are several
    key constraints to be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Medical images**: The model is not designed for interpreting specialized
    medical images such as CT scans and should not be used for medical advice'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-English text**: Performance may decline when dealing with images containing
    text in non-Latin alphabets, such as Japanese or Korean'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Small text**: To improve readability, enlarge small text within images, but
    ensure that no important details are cropped out'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rotation issues**: The model may misinterpret text or images that are rotated
    or upside down'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual elements**: Understanding graphs or text with varying colors or styles,
    such as solid, dashed, or dotted lines, can be challenging for the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spatial reasoning**: Tasks requiring precise spatial localization, such as
    identifying chess positions, are difficult for the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy**: In certain situations, the model may produce incorrect descriptions
    or captions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image shape**: Panoramic and fisheye images pose difficulties for the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata and resizing**: The model does not process original file names or
    metadata, and images are resized before analysis, potentially affecting their
    original dimensions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Counting**: The model may provide approximate counts of objects in images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CAPTCHAs**: For safety reasons, the system is configured to block the submission
    of CAPTCHAs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding these limitations can help customers set realistic expectations
    and ensure the model is applied appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen the capabilities and limitations of GPT4-o’s vision feature,
    let’s understand the cost model for image inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Image token cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Image inputs are measured and charged in tokens, much like text inputs. The
    token cost for an image is influenced by two main factors: its dimensions and
    the detail level specified for each image URL block. Here is a detailed breakdown:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low detail**: Each image with **Detail** set to **Low** costs 85 tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High detail**: For high-detail images, the process is more complex. First,
    the image is scaled to fit within a 2,048 x 2,048 square while maintaining its
    aspect ratio. Then, it is scaled again so that its shortest side is 768 pixels
    long. The number of 512-pixel squares that make up the image is then counted,
    and each of these squares costs 170 tokens. An additional 85 tokens are always
    added to the final total.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1,024 x 1,024 image in** **high detail**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No initial resizing is needed since 1,024 is less than 2,048
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The image is scaled down to 768 x 768, as the shortest side is 1,024
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This requires four tiles of 512 pixels each
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The token cost calculation is as follows: 170 tokens/tile * 4 tiles + 85 tokens
    = 765 tokens.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2,048 x 4,096 image in** **high detail**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image is first scaled to 1,024 x 2,048 to fit within the 2,048 square
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The shortest side is then 1,024, so it is further scaled to 768 x 1,536
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This requires 6 tiles of 512 pixels each
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The token cost calculation is as follows: 170 tokens/tile * 6 tiles + 85 tokens
    = 1,105 tokens'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**4,096 x 8,192 image in** **low detail**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regardless of the input size, low-detail images have a fixed cost
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The token cost is 85 tokens
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding this token cost structure can help manage resource usage effectively
    when working with image inputs.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve explored the self-contained capabilities of AOAI models. However,
    there are enterprise use cases where customers need to integrate LLMs with external
    systems or tools. This allows for the conversion of natural language queries into
    executable structured inputs for those systems. In the next section, we will discuss
    how developers can achieve this functionality using AOAI’s function calling feature.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI function calling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AOAI function calling allows you to connect models such as GPT-4o and other
    GPT models to external tools and systems. This is beneficial for tasks such as
    enhancing AI assistants’ abilities or creating seamless integrations between your
    applications and the models.
  prefs: []
  type: TYPE_NORMAL
- en: This feature doesn’t directly run functions for you. Instead, you define the
    functions in the API call, and the model figures out how to create the needed
    arguments. After these arguments are generated, you can use them to execute functions
    within your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Function calling is beneficial for numerous applications, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Allowing assistants to retrieve information**: An AI assistant might need
    to access the latest customer data from an internal system (such as Azure Cosmos
    DB or Azure SQL) to answer a user’s query about recent orders'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enabling assistants to execute tasks**: An AI assistant can arrange meetings
    by considering user preferences and calendar availability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assisting with computations**: A math tutor assistant can perform calculations
    as needed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creating complex workflows**: For instance, a data extraction process can
    gather raw text, convert it to structured data, and store it in a database'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Altering your application’s user interface**: Function calls can update the
    UI based on user actions, such as displaying a pin on a map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s talk about the life cycle of the AOAI function call.
  prefs: []
  type: TYPE_NORMAL
- en: Function call life cycle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are five different stages of function calls, as can be seen in *Figure
    3**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: Your code initiates the process by calling the API with a prompt and the functions
    the LLM can access.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model evaluates whether to respond directly to the user or whether one or
    more functions need to be invoked.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The API replies to your application, specifying which function should be called
    and what the necessary arguments are.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your application runs the specified function using the provided arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your application communicates back to the API with the initial prompt and the
    outcome of the executed function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.10: Function call life cycle](img/B21019_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Function call life cycle'
  prefs: []
  type: TYPE_NORMAL
- en: When using the OpenAI API with function calling, the model itself doesn’t execute
    the functions. Instead, in *Step 3*, the model generates parameters for your function,
    which your application can use. Your code decides how to handle these parameters,
    typically by calling the specified function. This ensures that your application
    retains complete control over the execution process.
  prefs: []
  type: TYPE_NORMAL
- en: Function calling is supported in the Chat Completions, Assistants, and Batch
    APIs. This section focuses on function calling using the Chat Completions API.
    The Assistant and Batch APIs will be covered in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Define the function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Start by specifying the function you intend to invoke. This function should
    be a Python function capable of accepting inputs and providing outputs. The inputs
    of the function will be generated by the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, suppose you wish to enable the model to execute the `get_weather`
    function within your codebase. This function takes a city as an argument to retrieve
    weather information from a weather API. Your function might look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11: get_weather function definition](img/B21019_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.11: get_weather function definition'
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Describe the function for model use
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have identified the function we want the model to call, we will
    develop a *function definition*. This will explain what the function accomplishes,
    when it might be used, and what parameters are needed to invoke it.
  prefs: []
  type: TYPE_NORMAL
- en: The parameters section in your function definition should be outlined using
    JSON Schema. When the model generates a function call, it will refer to this schema
    to create arguments appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, it may look like *Figure 3**.12.*
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12: JSON schema of the function](img/B21019_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: JSON schema of the function'
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Provide function definitions as “tools” to the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we need to include our function definitions within an array of available
    *tools* when using the Chat Completions API. As usual, we’ll provide an array
    of *messages*, which might include your prompt or an entire dialogue between the
    user and an assistant.
  prefs: []
  type: TYPE_NORMAL
- en: This example, as shown in *Figure 3**.13*, illustrates how you might call the
    Chat Completions API, supplying relevant functions and messages to properly generate
    the function call with a unique ID.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13: Generating the function call](img/B21019_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: Generating the function call'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the model is set to automatically decide which functions to invoke,
    based on the `tool_choice`: “`auto`” configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We provide three options to modify this default behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To mandate the model to always call one or more functions, you can set `tool_choice:
    "required"`. This ensures that the model will always select at least one function
    to execute, which can be useful when you want the model to choose between various
    actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To direct the model to use a specific function, you can define `tool_choice:
    {"type": "function", "function": {"name": "my_function"}}`. This will force the
    model to only call the specified function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To prevent any function calls and have the model respond with only a user-facing
    message, you can either omit tools entirely or set `tool_choice: "none"`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 4 – Making the actual function call
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As previously discussed, the AOAI function call doesn’t automatically trigger
    the actual function. Instead, you must add the code to trigger the call based
    on the input parameters generated by the model. To do this, refer to the code
    shown in *Figure 3**.14*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14: Making the function call](img/B21019_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: Making the function call'
  prefs: []
  type: TYPE_NORMAL
- en: When you include a function in your request, the function’s details (such as
    its definition and parameters) become part of the system message, which is then
    processed by the model along with the user input. This integration enables the
    model to assess whether the function should be invoked based on the prompt context.
  prefs: []
  type: TYPE_NORMAL
- en: This process does use tokens, as the function definition and parameters contribute
    to the overall token count. Therefore, employing prompt engineering strategies
    such as being concise, excluding unnecessary details, and focusing on essential
    parts of the prompt can improve function call efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some more ways to optimize function call efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Add more detail to your function definition**: Including detailed function
    definitions with meaningful descriptions is crucial for clarity and efficient
    function invocation. When defining a function, each parameter should be described
    in a way that both the model and any human reviewing the code can easily understand.
    Here’s a breakdown of how to make function definitions more comprehensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`search_hotels`, you could set a system message such as the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This informs the model when to invoke the function based on user input.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`search_hotels`, ask for location details if the user request is missing them.
    Include instructions such as this in your system message to guide the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Error handling**: Another key aspect of prompt engineering is minimizing
    errors in function calls. While models are trained to generate function calls
    according to your defined schema, they may sometimes create calls that don’t align
    with it or attempt to invoke functions that aren’t included.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To address this, you can add a statement such as the following in the system
    message:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This helps ensure that the model adheres strictly to the functions you’ve defined.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that you understand how to call AOAI functions, the next section will focus
    on the AOAI Assistants API, which simplifies the application development process
    for developers.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI Assistants API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The AOAI Assistants API enables the creation of AI-powered assistants that
    can be integrated directly into your own applications. These assistants operate
    based on a set of predefined instructions and can interact with users by utilizing
    various capabilities such as models, tools, and files. Currently, the Assistants
    API supports three key types of tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Code Interpreter**: This feature empowers the assistant to handle user requests
    for computations or script execution by allowing it to write and run Python code
    in a secure, multi-tenant Kubernetes environment. The Kubernetes sandbox uses
    nested hypervisor technology to isolate each container, offering a unique user-space
    kernel rather than a traditional kernel. This setup enhances security by isolating
    environments, reduces risks by preventing cross-container interference, and improves
    system flexibility. By facilitating code execution within a safe, virtualized
    environment, the assistant can dynamically respond to complex calculations, data
    processing, and file handling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With support for diverse data formats ([https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python#supported-file-types](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/code-interpreter?tabs=python#supported-file-types)),
    this tool can process files of various structures, extracting information as needed.
    Code Interpreter enables iterative code execution, making it possible for the
    assistant to adjust code and retry execution if initial attempts fail, which is
    particularly helpful for complex coding and mathematical problems. Common use
    cases include extracting data from CSV files, creating structured data visualizations
    such as charts and graphs, and solving math problems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**File Search**: The AOAI File Search tool allows the assistant to enhance
    file-based query handling by accessing and retrieving information from user-provided
    documents. Acting as an external knowledge base, it lets the assistant search
    beyond model-trained data to include proprietary content or other document-based
    information. AOAI’s system automatically chunks down as per chunking strategies
    and indexes documents by creating vector embeddings and storing them within a
    managed vector store powered by Azure AI Search. This enables both vector-based
    and keyword searches, facilitating more precise, context-driven information retrieval
    that supports diverse formats, as detailed at [https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/file-search?tabs=python#supported-file-types](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/file-search?tabs=python#supported-file-types).
    By transforming document content into vector embeddings, File Search allows searches
    to understand context and meaning instead of simply matching exact keywords. This
    capability is especially useful for tasks such as detailed Q&A, summarization,
    and data extraction where nuanced understanding and quick retrieval are required.
    The tool is also versatile, handling a wide array of document formats to enable
    easy integration into workflows requiring complex document interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Function calling**: This tool allows the assistant to invoke specific functions
    within an application, enabling it to execute tasks or retrieve data through API
    interfaces. We previously covered function calling in detail, highlighting its
    role in enhancing the assistant’s interactive capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools make it easier to build dynamic and responsive AI assistants tailored
    to specific use cases within an application.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss the process flow assistants.
  prefs: []
  type: TYPE_NORMAL
- en: Assistant process flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Setting up and running an AI assistant such as a finance bot generally involves
    four key steps. Using the finance bot as an example, and as shown in *Figure 3**.15,*
    the steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15: AOAI assistants’ process flow](img/B21019_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: AOAI assistants’ process flow'
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – create Assistants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An Assistant is an entity that can be tailored to respond to user inputs using
    various parameters such as models, instructions, and tools. Assistants can use
    OpenAI models via AOAI and can be customized to suit different personalities or
    capabilities based on provided instructions. You can equip the Assistant with
    tools, either pre-built ones such as `code_interpreter` and `file_search`, or
    custom ones via function calling. These tools help the Assistant perform specific
    tasks based on the user’s queries. The Assistants API has support for several
    parameters that let you customize the Assistants’ output. The `tool_choice` parameter
    lets you force the Assistant to use a specified tool.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – create a Thread
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Thread acts as a record of the conversation between the user and one or more
    Assistants. When a user (or AI application) initiates a conversation, a new Thread
    is created. The assistant can access persistent Threads, enabling continuous conversation
    without losing context. This simplifies AI application development by retaining
    message history and managing memory efficiently by truncating older data when
    the conversation exceeds the model’s context window. A Thread is created once
    and updated as new messages are exchanged.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – add a message to the Thread
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The messages exchanged during the conversation, whether from the user or the
    application, are stored as **Message** objects within the Thread. These messages
    can include both text and files. While a Thread can store up to 100,000 messages,
    the system intelligently manages the conversation by automatically truncating
    any content that exceeds the model’s context window. Assistants are designed to
    truncate text automatically to ensure that the conversation stays within the model’s
    maximum token limit. However, you can adjust this behavior by specifying how many
    tokens or recent messages should be included in each Run.
  prefs: []
  type: TYPE_NORMAL
- en: To manage token usage during a single Run, you can set `max_prompt_tokens` and
    `max_completion_tokens` at the start. These limits apply to all completions made
    throughout the Run. For instance, if `max_prompt_tokens` is set to `500` and `max_completion_tokens`
    to `1000`, the assistant will first truncate the prompt to fit within 500 tokens
    and cap the output at 1,000 tokens. If only 200 tokens are used for the prompt
    and 300 tokens for the completion, the next completion will have 300 prompt tokens
    and 700 completion tokens available.
  prefs: []
  type: TYPE_NORMAL
- en: If the completion hits the `max_completion_tokens` limit, the Run will stop
    with an incomplete status, and the reason will be included in the `incomplete_details`
    field of the `Run` object.
  prefs: []
  type: TYPE_NORMAL
- en: When using the File Search tool, it’s recommended to set `max_prompt_tokens`
    to no fewer than `20000`. For more extensive conversations or multiple interactions
    using File Search, consider increasing this limit to `50000` or even removing
    it entirely for optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also define a **truncation strategy** to control how the Thread should
    fit within the model’s context window:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `auto` truncation strategy will apply OpenAI’s default truncation
    behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `last_messages` strategy lets you specify how many of the most recent
    messages should be included in the context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach gives you more control over managing conversation length and ensuring
    optimal performance in each Run.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – create a Run
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After all user messages are added to the Thread, the conversation is processed
    by initiating a Run. The Run utilizes the models and tools defined for the Assistant
    to generate a response. The assistant’s response is then added to the Thread as
    a new message, continuing the flow of conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the assistant’s API process flow, let’s proceed with
    setting up the assistant.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI Assistants – code interpreter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll guide you through the step-by-step process of setting
    up an assistant in the Azure AI Foundry Portal and demonstrate how to use the
    code interpreter tool to handle user queries on CSV data. You can achieve the
    same functionality with an API-based approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to the Azure AI Foundry Portal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you haven’t yet set up a chat model, go to the **Deployments** section in
    the **Shared resources** menu. From there, initiate the deployment of a new chat
    model such as **GPT4-o**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to the **Assistants from Playgrounds** menu, choose the **GPT4-o**
    deployment, and click **Create** **an assistant**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **Setup** page, you’ll see that an assistant ID has been created. Here,
    you can assign a name to the assistant and write custom instructions to clearly
    define its objectives, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.16: Assistant setup](img/B21019_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16: Assistant setup'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Tools** section, enable **Code Interpreter** and upload a CSV file
    to query. For this example, we’ll use a sample CSV file containing retail order
    data. A snippet of this file is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.17: Sample CSV file](img/B21019_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.17: Sample CSV file'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the CSV data is uploaded, you can start by asking relevant queries. For
    example, you could ask: `"How many orders have been shipped so far?"`. Alternatively,
    you can also use the following prompt: `"Create a chart with order status on the
    x-axis and quantity on` `the y-axis."`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For this type of question, the Code Interpreter tool will execute Python code
    within a Microsoft-managed sandbox environment and provide you with the results,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18: Output of Code Interpreter](img/B21019_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.18: Output of Code Interpreter'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover how you can integrate File Search capabilities
    into the assistant using the SDK approach.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI Assistants – File Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the OpenAI SDK, you can directly program the assistant to search documents
    and extract specific function signatures as needed. File Search will let the Assistant
    parse, chunk, and index documents for efficient retrieval by leveraging the SDK.
    You can configure these tools seamlessly within your application, enhancing the
    assistant’s data retrieval and interactive capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create a new assistant with File Search as part of its set
    of tools, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.19: Creating the Assistant](img/B21019_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.19: Creating the Assistant'
  prefs: []
  type: TYPE_NORMAL
- en: 'Upload your files to enable automated chunking (with a chunk size of 800 tokens
    and a chunk overlap of 400 tokens). Embed and create a vector store that powers
    the file search tool, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.20: Vector store creation](img/B21019_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.20: Vector store creation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Attach the vector store to the assistant to give access to the files, as shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.21: Attaching vector store to the assistant](img/B21019_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.21: Attaching vector store to the assistant'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a thread and run the assistant, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.22: Create a thread and run](img/B21019_03_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.22: Create a thread and run'
  prefs: []
  type: TYPE_NORMAL
- en: 'Display the assistant response with citations, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.23: Assistant response with citations](img/B21019_03_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.23: Assistant response with citations'
  prefs: []
  type: TYPE_NORMAL
- en: Now, you can see how simple it is to call the Assistant API with various tools
    such as File Search and Code Interpreter. Additionally, you can use the Assistant
    API to interact with external systems through the function calling feature. These
    assistants can take advantage of OpenAI’s advanced language models, utilize tools
    such as Code Interpreter and File Search, and retain context throughout conversations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss AOAI Batch, which is designed for use cases
    that do not require real-time processing.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI Batch API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some applications require synchronous request handling, also known as real-time
    inferencing, where immediate responses are necessary. However, there are numerous
    situations wherein responses can be deferred or rate limits may restrict the speed
    at which multiple queries can be processed. In such cases, batch processing jobs
    prove useful, particularly for tasks such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Large-scale data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating large volumes of content transforming data at scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating LLM models and assess comprehensive performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AOAI Batch API provides a user-friendly suite of endpoints. These allow
    you to bundle multiple requests into a single file, initiate a batch job to process
    these requests asynchronously, check the batch’s status as the tasks run, and,
    finally, retrieve the consolidated results once processing is complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to traditional PAUG deployments, the Batch API offers the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost efficiency**: Provides a 50% cost reduction relative to standard PAUG
    deployment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dedicated quota**: Operates with a separate enqueued token quota, distinct
    from the online endpoint quota, ensuring that online workloads remain unaffected;
    the batch quota is also significantly larger'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**24-hour turnaround**: Each batch completes within 24 hours, often achieving
    results even faster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Submitting a batch and retrieving the results involves a six-step process.
    Let’s go through each step in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch deployment creation**: You need to first create a separate deployment
    for the batch. To do that, follow these steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to Azure AI Foundry.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to **Deployments** under **Shared resources**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Deploy model** and choose **Deploy** **base model**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose any chat completion model such as gpt-4o-mini and click **Confirm**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a value under **Deployment name**, set the **Deployment** type to **Global
    Batch**, adjust the **Enqueued tokens** value to the max limit, and click **Deploy**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.24: Batch deployment](img/B21019_03_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.24: Batch deployment'
  prefs: []
  type: TYPE_NORMAL
- en: You can also toggle **Enable dynamic quota**, which allows you to utilize additional
    quota when extra capacity is available.
  prefs: []
  type: TYPE_NORMAL
- en: After completing the preceding steps, your **Global Batch** deployment will
    be created. This deployment will then be used to run the batch job.
  prefs: []
  type: TYPE_NORMAL
- en: '`.jsonl` file, where each line specifies the details of an individual API request.
    Currently, the supported endpoints are `/chat/completions` (for the Chat Completions
    API).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this input file, the parameters in the body field of each line should match
    the parameters for the corresponding endpoint. Each request must include a unique
    `custom_id` value, which will help reference the results once processing is complete.
    The `model` name in the file should match the deployment name you created in the
    previous step.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following is an example of an input file, as shown below, containing three
    requests. Each input file must be limited to requests for a single model only:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.25: Batch input .jsonl file](img/B21019_03_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.25: Batch input .jsonl file'
  prefs: []
  type: TYPE_NORMAL
- en: '**Upload a batch input file**: After preparing your input file, you’ll need
    to upload it before starting a batch job. You can upload the file either programmatically
    or through the **Studio** interface. In this example, we are using the Python
    SDK approach to upload the file from the local drive. See *Figure 3**.26* for
    reference.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.26: Batch file upload](img/B21019_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.26: Batch file upload'
  prefs: []
  type: TYPE_NORMAL
- en: '`File` object to initiate a batch. In this example, the file ID is `file-0a27a5cd4d94440789971497e6d80391`.
    Currently, the completion window is fixed at 24 hours. You can also include custom
    metadata using the optional `metadata` parameter, as shown in *Figure 3**.27*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will return a batch object with a batch ID and status field. You can find
    the complete details of the batch object at [https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&pivots=programming-language-python#batch-object](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&pivots=programming-language-python#batch-object).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.27: Batch job submission](img/B21019_03_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.27: Batch job submission'
  prefs: []
  type: TYPE_NORMAL
- en: '**Track the batch job status**: After successfully creating the batch job,
    you can monitor its progress either through **Studio** or programmatically. When
    checking the status, it is recommended to wait at least 60 seconds between each
    status call, as shown in *Figure 3**.28*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.28: Batch job status check](img/B21019_03_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.28: Batch job status check'
  prefs: []
  type: TYPE_NORMAL
- en: The status of a given Batch object can be any of the following shown in *Table
    3.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Status** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `validating` | The input file is being validated before the batch can start
    |'
  prefs: []
  type: TYPE_TB
- en: '| `failed` | The input file failed the validation process |'
  prefs: []
  type: TYPE_TB
- en: '| `in_progress` | The input file was validated successfully, and the batch
    is running |'
  prefs: []
  type: TYPE_TB
- en: '| `finalizing` | The batch has finished, and the results are being prepared
    |'
  prefs: []
  type: TYPE_TB
- en: '| `completed` | The batch has been completed, and the results are ready |'
  prefs: []
  type: TYPE_TB
- en: '| `expired` | The batch was not completed within the 24-hour time frame |'
  prefs: []
  type: TYPE_TB
- en: '| `cancelling` | The batch is in the process of being canceled (may take up
    to 10 minutes) |'
  prefs: []
  type: TYPE_TB
- en: '| `cancelled` | The batch was canceled |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.1: Batch job status table'
  prefs: []
  type: TYPE_NORMAL
- en: '`output_file_id` from the `Batch` object. Then save it to a file on your machine,
    such as `batch_output.jsonl`. The `output .jsonl` file will contain one response
    per successful request from the input file. Any failed requests will include their
    error details in a separate error file, accessible via the batch’s `error_file_id`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The order of output lines may not match the input order. Instead of relying
    on the sequence, use the `custom_id` field present in each output line to correlate
    input requests with their corresponding results.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29: Retrieving the output file](img/B21019_03_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.29: Retrieving the output file'
  prefs: []
  type: TYPE_NORMAL
- en: By following the steps outlined, you can submit batch jobs manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'While these steps are suitable for demonstration purposes, enterprises with
    millions of files in a blob storage account will require an automated solution
    to submit batches and retrieve results efficiently. For such cases, you can use
    the following solution accelerator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Azure-Samples/aoai-batch-api-accelerator](https://github.com/Azure-Samples/aoai-batch-api-accelerator)'
  prefs: []
  type: TYPE_NORMAL
- en: The AOAI Batch API has certain service limits, which can be found at [https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&pivots=programming-language-python#global-batch-limits](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&pivots=programming-language-python#global-batch-limits).
    Quota limits are outlined at [https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&pivots=programming-language-python#global-batch-quota](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/batch?tabs=standard-input%2Cpython-key&pivots=programming-language-python#global-batch-quota).
    These limits can be increased based on your workload; to request an increase,
    you will need to contact Microsoft.
  prefs: []
  type: TYPE_NORMAL
- en: You can also process images in bulk using the AOAI Batch API. This feature is
    available with specific multi-modal models, and currently, only GPT-4o supports
    images in batch requests. Images can be provided either as a URL or as base64-encoded
    data. Note that GPT-4 Turbo does not support image inputs for batch processing
    at this time.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI batch processing is global by nature, meaning data processing could occur
    anywhere in the world. This may raise concern for industries with strict regulatory
    requirements. However, you can select a data zone for the AOAI Batch, which restricts
    data processing to specific geos. By choosing the US data zone, processing will
    occur in one of the US regions, and by selecting the EU data zone, processing
    will take place in one of the EU regions. This ensures enterprises can comply
    with data compliance and regulatory requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the AOAI Batch API, the next section will focus on fine-tuning,
    which allows you to customize your model for specific tasks.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fine-tuning allows you to maximize the potential of models available through
    the Azure AI Foundry or API by providing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Improved response quality compared to basic prompting alone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The capability to train on larger datasets, surpassing the limitations model
    context window
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduced token usage by minimizing the prompt length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster response times with lower-latency requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOAI’s text generation models are pre-trained on an extensive corpus of text
    data. To use them effectively, users often include instructions and example cases
    in prompts—a method known as **few-shot learning**. Few-shot learning demonstrates
    how to complete a task by showing a small number of examples within the prompt
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning enhances the few-shot approach by training the model on significantly
    more examples than could fit into a single prompt, thereby improving performance
    across a wider range of tasks. After a model is fine-tuned, fewer examples are
    typically needed in the prompt, which reduces token costs and further lowers response
    latency.
  prefs: []
  type: TYPE_NORMAL
- en: Ideal situation to leverage fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When evaluating whether fine-tuning is the appropriate approach for a specific
    use case, it’s beneficial to understand some foundational concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt engineering**: This technique involves crafting prompts for natural
    language processing models with precision. By designing prompts carefully, users
    can improve the accuracy and relevance of model responses, enhancing overall performance.
    Chapter 13 provides an in-depth exploration of various prompt engineering techniques.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RAG**: RAG enhances the effectiveness of LLMs by integrating external data
    into the prompt. By retrieving relevant information from outside sources, RAG
    enables businesses to create tailored solutions that are both cost-effective and
    contextually accurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning**: Fine-tuning involves adapting an existing LLM by retraining
    it on specific example data, resulting in a *custom* model that is fine-tuned
    to reflect the nuances and requirements of the provided examples data set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOAI fine-tuning is a **supervised fine-tuning** process, as opposed to continuous
    pre-training or **Reinforcement Learning Through Human Feedback** (**RLHF**).
    Supervised fine-tuning involves retraining pre-trained models on carefully selected
    datasets to enhance performance on particular tasks. We recommend starting with
    techniques such as prompt engineering, prompt chaining (dividing complex tasks
    into smaller, manageable prompts), and utilizing function calling to achieve optimal
    results.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI uses **Low-Rank Approximation** (**LoRA**) to fine-tune models efficiently
    by lowering their complexity with minimal impact on performance. This approach
    approximates the model’s original high-dimensional matrix using a lower-dimensional
    one, allowing only a subset of key parameters to be fine-tuned during supervised
    training. By focusing on these *essential* parameters, the model remains both
    efficient and easier to manage. For users, this results in faster training and
    more cost-effective fine-tuning compared to traditional methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-tuning is a sophisticated process that demands a solid domain and data
    understanding to apply effectively. The five common questions that follow are
    designed to help you assess your readiness for fine-tuning, guiding you through
    key considerations and helping you decide whether fine-tuning is the best approach
    or whether alternative methods may be more suitable:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Why fine-tune** **a model?**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To proceed effectively with fine-tuning, you should clearly define a specific
    use case and identify the model you intend to fine-tune. Good candidates for fine-tuning
    include situations where you need the model to produce outputs in a particular
    style, tone, or format or when the instructions or data needed to guide the model
    are too complex or lengthy to fit into a standard prompt.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here are some indicators that you may not be ready for fine-tuning yet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Unclear use case**: If you can’t articulate a clear purpose beyond *I want
    to improve a model*, fine-tuning may not be the right next step.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-driven motivation**: Fine-tuning can reduce costs in cases where it
    allows for shorter prompts or smaller model usage. However, it also involves an
    upfront cost for training and hosting a custom model. Be mindful of these expenses
    and refer to AOAI’s pricing page for more detail on fine-tuning costs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Out-of-domain knowledge needs**: If your primary goal is to incorporate information
    beyond the model’s original training scope, consider starting with RAG. AOAI’s
    RAG features, such as embedding-based retrieval on your data, can offer a more
    flexible and often more affordable solution depending on your specific data and
    objectives.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What have you tried** **so far?**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning is an advanced capability and is not typically the first step in
    working with generative AI. It’s essential to be familiar with the fundamentals
    of LLMs and to start by testing the model’s performance with prompt engineering
    and/or RAG. These techniques help you establish a baseline performance level,
    which is critical for evaluating whether fine-tuning has genuinely improved your
    model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A performance baseline without fine-tuning also serves as a safeguard: it helps
    detect any negative impacts from fine-tuning, as poorly prepared training data
    can degrade model quality.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s look at some key indicators that you’re ready for fine-tuning:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Experience with prompt engineering and RAG**: You should be able to demonstrate
    knowledge and results from prompt engineering or RAG-based approaches'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Documented challenges and use case testing**: Have specific examples of where
    prompt engineering or RAG fell short in your use case'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantitative baseline assessments**: Whenever possible, have measurable benchmarks
    of model performance without fine-tuning'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let’s look at some common signs fine-tuning may not be suitable yet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Starting with fine-tuning without testing other available techniques
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lacking a clear understanding of how fine-tuning specifically enhances LLMs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No benchmark data to measure the impact of fine-tuning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Always return SQL` in the prompt and used RAG to retrieve the database schema,
    the model often produced incorrect syntax, particularly in edge cases. To address
    this, they gathered thousands of examples of questions and their equivalent database
    queries, including previous model failures, and used this data to fine-tune the
    model. The resulting fine-tuned model, combined with their engineered prompt and
    retrieval setup, achieved the accuracy needed for real-world application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some indicators that you’re ready for fine-tuning:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Documented examples of previous attempts**: You have tested various prompt
    engineering or RAG solutions and documented specific limitations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identified model shortcomings**: These could include inconsistent handling
    of edge cases, an inability to include enough few-shot prompts within the context
    window, or issues with latency'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, here are some signs that you may need to wait before fine-tuning:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Lack of in-depth understanding of the model’s limitations or the data needed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficulty identifying suitable data to train the model effectively
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What data will you use** **for fine-tuning?**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even with a strong use case, the success of fine-tuning largely depends on the
    quality of the data you provide. It’s crucial to invest the necessary time and
    resources into gathering high-quality, curated data. Different models may require
    varying volumes of data, but in most cases, you will need to provide a large quantity
    of well-curated examples to achieve meaningful improvements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition to data quality, the format of the data is equally important. Even
    high-quality data may require significant effort to format properly for fine-tuning.
    This may involve allocating engineering resources to ensure the data is structured
    correctly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Some indicators that you’re ready for fine-tuning include the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Identified dataset**: You have already selected the dataset you intend to
    use for fine-tuning'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correct format**: The dataset is structured in the appropriate format for
    the chosen model'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Curation effort**: Some level of dataset curation has been applied to ensure
    the data’s quality'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, here are some common signs you may not be ready for fine-tuning:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**No dataset identified**: You have not yet chosen the dataset for fine-tuning'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incorrect format**: The dataset format does not align with the requirements
    of the model you intend to fine-tune.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How will you measure the quality of your** **fine-tuned model?**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s no one-size-fits-all approach to measuring the success of a fine-tuned
    model, but it’s essential to have clear, well-defined goals. Success should not
    only be evaluated qualitatively but also include quantitative metrics. A good
    approach is to use a *holdout validation dataset* to assess performance objectively.
    Additionally, you can enhance your evaluation by conducting *user acceptance testing*
    or performing *A/B testing*, comparing the fine-tuned model to the base model
    to see whether the improvements meet your expectations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To optimize the model’s context, you should explore techniques such as prompt
    engineering and RAG. For optimizing the LLM itself, focus on prompt engineering
    followed by fine-tuning, as illustrated in *Figure 3**.30*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.30: Optimizing LLM accuracy](img/B21019_03_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.30: Optimizing LLM accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve identified fine-tuning as the appropriate strategy, you can follow
    the steps detailed in the subsequent section to effectively fine-tune the model.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At a high level, the fine-tuning process includes the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare and upload training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a new fine-tuned model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conduct a safety evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conduct a model performance evaluation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the fine-tuned model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s go through each step in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Preparing and uploading training data**: Your training and validation datasets
    should include input and output examples that demonstrate the desired model behavior.
    These datasets must be formatted as **JSON Lines** (**JSONL**) documents. For
    models such as gpt-3.5-turbo (all versions), gpt-4, gpt-4o, and gpt-4o-mini, the
    fine-tuning data should adhere to the conversational format used by the Chat completions
    API. Here is the sample format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| {“messages“: [{“role”: “system”, “content”: “You are an Ontoso customer support
    agent who helps customers with issues related to their Ontoso accounts and services.
    You are friendly and concise. You only provide factual answers to queries related
    to Ontoso.”}, {“role”: “user”, “content”: “I need help with my Ontoso account.
    I forgot my password.”}, {“role”: “assistant”, “content”: “Hi! I’m happy to help.
    Please provide your registered email address, and I’ll guide you through resetting
    your password.”}]}{“messages”: [{“role”: “system”, “content”: “You are an Ontoso
    customer support agent who helps customers with issues related to their Ontoso
    accounts and services. You are friendly and concise. You only provide factual
    answers to queries related to Ontoso.”}, {“role”: “user”, “content”: “Can I track
    my order on Ontoso?”}, {“role”: “assistant”, “content”: “Yes, you can track your
    order by logging into your Ontoso account and checking the ‘Order History’ section.
    There, you can see the status of your current and past orders.”}]} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.2: Sample training data JSONL format'
  prefs: []
  type: TYPE_NORMAL
- en: Besides being in JSONL format, the training and validation data files must be
    encoded in UTF-8 with a **byte-order mark** (**BOM**). Additionally, the file
    size must not exceed 512 MB.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also include multiple turns of a conversation in a single line of your
    JSONL training file. To exclude specific assistant messages from fine-tuning,
    you can add the optional weight key-value pair. Currently, the weight can be set
    to either `0` or `1`. Here is another multiturn sample format:'
  prefs: []
  type: TYPE_NORMAL
- en: '| {“messages”: [{“role”: “system”, “content”: “You are a Contoso customer support
    agent who is friendly and concise, but also has a sarcastic tone.”}, {“role”:
    “user”, “content”: “What are your business hours?”}, {“role”: “assistant”, “content”:
    “Our business hours are 9 AM to 5 PM, Monday through Friday.”, “weight”: 0}, {“role”:
    “user”, “content”: “Can you be more sarcastic?”}, {“role”: “assistant”, “content”:
    “Sure, we’re open during the highly unconventional hours of 9 AM to 5 PM, Monday
    through Friday. A real shocker, right?”, “weight”: 1}]}{“messages”: [{“role”:
    “system”, “content”: “You are a Contoso customer support agent who is friendly
    and concise, but also has a sarcastic tone.”}, {“role”: “user”, “content”: “How
    do I reset my password?”}, {“role”: “assistant”, “content”: “You can reset your
    password by clicking the ‘Forgot Password’ link on the login page.”, “weight”:
    0}, {“role”: “user”, “content”: “Can you be more sarcastic?”}, {“role”: “assistant”,
    “content”: “Oh, you just click the ‘Forgot Password’ link. It’s a groundbreaking
    feature that’s been around for ages.”, “weight”: 1}]} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.3: sample multiturn training data JSONL format'
  prefs: []
  type: TYPE_NORMAL
- en: The more training examples you provide, the better the fine-tuning results will
    be. While fine-tuning requires at least 10 training examples to proceed, this
    minimal number is usually insufficient to have a meaningful impact on the model’s
    performance. Ideally, you should provide hundreds or even thousands of examples
    for optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, increasing the dataset size can lead to a proportional improvement
    in model quality. However, it’s important to note that low-quality examples can
    harm the model’s performance. If you train the model with a large amount of internal
    data without carefully curating it to include only high-quality examples, the
    model’s performance may end up being worse than anticipated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training a new fine-tuned model**: Training a new fine-tuned model can be
    done either through Azure AI Foundry or using the API. For this demonstration,
    we will use Azure AI Foundry to illustrate how you can easily fine-tune a base
    chat completion model. AOAI supports a range of models for fine-tuning. To view
    the complete list of supported models, please refer to [https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning?tabs=azure-openai%2Ccompletionfinetuning%2Cpython-new&pivots=programming-language-studio#models](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/fine-tuning?tabs=azure-openai%2Ccompletionfinetuning%2Cpython-new&pivots=programming-language-studio#models):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To begin, open Azure AI Foundry at [https://oai.azure.com/](https://oai.azure.com/)
    and sign in using credentials that have access to your AOAI resource. During the
    sign-in process, ensure you select the correct directory, Azure subscription,
    and AOAI resource associated with your account.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In Azure AI Foundry, navigate to the **Tools** section in the left-hand menu,
    then select the **Fine-tuning** pane. From there, click on the **Fine-tune model**
    option to begin the fine-tuning process.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.31: AOAI fine-tuning wizard](img/B21019_03_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.31: AOAI fine-tuning wizard'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in creating a custom model is to select a base model. In the
    **Base model** pane, you can choose a base model from the **Base model type**
    drop-down menu. Your choice will impact both the performance and the cost of the
    custom model you’re creating. Once you’ve selected the base model, click **Next**
    to proceed with the fine-tuning process, as shown in *Figure 3**.31*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.32: Selecting base model to fine-tune](img/B21019_03_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.32: Selecting base model to fine-tune'
  prefs: []
  type: TYPE_NORMAL
- en: AOAI also supports incremental fine-tuning, meaning you can fine-tune a model
    that has already been fine-tuned. This allows you to continue improving the model’s
    performance by further training it on new or updated datasets, enhancing its ability
    to handle more specific tasks or respond to evolving needs.
  prefs: []
  type: TYPE_NORMAL
- en: To proceed, you can select from your previously uploaded training datasets or
    upload new ones, based on your specific customization needs. The **Training Data**
    section will show all available datasets, allowing you to review and choose from
    existing options or upload fresh data for training purposes. In this example,
    we demonstrate the process of uploading a new dataset directly from the local
    drive, as illustrated in *Figure 3**.32*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For handling large data files, it’s advisable to import them directly from Azure
    Blob Storage. Uploading sizable files through multipart forms can lead to instability,
    as these uploads rely on atomic requests, which means they cannot be resumed or
    retried if interrupted. Using Azure Blob Storage for such transfers ensures greater
    reliability and fault tolerance, especially when dealing with larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.33: Training data upload](img/B21019_03_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.33: Training data upload'
  prefs: []
  type: TYPE_NORMAL
- en: In the next step, you’ll find options to set up validation data for your model
    training process. If validation data is not required, simply select **Next** to
    proceed directly to the advanced configuration settings. However, if you wish
    to incorporate validation data, you can either select from your existing datasets
    or upload a new validation dataset specifically prepared for this purpose.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **Validation Data** section displays all available training and validation
    datasets, providing flexibility to either use existing data or add new validation
    data as needed for model customization. In this example, we demonstrate the process
    of uploading a new validation dataset directly from the local drive, as illustrated
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.34: Validation data upload](img/B21019_03_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.34: Validation data upload'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `batch_size` (integer): This specifies the number of training examples
    processed in a single forward and backward pass. Generally, larger batch sizes
    are recommended for larger datasets, as they tend to stabilize the training process.
    Higher batch sizes reduce the frequency of model parameter updates, leading to
    lower variance in updates.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`learning_rate_multiplier` (number): This is a multiplier applied to the pre-training
    learning rate to set the fine-tuning learning rate. Larger values can improve
    training efficiency with larger batch sizes but may risk overfitting if they’re
    too high. It’s often effective to experiment with values between `0.02` and `0.2`
    to find an optimal rate.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`n_epochs` (integer): This refers to the number of epochs, or complete passes
    through the dataset, for which the model is trained. Each epoch represents one
    full cycle of learning from the dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`seed` (integer): This controls the reproducibility of training runs. Setting
    a specific seed value ensures that the training results are consistent across
    runs, assuming the same job parameters. If left unspecified, a seed will automatically
    be generated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select **Default** to use the default values for the fine-tuning job or select
    **Custom** to display and edit the hyperparameter values. When **Default** is
    selected, Microsoft determines the correct value algorithmically based on your
    training data, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.35: Hyperparameter selection](img/B21019_03_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.35: Hyperparameter selection'
  prefs: []
  type: TYPE_NORMAL
- en: After you configure the advanced options, select **Next** to review your choices
    and train your fine-tuned model.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Review** pane of the wizard, you can view a summary of your selected
    configuration settings. Once you’ve verified that all configurations are correct,
    click **Submit** to initiate the finetuning job. After submission, you’ll be redirected
    to the **Models** pane, where you can monitor the status and progress of your
    fine-tuning task. This final step confirms your setup and starts the model training
    process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Models** pane provides a detailed overview of your custom model’s fine-tuning
    process, displaying key information about the fine-tuning job’s status and results,
    as shown in *Figure 3**.36*. Once you initiate a fine-tuning job, it may take
    some time to finish. Your job could be placed in a queue behind other jobs in
    the system. The duration of the training process can vary, taking anywhere from
    a few minutes to several hours, depending on the size of the model and dataset.
    Click **Refresh** to update the information on the status page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.36: Fine Tuning Status](img/B21019_03_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.36: Fine Tuning Status'
  prefs: []
  type: TYPE_NORMAL
- en: After completing the seven steps mentioned here, you will have a fine-tuned
    model. During the training process, a checkpoint is generated at the end of each
    training epoch. A checkpoint represents a fully functional version of the model
    that can be deployed and used as the base model for future fine-tuning jobs. These
    checkpoints are valuable because they offer a snapshot of your model before overfitting
    may occur. Upon completion of a fine-tuning job, you will have access to the three
    most recent versions of the model, which can be deployed as needed.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4o and GPT-4o-mini are our most advanced models, designed to be fine-tuned
    to meet your specific needs. However, as with all AOAI models, fine-tuned versions
    of these models come with added responsible AI challenges, including risks related
    to harmful content, manipulation, human-like behavior, privacy concerns, and more.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To mitigate these risks, additional evaluation steps have been implemented to
    detect and prevent harmful content in the training and outputs of fine-tuned models.
    These measures are aligned with the Microsoft Responsible AI Standard and AOAI
    Service content filtering policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key evaluation features include the following :'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dedicated private workspaces** for each customer to ensure security and privacy
    during evaluations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation endpoints** located within the same geographic region as the AOAI
    resource to maintain compliance with regional data policies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training data privacy** is ensured because data used in evaluations is not
    stored; only the final model assessment (whether deployable or not) is retained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predefined evaluation filters**: The filters for GPT-4o, GPT-4o-mini, and
    GPT-4 fine-tuned models are set to fixed thresholds and cannot be altered by customers;
    these filters are independent of any custom content filtering configurations you
    may have set up'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These steps are designed to help ensure that the fine-tuned models adhere to
    responsible AI practices and minimize the risk of generating harmful or inappropriate
    content.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AOAI fine-tuning service incorporates two key safeguards to promote the
    responsible and ethical use of the AOAI fine-tuning service during the training
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The provided training data failed RAI checks for harm types: [hate_fairness,
    self_harm, violence]. Please fix the data and` `try again.`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Table 3.3: Training data evaluation notification'
  prefs: []
  type: TYPE_NORMAL
- en: Your training data is automatically assessed during the data import process
    as part of enabling the fine-tuning feature. If harmful content is detected in
    the training data, causing the fine-tuning job to fail, you will not incur any
    charges.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model evaluation**: Once training is finished, before the fine-tuned model
    is deployed, it undergoes an evaluation to assess the potential for harmful responses
    using Azure’s built-in risk and safety metrics. This evaluation mirrors the testing
    process applied to base LLMs. It simulates a conversation with the fine-tuned
    model to determine whether it could produce harmful content based on predefined
    categories (violence, sexual content, hate speech, fairness issues, and self-harm).
    If the model generates harmful content at a rate above an acceptable threshold,
    you will be notified, as shown in *Table 3.5*, that the model is not ready for
    deployment. You’ll also be given details about the specific harmful content categories
    identified.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| This model is unable to be deployed. Model evaluation identified that this
    fine-tuned model scores above acceptable thresholds for [Violence, Self Harm].
    Please retrain your model with a safe dataset. |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'Table 3.4: Model evaluation notification'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to data evaluation, the model is automatically assessed during the fine-tuning
    job as part of the fine-tuning process. Only the final assessment—whether the
    model is deployable or not—is recorded by the service. If the deployment of the
    fine-tuned model fails due to harmful content detected in the model’s outputs,
    you will not be charged for the training session.
  prefs: []
  type: TYPE_NORMAL
- en: '`results.csv` for each job. This file helps you analyze the performance of
    your custom model during training and validation. You can find the file ID for
    the result file under the `Result file id` column on the **Models** pane in Azure
    AI Foundry, which allows you to download the file from the **Data** **files**
    pane.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `results.csv` file contains the following columns, as shown in *Table 3.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table: 3.5: Model training and validation performance](img/B21019_03_Table_3.5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table: 3.5: Model training and validation performance'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Azure AI Foundry, you can visualize the data from your `results.csv` file
    as graphs. By selecting the link for your trained model, you will be able to view
    two key charts: **Loss** and **Token accuracy**. If you’ve provided validation
    data, the results for both the training and validation datasets will be displayed
    on the same plot, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.37: Fine Tuning metrics](img/B21019_03_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.37: Fine Tuning metrics'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what to watch for in the plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss should decrease over time*, indicating that the model is improving as
    it learns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Accuracy should increase*, showing that the model is getting better at predicting
    tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you notice a *divergence* between the training and validation data (i.e.,
    training loss continues to decrease while validation loss increases or plateaus),
    this could be a sign of *overfitting*. In such cases, you may want to: do the
    following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the model with fewer epochs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a smaller learning rate multiplier to prevent the model from fitting too
    closely to the training data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When fine-tuning a model, there are several important considerations to ensure
    optimal performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing system message**: It’s crucial to provide a consistent system message
    during fine-tuning and when using the fine-tuned model. If the system message
    changes, the model may produce results that differ from what you intended during
    fine-tuning. Therefore, the system message you use for deployment should match
    the one you used in the training process to maintain consistency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not enough data**: While the minimum required data for the fine-tuning pipeline
    to run is 10 examples, using hundreds or even thousands of data points is recommended
    for teaching the model new skills. With too few data points, there is a risk of
    overfitting, whereby the model memorizes specific examples rather than generalizing
    patterns. This can lead to poor performance when applied to real-world, unseen
    data. To achieve the best results, aim to prepare a dataset with hundreds or thousands
    of diverse data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bad data**: The quality of your training data directly impacts the quality
    of your fine-tuned model. A poorly curated or biased dataset can lead the model
    to learn inaccurate patterns. For instance, if you train a customer service chatbot
    only with data for one scenario (e.g., returns), it will struggle to handle other
    situations. Additionally, if the training data contains incorrect or misleading
    information, the model will learn to generate faulty or biased responses. Always
    ensure that your dataset is diverse, accurate, and representative of the tasks
    you expect the model to handle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deploying the fine-tuned model**: Once the fine-tuning job is successful,
    you can deploy your custom model through the **Models** pane in Azure AI Foundry.
    Deployment is necessary for making the fine-tuned model available for use in completion
    calls. To deploy the model, simply select the custom model and click **Deploy
    model**, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.38: Fine-tuned model deployment](img/B21019_03_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.38: Fine-tuned model deployment'
  prefs: []
  type: TYPE_NORMAL
- en: When you open the **Deploy model** dialog box, you will need to enter a deployment
    name for your custom model. After entering the name, click **Deploy** to initiate
    the deployment process for your fine-tuned model. You can track the deployment
    progress in the **Deployments** pane of Azure AI Foundry.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI fine-tuning also supports the flexibility to deploy your custom model to
    a different region from where it was originally fine-tuned, including across different
    subscriptions and regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are a few key limitations to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: The target region must support fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If deploying across subscriptions, the account generating the authorization
    token must have access to both the source and destination subscription
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Once you deploy a customized model, if it remains inactive for more than *15
    days*, the deployment will automatically be deleted. A deployment is considered
    inactive if no *completion* or *chat completion* calls are made to the model over
    a continuous 15-day period. It’s important to note that the deletion of an inactive
    deployment does not affect the underlying customized model. The model itself is
    preserved and can be redeployed at any time. Additionally, each deployed fine-tuned
    model incurs an *hourly hosting cost*, even if no calls are made to the model
    during that time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once your custom model is deployed, you can use it just like any other deployed
    model. You can experiment with your new deployment using `temperature` and `max_tokens`,
    can be applied to your custom model, just like with other deployed models:'
  prefs: []
  type: TYPE_NORMAL
- en: For fine-tuned `babbage-002` and `davinci-002` models, you will use the **Completions**
    playground and the Completions API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For fine-tuned gpt-4o models, you will use the **Chat** playground and the Chat
    Completion API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools allow you to interact with and test the customizations made to your
    fine-tuned models.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on AOAI, which provides a comprehensive set of tools
    and services designed to enhance AI model capabilities and integration. At the
    core of these offerings is the AOAI model context window, which defines the amount
    of information the models can process at once. We learned that it’s crucial for
    maintaining coherence and understanding in complex tasks. We also learned that
    AOAI embedding models facilitate the conversion of text into numerical vectors,
    enabling better semantic understanding and similarity searches. These embeddings
    can efficiently be stored and queried using Azure vector databases, which are
    optimized for handling high-dimensional data, thereby enhancing the performance
    of AI applications. We also discussed the standard RAG pattern, outlining its
    step-by-step process flow. Furthermore, we learned that AOAI On Your Data allows
    organizations to do quick a prototype and leverage these models on their proprietary
    datasets, ensuring the AI solutions are tailored to specific business needs.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI capabilities extend into multimodal models, which can process and integrate
    information from multiple data types, such as text and images, broadening the
    scope of AI applications. We learned that the function calling feature allows
    seamless integration of AI models with Azure’s robust ecosystem, facilitating
    the execution of predefined functions based on AI outputs. Developers can leverage
    the AOAI Assistants API to create sophisticated, context-aware conversational
    agents, enhancing user interactions. For operations requiring high throughput,
    the AOAI Batch API provides a scalable solution for processing large volumes of
    data efficiently, as we learned in this chapter. Finally, we learned that AOAI
    fine-tuning empowers users to customize pre-trained models to better align with
    specific tasks or domains, improving performance and accuracy in specialized applications.
    Together, these tools offer a powerful and flexible platform for developing advanced
    AI solutions tailored to diverse business requirements, as we learned in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters, we will focus on practical examples of generative
    AI applications, accompanied by coding exercises to help you build these applications
    effortlessly. Through hands-on implementations, you’ll develop a comprehensive
    understanding of how to apply generative AI technologies to real-world use cases.
    Topics will include various scenarios such as document-based question answering
    and contact center analytics, as well as querying structured data, generating
    code using AOAI, creating recommender systems, generating text-to-video content,
    and building a multimodal multi-agent system using the Assistant API. Each example
    will include step-by-step guidance and code snippets to support you in integrating
    these features into your projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Practical Applications of Azure OpenAI: Real-World Use Cases'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Part 2, we transition from foundational concepts to hands-on implementations,
    exploring practical use cases that demonstrate the transformative potential of
    Azure OpenAI in solving real-world challenges. Each chapter presents a distinct
    application, offering detailed insights into the problem context, technical architecture,
    and step-by-step solution development. From creating enterprise-level document
    question-answering systems to building multimodal, multi-agent frameworks, this
    section equips readers with the knowledge and tools to harness Azure OpenAI for
    diverse and impactful applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B21019_04.xhtml#_idTextAnchor059), Developing an Enterprise Document
    Question-Answer Solution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B21019_05.xhtml#_idTextAnchor067), Building a Contact Center
    Analytics Solution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B21019_06.xhtml#_idTextAnchor077), *Querying From a Structured
    Database*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B21019_07.xhtml#_idTextAnchor088), *Code Generation and Documentation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21019_08.xhtml#_idTextAnchor095), *Creating a Basic Recommender
    Solution with Azure OpenAI*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B21019_09.xhtml#_idTextAnchor101), *Transforming Text to Video*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B21019_10.xhtml#_idTextAnchor109), *Creating a Multimodal Multi-Agent
    Framework with the Azure OpenAI Assistant API*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
