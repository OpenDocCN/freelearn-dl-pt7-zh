<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying and Maintaining AI Applications</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Throughout this book, we've learned all about how to create <strong>Artificial Intelligence</strong> (<strong>AI</strong>) applications to perform a variety of tasks. While writing these applications has been a considerable feat in itself, it's often only a small portion of what it takes to turn your model into a serviceable production system. </span><span>For many</span> <span>practitioners</span><span>, the workflow for deep learning models often ends at the validation stage. You've created a network that performs extremely well; We're done, right? </span></p>
<p>It's becoming increasingly common for data scientists and machine learning engineers to handle their applications from the discovery to deployment stages. According to Google, more than 60-70% of the time it takes to build ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>While this chapter will contain some materials that are typically part of the job of a DevOps engineer, we'll touch on these tools and topics on a need-to-know basis, and refer to other resources and tools that can help you learn about the topics in more depth. In this chapter, we'll be utilizing the following: </p>
<ul>
<li>TensorFlow</li>
<li>PyTorch</li>
<li>Docker, a containerization service for deploying our models </li>
<li>Amazon Web Services or Google Cloud Platform as a cloud provider</li>
<li>Introductory knowledge of Kubernetes</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>The deployment and maintenance of AI applications is more than just a single action; it's a process. In this section, we will work through creating sustainable applications in the cloud by creating a<span> </span><strong>deep learning deployment architecture</strong>.<strong> </strong>These architectures will help us create end-to-end systems: <strong>deep learning systems</strong>. </p>
<p>In many machine learning/AI applications, the typical project pipeline and workflow might look something like the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1317 image-border" src="Images/43eaa596-8282-48b3-bd65-a7f5273c2e90.png" style="width:39.67em;height:22.08em;" width="999" height="556"/></div>
<p>The training processes are strictly offline, and serialized models are pushed to the cloud and interact with a user through an API. These processes often leave us with several different ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying your applications</h1>
                </header>
            
            <article>
                
<p>So, what does it mean to deploy a model? Deployment is an all-encompassing term that covers the process of taking a tested and validated model from your local computer, and setting it up in a sustainable environment where it's accessible. Deployment can be handled in a myriad of ways; in this chapter, we'll focus on the knowledge and best practices that you should know about to get your models up into production. </p>
<p><span>Your choice of deployment architecture depends on a few things: </span></p>
<ul>
<li>Is your model being trained in one environment and productionalized in another?</li>
<li>How many times are you expecting your model to be called predictions to be made from it? </li>
<li>Is your data changing over time or is it static? Will you need to handle large inflows of data?</li>
</ul>
<p>Each of these questions can be answered by breaking down our model selection options into two buckets. We can break down our models by the location from which they are served, as well as the way in which they are trained. The following figure shows these options in a matrix, as well as the costs and benefits of each method: </p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1328 image-border" src="Images/b900735b-b3a0-486f-a85e-4244fb800d8c.png" style="width:52.50em;height:41.92em;" width="852" height="680"/></div>
<p>Models that are trained on a specific piece of data, in a separate environment from where they are deployed, are called <strong>offline models</strong>, whereas models that actively learn from new data in their deployment environment are called <strong>online models</strong>.</p>
<p>The simplest form of serving for offline models is called <strong>batch serving</strong>. If you are a data scientist or come from academia, you're probably very familiar with this model. Batch serving involves simply taking a static dataset, feeding it to your model, and receiving predictions back. Typically, you'll probably do this on your local machine, perhaps with a Jupyter Notebook or simply by running a script from your terminal or Command Prompt. In the majority of cases, we want our models to be accessible to larger groups of users so that we encase them in a <strong>web service</strong>.</p>
<p>Online models are more difficult to manage due to the complications that can arise from the handling of data flows and potentially bad input data. Microsoft's fated Tay Twitter bot was an example of a Fully online learning model which took tweets as input, and quickly became racist and crude. Managing these models can become complicated because of the open training process, and many safeguards must be put in place to ensure that your model does not deviate too far from its desired output.</p>
<p>Automated machine learning models, on the other hand, are becoming increasingly popular. They have controlled input, but are actively retraining to consider new data. Think about Netflix's recommendation system <span>–</span> it actively responds to your behavior by training on the data you generate based on your browsing and viewing activity. </p>
<p><span>Now that we have a grasp of our ecosystem, let's get started by learning how to set up a common web service deployment architecture with TensorFlow. If you are not interested in learning about manual deployment processes, and only wish to use deployment service, please feel free to skip the next section. </span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying models with TensorFlow Serving</h1>
                </header>
            
            <article>
                
<p>In general, when deploying models, we want the inner machinations of the model to be isolated from the public behind an HTTP interface. With a traditional machine learning model, we would wrap this serialized model in a deployment framework such as Flask to create an API, and serve our model from there. This could lead us to a myriad of issues with dependencies, versioning, and performance, so instead, we are going to use a tool provided to us by the TensorFlow authors called<span> </span><strong>TensorFlow Serving</strong>. This spins up a small server that runs a TensorFlow model and provides access to it.</p>
<p><span>TensorFlow Serving implements a specific type of remote procedure call known as</span> <strong>GPRC</strong>.<strong> </strong><span>In computer science, remote procedure ...</span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Utilizing docker</h1>
                </header>
            
            <article>
                
<p>Since we'll be deploying our model to the cloud, we'll need some type of mechanism to run the model itself. While we could spin up a virtual machine on AWS, it's a bit overkill for what we need and there are many simpler (and cheaper) processes that can help us. </p>
<p>Instead, we will utilize a tool known as a <strong>container</strong>. Containers are a lightweight virtualization technique that contain all of the necessary runtime packages and methods for running an application. The most popular container service is called <strong>Docker</strong>. </p>
<p>While we won't cover the Docker installation process here, you can install Docker by following the official installation guidelines:</p>
<ol>
<li>Create a <strong>Docker image</strong></li>
<li>Create a <strong>container from</strong> the Docker image</li>
<li>Build TensorFlow Serving on the container</li>
</ol>
<p>The configuration of a Docker image is defined in something called a <strong>Docker file</strong>. TensorFlow Serving gives these files to us, one for utilizing CPUs and one for utilizing GPUs. </p>
<p>Google's TensorFlow team maintains a Docker image that is ready to use for TensorFlow Serving:</p>
<ol>
<li>Once you have Docker installed, you can grab it easily with the <kbd>docker pull</kbd> command in your terminal or command prompt:</li>
</ol>
<pre style="padding-left: 60px"><strong>docker pull tensorflow/serving</strong></pre>
<p style="padding-left: 60px">You should see a series of messages that look something as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1321 image-border" src="Images/b62de008-df45-4acb-a98d-01528bbfe313.png" style="width:40.75em;height:13.50em;" width="1041" height="345"/></p>
<ol start="2">
<li><span>Once you've downloaded the Docker image, you can move on to creating a container on the image. We can easily do that by running the build command:</span></li>
</ol>
<pre style="padding-left: 60px"><strong>## Builds a docker container from the image</strong><br/><strong>docker build --pull -t $USER/tensorflow-serving-devel -f tensorflow_serving/tools/docker/Dockerfile.devel .</strong></pre>
<div class="packt_infobox">Building the docker container can take a while—don't worry, this is normal.</div>
<ol start="3">
<li>Once the container is built, go ahead and run the container:</li>
</ol>
<pre style="padding-left: 60px"><strong>## Run the container; we'll name it nn_container</strong><br/><strong>docker run --name=nn_container -it $USER/tensorflow-serving-devel</strong></pre>
<ol start="4">
<li>You should now have shell access to your Docker container. Next, we'll download the actual TensorFlow serving files into the container: </li>
</ol>
<pre style="padding-left: 60px"><strong> git clone -b r1.6 https://github.com/tensorflow/serving</strong><br/><strong> cd serving</strong></pre>
<ol start="5">
<li>Lastly, we'll need to install the TensorFlow modelserver on the container. Modelserver will be doing the actual serving for our model:</li>
</ol>
<pre style="padding-left: 60px"><strong>bazel build -c opt tensorflow_serving/model_servers:tensorflow_model_server</strong></pre>
<p>Once we have a container, our environment is configured. The next thing to do is place our saved model inside the docker container.</p>
<div class="packt_tip">When you exit the shell of a Docker container, the container shuts down. If you'd like to start the container again, you can do so by running <kbd>docker start -i nn_container</kbd><span> in the container's directory.</span></div>
<p>Let's create a directory to place our model in. While you are still in the command line for the container, create a new directory with: </p>
<pre><strong>mkdir model_serving</strong></pre>
<p>Next, we'll upload our saved model to this directory. From wherever you saved the classifier from previously, run the following commend. You'll replace <kbd>output_directory</kbd> with whatever the sub-folder is that the TensorFlow SavedModel binaries are saved in. </p>
<pre class="graf graf--pre graf-after--p"><strong>docker cp ./output_directory nn_container:/model_serving</strong></pre>
<p>Let's try serving our model. Run the following command inside the docker container: </p>
<pre><strong>tensorflow_model_server --port=9000 --model_name=nn --model_base_path=/model_serving/binaries &amp;&gt; nn_log &amp;</strong></pre>
<p>Your model should now be running locally with TensorFlow serving. We're not done, however, as we need to create a way that the model can interact with requests once it is deployed in the cloud. For that, we'll need to create something called a <strong>client</strong>, which is a small program that acts as a gatekeeper for the model to talk with the outside world.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a TensorFlow client</h1>
                </header>
            
            <article>
                
<p>Lastly, we need to build a client to interact with our TensorFlow model. Building a custom client to interact with your model is a bit beyond the scope of this book, so we've provided this in the corresponding GitHub repository:</p>
<ol>
<li>Go ahead and download it with the following code:</li>
</ol>
<pre style="padding-left: 60px" class="graf graf--pre graf-after--p"><strong>pip install git+ssh://git@github.com/PacktPublishing/hands-On-Artificial-Intelligence-for-Beginners/tf-client.git</strong></pre>
<ol start="2">
<li>Let's try using the client to send a request to the model:</li>
</ol>
<pre style="padding-left: 60px" class="graf graf--pre graf-after--p"><strong>from predict_client.prod_client import ProdClient</strong><strong>client = ProdClient('localhost:9000', 'simple', 1)</strong><strong>req_data = [{'in_tensor_name': 'a', 'in_tensor_dtype': 'DT_INT32', 'data': 2}]</strong><strong>client.predict(req_data)</strong></pre>
<p>What's happening here? </p>
<ul>
<li>The first line imports the client itself, ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training and deploying with the Google Cloud Platform</h1>
                </header>
            
            <article>
                
<p>For a much simpler deployment procedure, we can deploy a TensorFlow SavedModel to production with the <strong>Google Cloud Platform</strong> (<strong>GCP</strong>). In this section, we'll cover the basics of how to both train and deploy a model using GCP.</p>
<p>The GCP currently provides one of the most straightforward and easy interfaces for training and deploying models. If you are interested in getting your model up to production as quickly as possible, GCP is often your answer. Specifically, we'll be using the Cloud ML service, which is a compliment to AWS SageMaker that we just learned previously. Cloud ML is enabled currently enabled to run TensorFlow, Scikit-learn, and XGBoost right out of the box, although you can add your own packages manually. Compared to SageMaker, Cloud ML receives updates automatic updates to TensorFlow much at a rapid speed due to the library's Google integration, and hence it is recommended to use it for TensorFlow-based applications. </p>
<p>Before we get started, let's set up a new Google Cloud Storage Bucket that will be the basis for our application. Go ahead and log onto your GCP account, look for Cloud Storage, and click <span class="packt_screen">Create bucket</span>. You should see a screen that looks like the one as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1322 image-border" src="Images/173abe83-964f-4d1a-b7a4-55dec4314d84.png" style="width:45.42em;height:27.17em;" width="967" height="579"/></p>
<p>This bucket will act as the staging ground for our data, model, training checkpoints, and model binaries. G<span>o ahead and upload the <kbd>creditcard.csv</kbd> file that we've been using to the bucket , we'll be using it soon!</span></p>
<p>Next, let's make our model ready for training on GCP, we'll have to give it a couple lines of code so that it can run from the command line. In a script that contains the model code from previously, we'll add this to the bottom:</p>
<pre>if __name__ == '__main__':<br/>    parser = argparse.ArgumentParser()<br/>    parser.add_argument(<br/>      '--job-dir',<br/>      help='GCS location to write checkpoints and export models'<br/>     )<br/>    parser.add_argument(<br/>      '--train-file',<br/>      help='GCS or local paths to training data'<br/>      )<br/>     args, unknown = parser.parse_known_args()<br/>     c = SimpleClassifier()<br/>     c.train_model(**args.__dict__)</pre>
<p>This script will enable us to pass in the only parameter for the model, <kbd>job_dir</kbd>, from the command line. For the full GCP-ready code, check out the <kbd>simple_classifier.py</kbd> script in this chapter's GitHub. Once you have your Cloud Storage and script set up, we're ready to start our training and deployment!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training on GCP </h1>
                </header>
            
            <article>
                
<p>Google has made the entire deep learning training / deployment process streamlined and simple by allowing us to train models, store them, and deploy them with minimal code. Before we start training in the cloud, let's train our model locally to ensure that everything is working as intended. First, we need to set some environment variables. First and foremost, we'll have to put our files in a particular structure to train with Cloud ML. Look for the training folder in the chapter <kbd>GitHub</kbd> folder, and you will find the correct file structure. The <kbd>__init__.py</kbd> file that you see there will simply tell GCP that our file is an executable Python program. </p>
<p>First we'll define a job directory, which should be the folder where your <kbd>simple_classifier.py ...</kbd></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deploying for online learning on GCP</h1>
                </header>
            
            <article>
                
<p><span>When we deploy a TensorFlow SavedModel to the GCP platform, we either need to upload the entire SavedModel directory to a storage location on GCP or train in the cloud as we did previously. Regardless of what method you main use, your TensorFlow model's binaries should be stored in a Google Cloud Storage location. </span></p>
<div class="packt_tip packt_infobox">Your model binaries will be the final that was created after training, and will have the extension <kbd>.pb</kbd>.</div>
<p>To start our deployment process, we first need to create a deployed model object. You can create it with the command as follows:</p>
<pre>gcloud ml-engine models create "deployed_classifier"</pre>
<p>Next, we'll create an environment variable that will let GCP know where our saved model binaries are:</p>
<pre>DEPLOYMENT_SOURCE="gs://classifier_bucket123/classifier_model/binaries"</pre>
<p>All we have to do now is run the command as follows, and our classifier will be deployed! Keep in mind that deployment will take a few minutes to configure:</p>
<pre>gcloud ml-engine versions create "version1"\<br/>    --model "deployed_classifier" --origin $DEPLOYMENT_SOURCE --runtime-version 1.9</pre>
<p>As you see, we've done with a few lines of code what took us an entire section precedingly; platforms as services like AWS SageMaker and Google Cloud ML are extreme time savers in the modeling process. </p>
<p>Now, let's try getting predictions from our model. Before we try asking for a prediction, we'll need to go ahead and setups a few variables. The input data file will be a json file that contains a single line of data. To make it easy, we've included a line from the dataset as <kbd>test.json</kbd> in the <kbd>GitHub</kbd> folder:</p>
<pre>MODEL_NAME="deployed_classifier"<br/>INPUT_DATA_FILE="test.json"<br/>VERSION_NAME="version1"</pre>
<p>Lastly, go ahead and run the prediction request:</p>
<pre>gcloud ml-engine predict --model $MODEL_NAME \<br/>                   --version $VERSION_NAME \<br/>                   --json-instances $INPUT_DATA_FILE</pre>
<p class="mce-root">Congratulations! Your model is now hosted in the cloud. You should see a returned json object, with probabilities for both of the potential classifications, fraud or not-fraud. While the <kbd>gcloud</kbd> command previously is great for issuing individual requests, we often want to return requests as part of a web application. In the next segment, we'll run through a simple example of how we can do this with a simple Flask application.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using an API to Predict</h1>
                </header>
            
            <article>
                
<p>To get started, you'll have to create a Google Cloud service account key so that your application can access the model. Navigate to the link <a href="https://console.cloud.google.com/apis/credentials/serviceaccountkey">https://console.cloud.google.com/apis/credentials/serviceaccountkey</a>, and create a new account. Be sure to download the key as a JSON file. <span>To connect to GCP, you'll need to setup your account credentials as an environment variable that GCP can access. </span>Go ahead and set it's location as an environment variable: </p>
<pre>GOOGLE_APPLICATION_CREDENTIALS = your_key.json</pre>
<p>Let's create a script called <kbd>predict.py</kbd> (you can find the completed script in the <kbd>chapter</kbd> folder). First, we'll import the Google libraries that will allow our program to connect to the API. <kbd>GoogleCredidentials</kbd> will discover ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scaling your applications</h1>
                </header>
            
            <article>
                
<p>Scalability is the capacity for a system to handle greater and greater workloads. When we create a system or program, we want to make sure that it is scalable so that it doesn't crash upon receiving too many requests. Scaling can be done in one of two ways:</p>
<ul>
<li><strong>Scaling up</strong>: Increasing the hardware of your existing workers, such as upgrading from CPUs to GPUs.</li>
<li><strong>Scaling out</strong>: Distributing the workload among many workers. Spark is a common framework for doing this. </li>
</ul>
<p>Scaling up can be as easy as moving your model to a larger cloud instance. In this section, we'll be focus on how to distribute TensorFlow to scale out our applications. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scaling out with distributed TensorFlow</h1>
                </header>
            
            <article>
                
<p>What if we'd like to scale out our compute resources? We can distribute our TensorFlow processes over multiple workers to make training faster and easier. There are actually three frameworks for distributing TensorFlow: <em>native distributed TensorFlow</em>, <em>TensorFlowOnSpark</em>, and <em>Horovod</em>. In this section, we will be exclusively focusing on native distributed TensorFlow. </p>
<p>In the world of distributed processing, there are two approaches that we can take to distribute the computational load of our model, that is, <strong>model parallelism</strong> and <strong>data parallelism</strong>:</p>
<ul>
<li><strong>Model parallelism</strong>: <span>The distribution of the training layers of a model across various devices.</span></li>
<li><strong>Data parallelism</strong>: The distribution of the entire model across various ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Testing and maintaining your applications</h1>
                </header>
            
            <article>
                
<p>With either online or offline learning, we should always institute systems and safety checks that will tell us when our model's predictions, or even its critical deployment architecture, are out of whack. By testing, we are referring to the hard-coded checking of inputs, outputs, and errors to ensure that our model is performing as intended. In standard software testing, for every input, there should be a defined output. This becomes difficult in the field of machine learning, where models will have variable outputs depending on a host of factors - not the great for standard testing procedures, is it? In this section, we'll talk about the process of testing machine learning code, and discuss best practices. </p>
<p>Once deployed, AI applications also have to be maintained. DevOps tools like Jenkins can help ensure that tests pass before new versions of a model are pushed to production. While we certainly aren't expecting you as a machine learning engineer to create development pipelines, we'll review their principals in this section so that you can be familiar with best practices. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Testing deep learning algorithms</h1>
                </header>
            
            <article>
                
<p>The AI community is severely behind on adopting appropriate testing procedures. Some of the most advanced AI companies rely on manual checks instead of automating tests on their algorithms. You've already done a form of a test throughout this book; cross-validating our models with training, testing, and validation sets helps to verify that everything is working as intended. In this section, we'll instead focus on <strong>unit tests</strong>, which seek to test software at the smallest computational level possible. In other words, we want to test the little parts of our algorithms, so we can ensure the larger platform is running smoothly. </p>
<p>Testing our algorithms helps us keep track of <strong>non</strong>-<strong>breaking bugs</strong>, which have become ubiquitous ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Building AI applications goes beyond the basics of model construction <span>–</span> it takes deploying your models to a production environment in the cloud where they persist.In this chapter, we've discussed how to take a validated TensorFlow model and deploy it to production in the cloud. We also discussed ways that you can scale these models, and how you can test your applications for resiliency. </p>
<p>When taking a TensorFlow application from development to production, the first step is to create a TensorFlow SavedModel that can be stored in the cloud. From here, there are several services, including AWS Lambda and Google Cloud ML Engine, that can help make your deployment process easily. </p>
<p>Applications can be scaled up or out for more computing power and faster processing. By scaling up, we provide our algorithms with a larger computing resource. By scaling out, we provide our application with more resources at once. Remember, models that are deployed to production should also be tested, and basic tests like unit tests can help prevent your entire application from crashing!</p>
<p>We've now reached the end of the book. As you've worked through the content in the chapters, I hope you have been enlightened to the exciting possibilities that deep learning is creating for the artificial intelligence field. While there is no doubt that research will continue into many of these topics, and that new methods will we be created and used, the fundamental concepts that you've learned throughout these chapters will hold steady, and provide the basis for groundbreaking work going forward. Who knows, the person doing that groundbreaking work could be you!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ol>
<li>Mention Quora</li>
<li>Tensorflow client citation</li>
</ol>


            </article>

            
        </section>
    </div>



  </body></html>