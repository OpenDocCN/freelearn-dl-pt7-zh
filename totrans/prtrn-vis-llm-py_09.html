<html><head></head><body>
		<div id="_idContainer077">
			<h1 id="_idParaDest-118" class="chapter-number"><a id="_idTextAnchor138"/>9</h1>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor139"/>Advanced Training Concepts</h1>
			<p>In this chapter, we will cover advanced training concepts at scale, such as evaluating throughput, calculating <a id="_idIndexMarker441"/>model <strong class="bold">teraFLOPS</strong> (<strong class="bold">TFLOPS</strong>) per device, compiling, and using the scaling laws to determine the right length of training time. In the last chapter, you learned about how to do large-scale training on SageMaker, in general terms. In this chapter, you’ll learn about particularly complex and sophisticated techniques you can use to drive down the overall cost of your job. This lower cost directly translates to higher model performance because you can train for longer on the <span class="No-Break">same budget.</span></p>
			<p>We will cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Evaluating and improving throughput with <span class="No-Break">model TFLOPS</span></li>
				<li>Using FlashAttention to speed up your <span class="No-Break">training runs</span></li>
				<li>Speeding up your jobs <span class="No-Break">with compilation</span></li>
				<li>Amazon SageMaker Training Compiler <span class="No-Break">and Neo</span></li>
				<li>Running compiled models on Amazon’s Trainium and Inferentia <span class="No-Break">custom hardware</span></li>
				<li>Solving for an optimal <span class="No-Break">training time</span></li>
			</ul>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor140"/>Evaluating and improving throughput</h1>
			<p>As we’ve <a id="_idIndexMarker442"/>previously covered in <a id="_idIndexMarker443"/>the book, total job throughput is an important metric to track. On the one hand, you want to keep a batch size small enough to ensure your model is trained appropriately. On the other hand, you want to max out your overall job performance to get the most possibly accurate model you can. We learned in <a href="B18942_07.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> how to use hyperparameter tuning to solve both of those. We also covered other tips and tricks for reducing your <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) memory <a id="_idIndexMarker444"/>footprint in <a href="B18942_05.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> and <a href="B18942_08.xhtml#_idTextAnchor127"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>. Now, let’s close out a few more gaps in <span class="No-Break">this area.</span></p>
			<p>First, it’s important to consider how you measure throughput in general terms. You have probably used some logging packages in PyTorch that handily report iterations per second during the training loop. Obviously, this is extremely useful in clocking your training speed, but how would you take into account the size of the model? What if you wanted to compare your speed with others to see whether you’re in the <span class="No-Break">same ballpark?</span></p>
			<p>To solve this problem, many research teams calculate an aggregate term that combines both indicators for the model’s size with operations completed. Commonly, this is called <strong class="bold">model TFLOPS</strong>. These<a id="_idIndexMarker445"/> calculations will vary based on individual team preferences, but we’ll explore the setup from a recent Chinchilla <em class="italic">(11)</em> paper that just won a <strong class="bold">Neural Information Processing Systems</strong> (<strong class="bold">NeurIPS</strong>) best <a id="_idIndexMarker446"/>paper award. You’ll find this phrase is <a id="_idIndexMarker447"/>common in evaluating<a id="_idIndexMarker448"/> large-scale distributed <span class="No-Break">training systems.</span></p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor141"/>Calculating model TFLOPS</h2>
			<p>You’ve <a id="_idIndexMarker449"/>heard of <strong class="bold">floating operations per second</strong> (<strong class="bold">FLOPS</strong>). This <a id="_idIndexMarker450"/>is a simple way of presenting how many computations a given machine can perform. Higher is better because this means your machine can complete more tasks given the same amount of time. <strong class="bold">TFLOPS</strong> is an<a id="_idIndexMarker451"/> easier way of comparing performance for distributed <span class="No-Break">training solutions.</span></p>
			<p>In Chinchilla, the authors have a clean way of computing model TFLOPS. First, let’s consider that the performance of the forward pass and the backward pass is different. The backward pass is actually two times the compute costs of the forward pass because we need to both compute the gradients and update the weights and parameters. So, the model TFLOPS would then be <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B18942_09_001.jpg" alt=""/>
				</div>
			</div>
			<p>Simple enough? Now let’s unpack <span class="No-Break">that term.</span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B18942_09_002.jpg" alt=""/>
				</div>
			</div>
			<p>In <em class="italic">Appendix F</em> of their paper, they define the rest of their terminology in detail. Another much more simple but slightly less precise way of computing your total model TFLOPS is simply C=6⋅D⋅N, where <em class="italic">N</em> is the number of parameters in your model. Chinchilla actually found no significant difference between this computation and theirs presented in the <span class="No-Break">preceding formula.</span></p>
			<p>As you read through these metrics, consider that each term relates to a part of your neural network, specifically scoping how large it is. When you combine these with the number of tokens processed per second, you get a realistic metric for the efficiency of your overall training loop. This efficiency metric then becomes a single common denominator you can use to compare your experiments <span class="No-Break">at runtime.</span></p>
			<p>Remember that in <a href="B18942_03.xhtml#_idTextAnchor050"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, you learned how to think about your overall project as a series of experiments? While the accuracy of your project should no doubt be a key performance indicator, I would strongly recommend including an efficiency indicator as well. This helps ensure that you’re making the best use of your compute budget, which is relevant for the <a id="_idIndexMarker452"/>initial training runs, subsequent retraining, inference, monitoring, and overall <span class="No-Break">project maintenance.</span></p>
			<p>For example, you might consider the following <span class="No-Break">experiment schedule:</span></p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Phase</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Model type</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Model size</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Datase size</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Compute size</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Compute efficiency</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Experiment run-time</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>One – <span class="No-Break">small-scale testing</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Generic pretrained</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Base</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">5–30 GBs</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1–4 less <span class="No-Break">expensive GPUs</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Low</span></p>
						</td>
						<td class="No-Table-Style">
							<p>One full pass through a small <span class="No-Break">data sample</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Two – increase <span class="No-Break">the dataset</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Semi-custom</span></p>
						</td>
						<td class="No-Table-Style">
							<p>A <span class="No-Break">few billion</span> <span class="No-Break">parameters</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">100 GBs–</span> <span class="No-Break">TBs</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Tens to hundreds of <span class="No-Break">better GPUs</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Medium</span></p>
						</td>
						<td class="No-Table-Style">
							<p>A few steps <span class="No-Break">or epochs</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Three – <span class="No-Break">increase</span> model (<span class="No-Break">and data)</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Very custom</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Tens of billions <span class="No-Break">of parameters</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">TBs</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Hundreds to</span> <span class="No-Break">thousands of</span> <span class="No-Break">high-performance GPUs</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">High</span></p>
						</td>
						<td class="No-Table-Style">
							<p>A few steps <span class="No-Break">or epochs</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Four – maximize <span class="No-Break">compute budget</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Fully custom</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Tens to hundreds of billions <span class="No-Break">of parameters</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">TBs–PBs</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Thousands <span class="No-Break">or more</span> <span class="No-Break">high-performance GPUs</span></p>
						</td>
						<td class="No-Table-Style">
							<p>State of <span class="No-Break">the art</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Train <span class="No-Break">to optimal</span> <span class="No-Break">period</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Suggested phase of experiments for training foundation models at scale</p>
			<p>Let me be explicitly clear about this table; I do not under any circumstances expect every person to follow this exactly. There are countless nuances in novel training regimes, dataset sizes, models, GPU performance, modeling results, compute budget, and perspectives for a single table to encompass all of them. Some teams will never hit models over 1 billion parameters and still build something the world adores, such as Stable Diffusion! But I guarantee you that the lab staged its build across multiple phases that eventually culminated in a massive run. You want to learn how to increase the scope of your projects from very doable to very impressive. It’s up to you how to do that appropriately for<a id="_idIndexMarker453"/> the problem you’re solving <span class="No-Break">at hand.</span></p>
			<p>Now let’s look at a few more methods you can use to boost your training efficiency. Next up is <span class="No-Break">Flash Attention!</span></p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor142"/>Using Flash Attention to speed up your training runs</h1>
			<p>In earlier <a id="_idIndexMarker454"/>chapters, we learned about the core Transformer model, with its underlying self-attention mechanism that serves as the basis for most state-of-the-art models across vision, language, and generative use cases today. While Transformer models are easily parallelizable, they aren’t particularly good at optimizing for different memory speeds within modern GPUs. This becomes a problem when they materialize the Transformer in the slowest part of the GPU due to a naïve implementation. As you can imagine, that leaves performance gains on <span class="No-Break">the table.</span></p>
			<p>A Stanford-led research team realized that they could improve this and developed a novel implementation of the Transformer architecture. Simply put, it’s an extremely clever way to handle a quadratic nested for-loop. Let’s take a <span class="No-Break">closer look.</span></p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B18942_image_09_02.jpg" alt="Figure 9.2 – From FlashAttention by Tri Dao et al, 2022 (1)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – From FlashAttention by Tri Dao et al, 2022 <em class="italic">(1)</em></p>
			<p>This visual from the paper demonstrates three key concepts. On the left-hand side, we see a simple pyramid showing the three common types of compute available on most GPU servers. At the base, we have a lot of CPUs with more than 1 TB available of main memory. However, this peaks at 12.8 GB/s bandwidth. Next, we have the slower part of the GPU, with much less memory but much more bandwidth, only 40 GB of GPU HMB but up to 1.5 TBs. Finally, we <a id="_idIndexMarker455"/>have the fastest part of the GPU, with only 20 MB of memory but up to 19 TBs of bandwidth. Obviously, 19 TBs is more than 10 times faster than 1.5 TBs! This shows you right away that moving as much of the compute to the <strong class="bold">static random-access memory</strong> (<strong class="bold">SRAM</strong>) can<a id="_idIndexMarker456"/> save you a lot <span class="No-Break">of time.</span></p>
			<p>However, you’ll notice that this 10 times is in bandwidth, not necessarily throughput. While pure throughput means efficiently processing a large volume of data, bandwidth here helps<a id="_idIndexMarker457"/> optimize <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>). In this case, it refers to how data is passed between different data structures in this overall computer architecture. This is why it’s the bandwidth metric we’re most interested in; bandwidth controls the volume of data we can pass to or from a given compute. This means that when we have an I/O intensive process, such as the quadratic nested for-loop used in self-attention heads, pushing as much of the data to the part with the highest bandwidth is a way to increase the <span class="No-Break">overall speed.</span></p>
			<p>What types of gains does this give us? In the far right-hand side of the visual, you can see that this new <em class="italic">fused kernel</em>, provided by FlashAttention, finishes in the amount of time it takes just one of the five operations to complete in a naïve PyTorch implementation. While a naïve implementation needs about 17 seconds to finish all the <strong class="bold">Matrix multiplication</strong> (<strong class="bold">Matmul</strong>), Masking, Softmax, Dropout, and finally Matmul, the FlashAttention<a id="_idIndexMarker458"/> fused kernel can run all of these in <span class="No-Break">around seconds!</span></p>
			<p>FlashAttention hasn’t yet been upstreamed into PyTorch directly, although I would be shocked if that didn’t happen in the next 12 months. For the time being, you can use an open source implementation available here <em class="italic">(2)</em>. The authors show that this leads to a 3–5 times<a id="_idIndexMarker459"/> speedup for <strong class="bold">generative pre-trained transformer</strong> (<strong class="bold">GPT</strong>) models over the Hugging Face options, reaching up to 189 TFLOPS on each NVIDIA A100 GPU. While that may not sound like a big jump at smaller scales, once you’ve hit hundreds to thousands of GPUs, that can equal massive savings! Support for FlashAttention is available in the SageMaker Model <a id="_idIndexMarker460"/>Parallel library as of December <span class="No-Break">2022 </span><span class="No-Break"><em class="italic">(3).</em></span></p>
			<p>Now let’s look at another advanced training concept to help speed up your training <span class="No-Break">runs: compilation.</span></p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor143"/>Speeding up your jobs with compilation</h1>
			<p>Remember <a id="_idIndexMarker461"/>that in <a href="B18942_04.xhtml#_idTextAnchor066"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, we learned about some basic concepts in GPU systems architecture. We covered the foundational <strong class="bold">Compute Unified Device Architecture</strong> (<strong class="bold">CUDA</strong>) software <a id="_idIndexMarker462"/>framework that lets you run normal Python code on GPUs. We talked about managed containers and deep learning frameworks, such as PyTorch and TensorFlow, which are already tested and proven to run nicely on the AWS cloud. The problem with most neural network implementations is that they aren’t particularly optimized for GPUs. This is where compilation comes in; you can use it to eke out an extra two-times jump in speed for the <span class="No-Break">same model!</span></p>
			<p>In the context of compilers for deep learning, we’re mostly interested in <strong class="bold">accelerated linear algebra</strong> (<strong class="bold">XLA</strong>). This <a id="_idIndexMarker463"/>is a project Google originally developed for TensorFlow, which has since merged into the <em class="italic">Jax</em> framework. PyTorch developers will be happy to know that major compilation techniques have been upstreamed into PyTorch 2.0. You can now compile any arbitrary PyTorch function with the new <span class="No-Break"><strong class="source-inline">torch.compile</strong></span><span class="No-Break"> method.</span></p>
			<p>Before we get into any examples of using compilation, let’s first try to understand what it is and why it’s useful. Imagine you have two vectors (remember, think “lists”), both of size 1000. One of them is filled with zeros, and the other is filled with ones. Now imagine that you have a basic operation to apply to both of these vectors: addition. You want to add these two vectors to produce a third vector with a length of 1000, which is simply the direct sum of each item in both of the <span class="No-Break">original vectors.</span></p>
			<p>A naïve way of doing this would be to walk through both lists, compute the sum, and add it to the new list. But what if you knew ahead of time that one of these vectors was zero? Wouldn’t you want to then skip the addition operation altogether? If you did, it could save you a lot <span class="No-Break">of time!</span></p>
			<p>This jump is possible as a result of an intermediate representation. As presented in a 2020 survey <em class="italic">(4)</em>, deep learning compilers profile the graph that is your neural network. First, a frontend compiler computes a more optimal version of your graph, such as fusing operators, simplifying the algebraic expressions, performing static memory planning, and many more techniques. Next, a backend compiler computes this again for specific hardware, lower-level<a id="_idIndexMarker464"/> representations, memory allocation, custom kernels, and more. This then generates the new code that is consumed by <span class="No-Break">the accelerator.</span></p>
			<p> Now let’s learn how to add compilation to <span class="No-Break">your scripts!</span></p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor144"/>Integrating compilation into your PyTorch scripts</h2>
			<p>From the launch<a id="_idIndexMarker465"/> PyTorch documentation here <em class="italic">(5)</em>, you’ll see that there are three major ways to use compilation in your own PyTorch code. First, you can use any PyTorch built-in functions such as <strong class="source-inline">torch.sin</strong> or <strong class="source-inline">torch.cos</strong>, and then pass these into <strong class="source-inline">torch.compile</strong>. This uses a variety of the techniques we discussed previously to compile your function based on the available GPUs. Alternatively, you can add a decorator to your PyTorch function, simply <strong class="source-inline">@torch.compile</strong>, which provides the same functionality. Both of these features are also available for the <strong class="source-inline">torch.nn.Module</strong> base object, which means you should be able to use them for any of your <span class="No-Break">PyTorch models!</span></p>
			<p>If you’re thinking these speedups with compilation seem useful, but I don’t want to re-write my model code to use them, this next section will be extremely interesting for you! Let’s look at <a id="_idIndexMarker466"/>managed compilation features on AWS – SageMaker Training Compiler and <span class="No-Break">SageMaker Neo.</span></p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor145"/>Amazon SageMaker Training Compiler and Neo</h1>
			<p>If you use <a id="_idIndexMarker467"/>Hugging <a id="_idIndexMarker468"/>Face language models today, such<a id="_idIndexMarker469"/> as BERT, GPT, RoBERTa, AlBERT, DistiliBERT, or hundreds of others, then you are in luck! Without much work, you can easily speed up the run-time of your jobs by up to 50%. This is because of <strong class="bold">SageMaker Training Compiler</strong> (<strong class="bold">SMTC</strong>). As <a id="_idIndexMarker470"/>we learned earlier, compilation generally has the potential to increase the speed of your training. With SMTC, we provide a managed compilation feature within SageMaker training to easily enable this for your own models <span class="No-Break">and scripts.</span></p>
			<p>As you can see in the visual provided, enabling this is quite simple. Here we use the Hugging Face AWS-managed deep learning container and simply add <strong class="source-inline">TrainingCompilerConfig()</strong>. If you’re using a model with the Hugging Face <strong class="source-inline">Trainer</strong> API, this will automatically trigger <span class="No-Break">Training Compiler:</span></p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B18942_image_09_03.jpg" alt="Figure 9.3 – Configure SageMaker Training Compiler"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Configure SageMaker Training Compiler</p>
			<p>How does it work? SMTC uses a variety of compilation methods on three different levels: compilations for the graph, the data flow, and the backend. The graph-level optimizations include operator fusion, memory planning, and algebraic simplifications. The data flow-level optimizations include layout transformation and common sub-expression elimination. Backend optimizations include memory latency hiding and loop-oriented <a id="_idIndexMarker471"/>optimizations. This<a id="_idIndexMarker472"/> accelerates the <a id="_idIndexMarker473"/>training<a id="_idIndexMarker474"/> process by up to 50%, and the resultant model is the same as if SMTC had not been applied. For example, when fine-tuning Hugging Face’s GPT-2 model, SMTC reduced the training time from nearly 3 hours to just <span class="No-Break">90 minutes!</span></p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor146"/>Best practices for compilation</h2>
			<p>When working <a id="_idIndexMarker475"/>with compilers, you’ll want to ensure that you are updating your hyperparameters accordingly. This is because the net effect of a compiler is that it reduces the GPU memory footprint of the model. For example, without compiling, your model might consume a solid 10 GB of GPU memory. After compilation, you might get that down to 5 GB! This opens up more space for you to pack in objects in your batch size. As we learned earlier in the book, this directly increases your GPU utilization and, thus, your overall project efficiency. Just be careful not to over-increase batch size, which then makes it harder to converge. You’ll also want to increase your learning rate at the <span class="No-Break">same pace.</span></p>
			<p>As you might be anticipating, there are clear times when compilation is expected to be quite useful. There are also times when compilation could be a waste of time. This is because most compilers <em class="italic">take some time to run their compilation process</em> before executing your code. That means that, in contrast to normal Python code execution, the compiler will run its subprocess ahead of time to produce a more optimized version of your model. Once this is produced, your code will run <span class="No-Break">in earnest.</span></p>
			<p>This ahead-of-time compilation process introduces the key tradeoff for evaluating the impact of compilation as a whole. The longer your model training period, the larger the boost from the compilation. This means if you’re using a large number of epochs, or if your dataset is quite large, then compilation should be a useful way to save compute costs. Personally, I’d say if your model runs for anything longer than 30 or 40 minutes, try to find a way to drive that down <span class="No-Break">with compilation.</span></p>
			<p>Alternatively, if you have a frequent retraining pipeline or job, one that runs on a semi-frequent schedule, try to use compilation to drive down that time. Some of my customers retrain their models every day, every week, or even every few hours or minutes. We’ll <a id="_idIndexMarker476"/>dive into this and other topics about operations in <a href="B18942_14.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 14</em></span></a><span class="No-Break">.</span></p>
			<p>Now, let’s learn how using PyTorch compilation enables us to easily use Amazon’s custom hardware for machine learning: Trainium <span class="No-Break">and Inferentia!</span></p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor147"/>Running compiled models on Amazon’s Trainium and Inferentia custom hardware</h1>
			<p>So far in this <a id="_idIndexMarker477"/>book, most of the accelerators <a id="_idIndexMarker478"/>we evaluated have been GPUs designed and built by NVIDIA. As we learned earlier, NVIDIA’s excellent software enables the lion’s share of deep learning frameworks to run nicely on those same GPUs, which ends up being a primary deciding factor in using GPUs. We also learned earlier how those same GPUs are also available on AWS, notably through our machine learning service, <span class="No-Break">Amazon SageMaker.</span></p>
			<p>However, as you have no doubt realized by this point, the price tag of those same GPUs can be high! Even though AWS has generous enterprise discount programs, such as using reserved instances to save up to 75% <em class="italic">(6)</em>, you would still benefit from learning about alternatives. Basic economics tells us that when supply increases, such as through alternative accelerators, while demand stays constant, the price drops! This is exactly what we’re thrilled to provide customers: our custom accelerators for machine learning – Trainium and Inferentia. As you might have guessed, Trainium is dedicated to training machine learning models, while Inferentia does the same for hosting. As of this writing, these are available on EC2 and SageMaker as Inf1 and <span class="No-Break">Trn1 instances.</span></p>
			<p>Fortunately for those of you who made it through the previous section on compilation, many models compiled with XLA are supported by Trainium and Inferentia! This means that if you are already using XLA compilation, either through PyTorch or TensorFlow, you are well on your way to a successful migration onto Trainium and Inferentia. A word of caution, however, is that not every model and operation is supported by these yet. Expect some<a id="_idIndexMarker479"/> friction as you develop and<a id="_idIndexMarker480"/> test. The AWS Neuron <strong class="bold">software development kit</strong> (<strong class="bold">SDK</strong>) is a<a id="_idIndexMarker481"/> great way to test <span class="No-Break">compatibility </span><span class="No-Break"><em class="italic">(7)</em></span><span class="No-Break">.</span></p>
			<p>There are two<a id="_idIndexMarker482"/> reasons to evaluate our <span class="No-Break">custom accelerators:</span></p>
			<ul>
				<li>First, it’s a new type of hardware. This is particularly valuable for you scientists out there because you could quite literally be the first person to use a certain type of model on this hardware in the world. This may actually increase your odds of publication and recognition because you could develop a truly novel insight based on how this <span class="No-Break">model performs.</span></li>
				<li>Second, as with all of our new instances on AWS, the price-performance ratio should be substantially better than it <span class="No-Break">was previously.</span></li>
			</ul>
			<p>What is a price-performance ratio? Consider each task you need to complete. In the case of Inferentia, that would be model inference requests completed. For Trainium, that would be steps through your training loop. Then consider the cost for each task to be completed. Now you have your ratio! Our Trn1 instances offer up to 50% cost-to-train savings over comparable GPU instances, and Amazon Search reduced their inference costs by 85% with <span class="No-Break">Inferentia </span><span class="No-Break"><em class="italic">(8)</em></span><span class="No-Break">.</span></p>
			<p>Now that <a id="_idIndexMarker483"/>we’ve <a id="_idIndexMarker484"/>looked at Trainium and Inferentia at a very high level, let’s explore how to solve for an optimal training time using the <span class="No-Break">scaling laws.</span></p>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor148"/>Solving for an optimal training time</h1>
			<p>Time is an interesting<a id="_idIndexMarker485"/> construct in training large vision and language models. On the one hand, you might consider it a hyperparameter, simply the number of epochs. On the other hand, you might consider it a facet of your training data, its total number of tokens or images. You might also consider it a fixed input to your project, your total compute budget. Most research teams I work with use their intuition and good judgment to use a combination of all <span class="No-Break">of these.</span></p>
			<p>As we learned earlier in the book, the proposed <em class="italic">scaling laws</em> provide an interesting theoretical tool you can use to predict the performance of your model. Their original author, Kaplan et al. <em class="italic">(9)</em>, actually suggested that optimal usage of a given compute budget should stop “significantly before convergence.” They proposed this because of their proposed insight into large language models being more “sample efficient” than <span class="No-Break">smaller ones.</span></p>
			<p>However, 2022 saw these original laws being turn on their head. In this visual, you can see the theoretical predictions determined by a new set of scaling laws <span class="No-Break">from </span><span class="No-Break"><strong class="bold">Chinchilla</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B18942_image_09_04.jpg" alt="Figure 9.4 – Improved performance from Hoffman et al., 2022 (10)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Improved performance from Hoffman et al., 2022 <em class="italic">(10)</em></p>
			<p>Here Hoffman et al. make the elegant proposal that <em class="italic">training data and model sizes should be increased linearly</em>. That is to say, if you double the size of the model, you should double the size of the training data. I appreciate this natural symmetry and find it quite intuitive. Happily, these <a id="_idIndexMarker486"/>predictions were validated by extensive empirical evidence across no less than 400 models, 150 downstream tasks, and 6 domains, including language modeling, reading comprehension, question answering, common sense reasoning, and more. Per the authors, “Chinchilla uniformly and significantly outperforms <strong class="bold">Gopher (280 B)</strong>, <strong class="bold">GPT-3 (175 B)</strong>, <strong class="bold">Jurassic-1 (178 B)</strong>, and <strong class="bold">Megatron-Turing NLG (530B)</strong> on a large range of downstream evaluation tasks.” This implies that these models are undertrained, actually needing much larger datasets to validate their <span class="No-Break">parameter size.</span></p>
			<p>Using these equations and a massive set of experimental results, the authors suggest the following set of values for parameters, FLOPS, <span class="No-Break">and tokens:</span></p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B18942_image_09_05.jpg" alt="Figure 9.5 – Suggested FLOPS and tokens per model size"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Suggested FLOPS and tokens per model size</p>
			<p>Remember that when<a id="_idIndexMarker487"/> we look at this FLOPS value, we need to consider <span class="No-Break">the following:</span></p>
			<ol>
				<li>What value do I expect this model to bring to <span class="No-Break">my organization?</span></li>
				<li>From this, what total compute budget can I <span class="No-Break">plan for?</span></li>
				<li>How large is my <span class="No-Break">training data?</span></li>
				<li>What size of a model should I use based <span class="No-Break">on this?</span></li>
				<li>How efficient are my training loop and distributed system? Said another way, how many TFLOPS per GPU am I able to <span class="No-Break">eke out?</span></li>
				<li>How many GPUs can I get from my <span class="No-Break">cloud provider?</span></li>
				<li>How long will I need to run my entire training loop to train <span class="No-Break">to convergence?</span></li>
			</ol>
			<p>The answers to questions (<em class="italic">1</em>), (<em class="italic">2</em>), (<em class="italic">3</em>), and (<em class="italic">5</em>) can only come from you. The answers to questions (<em class="italic">4</em>) and (<em class="italic">7</em>) are functional derivatives of the previous answers. The answer to question (<em class="italic">6</em>), I would say is halfway between a functional derivative from the answer to (<em class="italic">1</em>), and a simple fact of the market at that point in time. If there’s a global supply chain issue for electronics, then accessing GPUs will <span class="No-Break">be hard.</span></p>
			<p>Whew, you <a id="_idIndexMarker488"/>made it through the advanced chapter! Now let’s do a quick concept recap, and then we’ll move into <em class="italic">Part Four: Evaluate </em><span class="No-Break"><em class="italic">your model</em></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor149"/>Summary</h1>
			<p>In this chapter, we covered some advanced concepts in training large-scale vision and language models. First, you learned how to evaluate and improve throughput by computing model TFLOPS per GPU, and using this as one of a number of metrics to compare experimental results. You learned about FlashAttention, and how its I/O-aware optimized quadratic for-loop speeds up the Transformer self-attention mechanism by as much as 3–5 times. You learned about compilation using methods built into PyTorch natively and those managed by AWS. You also learned about a few different types of compilation methods. You learned to update your hyperparameters for compilation, in addition to cases where the compilation is expected to provide a boost (<span class="No-Break">or not).</span></p>
			<p>You also learned about how to use compilers to run on Amazon’s custom hardware for machine learning, Trainium, and Inferentia. Lastly, we used the scaling laws to solve for an optimal <span class="No-Break">train time.</span></p>
			<p>In the next chapter, you’ll learn how to fine-tune your model and compare it with open <span class="No-Break">source alternatives.</span></p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor150"/>References</h1>
			<ol>
				<li>FlashAttention: Fast and Memory-Efficient Exact Attention with <span class="No-Break">IO-Awareness: </span><a href="https://arxiv.org/pdf/2205.14135.pdf"><span class="No-Break">https://arxiv.org/pdf/2205.14135.pdf</span></a></li>
				<li><span class="No-Break">HazyResearch/flash-attention: </span><a href="https://github.com/HazyResearch/flash-attention"><span class="No-Break">https://github.com/HazyResearch/flash-attention</span></a></li>
				<li>New performance improvements in Amazon SageMaker model parallel <span class="No-Break">library: </span><a href="https://aws.amazon.com/blogs/machine-learning/new-performance-improvements-in-amazon-sagemaker-model-parallel-library/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/new-performance-improvements-in-amazon-sagemaker-model-parallel-library/</span></a></li>
				<li>The Deep Learning Compiler: A Comprehensive <span class="No-Break">Survey: </span><a href="https://arxiv.org/pdf/2002.03794.pdf"><span class="No-Break">https://arxiv.org/pdf/2002.03794.pdf</span></a></li>
				<li>TORCH.COMPILE <span class="No-Break">TUTORIAL: </span><a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html"><span class="No-Break">https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html</span></a></li>
				<li>Enterprise <span class="No-Break">Customers: </span><a href="https://aws.amazon.com/pricing/enterprise/"><span class="No-Break">https://aws.amazon.com/pricing/enterprise/</span></a></li>
				<li>Welcome to AWS <span class="No-Break">Neuron: </span><a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/"><span class="No-Break">https://awsdocs-neuron.readthedocs-hosted.com/en/latest/</span></a></li>
				<li>How Amazon Search reduced ML inference costs by 85% with AWS <span class="No-Break">Inferentia: </span><a href="https://aws.amazon.com/blogs/machine-learning/how-amazon-search-reduced-ml-inference-costs-by-85-with-aws-inferentia/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/how-amazon-search-reduced-ml-inference-costs-by-85-with-aws-inferentia/</span></a></li>
				<li>Scaling Laws for Neural Language <span class="No-Break">Models: </span><a href="https://arxiv.org/abs/2001.08361"><span class="No-Break">https://arxiv.org/abs/2001.08361</span></a></li>
				<li>Training Compute-Optimal Large Language <span class="No-Break">Models: </span><a href="https://arxiv.org/pdf/2203.15556.pdf"><span class="No-Break">https://arxiv.org/pdf/2203.15556.pdf</span></a></li>
				<li>Training Compute-Optimal Large Language <span class="No-Break">Models: </span><a href="https://openreview.net/pdf?id=iBBcRUlOAPR"><span class="No-Break">https://openreview.net/pdf?id=iBBcRUlOAPR</span></a></li>
			</ol>
		</div>
	

		<div id="_idContainer078" class="Content">
			<h1 id="_idParaDest-131"><a id="_idTextAnchor151"/>Part 4: Evaluate Your Model</h1>
			<p>In part 4, you’ll learn how to evaluate your model. You’ll use the scaling laws to identify the shortest possible training time, fine-tune your model to compare with public benchmarks, and identify and <span class="No-Break">mitigate bias.</span></p>
			<p>This section has the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B18942_10.xhtml#_idTextAnchor152"><em class="italic">Chapter 10</em></a>, <em class="italic">Fine-Tuning and Evaluating </em></li>
				<li><a href="B18942_11.xhtml#_idTextAnchor167"><em class="italic">Chapter 11</em></a>, <em class="italic">Detecting, Mitigating, and Monitoring Bias</em></li>
				<li><a href="B18942_12.xhtml#_idTextAnchor178"><em class="italic">Chapter 12</em></a>, <em class="italic">How to Deploy Your Model</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer079" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer080">
			</div>
		</div>
	</body></html>