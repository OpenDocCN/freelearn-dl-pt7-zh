<html><head></head><body>
		<div id="_idContainer829" class="Content">
			<h1 id="_idParaDest-335"><a id="_idTextAnchor379"/>Appendix</h1>
		</div>
		<div id="_idContainer857" class="Content">
			<h1 id="_idParaDest-336"><a id="_idTextAnchor380"/>1. Introduction to Reinforcement Learning</h1>
			<h2 id="_idParaDest-337">Activity 1.01: Measuring t<a id="_idTextAnchor381"/>he Performance of a Random Agent</h2>
			<ol>
				<li>Import the required libraries – <strong class="source-inline">abc</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">gym</strong>:<p class="source-code">import abc</p><p class="source-code">import numpy as np</p><p class="source-code">import gym</p></li>
				<li>Define the abstract class representing the agent:<p class="source-code">"""</p><p class="source-code">Abstract class representing the agent</p><p class="source-code">Init with the action space and the function pi returning the action</p><p class="source-code">"""</p><p class="source-code">class Agent:</p><p class="source-code">    def __init__(self, action_space: gym.spaces.Space):</p><p class="source-code">        """</p><p class="source-code">        Constructor of the agent class.</p><p class="source-code">        Args:</p><p class="source-code">            action_space (gym.spaces.Space): environment action space</p><p class="source-code">        """</p><p class="source-code">        raise NotImplementedError("This class cannot be instantiated.")</p><p class="source-code">    @abc.abstractmethod</p><p class="source-code">    def pi(self, state: np.ndarray) -&gt; np.ndarray:</p><p class="source-code">        """</p><p class="source-code">        Agent's policy.</p><p class="source-code">        Args:</p><p class="source-code">            state (np.ndarray): environment state</p><p class="source-code">        Returns:</p><p class="source-code">            The selected action</p><p class="source-code">        """</p><p class="source-code">        pass</p><p>An agent is represented by only a constructor and an abstract method, <strong class="source-inline">pi</strong>. This method is the actual policy; it takes as input the environment state and returns the selected action.</p></li>
				<li>Define a continuous agent. A continuous agent has to initialize the probability distribution according to the action space passed as an input to the constructor:<p class="source-code">class ContinuousAgent(Agent):</p><p class="source-code">    def __init__(self, action_space: gym.spaces.Space, seed=46):</p><p class="source-code">        # setup seed</p><p class="source-code">        np.random.seed(seed)</p><p class="source-code">        # check the action space type</p><p class="source-code">        if not isinstance(action_space, gym.spaces.Box):</p><p class="source-code">            raise ValueError\</p><p class="source-code">                  ("This is a Continuous Agent pass as "\</p><p class="source-code">                   "input a Box Space.")</p></li>
				<li>If the upper and lower bounds are infinite, the probability distribution is simply a normal distribution centered at 0, with a scale that is equal to 1:<p class="source-code">        """</p><p class="source-code">        initialize the distribution according to the action space type</p><p class="source-code">        """</p><p class="source-code">        if (action_space.low == -np.inf) and \</p><p class="source-code">           (action_space.high == np.inf):</p><p class="source-code">            # the distribution is a normal distribution</p><p class="source-code">            self._pi = lambda: np.random.normal\</p><p class="source-code">                               (loc=0, scale=1, \</p><p class="source-code">                                size=action_space.shape)</p><p class="source-code">            return</p></li>
				<li>If the upper and lower bounds are both finite, the distribution is a uniform distribution defined in that range:<p class="source-code">        if (action_space.low != -np.inf) and \</p><p class="source-code">           (action_space.high != np.inf):</p><p class="source-code">            # the distribution is a uniform distribution</p><p class="source-code">            self._pi = lambda: np.random.uniform\</p><p class="source-code">                       (low=action_space.low, \</p><p class="source-code">                        high=action_space.high, \</p><p class="source-code">                        size=action_space.shape)</p><p class="source-code">            return</p><p>If the lower bound is <img src="image/B16182_01_a.png" alt="1"/>, the probability distribution is a shifted negative exponential distribution:</p><p class="source-code">        if action_space.low == -np.inf:</p><p class="source-code">            # negative exponential distribution</p><p class="source-code">            self._pi = (lambda: -np.random.exponential\</p><p class="source-code">                        (size=action_space.shape)</p><p class="source-code">                        + action_space.high)</p><p class="source-code">            return</p><p>If the upper bound is <img src="image/B16182_01_b.png" alt="2"/>, the probability distribution is a shifted exponential distribution:</p><p class="source-code">        if action_space.high == np.inf:</p><p class="source-code">            # exponential distribution</p><p class="source-code">            self._pi = (lambda: np.random.exponential\</p><p class="source-code">                        (size=action_space.shape)</p><p class="source-code">                        + action_space.low)</p><p class="source-code">            return</p></li>
				<li>Define the <strong class="source-inline">pi</strong> method, which is simply a call to the distribution defined in the constructor:<p class="source-code">    def pi(self, observation: np.ndarray) -&gt; np.ndarray:</p><p class="source-code">        """</p><p class="source-code">        Policy: simply call the internal _pi().</p><p class="source-code">        </p><p class="source-code">        This is a random agent, so the action is independent </p><p class="source-code">        from the observation.</p><p class="source-code">        For real agents the action depends on the observation.</p><p class="source-code">        """</p><p class="source-code">        return self._pi()</p></li>
				<li>We are ready to define the discrete agent. As before, the agent has to correctly initialize the action distribution according to the action space that is passed as a parameter:<p class="source-code">class DiscreteAgent(Agent):</p><p class="source-code">    def __init__(self, action_space: gym.spaces.Space, seed=46):</p><p class="source-code">        # setup seed</p><p class="source-code">        np.random.seed(seed)</p><p class="source-code">        # check the action space type</p><p class="source-code">        if not isinstance(action_space, gym.spaces.Discrete):</p><p class="source-code">            raise ValueError("This is a Discrete Agent pass "\</p><p class="source-code">                             "as input a Discrete Space.")</p><p class="source-code">        """</p><p class="source-code">        initialize the distribution according to the action </p><p class="source-code">        space n attribute</p><p class="source-code">        """</p><p class="source-code">        # the distribution is a uniform distribution</p><p class="source-code">        self._pi = lambda: np.random.randint\</p><p class="source-code">                   (low=0, high=action_space.n)</p><p class="source-code">    def pi(self, observation: np.ndarray) -&gt; np.ndarray:</p><p class="source-code">        """</p><p class="source-code">        Policy: simply call the internal _pi().</p><p class="source-code">        This is a random agent, so the action is independent </p><p class="source-code">        from the observation.</p><p class="source-code">        For real agents the action depends on the observation.</p><p class="source-code">        """</p><p class="source-code">        return self._pi()</p></li>
				<li>Now it is useful to define a utility function to create the correct agent type based on the action space:<p class="source-code">def make_agent(action_space: gym.spaces.Space, seed=46):</p><p class="source-code">    """</p><p class="source-code">    Returns the correct agent based on the action space type</p><p class="source-code">    """</p><p class="source-code">    if isinstance(action_space, gym.spaces.Discrete):</p><p class="source-code">        return DiscreteAgent(action_space, seed)</p><p class="source-code">    if isinstance(action_space, gym.spaces.Box):</p><p class="source-code">        return ContinuousAgent(action_space, seed)</p><p class="source-code">    raise ValueError("Only Box spaces or Discrete Spaces "\</p><p class="source-code">                     "are allowed, check the action space of "\</p><p class="source-code">                     "the environment")</p></li>
				<li>The last step is to define the RL loop in which the agent interacts with the environment and collects rewards.<p>Define the parameters, and then create the environment and the agent:</p><p class="source-code"># Environment Name</p><p class="source-code">env_name = "CartPole-v0"</p><p class="source-code"># Number of episodes</p><p class="source-code">episodes = 10</p><p class="source-code"># Number of Timesteps of each episode</p><p class="source-code">timesteps = 100</p><p class="source-code"># Discount factor</p><p class="source-code">gamma = 1.0</p><p class="source-code"># seed environment</p><p class="source-code">seed = 46</p><p class="source-code"># Needed to show the environment in a notebook</p><p class="source-code">from gym import wrappers</p><p class="source-code">env = gym.make(env_name)</p><p class="source-code">env.seed(seed)</p><p class="source-code"># the last argument is needed to record all episodes</p><p class="source-code"># otherwise gym would record only some of them</p><p class="source-code"># The monitor saves the episodes inside the folder ./gym-results</p><p class="source-code">env = wrappers.Monitor(env, "./gym-results", force=True, \</p><p class="source-code">                       video_callable=lambda episode_id: True)</p><p class="source-code">agent = make_agent(env.action_space, seed)</p></li>
				<li>We have to track the returns for each episode; to do this, we can use a simple list:<p class="source-code"># list of returns</p><p class="source-code">episode_returns = []</p></li>
				<li>Start a loop for each episode:<p class="source-code"># loop for the episodes</p><p class="source-code">for episode_number in range(episodes):</p><p class="source-code">    # here we are inside an episode</p></li>
				<li>Initialize the variables for the calculation of the cumulated discount factor and the current episode return:<p class="source-code">    # reset cumulated gamma</p><p class="source-code">    gamma_cum = 1</p><p class="source-code">    # return of the current episode</p><p class="source-code">    episode_return = 0</p></li>
				<li>Reset the environment and get the first observation:<p class="source-code">    # the reset function resets the environment and returns</p><p class="source-code">    # the first environment observation</p><p class="source-code">    observation = env.reset()</p></li>
				<li>Loop for the number of timesteps:<p class="source-code">    # loop for the given number of timesteps or</p><p class="source-code">    # until the episode is terminated</p><p class="source-code">    for timestep_number in range(timesteps):</p></li>
				<li>Render the environment, select the action, and then apply it:<p class="source-code">        # if you want to render the environment</p><p class="source-code">        # uncomment the following line</p><p class="source-code">        # env.render()</p><p class="source-code">        # select the action</p><p class="source-code">        action = agent.pi(observation)</p><p class="source-code">        # apply the selected action by calling env.step</p><p class="source-code">        observation, reward, done, info = env.step(action)</p></li>
				<li>Increment the return, and calculate the cumulated discount factor:<p class="source-code">        # increment the return</p><p class="source-code">        episode_return += reward * gamma_cum</p><p class="source-code">        # update the value of cumulated discount factor</p><p class="source-code">        gamma_cum = gamma_cum * gamma</p></li>
				<li>If the episode is terminated, break from the timestep's loop:<p class="source-code">        """</p><p class="source-code">        if done the episode is terminated, we have to reset</p><p class="source-code">        the environment</p><p class="source-code">        """</p><p class="source-code">        if done:</p><p class="source-code">            print(f"Episode Number: {episode_number}, \</p><p class="source-code">Timesteps: {timestep_number}, Return: {episode_return}")</p><p class="source-code">            # break from the timestep loop</p><p class="source-code">            break</p></li>
				<li>After the timestep loop, we have to record the current return by appending it to the list of returns for each episode:<p class="source-code">    episode_returns.append(episode_return)</p></li>
				<li>After the episode loop, close the environment and calculate <strong class="source-inline">statistics</strong>:<p class="source-code"># close the environment</p><p class="source-code">env.close()</p><p class="source-code"># Calculate return statistics</p><p class="source-code">avg_return = np.mean(episode_returns)</p><p class="source-code">std_return = np.std(episode_returns)</p><p class="source-code">var_return = std_return ** 2  # variance is std^2</p><p class="source-code">print(f"Statistics on Return: Average: {avg_return}, \</p><p class="source-code">Variance: {var_return}")</p><p>You will get the following results:</p><p class="source-code">Episode Number: 0, Timesteps: 27, Return: 28.0</p><p class="source-code">Episode Number: 1, Timesteps: 9, Return: 10.0</p><p class="source-code">Episode Number: 2, Timesteps: 13, Return: 14.0</p><p class="source-code">Episode Number: 3, Timesteps: 16, Return: 17.0</p><p class="source-code">Episode Number: 4, Timesteps: 31, Return: 32.0</p><p class="source-code">Episode Number: 5, Timesteps: 10, Return: 11.0</p><p class="source-code">Episode Number: 6, Timesteps: 14, Return: 15.0</p><p class="source-code">Episode Number: 7, Timesteps: 11, Return: 12.0</p><p class="source-code">Episode Number: 8, Timesteps: 10, Return: 11.0</p><p class="source-code">Episode Number: 9, Timesteps: 30, Return: 31.0</p><p class="source-code">Statistics on Return: Average: 18.1, Variance: 68.89000000000001</p></li>
			</ol>
			<p>In this activity, we implemented two different types of agents: a discrete agent, working with discrete environments, and a continuous agent, working with continuous environments.</p>
			<p>Additionally, you can render the episodes inside a notebook using the following code:</p>
			<p class="source-code"># Render the episodes</p>
			<p class="source-code">import io</p>
			<p class="source-code">import base64</p>
			<p class="source-code">from IPython.display import HTML, display</p>
			<p class="source-code">episodes_to_watch = 1</p>
			<p class="source-code">for episode in range(episodes_to_watch):</p>
			<p class="source-code">    video = io.open(f"./gym-results/openaigym.video\</p>
			<p class="source-code">.{env.file_infix}.video{episode:06d}.mp4", "r+b").read()</p>
			<p class="source-code">    encoded = base64.b64encode(video)</p>
			<p class="source-code">    display(</p>
			<p class="source-code">        HTML(</p>
			<p class="source-code">            data="""</p>
			<p class="source-code">        &lt;video width="360" height="auto" alt="test" controls&gt;</p>
			<p class="source-code">        &lt;source src="data:video/mp4;base64,{0}" type="video/mp4" /&gt;</p>
			<p class="source-code">        &lt;/video&gt;""".format(</p>
			<p class="source-code">                encoded.decode("ascii")</p>
			<p class="source-code">            )</p>
			<p class="source-code">        )</p>
			<p class="source-code">    )</p>
			<p>You can see the episode duration is not too long. This is because the actions are taken at random, so the pole falls after some timesteps.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fbxR3Y">https://packt.live/3fbxR3Y</a>.</p>
			<p class="callout">This section does not currently have an online interactive example and will need to be run locally.</p>
			<p>Discrete and continuous agents are two different possibilities when facing a new RL problem.</p>
			<p>We have designed our agents in a very flexible way so that they can be applied to almost all environments without having to change the code.</p>
			<p>We also implemented a simple RL loop and measured the performance of our agent on a classical RL problem. </p>
			<h1 id="_idParaDest-338"><a id="_idTextAnchor382"/>2. Markov Decision Processes and Bellman Equations</h1>
			<h2 id="_idParaDest-339"><a id="_idTextAnchor383"/>Activity 2.01: Solving Gridworld</h2>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">from enum import Enum, auto</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">from scipy import linalg</p><p class="source-code">from typing import Tuple</p></li>
				<li>Define the <strong class="source-inline">visualization</strong> function:<p class="source-code"># helper function</p><p class="source-code">def vis_matrix(M, cmap=plt.cm.Blues):</p><p class="source-code">    fig, ax = plt.subplots()</p><p class="source-code">    ax.matshow(M, cmap=cmap)</p><p class="source-code">    for i in range(M.shape[0]):</p><p class="source-code">        for j in range(M.shape[1]):</p><p class="source-code">            c = M[j, i]</p><p class="source-code">            ax.text(i, j, "%.2f" % c, va="center", ha="center")</p></li>
				<li>Define the possible actions:<p class="source-code"># Define the actions</p><p class="source-code">class Action(Enum):</p><p class="source-code">    UP = auto()</p><p class="source-code">    DOWN = auto()</p><p class="source-code">    LEFT = auto()</p><p class="source-code">    RIGHT = auto()</p></li>
				<li>Define the <strong class="source-inline">Policy</strong> class, representing the random policy:<p class="source-code"># Agent Policy, random</p><p class="source-code">class Policy:</p><p class="source-code">    def __init__(self):</p><p class="source-code">        self._possible_actions = [action for action in Action]</p><p class="source-code">        self._action_probs = {a: 1 / len(self._possible_actions) \</p><p class="source-code">                              for a in self._possible_actions}</p><p class="source-code">    def __call__(self, state: Tuple[int, int], \</p><p class="source-code">                 action: Action) -&gt; float:</p><p class="source-code">        """</p><p class="source-code">        Returns the action probability</p><p class="source-code">        """</p><p class="source-code">        assert action in self._possible_actions</p><p class="source-code">        # state is unused for this policy</p><p class="source-code">        return self._action_probs[action]</p></li>
				<li>Define the <strong class="source-inline">Environment</strong> class and the <strong class="source-inline">step</strong> function:<p class="source-code">class Environment:</p><p class="source-code">    def __init__(self):</p><p class="source-code">        self.grid_width = 5</p><p class="source-code">        self.grid_height = 5</p><p class="source-code">        self._good_state1 = (0, 1)</p><p class="source-code">        self._good_state2 = (0, 3)</p><p class="source-code">        self._to_state1 = (4, 2)</p><p class="source-code">        self._to_state2 = (2, 3)</p><p class="source-code">        self._bad_state1 = (1, 1)</p><p class="source-code">        self._bad_state2 = (4, 4)</p><p class="source-code">        self._bad_states = [self._bad_state1, self._bad_state2]</p><p class="source-code">        self._good_states = [self._good_state1, self._good_state2]</p><p class="source-code">        self._to_states = [self._to_state1, self._to_state2]</p><p class="source-code">        self._good_rewards = [10, 5]</p><p class="source-code">    def step(self, state, action):</p><p class="source-code">        i, j = state</p><p class="source-code">        # search among good states</p><p class="source-code">        for good_state, reward, \</p><p class="source-code">            to_state in zip(self._good_states, \</p><p class="source-code">                            self._good_rewards, \</p><p class="source-code">                            self._to_states):</p><p class="source-code">            if (i, j) == good_state:</p><p class="source-code">                return (to_state, reward)</p><p class="source-code">        reward = 0</p><p class="source-code">        # if the state is a bad state, the reward is -1</p><p class="source-code">        if state in self._bad_states:</p><p class="source-code">            reward = -1</p><p class="source-code">        # calculate next state based on the action</p><p class="source-code">        if action == Action.LEFT:</p><p class="source-code">            j_next = max(j - 1, 0)</p><p class="source-code">            i_next = i</p><p class="source-code">            if j - 1 &lt; 0:</p><p class="source-code">                reward = -1</p><p class="source-code">        elif action == Action.RIGHT:</p><p class="source-code">            j_next = min(j + 1, self.grid_width - 1)</p><p class="source-code">            i_next = i</p><p class="source-code">            if j + 1 &gt; self.grid_width - 1:</p><p class="source-code">                reward = -1</p><p class="source-code">        elif action == Action.UP:</p><p class="source-code">            j_next = j</p><p class="source-code">            i_next = max(i - 1, 0)</p><p class="source-code">            if i - 1 &lt; 0:</p><p class="source-code">                reward = -1</p><p class="source-code">        elif action == Action.DOWN:</p><p class="source-code">            j_next = j</p><p class="source-code">            i_next = min(i + 1, self.grid_height - 1)</p><p class="source-code">            if i + 1 &gt; self.grid_height - 1:</p><p class="source-code">                reward = -1</p><p class="source-code">        else:</p><p class="source-code">             raise ValueError("Invalid action")</p><p class="source-code">        return ((i_next, j_next), reward)</p></li>
				<li>Loop for all states and actions and build the transition and reward matrices:<p class="source-code">pi = Policy()</p><p class="source-code">env = Environment()</p><p class="source-code"># setup probability matrix and reward matrix</p><p class="source-code">P = np.zeros((env.grid_width*env.grid_height, \</p><p class="source-code">              env.grid_width*env.grid_height))</p><p class="source-code">R = np.zeros_like(P)</p><p class="source-code">possible_actions = [action for action in Action]</p><p class="source-code"># Loop for all states and fill up P and R</p><p class="source-code">for i in range(env.grid_height):</p><p class="source-code">    for j in range(env.grid_width):</p><p class="source-code">        state = (i, j)</p><p class="source-code">        # loop for all action and setup P and R</p><p class="source-code">        for action in possible_actions:</p><p class="source-code">            next_state, reward = env.step(state, action)</p><p class="source-code">            (i_next, j_next) = next_state</p><p class="source-code">            P[i*env.grid_width+j, \</p><p class="source-code">              i_next*env.grid_width \</p><p class="source-code">              + j_next] += pi(state, action)</p><p class="source-code">            """</p><p class="source-code">            the reward depends only on the starting state and </p><p class="source-code">            the final state</p><p class="source-code">            """</p><p class="source-code">            R[i*env.grid_width+j, \</p><p class="source-code">              i_next*env.grid_width + j_next] = reward</p></li>
				<li>Check the correctness of the matrix:<p class="source-code"># check the correctness</p><p class="source-code">assert((np.sum(P, axis=1) == 1).all())</p></li>
				<li>Calculate the expected reward for each state:<p class="source-code"># expected reward for each state</p><p class="source-code">R_expected = np.sum(P * R, axis=1, keepdims=True)</p></li>
				<li>Use the function to visualize the expected reward:<p class="source-code"># reshape the state values in a matrix</p><p class="source-code">R_square = R_expected.reshape((env.grid_height,env.grid_width))</p><p class="source-code"># Visualize</p><p class="source-code">vis_matrix(R_square, cmap=plt.cm.Reds)</p><p>The function visualizes the matrix using Matplotlib. You should see something similar to this:</p><div id="_idContainer832" class="IMG---Figure"><img src="image/B16182_02_62.jpg" alt="Figure 2.62: The expected reward for each state&#13;&#10;"/></div><p class="figure-caption">Figure 2.62: The expected reward for each state</p><p>The previous figure is a color representation of the expected reward associated with each state considering the current policy. Notice that the expected reward of bad states is exactly equal to <strong class="source-inline">-1</strong>. The expected reward of good states is exactly equal to <strong class="source-inline">10</strong> and <strong class="source-inline">5</strong>, respectively.</p></li>
				<li>Now set up the matrix form of the Bellman expectation equation:<p class="source-code"># define the discount factor</p><p class="source-code">gamma = 0.9</p><p class="source-code"># Now it is possible to solve the Bellman Equation</p><p class="source-code">A = np.eye(env.grid_width*env.grid_height) - gamma * P</p><p class="source-code">B = R_expected</p></li>
				<li>Solve the Bellman equation:<p class="source-code"># solve using scipy linalg</p><p class="source-code">V = linalg.solve(A, B)</p></li>
				<li>Visualize the result:<p class="source-code"># reshape the state values in a matrix</p><p class="source-code">V_square = V.reshape((env.grid_height,env.grid_width))</p><p class="source-code"># visualize results</p><p class="source-code">vis_matrix(V_square, cmap=plt.cm.Reds)</p></li>
			</ol>
			<div>
				<div id="_idContainer833" class="IMG---Figure">
					<img src="image/B16182_02_63.jpg" alt="Figure 2.63: State values of Gridworld&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.63: State values of Gridworld</p>
			<p>Note that the value of good states is less than the expected reward from those states. This is because landing states have an expected reward that is negative or because landing states are close to states for which the reward is negative. You can see that the state with the higher value is state <img src="image/B16182_02_63a.png" alt="a"/>, followed by state <img src="image/B16182_02_63b.png" alt="b"/>. It is also interesting to note the high value of the state in position (<strong class="source-inline">0, 2</strong>), which is close to the good states.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Al9xOB">https://packt.live/2Al9xOB</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2UChxBy">https://packt.live/2UChxBy</a>.</p>
			<p>In this activity, we experimented with the Gridworld environment, one of the most common toy RL environments. We defined a random policy, and we solved the Bellman expectation equation using <strong class="source-inline">scipy.linalg.solve</strong> to find the state values of the policy.</p>
			<p>It is important to visualize the results, when possible, to get a better understanding and to spot any errors.</p>
			<h1 id="_idParaDest-340"><a id="_idTextAnchor384"/>3. Deep Learning in Practice with TensorFlow 2</h1>
			<h2 id="_idParaDest-341"><a id="_idTextAnchor385"/>Activity 3.01: Classifying Fashion Clothes Using a TensorFlow Dataset and TensorFlow 2</h2>
			<ol>
				<li value="1">Import all the required modules:<p class="source-code">from __future__ import absolute_import, division, \</p><p class="source-code">print_function, unicode_literals</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code"># TensorFlow</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_datasets as tfds</p></li>
				<li>Import the Fashion MNIST dataset using TensorFlow datasets and split it into train and test splits. Then, create a list of classes:<p class="source-code"># Construct a tf.data.Dataset</p><p class="source-code">(train_images, train_labels), (test_images, test_labels) = \</p><p class="source-code">tfds.as_numpy(tfds.load('fashion_mnist', \</p><p class="source-code">                        split=['train', 'test'],\</p><p class="source-code">                        batch_size=-1, as_supervised=True,))</p><p class="source-code">train_images = np.squeeze(train_images)</p><p class="source-code">test_images = np.squeeze(test_images)</p><p class="source-code">classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', \</p><p class="source-code">           'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', \</p><p class="source-code">           'Ankle boot']</p></li>
				<li>Explore the dataset to get familiar with the input features, that is, shapes, labels, and classes:<p class="source-code">print("Training dataset shape =", train_images.shape)</p><p class="source-code">print("Training labels length =", len(train_labels))</p><p class="source-code">print("Some training labels =", train_labels[:5])</p><p class="source-code">print("Test dataset shape =", test_images.shape)</p><p class="source-code">print("Test labels length =", len(test_labels))</p><p>The output will be as follows:</p><p class="source-code">Training dataset shape = (60000, 28, 28)</p><p class="source-code">Training labels length = 60000</p><p class="source-code">Some training labels = [2 1 8 4 1]</p><p class="source-code">Test dataset shape = (10000, 28, 28)</p><p class="source-code">Test labels length = 10000</p></li>
				<li>Visualize some instances of the training set.<p>It is also useful to take a look at how the images will appear. The following code snippet shows the first training set instance:</p><p class="source-code">plt.figure()</p><p class="source-code">plt.imshow(train_images[0])</p><p class="source-code">plt.colorbar()</p><p class="source-code">plt.grid(False)</p><p class="source-code">plt.show()</p><p>The output image will be as follows:</p><div id="_idContainer836" class="IMG---Figure"><img src="image/B16182_03_30.jpg" alt="Figure 3.30: First training image plot&#13;&#10;"/></div><p class="figure-caption">Figure 3.30: First training image plot</p></li>
				<li>Perform feature normalization:<p class="source-code">train_images = train_images / 255.0</p><p class="source-code">test_images = test_images / 255.0</p></li>
				<li>Now, let's take a look at some instances of our training set by plotting <strong class="source-inline">25</strong> of them with their corresponding labels:<p class="source-code">plt.figure(figsize=(10,10))</p><p class="source-code">for i in range(25):</p><p class="source-code">    plt.subplot(5,5,i+1)</p><p class="source-code">    plt.xticks([])</p><p class="source-code">    plt.yticks([])</p><p class="source-code">    plt.grid(False)</p><p class="source-code">    plt.imshow(train_images[i], cmap=plt.cm.binary)</p><p class="source-code">    plt.xlabel(classes[train_labels[i]])</p><p class="source-code">plt.show()</p><p>The output image will be as follows:</p><div id="_idContainer837" class="IMG---Figure"><img src="image/B16182_03_31.jpg" alt="Figure 3.31: A set of 25 training samples and their corresponding labels&#13;&#10;"/></div><p class="figure-caption">Figure 3.31: A set of 25 training samples and their corresponding labels</p></li>
				<li>Build the classification model. First, create a model using a layers' sequence:<p class="source-code">model = tf.keras.Sequential\</p><p class="source-code">        ([tf.keras.layers.Flatten(input_shape=(28, 28)),\</p><p class="source-code">          tf.keras.layers.Dense(128, activation='relu'),\</p><p class="source-code">          tf.keras.layers.Dense(10)])</p></li>
				<li>Then, associate the model with an <strong class="source-inline">optimizer</strong>, a <strong class="source-inline">loss</strong> function, and a <strong class="source-inline">metrics</strong>:<p class="source-code">model.compile(optimizer='adam',\</p><p class="source-code">              loss=tf.keras.losses.SparseCategoricalCrossentropy\</p><p class="source-code">              (from_logits=True), metrics=['accuracy'])</p></li>
				<li>Train the deep neural network:<p class="source-code">model.fit(train_images, train_labels, epochs=10)</p><p>The last output lines will be as follows:</p><p class="source-code">Epoch 9/1060000/60000 [==============================] \</p><p class="source-code">- 2s 40us/sample - loss: 0.2467 - accuracy: 0.9076</p><p class="source-code">Epoch 10/1060000/60000 [==============================] \</p><p class="source-code">- 2s 40us/sample - loss: 0.2389 - accuracy: 0.9103</p></li>
				<li>Test the model's accuracy. The accuracy should be in excess of 88%.</li>
				<li>Evaluate the model on the test set and print the accuracy score:<p class="source-code">test_loss, test_accuracy = model.evaluate\</p><p class="source-code">                           (test_images, test_labels, verbose=2)</p><p class="source-code">print('\nTest accuracy:', test_accuracy)</p><p>The output will be as follows:</p><p class="source-code">10000/10000 - 0s - loss: 0.3221 - accuracy: 0.8878</p><p class="source-code">Test accuracy: 0.8878</p><p class="callout-heading">Note</p><p class="callout">The accuracy may show slightly different values due to random sampling with a variable random seed.</p></li>
				<li>Perform inference and check the predictions against the ground truth.<p>As a first step, add a <strong class="source-inline">softmax</strong> layer to the model so that it outputs probabilities instead of logits. Then, print out the probabilities of the first test instance with the following code:</p><p class="source-code">probability_model = tf.keras.Sequential\</p><p class="source-code">                    ([model,tf.keras.layers.Softmax()])</p><p class="source-code">predictions = probability_model.predict(test_images)</p><p class="source-code">print(predictions[0:3])</p><p>The output will be as follows:</p><p class="source-code">[[3.85897374e-06 2.33953915e-06 2.30801385e-02 4.74092474e-07</p><p class="source-code">  9.55752671e-01 1.56392260e-10 2.11589299e-02 8.57651870e-08</p><p class="source-code">  1.49855202e-06 1.05843508e-10]</p></li>
				<li>Next, compare one model prediction (that is, the class with the highest predicted probability), the one on the first test instance, with its ground truth:<p class="source-code">print("Class ID, predicted | real =", \</p><p class="source-code">      np.argmax(predictions[0]), "|", test_labels[0])</p><p>The output will be as follows:</p><p class="source-code">Class ID, predicted | real = 4 | 4</p></li>
				<li>In order to perform a comparison that's even clearer, create the following two functions. The first one plots the <strong class="source-inline">i</strong>-th test set instance image with a caption showing the predicted class with the highest probability, its probability in percent, and the ground truth between round brackets. This caption will be <strong class="source-inline">blue</strong> for correct predictions, and <strong class="source-inline">red</strong> for incorrect ones:<p class="source-code">def plot_image(i, predictions_array, true_label, img):</p><p class="source-code">    predictions_array, true_label, img = predictions_array,\</p><p class="source-code">                                         true_label[i], img[i]</p><p class="source-code">    plt.grid(False)</p><p class="source-code">    plt.xticks([])</p><p class="source-code">    plt.yticks([])</p><p class="source-code">    plt.imshow(img, cmap=plt.cm.binary)</p><p class="source-code">    predicted_label = np.argmax(predictions_array)</p><p class="source-code">    if predicted_label == true_label:</p><p class="source-code">        color = 'blue'</p><p class="source-code">    else:</p><p class="source-code">        color = 'red'</p><p class="source-code">    plt.xlabel("{} {:2.0f}% ({})".format\</p><p class="source-code">               (classes[predicted_label], \</p><p class="source-code">                100*np.max(predictions_array),\</p><p class="source-code">                classes[true_label]),\</p><p class="source-code">                color=color)</p></li>
				<li>The second function creates a second image showing a bar plot of all classes' predicted probabilities. It will color the highest probable one in <strong class="source-inline">blue</strong> if the prediction is correct, or in <strong class="source-inline">red</strong> if it is incorrect. In this second case, the bar corresponding to the correct label is colored in <strong class="source-inline">blue</strong>:<p class="source-code">def plot_value_array(i, predictions_array, true_label):</p><p class="source-code">    predictions_array, true_label = predictions_array,\</p><p class="source-code">                                    true_label[i]</p><p class="source-code">    plt.grid(False)</p><p class="source-code">    plt.xticks(range(10))</p><p class="source-code">    plt.yticks([])</p><p class="source-code">    thisplot = plt.bar(range(10), predictions_array,\</p><p class="source-code">               color="#777777")</p><p class="source-code">    plt.ylim([0, 1])</p><p class="source-code">    predicted_label = np.argmax(predictions_array)</p><p class="source-code">    thisplot[predicted_label].set_color('red')</p><p class="source-code">    thisplot[true_label].set_color('blue')</p></li>
				<li>Using these two functions, we can examine every instance of the test set. In the following snippet, the first test instance is being plotted:<p class="source-code">i = 0</p><p class="source-code">plt.figure(figsize=(6,3))</p><p class="source-code">plt.subplot(1,2,1)</p><p class="source-code">plot_image(i, predictions[i], test_labels, test_images)</p><p class="source-code">plt.subplot(1,2,2)</p><p class="source-code">plot_value_array(i, predictions[i],  test_labels)</p><p class="source-code">plt.show()</p><p>The output will be as follows:</p><div id="_idContainer838" class="IMG---Figure"><img src="image/B16182_03_32.jpg" alt="Figure 3.32: First test instance, correctly predicted&#13;&#10;"/></div><p class="figure-caption">Figure 3.32: First test instance, correctly predicted</p></li>
				<li>The very same approach can be used to plot a user-defined number of test instances, arranging the output in subplots, as follows:<p class="source-code">"""</p><p class="source-code">Plot the first X test images, their predicted labels, and the true labels.</p><p class="source-code">Color correct predictions in blue and incorrect predictions in red.</p><p class="source-code">"""</p><p class="source-code">num_rows = 5</p><p class="source-code">num_cols = 3</p><p class="source-code">num_images = num_rows*num_cols</p><p class="source-code">plt.figure(figsize=(2*2*num_cols, 2*num_rows))</p><p class="source-code">for i in range(num_images):</p><p class="source-code">    plt.subplot(num_rows, 2*num_cols, 2*i+1)</p><p class="source-code">    plot_image(i, predictions[i], test_labels, test_images)</p><p class="source-code">    plt.subplot(num_rows, 2*num_cols, 2*i+2)</p><p class="source-code">    plot_value_array(i, predictions[i], test_labels)</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p><p>The output will be as follows:</p><div id="_idContainer839" class="IMG---Figure"><img src="image/B16182_03_33.jpg" alt="Figure 3.33: First 25 test instances with their predicted classes and ground truth comparison&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.33: First 25 test instances with their predicted classes and ground truth comparison</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3dXv3am">https://packt.live/3dXv3am</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Ux5JR5">https://packt.live/2Ux5JR5</a>.</p>
			<p>In this activity, we faced a problem that is quite similar to a real-world one. We had to deal with complex high dimensional inputs – in our case, grayscale images – and we wanted to build a model capable of autonomously grouping them into 10 different categories. Thanks to the power of deep learning and state-of-the-art machine learning frameworks, we were able to build a fully connected neural network that achieves a classification accuracy in excess of 88%.</p>
			<h1 id="_idParaDest-342"><a id="_idTextAnchor386"/>4. Getting started with OpenAI and TensorFlow for Reinforcement Learning</h1>
			<h2 id="_idParaDest-343"><a id="_idTextAnchor387"/>Activity 4.01: Training a Reinforcement Learning Agent to Play a Classic Video Game</h2>
			<ol>
				<li value="1">Import all the required modules from OpenAI Baselines and TensorFlow in order to use the <strong class="source-inline">PPO</strong> algorithm:<p class="source-code">from baselines.ppo2.ppo2 import learn</p><p class="source-code">from baselines.ppo2 import defaults</p><p class="source-code">from baselines.common.vec_env import VecEnv, VecFrameStack</p><p class="source-code">from baselines.common.cmd_util import make_vec_env, make_env</p><p class="source-code">from baselines.common.models import register</p><p class="source-code">import tensorflow as tf</p></li>
				<li>Define and register a custom convolutional neural network for the policy network:<p class="source-code">@register("custom_cnn")</p><p class="source-code">def custom_cnn():</p><p class="source-code">    def network_fn(input_shape, **conv_kwargs):</p><p class="source-code">        """</p><p class="source-code">        Custom CNN</p><p class="source-code">        """</p><p class="source-code">        print('input shape is {}'.format(input_shape))</p><p class="source-code">        x_input = tf.keras.Input\</p><p class="source-code">                  (shape=input_shape, dtype=tf.uint8)</p><p class="source-code">        h = x_input</p><p class="source-code">        h = tf.cast(h, tf.float32) / 255.</p><p class="source-code">        h = tf.keras.layers.Conv2D\</p><p class="source-code">            (filters=32,kernel_size=8,strides=4, \</p><p class="source-code">             padding='valid', data_format='channels_last',\</p><p class="source-code">             activation='relu')(h)</p><p class="source-code">        h2 = tf.keras.layers.Conv2D\</p><p class="source-code">             (filters=64, kernel_size=4,strides=2,\</p><p class="source-code">              padding='valid', data_format='channels_last',\</p><p class="source-code">              activation='relu')(h)</p><p class="source-code">        h3 = tf.keras.layers.Conv2D\</p><p class="source-code">             (filters=64, kernel_size=3,strides=1,\</p><p class="source-code">              padding='valid', data_format='channels_last',\</p><p class="source-code">              activation='relu')(h2)</p><p class="source-code">        h3 = tf.keras.layers.Flatten()(h3)</p><p class="source-code">        h3 = tf.keras.layers.Dense\</p><p class="source-code">             (units=512, name='fc1', activation='relu')(h3)</p><p class="source-code">        network = tf.keras.Model(inputs=[x_input], outputs=[h3])</p><p class="source-code">        network.summary()</p><p class="source-code">        return network</p><p class="source-code">    return network_fn</p></li>
				<li>Create a function to build the environment in the format required by OpenAI Baselines:<p class="source-code">def build_env(env_id, env_type):</p><p class="source-code">    if env_type in {'atari', 'retro'}:</p><p class="source-code">        env = make_vec_env(env_id, env_type, 1, None, \</p><p class="source-code">                           gamestate=None, reward_scale=1.0)</p><p class="source-code">        env = VecFrameStack(env, 4)</p><p class="source-code">    else:</p><p class="source-code">        env = make_vec_env(env_id, env_type, 1, None,\</p><p class="source-code">                           reward_scale=1.0,\</p><p class="source-code">                           flatten_dict_observations=True)</p><p class="source-code">    return env</p></li>
				<li>Build the <strong class="source-inline">PongNoFrameskip-v4</strong> environment, choose the required policy network parameters, and train it:<p class="source-code">env_id = 'PongNoFrameskip-v0'</p><p class="source-code">env_type = 'atari'</p><p class="source-code">print("Env type = ", env_type)</p><p class="source-code">env = build_env(env_id, env_type)</p><p class="source-code">model = learn(network="custom_cnn", env=env, total_timesteps=1e4)</p><p>While training, the model produces an output similar to the following (only a few lines have been reported here):</p><p class="source-code">Env type =  atari</p><p class="source-code">Logging to /tmp/openai-2020-05-11-16-19-42-770612</p><p class="source-code">input shape is (84, 84, 4)</p><p class="source-code">Model: "model"</p><p class="source-code">_________________________________________________________________</p><p class="source-code">Layer (type)                 Output Shape              Param #  </p><p class="source-code">=================================================================</p><p class="source-code">input_1 (InputLayer)         [(None, 84, 84, 4)]       0        </p><p class="source-code">_________________________________________________________________</p><p class="source-code">tf_op_layer_Cast (TensorFlow [(None, 84, 84, 4)]       0        </p><p class="source-code">_________________________________________________________________</p><p class="source-code">tf_op_layer_truediv (TensorF [(None, 84, 84, 4)]       0        </p><p class="source-code">_________________________________________________________________</p><p class="source-code">conv2d (Conv2D)              (None, 20, 20, 32)        8224     </p><p class="source-code">_________________________________________________________________</p><p class="source-code">conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832    </p><p class="source-code">_________________________________________________________________</p><p class="source-code">conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928    </p><p class="source-code">_________________________________________________________________</p><p class="source-code">flatten (Flatten)            (None, 3136)              0        </p><p class="source-code">_________________________________________________________________</p><p class="source-code">fc1 (Dense)                  (None, 512)               1606144  </p><p class="source-code">=================================================================</p><p class="source-code">Total params: 1,684,128</p><p class="source-code">Trainable params: 1,684,128</p><p class="source-code">Non-trainable params: 0</p><p class="source-code">_________________________________________________________________</p><p class="source-code">--------------------------------------------</p><p class="source-code">| eplenmean               | 1e+03          |</p><p class="source-code">| eprewmean               | -20            |</p><p class="source-code">| fps                     | 213            |</p><p class="source-code">| loss/approxkl           | 0.00012817292  |</p><p class="source-code">| loss/clipfrac           | 0.0            |</p><p class="source-code">| loss/policy_entropy     | 1.7916294      |</p><p class="source-code">| loss/policy_loss        | -0.00050599687 |</p><p class="source-code">| loss/value_loss         | 0.06880974     |</p><p class="source-code">| misc/explained_variance | 0.000675       |</p><p class="source-code">| misc/nupdates           | 1              |</p><p class="source-code">| misc/serial_timesteps   | 2048           |</p><p class="source-code">| misc/time_elapsed       | 9.6            |</p><p class="source-code">| misc/total_timesteps    | 2048           |</p><p class="source-code">--------------------------------------------</p></li>
				<li>Run the trained agent in the environment and print the cumulative reward:<p class="source-code">obs = env.reset()</p><p class="source-code">if not isinstance(env, VecEnv):</p><p class="source-code">    obs = np.expand_dims(np.array(obs), axis=0)</p><p class="source-code">episode_rew = 0</p><p class="source-code">while True:</p><p class="source-code">    actions, _, state, _ = model.step(obs)</p><p class="source-code">    obs, reward, done, info = env.step(actions.numpy())</p><p class="source-code">    if not isinstance(env, VecEnv):</p><p class="source-code">        obs = np.expand_dims(np.array(obs), axis=0)</p><p class="source-code">    env.render()</p><p class="source-code">    print("Reward = ", reward)</p><p class="source-code">    episode_rew += reward</p><p class="source-code">    if done:</p><p class="source-code">        print('Episode Reward = {}'.format(episode_rew))</p><p class="source-code">        break</p><p class="source-code">env.close()</p><p>The following lines show the last part of the output:</p><p class="source-code">[...]</p><p class="source-code">Reward =  [0.]</p><p class="source-code">Reward =  [0.]</p><p class="source-code">Reward =  [0.]</p><p class="source-code">Reward =  [0.]</p><p class="source-code">Reward =  [0.]</p><p class="source-code">Reward =  [0.]</p><p class="source-code">Reward =  [0.]</p><p class="source-code">Reward =  [0.]</p><p class="source-code">Reward =  [0.]</p><p class="source-code">Reward =  [-1.]</p><p class="source-code">Episode Reward = [-17.]</p><p>It also renders the environment, showing what happens in the environment in real time:</p><div id="_idContainer840" class="IMG---Figure"><img src="image/B16182_04_14.jpg" alt="Figure 4.14: One frame of the real-time environment, after rendering&#13;&#10;"/></div><p class="figure-caption">Figure 4.14: One frame of the real-time environment, after rendering</p></li>
				<li>Use the built-in OpenAI Baselines run script to train PPO on the <strong class="source-inline">PongNoFrameskip-v0</strong> environment:<p class="source-code">!python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v0 </p><p class="source-code">--num_timesteps=2e7 --save_path=./models/Pong_20M_ppo2 </p><p class="source-code">--log_path=./logs/Pong/</p><p>The last few lines of the output will be similar to the following:</p><p class="source-code">Stepping environment...</p><p class="source-code">-------------------------------------------</p><p class="source-code">| eplenmean               | 867           |</p><p class="source-code">| eprewmean               | -20.8         |</p><p class="source-code">| fps                     | 500           |</p><p class="source-code">| loss/approxkl           | 4.795634e-05  |</p><p class="source-code">| loss/clipfrac           | 0.0           |</p><p class="source-code">| loss/policy_entropy     | 1.7456135     |</p><p class="source-code">| loss/policy_loss        | -0.0005875508 |</p><p class="source-code">| loss/value_loss         | 0.050125826   |</p><p class="source-code">| misc/explained_variance | 0.145         |</p><p class="source-code">| misc/nupdates           | 19            |</p><p class="source-code">| misc/serial_timesteps   | 2432          |</p><p class="source-code">| misc/time_elapsed       | 22            |</p><p class="source-code">| misc/total_timesteps    | 9728          |</p><p class="source-code">-------------------------------------------</p></li>
				<li>Use the built-in OpenAI Baselines run script to run the trained model on the <strong class="source-inline">PongNoFrameskip-v0</strong> environment:<p class="source-code">!python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v0</p><p class="source-code">    --num_timesteps=0 --load_path=./models/Pong_20M_ppo2 --play</p><p>The output will be similar to the following:</p><p class="source-code">episode_rew=-21.0</p><p class="source-code">episode_rew=-20.0</p><p class="source-code">episode_rew=-20.0</p><p class="source-code">episode_rew=-19.0</p></li>
				<li>Use the pretrained weights provided to see the trained agent in action:<p class="source-code">!wget -O pong_20M_ppo2.tar.gz \</p><p class="source-code">https://github.com/PacktWorkshops\</p><p class="source-code">/The-Reinforcement-Learning-Workshop/blob/master\</p><p class="source-code">/Chapter04/pong_20M_ppo2.tar.gz?raw=true</p><p>The output will be as follows:</p><p class="source-code">Saving to: 'pong_20M_ppo2.tar.gz'</p><p class="source-code">pong_20M_ppo2.tar.g 100%[===================&gt;]  17,44M  15,</p><p class="source-code">1MB/s    in 1,2s   </p><p class="source-code">2020-05-11 16:19:11 (15,1 MB/s) - 'pong_20M_ppo2.tar.gz' saved [18284569/18284569]</p><p>You can read the <strong class="source-inline">.tar</strong> file by using the following command:</p><p class="source-code">!tar xvzf pong_20M_ppo2.tar.gz</p><p>The output will be as follows:</p><p class="source-code">pong_20M_ppo2/ckpt-1.data-00000-of-00001</p><p class="source-code">pong_20M_ppo2/ckpt-1.index</p><p class="source-code">pong_20M_ppo2/</p><p class="source-code">pong_20M_ppo2/checkpoint</p></li>
				<li>Use the built-in OpenAI Baselines run script to train PPO on <strong class="source-inline">PongNoFrameskip-v0</strong>:<p class="source-code">!python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v0 --num_timesteps=0 --load_path=./pong_20M_ppo2 –play</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30yFmOi">https://packt.live/30yFmOi</a>.</p><p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p></li>
			</ol>
			<p>In this activity, we learned how to train a state-of-the-art reinforcement learning agent that, by only looking at screen pixels, is able to achieve better-than-human performance when playing a classic Atari video game. We made use of a convolutional neural network to encode environment observations and leveraged the state-of-the-art OpenAI tool to successfully train a PPO algorithm.</p>
			<h1 id="_idParaDest-344"><a id="_idTextAnchor388"/>5. Dynamic Programming</h1>
			<h2 id="_idParaDest-345"><a id="_idTextAnchor389"/>Activity 5.01: Implementing Policy and Value Iteration on the FrozenLake-v0 Environment</h2>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">import numpy as np</p><p class="source-code">import gym</p></li>
				<li>Initialize the environment and reset the current one. Set <strong class="source-inline">is_slippery=False</strong> in the initializer. Show the size of the action space and the number of possible states:<p class="source-code">def initialize_environment():</p><p class="source-code">    """initialize the OpenAI Gym environment"""</p><p class="source-code">    env = gym.make("FrozenLake-v0", is_slippery=False)</p><p class="source-code">    print("Initializing environment")</p><p class="source-code">    # reset the current environment</p><p class="source-code">    env.reset()</p><p class="source-code">    # show the size of the action space</p><p class="source-code">    action_size = env.action_space.n</p><p class="source-code">    print(f"Action space: {action_size}")</p><p class="source-code">    # Number of possible states</p><p class="source-code">    state_size = env.observation_space.n</p><p class="source-code">    print(f"State space: {state_size}")</p><p class="source-code">    return env</p></li>
				<li>Perform policy evaluation iterations until the smallest change is less than <strong class="source-inline">smallest_change</strong>:<p class="source-code">def policy_evaluation(V, current_policy, env, \</p><p class="source-code">                      gamma, small_change):</p><p class="source-code">    """</p><p class="source-code">    Perform policy evaluation iterations until the smallest </p><p class="source-code">    change is less than</p><p class="source-code">    'smallest_change'</p><p class="source-code">    Args:</p><p class="source-code">        V: the value function table</p><p class="source-code">        current_policy: current policy</p><p class="source-code">        env: the OpenAI FrozenLake-v0 environment</p><p class="source-code">        gamma: future reward coefficient</p><p class="source-code">        small_change: how small should the change be for the </p><p class="source-code">          iterations to stop</p><p class="source-code">    Returns:</p><p class="source-code">        V: the value function after convergence of the evaluation</p><p class="source-code">    """</p><p class="source-code">    state_size = env.observation_space.n</p><p class="source-code">    while True:</p><p class="source-code">        biggest_change = 0</p><p class="source-code">        # loop through every state present</p><p class="source-code">        for state in range(state_size):</p><p class="source-code">            old_V = V[state]</p></li>
				<li>Take the action according to the current policy:<p class="source-code">            action = current_policy[state]</p><p class="source-code">            prob, new_state, reward, done = env.env.P[state]\</p><p class="source-code">                                            [action][0]</p></li>
				<li>Use the Bellman optimality equation to update <img src="image/B16182_05_26a.png" alt="6"/>:<p class="source-code">            V[state] = reward + gamma * V[new_state]</p><p class="source-code">            # if the biggest change is small enough then it means</p><p class="source-code">            # the policy has converged, so stop.</p><p class="source-code">            biggest_change = max(biggest_change, abs(V[state] \</p><p class="source-code">                                 - old_V))</p><p class="source-code">        if biggest_change &lt; small_change:</p><p class="source-code">            break</p><p class="source-code">    return V</p></li>
				<li>Perform policy improvement using the Bellman optimality equation: <p class="source-code">def policy_improvement(V, current_policy, env, gamma):</p><p class="source-code">    """</p><p class="source-code">    Perform policy improvement using the Bellman Optimality Equation.</p><p class="source-code">    Args:</p><p class="source-code">        V: the value function table</p><p class="source-code">        current_policy: current policy</p><p class="source-code">        env: the OpenAI FrozenLake-v0 environment</p><p class="source-code">        gamma: future reward coefficient</p><p class="source-code">    Returns:</p><p class="source-code">        current_policy: the updated policy</p><p class="source-code">        policy_changed: True, if the policy was changed, else, </p><p class="source-code">          False</p><p class="source-code">    """</p><p class="source-code">    state_size = env.observation_space.n</p><p class="source-code">    action_size = env.action_space.n</p><p class="source-code">    policy_changed = False</p><p class="source-code">    for state in range(state_size):</p><p class="source-code">        best_val = -np.inf</p><p class="source-code">        best_action = -1</p><p class="source-code">        # loop over all actions and select the best one</p><p class="source-code">        for action in range(action_size):</p><p class="source-code">            prob, new_state, reward, done = env.env.\</p><p class="source-code">                                            P[state][action][0]</p></li>
				<li>Calculate the future reward by taking this action. Note that we are using the simplified equation because we don't have non-one transition probabilities:<p class="source-code">            future_reward = reward + gamma * V[new_state]</p><p class="source-code">            if future_reward &gt; best_val:</p><p class="source-code">                best_val = future_reward</p><p class="source-code">                best_action = action</p></li>
				<li>Using <strong class="source-inline">assert</strong> statements, we can avoid getting into unwanted situations:<p class="source-code">        assert best_action != -1</p><p class="source-code">        if current_policy[state] != best_action:</p><p class="source-code">            policy_changed = True</p></li>
				<li>Update the best action for this current state:<p class="source-code">        current_policy[state] = best_action</p><p class="source-code">    # if the policy didn't change, it means we have converged</p><p class="source-code">    return current_policy, policy_changed</p></li>
				<li>Find the most optimal policy for the FrozenLake-v0 environment using policy iteration:<p class="source-code">def policy_iteration(env):</p><p class="source-code">    """</p><p class="source-code">    Find the most optimal policy for the FrozenLake-v0 </p><p class="source-code">    environment using Policy</p><p class="source-code">    Iteration</p><p class="source-code">    Args:</p><p class="source-code">        env: FrozenLake-v0 environment</p><p class="source-code">    Returns:</p><p class="source-code">        policy: the most optimal policy</p><p class="source-code">    """</p><p class="source-code">    V = dict()</p><p class="source-code">    """</p><p class="source-code">    initially the value function for all states</p><p class="source-code">    will be random values close to zero</p><p class="source-code">    """</p><p class="source-code">    state_size = env.observation_space.n</p><p class="source-code">    for i in range(state_size):</p><p class="source-code">        V[i] = np.random.random()</p><p class="source-code">    # when the change is smaller than this, stop</p><p class="source-code">    small_change = 1e-20</p><p class="source-code">    # future reward coefficient</p><p class="source-code">    gamma = 0.9</p><p class="source-code">    episodes = 0</p><p class="source-code">    # train for these many episodes</p><p class="source-code">    max_episodes = 50000</p><p class="source-code">    # initially we will start with a random policy</p><p class="source-code">    current_policy = dict()</p><p class="source-code">    for s in range(state_size):</p><p class="source-code">        current_policy[s] = env.action_space.sample()</p><p class="source-code">    while episodes &lt; max_episodes:</p><p class="source-code">        episodes += 1</p><p class="source-code">        # policy evaluation</p><p class="source-code">        V = policy_evaluation(V, current_policy,\</p><p class="source-code">                              env, gamma, small_change)</p><p class="source-code">        # policy improvement</p><p class="source-code">        current_policy, policy_changed = policy_improvement\</p><p class="source-code">                                         (V, current_policy, \</p><p class="source-code">                                          env, gamma)</p><p class="source-code">        # if the policy didn't change, it means we have converged</p><p class="source-code">        if not policy_changed:</p><p class="source-code">            break</p><p class="source-code">    print(f"Number of episodes trained: {episodes}")</p><p class="source-code">    return current_policy</p></li>
				<li>Perform a test pass on the FrozenLake-v0 environment:<p class="source-code">def play(policy, render=False):</p><p class="source-code">    """</p><p class="source-code">    Perform a test pass on the FrozenLake-v0 environment</p><p class="source-code">    Args:</p><p class="source-code">        policy: the policy to use</p><p class="source-code">        render: if the result should be rendered at every step. </p><p class="source-code">          False by default</p><p class="source-code">    """</p><p class="source-code">    env = initialize_environment()</p><p class="source-code">    rewards = []</p></li>
				<li>Define the maximum number of steps the agent is allowed to take. If it doesn't reach a solution in this time, then we call it an episode and proceed ahead:<p class="source-code">    max_steps = 25</p><p class="source-code">    test_episodes = 50</p><p class="source-code">    for episode in range(test_episodes):</p><p class="source-code">        # reset the environment every new episode</p><p class="source-code">        state = env.reset()</p><p class="source-code">        total_rewards = 0</p><p class="source-code">        print("*" * 100)</p><p class="source-code">        print("Episode {}".format(episode))</p><p class="source-code">        for step in range(max_steps):</p></li>
				<li> Take the action that has the highest Q value in the current state:<p class="source-code">            action = policy[state]</p><p class="source-code">            new_state, reward, done, info = env.step(action)</p><p class="source-code">            if render:</p><p class="source-code">                env.render()</p><p class="source-code">            total_rewards += reward</p><p class="source-code">            if done:</p><p class="source-code">                rewards.append(total_rewards)</p><p class="source-code">                print("Score", total_rewards)</p><p class="source-code">                break</p><p class="source-code">            state = new_state</p><p class="source-code">    env.close()</p><p class="source-code">    print("Average Score", sum(rewards) / test_episodes)</p></li>
				<li>Step through the <strong class="source-inline">FrozenLake-v0</strong> environment randomly:<p class="source-code">def random_step(n_steps=5):</p><p class="source-code">    """</p><p class="source-code">    Steps through the FrozenLake-v0 environment randomly</p><p class="source-code">    Args:</p><p class="source-code">        n_steps: Number of steps to step through</p><p class="source-code">    """</p><p class="source-code">    # reset the environment</p><p class="source-code">    env = initialize_environment()</p><p class="source-code">    state = env.reset()</p><p class="source-code">    for i in range(n_steps):</p><p class="source-code">        # choose an action at random</p><p class="source-code">        action = env.action_space.sample()</p><p class="source-code">        env.render()</p><p class="source-code">        new_state, reward, done, info = env.step(action)</p><p class="source-code">        print(f"New State: {new_state}\n"\</p><p class="source-code">              f"reward: {reward}\n"\</p><p class="source-code">              f"done: {done}\n"\</p><p class="source-code">              f"info: {info}\n")</p><p class="source-code">        print("*" * 20)</p></li>
				<li>Perform value iteration to find the most optimal policy for the FrozenLake-v0 environment:<p class="source-code">def value_iteration(env):</p><p class="source-code">    """</p><p class="source-code">    Performs Value Iteration to find the most optimal policy for the</p><p class="source-code">    FrozenLake-v0 environment</p><p class="source-code">    Args:</p><p class="source-code">        env: FrozenLake-v0 Gym environment</p><p class="source-code">    Returns:</p><p class="source-code">        policy: the most optimum policy</p><p class="source-code">    """</p><p class="source-code">    V = dict()</p><p class="source-code">    gamma = 0.9</p><p class="source-code">    state_size = env.observation_space.n</p><p class="source-code">    action_size = env.action_space.n</p><p class="source-code">    policy = dict()</p></li>
				<li> Initialize the value table randomly and initialize the policy randomly:<p class="source-code">    for x in range(state_size):</p><p class="source-code">        V[x] = -1</p><p class="source-code">        policy[x] = env.action_space.sample()</p><p class="source-code">    """</p><p class="source-code">    this loop repeats until the change in value function</p><p class="source-code">    is less than delta</p><p class="source-code">    """</p><p class="source-code">    while True:</p><p class="source-code">        delta = 0</p><p class="source-code">        for state in reversed(range(state_size)):</p><p class="source-code">            old_v_s = V[state]</p><p class="source-code">            best_rewards = -np.inf</p><p class="source-code">            best_action = None</p><p class="source-code">            # for all the actions in current state</p><p class="source-code">            for action in range(action_size):</p></li>
				<li>Check the reward obtained if we were to perform this action:<p class="source-code">               prob, new_state, reward, done = env.env.P[state]\</p><p class="source-code">                                               [action][0]</p><p class="source-code">               potential_reward = reward + gamma * V[new_state]</p><p class="source-code">               """</p><p class="source-code">               select the one that has the best reward</p><p class="source-code">               and also save the action to the policy</p><p class="source-code">               """</p><p class="source-code">            if potential_reward &gt; best_rewards:</p><p class="source-code">                best_rewards = potential_reward</p><p class="source-code">                best_action = action</p><p class="source-code">            policy[state] = best_action</p><p class="source-code">            V[state] = best_rewards</p><p class="source-code">            # terminate if the change is not high</p><p class="source-code">            delta = max(delta, abs(V[state] - old_v_s))</p><p class="source-code">        if delta &lt; 1e-30:</p><p class="source-code">            break</p><p class="source-code">    print(policy)</p><p class="source-code">    print(V)</p><p class="source-code">    return policy</p></li>
				<li>Run the code and make sure the output matches the expectation by running it in the <strong class="source-inline">main</strong> block:<p class="source-code">if __name__ == '__main__':</p><p class="source-code">    env = initialize_environment()</p><p class="source-code">    # policy = policy_iteration(env)</p><p class="source-code">    policy = value_iteration(env)</p><p class="source-code">    play(policy, render=True)</p><p>After running this, you should be able to see the following output:</p><div id="_idContainer842" class="IMG---Figure"><img src="image/B16182_05_27.jpg" alt="Figure 5.27: FrozenLake-v0 environment output&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 5.27: FrozenLake-v0 environment output</p>
			<p>As can be seen from the output, we have successfully achieved the goal of retrieving the frisbee.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3fxtZuq">https://packt.live/3fxtZuq</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2ChI1Ss">https://packt.live/2ChI1Ss</a>.</p>
			<h1 id="_idParaDest-346"><a id="_idTextAnchor390"/>6. Monte Carlo Methods</h1>
			<h2 id="_idParaDest-347"><a id="_idTextAnchor391"/>Activity 6.01: Exploring the Frozen Lake Problem – the Reward Function</h2>
			<ol>
				<li value="1">Import the necessary libraries:<p class="source-code">import gym</p><p class="source-code">import numpy as np</p><p class="source-code">from collections import defaultdict</p></li>
				<li>Select the environment as <strong class="source-inline">FrozenLake</strong>. <strong class="source-inline">is_slippery</strong> is set to <strong class="source-inline">False</strong>. The environment is reset with the line <strong class="source-inline">env.reset()</strong> and rendered with the line <strong class="source-inline">env.render()</strong>:<p class="source-code">env = gym.make("FrozenLake-v0", is_slippery=False)</p><p class="source-code">env.reset()</p><p class="source-code">env.render()</p><p>You will get the following output:</p><div id="_idContainer843" class="IMG---Figure"><img src="image/B16182_06_15.jpg" alt="Figure 6.15: Frozen Lake state rendered&#13;&#10;"/></div><p class="figure-caption">Figure 6.15: Frozen Lake state rendered</p><p>This is a text grid with the letters <strong class="source-inline">S</strong>, <strong class="source-inline">F</strong>, <strong class="source-inline">G</strong>, and <strong class="source-inline">H</strong> used to represent the current environment of <strong class="source-inline">FrozenLake</strong>. The highlighted cell <strong class="source-inline">S</strong> is the current state of the agent.</p></li>
				<li>Print the possible values in the observation space and the number of action values using the <strong class="source-inline">print(env.observation_space)</strong> and <strong class="source-inline">print(env.action_space)</strong> functions respectively:<p class="source-code">print(env.observation_space)</p><p class="source-code">print(env.action_space)</p><p class="source-code">name_action = {0:'Left',1:'Down',2:'Right',3:'Up'}</p><p>You will get the following output:</p><p class="source-code">Discrete(16)</p><p class="source-code">Discrete(4)</p><p><strong class="source-inline">16</strong> is the number of cells in the grid, so <strong class="source-inline">print(env.observation_space)</strong> prints <strong class="source-inline">16</strong>. <strong class="source-inline">4</strong> is the number of possible actions, so <strong class="source-inline">print(env.action_space)</strong> prints <strong class="source-inline">4</strong>. <strong class="source-inline">Discrete</strong> shows the observation space and action space take only discrete values and do not take continuous values.</p></li>
				<li>The next step is to define a function to generate a frozen lake episode. We initialize <strong class="source-inline">episodes</strong> and the environment:<p class="source-code">def generate_frozenlake_episode():</p><p class="source-code">    episode = []</p><p class="source-code">    state = env.reset()</p><p class="source-code">    step = 0;</p></li>
				<li>Navigate step by step and store <strong class="source-inline">episode</strong> and return <strong class="source-inline">reward</strong>:<p class="source-code">    while (True):</p><p class="source-code">        action = env.action_space.sample()</p><p class="source-code">        next_state, reward, done, info = env.step(action)</p><p class="source-code">        episode.append((next_state, action, reward))</p><p class="source-code">        if done:</p><p class="source-code">            break</p><p class="source-code">        state = next_state</p><p class="source-code">        step += 1</p><p class="source-code">    return episode, reward</p><p>The action is obtained with <strong class="source-inline">env.action_space.sample()</strong>. <strong class="source-inline">next_state</strong>,<strong class="source-inline"> action</strong>, and <strong class="source-inline">reward</strong> are obtained by calling the <strong class="source-inline">env_step(action)</strong> function. They are then appended to an episode. The <strong class="source-inline">episode</strong> is now a list of states, actions, and rewards.</p><p>The key is now to calculate the success rate, which is the likelihood of success for a batch of episodes. The way we do this is by calculating the total number of attempts in a batch of episodes. We calculate how many of them successfully reached the goal. The ratio of the agent successfully reaching the goal to the number of attempts made by the agent is the success ratio.</p></li>
				<li>First, we initialize the total reward:<p class="source-code">def frozen_lake_prediction(batch):</p><p class="source-code">    for batch_number in range(batch+1):</p><p class="source-code">        total_reward = 0</p></li>
				<li>Generate the episode and reward for every iteration and calculate the total reward:<p class="source-code">        for i_episode in range(100):</p><p class="source-code">            episode, reward = generate_frozenlake_episode()</p><p class="source-code">            total_reward += reward</p></li>
				<li>The success ratio is calculated by dividing <strong class="source-inline">total_reward</strong> by <strong class="source-inline">100</strong> and is printed:<p class="source-code">        success_percent = total_reward/100</p><p class="source-code">        print("Episode", batch_number*100, \</p><p class="source-code">              "Policy Win Rate=&gt;", float(success_percent*100), \</p><p class="source-code">              "%")</p></li>
				<li>The frozen lake prediction is calculated using the <strong class="source-inline">frozen_lake_prediction</strong> function:<p class="source-code">frozen_lake_prediction(100)</p><p>You will get the following output:</p><div id="_idContainer844" class="IMG---Figure"><img src="image/B16182_06_16.jpg" alt="Figure 6.16: Output of Frozen Lake without learning&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.16: Output of Frozen Lake without learning</p>
			<p>The output prints the policy win ratio for the various episodes in batches of 100. The ratios are quite low as this is the simulation of an agent following a random policy. We will see in the next exercise how this can be improved by learning to a higher level by using a combination of a greedy policy and an epsilon soft policy.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Akh8Nm">https://packt.live/2Akh8Nm</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2zruU07">https://packt.live/2zruU07</a>.</p>
			<h2 id="_idParaDest-348"><a id="_idTextAnchor392"/>Activity 6.02 Solving Frozen Lake Using Monte Carlo Control Every Visit Epsilon Soft </h2>
			<ol>
				<li value="1">Import the necessary libraries:<p class="source-code">import gym</p><p class="source-code">import numpy as np</p></li>
				<li>Select the environment as <strong class="source-inline">FrozenLake</strong>. <strong class="source-inline">is_slippery</strong> is set to <strong class="source-inline">False</strong>:<p class="source-code">#Setting up the Frozen Lake environment</p><p class="source-code">env = gym.make("FrozenLake-v0", is_slippery=False)</p></li>
				<li>Initialize the <strong class="source-inline">Q</strong> value and <strong class="source-inline">num_state_action</strong> to zeros:<p class="source-code">#Initializing the Q and num_state_action</p><p class="source-code">Q = np.zeros([env.observation_space.n, env.action_space.n])</p><p class="source-code">num_state_action = np.zeros([env.observation_space.n, \</p><p class="source-code">                             env.action_space.n])</p></li>
				<li>Set the value of <strong class="source-inline">num_episodes</strong> to <strong class="source-inline">100000</strong> and create <strong class="source-inline">rewardsList</strong>. We set <strong class="source-inline">epsilon</strong> to <strong class="source-inline">0.30</strong>:<p class="source-code">num_episodes = 100000</p><p class="source-code">epsilon = 0.30</p><p class="source-code">rewardsList = []</p><p>Setting epsilon to <strong class="source-inline">0.30</strong> means we will explore with a likelihood of 0.30 and be greedy with a likelihood of 1-0.30 or 0.70.</p></li>
				<li>Run the loop till <strong class="source-inline">num_episodes</strong>. We initialize the environment, <strong class="source-inline">results_List</strong>, and <strong class="source-inline">result_sum</strong> to zero. Also, reset the environment:<p class="source-code">for x in range(num_episodes):</p><p class="source-code">    state = env.reset()</p><p class="source-code">    done = False</p><p class="source-code">    results_list = []</p><p class="source-code">    result_sum = 0.0</p></li>
				<li>Start a <strong class="source-inline">while</strong> loop, and check whether you need to pick a random action with a probability epsilon or greedy policy with a probability of 1-epsilon:<p class="source-code">    while not done:</p><p class="source-code">        </p><p class="source-code">        #random action less than epsilon</p><p class="source-code">        if np.random.rand() &lt; epsilon:</p><p class="source-code">            #we go with the random action</p><p class="source-code">            action = env.action_space.sample()</p><p class="source-code">        else:</p><p class="source-code">            """</p><p class="source-code">            1 - epsilon probability, we go with the greedy algorithm</p><p class="source-code">            """</p><p class="source-code">            action = np.argmax(Q[state, :])</p></li>
				<li>Now step through the <strong class="source-inline">action</strong> and get <strong class="source-inline">new_state</strong> and <strong class="source-inline">reward</strong>:<p class="source-code">        #action is performed and assigned to new_state, reward</p><p class="source-code">        new_state, reward, done, info = env.step(action)</p></li>
				<li>The result list is appended with the <strong class="source-inline">state</strong> and <strong class="source-inline">action</strong> pair. <strong class="source-inline">result_sum</strong> is incremented by the value of the result:<p class="source-code">        results_list.append((state, action))</p><p class="source-code">        result_sum += reward</p></li>
				<li><strong class="source-inline">new_state</strong> is assigned to <strong class="source-inline">state</strong> and <strong class="source-inline">result_sum</strong> is appended to <strong class="source-inline">rewardsList</strong>:<p class="source-code">        #new state is assigned as state</p><p class="source-code">        state = new_state</p><p class="source-code">    #appending the results sum to the rewards list</p><p class="source-code">    rewardsList.append(result_sum)</p></li>
				<li>Calculate <strong class="source-inline">Q[s,a]</strong> using the incremental method, as <strong class="source-inline">Q[s,a] + (result_sum – Q[s,a]) / N(s,a)</strong>:<p class="source-code">    for (state, action) in results_list:</p><p class="source-code">        num_state_action[state, action] += 1.0</p><p class="source-code">        sa_factor = 1.0 / num_state_action[state, action]</p><p class="source-code">        Q[state, action] += sa_factor * \</p><p class="source-code">                            (result_sum - Q[state, action])</p></li>
				<li>Print the value of the success rates in batches of <strong class="source-inline">1000</strong>:<p class="source-code">    if x % 1000 == 0 and x is not 0:</p><p class="source-code">        print('Frozen Lake Success rate=&gt;', \</p><p class="source-code">              str(sum(rewardsList) * 100 / x ), '%')</p></li>
				<li>Print the final success rate:<p class="source-code">print("Frozen Lake Success rate=&gt;", \</p><p class="source-code">      str(sum(rewardsList)/num_episodes * 100), "%")</p><p>You will get the following output initially:</p><div id="_idContainer845" class="IMG---Figure"><img src="image/B16182_06_17.jpg" alt="Figure 6.17: Initial output of the Frozen Lake success rate &#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.17: Initial output of the Frozen Lake success rate </p>
			<p>You will get the following output finally:</p>
			<div>
				<div id="_idContainer846" class="IMG---Figure">
					<img src="image/B16182_06_18.jpg" alt="Figure 6.18: Final output of the Frozen Lake success rate&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.18: Final output of the Frozen Lake success rate</p>
			<p>The success rate starts with a very low value close to 0% but with reinforcement learning, it learns, and the success rate increases incrementally going up to 60%.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Ync9Dq">https://packt.live/2Ync9Dq</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3cUJLxQ">https://packt.live/3cUJLxQ</a>.</p>
			<h1 id="_idParaDest-349"><a id="_idTextAnchor393"/>7. Temporal Difference Learning</h1>
			<h2 id="_idParaDest-350"><a id="_idTextAnchor394"/>Activity 7.01: Using TD(0) Q-Learning to Solve FrozenLake-v0 Stochastic Transitions</h2>
			<ol>
				<li value="1">Import the required modules:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code">import gym</p></li>
				<li>Instantiate the <strong class="source-inline">gym</strong> environment called <strong class="source-inline">FrozenLake-v0</strong> using the <strong class="source-inline">is_slippery</strong> flag set to <strong class="source-inline">True</strong> in order to enable stochasticity:<p class="source-code">env = gym.make('FrozenLake-v0', is_slippery=True)</p></li>
				<li>Take a look at the action and observation spaces:<p class="source-code">print("Action space = ", env.action_space)</p><p class="source-code">print("Observation space = ", env.observation_space)</p><p>This will print out the following:</p><p class="source-code">Action space =  Discrete(4)</p><p class="source-code">Observation space =  Discrete(16)</p></li>
				<li>Create two dictionaries to easily translate the <strong class="source-inline">actions</strong> numbers into moves:<p class="source-code">actionsDict = {}</p><p class="source-code">actionsDict[0] = " L "</p><p class="source-code">actionsDict[1] = " D "</p><p class="source-code">actionsDict[2] = " R "</p><p class="source-code">actionsDict[3] = " U "</p><p class="source-code">actionsDictInv = {}</p><p class="source-code">actionsDictInv["L"] = 0</p><p class="source-code">actionsDictInv["D"] = 1</p><p class="source-code">actionsDictInv["R"] = 2</p><p class="source-code">actionsDictInv["U"] = 3</p></li>
				<li>Reset the environment and render it to take a look at the grid problem:<p class="source-code">env.reset()</p><p class="source-code">env.render()</p><p>Its initial state is as follows:</p><div id="_idContainer847" class="IMG---Figure"><img src="image/B16182_07_39.jpg" alt="Figure 7.39: Environment's initial state&#13;&#10;"/></div><p class="figure-caption">Figure 7.39: Environment's initial state</p></li>
				<li>Visualize the optimal policy for this environment:<p class="source-code">optimalPolicy = ["  *  ","  U  ","L/R/D","  U  ",\</p><p class="source-code">                 "  L  ","  -  "," L/R ","  -  ",\</p><p class="source-code">                 "  U  ","  D  ","  L  ","  -  ",\</p><p class="source-code">                 "  -  ","  R  ","R/D/U","  !  ",]</p><p class="source-code">print("Optimal policy:")</p><p class="source-code">idxs = [0,4,8,12]</p><p class="source-code">for idx in idxs:</p><p class="source-code">    print(optimalPolicy[idx+0], optimalPolicy[idx+1], \</p><p class="source-code">          optimalPolicy[idx+2], optimalPolicy[idx+3])</p><p>This prints out the following output:</p><p class="source-code">Optimal policy:  </p><p class="source-code">  L/R/D  U    U    U</p><p class="source-code">    L    -   L/R   -</p><p class="source-code">    U    D    L    -</p><p class="source-code">    -    R    D    !</p></li>
				<li>Define the functions that will take ε-greedy actions:<p class="source-code">def action_epsilon_greedy(q, s, epsilon=0.05):</p><p class="source-code">    if np.random.rand() &gt; epsilon:</p><p class="source-code">        return np.argmax(q[s])</p><p class="source-code">    return np.random.randint(4)</p></li>
				<li>Define a function that will take greedy actions:<p class="source-code">def greedy_policy(q, s):</p><p class="source-code">    return np.argmax(q[s])</p></li>
				<li>Define a function that will calculate the agent's average performance:<p class="source-code">def average_performance(policy_fct, q):</p><p class="source-code">    acc_returns = 0.</p><p class="source-code">    n = 500</p><p class="source-code">    for i in range(n):</p><p class="source-code">        done = False</p><p class="source-code">        s = env.reset()</p><p class="source-code">        while not done:</p><p class="source-code">            a = policy_fct(q, s)</p><p class="source-code">            s, reward, done, info = env.step(a)</p><p class="source-code">            acc_returns += reward</p><p class="source-code">    return acc_returns/n</p></li>
				<li>Initialize the Q-table so that all the values are equal to <strong class="source-inline">1</strong>, except for the values at the terminal states:<p class="source-code">q = np.ones((16, 4))</p><p class="source-code"># Set q(terminal,*) equal to 0</p><p class="source-code">q[5,:] = 0.0</p><p class="source-code">q[7,:] = 0.0</p><p class="source-code">q[11,:] = 0.0</p><p class="source-code">q[12,:] = 0.0</p><p class="source-code">q[15,:] = 0.0</p></li>
				<li>Set the number of total episodes, the number of steps representing the interval by which we're evaluating the agent's average performance, the learning rate, the discounting factor, the <strong class="source-inline">ε</strong> value for the exploration policy, and an array to collect all the agent's performance evaluations during training:<p class="source-code">nb_episodes = 80000</p><p class="source-code">STEPS = 2000</p><p class="source-code">alpha = 0.01</p><p class="source-code">gamma = 0.99</p><p class="source-code">epsilon_expl = 0.2</p><p class="source-code">q_performance = np.ndarray(nb_episodes//STEPS)</p></li>
				<li>Train the Q-learning algorithm. Loop among all episodes:<p class="source-code">for i in range(nb_episodes):</p></li>
				<li>Reset the environment and start the in-episode loop:<p class="source-code">    done = False</p><p class="source-code">    s = env.reset()</p><p class="source-code">    while not done:</p></li>
				<li>Select the exploration action with an ε-greedy policy:<p class="source-code">        # behavior policy</p><p class="source-code">        a = action_epsilon_greedy(q, s, epsilon=epsilon_expl)</p></li>
				<li>Step the environment with the selected exploration action and retrieval of the new state, reward, and done conditions:<p class="source-code">        new_s, reward, done, info = env.step(a)</p></li>
				<li>Select a new action with the greedy policy:<p class="source-code">        a_max = np.argmax(q[new_s]) # estimation policy</p></li>
				<li>Update the Q-table with the Q-learning TD(0) rule:<p class="source-code">        q[s, a] = q[s, a] + alpha * \</p><p class="source-code">                  (reward + gamma * q[new_s, a_max] - q[s, a])</p></li>
				<li>Update the state with a new value:<p class="source-code">        s = new_s</p></li>
				<li>Evaluate the agent's average performance for every step:<p class="source-code">    if i%STEPS == 0:</p><p class="source-code">        q_performance[i//STEPS] = average_performance\</p><p class="source-code">                                  (greedy_policy, q)</p></li>
				<li>Plot the Q-learning agent's mean reward history during training:<p class="source-code">plt.plot(STEPS * np.arange(nb_episodes//STEPS), q_performance)</p><p class="source-code">plt.xlabel("Epochs")</p><p class="source-code">plt.ylabel("Average reward of an epoch")</p><p class="source-code">plt.title("Learning progress for Q-Learning")</p><p>This generates the following output, showing the learning progress for the Q-learning algorithm:</p><p class="source-code">Text(0.5, 1.0, 'Learning progress for Q-Learning')</p><p>The plot for this can be visualized as follows:</p><div id="_idContainer848" class="IMG---Figure"><img src="image/B16182_07_40.jpg" alt="Figure 7.40: Average reward of an epoch trend over training epochs&#13;&#10;"/></div><p class="figure-caption">Figure 7.40: Average reward of an epoch trend over training epochs</p><p>In this case, as in the case of Q-learning applied to the deterministic environment, the plot shows how quickly Q-learning performance grows over epochs as the agent collects more and more experience. It also demonstrates that the algorithm is not capable of reaching 100% success after learning due to the limitations of stochasticity. When compared with using the SARSA method on a stochastic environment, as seen in <em class="italic">Figure 7.15</em>, the algorithm's performance grows faster and more steadily.</p></li>
				<li>Evaluate the greedy policy's performance for the trained agent (Q-table):<p class="source-code">greedyPolicyAvgPerf = average_performance(greedy_policy, q=q)</p><p class="source-code">print("Greedy policy Q-learning performance =", \</p><p class="source-code">      greedyPolicyAvgPerf)</p><p>This prints out the following:</p><p class="source-code">Greedy policy Q-learning performance = 0.708</p></li>
				<li>Display the Q-table values:<p class="source-code">q = np.round(q,3)</p><p class="source-code">print("(A,S) Value function =", q.shape)</p><p class="source-code">print("First row")</p><p class="source-code">print(q[0:4,:])</p><p class="source-code">print("Second row")</p><p class="source-code">print(q[4:8,:])</p><p class="source-code">print("Third row")</p><p class="source-code">print(q[8:12,:])</p><p class="source-code">print("Fourth row")</p><p class="source-code">print(q[12:16,:])</p><p>This generates the following output:</p><p class="source-code">(A,S) Value function = (16, 4)</p><p class="source-code">First row</p><p class="source-code">[[0.543 0.521 0.516 0.515]</p><p class="source-code"> [0.319 0.355 0.322 0.493]</p><p class="source-code"> [0.432 0.431 0.425 0.461]</p><p class="source-code"> [0.32  0.298 0.296 0.447]]</p><p class="source-code">Second row</p><p class="source-code">[[0.559 0.392 0.396 0.393]</p><p class="source-code"> [0.    0.    0.    0.   ]</p><p class="source-code"> [0.296 0.224 0.327 0.145]</p><p class="source-code"> [0.    0.    0.    0.   ]]</p><p class="source-code">Third row</p><p class="source-code">[[0.337 0.366 0.42  0.595]</p><p class="source-code"> [0.484 0.639 0.433 0.415]</p><p class="source-code"> [0.599 0.511 0.342 0.336]</p><p class="source-code"> [0.    0.    0.    0.   ]]</p><p class="source-code">Fourth row</p><p class="source-code">[[0.    0.    0.    0.   ]</p><p class="source-code"> [0.46  0.53  0.749 0.525]</p><p class="source-code"> [0.711 0.865 0.802 0.799]</p><p class="source-code"> [0.    0.    0.    0.   ]]</p></li>
				<li>Print out the greedy policy that was found and compare it with the optimal policy:<p class="source-code">policyFound = [actionsDict[np.argmax(q[0,:])],\</p><p class="source-code">               actionsDict[np.argmax(q[1,:])],\</p><p class="source-code">               actionsDict[np.argmax(q[2,:])],\</p><p class="source-code">               actionsDict[np.argmax(q[3,:])],\</p><p class="source-code">               actionsDict[np.argmax(q[4,:])],\</p><p class="source-code">               " - ",\</p><p class="source-code">               actionsDict[np.argmax(q[6,:])],\</p><p class="source-code">               " - ",\</p><p class="source-code">               actionsDict[np.argmax(q[8,:])],\</p><p class="source-code">               actionsDict[np.argmax(q[9,:])],\</p><p class="source-code">               actionsDict[np.argmax(q[10,:])],\</p><p class="source-code">               " - ",\</p><p class="source-code">               " - ",\</p><p class="source-code">               actionsDict[np.argmax(q[13,:])],\</p><p class="source-code">               actionsDict[np.argmax(q[14,:])],\</p><p class="source-code">               " ! "]</p><p class="source-code">print("Greedy policy found:")</p><p class="source-code">idxs = [0,4,8,12]</p><p class="source-code">for idx in idxs:</p><p class="source-code">    print(policyFound[idx+0], policyFound[idx+1], \</p><p class="source-code">          policyFound[idx+2], policyFound[idx+3])</p><p class="source-code">print(" ")</p><p class="source-code">print("Optimal policy:")</p><p class="source-code">idxs = [0,4,8,12]</p><p class="source-code">for idx in idxs:</p><p class="source-code">    print(optimalPolicy[idx+0], optimalPolicy[idx+1], \</p><p class="source-code">          optimalPolicy[idx+2], optimalPolicy[idx+3])</p><p>This generates the following output:</p><p class="source-code">Greedy policy found:</p><p class="source-code">    L    U    U    U</p><p class="source-code">    L    -    R    -</p><p class="source-code">    U    D    L    -</p><p class="source-code">    -    R    D    !</p><p class="source-code">Optimal policy:  </p><p class="source-code">  L/R/D  U    U    U</p><p class="source-code">    L    -   L/R   -</p><p class="source-code">    U    D    L    -</p><p class="source-code">    -    R    D    !</p></li>
			</ol>
			<p>This output shows that, as for all the exercises in this chapter, the off-policy, one-step Q-learning algorithm is able to find the optimal policy by simply exploring the environment, even in the context of stochastic environment transitions. As anticipated, for this setting, it is not possible to achieve the maximum reward 100% of the time. </p>
			<p>As we can see, for every state of the grid world that the greedy policy obtained with the Q-table that was calculated by our algorithm, it prescribes an action that is in accordance with the optimal policy that was defined by analyzing the environment problem. As we already saw, there are two states in which many different actions are equally optimal, and the agent correctly implements one of them.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3elMxxu">https://packt.live/3elMxxu</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/37HSDWx">https://packt.live/37HSDWx</a>.</p>
			<h1 id="_idParaDest-351"><a id="_idTextAnchor395"/>8. The Multi-Armed Bandit Problem</h1>
			<h2 id="_idParaDest-352"><a id="_idTextAnchor396"/>Activity 8.01: Queueing Bandits</h2>
			<ol>
				<li value="1">Import the necessary libraries and tools, as follows:<p class="source-code">import numpy as np</p><p class="source-code">from utils import QueueBandit</p></li>
				<li>Declare the bandit object, as follows:<p class="source-code">N_CLASSES = 3</p><p class="source-code">queue_bandit = QueueBandit(filename='data.csv')</p><p>The <strong class="source-inline">N_CLASSES</strong> variable will be used by our subsequent code.</p></li>
				<li>Implement the Greedy algorithm, as follows:<p class="source-code">class GreedyQueue:</p><p class="source-code">    def __init__(self, n_classes=3):</p><p class="source-code">        self.n_classes = n_classes</p><p class="source-code">        self.time_history = [[] for _ in range(n_classes)]</p><p class="source-code">    </p><p class="source-code">    def decide(self, queue_lengths):</p><p class="source-code">        for class_ in range(self.n_classes):</p><p class="source-code">            if queue_lengths[class_] &gt; 0 and \</p><p class="source-code">               len(self.time_history[class_]) == 0:</p><p class="source-code">                return class_</p><p class="source-code">        mean_times = [np.mean(self.time_history[class_])\</p><p class="source-code">                      if queue_lengths[class_] &gt; 0 else np.inf\</p><p class="source-code">                      for class_ in range(self.n_classes)]</p><p class="source-code">        return int(np.random.choice\</p><p class="source-code">                  (np.argwhere\</p><p class="source-code">                  (mean_times == np.min(mean_times)).flatten()))</p><p class="source-code">    def update(self, class_, time):</p><p class="source-code">        self.time_history[class_].append(time)</p><p>Notice that we are taking care to avoid choosing a class that does not have any customers left in it by checking if <strong class="source-inline">queue_lengths[class_]</strong> is greater than 0 or not. The remaining code is analogous to what we had in our earlier discussion of Greedy.</p><p>Subsequently, apply the algorithm to the bandit object, as follows:</p><p class="source-code">cumulative_times = queue_bandit.repeat\</p><p class="source-code">                   (GreedyQueue, [N_CLASSES], \</p><p class="source-code">                    visualize_cumulative_times=True)</p><p class="source-code">np.max(cumulative_times), np.mean(cumulative_times)</p><p>This will generate the following graph:</p><div id="_idContainer849" class="IMG---Figure"><img src="image/B16182_08_24.jpg" alt="Figure 8.24: Distribution of cumulative waiting time from Greedy&#13;&#10;"/></div><p class="figure-caption">Figure 8.24: Distribution of cumulative waiting time from Greedy</p><p>Additionally, the following will be printed out as the max and mean cumulative waiting times:</p><p class="source-code">(1218887.7924350922, 45155.236786598274)</p><p>While these values might appear large compared to our earlier discussions, this is because the reward/cost distributions we are working with here take on higher values. We will use these values from Greedy as a frame of reference to analyze the performance of later algorithms.</p></li>
				<li>Implement the Explore-then-commit algorithm using the following code:<p class="source-code">class ETCQueue:</p><p class="source-code">    def __init__(self, n_classes=3, T=3):</p><p class="source-code">        self.n_classes = n_classes</p><p class="source-code">        self.T = T</p><p class="source-code">        self.time_history = [[] for _ in range(n_classes)]</p><p class="source-code">    def decide(self, queue_lengths):</p><p class="source-code">        for class_ in range(self.n_classes):</p><p class="source-code">            if queue_lengths[class_] &gt; 0 and \</p><p class="source-code">            len(self.time_history[class_]) &lt; self.T:</p><p class="source-code">                return class_</p><p class="source-code">        mean_times = [np.mean(self.time_history[class_])\</p><p class="source-code">                      if queue_lengths[class_] &gt; 0 else np.inf\</p><p class="source-code">                      for class_ in range(self.n_classes)]</p><p class="source-code">        return int(np.random.choice\</p><p class="source-code">                  (np.argwhere(mean_times == np.min(mean_times))\</p><p class="source-code">                  .flatten()))</p><p class="source-code">    def update(self, class_, time):</p><p class="source-code">        self.time_history[class_].append(time)</p></li>
				<li>Apply the algorithm to the bandit object, as follows:<p class="source-code">cumulative_times = queue_bandit.repeat\</p><p class="source-code">                   (ETCQueue, [N_CLASSES, 2],\</p><p class="source-code">                    visualize_cumulative_times=True)</p><p class="source-code">np.max(cumulative_times), np.mean(cumulative_times)</p><p>This will produce the following graph:</p><div id="_idContainer850" class="IMG---Figure"><img src="image/B16182_08_25.jpg" alt="Figure 8.25: Distribution of cumulative waiting time from Explore-then-commit&#13;&#10;"/></div><p class="figure-caption">Figure 8.25: Distribution of cumulative waiting time from Explore-then-commit</p><p>This will also produce the max and average cumulative waiting times: <strong class="source-inline">(1238591.3208636027, 45909.77140562623)</strong>. Compared to Greedy <strong class="source-inline">(1218887.7924350922, 45155.236786598274)</strong>, Explore-then-commit did relatively worse on this queueing bandit problem.</p></li>
				<li>Implement Thompson Sampling, as follows:<p class="source-code">class ExpThSQueue:</p><p class="source-code">    def __init__(self, n_classes=3):</p><p class="source-code">        self.n_classes = n_classes</p><p class="source-code">        self.time_history = [[] for _ in range(n_classes)]</p><p class="source-code">        self.temp_beliefs = [(0, 0) for _ in range(n_classes)]</p><p class="source-code">        </p><p class="source-code">    def decide(self, queue_lengths):</p><p class="source-code">        for class_ in range(self.n_classes):</p><p class="source-code">            if queue_lengths[class_] &gt; 0 and \</p><p class="source-code">            len(self.time_history[class_]) == 0:</p><p class="source-code">                return class_</p><p class="source-code">        </p><p class="source-code">        rate_draws = [np.random.gamma\</p><p class="source-code">                      (self.temp_beliefs[class_][0],1 \</p><p class="source-code">                       / self.temp_beliefs[class_][1])\</p><p class="source-code">                     if queue_lengths[class_] &gt; 0 else -np.inf\</p><p class="source-code">                     for class_ in range(self.n_classes)]</p><p class="source-code">        return int(np.random.choice\</p><p class="source-code">                  (np.argwhere(rate_draws == np.max(rate_draws))\</p><p class="source-code">                  .flatten()))</p><p class="source-code">    def update(self, class_, time):</p><p class="source-code">        self.time_history[class_].append(time)</p><p class="source-code">        </p><p class="source-code">        # Update parameters according to Bayes rule</p><p class="source-code">        alpha, beta = self.temp_beliefs[class_]</p><p class="source-code">        alpha += 1</p><p class="source-code">        beta += time</p><p class="source-code">        self.temp_beliefs[class_] = alpha, beta</p><p>Recall that in our initial discussion of Thompson Sampling, we draw random samples to estimate the reward expectation for each arm. Here, we drew random samples from the corresponding Gamma distributions (which are being used to model service rates) to estimate the rates (or the inverse job lengths) and choose the largest drawn sample.</p></li>
				<li>This can be applied to solve the bandit problem using the following code:<p class="source-code">cumulative_times = queue_bandit.repeat\</p><p class="source-code">                   (ExpThSQueue, [N_CLASSES], \</p><p class="source-code">                    visualize_cumulative_times=True)</p><p class="source-code">np.max(cumulative_times), np.mean(cumulative_times)</p><p>The following plot will be produced:</p><div id="_idContainer851" class="IMG---Figure"><img src="image/B16182_08_26.jpg" alt="Figure 8.26: Distribution of cumulative waiting time from Thompson Sampling&#13;&#10;"/></div><p class="figure-caption">Figure 8.26: Distribution of cumulative waiting time from Thompson Sampling</p><p>From the max and mean waiting time <strong class="source-inline">(1218887.7924350922, 45129.343871806814)</strong>, we can see that Thompson Sampling is able to improve on Greedy.</p></li>
				<li>The modified version of Thompson Sampling can be implemented as follows:<p class="source-code">class ExploitingThSQueue:</p><p class="source-code">    def __init__(self, n_classes=3, r=1):</p><p class="source-code">        self.n_classes = n_classes</p><p class="source-code">        self.time_history = [[] for _ in range(n_classes)]</p><p class="source-code">        self.temp_beliefs = [(0, 0) for _ in range(n_classes)]</p><p class="source-code">        self.t = 0</p><p class="source-code">        self.r = r</p><p class="source-code">        </p><p class="source-code">    def decide(self, queue_lengths):</p><p class="source-code">        for class_ in range(self.n_classes):</p><p class="source-code">            if queue_lengths[class_] &gt; 0 and \</p><p class="source-code">            len(self.time_history[class_]) == 0:</p><p class="source-code">                return class_</p><p class="source-code">        if self.t &gt; self.r * np.sum(queue_lengths):</p><p class="source-code">            mean_times = [np.mean(self.time_history[class_])\</p><p class="source-code">                          if queue_lengths[class_] &gt; 0 \</p><p class="source-code">                          else np.inf\</p><p class="source-code">                          for class_ in range(self.n_classes)]</p><p class="source-code">            return int(np.random.choice\</p><p class="source-code">                      (np.argwhere\</p><p class="source-code">                      (mean_times == np.min(mean_times))\</p><p class="source-code">                      .flatten()))</p><p class="source-code">        rate_draws = [np.random.gamma\</p><p class="source-code">                      (self.temp_beliefs[class_][0],\</p><p class="source-code">                       1 / self.temp_beliefs[class_][1])\</p><p class="source-code">                      if queue_lengths[class_] &gt; 0 else -np.inf\</p><p class="source-code">                      for class_ in range(self.n_classes)]</p><p class="source-code">        return int(np.random.choice\</p><p class="source-code">                  (np.argwhere\</p><p class="source-code">                  (rate_draws == np.max(rate_draws)).flatten()))</p><p>The initialization method of this class implementation has an additional attribute, <strong class="source-inline">r</strong>, which we will use to implement the exploitation logic.</p><p>In the <strong class="source-inline">decide()</strong> method, right before we draw samples to estimate the rates, we check to see if the current time (<strong class="source-inline">t</strong>) is greater than the current queue length (the sum of <strong class="source-inline">queue_lengths</strong>). This Boolean indicates whether we have processed more than half of the customers or not. If so, we simply implement the logic of the Greedy algorithm and return the arm with the optimal average rate. Otherwise, we have our actual Thompson Sampling logic.</p><p>The <strong class="source-inline">update()</strong> method should be the same as the actual Thompson Sampling algorithm from the previous step, as follows:</p><p class="source-code">    def update(self, class_, time):</p><p class="source-code">        self.time_history[class_].append(time)</p><p class="source-code">        self.t += 1</p><p class="source-code">        </p><p class="source-code">        # Update parameters according to Bayes rule</p><p class="source-code">        alpha, beta = self.temp_beliefs[class_]</p><p class="source-code">        alpha += 1</p><p class="source-code">        beta += time</p><p class="source-code">        self.temp_beliefs[class_] = alpha, beta</p></li>
				<li>Finally, apply the algorithm to the bandit problem:<p class="source-code">cumulative_times = queue_bandit.repeat\</p><p class="source-code">                   (ExploitingThSQueue, [N_CLASSES, 1], \</p><p class="source-code">                    visualize_cumulative_times=True)</p><p class="source-code">np.max(cumulative_times), np.mean(cumulative_times)</p><p>We will obtain the following graph:</p><div id="_idContainer852" class="IMG---Figure"><img src="image/B16182_08_27.jpg" alt="Figure 8.27: Distribution of cumulative waiting time from modified Thompson Sampling&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.27: Distribution of cumulative waiting time from modified Thompson Sampling</p>
			<p>Together with the max and mean waiting time <strong class="source-inline">(1218887.7924350922, 45093.244027644556)</strong>, we can see that this modified version of Thompson Sampling is more effective than the original at minimizing the cumulative waiting time across the experiments.</p>
			<p>This speaks to the potential benefit of designing algorithms that are tailored to the contextual bandit problem that they are trying to solve.</p>
			<p class="callout-heading">Note </p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Yuw2IQ">https://packt.live/2Yuw2IQ</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3hnK5Z5">https://packt.live/3hnK5Z5</a>.</p>
			<p>Throughout this activity, we have learned how to apply the approaches discussed in this chapter to a queueing bandit problem, that is, exploring an example of a potential contextual bandit process. Most notably, we have considered a variant of Thompson Sampling that has been modified to fit the context of the queueing problem, thus successfully lowering our cumulative regret compared to other algorithms. This activity also marks the end of this chapter.</p>
			<h1 id="_idParaDest-353"><a id="_idTextAnchor397"/>9. What Is Deep Q-Learning?</h1>
			<h2 id="_idParaDest-354"><a id="_idTextAnchor398"/>Activity 9.01: Implementing a Double Deep Q Network in PyTorch for the CartPole Environment</h2>
			<ol>
				<li value="1">Open a new Jupyter notebook and import all of the required libraries:<p class="source-code">import gym</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import torch</p><p class="source-code">import torch.nn as nn</p><p class="source-code">from torch import optim</p><p class="source-code">import numpy as np</p><p class="source-code">import random</p><p class="source-code">import math</p></li>
				<li>Write code that will create a device based on the availability of a GPU environment:<p class="source-code">use_cuda = torch.cuda.is_available()</p><p class="source-code">device = torch.device("cuda:0" if use_cuda else "cpu")</p><p class="source-code">print(device)</p></li>
				<li>Create a <strong class="source-inline">gym</strong> environment using the <strong class="source-inline">'CartPole-v0'</strong> environment:<p class="source-code">env = gym.make('CartPole-v0')</p></li>
				<li>Set the <strong class="source-inline">seed</strong> for torch and the environment for reproducibility:<p class="source-code">seed = 100</p><p class="source-code">env.seed(seed)</p><p class="source-code">torch.manual_seed(seed)</p><p class="source-code">random.seed(seed)</p></li>
				<li>Fetch the number of states and actions from the environment:<p class="source-code">number_of_states = env.observation_space.shape[0]</p><p class="source-code">number_of_actions = env.action_space.n</p><p class="source-code">print('Total number of States : {}'.format(number_of_states))</p><p class="source-code">print('Total number of Actions : {}'.format(number_of_actions))</p><p>The output is as follows:</p><p class="source-code">Total number of States : 4</p><p class="source-code">Total number of Actions : 2</p></li>
				<li>Set all of the hyperparameter values required for the DDQN process:<p class="source-code">NUMBER_OF_EPISODES = 500</p><p class="source-code">MAX_STEPS = 1000</p><p class="source-code">LEARNING_RATE = 0.01</p><p class="source-code">DISCOUNT_FACTOR = 0.99</p><p class="source-code">HIDDEN_LAYER_SIZE = 64</p><p class="source-code">EGREEDY = 0.9</p><p class="source-code">EGREEDY_FINAL = 0.02</p><p class="source-code">EGREEDY_DECAY = 500</p><p class="source-code">REPLAY_BUFFER_SIZE = 6000</p><p class="source-code">BATCH_SIZE = 32</p><p class="source-code">UPDATE_TARGET_FREQUENCY = 200</p></li>
				<li>Implement the <strong class="source-inline">calculate_epsilon</strong> function, as described in the previous exercises:<p class="source-code">def calculate_epsilon(steps_done):</p><p class="source-code">    """</p><p class="source-code">    Decays epsilon with increasing steps</p><p class="source-code">    Parameter:</p><p class="source-code">    steps_done (int) : number of steps completed</p><p class="source-code">    Returns:</p><p class="source-code">    int - decayed epsilon</p><p class="source-code">    """</p><p class="source-code">    epsilon = EGREEDY_FINAL + (EGREEDY - EGREEDY_FINAL) \</p><p class="source-code">              * math.exp(-1. * steps_done / EGREEDY_DECAY )</p><p class="source-code">    return epsilon</p></li>
				<li>Create a class, called <strong class="source-inline">DQN</strong>, that accepts the number of states as inputs and outputs Q values for the number of actions present in the environment, with the network that has a hidden layer of size <strong class="source-inline">64</strong>:<p class="source-code">class DQN(nn.Module):</p><p class="source-code">    def __init__(self , hidden_layer_size):</p><p class="source-code">        super().__init__()</p><p class="source-code">        self.hidden_layer_size = hidden_layer_size</p><p class="source-code">        self.fc1 = nn.Linear(number_of_states,\</p><p class="source-code">                             self.hidden_layer_size)</p><p class="source-code">        self.fc2 = nn.Linear(self.hidden_layer_size,\</p><p class="source-code">                             number_of_actions)</p><p class="source-code">    def forward(self, x):</p><p class="source-code">        output = torch.tanh(self.fc1(x))</p><p class="source-code">        output = self.fc2(output)</p><p class="source-code">        return output</p></li>
				<li>Implement the <strong class="source-inline">ExperienceReplay</strong> class, as described in the previous exercises:<p class="source-code">class ExperienceReplay(object):</p><p class="source-code">    def __init__(self , capacity):</p><p class="source-code">        self.capacity = capacity</p><p class="source-code">        self.buffer = []</p><p class="source-code">        self.pointer = 0</p><p class="source-code">    def push(self , state, action, new_state, reward, done):</p><p class="source-code">        experience = (state, action, new_state, reward, done)</p><p class="source-code">        if self.pointer &gt;= len(self.buffer):</p><p class="source-code">            self.buffer.append(experience)</p><p class="source-code">        else:</p><p class="source-code">            self.buffer[self.pointer] = experience</p><p class="source-code">        self.pointer = (self.pointer + 1) % self.capacity</p><p class="source-code">    def sample(self , batch_size):</p><p class="source-code">        return zip(*random.sample(self.buffer , batch_size))</p><p class="source-code">    def __len__(self):</p><p class="source-code">        return len(self.buffer)</p></li>
				<li>Instantiate the <strong class="source-inline">ExperienceReplay</strong> class by passing the buffer size as input:<p class="source-code">memory = ExperienceReplay(REPLAY_BUFFER_SIZE)</p></li>
				<li>Implement the DQN agent class with the changes discussed for the <strong class="source-inline">optimize</strong> function (from the code example given in the <em class="italic">Double Deep Q Network (DDQN)</em> section): <p class="source-code">class DQN_Agent(object):</p><p class="source-code">    def __init__(self):</p><p class="source-code">        self.dqn = DQN(HIDDEN_LAYER_SIZE).to(device)</p><p class="source-code">        self.target_dqn = DQN(HIDDEN_LAYER_SIZE).to(device)</p><p class="source-code">        self.criterion = torch.nn.MSELoss()</p><p class="source-code">        self.optimizer = optim.Adam\</p><p class="source-code">                         (params=self.dqn.parameters(), \</p><p class="source-code">                          lr=LEARNING_RATE)</p><p class="source-code">        self.target_dqn_update_counter = 0</p><p class="source-code">    def select_action(self,state,EGREEDY):</p><p class="source-code">        random_for_egreedy = torch.rand(1)[0]</p><p class="source-code">        if random_for_egreedy &gt; EGREEDY:</p><p class="source-code">            with torch.no_grad():</p><p class="source-code">                state = torch.Tensor(state).to(device)</p><p class="source-code">                q_values = self.dqn(state)</p><p class="source-code">                action = torch.max(q_values,0)[1]</p><p class="source-code">                action = action.item()</p><p class="source-code">        else:</p><p class="source-code">            action = env.action_space.sample()</p><p class="source-code">        return action</p><p class="source-code">    def optimize(self):</p><p class="source-code">        if (BATCH_SIZE &gt; len(memory)):</p><p class="source-code">            return</p><p class="source-code">        state, action, new_state, reward, done = memory.sample\</p><p class="source-code">                                                 (BATCH_SIZE)</p><p class="source-code">        state = torch.Tensor(state).to(device)</p><p class="source-code">        new_state = torch.Tensor(new_state).to(device)</p><p class="source-code">        reward = torch.Tensor(reward).to(device)</p><p class="source-code">        action = torch.LongTensor(action).to(device)</p><p class="source-code">        done = torch.Tensor(done).to(device)</p><p class="source-code">        """</p><p class="source-code">        select action : get the index associated with max q </p><p class="source-code">        value from prediction network</p><p class="source-code">        """</p><p class="source-code">        new_state_indxs = self.dqn(new_state).detach() </p><p class="source-code">        # to get the max new state indexes</p><p class="source-code">        max_new_state_indxs = torch.max(new_state_indxs, 1)[1]</p><p class="source-code">        """</p><p class="source-code">        Using the best action from the prediction nn get </p><p class="source-code">        the max new state value in target dqn</p><p class="source-code">        """</p><p class="source-code">        new_state_values = self.target_dqn(new_state).detach()</p><p class="source-code">        max_new_state_values = new_state_values.gather\</p><p class="source-code">                               (1, max_new_state_indxs\</p><p class="source-code">                                .unsqueeze(1))\</p><p class="source-code">                               .squeeze(1)</p><p class="source-code">        #when done = 1 then target = reward</p><p class="source-code">        target_value = reward + (1 - done) * DISCOUNT_FACTOR \</p><p class="source-code">                       * max_new_state_values</p><p class="source-code">        predicted_value = self.dqn(state).gather\</p><p class="source-code">                          (1, action.unsqueeze(1))\</p><p class="source-code">                          .squeeze(1)</p><p class="source-code">        loss = self.criterion(predicted_value, target_value)</p><p class="source-code">        self.optimizer.zero_grad()</p><p class="source-code">        loss.backward()</p><p class="source-code">        self.optimizer.step()</p><p class="source-code">        if self.target_dqn_update_counter \</p><p class="source-code">        % UPDATE_TARGET_FREQUENCY == 0:</p><p class="source-code">            self.target_dqn.load_state_dict(self.dqn.state_dict())</p><p class="source-code">        self.target_dqn_update_counter += 1</p></li>
				<li>Write the training process loop with the help of the following steps. First, instantiate the DQN agent using the class created earlier. Create a <strong class="source-inline">steps_total</strong> empty list to collect the total number of steps for each episode. Initialize <strong class="source-inline">steps_counter</strong> with zero and use it to calculate the decayed epsilon value for each step:<p class="source-code">dqn_agent = DQN_Agent()</p><p class="source-code">steps_total = []</p><p class="source-code">steps_counter = 0</p><p>Use two loops during the training process; the first one is to play the game for a certain number of steps. The second loop ensures that each episode goes on for a fixed number of steps. Inside the second <strong class="source-inline">for</strong> loop, the first step is to calculate the epsilon value for the current step.</p><p>Using the present state and epsilon value, you can select the action to perform. The next step is to take the action. Once you take the action, the environment returns the <strong class="source-inline">new_state</strong>, <strong class="source-inline">reward</strong>, and <strong class="source-inline">done</strong> flags.</p><p>Using the <strong class="source-inline">optimize</strong> function, perform one step of gradient descent to optimize the DQN. Now make the new state the present state for the next iteration. Finally, check whether the episode is over. If the episode is over, then you can collect and record the reward for the current episode:</p><p class="source-code">for episode in range(NUMBER_OF_EPISODES):</p><p class="source-code">    state = env.reset()</p><p class="source-code">    done = False</p><p class="source-code">    step = 0</p><p class="source-code">    for i in range(MAX_STEPS):</p><p class="source-code">        step += 1</p><p class="source-code">        steps_counter += 1</p><p class="source-code">        EGREEDY = calculate_epsilon(steps_counter)</p><p class="source-code">        action = dqn_agent.select_action(state, EGREEDY)</p><p class="source-code">        new_state, reward, done, info = env.step(action)</p><p class="source-code">        memory.push(state, action, new_state, reward, done)</p><p class="source-code">        dqn_agent.optimize()</p><p class="source-code">        state = new_state</p><p class="source-code">        if done:</p><p class="source-code">            steps_total.append(step)</p><p class="source-code">            break</p></li>
				<li>Now observe the reward. As the reward is scalar feedback and gives an indication of how well the agent is performing, you should look at the average reward and the average reward for the last 100 episodes. Also, perform the graphical representation of rewards. Check how the agent is performing while playing more episodes and what the reward average is for the last 100 episodes:<p class="source-code">print("Average reward: %.2f" \</p><p class="source-code">      % (sum(steps_total)/NUMBER_OF_EPISODES))</p><p class="source-code">print("Average reward (last 100 episodes): %.2f" \</p><p class="source-code">      % (sum(steps_total[-100:])/100))</p><p>The output will be as follows:</p><p class="source-code">Average reward: 174.09</p><p class="source-code">Average reward (last 100 episodes): 186.06</p></li>
				<li>Plot the rewards collected in the y axis and the number of episodes in the x axis to visualize how the rewards have been collected with the increasing number of episodes:<p class="source-code">Plt.figure(figsize=(12,5))</p><p class="source-code">plt.title("Rewards Collected")</p><p class="source-code">plt.xlabel('Steps')</p><p class="source-code">plt.ylabel('Reward')</p><p class="source-code">plt.bar(np.arange(len(steps_total)), steps_total, \</p><p class="source-code">        alpha=0.5, color='green', width=6)</p><p class="source-code">plt.show()</p><p>The output will be as follows:</p><div id="_idContainer853" class="IMG---Figure"><img src="image/B16182_09_37.jpg" alt="Figure 9.37: Plot for the rewards collected by the agent&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 9.37: Plot for the rewards collected by the agent</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hnLDTd">https://packt.live/3hnLDTd</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/37ol5MK">https://packt.live/37ol5MK</a>.</p>
			<p>The following is a comparison between different DQN techniques and DDQN:</p>
			<p><strong class="bold">Vanilla DQN Outputs:</strong></p>
			<p class="source-code">Average reward: 158.83</p>
			<p class="source-code">Average reward (last 100 episodes): 176.28</p>
			<p><strong class="bold">DQN with Experience Replay and Target Network Outputs:</strong></p>
			<p class="source-code">Average reward: 154.41</p>
			<p class="source-code">Average reward (last 100 episodes): 183.28</p>
			<p><strong class="bold">DDQN Outputs:</strong></p>
			<p class="source-code">Average reward: 174.09</p>
			<p class="source-code">Average reward (last 100 episodes): 186.06</p>
			<p>As you can see from the preceding figure, along with the comparison of the results shown earlier, DDQN has the highest average reward, compared to other DQN implementations, and the average reward for the last 100 episodes is also higher. We can say that DDQN improves performance significantly in comparison to the other two DQN techniques. After completing this whole activity, we have learned how to combine a DDQN network with experience replay to overcome the issues of a vanilla DQN and achieve more stable rewards.</p>
			<h1 id="_idParaDest-355"><a id="_idTextAnchor399"/>10. Playing an Atari Game with Deep Recurrent Q-Networks</h1>
			<h2 id="_idParaDest-356"><a id="_idTextAnchor400"/>Activity 10.01: Training a DQN with CNNs to Play Breakout</h2>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and import the relevant packages: <strong class="source-inline">gym</strong>, <strong class="source-inline">random</strong>, <strong class="source-inline">tensorflow</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">collections</strong>:<p class="source-code">import gym</p><p class="source-code">import random</p><p class="source-code">import numpy as np</p><p class="source-code">from collections import deque</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers import Dense, Conv2D, \</p><p class="source-code">MaxPooling2D, Flatten</p><p class="source-code">from tensorflow.keras.optimizers import RMSprop</p><p class="source-code">import datetime</p></li>
				<li>Set the seed for NumPy and TensorFlow to <strong class="source-inline">168</strong>:<p class="source-code">np.random.seed(168)</p><p class="source-code">tf.random.set_seed(168)</p></li>
				<li>Create the <strong class="source-inline">DQN</strong> class with the following methods: the <strong class="source-inline">build_model()</strong> method to instantiate a CNN, the <strong class="source-inline">get_action()</strong> method to apply the epsilon-greedy algorithm to choose the action to be played, the <strong class="source-inline">add_experience()</strong> method to store in memory the experience acquired by playing the game, the <strong class="source-inline">replay()</strong> method, which will perform experience replay by sampling experiences from the memory and train the DQN model with a callback to save the model every two episodes, and the <strong class="source-inline">update_epsilon()</strong> method to gradually decrease the epsilon value for epsilon-greedy:<p class="source-code-heading">Activity10_01.ipynb</p><p class="source-code">class DQN():</p><p class="source-code">    def __init__(self, env, batch_size=64, max_experiences=5000):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.input_size = self.env.observation_space.shape[0]</p><p class="source-code">        self.action_size = self.env.action_space.n</p><p class="source-code">        self.max_experiences = max_experiences</p><p class="source-code">        self.memory = deque(maxlen=self.max_experiences)</p><p class="source-code">        self.batch_size = batch_size</p><p class="source-code">        self.gamma = 1.0</p><p class="source-code">        self.epsilon = 1.0</p><p class="source-code">        self.epsilon_min = 0.01</p><p class="source-code">        self.epsilon_decay = 0.995</p><p class="source-code">        self.model = self.build_model()</p><p class="source-code">        self.target_model = self.build_model()</p><p class="source-code">             </p><p class="source-code">    def build_model(self):</p><p class="source-code">        model = Sequential()</p><p class="source-code">        model.add(Conv2D(32, 8, (4,4), activation='relu', \</p><p class="source-code">                         padding='valid',\</p><p class="source-code">                         input_shape=(IMG_SIZE, IMG_SIZE, 1)))</p><p class="source-code">        model.add(Conv2D(64, 4, (2,2), activation='relu', \</p><p class="source-code">                         padding='valid'))</p><p class="source-code">        model.add(Conv2D(64, 3, (1,1), activation='relu', \</p><p class="source-code">                         padding='valid'))</p><p class="source-code">        model.add(Flatten())</p><p class="source-code">        model.add(Dense(256, activation='relu'))</p><p class="source-code">        model.add(Dense(self.action_size))</p><p class="source-code">        model.compile(loss='mse', \</p><p class="source-code">                      optimizer=RMSprop(lr=0.00025, \</p><p class="source-code">                      epsilon=self.epsilon_min), \</p><p class="source-code">                      metrics=['accuracy'])</p><p class="source-code">        return model</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/3hoZXdV">https://packt.live/3hoZXdV</a>.</p></li>
				<li>Create the <strong class="source-inline">initialize_env()</strong> function, which will initialize the Breakout environment:<p class="source-code">def initialize_env(env):</p><p class="source-code">    initial_state = env.reset()</p><p class="source-code">    initial_done_flag = False</p><p class="source-code">    initial_rewards = 0</p><p class="source-code">    return initial_state, initial_done_flag, initial_rewards</p></li>
				<li>Create the <strong class="source-inline">preprocess_state()</strong> function to preprocess the input images:<p class="source-code">def preprocess_state(image, img_size):</p><p class="source-code">    img_temp = image[31:195]</p><p class="source-code">    img_temp = tf.image.rgb_to_grayscale(img_temp)</p><p class="source-code">    img_temp = tf.image.resize\</p><p class="source-code">               (img_temp, [img_size, img_size],\</p><p class="source-code">                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)</p><p class="source-code">    img_temp = tf.cast(img_temp, tf.float32)</p><p class="source-code">    return img_temp</p></li>
				<li>Create the <strong class="source-inline">play_game()</strong> function, which will play an entire game of Breakout:<p class="source-code">def play_game(agent, state, done, rewards):</p><p class="source-code">    while not done:</p><p class="source-code">        action = agent.get_action(state)</p><p class="source-code">        next_state, reward, done, _ = env.step(action)</p><p class="source-code">        next_state = preprocess_state(next_state, IMG_SIZE)</p><p class="source-code">        agent.add_experience(state, action, reward, \</p><p class="source-code">                             next_state, done)</p><p class="source-code">        state = next_state</p><p class="source-code">        rewards += reward</p><p class="source-code">    return rewards</p></li>
				<li>Create the <strong class="source-inline">train_agent()</strong> function, which will iterate through a number of episodes where the agent will play a game and perform experience replay:<p class="source-code">def train_agent(env, episodes, agent):</p><p class="source-code">  from collections import deque</p><p class="source-code">  import numpy as np</p><p class="source-code">  scores = deque(maxlen=100)</p><p class="source-code">  for episode in range(episodes):</p><p class="source-code">    state, done, rewards = initialize_env(env)</p><p class="source-code">    state = preprocess_state(state, IMG_SIZE)</p><p class="source-code">    rewards = play_game(agent, state, done, rewards)</p><p class="source-code">    scores.append(rewards)</p><p class="source-code">    mean_score = np.mean(scores)</p><p class="source-code">    if episode % 50 == 0:</p><p class="source-code">        print(f'[Episode {episode}] \</p><p class="source-code">- Average Score: {mean_score}')</p><p class="source-code">        agent.target_model.set_weights(agent.model.get_weights())</p><p class="source-code">        agent.target_model.save_weights\</p><p class="source-code">        (f'dqn/dqn_model_weights_{episode}')</p><p class="source-code">    agent.replay(episode)</p><p class="source-code">  print(f"Average Score: {np.mean(scores)}")</p></li>
				<li>Instantiate a Breakout environment called <strong class="source-inline">env</strong> with the <strong class="source-inline">gym.make()</strong> function:<p class="source-code">env = gym.make('BreakoutDeterministic-v4')</p></li>
				<li>Create two variables, <strong class="source-inline">IMG_SIZE</strong> and <strong class="source-inline">SEQUENCE</strong>, that will take the values <strong class="source-inline">84</strong> and <strong class="source-inline">4</strong>, respectively:<p class="source-code">IMG_SIZE = 84</p><p class="source-code">SEQUENCE = 4</p></li>
				<li>Instantiate a <strong class="source-inline">DQN</strong> object called <strong class="source-inline">agent</strong>:<p class="source-code">agent = DQN(env)</p></li>
				<li>Create a variable called <strong class="source-inline">episodes</strong> that will take the value <strong class="source-inline">50</strong>:<p class="source-code">episodes = 50</p></li>
				<li>Call the <strong class="source-inline">train_agent</strong> function by providing <strong class="source-inline">env</strong>, <strong class="source-inline">episodes</strong>, and <strong class="source-inline">agent</strong>:<p class="source-code">train_agent(env, episodes, agent)</p><p>The following is the output of the code:</p><p class="source-code">[Episode 0] - Average Score: 3.0</p><p class="source-code">Average Score: 0.59</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hoZXdV">https://packt.live/3hoZXdV</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3dWLwfa">https://packt.live/3dWLwfa</a>.</p></li>
			</ol>
			<p>You just completed the first activity of this chapter. You successfully built and trained a DQN agent combined with CNNs to play the game Breakout. The performance of this model is very similar to the random agent (average score of 0.6). However, if you train it for longer (by increasing the number of episodes), it may achieve a better score.</p>
			<h2 id="_idParaDest-357"><a id="_idTextAnchor401"/>Activity 10.02: Training a DRQN to Play Breakout</h2>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and import the relevant packages: <strong class="source-inline">gym</strong>, <strong class="source-inline">random</strong>, <strong class="source-inline">tensorflow</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">collections</strong>:<p class="source-code">import gym</p><p class="source-code">import random</p><p class="source-code">import numpy as np</p><p class="source-code">from collections import deque</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers import Dense, Conv2D, \</p><p class="source-code">MaxPooling2D, TimeDistributed, Flatten, LSTM</p><p class="source-code">from tensorflow.keras.optimizers import RMSprop</p><p class="source-code">import datetime</p></li>
				<li>Set the seed for NumPy and TensorFlow to <strong class="source-inline">168</strong>:<p class="source-code">np.random.seed(168)</p><p class="source-code">tf.random.set_seed(168)</p></li>
				<li>Create the <strong class="source-inline">DRQN</strong> class with the following methods: the <strong class="source-inline">build_model()</strong> method to instantiate a CNN combined with a RNN model, the <strong class="source-inline">get_action()</strong> method to apply the epsilon-greedy algorithm to choose the action to be played, the <strong class="source-inline">add_experience()</strong> method to store in memory the experience acquired by playing the game, the <strong class="source-inline">replay()</strong> method, which will perform experience replay by sampling experiences from the memory and train the DRQN model with a callback to save the model every two episodes, and the <strong class="source-inline">update_epsilon()</strong> method to gradually decrease the epsilon value for epsilon-greedy:<p class="source-code-heading">Activity10_02.ipynb</p><p class="source-code">class DRQN():</p><p class="source-code">    def __init__(self, env, batch_size=64, max_experiences=5000):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.input_size = self.env.observation_space.shape[0]</p><p class="source-code">        self.action_size = self.env.action_space.n</p><p class="source-code">        self.max_experiences = max_experiences</p><p class="source-code">        self.memory = deque(maxlen=self.max_experiences)</p><p class="source-code">        self.batch_size = batch_size</p><p class="source-code">        self.gamma = 1.0</p><p class="source-code">        self.epsilon = 1.0</p><p class="source-code">        self.epsilon_min = 0.01</p><p class="source-code">        self.epsilon_decay = 0.995</p><p class="source-code">       </p><p class="source-code">        self.model = self.build_model()</p><p class="source-code">        self.target_model = self.build_model()</p><p class="source-code">             </p><p class="source-code">    def build_model(self):</p><p class="source-code">        model = Sequential()</p><p class="source-code">        model.add(TimeDistributed(Conv2D(32, 8, (4,4), \</p><p class="source-code">                                  activation='relu', \</p><p class="source-code">                                  padding='valid'), \</p><p class="source-code">                  input_shape=(SEQUENCE, IMG_SIZE, IMG_SIZE, 1)))</p><p class="source-code">        model.add(TimeDistributed(Conv2D(64, 4, (2,2), \</p><p class="source-code">                                  activation='relu', \</p><p class="source-code">                                  padding='valid')))</p><p class="source-code">        model.add(TimeDistributed(Conv2D(64, 3, (1,1), \</p><p class="source-code">                                  activation='relu', \</p><p class="source-code">                                  padding='valid')))</p><p class="source-code">        model.add(TimeDistributed(Flatten()))</p><p class="source-code">        model.add(LSTM(512))</p><p class="source-code">        model.add(Dense(128, activation='relu'))</p><p class="source-code">        model.add(Dense(self.action_size))</p><p class="source-code">        model.compile(loss='mse', \</p><p class="source-code">                      optimizer=RMSprop(lr=0.00025, \</p><p class="source-code">                                        epsilon=self.epsilon_min), \</p><p class="source-code">                      metrics=['accuracy'])</p><p class="source-code">        return model</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/2AjdgMx ">https://packt.live/2AjdgMx </a>.</p></li>
				<li>Create the <strong class="source-inline">initialize_env()</strong> function, which will initialize the Breakout environment:<p class="source-code">def initialize_env(env):</p><p class="source-code">  initial_state = env.reset()</p><p class="source-code">  initial_done_flag = False</p><p class="source-code">  initial_rewards = 0</p><p class="source-code">  return initial_state, initial_done_flag, initial_rewards</p></li>
				<li>Create the <strong class="source-inline">preprocess_state()</strong> function to preprocess the input images:<p class="source-code">def preprocess_state(image, img_size):</p><p class="source-code">    img_temp = image[31:195]</p><p class="source-code">    img_temp = tf.image.rgb_to_grayscale(img_temp)</p><p class="source-code">    img_temp = tf.image.resize\</p><p class="source-code">               (img_temp, [img_size, img_size], \</p><p class="source-code">                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)</p><p class="source-code">    img_temp = tf.cast(img_temp, tf.float32)</p><p class="source-code">    return img_temp</p></li>
				<li>Create the <strong class="source-inline">combine_images()</strong> function to stack the previous four screenshots:<p class="source-code">def combine_images(new_img, prev_img, img_size, seq=4):</p><p class="source-code">    if len(prev_img.shape) == 4 and prev_img.shape[0] == seq:</p><p class="source-code">        im = np.concatenate\</p><p class="source-code">             ((prev_img[1:, :, :], \</p><p class="source-code">               tf.reshape(new_img, [1, img_size, img_size, 1])), \</p><p class="source-code">               axis=0)</p><p class="source-code">    else:</p><p class="source-code">        im = np.stack([new_img] * seq, axis=0)</p><p class="source-code">    return im</p></li>
				<li>Create the <strong class="source-inline">play_game()</strong> function, which will play an entire game of Breakout:<p class="source-code">def play_game(agent, state, done, rewards):</p><p class="source-code">    while not done:</p><p class="source-code">        action = agent.get_action(state)</p><p class="source-code">        next_state, reward, done, _ = env.step(action)</p><p class="source-code">        next_state = preprocess_state(next_state, IMG_SIZE)</p><p class="source-code">        next_state = combine_images\</p><p class="source-code">                     (new_img=next_state, prev_img=state, \</p><p class="source-code">                      img_size=IMG_SIZE, seq=SEQUENCE)</p><p class="source-code">        agent.add_experience(state, action, \</p><p class="source-code">                             reward, next_state, done)</p><p class="source-code">        state = next_state</p><p class="source-code">        rewards += reward </p><p class="source-code">    return rewards</p></li>
				<li>Create the <strong class="source-inline">train_agent()</strong> function, which will iterate through a number of episodes where the agent will play a game and perform experience replay:<p class="source-code">def train_agent(env, episodes, agent):</p><p class="source-code">  from collections import deque</p><p class="source-code">  import numpy as np</p><p class="source-code">  scores = deque(maxlen=100)</p><p class="source-code">  for episode in range(episodes):</p><p class="source-code">    state, done, rewards = initialize_env(env)</p><p class="source-code">    state = preprocess_state(state, IMG_SIZE)</p><p class="source-code">    state = combine_images(new_img=state, prev_img=state, \</p><p class="source-code">                           img_size=IMG_SIZE, seq=SEQUENCE)</p><p class="source-code">    rewards = play_game(agent, state, done, rewards)</p><p class="source-code">    scores.append(rewards)</p><p class="source-code">    mean_score = np.mean(scores)</p><p class="source-code">    if episode % 50 == 0:</p><p class="source-code">        print(f'[Episode {episode}] - Average Score: {mean_score}')</p><p class="source-code">        agent.target_model.set_weights\</p><p class="source-code">        (agent.model.get_weights())</p><p class="source-code">        agent.target_model.save_weights\</p><p class="source-code">        (f'drqn_model_weights_{episode}')</p><p class="source-code">    agent.replay(episode)</p><p class="source-code">  print(f"Average Score: {np.mean(scores)}")</p></li>
				<li>Instantiate a Breakout environment called <strong class="source-inline">env</strong> with <strong class="source-inline">gym.make()</strong>:<p class="source-code">env = gym.make('BreakoutDeterministic-v4')</p></li>
				<li>Create two variables, <strong class="source-inline">IMG_SIZE</strong> and <strong class="source-inline">SEQUENCE</strong>, that will take the values <strong class="source-inline">84</strong> and <strong class="source-inline">4</strong>, respectively:<p class="source-code">IMG_SIZE = 84</p><p class="source-code">SEQUENCE = 4</p></li>
				<li>Instantiate a <strong class="source-inline">DRQN</strong> object called <strong class="source-inline">agent</strong>:<p class="source-code">agent = DRQN(env)</p></li>
				<li>Create a variable called <strong class="source-inline">episodes</strong> that will take the value <strong class="source-inline">200</strong>:<p class="source-code">episodes = 200</p></li>
				<li>Call the <strong class="source-inline">train_agent</strong> function by providing <strong class="source-inline">env</strong>, <strong class="source-inline">episodes</strong>, and <strong class="source-inline">agent</strong>:<p class="source-code">train_agent(env, episodes, agent)</p><p>The following is the output of the code:</p><p class="source-code">[Episode 0] - Average Score: 0.0</p><p class="source-code">[Episode 50] - Average Score: 0.43137254901960786</p><p class="source-code">[Episode 100] - Average Score: 0.4</p><p class="source-code">[Episode 150] - Average Score: 0.54</p><p class="source-code">Average Score: 0.53</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2AjdgMx">https://packt.live/2AjdgMx</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/37mhlLM">https://packt.live/37mhlLM</a>.</p></li>
			</ol>
			<p>In this activity, we added an LSTM layer and built a DRQN agent. It learned how to play the Breakout game, but didn't achieve satisfactory results even after 200 episodes. It seems this is still at the exploratory stage. You may try to train it for more episodes.</p>
			<h2 id="_idParaDest-358"><a id="_idTextAnchor402"/>Activity 10.03: Training a DARQN to Play Breakout</h2>
			<p><strong class="bold">Solution</strong></p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and import the relevant packages: <strong class="source-inline">gym</strong>, <strong class="source-inline">random</strong>, <strong class="source-inline">tensorflow</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">collections</strong>:<p class="source-code">import gym</p><p class="source-code">import random</p><p class="source-code">import numpy as np</p><p class="source-code">from collections import deque</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers import Dense, Conv2D, \</p><p class="source-code">MaxPooling2D, TimeDistributed, Flatten, GRU, Attention</p><p class="source-code">from tensorflow.keras.optimizers import RMSprop</p><p class="source-code">import datetime</p></li>
				<li>Set the seed for NumPy and TensorFlow to <strong class="source-inline">168</strong>:<p class="source-code">np.random.seed(168)</p><p class="source-code">tf.random.set_seed(168)</p></li>
				<li>Create the <strong class="source-inline">DARQN</strong> class and create the following methods: the <strong class="source-inline">build_model()</strong> method to instantiate a CNN combined with an RNN model, the <strong class="source-inline">get_action()</strong> method to apply the epsilon-greedy algorithm to choose the action to be played, the <strong class="source-inline">add_experience()</strong> method to store in memory the experience acquired by playing the game, the <strong class="source-inline">replay()</strong> method, which will perform experience replay by sampling experiences from the memory and train the DARQN model with a callback to save the model every two episodes, and the <strong class="source-inline">update_epsilon()</strong> method to gradually decrease the epsilon value for epsilon-greedy:<p class="source-code-heading">Activity10_03.ipynb</p><p class="source-code">class DARQN():</p><p class="source-code">    def __init__(self, env, batch_size=64, max_experiences=5000):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.input_size = self.env.observation_space.shape[0]</p><p class="source-code">        self.action_size = self.env.action_space.n</p><p class="source-code">        self.max_experiences = max_experiences</p><p class="source-code">        self.memory = deque(maxlen=self.max_experiences)</p><p class="source-code">        self.batch_size = batch_size</p><p class="source-code">        self.gamma = 1.0</p><p class="source-code">        self.epsilon = 1.0</p><p class="source-code">        self.epsilon_min = 0.01</p><p class="source-code">        self.epsilon_decay = 0.995</p><p class="source-code">        self.model = self.build_model()</p><p class="source-code">        self.target_model = self.build_model()</p><p class="source-code">    def build_model(self):</p><p class="source-code">        inputs = Input(shape=(SEQUENCE, IMG_SIZE, IMG_SIZE, 1))</p><p class="source-code">        conv1 = TimeDistributed(Conv2D(32, 8, (4,4), \</p><p class="source-code">                                activation='relu', \</p><p class="source-code">                                padding='valid'))(inputs)</p><p class="source-code">        conv2 = TimeDistributed(Conv2D(64, 4, (2,2), \</p><p class="source-code">                                activation='relu', \</p><p class="source-code">                                padding='valid'))(conv1)</p><p class="source-code">        conv3 = TimeDistributed(Conv2D(64, 3, (1,1), \</p><p class="source-code">                                activation='relu', \</p><p class="source-code">                                padding='valid'))(conv2)</p><p class="source-code">        flatten = TimeDistributed(Flatten())(conv3)</p><p class="source-code">        out, states = GRU(512, return_sequences=True, \</p><p class="source-code">                          return_state=True)(flatten)</p><p class="source-code">        att = Attention()([out, states])</p><p class="source-code">        output_1 = Dense(256, activation='relu')(att)</p><p class="source-code">        predictions = Dense(self.action_size)(output_1)</p><p class="source-code">        model = Model(inputs=inputs, outputs=predictions)</p><p class="source-code">        model.compile(loss='mse', \</p><p class="source-code">                      optimizer=RMSprop(lr=0.00025, \</p><p class="source-code">                                        epsilon=self.epsilon_min), \</p><p class="source-code">                      metrics=['accuracy'])</p><p class="source-code">        return model</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/2XUDZrH">https://packt.live/2XUDZrH</a>.</p></li>
				<li>Create the <strong class="source-inline">initialize_env()</strong> function, which will initialize the Breakout environment:<p class="source-code">def initialize_env(env):</p><p class="source-code">  initial_state = env.reset()</p><p class="source-code">  initial_done_flag = False</p><p class="source-code">  initial_rewards = 0</p><p class="source-code">  return initial_state, initial_done_flag, initial_rewards</p></li>
				<li>Create the <strong class="source-inline">preprocess_state()</strong> function to preprocess the input images:<p class="source-code">def preprocess_state(image, img_size):</p><p class="source-code">    img_temp = image[31:195]</p><p class="source-code">    img_temp = tf.image.rgb_to_grayscale(img_temp)</p><p class="source-code">    img_temp = tf.image.resize\</p><p class="source-code">               (img_temp, [img_size, img_size],\</p><p class="source-code">               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)</p><p class="source-code">    img_temp = tf.cast(img_temp, tf.float32)</p><p class="source-code">    return img_temp</p></li>
				<li>Create the <strong class="source-inline">combine_images()</strong> function to stack the previous four screenshots:<p class="source-code">def combine_images(new_img, prev_img, img_size, seq=4):</p><p class="source-code">    if len(prev_img.shape) == 4 and prev_img.shape[0] == seq:</p><p class="source-code">        im = np.concatenate((prev_img[1:, :, :], \</p><p class="source-code">                             tf.reshape\</p><p class="source-code">                             (new_img, [1, img_size, \</p><p class="source-code">                                        img_size, 1])), axis=0)</p><p class="source-code">    else:</p><p class="source-code">        im = np.stack([new_img] * seq, axis=0)</p><p class="source-code">    return im</p></li>
				<li>Create the <strong class="source-inline">preprocess_state()</strong> function to preprocess the input images:<p class="source-code">def play_game(agent, state, done, rewards):</p><p class="source-code">    while not done:</p><p class="source-code">        action = agent.get_action(state)</p><p class="source-code">        next_state, reward, done, _ = env.step(action)</p><p class="source-code">        next_state = preprocess_state(next_state, IMG_SIZE)</p><p class="source-code">        next_state = combine_images\</p><p class="source-code">                     (new_img=next_state, prev_img=state, \</p><p class="source-code">                      img_size=IMG_SIZE, seq=SEQUENCE)</p><p class="source-code">        agent.add_experience(state, action, reward, \</p><p class="source-code">                             next_state, done)</p><p class="source-code">         state = next_state</p><p class="source-code">       rewards += reward</p><p class="source-code">    return rewards</p></li>
				<li>Create the <strong class="source-inline">train_agent()</strong> function, which will iterate through a number of episodes where the agent will play a game and perform experience replay:<p class="source-code">def train_agent(env, episodes, agent):</p><p class="source-code">  from collections import deque</p><p class="source-code">  import numpy as np</p><p class="source-code">  scores = deque(maxlen=100)</p><p class="source-code">  for episode in range(episodes):</p><p class="source-code">    state, done, rewards = initialize_env(env)</p><p class="source-code">    state = preprocess_state(state, IMG_SIZE)</p><p class="source-code">    state = combine_images\</p><p class="source-code">            (new_img=state, prev_img=state, \</p><p class="source-code">             img_size=IMG_SIZE, seq=SEQUENCE)</p><p class="source-code">    rewards = play_game(agent, state, done, rewards)</p><p class="source-code">    scores.append(rewards)</p><p class="source-code">    mean_score = np.mean(scores)</p><p class="source-code">    if episode % 50 == 0:</p><p class="source-code">        print(f'[Episode {episode}] - Average Score: {mean_score}')</p><p class="source-code">        agent.target_model.set_weights\</p><p class="source-code">        (agent.model.get_weights())</p><p class="source-code">        agent.target_model.save_weights\</p><p class="source-code">        (f'drqn_model_weights_{episode}')</p><p class="source-code">    agent.replay(episode)</p><p class="source-code">  print(f"Average Score: {np.mean(scores)}")</p></li>
				<li>Instantiate a Breakout environment called <strong class="source-inline">env</strong> with <strong class="source-inline">gym.make()</strong>:<p class="source-code">env = gym.make('BreakoutDeterministic-v4')</p></li>
				<li>Create two variables, <strong class="source-inline">IMG_SIZE</strong> and <strong class="source-inline">SEQUENCE</strong>, that will take the values <strong class="source-inline">84</strong> and <strong class="source-inline">4</strong>, respectively:<p class="source-code">IMG_SIZE = 84</p><p class="source-code">SEQUENCE = 4</p></li>
				<li>Instantiate a <strong class="source-inline">DRQN</strong> object called <strong class="source-inline">agent</strong>:<p class="source-code">agent = DRQN(env)</p></li>
				<li>Create a variable called <strong class="source-inline">episodes</strong> that will take the value <strong class="source-inline">400</strong>:<p class="source-code">episodes = 400</p></li>
				<li>Call the <strong class="source-inline">train_agent</strong> function by providing <strong class="source-inline">env</strong>, <strong class="source-inline">episodes</strong>, and <strong class="source-inline">agent</strong>:<p class="source-code">train_agent(env, episodes, agent)</p><p>The following is the output of the code:</p><p class="source-code">[Episode 0] - Average Score: 1.0</p><p class="source-code">[Episode 50] - Average Score: 2.4901960784313726</p><p class="source-code">[Episode 100] - Average Score: 3.92</p><p class="source-code">[Episode 150] - Average Score: 7.37</p><p class="source-code">[Episode 200] - Average Score: 7.76</p><p class="source-code">[Episode 250] - Average Score: 7.91</p><p class="source-code">[Episode 300] - Average Score: 10.33</p><p class="source-code">[Episode 350] - Average Score: 10.94</p><p class="source-code">Average Score: 10.83</p></li>
			</ol>
			<p>In this activity, we built and trained a <strong class="source-inline">DARQN</strong> agent. It successfully learned how to play the Breakout game. It started with a score of <strong class="source-inline">1.0</strong> and achieved a final score of over <strong class="source-inline">10</strong> after <strong class="source-inline">400</strong> episodes, as shown in the preceding results. This is quite remarkable performance.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2XUDZrH">https://packt.live/2XUDZrH</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2UDCsUP">https://packt.live/2UDCsUP</a>.</p>
			<h1 id="_idParaDest-359"><a id="_idTextAnchor403"/>11. Policy-Based Methods for Reinforcement Learning</h1>
			<h2 id="_idParaDest-360"><a id="_idTextAnchor404"/>Activity 11.01: Creating an Agent That Learns a Model Using DDPG</h2>
			<ol>
				<li value="1">Import the necessary libraries (<strong class="source-inline">os</strong>, <strong class="source-inline">gym</strong>, and <strong class="source-inline">ddpg</strong>):<p class="source-code">import os</p><p class="source-code">import gym</p><p class="source-code">from ddpg import *</p></li>
				<li>First, we create our Gym environment (<strong class="source-inline">LunarLanderContinuous-v2</strong>), as we did previously:<p class="source-code">env = gym.make("LunarLanderContinuous-v2")</p></li>
				<li>Initialize the agent with some sensible hyperparameters, as in <em class="italic">Exercise 11.02</em>, <em class="italic">Creating a Learning Agent</em>:<p class="source-code">agent = Agent(alpha=0.000025, beta=0.00025, \</p><p class="source-code">              inp_dimensions=[8], tau=0.001,\</p><p class="source-code">              env=env, bs=64, l1_size=400, l2_size=300, \</p><p class="source-code">              nb_actions=2)</p></li>
				<li>Set up a random seed so that our experiments are reproducible.<p class="source-code">np.random.seed(0)</p></li>
				<li>Create a blank array to story the scores; you can name it <strong class="source-inline">history</strong>. Iterate for at least <strong class="source-inline">1000</strong> episodes and in each episode, set a running score variable to <strong class="source-inline">0</strong> and the <strong class="source-inline">done</strong> flag to <strong class="source-inline">False</strong>, then reset the environment. Then, when the <strong class="source-inline">done</strong> flag is not <strong class="source-inline">True</strong>, carry out the following step:<p class="source-code">history = []</p><p class="source-code">for i in np.arange(1000):</p><p class="source-code">    observation = env.reset()</p><p class="source-code">    score = 0</p><p class="source-code">    done = False</p><p class="source-code">    while not done:</p></li>
				<li>Select the observations and get the new <strong class="source-inline">state</strong>, <strong class="source-inline">reward</strong>, and <strong class="source-inline">done</strong> flags. Save the <strong class="source-inline">observation</strong>, <strong class="source-inline">action</strong>, <strong class="source-inline">reward</strong>, <strong class="source-inline">state_new</strong>, and <strong class="source-inline">done</strong> flags. Call the <strong class="source-inline">learn</strong> function of the agent and add the current reward to the running score. Set the new state as the observation and finally, when the done flag is <strong class="source-inline">True</strong>, append <strong class="source-inline">score</strong> to <strong class="source-inline">history</strong>:<p class="source-code">history = []</p><p class="source-code">for i in np.arange(1000):</p><p class="source-code">    observation = env.reset()</p><p class="source-code">    score = 0</p><p class="source-code">    done = False</p><p class="source-code">    while not done:</p><p class="source-code">        action = agent.select_action(observation)</p><p class="source-code">        state_new, reward, done, info = env.step(action)</p><p class="source-code">        agent.remember(observation, action, reward, \</p><p class="source-code">                       state_new, int(done))</p><p class="source-code">        agent.learn()</p><p class="source-code">        score += reward</p><p class="source-code">        observation = state_new</p><p class="source-code">        # env.render() # Uncomment to see the game window</p><p class="source-code">    history.append(score)</p><p>You can print out <strong class="source-inline">score</strong> and mean <strong class="source-inline">score_history</strong> results to see how the agent is learning over time.</p><p class="callout-heading">Note</p><p class="callout">To observe the rewards, we can simply add the <strong class="source-inline">print</strong> statement. The rewards will be similar to those in the previous exercise.</p><p>Run the code for at least 1,000 iterations and watch your lander attempt to land on the lunar surface.</p><p class="callout-heading">Note</p><p class="callout">To see the Lunar Lander simulation once the policy is learned, we just need to uncomment the <strong class="source-inline">env.render()</strong> code from the preceding code block. As seen in the previous exercise, this will open another window, where we will be able to see the game simulation.</p><p>Here's a glimpse of how your lunar lander might behave once it has learned the policy:</p><div id="_idContainer854" class="IMG---Figure"><img src="image/B16182_11_16.jpg" alt="Figure 11.16: Screenshots from the environment after 1,000 rounds of training&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.16: Screenshots from the environment after 1,000 rounds of training</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30X03Ul">https://packt.live/30X03Ul</a>.</p>
			<p class="callout">This section does not currently have an online interactive example and will need to be run locally.</p>
			<h2 id="_idParaDest-361"><a id="_idTextAnchor405"/>Activity 11.02: Loading the Saved Policy to Run the Lunar Lander Simulation</h2>
			<ol>
				<li value="1">Import the essential Python libraries:<p class="source-code">import os</p><p class="source-code">import gym</p><p class="source-code">import torch as T</p><p class="source-code">import numpy as np</p><p class="source-code">from PIL import Image</p></li>
				<li>Set your device using the <strong class="source-inline">device</strong> parameter:<p class="source-code">device = T.device("cuda:0" if T.cuda.is_available() else "cpu")</p></li>
				<li>Define the <strong class="source-inline">ReplayBuffer</strong> class, as we did in the previous exercise:<p class="source-code">class ReplayBuffer:</p><p class="source-code">    def __init__(self):</p><p class="source-code">        self.memory_actions = []</p><p class="source-code">        self.memory_states = []</p><p class="source-code">        self.memory_log_probs = []</p><p class="source-code">        self.memory_rewards = []</p><p class="source-code">        self.is_terminals = []</p><p class="source-code">    def clear_memory(self):</p><p class="source-code">        del self.memory_actions[:]</p><p class="source-code">        del self.memory_states[:]</p><p class="source-code">        del self.memory_log_probs[:]</p><p class="source-code">        del self.memory_rewards[:]</p><p class="source-code">        del self.is_terminals[:]</p></li>
				<li>Define the <strong class="source-inline">ActorCritic</strong> class, as we did in the previous exercise:<p class="source-code-heading">Activity11_02.ipynb</p><p class="source-code">class ActorCritic(T.nn.Module):</p><p class="source-code">    def __init__(self, state_dimension, action_dimension, \</p><p class="source-code">                 nb_latent_variables):</p><p class="source-code">        super(ActorCritic, self).__init__()</p><p class="source-code">        self.action_layer = T.nn.Sequential\</p><p class="source-code">                            (T.nn.Linear(state_dimension, \</p><p class="source-code">                                         nb_latent_variables),\</p><p class="source-code">                            T.nn.Tanh(),\</p><p class="source-code">                            T.nn.Linear(nb_latent_variables, \</p><p class="source-code">                                        nb_latent_variables),\</p><p class="source-code">                            T.nn.Tanh(),\</p><p class="source-code">                            T.nn.Linear(nb_latent_variables, \</p><p class="source-code">                                        action_dimension),\</p><p class="source-code">                            T.nn.Softmax(dim=-1))</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/2YhzrvD">https://packt.live/2YhzrvD</a>.</p></li>
				<li>Define the <strong class="source-inline">Agent</strong> class, as we did in the previous exercise:<p class="source-code-heading">Activity11_02.ipynb</p><p class="source-code">class Agent:</p><p class="source-code">    def __init__(self, state_dimension, action_dimension, \</p><p class="source-code">    nb_latent_variables, lr, betas, gamma, K_epochs, eps_clip):\</p><p class="source-code">        self.lr = lr</p><p class="source-code">        self.betas = betas</p><p class="source-code">        self.gamma = gamma</p><p class="source-code">        self.eps_clip = eps_clip</p><p class="source-code">        self.K_epochs = K_epochs</p><p class="source-code">      </p><p class="source-code">        self.policy = ActorCritic(state_dimension,\</p><p class="source-code">                                  action_dimension,\</p><p class="source-code">                                  nb_latent_variables).to(device)</p><p class="source-code">        self.optimizer = T.optim.Adam\</p><p class="source-code">                         (self.policy.parameters(), \</p><p class="source-code">                          lr=lr, betas=betas)</p><p class="source-code">        self.policy_old = ActorCritic(state_dimension,\</p><p class="source-code">                                      action_dimension,\</p><p class="source-code">                                      nb_latent_variables)\</p><p class="source-code">                                      .to(device)</p><p class="source-code">        self.policy_old.load_state_dict(self.policy.state_dict())</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/2YhzrvD">https://packt.live/2YhzrvD</a>.</p></li>
				<li>Create the Lunar Lander environment. Initialize the random seed:<p class="source-code">env = gym.make(„LunarLander-v2")</p><p class="source-code">np.random.seed(0)</p><p class="source-code">render = True</p></li>
				<li>Create the memory buffer and initialize the agent with hyperparameters, as in the previous exercise:<p class="source-code">memory = ReplayBuffer()</p><p class="source-code">agent = Agent(state_dimension=env.observation_space.shape[0],\</p><p class="source-code">              action_dimension=4, nb_latent_variables=64,\</p><p class="source-code">              lr=0.002, betas=(0.9, 0.999), gamma=0.99,\</p><p class="source-code">              K_epochs=4,<strong class="source-inline"> </strong>eps_clip=0.2)</p></li>
				<li>Load the saved policy as an old policy from the <strong class="source-inline">Exercise11.03</strong> folder:<p class="source-code">agent.policy_old.load_state_dict\</p><p class="source-code">(T.load("../Exercise11.03/PPO_LunarLander-v2.pth"))</p></li>
				<li>Finally, loop through your desired number of episodes. In every iteration, start by initializing the episode reward as <strong class="source-inline">0</strong>. Do not forget to reset the state. Run another loop, specifying the <strong class="source-inline">max</strong> timestamp. Get the <strong class="source-inline">state</strong>, <strong class="source-inline">reward</strong>, and <strong class="source-inline">done</strong> flags for each action taken and add the reward to the episode reward. Render the environment to see how your Lunar Lander is doing:<p class="source-code">for ep in range(5):</p><p class="source-code">    ep_reward = 0</p><p class="source-code">    state = env.reset()</p><p class="source-code">    for t in range(300):</p><p class="source-code">        action = agent.policy_old.act(state, memory)</p><p class="source-code">        state, reward, done, _ = env.step(action)</p><p class="source-code">        ep_reward += reward</p><p class="source-code">        if render:</p><p class="source-code">            env.render()</p><p class="source-code">            img = env.render(mode = „rgb_array")</p><p class="source-code">            img = Image.fromarray(img)</p><p class="source-code">            image_dir = "./gif"</p><p class="source-code">            if not os.path.exists(image_dir):</p><p class="source-code">                os.makedirs(image_dir)</p><p class="source-code">            img.save(os.path.join(image_dir, "{}.jpg".format(t)))</p><p class="source-code">        if done:</p><p class="source-code">            break</p><p class="source-code">    print("Episode: {}, Reward: {}".format(ep, int(ep_reward)))</p><p class="source-code">    ep_reward = 0</p><p class="source-code">    env.close()</p><p>The following is the output of the code:</p><p class="source-code">Episode: 0, Reward: 272</p><p class="source-code">Episode: 1, Reward: 148</p><p class="source-code">Episode: 2, Reward: 249</p><p class="source-code">Episode: 3, Reward: 169</p><p class="source-code">Episode: 4, Reward: 35</p><p>You'll see the reward oscillate in the positive zone as our Lunar Lander now has some idea of what a good policy can be. The reward may oscillate as there is more scope for learning. You might iterate over a few thousand more iterations to make your agent learn a better policy. Do not hesitate to tinker with the parameters specified in the code. The following screenshot shows the simulation output of some of the stages:</p><div id="_idContainer855" class="IMG---Figure"><img src="image/B16182_11_17.jpg" alt="Figure 11.17: The environment showing the simulation of the Lunar Lander&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.17: The environment showing the simulation of the Lunar Lander</p>
			<p>Before this activity, we explained some necessary concepts, such as creating a learning agent, training a policy, saving and loading the learned policies, and so on, in isolation. Through carrying out this activity, you learned how to build a complete RL project or a working prototype on your own by combining all that you have learned in this chapter.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The complete simulation output can be found in the form of images at <a href="https://packt.live/3ehPaAj">https://packt.live/3ehPaAj</a>.</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YhzrvD">https://packt.live/2YhzrvD</a>.</p>
			<p class="callout">This section does not currently have an online interactive example and will need to be run locally.</p>
			<h1 id="_idParaDest-362"><a id="_idTextAnchor406"/>12. Evolutionary Strategies for RL</h1>
			<h2 id="_idParaDest-363"><a id="_idTextAnchor407"/>Activity 12.01: Cart-Pole Activity</h2>
			<ol>
				<li value="1">Import the required packages as follows:<p class="source-code">import gym </p><p class="source-code">import numpy as np </p><p class="source-code">import math </p><p class="source-code">import tensorflow as tf</p><p class="source-code">from matplotlib import pyplot as plt</p><p class="source-code">from random import randint</p><p class="source-code">from statistics import median, mean</p></li>
				<li>Initialize the environment and the state and action space shapes:<p class="source-code">env = gym.make('CartPole-v0')</p><p class="source-code">no_states = env.observation_space.shape[0]</p><p class="source-code">no_actions = env.action_space.n</p></li>
				<li>Create a function to generate randomly selected initial network parameters:<p class="source-code">def initial(run_test):</p><p class="source-code">    #initialize arrays</p><p class="source-code">    i_w = []</p><p class="source-code">    i_b = []</p><p class="source-code">    h_w = []</p><p class="source-code">    o_w = []</p><p class="source-code">    no_input_nodes = 8</p><p class="source-code">    no_hidden_nodes = 4</p><p class="source-code">    </p><p class="source-code">    for r in range(run_test):</p><p class="source-code">        input_weight = np.random.rand(no_states, no_input_nodes)</p><p class="source-code">        input_bias = np.random.rand((no_input_nodes))</p><p class="source-code">        hidden_weight = np.random.rand(no_input_nodes,\</p><p class="source-code">                                       no_hidden_nodes)</p><p class="source-code">        output_weight = np.random.rand(no_hidden_nodes, \</p><p class="source-code">                                       no_actions)</p><p class="source-code">        i_w.append(input_weight)</p><p class="source-code">        i_b.append(input_bias)</p><p class="source-code">        h_w.append(hidden_weight)</p><p class="source-code">        o_w.append(output_weight)</p><p class="source-code">    chromosome =[i_w, i_b, h_w, o_w]</p><p class="source-code">    return chromosome</p></li>
				<li>Create a function to generate the neural network using the set of parameters:<p class="source-code">def nnmodel(observations, i_w, i_b, h_w, o_w):</p><p class="source-code">    alpha = 0.199</p><p class="source-code">    observations = observations/max\</p><p class="source-code">                   (np.max(np.linalg.norm(observations)),1)</p><p class="source-code">    #apply relu on layers</p><p class="source-code">    funct1 = np.dot(observations, i_w)+ i_b.T</p><p class="source-code">    layer1= tf.nn.relu(funct1)-alpha*tf.nn.relu(-funct1)</p><p class="source-code">    funct2 = np.dot(layer1,h_w)</p><p class="source-code">    layer2 = tf.nn.relu(funct2) - alpha*tf.nn.relu(-funct2)</p><p class="source-code">    funct3 = np.dot(layer2, o_w)</p><p class="source-code">    layer3 = tf.nn.relu(funct3)-alpha*tf.nn.relu(-funct3)</p><p class="source-code">    #apply softmax</p><p class="source-code">    layer3 = np.exp(layer3)/np.sum(np.exp(layer3))</p><p class="source-code">    output = layer3.argsort().reshape(1,no_actions)</p><p class="source-code">    action = output[0][0]</p><p class="source-code">    return action</p></li>
				<li>Create a function to get the total reward for <strong class="source-inline">300</strong> steps when using the neural network:<p class="source-code">def get_reward(env, i_w, i_b, h_w, o_w):</p><p class="source-code">    current_state = env.reset()</p><p class="source-code">    total_reward = 0</p><p class="source-code">    for step in range(300):</p><p class="source-code">        action = nnmodel(current_state, i_w, i_b, h_w, o_w)</p><p class="source-code">        next_state, reward, done, info = env.step(action)</p><p class="source-code">        total_reward += reward</p><p class="source-code">        current_state = next_state</p><p class="source-code">        if done:</p><p class="source-code">            break</p><p class="source-code">    return total_reward</p></li>
				<li>Create a function to get the fitness scores for each element of the population when running the initial random selection:<p class="source-code">def get_weights(env, run_test):</p><p class="source-code">    rewards = []</p><p class="source-code">    chromosomes = initial(run_test)</p><p class="source-code">    for trial in range(run_test):</p><p class="source-code">        i_w = chromosomes[0][trial]</p><p class="source-code">        i_b = chromosomes[1][trial]</p><p class="source-code">        h_w = chromosomes[2][trial]</p><p class="source-code">        o_w = chromosomes[3][trial]</p><p class="source-code">        total_reward = get_reward(env, i_w, i_b, h_w, o_w)</p><p class="source-code">        rewards = np.append(rewards, total_reward)</p><p class="source-code">    chromosome_weight = [chromosomes, rewards]</p><p class="source-code">    return chromosome_weight</p></li>
				<li>Create a mutation function:<p class="source-code">def mutate(parent):</p><p class="source-code">    index = np.random.randint(0, len(parent))</p><p class="source-code">    if(0 &lt; index &lt; 10):</p><p class="source-code">        for idx in range(index):</p><p class="source-code">            n = np.random.randint(0, len(parent))</p><p class="source-code">            parent[n] = parent[n] + np.random.rand()</p><p class="source-code">    mutation = parent</p><p class="source-code">    return mutation</p></li>
				<li>Create a single-point crossover function:<p class="source-code">def crossover(list_chr):</p><p class="source-code">    gen_list = []</p><p class="source-code">    gen_list.append(list_chr[0])</p><p class="source-code">    gen_list.append(list_chr[1])</p><p class="source-code">    for i in range(10):</p><p class="source-code">        m = np.random.randint(0, len(list_chr[0]))</p><p class="source-code">        parent = np.append(list_chr[0][:m], list_chr[1][m:])</p><p class="source-code">        child = mutate(parent)</p><p class="source-code">        gen_list.append(child)</p><p class="source-code">    return gen_list</p></li>
				<li>Create a function for creating the next generation by selecting the pair with the highest rewards:<p class="source-code">def generate_new_population(rewards, chromosomes):</p><p class="source-code">    #2 best reward indexes selected</p><p class="source-code">    best_reward_idx = rewards.argsort()[-2:][::-1]</p><p class="source-code">    list_chr = []</p><p class="source-code">    new_i_w =[]</p><p class="source-code">    new_i_b = []</p><p class="source-code">    new_h_w = []</p><p class="source-code">    new_o_w = []</p><p class="source-code">    new_rewards = []</p></li>
				<li>Get the current parameters for the weights and bias using a <strong class="source-inline">for</strong> loop to go through the indices:<p class="source-code">    for ind in best_reward_idx:</p><p class="source-code">        weight1 = chromosomes[0][ind]</p><p class="source-code">        w1 = weight1.reshape(weight1.shape[1], -1)</p><p class="source-code">        bias1 = chromosomes[1][ind]</p><p class="source-code">        b1 = np.append(w1, bias1)</p><p class="source-code">        weight2 = chromosomes[2][ind]</p><p class="source-code">        w2 = np.append\</p><p class="source-code">             (b1, weight2.reshape(weight2.shape[1], -1))</p><p class="source-code">        weight3 = chromosomes[3][ind]</p><p class="source-code">        chr = np.append(w2, weight3)</p><p class="source-code">        #the 2 best parents are selected</p><p class="source-code">        list_chr.append(chr)</p><p class="source-code">    gen_list = crossover(list_chr)</p></li>
				<li>Build the neural network using the identified parameters and obtain a new reward based on the constructed neural network:<p class="source-code">    for l in gen_list:</p><p class="source-code">        chromosome_w1 = np.array(l[:chromosomes[0][0].size])</p><p class="source-code">        new_input_weight = np.reshape(chromosome_w1,(-1,chromosomes[0][0].shape[1]))</p><p class="source-code">        new_input_bias = np.array\</p><p class="source-code">                         ([l[chromosome_w1.size:chromosome_w1\</p><p class="source-code">                           .size+chromosomes[1][0].size]]).T</p><p class="source-code">        hidden = chromosome_w1.size + new_input_bias.size</p><p class="source-code">        chromosome_w2 = np.array\</p><p class="source-code">                        ([l[hidden:hidden \</p><p class="source-code">                         + chromosomes[2][0].size]])</p><p class="source-code">        new_hidden_weight = np.reshape\</p><p class="source-code">                            (chromosome_w2, \</p><p class="source-code">                            (-1, chromosomes[2][0].shape[1]))</p><p class="source-code">        final = chromosome_w1.size+new_input_bias.size\</p><p class="source-code">                +chromosome_w2.size</p><p class="source-code">        new_output_weight = np.array([l[final:]]).T</p><p class="source-code">        new_output_weight = np.reshape\</p><p class="source-code">                            (new_output_weight,\</p><p class="source-code">                            (-1, chromosomes[3][0].shape[1]))</p><p class="source-code">        new_i_w.append(new_input_weight)</p><p class="source-code">        new_i_b.append(new_input_bias)</p><p class="source-code">        new_h_w.append(new_hidden_weight)</p><p class="source-code">        new_o_w.append(new_output_weight)</p><p class="source-code">        new_reward = get_reward(env, new_input_weight, \</p><p class="source-code">                                new_input_bias, new_hidden_weight, \</p><p class="source-code">                                new_output_weight)</p><p class="source-code">        new_rewards = np.append(new_rewards, new_reward)</p><p class="source-code">    generation = [new_i_w, new_i_b, new_h_w, new_o_w]</p><p class="source-code">    return generation, new_rewards</p></li>
				<li>Create a function to output the convergence graph:<p class="source-code">def graphics(act):</p><p class="source-code">    plt.plot(act)</p><p class="source-code">    plt.xlabel('No. of generations')</p><p class="source-code">    plt.ylabel('Rewards')</p><p class="source-code">    plt.grid()</p><p class="source-code">    print('Mean rewards:', mean(act))</p><p class="source-code">    return plt.show()</p></li>
				<li>Create a function for the genetic algorithm that outputs the parameters of the neural network based on the highest average reward:<p class="source-code">def ga_algo(env, run_test, no_gen):</p><p class="source-code">    weights = get_weights(env, run_test)</p><p class="source-code">    chrom = weights[0]</p><p class="source-code">    current_rewards = weights[1]</p><p class="source-code">    act = []</p><p class="source-code">    for n in range(no_gen):</p><p class="source-code">        gen, new_rewards = generate_new_population\</p><p class="source-code">                           (current_rewards, chrom)</p><p class="source-code">        average = np.average(current_rewards)</p><p class="source-code">        new_average = np.average(new_rewards)</p><p class="source-code">        if average &gt;  new_average:</p><p class="source-code">            parameters = [chrom[0][0], chrom[1][0], \</p><p class="source-code">                          chrom[2][0], chrom[3][0]]</p><p class="source-code">        else:</p><p class="source-code">             parameters = [gen[0][0], gen[1][0], \</p><p class="source-code">                           gen[2][0], gen[3][0]]</p><p class="source-code">        chrom = gen</p><p class="source-code">        current_rewards = new_rewards</p><p class="source-code">        max_arg = np.amax(current_rewards)</p><p class="source-code">        print('Generation:{}, max reward:{}'.format(n+1, max_arg))</p><p class="source-code">        act = np.append(act, max_arg)</p><p class="source-code">    graphics(act)</p><p class="source-code">    return parameters</p></li>
				<li>Create a function that decodes the array of parameters to each neural network parameter:<p class="source-code">def params(parameters):</p><p class="source-code">    i_w = parameters[0]</p><p class="source-code">    i_b = parameters[1]</p><p class="source-code">    h_w = parameters[2]</p><p class="source-code">    o_w = parameters[3]</p><p class="source-code">    return i_w,i_b,h_w,o_w</p></li>
				<li>Set the generations to <strong class="source-inline">50</strong>, the number of trial tests to <strong class="source-inline">15</strong>, and the number of steps and trials to <strong class="source-inline">500</strong>:<p class="source-code">generations = []</p><p class="source-code">no_gen = 50</p><p class="source-code">run_test = 15</p><p class="source-code">trial_length = 500</p><p class="source-code">no_trials = 500</p><p class="source-code">rewards = []</p><p class="source-code">final_reward = 0</p><p class="source-code">parameters = ga_algo(env, run_test, no_gen)</p><p class="source-code">i_w, i_b, h_w, o_w = params(parameters)</p><p class="source-code">for trial in range(no_trials):</p><p class="source-code">    current_state = env.reset()</p><p class="source-code">    total_reward = 0</p><p class="source-code">    for step in range(trial_length):</p><p class="source-code">        env.render()</p><p class="source-code">        action = nnmodel(current_state, i_w,i_b, h_w, o_w)</p><p class="source-code">        next_state,reward, done, info = env.step(action)</p><p class="source-code">        total_reward += reward</p><p class="source-code">        current_state = next_state</p><p class="source-code">        if done:</p><p class="source-code">            break</p><p class="source-code">    print('Trial:{}, total reward:{}'.format(trial, total_reward))</p><p class="source-code">    final_reward +=total_reward</p><p class="source-code">print('Average reward:', final_reward/no_trials)</p><p class="source-code">env.close()</p><p>The output (just the first few lines are shown here) will be similar to the following:</p><p class="source-code">Generation:1, max reward:11.0</p><p class="source-code">Generation:2, max reward:11.0</p><p class="source-code">Generation:3, max reward:10.0</p><p class="source-code">Generation:4, max reward:10.0</p><p class="source-code">Generation:5, max reward:11.0</p><p class="source-code">Generation:6, max reward:10.0</p><p class="source-code">Generation:7, max reward:10.0</p><p class="source-code">Generation:8, max reward:10.0</p><p class="source-code">Generation:9, max reward:11.0</p><p class="source-code">Generation:10, max reward:10.0</p><p class="source-code">Generation:11, max reward:10.0</p><p class="source-code">Generation:12, max reward:10.0</p><p class="source-code">Generation:13, max reward:10.0</p><p class="source-code">Generation:14, max reward:10.0</p><p class="source-code">Generation:15, max reward:10.0</p><p class="source-code">Generation:16, max reward:10.0</p><p class="source-code">Generation:17, max reward:10.0</p><p class="source-code">Generation:18, max reward:10.0</p><p class="source-code">Generation:19, max reward:11.0</p><p class="source-code">Generation:20, max reward:11.0</p><p>The output can be visualized in a plot as follows:</p><div id="_idContainer856" class="IMG---Figure"><img src="image/B16182_12_15.jpg" alt="Figure 12.15: Rewards obtained over the generations&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 12.15: Rewards obtained over the generations</p>
			<p>The average of the rewards output (just the last few lines are shown here) will be similar to the following:</p>
			<p class="source-code">Trial:486, total reward:8.0</p>
			<p class="source-code">Trial:487, total reward:9.0</p>
			<p class="source-code">Trial:488, total reward:10.0</p>
			<p class="source-code">Trial:489, total reward:10.0</p>
			<p class="source-code">Trial:490, total reward:8.0</p>
			<p class="source-code">Trial:491, total reward:9.0</p>
			<p class="source-code">Trial:492, total reward:9.0</p>
			<p class="source-code">Trial:493, total reward:10.0</p>
			<p class="source-code">Trial:494, total reward:10.0</p>
			<p class="source-code">Trial:495, total reward:9.0</p>
			<p class="source-code">Trial:496, total reward:10.0</p>
			<p class="source-code">Trial:497, total reward:9.0</p>
			<p class="source-code">Trial:498, total reward:10.0</p>
			<p class="source-code">Trial:499, total reward:9.0</p>
			<p class="source-code">Average reward: 9.384</p>
			<p>You will notice that depending on the start state, the convergence of the GA algorithm to the highest score will vary; also, the neural network model will not always achieve the optimal solution. The purpose of this activity was for you to implement the genetic algorithm techniques studied in this chapter and to see how you can combine evolutionary methods of neural network parameter tuning for action selection.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2AmKR8m">https://packt.live/2AmKR8m</a>.</p>
			<p class="callout">This section does not currently have an online interactive example and will need to be run locally.</p>
		</div>
		<div>
			<div id="_idContainer858" class="Content">
			</div>
		</div>
	</body></html>