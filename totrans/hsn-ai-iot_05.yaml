- en: Genetic Algorithms for IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we looked at different deep learning-based algorithms;
    these algorithms have shown their success in the fields of recognition, detection,
    reconstruction, and even in the generation of vision, speech, and text data. While,
    at present, **deep learning** (**DL**) is on top in terms of both application
    and employability, it has close competition with evolutionary algorithms. The
    algorithms are inspired by the natural process of evolution, the world''s best
    optimizers. Yes, even we are the result of years of genetic evolution. In this
    chapter, you will be introduced to the fascinating world of evolutionary algorithms
    and learn about a specific type of evolutionary algorithm, genetic algorithms,
    in more detail. In this chapter, you will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different methods to solve an optimization problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the intuition behind genetic algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advantages of genetic algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand and implement the processes of cross-over, mutation, and fitness
    function selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a genetic algorithm to find a lost password
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various uses of genetic algorithms in optimizing your models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Distributed Evolutionary Algorithms in the Python genetic algorithm library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimization is not a new word; we have used it earlier with respect to both
    machine learning and DL algorithms, where we used the TensorFlow auto differentiator
    to find the optimum model weights and biases using a form of gradient descent
    algorithm. In this section, we will learn a little more about optimization, optimization
    problems, and different techniques used to perform optimization.
  prefs: []
  type: TYPE_NORMAL
- en: In its most basic terms, **optimization** is the process of making something
    better. The idea is to find the best solution, and obviously when we talk about
    the best solution, it means there exists more than one solution. In optimization,
    we try to adjust our variable parameters/processes/inputs so that we can find
    the minimum or maximum output. Normally, the variables constitute the inputs,
    we have a function called an **objective** **function**, **loss** **function**,
    or **fitness** **function**, and as output we expect the cost/loss or fitness.
    The cost or loss should be minimized, and if we define fitness, then it should
    be maximized. Here, we vary the inputs (variables) to achieve a desired (optimized)
    output.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you can appreciate that calling it loss/cost or fitness is just a matter
    of choice, the function which calculates the cost and needs to be minimized, if
    we just add a negative sign to it then we expect the modified function to be maximized. As
    an example, minimizing *2 - x²* over the interval *-2 < x< 2* is the same as maximizing *x*² -
    2 over the same interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our daily lives are full of many such optimization tasks. What will be the
    best route to take to the office? Which project should I do first? Preparing for
    an interview what topics to read such that your success rate in the interview
    is maximized. The following diagram shows the basic relationship between **Input
    Variables**, the **Function** to be optimized, and the **Output/Cos****t**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5faa50e4-a362-4b51-9857-706e8d811926.png)'
  prefs: []
  type: TYPE_IMG
- en: Relationship between input, the function to be optimized, and the output
  prefs: []
  type: TYPE_NORMAL
- en: 'The aim is to minimize the cost, such that the constraints specified by the
    function are satisfied by the input variables. The mathematical relationship between
    the cost function, constraints, and input variables determines the complexity
    of the optimization problem. One of the key issues is whether the cost function
    and constraints are convex or non-convex. If the cost function and constraints
    are convex, we can be confident that there does exist a feasible solution, and
    if we search in a sufficiently large domain, we will find one. The following figure
    shows an example of a convex cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cccbeb8-8a44-49be-825a-d01a8de0aa69.png)'
  prefs: []
  type: TYPE_IMG
- en: A convex cost function. The one on the left is the surface plot and the one
    on the right shows the contour plot of the same cost function. The darkest red
    point in the image corresponds to the optimum solution point.
  prefs: []
  type: TYPE_NORMAL
- en: If, on other hand, the cost function or constraints are non-convex, the optimization
    problem becomes harder and we cannot be sure that there does exist a solution,
    or that we can even find one.
  prefs: []
  type: TYPE_NORMAL
- en: There are various methods to solve optimization problems in mathematics and
    computer programming. Let's find out a little about each of them next.
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic and analytic methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When the objective function is smooth with a continuous second derivative,
    then we know from the knowledge of calculus that at a local minimum the following
    are true:'
  prefs: []
  type: TYPE_NORMAL
- en: The gradient of the objective function at minima *x**, that is, *f*'(*x**) = *0*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second derivative (Hessian *H*(*x**) = ∇²*f*(*x*)) is positively definite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In such conditions, for some problems, it is possible to find the solution analytically
    by determining the zeros of the gradient and verifying the positive definiteness
    of the Hessian matrix at the zeros. So, in these cases, we can explore the search
    space iteratively for the minima of the objective function. There are various
    search methods; let's see them.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned about gradient descent and how it works in earlier chapters, and
    we saw that the search direction is the direction of the gradient descent, -∇*f*(*x*).
    It is also called the **Cauchy** **method** because it was given by Cauchy, in
    1847, and since then it has been very popular. We start from an arbitrary point
    on the objective function surface and change the variables (in earlier chapters,
    these were the weights and biases) along the direction of the gradient. Mathematically,
    it is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a2c400a-a638-4540-80fe-16d068edddeb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here *α[n]* is the step size (variation/learning rate) at iteration *n*. Gradient
    descent algorithms have worked well in training DL models, but they have some
    severe drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: The performance of the optimizer used depends greatly on the learning rate and
    other constants. If you change them even slightly, there is a big possibility
    that the network may not converge. And it is because of this that sometimes researchers
    call training a model an art, or alchemy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since these methods are based on derivatives, they do not work for discrete
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We cannot reliably apply it when the objective function is non-convex, which
    is the case in many DL networks (especially models using a non-linear activation
    function). The presence of many hidden layers can result in many local minima,
    and there is a strong possibility that the model gets stuck in a local minimum.
    Here, you can see an example of the objective function with many local minima:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/461829bc-e0cd-4ba9-afb6-537412c2290b.png)'
  prefs: []
  type: TYPE_IMG
- en: A cost function with many local minima. The one on the left is the surface plot
    and the one on the right shows the contour plot of the same cost function. The
    dark red points in the image correspond to minima.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many variants of the gradient descent method, and the most popular
    of them are available in the TensorFlow optimizers, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adagrad optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSProp optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can learn more about the different optimizers available in TensorFlow from
    the TensorFlow documentation at [https://www.tensorflow.org/api_guides/python/train](https://www.tensorflow.org/api_guides/python/train).
  prefs: []
  type: TYPE_NORMAL
- en: A nice source is a blog ([http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms](http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms))
    by Sebastian Ruder based on his arXiv paper at [https://arxiv.org/abs/1609.04747](https://arxiv.org/abs/1609.04747).
  prefs: []
  type: TYPE_NORMAL
- en: Newton-Raphson method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This method is based on the second order Taylor series expansion of the objective
    function, *f*(*x*), around the point *x^*:*
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/a11222e4-a00f-4af3-a479-040ed79a23e1.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *x** is the point about which the Taylor series is expanded, *x* is a
    point near *x**, the superscript *T* represents the transpose, and *H* is the
    Hessian matrix with elements given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/937ab061-0f44-4ee3-928f-32bcb2750d9f.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking the gradient of the Taylor series expansion and equating to **0**, we
    get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/59bfab87-627a-4859-9628-f9b3601c22af.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming the initial guess as *x*[0], the next point *x*[n+1] can be obtained
    from the previous point *x*[n] using this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/015ff534-b7eb-48cc-85e1-e831ef67b9b5.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: The method uses both the first and the second partial derivatives of the objective
    function to find the minima. At iteration *k*, it approximates the objective function
    by a quadratic function around *x*(*k*) and moves toward its minima.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since computing the Hessian matrix is computationally expensive and not normally
    known, a large number of algorithms exist around approximating the Hessian; these
    techniques are called **quasi-Newton methods**. They can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/fd1156ee-4259-4c4f-b0c2-10b26246b773.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*α[n]* is the step size (variation/learning rate) at iteration *n*, and *A[n]* is
    the approximation to the Hessian matrix at iteration *n*. We construct a sequence
    of approximations to the Hessian, such that the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '*![](img/6b0712ed-5a5b-4685-b6d3-6f4ac7fa192e.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two popular quasi-Newton methods are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Davidon-Fletcher-Powell algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Broyden-Fletcher-Goldfarb-Shanno algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the approximation *A[n]* to the Hessian is the identity matrix, the Newton
    method becomes the gradient descent method.
  prefs: []
  type: TYPE_NORMAL
- en: The major disadvantage of the Newton method is that it is not scalable to problems
    with a high-dimensional input feature space.
  prefs: []
  type: TYPE_NORMAL
- en: Natural optimization methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Natural optimization methods are inspired by some natural processes, that is,
    a process, existing in nature, that is remarkably successful at optimizing some
    natural phenomena. These algorithms do not require taking objective function derivatives,
    and thus can be employed even for discrete variables and non-continuous objective
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Simulated annealing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simulated annealing is a stochastic method. It is inspired by the physical process
    of annealing, in which a solid is first heated to a high enough temperature so
    that it melts, and then the temperature is decreased slowly; this allows the particles
    of the solid to arrange themselves in the lowest possible energy state and thus
    produce a highly structured lattice.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with some random values assigned to each variable; this represents
    the initial state. At each step, we pick a variable (or group of variables) at
    random and then select a random value. If, upon the assignment of that value to
    the variable, there is an improvement in the objective function, the algorithm
    accepts the assignment, there is a new current assignment, and the state of the
    system changes. Otherwise, it accepts the assignment with some probability *P*,
    which depends on the temperature *T* and the difference between the values of
    the objective function in the current state and the new state. If the change is
    not accepted, the current state remains unchanged. The probability *P* that we
    will change from state *i* to state *j* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5e95f0e-aad9-4b60-9300-738600d75d2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *T* represents a variable analogous to temperature in the physical system.
    As the temperature approaches *0*, the simulated annealing algorithm reduces to
    the gradient descent algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Particle Swarm Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Particle Swarm Optimization** (**PSO**) was developed by Edward and Kennedy
    in 1995\. It is based on the social behavior of animals, such as a flock of birds.
    You must have noticed in the sky, birds fly in a V shape. Those who have studied
    bird behavior tell us that birds fly like this when in search of food or a better
    location, with the one leading being nearest to the desired source.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, when they fly, the leading bird does not remain the same; instead, it changes
    as they move. The bird in the flock that sees the food sends a sound signal, and
    all other birds then collect around that bird in a V fashion. This is a continuous
    repetitive process, and has served birds well for millions of years.
  prefs: []
  type: TYPE_NORMAL
- en: PSO takes inspiration from this bird behavior and uses it to solve optimization
    problems. In PSO, every single solution is a bird (called a **particle**) in the
    search space. Each particle has a fitness value, which is evaluated by the fitness
    function to be optimized; they also have velocities, which direct the flying of
    the particles. The particles fly through the problem search space by following
    the current optimum particle.
  prefs: []
  type: TYPE_NORMAL
- en: 'The particles are moved around in the search space guided by two best fitness
    values, one their own best known position in the search space (**pbest**: particle
    best), the other the entire swarm''s best known fitness value (**gbest**: global
    best). As improved positions are discovered, they are used to guide the movements
    of the particles of the swarm. This process is repeated and it is hoped that an
    optimum solution will eventually be discovered.'
  prefs: []
  type: TYPE_NORMAL
- en: Genetic algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we look around the world and see different species, a question naturally
    arises: why are these sets of features stable and not others; why should the majority
    of animals have two legs or four legs, and why not three? Is it possible that
    the world that we see today is the result of many iterations in a grand optimization
    algorithm?'
  prefs: []
  type: TYPE_NORMAL
- en: Let's imagine there is a cost function that measures survivability, which should
    be maximized. The characteristics of the organisms of the natural world fit into
    a topological landscape. The level of survivability (measured through adaptation)
    represents the elevation of the landscape. The highest points correspond to the
    most-fit conditions, and the constraints are provided by the environment and through
    interaction between different species.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the process of evolution can be thought of as a grand optimization algorithm
    that selects which characteristics produce a species of organism fit for survival.
    The peaks of the landscape are populated by living organisms. Some peaks are broad
    and hold a wide range of characteristics encompassing many organisms, while other
    peaks are very narrow and allow only very specific characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: We can extend this analogy to include valleys between the peaks separating different
    species. And, we can think that humankind might be at the global maximum peak
    of this landscape, since we have intelligence and the ability to alter the environment
    and ensure better survivability, even in extreme environments.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the world with different life forms can be thought of as a big search
    space, with different species as the result of many iterations of a grand optimization
    algorithm. This idea forms the basis of genetic algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Since the main theme of this chapter is genetic algorithms, let's dive into
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to genetic algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to the work of Charles Darwin, the famous biologist, the animal and
    plant species that we see today have emerged due to millions of years of evolution.
    The process of evolution is guided by the principle of *survival of the fittest*,
    selecting the organisms that have a better chance of survivability. The plants
    and animals that we see today are the results of millions of years of adaptation
    to the constraints imposed by the environment. At any given time, a large number
    of varied organisms may coexist and compete for the same environmental resources.
  prefs: []
  type: TYPE_NORMAL
- en: The organisms that are most capable of both acquiring the resources and procreation
    are the ones whose descendants will have more chances of survival. Organisms that
    are less capable, on the other hand, will tend to have few or no descendants.
    Over time, the entire population will evolve, containing on average organisms
    that are more fit than the previous generations.
  prefs: []
  type: TYPE_NORMAL
- en: 'What makes this possible? What decides that a person will be tall, and a plant
    will have a particular shape of leaf? All this is encoded like a set of rules
    in the program on the blueprint of life itself—genes. Every living organism on
    Earth has this set of rules and they describe how that organism is designed (created).
    Genes reside in chromosomes. Each being has a different number of chromosomes
    and they contain thousands of genes. For example, we homo sapiens have 46 chromosomes
    and these chromosomes contain about 20,000 genes. Each gene represents a specific
    rule: a person will have blue eyes, will have brown hair, will be a female, and
    so on. These genes pass from parents to offspring through the process of reproduction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways by which genes pass from parents to offspring:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Asexual reproduction**: In this, the child is the duplicate copy of the parent.
    It happens during a biological process called **mitosis**; lower organisms such
    as bacteria and fungi reproduce via mitosis. Only one parent is needed in this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1008f76b-7eca-4a10-b1e9-8247fb6e530f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The process of mitosis: the chromosomes of the parent first double, and the
    cell divides into two'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sexual reproduction**: This happens via a biological process called **meiosis**.
    In this, two parents are involved initially; each parent cell undergoes a process
    of crossover, where a part of one chromosome gets interchanged with a part of
    another chromosome. This modifies the genetic sequence; the cells then divide
    into two, but with only half the number of chromosomes each. The cells containing
    half the number of chromosomes (haploid) from the two parents then meet together
    to form a zygote, which later through mitosis and cell differentiation results
    in the production of an **offspring** similar to, yet different from, the parents:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7af1402c-aac5-4760-a6ef-88500a7da4bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The process of meiosis: parents'' cell chromosomes undergo crossover, a part
    of one chromosome overlaps and changes position with part of another chromosome.
    The cells then divide into two, with each divided cell containing only one chromosome
    (haploids). The two haploids from two parents then meet together to complete the
    total number of chromosomes.'
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting thing that happens in the natural process of selection and
    evolution is the phenomenon of mutation. Here, the genes undergo a sudden change
    and generate a completely new gene, which was not present in either parent. This
    phenomenon generates further diversity.
  prefs: []
  type: TYPE_NORMAL
- en: Sexual reproduction through generations is supposed to bring about evolution
    and ensure that organisms with the fittest characteristics have more descendants.
  prefs: []
  type: TYPE_NORMAL
- en: The genetic algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now learn how can we implement the genetic algorithm. This method was
    developed by John Holland in 1975\. It was shown that it can be used to solve
    an optimization problem by his student Goldberg, who used genetic algorithms to
    control gas pipeline transmission. Since then, genetic algorithms have remained
    popular, and have inspired various other evolutionary programs.
  prefs: []
  type: TYPE_NORMAL
- en: To apply genetic algorithms to solving optimization problems using the computer,
    as the first step we will need to **encode the problem variables into genes**.
    The genes can be a string of real numbers or a binary bit string (series of 0s
    and 1's). This represents a potential solution (individual) and many such solutions
    together form the population at time *t*. For instance, consider a problem where
    we need to find two variables, a and b, such that the two lie in the range (0,
    255). For binary gene representation, these two variables can be represented by
    a 16-bit chromosome, with the higher 8 bits representing gene a and the lower
    8 bits for b. The encoding will need to be later decoded to get the real values
    of the variables a and b.
  prefs: []
  type: TYPE_NORMAL
- en: The second important requirement for genetic algorithms is defining a proper **fitness
    function**, which calculates the fitness score of any potential solution (in the
    preceding example, it should calculate the fitness value of the encoded chromosome).
    This is the function that we want to optimize by finding the optimum set of parameters
    of the system or the problem at hand. The fitness function is problem-dependent.
    For example, in the natural process of evolution, the fitness function represents
    the organism's ability to operate and to survive in its environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have decided the encoding of the problem solution in genes and decided
    upon the fitness function, the genetic algorithm will then follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Population Initialization**: We need to create an initial population, where
    all chromosomes are (usually) randomly generated to yield an entire range of possible
    solutions (the search space). Occasionally, the solutions may be seeded in areas
    where optimal solutions are likely to be found. The population size depends on
    the nature of the problem, but typically contains several hundred potential solutions
    encoded into chromosomes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Parent Selection**: For each successive generation, based on the fitness
    function (or randomly), we next select a certain proportion of the existing population.
    This selected proportion of the population will then breed to form a new generation.
    This is done by the method of tournament selection: a fixed number of individuals
    are randomly selected (tournament size) and the individual with the best fitness
    score is chosen as one of the parents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reproduction**: We next generate the successive generation from those selected
    in step 2, through genetic operators such as crossover and mutation. These genetic
    operators ultimately result in a child (next generation) population of chromosomes
    that is different from the initial generation but at the same time shares many
    of the characteristics of its parents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluation**: The offspring generated are then evaluated using the fitness
    function, and they replace the least-fit individuals in the population to keep
    the population size unchanged.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Termination**: During the *Evaluation* step, if any of the offspring achieve
    the objective fitness score or the maximum number of generations is reached, then
    the genetic algorithm process is terminated. Otherwise, steps *2* to *4* are repeated
    to produce the next generation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Two operators that are important for the success of genetic algorithms are crossover
    and mutation.
  prefs: []
  type: TYPE_NORMAL
- en: Crossover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To perform the crossover operation, we select a random position on the chromosome
    of two parents, then the genetic information is swapped between them about this
    point, with a probability *P[x]*. This results in two new offspring. When the
    crossover takes place over a random point, it is called a **one-point crossover** (or **Single
    Point Crossover**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66fa82f1-5570-4f15-83bf-c73111bfbaf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One-point crossover: a random point is selected in the parent''s chromosomes
    and the corresponding gene bits are swapped'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also have more than one point where parents'' genes are swapped; this
    is called a **Mult****i-Point Crossover**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0da5152-3b46-4ead-9037-328ce748624e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Multi-point crossover: there is more than one point where the genes of the
    parents are swapped. This is an example of a double-point crossover.'
  prefs: []
  type: TYPE_NORMAL
- en: There exist a lot of crossovers people have tried, for example, uniform crossover,
    order-based crossover, and cyclic crossover.
  prefs: []
  type: TYPE_NORMAL
- en: Mutation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the crossover operation ensures variety and can help speed up the search,
    it does not generate new solutions. This is the work of the mutation operator,
    which helps in maintaining and introducing diversity in the population. The mutation
    operator is applied to certain genes (bits) of the child chromosomes with a probability, *P[m]*.
  prefs: []
  type: TYPE_NORMAL
- en: We can have a bit flip mutation; if we consider our earlier example, then in the
    16-bit chromosome, the bit flip mutation will cause a single bit to change its
    state (from *0* to* 1* or from *1* to *0*).
  prefs: []
  type: TYPE_NORMAL
- en: There is a possibility that we set the gene to a random value for all possible
    values. This is called **random resetting**.
  prefs: []
  type: TYPE_NORMAL
- en: The probability *P[m] *plays an important role; if we assign a very low value
    to *P[m]* it can lead to genetic drift, but on the other hand, a high *P[m]* may
    result in a loss of good solutions. We choose a mutation probability such that the algorithm
    learns to sacrifice short-term fitness in order to gain longer-term fitness.
  prefs: []
  type: TYPE_NORMAL
- en: Pros and cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Genetic algorithms sound cool, right! Now, before we try and build a code around
    them, let's point out certain advantages and disadvantages of genetic algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Genetic algorithms offer some intriguing advantages and can produce results
    when the tradition gradient-based approaches fail:'
  prefs: []
  type: TYPE_NORMAL
- en: They can be used to optimize either continuous or discrete variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike gradient descent, we do not require derivative information, which also
    means that there is no need for the fitness function to be continuous and differentiable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can simultaneously search from a wide sampling of the cost surface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can deal with a large number of variables without a significant increase
    in computation time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generation of the population and calculating their fitness values can be
    performed in parallel, and hence genetic algorithms are well suited for parallel
    computers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can work even when the topological surface is extremely complex because
    crossover and mutation operators help them in jumping out of a local minimum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can provide more than one optimum solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use them with numerically generated data, experimental data, or even
    analytical functions. They specifically work well for large-scale optimization
    problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disadvantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Despite the previously mentioned advantages, we still do not find genetic algorithms
    to be a ubiquitous solution to all optimization problems. This is for the following
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: If the optimization function is a well-behaved convex function, then gradient-based
    methods will give a faster convergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The large population of solutions that helps genetic algorithms cover the search
    space more extensively also results in slow convergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing a fitness function can be a daunting task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding genetic algorithms using Distributed Evolutionary Algorithms in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we understand how genetic algorithms work, let''s try solving some
    problems with them. They have been used to solve NP-hard problems such as the
    traveling salesman problem. To make the task of generating a population, performing
    the crossover, and performing mutation operations easy, we will make use of **Distributed
    Evolutionary Algorithms in Python** (**DEAP**). It supports multiprocessing and
    we can use it for other evolutionary algorithms as well. You can download DEAP
    directly from PyPi using this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It is compatible with Python 3.
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about DEAP, you can refer to its GitHub repository ([https://github.com/DEAP/deap](https://github.com/DEAP/deap))
    and its user's guide ([http://deap.readthedocs.io/en/master/](http://deap.readthedocs.io/en/master/)).
  prefs: []
  type: TYPE_NORMAL
- en: Guess the word
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this program, we use genetic algorithms to guess a word. The genetic algorithm
    will know the number of letters in the word and will guess those letters until
    it finds the right answer. We decide to represent the genes as a single alphanumeric
    character; strings of these characters thus constitute a chromosome. And our fitness
    function is the sum of the characters matching in the individual and the right
    word:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the first step, we import the modules we will need. We use the `string` module
    and the `random` module to generate random characters from (a—z*,* A—Z*,* and
    0—9). From the DEAP module, we use `creator`, `base`, and `tools`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In DEAP, we start with creating a class that inherits from the `deep.base` module.
    We need to tell it whether we are going to have a minimization or maximization
    of the function; this is done using the weights parameter. A value of `+1` means
    we are maximizing (for minimizing, we give the value `-1.0`). The following code
    line will create a class, `FitnessMax`, that will maximize the function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We also define an `Individual` class, which will inherit the class list, and
    tell the DEAP creator module to assign `FitnessMax` as its `fitness` attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, with the `Individual` class defined, we use the `toolbox` of DEAP defined
    in the base module. We will use it to create a population and define our gene
    pool. All the objects that we will need from now onward—an individual, the population,
    the functions, the operators, and the arguments—are stored in a container called `toolbox`. We
    can add or remove content to/from the `toolbox` container using the `register()` and `unregister()` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have defined how the gene pool will be created, we create an individual
    and then a population by repeatedly using the `Individual` class. We will pass
    the class to the toolbox responsible for creating a `N` parameter , telling it
    how many genes to produce:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the `fitness` function. Note the comma in the return statement. This
    is because the fitness function in DEAP is returned as a tuple to allow multi-objective `fitness` functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the fitness function to the container. Also, add the crossover operator,
    mutation operator, and parent selector operator. You can see that, for this, we
    are using the register function. In the first statement, we register the fitness
    function that we have defined, along with the additional arguments it will take.
    The next statement registers the crossover operation; it specifies that here we
    are using a two-point crossover (`cxTwoPoint`). Next, we register the mutation
    operator; we choose the `mutShuffleIndexes` option, which shuffles the attributes
    of the input individual with a probability `indpb=0.05`. And finally, we define
    how the parents are selected; here, we have defined the method of selection as
    tournament selection with a tournament size of `3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have all the ingredients, so we will write down the code of the genetic
    algorithm, which will perform the steps we mentioned earlier in a repetitive manner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you can see the result of this genetic algorithm. In seven generations,
    we reached the right word:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7e2a7324-5c11-46c4-9a42-61c0ef909cf3.png)'
  prefs: []
  type: TYPE_IMG
- en: DEAP has options to select various crossover tools, different mutation operators,
    and even how the tournament selection takes place. The complete list of all evolutionary
    tools offered by DEAP and their description is available at [http://deap.readthedocs.io/en/master/api/tools.html.](http://deap.readthedocs.io/en/master/api/tools.html#deap.tools.mutFlipBit)
  prefs: []
  type: TYPE_NORMAL
- en: Genetic algorithm for CNN architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 4](cb9d27c5-e98d-44b6-a947-691b0bc64766.xhtml), *Deep Learning for
    IoT,* we learned about different DL models, such as MLP, CNN, RNN, and so on.
    Now, we will see how we can use genetic algorithms with these DL models. Genetic
    algorithms can be used to find the optimized weights and biases, and people have
    tried them. But the most common use of genetic algorithms in DL models has been
    to find optimum hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use genetic algorithms to find the optimum CNN architecture. The solution
    here is based on the paper *Genetic CNN* by Lingxi Xie and Alan Yuille ([https://arxiv.org/abs/1703.01513](https://arxiv.org/abs/1703.01513)).
    The first step will be finding the right representation of the problem. The authors
    presented a binary string representation for the network architecture. The family
    of the network is encoded into fixed-length binary strings. The network is composed
    of *S* stages where the *s-th* stage *s*= *1*, *2*,....*S*, contains *K[s]* nodes
    denoted by ![](img/25dcaeb0-a94e-4f85-a30a-48c678a8912f.png), here *k[s]* = *1*, *2*,..., *K[s][.]* The
    nodes in each stage are ordered and for proper representation they allow only
    connections from a lower-numbered node to a higher-numbered node. Each node represents
    a convolution layer operation, followed by batch normalization and ReLU activation.
    Each bit of the bit string represents the presence or absence of the connection
    between one convolution layer (node) and the other, the ordering of bits being
    as follows: the first bit represents the connection between (*v*[*s*,1], *v*[*s*,2]),
    the following two bits represent the connection between (*v*[*s*,1], *v*[*s*,3])
    and (*v*[*s*,2], *v*[*s*,3]), the following three bits will be (*v*[*s*,1], *v*[*s*,3]), (*v*[*s*,1], *v*[*s*,4]),
    and (*v*[*s*,2], *v*[*s*,4]), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand it better, let''s consider a two-stage network (each stage will
    have the same number of filters and filter size). Stage *S*[1] let''s say consists
    of four nodes (that is *K[s]* = 4), thus the total number of bits required to
    encode it is (*4×3×½ =*) 6\. The number of convolutional filters in stage *1* is
    3*2;* also we ensure that convolutional operation does not change the spatial
    dimensions of the image (for example, padding is the same). The following diagram
    shows the respective bit string encoded and corresponding convolution layer connections.
    The connections in red are default connections and are not encoded in the bit
    string. The first bit encodes the connection between (*a1*, *a2*), the next two
    bits encode the connection between (*a1*, *a3*) and (*a2*, *a3*), and the last
    three bits encode the connection between (*a1*, *a4*), (*a2*, *a4*), and (*a3*, *a4*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c83c821a-18a1-4f98-aab2-18f165068aee.png)'
  prefs: []
  type: TYPE_IMG
- en: Bit string encoded and corresponding convolution layer connections
  prefs: []
  type: TYPE_NORMAL
- en: Stage *1* takes a **32 × 32 × 3** input; all the convolution nodes in this stage
    have 32 filters. The red connections are default connections not encoded in the
    bit string. The green connection represents the connections according to the encoded
    bit string 1-00-111\. The output of stage *1* goes to the pooling layer and reduces
    by half in the spatial dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Stage 2 has five nodes, and thus will need (5×4×½ =) 10 bits. It will take the
    input from stage *1* with dimensions **16 × 16 × 32**. Now, if we take the number
    of convolution filters in stage *2* as *64*, then after the pooling the output
    would be 8 × 8 × 64.
  prefs: []
  type: TYPE_NORMAL
- en: The code presented here is taken from [https://github.com/aqibsaeed/Genetic-CNN](https://github.com/aqibsaeed/Genetic-CNN).
    Since we need to represent a graph structure, the network is built using **directed
    acyclic graph** (**DAG**). To represent DAG, we define a class, DAG, in which
    we define methods for adding a new node, deleting an existing node, adding an
    edge (connection) between two nodes, and deleting an edge between two nodes. Besides
    these, the methods are defined to find a node predecessor, the nodes it is connected
    to, and a list of leaves of the graph. The complete code is in `dag.py`, which
    you can access from the GitHub link.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main code is given in the `Genetic_CNN.ipynb` Jupyter Notebook. We use
    the DEAP library to run the genetic algorithm, and TensorFlow to construct a CNN
    from the graph constructed by the genetic algorithm. The fitness function is the
    accuracy. The code is built to find the CNN that will give the highest accuracy
    on the MNIST dataset (the handwritten digits, which we used in [Chapter 4](cb9d27c5-e98d-44b6-a947-691b0bc64766.xhtml), *Deep
    Learning for IoT*; here, we access them directly from the TensorFlow library):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to import the modules. Here, we will need DEAP and TensorFlow,
    and we also will import the DAG class we created in `dag.py` and the standard
    Numpy and Random modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We read the data directly from the TensorFlow examples library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we build the bit data structure that will hold the network information.
    The network we are designing is a three-stage network, with three nodes in stage
    1 (3 bits), four nodes in stage 2 (6 bits), and five nodes in stage 3 (10 bits).
    Thus, one Individual will be represented by a binary string of *3 + 6 + 10 = 19* bits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the part where we build the graph according to the encoded bit string.
    This will help build the population for the genetic algorithm. First, we define
    the functions we will need to build a CNN (`weight_variable`: creates the weight
    variable for a convolutional node; `bias_variable`: creates the bias variable for
    a convolutional node; `apply_convolution`: the function that performs the convolution
    operation; `apply_pool`: the function that will perform the pooling operation
    after each stage; and finally the last fully connected layer using the `linear_layer` function):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build the network based on the encoded bit string. So, we generate
    the DAG using the `generate_dag` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph generated is used to build the TensorFlow graph using the `generate_tensorflow_graph` function.
    This function makes use of the `add_node` function to add a convolution layer,
    and the `sum_tensors` function to combine the input of more than one convolution
    layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The fitness function evaluates the accuracy of the generated CNN architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now we are ready to implement the genetic algorithm: our fitness function
    will be a max function (`weights=(1.0,)`), we initialize the binary string using
    Bernoulli''s distribution (`bernoulli.rvs`), the individuals are created of length `L=
    19`, and the population is generated with each population consisting of `20` individuals.
    This time, we chose an ordered crossover, where a substring is selected from the
    first parent and copied into the offspring in the same location; the remaining
    positions are filled from the second parent, ensuring the nodes in the sub-string
    are not repeated. We kept the same mutation operator as before, `mutShuffleIndexes`;
    the tournament selection method is `selRoulette`, which makes the selection using the roulette
    wheel selection method (we choose `k` individuals and from them select the ones
    with the highest fitness). This time, instead of coding the genetic algorithm,
    we make use of the DEAP eaSimple algorithm, which is the basic genetic algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm will take some time; on i7 with NVIDIA 1070 GTX GPU it took about
    1.5 hours. The best three solutions are the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/df1133c2-6bcc-49c0-9f8e-cb070b515cd3.png)'
  prefs: []
  type: TYPE_IMG
- en: Genetic algorithm for LSTM optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a genetic CNN, we use genetic algorithms to estimate the optimum CNN architecture;
    in genetic RNN, we will now use a genetic algorithm to find the optimum hyperparameters
    of the RNN, the window size, and the number of hidden units. We will find the
    parameters that reduce the **root-mean-square error** (**RMSE**) of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The hyperparameters window size and number of units are again encoded in a binary
    string with 6 bits for window size and 4 bits for the number of units. Thus, the
    complete encoded chromosome will be of 10 bits. The LSTM is implemented using
    Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code we implement is taken from [https://github.com/aqibsaeed/Genetic-Algorithm-RNN](https://github.com/aqibsaeed/Genetic-Algorithm-RNN):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The necessary modules are imported. This time, we are using Keras to implement the LSTM
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset we need for LSTM has to be time series data; we use the wind-power
    forecasting data from Kaggle ([https://www.kaggle.com/c/GEF2012-wind-forecasting/data](https://www.kaggle.com/c/GEF2012-wind-forecasting/data)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to prepare the dataset depending upon the chosen `window_size`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train_evaluate` function creates the LSTM network for a given individual
    and returns its RMSE value (fitness function):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we use DEAP tools to define Individual (again, since the chromosome is
    represented by a binary encoded string (10 bits), we use Bernoulli''s distribution),
    create the population, use ordered crossover, use mutShuffleIndexes mutation,
    and use the roulette wheel selection for selecting the parents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the best solution, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, we implement the best LSTM solution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Yay! Now, you have the best LSTM network for predicting wind power.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter introduced an interesting nature-inspired algorithm family: genetic
    algorithms. We covered various standard optimization algorithms, varying from
    deterministic models, to gradient-based algorithms, to evolutionary algorithms.
    The biological process of evolution through natural selection was covered. We
    then learned how to convert our optimization problems into a form suitable for
    genetic algorithms. Crossover and mutation, two very crucial operations in genetic
    algorithms, were explained. While it is not possible to extensively cover all
    the crossover and mutation methods, we did learn about the popular ones.'
  prefs: []
  type: TYPE_NORMAL
- en: We applied what we learned on three very different optimization problems. We
    used it to guess a word. The example was of a five-letter word; had we used simple
    brute force, it would take a search of a *61⁵* search space. We used genetic algorithms
    to optimize the CNN architecture; again note that, with *19* possible bits, the
    search space is *2^(19)*. Then, we used it to find the optimum hyperparameters
    for an LSTM network.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will talk about another intriguing learning paradigm:
    reinforcement learning. This is another natural learning paradigm, in the sense
    that in nature we normally do not have supervised learning; rather, we learn through
    our interactions with the environment. In the same manner, here the agent is not
    told anything except the rewards and punishments it receives from the environment
    after its action.'
  prefs: []
  type: TYPE_NORMAL
