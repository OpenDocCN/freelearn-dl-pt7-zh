<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Cats Versus Dogs - Image Classification Using CNNs</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we will use <strong class="calibre4">convolutional neural networks</strong> (<strong class="calibre4">CNNs</strong>) to create a classifier that can predict whether a given image contains a cat or a dog.</p>
<p class="calibre2">This project marks the first in a series of projects where we will use neural networks for image recognition and computer vision problems. As we shall see, neural networks have proven to be an extremely effective tool for solving problems in computer vision.</p>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre11">
<li class="calibre12">Motivation for the problem that we're trying to tackle: image recognition</li>
<li class="calibre12">Neural networks and deep learning for computer vision</li>
<li class="calibre12">Understanding convolution and max pooling</li>
<li class="calibre12">Architecture of CNNs</li>
<li class="calibre12">Training CNNs in Keras</li>
<li class="calibre12">Using transfer learning to leverage on a state-of-the art neural network</li>
<li class="calibre12">Analysis of our results</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="calibre2">The key Python libraries required for this chapter are:</p>
<ul class="calibre11">
<li class="calibre12">matplotlib 3.0.2</li>
<li class="calibre12">Keras 2.2.4</li>
<li class="calibre12">Numpy 1.15.2</li>
<li class="calibre12">Piexif 1.1.2</li>
</ul>
<div class="packtinfobox">To download the dataset required for this project, please refer to the directions at <a href="https://github.com/PacktPublishing/Neural-Network-Projects-with-Python/blob/master/chapter4/how_to_download_the_dataset.txt" target="_blank" class="calibre20">https://github.com/PacktPublishing/Neural-Network-Projects-with-Python/blob/master/Chapter04/how_to_download_the_dataset.txt</a>.</div>
<p class="calibre2">The code for this chapter can be found in the GitHub repository for the book at <a href="https://github.com/PacktPublishing/Neural-Network-Projects-with-Python" target="_blank" class="calibre10">https://github.com/PacktPublishing/Neural-Network-Projects-with-Python</a>.</p>
<p class="calibre2">To download the code into your computer, you may run the following <kbd class="calibre13">git clone</kbd> command:</p>
<pre class="calibre17"><strong class="calibre1">$ git clone https://github.com/PacktPublishing/Neural-Network-Projects-with-Python.git</strong></pre>
<p class="calibre2">After the process is complete, there will be a folder titled <kbd class="calibre13">Neural-Network-Projects-with-Python</kbd><span class="calibre5">. Enter the folder by running the following:</span></p>
<pre class="calibre17"><strong class="calibre1">$ cd Neural-Network-Projects-with-Python</strong></pre>
<p class="calibre2">To install the required Python libraries in a virtual environment, run the following command:</p>
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre17"><strong class="calibre1"><span>$ conda</span> <span>env</span> <span>create</span> <span>-</span><span>f</span> <span>environment</span><span>.</span><span>yml</span></strong></pre></div>
</div>
<p class="calibre2">Note that you should have installed Anaconda in your computer first, before running this command. To enter the virtual environment, run the following command:</p>
<pre class="calibre17"><strong class="calibre1">$ conda activate neural-network-projects-python</strong></pre>
<div class="packtinfobox"><span>Important<br class="title-page-name"/></span>This chapter requires an additional image processing library known as <strong class="calibre1"><kbd class="calibre19">Piexif</kbd></strong>.</div>
<p class="calibre2">To download <strong class="calibre4"><kbd class="calibre13">Piexif</kbd></strong>, please run the following command:</p>
<pre class="calibre17"><strong class="calibre1">$ pip install piexif</strong></pre>
<p class="calibre2">Navigate to the folder <kbd class="calibre13">Chapter04</kbd> by running the following command:</p>
<pre class="calibre17"><strong class="calibre1">$ cd Chapter04</strong></pre>
<p class="calibre2">The following files are located in the folder:</p>
<ul class="calibre11">
<li class="calibre12"><kbd class="calibre13">main_basic_cnn.py</kbd>: This is the main code for the basic CNN</li>
<li class="calibre12"><kbd class="calibre13">main_vgg16.py</kbd>: This is the main code for the VGG16 network</li>
<li class="calibre12"><kbd class="calibre13">utils.py</kbd><span>:</span> This file contains <span>auxiliary utility code that will help us in the implementation of our neural network</span></li>
<li class="calibre12"><kbd class="calibre13">visualize_dataset.py</kbd><span>:</span> This file contains the code for exploratory data analysis and data visualization</li>
<li class="calibre12"><kbd class="calibre13">image_augmentation.py</kbd>: This file contains sample code for image augmentation</li>
</ul>
<p class="calibre2">To run the code for the neural network, simply execute the <kbd class="calibre13">main_basic_cnn.py</kbd> <span class="calibre5">and <kbd class="calibre13">main_vgg16.py</kbd></span> files:</p>
<pre class="calibre17"><strong class="calibre1">$ python main_basic_cnn.py</strong><br class="title-page-name"/><strong class="calibre1">$ python main_vgg16.py</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computer vision and object recognition</h1>
                </header>
            
            <article>
                
<p class="calibre2">Computer vision is an engineering field where the objective is to create programs that can extract meaning from images. According to an urban legend, computer vision first started in the 1960s when Professor <span class="calibre5">Marvin Minsky from MIT assigned a summer project to a group of undergraduates, with the requirement that they should attach a camera to a computer and to have the computer describe everything that it sees. The project was expected to be completed in just one summer. Needless to say, it wasn't completed within that summer as computer vision is an extremely complex field that scientists are continuously working on even today.</span></p>
<p class="calibre2">Early progression in computer vision was modest. In the 1960s, researchers started by creating algorithms to detect shapes, lines, and edges in photographs. The following decades saw the evolution of computer vision into several subfields. Computer vision researchers worked on signal processing, image processing, computer photometry, object recognition, and so on.</p>
<p class="calibre2"/>
<p class="calibre2">Object recognition is perhaps one of the most ubiquitous applications in computer vision. Researchers had worked on object recognition for a long time. The challenge faced by early object recognition researchers was that the dynamic appearance of objects made it difficult to teach computers to recognize them. Early computer vision researchers focused on template matching for object recognition, but often faced difficulties due to variations in angle, lighting, and occlusions.</p>
<p class="calibre2">The field of object recognition has grown exponentially in recent years, propelled by the advancements in neural networks and deep learning. In 2012, Alex Krizhevsky et al. won the <strong class="calibre4">ImageNet Large Scale Visual Recognition Challenge</strong> (<strong class="calibre4">ILSVRC</strong>) by a significant margin over other contenders. The winning idea proposed by <span class="calibre5">Alex</span> <span class="calibre5">Krizhevsky et al. was to use a CNN (an architecture termed the AlexNet) for object recognition. AlexNet was a significant breakthrough for object recognition. Since then, neural networks have become the number one technique for object recognition and computer vision related tasks. In this project, you will create a CNN similar to AlexNet.</span></p>
<p class="calibre2">The breakthrough in object recognition also led to the rise of AI that we know today. Facebook uses facial recognition to automatically tag and classify photos of you and your friends. Security systems use facial recognition to detect intrusions and persons of interest. Self-driving cars use object recognition to detect pedestrians, traffic signs, and other road objects. In many ways, society is starting to view object recognition, computer vision, and AI as one entity, even though their roots are very much different.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of object recognition tasks</h1>
                </header>
            
            <article>
                
<p class="calibre2">It is important to understand the different kinds of object recognition tasks, as the <span class="calibre5">required</span> neural network architecture greatly depends on the task. Object recognition tasks can be broadly classified into three different types:</p>
<ul class="calibre11">
<li class="calibre12">Image classification</li>
<li class="calibre12">Object detection</li>
<li class="calibre12">Instance segmentation</li>
</ul>
<p class="calibre2">The following diagram depicts the difference between each task:</p>
<p class="mce-root"><img src="assets/ce20900f-6812-4db0-863f-02f28952210f.png" class="calibre28"/></p>
<p class="calibre2">In <strong class="calibre4">Image Classification</strong>, the input to the problem is an image and the required output is simply a prediction of the class that the image belongs to. This is analogous to our first project, where we constructed a classifier to predict whether a patient is at risk of diabetes. In image classification, the problem is applied on pixels as our input data (specifically, the intensity value of each pixel), instead of tabular data represented by pandas DataFrames. In this project, we will focus on image classification.</p>
<p class="calibre2">In <strong class="calibre4">Object Detection</strong>, the input to the problem is an image and the required output are bounding boxes surrounding the detected objects. You can think of this as a step up from the image classification task. The neural network can no longer assume that there is only one class present in the image, and must assume that the image contains multiple classes. The neural network must then identify the presence of each class in the image, and to draw a bounding box around each of them. As you can imagine, this task is not trivial and object detection was a really difficult problem before neural networks came about. Today, neural networks can perform object detection efficiently. In 2014, 2 years after AlexNet was first developed, <span class="calibre5">Girshick et al. showed that the results in image classification can be generalized to <strong class="calibre4">Object</strong> <strong class="calibre4">Detection</strong></span>. <span class="calibre5">The intuitive idea behind their approach is to propose multiple boxes where objects of interest may exist, and then to use a CNN to predict the most likely class inside each bounding box. This approach is known as Regions with CNN (R-CNN).</span></p>
<p class="calibre2">Lastly, in <strong class="calibre4">Instance Segmentation</strong>, the input to the problem is an image and the output are pixel groupings that correspond to each class. You can think of instance segmentation as a refinement of object <span class="calibre5">d</span>etection. Instance segmentation is especially useful and prevalent in technology today. The portrait mode function in many smartphone cameras relies on instance segmentation to separate objects in the foreground from the background, creating a nice depth of field (bokeh) effect. Instance segmentation is also crucial in self-driving cars, as the location of each object around the car must be identified with pinpoint precision. In 2017, an adaption of R-CNN, known as Mask R-CNN, was shown to be extremely effective at instance segmentation.</p>
<p class="calibre2">As we can see, recent advancements in object recognition are driven by CNNs. In this project, we will gain an in-depth understanding of CNNs, and we will train and create one from scratch in Keras.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Digital images as neural network input</h1>
                </header>
            
            <article>
                
<p class="calibre2">Recall that in previous chapters, we made the distinction that neural networks require numerical inputs. We saw how we can encode categorical features, such as day of week, into numerical features using one-hot encoding. How then do we use an image as input for our neural network? Well, the short answer is that all digital images are numerical in nature!</p>
<p class="calibre2">To see why this is so, consider a 28 x 28 image of a handwritten digit 3, as shown in the following screenshot. Let's assume for now that the image is in grayscale (black and white). If we look at the intensity of each pixel that makes up the image, we can see that certain pixels are totally white, while some pixels are gray and black. In a computer, white pixels are represented with the value <span class="calibre5">0</span> and black pixels are represented with a value of <span class="calibre5">255</span>. Everything else in between white and black (that is, shades of gray) has a value in between <span class="calibre5">0</span> and <span class="calibre5">255</span>. Therefore, digital images are essentially numerical data and neural networks are perfectly able to learn from them:</p>
<p class="mce-root"><img src="assets/74113975-2997-4039-88a7-b7556eaf7e58.png" class="calibre29"/></p>
<p class="calibre2">What about color images? Color images are simply images with three channels—red, green, and blue (commonly known as RGB). The pixel values in each channel then represent the red intensity, green intensity, and blue intensity. Another way to think about it is, that for a pure red image, the pixels value will be 255 in the red channel and 0 for the green and blue channels.</p>
<p class="calibre2">The following depicts a color image, and the separation of the color image into its RGB channels. Notice how a color image is stacked in a three-dimensional manner. In contrast, a grayscale image has only two dimensions:</p>
<p class="mce-root"><img src="assets/f72ca293-4530-4c09-8e83-9015b660530b.png" class="calibre30"/></p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building blocks of CNNs</h1>
                </header>
            
            <article>
                
<p class="calibre2">One of the challenges faced in image classification is that the appearance of objects is dynamic. Just as there are many different breeds of cats and dogs, there are an infinite number of ways cats and dogs can appear in images. This makes it difficult for rudimentary image classification techniques, as it is impossible to show an infinite number of photos of cats and dogs to a computer.</p>
<p class="calibre2">However, this really shouldn't be a problem at all. Humans don't require an infinite number of photos of cats and dogs to differentiate between the two. A toddler can easily differentiate cats and dogs once he has seen just a few of them. If we think about how humans approach image classification, we notice that humans tend to look for landmark features while trying to identify an object. For example, we know that cats tend to be smaller in size compared to dogs, cats tend to have pointy ears, and cats have a shorter snout compared to dogs. Instinctively, humans look for these features while classifying an image.</p>
<p class="calibre2">Can we then teach a computer to look for these features within the entire image? The answer is a resounding yes! and the key lies in <strong class="calibre4">convolution</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Filtering and convolution</h1>
                </header>
            
            <article>
                
<p class="calibre2">Before we can understand what convolution is, it is important to first understand filtering.</p>
<p class="calibre2">Suppose we have a 9 x 9 image as our input, and we need to classify the image as an X or an O. The following diagram illustrates some sample input images.</p>
<p class="calibre2">A perfectly drawn O is shown in the leftmost box in the following diagram, while the other two boxes show badly drawn Os:</p>
<p class="mce-root"><img src="assets/0e4ec0ce-4faf-4f96-9a19-3086ace1506a.png" class="calibre31"/></p>
<p class="calibre2">A perfectly drawn X is shown in the leftmost box, while the other two boxes show badly drawn Xs:</p>
<p class="mce-root"><img src="assets/10a3b27d-a4f2-4509-b602-e213a1b21405.png" class="calibre32"/></p>
<p class="calibre2">In either case, we cannot expect the figures to be drawn perfectly. This is no problem for human beings, as we can all differentiate between Os and Xs even for the badly drawn cases.</p>
<p class="calibre2">Let's think about what makes it easy for human beings to differentiate between the two. What are the characteristic features in the images that allows us to differentiate them easily? Well, we know that Os tend to have flat horizontal edges, while Xs tend to have diagonal lines.</p>
<p class="calibre2">The following diagram depicts one such characteristic <span class="calibre5">feature</span> for Os:</p>
<p class="mce-root"><img src="assets/69e1a3e5-4b5e-4e2d-bd9c-c8468ce9f09d.png" class="calibre33"/></p>
<p class="calibre2"/>
<p class="calibre2">And the following diagram depicts one such characteristic feature for Xs:</p>
<p class="mce-root"><img src="assets/6bf69568-bdd4-4a90-b2ff-cefb5cc8d109.png" class="calibre34"/></p>
<p class="calibre2">In this case, the characteristic feature (also known as the filter) is of size 3 × 3. The presence of the characteristic feature in an image gives us a big hint on the class of the image. For example, if an image contains an horizontal edge, the characteristic feature for O, then the image is probably an O.</p>
<p class="calibre2">How then do we search for the presence of the characteristic feature in an image? We can simply do a brute force search by taking the 3 x 3 filter, before sliding it through every single pixel in the image to look for a match.</p>
<p class="calibre2">Let's start from the top left-hand corner of the image. The mathematical function performed by the filter (known as filtering) is the element-wise multiplication of the sliding window with the filter. In the top left-hand corner, the output from the filter is <span class="calibre5">2</span> (notice that this is a perfect match since the window is identical to the filter).</p>
<p class="calibre2">The following diagram shows the filtering operation on the top left-hand corner of the image. Note that for simplicity, we assume that pixel intensity values are <strong class="calibre4">0</strong> or <strong class="calibre4">1</strong> (instead of 0-255 in real digital images):</p>
<p class="mce-root"><img src="assets/5a3148ca-59dd-4331-a23c-31a56289ba70.png" class="calibre35"/></p>
<p class="calibre2">Next, we slide the window toward the right to cover the next 3 x 3 section in the image. The following diagram shows the filtering operation on the next 3 x 3 section:</p>
<p class="mce-root"><img src="assets/d3e52908-0a61-4b38-9c31-afa013b52b51.png" class="calibre35"/></p>
<p class="calibre2">The process of sliding the window through the entire image and calculating the filtered value is known as <strong class="calibre4">convolution</strong>. The layer in the neural network that performs convolution is known as the convolutional layer. Essentially, convolution provides us with a map to the areas where the characteristic feature is found in each image. This ensures that our neural network is able to perform intelligent, dynamic object recognition just like a human being!</p>
<p class="calibre2">In the preceding example, we handcrafted the filter based on our own knowledge of Os and Xs. Note that, when we train a neural network, it will automatically learn the most appropriate filter to use. Recall that in previous chapters, the fully connected layer (dense layer) was used and the weights of the layers were tuned during training. Similarly, the weights of a convolutional layer will be tuned during training.</p>
<p class="calibre2">Lastly, note that there are two main hyperparameters in a convolutional layer:</p>
<ul class="calibre11">
<li class="calibre12"><strong class="calibre1">Number of filters</strong>: In the preceding example, we have used just one filter. We can increase the number of filters to find multiple characteristic features.</li>
<li class="calibre12"><strong class="calibre1">Filter size</strong>: In the preceding example, we have used a 3 x 3 filter size. We can tune the filter size to represent larger <span>characteristic</span> features.</li>
</ul>
<p class="calibre2">We will talk about these hyperparameters in further detail when we construct our neural network later on in the chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Max pooling</h1>
                </header>
            
            <article>
                
<p class="calibre2">In CNNs, it is common to place a max pooling layer immediately after a convolution layer. The objective of the max pooling layer is to reduce the number of weights after each convolution layer, thereby reducing model complexity and avoiding overfitting.</p>
<p class="calibre2">The max pooling layer does this simply by looking at each subset of the input passed to it, and throwing out all but the maximum value in the subset. Let's take a look at an example to see what this means. Assume that our input to the max pooling layer is a 4 x 4 tensor (a tensor is just an n-dimensional array, such as those output by a convolutional layer), and we are using a 2 x 2 max pooling layer. The following diagram illustrates the <strong class="calibre4">Max Pooling</strong> operation:</p>
<p class="mce-root"><img src="assets/84ff5c56-a00e-4df8-a0ac-bf443474f5c9.png" class="calibre36"/></p>
<p class="calibre2">As we can see from the preceding diagram, <strong class="calibre4">Max Pooling</strong> simply looks at each 2 x 2 region of the input, and discards all but the maximum value in that region (boxed up in the preceding diagram). This effectively halves the height and width of the original input, reducing the number of parameters before passing it to the next layer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic architecture of CNNs</h1>
                </header>
            
            <article>
                
<p class="calibre2">We have seen the basic building blocks of CNNs in the previous section. Now, we'll put these building blocks together and see what a complete CNN looks like.</p>
<p class="calibre2">CNNs are almost always stacked together in a block of convolution and pooling pattern. The activation function used for the convolution layer is usually ReLU, as discussed in the previous chapters.</p>
<p class="calibre2">The following diagram shows the first few layers in a typical CNN, made up of a series of <span class="calibre5">convolution and pooling layers:</span></p>
<p class="mce-root"><img src="assets/033458e4-2f17-4317-8f90-2b3f9d981189.png" class="calibre37"/></p>
<p class="calibre2">The final layers in a CNN will always be <strong class="calibre4">Fully Connected</strong> layers (dense layers) with a sigmoid or softmax activation function. Note that the sigmoid activation function is used for binary classification problems, whereas the softmax activation function is used for multiclass classification problems.</p>
<p class="calibre2">The <strong class="calibre4">Fully Connected</strong> layer is identical to those that we have seen in the first two chapters: <a href="1068b86b-d786-48ba-b91c-35d0ff569460.xhtml" target="_blank" class="calibre10">Chapter 1</a>, <em class="calibre8">Machine Learning and Neural Networks 101</em> and <a href="81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml" target="_blank" class="calibre10">Chapter 2</a>, <em class="calibre8">Diabetes Prediction with Multilayer Perceptrons</em>. At this point, you might be wondering what is the rationale for placing the fully connected layer at the end of a CNN? In CNNs, the early layers learn and extract the characteristic features of the data they are trying to predict. For example, we have seen how a convolutional layer learns the characteristic spatial features of Os and Xs. The convolutional layers then pass this information on to the fully connected layers, which then learn how to make accurate predictions, just like in an MLP.</p>
<p class="calibre2"/>
<p class="calibre2">Essentially, the early layers of a CNN are responsible for identifying the characteristic spatial features, and the fully connected layers at the end are responsible for making predictions. The implication of this is significant. Instead of handcrafting features (that is, day of week, distance, and so on) for the machine learning algorithm, as we did in the previous chapter, <a href="bf157365-e4d3-42ae-89f4-58c9047e6500.xhtml" target="_blank" class="calibre10">Chapter 3</a>, <em class="calibre8">Predicting Taxi Fares with Deep Feedforward Nets</em>, we're simply providing all the data to the CNN as it is. The CNN then automatically learns the best characteristic features to differentiate the classes. This is true AI!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A review of modern CNNs</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we've seen the basic architecture of CNNs, let's take a look at modern, state-of-the-art CNNs. We'll do a walk-through of the evolution of CNNs, and see how they have changed over the years. We'll not go into the technical and mathematical details behind the implementation. Instead, we'll provide an intuitive overview of some of the most important CNNs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LeNet (1998)</h1>
                </header>
            
            <article>
                
<p class="calibre2">The first CNN was developed by Yann LeCun in 1998, with the architecture known as LeNet. LeCun was the first to prove that CNNs were effective in image recognition, particularly in the domain of handwritten digits recognition. However, throughout the 2000s, few scientists managed to build on the work done by LeCun and there were few breakthroughs in CNNs (and AI in general).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AlexNet (2012)</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we mentioned earlier, AlexNet was developed by <span class="calibre5">Alex</span> <span class="calibre5">Krizhevsky et al. and it was used to win the ILSVRC in 2012. AlexNet was built on the same principles as LeNet, although AlexNet used a much deeper architecture. The overall number of trainable parameters in AlexNet is around 60 million, over 1,000 times more than LeNet.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">VGG16 (2014)</h1>
                </header>
            
            <article>
                
<p class="calibre2">VGG16 was developed by Oxford's <strong class="calibre4">Visual Geometry Group</strong> (<strong class="calibre4">VGG</strong>) and it was considered to be a very important neural network. VGG16 was one of the first CNNs to deviate from large filter sizes, instead using a convolution filter size of 3 x 3.</p>
<p class="calibre2">VGG16 finished second in the image recognition task in the <span class="calibre5">ILSVRC</span> <span class="calibre5">in 2014. A downside to VGG16 is that there are many more parameters to be trained, leading to a</span> <span class="calibre5">significant training time.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inception (2014)</h1>
                </header>
            
            <article>
                
<p class="calibre2">The Inception network was developed by researchers from Google and it won <span class="calibre5">the ILSVRC</span> <span class="calibre5">in 2014. The guiding principle for the Inception network was to provide highly accurate predictions</span> efficiently<span class="calibre5">. Google's interest was to create a CNN that could be trained and deployed in real time across their network of servers. To do that, the researchers developed something known as the Inception module, that vastly improved training time while maintaining its accuracy. In fact, in the 2014 ILSVRC, the Inception network managed to achieve a higher accuracy than VGG16, despite having far fewer parameters.</span></p>
<p class="calibre2">The Inception network has been continuously improved upon. At the time of writing, the latest Inception network is at its 4th version (commonly known as Inception-v4).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ResNet (2015)</h1>
                </header>
            
            <article>
                
<p class="calibre2">The <strong class="calibre4">residual neural network</strong> (<strong class="calibre4">ResNet</strong>) was introduced by Kaiming He et al. at the 2015 ILSVRC <span class="calibre5">(by now, you should notice that this competition is extremely important for neural networks and computer vision, and new state-of-the-art techniques are revealed during the annual competition).</span></p>
<p class="calibre2">The salient feature of ResNet was the residual block technique, which allowed the neural network to be deeper while keeping the number of parameters moderate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Where we stand today</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we have seen, CNNs have progressed and improved exponentially in the past few years. In fact, recent CNNs can outperform humans at certain image recognition tasks. The recurring theme in recent years is to use innovative techniques to improve model performance, while preserving the model complexity. Clearly, the speed of the neural network is just as important as the accuracy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The cats and dogs dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we understand the theory behind CNNs, let's dive into data exploration. The cats and dogs dataset is provided by Microsoft. The instructions for the downloading and setting up of the dataset can be found in the <em class="calibre8">Technical requirements</em> section of this chapter.</p>
<p class="calibre2">Let's plot the images to better understand the kind of data we're working with. To do that, we can simply run the following code:</p>
<pre class="calibre17">from matplotlib import pyplot as plt<br class="title-page-name"/>import os<br class="title-page-name"/>import random<br class="title-page-name"/><br class="title-page-name"/># Get list of file names<br class="title-page-name"/>_, _, cat_images = next(os.walk('Dataset/PetImages/Cat'))<br class="title-page-name"/><br class="title-page-name"/># Prepare a 3x3 plot (total of 9 images)<br class="title-page-name"/>fig, ax = plt.subplots(3,3, figsize=(20,10))<br class="title-page-name"/><br class="title-page-name"/># Randomly select and plot an image<br class="title-page-name"/>for idx, img in enumerate(random.sample(cat_images, 9)):<br class="title-page-name"/>    img_read = plt.imread('Dataset/PetImages/Cat/'+img)<br class="title-page-name"/>    ax[int(idx/3), idx%3].imshow(img_read)<br class="title-page-name"/>    ax[int(idx/3), idx%3].axis('off')<br class="title-page-name"/>    ax[int(idx/3), idx%3].set_title('Cat/'+img)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">We'll see the following output:</p>
<p class="mce-root"><img src="assets/7fb5749e-b73c-4362-ae4f-32c2e59ad805.png" class="calibre38"/></p>
<p class="calibre2">We can make some observations about our data:</p>
<ul class="calibre11">
<li class="calibre12">The images have different dimensions.</li>
<li class="calibre12">The subjects (cat/dog) are mostly centered in the image.</li>
<li class="calibre12">The subjects (cat/dog) have different orientations, and they may be occluded in the image. In other words, there's no guarantee that we'll always see the tail of the cat in the image.</li>
</ul>
<p class="calibre2">Now, let's do the same for the dog images:</p>
<pre class="calibre17"># Get list of file names<br class="title-page-name"/>_, _, dog_images = next(os.walk('Dataset/PetImages/Dog'))<br class="title-page-name"/><br class="title-page-name"/># Prepare a 3x3 plot (total of 9 images)<br class="title-page-name"/>fig, ax = plt.subplots(3,3, figsize=(20,10))<br class="title-page-name"/><br class="title-page-name"/># Randomly select and plot an image<br class="title-page-name"/>for idx, img in enumerate(random.sample(dog_images, 9)):<br class="title-page-name"/>    img_read = plt.imread('Dataset/PetImages/Dog/'+img)<br class="title-page-name"/>    ax[int(idx/3), idx%3].imshow(img_read)<br class="title-page-name"/>    ax[int(idx/3), idx%3].axis('off')<br class="title-page-name"/>    ax[int(idx/3), idx%3].set_title('Dog/'+img)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2"/>
<p class="calibre2">We'll see the following output:</p>
<p class="mce-root"><img src="assets/6506c687-6339-4c5a-ac97-2cb2ba0e5cea.png" class="calibre39"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Managing image data for Keras</h1>
                </header>
            
            <article>
                
<p class="calibre2">One common problem encountered in neural network projects for image classification is that most computers do not have sufficient RAM to load the entire set of data into memory. Even for relatively modern and powerful computers, it would be far too slow to load the entire set of images into memory and to train a CNN from there.</p>
<p class="calibre2">To alleviate this problem, Keras provides a useful <kbd class="calibre13">flow_from_directory</kbd> method that takes as an input the path to the images, and generates batches of data as output. The batches of data are loaded into memory, as required before model training. This way, we can train a deep neural network on a huge number of images without worrying about memory issues. Furthermore, the <kbd class="calibre13">flow_from_directory</kbd> method allows us to perform image preprocessing steps such as resizing and other image augmentation techniques by simply passing an argument. The <kbd class="calibre13">flow_from_directory</kbd> method would then perform the necessary image preprocessing steps in real time before passing the data for model training.</p>
<p class="calibre2">To do all these, there are certain schemas for file and folder management that we must abide by, in order for <kbd class="calibre13">flow_from_directory</kbd> to work. In particular, we are required to create subdirectories for training and testing data, and within the training and testing subdirectories, we need to further create one subdirectory per class. The following diagram illustrates the required folder structure:</p>
<p class="mce-root"><img src="assets/75e11cdf-ef7e-43d4-a7b7-015930412358.png" class="calibre40"/></p>
<p class="calibre2">The <kbd class="calibre13">flow_from_directory</kbd> method would then infer the class of the images from the folder structure.</p>
<p class="calibre2">The raw data is provided in a <kbd class="calibre13">Cat</kbd> and <kbd class="calibre13">Dog</kbd> folder, without separation of training and testing data. Therefore, we need to split the data into a <kbd class="calibre13">Train</kbd> and <kbd class="calibre13">Test</kbd> folder as per the preceding schema. To do that, we need to perform the following steps:</p>
<ol class="calibre14">
<li class="calibre12">Create <kbd class="calibre13">/Train/Cat</kbd>, <span><kbd class="calibre13">/Train/Dog</kbd>, <kbd class="calibre13">/Test/Cat</kbd>, and <kbd class="calibre13">/Test/Dog</kbd> folders.</span></li>
<li class="calibre12">Randomly assign 80% of the the images as train images and 20% of the images as test images.</li>
<li class="calibre12">Copy those images into the respective folders.</li>
</ol>
<p class="calibre2"/>
<p class="calibre2">We have provided a helper function in <kbd class="calibre13">utils.py</kbd> to do these steps. We simply need to invoke the function, as follows:</p>
<pre class="calibre17">from utils import train_test_split<br class="title-page-name"/><br class="title-page-name"/>src_folder = 'Dataset/PetImages/'<br class="title-page-name"/>train_test_split(src_folder)</pre>
<div class="packtinfobox">If you run into an error while executing this code block, with the error message <span>ImportError: No Module Named Piexif</span>, it means that you have not installed Piexif in your Python virtual environment. This chapter requires an additional library for image processing. To download Piexif, please follow the instructions in the <em class="calibre18">Technical requirements section</em> at the start of this chapter.</div>
<p class="calibre2">Great! Our images are now placed in the appropriate folders for Keras.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image augmentation</h1>
                </header>
            
            <article>
                
<p class="calibre2">Before we start building our CNN, let's take a look at image augmentation, which is an important technique in image classification projects. Image augmentation is the creation of additional training data by making minor alterations to images in certain ways in order to create new images. For example, we can do the following:</p>
<ul class="calibre11">
<li class="calibre12">Image rotation</li>
<li class="calibre12">Image translation</li>
<li class="calibre12">Horizontal flip</li>
<li class="calibre12">Zooming into the image</li>
</ul>
<p class="calibre2">The motivation for image augmentation is that CNNs require a huge amount of training data before they can generalize well. However, it is often difficult to collect data, more so for images. With image augmentation, we can artificially create new training data based on the existing images.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">As always, Keras provides a handy <kbd class="calibre13">ImageDataGenerator</kbd> class to help us easily perform image augmentation. Let's create a new instance of the class:</p>
<pre class="calibre17">from keras.preprocessing.image import ImageDataGenerator<br class="title-page-name"/><br class="title-page-name"/>image_generator = ImageDataGenerator(rotation_range = 30,<br class="title-page-name"/>                                     width_shift_range = 0.2,<br class="title-page-name"/>                                     height_shift_range = 0.2,<br class="title-page-name"/>                                     zoom_range = 0.2,<br class="title-page-name"/>                                     horizontal_flip=True,<br class="title-page-name"/>                                     fill_mode='nearest')</pre>
<p class="calibre2">As we can see from this code snippet, there are several arguments that we can provide to the <kbd class="calibre13">ImageDataGenerator</kbd> class. Each of the arguments control how much of a modification is done to the existing image. We should avoid extreme transformations, as those extremely distorted images do not represent images from the real world and may introduce noise into our model.</p>
<p class="calibre2">Next, let's use it to augment a randomly selected image from the <kbd class="calibre13">/Train/Dog/</kbd> folder. Then, we can plot it to compare the augmented images with the original image. We can do this by running the following code:</p>
<pre class="calibre17">fig, ax = plt.subplots(2,3, figsize=(20,10))<br class="title-page-name"/>all_images = []<br class="title-page-name"/><br class="title-page-name"/>_, _, dog_images = next(os.walk('Dataset/PetImages/Train/Dog/'))<br class="title-page-name"/>random_img = random.sample(dog_images, 1)[0]<br class="title-page-name"/>random_img = plt.imread('Dataset/PetImages/Train/Dog/'+random_img)<br class="title-page-name"/>all_images.append(random_img)<br class="title-page-name"/><br class="title-page-name"/>random_img = random_img.reshape((1,) + random_img.shape)<br class="title-page-name"/>sample_augmented_images = image_generator.flow(random_img)<br class="title-page-name"/><br class="title-page-name"/>for _ in range(5):<br class="title-page-name"/>    augmented_imgs = sample_augmented_images.next()<br class="title-page-name"/>    for img in augmented_imgs:<br class="title-page-name"/>        all_images.append(img.astype('uint8'))<br class="title-page-name"/><br class="title-page-name"/>for idx, img in enumerate(all_images):<br class="title-page-name"/>    ax[int(idx/3), idx%3].imshow(img)<br class="title-page-name"/>    ax[int(idx/3), idx%3].axis('off')<br class="title-page-name"/>    if idx == 0:<br class="title-page-name"/>        ax[int(idx/3), idx%3].set_title('Original Image')<br class="title-page-name"/>    else:<br class="title-page-name"/>        ax[int(idx/3), idx%3].set_title('Augmented Image {}'.format(idx))<br class="title-page-name"/><br class="title-page-name"/>plt.show()</pre>
<p class="calibre2"/>
<p class="calibre2">We'll see the following output:</p>
<p class="mce-root"><img class="alignnone67" src="assets/a1821f8c-9640-4c89-bfba-1d055c090322.png"/></p>
<p class="calibre2">As we can see, each augmented image is randomly shifted or rotated by a certain amount as controlled by the arguments passed into the <kbd class="calibre13">ImageDataGenerator</kbd> class. These augmented images will provide supplemental training data for our CNN, increasing the robustness of our model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model building</h1>
                </header>
            
            <article>
                
<p class="calibre2">We're finally ready to start building our CNN in Keras. In this section, we'll take two different approaches to model building. First, we'll start by building a relatively simple CNN consisting of a few layers. We'll take a look at the performance of the simple model, and discuss its pros and cons. Next, we'll use a model that was considered state-of-the art just a few years ago—the VGG16 model. We'll see how we can leverage on the pre-trained weights to adapt the VGG16 model for cats versus dogs image classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a simple CNN</h1>
                </header>
            
            <article>
                
<p class="calibre2">In an earlier section, we showed how the fundamental building blocks of a CNN consist of a series of convolutional and pooling layers. In this section, we're going to build a basic CNN consisting of this repeating pattern, as shown in the following diagram:</p>
<p class="mce-root"><img src="assets/c38754ca-f2ea-425a-b7a6-1fe0f2f5074e.png" class="calibre41"/></p>
<p class="calibre2">This basic CNN consists of two repeated blocks of <strong class="calibre4">Convolution</strong> and <strong class="calibre4">Max</strong> <strong class="calibre4">Pooling</strong>, following by two <strong class="calibre4">Fully Connected</strong> layers. As discussed in a previous section, the convolution and max pooling layers are responsible for learning the spatial characteristics of the classes (for example, identifying the ears of cats), whereas the <strong class="calibre4">Fully Connected</strong> layers learn to make predictions using these spatial <span class="calibre5">characteristics</span>. We can thus represent the architecture of our basic CNN in another manner (we shall see why it is useful to visualize our neural network in this manner in the next subsection):</p>
<p class="mce-root"><img src="assets/79d1c8a6-e093-45eb-b873-5c137649b1af.png" class="calibre42"/></p>
<p class="calibre2"/>
<p class="calibre2">Building a CNN is similar to building an MLP or a feedforward neural network, as we've done in the previous chapters. We'll start off by declaring a new <kbd class="calibre13">Sequential</kbd> model instance:</p>
<pre class="calibre17">from keras.models import Sequential<br class="title-page-name"/>from keras.layers import Conv2D, MaxPooling2D<br class="title-page-name"/>from keras.layers import Dropout, Flatten, Dense<br class="title-page-name"/>from keras.preprocessing.image import ImageDataGenerator<br class="title-page-name"/><br class="title-page-name"/>model = Sequential()</pre>
<p class="calibre2">Before we add any convolutional layers, it is useful to think about the hyperparameters that we are going to use. For a CNN, there are several hyperparameters:</p>
<ul class="calibre11">
<li class="calibre12"><strong class="calibre1">Convolutional layer filter size</strong>: Most modern CNNs use a small filter size of <kbd class="calibre13">3</kbd> x <kbd class="calibre13">3</kbd>.</li>
<li class="calibre12"><strong class="calibre1">Number of filters</strong>: Let's use a filter number of <kbd class="calibre13">32</kbd>. This is a good balance between speed and performance.</li>
<li class="calibre12"><strong class="calibre1">Input size</strong>: As we've seen in an earlier section, the input images have different sizes, with their width and height approximately 150 px. Let's use an input size of <kbd class="calibre13">32</kbd> x <kbd class="calibre13">32</kbd> pixels. This compresses the original image, which can result in some information loss, but helps to speed up the training of our neural network.</li>
<li class="calibre12"><strong class="calibre1">Max pooling size</strong>: A common max pooling size is <kbd class="calibre13">2</kbd> x <kbd class="calibre13">2</kbd>. This will halve the input layer dimensions.</li>
<li class="calibre12"><strong class="calibre1">Batch size</strong>: This corresponds to the number of training samples to use in each mini batch during gradient descent. A large batch size results in more accurate training but longer training time and memory usage. Let's use a batch size of <kbd class="calibre13">16</kbd>.</li>
<li class="calibre12"><strong class="calibre1">Steps per epoch</strong>: This is the number of iterations in each training epoch. Typically, this is equal to the number of training samples divided by the batch size.</li>
<li class="calibre12"><strong class="calibre1">Epochs</strong>: The number of epochs to train our data. Note that, in neural networks, the number of epochs refers to the number of times the model sees each training sample during training. Multiple epochs are usually needed, as gradient descent is an iterative optimization method. Let's train our model for <kbd class="calibre13">10</kbd> epochs. This means that each training sample will be passed to the the model 10 times during training.</li>
</ul>
<p class="calibre2"/>
<p class="calibre2">Let's declare variables for these hyperparameters so that they are constant throughout our code:</p>
<pre class="calibre17">FILTER_SIZE = 3<br class="title-page-name"/>NUM_FILTERS = 32<br class="title-page-name"/>INPUT_SIZE  = 32<br class="title-page-name"/>MAXPOOL_SIZE = 2<br class="title-page-name"/>BATCH_SIZE = 16<br class="title-page-name"/>STEPS_PER_EPOCH = 20000//BATCH_SIZE<br class="title-page-name"/>EPOCHS = 10</pre>
<p class="calibre2">We can now add the first convolutional layer, with <kbd class="calibre13">32</kbd> filters, each of size (<kbd class="calibre13">3</kbd> x <kbd class="calibre13">3</kbd>):</p>
<pre class="calibre17">model.add(Conv2D(NUM_FILTERS, (FILTER_SIZE, FILTER_SIZE),<br class="title-page-name"/>                 input_shape = (INPUT_SIZE, INPUT_SIZE, 3),<br class="title-page-name"/>                 activation = 'relu'))</pre>
<p class="calibre2">Next, we add a max pooling layer:</p>
<pre class="calibre17">model.add(MaxPooling2D(pool_size = (MAXPOOL_SIZE, MAXPOOL_SIZE)))</pre>
<p class="calibre2">This is the basic convolution-pooling pattern of our CNN. Let's repeat this once more according to our model architecture:</p>
<pre class="calibre17">model.add(Conv2D(NUM_FILTERS, (FILTER_SIZE, FILTER_SIZE),<br class="title-page-name"/>                 input_shape = (INPUT_SIZE, INPUT_SIZE, 3),<br class="title-page-name"/>                 activation = 'relu'))<br class="title-page-name"/><br class="title-page-name"/>model.add(MaxPooling2D(pool_size = (MAXPOOL_SIZE, MAXPOOL_SIZE)))</pre>
<p class="calibre2">We are now done with the convolution and pooling layers. Before we move on to the fully connected layers, we need to flatten its input. <kbd class="calibre13">Flatten</kbd> is a function in Keras that transforms a multidimensional vector into a single dimensional vector. For example, if the vector is of shape (5,5,3) before passing to <kbd class="calibre13">Flatten</kbd>, the output vector will be of shape (75) after passing to <kbd class="calibre13">Flatten</kbd>.</p>
<p class="calibre2">To add a <kbd class="calibre13">Flatten</kbd> layer, we simply run the following code:</p>
<pre class="calibre17">model.add(Flatten())</pre>
<p class="calibre2">We can now add a fully connected layer with <kbd class="calibre13">128</kbd> nodes:</p>
<pre class="calibre17">model.add(Dense(units = 128, activation = 'relu'))</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">Before we add our last fully connected layer, it is a good practice to add a dropout layer. The dropout layer randomly sets a certain fraction of its input to 0. This helps to reduce overfitting, by ensuring that the model does not place too much emphasis on certain weights:</p>
<pre class="calibre17"># Set 50% of the weights to 0 <br class="title-page-name"/>model.add(Dropout(0.5))</pre>
<p class="calibre2"><span class="calibre5">We add one last fully connected layer to our model:</span></p>
<pre class="calibre17">model.add(Dense(units = 1, activation = 'sigmoid'))</pre>
<div class="packtinfobox">Note that the last fully connected layer should have only one node, as we're doing binary classification (cat or dog) in this project.</div>
<p class="calibre2">We'll compile our model using the <kbd class="calibre13">adam</kbd> optimizer. The <kbd class="calibre13">adam</kbd> optimizer is a generalization of the <strong class="calibre4">stochastic gradient descent</strong> (<strong class="calibre4">SGD</strong>) algorithm that we've seen in <a href="1068b86b-d786-48ba-b91c-35d0ff569460.xhtml" target="_blank" class="calibre10">Chapter 1</a>, <em class="calibre8">Machine Learning and Neural Networks 101</em> and it is widely used to train CNNs. The loss function is <kbd class="calibre13">binary_crossentropy</kbd> since we're doing a binary classification:</p>
<pre class="calibre17">model.compile(optimizer = 'adam', loss = 'binary_crossentropy',<br class="title-page-name"/>              metrics = ['accuracy'])</pre>
<div class="packtinfobox">In general, we use <kbd class="calibre19">binary_crossentropy</kbd> for binary classification problems and <kbd class="calibre19">categorical_crossentropy</kbd> for multiclass classification problems.</div>
<p class="calibre2">We're now ready to train our CNN. Notice that we have not loaded any of the data into memory. We'll use the <kbd class="calibre13">ImageDataGenerator</kbd> and <kbd class="calibre13">flow_from_directory</kbd> method to train our model in real time, which loads batches of the dataset into memory only as required:</p>
<pre class="calibre17">training_data_generator = ImageDataGenerator(rescale = 1./255)<br class="title-page-name"/><br class="title-page-name"/>training_set = training_data_generator. \<br class="title-page-name"/>                   flow_from_directory('Dataset/PetImages/Train/',<br class="title-page-name"/>                                       target_size=(INPUT_SIZE,INPUT_SIZE), <br class="title-page-name"/>                                                    batch_size=BATCH_SIZE,<br class="title-page-name"/>                                                    class_mode='binary')<br class="title-page-name"/><br class="title-page-name"/>model.fit_generator(training_set, steps_per_epoch = STEPS_PER_EPOCH, <br class="title-page-name"/>                    epochs=EPOCHS, verbose=1)</pre>
<p class="calibre2"/>
<p class="calibre2">This will start the training and once it is complete, you will see the following output:</p>
<p class="mce-root"><img class="alignnone68" src="assets/0b1c40fb-6e06-4ea1-9067-5e87e701013c.png"/></p>
<p class="CDPAlignLeft1"><span class="calibre5">We can clearly see that the loss decreases while the accuracy increases with each epoch.</span></p>
<p class="calibre2">Now that our model is trained, let's evaluate it on the testing set. We'll create a new <kbd class="calibre13">ImageDataGenerator</kbd> and call <kbd class="calibre13">flow_from_directory</kbd> on the images in the <kbd class="calibre13">test</kbd> folder:</p>
<pre class="calibre17">testing_data_generator = ImageDataGenerator(rescale = 1./255)<br class="title-page-name"/><br class="title-page-name"/>test_set = testing_data_generator. \<br class="title-page-name"/>               flow_from_directory('Dataset/PetImages/Test/',<br class="title-page-name"/>                                   target_size=(INPUT_SIZE,INPUT_SIZE),<br class="title-page-name"/>                                   batch_size=BATCH_SIZE,<br class="title-page-name"/>                                   class_mode = 'binary')<br class="title-page-name"/>                                   <br class="title-page-name"/>score = model.evaluate_generator(test_set, steps=len(test_set))<br class="title-page-name"/>for idx, metric in enumerate(model.metrics_names):<br class="title-page-name"/>    print("{}: {}".format(metric, score[idx]))</pre>
<p class="calibre2">We'll get the following output:</p>
<p class="mce-root"><img src="assets/93b08d19-3f6c-493a-b10e-7aff0f2df243.png" class="calibre43"/></p>
<p class="calibre2">We obtained an accuracy of 80%! That's pretty impressive considering that we only used a basic CNN. This shows the power of <span class="calibre5">CNNs; we obtained an accuracy close to human performance from just a few lines of code.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Leveraging on pre-trained models using transfer learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">Can we take our model further? Can we achieve close to 90%, reaching human level performance? As we shall see in this section, we can obtain better performance by leveraging on transfer learning.</p>
<p class="calibre2">Transfer learning is a technique in machine learning where a model trained for a certain task is modified to make predictions for another task. For example, we may use a model trained to classify cars to classify trucks instead, since they are similar. In the context of CNN, transfer learning involves freezing the convolution-pooling layers, and only retraining the final fully connected layers. The following diagram illustrates this process:</p>
<p class="mce-root"><img src="assets/bc182395-a155-4b17-bdd9-796bd417fb1c.png" class="calibre44"/></p>
<p class="calibre2">How does transfer learning work? Intuitively, the purpose of the convolution and pooling layers is to learn the spatial characteristics of the classes. We can therefore reuse these layers since the spatial characteristics are similar in both tasks. We just need to retrain the final fully connected layers to re-purpose the neural network to make predictions for the new class. Naturally, a crucial requirement for transfer learning is that tasks A and B must be similar to one another.</p>
<p class="calibre2">In this section, we're going to re-purpose the VGG16 model to make predictions on images of cats and dogs. The VGG16 model was originally developed for the ILSVRC<span class="calibre5">, which required the model to make a 1,000 class multiclass classification. Among the 1,000 classes are specific breeds of cats and dogs. In other words, VGG16 knows how to recognize specific breeds of cats and dogs, and not just cats and dogs in general. It is therefore a viable approach to use transfer learning using the VGG16 model for our cats and dogs image classification problem.</span></p>
<p class="calibre2">The VGG16 model and its trained weights are provided directly in Keras. Let's create a new <kbd class="calibre13">VGG16</kbd> model, as shown in the following code:</p>
<pre class="calibre17">from keras.applications.vgg16 import VGG16<br class="title-page-name"/><br class="title-page-name"/>INPUT_SIZE = 128 # Change this to 48 if the code takes too long to run<br class="title-page-name"/>vgg16 = VGG16(include_top=False, weights='imagenet', <br class="title-page-name"/>              input_shape=(INPUT_SIZE,INPUT_SIZE,3))</pre>
<p class="calibre2">Note that we used <kbd class="calibre13">include_top=False</kbd> when we created a new VGG16 model. This argument tells Keras not to import the fully connected layers at the end of the VGG16 network.</p>
<p class="calibre2">We're now going to freeze the rest of the layers in the VGG16 model, since we're not going to retrain them from scratch. We can freeze the layers by running the following code snippet:</p>
<pre class="calibre17">for layer in vgg16.layers:<br class="title-page-name"/>    layer.trainable = False</pre>
<p class="calibre2">Next, we're going to add a fully connected layer with <kbd class="calibre13">1</kbd> node right at the end of the neural network. <span class="calibre5">The syntax to do this is slightly different, since the VGG16 model is not a Keras</span> <kbd class="calibre13">Sequential</kbd> <span class="calibre5">model that we're used to. In any case, we can add the layers by running the following code:</span></p>
<pre class="calibre17">from keras.models import Model<br class="title-page-name"/><br class="title-page-name"/>input_ = vgg16.input<br class="title-page-name"/>output_ = vgg16(input_)<br class="title-page-name"/>last_layer = Flatten(name='flatten')(output_)<br class="title-page-name"/>last_layer = Dense(1, activation='sigmoid')(last_layer)<br class="title-page-name"/>model = Model(input=input_, output=last_layer)</pre>
<p class="calibre2"/>
<p class="calibre2">This is just a manual way of adding layers in Keras, which the <kbd class="calibre13">.</kbd><kbd class="calibre13">add()</kbd> function in <kbd class="calibre13">Sequential</kbd> model has simplified for us so far. The rest of the code is similar to what we have seen in the previous section. We declare a training data generator, and we train the model (only the newly added layers) by calling <kbd class="calibre13">flow_from_directory()</kbd>. Since we only need to train the final layer, we'll just train the model for <kbd class="calibre13">3</kbd> epochs:</p>
<div class="packtinfobox"><span>Caution</span><br class="title-page-name"/>
The following code block takes around an hour to run if you are not running Keras on a GPU (graphics card). If the code takes too long to run on your computer, you may reduce the <kbd class="calibre19">INPUT_SIZE</kbd> parameter to speed up model training. However, note that this will lower the accuracy of your model.</div>
<pre class="calibre17"># Define hyperparameters<br class="title-page-name"/>BATCH_SIZE = 16<br class="title-page-name"/>STEPS_PER_EPOCH = 200<br class="title-page-name"/>EPOCHS = 3<br class="title-page-name"/><br class="title-page-name"/>model.compile(optimizer = 'adam', loss = 'binary_crossentropy',<br class="title-page-name"/>              metrics = ['accuracy'])<br class="title-page-name"/><br class="title-page-name"/>training_data_generator = ImageDataGenerator(rescale = 1./255)<br class="title-page-name"/>testing_data_generator = ImageDataGenerator(rescale = 1./255)<br class="title-page-name"/><br class="title-page-name"/>training_set = training_data_generator. \<br class="title-page-name"/>                   flow_from_directory('Dataset/PetImages/Train/',<br class="title-page-name"/>                                       target_size=(INPUT_SIZE,INPUT_SIZE),<br class="title-page-name"/>                                       batch_size = BATCH_SIZE,<br class="title-page-name"/>                                       class_mode = 'binary')<br class="title-page-name"/><br class="title-page-name"/>test_set = testing_data_generator. \<br class="title-page-name"/>               flow_from_directory('Dataset/PetImages/Test/',<br class="title-page-name"/>                                   target_size=(INPUT_SIZE,INPUT_SIZE),<br class="title-page-name"/>                                   batch_size = BATCH_SIZE,<br class="title-page-name"/>                                   class_mode = 'binary')<br class="title-page-name"/><br class="title-page-name"/>model.fit_generator(training_set, steps_per_epoch = STEPS_PER_EPOCH, <br class="title-page-name"/>                    epochs = EPOCHS, verbose=1)</pre>
<p class="calibre2">We'll get the following output:</p>
<p class="mce-root"><img class="alignnone69" src="assets/725f105b-08fe-45b7-b095-a9887fd3cf0e.png"/></p>
<p class="calibre2">The training accuracy doesn't look much different to the basic CNN in the previous section. This is expected, since both neural networks do really well in the training set. However, the testing accuracy is ultimately the metric which we will use to evaluate the performance of our model. Let's see how well it does on the testing set:</p>
<pre class="calibre17">score = model.evaluate_generator(test_set, len(test_set))<br class="title-page-name"/><br class="title-page-name"/>for idx, metric in enumerate(model.metrics_names):<br class="title-page-name"/>    print("{}: {}".format(metric, score[idx]))</pre>
<p class="calibre2">We'll see the following output:</p>
<p class="mce-root"><img src="assets/a0aabb03-fbb3-4ce4-a0b4-a43a08c6e82a.png" class="calibre45"/></p>
<p class="calibre2">That's amazing! By making use of transfer learning, we managed to obtain a testing accuracy of 90.5%. Note that the training time here is much shorter than training a VGG16 model from scratch (it would probably take days to train a VGG16 model from scratch, even with a powerful GPU!), since we are only training the last layer. This shows that we can leverage on a pre-trained state-of-the art model like VGG16 to make predictions for our own projects.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Results analysis</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's take a deeper look into our results. In particular, we would like to know what kind of images our CNN does well in, and what kind of images it gets wrong.</p>
<p class="calibre2">Recall that the output of the sigmoid activation function in the last layer of our CNN is a list of values between 0 and 1 (one value/prediction per image). If the output value is &lt; <kbd class="calibre13">0.5</kbd>, then the prediction is class 0 (that is, cat) and if the output value is &gt;= <kbd class="calibre13">0.5</kbd>, then the prediction is class 1 (that is, dog). Therefore, an output value close to <kbd class="calibre13">0.5</kbd> means that the model isn't so sure, while an output value very close to <kbd class="calibre13">0.0</kbd> or <kbd class="calibre13">1.0</kbd> means that the model is very sure about its predictions.</p>
<p class="calibre2"/>
<p class="calibre2">Let's run through the images in the testing set one by one, using our model to make predictions on the class of the image, and classify the images according to three categories:</p>
<ul class="calibre11">
<li class="calibre12"><strong class="calibre1">Strongly right predictions</strong>: The model predicted these images correctly, and the output value is <kbd class="calibre13">&gt; 0.8</kbd> or <kbd class="calibre13">&lt; 0.2</kbd></li>
<li class="calibre12"><strong class="calibre1">Strongly wrong predictions</strong>: <span>The model predicted these images wrongly, and the output value is <kbd class="calibre13">&gt; 0.8</kbd> or <kbd class="calibre13">&lt; 0.2</kbd></span></li>
<li class="calibre12"><strong class="calibre1">Weakly wrong predictions</strong>: <span>The model predicted these images wrongly, and the output value is between <kbd class="calibre13">0.4</kbd> and <kbd class="calibre13">0.6</kbd></span></li>
</ul>
<p class="calibre2">The following code snippet will do this for us:</p>
<pre class="calibre17"># Generate test set for data visualization<br class="title-page-name"/>test_set = testing_data_generator. \<br class="title-page-name"/>               flow_from_directory('Dataset/PetImages/Test/',<br class="title-page-name"/>                                    target_size = (INPUT_SIZE,INPUT_SIZE),<br class="title-page-name"/>                                    batch_size = 1,<br class="title-page-name"/>                                    class_mode = 'binary')<br class="title-page-name"/><br class="title-page-name"/>strongly_wrong_idx = []<br class="title-page-name"/>strongly_right_idx = []<br class="title-page-name"/>weakly_wrong_idx = []<br class="title-page-name"/><br class="title-page-name"/>for i in range(test_set.__len__()):<br class="title-page-name"/>    img = test_set.__getitem__(i)[0]<br class="title-page-name"/>    pred_prob = model.predict(img)[0][0]<br class="title-page-name"/>    pred_label = int(pred_prob &gt; 0.5)<br class="title-page-name"/>    actual_label = int(test_set.__getitem__(i)[1][0])<br class="title-page-name"/>    if pred_label != actual_label and (pred_prob &gt; 0.8 or <br class="title-page-name"/>        pred_prob &lt; 0.2): strongly_wrong_idx.append(i)<br class="title-page-name"/>    elif pred_label != actual_label and (pred_prob &gt; 0.4 and <br class="title-page-name"/>        pred_prob &lt; 0.6): weakly_wrong_idx.append(i)<br class="title-page-name"/>    elif pred_label == actual_label and (pred_prob &gt; 0.8 or<br class="title-page-name"/>        pred_prob &lt; 0.2): strongly_right_idx.append(i)<br class="title-page-name"/>    # stop once we have enough images to plot<br class="title-page-name"/>    if (len(strongly_wrong_idx)&gt;=9 and len(strongly_right_idx)&gt;=9 <br class="title-page-name"/>        and len(weakly_wrong_idx)&gt;=9): break</pre>
<p class="calibre2">Let's visualize the images from these three groups by randomly selecting <kbd class="calibre13">9</kbd> of the images in each group, and plot them on a 3 × 3 grid. The following helper function allows us to do that:</p>
<pre class="calibre17">from matplotlib import pyplot as plt<br class="title-page-name"/>import random<br class="title-page-name"/><br class="title-page-name"/>def plot_on_grid(test_set, idx_to_plot, img_size=INPUT_SIZE):<br class="title-page-name"/>    fig, ax = plt.subplots(3,3, figsize=(20,10))<br class="title-page-name"/>    for i, idx in enumerate(random.sample(idx_to_plot,9)):<br class="title-page-name"/>        img = test_set.__getitem__(idx)[0].reshape(img_size, img_size ,3)<br class="title-page-name"/>        ax[int(i/3), i%3].imshow(img)<br class="title-page-name"/>        ax[int(i/3), i%3].axis('off')</pre>
<p class="calibre2">We can now plot <kbd class="calibre13">9</kbd> randomly selected images from the strongly right predictions group:</p>
<pre class="calibre17">plot_on_grid(test_set, strongly_right_idx)</pre>
<p class="calibre2">We'll see the following output:</p>
<p class="mce-root"><img src="assets/6c6063b9-1286-45bc-aa15-7a2b8105ff50.png" class="calibre46"/></p>
<div class="packtfigref">Selected images that have strong predictions, and are correct</div>
<p class="calibre2">No surprises there! These are almost classical images of cats and dogs. Notice that the pointy ears of cats and the dark eyes of dogs can all be seen in the preceding images. These characteristic features allow our CNN to easily identify them.</p>
<p class="calibre2">Let's now take a look at the <span class="calibre5">strongly wrong predictions group:</span></p>
<pre class="calibre17">plot_on_grid(test_set, strongly_wrong_idx)</pre>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2">We'll get the following output:</p>
<p class="mce-root"><img src="assets/5c9e0e1f-4199-471b-8381-526474ead5ed.png" class="calibre47"/></p>
<div class="packtfigref">Selected images that have strong predictions, but are wrong</div>
<p class="calibre2">We notice a few commonalities among these strongly wrong predictions. The first thing we notice is that certain dogs do resemble cats with their pointy ears. Perhaps our neural network placed too much emphasis on the pointy ears and classified these dogs as cats. Another thing we notice is that some of the subjects were not facing the camera, making it really difficult to identify them. No wonder our neural network got them wrong.</p>
<p class="calibre2">Finally, let's take a look at <span class="calibre5">the</span> <span class="calibre5">weakly wrong predictions group:</span></p>
<pre class="calibre17">plot_on_grid(test_set, weakly_wrong_idx)</pre>
<p class="calibre2"/>
<p class="calibre2">We'll get the following output:</p>
<p class="mce-root"><img src="assets/7d65f062-df15-4c19-9741-77903ddf6738.png" class="calibre48"/></p>
<div class="packtfigref">Selected images that have weak predictions, and are wrong</div>
<p class="calibre2">These images are ones that our model is on the fence with. Perhaps there is an equal number of characteristics to suggest that the object could be a dog or a cat. This is perhaps the most obvious with the images in the first row, where the puppies in the first row have a small frame like a cat, which could have confused the neural network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we built a classifier that can predict whether an image contains a cat or a dog by using two different CNNs. We first went through the theory behind CNNs, and we understood that the fundamental building blocks of a CNN are the convolution, pooling, and fully connected layers. In particular, the front of the CNN consists of a block of convolution-pooling layers, repeated an arbitrary number of times. This block is responsible for identifying spatial characteristics in the images, which can be used to classify the images. The back of the CNN consists of fully connected layers, similar to an MLP. This block is responsible for making the final predictions.</p>
<p class="calibre2">In the first CNN, we used a basic architecture that achieved 80% accuracy on the testing set. This basic CNN consists of two convolutional-max pooling layers, followed by two fully connected layers. In the second CNN, we used transfer learning to leverage on the pre-trained VGG16 network for our classification. We removed the final fully connected layer with 1,000 nodes in the pre-trained network, and added our own fully connected layer with one node (for our binary classification task). We managed to obtain an accuracy of 90% using the fine-tuned VGG16 model.</p>
<p class="calibre2">Lastly, we visualized the images that our model did well in, as well as the images that our model struggled with. We saw that our model could not be certain when the subject is not facing the camera or when the subject has characteristics resembling both a cat and a dog (for example, a small puppy with pointy ears).</p>
<p class="calibre2">That concludes the chapter on using CNNs for image recognition. In the next chapter, <a href="16d0775b-23ec-456a-a57b-cba2e9da7570.xhtml" target="_blank" class="calibre10">Chapter 5</a>, <em class="calibre8">Removing Noise from Images Using Autoencoders</em>, we'll use an autoencoder neural network to remove noise from images.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol class="calibre14">
<li class="calibre12">How are images represented in computers?</li>
</ol>
<p class="calibre26">Images are represented in computers as a group of pixels, with each pixel having its own intensity (value between 0 and 255). Color images have three channels (red, green, and blue) while grayscale images have only one channel.</p>
<ol start="2" class="calibre14">
<li class="calibre12">What are the fundamental building blocks of a CNN?</li>
</ol>
<p class="calibre26">All convolutional neural network consists of convolution layers, pooling layers, and fully connected layers.</p>
<ol start="3" class="calibre14">
<li class="calibre12">What is the role of the convolutional and pooling layers?</li>
</ol>
<p class="calibre26">The convolutional and pooling layers are responsible for extracting spatial characteristics from the images. For example, when training a CNN to identify images of cats, one such spatial characteristic would be the pointy ears of cats.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<ol start="4" class="calibre14">
<li class="calibre12">What is the role of the fully connected layers?</li>
</ol>
<p class="calibre26">The fully connected layers are similar to the those in MLPs and feedforward neural networks. Their role is to use the spatial characteristics as input, and to output predicted classes.</p>
<ol start="5" class="calibre14">
<li class="calibre12">What is transfer learning, and how is it useful?</li>
</ol>
<p class="calibre26">Transfer learning is a technique in machine learning where a model trained for a certain task is modified to make predictions for another task. Transfer learning allows us to leverage on state-of-the art models, such as VGG16, for our own purposes, with minimal training time.</p>


            </article>

            
        </section>
    </body></html>