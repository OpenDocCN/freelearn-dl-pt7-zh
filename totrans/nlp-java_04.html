<html><head></head><body><div class="chapter" title="Chapter&#xA0;1.&#xA0;Simple Classifiers"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Simple Classifiers</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Deserializing and running a classifier</li><li class="listitem" style="list-style-type: disc">Getting confidence estimates from a classifier</li><li class="listitem" style="list-style-type: disc">Getting data from the Twitter API</li><li class="listitem" style="list-style-type: disc">Applying a classifier to a <code class="literal">.csv</code> file</li><li class="listitem" style="list-style-type: disc">Evaluation of classifiers – the confusion matrix</li><li class="listitem" style="list-style-type: disc">Training your own language model classifier</li><li class="listitem" style="list-style-type: disc">How to train and evaluate with cross validation</li><li class="listitem" style="list-style-type: disc">Viewing error categories – false positives</li><li class="listitem" style="list-style-type: disc">Understanding precision and recall</li><li class="listitem" style="list-style-type: disc">How to serialize a LingPipe object – classifier example</li><li class="listitem" style="list-style-type: disc">Eliminate near duplicates with the Jaccard distance</li><li class="listitem" style="list-style-type: disc">How to classify sentiment – simple version</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec08"/>Introduction</h1></div></div></div><p>This chapter introduces the LingPipe toolkit in the context of its competition and then dives straight into text classifiers. Text classifiers assign a category to text, for example, they assign the language to a sentence or tell us if a tweet is positive, negative, or neutral in sentiment. This chapter covers how to use, evaluate, and create text classifiers based on language models. These are the simplest machine learning-based classifiers in the LingPipe API. What makes them simple is that they operate over characters only—later, classifiers will have notions of words/tokens and even more. However, don't be fooled, character-language models are ideal for language identification, and they were the basis of some of the world's earliest commercial sentiment systems.</p><p>This chapter also covers crucial evaluation infrastructure—it turns out that almost everything we do turns out to be a classifier at some level of interpretation. So, do not skimp on the power of cross validation, definitions of precision/recall, and F-measure.</p><p>The best part is that you will learn how to programmatically access Twitter data to train up and evaluate your own classifiers. There is a boring bit concerning the mechanics of reading and writing LingPipe objects from/to disk, but other than that, this is a fun chapter. The goal of this chapter is to get you up and running quickly with the basic care and feeding of machine-learning techniques<a class="indexterm" id="id0"/> in the domain of <span class="strong"><strong>natural language processing</strong></span> (<span class="strong"><strong>NLP</strong></span>).</p><p>LingPipe is a Java toolkit for NLP-oriented applications. This book will show you how to solve common NLP problems with LingPipe in a problem/solution format that allows developers to quickly deploy solutions to common tasks.</p><div class="section" title="LingPipe and its installation"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec07"/>LingPipe and its installation</h2></div></div></div><p>LingPipe 1.0<a class="indexterm" id="id1"/> was <a class="indexterm" id="id2"/>released in 2003 as a dual-licensed open source NLP Java<a class="indexterm" id="id3"/> library. At the time of writing this book, we are coming up on 2000 hits on Google Scholar and have thousands of commercial installs, ranging from universities to government agencies to Fortune 500 companies.</p><p>Current licensing is either<a class="indexterm" id="id4"/> AGPL (<a class="ulink" href="http://www.gnu.org/licenses/agpl-3.0.html">http://www.gnu.org/licenses/agpl-3.0.html</a>) or our commercial license that offers more traditional features such as indemnification and non-sharing of code as well as support.</p><div class="section" title="Projects similar to LingPipe"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec01"/>Projects similar to LingPipe</h3></div></div></div><p>Nearly all NLP projects have awful acronyms so we will lay bare our own. <span class="strong"><strong>LingPipe</strong></span><a class="indexterm" id="id5"/> is the short form for <span class="strong"><strong>linguistic pipeline</strong></span>, which was the name of the <code class="literal">cvs</code> directory in which Bob Carpenter put the initial code.</p><p>LingPipe has lots of competition in the NLP space. The following are some of the more popular ones with a focus on Java:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>NLTK</strong></span>: This is the <a class="indexterm" id="id6"/>dominant <a class="indexterm" id="id7"/>Python library for NLP processing.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>OpenNLP</strong></span>: This is an <a class="indexterm" id="id8"/>Apache project built by a bunch of smart<a class="indexterm" id="id9"/> folks.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>JavaNLP</strong></span>: This is a<a class="indexterm" id="id10"/> rebranding of Stanford NLP tools, again built by a<a class="indexterm" id="id11"/> bunch of smart folks.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>ClearTK</strong></span>: This is a University<a class="indexterm" id="id12"/> of Boulder toolkit that wraps lots of popular machine learning <a class="indexterm" id="id13"/>frameworks.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>DkPro</strong></span>: Technische <a class="indexterm" id="id14"/>Universität Darmstadt from Germany produced this<a class="indexterm" id="id15"/> UIMA-based project that wraps many common components in a useful manner. UIMA is a common framework for NLP.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>GATE</strong></span>: GATE<a class="indexterm" id="id16"/> is really <a class="indexterm" id="id17"/>more of a framework than competition. In fact, LingPipe components are part of their standard distribution. It has a nice graphical "hook the components up" capability.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Learning Based Java</strong></span> (<span class="strong"><strong>LBJ</strong></span>): LBJ<a class="indexterm" id="id18"/> is a special-purpose programming language based on Java, and it is geared toward machine learning and NLP. It was developed at the Cognitive Computation Group of the University of Illinois at Urbana Champaign.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Mallet</strong></span>: This name is the <a class="indexterm" id="id19"/>short form of <span class="strong"><strong>MAchine Learning for LanguagE Toolkit</strong></span>. Apparently, reasonable<a class="indexterm" id="id20"/> acronym generation is short in supply these days. Smart folks built this too.</li></ul></div><p>Here are some pure machine learning frameworks that have broader appeal but are not necessarily tailored for NLP tasks:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Vowpal Wabbit</strong></span>: This is <a class="indexterm" id="id21"/>very focused on scalability around Logistic <a class="indexterm" id="id22"/>Regression, Latent Dirichelet Allocation, and so on. Smart folks drive this.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Factorie</strong></span>: It is from UMass, Amherst and an alternative offering to Mallet. Initially it focused primarily on graphic models, but now it also supports NLP tasks.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Support Vector Machine</strong></span> (<span class="strong"><strong>SVM</strong></span>): SVM light and <code class="literal">libsvm</code> are <a class="indexterm" id="id23"/>very popular <a class="indexterm" id="id24"/>SVM implementations. There is no SVM implementation in LingPipe, because logistic regression does this as well.</li></ul></div></div><div class="section" title="So, why use LingPipe?"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec02"/>So, why use LingPipe?</h3></div></div></div><p>It is very reasonable to ask why choose LingPipe with such outstanding free competition mentioned earlier. There are a few reasons:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Documentation</strong></span>: The <a class="indexterm" id="id25"/>class-level documentation in LingPipe is very thorough. If the work is based on academic work, that work is cited. Algorithms are laid out, the underlying math is explained, and explanations are precise. What the documentation lacks is a "how to get things done" perspective; however, this is covered in this book.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Enterprise/server optimized</strong></span>: LingPipe is designed from the ground up for server applications, not for command-line usage (though we will be using the command line extensively throughout the book).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Coded in the Java dialect</strong></span>: LingPipe is a native Java API that is designed according to standard Java class design principles (Joshua Bloch's <span class="emphasis"><em>Effective Java</em></span>, by Addison-Wesley), such as consistency checks on construction, immutability, type safety, backward-compatible serializability, and thread safety.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Error handling</strong></span>: Considerable attention is paid to error handling through exceptions and configurable message streams for long-running processes.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Support</strong></span>: LingPipe has paid employees whose job is to answer your questions and make sure that LingPipe is doing its job. The rare bug gets fixed in under 24 hours typically. They respond to questions very quickly and are very willing to help people.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Consulting</strong></span>: You can hire experts in LingPipe to build systems for you. Generally, they teach developers how to build NLP systems as a byproduct.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Consistency</strong></span>: The LingPipe API was designed by one person, Bob Carpenter, with an obsession of consistency. While it is not perfect, you will find a regularity and eye to design that can be missing in academic efforts. Graduate students come and go, and the resulting contributions to university toolkits can be quite varied.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Open source</strong></span>: There are many commercial providers, but their software is a black box. The open source nature of LingPipe provides transparency and confidence that the code is doing what we ask it to do. When the documentation fails, it is a huge relief to have access to code to understand it better.</li></ul></div></div><div class="section" title="Downloading the book code and data"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec03"/>Downloading the book code and data</h3></div></div></div><p>You will need to download <a class="indexterm" id="id26"/>the source code for this cookbook, with <a class="indexterm" id="id27"/>supporting models and data from <a class="ulink" href="http://alias-i.com/book.html">http://alias-i.com/book.html</a>. Untar and uncompress it using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>tar –xvzf lingpipeCookbook.tgz</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip02"/>Tip</h3><p>
<span class="strong"><strong>Downloading the example code</strong></span>
</p><p>You can download the example code files for all Packt books you have purchased from your account at <a class="ulink" href="http://www.packtpub.com">http://www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div></div><p>Alternatively, your operating system might provide other ways of extracting the archive. All recipes assume that you are running the commands in the resulting cookbook directory.</p></div><div class="section" title="Downloading LingPipe"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec04"/>Downloading LingPipe</h3></div></div></div><p>Downloading <a class="indexterm" id="id28"/>LingPipe is not strictly necessary, but you will likely want to be able to look at the source and have a local copy of the Javadoc.</p><p>The download and installation instructions for<a class="indexterm" id="id29"/> LingPipe can be found at <a class="ulink" href="http://alias-i.com/lingpipe/web/install.html">http://alias-i.com/lingpipe/web/install.html</a>.</p><p>The examples from this chapter use command-line invocation, but it is assumed that the reader has sufficient development skills to map the examples to their preferred IDE/ant or other environment.</p></div></div></div></div>
<div class="section" title="Deserializing and running a classifier"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/>Deserializing and running a classifier</h1></div></div></div><p>This recipe does two <a class="indexterm" id="id30"/>things: introduces a very simple and effective language ID classifier<a class="indexterm" id="id31"/> and demonstrates how to deserialize a LingPipe class. If you find yourself here from a later chapter, trying to understand deserialization, I encourage you to run the example program anyway. It will take 5 minutes, and you might learn something useful.</p><p>Our language ID classifier is based on character language models. Each language model gives you the probability of the text, given that it is generated in that language. The model that is most familiar with the text is the first best fit. This one has already been built, but later in the chapter, you will learn to make your own.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec08"/>How to do it...</h2></div></div></div><p>Perform the following steps to <a class="indexterm" id="id32"/>deserialize and run a <a class="indexterm" id="id33"/>classifier:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Go to the <code class="literal">cookbook</code> directory for the book and run the command for OSX, Unix, and Linux:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter1.RunClassifierFromDisk</strong></span>
</pre></div><p>For Windows invocation (quote the classpath and use <code class="literal">;</code> instead of <code class="literal">:</code>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp "lingpipe-cookbook.1.0.jar;lib\lingpipe-4.1.0.jar" com.lingpipe.cookbook.chapter1.RunClassifierFromDisk</strong></span>
</pre></div><p>We will use the Unix style command line in this book.</p></li><li class="listitem">The program reports the model being loaded and a default, and prompts for a sentence to classify:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Loading: models/3LangId.LMClassifier</strong></span>
<span class="strong"><strong>Type a string to be classified. Empty string to quit.</strong></span>
<span class="strong"><strong>The rain in Spain falls mainly on the plain.</strong></span>
<span class="strong"><strong>english</strong></span>
<span class="strong"><strong>Type a string to be classified. Empty string to quit.</strong></span>
<span class="strong"><strong>la lluvia en España cae principalmente en el llano.</strong></span>
<span class="strong"><strong>spanish</strong></span>
<span class="strong"><strong>Type a string to be classified. Empty string to quit.</strong></span>
<span class="strong"><strong>スペインの雨は主に平野に落ちる。</strong></span>
<span class="strong"><strong>japanese</strong></span>
</pre></div></li><li class="listitem">The classifier is trained on English, Spanish, and Japanese. We have entered an example of each—to get some <a class="indexterm" id="id34"/>Japanese, go to <a class="ulink" href="http://ja.wikipedia.org/wiki/">http://ja.wikipedia.org/wiki/</a>. These are the only languages it knows about, but it will guess on any text. So, let's try some Arabic:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Type a string to be classified. Empty string to quit.</strong></span>
<span class="strong"><strong>المطر في اسبانيا يقع أساسا على سهل.</strong></span>
<span class="strong"><strong>japanese</strong></span>
</pre></div></li><li class="listitem">It thinks it is Japanese because this language has more characters than English or Spanish. This in turn leads that model to expect more unknown characters. All the Arabic characters are unknown.</li><li class="listitem">If you are working with a Windows terminal, you might encounter difficulty entering UTF-8 characters.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec09"/>How it works...</h2></div></div></div><p>The code<a class="indexterm" id="id35"/> in the jar is <code class="literal">cookbook/src/com/lingpipe/cookbook/chapter1/ RunClassifierFromDisk.java</code>. What is happening is that a pre-built model for <a class="indexterm" id="id36"/>language identification is deserialized and made available. It has been trained on English, Japanese, and Spanish. The training data came from Wikipedia pages for each language. You can see the data in <code class="literal">data/3LangId.csv</code>. The focus of this recipe is to show you how to deserialize the classifier and run it—training is handled in the <span class="emphasis"><em>Training your own language model classifier</em></span> recipe in this chapter. The entire code for the <code class="literal">RunClassifier FromDisk.java</code> class starts with the package; then it imports the start of the <code class="literal">RunClassifierFromDisk</code> class and the start of <code class="literal">main()</code>:</p><div class="informalexample"><pre class="programlisting">package com.lingpipe.cookbook.chapter1;
import java.io.File;
import java.io.IOException;

import com.aliasi.classify.BaseClassifier;
import com.aliasi.util.AbstractExternalizable;
import com.lingpipe.cookbook.Util;
public class RunClassifierFromDisk {
  public static void main(String[] args) throws
  IOException, ClassNotFoundException {</pre></div><p>The preceding code is a very standard Java code, and we present it without explanation. Next is a feature in most recipes that supplies a default value for a file if the command line does not contain one. This allows you to use your own data if you have it, otherwise it will run from files in the distribution. In this case, a default classifier is supplied if there is no argument on the command line:</p><div class="informalexample"><pre class="programlisting">String classifierPath = args.length &gt; 0 ? args[0] :  "models/3LangId.LMClassifier";
System.out.println("Loading: " + classifierPath);</pre></div><p>Next, we will see how to deserialize a classifier or another LingPipe object from disk:</p><div class="informalexample"><pre class="programlisting">File serializedClassifier = new File(classifierPath);
@SuppressWarnings("unchecked")
BaseClassifier&lt;String&gt; classifier
  = (BaseClassifier&lt;String&gt;)
  AbstractExternalizable.readObject(serializedClassifier);</pre></div><p>The preceding code snippet is the first LingPipe-specific code, where the classifier is built using the static <code class="literal">AbstractExternalizable.readObject</code> method.</p><p>This class is<a class="indexterm" id="id37"/> employed throughout LingPipe to carry out a compilation of classes<a class="indexterm" id="id38"/> for two reasons. First, it allows the compiled objects to have final variables set, which supports LingPipe's extensive use of immutables. Second, it avoids the messiness of exposing the I/O methods required for externalization and deserialization, most notably, the no-argument constructor. This class is used as the superclass of a private internal class that does the actual compilation. This private internal class implements the required <code class="literal">no-arg</code> constructor and stores the object required for <code class="literal">readResolve()</code>.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note02"/>Note</h3><p>The reason we <a class="indexterm" id="id39"/>use <code class="literal">Externalizable</code> instead of <code class="literal">Serializable</code> is to avoid breaking backward compatibility when changing any method signatures or member variables. <code class="literal">Externalizable</code> extends <code class="literal">Serializable</code> and allows <a class="indexterm" id="id40"/>control of how the object is read or written. For more information on this, refer to the excellent chapter on serialization in Josh Bloch's book, <span class="emphasis"><em>Effective Java, 2nd Edition</em></span>.</p></div></div><p>
<code class="literal">BaseClassifier&lt;E&gt;</code> is the foundational <a class="indexterm" id="id41"/>classifier interface, with <code class="literal">E</code> being the type of object being classified in LingPipe. Look at the Javadoc to see the range of classifiers that implements the interface—there are 10 of them. Deserializing to <code class="literal">BaseClassifier&lt;E&gt;</code> hides a good bit of complexity, which we will explore later in the <span class="emphasis"><em>How to serialize a LingPipe object – classifier example</em></span> recipe in this chapter.</p><p>The last line calls a utility method, which we will use frequently in this book:</p><div class="informalexample"><pre class="programlisting">Util.consoleInputBestCategory(classifier);</pre></div><p>This method handles interactions with the command line. The code is in <code class="literal">src/com/lingpipe/cookbook/Util.java</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>public static void</strong></span> consoleInputBestCategory(
BaseClassifier&lt;CharSequence&gt; classifier) <span class="strong"><strong>throws</strong></span> IOException {
  BufferedReader reader = <span class="strong"><strong>new</strong></span> BufferedReader(<span class="strong"><strong>new</strong></span> InputStreamReader(System.in));
  <span class="strong"><strong>while (true)</strong></span> {
    System.out.println("\nType a string to be classified. " + " Empty string to quit.");
    String data = reader.readLine();
    <span class="strong"><strong>if</strong></span> (data.equals("")) {
      <span class="strong"><strong>return</strong></span>;
    }
    Classification classification = classifier.classify(data);
    System.out.println("Best Category: " + classification.bestCategory());
  }
}</pre></div><p>Once the string is<a class="indexterm" id="id42"/> read in from the console, then <code class="literal">classifier.classify(input)</code> is called, which<a class="indexterm" id="id43"/> returns <code class="literal">Classification</code>. This, in turn, provides a <code class="literal">String</code> label that is printed out. That's it! You have run a classifier.</p></div></div>
<div class="section" title="Getting confidence estimates from a classifier"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Getting confidence estimates from a classifier</h1></div></div></div><p>Classifiers tend to be a lot more useful if they give more information about how confident they are of the classification—this is usually a score or a probability. We often threshold classifiers to help fit the performance requirements of an installation. For example, if it was vital that the classifier never makes a mistake, then we could require that the classification be very confident before committing to a decision.</p><p>LingPipe classifiers exist on a <a class="indexterm" id="id44"/>hierarchy based on the kinds of <a class="indexterm" id="id45"/>estimates they provide. The backbone is a series of interfaces—don't freak out; it is actually pretty simple. You don't need to understand it now, but we do need to write it down somewhere for future reference:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">BaseClassifier&lt;E&gt;</code>: This is <a class="indexterm" id="id46"/>just your basic classifier of objects of type <code class="literal">E</code>. It has a <code class="literal">classify()</code> method that returns a classification, which in turn has a <code class="literal">bestCategory()</code> method and a <code class="literal">toString()</code> method that is of some informative use.</li><li class="listitem" style="list-style-type: disc"><code class="literal">RankedClassifier&lt;E&gt; extends BaseClassifier&lt;E&gt;</code>: The <code class="literal">classify()</code> method<a class="indexterm" id="id47"/> returns <code class="literal">RankedClassification</code>, which extends <code class="literal">Classification</code> and adds methods for <code class="literal">category(int rank)</code> that says what the 1st to <span class="emphasis"><em>n</em></span>th classifications are. There is also a <code class="literal">size()</code> method that indicates how many classifications there are.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ScoredClassifier&lt;E&gt; extends RankedClassifier&lt;E&gt;</code>: The returned <code class="literal">ScoredClassification</code> <a class="indexterm" id="id48"/>adds a <code class="literal">score(int rank)</code> method.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ConditionalClassifier&lt;E&gt; extends RankedClassifier&lt;E&gt;</code>: <code class="literal">ConditionalClassification</code> produced <a class="indexterm" id="id49"/>by this has the property that the sum of scores for all categories must sum to 1 as accessed via the <code class="literal">conditionalProbability(int rank)</code> method and <code class="literal">conditionalProbability(String category)</code>. There's more; you can read the Javadoc for this. This classification will be the work horse of the book when things get fancy, and we want to know the confidence that the tweet is English versus the tweet is Japanese versus the tweet is Spanish. These estimates will have to sum to 1.</li><li class="listitem" style="list-style-type: disc"><code class="literal">JointClassifier&lt;E&gt; extends ConditionalClassifier&lt;E&gt;</code>: This provides <code class="literal">JointClassification</code> of the <a class="indexterm" id="id50"/>input and category in the space of all the possible inputs, and all such estimates sum to 1. This is a sparse space, so values are log based to avoid underflow errors. We don't see a lot of use of this estimate directly in production.</li></ul></div><p>It is obvious that <a class="indexterm" id="id51"/>there has been a great deal of thought <a class="indexterm" id="id52"/>put into the classification stack presented. This is because huge numbers of industrial NLP problems are handled by a classification system in the end.</p><p>It turns out that our simplest classifier—in some arbitrary sense of simple—produces the richest estimates, which are joint classifications. Let's dive in.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec10"/>Getting ready</h2></div></div></div><p>In the previous recipe, we blithely deserialized to <code class="literal">BaseClassifier&lt;String&gt;</code> that hid all the details of what was going on. The reality is a bit more complex than suggested by the hazy abstract class. Note that the file on disk that was loaded is named <code class="literal">3LangId.LMClassifier</code>. By convention, we name serialized models with the type of object it will deserialize to, which, in this case, is <code class="literal">LMClassifier</code>, and it extends <code class="literal">BaseClassifier</code>. The most specific typing for the classifier is:</p><div class="informalexample"><pre class="programlisting">LMClassifier&lt;CompiledNGramBoundaryLM, MultivariateDistribution&gt; classifier = (LMClassifier &lt;CompiledNGramBoundaryLM, MultivariateDistribution&gt;) AbstractExternalizable.readObject(new File(args[0]));</pre></div><p>The cast to <code class="literal">LMClassifier&lt;CompiledNGramBoundaryLM, MultivariateDistribution&gt;</code> specifies the type of distribution to be <code class="literal">MultivariateDistribution</code>. The Javadoc for <code class="literal">com.aliasi.stats.MultivariateDistribution</code> is quite explicit and helpful in describing what this is.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>
<code class="literal">MultivariateDistribution</code> implements <a class="indexterm" id="id53"/>a discrete distribution over a finite set of outcomes, numbered consecutively from zero.</p></div></div><p>The Javadoc goes into a lot of detail about <code class="literal">MultivariateDistribution</code>, but it basically means that we can have an n-way assignment of probabilities that sum to 1.</p><p>The next class in <a class="indexterm" id="id54"/>the cast is for <code class="literal">CompiledNGramBoundaryLM</code>, which is the "memory" of the <code class="literal">LMClassifier</code>. In fact, each language gets its<a class="indexterm" id="id55"/> own. This means that English will have a separate language model from Spanish and so on. There are eight different kinds of language models that could have been used as this part of the classifier—consult the Javadoc for the <code class="literal">LanguageModel</code> interface. Each <span class="strong"><strong>language model</strong></span> (<span class="strong"><strong>LM</strong></span>)<a class="indexterm" id="id56"/> has the following properties:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The LM will provide a probability that it generated the text provided. It is robust against data that it has not seen before, in the sense that it won't crash or give a zero probability. Arabic just comes across as a sequence of unknown characters for our example.</li><li class="listitem" style="list-style-type: disc">The sum of all the possible character sequence probabilities of any length is 1 for boundary LMs. Process LMs sum the probability to 1 over all sequences of the same length. Look at the Javadoc for how this bit of math is done.</li><li class="listitem" style="list-style-type: disc">Each language model has no knowledge of data outside of its category.</li><li class="listitem" style="list-style-type: disc">The classifier keeps track of the marginal probability of the category and factors this into the results for the category. Marginal probability is saying that we tend to see two-thirds English, one-sixth Spanish, and one-sixth Japanese in Disney tweets. This information is combined with the LM estimates.</li><li class="listitem" style="list-style-type: disc">The LM is a<a class="indexterm" id="id57"/> compiled version of <code class="literal">LanguageModel.Dynamic</code> that we will cover in the later recipes that discuss training.</li></ul></div><p>
<code class="literal">LMClassifier</code> that is<a class="indexterm" id="id58"/> constructed wraps these components into a classifier.</p><p>Luckily, the interface saves the day with a more aesthetic deserialization:</p><div class="informalexample"><pre class="programlisting">JointClassifier&lt;String&gt; classifier = (JointClassifier&lt;String&gt;) AbstractExternalizable.readObject(new File(classifierPath));</pre></div><p>The interface hides the guts of the implementation nicely and this is what we are going with in the example program.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec11"/>How to do it…</h2></div></div></div><p>This recipe is the <a class="indexterm" id="id59"/>first time we start peeling away from what classifiers can do, but first, let's play with it a bit:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Get your magic shell genie to conjure a command prompt with a Java interpreter and type:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter1.RunClassifierJoint </strong></span>
</pre></div></li><li class="listitem">We will enter the<a class="indexterm" id="id60"/> same data as we did earlier:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Type a string to be classified. Empty string to quit.</strong></span>
<span class="strong"><strong>The rain in Spain falls mainly on the plain.</strong></span>
<span class="strong"><strong>Rank Categ Score   P(Category|Input) log2 P(Category,Input)</strong></span>
<span class="strong"><strong>0=english -3.60092 0.9999999999         -165.64233893156052</strong></span>
<span class="strong"><strong>1=spanish -4.50479 3.04549412621E-13    -207.2207276413206</strong></span>
<span class="strong"><strong>2=japanese -14.369 7.6855682344E-150    -660.989401136873</strong></span>
</pre></div></li></ol></div><p>As described, <code class="literal">JointClassification</code> carries through all the classification metrics in the hierarchy rooted at <code class="literal">Classification</code>. Each level of classification shown as follows adds to the classifiers preceding it:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Classification</code> provides the first best category as the rank 0 category.</li><li class="listitem" style="list-style-type: disc"><code class="literal">RankedClassification</code> adds an ordering of all the possible categories with a lower rank corresponding to greater likelihood of the category. The <code class="literal">rank</code> column reflects this ordering.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ScoredClassification</code> adds a numeric score to the ranked output. Note that scores might or might not compare well against other strings being classified depending on the type of classifier. This is the column labeled <code class="literal">Score</code>. To understand the basis of this score, consult the relevant Javadoc.</li><li class="listitem" style="list-style-type: disc"><code class="literal">ConditionalClassification</code> further refines the score by making it a category probability conditioned on the input. The probabilities of all categories will sum up to 1. This is the column labeled <code class="literal">P(Category|Input)</code>, which is the traditional way to write <span class="emphasis"><em>probability of the category given the input</em></span>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">JointClassification</code> adds the log2 (log base 2) probability of the input and the category—this is the joint probability. The probabilities of all categories and inputs will sum up to 1, which is a very large space indeed with very low probabilities assigned to any pair of category and string. This is why log2 values are used to prevent numerical underflow. This is the column labeled <code class="literal">log 2 P(Category, Input)</code>, which is translated as <span class="emphasis"><em>the log</em></span><span class="emphasis"><em>2</em></span><span class="emphasis"><em> probability of the category and input</em></span>.</li></ul></div><p>Look at the Javadoc for the <code class="literal">com.aliasi.classify</code> package for more information on the metrics and classifiers that implement them.</p></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec12"/>How it works…</h2></div></div></div><p>The code<a class="indexterm" id="id61"/> is<a class="indexterm" id="id62"/> in <code class="literal">src/com/lingpipe/cookbook/chapter1/RunClassifierJoint.java</code>, and it deserializes to a <code class="literal">JointClassifier&lt;CharSequence&gt;</code>:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException, ClassNotFoundException {
  String classifierPath  = args.length &gt; 0 ? args[0] : "models/3LangId.LMClassifier";
  @SuppressWarnings("unchecked")
    JointClassifier&lt;CharSequence&gt; classifier = (JointClassifier&lt;CharSequence&gt;) AbstractExternalizable.readObject(new File(classifierPath));
  Util.consoleInputPrintClassification(classifier);
}</pre></div><p>It makes a call to <code class="literal">Util.consoleInputPrintClassification(classifier)</code>, which minimally differs from <code class="literal">Util.consoleInputBestCategory(classifier)</code>, in that it uses the <code class="literal">toString()</code> method of classification to print. The code is as follows:</p><div class="informalexample"><pre class="programlisting">public static void consoleInputPrintClassification(BaseClassifier&lt;CharSequence&gt; classifier) throws IOException {
  BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
  while (true) {
    System.out.println("\nType a string to be classified." + Empty string to quit.");
    String data = reader.readLine();
    if (data.equals("")) {
      return;
    }
    Classification classification = classifier.classify(data);
    System.out.println(classification);
  }
}</pre></div><p>We got a richer output than we expected, because the type is <code class="literal">Classification</code>, but the <code class="literal">toString()</code> method will be<a class="indexterm" id="id63"/> applied to the <a class="indexterm" id="id64"/>runtime type <code class="literal">JointClassification</code>.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec13"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">There is detailed information in <a class="link" href="ch06.html" title="Chapter 6. String Comparison and Clustering">Chapter 6</a>, <span class="emphasis"><em>Character Language Models</em></span> of <span class="emphasis"><em>Text Analysis with LingPipe 4</em></span>, by <span class="emphasis"><em>Bob Carpenter</em></span> and <span class="emphasis"><em>Breck Baldwin</em></span>, <span class="emphasis"><em>LingPipe Publishing</em></span> (<a class="ulink" href="http://alias-i.com/lingpipe-book/lingpipe-book-0.5.pdf">http://alias-i.com/lingpipe-book/lingpipe-book-0.5.pdf</a>) on <a class="indexterm" id="id65"/>language models.</li></ul></div></div></div>
<div class="section" title="Getting data from the Twitter API"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec11"/>Getting data from the Twitter API</h1></div></div></div><p>We <a class="indexterm" id="id66"/>use the popular <code class="literal">twitter4j</code> package to invoke the <a class="indexterm" id="id67"/>Twitter Search API, and search for tweets and save them to disk. The Twitter API requires authentication as of Version 1.1, and we will need to get authentication tokens and save them in the <code class="literal">twitter4j.properties</code> file before we get started.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec14"/>Getting ready</h2></div></div></div><p>If you don't have a Twitter<a class="indexterm" id="id68"/> account, go to <a class="ulink" href="http://twitter.com/signup">twitter.com/signup</a> and create an account. You will also need to go to <a class="ulink" href="http://dev.twitter.com">dev.twitter.com</a> and sign in to enable yourself for the developer account. Once you have a Twitter login, we'll be on our way to creating the Twitter OAuth credentials. Be prepared for this process to be different from what we are presenting. In any case, we will supply example results in the <code class="literal">data</code> directory. Let's now create the Twitter OAuth credentials:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Log in to <a class="ulink" href="http://dev.twitter.com">dev.twitter.com</a>.</li><li class="listitem">Find the little pull-down menu next to your icon on the top bar.</li><li class="listitem">Choose <span class="strong"><strong>My Applications</strong></span>.</li><li class="listitem">Click on <span class="strong"><strong>Create a new application</strong></span>.</li><li class="listitem">Fill in the form and click on <span class="strong"><strong>Create a Twitter application</strong></span>.</li><li class="listitem">The next page contains the OAuth settings.</li><li class="listitem">Click on the <span class="strong"><strong>Create my access token</strong></span> link.</li><li class="listitem">You will need to copy <span class="strong"><strong>Consumer key</strong></span> and <span class="strong"><strong>Consumer secret</strong></span>.</li><li class="listitem">You will also need to copy <span class="strong"><strong>Access token</strong></span> and <span class="strong"><strong>Access token secret</strong></span>.</li><li class="listitem">These values should go into the <code class="literal">twitter4j.properties</code> file in the appropriate locations. The properties are as follows:<div class="informalexample"><pre class="programlisting">debug=false
oauth.consumerKey=ehUOExampleEwQLQpPQ
oauth.consumerSecret=aTHUGTBgExampleaW3yLvwdJYlhWY74
oauth.accessToken=1934528880-fiMQBJCBExamplegK6otBG3XXazLv
oauth.accessTokenSecret=y0XExampleGEHdhCQGcn46F8Vx2E</pre></div></li></ol></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec15"/>How to do it...</h2></div></div></div><p>Now, we're ready to<a class="indexterm" id="id69"/> access Twitter and get some search data using<a class="indexterm" id="id70"/> the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Go to the directory of this chapter and run the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/twitter4j-core-4.0.1.jar:lib/opencsv-2.4.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter1.TwitterSearch</strong></span>
</pre></div></li><li class="listitem">The code displays the output file (in this case, a default value). Supplying a path as an argument will write to this file. Then, type in your query at the prompt:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Writing output to data/twitterSearch.csv</strong></span>
<span class="strong"><strong>Enter Twitter Query:disney</strong></span>
</pre></div></li><li class="listitem">The code then queries Twitter and reports every 100 tweets found (output truncated):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Tweets Accumulated: 100</strong></span>
<span class="strong"><strong>Tweets Accumulated: 200</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>Tweets Accumulated: 1500</strong></span>
<span class="strong"><strong>writing to disk 1500 tweets at data/twitterSearch.csv  </strong></span>
</pre></div></li></ol></div><p>This program uses the search query, searches Twitter for the term, and writes the output (limited to 1500 tweets) to the <code class="literal">.csv</code> file name that you specified on the command line or uses a default.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec16"/>How it works...</h2></div></div></div><p>The code uses the <code class="literal">twitter4j</code> library to instantiate <code class="literal">TwitterFactory</code> and searches Twitter using the user-entered query. The start of <code class="literal">main()</code> at <code class="literal">src/com/lingpipe/cookbook/chapter1/TwitterSearch.java</code> is:</p><div class="informalexample"><pre class="programlisting">String outFilePath = args.length &gt; 0 ? args[0] : "data/twitterSearch.csv";
File outFile = new File(outFilePath);
System.out.println("Writing output to " + outFile);
BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
System.out.print("Enter Twitter Query:");
String queryString = reader.readLine();</pre></div><p>The preceding<a class="indexterm" id="id71"/> code gets the outfile, supplying a default if none is <a class="indexterm" id="id72"/>provided, and takes the query from the command line.</p><p>The following code sets up the query according to the vision of the twitter4j developers. For more information on this process, read their Javadoc. However, it should be fairly straightforward. In order to make our result set more unique, you'll notice that when we create the query string, we will filter out retweets using the <code class="literal">-filter:retweets</code> option. This is only somewhat effective; see the <span class="emphasis"><em>Eliminate near duplicates with the Jaccard distance</em></span> recipe later in this chapter for a more complete solution:</p><div class="informalexample"><pre class="programlisting">Twitter twitter = new TwitterFactory().getInstance();
Query query = new Query(queryString + " -filter:retweets"); query.setLang("en");//English
query.setCount(TWEETS_PER_PAGE);
query.setResultType(Query.RECENT);</pre></div><p>We will get the following result:</p><div class="informalexample"><pre class="programlisting">List&lt;String[]&gt; csvRows = new ArrayList&lt;String[]&gt;();
while(csvRows.size() &lt; MAX_TWEETS) {
  QueryResult result = twitter.search(query);
  List&lt;Status&gt; resultTweets = result.getTweets();
  for (Status tweetStatus : resultTweets) {
    String row[] = new String[Util.ROW_LENGTH];
    row[Util.TEXT_OFFSET] = tweetStatus.getText();
    csvRows.add(row);
  }
  System.out.println("Tweets Accumulated: " + csvRows.size());
  if ((query = result.nextQuery()) == null) {
    break;
  }
}</pre></div><p>The preceding snippet is a pretty standard code slinging, albeit without the usual hardening for external facing code—try/catch, timeouts, and retries. One potentially confusing bit is the use of <code class="literal">query</code> to handle paging through the search results—it returns <code class="literal">null</code> when no more pages are available. The current Twitter API allows a maximum of 100 results per page, so in order to get 1500 results, we need to rerun the search until there are no more <a class="indexterm" id="id73"/>results, or until we get 1500 tweets. The next step<a class="indexterm" id="id74"/> involves a bit of reporting and writing:</p><div class="informalexample"><pre class="programlisting">System.out.println("writing to disk " + csvRows.size() + " tweets at " + outFilePath);
Util.writeCsvAddHeader(csvRows, outFile);</pre></div><p>The list of tweets is then written to a <code class="literal">.csv</code> file using the <code class="literal">Util.writeCsvAddHeader</code> method:</p><div class="informalexample"><pre class="programlisting">public static void writeCsvAddHeader(List&lt;String[]&gt; data, File file) throws IOException {
  CSVWriter csvWriter = new CSVWriter(new OutputStreamWriter(new FileOutputStream(file),Strings.UTF8));
  csvWriter.writeNext(ANNOTATION_HEADER_ROW);
  csvWriter.writeAll(data);
  csvWriter.close();
}</pre></div><p>We will be using this <code class="literal">.csv</code> file to run the language ID test in the next section.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec17"/>See also</h2></div></div></div><p>For more details on using the Twitter API<a class="indexterm" id="id75"/> and <a class="indexterm" id="id76"/>twitter4j, please go to their documentation pages:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://twitter4j.org/javadoc/">http://twitter4j.org/javadoc/</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="https://dev.twitter.com/docs">https://dev.twitter.com/docs</a></li></ul></div></div></div>
<div class="section" title="Applying a classifier to a .csv file"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec12"/>Applying a classifier to a .csv file</h1></div></div></div><p>Now, we can test our<a class="indexterm" id="id77"/> language ID classifier on the data we downloaded from Twitter. This<a class="indexterm" id="id78"/> recipe will show you how to run the classifier on the <code class="literal">.csv</code> file and will set the stage for the evaluation step in the next recipe.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec18"/>How to do it...</h2></div></div></div><p>Applying a classifier to the <code class="literal">.csv</code> file is straightforward! Just perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Get a command prompt and run:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/twitter4j-core-4.0.1.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter1.ReadClassifierRunOnCsv</strong></span>
</pre></div></li><li class="listitem">This will use the default CSV file from the <code class="literal">data/disney.csv</code> distribution, run over each line of the CSV file, and apply a language ID classifier from <code class="literal">models/ 3LangId.LMClassifier</code> to it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>InputText: When all else fails #Disney</strong></span>
<span class="strong"><strong>Best Classified Language: english</strong></span>
<span class="strong"><strong>InputText: ES INSUPERABLE DISNEY !! QUIERO VOLVER:(</strong></span>
<span class="strong"><strong>Best Classified Language: Spanish</strong></span>
</pre></div></li><li class="listitem">You can also specify the input as the first argument and the classifier as the second one.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec19"/>How it works…</h2></div></div></div><p>We will deserialize a classifier from the externalized model that was described in the previous recipes. Then, we will iterate through each line of the <code class="literal">.csv</code> file and call the classify method of the classifier. The code in <code class="literal">main()</code> is:</p><div class="informalexample"><pre class="programlisting">String inputPath = args.length &gt; 0 ? args[0] : "data/disney.csv";
String classifierPath = args.length &gt; 1 ? args[1] : "models/3LangId.LMClassifier";
@SuppressWarnings("unchecked") BaseClassifier&lt;CharSequence&gt; classifier = (BaseClassifier&lt;CharSequence&gt;) AbstractExternalizable.readObject(new File(classifierPath));
List&lt;String[]&gt; lines = Util.readCsvRemoveHeader(new File(inputPath));
for(String [] line: lines) {
  String text = line[Util.TEXT_OFFSET];
  Classification classified = classifier.classify(text);
  System.out.println("InputText: " + text);
  System.out.println("Best Classified Language: " + classified.bestCategory());
}</pre></div><p>The preceding code builds on the previous recipes with nothing particularly new. <code class="literal">Util.readCsvRemoveHeader</code>, shown as follows, just skips the first line of the <code class="literal">.csv</code> file before reading from disk and returning<a class="indexterm" id="id79"/> the rows that have non-null values and non-empty<a class="indexterm" id="id80"/> strings in the <code class="literal">TEXT_OFFSET</code> position:</p><div class="informalexample"><pre class="programlisting">public static List&lt;String[]&gt; readCsvRemoveHeader(File file) throws IOException {
  FileInputStream fileIn = new FileInputStream(file);
  InputStreamReader inputStreamReader = new InputStreamReader(fileIn,Strings.UTF8);
  CSVReader csvReader = new CSVReader(inputStreamReader);
  csvReader.readNext();  //skip headers
  List&lt;String[]&gt; rows = new ArrayList&lt;String[]&gt;();
  String[] row;
  while ((row = csvReader.readNext()) != null) {
    if (row[TEXT_OFFSET] == null || row[TEXT_OFFSET].equals("")) {
      continue;
    }
    rows.add(row);
  }
  csvReader.close();
  return rows;
}</pre></div></div></div>
<div class="section" title="Evaluation of classifiers &#x2013; the confusion matrix"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Evaluation of classifiers – the confusion matrix</h1></div></div></div><p>Evaluation is incredibly <a class="indexterm" id="id81"/>important in building solid NLP systems. It allows developers and management to map a business need to system performance, which, in turn, helps communicate system improvement to vested parties. "Well, uh, the system seems to be doing better" does not hold the gravitas of "Recall has improved 20 percent, and the specificity is holding well with 50 percent more training data".</p><p>This recipe provides the steps for the creation of truth or <span class="emphasis"><em>gold standard</em></span> data and tells us how to use this data to evaluate the performance of our precompiled classifier. It is as simple as it is powerful.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec20"/>Getting ready</h2></div></div></div><p>You might have noticed the headers from the output of the CSV writer and the suspiciously labeled column, <code class="literal">TRUTH</code>. Now, we get to use it. Load up the tweets we provided earlier or convert your data into the format used in our <code class="literal">.csv</code> format. An easy way to get novel data is to run a query against Twitter with a multilingual friendly query such as <code class="literal">Disney</code>, which is our default supplied data.</p><p>Open the CSV file and annotate the language you think the tweet is in for at least 10 examples each of <span class="emphasis"><em>e</em></span> for English and <span class="emphasis"><em>n</em></span> for non-English. There is a <code class="literal">data/disney_e_n.csv</code> file in the distribution; you can use this if you don't want to deal with annotating data. If you are not sure about a tweet, feel free to ignore it. Unannotated data is ignored. Have a look at the following screenshot:</p><div class="mediaobject"><img alt="Getting ready" src="graphics/4672OS_01_01.jpg"/><div class="caption"><p>Screenshot of the spreadsheet with human annotations for English 'e' and non-English 'n'. It is known as truth data or gold standard data because it represents the phenomenon correctly.</p></div></div><p>Often, this data is called <a class="indexterm" id="id82"/>
<span class="strong"><strong>gold standard data</strong></span>, because it represents the truth. The "gold" in "gold standard" is quite literal. Back it up and store it with longevity in mind—it is most likely that it is the single-most valuable collection of bytes on your hard drive, because it is expensive to produce in any quantity and the cleanest articulation of what is being done. Implementations come and go; evaluation data lives on forever. The John Smith corpus from the <span class="emphasis"><em>The John Smith problem</em></span> recipe, in <a class="link" href="ch07.html" title="Chapter 7. Finding Coreference Between Concepts/People">Chapter 7</a>, <span class="emphasis"><em>Finding Coreference Between Concepts/People</em></span>, is the canonical evaluation corpus for that particular problem and lives on as the point of comparison for a line of research that started in 1997. The original implementation is long forgotten.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec21"/>How to do it...</h2></div></div></div><p>Perform the following steps to<a class="indexterm" id="id83"/> evaluate the classifiers:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Enter the following in the command prompt; this will run the default classifier on the texts in the default gold standard data. Then, it will compare the classifier's best category against what was annotated in the <code class="literal">TRUTH</code> column:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/opencsv-2.4.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter1.RunConfusionMatrix</strong></span>
</pre></div></li><li class="listitem">This class will then produce the confusion matrix:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>reference\response</strong></span>
<span class="strong"><strong>      \e,n,</strong></span>
<span class="strong"><strong>      e 11,0,</strong></span>
<span class="strong"><strong>      n 1,9,</strong></span>
</pre></div></li></ol></div><p>The confusion matrix<a class="indexterm" id="id84"/> is aptly named since it confuses almost everyone initially, but it is, without a doubt, the best representation of classifier output, because it is very difficult to hide bad classifier performance with it. In other words, it is an excellent BS detector. It is the unambiguous view of what the classifier got right, what it got wrong, and what it thought was the right answer.</p><p>The sum of each row represents the items that are known by truth/reference/gold standard to belong to the category. For English (e) there were 11 tweets. Each column represents what the system thought was in the same labeled category. For English (e), the system thought 11 tweets were English and none were non-English (n). For the non-English category (n), there are 10 cases in truth, of which the classifier thought 1 was English (incorrectly) and 9 were non-English (correctly). Perfect system performance will have zeros in all the cells that are not located diagonally, from the top-left corner to the bottom-right corner.</p><p>The real reason it is called a confusion matrix is that it is relatively easy to see categories that the classifier is confusing. For example, British English and American English would likely be highly confusable. Also, confusion matrices scale to multiple categories quite nicely, as will be seen later. Visit the Javadoc for a more detailed explanation of the confusion matrix—it is well worth mastering.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec22"/>How it works...</h2></div></div></div><p>Building on the code from<a class="indexterm" id="id85"/> the previous recipes in this chapter, we will focus on what is novel in the evaluation setup. The entirety of the code is in the distribution at <code class="literal">src/com/lingpipe/cookbook/chapter1/RunConfusionMatrix.java</code>. The start of <code class="literal">main()</code> is shown in the following code snippet. The code starts by reading from the arguments that look for non-default CSV data and serialized classifiers. Defaults, which this recipe uses, are shown here:</p><div class="informalexample"><pre class="programlisting">String inputPath = args.length &gt; 0 ? args[0] : "data/disney_e_n.csv";
String classifierPath = args.length &gt; 1 ? args[1] : "models/1LangId.LMClassifier";</pre></div><p>Next, the language model and the <code class="literal">.csv</code> data will be loaded. The method differs slightly from the <code class="literal">Util.CsvRemoveHeader</code> explanation, in that it only accepts rows that have a value in the <code class="literal">TRUTH</code> column—see <code class="literal">src/com/lingpipe/cookbook/Util.java</code> if this is not clear:</p><div class="informalexample"><pre class="programlisting">@SuppressWarnings("unchecked")
BaseClassifier&lt;CharSequence&gt; classifier = (BaseClassifier&lt;CharSequence&gt;) AbstractExternalizable.readObject(new File(classifierPath));

List&lt;String[]&gt; rows = Util.readAnnotatedCsvRemoveHeader(new File(inputPath));</pre></div><p>Next, the categories will be found:</p><div class="informalexample"><pre class="programlisting">String[] categories = Util.getCategories(rows);</pre></div><p>The method will accumulate all the category labels from the <code class="literal">TRUTH</code> column. The code is simple and is shown here:</p><div class="informalexample"><pre class="programlisting">public static String[] getCategories(List&lt;String[]&gt; data) {
  Set&lt;String&gt; categories = new HashSet&lt;String&gt;();
  for (String[] csvData : data) {
    if (!csvData[ANNOTATION_OFFSET].equals("")) {
      categories.add(csvData[ANNOTATION_OFFSET]);
    }
  }
  return categories.toArray(new String[0]);
}</pre></div><p>The code will be useful when we run arbitrary data, where the labels are not known at compile time.</p><p>Then, we will <a class="indexterm" id="id86"/>set up <code class="literal">BaseClassfierEvaluator</code>. This requires the classifier to be evaluated. The categories and a <code class="literal">boolean</code> value that controls whether inputs are stored in the classifier for construction will also be set up:</p><div class="informalexample"><pre class="programlisting">boolean storeInputs = false;
BaseClassifierEvaluator&lt;CharSequence&gt; evaluator = new BaseClassifierEvaluator&lt;CharSequence&gt;(classifier, categories, storeInputs);</pre></div><p>Note that the classifier can be null and specified at a later time; the categories must exactly match those produced by the annotation and the classifier. We will not bother configuring the evaluator to store the inputs, because we are not going to use this capability in this recipe. See the <span class="emphasis"><em>Viewing error categories – false positives</em></span> recipe for an example in which the inputs are stored and accessed.</p><p>Next, we will do the<a class="indexterm" id="id87"/> actual evaluation. The loop will iterate over each row of the information in the <code class="literal">.csv</code> file, build a <code class="literal">Classified&lt;CharSequence&gt;</code>, and pass it off to the evaluator's <code class="literal">handle()</code> method:</p><div class="informalexample"><pre class="programlisting">for (String[] row : rows) {
  String truth = row[Util.ANNOTATION_OFFSET];
  String text = row[Util.TEXT_OFFSET];
  Classification classification = new Classification(truth);
  Classified&lt;CharSequence&gt; classified = new Classified&lt;CharSequence&gt;(text,classification);
  evaluator.handle(classified);
}</pre></div><p>The fourth line will create a classification object with the value from the truth annotation—<span class="emphasis"><em>e</em></span> or <span class="emphasis"><em>n</em></span> in this case. This is the same type as  the one <code class="literal">BaseClassifier&lt;E&gt;</code> returns for the <code class="literal">bestCategory()</code> method. There is no special type for truth annotations. The next line adds in the text that the classification applies to and we get a <code class="literal">Classified&lt;CharSequence&gt;</code> object.</p><p>The last line of the loop will apply the handle method to the created classified object. The evaluator assumes that data supplied to its handle method is a truth annotation, which is handled by extracting the data being classified, applying the classifier to this data, getting the resulting <code class="literal">firstBest()</code> classification, and finally noting whether the classification matches that of what was just constructed with the truth. This happens for each row of the <code class="literal">.csv</code> file.</p><p>Outside the loop, we will print out the confusion matrix with <code class="literal">Util.createConfusionMatrix()</code>:</p><div class="informalexample"><pre class="programlisting">System.out.println(Util.confusionMatrixToString(evaluator.confusionMatrix()));</pre></div><p>Examining this code is left to the reader. That's it; we have evaluated our classifier and printed out the confusion matrix.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec23"/>There's more...</h2></div></div></div><p>The evaluator has a complete <code class="literal">toString()</code> method<a class="indexterm" id="id88"/> that is a bit of a fire hose for information on just how well your classifier did. Those aspects of the output will be covered in later recipes. The Javadoc is quite extensive and well worth reading.</p></div></div>
<div class="section" title="Training your own language model classifier"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec14"/>Training your own language model classifier</h1></div></div></div><p>The world of NLP really opens up when classifiers are customized. This recipe provides details on how to customize a classifier by collecting examples for the classifier to learn from—this is called training data. It is also called gold standard data, truth, or ground truth. We have some from the previous recipe that we will use.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec24"/>Getting ready</h2></div></div></div><p>We will create a<a class="indexterm" id="id89"/> customized language ID classifier for English and other languages. Creation of training data involves getting access to text data and then annotating it for the categories of the classifier—in this case, annotation is the language. Training data can come from a range of sources. Some possibilities include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Gold standard data such as the one created in the preceding evaluation recipe.</li><li class="listitem" style="list-style-type: disc">Data that is somehow already annotated for the categories you care about. For example, Wikipedia has language-specific versions, which make easy pickings to train up a language ID classifier. This is how we created the <code class="literal">3LangId.LMClassifier</code> model.</li><li class="listitem" style="list-style-type: disc">Be creative—where is the data that helps guide a classifier in the right direction?</li></ul></div><p>Language ID doesn't require much data to work well, so 20 tweets per language will start to reliably distinguish strongly different languages. The amount of training data will be driven by evaluation—more data generally improves performance.</p><p>The example assumes that around 10 tweets of English and 10 non-English tweets have been annotated by people and put in <code class="literal">data/disney_e_n.csv</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec25"/>How to do it...</h2></div></div></div><p>In order to train your own language model classifier, perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Fire up a terminal and type the following:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/opencsv-2.4.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter1.TrainAndRunLMClassifier</strong></span>
</pre></div></li><li class="listitem">Then, type some English in the command prompt, perhaps, a Kurt Vonnegut quotation, to see the resulting <code class="literal">JointClassification</code>. See the <span class="emphasis"><em>Getting confidence estimates from a classifier</em></span> recipe for the explanation of the following output:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Type a string to be classified. Empty string to quit.</strong></span>
<span class="strong"><strong>So it goes.</strong></span>
<span class="strong"><strong>Rank Categ Score  P(Category|Input)  log2 P(Category,Input)</strong></span>
<span class="strong"><strong>0=e -4.24592987919 0.9999933712053  -55.19708842949149</strong></span>
<span class="strong"><strong>1=n -5.56922173547 6.62884502334E-6 -72.39988256112824</strong></span>
</pre></div></li><li class="listitem">Type in some non-English, such as the Spanish title of Borge's <span class="emphasis"><em>The Garden of the Forking Paths</em></span>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Type a string to be classified. Empty string to quit.</strong></span>
<span class="strong"><strong>El Jardín de senderos que se bifurcan </strong></span>
<span class="strong"><strong>Rank Categ Score  P(Category|Input)  log2 P(Category,Input)</strong></span>
<span class="strong"><strong>0=n -5.6612148689 0.999989087229795 -226.44859475801326</strong></span>
<span class="strong"><strong>1=e -6.0733050528 1.091277041753E-5 -242.93220211249715</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec26"/>How it works...</h2></div></div></div><p>The program<a class="indexterm" id="id90"/> is in <code class="literal">src/com/lingpipe/cookbook/chapter1/TrainAndRunLMClassifier.java</code>; the contents of the <code class="literal">main()</code> method start with:</p><div class="informalexample"><pre class="programlisting">String dataPath = args.length &gt; 0 ? args[0] : "data/disney_e_n.csv";
List&lt;String[]&gt; annotatedData = Util.readAnnotatedCsvRemoveHeader(new File(dataPath));
String[] categories = Util.getCategories(annotatedData);</pre></div><p>The preceding code gets the contents of the <code class="literal">.csv</code> file and then extracts the list of categories that were annotated; these categories will be all the non-empty strings in the annotation column.</p><p>The following <code class="literal">DynamicLMClassifier</code> is created using a static method that requires the array of categories and <code class="literal">int</code>, which is the order of the language models. With an order of 3, the language model will be trained on all 1 to 3 character sequences of the text training data. So "I luv Disney" will produce training instances of "I", "I ", "I l", " l", " lu", "u", "uv", "luv", and so on. The <code class="literal">createNGramBoundary</code> method appends a special token to the beginning and end of each text sequence; this token can help if the beginnings or ends are informative for classification. Most text data is sensitive to beginnings/ends, so we will choose this model:</p><div class="informalexample"><pre class="programlisting">int maxCharNGram = 3;
DynamicLMClassifier&lt;NGramBoundaryLM&gt; classifier = DynamicLMClassifier.createNGramBoundary(categories,maxCharNGram);</pre></div><p>The following code iterates over the rows of training data and creates <code class="literal">Classified&lt;CharSequence&gt;</code> in the same way as shown in the <span class="emphasis"><em>Evaluation of classifiers – the confusion matrix</em></span> recipe for evaluation. However, instead of passing the <code class="literal">Classified</code> object to an evaluation handler, it is used to train the classifier.</p><div class="informalexample"><pre class="programlisting">for (String[] row: annotatedData) {
  String truth = row[Util.ANNOTATION_OFFSET];
  String text = row[Util.TEXT_OFFSET];
  Classification classification 
    = new Classification(truth);
  Classified&lt;CharSequence&gt; classified = new Classified&lt;CharSequence&gt;(text,classification);
  classifier.handle(classified);
}</pre></div><p>No further <a class="indexterm" id="id91"/>steps are necessary, and the classifier is ready for use by the console:</p><div class="informalexample"><pre class="programlisting">Util.consoleInputPrintClassification(classifier);</pre></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec27"/>There's more...</h2></div></div></div><p>Training and using the classifier can be interspersed for classifiers based on <code class="literal">DynamicLM</code>. This is generally not the case with other classifiers such as <code class="literal">LogisticRegression</code>, because they use all the data to compile a model that can carry out classifications.</p><p>There is another method for training the classifier that gives you more control over how the training goes. The following is the code snippet for this:</p><div class="informalexample"><pre class="programlisting">Classification classification = new Classification(truth);
Classified&lt;CharSequence&gt; classified = new Classified&lt;CharSequence&gt;(text,classification);
classifier.handle(classified);</pre></div><p>Alternatively, we can have the same effect with:</p><div class="informalexample"><pre class="programlisting">int count = 1;
classifier.train(truth,text,count);</pre></div><p>The <code class="literal">train()</code> method<a class="indexterm" id="id92"/> allows an extra degree of control for training, because it allows for the count to be explicitly set. As we explore LingPipe classifiers, we will often see an alternate way of training that allows for some additional control beyond what the <code class="literal">handle()</code> method<a class="indexterm" id="id93"/> provides.</p><p>Character-language model-based classifiers work very well for tasks where character sequences are distinctive. Language identification is an ideal candidate for this, but it can also be used for tasks such as sentiment, topic assignment, and question answering.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec28"/>See also</h2></div></div></div><p>The Javadoc for LingPipe's classifiers are quite extensive on the underlying math that drives the technology.</p></div></div>
<div class="section" title="How to train and evaluate with cross validation"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec15"/>How to train and evaluate with cross validation</h1></div></div></div><p>The earlier <a class="indexterm" id="id94"/>recipes have shown how to evaluate classifiers<a class="indexterm" id="id95"/> with truth data and how to train with truth data but how<a class="indexterm" id="id96"/> about doing both? This great idea is called <a class="indexterm" id="id97"/>cross validation, and it works as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Split the data into <span class="emphasis"><em>n</em></span> distinct sets or folds—the standard <span class="emphasis"><em>n</em></span> is 10.</li><li class="listitem">For <span class="emphasis"><em>i</em></span> from 1 to <span class="emphasis"><em>n</em></span>:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Train on the <span class="emphasis"><em>n - 1</em></span> folds defined by the exclusion of fold <span class="emphasis"><em>i</em></span></li><li class="listitem" style="list-style-type: disc">Evaluate on fold <span class="emphasis"><em>i</em></span></li></ul></div></li><li class="listitem">Report the evaluation results across all folds <span class="emphasis"><em>i</em></span>.</li></ol></div><p>This is how most machine-learning systems are tuned for performance. The work flow is as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">See what the cross validation performance is.</li><li class="listitem">Look at the error as determined by an evaluation metric.</li><li class="listitem">Look at the actual errors—yes, the data—for insights into how the system can be improved.</li><li class="listitem">Make some changes</li><li class="listitem">Evaluate it again.</li></ol></div><p>Cross validation<a class="indexterm" id="id98"/> is an excellent way to compare different approaches to a problem, try different classifiers, motivate normalization approaches, explore feature<a class="indexterm" id="id99"/> enhancements, and so on. Generally, a system <a class="indexterm" id="id100"/>configuration that shows increased performance<a class="indexterm" id="id101"/> on cross validation will also show <a class="indexterm" id="id102"/>increased performance on new data. What cross validation does not do, particularly with active learning strategies discussed later, is reliably predict performance on new data. Always apply the classifier to new data before releasing production systems as a final sanity check. You have been warned.</p><p>Cross validation also imposes a negative bias compared to a classifier trained on all possible training data, because each fold is a slightly weaker classifier, in that it only has 90 percent of the data on 10 folds.</p><p>
<span class="emphasis"><em>Rinse, lather, and repeat</em></span> is the mantra of building state-of-the-art NLP systems.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec29"/>Getting ready</h2></div></div></div><p>Note how different this approach is from other classic computer-engineering approaches that focus on developing against a functional specification driven by unit tests. This process is more about refining and adjusting the code to work better as determined by the evaluation metrics.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec30"/>How to do it...</h2></div></div></div><p>To run the code, perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Get to a command prompt and type:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/opencsv-2.4.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter1.RunXValidate</strong></span>
</pre></div></li><li class="listitem">The result will be:<div class="informalexample"><pre class="programlisting">Training data is: data/disney_e_n.csv
Training on fold 0
Testing on fold 0
Training on fold 1
Testing on fold 1
Training on fold 2
Testing on fold 2
Training on fold 3
Testing on fold 3
reference\response
    \e,n,
    e 10,1,
    n 6,4,</pre></div><p>The preceding output will make more sense in the following section.</p></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec31"/>How it works…</h2></div></div></div><p>This recipe<a class="indexterm" id="id103"/> introduces an <code class="literal">XValidatingObjectCorpus</code> object that <a class="indexterm" id="id104"/>manages cross<a class="indexterm" id="id105"/> validation. It is used heavily in training <a class="indexterm" id="id106"/>classifiers. Everything else should be familiar from the previous recipes. The <code class="literal">main()</code> method starts with:</p><div class="informalexample"><pre class="programlisting">String inputPath = args.length &gt; 0 ? args[0] : "data/disney_e_n.csv";
System.out.println("Training data is: " + inputPath);
List&lt;String[]&gt; truthData = Util.readAnnotatedCsvRemoveHeader(new File(inputPath));</pre></div><p>The preceding code gets us the data from the default or a user-entered file. The next two lines introduce <code class="literal">XValidatingObjectCorpus</code>—the star of this recipe:</p><div class="informalexample"><pre class="programlisting">int numFolds = 4;
XValidatingObjectCorpus&lt;Classified&lt;CharSequence&gt;&gt; corpus = Util.loadXValCorpus(truthData, numFolds);</pre></div><p>The <code class="literal">numFolds</code> variable controls how the data that is just loaded will be partitioned—it will be in four partitions in this case. Now, we will look at the <code class="literal">Util.loadXValCorpus(truthData, numfolds)</code> subroutine:</p><div class="informalexample"><pre class="programlisting">public static XValidatingObjectCorpus&lt;Classified&lt;CharSequence&gt;&gt; loadXValCorpus(List&lt;String[]&gt; rows, int numFolds) throws IOException {
  XValidatingObjectCorpus&lt;Classified&lt;CharSequence&gt;&gt; corpus = new XValidatingObjectCorpus&lt;Classified&lt;CharSequence&gt;&gt;(numFolds);
  for (String[] row : rows) {
    Classification classification = new Classification(row[ANNOTATION_OFFSET]);
    Classified&lt;CharSequence&gt; classified = new Classified&lt;CharSequence&gt;(row[TEXT_OFFSET],classification);
    corpus.handle(classified);
  }
  return corpus;
}</pre></div><p>
<code class="literal">XValidatingObjectCorpus&lt;E&gt;</code> constructed will contain all the truth data in the form of <code class="literal">Objects E</code>. In this<a class="indexterm" id="id107"/> case, we are filling the corpus with the same object <a class="indexterm" id="id108"/>used to train and evaluate in the <a class="indexterm" id="id109"/>previous recipes in this chapter—<code class="literal">Classified&lt;CharSequence&gt;</code>. This will be handy, because we will be using the objects to both train and test our classifier. The <code class="literal">numFolds</code> parameter specifies how many partitions of the data to make. It can be changed later.</p><p>The following <code class="literal">for</code> loop<a class="indexterm" id="id110"/> should be familiar, in that, it should iterate over all the annotated data and creates the <code class="literal">Classified&lt;CharSequence&gt;</code> object before applying the <code class="literal">corpus.handle()</code> method, which adds it to the corpus. Finally, we will return the corpus. It is worth taking a look at the Javadoc for <code class="literal">XValidatingObjectCorpus&lt;E&gt;</code> if you have any questions.</p><p>Returning to the body of <code class="literal">main()</code>, we will permute the corpus to mix the data, get the categories, and set up <code class="literal">BaseClassifierEvaluator&lt;CharSequence&gt;</code> with a null value where we supplied a classifier in a previous recipe:</p><div class="informalexample"><pre class="programlisting">corpus.permuteCorpus(new Random(123413));
String[] categories = Util.getCategories(truthData);
boolean storeInputs = false;
BaseClassifierEvaluator&lt;CharSequence&gt; evaluator = new BaseClassifierEvaluator&lt;CharSequence&gt;(null, categories, storeInputs);</pre></div><p>Now, we are ready to do the cross validation:</p><div class="informalexample"><pre class="programlisting">int maxCharNGram = 3;
for (int i = 0; i &lt; numFolds; ++i) {
  corpus.setFold(i);
  DynamicLMClassifier&lt;NGramBoundaryLM&gt; classifier = DynamicLMClassifier.createNGramBoundary(categories, maxCharNGram);
  System.out.println("Training on fold " + i);
  corpus.visitTrain(classifier);
  evaluator.setClassifier(classifier);
  System.out.println("Testing on fold " + i);
  corpus.visitTest(evaluator);
}</pre></div><p>On each iteration of the <code class="literal">for</code> loop, we will set which fold is being used, which, in turn, will select the training and testing partition. Then, we will construct <code class="literal">DynamicLMClassifier</code> and train it by supplying the classifier to <code class="literal">corpus.visitTrain(classifier)</code>. Next, we will set the evaluator's classifier to the one we just trained. The <a class="indexterm" id="id111"/>evaluator is passed to the <code class="literal">corpus.visitTest(evaluator)</code> method where the contained classifier is applied to the test data that it<a class="indexterm" id="id112"/> was not trained on. With four folds, 25<a class="indexterm" id="id113"/> percent of the data will be test data at any <a class="indexterm" id="id114"/>given iteration, and 75 percent of the data will be training data. Data will be in the test partition exactly once and three times in the training. The training and test partitions will never contain the same data unless there are duplicates in the data.</p><p>Once the loop has finished all iterations, we will print a confusion matrix discussed in the <span class="emphasis"><em>Evaluation of classifiers – the confusion matrix</em></span> recipe:</p><div class="informalexample"><pre class="programlisting">System.out.println(
  Util.confusionMatrixToString(evaluator.confusionMatrix()));</pre></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec32"/>There's more…</h2></div></div></div><p>This recipe introduces quite a few moving parts, namely, cross validation and a corpus object that supports it. The <code class="literal">ObjectHandler&lt;E&gt;</code> interface is also used a lot; this can be confusing to developers not familiar with the pattern. It is used to train and test the classifier. It can also be used to print the contents of the corpus. Change the contents of the <code class="literal">for</code> loop to <code class="literal">visitTrain</code> with <code class="literal">Util.corpusPrinter</code>:</p><div class="informalexample"><pre class="programlisting">System.out.println("Training on fold " + i);
corpus.visitTrain(Util.corpusPrinter());
corpus.visitTrain(classifier);
evaluator.setClassifier(classifier);
System.out.println("Testing on fold " + i);
corpus.visitTest(Util.corpusPrinter());</pre></div><p>Now, you will get an output that looks like:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Training on fold 0</strong></span>
<span class="strong"><strong>Malis?mos los nuevos dibujitos de disney, nickelodeon, cartoon, etc, no me gustannn:n</strong></span>
<span class="strong"><strong>@meeelp mas que venha um filhinho mais fofo que o pr?prio pai, com covinha e amando a Disney kkkkkkkkkkkkkkkkk:n</strong></span>
<span class="strong"><strong>@HedyHAMIDI au quartier pas a Disney moi:n</strong></span>
<span class="strong"><strong>I fully love the Disney Channel I do not care ?:e</strong></span>
</pre></div><p>The text is followed by <code class="literal">:</code> and the category. Printing the training/test folds is a good sanity check for whether the corpus is properly populated. It is also a nice glimpse into how the <code class="literal">ObjectHandler&lt;E&gt;</code> interface works—here, the source is from <code class="literal">com/lingpipe/cookbook/Util.java</code>:</p><div class="informalexample"><pre class="programlisting">public static ObjectHandler&lt;Classified&lt;CharSequence&gt;&gt; corpusPrinter () {
  return new ObjectHandler&lt;Classified&lt;CharSequence&gt;&gt;() {
    @Override
    public void handle(Classified&lt;CharSequence&gt; e) {
      System.out.println(e.toString());
    }
  };
}</pre></div><p>There is not much to <a class="indexterm" id="id115"/>the returned class. There is a single <code class="literal">handle()</code>method that just prints the <code class="literal">toString()</code> method of <code class="literal">Classified&lt;CharSequence&gt;</code>. In the context of this recipe, the classifier instead invokes <code class="literal">train()</code> on the text and <a class="indexterm" id="id116"/>classification, and the<a class="indexterm" id="id117"/> evaluator takes the text, runs it past the classifier, and compares the result to the truth.</p><p>Another good experiment to <a class="indexterm" id="id118"/>run is to report performance on each fold instead of all folds. For small datasets, you will see very large variations in performance. Another worthwhile experiment is to permute the corpus 10 times and see the variations in performance that come from different partitioning of the data.</p><p>Another issue is how data is selected for evaluation. To text process applications, it is important to not leak information between test data and training data. Cross validation over 10 days of data will be much more realistic if each day is a fold rather than a 10-percent slice of all 10 days. The reason is that a day's data will likely be correlated, and this correlation will produce information about that day in training and testing, if days are allowed to be in both train and test. When evaluating the final performance, always select data from after the training data epoch if possible, to better emulate production environments where the future is not known.</p></div></div>
<div class="section" title="Viewing error categories &#x2013; false positives"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec16"/>Viewing error categories – false positives</h1></div></div></div><p>We can achieve the best <a class="indexterm" id="id119"/>possible classifier performance by examining the errors and making changes to the system. There is a very bad habit among developers and machine-learning folks to not look at errors, particularly as systems mature. Just to be clear, at the end of a project, the developers responsible for tuning the classifier should be very familiar with the domain being classified, if not expert in it, because they have looked at so much data while tuning the system. If the developer cannot do a reasonable job of emulating the classifiers that you are tuning, then you are not looking at enough data.</p><p>This recipe performs the most basic form of looking at what the system got wrong in the form of false positives, which are examples from training data that the classifier assigned to a category, but the correct category was something else.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec33"/>How to do it...</h2></div></div></div><p>Perform the following steps in<a class="indexterm" id="id120"/> order to view error categories using false positives:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">This recipe extends the previous <span class="emphasis"><em>How to train and evaluate with cross validation</em></span> recipe by accessing more of what the evaluation class provides. Get a command prompt and type:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/opencsv-2.4.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter1.ReportFalsePositivesOverXValidation</strong></span>
</pre></div></li><li class="listitem">This will result in:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Training data is: data/disney_e_n.csv</strong></span>
<span class="strong"><strong>reference\response</strong></span>
<span class="strong"><strong>          \e,n,</strong></span>
<span class="strong"><strong>         e 10,1,</strong></span>
<span class="strong"><strong>         n 6,4,</strong></span>
<span class="strong"><strong>False Positives for e</strong></span>
<span class="strong"><strong>Malisímos los nuevos dibujitos de disney, nickelodeon, cartoon, etc, no me gustannn : n</strong></span>
<span class="strong"><strong>@meeelp mas que venha um filhinho mais fofo que o próprio pai, com covinha e amando a Disney kkkkkkkkkkkkkkkkk : n</strong></span>
<span class="strong"><strong>@HedyHAMIDI au quartier pas a Disney moi : n</strong></span>
<span class="strong"><strong>@greenath_ t'as de la chance d'aller a Disney putain j'y ai jamais été moi. : n</strong></span>
<span class="strong"><strong>Prefiro gastar uma baba de dinheiro pra ir pra cancun doq pra Disney por exemplo : n</strong></span>
<span class="strong"><strong>ES INSUPERABLE DISNEY !! QUIERO VOLVER:( : n</strong></span>
<span class="strong"><strong>False Positives for n</strong></span>
<span class="strong"><strong>request now "let's get tricky" by @bellathorne and @ROSHON on @radiodisney!!! just call 1-877-870-5678 or at http://t.co/cbne5yRKhQ!! &lt;3 : e</strong></span>
</pre></div></li><li class="listitem">The output starts<a class="indexterm" id="id121"/> with a confusion matrix. Then, we will see the actual six instances of false positives for <code class="literal">p</code> from the lower left-hand side cell of the confusion matrix labeled with the category that the classifier guessed. Then, we will see false positives for <code class="literal">n</code>, which is a single example. The true category is appended with <code class="literal">:</code>, which is helpful for classifiers that have more than two categories.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec34"/>How it works…</h2></div></div></div><p>This recipe is based on the previous one, but it has its own source in <code class="literal">com/lingpipe/cookbook/chapter1/ReportFalsePositivesOverXValidation.java</code>. There are two differences. First, <code class="literal">storeInputs</code> is set to <code class="literal">true</code> for the evaluator:</p><div class="informalexample"><pre class="programlisting">boolean storeInputs = true;
BaseClassifierEvaluator&lt;CharSequence&gt; evaluator = new BaseClassifierEvaluator&lt;CharSequence&gt;(null, categories, storeInputs);</pre></div><p>Second, a <code class="literal">Util</code> method is added to print false positives:</p><div class="informalexample"><pre class="programlisting">for (String category : categories) {
  Util.printFalsePositives(category, evaluator, corpus);
}</pre></div><p>The preceding code works by identifying a category of focus—<code class="literal">e</code> or English tweets—and extracting all the false positives from the classifier evaluator. For this category, false positives are tweets that are non-English in truth, but the classifier thought they were English. The referenced <code class="literal">Util</code> method is as follows:</p><div class="informalexample"><pre class="programlisting">public static &lt;E&gt; void printFalsePositives(String category, BaseClassifierEvaluator&lt;E&gt; evaluator, Corpus&lt;ObjectHandler&lt;Classified&lt;E&gt;&gt;&gt; corpus) throws IOException {
  final Map&lt;E,Classification&gt; truthMap = new HashMap&lt;E,Classification&gt;();
  corpus.visitCorpus(new ObjectHandler&lt;Classified&lt;E&gt;&gt;() {
    @Override
    public void handle(Classified&lt;E&gt; data) {
      truthMap.put(data.getObject(),data.getClassification());
    }
  });</pre></div><p>The preceding code takes the corpus that contains all the truth data and populates <code class="literal">Map&lt;E,Classification&gt;</code> to allow for lookup of the truth annotation, given the input. If the same input exists in two categories, then this method will not be robust but will record the last example seen:</p><div class="informalexample"><pre class="programlisting">List&lt;Classified&lt;E&gt;&gt; falsePositives = evaluator.falsePositives(category);
System.out.println("False Positives for " + category);
for (Classified&lt;E&gt; classified : falsePositives) {
  E data = classified.getObject();
  Classification truthClassification = truthMap.get(data);
  System.out.println(data + " : " + truthClassification.bestCategory());
  }
}</pre></div><p>The code gets the false positives from the evaluator and then iterates over all them with a lookup into <code class="literal">truthMap</code> built in the preceding code and prints out the relevant information. There are also methods to get false negatives, true positives, and true negatives in <code class="literal">evaluator</code>.</p><p>The ability to identify mistakes is crucial to improving performance. The advice seems obvious, but it is very common for developers to not look at mistakes. They will look at system output and make a rough estimate of whether the system is good enough; this does not result in top-performing classifiers.</p><p>The next recipe works <a class="indexterm" id="id122"/>through more evaluation metrics and their definition.</p></div></div>
<div class="section" title="Understanding precision and recall"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec17"/>Understanding precision and recall</h1></div></div></div><p>The false positive from the <a class="indexterm" id="id123"/>preceding recipe is one of the four possible error categories. All the <a class="indexterm" id="id124"/>categories and their interpretations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">For a given category X:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>True positive</strong></span>: The classifier guessed X, and the true category is X</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>False positive</strong></span>: The classifier guessed X, but the true category is a category that is different from X</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>True negative</strong></span>: The classifier guessed a category that is different from X, and the true category is different from X</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>False negative</strong></span>: The classifier guessed a category different from X, but the true category is X</li></ul></div></li></ul></div><p>With these definitions in hand, we can define the additional common evaluation metrics as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Precision for a category X is true positive / (false positive + true positive)<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The degenerate case is to make one very confident guess for 100 percent precision. This minimizes the false positives but will have a horrible recall.</li></ul></div></li><li class="listitem" style="list-style-type: disc">Recall or sensitivity for a category X is true positive / (false negative + true positive)<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The degenerate case is to guess all the data as belonging to category X for 100 percent recall. This minimizes false negatives but will have horrible precision.</li></ul></div></li><li class="listitem" style="list-style-type: disc">Specificity for a category X is true negative / (true negative + false positive)<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The degenerate case is to guess that all data is not in category X.</li></ul></div></li></ul></div><p>The degenerate cases are provided to make clear what the metric is focused on. There are metrics such as f-measure that balance precision and recall, but even then, there is no inclusion of true negatives, which can be highly informative. See the Javadoc at <code class="literal">com.aliasi.classify.PrecisionRecallEvaluation</code> for more details on evaluation.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">In our experience, most business needs map to one of the three scenarios:</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>High precision</strong></span> / <span class="strong"><strong>high recall</strong></span>: The<a class="indexterm" id="id125"/> language ID needs to have both good coverage and good accuracy; otherwise, lots of stuff will go wrong. Fortunately, for distinct languages where a mistake will be costly (such as Japanese versus English or English versus Spanish), the LM classifiers perform quite well.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>High precision</strong></span> / <span class="strong"><strong>usable recall</strong></span>: Most<a class="indexterm" id="id126"/> business use cases have this shape. For example, a search engine that automatically changes a query if it is misspelled better not make lots of mistakes. This means it looks pretty bad to change "Breck Baldwin" to "Brad Baldwin", but no one really notices if  "Bradd Baldwin" is not corrected.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>High recall</strong></span> / <span class="strong"><strong>usable precision</strong></span>: Intelligence analysis looking for a particular needle in a haystack will tolerate a lot of false positives in support of finding the intended target. This was an early lesson from our DARPA days.</li></ul></div></div>
<div class="section" title="How to serialize a LingPipe object &#x2013; classifier example"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec18"/>How to serialize a LingPipe object – classifier example</h1></div></div></div><p>In a deployment situation, trained<a class="indexterm" id="id127"/> classifiers, other Java objects with complex configuration, or training are best accessed by deserializing them from a disk. The first recipe did exactly this by reading in <code class="literal">LMClassifier</code> from the disk with <code class="literal">AbstractExternalizable</code>. This recipe shows how to get the language ID classifier written out to the disk for later use.</p><p>Serializing <code class="literal">DynamicLMClassiﬁer</code> and reading it back in results in a different class, which is an instance of <code class="literal">LMClassifier</code> that performs the same as the one just trained except that it can no longer accept training instances because counts have been converted to log probabilities and the backoff smoothing arcs are stored in suffix trees. The resulting classifier is much faster.</p><p>In general, most of the LingPipe classifiers, language models, and <span class="strong"><strong>hidden Marcov models</strong></span> (<span class="strong"><strong>HMM</strong></span>)<a class="indexterm" id="id128"/> implement both the <code class="literal">Serializable</code> and <code class="literal">Compilable</code> interfaces.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec35"/>Getting ready</h2></div></div></div><p>We will work with the same data as we did in the <span class="emphasis"><em>Viewing error categories – false positives</em></span> recipe.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec36"/>How to do it...</h2></div></div></div><p>Perform the following steps to serialize a LingPipe object:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Go to the command prompt and convey:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/opencsv-2.4.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter1.TrainAndWriteClassifierToDisk</strong></span>
</pre></div></li><li class="listitem">The program will respond with the default file values for input/output:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Training on data/disney_e_n.csv</strong></span>
<span class="strong"><strong>Wrote model to models/my_disney_e_n.LMClassifier</strong></span>
</pre></div></li><li class="listitem">Test if the model works by invoking the <span class="emphasis"><em>Deserializing and running a classifier</em></span> recipe while specifying the classifier file to be read in:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter1.LoadClassifierRunOnCommandLine models/my_disney_e_n.LMClassifier</strong></span>
</pre></div></li><li class="listitem">The usual interaction follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Type a string to be classified. Empty string to quit.</strong></span>
<span class="strong"><strong>The rain in Spain</strong></span>
<span class="strong"><strong>Best Category: e </strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec37"/>How it works…</h2></div></div></div><p>The contents<a class="indexterm" id="id129"/> of <code class="literal">main()</code> from <code class="literal">src/com/lingpipe/cookbook/chapter1/ TrainAndWriteClassifierToDisk.java</code> start with the materials covered in the previous recipes of the chapter to read the <code class="literal">.csv</code> files, set up a classifier, and train it. Please refer back to it if any code is unclear.</p><p>The new bit for this recipe happens when we invoke the <code class="literal">AbtractExternalizable.compileTo()</code> method on <code class="literal">DynamicLMClassifier</code>, which compiles the model and writes it to a file. This method is used like the <code class="literal">writeExternal</code> method from Java's <code class="literal">Externalizable</code> interface:</p><div class="informalexample"><pre class="programlisting">AbstractExternalizable.compileTo(classifier,outFile);</pre></div><p>This is all you need to know folks to write a classifier to a disk.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec38"/>There's more…</h2></div></div></div><p>There is an alternate way to serialize that is amenable to more variations of data sources for serializations that are not based on the <code class="literal">File</code> class. An alternate way to write a classifier is:</p><div class="informalexample"><pre class="programlisting">FileOutputStream fos = new FileOutputStream(outFile);
ObjectOutputStream oos = new ObjectOutputStream(fos);
classifier.compileTo(oos);
oos.close();
fos.close();</pre></div><p>Additionally, <code class="literal">DynamicLM</code> can be compiled without involving the disk with a static <code class="literal">AbstractExternalizable.compile()</code> method. It will be used in the following fashion:</p><div class="informalexample"><pre class="programlisting">@SuppressWarnings("unchecked")
LMClassifier&lt;LanguageModel, MultivariateDistribution&gt; compiledLM = (LMClassifier&lt;LanguageModel, MultivariateDistribution&gt;) AbstractExternalizable.compile(classifier);</pre></div><p>The compiled version is a lot faster but does not allow further training instances.</p></div></div>
<div class="section" title="Eliminate near duplicates with the Jaccard distance"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec19"/>Eliminate near duplicates with the Jaccard distance</h1></div></div></div><p>It often happens that the data <a class="indexterm" id="id130"/>has duplicates or near duplicates that <a class="indexterm" id="id131"/>should be filtered. Twitter data has lots of duplicates that can be quite frustrating to work with even with the <code class="literal">-filter:retweets</code> option available for the search API. A quick way to see this is to sort the text in the spreadsheet, and tweets with common prefixes will be neighbors:</p><div class="mediaobject"><img alt="Eliminate near duplicates with the Jaccard distance" src="graphics/4672OS_01_02.jpg"/><div class="caption"><p>Duplicate tweets that share a prefix</p></div></div><p>This sort only reveals shared prefixes; there are many more that don't share a prefix. This recipe will allow you to find other sources of overlap and threshold, the point at which duplicates are removed.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec39"/>How to do it…</h2></div></div></div><p>Perform the following steps to eliminate near duplicates with the Jaccard distance:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Type in the command prompt:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/opencsv-2.4.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter1.DeduplicateCsvData</strong></span>
</pre></div></li><li class="listitem">You will be overwhelmed with a torrent of text:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Tweets too close, proximity 1.00</strong></span>
<span class="strong"><strong>   @britneyspears do you ever miss the Disney days? and iilysm   please follow me. kiss from Turkey #AskBritneyJean ??</strong></span>
<span class="strong"><strong>  @britneyspears do you ever miss the Disney days? and iilysm please follow me. kiss from Turkey #AskBritneyJean ??? </strong></span>
<span class="strong"><strong>Tweets too close, proximity 0.50</strong></span>
<span class="strong"><strong>  Sooo, I want to have a Disney Princess movie night....</strong></span>
<span class="strong"><strong>  I just want to be a Disney Princess</strong></span>
</pre></div></li><li class="listitem">Two example outputs are shown—the first is a near-exact duplicate with only a difference in a final <code class="literal">?</code>. It has a proximity of <code class="literal">1.0</code>; the next example has proximity of <code class="literal">0.50</code>, and the tweets are different but have a good deal of word overlap. Note that the second case does not share a prefix.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec40"/>How it works…</h2></div></div></div><p>This recipe<a class="indexterm" id="id132"/> jumps a bit ahead of the sequence, using a<a class="indexterm" id="id133"/> tokenizer to drive the deduplication process. It is here because the following recipe, for sentiment, really needs deduplicated data to work well. <a class="link" href="ch02.html" title="Chapter 2. Finding and Working with Words">Chapter 2</a>, <span class="emphasis"><em>Finding and Working with Words</em></span>, covers tokenization in detail.</p><p>The source for <code class="literal">main()</code> is:</p><div class="informalexample"><pre class="programlisting">String inputPath = args.length &gt; 0 ? args[0] : "data/disney.csv";
String outputPath = args.length &gt; 1 ? args[1] : "data/disneyDeduped.csv";  
List&lt;String[]&gt; data = Util.readCsvRemoveHeader(new File(inputPath));
System.out.println(data.size());</pre></div><p>There is nothing new in the preceding code snippet, but the following code snippet has <code class="literal">TokenizerFactory</code>:</p><div class="informalexample"><pre class="programlisting">TokenizerFactory tokenizerFactory = new RegExTokenizerFactory("\\w+");</pre></div><p>Briefly, the tokenizer breaks the text into text sequences defined by matching the regular expression <code class="literal">\w+</code> (the first <code class="literal">\</code> escapes the second one in the preceding code—it is a Java thing). It matches contiguous word characters. The string "Hi, you here??" produces tokens "Hi", "you", and "here". The punctuation is ignored.</p><p>Next up, <code class="literal">Util.filterJaccard</code> is called with a cutoff of <code class="literal">.5</code>, which roughly eliminates tweets that overlap with half their words. Then, the filter data is written to disk:</p><div class="informalexample"><pre class="programlisting">double cutoff = .5;
List&lt;String[]&gt; dedupedData = Util.filterJaccard(data, tokenizerFactory, cutoff);
System.out.println(dedupedData.size());
Util.writeCsvAddHeader(dedupedData, new File(outputPath));
}</pre></div><p>The <code class="literal">Util.filterJaccard()</code> method's source is as follows:</p><div class="informalexample"><pre class="programlisting">public static List&lt;String[]&gt; filterJaccard(List&lt;String[]&gt; texts, TokenizerFactory tokFactory, double cutoff) {
  JaccardDistance jaccardD = new JaccardDistance(tokFactory);</pre></div><p>In the <a class="indexterm" id="id134"/>preceding snippet, a <code class="literal">JaccardDistance</code> class <a class="indexterm" id="id135"/>is constructed with a tokenizer factory. The Jaccard distance divides the intersection of tokens from the two strings over the union of tokens from both strings. Look at the Javadoc for more information.</p><p>The nested <code class="literal">for</code> loops in the following example explore each row with every other row until a higher threshold proximity is found or until all data has been looked at. Do not use this for large datasets because it is the O(n<sup>2</sup>)algorithm. If no row is above proximity, then the row is added to <code class="literal">filteredTexts</code>:</p><div class="informalexample"><pre class="programlisting">List&lt;String[]&gt; filteredTexts = new ArrayList&lt;String[]&gt;();
for (int i = 0; i &lt; texts.size(); ++i) {
  String targetText = texts.get(i)[TEXT_OFFSET];
  boolean addText = true;
  for (int j = i + 1; j &lt; texts.size(); ++j ) {
    String comparisionText = texts.get(j)[TEXT_OFFSET];
    double proximity = jaccardD.proximity(targetText,comparisionText);
    if (proximity &gt;= cutoff) {
      addText = false;
      System.out.printf(" Tweets too close, proximity %.2f\n", proximity);
      System.out.println("\t" + targetText);
      System.out.println("\t" + comparisionText);
      break;
    }
  }
  if (addText) {
    filteredTexts.add(texts.get(i));
  }
}
return filteredTexts;
}</pre></div><p>There are much better ways to efficiently filter the texts at a cost of extra complexity—a simple reverse-word lookup index to compute an initial covering set will be vastly more efficient—search for a shingling text lookup for O(n) to O(n log(n)) approaches.</p><p>Setting<a class="indexterm" id="id136"/> the threshold can be a bit tricky, but looking a <a class="indexterm" id="id137"/>bunch of data should make the appropriate cutoff fairly clear for your needs.</p></div></div>
<div class="section" title="How to classify sentiment &#x2013; simple version"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec20"/>How to classify sentiment – simple version</h1></div></div></div><p>Sentiment has<a class="indexterm" id="id138"/> become the classic business-oriented classification task—what executive can resist an ability to know on a constant basis what positive and negative things are being said about their business? Sentiment classifiers offer this capability by taking text data and classifying it into positive and negative categories. This recipe addresses the process of creating a simple sentiment classifier, but more generally, it addresses how to create classifiers for novel categories. It is also a 3-way classifier, unlike the 2-way classifiers we have been working with.</p><p>Our first sentiment system was built for BuzzMetrics in 2004 using language model classifiers. We tend to use logistic regression classifiers now, because they tend to perform better. <a class="link" href="ch03.html" title="Chapter 3. Advanced Classifiers">Chapter 3</a>, <span class="emphasis"><em>Advanced Classifiers</em></span>, covers logistic regression classifiers.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec41"/>How to do it…</h2></div></div></div><p>The previous recipes focused on language ID—how do we shift the classifier over to the very different task of sentiment? This will be much simpler than one might think—all that needs to change is the training data, believe it or not. The steps are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Use the Twitter search recipe to download tweets about a topic that has positive/negative tweets about it. A search on <code class="literal">disney</code> is our example, but feel free to branch out. This recipe will work with the supplied CSV file, <code class="literal">data/disneySentiment_annot.csv</code>.</li><li class="listitem">Load the created <code class="literal">data/disneySentiment_annot.csv</code> file into your spreadsheet of choice. There are already some annotations done.</li><li class="listitem">As in the <span class="emphasis"><em>Evaluation of classifiers – the confusion matrix</em></span> recipe, annotate the <code class="literal">true class</code> column for one of the three categories:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">p</code> annotation stands for positive. The example is "Oh well, I love Disney movies. #hateonit".</li><li class="listitem" style="list-style-type: disc">The <code class="literal">n</code> annotation stands for negative. The example is "Disney really messed me up yo, this is not the way things are suppose to be".</li><li class="listitem" style="list-style-type: disc">The <code class="literal">o</code> annotation stands for other. The example is "Update on Downtown Disney. <a class="ulink" href="http://t.co/SE39z73vnw">http://t.co/SE39z73vnw</a>.</li><li class="listitem" style="list-style-type: disc">Leave blank tweets that are not in English, irrelevant, both positive and negative, or you are unsure about.</li></ul></div></li><li class="listitem">Keep annotating until the smallest category has at least 10 examples.</li><li class="listitem">Save the annotations.</li><li class="listitem">Run the previous recipe for cross validation, providing the annotated file's name:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter1.RunXValidate data/disneyDedupedSentiment.csv</strong></span>
</pre></div></li><li class="listitem">The system will then run a four-fold cross validation and print a confusion matrix. Look at the <span class="emphasis"><em>How to train and evaluate with cross validation</em></span> recipe if you need further explanation:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Training on fold 0</strong></span>
<span class="strong"><strong>Testing on fold 0</strong></span>
<span class="strong"><strong>Training on fold 1</strong></span>
<span class="strong"><strong>Testing on fold 1</strong></span>
<span class="strong"><strong>Training on fold 2</strong></span>
<span class="strong"><strong>Testing on fold 2</strong></span>
<span class="strong"><strong>Training on fold 3</strong></span>
<span class="strong"><strong>Testing on fold 3</strong></span>
<span class="strong"><strong>reference\response</strong></span>
<span class="strong"><strong>    \p,n,o,</strong></span>
<span class="strong"><strong>    p 14,0,10,</strong></span>
<span class="strong"><strong>    n 6,0,4,</strong></span>
<span class="strong"><strong>    o 7,1,37,</strong></span>
</pre></div></li></ol></div><p>That's it! Classifiers<a class="indexterm" id="id139"/> are entirely dependent on training data for what they classify. More sophisticated techniques will bring richer features into the mix than character ngrams, but ultimately, the labels imposed by training data are the knowledge being imparted to the classifier. Depending on your view, the underlying technology is magical or astoundingly simple minded.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec42"/>How it works...</h2></div></div></div><p>Most developers are surprised that the only difference between language ID and sentiment is the labeling applied to the data for training. The language model classifier is applying an individual language model for each category and also noting the marginal distribution of the categories in the estimates.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec43"/>There's more…</h2></div></div></div><p>Classifiers are pretty dumb but very useful if they are not expected to work outside their capabilities. Language ID works great as a classification problem because the observed events are tightly tied to the classification being done—the words and characters of a language. Sentiment is more difficult because the observed events, in this case, are exactly the same as the language ID and are less strongly associated with the end classification. For example, the phrase "I love" is a good predictor of the sentence being English but not as clear a predictor that the sentiment is positive, negative, or other. If the tweet is "I love Disney", then we have a positive statement. If the tweet is "I love Disney, not", then it is negative. Addressing the complexities of sentiment and other more complex phenomenon tends to be resolved in the following ways:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Create more training data. Even relatively dumb techniques such as language model classifiers can perform very well given enough data. Humanity is just not that creative in ways to gripe about, or praise, something. The <span class="emphasis"><em>Train a little, learn a little – active learning</em></span> recipe of <a class="link" href="ch03.html" title="Chapter 3. Advanced Classifiers">Chapter 3</a>, <span class="emphasis"><em>Advanced Classifiers</em></span>, presents a clever way to do this.</li><li class="listitem" style="list-style-type: disc">Use fancier classifiers that in turn use fancier features (observations) about the data to get the job done. Look at the logistic regression recipes for more information. For the negation case, a feature that looked for a negative phrase in the tweet might help. This could get arbitrarily sophisticated.</li></ul></div><p>Note that a more <a class="indexterm" id="id140"/>appropriate way to take on the sentiment problem can be to create a binary classifier for <span class="emphasis"><em>positive</em></span> and <span class="emphasis"><em>not positive</em></span> and a binary classifier for <span class="emphasis"><em>negative</em></span> and <span class="emphasis"><em>not negative</em></span>. The classifiers will have separate training data and will allow for a tweet to be both positive and negative.</p><div class="section" title="Common problems as a classification problem"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec05"/>Common problems as a classification problem</h3></div></div></div><p>Classifiers form the foundations <a class="indexterm" id="id141"/>of many industrial NLP problems. This recipe goes through the process of encoding some common problems into a classification-based solution. We will pull from real-world examples that we have built whenever possible. You can think of them as mini recipes.</p><div class="section" title="Topic detection"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl4sec01"/>Topic detection</h4></div></div></div><p>Problem: Take footnotes from financial documents (10Qs and 10Ks) and determine whether an <a class="indexterm" id="id142"/>
<span class="strong"><strong>eXtensible Business Reporting Language</strong></span> (<span class="strong"><strong>XBRL</strong></span>) category is applied like "forward looking financial statements". Turns out that foot notes are where all the action happens. For example, is the footnote referring to retired debt? Performance <a class="indexterm" id="id143"/>needed to be greater than 90 percent precision with acceptable recall.</p><p>Solution: This problem closely mirrors how we approached language ID and sentiment. The actual solution involves a sentence recognizer that detects the footnotes—see <a class="link" href="ch05.html" title="Chapter 5. Finding Spans in Text – Chunking">Chapter 5</a>, <span class="emphasis"><em>Finding Spans in Text – Chunking</em></span>—and then creates training data for each of the XBRL categories. We used the confusion matrix output to help refine the XBRL categories that the system was struggling to distinguish. Merging categories was a possibility, and we did merge them. This system is based on language model classifiers. If done now, we would use logistic regression.</p></div><div class="section" title="Question answering"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl4sec02"/>Question answering</h4></div></div></div><p>Problem: Identify FAQs in a large <a class="indexterm" id="id144"/>dataset of text-based customer support data and develop the answers and ability to automatically deliver answers with 90 percent precision.</p><p>Solution: Perform clustering analysis over logs to find FAQs—see <a class="link" href="ch06.html" title="Chapter 6. String Comparison and Clustering">Chapter 6</a>, <span class="emphasis"><em>String Comparison and Clustering</em></span>. This will result in a very large set of FAQs that are really <a class="indexterm" id="id145"/>
<span class="strong"><strong>Infrequently Asked Questions</strong></span> (<span class="strong"><strong>IAQs</strong></span>); this means that the prevalence of an IAQ can be as low as 1/20000. Positive data is fairly easy to find for a classifier, but negative data is too expensive to create on any kind of balanced distribution—for every positive case, one will expect 19999 negative case. The solution is to assume that any random sample of a large size will contain very few positives and to just use this as negative data. A refinement is to run a trained classifier over the negatives to find high-scoring cases and annotate them to pull out the positives that might be found.</p></div><div class="section" title="Degree of sentiment"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl4sec03"/>Degree of sentiment</h4></div></div></div><p>Problem: Classify a sentiment on<a class="indexterm" id="id146"/> a scale of 1 to 10 based on the degree of negativeness to positiveness.</p><p>Solution: Even though our classifiers provide a score that can be mapped on a 1-to-10 scale, this is not what the background computation is doing. To correctly map to a degree scale, one will have to annotate the distinction in training data—this tweet is a 1, this tweet is a 3, and so on. We will then train a 10-way classifier, and the first best category should, in theory, be the degree. We write <span class="emphasis"><em>in theory</em></span> because despite regular customer requests for this, we have never found a customer that was willing to support the required annotation.</p></div><div class="section" title="Non-exclusive category classification"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl4sec04"/>Non-exclusive category classification</h4></div></div></div><p>Problem: The desired classifications are<a class="indexterm" id="id147"/> not mutually exclusive. A tweet can say both positive and negative things, for example, "Loved Mickey, hated Pluto". Our classifiers assume that categories are mutually exclusive.</p><p>Solution: We regularly use multiple binary classifiers in place of one <span class="emphasis"><em>n</em></span>-way or multinomial classifiers. The classifiers will be trained for positive/non-positive and negative/non-negative. A tweet can then be annotated <code class="literal">n</code> and <code class="literal">p</code>.</p></div><div class="section" title="Person/company/location detection"><div class="titlepage"><div><div><h4 class="title"><a id="ch01lvl4sec05"/>Person/company/location detection</h4></div></div></div><p>Problem: Detect <a class="indexterm" id="id148"/>mentions of people in text data.</p><p>Solution: Believe it or not, this breaks down into a word classification problem. See <a class="link" href="ch06.html" title="Chapter 6. String Comparison and Clustering">Chapter 6</a>, <span class="emphasis"><em>String Comparison and Clustering</em></span>.</p><p>It is generally fruitful to look at any novel problem as a classification problem, even if classifiers don't get used as the underlying technology. It can help clarify what the underlying technology actually needs to do.</p></div></div></div></div></body></html>