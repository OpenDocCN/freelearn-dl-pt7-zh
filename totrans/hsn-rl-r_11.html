<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Reinforcement Learning in Game Applications</h1>
                </header>
            
            <article>
                
<p class="mce-root">Games have always been a phenomenon of human culture where people manifest intelligence, interaction, and competition. But games are also an important theoretical paradigm in logic, <strong>artificial intelligence</strong> (<strong>AI</strong>), computer science, linguistics, biology, and lately, increasingly in the social sciences and in psychology. Games, especially strategy games, offer reinforcement learning algorithms an ideal and privileged environment for testing, as they can act as models for real problems.</p>
<p class="mce-root">In this chapter, we will learn how to use reinforcement learning algorithms to address a problem in game theory. By<span> the end of the chapter, we will have learned the fundamental concepts of game theory. We will also learn how to install and configure the OpenAI Gym library, understand how the OpenAI Gym library works, and learn how to use Q-learning to solve game problems. Apart from that, we will understand how to make a learning and testing phase, and learn how to develop OpenAI Gym applications using R.</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Understanding game theory essentials</li>
<li>Exploring game theory applications</li>
<li>Playing the tic-tac-toe game</li>
<li>Introducing the OpenAI Gym library</li>
<li>Robot control system using the FrozenLake environment</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2tffzMD">http://bit.ly/2tffzMD</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding game theory essentials</h1>
                </header>
            
            <article>
                
<p><span>Game theory is a mathematical science</span> that studies and analyzes the individual decisions of a subject in situations of conflict, or strategic interaction, with other rival subjects aimed at the maximum profit of each subject. In such situations, the decisions of one can influence the results achieved by the other(<em>s</em>), and vice versa, according to a feedback mechanism, by seeking competitive and or cooperative solutions through models.</p>
<p>The theory of games has its distant origins in 1654, following correspondence between Blaise Pascal and Pierre de Fermat on the calculation of probabilities for gambling. The expression <strong>game theory</strong> was first used by Emil Borel in the 1920s. Borel developed the <strong>Théorie des jeux</strong>, a zero-sum game with two players, and tried to find a solution known as Von Neumann's concept of solving a zero-sum game. It is generally acknowledged that the release of the book <em>Theory of Games and Economic Behavior</em>, by John von Neumann and Oskar Morgenstern in 1944, marked the birth of modern game theory, although other authors (such as Ernst Zermelo and Armand Borel) had written about game theory.</p>
<p>The idea of these two scholars can be described informally as an attempt to describe human behavior mathematically in the cases in which the interaction between men involves the winning, or dividing, of some kind of resource. The most famous scholar to have subsequently dealt with the theory of games, in particular with regard to <strong>non-cooperative games</strong>, is the mathematician John Forbes Nash jr., to whom the Ron Howard film <em>A Beautiful Mind </em>is dedicated.</p>
<div class="packt_infobox">Eight Nobel prizes in economics were awarded to scholars who dealt with game theory. A Crafoord Prize has also been awarded to John Maynard Smith, a long-time distinguished biologist, geneticist and professor at the University of Sussex, for his contribution to this field.</div>
<p>In the following sections, we will introduce the concepts underlying game theory and then analyze the main types of games faced by researchers.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic concepts of game theory</h1>
                </header>
            
            <article>
                
<p>Game theory has victory as its main objective. Everyone must be aware of the rules of the game and be aware of the consequences of every single move. The set of moves that an individual intends to do is called a strategy. Depending on the strategies adopted by all the players, each one receives a pay-off according to an adequate unit of measurement. The reward can be positive, negative, or null. A game is called a <strong>constant sum</strong> if, for each player's payout, there is a corresponding loss for others. A <strong>zero-sum</strong> game between two players represents a situation in which the reward is paid from one player to another. The strategy to follow is satisfactory for all players; otherwise, it is necessary to calculate and maximize the player's mathematical hope or expected value, which is the weighted average of the possible rewards, each weighed for the respective probabilities of the event.</p>
<p>In a game, there are one or more contenders who try to win the game, that is, to maximize their winnings. The winnings are defined by a rule that establishes quantitatively what the contenders win according to their behavior. This function is called a function of payments. Each player can undertake a finite-infinite number of actions or decisions that determine a strategy. Each strategy is characterized by a consequence for the player who has adopted it and which can be a reward (positive/negative). The result of the game is completely determined by the sequence of their strategies and the strategies adopted by the other players.</p>
<p>How do you characterize the result of the game for each player? If you measure the consequence of a strategy in terms of reward, each strategy can be matched with a value: a negative value will indicate a payment to the opponent, such as a penalty; while a positive value will indicate winnings, that is, the collection of a prize. The gain or loss due to the generic player associated with his or her strategy and the strategies taken at a given moment by all the remaining players is expressed by the monetary value indicated by the payment function.</p>
<div class="packt_tip">The decisions taken by a player naturally collide, or are in accordance with the decisions made by the other players, and from such situations derive various types of games.</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>A useful tool to represent the interactions between two players, two companies, or two individuals is a double-entry decision matrix or table. This decision table shows the strategies and winnings of a game conducted by two players. The decision matrix is therefore a representation through which we catalog all the possible results of the interactions between players, and we assign the value of the reward that in each situation competes to each player. Another form of representation concerns the sequence with which each decision is taken, or the actions are conducted. This characteristic of each game can be described by means of a tree graph, representing every possible combination of how the contenders play from the initial state to the final states where the winnings are distributed.</p>
<p>To describe a strategic situation, four basic elements are required:</p>
<ul>
<li><strong>Players</strong>: The decision makers in the game (who is involved?)</li>
<li><strong>Actions</strong>: The possible actions, or moves, that players can choose from (what can they do?)</li>
<li><strong>Strategies</strong>: The action plans of the players (what are they going to do?)</li>
<li><strong>Winnings</strong>: The possible gains that players get (what do they earn?)</li>
</ul>
<p>A strategy is therefore a complete and contingent plan, or decision-making decision, that specifies how the player must act in any possible circumstances in which he may be called upon to decide. Being a complete contingent plan, a strategy often defines which action a player must choose in circumstances that may not be achieved during the game. In the following section, the games will be classified and a short description of each topic will be proposed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Game types</h1>
                </header>
            
            <article>
                
<p>Games can be classified according to different paradigms, such as the following:</p>
<ul>
<li>Cooperation</li>
<li>Symmetry</li>
<li>Sum</li>
<li>Sequencing</li>
</ul>
<p>In the following sections, we will see a short description of these topics.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cooperative game</h1>
                </header>
            
            <article>
                
<p>A cooperative game is presented when the interests of the players are not in direct opposition to each other, but there is a commonality of interests. Players pursue a common goal, at least for the duration of the game; some of them may tend to associate to improve their <strong>pay-off</strong>. The guarantee is given by the binding agreements. What is the mathematical representation of a shared interest? The concept of the union of individual interests in a coalition or alliance is expressed by the definition of essential play; while the value of a generic coalition is measured by a function called a characteristic function.</p>
<p>In contrast, in non-cooperative games, also called <strong>competitive games</strong>, players cannot enter into binding agreements (even by regulation), regardless of their objectives. The solution given by John Nash, with his <em>Equilibrium of Nash</em>, applies to this category, and it is probably the most famous notion of the whole theory, thanks to its vast field of applicability. The criterion of rational behavior adopted in non-cooperative games is individual and is called the maximum strategy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Symmetric game</h1>
                </header>
            
            <article>
                
<p>In a symmetrical game, the profits deriving from the adoption of a strategy depend only on the other strategies employed, not by those who are playing them. If players' identities can be changed without changing the payoff, then a game is symmetrical.</p>
<p>In contrast, in asymmetric games there are no identical series of strategies for both players. It is possible, however, that a game has identical strategies for both players, but that it is asymmetric.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Zero-sum game</h1>
                </header>
            
            <article>
                
<p>Zero-sum games are a special case of constant-sum games in which the constant is zero. Zero-sum games model all the conflicting situations in which the contrast of the two players is total: the winning of a player coincides exactly with the loss of the other. In other words, the sum of the winnings of the two contenders according to the strategies used is always zero. In chess, for example, this means that the only three possible results are: victory, defeat, and draw (reward: +1, -1, and 0).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sequential games</h1>
                </header>
            
            <article>
                
<p>In sequential games, subsequent players retain some knowledge of previous actions. This does not mean that they know every action of previous players. For example, a player may know that a previous player has not performed a certain action, while he does not know which other available actions the first player performed.</p>
<p>Now we have learned to classify games based on some paradigms. Why is analyzing games so important? This is due to the fact that many real-life problems can be tackled by deriving the solutions obtained from game theory. In the next section, we will see examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring game theory applications</h1>
                </header>
            
            <article>
                
<p>Game theory has always interested many researchers because of its usefulness in the practical field and in all fields of human work, such as the following:</p>
<ul>
<li><strong>Philosophy</strong>: This has always analyzed game theory because it provides a way to clarify the logical difficulty of some philosophers, such as Kant, Rousseau, Hobbes, and other social and political theorists.</li>
<li><strong>Economy</strong>: Many of the speculations in the business world can be modeled using the methodology of game theory. A famous example is that of the similarity between the setting of oligopoly prices and the prisoner's dilemma.</li>
<li><strong>Biology</strong>: Although nature is often considered brutal, there is cooperation between many different species. The reason for this coexistence can be modeled using game theory.</li>
<li><strong>AI</strong>: The human being can make decisions based on the environmental stimuli he receives. Instead, machines can plan only if programmed with decision lists based on several conditions. This limit can be overcome by artificial intelligence that can give the machine the ability to make new unplanned decisions from their creators. To do this, programs must generate new payoff matrices based on observed stimuli and experience.</li>
</ul>
<p>In the following section, we will analyze a widespread game and see how to deal with reinforcement learning.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Playing the tic-tac-toe game</h1>
                </header>
            
            <article>
                
<p>The game of tic-tac-toe is perfect as a first example of a game solved with the use of reinforcement learning: In fact, compared to other strategy games, it has a few simple rules. Furthermore, it is relatively easy to program and, since a game can last up to nine moves, the training of an evaluation function is extremely rapid. The tic-tac-toe is a game with perfect information for two players, where each player is assigned a symbol to play with. The symbols usually used are the cross and the circle. The game is started by the player who uses the cross.</p>
<p>The game grid has a 3x3 structure and presents nine initially empty cells as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-623 image-border" src="assets/d3cf2eff-9c0b-47a8-a384-34b51a504344.png" style="width:13.25em;height:8.92em;"/></p>
<p>In turn, players choose an empty cell and draw their own symbol. The player who manages to place three of his symbols in a horizontal, vertical, or diagonal line wins. If the game grid is filled without any of the players having succeeded in completing a straight line of three symbols, the game ends in a draw. So, if played correctly, tic-tac-toe will end in a draw, making the game useless.</p>
<p>In the following section, we will introduce the <span><kbd>tictactoe</kbd> package to play the game using the Q-learning algorithm.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The tictactoe package</h1>
                </header>
            
            <article>
                
<p>To address the tic-tac-toe game, we will use the <kbd>tictactoe</kbd> package available on the CRAN website. This package implements the tic-tac-toe game to play on a console, either with human or AI players. Various levels of AI players are trained through the Q-learning algorithm.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following table gives some information about this package:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Package</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><kbd>tictactoe</kbd></p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Date</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">2017-05-26</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Version</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">0.2.2</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Title</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Tic-Tac-Toe Game</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Author</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Kota Mori</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>With the help of this package, we will play a first part with the computer to highlight the features of the game, and then we will train an artificial agent to play a game by following the best policy to get the maximum number of wins.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Playing a tic-tac-toe game</h1>
                </header>
            
            <article>
                
<p>To begin, we will see how to set the game environment, and we will start a first game:</p>
<ol>
<li>First we have to import the library:</li>
</ol>
<pre style="padding-left: 60px">library(tictactoe)</pre>
<ol start="2">
<li>Then we can start the tic-tac-toe game on the R console using the <kbd>ttt()</kbd> function as follows:</li>
</ol>
<pre style="padding-left: 60px">ttt(ttt_human(name = "GIUSEPPE"), ttt_random())</pre>
<p class="mce-root" style="padding-left: 60px">Note the following:</p>
<ul>
<li style="padding-left: 60px"><kbd>ttt_human()</kbd> creates a human tic-tac-toe player; if we wish we can also set a name using the name attribute (for example <kbd>name = "GIUSEPPE"</kbd>).</li>
<li style="padding-left: 60px"><kbd>ttt_random()</kbd> sets a random player, which merely places the opposite symbol (the circle) in one of the places available in a completely random way.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The console returns the following grid:</p>
<pre style="padding-left: 60px"><strong>A B C</strong><br/><strong>   ------</strong><br/><strong> 1| . . .</strong><br/><strong> 2| . . .</strong><br/><strong> 3| . . .</strong><br/><br/><strong> Player 1 (GIUSEPPE) to play</strong><br/><strong>choose move (e.g. A1) &gt;</strong></pre>
<p style="padding-left: 60px">As anticipated the game is based on a simple 3 x 3 grid; to facilitate the identification of a cell, the columns are named with the letters A, B, C, while the rows with the numbers 1,2,3. This means that the first cell on the top left will be identified with the symbol A1. The last line printed on the console invites player 1 (<kbd>GIUSEPPE</kbd>) to make his move.</p>
<ol start="3">
<li>Let's start by placing the X in cell A1, the following result is returned:</li>
</ol>
<pre style="padding-left: 60px">action = A1<br/><br/><strong>    A B C</strong><br/><strong>   ------</strong><br/><strong> 1| X . . </strong><br/><strong> 2| . . .</strong><br/><strong> 3| . . .</strong><br/> <br/><strong>Player 2 (random AI) to play</strong><br/><br/>action = B1<br/><br/><strong>    A B C</strong><br/><strong>   ------</strong><br/><strong> 1| X O .</strong><br/><strong> 2| . . .</strong><br/><strong> 3| . . .</strong><br/> <br/><strong>Player 1 (GIUSEPPE) to play</strong><br/><strong>choose move (e.g. A1) &gt;</strong></pre>
<p style="padding-left: 60px">As we can see <span>in the preceding code block, </span>the X has been positioned correctly in the upper-left cell, then the player 2 (computer) has placed his symbol (O) in a free position in a random way. Once again the last row invites player 1 to make the next move. We can proceed this way until the game is completed.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">Three results are available—0, 1, and 2, which indicate the draw, the victory of player 1, and the victory of player 2, respectively, for example:</p>
<pre style="padding-left: 60px">action = B2<br/><strong>game over</strong><br/><strong>    A B C</strong><br/><strong>   ------</strong><br/><strong> 1| X . X</strong><br/><strong> 2| O X O</strong><br/><strong> 3| X . O</strong><br/><br/><strong>won by Player 1 (GIUSEPPE)!</strong></pre>
<p>In this case, I won, but only because the computer put its symbols at random without following a strategy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the agent using Q-learning</h1>
                </header>
            
            <article>
                
<p>We can train the agent to follow a strategy. Let's see how:</p>
<ol>
<li>To start we can simulate games in order to check the results obtained by two artificial agents when they play with each other:</li>
</ol>
<pre style="padding-left: 60px">player1&lt;- ttt_ai()<br/>player2&lt;- ttt_ai()<br/>SimulatedGame &lt;- ttt_simulate(player1, player2, N = 100)</pre>
<p style="padding-left: 60px">In the preceding code, the following functions have been used:</p>
<ul>
<li style="padding-left: 60px"><kbd>ttt_ai()</kbd></li>
<li style="padding-left: 60px"><kbd>ttt_simulate</kbd></li>
</ul>
<p style="padding-left: 60px">The <kbd>ttt_ai()</kbd> function creates an artificial tic-tac-toe game player. We have not used any arguments, in reality it is possible to use the following:</p>
<ul>
<li style="padding-left: 60px"><kbd>name</kbd>: Player name.</li>
<li style="padding-left: 60px"><kbd>level</kbd>: The AI strength must be an integer from 0 (weakest) to 5 (strongest).</li>
</ul>
<p style="padding-left: 60px">The <kbd>level</kbd> argument defines the effectiveness of the agent we are creating; from 0 to 5, its skill in game management increase, making the opponent's game more difficult. In the previous instructions, we already created an artificial agent using the <kbd>ttt_random ()</kbd> function that represents an alias of the  <kbd>ttt_ai ()</kbd> function in which the level of the agent is set by default equal to 0.</p>
<p class="mce-root"/>
<p style="padding-left: 60px">The <kbd>ttt_ai ()</kbd> function returns an object to which the <kbd>getmove ()</kbd> function is associated; this function accepts an object of the <kbd>ttt_game</kbd> type, and returns an optimal move using the political function.</p>
<p style="padding-left: 60px">A <kbd>ttt_ai</kbd> object has the value and policy functions. The value function associates a game state with the evaluation from the point of view of the first player. The political function associates a game state with a set of optimal actions obtained from the evaluation of the value function. These functions are trained through an algorithm based on Q-learning that we discussed extensively in <a href="9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml">Chapter 7</a>, <em>Temporal Difference Learning</em>.</p>
<p style="padding-left: 60px">The second function used, I refer to the <kbd>ttt_simulate()</kbd> function, simulates a tic-tac-toe game between two artificial agents. The following arguments are passed:</p>
<ul>
<li style="padding-left: 60px"><kbd>player1</kbd>, <kbd>player2</kbd>: Artificial players to simulate</li>
<li style="padding-left: 60px"><kbd>N</kbd>: Number of simulation games</li>
</ul>
<p style="padding-left: 60px">In addition to these, the following additional arguments are available:</p>
<ul>
<li style="padding-left: 60px"><kbd>verbose</kbd>: If true, shows a progress report.</li>
<li style="padding-left: 60px"><kbd>showboard</kbd>: If true, a game transition is displayed.</li>
<li style="padding-left: 60px"><kbd>pauseif</kbd>: Pauses the simulation when specified results occur. This can be useful for exploratory purposes.</li>
</ul>
<p style="padding-left: 60px">The function returns a vector of integers with the results of the simulations carried out. In practice, each simulation will return a value between 0,1, and 2 that, as we have already said, means a draw, a victory for player 1, or a victory for player 2, respectively.</p>
<ol start="2">
<li>We can verify what we have said using the <kbd>str()</kbd> function, which returns a compact view of the internal structure of an object R:</li>
</ol>
<pre style="padding-left: 60px">str(SimulatedGame)</pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>int [1:100] 1 2 2 0 1 1 2 1 2 2 ...</strong></pre>
<ol start="3">
<li>We can do more; for example, we can verify the occurrences of the three game results in the whole simulation using the <kbd>prop.table()</kbd> function as follows:</li>
</ol>
<pre style="padding-left: 60px">prop.table(table(SimulatedGame))</pre>
<p class="mce-root"/>
<ol start="4">
<li>This function accepts a table as an argument and calculates the proportions of the data it contains. The following results are returned:</li>
</ol>
<pre style="padding-left: 60px"><strong>SimulatedGame</strong><br/><strong>   0   1    2</strong><br/><strong>0.12 0.51 0.37</strong></pre>
<p style="padding-left: 60px">In this way, we can see that in the performed simulation player 1 is the one who wins more (51%) than player 2 (37%), and a draw is repeated a decidedly lower number of times (12%).</p>
<ol start="5">
<li>Now, we repeat the experiment, but this time we will try to improve one of the two players through training based on the Q learning algorithm. As already done, we first create the two agents:</li>
</ol>
<pre style="padding-left: 60px">player3&lt;- ttt_ai()<br/>player4&lt;- ttt_ai()</pre>
<ol start="6">
<li>After doing this, we focus on player 4 trying to improve his performance through a training phase in which he will learn to follow the best strategy:</li>
</ol>
<pre style="padding-left: 60px">TrainPlayer4 &lt;- ttt_qlearn(player4, N = 500, verbose = FALSE)</pre>
<p style="padding-left: 60px">The <kbd>ttt_qlearn()</kbd> function trains a tic-tac-toe agent through Q-learning. The following arguments are available:</p>
<ul>
<li style="padding-left: 60px"><kbd>player</kbd>: Artificial player to train.</li>
<li style="padding-left: 60px"><kbd>N</kbd>: Number of episodes.</li>
<li style="padding-left: 60px"><kbd>epsilon</kbd>: The fraction of a random exploration move</li>
<li style="padding-left: 60px"><kbd>alpha</kbd>: Learning rate.</li>
<li style="padding-left: 60px"><kbd>gamma</kbd>: Discount factor.</li>
<li style="padding-left: 60px"><kbd>simulate</kbd>: If true, conduct simulation during training.</li>
<li style="padding-left: 60px"><kbd>sim_every</kbd>: Conduct simulation after this many training games.</li>
<li style="padding-left: 60px"><kbd>N_sim</kbd>: Number of simulation games.</li>
<li style="padding-left: 60px"><kbd>verbose</kbd>: If true, a progress report is shown.</li>
</ul>
<p style="padding-left: 60px">In the Q-learning-based training process, the agent plays against himself to update the value function and its policies. The algorithm used is Q-learning with epsilon greedy.</p>
<p style="padding-left: 60px">For each state s, the player updates the value function according to the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/99a91547-fe4d-4dd7-a5a8-21469f1dc818.png" style="width:17.17em;height:1.33em;"/></p>
<p style="padding-left: 60px">This happens on the first player's turn. When the second player's turn arrives, the formula to be used will always be the same, provided you replace the maximum with the minimum. In a similar way, we proceed to update the policy; that is, we look for the set of actions that allows us to reach the next state in order to maximize the value function.</p>
<p style="padding-left: 60px">The parameters that govern the process are as follows:</p>
<ul>
<li style="padding-left: 60px">The learning rate that determines how often new information is acquired and will replace old information. A factor of 0 would prevent the agent from learning; however, a factor of 1 would cause the agent to be interested only in recent information.</li>
<li style="padding-left: 60px">The discount factor that determines the importance of future rewards. A factor of 0 will cause the agent to use only the current rewards, while a factor tending to 1 will make the agent also attentive to the rewards he will receive in the long-term future.</li>
</ul>
<div class="packt_tip">The strategy used in the algorithm causes the player to choose the next action with the e-greedy method. This means that the agent will follow his policy with the probability 1-e, and will choose random actions with the probability e.</div>
<p style="padding-left: 60px">At the end of a game, the player sets the final status as follows:</p>
<ul>
<li style="padding-left: 60px">100 if player 1 wins</li>
<li style="padding-left: 60px">-100 if player 2 wins</li>
<li style="padding-left: 60px">0 if a draw</li>
</ul>
<p style="padding-left: 60px">The learning process is repeated N times, the value set by the user:</p>
<ol start="7">
<li>After training player 4, we can simulate the game:</li>
</ol>
<pre style="padding-left: 60px">SimulatedGameQLearn &lt;- ttt_simulate(player3, player4, N = 100)</pre>
<ol start="8">
<li>Once again, we verify the number of occurrences of the results:</li>
</ol>
<pre style="padding-left: 60px">prop.table(table(SimulatedGameQLearn))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>The following results are returned:</p>
<pre><strong>SimulatedGameQLearn</strong><br/><strong>   0    1    2</strong><br/><strong>0.31 0.21 0.48</strong></pre>
<p>In this case, it is the trained agent (player 4) who wins the games with the most occurrences (48%), then there are the draws (31%), and finally the games won by player 3 (21%). Thus, the player's training reversed the results by creating an agent who was able to identify a policy that allowed him to get more wins.</p>
<p>In the following section, the <span>OpenAI Gym library will be introduced; this library contains many environments which will allow us to train agents using reinforcement learning.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing OpenAI Gym library</h1>
                </header>
            
            <article>
                
<p>OpenAI Gym is a library that helps us to implement algorithms based on reinforcement learning. It focuses on the episodic setting of reinforcement learning. In other words, the agent's experience is divided into a series of episodes. The initial state of the agent is randomly sampled by a distribution, and the interaction proceeds until the environment reaches a terminal state. This procedure is repeated for each episode, with the aim of maximizing the total reward expectation per episode and achieving a high level of performance in the fewest possible episodes.</p>
<div class="packt_infobox">Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports the ability to teach agents everything from walking to playing games such as Pong or pinball. The library is available at the following link: <a href="https://gym.openai.com">https://gym.openai.com/</a>. </div>
<p>OpenAI Gym is part of a much more ambitious project, known as the OpenAI project. OpenAI is an <strong>artificial intelligence</strong> (<strong>AI</strong>) research company founded by Elon Musk and Sam Altman. It is a non-profit project that aims to promote and develop friendly AI in such a way as to benefit humanity as a whole. The organization aims to collaborate freely with other institutions and researchers by making their patents and research open to the public. The founders decided to undertake this project as they were concerned by the existential risk deriving from the indiscriminate use of AI.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>OpenAI Gym is a library of programs that allows you to develop AIs, measure intellectual abilities, and enhance learning abilities. In short, a <em>gym</em> in the form of algorithms that trains the present digital brains to the OpenAI Gym and projects them into the future. But there is also another goal. OpenAI wants to stimulate research in the AI sector by funding projects that make humanity progress even in those fields where there is no economic return. With Gym, on the other hand, it intends to standardize the measurement of AI so that researchers can compete on equal terms and know where their colleagues have come from, but above all it focuses on results that are really useful for everyone.</p>
<p>The tools available are many. From the ability to play old video games like Pong to that of fighting in  GO to control a robot, we just enter our algorithm in this digital place to see how it works. The second step is to compare the benchmarks obtained with the other ones to see where we stand compared to others, and maybe we can collaborate with them to get mutual benefits.</p>
<p>The following list shows some environments available in the library; the environments are grouped by category to simplify the search:</p>
<ul>
<li><strong>Algorithms</strong>: Perform computations such as adding multi-digit numbers and reversing sequences. You might object that these tasks are easy for a computer, but the challenge is to learn these algorithms purely from examples. These tasks have the nice property that it's easy to vary the difficulty by varying the sequence length.</li>
<li><strong>Atari</strong>: Plays classic Atari games. We've integrated the Arcade learning Environment (which has had a big impact on reinforcement learning research) in an easy-to-install form.</li>
<li><strong>Box2D</strong>: Continuous control tasks in the Box2D simulator.</li>
<li><strong>Classic control</strong>: Complete small-scale tasks, mostly from the RL literature. They're here to get you started.</li>
<li><strong>MuJoCo</strong>: Continuous control tasks, running in a fast physics simulator. This task uses the MuJoCo physics engine, which was designed for fast and accurate robot simulation.</li>
<li><strong>Robotics</strong>: Simulated goal-based tasks for the Fetch and ShadowHand robots.</li>
<li><strong>Toy text</strong>: Simple text environments to get you started.</li>
</ul>
<p>In particular, the classic control category offers very useful environments to reproduce the scenarios of important physical experiments.</p>
<p class="mce-root"/>
<p>OpenAI Gym makes no assumptions about the structure of our agent and is compatible with any numerical computation library. The Gym library is a collection of test problems—environments—that we can use to work out our reinforcement learning algorithms. The environments have a shared interface, allowing you to write general algorithms. To begin, let's see how to install the library.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenAI Gym installation</h1>
                </header>
            
            <article>
                
<p>As already mentioned, OpenAI Gym is a library written in a Python environment. To be able to integrate it into the R environment, a version of Python must be installed on the computer. First we need to install the OpenAI <kbd>gym-http-api</kbd> API; these are APIs that allow access to an ever-increasing variety of environments.</p>
<div>
<p>The <strong>APIs</strong>, that is an abbreviation for <strong>application programming interfaces</strong>, are sets of definitions and protocols with which application software is created and integrated. They allow your products or services to communicate with other products or services without knowing how they are implemented, thus simplifying the development of the app and allowing a net saving of time and money. When creating new tools and products or managing existing ones, the APIs offer flexibility, guarantee opportunities for innovation, and simplify design, administration, and use.</p>
</div>
<ol>
<li>To download and install the OpenAI <kbd>gym-http-api</kbd> API, you can run the following shell commands. To execute these commands, it is necessary to use a command window:</li>
</ol>
<pre style="padding-left: 60px"><strong>git clone https://github.com/openai/gym-http-api</strong><br/><strong>cd gym-http-api</strong><br/><strong>pip install -r requirements.txt</strong></pre>
<p style="padding-left: 60px"><span>The first line of code in the preceding code block uses</span> the Git software to download the bees to install from the github repository. The Git software allows you to manage a project while maintaining control of the source code and its history, and allows more developers to collaborate on it; it is essentially a version control system. Git is the de facto standard in the open source community, created in 2005 by Linus Torvald to work on the Linux kernel, and has been maintained by Junio Hamano (a Google developer) two months after its creation; Git is constantly evolving and completely free and open source. The second line of code moves the command line to the folder where we copied the repository. Finally the third line uses the pip software to install the API.</p>
<p style="padding-left: 60px">The preceding code is intended to be run locally by a single user. A Python client is included, to demonstrate how to interact with the server.</p>
<ol start="2">
<li>To start the server from the command line, go to the folder where we installed the API, and run the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>python gym_http_server.py</strong></pre>
<p style="padding-left: 60px">The following result is returned:</p>
<pre style="padding-left: 60px"><strong>Server starting at: http://127.0.0.1:5000</strong></pre>
<ol start="3">
<li>Now the server is ready to interact with our scripts. We can install the <kbd>gym</kbd> package:</li>
</ol>
<pre style="padding-left: 60px">install.packages("gym")</pre>
<p>The following table gives some information about the <kbd>gym</kbd> package:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Package</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><kbd>gym</kbd></p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Date</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">2016-10-25</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Version</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">0.1.0</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Title</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Provides Access to the OpenAI Gym API</p>
</td>
</tr>
<tr>
<td>
<p class="CDPAlignCenter CDPAlign">Author</p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign">Paul Hendricks</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>To verify the functioning of the package, we can execute the example script supplied with the documentation.</p>
<ol>
<li>Let's start by importing the library:</li>
</ol>
<pre style="padding-left: 60px">library(gym)</pre>
<ol start="2">
<li>To start, we will establish the connection with the client to interact with the server:</li>
</ol>
<pre style="padding-left: 60px">RemoteBase &lt;- "http://127.0.0.1:5000"<br/>Client &lt;- create_GymClient(RemoteBase)<br/>print(Client)</pre>
<p style="padding-left: 60px">The <kbd>create_GymClient()</kbd> function instantiates a <kbd>GymClient</kbd> instance to integrate with an OpenAI Gym server. The following result is printed:</p>
<pre style="padding-left: 60px"><strong>&lt;GymClient: http://127.0.0.1:5000&gt;</strong></pre>
<p class="mce-root"/>
<ol start="3">
<li>The connection has been established, and now we can create the environment:</li>
</ol>
<pre style="padding-left: 60px">EnvId &lt;- "CartPole-v0"<br/>InstanceId &lt;- env_create(Client, EnvId)<br/>print(InstanceId)</pre>
<p style="padding-left: 60px">The following result is printed:</p>
<pre style="padding-left: 60px"><strong>[1] "376e0df2"</strong></pre>
<ol start="4">
<li>Now we can list all the environments created with the bees available to us in the current work session:</li>
</ol>
<pre style="padding-left: 60px">AllEnvsAvailable &lt;- env_list_all(Client)<br/>print(AllEnvsAvailable)</pre>
<p style="padding-left: 60px">The following result is printed:</p>
<pre style="padding-left: 60px"><strong>$`376e0df2`</strong><br/><strong>[1] "CartPole-v0"</strong></pre>
<ol start="5">
<li>In this way, we have confirmation that the simulation environment has been correctly created, now we can request the information needed to interact with it:</li>
</ol>
<pre style="padding-left: 60px">ActionSpaceInfo &lt;- env_action_space_info(Client, InstanceId)<br/>print(ActionSpaceInfo)</pre>
<p style="padding-left: 60px">The <kbd>env_action_space_info()</kbd> function evaluates whether an action is a member of an environment action space.  The following results are printed:</p>
<pre style="padding-left: 60px">$n<br/><strong>[1] 2</strong><br/><br/>$name<br/><strong>[1] "Discrete"</strong></pre>
<ol start="6">
<li>Only two actions are available in this environment. Let's create the agent now:</li>
</ol>
<pre style="padding-left: 60px">agent &lt;- random_discrete_agent(ActionSpaceInfo[["n"]])</pre>
<p style="padding-left: 60px">The <kbd>random_discrete_agent()</kbd> function simply creates a sample random discrete agent.</p>
<ol start="7">
<li>Now we will set the folder in which to save the results, and open a window to monitor changes in the environment:</li>
</ol>
<pre style="padding-left: 60px">OutDir &lt;- "/TempFolder/results"<br/>env_monitor_start(Client, InstanceId, OutDir, force = TRUE, resume = FALSE)</pre>
<ol start="8">
<li>Let's initialize some variables that we will need in the simulation:</li>
</ol>
<pre style="padding-left: 60px">EpisodeCount &lt;- 100<br/>MaxSteps &lt;- 200<br/>Reward &lt;- 0<br/>done &lt;- FALSE</pre>
<ol start="9">
<li>Now we will implement two loops to interact with the environment using random actions:</li>
</ol>
<pre style="padding-left: 60px">for (i in 1: EpisodeCount) {<br/>  Object &lt;- env_reset(Client, InstanceId)<br/>  for (i in 1: MaxSteps) {<br/>    action &lt;- env_action_space_sample(Client, InstanceId)<br/>    results &lt;- env_step(Client, InstanceId, action, render = TRUE)<br/>    if (results[["done"]]) break<br/>  }<br/>}</pre>
<p style="padding-left: 60px">The following screenshot is returned:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/016ee645-08c2-4be8-a4a2-d24226453de5.png" style="width:31.50em;height:22.42em;"/></p>
<p style="padding-left: 60px">In the screenshot, you can see the cart-pole that will move according to the indications provided by the algorithm.</p>
<ol start="10">
<li>Finally, we will close the window, <span>using the following command</span>:</li>
</ol>
<pre style="padding-left: 60px">env_monitor_close(Client, InstanceId)</pre>
<p>This example helped us to start interacting with the environments available in OpenAI Gym. If you have not understood some passages, don't worry, in the following section we can learn more about them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenAI Gym methods</h1>
                </header>
            
            <article>
                
<p>OpenAI Gym provides the <kbd>env</kbd> class, which encapsulates the environment and its possible internal dynamics. The class has different methods and attributes to implement to create a new environment. The most important methods are called reset, step, and render, let's look at<span> them </span>briefly:</p>
<ul>
<li>The <strong>reset method</strong> has the task of resetting the environment, initializing it to the initial state. Within the reset method, the definitions of the elements that make up the environment must be contained, in this case the definition of the mechanical arm, of the object to be grasped and its support.</li>
<li>The <strong>step method</strong> has the task of moving the environment forward by one step. It requires the action to be performed as input, and returns the new observation to the agent. Within the method, the management of the dynamics of the movements, the calculation of the status and of the reward, and the controls for completing the episode must be defined.</li>
<li>The third and last method is<span> that we must define </span><span>which interior</span> to <strong>render</strong> to, as the elements at each step must be represented. The method involves different types of rendering, such as human, <kbd>rgb_array</kbd>, or <kbd>ansi</kbd>. With the human type, the rendering is done on the screen or command-line interface, and the method does not return anything; with the <kbd>rgb_array</kbd> type, invoking the method returns an n-dimensional array representing the RGB pixels of the screen; choosing the third type, the return method returns a string containing a textual representation. To render, OpenAI Gym provides the viewer class, through which you can draw the elements of the environment as a set of polygons and circles.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Regarding the attributes of the environment, the <kbd>env</kbd> class provides the definition of action space, observation space, and reward range. The action space attribute represents the action space, which is the set of possible actions that the agent can perform within the environment. Using the observation space attribute, the number of parameters that makes up the state is defined, and for each of them the range of values that can be assumed. The reward range attribute contains the minimum and maximum rewards obtainable in the environment, by default set to (-∞, + ∞).</p>
<div class="packt_tip">Using the <kbd>env</kbd> class proposed by the framework as a basis for new environments, the common interface provided by the toolkit is adopted. In this way, the environments created can be integrated into the toolkit library, and their dynamics can be learned from algorithms already implemented by users of the OpenAI Gym community.</div>
<p>In the following section, we will implement a robot control system using an OpenAI Gym environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Robot control system using the FrozenLake environment</h1>
                </header>
            
            <article>
                
<p>Technically speaking, a robot can be seen as a particular type of automatic control, that is, an automaton physically located in an environment of which it can perceive certain characteristics through components called sensors, and on which it can perform actions with the aim of making changes to it. These actions are performed by so-called actuators.</p>
<p>All that is interposed between the measurements made by the sensors and the commands given to the actuators can be defined as the control program or the controller of the robot. This is the component in which the intelligence of the robot is encoded, and in a certain sense it constitutes the brain that must guide its actions in order to obtain the desired behavior. A controller can be implemented in various ways: usually it is software running on one or more microcontrollers physically integrated into the system (onboard), but it can also be obtained through electronic circuits (analog or digital) directly wired into the hardware of the robot. Let's start by taking a look at the environment</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The FrozenLake environment</h1>
                </header>
            
            <article>
                
<p>The FrozenLake environment (<a href="https://gym.openai.com/envs/FrozenLake-v0/">https://gym.openai.com/envs/FrozenLake-v0/</a><a href="https://gym.openai.com/envs/FrozenLake-v0/">)</a> is a 4 x 4 grid that contains four possible areas: <strong>Safe</strong> (<strong>S</strong>), <strong>Frozen</strong> (<strong>F</strong>), <strong>Hole</strong> (<strong>H</strong>), and <strong>Goal</strong> (<strong>G</strong>). The agent controls the movement of a character in a grid world, and moves around the grid until it reaches the goal or the hole. Some tiles of the grid are walkable, and others lead to the agent falling into the water. If it falls into the hole, it must start from the beginning and is rewarded the value 0. The agent moves following an uncertain path that depends only in part on the chosen direction. The agent is rewarded when he finds a possible path to the set goal. The agent has four possible moves: up, down, left, and right. The process continues until it learns from every mistake and reaches the goal eventually.</p>
<p>The surface is described using a grid like the following:</p>
<ul>
<li>SFFF (<strong>S</strong>: starting point, safe)</li>
<li>FHFH (<strong>F</strong>: frozen surface, safe)</li>
<li>FFFH (<strong>H</strong>: hole, fall to your doom)</li>
<li>HFFG (<strong>G</strong>: goal, where the frisbee is located)</li>
</ul>
<p>In the following diagram, we can see the FrozenLake grid (4 x 4):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f63117d0-0eed-4b9c-951a-9918920fdb17.png" style="width:6.75em;height:6.75em;"/></p>
<p>The episode ends when you reach the goal or fall in a hole. You receive a reward of one if you reach the goal, and zero otherwise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Q-learning solution</h1>
                </header>
            
            <article>
                
<p>As we said in <a href="9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml">Chapter 7</a>, <em>Temporal Difference Learning</em>, Q-learning is one of the most used reinforcement learning algorithms. This is due to its ability to compare the expected utility of the available actions without requiring an environment model. Thanks to this technique it is possible to find an optimal action for every given state in a finished <strong>Markov decision process</strong> (<strong>MDP</strong>).</p>
<p>In the following code, we will use a Q-learning approach to find the right path from the start cell to goal cell in a 4 x 4 grid environment:</p>
<ol>
<li>As always, we will analyze the code line by line. Let's start by importing the library:</li>
</ol>
<pre style="padding-left: 60px">library(gym)</pre>
<ol start="2">
<li>To start, we will establish the connection with the client to interact with the server:</li>
</ol>
<pre style="padding-left: 60px">remote_base &lt;- "http://127.0.0.1:5000"<br/>client &lt;- create_GymClient(remote_base)<br/>print(client)</pre>
<p style="padding-left: 60px">The <kbd>create_GymClient()</kbd> function instantiates a <kbd>GymClient</kbd> instance to integrate with an OpenAI Gym server. The following result is printed:</p>
<pre style="padding-left: 60px"><strong>&lt;GymClient: http://127.0.0.1:5000&gt;</strong></pre>
<ol start="3">
<li>The connection has been established; now we can create the environment:</li>
</ol>
<pre style="padding-left: 60px">env_id &lt;- "FrozenLake-v0"<br/>instance_id &lt;- env_create(client, env_id)<br/>print(instance_id)</pre>
<p style="padding-left: 60px">The following result is printed:</p>
<pre style="padding-left: 60px"><strong>[1] "af775b0a"</strong></pre>
<ol start="4">
<li>Now we can list all the environments created with the bees available to us in the current work session:</li>
</ol>
<pre style="padding-left: 60px">AllEnvsAvailable &lt;- env_list_all(Client)<br/>print(AllEnvsAvailable)</pre>
<p style="padding-left: 60px">The following result is printed:</p>
<pre style="padding-left: 60px"><strong>$af775b0a</strong><br/><strong>[1] "FrozenLake-v0"</strong></pre>
<p style="padding-left: 60px">In this way, we have confirmation that the simulation environment has been correctly created.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>Now, we will retrieve some information from the environment:</li>
</ol>
<pre style="padding-left: 60px">ActionSpaceInfo &lt;- env_action_space_info(client, instance_id)<br/>print(ActionSpaceInfo)</pre>
<p style="padding-left: 60px">The <kbd>env_action_space_info()</kbd> function evaluates whether an action is a member of an environment action space.  The following results are printed:</p>
<pre style="padding-left: 60px"><strong>$n</strong><br/><strong>[1] 4</strong><br/><br/><strong>$name</strong><br/><strong>[1] "Discrete"</strong></pre>
<p style="padding-left: 60px">Four actions are available in this environment, as follows:</p>
<ul>
<li style="padding-left: 30px">0: Move left</li>
<li style="padding-left: 30px">1: Move down</li>
<li style="padding-left: 30px">2: Move right</li>
<li style="padding-left: 30px">3: Move up</li>
</ul>
<ol start="6">
<li>Now let's take a look at the states that an agent that moves in this environment can take:</li>
</ol>
<pre style="padding-left: 60px">ObservationSpaceInfo &lt;- env_observation_space_info(client, instance_id)<br/>print(ObservationSpaceInfo)</pre>
<p style="padding-left: 60px">The <kbd>env_observation_space_info()</kbd> function gets information (name and dimensions/bounds) of the environment observation space. The following results are printed:</p>
<pre style="padding-left: 60px"><strong>$n</strong><br/><strong>[1] 16</strong><br/><br/><strong>$name</strong><br/><strong>[1] "Discrete"</strong></pre>
<p style="padding-left: 60px"><span>From the following diagram, we can see that sixteen states are</span> available in this environment (0 – 15) covering a 4x4 grid, counting each position from left to right, top to bottom as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/de441345-9210-4c14-bf3c-b9e38f7935c6.png" style="width:14.92em;height:15.08em;"/></p>
<ol start="7">
<li>We will then extract these values in order to reuse them:</li>
</ol>
<pre style="padding-left: 60px">ActionSize = action_space_info$n<br/>StateSize = observation_space_info$n</pre>
<ol start="8">
<li>Now, let's initialize the parameters starting with <kbd>QTable</kbd>:</li>
</ol>
<pre style="padding-left: 60px">Qtable &lt;- matrix(data = 0, nrow = StateSize, ncol = ActionSize)</pre>
<p style="padding-left: 60px"><kbd>QTable</kbd> has a number of rows equal to the size of the observation space (<kbd>observation_space_info$n</kbd>), while the columns are equal to the size of the action space (<kbd>action_space_info$n</kbd>). As we said, the FrozenLake environment provides a state for each cell in the 4 x 4 grid, and four actions, returning a 16 x 4 table.</p>
<p style="padding-left: 60px">This table is initialized with all zeros using the <kbd>matrix()</kbd> function as follows:</p>
<pre style="padding-left: 60px">print(Qtable)</pre>
<p style="padding-left: 60px">The following table is printed:</p>
<pre style="padding-left: 60px"><strong>      [,1] [,2] [,3] [,4]</strong><br/><strong> [1,]    0    0    0    0</strong><br/><strong> [2,]    0    0    0    0</strong><br/><strong> [3,]    0    0    0    0</strong><br/><strong> [4,]    0    0    0    0</strong><br/><strong> [5,]    0    0    0    0</strong><br/><strong> [6,]    0    0    0    0</strong><br/><strong> [7,]    0    0    0    0</strong><br/><strong> [8,]    0    0    0    0</strong><br/><strong> [9,]    0    0    0    0</strong><br/><strong>[10,]    0    0    0    0</strong><br/><strong>[11,]    0    0    0    0</strong><br/><strong>[12,]    0    0    0    0</strong><br/><strong>[13,]    0    0    0    0</strong><br/><strong>[14,]    0    0    0    0</strong><br/><strong>[15,]    0    0    0    0</strong><br/><strong>[16,]    0    0    0    0</strong></pre>
<p style="padding-left: 60px">Now, we define some parameters:</p>
<pre style="padding-left: 60px">alpha = 0.80<br/>gamma = 0.95</pre>
<p style="padding-left: 60px">Basically, <kbd>alpha</kbd> is the learning rate, and <kbd>gamma</kbd> is the discount factor. The learning rate handles the updating of the information acquired, in the sense that it establishes when it is time to replace the old acquisitions with the new ones. Setting the learning rate with a value of 0 means that the agent does not learn anything. This will only exploit previous knowledge. With a value of 1 the agent considers only the most recent information and ignores previous knowledge. This is the exploration-exploitation dilemma. Ideally, the agent must explore all possible actions for each state, finding the one that is actually most rewarded for exploiting it in achieving its goal. The discount factor determines the importance of future rewards. A factor of 0 will consider only current rewards, while a factor approaching 1 will make it strive for a long-term high reward.</p>
<ol start="9">
<li>Now, we set the number of episodes:</li>
</ol>
<pre style="padding-left: 60px">NumEpisodes = 200</pre>
<p style="padding-left: 60px">The <kbd>NumEpisodes</kbd> parameter has the following meaning. The agent learns through experience, without a tutor to guide him; this way represents a learning without supervision. The agent will explore until he reaches the goal, moving from one state to the next. Every exploration is called an episode. Each episode consists of the movement of the agent that moves from the initial state to the objective state. Each time the agent arrives at the target state, we move on to the next episode.</p>
<ol start="10">
<li>Now, we will create a list to contain total rewards:</li>
</ol>
<pre style="padding-left: 60px">RewardsList = list()</pre>
<p style="padding-left: 60px">Then two more parameters are initialized; we will need them to recover the results:</p>
<pre style="padding-left: 60px">IndxList = 1<br/>NumGoal=0</pre>
<ol start="11">
<li>At this point, after setting the parameters, it is possible to start the Q-learning cycle:</li>
</ol>
<pre style="padding-left: 60px">for (i in 1:NumEpisodes) {<br/>  cat("######Episode ", i, " ######", "\n")</pre>
<ol start="12">
<li>So, we initialize the system using the <kbd>env_reset()</kbd> method: </li>
</ol>
<pre style="padding-left: 60px">  state = env_reset(client, instance_id)</pre>
<p style="padding-left: 60px">Then, a reward-counter and cycle-counter are initialized: </p>
<pre style="padding-left: 60px">  SumReward = 0<br/>  j = 0</pre>
<p style="padding-left: 60px">From this, the Q-learning table algorithm is implemented:</p>
<pre style="padding-left: 60px">  while (j &lt; 99){</pre>
<ol start="13">
<li>We increase the cycle counter at each new step:</li>
</ol>
<pre style="padding-left: 60px">    j=j+1</pre>
<ol start="14">
<li>Now, we have to choose an action:</li>
</ol>
<pre>    action = which.max(Qtable[state+1,] + runif(4,0, 1)*(1./(i+1)))<br/>    action = action - 1</pre>
<p style="padding-left: 60px">An action is chosen by greedily method-picking from <kbd>Qtable</kbd>. A noise is added because the environment is unknown, so it has to be explored in some way—your agent will do so using the power of randomness. Two functions are used, <kbd>which.max()</kbd> and <kbd>runif()</kbd>. The <kbd>which.max()</kbd> function returns the indices of the maximum values along an axis. The <kbd>runif()</kbd> function returns a sample (or samples) from the standard normal distribution. In the second line of code of the block just analyzed, we reduce the action index by one unit. This is necessary because as already mentioned the actions available in the environment range from 0 to 3 while in r the indexes of the tables start at 1 (unlike Python which instead starts from 0). Then to make the results compatible with the environment r it is necessary to make this correction.</p>
<ol start="15">
<li>Now, we will use the <kbd>env_step()</kbd> method to return the new states in response to the actions with which we call it. Obviously, the action we pass to the method is the one we have just decided:   </li>
</ol>
<pre style="padding-left: 60px">Results&lt;- env_step(client, instance_id, action, render = TRUE)</pre>
<p style="padding-left: 60px">The <kbd>env_step()</kbd> method returns a list consisting of the following:</p>
<ul>
<li style="padding-left: 60px"><kbd>action</kbd>: An action to take in the environment</li>
<li style="padding-left: 60px"><kbd>observation</kbd>: An agent's observation of the current environment</li>
<li style="padding-left: 60px"><kbd>reward</kbd>: The amount of reward returned after previous action</li>
<li style="padding-left: 60px"><kbd>done</kbd>: Whether the episode has ended</li>
<li style="padding-left: 60px"><kbd>info</kbd>: A list containing auxiliary diagnostic information</li>
</ul>
<ol start="16">
<li>For now, we are only interested in the following code: </li>
</ol>
<pre style="padding-left: 60px">NewState&lt;-Results$observation<br/>ActualReward&lt;-Results$reward</pre>
<ol start="17">
<li>We can then update the <kbd>Qtable</kbd> field with new knowledge:</li>
</ol>
<pre style="padding-left: 60px">    Qtable[state+1, action+1] = (1 - alpha) * Qtable[state+1, action+1] + alpha * (ActualReward + gamma * max(Qtable[NewState+1,]))</pre>
<p style="padding-left: 60px">The formula used for the Q-function update was the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b2d791ea-9828-439f-a82e-5367a8efd8c6.png" style="width:26.83em;height:1.25em;"/></p>
<p style="padding-left: 60px">Also, in this case, we had to introduce a correction for the state and the action (state + 1, action + 1) that the FrozenLake environment expects to assume the following values 0-15 for the state and 0-3 for the actions, but as we know the row and column index in r starts from 1.</p>
<ol start="18">
<li>Now, we will update the sum of the rewards with the one just obtained and the state of the environment:</li>
</ol>
<pre style="padding-left: 60px">    SumReward = SumReward + ActualReward<br/>    state = NewState</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="19">
<li>Finally, we insert a check to see if we have reached the end of the episode. We remember in this regard that the episode is considered finished when we reach the goal or end up in a hole:</li>
</ol>
<pre style="padding-left: 60px">    if (Results[["done"]]) {<br/>      print("#####STEP######")<br/>      print(j)<br/>      cat("State achieved", NewState+1, "\n")<br/>      cat("Reward", ActualReward, "\n")<br/><br/>      if (state==15) {<br/>        NumGoal=NumGoal+1<br/><br/>      }<br/>      break<br/>    }<br/>  }</pre>
<p style="padding-left: 60px">The <kbd>if</kbd> loop used for the end-of-episode check contains a series of instructions that guide us in finding the solution. In fact, I inserted a control that discriminates between reaching the objective and falling into a hole and then a series of prints that allow us to verify the success of the algorithm.</p>
<ol start="20">
<li>Before leaving the cycle that runs through the episodes definitively, we will try to update the sum of the rewards and the index of the rewards list:</li>
</ol>
<pre style="padding-left: 60px">  RewardsList[IndxList]&lt;-SumReward<br/>  IndxList = IndxList +1<br/>}</pre>
<ol start="21">
<li>Finally, we print the results, first the score:</li>
</ol>
<pre style="padding-left: 60px">cat("Numbers of goal achieved ", NumGoal, "\n")<br/>print ("Score: ")<br/>print(do.call(sum,RewardsList)/NumEpisodes)</pre>
<p style="padding-left: 60px">Then, we print the <kbd>Qtable</kbd> values:</p>
<pre style="padding-left: 60px">print ("Final Q-Table Values")<br/>print (Qtable)</pre>
<p style="padding-left: 60px">The following table is returned:</p>
<pre style="padding-left: 60px"><strong>              [,1]         [,2]         [,3]         [,4]</strong><br/><strong> [1,] 8.269208e-02 2.915901e-03 0.0026752394 2.927068e-03</strong><br/><strong> [2,] 3.346197e-05 4.689209e-05 0.0001546271 1.524209e-01</strong><br/><strong> [3,] 6.866991e-04 9.183121e-04 0.0032547141 2.434100e-01</strong><br/><strong> [4,] 1.191637e-03 5.130816e-04 0.0005775460 7.667945e-02</strong><br/><strong> [5,] 8.717573e-02 2.206559e-04 0.0002355794 3.511920e-04</strong><br/><strong> [6,] 0.000000e+00 0.000000e+00 0.0000000000 0.000000e+00</strong><br/><strong> [7,] 6.317772e-05 4.168548e-05 0.1003525858 6.572051e-05</strong><br/><strong> [8,] 0.000000e+00 0.000000e+00 0.0000000000 0.000000e+00</strong><br/><strong> [9,] 2.501462e-03 9.431424e-04 0.0022643051 7.393004e-02</strong><br/><strong>[10,] 4.658178e-05 5.656864e-01 0.0000000000 1.469386e-04</strong><br/><strong>[11,] 1.261091e-02 4.726838e-04 0.0005489759 2.321079e-03</strong><br/><strong>[12,] 0.000000e+00 0.000000e+00 0.0000000000 0.000000e+00</strong><br/><strong>[13,] 0.000000e+00 0.000000e+00 0.0000000000 0.000000e+00</strong><br/><strong>[14,] 8.009153e-04 0.000000e+00 0.8782760471 2.432799e-04</strong><br/><strong>[15,] 0.000000e+00 9.929596e-01 0.0000000000 0.000000e+00</strong><br/><strong>[16,] 0.000000e+00 0.000000e+00 0.0000000000 0.000000e+00</strong></pre>
<p style="padding-left: 60px">The table containing the value function is now ready, and we can use it to extract the paths that will take us from the starting cell to the goal cell without falling into the holes.</p>
<ol start="22">
<li>To begin, we limit our search to five possible paths:</li>
</ol>
<pre style="padding-left: 60px">for (episode in 1:5) {<br/>  print("****************EPISODE**********************")<br/>  print(episode)</pre>
<ol start="23">
<li>To do this, we used a cycle and we included the print of each episode for a check. Now, as we did in the training phase, let's start: reset the environment using the <kbd>env_reset()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">  state = env_reset(client, instance_id)</pre>
<p style="padding-left: 60px">We recall in this regard that the <kbd>env_reset()</kbd> function returns the environment to the initial state, that is, the state 0 which corresponds to the first cell. As we have already highlighted previously to make the OpenAI Gym environment designed for Python compatible with the environment, it is necessary to make a correction by adding a unit. In this way, the state 0 will correspond to the first line of the Qtable.</p>
<ol start="24">
<li>The next <kbd>for</kbd> loop will allow us to move within the grid to reach the goal following the directions provided by <kbd>Qtable</kbd>:</li>
</ol>
<pre style="padding-left: 60px">for (step in 1:100) {</pre>
<ol start="25">
<li>Let's now initialize a rewards counter:</li>
</ol>
<pre style="padding-left: 60px">TotRew = 0</pre>
<ol start="26">
<li>Now, we will extract the actions that have the maximum expected future reward given the actual state, that is, related to the current step. The Qtable returns this value as the index of the maximum value in the row corresponding to the current state:</li>
</ol>
<pre style="padding-left: 60px">action = which.max(Qtable[state+1,])<br/>action = action - 1</pre>
<p style="padding-left: 60px">Once again, we reduce the action index by one unit.</p>
<ol start="27">
<li>Now, we will use the <kbd>env_step()</kbd> method to return the new states in response to the actions extracted:     </li>
</ol>
<pre style="padding-left: 60px">Results&lt;- env_step(client, instance_id, action, render = TRUE)</pre>
<p style="padding-left: 60px">Of the values returned by the <kbd>env_step()</kbd> function, we are currently only interested in the status and reward:</p>
<pre style="padding-left: 60px">Newstate&lt;-Results$observation<br/>reward&lt;-Results$reward</pre>
<ol start="28">
<li>Next, we will update the reward counter:</li>
</ol>
<pre style="padding-left: 60px">TotRew &lt;- TotRew + reward</pre>
<ol start="29">
<li>Now, we will check whether we have reached the end of the episode:       </li>
</ol>
<pre style="padding-left: 60px">    if (Results[["done"]]) {<br/>      cat("Number of steps", step, "\n")<br/>      print(TotRew)<br/>      cat("STATO", Newstate, "\n")<br/>      break<br/><br/>    }<br/>    state = Newstate<br/>  }<br/>}</pre>
<p>In this way, we have obtained five of the best routes to reach the goal without falling into the holes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned the general concepts of game theory. We learned about the essential characteristics of the games and how the solutions adopted can help us solve real-life problems. We analyzed a series of real applications in which the theories are used to obtain solutions. Following that, we analyzed in detail the OpenAI Gym library and how to interact with the available environments to simulate real problems.</p>
<p>Then, we tackled the tic-tac-toe game using reinforcement learning. We used the <kbd>tictactoe</kbd> package to set the environment and to train an agent using Q-learning to play the game. Finally, we looked at the FrozenLake environment. This is a 4 × 4 grid that contains four possible areas: Safe (S), Frozen (F), Hole (H), and Goal (G). The agent controls the movement of a character in a grid world, and moves around the grid until it reaches the goal or the hole. This environment is particularly suitable for simulating problems related to the mobility of a robot in an environment full of obstacles. After defining the environment, we created an agent that is able to move within the environment and find the goal using an algorithm based on Q-learning.</p>
<p>In the next chapter, we will learn the fundamental concepts of finance problems; how to forecast stock market prices, and how to optimize equity portfolios. Then, we will look at how to implement fraud detection techniques.</p>


            </article>

            
        </section>
    </body></html>