["```py\nfor iteration=1, 2, 3, … do\n    Execute the current policy and collect a set of trajectories\n    At each timestep in each trajectory, compute\n        the return Rt and the advantage estimate .\n    Refit the baseline minimizing ,\n        summed over all trajectories and timesteps.\n    Update the policy using the policy gradient estimate \nend for\n```", "```py\npip install torch==0.4.1\npip install pillow\npip install gym \"gym[box2d]\"\n```", "```py\n    import gym\n    import torch as T\n    import numpy as np\n    ```", "```py\n    class ActorCritic(T.nn.Module):\n        def __init__(self):\n            super(ActorCritic, self).__init__()\n            self.transform = T.nn.Linear(8, 128)\n            self.act_layer = T.nn.Linear(128, 4) # Action layer\n            self.val_layer = T.nn.Linear(128, 1) # Value layer\n            self.log_probs = []\n            self.state_vals = []\n            self.rewards = []\n    ```", "```py\n        def forward(self, state):\n            state = T.from_numpy(state).float()\n            state = T.nn.functional.relu(self.transform(state))\n            state_value = self.val_layer(state)\n            act_probs = T.nn.functional.softmax\\\n                        (self.act_layer(state))\n            act_dist = T.distributions.Categorical(act_probs)\n            action = act_dist.sample()\n            self.log_probs.append(act_dist.log_prob(action))\n            self.state_vals.append(state_value)\n            return action.item()\n    ```", "```py\n        def computeLoss(self, gamma=0.99):\n            rewards = []\n            discounted_reward = 0\n            for reward in self.rewards[::-1]:\n                discounted_reward = reward + gamma \\\n                                    * discounted_reward\n                rewards.insert(0, discounted_reward)\n            rewards = T.tensor(rewards)\n            rewards = (rewards – rewards.mean()) / (rewards.std())\n            loss = 0\n            for log_probability, value, reward in zip\\\n            (self.log_probs, self.state_vals, rewards):\n                advantage = reward – value.item()\n                act_loss = -log_probability * advantage\n                val_loss = T.nn.functional.smooth_l1_loss\\\n                           (value, reward)\n                loss += (act_loss + val_loss)\n            return loss\n    ```", "```py\n        def clear(self):\n            del self.log_probs[:]\n            del self.state_vals[:]\n            del self.rewards[:]\n    ```", "```py\n    np.random.seed(0)\n    ```", "```py\n    env = gym.make(\"\"LunarLander-v2\"\")\n    policy = ActorCritic()\n    optimizer = T.optim.Adam(policy.parameters(), \\\n                             lr=0.02, betas=(0.9, 0.999))\n    ```", "```py\n    render = True\n    np.random.seed(0)\n    running_reward = 0\n    for i in np.arange(0, 10000):\n        state = env.reset()\n        for t in range(10000):\n            action = policy(state)\n            state, reward, done, _ = env.step(action)\n            policy.rewards.append(reward)\n            running_reward += reward\n            if render and i > 1000:\n                env.render()\n            if done:\n                break\n        print(\"Episode {}\\tReward: {}\".format(i, running_reward))\n        # Updating the policy\n        optimizer.zero_grad()\n        loss = policy.computeLoss(0.99)\n        loss.backward()\n        optimizer.step()\n        policy.clear()\n        if i % 20 == 0:\n            running_reward = running_reward / 20\n            running_reward = 0\n    ```", "```py\n    Episode 0\tReward: -320.65657506841114\n    Episode 1\tReward: -425.64874914703705\n    Episode 2\tReward: -671.2867424162646\n    Episode 3\tReward: -1032.281198268248\n    Episode 4\tReward: -1224.3354097571892\n    Episode 5\tReward: -1543.1792365484055\n    Episode 6\tReward: -1927.4910808775028\n    Episode 7\tReward: -2023.4599189797761\n    Episode 8\tReward: -2361.9002491621986\n    Episode 9\tReward: -2677.470775357419\n    Episode 10\tReward: -2932.068423127369\n    Episode 11\tReward: -3204.4024449864355\n    Episode 12\tReward: -3449.3136628102934\n    Episode 13\tReward: -3465.3763860613317\n    Episode 14\tReward: -3617.162199366013\n    Episode 15\tReward: -3736.83983321837\n    Episode 16\tReward: -3883.140249551331\n    Episode 17\tReward: -4100.137703945375\n    Episode 18\tReward: -4303.308164747067\n    Episode 19\tReward: -4569.71587308837\n    Episode 20\tReward: -4716.304224574078\n    ```", "```py\nimport os\nimport gym\nimport torch as T\nimport numpy as np\n```", "```py\nclass OUActionNoise(object):\n    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\n        self.theta = theta\n        self.mu = mu\n        self.sigma = sigma\n        self.dt = dt\n        self.x0 = x0\n        self.reset()\n    def __call__(self):\n        x = self.x_previous\n        dx = self.theta * (self.mu –- x) * self.dt + self.sigma \\\n             * np.sqrt(self.dt) * np.random.normal\\\n             (size=self.mu.shape)\n        self.x_previous = x + dx\n        return x\n    def reset(self):\n        self.x_previous = self.x0 if self.x0 is not None \\\n                          else np.zeros_like(self.mu)\n```", "```py\nclass ReplayBuffer(object):\n    def __init__(self, max_size, inp_shape, nb_actions):\n        self.memory_size = max_size\n        self.memory_counter = 0\n        self.memory_state = np.zeros\\\n                            ((self.memory_size, *inp_shape))\n        self.new_memory_state = np.zeros\\\n                                ((self.memory_size, *inp_shape))\n        self.memory_action = np.zeros\\\n                             ((self.memory_size, nb_actions))\n        self.memory_reward = np.zeros(self.memory_size)\n    self.memory_terminal = np.zeros(self.memory_size, \\\n                                    dtype=np.float32)\n```", "```py\n    def store_transition(self, state, action, \\\n                         reward, state_new, done):\n        index = self.memory_counter % self.memory_size\n        self.memory_state[index] = state\n        self.new_memory_state[index] = state_new\n        self.memory_action[index] = action\n        self.memory_reward[index] = reward\n        self.memory_terminal[index] = 1  - done\n        self.memory_counter += 1\n```", "```py\n    def sample_buffer(self, bs):\n        max_memory = min(self.memory_counter, self.memory_size)\n        batch = np.random.choice(max_memory, bs)\n        states = self.memory_state[batch]\n        actions = self.memory_action[batch]\n        rewards = self.memory_reward[batch]\n        states_ = self.new_memory_state[batch]\n        terminal = self.memory_terminal[batch]\n        return states, actions, rewards, states_, terminal\n```", "```py\nDDPG_Example.ipynb\nclass ReplayBuffer(object):\n    def __init__(self, max_size, inp_shape, nb_actions):\n        self.memory_size = max_size\n        self.memory_counter = 0\n        self.memory_state = np.zeros((self.memory_size, *inp_shape))\n        self.new_memory_state = np.zeros\\\n                                ((self.memory_size, *inp_shape))\n        self.memory_action = np.zeros\\\n                             ((self.memory_size, nb_actions))\n        self.memory_reward = np.zeros(self.memory_size)\n        self.memory_terminal = np.zeros(self.memory_size, \\\n                                        dtype=np.float32)\nThe complete code for this example can be found at https://packt.live/2YNL2BO.\n```", "```py\nclass CriticNetwork(T.nn.Module):\n    def __init__(self, beta, inp_dimensions,\\\n                 fc1_dimensions, fc2_dimensions,\\\n                 nb_actions):\n        super(CriticNetwork, self).__init__()\n        self.inp_dimensions = inp_dimensions\n        self.fc1_dimensions = fc1_dimensions\n        self.fc2_dimensions = fc2_dimensions\n        self.nb_actions = nb_actions\n        self.fc1 = T.nn.Linear(*self.inp_dimensions, \\\n                               self.fc1_dimensions)\n        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)\n        self.fc2 = T.nn.Linear(self.fc1_dimensions, \\\n                               self.fc2_dimensions)\n        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)\n        self.action_value = T.nn.Linear(self.nb_actions, \\\n                                        self.fc2_dimensions)\n        f3 = 0.003\n        self.q = T.nn.Linear(self.fc2_dimensions, 1)\n        T.nn.init.uniform_(self.q.weight.data, -f3, f3)\n        T.nn.init.uniform_(self.q.bias.data, -f3, f3)\n        self.optimizer = T.optim.Adam(self.parameters(), lr=beta)\n        self.device = T.device(\"\"gpu\"\" if T.cuda.is_available() \\\n                               else \"\"cpu\"\")\n        self.to(self.device)\n```", "```py\ndef forward(self, state, action):\n    state_value = self.fc1(state)\n    state_value = self.bn1(state_value)\n    state_value = T.nn.functional.relu(state_value)\n    state_value = self.fc2(state_value)\n    state_value = self.bn2(state_value)\n    action_value = T.nn.functional.relu(self.action_value(action))\n    state_action_value = T.nn.functional.relu\\\n                         (T.add(state_value, action_value))\n    state_action_value = self.q(state_action_value)\n    return state_action_value\n```", "```py\nDDPG_Example.ipynb\nclass CriticNetwork(T.nn.Module):\n    def __init__(self, beta, inp_dimensions,\\\n                 fc1_dimensions, fc2_dimensions,\\\n                 nb_actions):\n        super(CriticNetwork, self).__init__()\n        self.inp_dimensions = inp_dimensions\n        self.fc1_dimensions = fc1_dimensions\n        self.fc2_dimensions = fc2_dimensions\n        self.nb_actions = nb_actions\nThe complete code for this example can be found at https://packt.live/2YNL2BO.\n```", "```py\nclass ActorNetwork(T.nn.Module):\n    def __init__(self, alpha, inp_dimensions,\\\n                 fc1_dimensions, fc2_dimensions, nb_actions):\n        super(ActorNetwork, self).__init__()\n        self.inp_dimensions = inp_dimensions\n        self.fc1_dimensions = fc1_dimensions\n        self.fc2_dimensions = fc2_dimensions\n        self.nb_actions = nb_actions\n        self.fc1 = T.nn.Linear(*self.inp_dimensions, \\\n                               self.fc1_dimensions)\n        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n        self.bn1 = T.nn.LayerNorm(self.fc1_dimensions)\n        self.fc2 = T.nn.Linear(self.fc1_dimensions, \\\n                               self.fc2_dimensions)\n        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n        self.bn2 = T.nn.LayerNorm(self.fc2_dimensions)\n        f3 = 0.003\n        self.mu = T.nn.Linear(self.fc2_dimensions, \\\n                              self.nb_actions)\n        T.nn.init.uniform_(self.mu.weight.data, -f3, f3)\n        T.nn.init.uniform_(self.mu.bias.data, -f3, f3)\n        self.optimizer = T.optim.Adam(self.parameters(), lr=alpha)\n        self.device = T.device(\"gpu\" if T.cuda.is_available() \\\n                               else \"cpu\")\n        self.to(self.device)\n    def forward(self, state):\n        x = self.fc1(state)\n        x = self.bn1(x)\n        x = T.nn.functional.relu(x)\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = T.nn.functional.relu(x)\n        x = T.tanh(self.mu(x))\n        return x\n```", "```py\n    class Agent(object):\n        def __init__(self, alpha, beta, inp_dimensions, \\\n                     tau, env, gamma=0.99, nb_actions=2, \\\n                     max_size=1000000, l1_size=400, \\\n                     l2_size=300, bs=64):\n            self.gamma = gamma\n            self.tau = tau\n            self.memory = ReplayBuffer(max_size, inp_dimensions, \\\n                                       nb_actions)\n            self.bs = bs\n            self.actor = ActorNetwork(alpha, inp_dimensions, \\\n                                      l1_size, l2_size, \\\n                                      nb_actions=nb_actions)\n            self.critic = CriticNetwork(beta, inp_dimensions, \\\n                                        l1_size, l2_size, \\\n                                        nb_actions=nb_actions)\n            self.target_actor = ActorNetwork(alpha, inp_dimensions, \\\n                                             l1_size, l2_size, \\\n                                             nb_actions=nb_actions)\n            self.target_critic = CriticNetwork(beta, inp_dimensions, \\\n                                               l1_size, l2_size, \\\n                                               nb_actions=nb_actions)\n            self.noise = OUActionNoise(mu=np.zeros(nb_actions))\n            self.update_params(tau=1)\n    ```", "```py\n    def select_action(self, observation):\n        self.actor.eval()\n        observation = T.tensor(observation, dtype=T.float)\\\n                      .to(self.actor.device)\n        mu = self.actor.forward(observation).to(self.actor.device)\n        mu_prime = mu + T.tensor(self.noise(),\\\n                                 dtype=T.float).to(self.actor.device)\n        self.actor.train()\n        return mu_prime.cpu().detach().numpy()\n    ```", "```py\n    def remember(self, state, action, reward, new_state, done):\n        self.memory.store_transition(state, action, reward, \\\n                                     new_state, done)\n    ```", "```py\n    def learn(self):\n        if self.memory.memory_counter < self.bs:\n            return\n        state, action, reward, new_state, done = self.memory\\\n                                                 .sample_buffer\\\n                                                 (self.bs)\n        reward = T.tensor(reward, dtype=T.float)\\\n                 .to(self.critic.device)\n        done = T.tensor(done).to(self.critic.device)\n        new_state = T.tensor(new_state, dtype=T.float)\\\n                    .to(self.critic.device)\n        action = T.tensor(action, dtype=T.float).to(self.critic.device)\n        state = T.tensor(state, dtype=T.float).to(self.critic.device)\n        self.target_actor.eval()\n        self.target_critic.eval()\n        self.critic.eval()\n        target_actions = self.target_actor.forward(new_state)\n        critic_value_new = self.target_critic.forward\\\n                           (new_state, target_actions)\n        critic_value = self.critic.forward(state, action)\n        target = []\n        for j in range(self.bs):\n            target.append(reward[j] + self.gamma\\\n                          *critic_value_new[j]*done[j])\n        target = T.tensor(target).to(self.critic.device)\n        target = target.view(self.bs, 1)\n        self.critic.train()\n        self.critic.optimizer.zero_grad()\n        critic_loss = T.nn.functional.mse_loss(target, critic_value)\n        critic_loss.backward()\n        self.critic.optimizer.step()\n        self.critic.eval()\n        self.actor.optimizer.zero_grad()\n        mu = self.actor.forward(state)\n        self.actor.train()\n        actor_loss = -self.critic.forward(state, mu)\n        actor_loss = T.mean(actor_loss)\n        actor_loss.backward()\n        self.actor.optimizer.step()\n        self.update_params()\n    ```", "```py\n    def update_params(self, tau=None):\n        if tau is None:\n            tau = self.tau # tau is 1\n        actor_params = self.actor.named_parameters()\n        critic_params = self.critic.named_parameters()\n        target_actor_params = self.target_actor.named_parameters()\n        target_critic_params = self.target_critic.named_parameters()\n        critic_state_dict = dict(critic_params)\n        actor_state_dict = dict(actor_params)\n        target_critic_dict = dict(target_critic_params)\n        target_actor_dict = dict(target_actor_params)\n        for name in critic_state_dict:\n            critic_state_dict[name] = tau*critic_state_dict[name]\\\n                                      .clone() + (1-tau)\\\n                                      *target_critic_dict[name]\\\n                                      .clone()\n        self.target_critic.load_state_dict(critic_state_dict)\n        for name in actor_state_dict:\n            actor_state_dict[name] = tau*actor_state_dict[name]\\\n                                     .clone() + (1-tau)\\\n                                     *target_actor_dict[name]\\\n                                     .clone()\n        self.target_actor.load_state_dict(actor_state_dict)\n    ```", "```py\n    env = gym.make(\"LunarLanderContinuous-v2\")\n    agent = Agent(alpha=0.000025, beta=0.00025, \\\n                  inp_dimensions=[8], tau=0.001, \\\n                  env=env, bs=64, l1_size=400, \\\n                  l2_size=300, nb_actions=2)\n    for i in np.arange(100):\n        observation = env.reset()\n        action = agent.select_action(observation)\n        state_new, reward, _, _ = env.step(action)\n        observation = state_new\n        env.render()\n        print(\"Episode {}\\tReward: {}\".format(i, reward))\n    ```", "```py\n    Episode 0\tReward: -0.2911892911560017\n    Episode 1\tReward: -0.4945150137594737\n    Episode 2\tReward: 0.5150667951556557\n    Episode 3\tReward: -1.33324749569461\n    Episode 4\tReward: -0.9969126433110092\n    Episode 5\tReward: -1.8466220765944854\n    Episode 6\tReward: -1.6207456680346013\n    Episode 7\tReward: -0.4027838988393455\n    Episode 8\tReward: 0.42631743995534066\n    Episode 9\tReward: -1.1961709218053898\n    Episode 10\tReward: -1.0679394471159185\n    ```", "```py\n    import gym\n    import torch as T\n    import numpy as np\n    ```", "```py\n    device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n    ```", "```py\n    class ReplayBuffer:\n        def __init__(self):\n            self.memory_actions = []\n            self.memory_states = []\n            self.memory_log_probs = []\n            self.memory_rewards = []\n            self.is_terminals = []\n        def clear_memory(self):\n            del self.memory_actions[:]\n            del self.memory_states[:]\n            del self.memory_log_probs[:]\n            del self.memory_rewards[:]\n            del self.is_terminals[:]\n    ```", "```py\n    class ActorCritic(T.nn.Module):\n    def __init__(self, state_dimension, action_dimension, \\\n                 nb_latent_variables):\n        super(ActorCritic, self).__init__()\n        self.action_layer = T.nn.Sequential\\\n                            (T.nn.Linear(state_dimension, \\\n                                         nb_latent_variables),\\\n                            T.nn.Tanh(),\\\n                            T.nn.Linear(nb_latent_variables, \\\n                                        nb_latent_variables),\\\n                            T.nn.Tanh(),\\\n                            T.nn.Linear(nb_latent_variables, \\\n                                        action_dimension),\\\n                            T.nn.Softmax(dim=-1))\n        self.value_layer = T.nn.Sequential\\\n                           (T.nn.Linear(state_dimension, \\\n                                        nb_latent_variables),\\\n                           T.nn.Tanh(), \\\n                           T.nn.Linear(nb_latent_variables, \\\n                                       nb_latent_variables),\\\n                           T.nn.Tanh(),\\\n                           T.nn.Linear(nb_latent_variables, 1))\n    ```", "```py\n    # Sample from the action space\n    def act(self, state, memory):\n        state = T.from_numpy(state).float().to(device)\n        action_probs = self.action_layer(state)\n        dist = T.distributions.Categorical(action_probs)\n        action = dist.sample()\n        memory.memory_states.append(state)\n        memory.memory_actions.append(action)\n        memory.memory_log_probs.append(dist.log_prob(action))\n        return action.item()\n    # Evaluate log probabilities\n    def evaluate(self, state, action):\n        action_probs = self.action_layer(state)\n        dist = T.distributions.Categorical(action_probs)\n        action_log_probs = dist.log_prob(action)\n        dist_entropy = dist.entropy()\n        state_value = self.value_layer(state)\n        return action_log_probs, \\\n               T.squeeze(state_value), dist_entropy\n    ```", "```py\n    Exercise11_03.ipynb\n    class ActorCritic(T.nn.Module):\n        def __init__(self, state_dimension, \\\n                     action_dimension, nb_latent_variables):\n            super(ActorCritic, self).__init__()\n            self.action_layer = T.nn.Sequential(T.nn.Linear\\\n                                               (state_dimension, \\\n                                                nb_latent_variables),\\\n                                T.nn.Tanh(), \\\n                                T.nn.Linear(nb_latent_variables, \\\n                                            nb_latent_variables),\\\n                                T.nn.Tanh(),\\\n                                T.nn.Linear(nb_latent_variables, \\\n                                            action_dimension),\\\n                                T.nn.Softmax(dim=-1))\n    The complete code for this example can be found at https://packt.live/2zM1Z6Z.\n    ```", "```py\n    class Agent:\n        def __init__(self, state_dimension, action_dimension, \\\n                     nb_latent_variables, lr, betas, gamma, \\\n                     K_epochs, eps_clip):\n            self.lr = lr\n            self.betas = betas\n            self.gamma = gamma\n            self.eps_clip = eps_clip\n            self.K_epochs = K_epochs\n            self.policy = ActorCritic(state_dimension,\\\n                                      action_dimension,\\\n                                      nb_latent_variables)\\\n                                      .to(device)\n            self.optimizer = T.optim.Adam\\\n                             (self.policy.parameters(), \\\n                              lr=lr, betas=betas)\n            self.policy_old = ActorCritic(state_dimension,\\\n                                          action_dimension,\\\n                                          nb_latent_variables)\\\n                                          .to(device)\n            self.policy_old.load_state_dict(self.policy.state_dict())\n            self.MseLoss = T.nn.MSELoss()\n    ```", "```py\n        def update(self, memory):\n            # Monte Carlo estimate\n            rewards = []\n            discounted_reward = 0\n            for reward, is_terminal in \\\n                zip(reversed(memory.memory_rewards), \\\n                             reversed(memory.is_terminals)):\n                if is_terminal:\n                    discounted_reward = 0\n                discounted_reward = reward + \\\n                                    (self.gamma * discounted_reward)\n                rewards.insert(0, discounted_reward)\n    ```", "```py\n            rewards = T.tensor(rewards).to(device)\n            rewards = (rewards - rewards.mean()) \\\n                       / (rewards.std() + 1e-5)\n            # Convert to Tensor\n            old_states = T.stack(memory.memory_states)\\\n                         .to(device).detach()\n            old_actions = T.stack(memory.memory_actions)\\\n                          .to(device).detach()\n            old_log_probs = T.stack(memory.memory_log_probs)\\\n                            .to(device).detach()\n            # Policy Optimization\n            for _ in range(self.K_epochs):\n                log_probs, state_values, dist_entropy = \\\n                self.policy.evaluate(old_states, old_actions)\n    ```", "```py\n                # Finding ratio: pi_theta / pi_theta__old\n                ratios = T.exp(log_probs - old_log_probs.detach())\n                # Surrogate Loss\n                advantages = rewards - state_values.detach()\n                surr1 = ratios * advantages\n                surr2 = T.clamp(ratios, 1-self.eps_clip, \\\n                                1+self.eps_clip) * advantages\n                loss = -T.min(surr1, surr2) \\\n                       + 0.5*self.MseLoss(state_values, rewards) \\\n                       - 0.01*dist_entropy\n                # Backpropagation\n                self.optimizer.zero_grad()\n                loss.mean().backward()\n                self.optimizer.step()\n    ```", "```py\n            # New weights to old policy\n            self.policy_old.load_state_dict(self.policy.state_dict())\n    ```", "```py\nExercise11_03.ipynb\nenv = gym.make(\"LunarLander-v2\")\nrender = False\nsolved_reward = 230\nlogging_interval = 20\nupdate_timestep = 2000\nnp.random.seed(0)\nmemory = ReplayBuffer()\nagent = Agent(state_dimension=env.observation_space.shape[0],\\\n              action_dimension=4, nb_latent_variables=64, \\\n              lr=0.002, betas=(0.9, 0.999), gamma=0.99,\\\n              K_epochs=4, eps_clip=0.2)\ncurrent_reward = 0\navg_length = 0\ntimestep = 0\nfor i_ep in range(50000):\n    state = env.reset()\n    for t in range(300):\n        timestep += 1\nThe complete code for this example can be found at https://packt.live/2zM1Z6Z.\n```", "```py\nEpisode 0, reward: -8\nEpisode 20, reward: -182\nEpisode 40, reward: -154\nEpisode 60, reward: -175\nEpisode 80, reward: -136\nEpisode 100, reward: -178\nEpisode 120, reward: -128\nEpisode 140, reward: -137\nEpisode 160, reward: -140\nEpisode 180, reward: -150\n```", "```py\nif render:\n    env.render()\n\n    img = env.render(mode = \"rgb_array\")\n    img = Image.fromarray(img)\n    image_dir = \"./gif\"\n    if not os.path.exists(image_dir):\n        os.makedirs(image_dir)\n    img.save(os.path.join(image_dir, \"{}.jpg\".format(t)))\n```", "```py\n    Episode: 0, Reward: 272\n    Episode: 1, Reward: 148\n    Episode: 2, Reward: 249\n    Episode: 3, Reward: 169\n    Episode: 4, Reward: 35\n    ```"]