<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Working with Features</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we are going to take a close look at how features play an important role in the feature engineering technique. We'll learn some techniques that will allow us to improve our predictive analytics models in two ways: in terms of the performance metrics of our models and to understand the relationship between the features and the target variables that we are trying to predict.</p>
<p class="mce-root"><span>In this chapter, we are going to cover the following topics:</span></p>
<ul>
<li>Feature selection methods</li>
<li>Dimensionality reduction and PCA</li>
<li>Creating new features</li>
<li>Improving models with feature engineering</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature selection methods </h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Feature selection methods are used for selecting features that are likely to help with predictions.</span> The following are the three methods for feature selection:</p>
<ul>
<li class="mce-root">Removing dummy features with low variance</li>
<li class="mce-root">Identifying important features statistically</li>
<li class="mce-root">Recursive feature elimination</li>
</ul>
<p class="mce-root">When building predictive analytics models, some features won't be related to the target and this will prove to be less helpful in prediction. Now, the problem is that including irrelevant features in the model can introduce noise and add bias to the model. So, feature selection techniques are a set of techniques used to select the most relevant and useful features that will help either with prediction or with understanding our model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing dummy features with low variance</h1>
                </header>
            
            <article>
                
<p class="mce-root">The first technique of feature selection that we will learn about is removing dummy features with low variance. The only transformation that we have been applying so far to our features is to transform the categorical features using the encoding technique. If we take one categorical feature and use this encoding technique, we get a set of dummy features, which are to be examined to see whether they have variability or not. So, features with a very low variance are likely to have little impact on prediction. Now, why is that? Imagine that you have a dataset where you have a gender feature and that 98% of the observations correspond to just the female gender. This feature won't have any impact on prediction because almost all of the cases are just of a single category, so there is not enough variability. These cases become candidates lined up for elimination and such features should be examined more carefully. Now, take a look at the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5fc96b69-e4fa-497c-9c96-25fc7005239d.png" style="width:12.67em;height:2.92em;"/></p>
<p class="mce-root"/>
<p class="mce-root"><span>You can remove all dummy features that are either 0 or 1 in more than x% of the samples, or what you can do is to establish a minimum threshold for the variance of such features. Now, the variance of such features can be obtained with the preceding formula, </span>where <strong>p</strong> is the number or the proportion of <strong>1</strong> in your dummy features. We will see how this works in a Jupyter Notebook.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Identifying important features statistically</h1>
                </header>
            
            <article>
                
<p class="mce-root">This method will help you make use of some statistical tests for identifying and selecting relevant features. So, for example, for classification tasks we can use an ANOVA F-statistic to evaluate the relationship between numerical features and the target, which will be a categorical feature because this is an example of a classic task. Or, to evaluate the statistical relationship between a categorical feature and the target, we will use the chi-squared test to evaluate such a relationship. In <kbd>scikit-learn</kbd>, we can use the <kbd>SelectKBest</kbd> object and we will see how to use these objects in a Jupyter Notebook.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recursive feature elimination</h1>
                </header>
            
            <article>
                
<p class="mce-root">The process of identifying important features and removing the ones that we think are not important for our model is called <strong>recursive feature elimination</strong> (<strong>RFE</strong>). RFE can also be applied in <kbd>scikit-learn</kbd> and we can use this technique for calculating coefficients, such as linear, logistic regression, or with models to calculate something called <strong>feature importance</strong>. The random forests model provides us with those feature importance metrics. So, for models that don't calculate either coefficients or feature importance, these methods cannot be used; for example, for KNN models, you cannot apply the RFE technique because this begins by predefining the required features to use in your model. Using all features, this method fits the model and then, based on the coefficients or the feature importance, the least important features are eliminated. This procedure is recursively repeated on the selected set of features until the desired number of features to select is eventually reached.</p>
<p class="mce-root">There are the following few methods to select important features in your models:</p>
<ul>
<li class="mce-root">L1 feature</li>
<li class="mce-root">Selection threshold methods</li>
<li>Tree-based methods</li>
</ul>
<p>Let's go to our Jupyter Notebook to see how we actually apply these methods in <kbd>scikit-learn</kbd>. The following screenshot depicts the necessary libraries and modules to import: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d000765a-0b4c-4f0a-8728-f5099e1253e0.png" style="width:47.00em;height:27.75em;"/></p>
<p>In the following screenshot, we have <span>first</span><span> </span><span>used <span>the </span>credit card default dataset and we are applying the traditional transformations that we do to the raw data:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9f819808-eb10-415a-932d-7e3ea519fb41.png"/></p>
<p>The following screenshot shows the dummy features that we have in our dataset and the numerical features, depending on the type of feature:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4060697e-b56b-42ff-a373-fba66f231307.png"/></p>
<p>Here, we are applying the scaling operation for feature modeling:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/89e9ea9d-aabf-4dc1-9df0-c8b36d6e2b98.png" style="width:22.67em;height:8.25em;"/></p>
<p>The first method that we talked about in the presentation was removing dummy features with low variance to get the variances from our features using the <kbd>var()</kbd> method:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9f9a3dae-0abb-442a-a84e-06eee184e9e1.png" style="width:28.17em;height:3.25em;"/></p>
<p>Let's see the variances only for the dummy features; f<span>or example, a threshold for the variance will consider only the dummy features with a variance over <kbd>0.1</kbd>. In that case, with such a threshold of 0.1, the two candidates for elimination, </span><kbd>pay_5</kbd><span> and </span><kbd>pay_6</kbd><span>, would be the first few unnecessary dummy features with low variance that will be removed. Take a look at the following screenshot, which depicts the candidates for elimination:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/36866649-2516-4eca-b8ec-7d2e8624cf8f.png"/></p>
<p>The second approach that we talked about is statistically selecting the features that are related to the target, and we have two cases, dummy features and numerical features.</p>
<p>Let's perform the statistical tests for the dummy features. We are going to import objects in the <kbd>chi2</kbd> object from the <kbd>feature_selection</kbd> module in the <kbd>scikit-learn</kbd> library. We will also use the <kbd>SelectKBest</kbd> object to perform the statistical tests in all of the dummy features as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/22827f00-aeb2-41ce-8d4f-7fa9e1b74377.png" style="width:29.75em;height:3.67em;"/></p>
<p>Here, we instantiate an object called <kbd>dummy _selector</kbd> and pass the required statistical test to apply to it. Here, we are passing the <kbd>k ="all"</kbd> <span>argument </span>because this statistical test is to be applied to all of the dummy features. After instantiating this object, the <kbd>fit()</kbd> method is called. Take a look at the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fb58b92b-6578-4731-9691-e36707a4aca6.png" style="width:41.58em;height:5.08em;"/></p>
<p>In the following screenshot, we have the chi-squared scores. This isn't a statistical test and, the larger the number, the stronger the relationship between the feature and the target:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9a4ab20a-c9c9-447b-99b6-e9127829ea6f.png" style="width:41.50em;height:6.17em;"/></p>
<p>Now, if you remember your statistics class, this is a hypothesis testing setting. So, we can also calculate the p values and we can say that the features where <kbd>pvalues_</kbd> is greater than <kbd>0.05</kbd> are not related to the target. Now, in this case, we get very small p values for all of the features, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a45bc992-e373-4029-a504-a3a1cab481d5.png" style="width:33.17em;height:6.92em;"/></p>
<p>There is a relationship between the target and all of the dummy features, so under this methodology, we shouldn't eliminate any of these dummy features.</p>
<p>Now, we can use another statistical test called <kbd>f_ classif</kbd> <span>to evaluate the relationship between numerical features and the target, as shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a383b1d7-db46-402f-8685-561a59bb9efe.png" style="width:34.00em;height:3.33em;"/></p>
<p>Reusing this <kbd>f_classif</kbd> object, we will pass the required statistical tests and number of features. In this case, we want to apply the test to all numerical features and then use the <kbd>fit()</kbd> method again with the numerical features and the target:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7d176673-5b2a-4baf-bcca-c02a1d8d8d1c.png" style="width:39.42em;height:4.50em;"/></p>
<p>The p values that we receive from the application of this statistical test are shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1f7a09ea-105f-424e-8e4c-9c4887a7ced9.png" style="width:40.08em;height:8.42em;"/><br/></p>
<p>We can pass the <kbd>f_classif</kbd> statistical test and then select the numerical features that have a p value greater than <kbd>0.05</kbd>, which is the usual threshold for statistical tests; the resulting features here are <kbd>bill_amt4</kbd>, <kbd>bill_amt5</kbd>, and <kbd>bill_amt6</kbd>, which are likely to be irrelevant, or not related to the target:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1d128d68-8142-47e4-b6d0-95982e12dc43.png" style="width:34.08em;height:7.17em;"/></p>
<p class="mce-root"/>
<p>We have three candidates for elimination which can be eliminated or can be applied. We have used the second technique in the preceding steps and now we will use the third one in the following section.</p>
<p>The RFE is the third technique in which we will use the <kbd>RandomForestClassifier</kbd> model, and remember that we have 25 features here:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ccc37681-6a78-4244-ac36-8cef9d4360ee.png" style="width:27.17em;height:4.33em;"/></p>
<p>So, let's assume that we want to select only 12 features and we want a model that uses only 12 features. So, we are using about half of the features. We can use <span>the </span><kbd>RFE</kbd> object present in <kbd>scikit-learn</kbd> from the <kbd>feature_selection</kbd> module. We can use this to actually select these 12 features using the RFE technique. So, we instantiate this object by passing <span>the </span>required estimator and the number of features to select:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/515cf59d-8bc9-428d-99ee-c0693af054cc.png" style="width:48.58em;height:8.75em;"/></p>
<p>Now, remember that random forest provides us with a metric of feature importance, which can be used with the RFE technique:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7c041ba0-ebbf-4f72-acb1-331a690b14fe.png" style="width:34.67em;height:2.42em;"/></p>
<p><span>After using the <kbd>fit()</kbd> method on the whole dataset, </span>we get <kbd>recursive_selector.support_</kbd> and <kbd>True</kbd> for the features that are included in our model, the 12 that we wanted, and we get <kbd>False</kbd> for the ones that should be eliminated:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/316a9ba5-01e9-4406-b46e-7f3db0970ea8.png" style="width:42.08em;height:6.00em;"/></p>
<p>So, according to this object and method, we should include <span>the 12 most important features </span>in our random forest model in order to predict targets such as <kbd>limit_bal</kbd>, <kbd>age</kbd>, <kbd>pay</kbd>; all of the bill amounts; and <kbd>pay_amt1</kbd>, <kbd>pay_amt2</kbd>, and <kbd>pay_amt3</kbd>, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dc1ad6e3-8378-4971-b073-ec6ffe1631a1.png" style="width:35.00em;height:20.42em;"/></p>
<p>These are the features that should be eliminated because they are not very relevant according to this method and this model for predicting the target:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a290e198-14e2-4776-aa59-c31137a3b76c.png" style="width:35.50em;height:22.50em;"/></p>
<p>Now we can evaluate the simpler model, the one with the 12 features against the full model that we have been using so far, after which we can calculate the metrics using cross-validation. So, in this example, we are using 10-fold cross-validation to get an estimation of the performance of these two models. Remember, this selector model is the full model according to the RFE technique and these are the results:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c19b384c-506d-4e42-85bc-8cfe711c1e3f.png"/></p>
<p>The full model has a recall of <kbd>0.361365</kbd>, and the model that includes only 12 features has a recall of <kbd>0.355791</kbd>. Since this model has less recall, the full model remains the best one. But if we use half of the features, the full model will also give us similar performance:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a2f3dcbb-fedd-4e46-b2e9-3a4d52135093.png" style="width:34.58em;height:5.75em;"/></p>
<p>As you can see in the following screenshot, the values are really close:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d85e222b-999c-40d8-bf81-05b389a3b7c6.png"/></p>
<p>Now you can decide whether you want to use the full model or you want to use the simpler model. This is up to you, but in terms of accuracy we get almost the same, although with still a little bit more accuracy for the full model:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/de3fc718-3686-4136-a8e3-b483cdfa8f62.png"/></p>
<p>Now, you have a technique to decide whether you want to use a more complicated model that uses more features, or a simpler model. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dimensionality reduction and PCA</h1>
                </header>
            
            <article>
                
<p><span>The dimensionality reduction method is the process of reducing the number of features under consideration by obtaining a set of principal variables. The <strong>Principal Component Analysis </strong>(<strong>PCA</strong>) technique is the most important technique </span>used for dimensionality reduction. Here, we will talk about why we need dimensionality reduction, and we will also see how to perform the PCA technique in <kbd>scikit-learn</kbd>.</p>
<p>These <span>are the reasons for h</span>aving a high number of features while working on predictive analytics:</p>
<ul>
<li>It enables the simplification of models, in order to make them easier to understand and to interpret. There might be some computational considerations if you are dealing with thousands of features. It might be a good idea to reduce the number of features in order to save computational resources.</li>
<li>Another reason is to avoid <span>the </span>"curse of dimensionality." Now, this is a technical term and a set of problems that arise when you are working with high-dimensional data.</li>
<li>This also helps us to minimize overfitting because if you are including a lot of irrelevant features to predict the target, then your model can overfit to that noise. So, removing irrelevant features will help you with overfitting.</li>
</ul>
<p>Feature selection, seen<span> </span><span>earlier</span><span> in this chapter, can be considered a form of dimensionality reduction.</span> <span>When you have a set of features that are closely related or even redundant, PCA will be the preferred technique</span> <span>to encode the same information using less features. </span><span>So, what is PCA? It's a statistical procedure that converts a set of observations of possibly correlated variables into a set of linearly uncorrelated variables called <strong>principal components</strong>. Let's not go into the mathematical details about what's going on with PCA.</span></p>
<p><span>Let's assume we have a dataset that is two-dimensional. PCA identifies a direction where the dataset varies the most and encodes the maximum amount of information on these two features into one single feature to reduce the dimensions from two to one. This method projects every point onto these axes or new dimensions.</span></p>
<p class="mce-root"/>
<p><span>As you can see in the following screenshot, the first principal component of these two features would be the projections of the points onto the red line, which is the main mathematical intuition behind what's going on in PCA:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/941fd408-2576-441e-b2eb-37e2ea9a84fa.png" style="width:28.75em;height:28.08em;"/></p>
<p><span>Now, let's go to the Jupyter Notebook to see how to implement the dimensionality reduction method and to apply PCA on the given dataset:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0029c5de-ae04-47d1-b311-e42d0d1bff40.png" style="width:25.92em;height:10.17em;"/></p>
<p><span>In this case, we will use the credit card default dataset. So, here we are doing the transformations that we have covered so far:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c5fd83e2-cab0-468d-af4d-1270ef9b3656.png"/></p>
<p><span>Now, let's take a look at the bill amount features. We have six of these features, the history of the bill amounts from one to six months ago, which are closely related, as you can see from the visualization generated from the following screenshot of code snippets:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/360ba160-c0a2-457e-9435-abe501b4c447.png" style="width:35.50em;height:4.75em;"/></p>
<p><span>So, they represent the same information. If you see a customer with a very high bill amount two or three months ago, it is very likely that they also got a very high bill amount one month ago. So, these features, as you can see from the visualization shown in the following screenshot, are really closely related:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/11d8b84c-f9c7-4cb8-a6d2-8bea6e03ad7e.png"/></p>
<p><span>We confirm this with the calculation of the correlation coefficient. As you can see, they are really highly correlated:</span></p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c8a4f728-6e65-43d6-887f-331d841c3298.png" style="width:41.33em;height:18.83em;"/></p>
<p><span>The correlation between the bill amount one month ago and two months ago is <kbd>0.95</kbd>. We have very high correlations, which is a good opportunity to apply a dimensionality reduction technique, such as PCA in <kbd>scikit-learn</kbd>, for which we import it from <kbd>sklearn.decomposition</kbd> , as shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/14f1e55c-9574-4e3f-a89e-902ba51eb556.png" style="width:25.58em;height:2.67em;"/></p>
<p><span>After that, we instantiate this <kbd>PCA</kbd> object. Then, we pass the columns or the features that we want to apply PCA decomposition to:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f4998794-1b14-4646-83ca-ab9e871a4de8.png" style="width:48.42em;height:6.58em;"/></p>
<p><span>So after using the <kbd>fit()</kbd> method derived from this object, we receive one of the attributes, the explained variance ratio, as shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b99b1080-769c-40c4-8c7f-23e63fe9adbd.png" style="width:55.42em;height:4.17em;"/></p>
<p class="mce-root"/>
<p><span>Let's plot this quantity to get a feel for what's going on with these features. As you can see here, we get the explained variance of all six components:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e63ef069-1047-4913-b7d1-396f817af0df.png"/></p>
<p><span>The way to read this plot is that the first component of the PCA that we did on these six features encodes more than 90% of the total variance of all six features. The second one shows a very small variance, and the third, fourth, fifth, and sixth components also have minimal variance.</span></p>
<p><span>Now, we can see this in the plot of cumulative explained variance shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ef93a131-19d8-4ce0-b10e-698b21ef1cfa.png"/></p>
<p><span>As you can see, the first component encodes more than 90% of the variance of the six features that we used. So, you are getting more than 90% of the information in just one feature. Therefore, instead of using six features, you can use just one single feature and still get more than 90% of the variance. Or, you can use the first two components and get more than 95% of the total information contained in the six features in just two features, the first and second components of this PCA. So, this is how this works in practice and we can use this as one technique for performing feature engineering.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Feature engineering</h1>
                </header>
            
            <article>
                
<p>Feature engineering plays a vital role in making machine learning algorithms work and, if carried out properly, it enhances the predictive ability of machine learning algorithms. In other words, feature engineering is the process of extracting existing features or creating new features from the raw data using domain knowledge, the context of the problem, or specialized techniques that result in more accurate predictive models. This is an activity where domain knowledge and creativity play a very important role. This is an important process, which can significantly improve the performance of our predictive models. The more context you have about a problem, the better your ability to create new and useful features. Basically, the feature engineering process converts the features into input values that algorithms can understand.<br/>
There are various ways of implementing feature engineering. You might not find all of the techniques feasible and may end up excluding a few. The motive here is not to have an academic discussion about this topic, but to show you some of the common things that we do when we work with features and when we try to create new features. The first one is scaling features, used to transform their range to a more suitable one. The other one is to encode information in a better way, and we will see an example of this later in this chapter. Feature engineering involves creating new features from existing ones so that you can combine existing features by performing some mathematical operations on them.<br/>
Another way of creating new features is by using a dimensionality reduction technique, such as PCA, which we saw previously. It doesn't matter what technique you use, as long as you make it creative. As mentioned previously, the more knowledge you have about the problem, the better.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating new features</h1>
                </header>
            
            <article>
                
<p>We will be using the credit card default and diamond datasets here. Now, let’s go to the Jupyter Notebook to create new features and see what these techniques are in practice:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1f149084-6127-4ec7-be75-0550b89c0895.png" style="width:24.50em;height:9.25em;"/></p>
<p class="mce-root"/>
<p><span>Let's import the credit card default dataset by executing a few commands, as shown in the following screenshot:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2fcfac44-86fd-447e-a414-866ba15d6164.png"/></p>
<p>The first transformation that we will do is to create another way to encode the information that we have in the <kbd>education</kbd> feature. So far, we have been using one encoding technique in the <kbd>education</kbd> feature, and we will use the context of the <kbd>x</kbd> variable to come up with another encoding. People with graduate-level education are more highly educated than people with other levels of education. So, we can come up with some sort of points system for these features; for example, we can assign two points for people with graduate-level education, maybe one point for people with university-level education, and negative points for the other levels of education that we have in this dataset.<br/>
Let's take a look at the following screenshot to see how this is done:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c0580fcd-23d3-41a8-b5e1-9ec4978e0dd3.png"/></p>
<p>The previous screenshot reflects the sequence that we have in these education levels, so this could be an another way to encode information. This might or might not be helpful in predicting defaulters for the next month. However, we can try this new technique to encode this information and see the results in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/be40f9c7-3a02-4f08-a59f-befb73a3a7c0.png" style="width:41.58em;height:11.00em;"/></p>
<p class="NormalPACKT">Another technique that we can use in this dataset is to use the bill amount and payment amount features in the context of this problem to calculate the difference between these two variables/features. So, if we take the bill amount from a particular month and subtract the payment amount for that month, we will get an amount or quantity. In this example, we are calling the <kbd>bill_minus_pay</kbd> variable, which represents the payment made by the client against the bill for that month. So, this newly derived quantity can be used to predict defaulters for the next month. We have included them in a potential predictive model for this dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1275ad80-b856-4812-ac04-b562ac2fc4a7.png" style="width:50.58em;height:11.50em;"/></p>
<p class="NormalPACKT">Let's now take a look at the following output, which depicts the defaulters for a particular month:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7a5678a6-7c8b-4658-9c06-d20905e46e44.png"/></p>
<p>Another method that we can use here, now that we have part of the information of these features in a new feature called <kbd>bill_minus_pay</kbd>, is that we can summarize the main information of the six features shown in the preceding screenshot in just one feature using the PCA technique:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fbf447a5-af00-402d-8bb9-b5c0d34cb41d.png"/></p>
<p>We can do the same operation with the pay features. From the previous analysis, we know that the <kbd>pay_1</kbd> feature is very important for predicting who is going to pay next. So, in order to reduce the other five <kbd>pay_i</kbd> features to just two, we are reducing the six bill amount features to just one, and the six <kbd>pay_i</kbd> features to two. Apart from this, we again apply the PCA technique on the remaining five <kbd>pay_i</kbd> features to reduce these five to just one. Take a look at the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3b49bf31-73fe-48a7-9e42-dc69ef617e7e.png" style="width:46.25em;height:13.08em;"/></p>
<p>These are some of the feature engineering techniques, with examples, that you can perform on your datasets, but you might want to create other transformations or variables from the existing ones.</p>
<p>Now, let's see a couple of examples in the diamonds dataset. We need to import the diamonds dataset by executing a few commands, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1a5b38e2-d2e8-4ed0-b02e-add5805bddac.png"/></p>
<p>As seen in the preceding screenshot, we have transformed some of the categorical features using the encoding technique. Now, let's take a look at our imported dataset, shown in the following screenshot:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fa90eb24-2ac3-4ab2-8541-8e645dc22c8b.png"/></p>
<p>This is what our scatter plot matrix with four features, <kbd>x</kbd>, <kbd>y</kbd>, <kbd>z</kbd>, and <kbd>price</kbd>, looks like. The first three features refer to the measurements of the diamond, and <kbd>price</kbd> represents how those three features are related to the pricing of the diamond:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/050dd302-e6ac-4f0e-a5e2-d12c18a1ec60.png"/></p>
<p>In the preceding screenshot, as you can see, there is a very strong linear relationship between the first three features, which is one of the interesting things in this scatter plot matrix.</p>
<p>As diamonds are three-dimensional objects, we will combine these three features into just a single feature, called volume.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, we will multiply the measurements of the <kbd>x</kbd>, <kbd>y</kbd>, and <kbd>z</kbd> axes, which will derive a number that is close to the volume of that object:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d50740ee-536d-4157-bc16-bd22cdc23c3c.png" style="width:39.75em;height:3.00em;"/></p>
<p>Now, we know that they are not boxes, and they don't have any fixed shape. However, this will be a really good approximation of the volume of the diamonds. So, this can be another way in which we can create a new feature volume from the existing features in this dataset.</p>
<p>In the next screenshot, we have the volume of our object and also the weight of the diamond, which is measured as <kbd>carat</kbd>, and we will use these to create a new feature called <kbd>density</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3af98542-6143-49f9-97c4-1b739b8fe2f1.png" style="width:41.08em;height:3.08em;"/></p>
<p>As you can see in the preceding screenshot, we have divided the carat by the volume in order to get the density of the diamond object.</p>
<p>This is how we created two features from the given context, which justifies the statement: "the more the knowledge or context of the problem, the better". As you can see, with <span>just</span><span> </span><span>the provided knowledge of the problem, we were able to come up with new features.</span></p>
<p>Now, let's try and see how helpful these features might be in predicting models. The example we will use here is, how you can combine existing features to produce new features. The following plot shows the close relationship between the volume and price:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b91bb49e-1916-444c-9476-2e49dd2d06cb.png"/></p>
<p>We can assume that the volume will be helpful in predicting the price.</p>
<p>However, in the following scatterplot of density and price, we see that all diamonds have the same expected density:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c4374d52-538a-45a8-841e-ef21e716744d.png"/></p>
<p class="mce-root"/>
<p>When we see the correlation between <kbd>price</kbd> and <kbd>carat</kbd>, which we already had, it seems that <kbd>density</kbd> might not relate to <kbd>price</kbd> much:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/939ece93-e875-418f-b250-fa4e70de9f2a.png" style="width:37.33em;height:14.33em;"/></p>
<p>So, this new feature might not help much in prediction. The volume and carat features have the same kind of relationship. We might not gain a lot of predictive power with this feature, but the main goal behind explaining this example was to show how to combine different features that you already have in your dataset to create new features.</p>
<p>This is what feature engineering is all about. You might <span>also</span><span> </span><span>come up with other features for this dataset.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving models with feature engineering</h1>
                </header>
            
            <article>
                
<p>Now that we have seen how feature engineering techniques help in building predictive models, let's try and improve the performance of these models and evaluate whether the newly built model works better than the previous built model. Then, we will talk about two very important concepts that you must always keep in mind when doing predictive analytics, and these are the reducible and irreducible errors in your predictive models.</p>
<p>Let's first import the necessary modules, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d9b2c9df-4eeb-4402-82d7-a47930ea0880.png"/></p>
<p>So, let's go to the Jupyter Notebook and take a look at the imported credit card default dataset that we saw earlier in this chapter, but as you can see, some modifications have been made to this dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/18835e99-b7bb-4d75-bc85-c5ff4582984f.png"/></p>
<p>For this model, instead of transforming the <kbd>sex</kbd> and <kbd>marriage</kbd> features into two dummy features, the ones that we have been using were <kbd>male</kbd> and <kbd>married</kbd>; therefore, let's encode the information in a slightly different way to see if this works better. So, we will encode the information as <kbd>married_male</kbd> and <kbd>not_married_female</kbd>, and see if this works better. This is the first transformation that we are doing here. This is what the dataset looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a9782f93-7ebb-4755-89b9-1a18495aad5c.png"/></p>
<p>Now, let's do a little bit more feature engineering. The first thing that we will do is calculate these new features, which are built from subtracting the payment amount from the bill amount, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a543d2e0-8351-4c63-b35c-e118875fb5b4.png"/></p>
<p>For this problem, we will perform one mathematical operation. We will use the new features shown in the preceding screenshot to predict the target. Most of the information in the bill amount features is now encoded in these features, which are not needed anymore, but instead of throwing them away, what we can do is reduce the six bill amount features to just one using the PCA technique. So, let's apply the PCA technique to reduce the six features to just one component. Now there is a new feature called <kbd>bill_amt_new_feat</kbd>. So, this was the second feature engineering step that we performed. Finally, for the <kbd>pay_i</kbd> features, we will preserve the first one as is, and <span>apply the PCA technique to</span><span> the last five features,</span> <kbd>pay_2</kbd><span>, <kbd>pay_3</kbd>, <kbd>pay_4</kbd>, <kbd>pay_5</kbd>, and <kbd>pay_6</kbd>, to reduce these five features to just two components. You can use the</span> <kbd>fit_transform</kbd> <span>method on the <kbd>PCA</kbd> object to get the components.</span></p>
<p>Now, let's take a look at the following screenshot, showing all of the features that have to do with money. As you can see, the variances here are really huge because the currency <span>amounts are</span><span> large:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d9405c76-fd54-48c7-bb70-94af065be775.png"/></p>
<p class="mce-root"/>
<p>Now, rescale these features by dividing them by 1,000 in order to reduce the variances, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3ef1e002-540e-4c2b-8e7a-858c96a183b0.png" style="width:33.83em;height:22.83em;"/></p>
<p>This helps us to make these numbers understandable. So, this is the other transformation that we did, and now let's train our model with these new features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training your model</h1>
                </header>
            
            <article>
                
<p>The following model is a new module, as it has different features compared to the other models. Since the features have changed, we need to find the best hyperparameters for the <kbd>RandomForestClassifier</kbd> module using the <kbd>GridSearchCV</kbd> module. So, perhaps the previously found best parameters are not the best for these new features; therefore, we will run the <kbd>GridSearchCV</kbd> algorithm again:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7e2ac386-5dc1-4970-b04a-44456a49da23.png" style="width:39.50em;height:18.67em;"/></p>
<p>As shown in the following screenshot, in this case the best combination of parameters for these new features is <kbd>max _depth</kbd> of <kbd>30</kbd>, <kbd>max_features</kbd> in <kbd>auto</kbd>, and <kbd>n_estimators</kbd> (number of estimators) should be <kbd>100</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b4892e44-1011-4f54-9605-577ce1933e06.png" style="width:42.17em;height:4.92em;"/></p>
<p>Now, let's evaluate this new model that we have built using feature engineering, and let's compare it with the previous metrics that we have from the previously built model:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/af57d7ee-8abb-400f-b988-7d53e42ed206.png"/></p>
<p>As you can see in the preceding screenshot, we are using a threshold of 0.2. This model generates a recall of <kbd>71.39</kbd>% and a precision of <kbd>37.38</kbd>. Here, the precisions are similar, but, as mentioned earlier, the recall might be the metric that we should care about, as it's slightly different compared to the previous one. We got a little better recall for this model; the change may only be 2% or 3% , which might not look like much, but remember that in these financial applications, an improvement of 1% or 2% could, in practice, mean a lot of money. So, we got a slight improvement in the predictive power of our model using this little feature engineering technique; let's take a look at the feature importance in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9225a3eb-041c-46f0-bc51-d6724b30d4f0.png"/></p>
<p>You can assess whether this feature importance make sense in the following screenshot of the random forest model:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/6062dd52-525e-48e5-90ed-caa3e760dbe1.png"/></p>
<p>You can compare this feature importance with the previous ones. There are a lot of things that you can do after you have applied feature engineering. We may improve performance and gain insight from the model. It's been observed that we improved our model a little bit by using this technique. Now, you can come up with different ways to combine the existing features to improve the model even more. This was just a small, simple example to show you that you can actually play around with the features in a way that actually makes sense.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reducible and irreducible error</h1>
                </header>
            
            <article>
                
<p>Before moving on, there are two really important concepts to be covered for predictive analytics. Errors can be divided into the following two types:</p>
<ul>
<li><strong>Reducible errors</strong>: These errors can be reduced by making certain improvements to the model</li>
<li><strong>Irreducible errors</strong>: These errors cannot be reduced at all</li>
</ul>
<p>Let's assume that, in machine learning, there is a relationship between features and target that is represented with a function, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4dcab463-bc46-4047-9f46-8d6b362ce700.png" style="width:9.58em;height:2.50em;"/></p>
<p>Let’s assume that the target (<strong>y</strong>) is the underlying supposition of machine learning, and the relationship between the features and the target is given by a function. Since, in most cases we consider that there is some randomness in the relationship between features and target, we add a noise term here, which will always <span>be</span><span> </span><span>present in reality. This is the underlying supposition in machine learning.</span></p>
<p>In models, we try to approximate the theoretical function by using an actual function while performing feature engineering, tuning the parameters, and so on:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/86502f63-aa51-40f7-8605-e6feaecaf2c3.png" style="width:11.83em;height:4.17em;"/></p>
<p>So, our predictions are the results of the application of these approximations to the conceptual or theoretical <strong>f</strong>. All that we do in machine learning is try to approximate this <strong>f</strong> function by training the model.</p>
<p class="mce-root"/>
<p>Training a model means approximating this function. It is possible to show mathematically that the expected error, defined as the difference between the real <strong>y</strong> and the predicted <strong>y</strong>, can be decomposed into two terms. One term is called <strong>Reducible error</strong> and the other one is called <strong>Irreducible error</strong>, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ba4c0671-5d52-4204-806c-85b9c4ecfcd9.png" style="width:36.00em;height:6.33em;"/></p>
<p>Now, the <strong>Irreducible error</strong> term is the variance of this random term. You don't have any control over this term. There will always <span>be</span><span> </span><span>an irreducible error</span><span> </span><span>component</span><span>. So, your model will always make mistakes; it doesn't matter how many features and data points you have, your model cannot always </span><span>be</span><span> 100% correct. What we must try to do is to use better and more sophisticated methods to perform feature engineering, and try to approximate our estimation to the real function. Just because you are working with more sophisticated models or you have more data, your model will not be perfect and you will not be able to predict exactly what</span> <strong>y</strong> <span>is, because there is some randomness in almost all the processes that you will work with. So this is the end of a very interesting section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we talked about feature selection methods, how to distinguish between useful features, and features that are not likely to be helpful in prediction. We talked about dimensionality reduction and we learned how to perform PCA in <kbd>scikit-learn</kbd>. We also talked about feature engineering, and we tried to come up with new features in the datasets that we have been using so far. Finally, we tried to improve our credit card model by coming up with new features, and by working with all of the techniques that we learned in this chapter. I hope you have enjoyed this chapter. </p>
<p>In the next chapter, we will learn about artificial neural networks and how the <kbd>tensorflow</kbd> library is used when working with neural networks and artificial intelligence.</p>


            </article>

            
        </section>
    </body></html>