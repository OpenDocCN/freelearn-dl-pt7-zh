<html><head></head><body>
		<div id="_idContainer034">
			<h1 id="_idParaDest-24" class="chapter-number"><a id="_idTextAnchor023"/>2</h1>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor024"/>Azure OpenAI Fundamentals</h1>
			<p>In the previous chapter, we talked<a id="_idIndexMarker039"/> about <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>), LLM concepts, and different enterprise-ready LLM examples. We also talked about foundation model concepts and discussed different use cases for LLMs. In this chapter, we’re going to dive<a id="_idIndexMarker040"/> into <strong class="bold">Azure OpenAI</strong> (<strong class="bold">AOAI</strong>) Service, different model types, how to deploy models, and various <span class="No-Break">pricing aspects.</span></p>
			<p>We will navigate the following sections in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>What is <span class="No-Break">AOAI Service?</span></li>
				<li>AOAI <span class="No-Break">model types</span></li>
				<li>Accessing <span class="No-Break">AOAI Service</span></li>
				<li>Creating <span class="No-Break">AOAI resources</span></li>
				<li>Deploying <span class="No-Break">AOAI models</span></li>
				<li>Utilizing <span class="No-Break">AOAI models</span></li>
				<li><span class="No-Break">Pricing</span></li>
			</ul>
			<p>But before we get into all that, let’s understand the Microsoft and OpenAI partnership a <span class="No-Break">bit better.</span></p>
			<p>Microsoft has made a multi-billion-dollar investment into OpenAI to make sure they can develop advanced AI technology and share its benefits with everyone. This partnership builds upon Microsoft’s previous investments in 2019 and 2021. This allows both Microsoft and OpenAI to use the advanced AI technology they create for their businesses. Microsoft is also invested in powerful supercomputers to help OpenAI with their important <span class="No-Break">AI research.</span></p>
			<p>Microsoft has formed a strategic partnership with OpenAI, integrating their advanced LLMs while doing so. They use OpenAI’s models in various products and are creating innovative digital experiences. Microsoft has a service called AOAI where developers can harness the power of cutting-edge models, combined with Microsoft’s robust tools. Microsoft is the only company providing cloud services for OpenAI, which means they will handle all the computing work for OpenAI’s research, products, and services to build sophisticated AI applications. This collaboration enables the creation of transformative solutions, unlocking new possibilities in AI-driven development. In the next section, we’ll dive deeper into AOAI Service and how to <span class="No-Break">use it.</span></p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor025"/>What is AOAI Service?</h1>
			<p>Microsoft <a id="_idIndexMarker041"/>offers a wide range of AI tools and solutions to help customers at every stage of their AI journey, regardless of their team’s expertise. Whether you’re new to AI or have specific use cases in mind, Microsoft has you covered. They provide easy-to-use options for those starting while also supporting data scientists with more advanced needs. When you explore the Azure AI offerings, you have the freedom to start with high-level AI services and dig deeper into the Azure Machine Learning platform to build, train, tune, and deploy deep learning models at scale. The overall Azure AI stack is shown in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B21019_02_1.jpg" alt="Figure 2.1 – Azure AI stack"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Azure AI stack</p>
			<p>At the top layer, you have AI services for specific applications, such as <strong class="bold">Cognitive Search</strong>, <strong class="bold">Bot Service</strong>, and <strong class="bold">Document Intelligence</strong>. There are also domain-specific pretrained models such as <strong class="bold">Vision</strong>, <strong class="bold">Speech</strong>, <strong class="bold">Language</strong>, <strong class="bold">Decision</strong>, and <strong class="bold">Azure OpenAI Service</strong>. These are built on the foundation of Azure Machine Learning as a managed endpoint. If you’re using AI services, you don’t need to worry about the foundation layer. However, if you want to access the foundation layer so that you have control, you can use the bottom layer: <strong class="bold">Azure Machine Learning</strong>. It’s a managed end-to-end machine learning platform for building, training, deploying, and operating machine learning models responsibly and securely <span class="No-Break">at scale.</span></p>
			<p>AOAI Service (marked in a red rectangular box in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em>) is a new Azure AI Service that provides <a id="_idIndexMarker042"/>REST API access to OpenAI’s powerful language models, including the GPT-4 Turbo, GPT-4o, GPT4-o mini, GPT-3.5 Turbo, Whisper, DALL-E 3, and Embeddings model series. They have enterprise capabilities such as security, private networking, compliance, regional availability, and responsible AI content filtering that are available only on Microsoft Azure. These models can be easily adapted for a variety of tasks, including but not limited to content generation, summarization, semantic search, and natural language to code translation. Users can access the various services through REST APIs, the Python SDK, or a web-based interface in Azure <span class="No-Break">AI Foundry.</span></p>
			<p>Now, let’s talk about the different model types <span class="No-Break">in AOAI.</span></p>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>AOAI model types</h1>
			<p>AOAI Service <a id="_idIndexMarker043"/>offers various models that can do different things and have different costs. Let’s dive into the different model types and <span class="No-Break">their usability:</span></p>
			<ul>
				<li><strong class="bold">GPT base</strong>: GPT base models<a id="_idIndexMarker044"/> can comprehend<a id="_idIndexMarker045"/> and produce both natural language and code but lack specific training in following instructions. They’re designed to serve as alternatives to original GPT-3 base models and rely on the legacy Completions API. For most users, we recommend utilizing GPT-3.5 or GPT-4 for their tasks. These models come in two <span class="No-Break">different variations:</span><ul><li><strong class="bold">Babbage-002</strong>: Serves<a id="_idIndexMarker046"/> as a replacement for the GPT-3 ada and babbage base models. It can support up to 16,384 tokens and can <span class="No-Break">be fine-tuned.</span></li><li><strong class="bold">Davinci-002</strong>: Serves <a id="_idIndexMarker047"/>as a replacement for the GPT-3 curie and davinci base models. It can support up to 16,384 tokens and can <span class="No-Break">be fine-tuned.</span></li></ul></li>
				<li><strong class="bold">GPT-4</strong>: This<a id="_idIndexMarker048"/> is the latest model and is used for solving complex problems. It’s even more accurate than any of OpenAI’s earlier models. GPT4 models are <a id="_idIndexMarker049"/>capable of understanding and producing both natural language and coden along with advanced<a id="_idIndexMarker050"/> reasoning capabilities. It’s an optimized chat completion model, which means it’s best suited for interactive chat applications and performs exceptionally well with regular completion tasks. Being a large, highly optimized model, GPT-4 is capable of excelling at both interactive chats as well as completion activities. The latest GPT-4 class has three types of <span class="No-Break">flagship </span><span class="No-Break">models:</span><ul><li><strong class="bold">GPT-4 Turbo</strong>: This <a id="_idIndexMarker051"/>model supports a maximum of 128,000 input tokens/context window and 4,096 <span class="No-Break">output tokens</span></li><li><strong class="bold">GPT-4o</strong>: This <a id="_idIndexMarker052"/>model supports a maximum of 128,000 input tokens/context window and 4,096 <span class="No-Break">output tokens</span></li><li><strong class="bold">GPT-4o mini</strong>: This <a id="_idIndexMarker053"/>model supports a maximum of 128,000 input tokens/context window and 16,384 <span class="No-Break">output tokens</span></li></ul></li>
				<li><strong class="bold">GPT-3.5</strong>: GPT-3.5<a id="_idIndexMarker054"/> represents <a id="_idIndexMarker055"/>a series of models that build upon the capabilities of GPT-3. These models excel at comprehending and generating both human language and computer code. Among the GPT-3.5 models, the most capable and cost-efficient one is GPT-3.5 Turbo. It’s specifically fine-tuned for interactive conversations and performs well when it comes to regular completion tasks. The latest version of GPT-3.5 also comes in two <span class="No-Break">different flavors:</span><ul><li><strong class="bold">gpt-35-turbo-1106</strong>: This <a id="_idIndexMarker056"/>model supports a maximum of 16,385 input tokens/context window and 4,096 <span class="No-Break">output tokens.</span></li><li><strong class="bold">gpt-35-turbo-0125</strong>: This <a id="_idIndexMarker057"/>model supports a maximum of 16,385 input tokens/context window and 4,096 <span class="No-Break">output tokens.</span></li><li><strong class="bold">isgpt-35-turbo-instruct</strong>: This<a id="_idIndexMarker058"/> model supports a maximum of 4,097 input tokens/context window. This<a id="_idIndexMarker059"/> model cannot <span class="No-Break">be fine-tuned.</span></li></ul></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">GPT-3.5 Turbo Instruct offers comparable capabilities to text-davinci-003 but utilizes the Completions API, not the Chat Completions API. We strongly advise utilizing GPT-3.5 Turbo and GPT-3.5 Turbo Instruct rather than the older GPT-3.5 and <span class="No-Break">GPT-3 models.</span></p>
			<p class="list-inset">Here’s an<a id="_idIndexMarker060"/> interactive chat example that can employ either GPT-3.5 or GPT-4. Typically, you input a prompt as the user, and the model responds to the completion (model output). This can be a continuous exchange <span class="No-Break">in conversation:</span></p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B21019_02_2.jpg" alt="Figure 2.2: Basic prompt and completion example"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2: Basic prompt and completion example</p>
			<p class="list-inset">Along with prompt completion, there’s one more concept you need to understand about tokens. When you send a prompt to GPT-3.5 or GPT-4, it undergoes tokenization through the embedding process, where words or – more commonly – parts of words are converted into numeric vector representations. Numeric tokens are used instead of full words or sentences to process <a id="_idIndexMarker061"/>the information. This design allows the GPT model to handle relatively large volumes of text. However, there is a constraint regarding tokens in GPT-3.5 and GPT-4 (depending on which model type you choose, as mentioned previously) for both the input prompt and the generated <span class="No-Break">completion combined.</span></p>
			<p class="list-inset">To ensure you stay within the token limit, you can estimate the number of tokens required for your prompt and the resulting completion. As a rough guideline, in English, every four characters typically correspond to one token. Therefore, you can calculate the tokens needed by adding the character count of your prompt to the desired response length and dividing the total by four. This calculation provides you with a rough estimate of the token count required, which is valuable for planning tasks where token constraints must be kept in mind. <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.3</em> shows an example of the tokens for a <span class="No-Break">given sentence:</span></p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B21019_02_3.jpg" alt="Figure 2.3: Token counter"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3: Token counter</p>
			<p class="list-inset">In this example, there are a total of seven tokens. Notably, the word “generative” is represented by two distinct tokens, while the remaining words are each represented by a <span class="No-Break">single token.</span></p>
			<ul>
				<li><strong class="bold">Embedding models</strong>: An <a id="_idIndexMarker062"/>embedding is a<a id="_idIndexMarker063"/> list of vectors of floating-point numbers. When <a id="_idIndexMarker064"/>we measure how far apart two of these vectors are, it tells us how similar or different they are. If the distance is small, it means they’re very similar, but if it’s large, it means they’re quite different. When you input raw text into an embedding model, it produces a list of vector representations for the <span class="No-Break">provided text:</span></li>
			</ul>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B21019_02_4.jpg" alt="Figure 2.4: Basic embedding process"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4: Basic embedding process</p>
			<p class="list-inset">In <a id="_idIndexMarker065"/>practice, you <a id="_idIndexMarker066"/>provide a collection of document words to an embedding model as input, which then uses them to create an embedding vector. This vector is usually stored in a vector database, such as Azure Cognitive Search or Azure Cosmos DB. Subsequently, when a user submits a query, it passes through the same embedding model to generate a query vector, which is used to search for similar vectors in the vector database. This<a id="_idIndexMarker067"/> pattern is called <strong class="bold">retrieval-augmented </strong><span class="No-Break"><strong class="bold">generation</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">RAG</strong></span><span class="No-Break">).</span></p>
			<p class="list-inset">The following figure illustrates <span class="No-Break">this process:</span></p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B21019_02_5.jpg" alt="Figure 2.5: Document embedding process"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5: Document embedding process</p>
			<p class="list-inset">At the <a id="_idIndexMarker068"/>time of writing, AOAI service offers four different types of <span class="No-Break">embedding models:</span></p>
			<ul>
				<li><strong class="bold">text-embedding-ada-002 (version 1)</strong>: This version uses the GPT-2/GPT-3 tokenizer. It <a id="_idIndexMarker069"/>can handle a maximum of <strong class="bold">2,046</strong> input tokens and provides an output with <span class="No-Break"><strong class="bold">1,024</strong></span><span class="No-Break"> dimensions.</span></li>
				<li><strong class="bold">text-embedding-ada-002 (version 2)</strong>: This version uses the cl100k_base tokenizer. It <a id="_idIndexMarker070"/>supports a four times larger input with a maximum of <strong class="bold">8,191</strong> tokens and returns an output with <strong class="bold">1,536</strong> dimensions. This is the second-generation embedding model. We highly recommend using text-embedding-ada-002 version 2 as it provides parity with OpenAI’s text-embedding-ada-002 model in terms of its capabilities and performance. This model is not only more cost-effective but also simpler and <span class="No-Break">more efficient.</span></li>
				<li><strong class="bold">text-embedding-3-small</strong>: The<a id="_idIndexMarker071"/> new text-embedding-3-small model significantly outperforms its predecessor, text-embedding-ada-002, with an increase in benchmark scores from 31.4% to 44.0% for multi-language retrieval and from 61.0% to 62.3% for English tasks. Additionally, it is five times more cost-effective, reducing the price from $0.0001 to $0.00002 per 1k tokens. While text-embedding-ada-002 will remain available, text-embedding-3-small is recommended for its improved efficiency and performance. A new model, text-embedding-3-large, offers even greater capacity with embeddings for up to <span class="No-Break">3,072 dimensions.</span></li>
				<li><strong class="bold">text-embedding-3-large</strong>: The <a id="_idIndexMarker072"/>new text-embedding-3-large model is the top-performing embedding model, showing substantial improvements over text-embedding-ada-002. It achieves an average score of 54.9% on the MIRACL benchmark and 64.6% on the MTEB benchmark, up from 31.4% and 61.0%, respectively. Priced at $0.00013 per 1k tokens, text-embedding-3-large offers the highest performance among other embedding models, surpassing both text-embedding-3-small <span class="No-Break">and text-embedding-ada-002.</span></li>
			</ul>
			<p>To create a generative AI application, the key models you’ll primarily need are the ones mentioned here. It’s important to note that these models have only been trained on an extensive dataset that goes up to September 2021. Therefore, they don’t possess any knowledge or information beyond that date. If you want to create a generative AI application that incorporates the most up-to-date information, you can explore a technique known <a id="_idIndexMarker073"/>as <strong class="bold">RAG</strong>. We’ll explore this technique in more detail later in this book, providing hands-on tutorials and <span class="No-Break">in-depth discussions.</span></p>
			<p>There are<a id="_idIndexMarker074"/> additional models available as part of AOAI Service, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">DALL-E 3</strong>: This<a id="_idIndexMarker075"/> model can generate realistic images <a id="_idIndexMarker076"/>and artwork based on a description provided in <span class="No-Break">natural language.</span></li>
				<li><strong class="bold">Whisper</strong>: This<a id="_idIndexMarker077"/> is a general-purpose speech recognition (ASR) model. It has been trained on 680,000 hours of a vast and varied dataset containing both audio and text, and it can handle various tasks, such as recognizing speech in multiple languages, translating spoken words, and identifying languages. Whisper <a id="_idIndexMarker078"/>utilizes an encoder-decoder architecture based on Transformers, allowing it to convert spoken words into written text, and it can also handle special tokens to indicate the task or language involved. This model can be accessed through AOAI Service or <span class="No-Break">Azure Speech.</span></li>
			</ul>
			<p>Now that we’ve discussed various types of models, let’s discuss how to get access to <span class="No-Break">AOAI Service.</span></p>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/>Accessing AOAI Service</h1>
			<p>To <a id="_idIndexMarker079"/>access AOAI, you need to have an Azure account with an active subscription and AOAI access enabled. This section we will walk you through how to get AOAI <span class="No-Break">Service access.</span></p>
			<p><em class="italic">Step 1: Create an </em><span class="No-Break"><em class="italic">Azure account.</em></span></p>
			<p>At the time of writing, AOAI access can only be approved for enterprise customers and partners. So, technically, you can create an Azure account using one of four options (<a href="https://learn.microsoft.com/en-us/dotnet/azure/create-azure-account">https://learn.microsoft.com/en-us/dotnet/azure/create-azure-account</a>). However, when creating an account, you must use your company email address; personal email accounts will not be approved for <span class="No-Break">AOAI access:</span></p>
			<ul>
				<li><strong class="bold">Option 1</strong>: Visual Studio subscribers can use monthly Azure credits. This option allows you to activate your credits and use them <span class="No-Break">for AOAI.</span></li>
				<li><strong class="bold">Option 2</strong>: Sign up for a free Azure account. This option gives you 12 months of free services and $200 credit to explore Azure for 30 days. You can use this credit for AOAI if you’ve <span class="No-Break">been approved.</span></li>
				<li><strong class="bold">Option 3</strong>: Sign up for a pay-as-you-go account. This option charges you for what you use beyond the free limits. You can pay for AOAI with this option if you’ve <span class="No-Break">been approved.</span></li>
				<li><strong class="bold">Option 4</strong>: Use a corporate account. This option requires your company to have a cloud administrator who can grant you access <span class="No-Break">to AOAI.</span></li>
			</ul>
			<p>Once you create an Azure account, you will get a default Azure subscription. You can use that or create a new one to <span class="No-Break">access AOAI:</span></p>
			<ol>
				<li>You can find your subscription information under the <span class="No-Break"><strong class="bold">Subscriptions</strong></span><span class="No-Break"> resource:</span></li>
			</ol>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B21019_02_6.jpg" alt="Figure 2.6: Locating Subscriptions"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6: Locating Subscriptions</p>
			<ol>
				<li value="2">Note<a id="_idIndexMarker080"/> down the <strong class="bold">Subscription ID</strong> value; you’ll need it to apply for <span class="No-Break">AOAI access:</span></li>
			</ol>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B21019_02_7.jpg" alt="Figure 2.7: Retrieving subscriptions details"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7: Retrieving subscriptions details</p>
			<p><em class="italic">Step 2: Apply </em><span class="No-Break"><em class="italic">AOAI access.</em></span></p>
			<p>To apply AOAI access, follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>AOAI is a restricted general availability service. Typically, you will have access to create AOAI resources in your subscriptions. If you don’t have access, you must complete the form provided at <a href="https://aka.ms/oaiapply">https://aka.ms/oaiapply</a>. Please ensure that you use your company email address when filling out <span class="No-Break">this form.</span></li>
				<li>Upon completing the form, it will take approximately 3-4 days for it to be approved. Once your access is approved, you will receive an email from the Azure Cognitive <span class="No-Break">Service Team.</span><p class="list-inset">This <a id="_idIndexMarker081"/>approval will allow you to access and use <span class="No-Break">AOAI models.</span></p></li>
			</ol>
			<p>With that, we’ve covered how to obtain access to AOAI Service. Now, let’s turn our attention to how to utilize these <span class="No-Break">models effectively.</span></p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Creating AOAI resources</h1>
			<p>After gaining <a id="_idIndexMarker082"/>access to AOAI models, you can create an AOAI resource to deploy the models. This section will guide you through the process of creating <span class="No-Break">AOAI resources:</span></p>
			<ol>
				<li>Log in to the Azure subscription that you created previously within the Azure <span class="No-Break">portal (</span><a href="https://portal.azure.com/"><span class="No-Break">https://portal.azure.com/</span></a><span class="No-Break">).</span></li>
				<li>Navigate to <strong class="bold">Create a resource</strong> and search for <strong class="source-inline">Azure OpenAI</strong>. Once you find the service, <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Create</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B21019_02_8.jpg" alt="Figure 2.﻿8: Creating an AOAI resource"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8: Creating an AOAI resource</p>
			<ol>
				<li value="3">On the<a id="_idIndexMarker083"/> AOAI <strong class="bold">Creation</strong> page, provide the subsequent details within the <strong class="bold">Basics</strong> <span class="No-Break">tab’s fields:</span><table id="table001" class="T---Table _idGenTablePara-1"><colgroup><col/><col/></colgroup><tbody><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break"><strong class="bold">Field</strong></span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break"><strong class="bold">Description</strong></span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break"><strong class="bold">Subscriptions</strong></span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset">The Azure subscription mentioned in your application for onboarding <span class="No-Break">AOAI Service.</span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break"><strong class="bold">Resource Group</strong></span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset">The Azure resource group that will contain your AOAI resource. You can create a new group or use a <span class="No-Break">pre-existing one.</span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break"><strong class="bold">Region</strong></span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset">The geographical location of your instance. Varying locations can potentially introduce latency but won’t impact the operational availability of <span class="No-Break">your resource.</span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break"><strong class="bold">Name</strong></span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset">A descriptive name for your AOAI <span class="No-Break">Service resource.</span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break"><strong class="bold">Pricing Tier</strong></span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset">The pricing tier for the resource. At the time of writing, AOAI Service only offers the <span class="No-Break">Standard tier.</span></p></td></tr></tbody></table></li>
			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.1: AOAI resource creation details</p>
			<ol>
				<li value="4">Now, let’s create the <a id="_idIndexMarker084"/>AOAI resource by providing these details in the <span class="No-Break"><strong class="bold">Basics</strong></span><span class="No-Break"> tab:</span></li>
			</ol>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B21019_02_9.jpg" alt="Figure 2.﻿9: Basic information to create an AOAI resource"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9: Basic information to create an AOAI resource</p>
			<ol>
				<li value="5"><span class="No-Break">Click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
				<li>The <strong class="bold">Network</strong> tab offers three options for security. You can select option 2 or 3 from within the <strong class="bold">Network</strong> tab if you require enhanced security during deployment. This <a id="_idIndexMarker085"/>choice allows you to configure AOAI for increased security, making it accessible through Azure Virtual Network integration and a private endpoint connection. Later in this book, we will focus on exploring options 2 and 3. For now, choose option 1 (<strong class="bold">All networks, including the internet, can access this resource.</strong>) to access <span class="No-Break">the resource:</span></li>
			</ol>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B21019_02_10.jpg" alt="Figure 2.1﻿0: Public access network settings to create an AOAI resource"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10: Public access network settings to create an AOAI resource</p>
			<ol>
				<li value="7">Proceed by clicking <strong class="bold">Next</strong> and configure any tags for your resource <span class="No-Break">as needed.</span></li>
				<li>Click <strong class="bold">Next</strong> to advance to the final stage of the process: <strong class="bold">Review + </strong><span class="No-Break"><strong class="bold">submit</strong></span><span class="No-Break">.</span></li>
				<li>Verify your configuration settings, then click <strong class="bold">Create</strong> to initiate the process. Please be patient; this process may take 2-3 minutes <span class="No-Break">to complete.</span><p class="list-inset">The Azure portal will notify you once the new resource becomes available. Click <strong class="bold">Go to resource</strong> to access the newly <span class="No-Break">created resource.</span></p></li>
			</ol>
			<p>So far, we’ve set up AOAI Service. In the next section, we’ll deploy <span class="No-Break">the model.</span></p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Deploying AOAI models</h1>
			<p>In the previous <a id="_idIndexMarker086"/>section, you created an AOAI resource. With this in hand, you can proceed to deploy and utilize your models through various means, such as Azure AI Foundry, REST APIs, or an SDK. This section will guide you through the process of deploying these models from Azure AI Foundry and accessing them via Studio and the <span class="No-Break">Python SDK:</span></p>
			<ol>
				<li>Log in to your Azure subscription within the Azure <span class="No-Break">portal (</span><a href="https://portal.azure.com/"><span class="No-Break">https://portal.azure.com/</span></a><span class="No-Break">).</span></li>
				<li>In the resource search bar, search for <strong class="source-inline">Azure OpenAI</strong>. Locate the AOAI resource that you created in the <span class="No-Break">previous section:</span></li>
			</ol>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B21019_02_11.jpg" alt="Figure 2.1﻿1: Locating the AOAI resource created previously"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11: Locating the AOAI resource created previously</p>
			<ol>
				<li value="3">Click on the correct service name (highlighted by a red rectangle in the <span class="No-Break">preceding figure).</span></li>
				<li>Navigate to Azure AI Foundry by clicking either <strong class="bold">Explore Azure AI Foundry portal</strong> or <strong class="bold">Go to Azure AI </strong><span class="No-Break"><strong class="bold">Foundry portal</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B21019_02_12.jpg" alt="Figure 2.1﻿2: Accessing ﻿Azure AI Foundry"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12: Accessing Azure AI Foundry</p>
			<ol>
				<li value="5">Within<a id="_idIndexMarker087"/> the <strong class="bold">Management</strong> section, <span class="No-Break">choose </span><span class="No-Break"><strong class="bold">Deployments</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B21019_02_13.jpg" alt="Figure 2.1﻿3: Creating a new AOAI model deployment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13: Creating a new AOAI model deployment</p>
			<ol>
				<li value="6">Choose <strong class="bold">Deploy model</strong> and proceed to configure the <span class="No-Break">following fields:</span></li>
			</ol>
			<table id="table002" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Fi</strong></span><span class="No-Break"><strong class="bold">el</strong></span><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">d</strong></span></p>
						</td>
						<td class="No-Table-Style T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Description</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style T---Body">
							<p class="list-inset"><strong class="bold">Select </strong><span class="No-Break"><strong class="bold">a model</strong></span></p>
						</td>
						<td class="No-Table-Style T---Body">
							<p class="list-inset">Choose a model from the drop-down list. Model availability is subject to regional variations. To view a list of available models per region, please refer to the model summary table and region-specific availability provided <span class="No-Break">at </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability"><span class="No-Break">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability</span></a><span class="No-Break">.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="bold">Deployment name</strong></span></p>
						</td>
						<td class="No-Table-Style T---Body">
							<p class="list-inset">Be thoughtful in selecting a deployment name. The deployment name is crucial as it will be used in your code to invoke the model using client libraries and <span class="No-Break">REST APIs.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="bold">Advance Options</strong></span></p>
							<p class="list-inset">(<span class="No-Break">optional)</span></p>
						</td>
						<td class="No-Table-Style T---Body">
							<p class="list-inset">You have the option to configure advanced settings as required for your resource, such as content filter, <strong class="bold">tokens per minute</strong> (<strong class="bold">TPM</strong>), <span class="No-Break">and more.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="list-inset">For your<a id="_idIndexMarker088"/> initial deployment, keep <strong class="bold">Advanced options</strong> as-is. Further details on content filtering and TPM will be provided in <span class="No-Break">upcoming chapters:</span></p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B21019_02_14.jpg" alt="Figure 2.1﻿4: Deploying the gpt-35-turbo model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14: Deploying the gpt-35-turbo model</p>
			<ol>
				<li value="7">Click <strong class="bold">Create</strong> <span class="No-Break">to proceed.</span></li>
				<li>The <strong class="bold">Deployments</strong> table <a id="_idIndexMarker089"/>will display a new entry corresponding to the model you just created with a status <span class="No-Break">of </span><span class="No-Break"><strong class="bold">Succeeded</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B21019_02_15.jpg" alt="Figure 2.1﻿5: The gpt-35-turbo model’s deployment status"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.15: The gpt-35-turbo model’s deployment status</p>
			<p class="list-inset">(<strong class="bold">Optional</strong>): If you have access to GPT-4, you can repeat all the previous steps to deploy the GPT-4 or GPT base model <span class="No-Break">as well.</span></p>
			<p>Now that you’ve successfully deployed the model using Azure AI Foundry, let’s explore how to utilize these models both within Azure AI Foundry and via the <span class="No-Break">Python SDK.</span></p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Utilizing AOAI models</h1>
			<p>To<a id="_idIndexMarker090"/> make effective use of the model, it’s important to understand some basic elements associated with chat models. These concepts are essential for utilizing the <span class="No-Break">model properly.</span></p>
			<p>The GPT-3.5 Turbo and GPT-4 models are LLMs that have been optimized for conversational interfaces. They differ from the older GPT-3 models in how they operate. GPT-3 models are text-in, text-out, meaning they accept an input (prompt) string and return an output (completion); the GPT-3.5 Turbo and GPT-4 models are designed for conversation-style interactions. These models expect input to be formatted in a chat-like transcript structure and return a completion that represents a message generated by the model in the chat. While this format was primarily designed for multi-turn conversations, it can also be effectively used in <span class="No-Break">non-chat scenarios.</span></p>
			<p>In AOAI, there are two distinct options for interacting with models such as GPT-3.5 Turbo <span class="No-Break">and GPT-4:</span></p>
			<ul>
				<li><strong class="bold">Chat Completion API</strong>: The Chat Completion API is a specialized API designed specifically<a id="_idIndexMarker091"/> for interacting with GPT-3.5 Turbo and GPT-4 models. It is the recommended and preferred method for accessing these models, and it is also the only way to utilize the new GPT-4 models. This API provides a streamlined and optimized approach for engaging with <span class="No-Break">these LLMs.</span><p class="list-inset">OpenAI has trained the GPT-3.5 Turbo and GPT-4 models to accept input formatted as a conversation. To structure the input, you should utilize the <strong class="source-inline">messages</strong> parameter, which expects an array of message objects organized by role. When interacting with these models using the Python API, you typically provide a list of dictionaries to represent the conversation format effectively. Each dictionary in the list should include the role and content of a message, enabling you to have meaningful back-and-forth exchanges with <span class="No-Break">the model.</span></p><p class="list-inset">The format of a basic Chat Completion API is shown in the <span class="No-Break">following code:</span></p><pre class="source-code">
response = openai.ChatCompletion.create(
    engine="gpt-35-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful assistant that helps people find information"},
        {"role": "user", "content": "Who won the ICC world cup series in 2011?"},
        {"role": "assistant", "content": "India won the ICC world Cup series in 2011."},
        {"role": "user", "content": "Where the final match played?"},
        {"role": "assistant", "content": "The final match of the ICC World Cup 2011 was played at the Wankhede Stadium in Mumbai, India"}
    ]
)</pre><p class="list-inset">The <a id="_idIndexMarker092"/>primary input for interacting <a id="_idIndexMarker093"/>with these models is the <strong class="source-inline">messages</strong> parameter, which expects an array of message objects. Each message object consists of a <strong class="source-inline">role</strong> type (either <strong class="source-inline">system</strong>, <strong class="source-inline">user</strong>, or <strong class="source-inline">assistant</strong>) and <strong class="source-inline">content</strong>. Conversations can be as brief as a single message or involve multiple back-and-forth exchanges. Let’s take a closer look at these <span class="No-Break">different roles:</span></p><ul><li><strong class="source-inline">system</strong>: The <strong class="source-inline">system</strong> role, also referred to as the system message, is typically placed at the beginning of the message array. It serves as the initial instructions to the model to guide the model’s responses and interactions. These roles are set by the developers and can shape how the model handles various tasks and communicates with users. In the <strong class="source-inline">system</strong> role, you have the flexibility to provide various types <span class="No-Break">of information:</span><ul><li><strong class="bold">Guidance for interaction</strong>: The <strong class="source-inline">system</strong> role defines how the model should interact with users. For instance, a model might have a role that prioritizes being helpful and informative or one that ensures it maintains a neutral and <span class="No-Break">unbiased stance.</span></li><li><strong class="bold">Behavioral constraints</strong>: The role establishes boundaries for what the model should and shouldn’t do. This can include avoiding certain topics, adhering<a id="_idIndexMarker094"/> to privacy guidelines, or refraining from giving medical or <span class="No-Break">legal advice:</span></li></ul></li></ul></li>			</ul>
			<ol>
				<li><strong class="bold">Personality traits of the assistant</strong>: You can customize the assistant’s personality traits to make it more suitable for your specific <span class="No-Break">use case</span><ul><li><strong class="bold">Response style</strong>: The <strong class="source-inline">system</strong> role can influence the tone and style of responses. For example, the model might be set to respond formally to professional contexts or adopt a casual tone for more <span class="No-Break">relaxed interactions.</span></li><li><strong class="bold">Task management</strong>: This role helps the model handle specific types of queries more effectively. For example, if the <strong class="source-inline">system</strong> role is designed for customer support, the model might be optimized for troubleshooting and providing assistance related to specific products <span class="No-Break">or services.</span></li><li><strong class="bold">Ethical and safety considerations</strong>: <strong class="source-inline">system</strong> roles often include parameters to ensure that responses adhere to ethical guidelines and safety protocols, helping to prevent the generation of harmful or <span class="No-Break">inappropriate content.</span></li></ul><p class="list-inset">Overall, the <strong class="source-inline">system</strong> role helps tailor the behavior of the LLM to better meet the needs of users and align with the intended use case, whether it’s for specific and detailed instructions or just basic guidance. While the <strong class="source-inline">system</strong> role is optional, it’s generally recommended to include at least a basic message to ensure you get the best results and guide the assistant’s <span class="No-Break">behavior effectively.</span></p><ul><li><strong class="source-inline">user</strong>: The <strong class="source-inline">user</strong> role refers to the function or context that the user assumes during the interaction. It influences how the user frames questions and what they seek from the model, such as information, advice, or creative assistance. The <strong class="source-inline">user</strong> role affects the model’s responses by providing context for the interaction and shaping the style and detail of answers based on the user’s needs and expectations. It also impacts how the model adapts its responses to fit the user’s goals, whether it’s for educational purposes, technical support, or casual conversation. Overall, the <strong class="source-inline">user</strong> role helps tailor the interaction so that it’s more effective and relevant to the user’s <span class="No-Break">specific needs.</span></li><li><strong class="source-inline">assistant</strong>: The <strong class="source-inline">assistant</strong> role represents the model that’s chatting with the user. It’s used to distinguish the messages that are written by the model from those that are written by the user. The <strong class="source-inline">assistant</strong> role is also used to indicate who the model is addressing in the chat. For example, if the last message in the chat transcript has the <strong class="source-inline">assistant</strong> role, then the model will generate a response as if it’s continuing the conversation with <span class="No-Break">the user.</span></li></ul><p class="list-inset">Typically, a <a id="_idIndexMarker095"/>conversation is structured with a system message at the beginning, followed by alternating user and assistant messages. The system message plays a crucial role in setting the behavior of the assistant. It can be used to customize the assistant’s personality or provide specific instructions on how it should respond throughout the conversation. User messages are used to convey requests or comments for the assistant to address. Assistant messages not only store prior assistant responses but can also be authored by you to provide examples of the desired behavior you expect from the assistant during the conversation. This structured format allows for effective communication with the model, guiding its responses in a controlled manner. <span class="No-Break">Figure 2</span>.16 provides a clearer understanding of <span class="No-Break">this structure:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B21019_02_16.jpg" alt="Figure 2.1﻿6: Skeleton of the system, user, and assistant roles/messages"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.16: Skeleton of the system, user, and assistant roles/messages</p>
			<p class="list-inset">The<a id="_idIndexMarker096"/> first square box represents the system message, where you set the meta prompt to guide the LLM’s responses. The second box is the user message, where users ask their questions. The last box shows the assistant message, which provides responses to the <span class="No-Break">user’s queries.</span></p>
			<ul>
				<li><strong class="bold">Completion API with Chat Markup Language (ChatML)</strong>: ChatML employs <a id="_idIndexMarker097"/>the same Completion API that you use for other GPT-3 models, such as davinci-002 or babbage-002, but it utilizes a distinct token-based prompt format. While ChatML offers lower-level access compared to the dedicated Chat Completion API, it comes<a id="_idIndexMarker098"/> with certain limitations <span class="No-Break">and considerations:</span><ul><li><strong class="bold">Input validation</strong>: You must perform additional input validation when using ChatML to ensure the correct format and structure of <span class="No-Break">your messages.</span></li><li><strong class="bold">Model compatibility</strong>: ChatML is only compatible with the GPT-3.5 Turbo models and doesn’t work with the new <span class="No-Break">GPT-4 models.</span></li><li><strong class="bold">Potential format changes</strong>: The underlying format for ChatML may change <span class="No-Break">over time</span></li></ul><p class="list-inset">Therefore, while ChatML provides flexibility, it’s essential to be aware of these constraints and the potential for evolving formats when using it with GPT-3.5 <span class="No-Break">Turbo models.</span></p></li>
			</ul>
			<p>Now that you <a id="_idIndexMarker099"/>have a fundamental understanding of the Chat Completion API and its structure, let’s explore how to use the model from both Azure AI Foundry and the <span class="No-Break">Python SDK.</span></p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>The Azure AI Foundry experience</h2>
			<p>To<a id="_idIndexMarker100"/> utilize the model in Azure AI Foundry, follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Log in to your Azure subscription within the Azure <span class="No-Break">portal (</span><a href="https://portal.azure.com/"><span class="No-Break">https://portal.azure.com/</span></a><span class="No-Break">).</span></li>
				<li>In the resource search bar, search for <strong class="source-inline">Azure OpenAI</strong>. Locate the AOAI resource that you created in the <span class="No-Break">previous section:</span></li>
			</ol>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B21019_02_17.jpg" alt="Figure 2.1﻿7: Locating the AOAI instance created previously"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.17: Locating the AOAI instance created previously</p>
			<ol>
				<li value="3">Click on the service name highlighted within the red rectangle in the <span class="No-Break">preceding figure.</span></li>
				<li>Navigate to<a id="_idIndexMarker101"/> Azure AI Foundry by clicking either <strong class="bold">Explore Azure AI Foundry portal</strong> or <strong class="bold">Go to Azure AI </strong><span class="No-Break"><strong class="bold">Foundry portal</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B21019_02_18.jpg" alt="Figure 2.1﻿8: Launching ﻿Azure AI Foundry"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.18: Launching Azure AI Foundry</p>
			<ol>
				<li value="5">Within the <strong class="bold">Playground</strong> section, <span class="No-Break">choose </span><span class="No-Break"><strong class="bold">Chat</strong></span><span class="No-Break">:</span></li>
			</ol>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B21019_02_19.jpg" alt="Figure 2.﻿19: Testing a prompt with ﻿Azure AI Foundry's Chat playground"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.19: Testing a prompt with Azure AI Foundry's Chat playground</p>
			<p class="list-inset">In <strong class="bold">Chat playground</strong>, you have the flexibility to provide a custom <strong class="bold">System message</strong> tailored to your specific business requirements. We’ll delve into various prompting techniques in later chapters. However, for now, feel free to use the default system <span class="No-Break">message provided.</span></p>
			<p class="list-inset">Navigate to the <strong class="bold">Deployment</strong> tab within the <strong class="bold">Configuration</strong> panel and select <strong class="bold">Deployment name</strong> (which you created in the previous section) from the <strong class="bold">Deployment</strong> dropdown. Here, you have the option to choose either the gpt-3.5 or gpt-4 model to <span class="No-Break">interact with.</span></p>
			<ol>
				<li value="6">Opt for<a id="_idIndexMarker102"/> the default <strong class="bold">Session settings</strong>. This setting determines how many ongoing conversations you can retain in the model’s context to facilitate responses to user prompts. It serves to manage token limit restrictions, which we covered at the beginning of <span class="No-Break">this chapter.</span></li>
				<li>Next, select the <strong class="bold">Parameters</strong> tab within the <strong class="bold">Configuration</strong> panel. Select the default settings for <span class="No-Break">the parameters:</span></li>
			</ol>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B21019_02_20.jpg" alt="Figure 2.﻿20: Setting up parameters"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.20: Setting up parameters</p>
			<p class="list-inset">There are<a id="_idIndexMarker103"/> several parameters to consider. Let’s look at each one <span class="No-Break">by one:</span></p>
			<ul>
				<li><strong class="bold">Max response</strong>: The maximum number of tokens to generate in the chat completion reply. The total length of input tokens and generated tokens is limited by the model’s <span class="No-Break">context length.</span></li>
				<li><strong class="bold">Temperature</strong>: Which sampling temperature to employ, ranging from 0 to 1, is a crucial decision. Opting for higher values, such as 0.9, will introduce greater randomness into the output, whereas selecting lower values, such as 0.1, will enhance its focus and determinism. It’s important to note that a setting of 0 will <em class="italic">not</em> make the model deterministic; instead, it will reduce the overall variability <span class="No-Break">in replies.</span></li>
				<li><strong class="bold">Top P</strong>: An alternative to utilizing sampling temperature is nucleus sampling, ranging from 0 to 1, in which the model takes into account tokens that fall within the <strong class="bold">Top P</strong> probability mass. In this context, setting <strong class="bold">Top P</strong> to <strong class="source-inline">0.1</strong> would imply that only tokens within the top 10% probability mass are taken into consideration. It is generally advisable to adjust either the <strong class="bold">Top P</strong> parameter or the <strong class="bold">Temperature</strong> parameter, but not <span class="No-Break">both simultaneously.</span></li>
				<li><strong class="bold">Stop sequence</strong>: This<a id="_idIndexMarker104"/> helps you conclude the model’s response at a specific point and ensure it doesn’t include the provided stop sequence text. By doing so, you can prevent the model from generating a follow-up user query. You have the option to include up to four different stop sequences for <span class="No-Break">this purpose.</span></li>
				<li><strong class="bold">Frequency penalty</strong>: This number falls within the range of 0 to 2.0. It lowers the likelihood of token repetition in proportion to its prior occurrence in the text, thus reducing the chances of reiterating identical text <span class="No-Break">in response.</span></li>
				<li><strong class="bold">Presence penalty</strong>: This number falls within the range of 0 to 2.0. It minimizes the probability of reusing any token that has already been used in the text, thereby enhancing the chances of introducing fresh topics <span class="No-Break">in response.</span></li>
			</ul>
			<p class="list-inset">Among these parameters, the one you’re most likely to adjust according to your specific use cases is the <strong class="bold">Temperature</strong> parameter. When you’re working on use cases that demand creativity, such as content generation, it’s common to set this value closer to 1. In contrast, for use cases that require factual and precise answers, you’d typically opt for a value closer <span class="No-Break">to 0.</span></p>
			<ol>
				<li value="8">Now, let’s use the GPT-3.5 model for chat interaction, as specified in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.21</em>. In this instance, we’ll begin by posing the prompt, “Who emerged as the victor in the ICC World Cup series of 2011?” The model’s response is, “India emerged as the victor in the ICC World Cup series of 2011.” Subsequently, we pose a follow-up question, stating “Where was the final match held?” The model’s response is, “The final match of the ICC World Cup series in 2011 took place at the Wankhede Stadium in Mumbai, India.” This showcases the conversational-style interaction that can <span class="No-Break">be sustained:</span></li>
			</ol>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B21019_02_21.jpg" alt="Figure 2.﻿21: Describing agent and user chat interactions"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.21: Describing agent and user chat interactions</p>
			<p>With<a id="_idIndexMarker105"/> that, you’ve effectively utilized AOAI models to engage in a ChatGPT-style conversation. In the next section, we’ll discuss how to utilize the same GPT-3.5 Turbo or GPT-4 model using the <span class="No-Break">Python SDK.</span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>Programmatic experience</h2>
			<p>In this<a id="_idIndexMarker106"/> section, we’ll guide you through the process of making your first call to AOAI using the <span class="No-Break">Python SDK:</span></p>
			<ol>
				<li>Install Python version 3.7.1 or a more recent version on your machine. Alternatively, you can utilize an Azure Machine Learning notebook to obtain the Python environment. In this example, we’ve used Anaconda with Visual Studio Code as <span class="No-Break">an IDE.</span></li>
				<li>Install the OpenAI Python client library by running <strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install openai</strong></span><span class="No-Break">.</span></li>
				<li>To make a request to AOAI Service, you’ll require the <span class="No-Break">following inputs:</span><table id="table003" class="No-Table-Style _idGenTablePara-2"><colgroup><col/><col/></colgroup><thead><tr class="No-Table-Style"><td class="No-Table-Style T---Body"><p class="list-inset"><span class="No-Break"><strong class="bold">Variable name</strong></span></p></td><td class="No-Table-Style T---Body"><p class="list-inset"><span class="No-Break"><strong class="bold">Value</strong></span></p></td></tr></thead><tbody><tr class="No-Table-Style"><td class="No-Table-Style T---Body"><p class="list-inset"><span class="No-Break"><strong class="source-inline">ENDPOINT</strong></span></p></td><td class="No-Table-Style T---Body"><p class="list-inset">You can locate this value in the <strong class="bold">Keys and Endpoint</strong> section when inspecting AOAI resources in the <span class="No-Break">Azure portal.</span></p></td></tr><tr class="No-Table-Style"><td class="No-Table-Style T---Body"><p class="list-inset"><span class="No-Break"><strong class="source-inline">API-KEY</strong></span></p></td><td class="No-Table-Style T---Body"><p class="list-inset">You can find this value in the <strong class="bold">Keys and Endpoint</strong> section when reviewing your resource in the Azure portal. You can use either <strong class="bold">KEY 1</strong> or <span class="No-Break"><strong class="bold">KEY 2</strong></span><span class="No-Break">.</span></p></td></tr><tr class="No-Table-Style"><td class="No-Table-Style T---Body"><p class="list-inset"><span class="No-Break"><strong class="source-inline">DEPLOYMENT-NAME</strong></span></p></td><td class="No-Table-Style T---Body"><p class="list-inset">This value will correspond to the custom name you selected for your deployment during the model deployment process in the <em class="italic">Deploying AOAI </em><span class="No-Break"><em class="italic">models</em></span><span class="No-Break"> section.</span></p></td></tr></tbody></table><p class="list-inset">To get the <a id="_idIndexMarker107"/>first two values, navigate to your resource within the Azure portal. You can find the <strong class="bold">Keys and Endpoint</strong> under the <strong class="bold">Resource Management</strong> section. Make sure you copy both your endpoint and access key; you’ll require both to authenticate your API calls. You have the option to use either <strong class="bold">KEY 1</strong> or <strong class="bold">KEY 2</strong>. The presence of two keys enables secure key rotation and regeneration without service interruptions <span class="No-Break">being made:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B21019_02_22.jpg" alt="Figure 2.﻿22: Retrieving AOAI keys and endpoint information"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.22: Retrieving AOAI keys and endpoint information</p>
			<ol>
				<li value="4">In your<a id="_idIndexMarker108"/> preferred IDE, create a Python file called <strong class="source-inline">quickstart.py</strong> and execute the <span class="No-Break">following code:</span><ol><li class="upper-roman">Import the necessary Python packages and set up the AOAI keys and endpoint information. Make sure you change the deployment name’s value to the custom name you provided while creating <span class="No-Break">the deployment:</span><pre class="source-code">
import os
import requests
import json
import openai
openai.api_key = "&lt;ENTER YOUR API KEY&gt;"
openai.api_base = "&lt;ENTER YOUR ENDPOINT&gt;"
openai.api_type = 'azure'
openai.api_version = '2023-08-01-preview' # API version may change in the future
deployment_name='gpt-35-turbo'  # Enter your Deployment Name.</pre></li><li class="upper-roman">Call the <a id="_idIndexMarker109"/>AOAI Chat Completion API and provide the AOAI deployment name, along with system and user <span class="No-Break">message details:</span><pre class="source-code"># Send a chat completion call to generate an answer
response = openai.ChatCompletion.create(  /
           engine="gpt-35-turbo",
# This is your deployment name
messages=[
{"role": "system", "content": "You are an AI assistant that helps      people find information."},
{"role": "user", "content": "Who won the ICC world cup series in 2011?"},
{"role": "assistant", "content": "India won the ICC world Cup series in 2011."},
 {"role": "user", "content": "Where the final match played?"}]
)</pre></li><li class="upper-roman">Print the final response from <span class="No-Break">the model:</span><pre class="source-code">print(response['choices'][0]['message']['content'])</pre></li></ol><p class="list-inset">The output should be similar to <span class="No-Break">the following:</span></p><pre class="source-code"><strong class="bold">The final match of the ICC World Cup 2011 was played at the Wankhede Stadium in Mumbai, India.</strong></pre></li>			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">In a production environment, it’s recommended to use a secure method for storing and accessing your credentials, such as Azure Key Vault. This ensures the highest level of security for your <span class="No-Break">sensitive information.</span></p>
			<p>With that, you<a id="_idIndexMarker110"/> understand how to utilize the AOAI GPT-3.5 and GPT-4 models. Something we haven’t discussed yet is how to use the embedding model; we’ll cover that in the next chapter. In the next section, we’ll talk about <span class="No-Break">AOAI pricing.</span></p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>Pricing</h1>
			<p>In this<a id="_idIndexMarker111"/> section, we’ll discuss the various AOAI pricing options and help you select the most suitable plan for <span class="No-Break">your needs.</span></p>
			<p>AOAI has two distinct <span class="No-Break">pricing plans:</span></p>
			<ul>
				<li><strong class="bold">Pay-As-You-Go</strong>: Under<a id="_idIndexMarker112"/> this pricing model, you’re billed based on the consumption of 1,000 tokens for both prompts and completions. Note that each model has its distinct pricing for both prompts and completions. This plan is better suited for development and testing environments, as well as specific production workloads where the volume of API calls and processed tokens isn’t substantial. For the complete pricing table, please refer to <a href="https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/">https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/</a>. It’s important to note that these costs may change in the future, so it’s advisable to check the preceding link provided for the most up-to-date <span class="No-Break">pricing information:</span></li>
			</ul>
			<table id="table004" class="T---Table _idGenTablePara-2">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="bold">Models</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="bold">Context Window</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="bold">Prompt</strong></span></p>
							<p class="list-inset"><strong class="bold">(per </strong><span class="No-Break"><strong class="bold">1,000 Tokens)</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break"><strong class="bold">Completion</strong></span></p>
							<p class="list-inset"><strong class="bold">(er </strong><span class="No-Break"><strong class="bold">1,000 Tokens)</strong></span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break">gpt-35-turbo</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break">4K</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">$<span class="No-Break">0.0015</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">$<span class="No-Break">0.002</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break">gpt-35-turbo</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break">16K</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">$<span class="No-Break">0.003</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">$<span class="No-Break">0.004</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break">gpt-4</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break">8K</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">$<span class="No-Break">0.03</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">$<span class="No-Break">0.06</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break">gpt-4</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break">32K</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">$<span class="No-Break">0.06</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">$<span class="No-Break">0.12</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break">text-embedding-ada-002</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset"><span class="No-Break">8K</span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p class="list-inset">$<span class="No-Break">0.0001</span></p>
						</td>
						<td class="T---Table T---Body T---Body"/>
					</tr>
				</tbody>
			</table>
			<ul>
				<li><strong class="bold">PAUG</strong>: In this<a id="_idIndexMarker113"/> pricing plan, you’re<a id="_idIndexMarker114"/> initially provided with a default level of throughput, also known as a quota. This implies that the models will process a set number of tokens per minute based on your chosen region. You have the option to request an additional quota up to a certain threshold. If your needs exceed the predefined limit, you should <a id="_idIndexMarker115"/>consider the <strong class="bold">Provisioned Throughput Unit</strong> (<strong class="bold">PTU</strong>) pricing plan, which we’ll discuss shortly. We’ll delve into quota management in later chapters. For the most up-to-date quotas and limits, please refer to the <span class="No-Break">following link:</span><p class="list-inset">The following table will help you determine the amount of throughput each model <span class="No-Break">can deliver:</span></p><table id="table005" class="T---Table _idGenTablePara-2"><colgroup><col/><col/><col/></colgroup><thead><tr class="T---Table"><td class="T---Table T---Body T---Header"><p class="list-inset"><span class="No-Break"><strong class="bold">Models</strong></span></p></td><td class="T---Table T---Body T---Header"><p class="list-inset"><span class="No-Break"><strong class="bold">Context Window</strong></span></p></td><td class="T---Table T---Body T---Header"><p class="list-inset"><strong class="bold">Default </strong><span class="No-Break"><strong class="bold">Throughput (Tokens/Minute)</strong></span></p></td></tr></thead><tbody><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">gpt-35-turbo</span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">4K</span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">240K</span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">gpt-35-turbo</span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">16K</span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">240K</span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">gpt-4</span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">8K</span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">20K</span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">gpt-4</span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">32K</span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">40K</span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">text-embedding-ada-002</span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">8K</span></p></td><td class="T---Table T---Body T---Body"><p class="list-inset"><span class="No-Break">240K</span></p></td></tr></tbody></table><p class="list-inset">Please note that this information may change over time, so always consult the latest documentation for the most up-to-date <span class="No-Break">information: </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits"><span class="No-Break">https://learn.microsoft.com/en-us/azure/ai-services/openai/quotas-limits</span></a><span class="No-Break">.</span></p><ul><li><strong class="bold">Provisioned Throughput Unit - Managed (PTU-M)</strong>: PTU-M is a new feature in AOAI Service that allows you to reserve processing capacity specifically <a id="_idIndexMarker116"/>tailored for high-volume <a id="_idIndexMarker117"/>or latency-critical workloads. PTU processing capacity delivers consistent levels of latency and throughput, which is ideal for workloads with stable characteristics, such as uniform prompt size, completion size, and concurrent API request numbers. PTUs can be procured hourly (no commitment) or monthly or yearly (commitment). In this pricing plan, you pay a fixed, flat rate, and you have the freedom to send an unlimited number of tokens throughout the commitment period. Additionally, this plan offers superior throughput compared to the standard pay-as-you-go option. PTU throughput can fluctuate based on factors such as input tokens, output tokens, and the number of API calls per minute. To accurately assess throughput for a specific PTU unit, you can utilize the benchmark script provided by Microsoft. You can access the benchmarking tool <span class="No-Break">here: </span><a href="https://github.com/Azure/azure-openai-benchmark"><span class="No-Break">https://github.com/Azure/azure-openai-benchmark</span></a><span class="No-Break">.</span></li></ul><p class="list-inset">Let’s break down the pricing structure for PTU. For hourly PTU usage without any commitment, the cost is $2 per PTU per hour. For a monthly commitment, the rate is $260 per PTU per month, while for a yearly commitment, it’s $221 per PTU <span class="No-Break">per year.</span></p><p class="list-inset">Each model has a minimum PTU requirement and minimum scaling increment. For example, GPT-4o requires a minimum of 50 PTUs to operate and 50 units for incremental scaling. Therefore, if you opt for a monthly commitment, GPT-4o will cost $260 multiplied by 50 PTUs, totaling $13,000 per month. If you need to scale up your usage of GPT-4o, you will still need to add 50 PTUs as <span class="No-Break">an increment.</span></p><p class="list-inset">For detailed pricing information on various models, please refer to the table provided table. Now, let’s delve into how the pricing structure operates. Each PTU costs $312 per month. Different model types require varying amounts of PTUs to operate optimally. For instance, the GPT-35 Turbo with a 4K context window necessitates a minimum of 300 PTUs, providing a throughput ranging from 900K to 2.7 million TPM. If you require additional throughput, you can incrementally add 100 PTUs, resulting in an extra throughput of 300K to <span class="No-Break">900K TPM.</span></p><p class="list-inset">On the other<a id="_idIndexMarker118"/> hand, the GPT-35 Turbo with a 16K context window requires a minimum of 600 PTUs, offering a throughput between 1.8 million and 5.4 million TPM. To augment the throughput further, you can add 200 PTUs as an increment, resulting in an additional throughput of 600K to 1.8 million tokens per minute. For comprehensive details on the minimum PTU requirements, associated costs, and throughput capabilities for each model type, please refer to the <span class="No-Break">following table:</span></p></li>
			</ul>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B21019_02_23.jpg" alt="Figure 2.﻿23: PTU pricing information"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.23: PTU pricing information</p>
			<p>Although AOAI offers a diverse set of LLMs, customers often choose different models based on their specific use cases to manage cost and accuracy. For complex tasks such as clinical protocol writing, drug discovery, and clinical trial matching, customers typically use<a id="_idIndexMarker119"/> GPT-4o due to its superior accuracy. For less complex tasks, such as customer support chatbots or entity extraction, customers might opt for GPT-3.5 or GPT-4o mini, which provide better price performance and are more <span class="No-Break">economically feasible.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">It’s important to note that you cannot directly purchase PTUs from the Azure portal as you would with other services. To acquire PTUs for your subscription, you must reach out to a Microsoft account representative, who will assist you in gaining approval for PTUs within your subscription. Once approved, you can proceed with the purchase of PTUs. The price for PTUs is subject to change in the future. Therefore, it’s advisable to always consult with a Microsoft account representative for the most up-to-date <span class="No-Break">pricing information.</span></p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Summary</h1>
			<p>This chapter provided an in-depth exploration of AOAI Service. We began by defining what AOAI Service is and went on to discuss the various model types it offers. We also guided you through the steps to access this service, from creating an AOAI resource to deploying the models and utilizing them for practical applications. Lastly, we shed light on the pricing structure, ensuring you have a clear understanding of how to leverage this powerful AI service within the Azure ecosystem. With this knowledge, you’re well-equipped to harness the capabilities of AOAI Service for your AI and machine <span class="No-Break">learning projects.</span></p>
			<p>Looking ahead to the next chapter, our attention will shift toward delving into advanced topics within AOAI. We’ll explore embedding models and discover how to store these embeddings within a vector database, empowered by the Azure Cognitive Search service. Additionally, we’ll delve into the concept of model grounding and the intricacies of fine-tuning models to meet <span class="No-Break">specific requirements.</span></p>
			<p>Furthermore, we’ll engage in discussions about some of the latest features, such as function calling, assistant API, fine-tuning, and the Batch API. These advanced topics will provide a deeper understanding of AOAI’s capabilities and empower you to leverage its full potential. We’ll conclude by highlighting the significance of LLM application development frameworks such as LangChain and Semantic Kernel. These frameworks play a pivotal role in simplifying the creation of applications. By leveraging the capabilities of these frameworks, developers can streamline the development process, harnessing the power of LLMs to build innovative and intelligent applications with ease. As we move forward in the field of AI and language processing, these frameworks serve as essential tools in harnessing the full potential of LLMs for <span class="No-Break">diverse applications.</span></p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
			<ul>
				<li><em class="italic">Introduction to AOAI </em><span class="No-Break"><em class="italic">Service</em></span><span class="No-Break"> (</span><a href="https://learn.microsoft.com/en-us/training/modules/explore-azure-openai/"><span class="No-Break">https://learn.microsoft.com/en-us/training/modules/explore-azure-openai/</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">AOAI: Generative AI Models and How to Use </em><span class="No-Break"><em class="italic">Them</em></span><span class="No-Break"> (</span><a href="https://www.linkedin.com/learning/azure-openai-generative-ai-models-and-how-to-use-them"><span class="No-Break">https://www.linkedin.com/learning/azure-openai-generative-ai-models-and-how-to-use-them</span></a><span class="No-Break">)</span></li>
			</ul>
		</div>
	</body></html>