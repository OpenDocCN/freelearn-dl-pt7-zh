["```py\n# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function    # (at top of module)\nimport sys\nimport time\nimport requests\n\n# Note: in order to use this example, you need to have at least one account\n# that you can send money from (i.e. be the owner).\n# All properties are now kept in one central place\n\nfrom props.default import *\n\n# You probably don't need to change those\n...\n\n#add the following lines to hello-obp.py before running it\n#add lines to download the file\nprint(\"\")\nprint(\" --- export json\")\nimport json\nf_json = open('transactions.json','w+')\njson.dump(transactions,f_json,sort_keys=True, indent=4)\n```", "```py\nfrom pymongo import MongoClient\nimport json\nimport pprint\n\n#client = MongoClient()\nclient = MongoClient('mongodb://localhost:27017/')\ndb_name = 'AIFinance8A'\ncollection_name = 'transactions_obp'\n\nf_json = open('transactions.json', 'r')\njson_data = json.loads(f_json)\n\n...\n\n#to check if all documents are inserted\n...\n```", "```py\n#define libraries and variables\nimport sqlite3\nfrom pymongo import MongoClient\nimport json\nfrom flatten_dict import flatten\n\nclient = MongoClient('mongodb://localhost:27017/')\ndb_name = 'AIFinance8A'\ncollection_name = 'transactions_obp'\n\ndb = client[db_name]\ncollection = db[collection_name]\nposts = db.posts\n\n...\n\n#flatten the dictionary\n...\n\n#create the database schema\n#db file\ndb_path = 'parsed_obp.db'\ndb_name = 'obp_db'\n\n#sql db\n...\nsqlstr = 'drop table '+db_name\n...\nprint('create')\n...\n#loop through the dict and insert them into the db\n...\n\nfor cnt in dict_cnt:\n    ...\n    for fld in tuple_fields_list:\n        ...\n    ...\n    sqlstr = 'insert into '+ db_name+ '(' + str(fld_list_str)+') VALUES \\\n                                       ('+question_len[1:]+')'\n    ...\n```", "```py\n#Libraries\nfrom flask import Flask, request, jsonify\nfrom sklearn.externals import joblib\nimport traceback\nimport pandas as pd\nimport numpy as np\n\n# Your API definition\napp = Flask(__name__)\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    ...\n\n#Run the server\nif __name__ == '__main__':\n    ...\n```", "```py\nimport requests\n\nhost = 'http://127.0.0.1:12345/'\n\nr = requests.post(host+'predict', json={\"key\": \"value\"})\nprint(r)\n```", "```py\nimport os\nimport pandas as pd\nfrom numpy import genfromtxt\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom gensim.models.keyedvectors import WordEmbeddingsKeyedVectors\nimport gensim.downloader as api\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim.parsing.preprocessing import preprocess_string, strip_tags, remove_stopwords,strip_numeric,strip_multiple_whitespaces\nfrom scipy import linalg as LA\nimport pickle\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report,roc_curve, auc,confusion_matrix,f1_score\n\n#please run this in terminal: sudo apt-get install libopenblas-dev\nmodel_word2vec = api.load(\"text8\") # load pre-trained words vectors\n```", "```py\n#2\\. Define functions relevant for works\n##2A Neural Network\n##2A_i. Grid search that simulate the performance of different neural network design\ndef grid_search(X_train,X_test, Y_train,Y_test,num_training_sample):\n...\n##2A_ii train network\ndef train_NN(X,Y,target_names):\n...\n#2B: prepare the text data series into numeric data series\n#2B.i: cleanse text by removing multiple whitespaces and converting to lower cases\ndef cleanse_text(sentence,re_sub):\n...\n#2B.ii: convert text to numeric numbers\ndef text_series_to_np(txt_series,model,re_sub):\n...\n\n```", "```py\n#3\\. Loop through the files to prepare the dataset for training and testing\n#loop through folders (represent different sources)\nfor folder in list_of_dir:\n    files = os.path.join(path,folder)\n    #loop through folders (represent different filing of the same \n     source)\n    for file in os.listdir(files):\n        if file.endswith(truth_file_ext):\n        #define the file names to be read\n         ...\n\n        #merge ground truth (aka target variables) with the blocks \n         ...\n\n        #convert the text itself into vectors and lastly a single \n        value using Eigenvalue\n        text_df = f_df['text']\n        text_np = text_series_to_np(text_df,model_word2vec,re_sub)\n\n        label_df = f_df['text_label'] \n        label_np = text_series_to_np(label_df, model_word2vec, \\\n                                     re_sub)\n         ...\nY_pd = pd.get_dummies(targets_df)\nY_np = Y_pd.values\n```", "```py\n#4\\. Execute the training and test the outcome\nNN_clf, f1_clf = train_NN(full_X_np,Y_np,dummy_header)\n...\n```"]