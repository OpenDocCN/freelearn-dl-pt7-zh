- en: 2\. An Introduction to Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to regression. Regression comes in handy
    when you are trying to predict future variables using historical data. You will
    learn various regression techniques such as linear regression with single and
    multiple variables, along with polynomial and Support Vector Regression (SVR).
    You will use these techniques to predict future stock prices from a stock price
    data. By the end of this chapter, you will be comfortable using regression techniques
    to solve practical problems in a variety of fields.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you were introduced to the fundamentals of **Artificial
    Intelligence** (**AI**), which helped you create the game Tic-Tac-Toe. In this
    chapter, we will be looking at regression, which is a machine learning algorithm
    that can be used to measure how closely related independent variable(s), called
    **features**, relate to a dependent variable called a **label**.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is a concept with many applications a variety of fields, ranging
    from finance (predicting the price of an asset) to business (predicting the sales
    of a product) and even the economy (predicting economy growth).
  prefs: []
  type: TYPE_NORMAL
- en: Most of this chapter will deal with different forms of linear regression, including
    linear regression with one variable, linear regression with multiple variables,
    polynomial regression with one variable, and polynomial regression with multiple
    variables. Python provides lots of forms of support for performing regression
    operations and we will also be looking at these later on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will also use an alternative regression model, called **Support Vector Regression**
    (**SVR**), with different forms of linear regression. Throughout this chapter,
    we will be using a few sample datasets along with the stock price data loaded
    from the **Quandl** Python library to predict future prices using different types
    of regression.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although it is not recommended that you use the models in this chapter to provide
    trading or investment advice, this is a very exciting and interesting journey
    that explains the fundamentals of regression.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression with One Variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A general regression problem can be defined with the following example. Suppose
    we have a set of data points and we need to figure out the best fit curve to approximately
    fit the given data points. This curve will describe the relationship between our
    input variable, `x`, which is the data point, and the output variable, `y`, which
    is the curve.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, in real life, we often have more than one input variable determining
    the output variable. However, linear regression with one variable will help us
    to understand how the input variable impacts the output variable.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will work with regression on the two-dimensional plane.
    This means that our data points are two-dimensional, and we are looking for a
    curve to approximate how to calculate one variable from another.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will come across the following types of regression in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear regression with one variable using a polynomial of degree 1**: This
    is the most basic form of regression, where a straight line approximates the trajectory
    of future data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linear regression with multiple variables using a polynomial of degree 1**:
    We will be using equations of degree 1, but we will also allow multiple input
    variables, called features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial regression with one variable**: This is a generic form of the
    linear regression of one variable. As the polynomial used to approximate the relationship
    between the input and the output is of an arbitrary degree, we can create curves
    that fit the data points better than a straight line. The regression is still
    linear – not because the polynomial is linear, but because the regression problem
    can be modeled using linear algebra.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Polynomial regression with multiple variables**: This is the most generic
    regression problem, using higher degree polynomials and multiple features to predict
    the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SVR**: This form of regression uses **Support Vector Machines** (**SVMs**)
    to predict data points. This type of regression is included to explain SVR''s
    usage compared to the other four regression types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now we will deal with the first type of linear regression: we will use one
    variable, and the polynomial of the regression will describe a straight line.'
  prefs: []
  type: TYPE_NORMAL
- en: On the two-dimensional plane, we will use the Déscartes coordinate system, more
    commonly known as the *Cartesian coordinate system*. We have an *x* and a *y*-axis,
    and the intersection of these two axes is the origin. We denote points by their
    *x* and *y* coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, point *(2, 1)* corresponds to the black point on the following
    coordinate system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Representation of point (2,1) on the coordinate system'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.1: Representation of point (2,1) on the coordinate system'
  prefs: []
  type: TYPE_NORMAL
- en: A straight line can be described with the equation *y = a*x + b*, where *a*
    is the slope of the equation, determining how steeply the equation climbs up,
    and *b* is a constant determining where the line intersects the *y*-axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 2.2*, you can see three equations:'
  prefs: []
  type: TYPE_NORMAL
- en: The straight line is described with the equation *y = 2*x + 1*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dashed line is described with the equation *y = x + 1*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dotted line is described with the equation *y = 0.5*x + 1*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see that all three equations intersect the *y*-axis at *1*, and their
    slope is determined by the factor by which we multiply *x*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you know *x*, you can solve *y*. Similarly, if you know *y*, you can solve
    *x*. This equation is a polynomial equation of degree *1*, which is the base of
    linear regression with one variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: Representation of the equations y = 2*x + 1, y = x + 1, and y
    = 0.5*x + 1 on the coordinate system'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.2: Representation of the equations y = 2*x + 1, y = x + 1, and y =
    0.5*x + 1 on the coordinate system'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can describe curves instead of straight lines using polynomial equations;
    for example, the polynomial equation *4x*4*-3x*3*-x*2*-3x+3* will result in *Figure
    2.3*. This type of equation is the base of polynomial regression with one variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: Representation of the polynomial equation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.3: Representation of the polynomial equation'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to experiment further with the Cartesian coordinate system,
    you can use the following plotter: [https://s3-us-west-2.amazonaws.com/oerfiles/College+Algebra/calculator.html](https://s3-us-west-2.amazonaws.com/oerfiles/College+Algebra/calculator.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Features and Labels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, we differentiate between features and labels. Features
    are considered our **input** variables, and labels are our **output** variables.
  prefs: []
  type: TYPE_NORMAL
- en: When talking about regression, the possible value of the labels is a continuous
    set of rational numbers. Think of features as the values on the *x*-axis and labels
    as the values on the *y*-axis.
  prefs: []
  type: TYPE_NORMAL
- en: The task of regression is to predict label values based on feature values.
  prefs: []
  type: TYPE_NORMAL
- en: We often create a label by projecting the values of a feature in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if we would like to predict the price of a stock for next month
    using historical monthly data, we would create the label by shifting the stock
    price feature one month into the future:'
  prefs: []
  type: TYPE_NORMAL
- en: For each stock price feature, the label would be the stock price feature of
    the next month.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the last month, prediction data would not be available, so these values
    are all `NaN` (Not a Number).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's say we have data for the months of `January`, `February`, and `March`,
    and we want to predict the price for `April`. Our feature for each month will
    be the current monthly price and the label will be the price of the next month.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: Example of a feature and a label'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.4: Example of a feature and a label'
  prefs: []
  type: TYPE_NORMAL
- en: This means that the label for `January` is the price of `February` and that
    the label for `February` is actually the price of `March`. The label for `March`
    is unknown (`NaN`) as this is the value we are trying to predict.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At times, we have multiple features (inputs) that may have values within completely
    different ranges. Imagine comparing micrometers on a map to kilometers in the
    real world. They won't be easy to handle because of the difference in magnitude
    of nine zeros.
  prefs: []
  type: TYPE_NORMAL
- en: A less dramatic difference is the difference between imperial and metric data.
    For instance, pounds and kilograms, and centimeters and inches, do not compare
    that well.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we often scale our features to normalized values that are easier
    to handle, as we can compare the values of these ranges more easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will demonstrate two types of scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: Min-max normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min-max normalization is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![1](img/B16060_02_4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *X*MIN is the minimum value of the feature and *X*MAX is the maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: The feature-scaled values will be within the range of `[0;1]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean normalization is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![2](img/B16060_02_4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, `AVG` is the average.
  prefs: []
  type: TYPE_NORMAL
- en: The feature-scaled values will be within the range of `[-1;1]`.
  prefs: []
  type: TYPE_NORMAL
- en: Here's an example of both normalizations applied on the first 13 numbers of
    the Fibonacci sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin with finding the min-max normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, take a look at the following code snippet to find the mean normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Scaling could add to the processing time, but, often, it is an important step
    to add.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the scikit-learn library, we have access to the `preprocessing.scale` function,
    which scales NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `scale` method performs a standardization, which is another type of normalization.
    Notice that the result is a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting Data into Training and Testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have learned how to normalize our dataset, we need to learn about
    the training-testing split. In order to measure how well our model can generalize
    its predictive performance, we need to split our dataset into a training set and
    a testing set. The training set is used by the model to learn from so that it
    can build predictions. Then, the model will use the testing set to evaluate the
    performance of its prediction.
  prefs: []
  type: TYPE_NORMAL
- en: When we split the dataset, we first need to shuffle it to ensure that our testing
    set will be a generic representation of our dataset. The split is usually 90%
    for the training set and 10% for the testing set.
  prefs: []
  type: TYPE_NORMAL
- en: With training and testing, we can measure whether our model is overfitting or
    underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**Overfitting** occurs when the trained model fits the training dataset too
    well. The model will be very accurate on the training data, but it will not be
    usable in real life, as its accuracy will decrease when used on any other data.
    The model adjusts to the random noise in the training data and assumes patterns
    on this noise that yield false predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Underfitting** occurs when the trained model does not fit the training data
    well enough to recognize important patterns in the data. As a result, it cannot
    make accurate predictions on new data. One example of this is when we attempt
    to do linear regression on a dataset that is not linear. For example, the Fibonacci
    sequence is not linear; therefore, a model on a Fibonacci-like sequence cannot
    be linear either.'
  prefs: []
  type: TYPE_NORMAL
- en: We can do the training-testing split using the `model_selection` library of
    scikit- learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose, in our example, that we have scaled the Fibonacci data and defined
    its indices as labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use 10% of the data as test data, `test_size=0.1`, and specify
    `random_state` parameter in order to get the exact same split every time we run
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Our dataset has been split into test and training sets for our features (`x_train`
    and `x_test`) and for our labels (`y_train` and `y_test`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let''s check each set, beginning with the `x_train` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we check for `x_test`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we check for `y_train`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we check for `y_test`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding output, we can see that our split has been properly executed;
    for instance, our label has been split into `y_test`, which contains the `7` and
    `10` indexes, and `y_train` which contains the remaining `11` indexes. The same
    logic has been applied to our features and we have `2` values in `x_test` and
    `11` values in `x_train`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you remember the Cartesian coordinate system, you know that the horizontal
    axis is the *x*-axis and that the vertical axis is the *y*-axis. Our features
    are on the *x*-axis, while our labels are on the *y*-axis. Therefore, we use features
    and *x* as synonyms, while labels are often denoted by *y*. Therefore, `x_test`
    denotes feature test data, `x_train` denotes feature training data, `y_test` denotes
    label test data, and `y_train` denotes label training data.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a Model on Data with scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are now going to illustrate the process of regression on an example where
    we only have one feature and minimal data.
  prefs: []
  type: TYPE_NORMAL
- en: As we only have one feature, we have to format `x_train` by reshaping it with
    `x_train.reshape (-1,1)` to a NumPy array containing one feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, before executing the code on fitting the best line, execute the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can fit a linear regression model on our data with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also calculate the score associated with the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This score represents the accuracy of the model and is defined as the R2 or
    **coefficient of determination**. It represents how well we can predict the features
    from the labels.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, an R2 of `-1.8268` indicates a very bad model as the best possible
    score is **1**. A score of **0** can be achieved if we constantly predict the
    labels by using the average value of the features.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will omit the mathematical background of this score in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model does not perform well for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: If we check our previous Fibonacci sequence, 11 training data points and 2 testing
    data points are simply not enough to perform a proper predictive analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if we ignore the number of points, the Fibonacci sequence does not describe
    a linear relationship between *x* and *y*. Approximating a nonlinear function
    with a line is only useful if we are looking at two very close data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear Regression Using NumPy Arrays
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One reason why NumPy arrays are handier than Python lists is that they can be
    treated as vectors. There are a few operations defined on vectors that can simplify
    our calculations. We can perform operations on vectors of similar lengths.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take, for example, two vectors, V1 and V2, with three coordinates each:'
  prefs: []
  type: TYPE_NORMAL
- en: V1 = (a, b, c) with a=1, b=2, and c=3
  prefs: []
  type: TYPE_NORMAL
- en: V2 = (d, e, f) with d=2, e=0, and f=2
  prefs: []
  type: TYPE_NORMAL
- en: 'The addition of these two vectors will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: V1 + V2 = (a+d, b+e, c+f) = (1+2, 2+0, 3+2) = (3,2,5)
  prefs: []
  type: TYPE_NORMAL
- en: 'The product of these two vectors will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: V1 + V2 = (a*d, b*e, c*f) = (1*2, 2*0, 3*2) = (2,0,6)
  prefs: []
  type: TYPE_NORMAL
- en: You can think of each vector as our datasets with, for example, the first vector
    as our **features set** and the second vector as our **labels set**. With Python
    being able to do vector calculations, this will greatly simplify the calculations
    required for our linear regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's build a linear regression using NumPy in the following example.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have two sets of data with 13 data points each; we want to build
    a linear regression that best fits all the data points for each set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first set is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If we plot this dataset with the values (`2,8,8,18,25,21,32,44,32,48,61,45,62`)
    as the *y*-axis, and the index of each value (`1,2,3,4,5,6,7,8,9,10,11,12,13`)
    as the *x*-axis, we will get the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: Plotted graph of the first dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.5: Plotted graph of the first dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that this dataset's distribution seems linear in nature, and if we
    wanted to draw a line that was as close as possible to each dot, it wouldn't be
    too hard. A simple linear regression appears appropriate in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our second set is the first 13 values scaled in the Fibonacci sequence that
    we saw earlier in the *Feature Scaling* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If we plot this dataset with the values as the *y*-axis and the index of each
    value as the *x*-axis, we will get the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6: Plotted graph of the second dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.6: Plotted graph of the second dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that this dataset's distribution doesn't appear to be linear, and
    if we wanted to draw a line that was as close as possible to each dot, our line
    would miss quite a lot of dots. A simple linear regression will probably struggle
    in this situation.
  prefs: []
  type: TYPE_NORMAL
- en: We know that the equation of a straight line is ![3](img/B16060_02_6a.png).
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, ![4](img/B16060_02_6b.png) is the slope, and ![5](img/B16060_02_6c.png)
    is the *y* intercept. To find the line of best fit, we must find the coefficients
    of ![6](img/B16060_02_6b.png) and ![7](img/B16060_02_6c.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do this, we will use the least-squares method, which can be achieved
    by completing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: For each data point, calculate *x*2 and *xy*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sum all of *x*, *y*, *x*2, and *x * y*, which gives us ![8](img/B16060_02_6f.png)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Calculate the slope, ![9](img/B16060_02_6b.png), as ![10](img/B16060_02_6h.png)
    with *N* as the total number of data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the *y* intercept, ![11](img/B16060_02_6c.png), as ![12](img/B16060_02_6j.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's apply these steps using NumPy as an example for the first dataset
    in the following code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the first step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'For `x_2`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'For `xy`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s move on to the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'For `sum_x`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'For `sum_y`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'For `sum_x_2`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For `sum_xy`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s move on to the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'For `N`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For `a`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s move on to the final step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'For `b`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we plot the line ![13](img/B16060_02_6l.png) with the preceding coefficients,
    we get the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7: Plotted graph of the linear regression for the first dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.7: Plotted graph of the linear regression for the first dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, our linear regression model works quite well on this dataset,
    which has a linear distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can find a linear regression calculator at [http://www.endmemo.com/statistics/lr.php](http://www.endmemo.com/statistics/lr.php).
    You can also check the calculator to get an idea of what lines of best fit look
    like on a given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now repeat the exact same steps for the second dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'For `a`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'For `b`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we plot the line ![14](img/B16060_02_6l1.png) with the preceding coefficients,
    we get the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8: Plotted graph of the linear regression for the second dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.8: Plotted graph of the linear regression for the second dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, with a nonlinear distribution, our linear regression model struggles
    to fit the data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We don't have to use this method to perform linear regression. Many libraries,
    including scikit-learn, will help us to automate this process. Once we perform
    linear regression with multiple variables, we are better off using a library to
    perform the regression for us.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a Model Using NumPy Polyfit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NumPy Polyfit can also be used to create a line of best fit for linear regression
    with one variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the calculation for the line of best fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The equation for finding the coefficients ![15](img/B16060_02_6b.png) and ![16](img/B16060_02_6c.png)
    is quite long. Fortunately, `numpy.polyfit` in Python performs these calculations
    to find the coefficients of the line of best fit. The `polyfit` function accepts
    three arguments: the array of `x` values, the array of `y` values, and the degree
    of polynomial to look for. As we are looking for a straight line, the highest
    power of `x` is `1` in the polynomial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'For `[a,b]`, the output will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Plotting the Results in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose you have a set of data points and a regression line; our task is to
    plot the points and the line together so that we can see the results with our
    eyes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `matplotlib.pyplot` library for this. This library has two
    important functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scatter`: This displays scattered points on the plane, defined by a list of
    *x* coordinates and a list of *y* coordinates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plot`: Along with two arguments, this function plots a segment defined by
    two points or a sequence of segments defined by multiple points. A plot is like
    a scatter, except that instead of displaying the points, they are connected by
    lines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A plot with three arguments plots a segment and/or two points formatted according
    to the third argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'A segment is defined by two points. As *x* ranges between 1 and 13 (remember
    the dataset contains 13 data points), it makes sense to display a segment between
    0 and 15\. We must substitute the value of *x* in the equation ![17](img/B16060_02_8c.png)
    to get the corresponding *y* values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9: Plotted graph of the linear regression for the first dataset
    using matplotlib'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.9: Plotted graph of the linear regression for the first dataset using
    matplotlib'
  prefs: []
  type: TYPE_NORMAL
- en: The regression line and the scattered data points are displayed as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the plot has an advanced signature. You can use `plot` to draw scattered
    dots, lines, and any curves on this figure. These variables are interpreted in
    groups of three:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x` values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y` values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formatting options in the form of a string
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s create a function for deriving an array of approximated `y` values from
    an array of approximated `x` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the `fit` function to plot the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Every third argument handles formatting. The letter `g` stands for green, while
    the letter `r` stands for red. You could have used `b` for blue and `y` for yellow,
    among other examples. In the absence of a color, each triple value will be displayed
    using a different color. The `o` character symbolizes that we want to display
    a dot where each data point lies. Therefore, `go` has nothing to do with movement
    – it requests the plotter to plot green dots. The `-` characters are responsible
    for displaying a dashed line. If you just use -1, a straight line appears instead
    of the dashed line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10: Graph for the plot function using the fit function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.10: Graph for the plot function using the fit function'
  prefs: []
  type: TYPE_NORMAL
- en: The Python plotter library offers a simple solution for most of your graphing
    problems. You can draw as many lines, dots, and curves as you want on this graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'When displaying curves, the plotter connects the dots with segments. Also,
    bear in mind that even a complex sequence of curves is an approximation that connects
    the dots. For instance, if you execute the code from [https://gist.github.com/traeblain/1487795](https://gist.github.com/traeblain/1487795),
    you will recognize the segments of the `batman` function as connected lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11: Graph for the batman function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.11: Graph for the batman function'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a large variety of ways to plot curves. We have seen that the `polyfit`
    method of the NumPy library returns an array of coefficients to describe a linear equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This array describes the equation *4.85714286 * x - 2.76923077*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we now want to plot a curve, ![18](img/B16060_02_11a.png). This quadratic
    equation is described by the coefficient array `[-1, 3, -2]` as ![19](img/B16060_02_11b.png).
    We could write our own function to calculate the `y` values belonging to `x` values.
    However, the NumPy library already has a feature that can do this work for us
    – `np.poly1d`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The `f` function that''s created by the `poly1d` call not only works with single
    values but also with lists or NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, for `f(x)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use these values to plot a nonlinear curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12: Graph for a nonlinear curve'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.12: Graph for a nonlinear curve'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we can use the `pyplot` library to easily create the plot of
    a nonlinear curve.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Values with Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose we are interested in the `y` value belonging to the `x` coordinate
    `20`. Based on the linear regression model, all we need to do is substitute the
    value of `20` in the place of `x` on the previously used code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13: Graph showing the predicted value using linear regression'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.13: Graph showing the predicted value using linear regression'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we denoted the predicted value with red. This red point is on the best
    line of fit.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at next exercise where we will be predicting populations based on
    linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.01: Predicting the Student Capacity of an Elementary School'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will be trying to forecast the need for elementary school
    capacity. Your task is to figure out 2025 and 2030 predictions for the number
    of children starting elementary school.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The data is contained inside the `population.csv` file, which you can find
    on our GitHub repository: [https://packt.live/2YYlPoj](https://packt.live/2YYlPoj).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import `pandas` and `numpy`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, load the CSV file as a DataFrame on the Notebook and read the CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.14: Reading the CSV file'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.14: Reading the CSV file'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, convert the DataFrame into two NumPy arrays. For simplicity, we can indicate
    that the `year` feature, which is from `2001` to `2018`, is the same as `1` to `18`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `x` output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `y` output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, with the two NumPy arrays, use the `polyfit` method (with a degree of
    `1` as we only have one feature) to determine the coefficients of the regression
    line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `[a, b]` will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, plot the results using `matplotlib.pyplot` and predict the future until `2030`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output is this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.15: Plot showing the future for 2030'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.15: Plot showing the future for 2030'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, the data appears linear and our model seems to be a good fit.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, predict the population for `2025` and `2030`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `population_2025` will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `population_2030` will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/31dvuKt](https://packt.live/31dvuKt).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/317qeIc](https://packt.live/317qeIc).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By completing this exercise, we can now conclude that the population of children
    starting elementary school is going to decrease in the future and that there is
    no need to increase the elementary school capacity if we are currently meeting
    the needs.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression with Multiple Variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we dealt with linear regression with one variable.
    Now we will learn an extended version of linear regression, where we will use
    multiple input variables to predict the output.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you recall the formula for the line of best fit in linear regression, it
    was defined as ![20](img/B16060_02_15a.png), where ![21](img/B16060_02_6b.png)
    is the slope of the line, ![22](img/B16060_02_6c.png) is the *y* intercept of
    the line, *x* is the feature value, and *y* is the calculated label value.
  prefs: []
  type: TYPE_NORMAL
- en: In multiple regression, we have multiple features and one label. If we have
    three features, *x*1, *x*2, and *x*3, our model changes to ![23](img/B16060_02_15d.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'In NumPy array format, we can write this equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'For convenience, it makes sense to define the whole equation in a vector multiplication
    format. The coefficient of ![24](img/B16060_02_6c.png) is going to be `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Multiple linear regression is a simple scalar product of two vectors, where
    the coefficients ![25](img/B16060_02_6c.png), ![26](img/B16060_02_15g.png), ![27](img/B16060_02_15h.png),
    and ![28](img/B16060_02_15i.png) determine the best fit equation in a four-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the formula of multiple linear regression, you will need the
    scalar product of two vectors. As the other name for a scalar product is a dot
    product, the NumPy function performing this operation is called `dot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The output will be `32` as `np.dot(v1, v2)= 1 * 4 + 2 * 5 + 3 * 6 = 32`.
  prefs: []
  type: TYPE_NORMAL
- en: We simply sum the product of each respective coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: We can determine these coefficients by minimizing the error between the data
    points and the nearest points described by the equation. For simplicity, we will
    omit the mathematical solution of the best-fit equation and use scikit-learn instead.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In *n*-dimensional spaces, where *n* is greater than 3, the number of dimensions
    determines the different variables that are in our model. In the preceding example,
    we have three features (*x*1, *x*2, and *x*3) and one label, *y*. This yields
    four dimensions. If you want to imagine a four-dimensional space, you can imagine
    a three-dimensional space with a fourth dimension of time. A five-dimensional
    space can be imagined as a four-dimensional space, where each point in time has
    a temperature. Dimensions are just features (and labels); they do not necessarily
    correlate with our concept of three-dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: The Process of Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will follow the following simple steps to solve linear regression problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Load data from the data sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare data for prediction. Data is prepared in this (`normalize`, `format`,
    and `filter`) format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the parameters of the regression line. Regardless of whether we use
    linear regression with one variable or with multiple variables, we will follow
    these steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Importing Data from Data Sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple libraries that can provide us with access to data sources.
    As we will be working with stock data, let''s cover two examples that are geared
    toward retrieving financial data: Quandl and Yahoo Finance. Take a look at these
    important points before moving ahead:'
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn comes with a few datasets that can be used for practicing your
    skills.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.quandl.com](https://www.quandl.com) provides you with free and
    paid financial datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://pandas.pydata.org/](https://pandas.pydata.org/) helps you load any
    CSV, Excel, JSON, or SQL data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yahoo Finance provides you with financial datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading Stock Prices with Yahoo Finance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The process of loading stock data with Yahoo Finance is straightforward. All
    you need to do is install the `yfinance` package using the following command in
    Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We will download a dataset that has an open price, high price, low price, close
    price, adjusted close price, and volume values of the S&P 500 index starting from
    2015 to January 1, 2020\. The S&P 500 index is the stock market index that measures
    the stock performance of 500 large companies listed in the United States:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset file can also be found in our GitHub repository: [https://packt.live/3fRI5Hk](https://packt.live/3fRI5Hk).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original dataset can be found here: [https://github.com/ranaroussi/yfinance](https://github.com/ranaroussi/yfinance).'
  prefs: []
  type: TYPE_NORMAL
- en: That's all you need to do. The DataFrame containing the S&P 500 index is ready.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can plot the index closing prices using the `plot` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.16: Graph showing the S&P 500 index closing price since 2015'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.16: Graph showing the S&P 500 index closing price since 2015'
  prefs: []
  type: TYPE_NORMAL
- en: The data does not appear to be linear; a polynomial regression might be a better
    model for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to save data to a CSV file using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.quandl.com](https://www.quandl.com) is a reliable source of financial
    and economic datasets that we will be using in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.02: Using Quandl to Load Stock Prices'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this exercise is to download data from the Quandl package and load
    it into a DataFrame like we previously did with Yahoo Finance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install `Quandl` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download the data into a DataFrame using Quandl for the S&P 500\. Its ticker
    is `“YALE/SPCOMP”`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the DataFrame `head()` method to inspect the first five rows of data in
    your DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.17: Dataset displayed as the output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.17: Dataset displayed as the output'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3dwDUz6](https://packt.live/3dwDUz6).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/31812B6](https://packt.live/31812B6).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: By completing this exercise, we have learned how to download an external dataset
    in `CSV` format and import it as a DataFrame. We also learned about the `.head()`
    method, which provides a quick view of the first five rows of your DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will be moving on to prepare the dataset to perform
    multiple linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing Data for Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we perform multiple linear regression on our dataset, we must choose
    the relevant features and the data range on which we will perform the regression.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data for prediction is the second step in the regression process.
    This step also has several sub-steps. We will go through these sub-steps in the
    following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.03: Preparing the Quandl Data for Prediction'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this exercise is to download an external dataset from the Quandl
    library and then prepare it so that it is ready for use in our linear regression
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the Qaundl library is not installed on your system, remember to run the command
    `!pip install quandl`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, download the data into a DataFrame using Quandl for the S&P 500 between
    1950 and 2019\. Its ticker is `“YALE/SPCOMP”`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `head()` method to visualize the columns inside the `data_frame.head()`
    DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.18: Dataset displayed as the output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.18: Dataset displayed as the output'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A few features seem to highly correlate with each other. For instance, the `Real
    Dividend` column grows proportionally with `Real Price`. The ratio between them
    is not always similar, but they do correlate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As regression is not about detecting the correlation between features, we would
    rather get rid of the features that we know are correlated and perform regression
    on the features that are non-correlated. In this case, we will keep the `Long
    Interest Rate`, `Real Price`, and `Real Dividend` columns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Keep only the relevant columns in the `Long Interest Rate`, `Real Price`, and
    `Real Dividend` DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.19: Dataset showing only the relevant columns'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.19: Dataset showing only the relevant columns'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can see that the DataFrame contains a few missing values `NaN`. As regression
    doesn't work with missing values, we need to either replace them or delete them.
    In the real world, we will usually choose to replace them. In this case, we will
    replace the missing values by the preceding values using a method called **forward
    filling**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can replace the missing values with a forward filling as shown in the following
    code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.20: Missing values have been replaced'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.20: Missing values have been replaced'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have cleaned the missing data, we need to create our label. We want
    to predict the `Real Price` column 3 months in advance using the current `Real
    Price`, `Long Interest Rate`, and `Real Dividend` columns. In order to create
    our label, we need to shift the `Real Price` values up by three units and call
    it `Real Price Label`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the `Real Price Label` label by shifting `Real Price` by 3 months as
    shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.21: New labels have been created'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.21: New labels have been created'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The side effect of shifting these values is that missing values will appear
    in the last three rows for `Real Price Label`, so we need to remove the last three
    rows of data. However, before that, we need to convert the features into a NumPy
    array and scale it. We can use the `drop` method of the DataFrame to remove the
    label column and the preprocessing function from `sklearn` to scale the features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a NumPy array for the features and scale it in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `1` in the second argument specifies that we are dropping columns. As the
    original DataFrame was not modified, the label can be directly extracted from
    it. Now that the features are scaled, we need to remove the last three values
    of the features as they are the features of the missing values in the label column.
    We will save them for later in the prediction part.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Remove the last three values of the `features` array and save them into another
    array using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `scaled_features` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `scaled_features` variable doesn't contain the three data points anymore
    as they are now in `scaled_features_latest_3`. Now we can remove the last three
    rows with missing data from the DataFrame, then convert the label into a NumPy
    array using `sklearn`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Remove the rows with missing data in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `data_frame` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.22: Dataset updated with the removal of missing values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.22: Dataset updated with the removal of missing values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, the last three rows were also removed from the DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now let''s see if we have accurately created our label. Go ahead and run the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for the `label` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.23: Output showing the expected labels'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.23: Output showing the expected labels'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Our variable contains all the labels and is exactly the same as the `Real Price
    Label` column in the DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Our next task is to separate the training and testing data from each other.
    As we saw in the *Splitting Data into Training and Testing* section, we will use
    90% of the data as the training data and the remaining 10% as the test data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split the `features` data into training and test sets using `sklearn` with
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `train_test_split` function shuffles the lines of our data, keeps the correspondence,
    and puts approximately 10% of all data in the test variables, keeping 90% for
    the training variables. We also use `random_state=8` in order to reproduce the
    results. Our data is now ready to be used for the multiple linear regression model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2zZssOG](https://packt.live/2zZssOG).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2zW8WCH](https://packt.live/2zW8WCH).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By completing this exercise, we have learned all the required steps for data
    preparation before performing a regression.
  prefs: []
  type: TYPE_NORMAL
- en: Performing and Validating Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that our data has been prepared, we can perform our linear regression. After
    that, we will measure our model performance and see how well it performs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now create the linear regression model based on the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is ready, we can use it to predict the labels belonging to the
    test feature values and use the `score` method from the model to see how accurate
    it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: With a score or R2 of `0.985`, we can conclude that the model is very accurate.
    This is not a surprise since the financial market grows at around 6-7% a year.
    This is linear growth, and the model essentially predicts that the markets will
    continue growing at a linear rate. Concluding that markets tend to increase in
    the long run is not rocket science.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the Future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that our model has been trained, we can use it to predict future values.
    We will use the `scaled_features_latest_3` variable that we created by taking
    the last three values of the features NumPy array and using it to predict the
    index price of the next three months in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: By looking at the output, you might think it seems easy to forecast the value
    of the S&P 500 and use it to earn money by investing in it. Unfortunately, in
    practice, using this model for making money by betting on the forecast is by no
    means better than gambling in a casino. This is just an example to illustrate
    prediction; it is not enough to be used for short-term or long-term speculation
    on market prices. In addition to this, stock prices are sensitive to many external
    factors, such as economic recession and government policy. This means that past
    patterns do not necessarily reflect any patterns in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial and Support Vector Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When performing a polynomial regression, the relationship between *x* and *y*,
    or using their other names, features, and labels, is not a linear equation, but
    a polynomial equation. This means that instead of the ![29](img/B16060_02_6l.png)
    equation, we can have multiple coefficients and multiple powers of *x* in the
    equation.
  prefs: []
  type: TYPE_NORMAL
- en: To make matters even more complicated, we can perform polynomial regression
    using multiple variables, where each feature may have coefficients multiplying
    different powers of the feature.
  prefs: []
  type: TYPE_NORMAL
- en: Our task is to find a curve that best fits our dataset. Once polynomial regression
    is extended to multiple variables, we will learn the SVM model to perform polynomial regression.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial Regression with One Variable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a recap, we have performed two types of regression so far:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simple linear regression: ![30](img/B16060_02_6l.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple linear regression: ![31](img/B16060_02_23c_New.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now learn how to do polynomial linear regression with one variable.
    The equation for polynomial linear regression is ![33](img/B16060_02_23d.png).
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial linear regression has a vector of coefficients, ![34](img/B16060_02_23e.png),
    multiplying a vector of degrees of *x* in the polynomial, ![35](img/B16060_02_23f.png).
  prefs: []
  type: TYPE_NORMAL
- en: At times, polynomial regression works better than linear regression. If the
    relationship between labels and features can be described using a linear equation,
    then using a linear equation makes perfect sense. If we have a nonlinear growth,
    polynomial regression tends to approximate the relationship between features and
    labels better.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest implementation of linear regression with one variable was the `polyfit`
    method of the NumPy library. In the next exercise, we will perform multiple polynomial
    linear regression with degrees of 2 and 3.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Even though our polynomial regression has an equation containing coefficients
    of *x*n, this equation is still referred to as polynomial linear regression in
    literature. Regression is made linear not because we restrict the usage of higher
    powers of *x* in the equation, but because the coefficients *a*1,*a*2 … and so
    on are linear in the equation. This means that we use the toolset of linear algebra
    and work with matrices and vectors to find the missing coefficients that minimize
    the error of the approximation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.04: First-, Second-, and Third-Degree Polynomial Regression'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this exercise is to perform first-, second-, and third-degree polynomial
    regression on the two sample datasets that we used earlier in this chapter. The
    first dataset has a linear distribution and the second one is the Fibonacci sequence
    and has a nonlinear distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `numpy` and `matplotlib` packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the first dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the second dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform a polynomial regression of degrees `1`, `2`, and `3` on the first dataset
    using the `polyfit` method from `numpy` in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `f1` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, a polynomial regression of degree `1` has two coefficients.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output for `f2` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, a polynomial regression of degree `2` has three coefficients.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output for `f3` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, a polynomial regression of degree `3` has four coefficients.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we have calculated the three polynomial regressions, we can plot them
    together with the data on a graph to see how they behave.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the three polynomial regressions and the data on a graph in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.24: Graph showing the polynomial regressions for the first dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.24: Graph showing the polynomial regressions for the first dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As the coefficients are enumerated from left to right in order of decreasing
    degree, we can see that the higher-degree coefficients stay close to negligible.
    In other words, the three curves are almost on top of each other, and we can only
    detect a divergence near the right edge. This is because we are working on a dataset
    that can be very well approximated with a linear model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In fact, the first dataset was created out of a linear function. Any non-zero
    coefficients for *x*2 and *x*3 are the result of overfitting the model based on
    the available data. The linear model is better for predicting values outside the
    range of the training data than any higher-degree polynomial.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's contrast this behavior with the second example. We know that the Fibonacci
    sequence is nonlinear. So, using a linear equation to approximate it is a clear
    case for underfitting. Here, we expect a higher polynomial degree to perform better.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform a polynomial regression of degrees `1`, `2`, and `3` on the second
    dataset using the `polyfit` method from `numpy` with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for `g1` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, a polynomial regression of degree `1` has `2` coefficients.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output for `g2` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, a polynomial regression of degree `2` has `3` coefficients.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output for `g3` is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, a polynomial regression of degree `3` has `4` coefficients.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the three polynomial regressions and the data on a graph in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.25: Graph showing the second dataset points and three polynomial
    curves'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16060_02_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.25: Graph showing the second dataset points and three polynomial curves'
  prefs: []
  type: TYPE_NORMAL
- en: The difference is clear. The quadratic curve fits the points a lot better than
    the linear one. The cubic curve is even better.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3dpCgyY](https://packt.live/3dpCgyY).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2B09xDN](https://packt.live/2B09xDN).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: If you research Binet's formula, you will find out that the Fibonacci function
    is an exponential function, as the *n*th Fibonacci number is calculated as the
    *n*th power of a constant. Therefore, the higher the polynomial degree we use,
    the more accurate our approximation will be.
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial Regression with Multiple Variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we have one variable of degree *n*, we have *n+1* coefficients in the equation
    as ![36](img/B16060_02_23d.png).
  prefs: []
  type: TYPE_NORMAL
- en: Once we deal with multiple features, *x*1, *x*2, …, *x*m, and their powers of
    up to the *n*th degree, we get an *m * (n+1)* matrix of coefficients. The math
    will become quite lengthy when we start exploring the details and prove how a
    polynomial model works. We will also lose the nice visualizations of two-dimensional
    curves.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we will apply the concepts learned in the previous section on polynomial
    regression with one variable and omit the math. When training and testing a linear
    regression model, we can calculate the mean square error to see how good an approximation
    a model is.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, the degree of the polynomials used in the approximation is
    a simple parameter in the model.
  prefs: []
  type: TYPE_NORMAL
- en: As polynomial regression is a form of linear regression, we can perform polynomial
    regression without changing the regression model. All we need to do is to transform
    the input and keep the linear regression model. The transformation of the input
    is performed by the `fit_transform` method of the `PolynomialFeatures` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can reuse the code from *Exercise 2.03*, *Preparing the Quandl Data
    for Prediction*, up to *Step 9* and import `PolynomialFeatures` from the `preprocessing`
    module of `sklearn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create a polynomial regression of degree `3` using the `fit_transform`
    method of `PolynomialFeatures`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of `poly_scaled_features` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to split the data into testing and training sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: The `train_test_split` function shuffles the lines of our data, keeps the correspondence,
    and puts approximately 10% of all data in the test variables, keeping 90% for
    the training variables. We also use `random_state=8` in order to reproduce the
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our data is now ready to be used for the multiple polynomial regression model;
    we will also measure its performance with the `score` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: With a score or R2 of `0.988`, our multiple polynomial regression model is slightly
    better than our multiple linear regression model (`0.985`), which we built in
    *Exercise 2.03*, *Preparing the Quandl Data for Prediction*. It might be possible
    that both models are overfitting the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There is another model in scikit-learn that performs polynomial regression,
    called the SVM model.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SVMs are binary classifiers and are usually used in classification problems
    (you will learn more about this in *Chapter 3*, *An Introduction to Classification*).
    An SVM classifier takes data and tries to predict which class it belongs to. Once
    the classification of a data point is determined, it gets labeled. But SVMs can
    also be used for regression; that is, instead of labeling data, it can predict
    future values in a series.
  prefs: []
  type: TYPE_NORMAL
- en: The SVR model uses the space between our data as a margin of error. Based on
    the margin of error, it makes predictions regarding future values.
  prefs: []
  type: TYPE_NORMAL
- en: If the margin of error is too small, we risk overfitting the existing dataset.
    If the margin of error is too big, we risk underfitting the existing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a classifier, the kernel describes the surface dividing the state
    space, whereas, in a regression, the kernel measures the margin of error. This
    kernel can use a linear model, a polynomial model, or many other possible models.
    The default kernel is **RBF**, which stands for **Radial Basis Function**.
  prefs: []
  type: TYPE_NORMAL
- en: SVR is an advanced topic that is outside the scope of this book. Therefore,
    we will only stick to an easy walk-through as an opportunity to try out another
    regression model on our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can reuse the code from *Exercise 2.03*, *Preparing the Quandl Data for
    Prediction*, up to *Step 11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can perform a regression with `svm` by simply changing the linear
    model to a support vector model by using the `svm` method from `sklearn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, performing an SVR is exactly the same as performing a linear
    regression, with the exception of defining the model as `svm.SVR()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can predict and measure the performance of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the score or R2 is quite low, our SVR's parameters need to be
    optimized in order to increase the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines with a 3-Degree Polynomial Kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s switch the kernel of the SVM to a polynomial function (the default degree
    is `3`) and measure the performance of the new model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: We managed to increase the performance of the SVM by simply changing the kernel
    function to a polynomial function; however, the model still needs a lot of tuning
    to reach the same performance as the linear regression models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2.01: Boston House Price Prediction with Polynomial Regression of
    Degrees 1, 2, and 3 on Multiple Variables'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, you will need to perform linear polynomial regression of degrees
    1, 2, and 3 with scikit-learn and find the best model. You will work on the Boston
    House Prices dataset. The Boston House Price dataset is very famous and has been
    used as an example for research on regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: More details about the Boston House Prices dataset can be found at [https://archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset file can also be found in our GitHub repository: [https://packt.live/2V9kRUU](https://packt.live/2V9kRUU).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to predict the prices of houses in Boston (label) based on their
    characteristics (features). Your main goal will be to build 3 linear models using
    polynomial regressions of degrees `1`, `2`, and `3` with all the features of the
    dataset. You can find the following dataset description:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.26: Boston housing dataset description'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.26: Boston housing dataset description'
  prefs: []
  type: TYPE_NORMAL
- en: We will define our label as the `MEDV` field, which is the median value of the
    house in $1,000s. All of the other fields will be used as our features for our
    models. As this dataset does not contain any missing values, we won't have to
    replace missing values as we did in the previous exercises.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the required packages and load the Boston House Prices data into a DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the dataset for prediction by converting the label and features into
    NumPy arrays and scaling the features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create three different sets of features by transforming the scaled features
    into suitable formats for each of the polynomial regressions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into training and testing sets with `random state = 8`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a polynomial regression of degree `1` and evaluate whether the model
    is overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a polynomial regression of degree `2` and evaluate whether the model
    is overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a polynomial regression of degree `3` and evaluate whether the model
    is overfitting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the predictions of the three models against the label on the testing
    set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The expected output is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.27: Expected output based on the predictions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_02_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.27: Expected output based on the predictions'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity is available on page 334.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned the fundamentals of linear regression. After
    going through some basic mathematics, we looked at the mathematics of linear regression
    using one variable and multiple variables.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned how to load external data from sources such as a CSV file,
    Yahoo Finance, and Quandl. After loading the data, we learned how to identify
    features and labels, how to scale data, and how to format data to perform regression.
  prefs: []
  type: TYPE_NORMAL
- en: We learned how to train and test a linear regression model, and how to predict
    the future. Our results were visualized by an easy-to-use Python graph plotting
    library called `pyplot`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also learned about a more complex form of linear regression: linear polynomial
    regression using arbitrary degrees. We learned how to define these regression
    problems on multiple variables and compare their performance on the Boston House
    Price dataset. As an alternative to polynomial regression, we also introduced
    SVMs as a regression model and experimented with two kernels.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about classification and its models.
  prefs: []
  type: TYPE_NORMAL
