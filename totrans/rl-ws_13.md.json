["```py\n    import abc\n    import numpy as np\n    import gym\n    ```", "```py\n    \"\"\"\n    Abstract class representing the agent\n    Init with the action space and the function pi returning the action\n    \"\"\"\n    class Agent:\n        def __init__(self, action_space: gym.spaces.Space):\n            \"\"\"\n            Constructor of the agent class.\n            Args:\n                action_space (gym.spaces.Space): environment action space\n            \"\"\"\n            raise NotImplementedError(\"This class cannot be instantiated.\")\n        @abc.abstractmethod\n        def pi(self, state: np.ndarray) -> np.ndarray:\n            \"\"\"\n            Agent's policy.\n            Args:\n                state (np.ndarray): environment state\n            Returns:\n                The selected action\n            \"\"\"\n            pass\n    ```", "```py\n    class ContinuousAgent(Agent):\n        def __init__(self, action_space: gym.spaces.Space, seed=46):\n            # setup seed\n            np.random.seed(seed)\n            # check the action space type\n            if not isinstance(action_space, gym.spaces.Box):\n                raise ValueError\\\n                      (\"This is a Continuous Agent pass as \"\\\n                       \"input a Box Space.\")\n    ```", "```py\n            \"\"\"\n            initialize the distribution according to the action space type\n            \"\"\"\n            if (action_space.low == -np.inf) and \\\n               (action_space.high == np.inf):\n                # the distribution is a normal distribution\n                self._pi = lambda: np.random.normal\\\n                                   (loc=0, scale=1, \\\n                                    size=action_space.shape)\n                return\n    ```", "```py\n            if (action_space.low != -np.inf) and \\\n               (action_space.high != np.inf):\n                # the distribution is a uniform distribution\n                self._pi = lambda: np.random.uniform\\\n                           (low=action_space.low, \\\n                            high=action_space.high, \\\n                            size=action_space.shape)\n                return\n    ```", "```py\n            if action_space.low == -np.inf:\n                # negative exponential distribution\n                self._pi = (lambda: -np.random.exponential\\\n                            (size=action_space.shape)\n                            + action_space.high)\n                return\n    ```", "```py\n            if action_space.high == np.inf:\n                # exponential distribution\n                self._pi = (lambda: np.random.exponential\\\n                            (size=action_space.shape)\n                            + action_space.low)\n                return\n    ```", "```py\n        def pi(self, observation: np.ndarray) -> np.ndarray:\n            \"\"\"\n            Policy: simply call the internal _pi().\n\n            This is a random agent, so the action is independent \n            from the observation.\n            For real agents the action depends on the observation.\n            \"\"\"\n            return self._pi()\n    ```", "```py\n    class DiscreteAgent(Agent):\n        def __init__(self, action_space: gym.spaces.Space, seed=46):\n            # setup seed\n            np.random.seed(seed)\n            # check the action space type\n            if not isinstance(action_space, gym.spaces.Discrete):\n                raise ValueError(\"This is a Discrete Agent pass \"\\\n                                 \"as input a Discrete Space.\")\n            \"\"\"\n            initialize the distribution according to the action \n            space n attribute\n            \"\"\"\n            # the distribution is a uniform distribution\n            self._pi = lambda: np.random.randint\\\n                       (low=0, high=action_space.n)\n        def pi(self, observation: np.ndarray) -> np.ndarray:\n            \"\"\"\n            Policy: simply call the internal _pi().\n            This is a random agent, so the action is independent \n            from the observation.\n            For real agents the action depends on the observation.\n            \"\"\"\n            return self._pi()\n    ```", "```py\n    def make_agent(action_space: gym.spaces.Space, seed=46):\n        \"\"\"\n        Returns the correct agent based on the action space type\n        \"\"\"\n        if isinstance(action_space, gym.spaces.Discrete):\n            return DiscreteAgent(action_space, seed)\n        if isinstance(action_space, gym.spaces.Box):\n            return ContinuousAgent(action_space, seed)\n        raise ValueError(\"Only Box spaces or Discrete Spaces \"\\\n                         \"are allowed, check the action space of \"\\\n                         \"the environment\")\n    ```", "```py\n    # Environment Name\n    env_name = \"CartPole-v0\"\n    # Number of episodes\n    episodes = 10\n    # Number of Timesteps of each episode\n    timesteps = 100\n    # Discount factor\n    gamma = 1.0\n    # seed environment\n    seed = 46\n    # Needed to show the environment in a notebook\n    from gym import wrappers\n    env = gym.make(env_name)\n    env.seed(seed)\n    # the last argument is needed to record all episodes\n    # otherwise gym would record only some of them\n    # The monitor saves the episodes inside the folder ./gym-results\n    env = wrappers.Monitor(env, \"./gym-results\", force=True, \\\n                           video_callable=lambda episode_id: True)\n    agent = make_agent(env.action_space, seed)\n    ```", "```py\n    # list of returns\n    episode_returns = []\n    ```", "```py\n    # loop for the episodes\n    for episode_number in range(episodes):\n        # here we are inside an episode\n    ```", "```py\n        # reset cumulated gamma\n        gamma_cum = 1\n        # return of the current episode\n        episode_return = 0\n    ```", "```py\n        # the reset function resets the environment and returns\n        # the first environment observation\n        observation = env.reset()\n    ```", "```py\n        # loop for the given number of timesteps or\n        # until the episode is terminated\n        for timestep_number in range(timesteps):\n    ```", "```py\n            # if you want to render the environment\n            # uncomment the following line\n            # env.render()\n            # select the action\n            action = agent.pi(observation)\n            # apply the selected action by calling env.step\n            observation, reward, done, info = env.step(action)\n    ```", "```py\n            # increment the return\n            episode_return += reward * gamma_cum\n            # update the value of cumulated discount factor\n            gamma_cum = gamma_cum * gamma\n    ```", "```py\n            \"\"\"\n            if done the episode is terminated, we have to reset\n            the environment\n            \"\"\"\n            if done:\n                print(f\"Episode Number: {episode_number}, \\\n    Timesteps: {timestep_number}, Return: {episode_return}\")\n                # break from the timestep loop\n                break\n    ```", "```py\n        episode_returns.append(episode_return)\n    ```", "```py\n    # close the environment\n    env.close()\n    # Calculate return statistics\n    avg_return = np.mean(episode_returns)\n    std_return = np.std(episode_returns)\n    var_return = std_return ** 2  # variance is std^2\n    print(f\"Statistics on Return: Average: {avg_return}, \\\n    Variance: {var_return}\")\n    ```", "```py\n    Episode Number: 0, Timesteps: 27, Return: 28.0\n    Episode Number: 1, Timesteps: 9, Return: 10.0\n    Episode Number: 2, Timesteps: 13, Return: 14.0\n    Episode Number: 3, Timesteps: 16, Return: 17.0\n    Episode Number: 4, Timesteps: 31, Return: 32.0\n    Episode Number: 5, Timesteps: 10, Return: 11.0\n    Episode Number: 6, Timesteps: 14, Return: 15.0\n    Episode Number: 7, Timesteps: 11, Return: 12.0\n    Episode Number: 8, Timesteps: 10, Return: 11.0\n    Episode Number: 9, Timesteps: 30, Return: 31.0\n    Statistics on Return: Average: 18.1, Variance: 68.89000000000001\n    ```", "```py\n# Render the episodes\nimport io\nimport base64\nfrom IPython.display import HTML, display\nepisodes_to_watch = 1\nfor episode in range(episodes_to_watch):\n    video = io.open(f\"./gym-results/openaigym.video\\\n.{env.file_infix}.video{episode:06d}.mp4\", \"r+b\").read()\n    encoded = base64.b64encode(video)\n    display(\n        HTML(\n            data=\"\"\"\n        <video width=\"360\" height=\"auto\" alt=\"test\" controls>\n        <source src=\"img/mp4;base64,{0}\" type=\"video/mp4\" />\n        </video>\"\"\".format(\n                encoded.decode(\"ascii\")\n            )\n        )\n    )\n```", "```py\n    from enum import Enum, auto\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from scipy import linalg\n    from typing import Tuple\n    ```", "```py\n    # helper function\n    def vis_matrix(M, cmap=plt.cm.Blues):\n        fig, ax = plt.subplots()\n        ax.matshow(M, cmap=cmap)\n        for i in range(M.shape[0]):\n            for j in range(M.shape[1]):\n                c = M[j, i]\n                ax.text(i, j, \"%.2f\" % c, va=\"center\", ha=\"center\")\n    ```", "```py\n    # Define the actions\n    class Action(Enum):\n        UP = auto()\n        DOWN = auto()\n        LEFT = auto()\n        RIGHT = auto()\n    ```", "```py\n    # Agent Policy, random\n    class Policy:\n        def __init__(self):\n            self._possible_actions = [action for action in Action]\n            self._action_probs = {a: 1 / len(self._possible_actions) \\\n                                  for a in self._possible_actions}\n        def __call__(self, state: Tuple[int, int], \\\n                     action: Action) -> float:\n            \"\"\"\n            Returns the action probability\n            \"\"\"\n            assert action in self._possible_actions\n            # state is unused for this policy\n            return self._action_probs[action]\n    ```", "```py\n    class Environment:\n        def __init__(self):\n            self.grid_width = 5\n            self.grid_height = 5\n            self._good_state1 = (0, 1)\n            self._good_state2 = (0, 3)\n            self._to_state1 = (4, 2)\n            self._to_state2 = (2, 3)\n            self._bad_state1 = (1, 1)\n            self._bad_state2 = (4, 4)\n            self._bad_states = [self._bad_state1, self._bad_state2]\n            self._good_states = [self._good_state1, self._good_state2]\n            self._to_states = [self._to_state1, self._to_state2]\n            self._good_rewards = [10, 5]\n        def step(self, state, action):\n            i, j = state\n            # search among good states\n            for good_state, reward, \\\n                to_state in zip(self._good_states, \\\n                                self._good_rewards, \\\n                                self._to_states):\n                if (i, j) == good_state:\n                    return (to_state, reward)\n            reward = 0\n            # if the state is a bad state, the reward is -1\n            if state in self._bad_states:\n                reward = -1\n            # calculate next state based on the action\n            if action == Action.LEFT:\n                j_next = max(j - 1, 0)\n                i_next = i\n                if j - 1 < 0:\n                    reward = -1\n            elif action == Action.RIGHT:\n                j_next = min(j + 1, self.grid_width - 1)\n                i_next = i\n                if j + 1 > self.grid_width - 1:\n                    reward = -1\n            elif action == Action.UP:\n                j_next = j\n                i_next = max(i - 1, 0)\n                if i - 1 < 0:\n                    reward = -1\n            elif action == Action.DOWN:\n                j_next = j\n                i_next = min(i + 1, self.grid_height - 1)\n                if i + 1 > self.grid_height - 1:\n                    reward = -1\n            else:\n                 raise ValueError(\"Invalid action\")\n            return ((i_next, j_next), reward)\n    ```", "```py\n    pi = Policy()\n    env = Environment()\n    # setup probability matrix and reward matrix\n    P = np.zeros((env.grid_width*env.grid_height, \\\n                  env.grid_width*env.grid_height))\n    R = np.zeros_like(P)\n    possible_actions = [action for action in Action]\n    # Loop for all states and fill up P and R\n    for i in range(env.grid_height):\n        for j in range(env.grid_width):\n            state = (i, j)\n            # loop for all action and setup P and R\n            for action in possible_actions:\n                next_state, reward = env.step(state, action)\n                (i_next, j_next) = next_state\n                P[i*env.grid_width+j, \\\n                  i_next*env.grid_width \\\n                  + j_next] += pi(state, action)\n                \"\"\"\n                the reward depends only on the starting state and \n                the final state\n                \"\"\"\n                R[i*env.grid_width+j, \\\n                  i_next*env.grid_width + j_next] = reward\n    ```", "```py\n    # check the correctness\n    assert((np.sum(P, axis=1) == 1).all())\n    ```", "```py\n    # expected reward for each state\n    R_expected = np.sum(P * R, axis=1, keepdims=True)\n    ```", "```py\n    # reshape the state values in a matrix\n    R_square = R_expected.reshape((env.grid_height,env.grid_width))\n    # Visualize\n    vis_matrix(R_square, cmap=plt.cm.Reds)\n    ```", "```py\n    # define the discount factor\n    gamma = 0.9\n    # Now it is possible to solve the Bellman Equation\n    A = np.eye(env.grid_width*env.grid_height) - gamma * P\n    B = R_expected\n    ```", "```py\n    # solve using scipy linalg\n    V = linalg.solve(A, B)\n    ```", "```py\n    # reshape the state values in a matrix\n    V_square = V.reshape((env.grid_height,env.grid_width))\n    # visualize results\n    vis_matrix(V_square, cmap=plt.cm.Reds)\n    ```", "```py\n    from __future__ import absolute_import, division, \\\n    print_function, unicode_literals\n    import numpy as np\n    import matplotlib.pyplot as plt\n    # TensorFlow\n    import tensorflow as tf\n    import tensorflow_datasets as tfds\n    ```", "```py\n    # Construct a tf.data.Dataset\n    (train_images, train_labels), (test_images, test_labels) = \\\n    tfds.as_numpy(tfds.load('fashion_mnist', \\\n                            split=['train', 'test'],\\\n                            batch_size=-1, as_supervised=True,))\n    train_images = np.squeeze(train_images)\n    test_images = np.squeeze(test_images)\n    classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', \\\n               'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', \\\n               'Ankle boot']\n    ```", "```py\n    print(\"Training dataset shape =\", train_images.shape)\n    print(\"Training labels length =\", len(train_labels))\n    print(\"Some training labels =\", train_labels[:5])\n    print(\"Test dataset shape =\", test_images.shape)\n    print(\"Test labels length =\", len(test_labels))\n    ```", "```py\n    Training dataset shape = (60000, 28, 28)\n    Training labels length = 60000\n    Some training labels = [2 1 8 4 1]\n    Test dataset shape = (10000, 28, 28)\n    Test labels length = 10000\n    ```", "```py\n    plt.figure()\n    plt.imshow(train_images[0])\n    plt.colorbar()\n    plt.grid(False)\n    plt.show()\n    ```", "```py\n    train_images = train_images / 255.0\n    test_images = test_images / 255.0\n    ```", "```py\n    plt.figure(figsize=(10,10))\n    for i in range(25):\n        plt.subplot(5,5,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(train_images[i], cmap=plt.cm.binary)\n        plt.xlabel(classes[train_labels[i]])\n    plt.show()\n    ```", "```py\n    model = tf.keras.Sequential\\\n            ([tf.keras.layers.Flatten(input_shape=(28, 28)),\\\n              tf.keras.layers.Dense(128, activation='relu'),\\\n              tf.keras.layers.Dense(10)])\n    ```", "```py\n    model.compile(optimizer='adam',\\\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy\\\n                  (from_logits=True), metrics=['accuracy'])\n    ```", "```py\n    model.fit(train_images, train_labels, epochs=10)\n    ```", "```py\n    Epoch 9/1060000/60000 [==============================] \\\n    - 2s 40us/sample - loss: 0.2467 - accuracy: 0.9076\n    Epoch 10/1060000/60000 [==============================] \\\n    - 2s 40us/sample - loss: 0.2389 - accuracy: 0.9103\n    ```", "```py\n    test_loss, test_accuracy = model.evaluate\\\n                               (test_images, test_labels, verbose=2)\n    print('\\nTest accuracy:', test_accuracy)\n    ```", "```py\n    10000/10000 - 0s - loss: 0.3221 - accuracy: 0.8878\n    Test accuracy: 0.8878\n    ```", "```py\n    probability_model = tf.keras.Sequential\\\n                        ([model,tf.keras.layers.Softmax()])\n    predictions = probability_model.predict(test_images)\n    print(predictions[0:3])\n    ```", "```py\n    [[3.85897374e-06 2.33953915e-06 2.30801385e-02 4.74092474e-07\n      9.55752671e-01 1.56392260e-10 2.11589299e-02 8.57651870e-08\n      1.49855202e-06 1.05843508e-10]\n    ```", "```py\n    print(\"Class ID, predicted | real =\", \\\n          np.argmax(predictions[0]), \"|\", test_labels[0])\n    ```", "```py\n    Class ID, predicted | real = 4 | 4\n    ```", "```py\n    def plot_image(i, predictions_array, true_label, img):\n        predictions_array, true_label, img = predictions_array,\\\n                                             true_label[i], img[i]\n        plt.grid(False)\n        plt.xticks([])\n        plt.yticks([])\n        plt.imshow(img, cmap=plt.cm.binary)\n        predicted_label = np.argmax(predictions_array)\n        if predicted_label == true_label:\n            color = 'blue'\n        else:\n            color = 'red'\n        plt.xlabel(\"{} {:2.0f}% ({})\".format\\\n                   (classes[predicted_label], \\\n                    100*np.max(predictions_array),\\\n                    classes[true_label]),\\\n                    color=color)\n    ```", "```py\n    def plot_value_array(i, predictions_array, true_label):\n        predictions_array, true_label = predictions_array,\\\n                                        true_label[i]\n        plt.grid(False)\n        plt.xticks(range(10))\n        plt.yticks([])\n        thisplot = plt.bar(range(10), predictions_array,\\\n                   color=\"#777777\")\n        plt.ylim([0, 1])\n        predicted_label = np.argmax(predictions_array)\n        thisplot[predicted_label].set_color('red')\n        thisplot[true_label].set_color('blue')\n    ```", "```py\n    i = 0\n    plt.figure(figsize=(6,3))\n    plt.subplot(1,2,1)\n    plot_image(i, predictions[i], test_labels, test_images)\n    plt.subplot(1,2,2)\n    plot_value_array(i, predictions[i],  test_labels)\n    plt.show()\n    ```", "```py\n    \"\"\"\n    Plot the first X test images, their predicted labels, and the true labels.\n    Color correct predictions in blue and incorrect predictions in red.\n    \"\"\"\n    num_rows = 5\n    num_cols = 3\n    num_images = num_rows*num_cols\n    plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n    for i in range(num_images):\n        plt.subplot(num_rows, 2*num_cols, 2*i+1)\n        plot_image(i, predictions[i], test_labels, test_images)\n        plt.subplot(num_rows, 2*num_cols, 2*i+2)\n        plot_value_array(i, predictions[i], test_labels)\n    plt.tight_layout()\n    plt.show()\n    ```", "```py\n    from baselines.ppo2.ppo2 import learn\n    from baselines.ppo2 import defaults\n    from baselines.common.vec_env import VecEnv, VecFrameStack\n    from baselines.common.cmd_util import make_vec_env, make_env\n    from baselines.common.models import register\n    import tensorflow as tf\n    ```", "```py\n    @register(\"custom_cnn\")\n    def custom_cnn():\n        def network_fn(input_shape, **conv_kwargs):\n            \"\"\"\n            Custom CNN\n            \"\"\"\n            print('input shape is {}'.format(input_shape))\n            x_input = tf.keras.Input\\\n                      (shape=input_shape, dtype=tf.uint8)\n            h = x_input\n            h = tf.cast(h, tf.float32) / 255.\n            h = tf.keras.layers.Conv2D\\\n                (filters=32,kernel_size=8,strides=4, \\\n                 padding='valid', data_format='channels_last',\\\n                 activation='relu')(h)\n            h2 = tf.keras.layers.Conv2D\\\n                 (filters=64, kernel_size=4,strides=2,\\\n                  padding='valid', data_format='channels_last',\\\n                  activation='relu')(h)\n            h3 = tf.keras.layers.Conv2D\\\n                 (filters=64, kernel_size=3,strides=1,\\\n                  padding='valid', data_format='channels_last',\\\n                  activation='relu')(h2)\n            h3 = tf.keras.layers.Flatten()(h3)\n            h3 = tf.keras.layers.Dense\\\n                 (units=512, name='fc1', activation='relu')(h3)\n            network = tf.keras.Model(inputs=[x_input], outputs=[h3])\n            network.summary()\n            return network\n        return network_fn\n    ```", "```py\n    def build_env(env_id, env_type):\n        if env_type in {'atari', 'retro'}:\n            env = make_vec_env(env_id, env_type, 1, None, \\\n                               gamestate=None, reward_scale=1.0)\n            env = VecFrameStack(env, 4)\n        else:\n            env = make_vec_env(env_id, env_type, 1, None,\\\n                               reward_scale=1.0,\\\n                               flatten_dict_observations=True)\n        return env\n    ```", "```py\n    env_id = 'PongNoFrameskip-v0'\n    env_type = 'atari'\n    print(\"Env type = \", env_type)\n    env = build_env(env_id, env_type)\n    model = learn(network=\"custom_cnn\", env=env, total_timesteps=1e4)\n    ```", "```py\n    Env type =  atari\n    Logging to /tmp/openai-2020-05-11-16-19-42-770612\n    input shape is (84, 84, 4)\n    Model: \"model\"\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #  \n    =================================================================\n    input_1 (InputLayer)         [(None, 84, 84, 4)]       0        \n    _________________________________________________________________\n    tf_op_layer_Cast (TensorFlow [(None, 84, 84, 4)]       0        \n    _________________________________________________________________\n    tf_op_layer_truediv (TensorF [(None, 84, 84, 4)]       0        \n    _________________________________________________________________\n    conv2d (Conv2D)              (None, 20, 20, 32)        8224     \n    _________________________________________________________________\n    conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832    \n    _________________________________________________________________\n    conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928    \n    _________________________________________________________________\n    flatten (Flatten)            (None, 3136)              0        \n    _________________________________________________________________\n    fc1 (Dense)                  (None, 512)               1606144  \n    =================================================================\n    Total params: 1,684,128\n    Trainable params: 1,684,128\n    Non-trainable params: 0\n    _________________________________________________________________\n    --------------------------------------------\n    | eplenmean               | 1e+03          |\n    | eprewmean               | -20            |\n    | fps                     | 213            |\n    | loss/approxkl           | 0.00012817292  |\n    | loss/clipfrac           | 0.0            |\n    | loss/policy_entropy     | 1.7916294      |\n    | loss/policy_loss        | -0.00050599687 |\n    | loss/value_loss         | 0.06880974     |\n    | misc/explained_variance | 0.000675       |\n    | misc/nupdates           | 1              |\n    | misc/serial_timesteps   | 2048           |\n    | misc/time_elapsed       | 9.6            |\n    | misc/total_timesteps    | 2048           |\n    --------------------------------------------\n    ```", "```py\n    obs = env.reset()\n    if not isinstance(env, VecEnv):\n        obs = np.expand_dims(np.array(obs), axis=0)\n    episode_rew = 0\n    while True:\n        actions, _, state, _ = model.step(obs)\n        obs, reward, done, info = env.step(actions.numpy())\n        if not isinstance(env, VecEnv):\n            obs = np.expand_dims(np.array(obs), axis=0)\n        env.render()\n        print(\"Reward = \", reward)\n        episode_rew += reward\n        if done:\n            print('Episode Reward = {}'.format(episode_rew))\n            break\n    env.close()\n    ```", "```py\n    [...]\n    Reward =  [0.]\n    Reward =  [0.]\n    Reward =  [0.]\n    Reward =  [0.]\n    Reward =  [0.]\n    Reward =  [0.]\n    Reward =  [0.]\n    Reward =  [0.]\n    Reward =  [0.]\n    Reward =  [-1.]\n    Episode Reward = [-17.]\n    ```", "```py\n    !python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v0 \n    --num_timesteps=2e7 --save_path=./models/Pong_20M_ppo2 \n    --log_path=./logs/Pong/\n    ```", "```py\n    Stepping environment...\n    -------------------------------------------\n    | eplenmean               | 867           |\n    | eprewmean               | -20.8         |\n    | fps                     | 500           |\n    | loss/approxkl           | 4.795634e-05  |\n    | loss/clipfrac           | 0.0           |\n    | loss/policy_entropy     | 1.7456135     |\n    | loss/policy_loss        | -0.0005875508 |\n    | loss/value_loss         | 0.050125826   |\n    | misc/explained_variance | 0.145         |\n    | misc/nupdates           | 19            |\n    | misc/serial_timesteps   | 2432          |\n    | misc/time_elapsed       | 22            |\n    | misc/total_timesteps    | 9728          |\n    -------------------------------------------\n    ```", "```py\n    !python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v0\n        --num_timesteps=0 --load_path=./models/Pong_20M_ppo2 --play\n    ```", "```py\n    episode_rew=-21.0\n    episode_rew=-20.0\n    episode_rew=-20.0\n    episode_rew=-19.0\n    ```", "```py\n    !wget -O pong_20M_ppo2.tar.gz \\\n    https://github.com/PacktWorkshops\\\n    /The-Reinforcement-Learning-Workshop/blob/master\\\n    /Chapter04/pong_20M_ppo2.tar.gz?raw=true\n    ```", "```py\n    Saving to: 'pong_20M_ppo2.tar.gz'\n    pong_20M_ppo2.tar.g 100%[===================>]  17,44M  15,\n    1MB/s    in 1,2s   \n    2020-05-11 16:19:11 (15,1 MB/s) - 'pong_20M_ppo2.tar.gz' saved [18284569/18284569]\n    ```", "```py\n    !tar xvzf pong_20M_ppo2.tar.gz\n    ```", "```py\n    pong_20M_ppo2/ckpt-1.data-00000-of-00001\n    pong_20M_ppo2/ckpt-1.index\n    pong_20M_ppo2/\n    pong_20M_ppo2/checkpoint\n    ```", "```py\n    !python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v0 --num_timesteps=0 --load_path=./pong_20M_ppo2 –play\n    ```", "```py\n    import numpy as np\n    import gym\n    ```", "```py\n    def initialize_environment():\n        \"\"\"initialize the OpenAI Gym environment\"\"\"\n        env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n        print(\"Initializing environment\")\n        # reset the current environment\n        env.reset()\n        # show the size of the action space\n        action_size = env.action_space.n\n        print(f\"Action space: {action_size}\")\n        # Number of possible states\n        state_size = env.observation_space.n\n        print(f\"State space: {state_size}\")\n        return env\n    ```", "```py\n    def policy_evaluation(V, current_policy, env, \\\n                          gamma, small_change):\n        \"\"\"\n        Perform policy evaluation iterations until the smallest \n        change is less than\n        'smallest_change'\n        Args:\n            V: the value function table\n            current_policy: current policy\n            env: the OpenAI FrozenLake-v0 environment\n            gamma: future reward coefficient\n            small_change: how small should the change be for the \n              iterations to stop\n        Returns:\n            V: the value function after convergence of the evaluation\n        \"\"\"\n        state_size = env.observation_space.n\n        while True:\n            biggest_change = 0\n            # loop through every state present\n            for state in range(state_size):\n                old_V = V[state]\n    ```", "```py\n                action = current_policy[state]\n                prob, new_state, reward, done = env.env.P[state]\\\n                                                [action][0]\n    ```", "```py\n                V[state] = reward + gamma * V[new_state]\n                # if the biggest change is small enough then it means\n                # the policy has converged, so stop.\n                biggest_change = max(biggest_change, abs(V[state] \\\n                                     - old_V))\n            if biggest_change < small_change:\n                break\n        return V\n    ```", "```py\n    def policy_improvement(V, current_policy, env, gamma):\n        \"\"\"\n        Perform policy improvement using the Bellman Optimality Equation.\n        Args:\n            V: the value function table\n            current_policy: current policy\n            env: the OpenAI FrozenLake-v0 environment\n            gamma: future reward coefficient\n        Returns:\n            current_policy: the updated policy\n            policy_changed: True, if the policy was changed, else, \n              False\n        \"\"\"\n        state_size = env.observation_space.n\n        action_size = env.action_space.n\n        policy_changed = False\n        for state in range(state_size):\n            best_val = -np.inf\n            best_action = -1\n            # loop over all actions and select the best one\n            for action in range(action_size):\n                prob, new_state, reward, done = env.env.\\\n                                                P[state][action][0]\n    ```", "```py\n                future_reward = reward + gamma * V[new_state]\n                if future_reward > best_val:\n                    best_val = future_reward\n                    best_action = action\n    ```", "```py\n            assert best_action != -1\n            if current_policy[state] != best_action:\n                policy_changed = True\n    ```", "```py\n            current_policy[state] = best_action\n        # if the policy didn't change, it means we have converged\n        return current_policy, policy_changed\n    ```", "```py\n    def policy_iteration(env):\n        \"\"\"\n        Find the most optimal policy for the FrozenLake-v0 \n        environment using Policy\n        Iteration\n        Args:\n            env: FrozenLake-v0 environment\n        Returns:\n            policy: the most optimal policy\n        \"\"\"\n        V = dict()\n        \"\"\"\n        initially the value function for all states\n        will be random values close to zero\n        \"\"\"\n        state_size = env.observation_space.n\n        for i in range(state_size):\n            V[i] = np.random.random()\n        # when the change is smaller than this, stop\n        small_change = 1e-20\n        # future reward coefficient\n        gamma = 0.9\n        episodes = 0\n        # train for these many episodes\n        max_episodes = 50000\n        # initially we will start with a random policy\n        current_policy = dict()\n        for s in range(state_size):\n            current_policy[s] = env.action_space.sample()\n        while episodes < max_episodes:\n            episodes += 1\n            # policy evaluation\n            V = policy_evaluation(V, current_policy,\\\n                                  env, gamma, small_change)\n            # policy improvement\n            current_policy, policy_changed = policy_improvement\\\n                                             (V, current_policy, \\\n                                              env, gamma)\n            # if the policy didn't change, it means we have converged\n            if not policy_changed:\n                break\n        print(f\"Number of episodes trained: {episodes}\")\n        return current_policy\n    ```", "```py\n    def play(policy, render=False):\n        \"\"\"\n        Perform a test pass on the FrozenLake-v0 environment\n        Args:\n            policy: the policy to use\n            render: if the result should be rendered at every step. \n              False by default\n        \"\"\"\n        env = initialize_environment()\n        rewards = []\n    ```", "```py\n        max_steps = 25\n        test_episodes = 50\n        for episode in range(test_episodes):\n            # reset the environment every new episode\n            state = env.reset()\n            total_rewards = 0\n            print(\"*\" * 100)\n            print(\"Episode {}\".format(episode))\n            for step in range(max_steps):\n    ```", "```py\n                action = policy[state]\n                new_state, reward, done, info = env.step(action)\n                if render:\n                    env.render()\n                total_rewards += reward\n                if done:\n                    rewards.append(total_rewards)\n                    print(\"Score\", total_rewards)\n                    break\n                state = new_state\n        env.close()\n        print(\"Average Score\", sum(rewards) / test_episodes)\n    ```", "```py\n    def random_step(n_steps=5):\n        \"\"\"\n        Steps through the FrozenLake-v0 environment randomly\n        Args:\n            n_steps: Number of steps to step through\n        \"\"\"\n        # reset the environment\n        env = initialize_environment()\n        state = env.reset()\n        for i in range(n_steps):\n            # choose an action at random\n            action = env.action_space.sample()\n            env.render()\n            new_state, reward, done, info = env.step(action)\n            print(f\"New State: {new_state}\\n\"\\\n                  f\"reward: {reward}\\n\"\\\n                  f\"done: {done}\\n\"\\\n                  f\"info: {info}\\n\")\n            print(\"*\" * 20)\n    ```", "```py\n    def value_iteration(env):\n        \"\"\"\n        Performs Value Iteration to find the most optimal policy for the\n        FrozenLake-v0 environment\n        Args:\n            env: FrozenLake-v0 Gym environment\n        Returns:\n            policy: the most optimum policy\n        \"\"\"\n        V = dict()\n        gamma = 0.9\n        state_size = env.observation_space.n\n        action_size = env.action_space.n\n        policy = dict()\n    ```", "```py\n        for x in range(state_size):\n            V[x] = -1\n            policy[x] = env.action_space.sample()\n        \"\"\"\n        this loop repeats until the change in value function\n        is less than delta\n        \"\"\"\n        while True:\n            delta = 0\n            for state in reversed(range(state_size)):\n                old_v_s = V[state]\n                best_rewards = -np.inf\n                best_action = None\n                # for all the actions in current state\n                for action in range(action_size):\n    ```", "```py\n                   prob, new_state, reward, done = env.env.P[state]\\\n                                                   [action][0]\n                   potential_reward = reward + gamma * V[new_state]\n                   \"\"\"\n                   select the one that has the best reward\n                   and also save the action to the policy\n                   \"\"\"\n                if potential_reward > best_rewards:\n                    best_rewards = potential_reward\n                    best_action = action\n                policy[state] = best_action\n                V[state] = best_rewards\n                # terminate if the change is not high\n                delta = max(delta, abs(V[state] - old_v_s))\n            if delta < 1e-30:\n                break\n        print(policy)\n        print(V)\n        return policy\n    ```", "```py\n    if __name__ == '__main__':\n        env = initialize_environment()\n        # policy = policy_iteration(env)\n        policy = value_iteration(env)\n        play(policy, render=True)\n    ```", "```py\n    import gym\n    import numpy as np\n    from collections import defaultdict\n    ```", "```py\n    env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n    env.reset()\n    env.render()\n    ```", "```py\n    print(env.observation_space)\n    print(env.action_space)\n    name_action = {0:'Left',1:'Down',2:'Right',3:'Up'}\n    ```", "```py\n    Discrete(16)\n    Discrete(4)\n    ```", "```py\n    def generate_frozenlake_episode():\n        episode = []\n        state = env.reset()\n        step = 0;\n    ```", "```py\n        while (True):\n            action = env.action_space.sample()\n            next_state, reward, done, info = env.step(action)\n            episode.append((next_state, action, reward))\n            if done:\n                break\n            state = next_state\n            step += 1\n        return episode, reward\n    ```", "```py\n    def frozen_lake_prediction(batch):\n        for batch_number in range(batch+1):\n            total_reward = 0\n    ```", "```py\n            for i_episode in range(100):\n                episode, reward = generate_frozenlake_episode()\n                total_reward += reward\n    ```", "```py\n            success_percent = total_reward/100\n            print(\"Episode\", batch_number*100, \\\n                  \"Policy Win Rate=>\", float(success_percent*100), \\\n                  \"%\")\n    ```", "```py\n    frozen_lake_prediction(100)\n    ```", "```py\n    import gym\n    import numpy as np\n    ```", "```py\n    #Setting up the Frozen Lake environment\n    env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n    ```", "```py\n    #Initializing the Q and num_state_action\n    Q = np.zeros([env.observation_space.n, env.action_space.n])\n    num_state_action = np.zeros([env.observation_space.n, \\\n                                 env.action_space.n])\n    ```", "```py\n    num_episodes = 100000\n    epsilon = 0.30\n    rewardsList = []\n    ```", "```py\n    for x in range(num_episodes):\n        state = env.reset()\n        done = False\n        results_list = []\n        result_sum = 0.0\n    ```", "```py\n        while not done:\n\n            #random action less than epsilon\n            if np.random.rand() < epsilon:\n                #we go with the random action\n                action = env.action_space.sample()\n            else:\n                \"\"\"\n                1 - epsilon probability, we go with the greedy algorithm\n                \"\"\"\n                action = np.argmax(Q[state, :])\n    ```", "```py\n            #action is performed and assigned to new_state, reward\n            new_state, reward, done, info = env.step(action)\n    ```", "```py\n            results_list.append((state, action))\n            result_sum += reward\n    ```", "```py\n            #new state is assigned as state\n            state = new_state\n        #appending the results sum to the rewards list\n        rewardsList.append(result_sum)\n    ```", "```py\n        for (state, action) in results_list:\n            num_state_action[state, action] += 1.0\n            sa_factor = 1.0 / num_state_action[state, action]\n            Q[state, action] += sa_factor * \\\n                                (result_sum - Q[state, action])\n    ```", "```py\n        if x % 1000 == 0 and x is not 0:\n            print('Frozen Lake Success rate=>', \\\n                  str(sum(rewardsList) * 100 / x ), '%')\n    ```", "```py\n    print(\"Frozen Lake Success rate=>\", \\\n          str(sum(rewardsList)/num_episodes * 100), \"%\")\n    ```", "```py\n    import numpy as np\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    import gym\n    ```", "```py\n    env = gym.make('FrozenLake-v0', is_slippery=True)\n    ```", "```py\n    print(\"Action space = \", env.action_space)\n    print(\"Observation space = \", env.observation_space)\n    ```", "```py\n    Action space =  Discrete(4)\n    Observation space =  Discrete(16)\n    ```", "```py\n    actionsDict = {}\n    actionsDict[0] = \" L \"\n    actionsDict[1] = \" D \"\n    actionsDict[2] = \" R \"\n    actionsDict[3] = \" U \"\n    actionsDictInv = {}\n    actionsDictInv[\"L\"] = 0\n    actionsDictInv[\"D\"] = 1\n    actionsDictInv[\"R\"] = 2\n    actionsDictInv[\"U\"] = 3\n    ```", "```py\n    env.reset()\n    env.render()\n    ```", "```py\n    optimalPolicy = [\"  *  \",\"  U  \",\"L/R/D\",\"  U  \",\\\n                     \"  L  \",\"  -  \",\" L/R \",\"  -  \",\\\n                     \"  U  \",\"  D  \",\"  L  \",\"  -  \",\\\n                     \"  -  \",\"  R  \",\"R/D/U\",\"  !  \",]\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Optimal policy:  \n      L/R/D  U    U    U\n        L    -   L/R   -\n        U    D    L    -\n        -    R    D    !\n    ```", "```py\n    def action_epsilon_greedy(q, s, epsilon=0.05):\n        if np.random.rand() > epsilon:\n            return np.argmax(q[s])\n        return np.random.randint(4)\n    ```", "```py\n    def greedy_policy(q, s):\n        return np.argmax(q[s])\n    ```", "```py\n    def average_performance(policy_fct, q):\n        acc_returns = 0.\n        n = 500\n        for i in range(n):\n            done = False\n            s = env.reset()\n            while not done:\n                a = policy_fct(q, s)\n                s, reward, done, info = env.step(a)\n                acc_returns += reward\n        return acc_returns/n\n    ```", "```py\n    q = np.ones((16, 4))\n    # Set q(terminal,*) equal to 0\n    q[5,:] = 0.0\n    q[7,:] = 0.0\n    q[11,:] = 0.0\n    q[12,:] = 0.0\n    q[15,:] = 0.0\n    ```", "```py\n    nb_episodes = 80000\n    STEPS = 2000\n    alpha = 0.01\n    gamma = 0.99\n    epsilon_expl = 0.2\n    q_performance = np.ndarray(nb_episodes//STEPS)\n    ```", "```py\n    for i in range(nb_episodes):\n    ```", "```py\n        done = False\n        s = env.reset()\n        while not done:\n    ```", "```py\n            # behavior policy\n            a = action_epsilon_greedy(q, s, epsilon=epsilon_expl)\n    ```", "```py\n            new_s, reward, done, info = env.step(a)\n    ```", "```py\n            a_max = np.argmax(q[new_s]) # estimation policy\n    ```", "```py\n            q[s, a] = q[s, a] + alpha * \\\n                      (reward + gamma * q[new_s, a_max] - q[s, a])\n    ```", "```py\n            s = new_s\n    ```", "```py\n        if i%STEPS == 0:\n            q_performance[i//STEPS] = average_performance\\\n                                      (greedy_policy, q)\n    ```", "```py\n    plt.plot(STEPS * np.arange(nb_episodes//STEPS), q_performance)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Average reward of an epoch\")\n    plt.title(\"Learning progress for Q-Learning\")\n    ```", "```py\n    Text(0.5, 1.0, 'Learning progress for Q-Learning')\n    ```", "```py\n    greedyPolicyAvgPerf = average_performance(greedy_policy, q=q)\n    print(\"Greedy policy Q-learning performance =\", \\\n          greedyPolicyAvgPerf)\n    ```", "```py\n    Greedy policy Q-learning performance = 0.708\n    ```", "```py\n    q = np.round(q,3)\n    print(\"(A,S) Value function =\", q.shape)\n    print(\"First row\")\n    print(q[0:4,:])\n    print(\"Second row\")\n    print(q[4:8,:])\n    print(\"Third row\")\n    print(q[8:12,:])\n    print(\"Fourth row\")\n    print(q[12:16,:])\n    ```", "```py\n    (A,S) Value function = (16, 4)\n    First row\n    [[0.543 0.521 0.516 0.515]\n     [0.319 0.355 0.322 0.493]\n     [0.432 0.431 0.425 0.461]\n     [0.32  0.298 0.296 0.447]]\n    Second row\n    [[0.559 0.392 0.396 0.393]\n     [0\\.    0\\.    0\\.    0\\.   ]\n     [0.296 0.224 0.327 0.145]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Third row\n    [[0.337 0.366 0.42  0.595]\n     [0.484 0.639 0.433 0.415]\n     [0.599 0.511 0.342 0.336]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    Fourth row\n    [[0\\.    0\\.    0\\.    0\\.   ]\n     [0.46  0.53  0.749 0.525]\n     [0.711 0.865 0.802 0.799]\n     [0\\.    0\\.    0\\.    0\\.   ]]\n    ```", "```py\n    policyFound = [actionsDict[np.argmax(q[0,:])],\\\n                   actionsDict[np.argmax(q[1,:])],\\\n                   actionsDict[np.argmax(q[2,:])],\\\n                   actionsDict[np.argmax(q[3,:])],\\\n                   actionsDict[np.argmax(q[4,:])],\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[6,:])],\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[8,:])],\\\n                   actionsDict[np.argmax(q[9,:])],\\\n                   actionsDict[np.argmax(q[10,:])],\\\n                   \" - \",\\\n                   \" - \",\\\n                   actionsDict[np.argmax(q[13,:])],\\\n                   actionsDict[np.argmax(q[14,:])],\\\n                   \" ! \"]\n    print(\"Greedy policy found:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(policyFound[idx+0], policyFound[idx+1], \\\n              policyFound[idx+2], policyFound[idx+3])\n    print(\" \")\n    print(\"Optimal policy:\")\n    idxs = [0,4,8,12]\n    for idx in idxs:\n        print(optimalPolicy[idx+0], optimalPolicy[idx+1], \\\n              optimalPolicy[idx+2], optimalPolicy[idx+3])\n    ```", "```py\n    Greedy policy found:\n        L    U    U    U\n        L    -    R    -\n        U    D    L    -\n        -    R    D    !\n    Optimal policy:  \n      L/R/D  U    U    U\n        L    -   L/R   -\n        U    D    L    -\n        -    R    D    !\n    ```", "```py\n    import numpy as np\n    from utils import QueueBandit\n    ```", "```py\n    N_CLASSES = 3\n    queue_bandit = QueueBandit(filename='data.csv')\n    ```", "```py\n    class GreedyQueue:\n        def __init__(self, n_classes=3):\n            self.n_classes = n_classes\n            self.time_history = [[] for _ in range(n_classes)]\n\n        def decide(self, queue_lengths):\n            for class_ in range(self.n_classes):\n                if queue_lengths[class_] > 0 and \\\n                   len(self.time_history[class_]) == 0:\n                    return class_\n            mean_times = [np.mean(self.time_history[class_])\\\n                          if queue_lengths[class_] > 0 else np.inf\\\n                          for class_ in range(self.n_classes)]\n            return int(np.random.choice\\\n                      (np.argwhere\\\n                      (mean_times == np.min(mean_times)).flatten()))\n        def update(self, class_, time):\n            self.time_history[class_].append(time)\n    ```", "```py\n    cumulative_times = queue_bandit.repeat\\\n                       (GreedyQueue, [N_CLASSES], \\\n                        visualize_cumulative_times=True)\n    np.max(cumulative_times), np.mean(cumulative_times)\n    ```", "```py\n    (1218887.7924350922, 45155.236786598274)\n    ```", "```py\n    class ETCQueue:\n        def __init__(self, n_classes=3, T=3):\n            self.n_classes = n_classes\n            self.T = T\n            self.time_history = [[] for _ in range(n_classes)]\n        def decide(self, queue_lengths):\n            for class_ in range(self.n_classes):\n                if queue_lengths[class_] > 0 and \\\n                len(self.time_history[class_]) < self.T:\n                    return class_\n            mean_times = [np.mean(self.time_history[class_])\\\n                          if queue_lengths[class_] > 0 else np.inf\\\n                          for class_ in range(self.n_classes)]\n            return int(np.random.choice\\\n                      (np.argwhere(mean_times == np.min(mean_times))\\\n                      .flatten()))\n        def update(self, class_, time):\n            self.time_history[class_].append(time)\n    ```", "```py\n    cumulative_times = queue_bandit.repeat\\\n                       (ETCQueue, [N_CLASSES, 2],\\\n                        visualize_cumulative_times=True)\n    np.max(cumulative_times), np.mean(cumulative_times)\n    ```", "```py\n    class ExpThSQueue:\n        def __init__(self, n_classes=3):\n            self.n_classes = n_classes\n            self.time_history = [[] for _ in range(n_classes)]\n            self.temp_beliefs = [(0, 0) for _ in range(n_classes)]\n\n        def decide(self, queue_lengths):\n            for class_ in range(self.n_classes):\n                if queue_lengths[class_] > 0 and \\\n                len(self.time_history[class_]) == 0:\n                    return class_\n\n            rate_draws = [np.random.gamma\\\n                          (self.temp_beliefs[class_][0],1 \\\n                           / self.temp_beliefs[class_][1])\\\n                         if queue_lengths[class_] > 0 else -np.inf\\\n                         for class_ in range(self.n_classes)]\n            return int(np.random.choice\\\n                      (np.argwhere(rate_draws == np.max(rate_draws))\\\n                      .flatten()))\n        def update(self, class_, time):\n            self.time_history[class_].append(time)\n\n            # Update parameters according to Bayes rule\n            alpha, beta = self.temp_beliefs[class_]\n            alpha += 1\n            beta += time\n            self.temp_beliefs[class_] = alpha, beta\n    ```", "```py\n    cumulative_times = queue_bandit.repeat\\\n                       (ExpThSQueue, [N_CLASSES], \\\n                        visualize_cumulative_times=True)\n    np.max(cumulative_times), np.mean(cumulative_times)\n    ```", "```py\n    class ExploitingThSQueue:\n        def __init__(self, n_classes=3, r=1):\n            self.n_classes = n_classes\n            self.time_history = [[] for _ in range(n_classes)]\n            self.temp_beliefs = [(0, 0) for _ in range(n_classes)]\n            self.t = 0\n            self.r = r\n\n        def decide(self, queue_lengths):\n            for class_ in range(self.n_classes):\n                if queue_lengths[class_] > 0 and \\\n                len(self.time_history[class_]) == 0:\n                    return class_\n            if self.t > self.r * np.sum(queue_lengths):\n                mean_times = [np.mean(self.time_history[class_])\\\n                              if queue_lengths[class_] > 0 \\\n                              else np.inf\\\n                              for class_ in range(self.n_classes)]\n                return int(np.random.choice\\\n                          (np.argwhere\\\n                          (mean_times == np.min(mean_times))\\\n                          .flatten()))\n            rate_draws = [np.random.gamma\\\n                          (self.temp_beliefs[class_][0],\\\n                           1 / self.temp_beliefs[class_][1])\\\n                          if queue_lengths[class_] > 0 else -np.inf\\\n                          for class_ in range(self.n_classes)]\n            return int(np.random.choice\\\n                      (np.argwhere\\\n                      (rate_draws == np.max(rate_draws)).flatten()))\n    ```", "```py\n        def update(self, class_, time):\n            self.time_history[class_].append(time)\n            self.t += 1\n\n            # Update parameters according to Bayes rule\n            alpha, beta = self.temp_beliefs[class_]\n            alpha += 1\n            beta += time\n            self.temp_beliefs[class_] = alpha, beta\n    ```", "```py\n    cumulative_times = queue_bandit.repeat\\\n                       (ExploitingThSQueue, [N_CLASSES, 1], \\\n                        visualize_cumulative_times=True)\n    np.max(cumulative_times), np.mean(cumulative_times)\n    ```", "```py\n    import gym\n    import matplotlib.pyplot as plt\n    import torch\n    import torch.nn as nn\n    from torch import optim\n    import numpy as np\n    import random\n    import math\n    ```", "```py\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n    print(device)\n    ```", "```py\n    env = gym.make('CartPole-v0')\n    ```", "```py\n    seed = 100\n    env.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n    ```", "```py\n    number_of_states = env.observation_space.shape[0]\n    number_of_actions = env.action_space.n\n    print('Total number of States : {}'.format(number_of_states))\n    print('Total number of Actions : {}'.format(number_of_actions))\n    ```", "```py\n    Total number of States : 4\n    Total number of Actions : 2\n    ```", "```py\n    NUMBER_OF_EPISODES = 500\n    MAX_STEPS = 1000\n    LEARNING_RATE = 0.01\n    DISCOUNT_FACTOR = 0.99\n    HIDDEN_LAYER_SIZE = 64\n    EGREEDY = 0.9\n    EGREEDY_FINAL = 0.02\n    EGREEDY_DECAY = 500\n    REPLAY_BUFFER_SIZE = 6000\n    BATCH_SIZE = 32\n    UPDATE_TARGET_FREQUENCY = 200\n    ```", "```py\n    def calculate_epsilon(steps_done):\n        \"\"\"\n        Decays epsilon with increasing steps\n        Parameter:\n        steps_done (int) : number of steps completed\n        Returns:\n        int - decayed epsilon\n        \"\"\"\n        epsilon = EGREEDY_FINAL + (EGREEDY - EGREEDY_FINAL) \\\n                  * math.exp(-1\\. * steps_done / EGREEDY_DECAY )\n        return epsilon\n    ```", "```py\n    class DQN(nn.Module):\n        def __init__(self , hidden_layer_size):\n            super().__init__()\n            self.hidden_layer_size = hidden_layer_size\n            self.fc1 = nn.Linear(number_of_states,\\\n                                 self.hidden_layer_size)\n            self.fc2 = nn.Linear(self.hidden_layer_size,\\\n                                 number_of_actions)\n        def forward(self, x):\n            output = torch.tanh(self.fc1(x))\n            output = self.fc2(output)\n            return output\n    ```", "```py\n    class ExperienceReplay(object):\n        def __init__(self , capacity):\n            self.capacity = capacity\n            self.buffer = []\n            self.pointer = 0\n        def push(self , state, action, new_state, reward, done):\n            experience = (state, action, new_state, reward, done)\n            if self.pointer >= len(self.buffer):\n                self.buffer.append(experience)\n            else:\n                self.buffer[self.pointer] = experience\n            self.pointer = (self.pointer + 1) % self.capacity\n        def sample(self , batch_size):\n            return zip(*random.sample(self.buffer , batch_size))\n        def __len__(self):\n            return len(self.buffer)\n    ```", "```py\n    memory = ExperienceReplay(REPLAY_BUFFER_SIZE)\n    ```", "```py\n    class DQN_Agent(object):\n        def __init__(self):\n            self.dqn = DQN(HIDDEN_LAYER_SIZE).to(device)\n            self.target_dqn = DQN(HIDDEN_LAYER_SIZE).to(device)\n            self.criterion = torch.nn.MSELoss()\n            self.optimizer = optim.Adam\\\n                             (params=self.dqn.parameters(), \\\n                              lr=LEARNING_RATE)\n            self.target_dqn_update_counter = 0\n        def select_action(self,state,EGREEDY):\n            random_for_egreedy = torch.rand(1)[0]\n            if random_for_egreedy > EGREEDY:\n                with torch.no_grad():\n                    state = torch.Tensor(state).to(device)\n                    q_values = self.dqn(state)\n                    action = torch.max(q_values,0)[1]\n                    action = action.item()\n            else:\n                action = env.action_space.sample()\n            return action\n        def optimize(self):\n            if (BATCH_SIZE > len(memory)):\n                return\n            state, action, new_state, reward, done = memory.sample\\\n                                                     (BATCH_SIZE)\n            state = torch.Tensor(state).to(device)\n            new_state = torch.Tensor(new_state).to(device)\n            reward = torch.Tensor(reward).to(device)\n            action = torch.LongTensor(action).to(device)\n            done = torch.Tensor(done).to(device)\n            \"\"\"\n            select action : get the index associated with max q \n            value from prediction network\n            \"\"\"\n            new_state_indxs = self.dqn(new_state).detach() \n            # to get the max new state indexes\n            max_new_state_indxs = torch.max(new_state_indxs, 1)[1]\n            \"\"\"\n            Using the best action from the prediction nn get \n            the max new state value in target dqn\n            \"\"\"\n            new_state_values = self.target_dqn(new_state).detach()\n            max_new_state_values = new_state_values.gather\\\n                                   (1, max_new_state_indxs\\\n                                    .unsqueeze(1))\\\n                                   .squeeze(1)\n            #when done = 1 then target = reward\n            target_value = reward + (1 - done) * DISCOUNT_FACTOR \\\n                           * max_new_state_values\n            predicted_value = self.dqn(state).gather\\\n                              (1, action.unsqueeze(1))\\\n                              .squeeze(1)\n            loss = self.criterion(predicted_value, target_value)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            if self.target_dqn_update_counter \\\n            % UPDATE_TARGET_FREQUENCY == 0:\n                self.target_dqn.load_state_dict(self.dqn.state_dict())\n            self.target_dqn_update_counter += 1\n    ```", "```py\n    dqn_agent = DQN_Agent()\n    steps_total = []\n    steps_counter = 0\n    ```", "```py\n    for episode in range(NUMBER_OF_EPISODES):\n        state = env.reset()\n        done = False\n        step = 0\n        for i in range(MAX_STEPS):\n            step += 1\n            steps_counter += 1\n            EGREEDY = calculate_epsilon(steps_counter)\n            action = dqn_agent.select_action(state, EGREEDY)\n            new_state, reward, done, info = env.step(action)\n            memory.push(state, action, new_state, reward, done)\n            dqn_agent.optimize()\n            state = new_state\n            if done:\n                steps_total.append(step)\n                break\n    ```", "```py\n    print(\"Average reward: %.2f\" \\\n          % (sum(steps_total)/NUMBER_OF_EPISODES))\n    print(\"Average reward (last 100 episodes): %.2f\" \\\n          % (sum(steps_total[-100:])/100))\n    ```", "```py\n    Average reward: 174.09\n    Average reward (last 100 episodes): 186.06\n    ```", "```py\n    Plt.figure(figsize=(12,5))\n    plt.title(\"Rewards Collected\")\n    plt.xlabel('Steps')\n    plt.ylabel('Reward')\n    plt.bar(np.arange(len(steps_total)), steps_total, \\\n            alpha=0.5, color='green', width=6)\n    plt.show()\n    ```", "```py\nAverage reward: 158.83\nAverage reward (last 100 episodes): 176.28\n```", "```py\nAverage reward: 154.41\nAverage reward (last 100 episodes): 183.28\n```", "```py\nAverage reward: 174.09\nAverage reward (last 100 episodes): 186.06\n```", "```py\n    import gym\n    import random\n    import numpy as np\n    from collections import deque\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Conv2D, \\\n    MaxPooling2D, Flatten\n    from tensorflow.keras.optimizers import RMSprop\n    import datetime\n    ```", "```py\n    np.random.seed(168)\n    tf.random.set_seed(168)\n    ```", "```py\n    Activity10_01.ipynb\n    class DQN():\n        def __init__(self, env, batch_size=64, max_experiences=5000):\n            self.env = env\n            self.input_size = self.env.observation_space.shape[0]\n            self.action_size = self.env.action_space.n\n            self.max_experiences = max_experiences\n            self.memory = deque(maxlen=self.max_experiences)\n            self.batch_size = batch_size\n            self.gamma = 1.0\n            self.epsilon = 1.0\n            self.epsilon_min = 0.01\n            self.epsilon_decay = 0.995\n            self.model = self.build_model()\n            self.target_model = self.build_model()\n\n        def build_model(self):\n            model = Sequential()\n            model.add(Conv2D(32, 8, (4,4), activation='relu', \\\n                             padding='valid',\\\n                             input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n            model.add(Conv2D(64, 4, (2,2), activation='relu', \\\n                             padding='valid'))\n            model.add(Conv2D(64, 3, (1,1), activation='relu', \\\n                             padding='valid'))\n            model.add(Flatten())\n            model.add(Dense(256, activation='relu'))\n            model.add(Dense(self.action_size))\n            model.compile(loss='mse', \\\n                          optimizer=RMSprop(lr=0.00025, \\\n                          epsilon=self.epsilon_min), \\\n                          metrics=['accuracy'])\n            return model\n    The complete code for this step can be found at https://packt.live/3hoZXdV.\n    ```", "```py\n    def initialize_env(env):\n        initial_state = env.reset()\n        initial_done_flag = False\n        initial_rewards = 0\n        return initial_state, initial_done_flag, initial_rewards\n    ```", "```py\n    def preprocess_state(image, img_size):\n        img_temp = image[31:195]\n        img_temp = tf.image.rgb_to_grayscale(img_temp)\n        img_temp = tf.image.resize\\\n                   (img_temp, [img_size, img_size],\\\n                    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        img_temp = tf.cast(img_temp, tf.float32)\n        return img_temp\n    ```", "```py\n    def play_game(agent, state, done, rewards):\n        while not done:\n            action = agent.get_action(state)\n            next_state, reward, done, _ = env.step(action)\n            next_state = preprocess_state(next_state, IMG_SIZE)\n            agent.add_experience(state, action, reward, \\\n                                 next_state, done)\n            state = next_state\n            rewards += reward\n        return rewards\n    ```", "```py\n    def train_agent(env, episodes, agent):\n      from collections import deque\n      import numpy as np\n      scores = deque(maxlen=100)\n      for episode in range(episodes):\n        state, done, rewards = initialize_env(env)\n        state = preprocess_state(state, IMG_SIZE)\n        rewards = play_game(agent, state, done, rewards)\n        scores.append(rewards)\n        mean_score = np.mean(scores)\n        if episode % 50 == 0:\n            print(f'[Episode {episode}] \\\n    - Average Score: {mean_score}')\n            agent.target_model.set_weights(agent.model.get_weights())\n            agent.target_model.save_weights\\\n            (f'dqn/dqn_model_weights_{episode}')\n        agent.replay(episode)\n      print(f\"Average Score: {np.mean(scores)}\")\n    ```", "```py\n    env = gym.make('BreakoutDeterministic-v4')\n    ```", "```py\n    IMG_SIZE = 84\n    SEQUENCE = 4\n    ```", "```py\n    agent = DQN(env)\n    ```", "```py\n    episodes = 50\n    ```", "```py\n    train_agent(env, episodes, agent)\n    ```", "```py\n    [Episode 0] - Average Score: 3.0\n    Average Score: 0.59\n    ```", "```py\n    import gym\n    import random\n    import numpy as np\n    from collections import deque\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Conv2D, \\\n    MaxPooling2D, TimeDistributed, Flatten, LSTM\n    from tensorflow.keras.optimizers import RMSprop\n    import datetime\n    ```", "```py\n    np.random.seed(168)\n    tf.random.set_seed(168)\n    ```", "```py\n    Activity10_02.ipynb\n    class DRQN():\n        def __init__(self, env, batch_size=64, max_experiences=5000):\n            self.env = env\n            self.input_size = self.env.observation_space.shape[0]\n            self.action_size = self.env.action_space.n\n            self.max_experiences = max_experiences\n            self.memory = deque(maxlen=self.max_experiences)\n            self.batch_size = batch_size\n            self.gamma = 1.0\n            self.epsilon = 1.0\n            self.epsilon_min = 0.01\n            self.epsilon_decay = 0.995\n\n            self.model = self.build_model()\n            self.target_model = self.build_model()\n\n        def build_model(self):\n            model = Sequential()\n            model.add(TimeDistributed(Conv2D(32, 8, (4,4), \\\n                                      activation='relu', \\\n                                      padding='valid'), \\\n                      input_shape=(SEQUENCE, IMG_SIZE, IMG_SIZE, 1)))\n            model.add(TimeDistributed(Conv2D(64, 4, (2,2), \\\n                                      activation='relu', \\\n                                      padding='valid')))\n            model.add(TimeDistributed(Conv2D(64, 3, (1,1), \\\n                                      activation='relu', \\\n                                      padding='valid')))\n            model.add(TimeDistributed(Flatten()))\n            model.add(LSTM(512))\n            model.add(Dense(128, activation='relu'))\n            model.add(Dense(self.action_size))\n            model.compile(loss='mse', \\\n                          optimizer=RMSprop(lr=0.00025, \\\n                                            epsilon=self.epsilon_min), \\\n                          metrics=['accuracy'])\n            return model\n    The complete code for this step can be found at https://packt.live/2AjdgMx .\n    ```", "```py\n    def initialize_env(env):\n      initial_state = env.reset()\n      initial_done_flag = False\n      initial_rewards = 0\n      return initial_state, initial_done_flag, initial_rewards\n    ```", "```py\n    def preprocess_state(image, img_size):\n        img_temp = image[31:195]\n        img_temp = tf.image.rgb_to_grayscale(img_temp)\n        img_temp = tf.image.resize\\\n                   (img_temp, [img_size, img_size], \\\n                    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        img_temp = tf.cast(img_temp, tf.float32)\n        return img_temp\n    ```", "```py\n    def combine_images(new_img, prev_img, img_size, seq=4):\n        if len(prev_img.shape) == 4 and prev_img.shape[0] == seq:\n            im = np.concatenate\\\n                 ((prev_img[1:, :, :], \\\n                   tf.reshape(new_img, [1, img_size, img_size, 1])), \\\n                   axis=0)\n        else:\n            im = np.stack([new_img] * seq, axis=0)\n        return im\n    ```", "```py\n    def play_game(agent, state, done, rewards):\n        while not done:\n            action = agent.get_action(state)\n            next_state, reward, done, _ = env.step(action)\n            next_state = preprocess_state(next_state, IMG_SIZE)\n            next_state = combine_images\\\n                         (new_img=next_state, prev_img=state, \\\n                          img_size=IMG_SIZE, seq=SEQUENCE)\n            agent.add_experience(state, action, \\\n                                 reward, next_state, done)\n            state = next_state\n            rewards += reward \n        return rewards\n    ```", "```py\n    def train_agent(env, episodes, agent):\n      from collections import deque\n      import numpy as np\n      scores = deque(maxlen=100)\n      for episode in range(episodes):\n        state, done, rewards = initialize_env(env)\n        state = preprocess_state(state, IMG_SIZE)\n        state = combine_images(new_img=state, prev_img=state, \\\n                               img_size=IMG_SIZE, seq=SEQUENCE)\n        rewards = play_game(agent, state, done, rewards)\n        scores.append(rewards)\n        mean_score = np.mean(scores)\n        if episode % 50 == 0:\n            print(f'[Episode {episode}] - Average Score: {mean_score}')\n            agent.target_model.set_weights\\\n            (agent.model.get_weights())\n            agent.target_model.save_weights\\\n            (f'drqn_model_weights_{episode}')\n        agent.replay(episode)\n      print(f\"Average Score: {np.mean(scores)}\")\n    ```", "```py\n    env = gym.make('BreakoutDeterministic-v4')\n    ```", "```py\n    IMG_SIZE = 84\n    SEQUENCE = 4\n    ```", "```py\n    agent = DRQN(env)\n    ```", "```py\n    episodes = 200\n    ```", "```py\n    train_agent(env, episodes, agent)\n    ```", "```py\n    [Episode 0] - Average Score: 0.0\n    [Episode 50] - Average Score: 0.43137254901960786\n    [Episode 100] - Average Score: 0.4\n    [Episode 150] - Average Score: 0.54\n    Average Score: 0.53\n    ```", "```py\n    import gym\n    import random\n    import numpy as np\n    from collections import deque\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Dense, Conv2D, \\\n    MaxPooling2D, TimeDistributed, Flatten, GRU, Attention\n    from tensorflow.keras.optimizers import RMSprop\n    import datetime\n    ```", "```py\n    np.random.seed(168)\n    tf.random.set_seed(168)\n    ```", "```py\n    Activity10_03.ipynb\n    class DARQN():\n        def __init__(self, env, batch_size=64, max_experiences=5000):\n            self.env = env\n            self.input_size = self.env.observation_space.shape[0]\n            self.action_size = self.env.action_space.n\n            self.max_experiences = max_experiences\n            self.memory = deque(maxlen=self.max_experiences)\n            self.batch_size = batch_size\n            self.gamma = 1.0\n            self.epsilon = 1.0\n            self.epsilon_min = 0.01\n            self.epsilon_decay = 0.995\n            self.model = self.build_model()\n            self.target_model = self.build_model()\n        def build_model(self):\n            inputs = Input(shape=(SEQUENCE, IMG_SIZE, IMG_SIZE, 1))\n            conv1 = TimeDistributed(Conv2D(32, 8, (4,4), \\\n                                    activation='relu', \\\n                                    padding='valid'))(inputs)\n            conv2 = TimeDistributed(Conv2D(64, 4, (2,2), \\\n                                    activation='relu', \\\n                                    padding='valid'))(conv1)\n            conv3 = TimeDistributed(Conv2D(64, 3, (1,1), \\\n                                    activation='relu', \\\n                                    padding='valid'))(conv2)\n            flatten = TimeDistributed(Flatten())(conv3)\n            out, states = GRU(512, return_sequences=True, \\\n                              return_state=True)(flatten)\n            att = Attention()([out, states])\n            output_1 = Dense(256, activation='relu')(att)\n            predictions = Dense(self.action_size)(output_1)\n            model = Model(inputs=inputs, outputs=predictions)\n            model.compile(loss='mse', \\\n                          optimizer=RMSprop(lr=0.00025, \\\n                                            epsilon=self.epsilon_min), \\\n                          metrics=['accuracy'])\n            return model\n    The complete code for this step can be found at https://packt.live/2XUDZrH.\n    ```", "```py\n    def initialize_env(env):\n      initial_state = env.reset()\n      initial_done_flag = False\n      initial_rewards = 0\n      return initial_state, initial_done_flag, initial_rewards\n    ```", "```py\n    def preprocess_state(image, img_size):\n        img_temp = image[31:195]\n        img_temp = tf.image.rgb_to_grayscale(img_temp)\n        img_temp = tf.image.resize\\\n                   (img_temp, [img_size, img_size],\\\n                   method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        img_temp = tf.cast(img_temp, tf.float32)\n        return img_temp\n    ```", "```py\n    def combine_images(new_img, prev_img, img_size, seq=4):\n        if len(prev_img.shape) == 4 and prev_img.shape[0] == seq:\n            im = np.concatenate((prev_img[1:, :, :], \\\n                                 tf.reshape\\\n                                 (new_img, [1, img_size, \\\n                                            img_size, 1])), axis=0)\n        else:\n            im = np.stack([new_img] * seq, axis=0)\n        return im\n    ```", "```py\n    def play_game(agent, state, done, rewards):\n        while not done:\n            action = agent.get_action(state)\n            next_state, reward, done, _ = env.step(action)\n            next_state = preprocess_state(next_state, IMG_SIZE)\n            next_state = combine_images\\\n                         (new_img=next_state, prev_img=state, \\\n                          img_size=IMG_SIZE, seq=SEQUENCE)\n            agent.add_experience(state, action, reward, \\\n                                 next_state, done)\n             state = next_state\n           rewards += reward\n        return rewards\n    ```", "```py\n    def train_agent(env, episodes, agent):\n      from collections import deque\n      import numpy as np\n      scores = deque(maxlen=100)\n      for episode in range(episodes):\n        state, done, rewards = initialize_env(env)\n        state = preprocess_state(state, IMG_SIZE)\n        state = combine_images\\\n                (new_img=state, prev_img=state, \\\n                 img_size=IMG_SIZE, seq=SEQUENCE)\n        rewards = play_game(agent, state, done, rewards)\n        scores.append(rewards)\n        mean_score = np.mean(scores)\n        if episode % 50 == 0:\n            print(f'[Episode {episode}] - Average Score: {mean_score}')\n            agent.target_model.set_weights\\\n            (agent.model.get_weights())\n            agent.target_model.save_weights\\\n            (f'drqn_model_weights_{episode}')\n        agent.replay(episode)\n      print(f\"Average Score: {np.mean(scores)}\")\n    ```", "```py\n    env = gym.make('BreakoutDeterministic-v4')\n    ```", "```py\n    IMG_SIZE = 84\n    SEQUENCE = 4\n    ```", "```py\n    agent = DRQN(env)\n    ```", "```py\n    episodes = 400\n    ```", "```py\n    train_agent(env, episodes, agent)\n    ```", "```py\n    [Episode 0] - Average Score: 1.0\n    [Episode 50] - Average Score: 2.4901960784313726\n    [Episode 100] - Average Score: 3.92\n    [Episode 150] - Average Score: 7.37\n    [Episode 200] - Average Score: 7.76\n    [Episode 250] - Average Score: 7.91\n    [Episode 300] - Average Score: 10.33\n    [Episode 350] - Average Score: 10.94\n    Average Score: 10.83\n    ```", "```py\n    import os\n    import gym\n    from ddpg import *\n    ```", "```py\n    env = gym.make(\"LunarLanderContinuous-v2\")\n    ```", "```py\n    agent = Agent(alpha=0.000025, beta=0.00025, \\\n                  inp_dimensions=[8], tau=0.001,\\\n                  env=env, bs=64, l1_size=400, l2_size=300, \\\n                  nb_actions=2)\n    ```", "```py\n    np.random.seed(0)\n    ```", "```py\n    history = []\n    for i in np.arange(1000):\n        observation = env.reset()\n        score = 0\n        done = False\n        while not done:\n    ```", "```py\n    history = []\n    for i in np.arange(1000):\n        observation = env.reset()\n        score = 0\n        done = False\n        while not done:\n            action = agent.select_action(observation)\n            state_new, reward, done, info = env.step(action)\n            agent.remember(observation, action, reward, \\\n                           state_new, int(done))\n            agent.learn()\n            score += reward\n            observation = state_new\n            # env.render() # Uncomment to see the game window\n        history.append(score)\n    ```", "```py\n    import os\n    import gym\n    import torch as T\n    import numpy as np\n    from PIL import Image\n    ```", "```py\n    device = T.device(\"cuda:0\" if T.cuda.is_available() else \"cpu\")\n    ```", "```py\n    class ReplayBuffer:\n        def __init__(self):\n            self.memory_actions = []\n            self.memory_states = []\n            self.memory_log_probs = []\n            self.memory_rewards = []\n            self.is_terminals = []\n        def clear_memory(self):\n            del self.memory_actions[:]\n            del self.memory_states[:]\n            del self.memory_log_probs[:]\n            del self.memory_rewards[:]\n            del self.is_terminals[:]\n    ```", "```py\n    Activity11_02.ipynb\n    class ActorCritic(T.nn.Module):\n        def __init__(self, state_dimension, action_dimension, \\\n                     nb_latent_variables):\n            super(ActorCritic, self).__init__()\n            self.action_layer = T.nn.Sequential\\\n                                (T.nn.Linear(state_dimension, \\\n                                             nb_latent_variables),\\\n                                T.nn.Tanh(),\\\n                                T.nn.Linear(nb_latent_variables, \\\n                                            nb_latent_variables),\\\n                                T.nn.Tanh(),\\\n                                T.nn.Linear(nb_latent_variables, \\\n                                            action_dimension),\\\n                                T.nn.Softmax(dim=-1))\n    The complete code for this step can be found at https://packt.live/2YhzrvD.\n    ```", "```py\n    Activity11_02.ipynb\n    class Agent:\n        def __init__(self, state_dimension, action_dimension, \\\n        nb_latent_variables, lr, betas, gamma, K_epochs, eps_clip):\\\n            self.lr = lr\n            self.betas = betas\n            self.gamma = gamma\n            self.eps_clip = eps_clip\n            self.K_epochs = K_epochs\n\n            self.policy = ActorCritic(state_dimension,\\\n                                      action_dimension,\\\n                                      nb_latent_variables).to(device)\n            self.optimizer = T.optim.Adam\\\n                             (self.policy.parameters(), \\\n                              lr=lr, betas=betas)\n            self.policy_old = ActorCritic(state_dimension,\\\n                                          action_dimension,\\\n                                          nb_latent_variables)\\\n                                          .to(device)\n            self.policy_old.load_state_dict(self.policy.state_dict())\n    The complete code for this step can be found at https://packt.live/2YhzrvD.\n    ```", "```py\n    env = gym.make(„LunarLander-v2\")\n    np.random.seed(0)\n    render = True\n    ```", "```py\n    memory = ReplayBuffer()\n    agent = Agent(state_dimension=env.observation_space.shape[0],\\\n                  action_dimension=4, nb_latent_variables=64,\\\n                  lr=0.002, betas=(0.9, 0.999), gamma=0.99,\\\n                  K_epochs=4,eps_clip=0.2)\n    ```", "```py\n    agent.policy_old.load_state_dict\\\n    (T.load(\"../Exercise11.03/PPO_LunarLander-v2.pth\"))\n    ```", "```py\n    for ep in range(5):\n        ep_reward = 0\n        state = env.reset()\n        for t in range(300):\n            action = agent.policy_old.act(state, memory)\n            state, reward, done, _ = env.step(action)\n            ep_reward += reward\n            if render:\n                env.render()\n                img = env.render(mode = „rgb_array\")\n                img = Image.fromarray(img)\n                image_dir = \"./gif\"\n                if not os.path.exists(image_dir):\n                    os.makedirs(image_dir)\n                img.save(os.path.join(image_dir, \"{}.jpg\".format(t)))\n            if done:\n                break\n        print(\"Episode: {}, Reward: {}\".format(ep, int(ep_reward)))\n        ep_reward = 0\n        env.close()\n    ```", "```py\n    Episode: 0, Reward: 272\n    Episode: 1, Reward: 148\n    Episode: 2, Reward: 249\n    Episode: 3, Reward: 169\n    Episode: 4, Reward: 35\n    ```", "```py\n    import gym \n    import numpy as np \n    import math \n    import tensorflow as tf\n    from matplotlib import pyplot as plt\n    from random import randint\n    from statistics import median, mean\n    ```", "```py\n    env = gym.make('CartPole-v0')\n    no_states = env.observation_space.shape[0]\n    no_actions = env.action_space.n\n    ```", "```py\n    def initial(run_test):\n        #initialize arrays\n        i_w = []\n        i_b = []\n        h_w = []\n        o_w = []\n        no_input_nodes = 8\n        no_hidden_nodes = 4\n\n        for r in range(run_test):\n            input_weight = np.random.rand(no_states, no_input_nodes)\n            input_bias = np.random.rand((no_input_nodes))\n            hidden_weight = np.random.rand(no_input_nodes,\\\n                                           no_hidden_nodes)\n            output_weight = np.random.rand(no_hidden_nodes, \\\n                                           no_actions)\n            i_w.append(input_weight)\n            i_b.append(input_bias)\n            h_w.append(hidden_weight)\n            o_w.append(output_weight)\n        chromosome =[i_w, i_b, h_w, o_w]\n        return chromosome\n    ```", "```py\n    def nnmodel(observations, i_w, i_b, h_w, o_w):\n        alpha = 0.199\n        observations = observations/max\\\n                       (np.max(np.linalg.norm(observations)),1)\n        #apply relu on layers\n        funct1 = np.dot(observations, i_w)+ i_b.T\n        layer1= tf.nn.relu(funct1)-alpha*tf.nn.relu(-funct1)\n        funct2 = np.dot(layer1,h_w)\n        layer2 = tf.nn.relu(funct2) - alpha*tf.nn.relu(-funct2)\n        funct3 = np.dot(layer2, o_w)\n        layer3 = tf.nn.relu(funct3)-alpha*tf.nn.relu(-funct3)\n        #apply softmax\n        layer3 = np.exp(layer3)/np.sum(np.exp(layer3))\n        output = layer3.argsort().reshape(1,no_actions)\n        action = output[0][0]\n        return action\n    ```", "```py\n    def get_reward(env, i_w, i_b, h_w, o_w):\n        current_state = env.reset()\n        total_reward = 0\n        for step in range(300):\n            action = nnmodel(current_state, i_w, i_b, h_w, o_w)\n            next_state, reward, done, info = env.step(action)\n            total_reward += reward\n            current_state = next_state\n            if done:\n                break\n        return total_reward\n    ```", "```py\n    def get_weights(env, run_test):\n        rewards = []\n        chromosomes = initial(run_test)\n        for trial in range(run_test):\n            i_w = chromosomes[0][trial]\n            i_b = chromosomes[1][trial]\n            h_w = chromosomes[2][trial]\n            o_w = chromosomes[3][trial]\n            total_reward = get_reward(env, i_w, i_b, h_w, o_w)\n            rewards = np.append(rewards, total_reward)\n        chromosome_weight = [chromosomes, rewards]\n        return chromosome_weight\n    ```", "```py\n    def mutate(parent):\n        index = np.random.randint(0, len(parent))\n        if(0 < index < 10):\n            for idx in range(index):\n                n = np.random.randint(0, len(parent))\n                parent[n] = parent[n] + np.random.rand()\n        mutation = parent\n        return mutation\n    ```", "```py\n    def crossover(list_chr):\n        gen_list = []\n        gen_list.append(list_chr[0])\n        gen_list.append(list_chr[1])\n        for i in range(10):\n            m = np.random.randint(0, len(list_chr[0]))\n            parent = np.append(list_chr[0][:m], list_chr[1][m:])\n            child = mutate(parent)\n            gen_list.append(child)\n        return gen_list\n    ```", "```py\n    def generate_new_population(rewards, chromosomes):\n        #2 best reward indexes selected\n        best_reward_idx = rewards.argsort()[-2:][::-1]\n        list_chr = []\n        new_i_w =[]\n        new_i_b = []\n        new_h_w = []\n        new_o_w = []\n        new_rewards = []\n    ```", "```py\n        for ind in best_reward_idx:\n            weight1 = chromosomes[0][ind]\n            w1 = weight1.reshape(weight1.shape[1], -1)\n            bias1 = chromosomes[1][ind]\n            b1 = np.append(w1, bias1)\n            weight2 = chromosomes[2][ind]\n            w2 = np.append\\\n                 (b1, weight2.reshape(weight2.shape[1], -1))\n            weight3 = chromosomes[3][ind]\n            chr = np.append(w2, weight3)\n            #the 2 best parents are selected\n            list_chr.append(chr)\n        gen_list = crossover(list_chr)\n    ```", "```py\n        for l in gen_list:\n            chromosome_w1 = np.array(l[:chromosomes[0][0].size])\n            new_input_weight = np.reshape(chromosome_w1,(-1,chromosomes[0][0].shape[1]))\n            new_input_bias = np.array\\\n                             ([l[chromosome_w1.size:chromosome_w1\\\n                               .size+chromosomes[1][0].size]]).T\n            hidden = chromosome_w1.size + new_input_bias.size\n            chromosome_w2 = np.array\\\n                            ([l[hidden:hidden \\\n                             + chromosomes[2][0].size]])\n            new_hidden_weight = np.reshape\\\n                                (chromosome_w2, \\\n                                (-1, chromosomes[2][0].shape[1]))\n            final = chromosome_w1.size+new_input_bias.size\\\n                    +chromosome_w2.size\n            new_output_weight = np.array([l[final:]]).T\n            new_output_weight = np.reshape\\\n                                (new_output_weight,\\\n                                (-1, chromosomes[3][0].shape[1]))\n            new_i_w.append(new_input_weight)\n            new_i_b.append(new_input_bias)\n            new_h_w.append(new_hidden_weight)\n            new_o_w.append(new_output_weight)\n            new_reward = get_reward(env, new_input_weight, \\\n                                    new_input_bias, new_hidden_weight, \\\n                                    new_output_weight)\n            new_rewards = np.append(new_rewards, new_reward)\n        generation = [new_i_w, new_i_b, new_h_w, new_o_w]\n        return generation, new_rewards\n    ```", "```py\n    def graphics(act):\n        plt.plot(act)\n        plt.xlabel('No. of generations')\n        plt.ylabel('Rewards')\n        plt.grid()\n        print('Mean rewards:', mean(act))\n        return plt.show()\n    ```", "```py\n    def ga_algo(env, run_test, no_gen):\n        weights = get_weights(env, run_test)\n        chrom = weights[0]\n        current_rewards = weights[1]\n        act = []\n        for n in range(no_gen):\n            gen, new_rewards = generate_new_population\\\n                               (current_rewards, chrom)\n            average = np.average(current_rewards)\n            new_average = np.average(new_rewards)\n            if average >  new_average:\n                parameters = [chrom[0][0], chrom[1][0], \\\n                              chrom[2][0], chrom[3][0]]\n            else:\n                 parameters = [gen[0][0], gen[1][0], \\\n                               gen[2][0], gen[3][0]]\n            chrom = gen\n            current_rewards = new_rewards\n            max_arg = np.amax(current_rewards)\n            print('Generation:{}, max reward:{}'.format(n+1, max_arg))\n            act = np.append(act, max_arg)\n        graphics(act)\n        return parameters\n    ```", "```py\n    def params(parameters):\n        i_w = parameters[0]\n        i_b = parameters[1]\n        h_w = parameters[2]\n        o_w = parameters[3]\n        return i_w,i_b,h_w,o_w\n    ```", "```py\n    generations = []\n    no_gen = 50\n    run_test = 15\n    trial_length = 500\n    no_trials = 500\n    rewards = []\n    final_reward = 0\n    parameters = ga_algo(env, run_test, no_gen)\n    i_w, i_b, h_w, o_w = params(parameters)\n    for trial in range(no_trials):\n        current_state = env.reset()\n        total_reward = 0\n        for step in range(trial_length):\n            env.render()\n            action = nnmodel(current_state, i_w,i_b, h_w, o_w)\n            next_state,reward, done, info = env.step(action)\n            total_reward += reward\n            current_state = next_state\n            if done:\n                break\n        print('Trial:{}, total reward:{}'.format(trial, total_reward))\n        final_reward +=total_reward\n    print('Average reward:', final_reward/no_trials)\n    env.close()\n    ```", "```py\n    Generation:1, max reward:11.0\n    Generation:2, max reward:11.0\n    Generation:3, max reward:10.0\n    Generation:4, max reward:10.0\n    Generation:5, max reward:11.0\n    Generation:6, max reward:10.0\n    Generation:7, max reward:10.0\n    Generation:8, max reward:10.0\n    Generation:9, max reward:11.0\n    Generation:10, max reward:10.0\n    Generation:11, max reward:10.0\n    Generation:12, max reward:10.0\n    Generation:13, max reward:10.0\n    Generation:14, max reward:10.0\n    Generation:15, max reward:10.0\n    Generation:16, max reward:10.0\n    Generation:17, max reward:10.0\n    Generation:18, max reward:10.0\n    Generation:19, max reward:11.0\n    Generation:20, max reward:11.0\n    ```", "```py\nTrial:486, total reward:8.0\nTrial:487, total reward:9.0\nTrial:488, total reward:10.0\nTrial:489, total reward:10.0\nTrial:490, total reward:8.0\nTrial:491, total reward:9.0\nTrial:492, total reward:9.0\nTrial:493, total reward:10.0\nTrial:494, total reward:10.0\nTrial:495, total reward:9.0\nTrial:496, total reward:10.0\nTrial:497, total reward:9.0\nTrial:498, total reward:10.0\nTrial:499, total reward:9.0\nAverage reward: 9.384\n```"]