- en: 4\. An Introduction to Decision Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 决策树简介
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter introduces you to two types of supervised learning algorithms in
    detail. The first algorithm will help you classify data points using decision
    trees, while the other algorithm will help you classify data points using random
    forests. Furthermore, you'll learn how to calculate the precision, recall, and
    F1 score of models, both manually and automatically. By the end of this chapter,
    you will be able to analyze the metrics that are used for evaluating the utility
    of a data model and classify data points based on decision trees and random forest
    algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将详细介绍两种类型的监督学习算法。第一个算法将帮助你使用决策树对数据点进行分类，而另一个算法则帮助你使用随机森林对数据点进行分类。此外，你还将学习如何手动和自动计算模型的精确度、召回率和F1分数。到本章结束时，你将能够分析用于评估数据模型效用的指标，并基于决策树和随机森林算法对数据点进行分类。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the previous two chapters, we learned the difference between regression
    and classification problems, and we saw how to train some of the most famous algorithms.
    In this chapter, we will look at another type of algorithm: tree-based models.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们学习了回归问题与分类问题的区别，并且我们看到了如何训练一些最著名的算法。本章中，我们将介绍另一种算法类型：基于树的模型。
- en: Tree-based models are very popular as they can model complex non-linear patterns
    and they are relatively easy to interpret. In this chapter, we will introduce
    you to decision trees and the random forest algorithms, which are some of the
    most widely used tree-based models in the industry.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的模型非常流行，因为它们可以建模复杂的非线性模式，并且相对容易理解。本章中，我们将介绍决策树和随机森林算法，这些是行业中最广泛使用的基于树的模型之一。
- en: Decision Trees
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: A decision tree has leaves, branches, and nodes. Nodes are where a decision
    is made. A decision tree consists of rules that we use to formulate a decision
    (or prediction) on the prediction of a data point.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有叶子节点、分支和节点。节点是做出决策的地方。决策树由我们用来制定决策（或预测）数据点的规则组成。
- en: Every node of the decision tree represents a feature, while every edge coming
    out of an internal node represents a possible value or a possible interval of
    values of the tree. Each leaf of the tree represents a label value of the tree.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的每个节点代表一个特征，而从内部节点出来的每条边代表树的一个可能值或值区间。树的每个叶子节点代表树的一个标签值。
- en: This may sound complicated, but let's look at an application of this.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能听起来有些复杂，但让我们来看一个应用实例。
- en: 'Suppose we have a dataset with the following features and the response variable
    is determining whether a person is creditworthy or not:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数据集，具有以下特征，并且响应变量是判断一个人是否有信用：
- en: '![Figure 4.1: Sample dataset to formulate the rules'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1：用于制定规则的示例数据集'
- en: '](img/B16060_04_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_01.jpg)'
- en: 'Figure 4.1: Sample dataset to formulate the rules'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：用于制定规则的示例数据集
- en: 'A decision tree, remember, is just a group of rules. Looking at the dataset
    in *Figure 4.1*, we can come up with the following rules:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，决策树只是一些规则的集合。查看*图 4.1*中的数据集，我们可以得出以下规则：
- en: All people with house loans are determined as creditworthy.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有有房贷的人都被确定为有信用的人。
- en: If debtors are employed and studying, then loans are creditworthy.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果债务人有工作并且在学习，那么贷款是有信用的。
- en: People with income above 75,000 a year are creditworthy.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年收入超过75,000的人是有信用的。
- en: At or below 75,000 a year, people with car loans and who are employed are creditworthy.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年收入在75,000以下的、有车贷并且有工作的人员是有信用的。
- en: 'Following the order of the rules we just defined, we can build a tree, as shown
    in *Figure 4.2* and describe one possible credit scoring method:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 按照我们刚才定义的规则顺序，我们可以建立一棵树，如*图 4.2*所示，并描述一种可能的信用评分方法：
- en: '![Figure 4.2: Decision tree for the loan type'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2：贷款类型的决策树'
- en: '](img/B16060_04_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_02.jpg)'
- en: 'Figure 4.2: Decision tree for the loan type'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：贷款类型的决策树
- en: First, we determine the loan type. House loans are automatically creditworthy
    according to the first rule. Study loans are described by the second rule, resulting
    in a subtree containing another decision on employment. Since we have covered
    both house and study loans, there are only car loans left. The third rule describes
    an income decision, while the fourth rule describes a decision on employment.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we must score a new debtor to determine whether they are creditworthy,
    we have to go through the decision tree from top to bottom and observe the true
    or false value at the bottom.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, a model based on seven data points is highly inaccurate because we
    can't generalize rules that simply do not match reality. Therefore, rules are
    often determined based on large amounts of data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: This is not the only way that we can create a decision tree. We can build decision
    trees based on other sequences of rules, too. Let's extract some other rules from
    the dataset in *Figure 4.1*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation 1**: Notice that individual salaries that are greater than 75,000
    are all creditworthy.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '`Income > 75,000 => CreditWorthy` is true.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Rule 1 classifies four out of seven data points (IDs C, E, F, G); we need more
    rules for the remaining three data points.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation 2**: Out of the remaining three data points, two are not employed.
    One is employed (ID D) and is creditworthy. With this, we can claim the following
    rule:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '`Income <= 75,000`, the following holds true: `Employed == true => CreditWorthy`.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that with this second rule, we can also classify the remaining two data
    points (IDs A and B) as not creditworthy. With just two rules, we accurately classified
    all the observations from this dataset:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: Decision tree for income'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_03.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.3: Decision tree for income'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The second decision tree is less complex. At the same time, we cannot overlook
    the fact that the model says, *employed people with a lower income are less likely
    to pay back their loans*. Unfortunately, there is not enough training data available
    (there are only seven observations in this example), which makes it likely that
    we'll end up with false conclusions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is a frequent problem in decision trees when making a decision based
    on a few data points. This decision is rarely representative.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Since we can build decision trees in any possible order, it makes sense to define
    an efficient way of constructing a decision tree. Therefore, we will now explore
    a measure for ordering the features in the decision process.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Entropy
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In information theory, entropy measures how randomly distributed the possible
    values of an attribute are. The higher the degree of randomness is, the higher
    the entropy of the attribute.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is the highest possibility of an event. If we know beforehand what the
    outcome will be, then the event has no randomness. So, entropy is **zero**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: We use entropy to order the splitting of nodes in the decision tree. Taking
    the previous example, which rule should we start with? Should it be `Income <=
    75000` or `is employed`? We need to use a metric that can tell us that one specific
    split is better than the other. A good split can be defined by the fact it clearly
    split the data into two homogenous groups. One of these metrics is information
    gain, and it is based on entropy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用熵来排序决策树中节点的分裂。以之前的例子为例，我们应该从哪个规则开始？是`Income <= 75000`还是`is employed`？我们需要使用一个度量标准来告诉我们哪个特定的分裂比另一个更好。一个好的分裂可以通过它清楚地将数据分成两个同质的组来定义。一个这样的度量是信息增益，它基于熵。
- en: 'Here is the formula for calculating entropy:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是计算熵的公式：
- en: '![Figure 4.4: Entropy formula'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4：熵公式'
- en: '](img/B16060_04_04.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_04.jpg)'
- en: 'Figure 4.4: Entropy formula'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：熵公式
- en: '*p*i represents the probability of one of the possible values of the target
    variable occurring. So, if this column has *n* different unique values, then we
    will have the probability for each of them *([p*1*, p*2*, ..., p*n*])* and apply
    the formula.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*i表示目标变量某一可能值发生的概率。所以，如果这一列有*n*个不同的唯一值，那么我们将为每个值计算概率*([p*1*, p*2*, ..., p*n*])*并应用公式。'
- en: To manually calculate the entropy of a distribution in Python, we can use the
    `np.log2` and `np.dot()` methods from the NumPy library. There is no function
    in `numpy` to automatically calculate entropy.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中手动计算分布的熵，我们可以使用NumPy库中的`np.log2`和`np.dot()`方法。在`numpy`中没有自动计算熵的函数。
- en: 'Have a look at the following example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的例子：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The probabilities are given as a NumPy array or a regular list on *line 2*:
    *p*i.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 概率以NumPy数组或常规列表的形式给出，在*第2行*：*p*i。
- en: 'We need to create a vector of the negated values of the distribution in *line
    3*: - *p*i.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个包含*第3行*分布中取反值的向量：- *p*i。
- en: In *line 4*, we must take the base two logarithms of each value in the distribution
    list: logi pi.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4行*，我们必须对分布列表中的每个值取以2为底的对数：logi pi。
- en: 'Finally, we calculate the sum with the scalar product, also known as the dot
    product of the two vectors:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过标量积来计算总和，也称为两个向量的点积：
- en: '![Figure 4.5: Dot product of two vectors'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.5：两个向量的点积'
- en: '](img/B16060_04_05.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_05.jpg)'
- en: 'Figure 4.5: Dot product of two vectors'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：两个向量的点积
- en: Note
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You learned about the dot product for the first time in *Chapter 2*, *An Introduction
    to Regression*. The dot product of two vectors is calculated by multiplying the
    *i*th coordinate of the first vector by the *i*th coordinate of the second vector,
    for each *i*. Once we have all the products, we sum the values:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你第一次在*第二章*《回归分析导论》中学习了点积。两个向量的点积是通过将第一个向量的第*i*个坐标与第二个向量的第*i*个坐标相乘来计算的，针对每个*i*。一旦我们得到所有的乘积，就将它们求和：
- en: '*np.dot([1, 2, 3], [4, 5, 6])*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*np.dot([1, 2, 3], [4, 5, 6])*'
- en: This results in 1*4 + 2*5 + 3*6 = 32.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到1*4 + 2*5 + 3*6 = 32。
- en: In the next exercise, we will be calculating entropy on a small sample dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将计算一个小样本数据集的熵。
- en: 'Exercise 4.01: Calculating Entropy'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习4.01：计算熵
- en: 'In this exercise, we will calculate the entropy of the features in the dataset
    in *Figure 4.6*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将计算数据集中特征的熵，如*图4.6*所示：
- en: '![Figure 4.6: Sample dataset to formulate the rules'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.6：用于制定规则的样本数据集'
- en: '](img/B16060_04_06.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_06.jpg)'
- en: 'Figure 4.6: Sample dataset to formulate the rules'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：用于制定规则的样本数据集
- en: Note
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset file can also be found in our GitHub repository:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集文件也可以在我们的GitHub仓库中找到：
- en: '[https://packt.live/2AQ6Uo9](https://packt.live/2AQ6Uo9).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.live/2AQ6Uo9](https://packt.live/2AQ6Uo9)。'
- en: We will calculate entropy for the `Employed`, `Income`, `LoanType`, and `LoanAmount` features.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为`Employed`、`Income`、`LoanType`和`LoanAmount`特征计算熵。
- en: 'The following steps will help you complete this exercise:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成这个练习：
- en: Open a new Jupyter Notebook file.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook文件。
- en: 'Import the `numpy` package as `np`:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`包并命名为`np`：
- en: '[PRE1]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define a function called `entropy()` that receives an array of probabilities
    and then returns the calculated entropy value, as shown in the following code snippet:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个名为`entropy()`的函数，它接收一个概率数组并返回计算得到的熵值，如下代码片段所示：
- en: '[PRE2]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will calculate the entropy of the `Employed` column. This column contains
    only two possible values: `true` or `false`. The `true` value appeared four times
    out of seven rows, so its probability is `4/7`. Similarly, the probability of
    the `false` value is `3/7` as it appeared three times in this dataset.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将计算 `Employed` 列的熵。该列仅包含两个可能的值：`true` 或 `false`。在七行数据中，`true` 出现了四次，因此其概率为
    `4/7`。类似地，`false` 的概率为 `3/7`，因为它在数据集中出现了三次。
- en: 'Use the `entropy()` function to calculate the entropy of the `Employed` column
    with the probabilities `4/7` and `3/7`:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `entropy()` 函数计算 `Employed` 列的熵，概率分别为 `4/7` 和 `3/7`：
- en: '[PRE3]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You should get the following output:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE4]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This value is quite close to zero, which means the groups are quite homogenous.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个值接近零，意味着这些组是相当同质的。
- en: 'Now, use the `entropy()` function to calculate the entropy of the `Income`
    column with its corresponding list of probabilities:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用 `entropy()` 函数计算 `Income` 列的熵及其对应的概率列表：
- en: '[PRE5]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should get the following output:'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE6]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Compared to the `Employed` column, the entropy for `Income` is higher. This
    means the probabilities of this column are more spread.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与 `Employed` 列相比，`Income` 的熵较高。这意味着该列的概率分布更为分散。
- en: 'Use the `entropy` function to calculate the entropy of the `LoanType` column
    with its corresponding list of probabilities:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `entropy` 函数计算 `LoanType` 列的熵及其对应的概率列表：
- en: '[PRE7]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You should get the following output:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE8]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This value is higher than 0, so the probabilities for this column are quite
    spread.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个值大于 0，因此该列的概率分布较为分散。
- en: 'Let''s use the `entropy` function to calculate the entropy of the `LoanAmount`
    column with its corresponding list of probabilities:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 `entropy` 函数计算 `LoanAmount` 列的熵及其对应的概率列表：
- en: '[PRE9]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You should get the following output:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE10]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The entropy for `LoanAmount` is quite high, so its values are quite random.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`LoanAmount` 的熵较高，因此其值相当随机。'
- en: Note
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/37T8DVz](https://packt.live/37T8DVz).
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/37T8DVz](https://packt.live/37T8DVz)。
- en: You can also run this example online at [https://packt.live/2By7aI6](https://packt.live/2By7aI6).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址为 [https://packt.live/2By7aI6](https://packt.live/2By7aI6)。你必须执行整个
    Notebook 才能得到预期的结果。
- en: Here, you can see that the `Employed` column has the lowest entropy among the
    four different columns because it has the least variation in terms of values.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到 `Employed` 列的熵在四个不同的列中是最低的，因为它的值变化最小。
- en: By completing this exercise, you've learned how to manually calculate the entropy
    for each column of a dataset.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个练习后，你已经学会了如何手动计算数据集每一列的熵。
- en: Information Gain
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息增益
- en: When we partition the data points in a dataset according to the values of an
    attribute, we reduce the entropy of the system.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们根据某个属性的值对数据集中的数据点进行划分时，我们减少了系统的熵。
- en: 'To describe information gain, we can calculate the distribution of the labels.
    Initially, in *Figure 4.1*, we had five creditworthy and two not creditworthy
    individuals in our dataset. The entropy belonging to the initial distribution
    is as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述信息增益，我们可以计算标签的分布。在 *图 4.1* 中，我们的数据集包含五个可信和两个不可信的个体。初始分布的熵如下：
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s see what happens if we partition the dataset based on whether the loan
    amount is greater than 15,000 or not:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果根据贷款金额是否大于 15,000 来划分数据集，会发生什么：
- en: In group 1, we get one data point belonging to the 15,000 loan amount. This
    data point is not creditworthy.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组 1 中，我们得到一个属于 15,000 贷款金额的数据点。这个数据点是不可信的。
- en: In group 2, we have five creditworthy individuals and one non-creditworthy individual.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在组 2 中，我们有五个可信的个体和一个不可信的个体。
- en: The entropy of the labels in each group is as follows.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组中标签的熵如下：
- en: 'For group 1, we have the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于组 1，我们有如下情况：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For group 2, we have the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于组 2，我们有如下情况：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To calculate the information gain, let''s calculate the weighted average of
    the group entropies:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算信息增益，让我们计算组熵的加权平均值：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, to find the information gain, we need to calculate the difference between
    the original entropy (`H_label`) and the one we just calculated:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By splitting the data with this rule, we gain a little bit of information.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: When creating the decision tree, on each node, our job is to partition the dataset
    using a rule that maximizes the information gain.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: We could also use Gini Impurity instead of entropy-based information gain to
    construct the best rules for splitting decision trees.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Gini Impurity
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instead of entropy, there is another widely used metric that can be used to
    measure the randomness of a distribution: Gini Impurity.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Gini Impurity is defined as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7: Gini Impurity'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_07.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.7: Gini Impurity'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '*p*i here represents the probability of one of the possible values of the target
    variable occurring.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Entropy may be a bit slower to calculate because of the usage of the logarithm.
    Gini Impurity, on the other hand, is less precise when it comes to measuring randomness.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Some programmers prefer Gini Impurity because you don''t have to calculate
    with logarithms. Computation-wise, none of the solutions are particularly complex,
    and so both can be used. When it comes to performance, the following study concluded
    that there are often just minimal differences between the two metrics: [https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have learned that we can optimize a decision tree by splitting
    the data based on information gain or Gini Impurity. Unfortunately, these metrics
    are only available for discrete values. What if the label is defined on a continuous
    interval such as a price range or salary range?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: We have to use other metrics. You can technically understand the idea behind
    creating a decision tree based on a continuous label, which was about regression.
    One metric we can reuse in this chapter is the mean squared error. Instead of
    Gini Impurity or information gain, we have to minimize the mean squared error
    to optimize the decision tree. As this is a beginner's course, we will omit this
    metric.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss the exit condition for a decision tree.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Exit Condition
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can continuously split the data points according to more and more specific
    rules until each leaf of the decision tree has an entropy of zero. The question
    is whether this end state is desirable.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Often, this is not what we expect, because we risk overfitting the model. When
    our rules for the model are too specific and too nitpicky, and the sample size
    that the decision was made on is too small, we risk making a false conclusion,
    thus recognizing a pattern in the dataset that simply does not exist in real life.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if we spin a roulette wheel three times and we get 12, 25, and
    12, this concludes that every odd spin resulting in the value 12 is not a sensible
    strategy. By assuming that every odd spin equals 12, we find a rule that is exclusively
    due to random noise.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们转动轮盘三次，得到的结果分别是12、25和12，这就得出结论：每次奇数次转动结果为12的策略并不明智。假设每次奇数次转动结果都是12，我们就发现了一个完全由随机噪音引起的规则。
- en: Therefore, posing a restriction on the minimum size of the dataset that we can
    still split is an exit condition that works well in practice. For instance, if
    you stop splitting as soon as you have a dataset that's lower than 50, 100, 200,
    or 500 in size, you avoid drawing conclusions on random noise, and so you minimize
    the risk of overfitting the model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对我们仍然可以分割的数据集的最小大小施加限制是一个在实际中效果良好的退出条件。例如，如果在数据集小于50、100、200或500时就停止分割，就可以避免对随机噪音得出结论，从而最大限度地减少过拟合模型的风险。
- en: Another popular exit condition is the maximum restriction on the depth of the
    tree. Once we reach a fixed tree depth, we classify the data points in the leaves.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的退出条件是树的最大深度限制。一旦达到固定的树深度，我们就在叶子节点对数据点进行分类。
- en: Building Decision Tree Classifiers Using scikit-learn
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn构建决策树分类器
- en: We have already learned how to load data from a `.csv` file, how to apply preprocessing
    to data, and how to split data into training and testing datasets. If you need
    to refresh yourself on this knowledge, you can go back to the previous chapters,
    where you can go through this process in the context of regression and classification.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何从`.csv`文件加载数据，如何对数据进行预处理，以及如何将数据拆分为训练集和测试集。如果你需要复习这些知识，可以回到前面的章节，在回归和分类的背景下重新学习这一过程。
- en: 'Now, we will assume that a set of training features, training labels, testing
    features, and testing labels have been given as a return value of the `scikit-learn
    train-test-split` call:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们假设已经通过`scikit-learn train-test-split`调用返回了一组训练特征、训练标签、测试特征和测试标签：
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code snippet, we used `train_test_split` to split the dataset
    (features and labels) into training and testing sets. The testing set represents
    10% of the observation (`test_size=0.1`). The `random_state` parameter is used
    to get reproducible results.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了`train_test_split`将数据集（特征和标签）拆分为训练集和测试集。测试集占观测数据的10%（`test_size=0.1`）。`random_state`参数用于获取可重复的结果。
- en: We will not focus on how we got these data points because this process is exactly
    the same as in the case of regression and classification.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会专注于如何获得这些数据点，因为这个过程与回归和分类的情况完全相同。
- en: 'It''s time to import and use the decision tree classifier of scikit-learn:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候导入并使用scikit-learn的决策树分类器了：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We set one optional parameter in `DecisionTreeClassifier`, that is, `max_depth`,
    to limit the depth of the decision tree.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`DecisionTreeClassifier`中设置了一个可选参数，即`max_depth`，用于限制决策树的深度。
- en: Note
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read the official documentation for the full list of parameters: [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以阅读官方文档，获取参数的完整列表：[http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)。
- en: 'Some of the more important parameters are as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些更重要的参数：
- en: '`criterion`: Gini stands for Gini Impurity, while entropy stands for information
    gain. This will define which measure will be used to assess the quality of a split
    at each node.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`criterion`: Gini代表基尼不纯度，entropy代表信息增益。这将定义在每个节点上用于评估分割质量的度量标准。'
- en: '`max_depth`: This is the parameter that defines the maximum depth of the tree.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`: 这是定义树的最大深度的参数。'
- en: '`min_samples_split`: This is the minimum number of samples needed to split
    an internal node.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`: 这是分割内部节点所需的最小样本数。'
- en: You can also experiment with all the other parameters that were enumerated in
    the documentation. We will omit them in this section.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以尝试文档中列出的所有其他参数。我们将在本节中省略它们。
- en: 'Once the model has been built, we can use the decision tree classifier to predict
    data:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型构建完成，我们就可以使用决策树分类器进行数据预测：
- en: '[PRE23]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You will build a decision tree classifier in the activity at the end of this
    section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本节末尾的活动中构建一个决策树分类器。
- en: Performance Metrics for Classifiers
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类器的性能评估指标
- en: 'After splitting the training and testing data, the decision tree model has
    a `score` method to evaluate how well testing data is classified by the model
    (also known as the accuracy score). We learned how to use the `score` method in
    the previous two chapters:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在拆分训练数据和测试数据之后，决策树模型有一个 `score` 方法，用来评估模型对测试数据分类的效果（也称为准确率）。我们在前两章中学习了如何使用 `score`
    方法：
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The return value of the `score` method is a number that's less than or equal
    to 1\. The closer we get to 1, the better our model is.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`score` 方法的返回值是一个小于或等于 1 的数字。我们越接近 1，模型就越好。'
- en: Now, we will learn about another way to evaluate the model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将学习另一种评估模型的方法。
- en: Note
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Feel free to use this method on the models you constructed in the previous chapter
    as well.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以将此方法应用于你在上一章构建的模型。
- en: 'Suppose we have one test feature and one test label:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个测试特征和一个测试标签：
- en: '[PRE25]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let''s use the previous creditworthy example and assume we trained a decision
    tree and now have its predictions:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前的信用评分示例，假设我们训练了一个决策树，现在有了它的预测结果：
- en: '![Figure 4.8: Sample dataset to formulate the rules'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8：用于制定规则的示例数据集'
- en: '](img/B16060_04_08.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_08.jpg)'
- en: 'Figure 4.8: Sample dataset to formulate the rules'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：用于制定规则的示例数据集
- en: Our model, in general, made good predictions but had few errors. It incorrectly
    predicted the results for IDs `A`, `D`, and `E`. Its accuracy score will be 4
    / 7 = 0.57.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型一般来说做出了很好的预测，但也有少数错误。它错误地预测了 ID `A`、`D` 和 `E` 的结果。它的准确率得分将是 4 / 7 = 0.57。
- en: 'We will use the following definitions to define some metrics that will help
    you evaluate how good your classifier is:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下定义来定义一些度量标准，帮助你评估分类器的好坏：
- en: '`Creditworthy` column, in our example) and the corresponding predictions both
    have the value `Yes`. In our example, IDs `C`, `F`, and `G` will fall under this category.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Creditworthy` 列（在我们的示例中）和相应的预测值都是 `Yes`。在我们的示例中，ID `C`、`F` 和 `G` 将属于这一类别。'
- en: '`No`. Only ID `B` will be classified as true negative.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`No`。只有 ID `B` 会被分类为真正负类。'
- en: '`Yes` but the true label is actually `No`. This will be the case for ID `A`.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Yes`，但真实标签实际上是 `No`。这种情况适用于 ID `A`。'
- en: '`No` but the true label is actually `Yes`, such as for IDs `D` and `E`.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`No`，但真实标签实际上是 `Yes`，例如 ID `D` 和 `E`。'
- en: 'Using the preceding four definitions, we can define four metrics that describe
    how well our model predicts the target variable. The `#( X )` symbol denotes the
    number of values in `X`. Using technical terms, `#( X )` denotes the cardinality
    of `X`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面四个定义，我们可以定义四个度量标准，用来描述我们的模型如何预测目标变量。`#( X )` 符号表示 `X` 中的值的数量。从技术术语上讲，`#(
    X )` 表示 `X` 的基数：
- en: '**Definition (Accuracy)**: *#( True Positives ) + #( True Negatives ) / #(
    Dataset )*'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义（准确率）**：*#（真正例）+ #(真正负类) / #(数据集)*'
- en: Accuracy is a metric that's used for determining how many times the classifier
    gives us the correct answer. This is the first metric we used to evaluate the
    score of a classifier.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是一个用来衡量分类器给出正确答案次数的指标。这是我们用来评估分类器得分的第一个度量标准。
- en: In our previous example (*Figure 4.8*), the accuracy score will be TP + TN /
    total = (3 + 1) / 7 = 4/7.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的示例中（*图 4.8*），准确率得分将是 TP + TN / 总数 = (3 + 1) / 7 = 4/7。
- en: 'We can use the function provided by scikit-learn to calculate the accuracy
    of a model:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 scikit-learn 提供的函数来计算模型的准确率：
- en: '[PRE26]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Definition (Precision)**: *#TruePositives / (#TruePositives + #FalsePositives)*'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义（精度）**：*#真正例 / (#真正例 + #假正例)*'
- en: Precision centers around values that our classifier found to be positive. Some
    of these results are true positive, while others are false positive. High precision
    means that the number of false positive results is very low compared to the true
    positive results. This means that a precise classifier rarely makes a mistake
    when finding a positive result.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 精度关注的是分类器认为是正类的值。这些结果中，有些是真正例，而有些是假正例。高精度意味着假正例的数量相对于真正例非常低。这意味着一个精确的分类器在找到正类时很少犯错。
- en: '**Definition (Recall)**: *#True Positives / (#True Positives + #False Negatives)*'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义（召回率）**：*#真正例 / (#真正例 + #假负例)*'
- en: Recall centers around values that are positive among the test data. Some of
    these results are found by the classifier. These are the true positive values.
    Those positive values that are not found by the classifier are false negatives.
    A classifier with a high recall value finds most of the positive values.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率关注的是测试数据中正类值的情况。分类器找到的这些结果是正例（True Positive）。那些分类器未找到的正类值是假阴性（False Negative）。一个召回率高的分类器能够找到大多数正类值。
- en: 'Using our previous example (*Figure 4.8*), we will get the following measures:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前的示例（*图 4.8*），我们将得到以下度量：
- en: Precision = TP / (TP + FP) = 4 / (4 + 1) = 4/6 = 0.8
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度 = TP / (TP + FP) = 4 / (4 + 1) = 4/6 = 0.8
- en: Recall = TP / (TP + FN) = 4 / (4 + 2) = 4/6 = 0.6667
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 召回率 = TP / (TP + FN) = 4 / (4 + 2) = 4/6 = 0.6667
- en: With these two measures, we can easily see where our model is performing better
    or worse. In this example, we know it tends to misclassify false negative cases.
    These measures are more granular than the accuracy score, which only gives you
    an overall score.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这两个度量，我们可以轻松看到我们的模型在哪些地方表现更好或更差。在这个例子中，我们知道它倾向于误分类假阴性案例。这些度量比准确率分数更为细致，准确率分数仅提供一个整体分数。
- en: The F1 score is a metric that combines precision and recall scores. Its value
    ranges between 0 and 1\. If the F1 score equals 1, it means the model is perfectly
    predicting the right outcomes. On the other hand, an F1 score of 0 means the model
    cannot predict the target variable accurately. The advantage of the F1 score is
    that it considers both false positives and false negatives.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数是一个结合精确度和召回率的度量。它的值范围在 0 到 1 之间。如果 F1 分数为 1，则表示模型完美地预测了正确的结果。另一方面，F1 分数为
    0 则表示模型无法准确预测目标变量。F1 分数的优点是它考虑了假阳性和假阴性。
- en: 'The formula for calculating the F1 score is as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 F1 分数的公式如下：
- en: '![Figure 4.9: Formula to calculate the F1 score'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.9：计算 F1 分数的公式'
- en: '](img/B16060_04_09.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16060_04_09.jpg)'
- en: 'Figure 4.9: Formula to calculate the F1 score'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：计算 F1 分数的公式
- en: 'As a final note, the scikit-learn package also provides a handy function that
    can show all these measures in one go: `classification_report()`. A classification
    report is useful to check the quality of our predictions:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要指出的是，scikit-learn 包还提供了一个非常实用的函数，可以一次性显示所有这些度量：`classification_report()`。分类报告有助于检查我们预测的质量：
- en: '[PRE27]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the next exercise, we will be practicing how to calculate these scores manually.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将练习如何手动计算这些分数。
- en: 'Exercise 4.02: Precision, Recall, and F1 Score Calculation'
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.02：精确度、召回率和 F1 分数计算
- en: In this exercise, we will calculate the precision, recall value, and the F1
    score of two different classifiers on a simulated dataset.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将计算两个不同分类器在模拟数据集上的精确度、召回率和 F1 分数。
- en: 'The following steps will help you complete this exercise:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成此练习：
- en: Open a new Jupyter Notebook file.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook 文件。
- en: 'Import the `numpy` package as `np` using the following code:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码导入`numpy`包，并将其命名为`np`：
- en: '[PRE28]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create a `numpy` array called `real_labels` that contains the values [`True,
    True, False, True, True]`. This list will represent the true values of the target
    variable for our simulated dataset. Print its content:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`real_labels`的`numpy`数组，包含值[`True, True, False, True, True`]。该列表表示我们模拟数据集的目标变量的真实值。打印其内容：
- en: '[PRE29]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The expected output will be as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE30]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Create a `numpy` array called `model_1_preds` that contains the values `[True,
    False, False, False, False]`. This list will represent the predicted values of
    the first classifier. Print its content:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_1_preds`的`numpy`数组，包含值`[True, False, False, False, False]`。该列表表示第一个分类器的预测值。打印其内容：
- en: '[PRE31]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The expected output will be as follows:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE32]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create another `numpy` array called `model_2_preds` that contains the values
    `[True, True, True, True, True]`. This list will represent the predicted values
    of the first classifier. Print its content:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建另一个名为`model_2_preds`的`numpy`数组，包含值`[True, True, True, True, True]`。该列表表示第一个分类器的预测值。打印其内容：
- en: '[PRE33]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The expected output will be as follows:'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE34]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create a variable called `model_1_tp_cond` that will find the true positives
    for the first model:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_1_tp_cond`的变量，用来找到第一个模型的真正例：
- en: '[PRE35]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The expected output will be as follows:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE36]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create a variable called `model_1_tp` that will get the number of true positives
    for the first model by summing `model_1_tp_cond`:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`model_1_tp`的变量，通过求和`model_1_tp_cond`来获取第一个模型的真正例数量：
- en: '[PRE37]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The expected output will be as follows:'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下：
- en: '[PRE38]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: There is only `1` true positive case for the first model.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_1_fp` that will get the number of false positives
    for the first model:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The expected output will be as follows:'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: There is no false positive for the first model.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_1_fn` that will get the number of false negatives
    for the first model:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The expected output will be as follows:'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The first classifier presents `3` false negative cases.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_1_precision` that will calculate the precision
    for the first model:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The expected output will be as follows:'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The first classifier has a precision score of `1`, so it didn't predict any
    false positives.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_1_recall` that will calculate the recall for
    the first model:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The expected output will be as follows:'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The recall score for the first model is only `0.25`, so it is predicting quite
    a lot of false negatives.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_1_f1` that will calculate the F1 score for
    the first model:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The expected output will be as follows:'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As expected, the F1 score is quite low for the first model.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_tp` that will get the number of true positives
    for the second model:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The expected output will be as follows:'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: There are `4` true positive cases for the second model.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_fp` that will get the number of false positives
    for the second model:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The expected output will be as follows:'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: There is only one false positive for the second model.
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_fn` that will get the number of false negatives
    for the second model:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The expected output will be as follows:'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: There is no false negative for the second classifier.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_precision` that will calculate precision
    for the second model:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The expected output will be as follows:'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The precision score for the second model is quite high: `0.8`. It is not making
    too many mistakes regarding false positives.'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_recall` that will calculate recall for the
    second model:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The expected output will be as follows:'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: In terms of recall, the second classifier did a great job and didn't misclassify
    observations to false negatives.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_f1` that will calculate the F1 score for
    the second model:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The expected output will be as follows:'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The F1 score is quite high for the second model.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3evqbtu](https://packt.live/3evqbtu).
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2NoxLdo](https://packt.live/2NoxLdo).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we saw how to manually calculate the precision, recall, and
    F1 score for two different models. The first classifier has excellent precision
    but bad recall, while the second classifier has excellent recall and quite good
    precision.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Performance of Classifiers with scikit-learn
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The scikit-learn package provides some functions for automatically calculating
    the precision, recall, and F1 score for you. You will need to import them first:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'To get the precision score, you will need to get the predictions from your
    model, as shown in the following code snippet:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Calculating the `recall_score` can be done like so:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Calculating the `f1_score` can be done like so:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: In the next section, we will learn how to use another tool, called the confusion
    matrix, to analyze the performance of a classifier.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: The Confusion Matrix
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously, we learned how to use some calculated metrics to assess the performance
    of a classifier. There is another very interesting tool that can help you evaluate
    the performance of a multi-class classification model: the confusion matrix.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: A confusion matrix is a square matrix where the number of rows and columns equals
    the number of distinct label values (or classes). In the columns of the matrix,
    we place each test label value. In the rows of the matrix, we place each predicted
    label value.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'A confusion matrix looks like this:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10: Sample confusion matrix'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_10.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.10: Sample confusion matrix'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, the first row of the confusion matrix is showing
    us that the model is doing the following:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Correctly predicting class A `88` times
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting class A when the true value is B `3` times
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting class A when the true value is C `2` times
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also see the scenario where the model is making a lot of mistakes when
    it is predicting C while the true value is A (16 times). A confusion matrix is
    a powerful tool to quickly and easily spot which classes your model is performing
    well or badly for.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-learn package provides a function to calculate and display a confusion matrix:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: In the next activity, you will be building a decision tree that will classify
    cars as unacceptable, acceptable, good, and very good for customers.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.01: Car Data Classification'
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you will build a reliable decision tree model that''s capable
    of aiding a company in finding cars that clients are likely to buy. We will be
    assuming that the car rental agency is focusing on building a lasting relationship
    with its clients. Your task is to build a decision tree model that classifies
    cars into one of four categories: unacceptable, acceptable, good, and very good.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset file can also be found in our GitHub repository: [https://packt.live/2V95I6h](https://packt.live/2V95I6h).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset for this activity can be accessed here: [https://archive.ics.uci.edu/ml/datasets/Car+Evaluation](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Citation – *Dua, D., & Graff, C.. (2017). UCI Machine Learning Repository*.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'It is composed of six different features: `buying`, `maintenance`, `doors`,
    `persons`, `luggage_boot`, and `safety`. The target variable ranks the level of
    acceptability for a given car. It can take four different values: `unacc`, `acc`,
    `good`, and `vgood`.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this activity:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset into Python and import the necessary libaries.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform label encoding with `LabelEncoder()` from scikit-learn.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the `label` variable using `pop()` from pandas.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, separate the training and testing data with `train_test_spit()` from scikit-learn.
    We will use 10% of the data as test data.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the decision tree classifier using `DecisionTreeClassifier()` and its
    methods, `fit()` and `predict()`.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the score of our model based on the test data with `score()`.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a deeper evaluation of the model using `classification_report()` from
    scikit-learn.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expected output:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11: Output showing the expected classification report'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_11.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.11: Output showing the expected classification report'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 353.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: In the next section we will be looking at Random Forest Classifier.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest Classifier
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you think about the name random forest classifier, it can be explained as
    follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: A forest consists of multiple trees.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These trees can be used for classification.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the only tree we have used so far for classification is a decision tree,
    it makes sense that the random forest is a forest of decision trees.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The random nature of the trees means that our decision trees are constructed
    in a randomized manner.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, we will base our decision tree construction on information gain or
    Gini Impurity.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Once you understand these basic concepts, you essentially know what a random
    forest classifier is all about. The more trees you have in the forest, the more
    accurate prediction is going to be. When performing prediction, each tree performs
    classification. We collect the results, and the class that gets the most votes
    wins.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Random forests can be used for regression as well as for classification. When
    using random forests for regression, instead of counting the most votes for a
    class, we take the average of the arithmetic mean (average) of the prediction
    results and return it. Random forests are not as ideal for regression as they
    are for classification, though, because the models that are used to predict values
    are often out of control, and often return a wide range of values. The average
    of these values is often not too meaningful. Managing the noise in a regression
    exercise is harder than in classification.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Random forests are often better than one simple decision tree because they provide
    redundancy. They treat outlier values better and have a lower probability of overfitting
    the model. Decision trees seem to behave great as long as you are using them on
    the data that was used when creating the model. Once you use them to predict new
    data, random forests lose their edge. Random forests are widely used for classification
    problems, whether it be customer segmentation for banks or e-commerce, classifying
    images, or medicine. If you own an Xbox with Kinect, your Kinect device contains
    a random forest classifier to detect your body.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Random forest is an ensemble algorithm. The idea behind ensemble learning is
    that we take an aggregated view of a decision of multiple agents that potentially
    have different weaknesses. Due to the aggregated vote, these weaknesses cancel
    out, and the majority vote likely represents the correct result.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest Classification Using scikit-learn
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you may have guessed, the scikit-learn package provides an implementation
    of the `RandomForest` classifier with the `RandomForestClassifier` class. This
    class provides the exact same methods as all the scikit-learn models you have
    seen so far – you need to instantiate a model, then fit it with the training set
    with `.fit()`, and finally make predictions with `.predict()`:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: In the next section, we will be looking at the parameterization of the random
    forest classifier.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: The Parameterization of the Random Forest Classifier
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be considering a subset of the possible parameters, based on what we
    already know, which is based on the description of constructing random forests:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: The number of trees in the random forest. The default value
    is 10.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`criterion`: Use Gini or entropy to determine whether you use Gini Impurity
    or information gain using the entropy in each tree. This will be used to find
    the best split at each node.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: The maximum number of features considered in any tree of the
    forest. Possible values include an integer. You can also add some strings such
    as `sqrt` for the square root of the number of features.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: The maximum depth of each tree.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: The minimum number of samples in the dataset in a given
    node to perform a split. This may also reduce the tree''s size.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bootstrap`: A Boolean that indicates whether to use bootstrapping on data
    points when constructing trees.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature Importance
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A random forest classifier gives you information on how important each feature
    in the data classification process is. Remember, we used a lot of randomly constructed
    decision trees to classify data points. We can measure how accurately these data
    points behave, and we can also see which features are vital when it comes to decision-making.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'We can retrieve the array of feature importance scores with the following query:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In this six-feature classifier, the fourth and sixth features are clearly a
    lot more important than any other features. The third feature has a very low importance
    score.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance scores come in handy when we have a lot of features and we
    want to reduce the feature size to avoid the classifier getting lost in the details.
    When we have a lot of features, we risk overfitting the model. Therefore, reducing
    the number of features by dropping the least significant ones is often helpful.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Validation
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier, we learned how to use different metrics to assess the performance of
    a classifier, such as the accuracy, precision, recall, or the F1 score on a training
    and testing set. The objective is to have a high score on both sets that are very
    close to each other. In that case, your model is performant and not prone to overfitting.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: The test set is used as a proxy to evaluate whether your model can generalize
    well to unseen data or whether it learns patterns that are only relevant to the
    training set.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: But in the case of having quite a few hyperparameters to tune (such as for `RandomForest`),
    you will have to train a lot of different models and test them on your testing
    set. This kind of defeats the purpose of the testing set. Think of the testing
    set as the final exam that will define whether you pass a subject or not. You
    will not be allowed to pass and repass it over and over.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: One solution for avoiding using the testing set too much is creating a validation
    set. You will train your model on the training set and use the validation set
    to assess its score according to different combinations of hyperparameters. Once
    you find your best model, you will use the testing set to make sure it doesn't
    overfit too much. This is, in general, the suggested approach for any data science
    project.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: The drawback of this approach is that you are reducing the number of observations
    for the training set. If you have a dataset with millions of rows, it is not a
    problem. But for a small dataset, this can be problematic. This is where cross-validation
    comes in.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: The following *Figure 4.12*, shows that this is a technique where you create
    multiple splits of the training data. For each split, the training data is separated
    into folds (five, in this example) and one of the folds will be used as the validation
    set while the others will be used for training.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, for the top split, fold 5 will be used for validation and the
    four other folds (1 to 4) will be used to train the model. You will follow the
    same process for each split. After going through each split, you will have used
    the entire training data and the final performance score will be the average of
    all the models that were trained on each split:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12: Cross-validation example'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_12.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.12: Cross-validation example'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 'With scikit-learn, you can easily perform cross-validation, as shown in the
    following code snippet:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '`cross_val_score` takes two parameters:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '`cv`: Specifies the number of splits.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scoring`: Defines which performance metrics you want to use. You can find
    the list of possible values here: [https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will look at a specific variant of `RandomForest`, called
    `extratrees`.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Extremely Randomized Trees
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Extremely randomized trees increase the randomization inside random forests
    by randomizing the splitting rules on top of the already randomized factors in
    random forests.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameterization is like the random forest classifier. You can see the full
    list of parameters here: [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html).'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python implementation is as follows:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: In the following activity, we will be optimizing the classifier built in *Activity
    4.01*, *Car Data Classification*.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.02: Random Forest Classification for Your Car Rental Company'
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, you will optimize your classifier so that you satisfy your
    clients more when selecting future cars for your car fleet. We will be performing
    random forest and extreme random forest classification on the car dealership dataset
    that you worked on in the previous activity of this chapter.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this activity:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Follow *Steps 1 - 4* of the previous *Activity 4.01*, *Car Data Classification*.
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a random forest using `RandomForestClassifier`.
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the models using `.fit()`.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the `confusion_matrix` function to find the quality of the `RandomForest`.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the classification report using `classification_report()`.
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the feature importance with `.feature_importance_`.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 2 to 6* with an `extratrees` model.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expected output:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Note
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 357.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: By completing this activity, you've learned how to fit the `RandomForest` and
    `extratrees` models and analyze their classification report and feature importance.
    Now, you can try different hyperparameters on your own and see if you can improve
    their results.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use decision trees for prediction. Using
    ensemble learning techniques, we created complex reinforcement learning models
    to predict the class of an arbitrary data point.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees proved to be very accurate on the surface, but they were prone
    to overfitting the model. Random forests and extremely randomized trees reduce
    overfitting by introducing some random elements and a voting algorithm, where
    the majority wins.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Beyond decision trees, random forests, and extremely randomized trees, we also
    learned about new methods for evaluating the utility of a model. After using the
    well-known accuracy score, we started using the precision, recall, and F1 score
    metrics to evaluate how well our classifier works. All of these values were derived
    from the confusion matrix.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will describe the clustering problem and compare and
    contrast two clustering algorithms.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
