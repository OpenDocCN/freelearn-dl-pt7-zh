- en: 4\. An Introduction to Decision Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces you to two types of supervised learning algorithms in
    detail. The first algorithm will help you classify data points using decision
    trees, while the other algorithm will help you classify data points using random
    forests. Furthermore, you'll learn how to calculate the precision, recall, and
    F1 score of models, both manually and automatically. By the end of this chapter,
    you will be able to analyze the metrics that are used for evaluating the utility
    of a data model and classify data points based on decision trees and random forest
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous two chapters, we learned the difference between regression
    and classification problems, and we saw how to train some of the most famous algorithms.
    In this chapter, we will look at another type of algorithm: tree-based models.'
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based models are very popular as they can model complex non-linear patterns
    and they are relatively easy to interpret. In this chapter, we will introduce
    you to decision trees and the random forest algorithms, which are some of the
    most widely used tree-based models in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree has leaves, branches, and nodes. Nodes are where a decision
    is made. A decision tree consists of rules that we use to formulate a decision
    (or prediction) on the prediction of a data point.
  prefs: []
  type: TYPE_NORMAL
- en: Every node of the decision tree represents a feature, while every edge coming
    out of an internal node represents a possible value or a possible interval of
    values of the tree. Each leaf of the tree represents a label value of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: This may sound complicated, but let's look at an application of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a dataset with the following features and the response variable
    is determining whether a person is creditworthy or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Sample dataset to formulate the rules'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.1: Sample dataset to formulate the rules'
  prefs: []
  type: TYPE_NORMAL
- en: 'A decision tree, remember, is just a group of rules. Looking at the dataset
    in *Figure 4.1*, we can come up with the following rules:'
  prefs: []
  type: TYPE_NORMAL
- en: All people with house loans are determined as creditworthy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If debtors are employed and studying, then loans are creditworthy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: People with income above 75,000 a year are creditworthy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At or below 75,000 a year, people with car loans and who are employed are creditworthy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following the order of the rules we just defined, we can build a tree, as shown
    in *Figure 4.2* and describe one possible credit scoring method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: Decision tree for the loan type'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.2: Decision tree for the loan type'
  prefs: []
  type: TYPE_NORMAL
- en: First, we determine the loan type. House loans are automatically creditworthy
    according to the first rule. Study loans are described by the second rule, resulting
    in a subtree containing another decision on employment. Since we have covered
    both house and study loans, there are only car loans left. The third rule describes
    an income decision, while the fourth rule describes a decision on employment.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we must score a new debtor to determine whether they are creditworthy,
    we have to go through the decision tree from top to bottom and observe the true
    or false value at the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, a model based on seven data points is highly inaccurate because we
    can't generalize rules that simply do not match reality. Therefore, rules are
    often determined based on large amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: This is not the only way that we can create a decision tree. We can build decision
    trees based on other sequences of rules, too. Let's extract some other rules from
    the dataset in *Figure 4.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation 1**: Notice that individual salaries that are greater than 75,000
    are all creditworthy.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Income > 75,000 => CreditWorthy` is true.'
  prefs: []
  type: TYPE_NORMAL
- en: Rule 1 classifies four out of seven data points (IDs C, E, F, G); we need more
    rules for the remaining three data points.
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation 2**: Out of the remaining three data points, two are not employed.
    One is employed (ID D) and is creditworthy. With this, we can claim the following
    rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Income <= 75,000`, the following holds true: `Employed == true => CreditWorthy`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that with this second rule, we can also classify the remaining two data
    points (IDs A and B) as not creditworthy. With just two rules, we accurately classified
    all the observations from this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: Decision tree for income'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.3: Decision tree for income'
  prefs: []
  type: TYPE_NORMAL
- en: The second decision tree is less complex. At the same time, we cannot overlook
    the fact that the model says, *employed people with a lower income are less likely
    to pay back their loans*. Unfortunately, there is not enough training data available
    (there are only seven observations in this example), which makes it likely that
    we'll end up with false conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is a frequent problem in decision trees when making a decision based
    on a few data points. This decision is rarely representative.
  prefs: []
  type: TYPE_NORMAL
- en: Since we can build decision trees in any possible order, it makes sense to define
    an efficient way of constructing a decision tree. Therefore, we will now explore
    a measure for ordering the features in the decision process.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In information theory, entropy measures how randomly distributed the possible
    values of an attribute are. The higher the degree of randomness is, the higher
    the entropy of the attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is the highest possibility of an event. If we know beforehand what the
    outcome will be, then the event has no randomness. So, entropy is **zero**.
  prefs: []
  type: TYPE_NORMAL
- en: We use entropy to order the splitting of nodes in the decision tree. Taking
    the previous example, which rule should we start with? Should it be `Income <=
    75000` or `is employed`? We need to use a metric that can tell us that one specific
    split is better than the other. A good split can be defined by the fact it clearly
    split the data into two homogenous groups. One of these metrics is information
    gain, and it is based on entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the formula for calculating entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Entropy formula'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.4: Entropy formula'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*i represents the probability of one of the possible values of the target
    variable occurring. So, if this column has *n* different unique values, then we
    will have the probability for each of them *([p*1*, p*2*, ..., p*n*])* and apply
    the formula.'
  prefs: []
  type: TYPE_NORMAL
- en: To manually calculate the entropy of a distribution in Python, we can use the
    `np.log2` and `np.dot()` methods from the NumPy library. There is no function
    in `numpy` to automatically calculate entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The probabilities are given as a NumPy array or a regular list on *line 2*:
    *p*i.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to create a vector of the negated values of the distribution in *line
    3*: - *p*i.'
  prefs: []
  type: TYPE_NORMAL
- en: In *line 4*, we must take the base two logarithms of each value in the distribution
    list: logi pi.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we calculate the sum with the scalar product, also known as the dot
    product of the two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5: Dot product of two vectors'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.5: Dot product of two vectors'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You learned about the dot product for the first time in *Chapter 2*, *An Introduction
    to Regression*. The dot product of two vectors is calculated by multiplying the
    *i*th coordinate of the first vector by the *i*th coordinate of the second vector,
    for each *i*. Once we have all the products, we sum the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '*np.dot([1, 2, 3], [4, 5, 6])*'
  prefs: []
  type: TYPE_NORMAL
- en: This results in 1*4 + 2*5 + 3*6 = 32.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will be calculating entropy on a small sample dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.01: Calculating Entropy'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will calculate the entropy of the features in the dataset
    in *Figure 4.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6: Sample dataset to formulate the rules'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.6: Sample dataset to formulate the rules'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset file can also be found in our GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.live/2AQ6Uo9](https://packt.live/2AQ6Uo9).'
  prefs: []
  type: TYPE_NORMAL
- en: We will calculate entropy for the `Employed`, `Income`, `LoanType`, and `LoanAmount` features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `numpy` package as `np`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function called `entropy()` that receives an array of probabilities
    and then returns the calculated entropy value, as shown in the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will calculate the entropy of the `Employed` column. This column contains
    only two possible values: `true` or `false`. The `true` value appeared four times
    out of seven rows, so its probability is `4/7`. Similarly, the probability of
    the `false` value is `3/7` as it appeared three times in this dataset.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `entropy()` function to calculate the entropy of the `Employed` column
    with the probabilities `4/7` and `3/7`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This value is quite close to zero, which means the groups are quite homogenous.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, use the `entropy()` function to calculate the entropy of the `Income`
    column with its corresponding list of probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Compared to the `Employed` column, the entropy for `Income` is higher. This
    means the probabilities of this column are more spread.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `entropy` function to calculate the entropy of the `LoanType` column
    with its corresponding list of probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This value is higher than 0, so the probabilities for this column are quite
    spread.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s use the `entropy` function to calculate the entropy of the `LoanAmount`
    column with its corresponding list of probabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The entropy for `LoanAmount` is quite high, so its values are quite random.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/37T8DVz](https://packt.live/37T8DVz).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2By7aI6](https://packt.live/2By7aI6).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, you can see that the `Employed` column has the lowest entropy among the
    four different columns because it has the least variation in terms of values.
  prefs: []
  type: TYPE_NORMAL
- en: By completing this exercise, you've learned how to manually calculate the entropy
    for each column of a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Information Gain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we partition the data points in a dataset according to the values of an
    attribute, we reduce the entropy of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'To describe information gain, we can calculate the distribution of the labels.
    Initially, in *Figure 4.1*, we had five creditworthy and two not creditworthy
    individuals in our dataset. The entropy belonging to the initial distribution
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what happens if we partition the dataset based on whether the loan
    amount is greater than 15,000 or not:'
  prefs: []
  type: TYPE_NORMAL
- en: In group 1, we get one data point belonging to the 15,000 loan amount. This
    data point is not creditworthy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In group 2, we have five creditworthy individuals and one non-creditworthy individual.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The entropy of the labels in each group is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'For group 1, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For group 2, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To calculate the information gain, let''s calculate the weighted average of
    the group entropies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to find the information gain, we need to calculate the difference between
    the original entropy (`H_label`) and the one we just calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: By splitting the data with this rule, we gain a little bit of information.
  prefs: []
  type: TYPE_NORMAL
- en: When creating the decision tree, on each node, our job is to partition the dataset
    using a rule that maximizes the information gain.
  prefs: []
  type: TYPE_NORMAL
- en: We could also use Gini Impurity instead of entropy-based information gain to
    construct the best rules for splitting decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Gini Impurity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instead of entropy, there is another widely used metric that can be used to
    measure the randomness of a distribution: Gini Impurity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gini Impurity is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7: Gini Impurity'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.7: Gini Impurity'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*i here represents the probability of one of the possible values of the target
    variable occurring.'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy may be a bit slower to calculate because of the usage of the logarithm.
    Gini Impurity, on the other hand, is less precise when it comes to measuring randomness.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Some programmers prefer Gini Impurity because you don''t have to calculate
    with logarithms. Computation-wise, none of the solutions are particularly complex,
    and so both can be used. When it comes to performance, the following study concluded
    that there are often just minimal differences between the two metrics: [https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf](https://www.unine.ch/files/live/sites/imi/files/shared/documents/papers/Gini_index_fulltext.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have learned that we can optimize a decision tree by splitting
    the data based on information gain or Gini Impurity. Unfortunately, these metrics
    are only available for discrete values. What if the label is defined on a continuous
    interval such as a price range or salary range?
  prefs: []
  type: TYPE_NORMAL
- en: We have to use other metrics. You can technically understand the idea behind
    creating a decision tree based on a continuous label, which was about regression.
    One metric we can reuse in this chapter is the mean squared error. Instead of
    Gini Impurity or information gain, we have to minimize the mean squared error
    to optimize the decision tree. As this is a beginner's course, we will omit this
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss the exit condition for a decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Exit Condition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can continuously split the data points according to more and more specific
    rules until each leaf of the decision tree has an entropy of zero. The question
    is whether this end state is desirable.
  prefs: []
  type: TYPE_NORMAL
- en: Often, this is not what we expect, because we risk overfitting the model. When
    our rules for the model are too specific and too nitpicky, and the sample size
    that the decision was made on is too small, we risk making a false conclusion,
    thus recognizing a pattern in the dataset that simply does not exist in real life.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if we spin a roulette wheel three times and we get 12, 25, and
    12, this concludes that every odd spin resulting in the value 12 is not a sensible
    strategy. By assuming that every odd spin equals 12, we find a rule that is exclusively
    due to random noise.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, posing a restriction on the minimum size of the dataset that we can
    still split is an exit condition that works well in practice. For instance, if
    you stop splitting as soon as you have a dataset that's lower than 50, 100, 200,
    or 500 in size, you avoid drawing conclusions on random noise, and so you minimize
    the risk of overfitting the model.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular exit condition is the maximum restriction on the depth of the
    tree. Once we reach a fixed tree depth, we classify the data points in the leaves.
  prefs: []
  type: TYPE_NORMAL
- en: Building Decision Tree Classifiers Using scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already learned how to load data from a `.csv` file, how to apply preprocessing
    to data, and how to split data into training and testing datasets. If you need
    to refresh yourself on this knowledge, you can go back to the previous chapters,
    where you can go through this process in the context of regression and classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will assume that a set of training features, training labels, testing
    features, and testing labels have been given as a return value of the `scikit-learn
    train-test-split` call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we used `train_test_split` to split the dataset
    (features and labels) into training and testing sets. The testing set represents
    10% of the observation (`test_size=0.1`). The `random_state` parameter is used
    to get reproducible results.
  prefs: []
  type: TYPE_NORMAL
- en: We will not focus on how we got these data points because this process is exactly
    the same as in the case of regression and classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s time to import and use the decision tree classifier of scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We set one optional parameter in `DecisionTreeClassifier`, that is, `max_depth`,
    to limit the depth of the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read the official documentation for the full list of parameters: [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the more important parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`criterion`: Gini stands for Gini Impurity, while entropy stands for information
    gain. This will define which measure will be used to assess the quality of a split
    at each node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: This is the parameter that defines the maximum depth of the tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: This is the minimum number of samples needed to split
    an internal node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also experiment with all the other parameters that were enumerated in
    the documentation. We will omit them in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been built, we can use the decision tree classifier to predict
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You will build a decision tree classifier in the activity at the end of this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Metrics for Classifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After splitting the training and testing data, the decision tree model has
    a `score` method to evaluate how well testing data is classified by the model
    (also known as the accuracy score). We learned how to use the `score` method in
    the previous two chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The return value of the `score` method is a number that's less than or equal
    to 1\. The closer we get to 1, the better our model is.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will learn about another way to evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to use this method on the models you constructed in the previous chapter
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have one test feature and one test label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use the previous creditworthy example and assume we trained a decision
    tree and now have its predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8: Sample dataset to formulate the rules'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.8: Sample dataset to formulate the rules'
  prefs: []
  type: TYPE_NORMAL
- en: Our model, in general, made good predictions but had few errors. It incorrectly
    predicted the results for IDs `A`, `D`, and `E`. Its accuracy score will be 4
    / 7 = 0.57.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following definitions to define some metrics that will help
    you evaluate how good your classifier is:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Creditworthy` column, in our example) and the corresponding predictions both
    have the value `Yes`. In our example, IDs `C`, `F`, and `G` will fall under this category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`No`. Only ID `B` will be classified as true negative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Yes` but the true label is actually `No`. This will be the case for ID `A`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`No` but the true label is actually `Yes`, such as for IDs `D` and `E`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the preceding four definitions, we can define four metrics that describe
    how well our model predicts the target variable. The `#( X )` symbol denotes the
    number of values in `X`. Using technical terms, `#( X )` denotes the cardinality
    of `X`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition (Accuracy)**: *#( True Positives ) + #( True Negatives ) / #(
    Dataset )*'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy is a metric that's used for determining how many times the classifier
    gives us the correct answer. This is the first metric we used to evaluate the
    score of a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: In our previous example (*Figure 4.8*), the accuracy score will be TP + TN /
    total = (3 + 1) / 7 = 4/7.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the function provided by scikit-learn to calculate the accuracy
    of a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**Definition (Precision)**: *#TruePositives / (#TruePositives + #FalsePositives)*'
  prefs: []
  type: TYPE_NORMAL
- en: Precision centers around values that our classifier found to be positive. Some
    of these results are true positive, while others are false positive. High precision
    means that the number of false positive results is very low compared to the true
    positive results. This means that a precise classifier rarely makes a mistake
    when finding a positive result.
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition (Recall)**: *#True Positives / (#True Positives + #False Negatives)*'
  prefs: []
  type: TYPE_NORMAL
- en: Recall centers around values that are positive among the test data. Some of
    these results are found by the classifier. These are the true positive values.
    Those positive values that are not found by the classifier are false negatives.
    A classifier with a high recall value finds most of the positive values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using our previous example (*Figure 4.8*), we will get the following measures:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision = TP / (TP + FP) = 4 / (4 + 1) = 4/6 = 0.8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall = TP / (TP + FN) = 4 / (4 + 2) = 4/6 = 0.6667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these two measures, we can easily see where our model is performing better
    or worse. In this example, we know it tends to misclassify false negative cases.
    These measures are more granular than the accuracy score, which only gives you
    an overall score.
  prefs: []
  type: TYPE_NORMAL
- en: The F1 score is a metric that combines precision and recall scores. Its value
    ranges between 0 and 1\. If the F1 score equals 1, it means the model is perfectly
    predicting the right outcomes. On the other hand, an F1 score of 0 means the model
    cannot predict the target variable accurately. The advantage of the F1 score is
    that it considers both false positives and false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for calculating the F1 score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9: Formula to calculate the F1 score'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.9: Formula to calculate the F1 score'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final note, the scikit-learn package also provides a handy function that
    can show all these measures in one go: `classification_report()`. A classification
    report is useful to check the quality of our predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the next exercise, we will be practicing how to calculate these scores manually.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4.02: Precision, Recall, and F1 Score Calculation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will calculate the precision, recall value, and the F1
    score of two different classifiers on a simulated dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter Notebook file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the `numpy` package as `np` using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `numpy` array called `real_labels` that contains the values [`True,
    True, False, True, True]`. This list will represent the true values of the target
    variable for our simulated dataset. Print its content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `numpy` array called `model_1_preds` that contains the values `[True,
    False, False, False, False]`. This list will represent the predicted values of
    the first classifier. Print its content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create another `numpy` array called `model_2_preds` that contains the values
    `[True, True, True, True, True]`. This list will represent the predicted values
    of the first classifier. Print its content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `model_1_tp_cond` that will find the true positives
    for the first model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `model_1_tp` that will get the number of true positives
    for the first model by summing `model_1_tp_cond`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is only `1` true positive case for the first model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_1_fp` that will get the number of false positives
    for the first model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is no false positive for the first model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_1_fn` that will get the number of false negatives
    for the first model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first classifier presents `3` false negative cases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_1_precision` that will calculate the precision
    for the first model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first classifier has a precision score of `1`, so it didn't predict any
    false positives.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_1_recall` that will calculate the recall for
    the first model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The recall score for the first model is only `0.25`, so it is predicting quite
    a lot of false negatives.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_1_f1` that will calculate the F1 score for
    the first model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As expected, the F1 score is quite low for the first model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_tp` that will get the number of true positives
    for the second model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are `4` true positive cases for the second model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_fp` that will get the number of false positives
    for the second model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is only one false positive for the second model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_fn` that will get the number of false negatives
    for the second model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is no false negative for the second classifier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_precision` that will calculate precision
    for the second model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The precision score for the second model is quite high: `0.8`. It is not making
    too many mistakes regarding false positives.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_recall` that will calculate recall for the
    second model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In terms of recall, the second classifier did a great job and didn't misclassify
    observations to false negatives.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable called `model_2_f1` that will calculate the F1 score for
    the second model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The F1 score is quite high for the second model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3evqbtu](https://packt.live/3evqbtu).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2NoxLdo](https://packt.live/2NoxLdo).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we saw how to manually calculate the precision, recall, and
    F1 score for two different models. The first classifier has excellent precision
    but bad recall, while the second classifier has excellent recall and quite good
    precision.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Performance of Classifiers with scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The scikit-learn package provides some functions for automatically calculating
    the precision, recall, and F1 score for you. You will need to import them first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the precision score, you will need to get the predictions from your
    model, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculating the `recall_score` can be done like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Calculating the `f1_score` can be done like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will learn how to use another tool, called the confusion
    matrix, to analyze the performance of a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The Confusion Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously, we learned how to use some calculated metrics to assess the performance
    of a classifier. There is another very interesting tool that can help you evaluate
    the performance of a multi-class classification model: the confusion matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: A confusion matrix is a square matrix where the number of rows and columns equals
    the number of distinct label values (or classes). In the columns of the matrix,
    we place each test label value. In the rows of the matrix, we place each predicted
    label value.
  prefs: []
  type: TYPE_NORMAL
- en: 'A confusion matrix looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10: Sample confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.10: Sample confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, the first row of the confusion matrix is showing
    us that the model is doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Correctly predicting class A `88` times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting class A when the true value is B `3` times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting class A when the true value is C `2` times
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also see the scenario where the model is making a lot of mistakes when
    it is predicting C while the true value is A (16 times). A confusion matrix is
    a powerful tool to quickly and easily spot which classes your model is performing
    well or badly for.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-learn package provides a function to calculate and display a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: In the next activity, you will be building a decision tree that will classify
    cars as unacceptable, acceptable, good, and very good for customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.01: Car Data Classification'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, you will build a reliable decision tree model that''s capable
    of aiding a company in finding cars that clients are likely to buy. We will be
    assuming that the car rental agency is focusing on building a lasting relationship
    with its clients. Your task is to build a decision tree model that classifies
    cars into one of four categories: unacceptable, acceptable, good, and very good.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset file can also be found in our GitHub repository: [https://packt.live/2V95I6h](https://packt.live/2V95I6h).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset for this activity can be accessed here: [https://archive.ics.uci.edu/ml/datasets/Car+Evaluation](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation).'
  prefs: []
  type: TYPE_NORMAL
- en: Citation – *Dua, D., & Graff, C.. (2017). UCI Machine Learning Repository*.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is composed of six different features: `buying`, `maintenance`, `doors`,
    `persons`, `luggage_boot`, and `safety`. The target variable ranks the level of
    acceptability for a given car. It can take four different values: `unacc`, `acc`,
    `good`, and `vgood`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset into Python and import the necessary libaries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform label encoding with `LabelEncoder()` from scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the `label` variable using `pop()` from pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, separate the training and testing data with `train_test_spit()` from scikit-learn.
    We will use 10% of the data as test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the decision tree classifier using `DecisionTreeClassifier()` and its
    methods, `fit()` and `predict()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the score of our model based on the test data with `score()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a deeper evaluation of the model using `classification_report()` from
    scikit-learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11: Output showing the expected classification report'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.11: Output showing the expected classification report'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 353.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section we will be looking at Random Forest Classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you think about the name random forest classifier, it can be explained as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A forest consists of multiple trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These trees can be used for classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the only tree we have used so far for classification is a decision tree,
    it makes sense that the random forest is a forest of decision trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The random nature of the trees means that our decision trees are constructed
    in a randomized manner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, we will base our decision tree construction on information gain or
    Gini Impurity.
  prefs: []
  type: TYPE_NORMAL
- en: Once you understand these basic concepts, you essentially know what a random
    forest classifier is all about. The more trees you have in the forest, the more
    accurate prediction is going to be. When performing prediction, each tree performs
    classification. We collect the results, and the class that gets the most votes
    wins.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests can be used for regression as well as for classification. When
    using random forests for regression, instead of counting the most votes for a
    class, we take the average of the arithmetic mean (average) of the prediction
    results and return it. Random forests are not as ideal for regression as they
    are for classification, though, because the models that are used to predict values
    are often out of control, and often return a wide range of values. The average
    of these values is often not too meaningful. Managing the noise in a regression
    exercise is harder than in classification.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests are often better than one simple decision tree because they provide
    redundancy. They treat outlier values better and have a lower probability of overfitting
    the model. Decision trees seem to behave great as long as you are using them on
    the data that was used when creating the model. Once you use them to predict new
    data, random forests lose their edge. Random forests are widely used for classification
    problems, whether it be customer segmentation for banks or e-commerce, classifying
    images, or medicine. If you own an Xbox with Kinect, your Kinect device contains
    a random forest classifier to detect your body.
  prefs: []
  type: TYPE_NORMAL
- en: Random forest is an ensemble algorithm. The idea behind ensemble learning is
    that we take an aggregated view of a decision of multiple agents that potentially
    have different weaknesses. Due to the aggregated vote, these weaknesses cancel
    out, and the majority vote likely represents the correct result.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest Classification Using scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you may have guessed, the scikit-learn package provides an implementation
    of the `RandomForest` classifier with the `RandomForestClassifier` class. This
    class provides the exact same methods as all the scikit-learn models you have
    seen so far – you need to instantiate a model, then fit it with the training set
    with `.fit()`, and finally make predictions with `.predict()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will be looking at the parameterization of the random
    forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The Parameterization of the Random Forest Classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be considering a subset of the possible parameters, based on what we
    already know, which is based on the description of constructing random forests:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: The number of trees in the random forest. The default value
    is 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`criterion`: Use Gini or entropy to determine whether you use Gini Impurity
    or information gain using the entropy in each tree. This will be used to find
    the best split at each node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: The maximum number of features considered in any tree of the
    forest. Possible values include an integer. You can also add some strings such
    as `sqrt` for the square root of the number of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: The maximum depth of each tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: The minimum number of samples in the dataset in a given
    node to perform a split. This may also reduce the tree''s size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bootstrap`: A Boolean that indicates whether to use bootstrapping on data
    points when constructing trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature Importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A random forest classifier gives you information on how important each feature
    in the data classification process is. Remember, we used a lot of randomly constructed
    decision trees to classify data points. We can measure how accurately these data
    points behave, and we can also see which features are vital when it comes to decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can retrieve the array of feature importance scores with the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: In this six-feature classifier, the fourth and sixth features are clearly a
    lot more important than any other features. The third feature has a very low importance
    score.
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance scores come in handy when we have a lot of features and we
    want to reduce the feature size to avoid the classifier getting lost in the details.
    When we have a lot of features, we risk overfitting the model. Therefore, reducing
    the number of features by dropping the least significant ones is often helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier, we learned how to use different metrics to assess the performance of
    a classifier, such as the accuracy, precision, recall, or the F1 score on a training
    and testing set. The objective is to have a high score on both sets that are very
    close to each other. In that case, your model is performant and not prone to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: The test set is used as a proxy to evaluate whether your model can generalize
    well to unseen data or whether it learns patterns that are only relevant to the
    training set.
  prefs: []
  type: TYPE_NORMAL
- en: But in the case of having quite a few hyperparameters to tune (such as for `RandomForest`),
    you will have to train a lot of different models and test them on your testing
    set. This kind of defeats the purpose of the testing set. Think of the testing
    set as the final exam that will define whether you pass a subject or not. You
    will not be allowed to pass and repass it over and over.
  prefs: []
  type: TYPE_NORMAL
- en: One solution for avoiding using the testing set too much is creating a validation
    set. You will train your model on the training set and use the validation set
    to assess its score according to different combinations of hyperparameters. Once
    you find your best model, you will use the testing set to make sure it doesn't
    overfit too much. This is, in general, the suggested approach for any data science
    project.
  prefs: []
  type: TYPE_NORMAL
- en: The drawback of this approach is that you are reducing the number of observations
    for the training set. If you have a dataset with millions of rows, it is not a
    problem. But for a small dataset, this can be problematic. This is where cross-validation
    comes in.
  prefs: []
  type: TYPE_NORMAL
- en: The following *Figure 4.12*, shows that this is a technique where you create
    multiple splits of the training data. For each split, the training data is separated
    into folds (five, in this example) and one of the folds will be used as the validation
    set while the others will be used for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, for the top split, fold 5 will be used for validation and the
    four other folds (1 to 4) will be used to train the model. You will follow the
    same process for each split. After going through each split, you will have used
    the entire training data and the final performance score will be the average of
    all the models that were trained on each split:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12: Cross-validation example'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16060_04_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4.12: Cross-validation example'
  prefs: []
  type: TYPE_NORMAL
- en: 'With scikit-learn, you can easily perform cross-validation, as shown in the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '`cross_val_score` takes two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cv`: Specifies the number of splits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scoring`: Defines which performance metrics you want to use. You can find
    the list of possible values here: [https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will look at a specific variant of `RandomForest`, called
    `extratrees`.
  prefs: []
  type: TYPE_NORMAL
- en: Extremely Randomized Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Extremely randomized trees increase the randomization inside random forests
    by randomizing the splitting rules on top of the already randomized factors in
    random forests.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameterization is like the random forest classifier. You can see the full
    list of parameters here: [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python implementation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: In the following activity, we will be optimizing the classifier built in *Activity
    4.01*, *Car Data Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 4.02: Random Forest Classification for Your Car Rental Company'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, you will optimize your classifier so that you satisfy your
    clients more when selecting future cars for your car fleet. We will be performing
    random forest and extreme random forest classification on the car dealership dataset
    that you worked on in the previous activity of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Follow *Steps 1 - 4* of the previous *Activity 4.01*, *Car Data Classification*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a random forest using `RandomForestClassifier`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the models using `.fit()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the `confusion_matrix` function to find the quality of the `RandomForest`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the classification report using `classification_report()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the feature importance with `.feature_importance_`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 2 to 6* with an `extratrees` model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 357.
  prefs: []
  type: TYPE_NORMAL
- en: By completing this activity, you've learned how to fit the `RandomForest` and
    `extratrees` models and analyze their classification report and feature importance.
    Now, you can try different hyperparameters on your own and see if you can improve
    their results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use decision trees for prediction. Using
    ensemble learning techniques, we created complex reinforcement learning models
    to predict the class of an arbitrary data point.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees proved to be very accurate on the surface, but they were prone
    to overfitting the model. Random forests and extremely randomized trees reduce
    overfitting by introducing some random elements and a voting algorithm, where
    the majority wins.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond decision trees, random forests, and extremely randomized trees, we also
    learned about new methods for evaluating the utility of a model. After using the
    well-known accuracy score, we started using the precision, recall, and F1 score
    metrics to evaluate how well our classifier works. All of these values were derived
    from the confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will describe the clustering problem and compare and
    contrast two clustering algorithms.
  prefs: []
  type: TYPE_NORMAL
