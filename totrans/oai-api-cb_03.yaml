- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Key Parameters and Their Impact on Generated Responses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned that the OpenAI API is not just one endpoint
    but also a collection of various endpoints. These endpoints are triggered with
    `model` and `messages` – and we’ve mainly seen how changing the messages parameter
    impacts the generated response. However, there is a vast collection of optional
    parameters that influence the behavior of the API, such as temperature, N, and
    the maximum number of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore these optional key parameters and understand
    how they influence the generated response. **Parameters** are like the dials and
    knobs you’d find on a complex machine. By adjusting these dials and knobs, you
    can change the behavior of the machine to your liking. Similarly, in the realm
    of ChatGPT, parameters allow us to tweak the finer details of the model’s behavior,
    influencing how it processes input and crafts its output. Each of these plays
    a unique role in shaping the response from OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will know how these parameters can be adjusted
    to better suit your specific needs, how they affect the quality, length, and style
    of the output, and how to make effective use of them to get the most desirable
    results. Learning this is important, as these parameters will need to change as
    we begin integrating the API for different use cases in intelligent applications,
    and understanding how generated responses change with these parameters will enable
    us to determine the correct settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following recipes, each of which will focus
    on a key parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: Changing the model parameter and understanding its impact on generated responses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling the number of generated responses using the n parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the randomness and creativity of generated responses using the temperature
    parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the recipes in this chapter require you to have access to the OpenAI API
    (via a generated API key) and have an API client installed, such as Postman. You
    can refer to the [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021) recipe *Making
    OpenAI API requests with Postman* for more information on how to obtain your API
    key and set up Postman.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the model parameter and understanding its impact on generated responses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In both [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021) and [*Chapter 2*](B21007_02.xhtml#_idTextAnchor044),
    the chat completion requests were made using both the model and messages parameters,
    with `model` always being equal to the `gpt-3.5-turbo` value. We essentially ignored
    the model parameter. However, this parameter likely has the biggest impact on
    the generated responses of any other parameter. Contrary to popular belief, the
    OpenAI API is not just one model; it’s powered by a diverse set of models with
    different capabilities and price points.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will cover two main models (*GPT-3.5* and *GPT-4*), learn
    how to change the `model` parameter, and observe how the generated responses vary
    between these two models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure you have an OpenAI Platform account with available usage credits. If
    you don’t, please follow the *Setting up your OpenAI Playground environment* recipe
    in [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, ensure that you have Postman installed, that you have created a
    new workspace, that you have created a new HTTP request, and that `Headers` for
    that request are correctly configured. This is important because, without the
    `Authorization` configured, you will not be able to use the API. If you don’t
    have Postman installed and configured as mentioned, follow the *Making OpenAI
    API requests with Postman* recipe in [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021).
    However, if you do not remember, *steps 1–4* in the next section explain the configuration
    process.
  prefs: []
  type: TYPE_NORMAL
- en: All the recipes in this chapter will have this same requirement.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In your Postman workspace, select the **New** button on the top-left menu bar,
    and then select **HTTP** from the list of options that appears. This will create
    a new **Untitled Request**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the HTTP request type from **GET** to **POST** by selecting the **Method**
    drop-down menu (by default, it will be set to **GET**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the following URL as the endpoint for chat completions: [https://api.openai.com/v1/chat/completions](https://api.openai.com/v1/chat/completions).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select **Headers** in the sub-menu, and add the following key-value pairs into
    the table below it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| *Key* | *Value* |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Content-Type` | `application/json` |'
  prefs: []
  type: TYPE_TB
- en: '| `Authorization` | `Bearer <your API` `key here>` |'
  prefs: []
  type: TYPE_TB
- en: 'Select **Body** in the sub-menu, and then select **raw** for the request type.
    Enter the following request body, which details to OpenAI the prompt, system message,
    chat log, and the set of other parameters that it needs to use to generate a completion
    response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '5. After sending the HTTP request, you should see the following response from
    the OpenAI API. Note that your response may be different. The section of the HTTP
    response that we particularly want to take note of is the `content` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '6. Let’s now repeat the HTTP request in *step 4* and keep everything else consistent,
    but modify the `model` parameter. Specifically, we will change the value of that
    parameter to `gpt-4`. Enter the following for the endpoint and request body, and
    then click **Send**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '7. You should see the following similar response from the OpenAI API. Note
    that the response is far different than what we received earlier. Notably, it
    more closely matches the instruction in the prompt of generating six five-letter
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '8. Repeat *steps 1–4*, but change the `content` parameter inside `messages`
    to the following prompt instead: `How many chemicals exist in cigarettes, how
    many of them are known to be harmful, and how many are known to cause cancer?
    Respond with just the numbers,` `nothing else`.'
  prefs: []
  type: TYPE_NORMAL
- en: Again, execute one chat completion request where the `model` parameter is `gpt-3.5-turbo`
    and one where the `model` parameter is `gpt-4`.
  prefs: []
  type: TYPE_NORMAL
- en: '9. The following are extracts of the HTTP response that I received using GPT-3.5-turbo
    and GPT-4:'
  prefs: []
  type: TYPE_NORMAL
- en: When **model** = **gpt-3.5-turbo:**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When **model** = **gpt-4:**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '10. Repeat *steps 4–7*, but change the `content` parameter inside `messages`
    to the following logical question prompt instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that HTTP requests, with the request body being in the JSON format, cannot
    handle multiline strings. As a result, if you need to write multiline strings
    into any of the API parameters (such as `messages` in this case), use the line
    break characters instead (`\n`):'
  prefs: []
  type: TYPE_NORMAL
- en: For example,
  prefs: []
  type: TYPE_NORMAL
- en: '`"`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Line 1`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Line 2`'
  prefs: []
  type: TYPE_NORMAL
- en: '`"`'
  prefs: []
  type: TYPE_NORMAL
- en: would become
  prefs: []
  type: TYPE_NORMAL
- en: '`"Line` `1\nLine 2"`'
  prefs: []
  type: TYPE_NORMAL
- en: '11. The following are extracts of the HTTP response that I received:'
  prefs: []
  type: TYPE_NORMAL
- en: When **model** = **gpt-3.5-turbo:**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When **model** = **gpt-4:**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we observed three different examples of how changing the `model`
    parameter affected the generated text. The following table summarizes the different
    responses generated by OpenAI, based on different model parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '| *Prompt* | *Response when model =* *gpt-3.5-turbo* | *Response when model
    =* *gpt-4* |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Describe Donald Trump’s time in office in a sentence that has six five-letter
    words. Remember, each word must have five letters |'
  prefs: []
  type: TYPE_TB
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| How many chemicals exist in cigarettes, how many of them are known to be
    harmful, and how many are known to cause cancer? Respond with just the numbers,
    nothing else |'
  prefs: []
  type: TYPE_TB
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Which conclusion follows from the statement with absolute certainty?'
  prefs: []
  type: TYPE_NORMAL
- en: None of the stamp collectors is an architect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the drones are stamp collectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: In all cases, the `gpt-4` model produced more accurate results than `gpt-3.5-turbo`.
    For example, in the first prompt about describing *Donald Trump’s time in office*,
    the `gpt-3.5-turbo` model did not understand that it should only use five-letter
    words, whereas `gpt-4` was able to answer it successfully.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 versus GPT-3.5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Why is that the case? The inner workings of the two models are different. In
    neural network models such as GPT, a parameter is a single numerical value that
    combines with others, which perform calculations that turn inputs (such as a prompt)
    into output data (such as a chat completion response). The larger the number of
    parameters, the greater the capacity for the model to accurately capture patterns
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: The GPT-3.5 set of models was trained with 175 billion parameters, whereas the
    GPT-4 set of models is estimated to be trained on more than 100 trillion parameters
    (collectively over an ensemble of smaller models), many order of magnitudes higher
    ([https://www.pcmag.com/news/the-new-chatgpt-what-you-get-with-gpt-4-vs-gpt-35](https://www.pcmag.com/news/the-new-chatgpt-what-you-get-with-gpt-4-vs-gpt-35)).
    The neural network behind GPT-4 is far denser, enabling it to understand nuances
    and answer more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: GPT models typically struggle with very complex and long instructions. For example,
    in the cigarette question, the instruction was clearly to `respond with just the
    numbers, nothing else`. GPT-3.5 provided a suitable answer, but not in the correct
    format, whereas the answer returned by GPT-4 was in the correct format.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, GPT-4 is more reliable and can handle much more nuanced instructions
    than GPT-3.5\. It is worth noting that the distinction can be subtle, even non-existent,
    for primarily easy tasks. To discern these differences, the two models were tested
    on a variety of benchmarks and common exams, which demonstrates the power of GPT-4\.
    You can learn about these test results here: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4).
    Overall, GPT-4 outperformed GPT-3.5 on various standardized exams, such as AP
    calculus, AP English literature, and LSAT.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other differences between GPT-4 and GPT-3.5 include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory and context size**: GPT-4 can retain more memory and has a bigger
    context window ([https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)),
    which means it can accept much larger and more complex prompts than GPT-3.5\.
    The **context window** refers to the amount of recent input (in terms of tokens
    or chunks of text) the model can consider when generating a response. Imagine
    reading a paragraph from the middle of a book; the more sentences you can see
    and remember, the better you understand that paragraph’s context. Similarly, with
    a larger context window, GPT-4 can *see* and *remember* more of the previous input,
    allowing it to generate more contextually relevant responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual input**: GPT-4 can accept both text and images, whereas GPT-3.5 is
    text-only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language**: Both GPT-3.5 and GPT-4 have multilingual capabilities, meaning
    they can understand, interpret, and respond in languages other than English. However,
    while GPT-3.5 can work in multiple languages, GPT-4 offers enhanced linguistic
    finesse and can go beyond simple speech in other languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alignment**: GPT-4 has been more *aligned*, meaning it has a bias to not
    provide harmful advice, buggy code, or inaccurate information, from human-based
    adversarial testing. In this context, **alignment** refers to the process of adjusting
    GPT-4’s responses to be more in line with ethical and safety standards, reducing
    the likelihood of it providing harmful advice, buggy code, or inaccurate information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost considerations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One important difference between GPT-4 and GPT-3.5 is cost. GPT-4 charges a
    much higher token rate, which also increases if models with larger context windows
    are chosen.
  prefs: []
  type: TYPE_NORMAL
- en: A **token** is a chunk of text that the model reads as input or generates as
    output. These tokens may be a single character, part of a word, or the word itself.
    As a rough rule of thumb, 1 token is equal to 0.75 words ([https://platform.openai.com/docs/introduction/key-concepts](https://platform.openai.com/docs/introduction/key-concepts)).
  prefs: []
  type: TYPE_NORMAL
- en: 'When making API requests for chat completions, the response always includes
    the number of tokens that was used in the request, in the `usage` object. For
    example, the following is an excerpt for the response in *step 5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This tells us that our `Describe Donald Trump''s time in office in a sentence
    that has six five-letter words. Remember, each word must have 5 letters` prompt
    was 33 tokens, and the following response was 12 tokens, making a combined total
    of 45 tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of tokens matters for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the model chosen, the number of total tokens cannot exceed the
    model’s *max token*, also known as the context window. For GPT-3.5-turbo, this
    is 4,096 tokens. This means that in any API request using that model, the sum
    of *content* in **messages** cannot exceed 4,096 tokens, or approximately 3,000
    words. In comparison, GPT-4 has a sub-model called **gpt-4-32k**, which has a
    context window of 32,768 tokens, or around 24,000 words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of total tokens and the model you use dictates how much you are charged
    for an API request. For example, in *step 5*, we used 45 tokens using the **gpt-3.5-turbo**
    model, which means that the request cost USD 0.0000675\. By comparison, the same
    45 tokens using **gpt-4** would have cost USD 0.00135, which is 20x the cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision criteria
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The determination of which model to use in chat completion requests should
    depend on the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context window**: Determine the likely context window of the chat completion
    requests. If your prompts are likely to be over 12,000 words, then you need to
    use GPT-4, as the biggest model underneath GPT-3.5 only has a maximum number of
    16,384 tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity**: Determine the complexity of your chat completion request. In
    general, if it requires nuance understanding and formatting instructions such
    as the first two examples in the recipe, or if it requires complex information
    synthesis and logical problem solving such as the third example in the recipe,
    then you need to use GPT-4\. This is especially the case with any mathematical
    or scientific reasoning – GPT-4 performs far better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost**: Evaluate the cost implications of choosing GPT-4 over GPT-3.5\. If
    you use the GPT-4 model with the highest context window, this can be 40x times
    the price of a request using GPT-3.5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, you should always use and test GPT-3.5 first to see whether it can
    provide suitable chat completions, and then move to GPT-4 if absolutely necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the `model` parameter influences the quality of generated responses,
    which is important, as different use cases of API requests will require different
    levels of sophisticated responses.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling the number of generated responses using the n parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For certain intelligent applications that you build, you want multiple generated
    texts from the same prompt. For example, if we’re building an app that generates
    company slogans, you likely want to generate not just one but also multiple responses
    so that the user can select the best one. The `n` parameter controls how many
    chat completion choices to generate for each input message. It can also control
    the number of images that are generated when using the *Images* endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will see how the `n` parameter affects the number of generated
    responses and understand the different use cases for it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Postman, enter the following URL as the endpoint for chat completions: [https://api.openai.com/v1/chat/completions](https://api.openai.com/v1/chat/completions).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the request body, type in the following and click **Send**. Note that we
    have added t**h**e **n** parameter and set it to the default value of **1** explicitly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After sending the HTTP request, you should see the following (similar, but
    not exact) response from the OpenAI API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we’ll repeat the request in *step 2*, but let’s change the **n** parameter
    to a value of **3**. After sending the HTTP request, we get the following response.
    Note that there are now three separate objects or responses within **choices**.
    We effectively received three different generated responses to the prompt:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s generate images and observe how the **n** parameter affects the
    number of images returned. In Postman, enter the following for the endpoint: [https://api.openai.com/v1/images/generations](https://api.openai.com/v1/images/generations).
    In the request body, type in the following, and then click **Send**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After sending the HTTP request, you should see the following response from
    the OpenAI API. Notably, you should see three different URLs, each corresponding
    to a generated image. The URLs in the following code block have been artificially
    condensed. After copying and pasting the URLs into your browser, you should see
    images of ice cream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: "![Figure 3.1 – Output of the OpenAI image endpoint (\uFEFFn=3)](img/B21007_03_1.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Output of the OpenAI image endpoint (n=3)
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `n` parameter simply specifies the number of generated responses from the
    OpenAI API. For chat completions, it can be any integer; this means you can ask
    the API to return thousands and thousands of responses. For image generations,
    this parameter has a max value of *10*, meaning you can only generate up to 10
    images per request.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of n
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The applications of having an `n` parameter are very broad – it’s often useful
    to have a parameter that controls and repeats generations for the same prompt,
    all in one HTTP request. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creativity**: For creative apps and tasks such as slogan generation, songwriting,
    or brainstorming, providing a richer set of materials to work from makes it easier
    for users'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Redundancy**: Since response generations from the OpenAI API with the same
    prompt can differ wildly, it’s useful to create multiple responses and cross-verify
    the information, especially in mission-critical workflows'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A/B testing**: Very common in marketing, the **n** parameter enables you
    to create multiple responses that users can experiment with to see which one performs
    better'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations of n
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: However, multiple generations do generally mean a lower speed and higher cost,
    which are considerations that need to be taken into account before deciding what
    value to set for the `n` parameter. For example, in our recipe, when we requested
    one generation, the cost was *33* tokens (as specified in the response). However,
    when `n = 3`, the total number of tokens jumped to *52* tokens. We learned in
    the previous recipe that the OpenAI API charges based on the total number of tokens
    generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the cost increase is not linear – generating three additional responses
    only cost ~60% more tokens, instead of the expected 3x. This is for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of prompt tokens remains fixed no matter how many generations are
    created, whether it’s 1 or 100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model finds computational savings when it knows to produce multiple completions
    instead of one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is also why using the `n` parameter is far better (from a cost point of
    view) than just executing the HTTP request multiple times. Under the hood, when
    you set `n = 3`, the model in parallel processes the requests during a single
    model inference, leveraging inherent efficiencies. We could have, for example,
    run the HTTP request three times instead of one HTTP request where `n = 3`, but
    that would mean spending ~3x more cost and overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the `n` parameter impacts the number of generated responses, which
    is tremendously valuable for particular use cases, resulting in lower costs as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the randomness and creativity of generated responses using the temperature
    parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Temperature** is likely to be one of the least understood parameters. Overall,
    it controls the creativity or randomness of text generations. The higher the temperature,
    the more diverse and creative the results will be – even for the same input. In
    practice, the temperature is set based on the use case. Applications where consistent
    and standard generations are needed should use a very low temperature, whereas
    solutions that require creative approaches should opt for higher temperatures.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn about the temperature parameter, observing how
    it can be used to influence the text generations produced by the OpenAI API.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Postman, enter the following for the endpoint: [https://api.openai.com/v1/chat/completions](https://api.openai.com/v1/chat/completions).
    In the request body, type in the following, and then click **Send**. Our prompt
    is **Explain gravity in one sentence**. Note that we have added the **temperature**
    parameter and set it to the value of **0** explicitly. We will repeat this *three*
    times and record the responses of the **content** parameter for each generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s edit our request body and change the **temperature** parameter
    to the highest value possible, which is **2**. Click **Send**, and then repeat
    this three times, recording the responses of the **content** parameter for each
    generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s repeat *steps 1–2* but use a more creative prompt, such as **Create
    a creative tag line for an AI learning book**. Again, we will first perform a
    chat completion with the temperature parameter equal to **0** three times. Then,
    we will increase the temperature parameter to **2** and run the request three
    times again. The responses of the **content** parameter for each generation are
    listed in the following code blocks. Note that yours will likely differ:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw in the recipe, the temperature parameter controls the randomness
    and creativity of the text generation. When the temperature was set very low,
    the API produced very consistent and deterministic results for the same prompt.
    In the first example, gravity was explained in the same exact way for each chat
    completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'When we increased the temperature, we saw very different, more creative, and
    unexpected responses, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Think of the temperature setting as the dial on a radio. A lower temperature
    is like tuning the radio to a well-established station where the signal is strong
    and clear, and you get a consistent, expected type of music or talk show. This
    is analogous to the model delivering responses that are reliable, straightforward,
    and closely aligned with the most likely answer.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, a higher temperature is similar to tuning the radio to a frequency
    where you might catch a variety of stations, some clear and some static-filled,
    playing an eclectic mix of genres. This creates an environment where unexpected,
    novel, and varied content comes through. In the context of the language model,
    this means generating more creative, diverse, and sometimes unpredictable responses,
    mirroring the eclectic and varied nature of a radio dial turned toward a less
    defined frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Temperature inner working
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed before, when a model generates text, it calculates probabilities
    for the next word based on the prompt and response it has built so far. In practice,
    temperature affects the response by changing the probability distribution of the
    next word.
  prefs: []
  type: TYPE_NORMAL
- en: With a higher temperature, this distribution becomes flatter, meaning less-probable
    words have a higher chance of being selected. At a lower temperature, the distribution
    becomes more pronounced or *sharper*, meaning the most probable words are likely
    to be chosen every time, which reduces randomness.
  prefs: []
  type: TYPE_NORMAL
- en: Decision based on use case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decision on which temperature to use depends solely on the particular use
    case. In general, there are three categories of this parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Low-temperature values (0.0 to 0.8)**: These should be used for primarily
    analytical, factual, or logical tasks so that the model is more deterministic
    and focused. In these use cases, traceability and repeatability is also important,
    and so a lower temperature is better, as it reduces randomness. A lower temperature
    also means adhering to established patterns and conventions, leading to more correct
    answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples include generating code, performing data analysis, and answering factual
    questions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Medium-temperature values (0.8 to 1.2)**: These should be used for general-purpose
    and chatbot-like tasks, where balancing coherence and creativity is critical.
    This enables the model to be flexible and produce new ideas, but it still remains
    focused to the prompt at-hand.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples include chatbots/conversational agents and Q&A systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**High-temperature values (1.2 to 2.0)**: These should be used for creative
    writing and brainstorming as the model is not constrained to follow established
    patterns and can explore very diverse styles. Here, a *correct* answer does not
    exist, and instead, the purpose is to create varying outputs. This does mean that
    you may get unexpected outputs that do not conform to the actual prompt at all.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples include storytelling, generating marketing slogans, and brainstorming
    company names.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the recipe, a lower temperature was far better when explaining gravity, as
    the prompt encourages a factual and straightforward answer. However, the second
    prompt, about creating a tagline, is far better suited for a higher temperature,
    as this is a task that requires creativity and out-of-the-box thinking.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, setting a temperature value means performing a trade-off between coherence
    and creativity, which shifts based on how you use the API within your application.
    As a rule of thumb, it’s best to set the temperature to 1 and then modify it in
    increments of 0.2 until you reach your desired output set.
  prefs: []
  type: TYPE_NORMAL
