["```py\nn_states = 3\nP = np.zeros((n_states, n_states), np.float)\nP[0, 1] = 0.7\nP[0, 2] = 0.3\nP[1, 0] = 0.5\nP[1, 2] = 0.5\nP[2, 1] = 0.1\nP[2, 2] = 0.9\n```", "```py\nR = np.zeros((n_states, n_states), np.float)\nR[0, 1] = 1\nR[0, 2] = 10\nR[1, 0] = 0\nR[1, 2] = 1\nR[2, 1] = -1\nR[2, 2] = 10\n```", "```py\nR_expected = np.sum(P * R, axis=1, keepdims=True)\n```", "```py\ngamma = 0.9\nA = np.eye(n_states) - gamma * P\nB = R_states\n# solve using scipy linalg solve\nV = linalg.solve(A, B)\n```", "```py\n    import numpy as np\n    from scipy import linalg\n    ```", "```py\n    # define the Transition Probability Matrix\n    n_states = 3\n    P = np.zeros((n_states, n_states), np.float)\n    P[0, 1] = 0.7\n    P[0, 2] = 0.3\n    P[1, 0] = 0.5\n    P[1, 2] = 0.5\n    P[2, 1] = 0.1\n    P[2, 2] = 0.9\n    print(P)\n    ```", "```py\n    array([[0\\. , 0.7, 0.3],\n           [0.5, 0\\. , 0.5],\n           [0\\. , 0.1, 0.9]])\n    ```", "```py\n    # the sum over columns is 1 for each row being a probability matrix\n    assert((np.sum(P, axis=1) == 1).all())\n    ```", "```py\n    # define the reward matrix\n    R = np.zeros((n_states, n_states), np.float)\n    R[0, 1] = 1\n    R[0, 2] = 10\n    R[1, 0] = 0\n    R[1, 2] = 1\n    R[2, 1] = -1\n    R[2, 2] = 10\n    \"\"\"\n    calculate expected reward for each state by multiplying the probability matrix for each reward\n    \"\"\"\n    #keepdims is required to obtain a column vector\n    R_expected = np.sum(P * R, axis=1, keepdims=True)\n    # The matrix R_expected\n    R_expected\n    ```", "```py\n    array([[3.7],\n           [0.5],\n           [8.9]])\n    ```", "```py\n    # define the discount factor\n    gamma = 0.9\n    # Now it is possible to solve the Bellman Equation\n    A = np.eye(n_states) - gamma * P\n    B = R_expected\n    # solve using scipy linalg\n    V = linalg.solve(A, B)\n    V\n    ```", "```py\n    array([[65.540732  ],\n           [64.90791027],\n           [77.5879575 ]])\n    ```", "```py\nimport numpy as np\nfrom scipy import linalg\n```", "```py\nn_states = 6\n# transition matrix together with policy\nP_pi = np.zeros((n_states, n_states))\nR = np.zeros_like(P_pi)\n```", "```py\nP_pi[0, 1] = 0.5\nP_pi[0, 3] = 0.5\nP_pi[1, 2] = 0.5\nP_pi[1, 5] = 0.5\nP_pi[2, 4] = 0.5\nP_pi[2, 5] = 0.5\nP_pi[4, 5] = 0.5\nP_pi[4, 0] = 0.5\nP_pi[3, 0] = 0.5\nP_pi[3, 3] = 0.5\nP_pi[5, 5] = 1\n```", "```py\nP_pi\n```", "```py\narray([[0\\. , 0.5, 0\\. , 0.5, 0\\. , 0\\. ],\n       [0\\. , 0\\. , 0.5, 0\\. , 0\\. , 0.5],\n       [0\\. , 0\\. , 0\\. , 0\\. , 0.5, 0.5],\n       [0.5, 0\\. , 0\\. , 0.5, 0\\. , 0\\. ],\n       [0.5, 0\\. , 0\\. , 0\\. , 0\\. , 0.5],\n       [0\\. , 0\\. , 0\\. , 0\\. , 0\\. , 1\\. ]])\n```", "```py\nR[0, 1] = -2\nR[0, 3] = -1\nR[1, 2] = -2\nR[1, 5] = 0\nR[2, 4] = 15\nR[2, 5] = 10\nR[4, 5] = 10\nR[4, 0] = -10\nR[3, 3] = -1\nR[3, 0] = -3\n```", "```py\nR\n```", "```py\narray([[  0.,  -2.,   0.,  -1.,   0.,   0.],\n       [  0.,   0.,  -2.,   0.,   0.,   0.],\n       [  0.,   0.,   0.,   0.,  15.,  10.],\n       [ -3.,   0.,   0.,  -1.,   0.,   0.],\n       [-10.,   0.,   0.,   0.,   0.,  10.],\n       [  0.,   0.,   0.,   0.,   0.,   0.]])\n```", "```py\n# check the correctness of P_pi\nassert((np.sum(P_pi, axis=1) == 1).all())\n```", "```py\n# expected reward for each state\nR_expected = np.sum(P_pi * R, axis=1, keepdims=True)\nR_expected\n```", "```py\narray([[-1.5],\n       [-1\\. ],\n       [12.5],\n       [-2\\. ],\n       [ 0\\. ],\n       [ 0\\. ]])\n```", "```py\n# Now it is possible to solve the Bellman Equation\ngamma = 0.9\nA = np.eye(n_states, n_states) - gamma * P_pi\nB = R_expected\n# solve using scipy linalg\nV = linalg.solve(A, B)\nV\n```", "```py\narray([[-1.78587056],\n       [ 4.46226255],\n       [12.13836121],\n       [-5.09753046],\n       [-0.80364175],\n       [ 0\\.       ]])\n```", "```py\ngamma = 0.\nA = np.eye(n_states, n_states) - gamma * P_pi\nB = R_expected\n# solve using scipy linalg\nV_gamma_zero = linalg.solve(A, B)\nV_gamma_zero\n```", "```py\narray([[-1.5],\n       [-1\\. ],\n       [12.5],\n       [-2\\. ],\n       [ 0\\. ],\n       [ 0\\. ]])\n```", "```py\nR_sa = np.zeros((n_states*2, 1))\nR_sa[0] = -2 # study in state 0\nR_sa[1] = -1 # social in state 0\nR_sa[2] = -2 # study in state 1\nR_sa[3] = 0 # sleep in state 1\nR_sa[4] = 10 # sleep in state 2\nR_sa[5] = +15 # beer in state 2\nR_sa[6] = -1 # social in state 3 (social)\nR_sa[7] = -3 # study in state 3 (social)\nR_sa[8] = 10 # sleep in state 4 (pub)\nR_sa[9] = -10 # study in state 4 (pub)\nR_sa.shape\n```", "```py\n(10, 1)\n```", "```py\n# Transition Matrix (states x action, states)\nP = np.zeros((n_states*2, n_states))\nP[0, 1] = 1 # study in state 0 -> state 1\nP[1, 3] = 1 # social in state 0 -> state 3\nP[2, 2] = 1 # study in state 1 -> state 2\nP[3, 5] = 1 # sleep in state 1 -> state 5 (bed)\nP[4, 5] = 1 # sleep in state 2 -> state 5 (bed)\nP[5, 4] = 1 # beer in state 2 -> state 4 (pub)\nP[6, 3] = 1 # social in state 3 -> state 3 (social)\nP[7, 0] = 1 # study in state 3 -> state 0 (Class 1)\nP[8, 5] = 1 # sleep in state 4 -> state 5 (bed)\nP[9, 0] = 1 # study in state 4 -> state 0 (class 1)\n```", "```py\ngamma = 0.9\nQ_sa_pi = R_sa + gamma * P @ V\nQ_sa_pi\n```", "```py\narray([[  2.01603629],\n       [ -5.58777741],\n       [  8.92452509],\n       [  0\\.        ],\n       [ 10\\.        ],\n       [ 14.27672242],\n       [ -5.58777741],\n       [ -4.60728351],\n       [ 10\\.        ],\n       [-11.60728351]])\n```", "```py\n\"\"\"\nreshape the column so that we obtain a vector with shape (n_states, n_actions)\n\"\"\"\nn_actions = 2\nQ_sa_pi2 = np.reshape(Q_sa_pi, (-1, n_actions))\nQ_sa_pi2\n```", "```py\narray([[  2.01603629,  -5.58777741],\n       [  8.92452509,   0\\.        ],\n       [ 10\\.        ,  14.27672242],\n       [ -5.58777741,  -4.60728351],\n       [ 10\\.        , -11.60728351]])\n```", "```py\nbest_actions = np.reshape(np.argmax(Q_sa_pi2, -1), (-1, 1))\nbest_actions\n```", "```py\narray([[0],\n       [0],\n       [1],\n       [1],\n       [0]])\n```", "```py\nQ_sa_pi_gamma_zero = R_sa\nQ_sa_pi_gamma_zero\n```", "```py\narray([[ -2.],\n       [ -1.],\n       [ -2.],\n       [  0.],\n       [ 10.],\n       [ 15.],\n       [ -1.],\n       [ -3.],\n       [ 10.],\n       [-10.]])\n```", "```py\nn_actions = 2\nQ_sa_pi_gamma_zero2 = np.reshape(Q_sa_pi_gamma_zero, \\\n                                 (-1, n_actions))\nQ_sa_pi_gamma_zero2\n```", "```py\narray([[ -2.,  -1.],\n       [ -2.,   0.],\n       [ 10.,  15.],\n       [ -1.,  -3.],\n       [ 10., -10.]])\n```", "```py\nbest_actions_gamma_zero = np.reshape(np.argmax\\\n                                     (Q_sa_pi_gamma_zero2, -1), \\\n                                     (-1, 1))\nbest_actions_gamma_zero\n```", "```py\narray([[1],\n       [1],\n       [1],\n       [0],\n       [0]])\n```", "```py\n    import numpy as np\n    import scipy.optimize\n    ```", "```py\n    # number of states and number of actions\n    n_states = 3\n    n_actions = 2\n    ```", "```py\n    # initial state distribution\n    mu = np.array([[1, 0, 0]]).T # only state 1\n    mu\n    ```", "```py\n    array([[1],\n           [0],\n           [0]])\n    ```", "```py\n    # Build the upper bound coefficients for the action A\n    # define the reward matrix for action A\n    R_A = np.zeros((n_states, 1), np.float)\n    R_A[0, 0] = 1\n    R_A[1, 0] = 0\n    R_A[2, 0] = 0\n    R_A\n    ```", "```py\n    array([[1.],\n           [0.],\n           [0.]])\n    ```", "```py\n    # Define the transition matrix for action A\n    P_A = np.zeros((n_states, n_states), np.float)\n    P_A[0, 1] = 1\n    P_A[1, 0] = 1\n    P_A[2, 1] = 1\n    P_A\n    ```", "```py\n    array([[0., 1., 0.],\n           [1., 0., 0.],\n           [0., 1., 0.]])\n    ```", "```py\n    gamma = 0.9\n    # Upper bound A matrix for action A\n    A_up_A = gamma * P_A - np.eye(3,3)\n    A_up_A\n    ```", "```py\n    array([[-1\\. ,  0.9,  0\\. ],\n           [ 0.9, -1\\. ,  0\\. ],\n           [ 0\\. ,  0.9, -1\\. ]])\n    ```", "```py\n    # The same for action B\n    # define the reward matrix for action B\n    R_B = np.zeros((n_states, 1), np.float)\n    R_B[0, 0] = 10\n    R_B[1, 0] = 1\n    R_B[2, 0] = 10\n    # Define the transition matrix for action B\n    P_B = np.zeros((n_states, n_states), np.float)\n    P_B[0, 2] = 1\n    P_B[1, 2] = 1\n    P_B[2, 2] = 1\n    # Upper bound A matrix for action B\n    A_up_B = gamma * P_B - np.eye(3,3)\n    A_up_B\n    ```", "```py\n    array([[-1\\. ,  0\\. ,  0.9],\n           [ 0\\. , -1\\. ,  0.9],\n           [ 0\\. ,  0\\. , -0.1]])\n    ```", "```py\n    # Upper bound matrix for all actions and all states\n    A_up = np.vstack((A_up_A, A_up_B))\n    \"\"\"\n    verify the shape: number of constraints are equal to |actions| * |states|\n    \"\"\"\n    assert(A_up.shape[0] == n_states * n_actions)\n    # Reward vector is obtained by stacking the two vectors\n    R = np.vstack((R_A, R_B))\n    ```", "```py\n    c = mu\n    b_up = -R\n    # Solve the linear program\n    res = scipy.optimize.linprog(c, A_up, b_up)\n    ```", "```py\n    # Obtain the results: state values\n    V_ = res.x\n    V_\n    V = V_.reshape((-1, 1))\n    V\n    np.savetxt(\"solution/V.txt\", V)\n    ```", "```py\n    \"\"\"\n    transition matrix. On the rows, we have states and actions, and on the columns, we have the next states\n    \"\"\"\n    P = np.vstack((P_A, P_B))\n    P\n    ```", "```py\n    array([[0., 1., 0.],\n           [1., 0., 0.],\n           [0., 1., 0.],\n           [0., 0., 1.],\n           [0., 0., 1.],\n           [0., 0., 1.]])\n    ```", "```py\n    \"\"\"\n    Use the action value formula to calculate the action values for each state action pair.\n    \"\"\"\n    Q_sa = R + gamma * P.dot(V)\n    \"\"\"\n    The first three rows are associated to action A, the last three are associated # to action B\n    \"\"\"\n    Q_sa\n    ```", "```py\n    array([[ 88.32127683],\n           [ 89.99999645],\n           [ 87.32127683],\n           [100.00000622],\n           [ 91.00000622],\n           [100.00000622]])\n    ```", "```py\n    Q_sa_2 = np.stack((Q_sa[:3, 0], Q_sa[3:, 0]), axis=1)\n    Q_sa_2\n    ```", "```py\n    array([[ 88.32127683, 100.00000622],\n           [ 89.99999645,  91.00000622],\n           [ 87.32127683, 100.00000622]])\n    ```", "```py\n    best_actions = np.reshape(np.argmax(Q_sa_2, axis=1), (3, 1))\n    best_actions\n    ```", "```py\n    array([[1],\n           [1],\n           [1]])\n    ```"]