<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Finding Spans in Text &#x2013; Chunking"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Finding Spans in Text – Chunking</h1></div></div></div><p>This chapter covers the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Sentence detection</li><li class="listitem" style="list-style-type: disc">Evaluation of sentence detection</li><li class="listitem" style="list-style-type: disc">Tuning sentence detection</li><li class="listitem" style="list-style-type: disc">Marking embedded chunks in a string – sentence chunk example</li><li class="listitem" style="list-style-type: disc">Paragraph detection</li><li class="listitem" style="list-style-type: disc">Simple noun phrases and verb phrases</li><li class="listitem" style="list-style-type: disc">Regular expression-based chunking for NER</li><li class="listitem" style="list-style-type: disc">Dictionary-based chunking for NER</li><li class="listitem" style="list-style-type: disc">Translating between word tagging and chunks – BIO codec</li><li class="listitem" style="list-style-type: disc">HMM-based NER</li><li class="listitem" style="list-style-type: disc">Mixing the NER sources</li><li class="listitem" style="list-style-type: disc">CRFs for chunking</li><li class="listitem" style="list-style-type: disc">NER using CRFs with better features</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec55"/>Introduction</h1></div></div></div><p>This chapter will tell us how to work with spans of text that typically cover one or more words/tokens. The LingPipe API represents this unit of text as a chunk with corresponding chunkers that produce chunkings. The following is some text with character offsets indicated:</p><div class="informalexample"><pre class="programlisting">LingPipe is an API. It is written in Java.
012345678901234567890123456789012345678901
          1         2         3         4           </pre></div><p>Chunking the preceding text into sentences will give us the following output:</p><div class="informalexample"><pre class="programlisting">Sentence start=0, end=18
Sentence start =20, end=41</pre></div><p>Adding in a chunking for named entities adds entities for LingPipe and Java:</p><div class="informalexample"><pre class="programlisting">Organization start=0, end=7
Organization start=37, end=40</pre></div><p>We can define the named-entity chunkings with respect to their offsets from the sentences that contain them; this will make no difference to LingPipe, but Java will be:</p><div class="informalexample"><pre class="programlisting">Organization start=17, end=20</pre></div><p>This is the basic idea of chunks. There are lots of ways to make them.</p></div></div>
<div class="section" title="Sentence detection"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec56"/>Sentence detection</h1></div></div></div><p>Sentences in written text roughly <a class="indexterm" id="id429"/>correspond to a spoken utterance. They are the standard unit of processing words in industrial applications. In almost all mature NLP applications, sentence detection is a part of the processing pipeline even in the case of tweets, which can have more than one sentence in the allotted 140 characters.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec127"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">As usual, we will play with some data first. Enter the following command in the console:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.SentenceDetection</strong></span>
</pre></div></li><li class="listitem">The program will provide a prompt for your sentence-detection experimentation. A new line / return terminates the text to be analyzed:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Enter text followed by new line</strong></span>
<span class="strong"><strong>&gt;A sentence. Another sentence.</strong></span>
<span class="strong"><strong>SENTENCE 1:</strong></span>
<span class="strong"><strong>A sentence.</strong></span>
<span class="strong"><strong>SENTENCE 2:</strong></span>
<span class="strong"><strong>Another sentence.</strong></span>
</pre></div></li><li class="listitem">It is worth playing around a bit with different inputs. The following are some examples that explore the properties of the sentence detector. Drop the capitalized beginning of a sentence; this will prevent the detection of the second sentence:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;A sentence. another sentence.</strong></span>
<span class="strong"><strong>SENTENCE 1:</strong></span>
<span class="strong"><strong>A sentence. another sentence.</strong></span>
</pre></div></li><li class="listitem">The detector does not require a final period—this is configurable:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;A sentence. Another sentence without a final period</strong></span>
<span class="strong"><strong>SENTENCE 1:A sentence.</strong></span>
<span class="strong"><strong>SENTENCE 2:Another sentence without a final period</strong></span>
</pre></div></li><li class="listitem">The detector balances parentheses, which will not allow sentences to break inside parentheses—this is also configurable:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;(A sentence. Another sentence.)</strong></span>
<span class="strong"><strong>SENTENCE 1: (A sentence. Another sentence.)</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec128"/>How it works...</h2></div></div></div><p>This sentence detector<a class="indexterm" id="id430"/> is <a class="indexterm" id="id431"/>a heuristic-based or rule-based sentence detector. A statistical sentence detector would be a reasonable approach as well. We will get through the entire source to run the detector, and later, we will discuss the modifications:</p><div class="informalexample"><pre class="programlisting">package com.lingpipe.cookbook.chapter5;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.Chunking;
import com.aliasi.sentences.IndoEuropeanSentenceModel;
import com.aliasi.sentences.SentenceChunker;
import com.aliasi.sentences.SentenceModel;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.Set;

public class SentenceDetection {

public static void main(String[] args) throws IOException {
  boolean endSent = true;
  boolean parenS = true;
  SentenceModel sentenceModel = new IndoEuropeanSentenceModel(endSent,parenS);</pre></div><p>Working from the top of the <code class="literal">main</code> class, the Boolean <code class="literal">endSent</code> parameter controls whether the string that is sentence detected is assumed to end with a sentence, no matter what—this means that the last character is a sentence boundary always—it does not need to be a period or other typical sentence-ending mark. Change it and try a sentence without a final period, and the result will be that no sentence is detected.</p><p>The next Boolean <code class="literal">parenS</code> declaration gives priority to parentheses over sentence makers when finding sentences. Next, the actual sentence chunker will be set up:</p><div class="informalexample"><pre class="programlisting">TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
Chunker sentenceChunker = new SentenceChunker(tokFactory,sentenceModel);</pre></div><p>The <code class="literal">tokFactory</code> should be<a class="indexterm" id="id432"/> familiar to you from <a class="link" href="ch02.html" title="Chapter 2. Finding and Working with Words">Chapter 2</a>, <span class="emphasis"><em>Finding and Working with Words</em></span>. The <code class="literal">sentenceChunker</code> then can be constructed. Following is the standard I/O code for command-line interaction:</p><div class="informalexample"><pre class="programlisting">BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
while (true) {
  System.out.print("Enter text followed by new line\n&gt;");
  String text = reader.readLine();</pre></div><p>Once we have the text, then the sentence detector is applied:</p><div class="informalexample"><pre class="programlisting">Chunking chunking = sentenceChunker.chunk(text);
Set&lt;Chunk&gt; sentences = chunking.chunkSet();</pre></div><p>The chunking provides a <code class="literal">Set&lt;Chunk&gt;</code> parameter, which will noncontractually provide an appropriate ordering of <code class="literal">Chunks</code>; they will be added as per the <code class="literal">ChunkingImpl</code> Javadoc. The truly paranoid programmer might impose the proper sort order, which we will cover later in the chapter when we have to handle overlapping chunks.</p><p>Next, we will check to see if any sentences were found, and if we don't find them, we will report to the console:</p><div class="informalexample"><pre class="programlisting">if (sentences.size() &lt; 1) {
  System.out.println("No sentence chunks found.");
  return;
}</pre></div><p>The following is the first exposure to the <code class="literal">Chunker</code> interface in the book, and a few comments are in order. The <code class="literal">Chunker</code> interface generates the <code class="literal">Chunk</code> objects, which are typed and scored contiguous-character sequences over <code class="literal">CharSequence</code>—usually, <code class="literal">String</code>. <code class="literal">Chunks</code> can overlap. The <code class="literal">Chunk</code> objects are stored in <code class="literal">Chunking</code>:</p><div class="informalexample"><pre class="programlisting">String textStored = chunking.charSequence().toString();
for (Chunk sentence : sentences) {
  int start = sentence.start();
  int end = sentence.end();
  System.out.println("SENTENCE :" 
    + textStored.substring(start,end));
  }
}</pre></div><p>First, we recovered the underlying text string <code class="literal">textStored</code> that the chunks are based on. It is the same string as <code class="literal">text</code>, but we wanted to illustrate this potentially useful method of the <code class="literal">Chunking</code> class, which can come up in recursive or other contexts when the chunking is far removed from where <code class="literal">CharSequence</code> that it uses is unavailable.</p><p>The remaining <code class="literal">for</code> loop<a class="indexterm" id="id433"/> iterates over the sentences and prints them out with the <code class="literal">substring()</code> method of <code class="literal">String</code>.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec129"/>There's more...</h2></div></div></div><p>Before moving on to how to roll your own sentence detector, it is worth mentioning that LingPipe has<a class="indexterm" id="id434"/> <code class="literal">MedlineSentenceModel</code>, which is oriented towards the kind of sentences found in the medical research literature. It has seen a lot of data and should be a starting place for your own sentence-detection efforts over these kinds of data.</p><div class="section" title="Nested sentences"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec17"/>Nested sentences</h3></div></div></div><p>Sentences, particularly in literature, can contain <a class="indexterm" id="id435"/>nested sentences. Consider the following:</p><div class="informalexample"><pre class="programlisting">John said "this is a nested sentence" and then shut up.</pre></div><p>The preceding sentence will be marked up properly as:</p><div class="informalexample"><pre class="programlisting">[John said "[this is a nested sentence]" and then shut up.]</pre></div><p>This sort of nesting is different from a linguist's concept of a nested sentence, which is based on grammatical role. Consider the following example:</p><div class="informalexample"><pre class="programlisting">[[John ate the gorilla] and [Mary ate the burger]].</pre></div><p>This sentence consists of two linguistically complete sentences joined by <code class="literal">and</code>. The difference between the two is that the former is determined by punctuation and the latter by a grammatical function. Whether this distinction is significant or not can be debated. However, the former case is much easier to recognize programmatically.</p><p>However, we have rarely needed to model nested sentences in industrial contexts, but we took it on in our MUC-6 system and various coreference resolution systems in research contexts. This is beyond the scope of a recipe book, but be aware of the issue. LingPipe has no out-of-the-box capabilities for nested sentence detection.</p></div></div></div>
<div class="section" title="Evaluation of sentence detection"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec57"/>Evaluation of sentence detection</h1></div></div></div><p>Like most of the things we do, we<a class="indexterm" id="id436"/> want to be able to evaluate the performance of our components. Sentence detection is no different. Sentence detection is a span annotation that differs from our previous evaluations for classifiers and tokenization. As text can have characters that are not in any sentence, there is a notion of sentence start and sentence end. An example of characters that don't belong in a sentence will be JavaScript from an HTML page.</p><p>The following recipe will take you through the steps of creating evaluation data and running it past an evaluation class.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec130"/>How to do it...</h2></div></div></div><p>Perform the following steps to evaluate sentence detection:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open a text editor and copy and paste some literary gem that you want to evaluate sentence detection with, or you can go with our supplied default text, which is used if you don't provide your own data. It is easiest if you stick to plain text.</li><li class="listitem">Insert balanced <code class="literal">[</code> and <code class="literal">]</code> to indicate the beginnings and ends of sentences in the text. If the text already contains either <code class="literal">[</code> or <code class="literal">]</code>, pick another character that is not in the text as a sentence delimiter—curly brackets or slashes are a good choice. If you use different delimiters, you will have to modify the source appropriately and recreate the JAR file. The code assumes a single-character text delimiter. An example of a sentence-annotated text from <span class="emphasis"><em>The Hitchhiker's Guide to the Galaxy</em></span> is as follows—note that not every character is in a sentence; some whitespaces are between sentences:<div class="informalexample"><pre class="programlisting">[The Guide says that the best drink in existence is the Pan Galactic Gargle Blaster.] [It says that the effect of a Pan Galactic Gargle Blaster is like having your brains smashed out by a slice of lemon wrapped round a large gold brick.]</pre></div></li><li class="listitem">Get yourself a command line and run the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.EvaluateAnnotatedSentences</strong></span>
<span class="strong"><strong>TruePos: 0-83/The Guide says that the best drink in existence is the Pan Galactic Gargle Blaster.:S</strong></span>
<span class="strong"><strong>TruePos: 84-233/It says that the effect of a Pan Galactic Gargle Blaster is like having your brains smashed out by a slice of lemon wrapped round a large gold brick.:S</strong></span>
</pre></div></li><li class="listitem">For this data, the code will display two sentences that match perfectly with the sentences annotated with <code class="literal">[]</code>, as indicated by the <code class="literal">TruePos</code> label.</li><li class="listitem">A good exercise is to modify the annotation a bit to force errors. We will move the first sentence boundary one character in:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>T[he Guide says that the best drink in existence is the Pan Galactic Gargle Blaster.] [It says that the effect of a Pan Galactic Gargle Blaster is like having your brains smashed out by a slice of lemon wrapped round a large gold brick.]</strong></span>
</pre></div></li><li class="listitem">Rerunning the modified annotation file after saving it yields:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>TruePos: 84-233/It says that the effect of a Pan Galactic Gargle Blaster is like having your brains smashed out by a slice of lemon wrapped round a large gold brick.:S</strong></span>
<span class="strong"><strong>FalsePos: 0-83/The Guide says that the best drink in existence is the Pan Galactic Gargle Blaster.:S</strong></span>
<span class="strong"><strong>FalseNeg: 1-83/he Guide says that the best drink in existence is the Pan Galactic Gargle Blaster.:S</strong></span>
</pre></div><p>By changing the truth annotation, a false negative is produced, because the sentence span was missed by one character. In addition, a false positive is created by the sentence detector that recognizes the 0-83 character sequence.</p></li><li class="listitem">It is a good idea to play around with the annotation and various kinds of data to get a feel of how evaluation works and the capabilities of the sentence detector.</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec131"/>How it works...</h2></div></div></div><p>The class starts by <a class="indexterm" id="id437"/>digesting the annotated text and storing the sentence chunks in an evaluation object. Then, the sentence detector is created, just as we did in the previous recipe. The code finishes by applying the created sentence detector to the text, and the results are printed.</p><div class="section" title="Parsing annotated data"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec18"/>Parsing annotated data</h3></div></div></div><p>Given text annotated with <code class="literal">[]</code> for sentence <a class="indexterm" id="id438"/>boundaries means that the correct offsets of the sentences have to be recovered, and the original unannotated text must be created, that is, without any <code class="literal">[]</code>. Span parsers can be a bit tricky to code, and the following is offered for simplicity rather than efficiency or proper coding technique:</p><div class="informalexample"><pre class="programlisting">String path = args.length &gt; 0 ? args[0] 
             : "data/hitchHikersGuide.sentDetected";
char[] chars 
  = Files.readCharsFromFile(new File(path), Strings.UTF8);
StringBuilder rawChars = new StringBuilder();
int start = -1;
int end = 0;
Set&lt;Chunk&gt; sentChunks = new HashSet&lt;Chunk&gt;();</pre></div><p>The preceding code reads in the entire file as a single <code class="literal">char[]</code> array with an appropriate character encoding. Also, note that for large files, a streaming approach will be more memory friendly. Next, an accumulator for unannotated chars is setup as a <code class="literal">StringBuilder</code> object with the <code class="literal">rawChars</code> variable. All characters encountered that are not either a <code class="literal">[</code> or <code class="literal">]</code> will be appended to the object. The remaining code sets up counters for sentence starts and ends that are indexed into the unannotated character array and an accumulator for <code class="literal">Set&lt;Chunk&gt;</code> for annotated sentence segments.</p><p>The following <code class="literal">for</code> loop advances one character at a time over the annotated character sequence:</p><div class="informalexample"><pre class="programlisting">for (int i=0; i &lt; chars.length; ++i) {
  if (chars[i] == '[') {
    start = rawChars.length();
  }
  else if (chars[i] == ']') {
    end = rawChars.length();
    
    Chunk chunk = ChunkFactory.createChunk(start,end, SentenceChunker.SENTENCE_CHUNK_TYPE);
    sentChunks.add(chunk);}
  else {
    rawChars.append(chars[i]);
  }
}
String originalText = rawChars.toString();</pre></div><p>The first <code class="literal">if (chars[i] == '[')</code> tests for starts of sentences in the annotation and sets the start variable<a class="indexterm" id="id439"/> to the length of <code class="literal">rawChars</code>. The iteration variable <code class="literal">i</code> includes the length added by the annotations. The corresponding <code class="literal">else if (chars[i] == ']')</code> statement handles the end of sentence case. Note that there are no error checks for this parser—this is a very bad idea because annotation errors are very likely if entered with a text editor. However, this is motivated by keeping the code as simple as possible. Later in the recipe, we will provide an example with some minimal error checking. Once the end of a sentence is found, a chunk is created for the sentence with <code class="literal">ChunkFactory.createChunk</code> with offsets and for the standard LingPipe sentence type <code class="literal">SentenceChunker.SENTENCE_CHUNK_TYPE</code>, which is required for the upcoming evaluation classes to work properly.</p><p>The remaining <code class="literal">else</code> statement applies for all the characters that are not sentence boundaries, and it simply adds the character to the <code class="literal">rawChars</code> accumulator. The result of this accumulator can be seen outside the <code class="literal">for</code> loop when <code class="literal">String unannotatedText</code> is created. Now, we have sentence chunks indexed correctly into a text string. Next, we will create a proper <code class="literal">Chunking</code> object:</p><div class="informalexample"><pre class="programlisting">ChunkingImpl sentChunking = new ChunkingImpl(unannotatedText);
for (Chunk chunk : sentChunks) {
  sentChunking.add(chunk);
}</pre></div><p>The <code class="literal">ChunkingImpl</code> implementing class (<code class="literal">Chunking</code> is an interface) requires the underlying text on construction, which is why we didn't just populate it in the preceding loop. LingPipe generally tries to make object construction complete. If Chunkings can be created without the underlying <code class="literal">CharSequence</code> method, then what will be returned when the <code class="literal">charSequence()</code> method is called? An empty string is actively misleading. Alternatively, returning <code class="literal">null</code> needs to be caught and dealt with. Better to just force the object to make sense of construction.</p><p>Moving on, we will see the standard configuration of the sentence chunker from the previous recipe:</p><div class="informalexample"><pre class="programlisting">boolean eosIsSentBoundary = false;
boolean balanceParens = true;
SentenceModel sentenceModel = new IndoEuropeanSentenceModel(eosIsSentBoundary, balanceParens);
TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
SentenceChunker sentenceChunker = new SentenceChunker(tokFactory,sentenceModel);</pre></div><p>The interesting <a class="indexterm" id="id440"/>stuff follows with an evaluator that takes <code class="literal">sentenceChunker</code> as being evaluated as a parameter:</p><div class="informalexample"><pre class="programlisting">SentenceEvaluator evaluator = new SentenceEvaluator(sentenceChunker);</pre></div><p>Next up, the <code class="literal">handle(sentChunking)</code> method will take the text we just parsed into <code class="literal">Chunking</code> and run the sentence detector on <code class="literal">CharSequence</code> supplied in <code class="literal">sentChunking</code> and set up the evaluation:</p><div class="informalexample"><pre class="programlisting">evaluator.handle(sentChunking);</pre></div><p>Then, we will just get the evaluation data and work our way through the differences between the truth sentence detection and what the system did:</p><div class="informalexample"><pre class="programlisting">SentenceEvaluation eval = evaluator.evaluation();
ChunkingEvaluation chunkEval = eval.chunkingEvaluation();
for (ChunkAndCharSeq truePos : chunkEval.truePositiveSet()) {
  System.out.println("TruePos: " + truePos);
}
for (ChunkAndCharSeq falsePos : chunkEval.falsePositiveSet()) {
  System.out.println("FalsePos: " + falsePos);
}
for (ChunkAndCharSeq falseNeg : chunkEval.falseNegativeSet()){
  System.out.println("FalseNeg: " + falseNeg);
}</pre></div><p>This recipe does not cover all the evaluation methods—check out the Javadoc—but it does provide what a sentence detection tuner will likely be most in need of; this is a listing of what the sentence detector got right (true positives), sentences it found but were wrong (false positives), and sentences it missed (false negatives). Note that true negatives don't make much sense <a class="indexterm" id="id441"/>in span annotations, because they will be the set of all the possible spans that are not in the truth sentence detection.</p></div></div></div>
<div class="section" title="Tuning sentence detection"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec58"/>Tuning sentence detection</h1></div></div></div><p>Lots of data will resist the<a class="indexterm" id="id442"/> charms of <code class="literal">IndoEuropeanSentenceModel</code>, so this recipe will provide a starting place to modify sentence detection to meet new kinds of sentences. Unfortunately, this is a very open-ended area of system building, so we will focus on techniques rather than likely formats for sentences.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec132"/>How to do it...</h2></div></div></div><p>This recipe will follow a well-worn pattern: create evaluation data, set up evaluation, and start hacking. Here we go:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Haul out your favorite text editor and mark up some data—we will stick to the <code class="literal">[</code> and <code class="literal">]</code> markup approach. The following is an example that runs afoul of our standard <code class="literal">IndoEuropeanSentenceModel</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[All decent people live beyond their incomes nowadays, and those who aren't respectable live beyond other people's.]  [A few gifted individuals manage to do both.]</strong></span>
</pre></div></li><li class="listitem">We will put the preceding sentence in <code class="literal">data/saki.sentDetected.txt</code> and run it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.EvaluateAnnotatedSentences data/saki.sentDetected </strong></span>
<span class="strong"><strong>FalsePos: 0-159/All decent people live beyond their incomes nowadays, and those who aren't respectable live beyond other people's.  A few gifted individuals manage to do both.:S</strong></span>
<span class="strong"><strong>FalseNeg: 0-114/All decent people live beyond their incomes nowadays, and those who aren't respectable live beyond other people's.:S</strong></span>
<span class="strong"><strong>FalseNeg: 116-159/A few gifted individuals manage to do both.:S</strong></span>
</pre></div></li></ol></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec133"/>There's more...</h2></div></div></div><p>The single false positive corresponds to the one sentence found, and the two false negatives are the two sentences not found that we annotated here. What happened? The sentence model missed <code class="literal">people's.</code> as a sentence end. If the apostrophe is removed, the sentence is detected properly—what is going on?</p><p>First, let's look at the code running in the background. <code class="literal">IndoEuropeanSentenceModel</code> extends <code class="literal">HeuristicSentenceModel</code> by configuring several categories of tokens from the Javadoc for <code class="literal">HeuristicSentenceModel</code>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Possible stops</strong></span>: These are<a class="indexterm" id="id443"/> tokens that are allowed to be the final ones in a sentence. This set typically includes sentence-final punctuation tokens, such as periods (.) and double quotes (").</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Impossible penultimates</strong></span>: These <a class="indexterm" id="id444"/>are tokens that might not be the penultimate (second-to-last) token in a sentence. This set is typically made up of abbreviations or acronyms, such as <code class="literal">Mr</code>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Impossible starts</strong></span>: These are<a class="indexterm" id="id445"/> tokens that might not be the first ones in a sentence. This set typically includes punctuation characters that should be attached to the previous sentence, such as end quotes ('').</li></ul></div><p>
<code class="literal">IndoEuropeanSentenceModel</code> is not configurable, but from the Javadoc, it is clear that all single characters are considered impossible penultimates. The words <code class="literal">people's</code> is tokenized into <code class="literal">people</code>, <code class="literal">'</code>, <code class="literal">s</code>, and <code class="literal">.</code>. The single character <code class="literal">s</code> is penultimate to the <code class="literal">.</code> and is thus blocked. How to fix this?</p><p>A few options present themselves:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Ignore the mistake assuming that it won't happen frequently</li><li class="listitem" style="list-style-type: disc">Fix by creating a custom sentence model</li><li class="listitem" style="list-style-type: disc">Fix by modifying the tokenizer to not separate apostrophes</li><li class="listitem" style="list-style-type: disc">Write a complete sentence-detection model for the interface</li></ul></div><p>The second option, create a custom sentence model, is handled most easily by copying the source from <code class="literal">IndoEuropeanSentenceModel</code> into a new class and modifying it, as the relevant data structures are private. This is done to simplify the serialization of the class—very little configuration needs to be written to disk. In the example classes, there is a <code class="literal">MySentenceModel.java</code> file that differs by obvious changes in the package name and imports:</p><div class="informalexample"><pre class="programlisting">IMPOSSIBLE_PENULTIMATES.add("R");
//IMPOSSIBLE_PENULTIMATES.add("S"); breaks on "people's."
//IMPOSSIBLE_PENULTIMATES.add("T"); breaks on "didn't."
IMPOSSIBLE_PENULTIMATES.add("U");</pre></div><p>The preceding code just comments out two of the likely single-letter cases of penultimate tokens that are a single-word character. To see it at work, change the sentence model to <code class="literal">SentenceModel sentenceModel = new MySentenceModel();</code> in the <code class="literal">EvaluateAnnotatedSentences.java</code> class and recompile and run it.</p><p>If you see the preceding code as a reasonable balancing of finding sentences that end in likely contractions versus non-sentence cases such as <code class="literal">[Hunter S. Thompson is a famous fellow.]</code>, which will detect <code class="literal">S.</code> as a sentence boundary.</p><p>Extending <code class="literal">HeuristicSentenceModel</code> can work well for many sorts of data. Mitzi Morris built <code class="literal">MedlineSentenceModel.java</code>, which is designed to work well with the abstracts provided in the MEDLINE research index.</p><p>One way to look at the preceding problem is that contractions should not be broken up into tokens for the purpose of sentence detection. <code class="literal">IndoEuropeanTokenizerFactory</code> should be tuned up to keep "people's" and other contractions together. While it initially seems slightly better that the first solution, it might well run afoul of the fact that <code class="literal">IndoEuropeanSentenceModel</code> was tuned with a particular tokenization in mind, and the consequences of the change are unknown in the absence of an evaluation corpus.</p><p>The other option <a class="indexterm" id="id446"/>is to write a completely novel sentence-detection class that supports the <code class="literal">SentenceModel</code> interface. Faced with a highly novel data collection such as Twitter feeds, we will consider using a machine-learning-driven span-annotation technique such as HMMs or CRFs covered in <a class="link" href="ch04.html" title="Chapter 4. Tagging Words and Tokens">Chapter 4</a>, <span class="emphasis"><em>Tagging Words and Tokens</em></span>, and at the end of this chapter.</p></div></div>
<div class="section" title="Marking embedded chunks in a string &#x2013; sentence chunk example"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec59"/>Marking embedded chunks in a string – sentence chunk example</h1></div></div></div><p>The method of displaying <a class="indexterm" id="id447"/>chunkings in the previous recipes is not well suited <a class="indexterm" id="id448"/>for applications that need to modify the <a class="indexterm" id="id449"/>underlying string. For example, a sentiment analyzer might want to highlight only sentences that are strongly positive and not mark up the remaining sentences while still displaying the entire text. The slight complication in producing the marked-up text is that adding markups changes the underlying string. This recipe provides working code to insert the chunking by adding chunks in reverse.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec134"/>How to do it...</h2></div></div></div><p>While this recipe may not be technically complex it is useful to get span annotations into a text without out having to invent the code from whole cloth. The <code class="literal">src/com/lingpipe/coobook/chapter5/WriteSentDetectedChunks</code> class has the referenced code:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The sentence chunking is created as per the first sentence-detection recipe. The following code extracts the chunks as <code class="literal">Set&lt;Chunk&gt;</code> and then sorts them by <code class="literal">Chunk.LONGEST_MATCH_ORDER_COMPARITOR</code>. In the Javadoc, the comparator is defined as:<div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>Compares two chunks based on their text position. A chunk is greater if it starts later than another chunk, or if it starts at the same position and ends earlier.</em></span></p></blockquote></div><p>There is<a class="indexterm" id="id450"/> also <code class="literal">TEXT_ORDER_COMPARITOR</code>, which is as follows:</p><div class="informalexample"><pre class="programlisting">String textStored = chunking.charSequence().toString();
Set&lt;Chunk&gt; chunkSet = chunking.chunkSet();
System.out.println("size: " + chunkSet.size());
Chunk[] chunkArray = chunkSet.toArray(new Chunk[0]);
Arrays.sort(chunkArray,Chunk.LONGEST_MATCH_ORDER_COMPARATOR);</pre></div></li><li class="listitem">Next, we will<a class="indexterm" id="id451"/> iterate over the chunks in the <a class="indexterm" id="id452"/>reverse order, which eliminates having to keep an offset variable for the changing length of the <code class="literal">StringBuilder</code> object. Offset variables are a common source of bugs, so this recipe avoids them as much as possible but does non-standard reverse loop iteration, which might be worse:<div class="informalexample"><pre class="programlisting">StringBuilder output = new StringBuilder(textStored);
int sentBoundOffset = 0;
for (int i = chunkArray.length -1; i &gt;= 0; --i) {
  Chunk chunk = chunkArray[i];
  String sentence = textStored.substring(chunk.start(), chunk.end());
  if (sentence.contains("like")) {
    output.insert(chunk.end(),"}");
    output.insert(chunk.start(),"{");
  }
}
System.out.println(output.toString());</pre></div></li><li class="listitem">The preceding code does a very simple sentiment analysis by looking for the string <code class="literal">like</code> in the sentence and marking that sentence if <code class="literal">true</code>. Note that this code cannot handle overlapping chunks or nested chunks. It assumes a single, non-overlapping chunk set. Some example output is:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.WriteSentDetectedChunks</strong></span>
<span class="strong"><strong>Enter text followed by new line</strong></span>
<span class="strong"><strong>&gt;People like to ski. But sometimes it is terrifying. </strong></span>
<span class="strong"><strong>size: 2</strong></span>
<span class="strong"><strong>{People like to ski.} But sometimes it is terrifying. </strong></span>
</pre></div></li><li class="listitem">To print<a class="indexterm" id="id453"/> nested chunks, look at the <span class="emphasis"><em>Paragraph</em></span><a class="indexterm" id="id454"/><span class="emphasis"><em> detection</em></span> recipe <a class="indexterm" id="id455"/>that follows.</li></ol></div></div></div>
<div class="section" title="Paragraph detection"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec60"/>Paragraph detection</h1></div></div></div><p>The typical containing <a class="indexterm" id="id456"/>structure of a set of sentences is a paragraph. It can be set off explicitly in a markup language such as <code class="literal">&lt;p&gt;</code> in HTML or with two or more new lines, which is how paragraphs are usually rendered. We are in the part of NLP where no hard-and-fast rules apply, so we apologize for the hedging. We will handle some common examples in this chapter and leave it to you to generalize.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec135"/>How to do it...</h2></div></div></div><p>We have never set up an evaluation harness for paragraph detection, but it can be done in ways similar to sentence detection. This recipe, instead, will illustrate a simple paragraph-detection routine that does something very important—maintain offsets into the original document with embedded sentence detection. This attention to detail will serve you well if you ever need to mark up the document in a way that is sensitive to sentences or other subspans of the document, such as named entities. Consider the following example:</p><div class="informalexample"><pre class="programlisting">Sentence 1. Sentence 2
Sentence 3. Sentence 4.</pre></div><p>It gets transformed into the following:</p><div class="informalexample"><pre class="programlisting">{[Sentence 1.] [Sentence 2]}

{[Sentence 3.] [Sentence 4.]
}</pre></div><p>In the preceding snippet, <code class="literal">[]</code>designates sentences, and <code class="literal">{}</code> designates paragraphs. We will jump right into the code on this recipe from <code class="literal">src/com/lingpipe/cookbook/chapter5/ParagraphSentenceDetection.java</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The example code has little to offer in paragraph-detection techniques. It is an open-ended problem, and you will have to use your wiles to solve it. Our paragraph detector is a pathetic <code class="literal">split("\n\n")</code> that, in a more sophisticated approach, will take into account context, characters, and other features that are far too idiosyncratic for us to cover. Here is the beginning of the code that reads the entire document as a string and splits it into an array. Note that <code class="literal">paraSeperatorLength</code> is the number of characters that form the basis of the paragraph split—if the length of the split varies, then that length will have to be associated with the corresponding paragraph:<div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException {
  String document = Files.readFromFile(new File(args[0]), Strings.UTF8);
  String[] paragraphs = document.split("\n\n");
  int paraSeparatorLength = 2;</pre></div></li><li class="listitem">The real point of the recipe is to help with the mechanics of maintaining character offsets into the original document and show embedded processing. This will be done by keeping two separate chunkings: one for paragraphs and one for sentences:<div class="informalexample"><pre class="programlisting">ChunkingImpl paraChunking = new ChunkingImpl(document.toCharArray(),0,document.length());
ChunkingImpl sentChunking = new ChunkingImpl(paraChunking.charSequence());</pre></div></li><li class="listitem">Next, the sentence detector will be set up in the same way as one in the previous recipe:<div class="informalexample"><pre class="programlisting">boolean eosIsSentBoundary = true;
boolean balanceParens = false;
SentenceModel sentenceModel = new IndoEuropeanSentenceModel(eosIsSentBoundary, balanceParens);
SentenceChunker sentenceChunker = new SentenceChunker(IndoEuropeanTokenizerFactory.INSTANCE, sentenceModel);</pre></div></li><li class="listitem">The chunking<a class="indexterm" id="id457"/> iterates over the array of paragraphs and builds a sentence chunking for each paragraph. The somewhat-complicated part of this approach is that the sentence chunk offsets are with respect to the paragraph string, not the entire document. So, the variables' starts and ends are updated with document offsets in the code. Chunks have no methods to adjust starts and ends, so a new chunk must be created, <code class="literal">adjustedSentChunk</code>, with appropriate offsets into the paragraph start and must be added to <code class="literal">sentChunking</code>:<div class="informalexample"><pre class="programlisting">int paraStart = 0;
for (String paragraph : paragraphs) {
  for (Chunk sentChunk : sentenceChunker.chunk(paragraph).chunkSet()) {
    Chunk adjustedSentChunk = ChunkFactory.createChunk(sentChunk.start() + paraStart,sentChunk.end() + paraStart, "S");
    sentChunking.add(adjustedSentChunk);
  }</pre></div></li><li class="listitem">The rest of the loop adds the paragraph chunk and then updates the start of the paragraph with the length of the paragraph plus the length of the paragraph separator. This will complete the creation of correctly offset sentences and paragraphs into the original document string:<div class="informalexample"><pre class="programlisting">paraChunking.add(ChunkFactory.createChunk(paraStart, paraStart + paragraph.length(),"P"));
paraStart += paragraph.length() + paraSeparatorLength;
}</pre></div></li><li class="listitem">The rest of the program is concerned with printing out the paragraphs and sentences with some markup. First, we will create a chunking that has both sentence and paragraph chunks:<div class="informalexample"><pre class="programlisting">String underlyingString = paraChunking.charSequence().toString();
ChunkingImpl displayChunking = new ChunkingImpl(paraChunking.charSequence());
displayChunking.addAll(sentChunking.chunkSet());
displayChunking.addAll(paraChunking.chunkSet());</pre></div></li><li class="listitem">Next, <code class="literal">displayChunking</code> will be sorted by recovering <code class="literal">chunkSet</code>, converting it into an array of chunks and the application of the static comparator:<div class="informalexample"><pre class="programlisting">Set&lt;Chunk&gt; chunkSet = displayChunking.chunkSet();
Chunk[] chunkArray = chunkSet.toArray(new Chunk[0]);
Arrays.sort(chunkArray, Chunk.LONGEST_MATCH_ORDER_COMPARATOR);</pre></div></li><li class="listitem">We will use the <a class="indexterm" id="id458"/>same trick as we did in the <span class="emphasis"><em>Marking embedded chunks in a string – sentence chunk example</em></span> recipe, which is to insert the markup backwards into the string. We will have to keep an offset counter, because nested sentences will extend the finishing paragraph mark placement. The approach assumes that no chunks overlap and that sentences are contained within paragraphs always:<div class="informalexample"><pre class="programlisting">StringBuilder output = new StringBuilder(underlyingString);
int sentBoundOffset = 0;
for (int i = chunkArray.length -1; i &gt;= 0; --i) {
  Chunk chunk = chunkArray[i];
  if (chunk.type().equals("P")) {
    output.insert(chunk.end() + sentBoundOffset,"}");
    output.insert(chunk.start(),"{");
    sentBoundOffset = 0;
  }
  if (chunk.type().equals("S")) {
    output.insert(chunk.end(),"]");
    output.insert(chunk.start(),"[");
    sentBoundOffset += 2;
  }
}
System.out.println(output.toString());</pre></div></li><li class="listitem">That's it for<a class="indexterm" id="id459"/> the recipe.</li></ol></div></div></div>
<div class="section" title="Simple noun phrases and verb phrases"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec61"/>Simple noun phrases and verb phrases</h1></div></div></div><p>This recipe will show you how to find simple <span class="strong"><strong>noun phrases</strong></span> (<span class="strong"><strong>NP</strong></span>)<a class="indexterm" id="id460"/> and <a class="indexterm" id="id461"/>
<span class="strong"><strong>verb phrases</strong></span> (<span class="strong"><strong>VP</strong></span>). By "simple", we mean that there is no complex structure within the phrases. For example, the complex NP "The rain in Spain" will be broken into two simple NP chunks "The rain" and "Spain". These phrases are also called "basal".</p><p>This recipe will not go into the details of how the basal NPs/VPs are calculated but rather how to use the class—it can come in handy, and the source can be included if you want to sort out how it works.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec136"/>How to do it…</h2></div></div></div><p>Like many of the recipes, we will provide a command-line-interactive interface here:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Haul up the command line and type:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.PhraseChunker</strong></span>
<span class="strong"><strong>INPUT&gt; The rain in Spain falls mainly on the plain.</strong></span>
<span class="strong"><strong>The/at rain/nn in/in Spain/np falls/vbz mainly/rb on/in the/at plain/jj ./. </strong></span>
<span class="strong"><strong>  noun(0,8) The rain</strong></span>
<span class="strong"><strong>  noun(12,17) Spain</strong></span>
<span class="strong"><strong>  verb(18,30) falls mainly</strong></span>
<span class="strong"><strong>  noun(34,43) the plain</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec137"/>How it works…</h2></div></div></div><p>The <code class="literal">main()</code> method starts by <a class="indexterm" id="id462"/>deserializing a part-of-speech tagger and then <a class="indexterm" id="id463"/>creating <code class="literal">tokenizerFactory</code>:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException, ClassNotFoundException {
  File hmmFile = new File("models/pos-en-general-brown.HiddenMarkovModel");
  HiddenMarkovModel posHmm = (HiddenMarkovModel) AbstractExternalizable.readObject(hmmFile);
  HmmDecoder posTagger  = new HmmDecoder(posHmm);
  TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;</pre></div><p>Next, <code class="literal">PhraseChunker</code> is constructed, which is a heuristic approach to the problem. Look at the source to see how it works—it scans the input left to right for NP/VP starts and attempts to add to the phrase incrementally:</p><div class="informalexample"><pre class="programlisting">PhraseChunker chunker = new PhraseChunker(posTagger,tokenizerFactory);</pre></div><p>Our standard console I/O code is next:</p><div class="informalexample"><pre class="programlisting">BufferedReader bufReader = new BufferedReader(new InputStreamReader(System.in));
while (true) {
  System.out.print("\n\nINPUT&gt; ");
  String input = bufReader.readLine();</pre></div><p>Then, the input is<a class="indexterm" id="id464"/> tokenized, POS is tagged, and the tokens and tags are <a class="indexterm" id="id465"/>printed out:</p><div class="informalexample"><pre class="programlisting">Tokenizer tokenizer = tokenizerFactory.tokenizer(input.toCharArray(),0,input.length());
String[] tokens = tokenizer.tokenize();
List&lt;String&gt; tokenList = Arrays.asList(tokens);
Tagging&lt;String&gt; tagging = posTagger.tag(tokenList);
for (int j = 0; j &lt; tokenList.size(); ++j) {
  System.out.print(tokens[j] + "/" + tagging.tag(j) + " ");
}
System.out.println();</pre></div><p>The NP/VP chunkings are then calculated and printed out:</p><div class="informalexample"><pre class="programlisting">Chunking chunking = chunker.chunk(input);
CharSequence cs = chunking.charSequence();
for (Chunk chunk : chunking.chunkSet()) {
  String type = chunk.type();
  int start = chunk.start();
  int end = chunk.end();
  CharSequence text = cs.subSequence(start,end);
  System.out.println("  " + type + "(" + start + ","+ end + ") " + text);
  }</pre></div><p>There is a more comprehensive tutorial at <a class="ulink" href="http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html</a>.</p></div></div>
<div class="section" title="Regular expression-based chunking for NER"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec62"/>Regular expression-based chunking for NER</h1></div></div></div><p>
<span class="strong"><strong>Named Entity Recognition</strong></span> (<span class="strong"><strong>NER</strong></span>) is<a class="indexterm" id="id466"/> the process of finding mentions of specific things in text. Consider a simple name; location-named entity recognizer might find <code class="literal">Ford Prefect</code> and <code class="literal">Guildford</code> as the name and location mentions, respectively, in the following text:</p><div class="informalexample"><pre class="programlisting">Ford Prefect used to live in Guildford before he needed to move.</pre></div><p>We will start by building rule-based NER systems and move up to machine-learning methods. Here, we'll take a look at building an NER system that can extract e-mail addresses from text.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec138"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Enter the following <a class="indexterm" id="id467"/>command into the <a class="indexterm" id="id468"/>command prompt:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java –cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter5.RegexNer</strong></span>
</pre></div></li><li class="listitem">Interaction with the program proceeds as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Enter text, . to quit:</strong></span>
<span class="strong"><strong>&gt;Hello,my name is Foo and my email is foo@bar.com or you can also contact me at foo.bar@gmail.com.</strong></span>
<span class="strong"><strong>input=Hello,my name is Foo and my email is foo@bar.com or you can also contact me at foo.bar@gmail.com.</strong></span>
<span class="strong"><strong>chunking=Hello,my name is Foo and my email is foo@bar.com or you can also contact me at foo.bar@gmail.com. : [37-48:email@0.0, 79-96:email@0.0]</strong></span>
<span class="strong"><strong>     chunk=37-48:email@0.0  text=foo@bar.com</strong></span>
<span class="strong"><strong>     chunk=79-96:email@0.0  text=foo.bar@gmail.com</strong></span>
</pre></div></li><li class="listitem">You can see that both <code class="literal">foo@bar.com</code> as well as <code class="literal">foo.bar@gmail.com</code> were returned as valid <code class="literal">e-mail</code> type chunks. Also, note that the final period in the sentence is not part of the second e-mail.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec139"/>How it works…</h2></div></div></div><p>A regular expression chunker finds chunks that match the given regular expression. Essentially, the <code class="literal">java.util.regex.Matcher.find()</code> method is used to iteratively find matching text segments, and these are then converted into the Chunk objects. The <code class="literal">RegExChunker</code> class wraps these steps. The code of <code class="literal">src/com/lingpipe/cookbook/chapter5/RegExNer.java</code> is described as follows:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException {
  String emailRegex = "[A-Za-z0-9](([_\\.\\-]?[a-zA-Z0-9]+)*)" + + "@([A-Za-z0-9]+)" + "(([\\.\\-]?[a-zA-Z0-9]+)*)\\.([A-Za-z]{2,})";
  String chunkType = "email";
  double score = 1.0;
  Chunker chunker = new RegExChunker(emailRegex,chunkType,score);</pre></div><p>All the interesting work was done in the preceding lines of code. The <code class="literal">emailRegex</code> is pulled off of the Internet—see the following for the source, and the remaining bits are setting up <code class="literal">chunkType</code> and <code class="literal">score</code>.</p><p>The rest of the code reads<a class="indexterm" id="id469"/> in the<a class="indexterm" id="id470"/> input and prints out the chunking:</p><div class="informalexample"><pre class="programlisting">BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
  String input = "";
  while (true) {
    System.out.println("Enter text, . to quit:");
    input = reader.readLine();
    if(input.equals(".")){
      break;
    }
    Chunking chunking = chunker.chunk(input);
    System.out.println("input=" + input);
    System.out.println("chunking=" + chunking);
    Set&lt;Chunk&gt; chunkSet = chunking.chunkSet();
    Iterator&lt;Chunk&gt; it = chunkSet.iterator();
    while (it.hasNext()) {
      Chunk chunk = it.next();
      int start = chunk.start();
      int end = chunk.end();
      String text = input.substring(start,end);
      System.out.println("     chunk=" + chunk + " text=" + text);
    }
  }
}</pre></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec140"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The regular expression for the e-mail address match<a class="indexterm" id="id471"/> is from <a class="ulink" href="http://regexlib.com">regexlib.com</a> at <a class="ulink" href="http://regexlib.com/DisplayPatterns.aspx?cattabindex=0&amp;categoryId=1">http://regexlib.com/DisplayPatterns.aspx?cattabindex=0&amp;categoryId=1</a></li></ul></div></div></div>
<div class="section" title="Dictionary-based chunking for NER"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec63"/>Dictionary-based chunking for NER</h1></div></div></div><p>In many websites and <a class="indexterm" id="id472"/>blogs and certainly on web forums, you might see <a class="indexterm" id="id473"/>keyword highlighting that links pages you can buy a product from. Similarly, news websites also provide topic pages<a class="indexterm" id="id474"/> for people, places, and trending events, such as the one at <a class="ulink" href="http://www.nytimes.com/pages/topics/">http://www.nytimes.com/pages/topics/</a>.</p><p>A lot of this is fully automated and is easy to do with a dictionary-based <code class="literal">Chunker</code>. It is straightforward to compile lists of names for entities and their types. An exact dictionary chunker extracts chunks based on exact matches of tokenized dictionary entries.</p><p>The implementation of the dictionary-based chunker in LingPipe is based on the Aho-Corasick algorithm which finds all matches against a dictionary in linear time independent of the number of matches or size of the dictionary. This makes it much more efficient than the naïve approach of doing substring searches or using regular expressions.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec141"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In the IDE of your choice run the <code class="literal">DictionaryChunker</code> class in the <code class="literal">chapter5</code> package or type the following using the command line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java –cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter5.DictionaryChunker</strong></span>
</pre></div></li><li class="listitem">Since this particular chunker example is biased (very heavily) towards the Hitchhikers Guide, let's use a sentence that involves some of the characters:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Enter text, . to quit:</strong></span>
<span class="strong"><strong>Ford and Arthur went up the bridge of the Heart of Gold with Marvin</strong></span>
<span class="strong"><strong>CHUNKER overlapping, case sensitive</strong></span>
<span class="strong"><strong>    phrase=|Ford| start=0 end=4 type=PERSON score=1.0</strong></span>
<span class="strong"><strong>    phrase=|Arthur| start=9 end=15 type=PERSON score=1.0</strong></span>
<span class="strong"><strong>    phrase=|Heart| start=42 end=47 type=ORGAN score=1.0</strong></span>
<span class="strong"><strong>   phrase=|Heart of Gold| start=42 end=55 type=SPACECRAFT score=1.0</strong></span>
<span class="strong"><strong>    phrase=|Marvin| start=61 end=67 type=ROBOT score=1.0</strong></span>
</pre></div></li><li class="listitem">Note that we have overlapping chunks from <code class="literal">Heart</code> and <code class="literal">Heart of Gold</code>. As we will see, this can be configured to behave differently.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec142"/>How it works…</h2></div></div></div><p>Dictionary-based NER drives a great deal of automatic linking against unstructured text data. We can build one using the following steps.</p><p>The first step of the code will create <code class="literal">MapDictionary&lt;String&gt;</code> to store the dictionary entries:</p><div class="informalexample"><pre class="programlisting">static final double CHUNK_SCORE = 1.0;

public static void main(String[] args) throws IOException {
  MapDictionary&lt;String&gt; dictionary = new MapDictionary&lt;String&gt;();
  MapDictionary&lt;String&gt; dictionary = new MapDictionary&lt;String&gt;();</pre></div><p>Next, we will populate the dictionary with <code class="literal">DictionaryEntry&lt;String&gt;</code>, which includes type information and a score that will be used to create chunks:</p><div class="informalexample"><pre class="programlisting">dictionary.addEntry(new DictionaryEntry&lt;String&gt;("Arthur","PERSON",CHUNK_SCORE));
dictionary.addEntry(new DictionaryEntry&lt;String&gt;("Ford","PERSON",CHUNK_SCORE));
dictionary.addEntry(new DictionaryEntry&lt;String&gt;("Trillian","PERSON",CHUNK_SCORE));
dictionary.addEntry(new DictionaryEntry&lt;String&gt;("Zaphod","PERSON",CHUNK_SCORE));
dictionary.addEntry(new DictionaryEntry&lt;String&gt;("Marvin","ROBOT",CHUNK_SCORE));
dictionary.addEntry(new DictionaryEntry&lt;String&gt;("Heart of Gold", "SPACECRAFT",CHUNK_SCORE));
dictionary.addEntry(new DictionaryEntry&lt;String&gt;("HitchhikersGuide", "PRODUCT",CHUNK_SCORE));</pre></div><p>In the <code class="literal">DictionaryEntry</code> constructor, the<a class="indexterm" id="id475"/> first argument is the phrase, the <a class="indexterm" id="id476"/>second string argument is the type, and the final double argument is the score for the chunk. Dictionary entries are always case sensitive. There is no limit to the number of different entity types in a dictionary. The scores will simply be passed along as chunk scores in the dictionary-based chunker.</p><p>Next, we will build <code class="literal">Chunker</code>:</p><div class="informalexample"><pre class="programlisting">boolean returnAllMatches = true;
boolean caseSensitive = true;
ExactDictionaryChunker dictionaryChunker = new ExactDictionaryChunker(dictionary, IndoEuropeanTokenizerFactory.INSTANCE, returnAllMatches,caseSensitive);</pre></div><p>An exact dictionary chunker might be configured either to extract all the matching chunks to restrict the results to a consistent set of non-overlapping chunks via the <code class="literal">returnAllMatches</code> boolean. Look at the Javadoc to understand the exact criteria. There is also a <code class="literal">caseSensitive</code> boolean. The chunker requires a tokenizer, as it matches tokens as symbols, and whitespaces are ignored in the matching process.</p><p>Next is our standard I/O code for console interaction:</p><div class="informalexample"><pre class="programlisting">BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
String text = "";
while (true) {
  System.out.println("Enter text, . to quit:");
  text = reader.readLine();
  if(text.equals(".")){
    break;
  }</pre></div><p>The remaining code creates a chunking, goes through the chunks, and prints them out:</p><div class="informalexample"><pre class="programlisting">System.out.println("\nCHUNKER overlapping, case sensitive");
Chunking chunking = dictionaryChunker.chunk(text);
  for (Chunk chunk : chunking.chunkSet()) {
    int start = chunk.start();
    int end = chunk.end();
    String type = chunk.type();
    double score = chunk.score();
    String phrase = text.substring(start,end);
    System.out.println("     phrase=|" + phrase + "|" + " start=" + start + " end=" + end + " type=" + type + " score=" + score);</pre></div><p>Dictionary chunkers <a class="indexterm" id="id477"/>are very useful even in machine-learning-based<a class="indexterm" id="id478"/> systems. There tends to always be a class of entities that are best identified this way. The <span class="emphasis"><em>Mixing the NER sources</em></span> recipe addresses how to work with multiple sources of named entities.</p></div></div>
<div class="section" title="Translating between word tagging and chunks &#x2013; BIO codec"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec64"/>Translating between word tagging and chunks – BIO codec</h1></div></div></div><p>In <a class="link" href="ch04.html" title="Chapter 4. Tagging Words and Tokens">Chapter 4</a>, <span class="emphasis"><em>Tagging Words and Tokens</em></span>, we used <a class="indexterm" id="id479"/>HMMs and CRFs to apply tags to words/tokens. This recipe addresses the case of creating chunks from taggings that use the <span class="strong"><strong>Begin, In, and Out</strong></span> (<span class="strong"><strong>BIO</strong></span>) tags<a class="indexterm" id="id480"/> to encode chunkings that can span multiple words/tokens. This, in turn, is the basis of modern named-entity detection systems.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec143"/>Getting ready</h2></div></div></div><p>The standard BIO-tagging scheme has the first token in a chunk of type X tagged B-X (begin), with all the subsequent tokens in the same chunk tagged I-X (in). All the tokens that are not in chunks are tagged O (out). For example, the string with character counts:</p><div class="informalexample"><pre class="programlisting">John Jones Mary and Mr. Jones
01234567890123456789012345678
0         1         2         </pre></div><p>It can be tagged as:</p><div class="informalexample"><pre class="programlisting">John  B_PERSON
Jones  I_PERSON
Mary  B_PERSON
and  O
Mr    B_PERSON
.    I_PERSON
Jones  I_PERSON</pre></div><p>The corresponding chunks will be:</p><div class="informalexample"><pre class="programlisting">0-10 "John Jones" PERSON
11-15 "Mary" PERSON
20-29 "Mr. Jones" PERSON</pre></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec144"/>How to do it…</h2></div></div></div><p>The program will show the <a class="indexterm" id="id481"/>simplest mapping between taggings and chunkings and the other way around:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Run the following:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.BioCodec</strong></span>
</pre></div></li><li class="listitem">The program first prints out the string that will be tagged with a tagging:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Tagging for :The rain in Spain.</strong></span>
<span class="strong"><strong>The/B_Weather</strong></span>
<span class="strong"><strong>rain/I_Weather</strong></span>
<span class="strong"><strong>in/O</strong></span>
<span class="strong"><strong>Spain/B_Place</strong></span>
<span class="strong"><strong>./O</strong></span>
</pre></div></li><li class="listitem">Next, the chunking is printed:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Chunking from StringTagging</strong></span>
<span class="strong"><strong>0-8:Weather@-Infinity</strong></span>
<span class="strong"><strong>12-17:Place@-Infinity</strong></span>
</pre></div></li><li class="listitem">Then, the tagging is created from the chunking just displayed:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>StringTagging from Chunking</strong></span>
<span class="strong"><strong>The/B_Weather</strong></span>
<span class="strong"><strong>rain/I_Weather</strong></span>
<span class="strong"><strong>in/O</strong></span>
<span class="strong"><strong>Spain/B_Place</strong></span>
<span class="strong"><strong>./O</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec145"/>How it works…</h2></div></div></div><p>The code starts by manually <a class="indexterm" id="id482"/>constructing <code class="literal">StringTagging</code>—we will see HMMs and CRFs do the same programmatically, but here it is explicit. It then prints out the created <code class="literal">StringTagging</code>:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) {
  List&lt;String&gt; tokens = new ArrayList&lt;String&gt;();
  tokens.add("The");
  tokens.add("rain");
  tokens.add("in");
  tokens.add("Spain");
  tokens.add(".");
  List&lt;String&gt; tags = new ArrayList&lt;String&gt;();
  tags.add("B_Weather");
  tags.add("I_Weather");
  tags.add("O");
  tags.add("B_Place");
  tags.add("O");
  CharSequence cs = "The rain in Spain.";
  //012345678901234567
  int[] tokenStarts = {0,4,9,12,17};
  int[] tokenEnds = {3,8,11,17,17};
  StringTagging tagging = new StringTagging(tokens, tags, cs, tokenStarts, tokenEnds);
  System.out.println("Tagging for :" + cs);
  for (int i = 0; i &lt; tagging.size(); ++i) {
    System.out.println(tagging.token(i) + "/" + tagging.tag(i));
  }</pre></div><p>Next, it will construct <code class="literal">BioTagChunkCodec</code> and convert the tagging just printed out to a chunking followed by printing the chunking:</p><div class="informalexample"><pre class="programlisting">BioTagChunkCodec codec = new BioTagChunkCodec();
Chunking chunking = codec.toChunking(tagging);
System.out.println("Chunking from StringTagging");
for (Chunk chunk : chunking.chunkSet()) {
  System.out.println(chunk);
}</pre></div><p>The remaining code <a class="indexterm" id="id483"/>reverses the process. First, a different <code class="literal">BioTagChunkCodec</code> is created with <code class="literal">boolean</code> <code class="literal">enforceConsistency</code>, which, if <code class="literal">true</code>, checks that the tokens created by the supplied tokenizer align exactly with the chunk begins and ends. Without the alignment we end up with a perhaps untenable relationship between chunks and tokens depending on the use case:</p><div class="informalexample"><pre class="programlisting">boolean enforceConsistency = true;
BioTagChunkCodec codec2 = new BioTagChunkCodec(IndoEuropeanTokenizerFactory.INSTANCE, enforceConsistency);
StringTagging tagging2 = codec2.toStringTagging(chunking);
System.out.println("StringTagging from Chunking");
for (int i = 0; i &lt; tagging2.size(); ++i) {
  System.out.println(tagging2.token(i) + "/" + tagging2.tag(i));
}</pre></div><p>The last <code class="literal">for</code> loop simply prints out the tagging returned by the <code class="literal">codec2.toStringTagging()</code> method.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec146"/>There's more…</h2></div></div></div><p>The recipe works through the simplest example of mapping between taggings and chunkings. <code class="literal">BioTagChunkCodec</code> also takes the <code class="literal">TagLattice&lt;String&gt;</code> objects to produce n-best output, as will be shown in the HMM and CRF chunkers to follow.</p></div></div>
<div class="section" title="HMM-based NER"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec65"/>HMM-based NER</h1></div></div></div><p>
<code class="literal">HmmChunker</code> uses <a class="indexterm" id="id484"/>an HMM to perform chunking over tokenized character sequences. Instances contain an HMM decoder for the model and tokenizer factory. The chunker requires the states of the HMM to conform to a token-by-token encoding of a chunking. It uses the tokenizer <a class="indexterm" id="id485"/>factory to break the chunks down into sequences of tokens and tags. Refer to the <span class="emphasis"><em>Hidden Markov Models (HMM) – part of speech</em></span> recipe in <a class="link" href="ch04.html" title="Chapter 4. Tagging Words and Tokens">Chapter 4</a>, <span class="emphasis"><em>Tagging Words and Tokens</em></span>.</p><p>We'll look at training <code class="literal">HmmChunker</code> and using it for the <code class="literal">CoNLL2002</code> Spanish task. You can and should use your own data, but this recipe assumes that training data will be in the <code class="literal">CoNLL2002</code> format.</p><p>Training is done using an <code class="literal">ObjectHandler</code> which supplies the training instances.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec147"/>Getting ready</h2></div></div></div><p>As we want to train this chunker, we need to either label some data using the <span class="strong"><strong>Computational Natural Language Learning</strong></span> (<span class="strong"><strong>CoNLL</strong></span>) <a class="indexterm" id="id486"/>schema or use the one that's publicly available. For speed, we'll choose to get a corpus that is available in the CoNLL 2002 task.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>The ConNLL is an annual meeting that sponsors a bakeoff. In 2002, the bakeoff involved Spanish and Dutch NER.</p></div></div><p>The data can be downloaded from <a class="ulink" href="http://www.cnts.ua.ac.be/conll2002/ner.tgz">http://www.cnts.ua.ac.be/conll2002/ner.tgz</a>.</p><p>Similar to what we showed<a class="indexterm" id="id487"/> in the preceding recipe; let's take a look at what this data looks like:</p><div class="informalexample"><pre class="programlisting">El       O 
Abogado     B-PER 
General     I-PER 
del     I-PER 
Estado     I-PER 
,       O 
Daryl     B-PER 
Williams     I-PER 
,       O</pre></div><p>With this encoding scheme, the phrases <span class="emphasis"><em>El Abogado General del Estado</em></span> and <span class="emphasis"><em>Daryl Williams</em></span> are coded as persons, with their beginning and continuing tokens picked out with tags B-PER and I-PER, respectively.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>There are a few formatting errors in the data that must be fixed before our parsers can handle them. After unpacking <code class="literal">ner.tgz</code> in the <code class="literal">data</code> directory you will have to go to <code class="literal">data/ner/data</code>, unzip the following files, and modify as indicated:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>esp.train, line 221619, change I-LOC to B-LOC</strong></span>
<span class="strong"><strong>esp.testa, line 30882, change I-LOC to B-LOC</strong></span>
<span class="strong"><strong>esp.testb, line 9291, change I-LOC to B-LOC</strong></span>
</pre></div></div></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec148"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Using the command line, type the following:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java –cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter5.HmmNeChunker</strong></span>
</pre></div></li><li class="listitem">It will run the training on the CoNLL training data if the model doesn't exist. It might take a while, so be patient. The output of the training will be:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Training HMM Chunker on data from: data/ner/data/esp.train</strong></span>
<span class="strong"><strong>Output written to : models/Conll2002_ESP.RescoringChunker</strong></span>
<span class="strong"><strong>Enter text, . to quit:</strong></span>
</pre></div></li><li class="listitem">Once the prompt to<a class="indexterm" id="id488"/> enter the text is presented, type in some Spanish text from the CoNLL test set:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>La empresa también tiene participación en Tele Leste Celular , operadora móvil de los estados de Bahía y Sergipe y que es controlada por la española Iberdrola , y además es socia de Portugal Telecom en Telesp Celular , la operadora móvil de Sao Paulo .</strong></span>
<span class="strong"><strong>Rank   Conf      Span         Type     Phrase</strong></span>
<span class="strong"><strong>0      1.0000   (105, 112)    LOC      Sergipe</strong></span>
<span class="strong"><strong>1      1.0000   (149, 158)    ORG      Iberdrola</strong></span>
<span class="strong"><strong>2      1.0000   (202, 216)    ORG      Telesp Celular</strong></span>
<span class="strong"><strong>3      1.0000   (182, 198)    ORG      Portugal Telecom</strong></span>
<span class="strong"><strong>4      1.0000   (97, 102)     LOC      Bahía</strong></span>
<span class="strong"><strong>5      1.0000   (241, 250)    LOC      Sao Paulo</strong></span>
<span class="strong"><strong>6      0.9907   (163, 169)    PER      además</strong></span>
<span class="strong"><strong>7      0.9736   (11, 18)      ORG      también</strong></span>
<span class="strong"><strong>8      0.9736   (39, 60)      ORG      en Tele Leste Celular</strong></span>
<span class="strong"><strong>9      0.0264   (42, 60)      ORG      Tele Leste Celular</strong></span>
</pre></div></li><li class="listitem">What we will see is a number of entities, their confidence score, the span in the original sentence, the type of entity, and the phrase that represents this entity.</li><li class="listitem">To find out the correct tags, take a look at the annotated <code class="literal">esp.testa</code> file, which contains the following tags for this sentence:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Tele B-ORG</strong></span>
<span class="strong"><strong>Leste I-ORG</strong></span>
<span class="strong"><strong>Celular I-ORG</strong></span>
<span class="strong"><strong>Bahía B-LOC</strong></span>
<span class="strong"><strong>Sergipe B-LOC</strong></span>
<span class="strong"><strong>Iberdrola B-ORG</strong></span>
<span class="strong"><strong>Portugal B-ORG</strong></span>
<span class="strong"><strong>Telecom I-ORG</strong></span>
<span class="strong"><strong>Telesp B-ORG</strong></span>
<span class="strong"><strong>Celular I-ORG</strong></span>
<span class="strong"><strong>Sao B-LOC</strong></span>
<span class="strong"><strong>Paulo I-LOC</strong></span>
</pre></div></li><li class="listitem">This can be read as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Tele Leste Celular      ORG</strong></span>
<span class="strong"><strong>Bahía                   LOC</strong></span>
<span class="strong"><strong>Sergipe                 LOC</strong></span>
<span class="strong"><strong>Iberdrola               ORG</strong></span>
<span class="strong"><strong>Portugal Telecom        ORG</strong></span>
<span class="strong"><strong>Telesp Celular          ORG</strong></span>
<span class="strong"><strong>Sao Paulo               LOC</strong></span>
</pre></div></li><li class="listitem">So, we got all the ones with 1.000 confidence correct and the rest wrong. This can help us set up a threshold in production.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec149"/>How it works…</h2></div></div></div><p>The<a class="indexterm" id="id489"/> <code class="literal">CharLmRescoringChunker</code> provides a long-distance character language model-based chunker that operates by rescoring the output of a contained character language model HMM chunker. The underlying chunker is an instance of <code class="literal">CharLmHmmChunker</code>, which is configured with the specified tokenizer factory, n-gram length, number of characters, and interpolation ratio provided in the constructor.</p><p>Let's start with the <code class="literal">main()</code> method; here, we will set up the chunker, train it if it doesn't exist, and then allow for some input to get the named entities out:</p><div class="informalexample"><pre class="programlisting">String modelFilename = "models/Conll2002_ESP.RescoringChunker";
String trainFilename = "data/ner/data/esp.train";</pre></div><p>The training file will be in the correct place if you unpack the CoNLL data (<code class="literal">tar –xvzf ner.tgz</code>) in the data directory. Remember to correct the annotation on line 221619 of <code class="literal">esp.train</code>. If you use other data, then modify and recompile the class.</p><p>The next bit of code trains the model if it doesn't exist and then loads the serialized version of the chunker. If you have questions about deserialization, see the <span class="emphasis"><em>Deserializing and running a classifier</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>. Consider the following code snippet:</p><div class="informalexample"><pre class="programlisting">File modelFile = new File(modelFilename);
if(!modelFile.exists()){
  System.out.println("Training HMM Chunker on data from: " + trainFilename);
  trainHMMChunker(modelFilename, trainFilename);
  System.out.println("Output written to : " + modelFilename);
}

@SuppressWarnings("unchecked")
RescoringChunker&lt;CharLmRescoringChunker&gt; chunker = (RescoringChunker&lt;CharLmRescoringChunker&gt;) AbstractExternalizable.readObject(modelFile);</pre></div><p>The <code class="literal">trainHMMChunker()</code> method starts with some <code class="literal">File</code> bookkeeping before setting up configuration <a class="indexterm" id="id490"/>parameters for <code class="literal">CharLmRescoringChunker</code>:</p><div class="informalexample"><pre class="programlisting">static void trainHMMChunker(String modelFilename, String trainFilename) throws IOException{
  File modelFile = new File(modelFilename);
  File trainFile = new File(trainFilename);
  
  int numChunkingsRescored = 64;
  int maxNgram = 12;
  int numChars = 256;
  double lmInterpolation = maxNgram; 
  TokenizerFactory factory
    = IndoEuropeanTokenizerFactory.INSTANCE;

CharLmRescoringChunker chunkerEstimator
  = new CharLmRescoringChunker(factory,numChunkingsRescored,
          maxNgram,numChars,
          lmInterpolation);</pre></div><p>Starting with the first parameter, <code class="literal">numChunkingsRescored</code> sets the number of chunkings from the embedded <code class="literal">Chunker</code> that will be rescored in an effort to improve performance. The implementation of this rescoring can vary, but generally, less-localized information is used to improve on the basic HMM output, which is contextually limited. The <code class="literal">maxNgram</code> sets the maximum character size for the rescoring-bounded character language model per chunk type, and <code class="literal">lmInterpolation</code> dictates how the models are interpolated. A good value is the character n-gram size. Finally, a tokenizer factory is created. There is a lot going on in this class; consult Javadoc for more information.</p><p>Next in the method, we will get a parser to be discussed in the following code snippet, that takes <code class="literal">chunkerEstimator</code> with the <code class="literal">setHandler()</code> method, and then, the <code class="literal">parser.parse()</code> method does the actual training. The last bit of code serializes the model to disk—see the <span class="emphasis"><em>How to serialize a LingPipe object – classifier example</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Simple Classifiers">Chapter 1</a>, <span class="emphasis"><em>Simple Classifiers</em></span>, to read about what is going on:</p><div class="informalexample"><pre class="programlisting">Conll2002ChunkTagParser parser = new Conll2002ChunkTagParser();
parser.setHandler(chunkerEstimator);
parser.parse(trainFile);
AbstractExternalizable.compileTo(chunkerEstimator,modelFile);</pre></div><p>Now, let's take a look at parsing the CoNLL data. The source for this class is <code class="literal">src/com/lingpipe/cookbook/chapter5/Conll2002ChunkTagParser</code>:</p><div class="informalexample"><pre class="programlisting">public class Conll2002ChunkTagParser extends StringParser&lt;ObjectHandler&lt;Chunking&gt;&gt;
{

  static final String TOKEN_TAG_LINE_REGEX = "(\\S+)\\s(\\S+\\s)?(O|[B|I]-\\S+)";
  static final int TOKEN_GROUP = 1;
  static final int TAG_GROUP = 3;
  static final String IGNORE_LINE_REGEX = "-DOCSTART(.*)";
  static final String EOS_REGEX = "\\A\\Z";
  static final String BEGIN_TAG_PREFIX = "B-";
  static final String IN_TAG_PREFIX = "I-";
  static final String OUT_TAG = "O";</pre></div><p>The statics <a class="indexterm" id="id491"/>set up the configuration of the <code class="literal">com.aliasi.tag.LineTaggingParser</code> LingPipe class. CoNLL, like many available data sets, uses a token/tag per line format, which is meant to be very easy to parse:</p><div class="informalexample"><pre class="programlisting">private final LineTaggingParser mParser = new LineTaggingParser(TOKEN_TAG_LINE_REGEX, TOKEN_GROUP, TAG_GROUP, IGNORE_LINE_REGEX, EOS_REGEX);</pre></div><p>The <code class="literal">LineTaggingParser</code> constructor requires a regular expression that identifies the token and tag strings via grouping. There is additionally a regular expression for lines to ignore and finally, a regular expression for sentence ends.</p><p>Next, we set up <code class="literal">TagChunkCodec</code>; this will handle the mapping from tagged tokens in the BIO format to proper chunks. See the previous recipe, <span class="emphasis"><em>Translating between word tagging and chunks – BIO codec</em></span>, for more about what is going on here. The remaining parameters customize the tags to match those of the CoNLL training data:</p><div class="informalexample"><pre class="programlisting">private final TagChunkCodec mCodec = new BioTagChunkCodec(null, false, BEGIN_TAG_PREFIX, IN_TAG_PREFIX, OUT_TAG);</pre></div><p>The rest of the class provides methods for <code class="literal">parseString()</code>, which is immediately sent to the <code class="literal">LineTaggingParser</code> class:</p><div class="informalexample"><pre class="programlisting">public void parseString(char[] cs, int start, int end) {
  mParser.parseString(cs,start,end);
}</pre></div><p>Next, the <code class="literal">ObjectHandler</code> parser is properly configured with the codec and supplied handler:</p><div class="informalexample"><pre class="programlisting">public void setHandler(ObjectHandler&lt;Chunking&gt; handler) {

  ObjectHandler&lt;Tagging&lt;String&gt;&gt; taggingHandler = TagChunkCodecAdapters.chunkingToTagging(mCodec, handler);
  mParser.setHandler(taggingHandler);
}

public TagChunkCodec getTagChunkCodec(){
  return mCodec;
}</pre></div><p>It's a lot of odd-looking code, but all this does is set up a parser to read the lines from the input file and extract chunkings out of them.</p><p>Finally, let's go<a class="indexterm" id="id492"/> back to the <code class="literal">main</code> method and look at the output loop. We will set up the <code class="literal">MAX_NBEST</code> chunkings value as 10 and then invoke the <code class="literal">nBestChunkings</code> method on the chunker. This provides the top 10 chunks and their probabilistic scores. Based on an evaluation, we can choose to cut off at a particular score:</p><div class="informalexample"><pre class="programlisting">char[] cs = text.toCharArray();
Iterator&lt;Chunk&gt; it = chunker.nBestChunks(cs,0,cs.length, MAX_N_BEST_CHUNKS);
System.out.println(text);
System.out.println("Rank          Conf      Span"    + "    Type     Phrase");
DecimalFormat df = new DecimalFormat("0.0000");

for (int n = 0; it.hasNext(); ++n) {

Chunk chunk = it.next();
double conf = chunk.score();
int start = chunk.start();
int end = chunk.end();
String phrase = text.substring(start,end);
System.out.println(n + " "       + "            "   + df.format(conf)     + "       (" + start  + ", " + end  + ")    " + chunk.type()      + "         " + phrase);
}</pre></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec150"/>There's more…</h2></div></div></div><p>For more details on running a complete evaluation, refer to the evaluation section of the tutorial at <a class="ulink" href="http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/ne/read-me.html</a>.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec151"/>See also</h2></div></div></div><p>For more details<a class="indexterm" id="id493"/> on <code class="literal">CharLmRescoringChunker</code> and <code class="literal">HmmChunker</code>, refer<a class="indexterm" id="id494"/> to:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/AbstractCharLmRescoringChunker.html">http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/AbstractCharLmRescoringChunker.html</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/HmmChunker.html">http://alias-i.com/lingpipe/docs/api/com/aliasi/chunk/HmmChunker.html</a></li></ul></div></div></div>
<div class="section" title="Mixing the NER sources"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec66"/>Mixing the NER sources</h1></div></div></div><p>Now that we've seen how<a class="indexterm" id="id495"/> to build a few different types of NERs, we can look at how to combine them. In this recipe, we will take a regular expression chunker, a dictionary-based chunker, and an HMM-based chunker and combine their outputs and look at overlaps.</p><p>We will just initialize a few chunkers in the same way we did in the past few recipes and then pass the same text through these chunkers. The easiest possibility is that each chunker returns a unique output. For example, let's consider a sentence such as "President Obama was scheduled to give a speech at the G-8 conference this evening". If we have a person chunker and an organization chunker, we might only get two unique chunks out. However, if we add a <code class="literal">Presidents of USA</code> chunker, we will get three chunks: <code class="literal">PERSON</code>, <code class="literal">ORGANIZATION</code>, and <code class="literal">PRESIDENT</code>. This very simple recipe will show us one way to handle these cases.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec152"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Using the command line or equivalent in your IDE, type the following:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java –cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter5.MultipleNer</strong></span>
</pre></div></li><li class="listitem">The usual interactive prompt follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Enter text, . to quit:</strong></span>
<span class="strong"><strong>President Obama is scheduled to arrive in London this evening. He will address the G-8 summit.</strong></span>
<span class="strong"><strong>neChunking: [10-15:PERSON@-Infinity, 42-48:LOCATION@-Infinity, 83-86:ORGANIZATION@-Infinity]</strong></span>
<span class="strong"><strong>pChunking: [62-66:MALE_PRONOUN@1.0]</strong></span>
<span class="strong"><strong>dChunking: [10-15:PRESIDENT@1.0]</strong></span>
<span class="strong"><strong>----Overlaps Allowed</strong></span>

<span class="strong"><strong> Combined Chunks:</strong></span>
<span class="strong"><strong>[83-86:ORGANIZATION@-Infinity, 10-15:PERSON@-Infinity, 10-15:PRESIDENT@1.0, 42-48:LOCATION@-Infinity, 62-66:MALE_PRONOUN@1.0]</strong></span>

<span class="strong"><strong>----Overlaps Not Allowed</strong></span>

<span class="strong"><strong> Unique Chunks:</strong></span>
<span class="strong"><strong>[83-86:ORGANIZATION@-Infinity, 42-48:LOCATION@-Infinity, 62-66:MALE_PRONOUN@1.0]</strong></span>

<span class="strong"><strong> OverLapped Chunks:</strong></span>
<span class="strong"><strong>[10-15:PERSON@-Infinity, 10-15:PRESIDENT@1.0]</strong></span>
</pre></div></li><li class="listitem">We see the output from the three chunkers: <code class="literal">neChunking</code> is the output of an HMM chunker that is trained to return the MUC-6 entities, <code class="literal">pChunking</code> is a simple regular expression that recognizes male pronouns, and <code class="literal">dChunking</code> is a dictionary chunker that recognizes US Presidents.</li><li class="listitem">With overlaps allowed, we will see the chunks for <code class="literal">PRESIDENT</code> as well as <code class="literal">PERSON</code> in the merged output.</li><li class="listitem">With overlaps disallowed, they will be added to the set overlapped chunks and removed from the unique chunks.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec153"/>How it works…</h2></div></div></div><p>We initialized three <a class="indexterm" id="id496"/>chunkers that should be familiar to you from the previous recipes in this chapter:</p><div class="informalexample"><pre class="programlisting">Chunker pronounChunker = new RegExChunker(" He | he | Him | him", "MALE_PRONOUN",1.0);
File MODEL_FILE = new File("models/ne-en-news.muc6." + "AbstractCharLmRescoringChunker");
Chunker neChunker = (Chunker) AbstractExternalizable.readObject(MODEL_FILE);

MapDictionary&lt;String&gt; dictionary = new MapDictionary&lt;String&gt;();
dictionary.addEntry(
  new DictionaryEntry&lt;String&gt;("Obama","PRESIDENT",CHUNK_SCORE));
dictionary.addEntry(
  new DictionaryEntry&lt;String&gt;("Bush","PRESIDENT",CHUNK_SCORE));
ExactDictionaryChunker dictionaryChunker = new ExactDictionaryChunker(dictionary, IndoEuropeanTokenizerFactory.INSTANCE);</pre></div><p>Now, we will just chunk our input text via all three chunkers, combine the chunks into one set, and pass our <code class="literal">getCombinedChunks</code> method to it:</p><div class="informalexample"><pre class="programlisting">Set&lt;Chunk&gt; neChunking = neChunker.chunk(text).chunkSet();
Set&lt;Chunk&gt; pChunking = pronounChunker.chunk(text).chunkSet();
Set&lt;Chunk&gt; dChunking = dictionaryChunker.chunk(text).chunkSet();
Set&lt;Chunk&gt; allChunks = new HashSet&lt;Chunk&gt;();
allChunks.addAll(neChunking);
allChunks.addAll(pChunking);
allChunks.addAll(dChunking);
getCombinedChunks(allChunks,true);//allow overlaps
getCombinedChunks(allChunks,false);//no overlaps</pre></div><p>The meat of this <a class="indexterm" id="id497"/>recipe is in the <code class="literal">getCombinedChunks</code> method. We will just loop through all the chunks and check each pair if they overlap in their starts and ends. If they overlap and overlaps are not allowed, they are added to an overlapped set; otherwise, they are added to a combined set:</p><div class="informalexample"><pre class="programlisting">static void getCombinedChunks(Set&lt;Chunk&gt; chunkSet, boolean allowOverlap){
  Set&lt;Chunk&gt; combinedChunks = new HashSet&lt;Chunk&gt;();
  Set&lt;Chunk&gt;overLappedChunks = new HashSet&lt;Chunk&gt;();
  for(Chunk c : chunkSet){
    combinedChunks.add(c);
    for(Chunk x : chunkSet){
      if (c.equals(x)){
        continue;
      }
      if (ChunkingImpl.overlap(c,x)) {
        if (allowOverlap){
          combinedChunks.add(x);
        } else {
          overLappedChunks.add(x);
          combinedChunks.remove(c);
        }
      }
    }
  }
}</pre></div><p>Here is the place <a class="indexterm" id="id498"/>to add more rules for overlapping chunks. For example, you can make it score based, so if the <code class="literal">PRESIDENT</code> chunk type has a higher score than the HMM-based one, you can choose it instead.</p></div></div>
<div class="section" title="CRFs for chunking"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec67"/>CRFs for chunking</h1></div></div></div><p>CRFs are best known to <a class="indexterm" id="id499"/>provide close to state-of-the-art performance for named-entity tagging. This recipe will tell us how to build one of these systems. The recipe assumes that you have read, understood, and played with the <span class="emphasis"><em>Conditional r</em></span>
<span class="emphasis"><em>andom fields – CRF for word/token tagging</em></span> recipe in <a class="link" href="ch04.html" title="Chapter 4. Tagging Words and Tokens">Chapter 4</a>, <span class="emphasis"><em>Tagging Words and Tokens</em></span>, which addresses the underlying technology. Like HMMs, CRFs treat named entity detection as a word-tagging problem, with an interpretation layer that provides chunkings. Unlike HMMs, CRFs use a logistic-regression-based classification approach, which, in turn, allows for random features to be included. Also, there is an excellent tutorial on CRFs<a class="indexterm" id="id500"/> that this recipe follows closely (but omits details) at <a class="ulink" href="http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html</a>. There is also a lot of information in the Javadoc.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec154"/>Getting ready</h2></div></div></div><p>Just as we did earlier, we will use a small hand-coded corpus to serve as training data. The corpus is in <code class="literal">src/com/lingpipe/cookbook/chapter5/TinyEntityCorpus.java</code>. It starts with:</p><div class="informalexample"><pre class="programlisting">public class TinyEntityCorpus extends Corpus&lt;ObjectHandler&lt;Chunking&gt;&gt; {

  public void visitTrain(ObjectHandler&lt;Chunking&gt; handler) {
    for (Chunking chunking : CHUNKINGS) handler.handle(chunking);
  }

  public void visitTest(ObjectHandler&lt;Chunking&gt; handler) {
    /* no op */
  }</pre></div><p>Since we are only using this corpus to train, the <code class="literal">visitTest()</code> method does nothing. However, the <code class="literal">visitTrain()</code> method exposes the handler to all the chunkings stored in the <code class="literal">CHUNKINGS</code> constant. This, in turn, looks like the following:</p><div class="informalexample"><pre class="programlisting">static final Chunking[] CHUNKINGS = new Chunking[] {
  chunking(""), chunking("The"), chunking("John ran.", chunk(0,4,"PER")), chunking("Mary ran.", chunk(0,4,"PER")), chunking("The kid ran."), chunking("John likes Mary.", chunk(0,4,"PER"), chunk(11,15,"PER")), chunking("Tim lives in Washington", chunk(0,3,"PER"), chunk(13,23,"LOC")), chunking("Mary Smith is in New York City", chunk(0,10,"PER"), chunk(17,30,"LOC")), chunking("New York City is fun", chunk(0,13,"LOC")), chunking("Chicago is not like Washington", chunk(0,7,"LOC"), chunk(20,30,"LOC"))
};</pre></div><p>We are still not done. Given <a class="indexterm" id="id501"/>that the creation of <code class="literal">Chunking</code> is fairly verbose, there are static methods to help dynamically create the requisite objects:</p><div class="informalexample"><pre class="programlisting">static Chunking chunking(String s, Chunk... chunks) {
  ChunkingImpl chunking = new ChunkingImpl(s);
  for (Chunk chunk : chunks) chunking.add(chunk);
  return chunking;
}

static Chunk chunk(int start, int end, String type) {
  return ChunkFactory.createChunk(start,end,type);
}</pre></div><p>This is all the setup; next, we will train and run a CRF on the preceding data.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec155"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Type the <code class="literal">TrainAndRunSimplCrf</code> class in the <a class="indexterm" id="id502"/>command line or run the equivalent in your IDE:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.TrainAndRunSimpleCrf</strong></span>
</pre></div></li><li class="listitem">This results in loads of screen output that report on the health and progress of the CRF, it is mostly information from the underlying logistic-regression classifier that drives the whole show. The fun bit is that we will get an invitation to play with the new CRF:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Enter text followed by new line</strong></span>
<span class="strong"><strong>&gt;John Smith went to New York.</strong></span>
</pre></div></li><li class="listitem">The chunker reports the first best output:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>FIRST BEST</strong></span>
<span class="strong"><strong>John Smith went to New York. : [0-10:PER@-Infinity, 19-27:LOC@-Infinity]</strong></span>
</pre></div></li><li class="listitem">The preceding output is the first best analysis by the CRF of what sorts of entities are in the sentence. It thinks that <code class="literal">John Smith</code> is <code class="literal">PER</code> with <code class="literal">the 0-10:PER@-Infinity</code> output. We know that it applies to the <code class="literal">John Smith</code> string by taking the substring from 0 to 10 in the input text. Ignore <code class="literal">–Infinity</code>, which is supplied for chunks that have no score. The first best chunking does not have scores. The other entity that it thinks is in the text is <code class="literal">New York</code> as an <code class="literal">LOC</code>.</li><li class="listitem">Immediately, the conditional probabilities follow:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>10 BEST CONDITIONAL</strong></span>
<span class="strong"><strong>Rank log p(tags|tokens)  Tagging</strong></span>
<span class="strong"><strong>0    -1.66335590 [0-10:PER@-Infinity, 19-27:LOC@-Infinity]</strong></span>
<span class="strong"><strong>1    -2.38671498 [0-10:PER@-Infinity, 19-28:LOC@-Infinity]</strong></span>
<span class="strong"><strong>2    -2.77341747 [0-10:PER@-Infinity]</strong></span>
<span class="strong"><strong>3    -2.85908677 [0-4:PER@-Infinity, 19-27:LOC@-Infinity]</strong></span>
<span class="strong"><strong>4    -3.00398856 [0-10:PER@-Infinity, 19-22:LOC@-Infinity]</strong></span>
<span class="strong"><strong>5    -3.23050827 [0-10:PER@-Infinity, 16-27:LOC@-Infinity]</strong></span>
<span class="strong"><strong>6    -3.49773765 [0-10:PER@-Infinity, 23-27:PER@-Infinity]</strong></span>
<span class="strong"><strong>7    -3.58244582 [0-4:PER@-Infinity, 19-28:LOC@-Infinity]</strong></span>
<span class="strong"><strong>8    -3.72315571 [0-10:PER@-Infinity, 19-22:PER@-Infinity]</strong></span>
<span class="strong"><strong>9    -3.95386735 [0-10:PER@-Infinity, 16-28:LOC@-Infinity]</strong></span>
</pre></div></li><li class="listitem">The preceding output<a class="indexterm" id="id503"/> provides the 10 best analyses of the whole phrase, along with their conditional (natural log) probabilities. In this case, we will see that the system isn't particularly confident of any of its analyses. For instance, the estimated probability of the first best analysis being correct is <code class="literal">exp(-1.66)=0.19</code>.</li><li class="listitem">Next, in the output, we see probabilities for individual chunks:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>MARGINAL CHUNK PROBABILITIES</strong></span>
<span class="strong"><strong>Rank Chunk Phrase</strong></span>
<span class="strong"><strong>0 0-10:PER@-0.49306887565189683 John Smith</strong></span>
<span class="strong"><strong>1 19-27:LOC@-1.1957935770408703 New York</strong></span>
<span class="strong"><strong>2 0-4:PER@-1.3270942262839682 John</strong></span>
<span class="strong"><strong>3 19-22:LOC@-2.484463373596263 New</strong></span>
<span class="strong"><strong>4 23-27:PER@-2.6919267821139776 York</strong></span>
<span class="strong"><strong>5 16-27:LOC@-2.881057607295971 to New York</strong></span>
<span class="strong"><strong>6 11-15:PER@-3.0868632773744222 went</strong></span>
<span class="strong"><strong>7 16-18:PER@-3.1583044940140192 to</strong></span>
<span class="strong"><strong>8 19-22:PER@-3.2036305275847825 New</strong></span>
<span class="strong"><strong>9 23-27:LOC@-3.536294896211011 York</strong></span>
</pre></div></li><li class="listitem">As with the previous conditional output, the probabilities are logs, so we can see that the <code class="literal">John Smith</code> chunk has estimated probability <code class="literal">exp(-0.49) = 0.61</code>, which makes sense because in training the CRF saw <code class="literal">John</code> at the beginning of <code class="literal">PER</code> and <code class="literal">Smith</code> at the end of another, but not <code class="literal">John Smith</code> directly.</li><li class="listitem">The preceding kind <a class="indexterm" id="id504"/>of probability distributions can really improve systems if there are sufficient resources to consider a broad range of analyses and ways of combining evidence to allow for improbable outcomes to be selected. First best analyses tend to be over committed to conservative outcomes that fit what training data looks like.</li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec156"/>How it works…</h2></div></div></div><p>The code in <code class="literal">src/com/lingpipe/cookbook/chapter5/TrainAndRunSimpleCRF.java</code> resembles our classifier and HMM recipes with a few differences. These differences are addressed as follows:</p><div class="informalexample"><pre class="programlisting">public static void main(String[] args) throws IOException {
  Corpus&lt;ObjectHandler&lt;Chunking&gt;&gt; corpus = new TinyEntityCorpus();

  TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
  boolean enforceConsistency = true;
  TagChunkCodec tagChunkCodec = new BioTagChunkCodec(tokenizerFactory, enforceConsistency);</pre></div><p>When we previously played with CRFs, the inputs were of the <code class="literal">Tagging&lt;String&gt;</code> type. Looking back at <code class="literal">TinyEntityCorpus.java</code>, the types are of the <code class="literal">Chunking</code> type. The preceding <code class="literal">BioTagChunkCodec</code> facilitates the translation of <code class="literal">Chunking</code> into <code class="literal">Tagging</code> via the efforts of a supplied <code class="literal">TokenizerFactory</code> and <code class="literal">boolean</code> that raise an exception if <code class="literal">TokenizerFactory</code> does not exactly agree with the <code class="literal">Chunk</code> starts and ends. Look back to the <span class="emphasis"><em>Translating between word tagging and chunks–BIO codec</em></span> recipe better<a class="indexterm" id="id505"/> understand the role of this class.</p><p>Let's take a look at the following:</p><div class="informalexample"><pre class="programlisting">John Smith went to New York City. : [0-10:PER@-Infinity, 19-32:LOC@-Infinity]</pre></div><p>This codec will translate into a tagging:</p><div class="informalexample"><pre class="programlisting">Tok    Tag
John   B_PER
Smith  I_PER
went  O
to     O
New    B_LOC
York  I_LOC
City  I_LOC
.    O</pre></div><p>The codec will do the reverse operation as well. The Javadoc is worth a visit. Once this mapping is established, the rest of the CRF is the same as the word tagging case behind the scenes, as shown by the fact that we use the same feature extractor in the <span class="emphasis"><em>Conditional random fields – CRF for word/token tagging</em></span> recipe, in <a class="link" href="ch04.html" title="Chapter 4. Tagging Words and Tokens">Chapter 4</a>, <span class="emphasis"><em>Tagging Words and Tokens</em></span>. Consider the following code snippet:</p><div class="informalexample"><pre class="programlisting">ChainCrfFeatureExtractor&lt;String&gt; featureExtractor = new SimpleCrfFeatureExtractor();</pre></div><p>All the mechanics are hidden inside a new <code class="literal">ChainCrfChunker</code> class, and it is initialized in a manner similar to logistic regression, which is the underlying technology. Refer to the <span class="emphasis"><em>Logistic regression</em></span> recipe of <a class="link" href="ch03.html" title="Chapter 3. Advanced Classifiers">Chapter 3</a>, <span class="emphasis"><em>Advanced Classifiers</em></span>, for more information on the configuration:</p><div class="informalexample"><pre class="programlisting">int minFeatureCount = 1;
boolean cacheFeatures = true;
boolean addIntercept = true;
double priorVariance = 4.0;
boolean uninformativeIntercept = true;
RegressionPrior prior = RegressionPrior.gaussian(priorVariance, uninformativeIntercept);
int priorBlockSize = 3;
double initialLearningRate = 0.05;
double learningRateDecay = 0.995;
AnnealingSchedule annealingSchedule = AnnealingSchedule.exponential(initialLearningRate, learningRateDecay);
double minImprovement = 0.00001;
int minEpochs = 10;
int maxEpochs = 5000;
Reporter reporter = Reporters.stdOut().setLevel(LogLevel.DEBUG);
System.out.println("\nEstimating");
ChainCrfChunker crfChunker = ChainCrfChunker.estimate(corpus, tagChunkCodec, tokenizerFactory, featureExtractor, addIntercept, minFeatureCount, cacheFeatures, prior, priorBlockSize, annealingSchedule, minImprovement, minEpochs, maxEpochs, reporter);</pre></div><p>The only new thing here is the <code class="literal">tagChunkCodec</code> parameter, which we just described.</p><p>Once the training is over, we <a class="indexterm" id="id506"/>will access the chunker for first best with the following code:</p><div class="informalexample"><pre class="programlisting">System.out.println("\nFIRST BEST");
Chunking chunking = crfChunker.chunk(evalText);
System.out.println(chunking);</pre></div><p>Conditional chunkings are delivered by:</p><div class="informalexample"><pre class="programlisting">int maxNBest = 10;
System.out.println("\n" + maxNBest + " BEST CONDITIONAL");
System.out.println("Rank log p(tags|tokens)  Tagging");
Iterator&lt;ScoredObject&lt;Chunking&gt;&gt; it = crfChunker.nBestConditional(evalTextChars,0, evalTextChars.length,maxNBest);

  for (int rank = 0; rank &lt; maxNBest &amp;&amp; it.hasNext(); ++rank) {
    ScoredObject&lt;Chunking&gt; scoredChunking = it.next();
    System.out.println(rank + "    " + scoredChunking.score() + " " + scoredChunking.getObject().chunkSet());
  }</pre></div><p>The individual chunks are accessed with:</p><div class="informalexample"><pre class="programlisting">System.out.println("\nMARGINAL CHUNK PROBABILITIES");
System.out.println("Rank Chunk Phrase");
int maxNBestChunks = 10;
Iterator&lt;Chunk&gt; nBestIt  = crfChunker.nBestChunks(evalTextChars,0, evalTextChars.length,maxNBestChunks);
for (int n = 0; n &lt; maxNBestChunks &amp;&amp; nBestIt.hasNext(); ++n) {
  Chunk chunk = nBestChunkIt.next();
  System.out.println(n + " " + chunk + " " + evalText.substring(chunk.start(),chunk.end()));
}</pre></div><p>That's it. You have access to one of the world's finest chunking technologies. Next, we will show you how to make it better.</p></div></div>
<div class="section" title="NER using CRFs with better features"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec68"/>NER using CRFs with better features</h1></div></div></div><p>In this recipe, we'll show<a class="indexterm" id="id507"/> you how to create a realistic, though not <a class="indexterm" id="id508"/>quite state-of-the-art, set of features for CRFs. The features will include normalized tokens, part-of-speech tags, word-shape features, position features, and token prefixes and suffixes. Substitute it for the <code class="literal">SimpleCrfFeatureExtractor</code> in the <span class="emphasis"><em>CRFs for chunking</em></span> recipe to use it.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec157"/>How to do it…</h2></div></div></div><p>The source for this recipe is in <code class="literal">src/com/lingpipe/cookbook/chapter5/FancyCrfFeatureExtractor.java</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open up your IDE or command prompt and type:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter5.FancyCrfFeatureExtractor</strong></span>
</pre></div></li><li class="listitem">Brace yourself for an explosion of features from the console. The data being used for feature extraction is <code class="literal">TinyEntityCorpus</code> of the previous recipe. Luckily, the first bit of data is just the node features for the "John" in the sentence <code class="literal">John ran.</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Tagging:  John/PN</strong></span>
<span class="strong"><strong>Node Feats:{PREF_NEXT_ra=1.0, PREF_Jo=1.0, POS_np=1.0, TOK_CAT_LET-CAP=1.0, SUFF_NEXT_an=1.0, PREF_Joh=1.0, PREF_NEXT_r=1.0, SUFF_John=1.0, TOK_John=1.0, PREF_NEXT_ran=1.0, BOS=1.0, TOK_NEXT_ran=1.0, SUFF_NEXT_n=1.0, SUFF_NEXT_ran=1.0, SUFF_ohn=1.0, PREF_J=1.0, POS_NEXT_vbd=1.0, SUFF_hn=1.0, SUFF_n=1.0, TOK_CAT_NEXT_ran=1.0, PREF_John=1.0}</strong></span>
</pre></div></li><li class="listitem">The next word in the sequence adds edge features—we won't bother showing you the node features:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Edge Feats:{PREV_TAG_TOKEN_CAT_PN_LET-CAP=1.0, PREV_TAG_PN=1.0}</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec158"/>How it works…</h2></div></div></div><p>As with other recipes, we won't bother discussing parts that are very similar to previous recipes—the relevant previous recipe here is the <span class="emphasis"><em>Modifying CRFs</em></span> recipe in <a class="link" href="ch04.html" title="Chapter 4. Tagging Words and Tokens">Chapter 4</a>, <span class="emphasis"><em>Tagging Words and Tokens</em></span>. This is exactly the same, except for the fact that we will add in a lot more features—perhaps, from unexpected sources.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>The tutorial for CRFs covers how to serialize/deserialize this class. This implementation does not cover it.</p></div></div><p>Object construction is similar to the <code class="literal">Modifying CRFs</code> recipe in <a class="link" href="ch04.html" title="Chapter 4. Tagging Words and Tokens">Chapter 4</a>, <span class="emphasis"><em>Tagging Words and Tokens</em></span>:</p><div class="informalexample"><pre class="programlisting">public FancyCrfFeatureExtractor()
  throws ClassNotFoundException, IOException {
  File posHmmFile = new File("models/pos-en-general" + "brown.HiddenMarkovModel");
  @SuppressWarnings("unchecked") HiddenMarkovModel posHmm = (HiddenMarkovModel)
  AbstractExternalizable.readObject(posHmmFile);

  FastCache&lt;String,double[]&gt; emissionCache = new FastCache&lt;String,double[]&gt;(100000);
  mPosTagger = new HmmDecoder(posHmm,null,emissionCache);
}</pre></div><p>The constructor sets up a <a class="indexterm" id="id509"/>part-of-speech tagger with a cache and<a class="indexterm" id="id510"/> shoves it into the <code class="literal">mPosTagger</code> member variable.</p><p>The following method does very little, except supplying an inner <code class="literal">ChunkerFeatures</code> class:</p><div class="informalexample"><pre class="programlisting">public ChainCrfFeatures&lt;String&gt; extract(List&lt;String&gt; tokens, List&lt;String&gt; tags) {
  return new ChunkerFeatures(tokens,tags);
}</pre></div><p>The <code class="literal">ChunkerFeatures</code> class is where things get more interesting:</p><div class="informalexample"><pre class="programlisting">class ChunkerFeatures extends ChainCrfFeatures&lt;String&gt; {
  private final Tagging&lt;String&gt; mPosTagging;

  public ChunkerFeatures(List&lt;String&gt; tokens, List&lt;String&gt; tags) {
    super(tokens,tags);
    mPosTagging = mPosTagger.tag(tokens);
  }</pre></div><p>The <code class="literal">mPosTagger</code> function is used to set up <code class="literal">Tagging&lt;String&gt;</code> for the tokens presented on class creation. This will be aligned with the <code class="literal">tag()</code> and <code class="literal">token()</code> superclass methods and be the source of part-of-speech tags as a node feature.</p><p>Now, we can get on with the feature extraction. We will start with edge features, as they are the simplest:</p><div class="informalexample"><pre class="programlisting">public Map&lt;String,? extends Number&gt; edgeFeatures(int n, int k) {
  ObjectToDoubleMap&lt;String&gt; feats = new ObjectToDoubleMap&lt;String&gt;();
  feats.set("PREV_TAG_" + tag(k),1.0);
  feats.set("PREV_TAG_TOKEN_CAT_"  + tag(k) + "_" + tokenCat(n-1), 1.0);
  return feats;
}</pre></div><p>The new feature<a class="indexterm" id="id511"/> is prefixed with <code class="literal">PREV_TAG_TOKEN_CAT_</code>, and <a class="indexterm" id="id512"/>the example is <code class="literal">PREV_TAG_TOKEN_CAT_PN_LET-CAP=1.0</code>. The <code class="literal">tokenCat()</code> method looks at the word shape feature for the previous token and returns it as a string. Look at the Javadoc for <code class="literal">IndoEuropeanTokenCategorizer</code> to see what is going on.</p><p>Next comes the node features. There are many of these; each will be presented in turn:</p><div class="informalexample"><pre class="programlisting">public Map&lt;String,? extends Number&gt; nodeFeatures(int n) {
  ObjectToDoubleMap&lt;String&gt; feats = new ObjectToDoubleMap&lt;String&gt;();</pre></div><p>The preceding code sets up the method with the appropriate return type. The next two lines set up some state to know where the feature extractor is in the string:</p><div class="informalexample"><pre class="programlisting">boolean bos = n == 0;
boolean eos = (n + 1) &gt;= numTokens();</pre></div><p>Next, we will compute the token categories, tokens, and part-of-speech tags for the current position, previous position, and the next position of the input:</p><div class="informalexample"><pre class="programlisting">String tokenCat = tokenCat(n);
String prevTokenCat = bos ? null : tokenCat(n-1);
String nextTokenCat = eos ? null : tokenCat(n+1);

String token = normedToken(n);
String prevToken = bos ? null : normedToken(n-1);
String nextToken = eos ? null : normedToken(n+1);

String posTag = mPosTagging.tag(n);
String prevPosTag = bos ? null : mPosTagging.tag(n-1);
String nextPosTag = eos ? null : mPosTagging.tag(n+1);</pre></div><p>The previous and next methods check if we're at the begin or end of the sentence and return <code class="literal">null</code> accordingly. The part-of-speech tagging is taken from the saved part-of-speech taggings computed in the constructor.</p><p>The token methods provide some normalization of tokens to compress all numbers to the same kind of value. This method is as follows:</p><div class="informalexample"><pre class="programlisting">public String normedToken(int n) {
  return token(n).replaceAll("\\d+","*$0*").replaceAll("\\d","D");
}</pre></div><p>This just takes every<a class="indexterm" id="id513"/> sequence of numbers and<a class="indexterm" id="id514"/> replaces it with <code class="literal">*D...D*</code>. For instance, <code class="literal">12/3/08</code> is converted to <code class="literal">*DD*/*D*/*DD*</code>.</p><p>We will then set feature values for the preceding, current, and following tokens. First, a flag indicates whether it begins or ends a sentence or an internal node:</p><div class="informalexample"><pre class="programlisting">if (bos) {
  feats.set("BOS",1.0);
}
if (eos) {
  feats.set("EOS",1.0);
}
if (!bos &amp;&amp; !eos) {
  feats.set("!BOS!EOS",1.0);
}</pre></div><p>Next, we will include the tokens, token categories, and their parts of speech:</p><div class="informalexample"><pre class="programlisting">feats.set("TOK_" + token, 1.0);
if (!bos) {
  feats.set("TOK_PREV_" + prevToken,1.0);
}
if (!eos) {
  feats.set("TOK_NEXT_" + nextToken,1.0);
}
feats.set("TOK_CAT_" + tokenCat, 1.0);
if (!bos) {
  feats.set("TOK_CAT_PREV_" + prevTokenCat, 1.0);
}
if (!eos) {
  feats.set("TOK_CAT_NEXT_" + nextToken, 1.0);
}
feats.set("POS_" + posTag,1.0);
if (!bos) {
  feats.set("POS_PREV_" + prevPosTag,1.0);
}
if (!eos) {
  feats.set("POS_NEXT_" + nextPosTag,1.0);
}</pre></div><p>Finally, we will add the <a class="indexterm" id="id515"/>prefix and suffix features, which add<a class="indexterm" id="id516"/> features for each suffix and prefix (up to a prespecified length):</p><div class="informalexample"><pre class="programlisting">for (String suffix : suffixes(token)) {
  feats.set("SUFF_" + suffix,1.0);
}
if (!bos) {
  for (String suffix : suffixes(prevToken)) {
    feats.set("SUFF_PREV_" + suffix,1.0);
    if (!eos) {
      for (String suffix : suffixes(nextToken)) {
        feats.set("SUFF_NEXT_" + suffix,1.0);
      }
      for (String prefix : prefixes(token)) {
        feats.set("PREF_" + prefix,1.0);
      }
      if (!bos) {
        for (String prefix : prefixes(prevToken)) {
          feats.set("PREF_PREV_" + prefix,1.0);
      }
      if (!eos) {
        for (String prefix : prefixes(nextToken)) {
          feats.set("PREF_NEXT_" + prefix,1.0);
        }
      }
      return feats;
    }</pre></div><p>After this, we will just return the feature mapping generated.</p><p>The <code class="literal">prefix</code> or <code class="literal">suffix</code> function is simply implemented with a list:</p><div class="informalexample"><pre class="programlisting">static int MAX_PREFIX_LENGTH = 4;
  static List&lt;String&gt; prefixes(String s) {
    int numPrefixes = Math.min(MAX_PREFIX_LENGTH,s.length());
    if (numPrefixes == 0) {
      return Collections.emptyList();
    }
    if (numPrefixes == 1) {
      return Collections.singletonList(s);
    }
    List&lt;String&gt; result = new ArrayList&lt;String&gt;(numPrefixes);
    for (int i = 1; i &lt;= Math.min(MAX_PREFIX_LENGTH,s.length()); ++i) {
      result.add(s.substring(0,i));
    }
    return result;
  }

  static int MAX_SUFFIX_LENGTH = 4;
  static List&lt;String&gt; suffixes(String s) {
    int numSuffixes = Math.min(s.length(), MAX_SUFFIX_LENGTH);
    if (numSuffixes &lt;= 0) {
      return Collections.emptyList();
    }
    if (numSuffixes == 1) {
      return Collections.singletonList(s);
    }
    List&lt;String&gt; result = new ArrayList&lt;String&gt;(numSuffixes);
    for (int i = s.length() - numSuffixes; i &lt; s.length(); ++i) {
      result.add(s.substring(i));
    }
    return result;
  }</pre></div><p>That's a nice<a class="indexterm" id="id517"/> feature set for your named-entity<a class="indexterm" id="id518"/> detector.</p></div></div></body></html>