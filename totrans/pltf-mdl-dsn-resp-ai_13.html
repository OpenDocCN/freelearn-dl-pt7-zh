<html><head></head><body>
		<div id="_idContainer268">
			<h1 id="_idParaDest-247" class="chapter-nu ber"><a id="_idTextAnchor267"/>13</h1>
			<h1 id="_idParaDest-248"><a id="_idTextAnchor268"/>Sustainable Model Life Cycle Management, Feature Stores, and Model Calibration</h1>
			<p>The primary objective of this chapter is to provide you with sustainability-related best practices that should be followed during the model development process. This aligns with the previous chapter, where we discussed organizational goals related to sustainable AI. Our aim is to encourage people across different levels of an organizational hierarchy to restructure the organizational roadmap and build trustworthy AI solutions. You will get an understanding of the importance of all aspects of AI ethics and the best practices that need to be followed. We will illustrate how to incorporate privacy, security, and sustainability in a feature <span class="No-Break">store example.</span></p>
			<p>In this chapter, you will learn how to improve model probability estimates to yield more accurate outcomes. We will look at adaptable systems in the context <span class="No-Break">of sustainability.</span></p>
			<p>By coupling sustainability with the concepts of model training and deployment, you will learn how to achieve sustainable, adaptable systems that facilitate collaboration and sharing. You will explore model calibration and learn about its importance when designing adaptable ethical <span class="No-Break">AI solutions.</span></p>
			<p>In this chapter, we will cover <span class="No-Break">the following:</span></p>
			<ul>
				<li>Sustainable model <span class="No-Break">development practices</span></li>
				<li>Explainability, privacy, and sustainability in <span class="No-Break">feature stores</span></li>
				<li>Exploring <span class="No-Break">model calibration</span></li>
				<li>Building sustainable, <span class="No-Break">adaptable systems</span></li>
			</ul>
			<h1 id="_idParaDest-249"><a id="_idTextAnchor269"/>Sustainable model development practices</h1>
			<p>Along with<a id="_idIndexMarker1578"/> the ethical deployment of AI, sustainability helps us all to move one step closer to better ecological integrity and social justice. Throughout the process of building AI products – from idea generation to training, retuning, implementation, and governance – we should be aware of the environmental impact of our actions and AI solutions and ensure that they are friendly to future generations. When we consider the impact of our work using metrics, we work more responsibly and ethically. It is essential to have an organizational roadmap that describes best practices at each stage of model development so that change management is minimal and easy to track. This will also allow innovation in sustainable AI and compel organizations to hit CO<span class="subscript">2</span> emission targets for AI training, validation, deployment, <span class="No-Break">and usage.</span></p>
			<p>In the next subsection, we will explore how organizations, people, and processes can be oriented toward sustainable AI model development. This will help teams to deploy models that are successful in the <span class="No-Break">long run.</span></p>
			<p>First, let’s look at how guidelines for building sustainable, trustworthy frameworks can be set out <span class="No-Break">by organizations.</span></p>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor270"/>Organizational standards for sustainable, trustworthy frameworks</h2>
			<p>In <a href="B18681_03.xhtml#_idTextAnchor066"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, we <a id="_idIndexMarker1579"/>learned about regulations and policies <a id="_idIndexMarker1580"/>surrounding trustworthy AI; the systems we deploy to production need to be 100% compliant with regulations. We have certain open source frameworks, as listed here, that can be reused and integrated into <span class="No-Break">existing platforms:</span></p>
			<ul>
				<li><strong class="bold">TensorFlow Model Remediation</strong>: This is a library developed by Google that aims to limit biases during model preprocessing training <span class="No-Break">or postprocessing.</span></li>
				<li><strong class="bold">TensorFlow Privacy</strong>: This is a library developed by Google that enables ML optimizers to optimize the objective function of ML models while incorporating <span class="No-Break">differential privacy.</span></li>
				<li><strong class="bold">AI Fairness 360</strong>: This is a library developed by IBM to detect and <span class="No-Break">mitigate bias.</span></li>
				<li><strong class="bold">Responsible AI Toolbox</strong>: This is a library developed by Microsoft to help access, develop, and deploy AI solutions ethically in a <span class="No-Break">trustworthy environment.</span></li>
				<li><strong class="bold">XAI</strong>: This is a library that facilitates model explainability through model evaluation <span class="No-Break">and monitoring.</span></li>
				<li><strong class="bold">TensorFlow Federated</strong>: This is a library to support distributed training involving <span class="No-Break">multiple clients.</span></li>
			</ul>
			<p>In addition, we need to know how to incorporate the best practices in existing technological platforms, which can act as pointers for the evaluation of trustworthy frameworks. Let’s summarize the common model governance and risk management activities that should be practiced by organizational leadership. To establish the best ethical<a id="_idIndexMarker1581"/> and <a id="_idIndexMarker1582"/>sustainable practices in the development and deployment life cycle, the questions listed in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.1</em> need to <span class="No-Break">be answered:</span></p>
			<table id="table001-7" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Model practice</strong></p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table"><strong class="bold">Key questions</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" colspan="2">
							<p class="P-Regular-Table"><strong class="bold">Model development phase</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Identification</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Has the organization identified and listed key regulatory tools based on federal guidance?</p>
							<p class="P-Regular-Table">For example, for banking and insurance, certain financial and risk models need to be used to evaluate a framework.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Inventorying</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">What is the model inventorying strategy?</p>
							<p class="P-Regular-Table">For example, is there a model classification process in place during the model development life cycle?</p>
							<p class="P-Regular-Table">Have different risks been accounted for, and how has this affected the ranking of the different models?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Naming policy and security</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">How is the model namespace managed?</p>
							<p class="P-Regular-Table">Does it take into consideration the domain problem as well as the business use case?</p>
							<p class="P-Regular-Table">How are the key stakeholders involved in model version control and security management?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Formalized policy and procedure</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Are there proper guidelines, audit checklists, and authorized personnel involved in formalizing the standards and guidelines for model development, validation, use, monitoring, and retirement?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Compliance</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Is there a regulatory requirements checklist?</p>
							<p class="P-Regular-Table">Are there frequent audits for regulatory requirements?</p>
							<p class="P-Regular-Table">Are there trained personnel involved who work closely with ML, data engineering, and analytics teams to verify that all enterprise systems abide by confirmation and compliance?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Research and the application of best practices</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Are there initiatives to deep dive and conduct research based on the problem that the model is serving?</p>
							<p class="P-Regular-Table">Are there properly trained personnel assigned to research and disseminate the current model’s best (standard) practices?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Documentation</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Are there well-documented models within the scope of the problem definition and business objectives?</p>
							<p class="P-Regular-Table">Are there definitions of feature stores and established communication processes when teams share and reuse features, models, and data?</p>
							<p class="P-Regular-Table">How do we ensure data and model availability without violating security, while ensuring awareness among team members?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Sharing and reuse</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">How are ML models and features shared in centralized and <strong class="bold">Federated Learning</strong> (<strong class="bold">FL</strong>)?</p>
							<p class="P-Regular-Table">How are built-in security, privacy, and notification strategies incorporated for feature updates across teams?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" colspan="2">
							<p class="P-Regular-Table"><strong class="bold">Model validation phase</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Testing and validation</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">How is the unit testing of models and system testing incorporated when models are put in place in the serving framework?</p>
							<p class="P-Regular-Table">What are the interventions and supervision techniques employed to ensure formal model validation procedures, along with the model-serving APIs?</p>
							<p class="P-Regular-Table">What are the guidelines and practices that are in place to certify model interoperability across platforms?</p>
							<p class="P-Regular-Table">What emission metrics have been considered?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Challenge/effective criticism</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">How are reviews/challenges encountered in model input, output, training, validation, and testing processes addressed?</p>
							<p class="P-Regular-Table">What agile processes are in place to incorporate incremental model changes to ensure that a model is accurate and, at the same time, private, fair, and interpretable?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" colspan="2">
							<p class="P-Regular-Table"><strong class="bold">Model implementation phase</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Implementation and use of governance</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">What are the standard operating procedures in place for models, with special reference to refreshes, queries, the use of links, data dumps, and the handling of input identification?</p>
							<p class="P-Regular-Table">Are the recommended practices for shared control tools being followed?</p>
							<p class="P-Regular-Table">Have shared techniques for users, such as watch windows, balance checks, and backups, been established?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" colspan="2">
							<p class="P-Regular-Table"><strong class="bold">Ongoing monitoring of models</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Remediation</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Is there the right sort of model tracking for remediation and the correction of errors?</p>
							<p class="P-Regular-Table">Are there defined metrics in place for sustainability across platforms, ML models, and data pipelines?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Change management</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">How are model changes tracked and updated in documents?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Auditing and reviewing</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Are the best audit and review processes established to validate lines of defense and ensure the quality of model output?</p>
							<p class="P-Regular-Table">How have benchmarks been established to differentiate and scale models in larger enterprise-grade platforms?</p>
							<p class="P-Regular-Table">How will we effectively control costs from our benchmarks across different client platforms?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Definition of risk tolerance, appetite, and risk thresholds</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">What are the different threshold levels of model acceptance?</p>
							<p class="P-Regular-Table">Have we formulated a risk tolerance level based on the problem scope, domain, and volume of data that can be accepted within certain margins of uncertainty?</p>
							<p class="P-Regular-Table">Are we maintaining and establishing financial impact/dollar variances relative to the expectation of each of the models?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Identifying secondary risks</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">What are the techniques to assess risks arising out of managing other risks, such as delays in meeting a deadline or disrupting data schema?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Identifying operational risks</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">How do we define operational risks in which a model is implemented?</p>
							<p class="P-Regular-Table">Does this involve the consideration and periodic review of different features, such as the discount rate, or ensure the validation of operational procedures, such as version control?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Identifying emerging risks</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">What are the new or potential emerging risks due to continuous refreshes of models over time?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Tracking</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">What are the steps for model tracking for traditional-based, deep learning-based, and FL-based models?</p>
							<p class="P-Regular-Table">How do the tracking steps allow revisions and provide room for additional use throughout the entire model life cycle?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Ongoing monitoring</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">What monitoring tools and dashboards are in place to allow periodic monitoring over time of accuracy, relevance, and interpretability?</p>
							<p class="P-Regular-Table">How are model interpretability tools used to communicate to business stakeholders about business model risks, data, and concept drift?</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style" colspan="2">
							<p class="P-Regular-Table"><strong class="bold">AI application usage</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p class="P-Regular-Table">Terms and conditions for licenses, terms of use, and click-thrus</p>
						</td>
						<td class="No-Table-Style">
							<p class="P-Regular-Table">What audit processes do we have to validate licenses, terms of use, and warranties? How do we ensure the proper usage of AI applications by including language dissuading the usage or liability of unintended applications? For example, allowing waivers for any use of life-saving equipment, mission-critical avionics, military applications, or munitions, and at the same time disavowing export to or use in embargoed countries.</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 13.1 – Key model practices and questions</p>
			<p>Each business unit needs to answer the preceding questions to ensure the proper alignment of modeling processes to corporate strategies. This, in turn, helps organizations to become aware of sustainability practices and directs their focus to data and model development and governance strategies. Overall, the aim is to facilitate reuse <a id="_idIndexMarker1583"/>and <a id="_idIndexMarker1584"/>coordination in an environmentally friendly way. Hence, we need a unified model of chain management that prevents the potential misuse of ML models by increasing transparency and interpretability and reducing key-person dependence. It is also highly important to set the foundations on which the board of directors governs a corporation and the ethical boundaries under which the CxO office operates. CxO (where CxO refers to the roles of CEO, CTO, and CIO) initiatives that structure and drive the aforementioned processes under the purview of data teams are key to the success of any AI-driven business. Using such initiatives, we can not only increase profit margins by several percentage points but also save billions of dollars by using resources judiciously and employing trained <span class="No-Break">data teams.</span></p>
			<p>Organizations that fail to answer the preceding questions and establish the required processes suffer from time and monetary losses. In addition, they fail to sustain their AI-driven <a id="_idIndexMarker1585"/>business <a id="_idIndexMarker1586"/>use cases in this <span class="No-Break">competitive world.</span></p>
			<p>Having understood this, we can now move on to looking at <span class="No-Break">feature stores.</span></p>
			<h1 id="_idParaDest-251"><a id="_idTextAnchor271"/>Explainability, privacy, and sustainability in feature stores</h1>
			<p>In <a href="B18681_10.xhtml#_idTextAnchor218"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we<a id="_idIndexMarker1587"/> introduced <a id="_idIndexMarker1588"/>the<a id="_idIndexMarker1589"/> concept of feature stores and demonstrated with an example how online feature stores can be used. Furthermore, in the previous section, we learned about the important aspects of sustainable model training and deployment and the best practices for tracking sustainable model metrics across different cloud providers. We also saw in <a href="B18681_12.xhtml#_idTextAnchor243"><span class="No-Break"><em class="italic">Chapter 12</em></span></a> that FL provides a training environment to allow sustainability, by allowing the local training of devices. Hence, we must try to leverage FL in training ML models in healthcare, retail, banking, and other industry verticals in scenarios where generic model representation plays an important role as computational power <span class="No-Break">is limited.</span></p>
			<p>In this section, let’s dig deeper into creating explainable, private, and sustainable <span class="No-Break">feature stores.</span></p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor272"/>Feature store components and functionalities</h2>
			<p>Let’s now explore<a id="_idIndexMarker1590"/> how the different components of a feature store serve their functional roles in a distributed architecture, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer254" class="IMG---Figure">
					<img src="image/Figure_13.1_B18681.jpg" alt="Figure 13.1 – The different components of an ethical feature store"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – The different components of an ethical feature store</p>
			<p>The following list looks at each component in <span class="No-Break">more detail:</span></p>
			<ul>
				<li>The first <a id="_idIndexMarker1591"/>component is the data source unit, where data can be received and aggregated from third-party sources. The data can be raw data, SQL data, or event data. To build sustainable ML models and feature stores, we need to have FL capabilities built into the infrastructure, where the data is mostly event data from mobile devices, IoT devices, or <strong class="bold">Internet of Medical Things </strong>(<strong class="bold">IoMTs</strong>). FL definitely provides us with an <a id="_idIndexMarker1592"/>opportunity to practice sustainable model development. However, without FL, having an automated feedback loop with re-training capabilities can also help us to create sustainable <span class="No-Break">ML models.</span></li>
				<li>The second component comprises the ingestion and feature engineering pipelines, where data needs to be anonymized to protect <strong class="bold">Personally Identifiable Information</strong> (<strong class="bold">PII</strong>), which we discussed in <a href="B18681_02.xhtml#_idTextAnchor040"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. To highlight and build <a id="_idIndexMarker1593"/>sustainable pipelines, we must support reuse in our design to extract relevant features that can be used across teams. In addition, to support federated collaboration-based learning methodologies and deployment strategies, feature engineering pipelines should be distributed across cloud and edge devices. This will help to distribute load and control the emission rates of centralized training procedures. For example, <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.2</em> illustrates a working methodology of FL in edge networks<a id="_idIndexMarker1594"/> using <strong class="bold">Deep Neural Networks</strong> (<strong class="bold">DNNs</strong>). We have also shown different kinds of cloud services (from Google Cloud Platform) that can be used in the <span class="No-Break">server component.</span></li>
			</ul>
			<div>
				<div id="_idContainer255" class="IMG---Figure">
					<img src="image/Figure_13.2_B18681.jpg" alt="Figure 13.2 – FL in edge networks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – FL in edge networks</p>
			<p>In the<a id="_idIndexMarker1595"/> preceding figure, <em class="italic">Figure 13.2</em>, the client aggregates data from numerous IoT devices and engages in local deep learning-based model training, while a few servers deployed at several edges are responsible for the first level of model aggregation, which ultimately gets aggregated in the cloud. The aggregated model is then pushed to the edges, and local clients continue with training on their local datasets.</p>
			<ul>
				<li>The third component involves feature management and explainability for online and offline processing. In <a href="B18681_04.xhtml#_idTextAnchor093"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, we discussed how we can create sandbox environments or isolation units through the use of appropriate security rules to process and store features as required by different teams with different levels of sensitivity (refer to <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.12</em> in <a href="B18681_04.xhtml#_idTextAnchor093"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>). This component is also engaged in satisfying the pillars of AI ethics that govern feature explainability, bias identification, and feature recommendations. The fairness side of data and features also needs to be satisfied by this component, as biased datasets and extracted features can lead to biased <span class="No-Break">ML models.</span></li>
				<li>The fourth component is storage, where metadata, online, and offline features can be stored. This includes both SQL and NoSQL databases and the cache, where storage can be maintained on disk as well as in memory for <span class="No-Break">fast retrieval.</span></li>
				<li>The fifth component involves the ways that users can perform time-travel queries that execute high-speed searches to return data at a given point in time (where we can learn about the history of the data and record its lineage), the data for a given time interval, and the changes made to data since a given point in time. Time-travel queries are executed efficiently using indexes (bloom filters, <em class="italic">z</em>-indexes, and data-skipping indexes), which deserve special mention, as they reduce the amount of data that needs to be read from a filesystem or <span class="No-Break">object store.</span></li>
				<li>The final component (along with model training, deployment, and monitoring) is also responsible for identifying model drift. With access privileges set on individual <a id="_idIndexMarker1596"/>models, this component promotes the comparison of model scoring metrics in order to take quick action on <span class="No-Break">model re-training.</span></li>
			</ul>
			<p>Next, let’s move on to learning about feature stores <span class="No-Break">for FL.</span></p>
			<h2 id="_idParaDest-253"><a id="_idTextAnchor273"/>Feature stores for FL</h2>
			<p>We saw <a id="_idIndexMarker1597"/>how feature stores play an important role in ML model reuse <a id="_idIndexMarker1598"/>and features for centralized learning in <a href="B18681_10.xhtml#_idTextAnchor218"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>. Now, let’s investigate how we can leverage some of the existing concepts of feature store pipelines for collaborative learning, as in the case of FL frameworks. This will facilitate the development and deployment of federated tools, allowing the technical teams of organizations, educational institutes, and partners to come together and share data, ML models, and <span class="No-Break">their features.</span></p>
			<p>We will see <a id="_idIndexMarker1599"/>how <strong class="bold">FeatureCloud AI Store</strong> for FL (primarily built for biomedical research) can be used across other domains by providing a platform to unify a set of <span class="No-Break">ready-to-use apps.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.3</em> demonstrates the different components of FeatureCloud, explaining how collaborating parties can work together to create a certified feature store in the cloud that can include new <span class="No-Break">third-party apps:</span></p>
			<div>
				<div id="_idContainer256" class="IMG---Figure">
					<img src="image/Figure_13.3_B18681.jpg" alt="Figure 13.3 – A private FeatureCloud store for FL"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – A private FeatureCloud store for FL</p>
			<p>These <a id="_idIndexMarker1600"/>unified<a id="_idIndexMarker1601"/> federated apps can yield similar results to centralized ML in scalable platforms. With increased collaboration, built-in privacy mechanisms<a id="_idIndexMarker1602"/> such as <strong class="bold">homomorphic encryption</strong>, secure multi-party computation, and differential privacy become of great importance, as they protect sensitive information. FeatureCloud AI Store is a feature store that removes the restrictions of conventional FL-based modeling by <a id="_idIndexMarker1603"/>redefining the <strong class="bold">Application Programming Interface</strong> (<strong class="bold">API</strong>), making it easier for developers to reuse and share novel apps from external developers. Along with an open API system, it has the support of deployment distribution, allowing the use of algorithms through configurable workflows. The feature store is transparent and open to external developers, who are free to add and publish their own federated apps, making the system an efficient collaboration medium for data, models, <span class="No-Break">and apps.</span></p>
			<p>The app interface available in the third-party apps, as shown in the preceding figure, provides detailed information on the different categories of apps by displaying basic information about them, including short descriptions, keywords, user ratings, and certification status. In addition, each app – as well as being classified into either preprocessing, analysis, or evaluation – is equipped with a graphical frontend or a simple configuration file to set app parameters and adapt them to <span class="No-Break">different contexts.</span></p>
			<p>Any app that is part of FeatureCloud runs inside a Docker container, which can exchange data and other essential information with other apps using the FeatureCloud API. The stores accelerate the development of federated applications by providing a template and a test simulator <span class="No-Break">for testing.</span></p>
			<p>This kind of shared app environment comes with app documentation, search and filter functionality, and an app certification process to promote privacy standards in the AI store. The certification process enforces strong guidelines about testing frequently for privacy leaks; the <a id="_idIndexMarker1604"/>failure of these tests leads to a notification for <a id="_idIndexMarker1605"/>concerned developers to address the issue. However, it also comes with a functionality whereby a new certification process is issued whenever an application <span class="No-Break">is updated.</span></p>
			<p>One key drawback of this kind of collaborative, sustainable platform is that the coordinator has access to all the individual models before aggregating them. Hence, the framework comes with different privacy measures, such as secure multi-party computation and differential privacy, to handle any <span class="No-Break">privacy leaks.</span></p>
			<p>A federated workflow can be designed by bringing in all collaborating partners that download and start the client-side FeatureCloud controller on their machines using Docker. The coordination can be smoothly established across user apps from the AI store, as users can create an account on the FeatureCloud website. It facilitates cross-institutional data and algorithm sharing and analysis by integrating <strong class="bold">Cross-Validation</strong> (<strong class="bold">CV</strong>), standardization, model<a id="_idIndexMarker1606"/> training, and model evaluation procedures through separate apps in the workflow, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer257" class="IMG---Figure">
					<img src="image/Figure_13.4_B18681.jpg" alt="Figure 13.4 – A FeatureCloud workflow using app-based FL"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – A FeatureCloud workflow using app-based FL</p>
			<p>As multiple apps form a workflow, the consecutive execution of those apps one after the other completes the running of a unique workflow. Outputs or results from one application can be consumed by another application, and the overall workflow progress can be tracked or monitored. The results from the workflow can be shared among the participating <a id="_idIndexMarker1607"/>entities<a id="_idIndexMarker1608"/> to understand and evaluate each step of the overall modeling process. FeatureCloud can solve practical problems in biomedicine and <span class="No-Break">other domains.</span></p>
			<p>Now that we understand how FeatureCloud operates, let’s try to understand some of its <span class="No-Break">important properties.</span></p>
			<h3>The properties of FeatureCloud</h3>
			<p>Here, we will learn <a id="_idIndexMarker1609"/>how a collaborative cloud environment created through a set of apps in a federated setup can help us to serve <span class="No-Break">better workflows:</span></p>
			<ul>
				<li>FeatureCloud offers different combinations of ML algorithms to solve common problems, by efficiently utilizing them from apps in the AI store or <span class="No-Break">app templates.</span></li>
				<li>It uses a common standardized data format that offers an easy way to compose apps in a workflow, from data ingestion and mutual data consumption to the generation of <span class="No-Break">the output.</span></li>
				<li>It drives collaborative research to serve broader objectives by bringing in less-experienced developers as well as experienced professionals to create <span class="No-Break">customized workflows.</span></li>
			</ul>
			<p>In this section, we learned about concepts related to federated feature stores that are ethically compliant. Now, let’s learn how to enhance predictability in AI/ML-governed systems by determining the likelihood of predictions for different classes. This will help us to design realistic systems with <span class="No-Break">minimal drift.</span></p>
			<p>So, let’s explore <a id="_idIndexMarker1610"/>model calibration, a postprocessing technique that not only improves model probability estimates but also helps to create sustainable, robust <span class="No-Break">model predictions.</span></p>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor274"/>Exploring model calibration</h1>
			<p>Calibration is a<a id="_idIndexMarker1611"/> model postprocessing technique that is used to improve probability estimates. The objective is to improve a model in such a way that the distribution and behaviors of the predicted and observed <span class="No-Break">probabilities match.</span></p>
			<p>Model calibration is required for mission-critical applications where the likelihood of a data point being associated with a class is very important – for instance, building a model to predict the likelihood of an individual <span class="No-Break">being ill.</span></p>
			<p>Let’s try to understand calibration better with the help of a classic cat-dog <span class="No-Break">classifier example.</span></p>
			<p>Let’s assume we’re working with a cat-dog classifier where all the input images are only cats or dogs. Now, if a model thinks that the input image is of a cat, it outputs <strong class="source-inline">1</strong>; conversely, if it thinks that the input image is of a dog, it outputs <strong class="source-inline">0</strong>. Our models are essentially continuous mapping functions – that is, they output values between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. This can be achieved mathematically, using approaches such as having a sigmoid function as the activation function in the final layer. A good classifier is likely to produce scores near <strong class="source-inline">1</strong> for cats and scores near <strong class="source-inline">0</strong> <span class="No-Break">for dogs.</span></p>
			<p>But are these scores between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> representative of <span class="No-Break">actual probabilities?</span></p>
			<div>
				<div id="_idContainer258" class="IMG---Figure">
					<img src="image/Figure_13.5_B18681.jpg" alt="Figure 13.5 – A cat﻿-dog classifier example"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – A cat<a id="_idTextAnchor275"/>-dog classifier example</p>
			<p>Does the score of 0.18 in the preceding figure mean that 18% of the input image is a cat? If a model is well calibrated, then hypothetically, yes! We can interpret its results as probabilities. The main concern is how to determine whether our model is well calibrated. Consider another real-life example – looking outside to see a storm brewing when the probability of rain given by your<a id="_idIndexMarker1612"/> weather app is less than 5%. This happens when a model is not well calibrated and, thus, performs poorly. Let’s look at how to determine whether a model is <span class="No-Break">well calibrated.</span></p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor276"/>Determining whether a model is well calibrated</h2>
			<p>Referring to the <a id="_idIndexMarker1613"/>previous example of the cat-dog classifier, in order to understand whether these numbers can be seen as probabilities, we need to plot a reliability graph. In a reliability graph, the <em class="italic">x</em> axis plots the score given by the model – that is, the predicted probabilities of the positive class (in our example, the <strong class="source-inline">cat</strong> class). Whenever an image is received, it will be put in the correct bracket based on the score provided by the model – for instance, if the image has a score of 0.9, it will be placed in the 0.8 to 1 bracket, where the model presumes that it is more likely to be an image of a cat. For an image score of 0.06, the image will be placed in the 0 to 0.2 bracket, for images that are very likely to be of a dog. We can continue in this manner for many images and keep populating our graph. Once have added a large number of images, we can look at the <em class="italic">y</em> axis, which plots the number of images that are actually of cats; in other words, the <em class="italic">y</em> axis represents the actual frequencies of the positive class. For a well-calibrated classifier, the points will be close to the <span class="No-Break">diagonal line.</span></p>
			<div>
				<div id="_idContainer259" class="IMG---Figure">
					<img src="image/Figure_13.6_B18681.jpg" alt="Figure 13.6 – The scores output by the model and the proportion of images that were actually of cats"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – The scores output by the model and the proportion of images that were actually of cats</p>
			<p>The preceding graph shows an example of a poorly calibrated model. For the range of 0.4 to 0.6, if the score truly represented a probability, then there would be a 40–60% chance of any particular image in the stack being of a cat. However, we only see a 25% chance of an image being of a cat in this bracket; this indicates a <a id="_idTextAnchor277"/>poorly <span class="No-Break">calibrated model.</span></p>
			<p>Calibration applies to both classification and regression tasks. Here, we have discussed an example of a classification task. In regression tasks, we focus on estimating the probability <a id="_idIndexMarker1614"/>distribution of the predicted values. The calibrated regressor model defines the mean prediction, and the expected distribution around this mean value reflects the uncertainty associated with <span class="No-Break">the prediction.</span></p>
			<p>Before delving into different calibration techniques, let’s first understand w<a id="_idTextAnchor278"/>hy <span class="No-Break">miscalibration occurs.</span></p>
			<h3>Why miscalibration occurs</h3>
			<p>Miscalibration is<a id="_idIndexMarker1615"/> a common problem for ML models that are not trained using a probabilistic framework and where there is bias in the training data. In most scenarios, an inherent  characteristic of a model is responsible for whether that model ends up being calibrated or not. In the case of logistic regression, we leverage the loss function; hence, no additional post-training is required. This is due to the fact that the probabilities generated by it are calibrated in advance. The independence assumption in Gaussian Naive Bayes can cause poorly calibrated probability estimates, nudging them close to 0 or 1. Nonetheless, in the case of a random forest classifier, values near 0 or 1 are rarely achieved, since an average of multiple inner decision trees is calculated. The only surefire way to accomplish 0 or 1 is when each model returns a value near 0 or 1, which is an intriguing<a id="_idIndexMarker1616"/> occasion from a <span class="No-Break">probability perspective.</span></p>
			<p>Let’s now learn about vari<a id="_idTextAnchor279"/>ous <span class="No-Break">calibration techniques.</span></p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor280"/>Calibration techniques</h2>
			<p>Some classification <a id="_idIndexMarker1617"/>models, such as <strong class="bold">Support Vector Machines</strong> (<strong class="bold">SVMs</strong>), <strong class="bold">k-nearest neighbors</strong>, and <a id="_idIndexMarker1618"/>decision <a id="_idIndexMarker1619"/>trees, either do not provide probability scores or give poor estimates. Such approaches need to be coerced into giving a probability-like score, and thus, calibratio<a id="_idTextAnchor281"/>n prior to usage <span class="No-Break">is required.</span></p>
			<p><strong class="bold">Platt scaling</strong> and <strong class="bold">isotonic regression</strong> are two of the most prominent calibration techniques in use. Both of these approaches transform the output of the model into a likelihood score and, hence, calibrate it. We will study th<a id="_idTextAnchor282"/>ese techniques in the <span class="No-Break">following sections.</span></p>
			<h3>Platt scaling</h3>
			<p>The principle of <a id="_idIndexMarker1620"/>this technique involves the<a id="_idIndexMarker1621"/> transformation of classification model output into a probability distribution. Put simply, Platt scaling is used when a calibration plot looks like a sigmoid curve and is efficient for <span class="No-Break">small datasets.</span></p>
			<div>
				<div id="_idContainer260" class="IMG---Figure">
					<img src="image/Figure_13.7_B18681.jpg" alt="Figure 13﻿.7 – Platt scaling illustration"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13<a id="_idTextAnchor283"/>.7 – Platt scaling illustration</p>
			<p>Platt scaling is a modified sigmoid function and solves an optimization problem to get A and B. It returns a degree of certainty about the actual outcome instead of returning class labels. With<a id="_idIndexMarker1622"/> classification models such as SVM, as<a id="_idIndexMarker1623"/> discussed previously, we utilize particular transformation techniques in order to calibrate our model and obtain a probability as <span class="No-Break">the outcome.</span></p>
			<h3><a id="_idTextAnchor284"/>Isotonic regression</h3>
			<p>Compared to Platt scaling, isotonic regression is a more powerful <a id="_idIndexMarker1624"/>calibration technique that is capable of correcting any <a id="_idIndexMarker1625"/>monotonic distortion. It is used when a calibration plot does not look like a sigmoid curve. Isotonic regression breaks a curve into multiple linear models and, hence, requires more points than Platt scaling. This technique tries to find the best set of predictions that are non-decreasing and are as close as possible to the primal (original data) points. The approach entails implementing regression on the primal, or original, calibration curve when applied to the issue of calibration. Unlike Platt scaling, isotonic regression is not recommended for small datasets to avoid overfitting and is intended to be used on large datasets, due to its <span class="No-Break">outlier sensitivity.</span></p>
			<p><a id="_idTextAnchor285"/>Next, we will see, with a hands-on example, how model calibration can affect <span class="No-Break">model reliability.</span></p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor286"/>Model calibration using scikit-learn</h2>
			<p>In this section, we will use the scikit-learn module to synthetically create data to compare the reliability and performance of uncalibrated and calibrated models. This is how we <span class="No-Break">do this:</span></p>
			<ol>
				<li>As a first step, we import all the necessary modules. Here, we will be using scikit-learn both for synthetic data generation and to build classifiers – both uncalibrated <span class="No-Break">and calibrated:</span><pre class="console">
from sklearn.datasets import make_classification
from sklearn.calibration import CalibratedClassifierCV
from sklearn.calibration import calibration_curve
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, brier_score_loss
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns</pre></li>
				<li>Here, we first consider a balanced dataset. The following statement generates 10,000 samples, each with 10 features, and equal distribution for the <span class="No-Break">two classes:</span><pre class="console">
X, y = make_classification(n_samples=10000, n_features=1000, n_redundant=10, random_state=37, weights=[0.5])</pre></li>
			</ol>
			<p>For convenience, we will put the features and labels in respective DataFrames:</p>
			<pre class="console">
Xs = pd.DataFrame(X)
ys = pd.DataFrame(y, columns=['label'])</pre>
			<ol>
				<li value="3">Now, let’s verify whether the data is balanced or not using the <span class="No-Break">following code:</span><pre class="console">
#plotting bar chart
ys.value_counts().plot(kind='bar')</pre></li>
			</ol>
			<p>In the following plot, you can see equal distribution of the labels:</p>
			<div>
				<div id="_idContainer261" class="IMG---Figure">
					<img src="image/Figure_13.8_B18681.jpg" alt="Figure 13.8 – A bar chart showing class distribution in the synthetic data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8 – A bar chart showing class distribution in the synthetic data</p>
			<ol>
				<li value="4">Next, we split our data into training, validation, and test datasets with a ratio <span class="No-Break">of 60:20:20:</span><pre class="console">
X_train_raw, X_test, y_train_raw, y_test = train_test_split(Xs, ys, test_size=0.20, shuffle=False)
X_train, X_val, y_train, y_val = train_test_split(X_train_raw, y_train_raw, test_size=0.20, shuffle=False)</pre></li>
			</ol>
			<p>Then, we train a simple logistic regression classifier on the training dataset:</p>
			<pre class="console">
clf = LogisticRegression()
clf.fit(X_train, y_train.values.ravel())
y_pred_uncal = clf.predict_proba(X_test)[:, 1]</pre>
			<p>Let’s see the performance of our classifier using the area under the <span class="No-Break">curve metric:</span></p>
			<pre class="console">
roc_auc_score(y_test, y_pred_uncal)
&gt;&gt;&gt; 0. 9185432154389126</pre>
			<p>The <strong class="source-inline">auc</strong> value of our classifier is <strong class="source-inline">0.92</strong> – <span class="No-Break">not bad!</span></p>
			<p>We also check our classifier performance using the <strong class="bold">Brier score</strong>. It is a measure of the accuracy of probabilistic predictions, mathematically <a id="_idIndexMarker1626"/>expressed <span class="No-Break">as follows:</span></p>
			<p>Brier score = 1<em class="italic">N∑i</em><em class="italic"> = </em>1<em class="italic">Np</em>(<em class="italic">y</em><span class="subscript">i</span>) <em class="italic">− </em><span class="No-Break"><em class="italic">o</em></span><span class="No-Break">(</span><span class="No-Break"><em class="italic">y</em></span><span class="No-Break"><span class="subscript">i</span></span><span class="No-Break">)</span><span class="No-Break"><span class="superscript">2</span></span></p>
			<p>Here, <em class="italic">N</em> is the number of instances, <em class="italic">p</em>(<em class="italic">y</em><span class="subscript">i</span>) is the predicted probability that an instance, <strong class="source-inline">i</strong>, belongs to the <em class="italic">y</em><span class="subscript">i</span> class, and <em class="italic">o</em>(<em class="italic">y</em><span class="subscript">i</span>) is the actual outcome of the instance, <strong class="source-inline">i</strong>, with <em class="italic">o</em>(<em class="italic">y</em><span class="subscript">i</span>) = 1 if <em class="italic">y</em><span class="subscript">i</span> is <a id="_idIndexMarker1627"/>the <a id="_idIndexMarker1628"/>true class and <em class="italic">o</em>(<em class="italic">y</em><span class="subscript">i</span>) = <span class="No-Break">0 otherwise:</span></p>
			<pre class="console">
brier_score_loss(y_test, y_pred_uncal)
&gt;&gt;&gt; 0.10919273032433353</pre>
			<p>The Brier score ranges from 0 to 1, with a score of 0 indicating perfect accuracy and a score of 1 indicating the worst possible performance. Our classifier shows a Brier score of 0.11, again a <span class="No-Break">good performance.</span></p>
			<p>Let’s now repeat the process for a calibrated classifier, for the same <span class="No-Break">balanced dataset:</span></p>
			<pre class="source-code">
Calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid')
calibrated_clf.fit(X_val, y_val.values.ravel())
y_pred_cal = calibrated_clf.predict_proba(X_test)[:, 1]
print(f'ROC Score {roc_auc_score(y_test, y_pred_cal):.2f} \nBrier Score {brier_score_loss(y_test, y_pred_cal):.2f})
&gt;&gt;&gt; ROC Score 0.92
&gt;&gt;&gt; Brier Score 0.11</pre>
			<p>From both the accuracy and Brier score values, we can see that for the balanced dataset, the<a id="_idIndexMarker1629"/> uncalibrated<a id="_idIndexMarker1630"/> classifier is as good as the <span class="No-Break">calibrated classifier:</span></p>
			<pre class="source-code">
plt.rcParams.update({'font.size': 10})
frac_of_positives_uncal, pred_prob_uncal = calibration_curve(y_test, y_pred_uncal, n_bins=10)
sns.lineplot(x=pred_prob_uncal, y=frac_of_positives_uncal)
frac_of_positives_cal, pred_prob_cal = calibration_curve(y_test, y_pred_cal, n_bins=10)
sns.lineplot(x=pred_prob_cal, y=frac_of_positives_cal)
plt.grid(linestyle='-', linewidth=0.2)
plt.title("Reliability curve balanced data")
xlabel = plt.xlabel("Probability of positive")
ylabel = plt.ylabel("Fraction of positives")
plt.legend(labels = ['Uncalibrated', 'Calibrated'])
ticks = [0, 0.2, 0.4, 0.6, 0.8, 1]
xticks = plt.xticks(ticks)
yticks = plt.yticks(ticks)</pre>
			<p>The following plotted reliability curves <span class="No-Break">confirm this:</span></p>
			<div>
				<div id="_idContainer262" class="IMG---Figure">
					<img src="image/Figure_13.9_B18681.jpg" alt="Figure 13.9 – A reliability curve for balanced data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.9 – A reliability curve for balanced data</p>
			<p>Let’s now repeat<a id="_idIndexMarker1631"/> the<a id="_idIndexMarker1632"/> process for an <span class="No-Break">imbalanced dataset:</span></p>
			<pre class="source-code">
X, y = make_classification(n_samples=10000, n_features=1000, n_redundant=10, random_state=37, weights=[0.7])
Xs = pd.DataFrame(X)
ys = pd.DataFrame(y, columns=['label'])</pre>
			<p>We can verify that the classes are highly imbalanced by plotting the frequency chart <span class="No-Break">for labels:</span></p>
			<pre class="source-code">
#plotting bar chart
ys.value_counts().plot(kind='bar')</pre>
			<p>Here is the plot <span class="No-Break">for it:</span></p>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<img src="image/Figure_13.10_B18681.jpg" alt="Figure 13.10 – A bar chart showing class distribution in the synthetic data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.10 – A bar chart showing class distribution in the synthetic data</p>
			<p>As before, we split <a id="_idIndexMarker1633"/>the<a id="_idIndexMarker1634"/> dataset into training, validation, and test datasets with a <span class="No-Break">60:20:20 ratio:</span></p>
			<pre class="source-code">
X_train_raw, X_test, y_train_raw, y_test = train_test_split(Xs, ys, test_size=0.20, shuffle=False)
X_train, X_val, y_train, y_val = train_test_split(X_train_raw, y_train_raw, test_size=0.20, shuffle=False)</pre>
			<p>Now, we explore the <span class="No-Break">uncalibrated model:</span></p>
			<pre class="source-code">
clf = LogisticRegression()
clf.fit(X_train, y_train.values.ravel())
y_pred_uncal = clf.predict_proba(X_test)[:, 1]
print(f'ROC Score {roc_auc_score(y_test, y_pred_uncal):.2f} \nBrier Score {brier_score_loss(y_test, y_pred_uncal):.2f}')
&gt;&gt;&gt; ROC Score 0.88
&gt;&gt;&gt; Brier Score 0.04</pre>
			<p>And finally, we explore the <span class="No-Break">calibrated model:</span></p>
			<pre class="source-code">
calibrated_clf = CalibratedClassifierCV(clf,cv=3, method='isotonic')
calibrated_clf.fit(X_val, y_val.values.ravel())
y_pred_cal = calibrated_clf.predict_proba(X_test)[:, 1]
print(f'ROC Score {roc_auc_score(y_test, y_pred_uncal):.2f} \nBrier Score {brier_score_loss(y_test, y_pred_uncal):.2f}')
&gt;&gt;&gt; ROC Score 0.88
&gt;&gt;&gt;Brier Score 0.04</pre>
			<p>Here is<a id="_idIndexMarker1635"/> the<a id="_idIndexMarker1636"/> graph <span class="No-Break">for it:</span></p>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<img src="image/Figure_13.11_B18681.jpg" alt="Figure 13.11 – A reliability curve for unbalanced data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.11 – A reliability curve for unbalanced data</p>
			<p>We can see that calibrated models give a more reliable prediction compared to the uncalibrated model for <span class="No-Break">unbalanced data.</span></p>
			<p>Let’s now explore<a id="_idIndexMarker1637"/> how<a id="_idIndexMarker1638"/> to design adaptable systems by considering dynamic <span class="No-Break">calibration curves.</span></p>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor287"/>Building sustainable, adaptable systems</h1>
			<p>We have <a id="_idIndexMarker1639"/>looked at the step-by-step processes<a id="_idIndexMarker1640"/> for model governance and sustainable model training and deployment. We also now understand how important it is to build reusable <span class="No-Break">feature stores.</span></p>
			<p>We understand that without a feature store, we will end up with a separate feature engineering pipeline for each model that we want to deploy. Duplicate pipelines inevitably lead to added compute costs and data lineage overheads, as well as lots of engineering effort. However, the endeavor of building a sustainable feature store will be fruitless if it’s not robust and resilient enough to adapt to data and <span class="No-Break">concept drift.</span></p>
			<p>Even when we design large-scale distributed ML systems, we should think about building an adaptable system with the ability to detect data drift, concept drift, and calibration drift. This will facilitate continuous monitoring and mean that we can manage new, incoming data from different sources. For example, in a retail system, we may encounter new customers with varied buying patterns, while in a healthcare system, we might see new patients with new diseases coming into the system. To build adaptable systems that can deal with continuous change, we must understand how calibration and dynamic calibration curves help us to detect and improve model performance. While calibration curves provide diagrammatic representations of model performance across a range of predicted probabilities, dynamic calibration curves help us to visualize the development of true calibration curves over a time series by considering each observation in place. Furthermore, a dynamic curve, by representing the weighted distribution of predicted probabilities, helps us to estimate the fitted values on an evaluation set. This kind of fitting helps to evaluate the performance of the fitted curve. You can read more about this <span class="No-Break">here: </span><a href="https://www.sciencedirect.com/science/article/pii/S1532046420302392"><span class="No-Break">https://www.sciencedirect.com/science/article/pii/S1532046420302392</span></a><span class="No-Break">.</span></p>
			<p>The feature stores we build should have automation built in to support dynamic calibration curves and evaluate changes in model performance from new observations. Feature store drift detectors can alert teams about miscalibration as new data accumulates and, furthermore, automate the re-training process using the recent data window. Most importantly, if we can accurately detect drift in time series, and the trends seem to be permanent, then we should also see a change <span class="No-Break">in calibration.</span></p>
			<p>MLOps teams should be cautious of the speed and magnitude of calibration drift once it begins, which can be efficiently detected by accurate monitoring systems. The change should come from a relatively high number of observations to give us confidence that there is indeed a transition from a calibrated to a <span class="No-Break">miscalibrated model.</span></p>
			<p>Dynamic calibration curves are not strong enough to learn miscalibration in low-density, high-probability ranges, due to limited knowledge in the concerned <span class="No-Break">probability area.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.12</em> illustrates a dynamic calibration curve, specifically how its response evolves over a period due to changes in <span class="No-Break">model performance.</span></p>
			<p>By miscalibration (e<span class="subscript">t </span>), we mean the deviation of the curve from the ideal calibration, which is obtained by computing the absolute difference between an observation’s predicted probability (p<span class="subscript">t</span>) and<a id="_idIndexMarker1641"/> the fitted value of the <a id="_idIndexMarker1642"/>current curve at the predicted <span class="No-Break">probability (</span><a href="https://www.sciencedirect.com/science/article/pii/S1532046420302392"><span class="No-Break">https://www.sciencedirect.com/science/article/pii/S1532046420302392</span></a><span class="No-Break">).</span></p>
			<div>
				<div id="_idContainer265" class="IMG---Figure">
					<img src="image/Figure_13.12_B18681.jpg" alt="Figure 13.12 – Dynamic calibration curves over a period"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.12 – Dynamic calibration curves over a period</p>
			<p>Some important areas of focus to detect calibration drift that necessitates model updates are <span class="No-Break">listed here:</span></p>
			<ul>
				<li>A dynamic calibration curve should aim to showcase performance degradation, even without highlighting the <span class="No-Break">specific degradation.</span></li>
				<li>Miscalibrations can exhibit fluctuations between over-prediction and under-prediction zones, with a wide boundary of probability. This is further signified by calibration errors varying before and <span class="No-Break">after drift.</span></li>
				<li>Calibration drift often results from <span class="No-Break">transitional states.</span></li>
				<li>Variance in dynamic calibration curves can be handled by tweaking the step size, or initial learning rate, of the Adam optimizer (a deep learning optimizer used for the first-order, gradient-based optimization of stochastic objective functions) to quickly respond to changes in <span class="No-Break">model performance.</span></li>
				<li>Significance <a id="_idIndexMarker1643"/>margins<a id="_idIndexMarker1644"/> for model performance need to be properly set and defined for very small changes <span class="No-Break">in calibration.</span></li>
				<li>Based on model complexity, the minimum window size needs to be defined to capture sufficient samples for <span class="No-Break">model updates.</span></li>
			</ul>
			<p>As you can see in <span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.13</em>, the online and offline feature spaces are shared to create a combined shared <a id="_idIndexMarker1645"/>feature called <strong class="bold">encoders</strong>. Refer to the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer266" class="IMG---Figure">
					<img src="image/Figure_13.13_B18681.jpg" alt=""/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.13 – The importance of a calibration drift detector in feature stores</p>
			<p>While <strong class="bold">Team B</strong> contributes to the offline feature generation process, <strong class="bold">Team A</strong> and <strong class="bold">Team C</strong> take part in<a id="_idIndexMarker1646"/> the <a id="_idIndexMarker1647"/>online feature generation process. If a sudden change in any of the online features triggers a calibration drift and harms the model’s performance, there must be an immediate alert triggered to <strong class="bold">Team B</strong>, as that team also accesses the shared feature space. A model trained using the shared features of <strong class="bold">Team B</strong> can also experience calibration drift and must take immediate corrective action by <span class="No-Break">triggering re-training.</span></p>
			<p>Here, we observe the importance of a calibration drift detection algorithm for running and <span class="No-Break">triggering alerts.</span></p>
			<p>In addition to calibration drift, we studied in <a href="B18681_07.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic">Chapter 7</em></span></a> how data and concept drift play an important role and how adaptable frameworks have evolved to handle those kinds of drift. Now, it is also important that we broaden our scope to apply that in the context of sustainable training environments and incorporate adaptable frameworks in feature stores. We have learned about sustainable feature stores; now, let’s integrate the idea of concept drift into <span class="No-Break">FL environments.</span></p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor288"/>Concept drift-aware federated averaging (CDA-FedAvg)</h2>
			<p>We have<a id="_idIndexMarker1648"/> seen <a id="_idIndexMarker1649"/>in FL design patterns that the asynchronous method of sending model updates serves power-hungry devices well, giving them the flexibility to participate in training whenever they have power available. It also aids in concept drift detection and adaptation techniques in a collaborative environment, with the server orchestrating <span class="No-Break">the process.</span></p>
			<p>The server acts as an orchestrator to aggregate models from individual devices, and the local devices become the deciding body to select the data and the time to trigger the process of <span class="No-Break">local training.</span></p>
			<p>The model aggregated by the central server is globally agreed upon (each individual model from the participating entities is agreed upon and averaged by the server), and the server broadcasts it to all the clients. The clients become responsible for detecting and handling changes in data and model patterns. The clients train the models on their local training datasets and manage drift in a <span class="No-Break">three-stage process:</span></p>
			<ol>
				<li>Timing-based <span class="No-Break">drift identification</span></li>
				<li>Drift <strong class="bold">Root Cause Analysis</strong> (<strong class="bold">RCA</strong>) to identify the cause of the drift and the data <a id="_idIndexMarker1650"/>instance where it starts <span class="No-Break">to occur</span></li>
				<li>Response and mitigation methodology to adapt drift to yield <span class="No-Break">high-accuracy models</span></li>
			</ol>
			<p>As clients are exposed to newly collected data from several input sources, they are equipped with drift detection algorithms to identify new concepts (drift detection) and learn from them. The drift adaptation techniques built in help to analyze how the data or model differs between two timestamps, as well as analyzing the drift’s nature or what causes it, taking remedial actions accordingly. The local clients are equipped with short-term memory and long-term memory to respond to drift as soon as it is detected. While short-term memory helps to store and compare data instances collected by the client in the latest time interval, long-term memory stores data samples for events or concepts that were old and appeared in previous instances, as compared to a current point <span class="No-Break">in time.</span></p>
			<p>This kind of adaptation with short-term and long-term memory helps us to understand data, model patterns, and concepts and trigger model training or re-training locally, by storing data records over a long period. This process happens in a sequence of events, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer267" class="IMG---Figure">
					<img src="image/Figure_13.14_B18681.jpg" alt="Figure 13.14 – CDA-FedAvg in FL clients"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.14 – CDA-FedAvg in FL clients</p>
			<p>As shown in <a id="_idIndexMarker1651"/>the<a id="_idIndexMarker1652"/> preceding figure, we take actions based on the presence or absence of drift in the <span class="No-Break">following manner:</span></p>
			<ol>
				<li>Both short-term and long-term memory are initialized as empty when the local training process has not kicked <span class="No-Break">off yet.</span></li>
				<li>Once the client acquires the first data, it gets stored in long-term memory as the initial concept and is used for the first training and <span class="No-Break">local update.</span></li>
				<li>Any subsequent data received by the client is stored in short-term memory to <span class="No-Break">evaluate drift.</span></li>
				<li>Upon identification of potential drift, the new data acquired that’s relevant to the new concept gets stored in <span class="No-Break">long-term memory.</span></li>
				<li>In addition, a new case of drift triggers a new round of training at the <span class="No-Break">client’s end.</span></li>
			</ol>
			<p>Thus, a well-designed system should not only run the best concept drift detection algorithm, whether in a centralized or FL environment, but also should be well calibrated to<a id="_idIndexMarker1653"/> experience <a id="_idIndexMarker1654"/>minimal drift due to changes in <span class="No-Break">input data.</span></p>
			<h1 id="_idParaDest-260"><a id="_idTextAnchor289"/>Summary</h1>
			<p>In this chapter, we have learned about the main guiding principles that leadership and stakeholders should use to take direct action to enable the best model-building practices in their organizational culture. This chapter gave us a deep insight into how to make the best use of FL in designing federated feature stores to encourage collaborative research through the use of APIs. In addition, we explored the concept of adaptable frameworks in feature stores that are also ethically compliant concerning privacy, interpretability, <span class="No-Break">and fairness.</span></p>
			<p>Furthermore, we learned how, with the help of calibration, to improve a model when its output suggests that it has regions of high probabilities, which may not be authentic. Metrics that take the prediction score as input can also be taken into consideration – for instance, the <strong class="bold">Area Under the Curve-Receiver Operating Characteristics</strong> (<strong class="bold">AUC-ROC</strong>) score is based on positioning predictions, but it falls short when it comes to accurately <span class="No-Break">calibrated probabili<a id="_idTextAnchor290"/>ties.</span></p>
			<p>Calibration is advantageous in complex ML systems and real-world scenarios. It modifies the results of ML models after training and preserves the consistency of the output. Performing calibration can affect model accuracy, and in general, it is observed that calibrated models tend to be slightly less accurate than uncalibrated ones. However, this detrimental effect on accuracy is extremely low, and the advantages calibration offers are much more significant. Calibrating a model is a crucial step in improving prediction performance if a model’s objective is to achieve good <span class="No-Break">probability prediction.</span></p>
			<p>In this chapter, we identified the sustainability aspects of ethical models and how FL and federated feature stores can be hooked together, with sustainable energy solutions, to compute and control carbon emissions. After gaining a thorough understanding of the best design frameworks for ethical ML modeling, in the next chapter, let’s study how to apply these patterns in different domains to solve real-world <span class="No-Break">use cases.</span></p>
			<h1 id="_idParaDest-261"><a id="_idTextAnchor291"/>Further reading</h1>
			<ul>
				<li><em class="italic">The FeatureCloud AI Store for Federated Learning in Biomedicine and </em><span class="No-Break"><em class="italic">Beyond</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2105.05734.pdf"><span class="No-Break">https://arxiv.org/pdf/2105.05734.pdf</span></a></li>
				<li><em class="italic">Detection of calibration drift in clinical prediction models to inform model </em><span class="No-Break"><em class="italic">updating</em></span><span class="No-Break">: </span><a href="https://www.sciencedirect.com/science/article/pii/S1532046420302392"><span class="No-Break">https://www.sciencedirect.com/science/article/pii/S1532046420302392</span></a></li>
				<li><em class="italic">Calibration Techniques and it’s importance in Machine </em><span class="No-Break"><em class="italic">Learning</em></span><span class="No-Break">: </span><a href="https://kingsubham27.medium.com/calibration-techniques-and-its-importance-in-machine-learning-71bec997b661"><span class="No-Break">https://kingsubham27.medium.com/calibration-techniques-and-its-importance-in-machine-learning-71bec997b661</span></a></li>
				<li><em class="italic">Calibration, Imbalanced </em><span class="No-Break"><em class="italic">Data</em></span><span class="No-Break">: </span><a href="https://amueller.github.io/aml/04-model-evaluation/11-calibration.html"><span class="No-Break">https://amueller.github.io/aml/04-model-evaluation/11-calibration.html</span></a></li>
				<li><em class="italic">Brier Score: Understanding Model </em><span class="No-Break"><em class="italic">Calibration</em></span><span class="No-Break">: </span><a href="https://neptune.ai/blog/brier-score-and-model-calibration"><span class="No-Break">https://neptune.ai/blog/brier-score-and-model-calibration</span></a></li>
				<li><em class="italic">How to Calibrate Probabilities for Imbalanced </em><span class="No-Break"><em class="italic">Classification</em></span><span class="No-Break">: </span><a href="https://machinelearningmastery.com/probability-calibration-for-imbalanced-classification/"><span class="No-Break">https://machinelearningmastery.com/probability-calibration-for-imbalanced-classification/</span></a></li>
				<li><em class="italic">Classifier </em><span class="No-Break"><em class="italic">calibration</em></span><span class="No-Break">: </span><a href="https://towardsdatascience.com/classifier-calibration-7d0be1e05452"><span class="No-Break">https://towardsdatascience.com/classifier-calibration-7d0be1e05452</span></a></li>
				<li><em class="italic">Why model calibration matters and how to achieve </em><span class="No-Break"><em class="italic">it</em></span><span class="No-Break">: </span><a href="https://www.unofficialgoogledatascience.com/2021/04/why-model-calibration-matters-and-how.html"><span class="No-Break">https://www.unofficialgoogledatascience.com/2021/04/why-model-calibration-matters-and-how.html</span></a></li>
				<li><em class="italic">What does model calibration mean in ML in </em><span class="No-Break"><em class="italic">Python</em></span><span class="No-Break">: </span><a href="https://www.projectpro.io/recipes/what-does-model-calibration-mean"><span class="No-Break">https://www.projectpro.io/recipes/what-does-model-calibration-mean</span></a></li>
				<li><em class="italic">A guide to model </em><span class="No-Break"><em class="italic">calibration</em></span><span class="No-Break">: </span><a href="https://wttech.blog/blog/2021/a-guide-to-model-calibration/"><span class="No-Break">https://wttech.blog/blog/2021/a-guide-to-model-calibration/</span></a></li>
				<li><em class="italic">Calibration in Machine </em><span class="No-Break"><em class="italic">Learning</em></span><span class="No-Break">: </span><a href="https://medium.com/analytics-vidhya/calibration-in-machine-learning-e7972ac93555"><span class="No-Break">https://medium.com/analytics-vidhya/calibration-in-machine-learning-e7972ac93555</span></a></li>
				<li><em class="italic">Prediction &amp; Calibration Techniques to Optimize Performance of Machine Learning </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break">: </span><a href="https://towardsdatascience.com/calibration-techniques-of-machine-learning-models-d4f1a9c7a9cf"><span class="No-Break">https://towardsdatascience.com/calibration-techniques-of-machine-learning-models-d4f1a9c7a9cf</span></a></li>
			</ul>
		</div>
	</body></html>