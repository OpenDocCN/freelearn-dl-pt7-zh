- en: Personal and Home IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you are fully equipped with **machine learning** (**ML**) and **deep
    learning** (**DL**) knowledge, and have learned how to use it for big data, image
    tasks, text tasks, and time series data, it is time to explore some real uses
    of the algorithms and the techniques that you have learned. This chapter and the
    following two chapters will now concentrate on some specific case studies. This
    chapter will focus on personal and home **Internet of Things** (**IoT**) use cases.
    We will cover the following in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Successful IoT applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wearables and their role in personal IoT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to monitor heart using ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What makes home smart home
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Devices used in smart home
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application of Artificial Intelligence in predicting human activity recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personal IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The personal IoT is dominated by the use of wearables, technological devices
    designed to be worn on body, they are used in tandem with an app on a smartphone.
    The first wearable available was the Pulsar Calculator watch made by Time Computer
    Inc, USA (at that time known as **Hamilton Watch Company**). It was a standalone
    device not connected to the internet. Soon, with the growth of the internet, wearables
    that can connect to the internet became a fad. The wearables market is expected
    to jump from an estimate of **325** **million** in **2016** to over **830** **million**
    by **2020**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11b5c44d-8c4c-4040-adeb-2c5498122d54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This graph shows the number of wearables worldwide from 2016–2021 (data source:
    Statista). With so many devices connected online, continuously generating data,
    AI/ML tools are a natural choice to analyze this data and make informed decisions.
    In this section, you will learn about some successful personal IoT applications.'
  prefs: []
  type: TYPE_NORMAL
- en: SuperShoes by MIT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Holding your mobile in one hand and navigating your way along the road with
    the help of *Google Maps*, how often have you thought that it is cumbersome? How
    often have you wished for magic slippers that will take you anywhere you want?
    SuperShoes by *MIT Media Lab* ([https://www.media.mit.edu/projects/supershoes/overview/](https://www.media.mit.edu/projects/supershoes/overview/))
    are almost like those magic slippers; they allow the user to navigate through
    the sidewalks without any need to check the smartphone screen.
  prefs: []
  type: TYPE_NORMAL
- en: SuperShoes have flexible insoles, embedded with vibrating motors under the toes.
    They connect wirelessly to an app on the smartphone. The app not only allows the
    user to interface with the SuperShoes, but it also stores likes/dislikes, hobbies,
    shops, foods, people, interests, and so on in a cloud account. The vibrating motors
    generate tickles that communicate with the user. Once the user enters a destination
    on the app, the shoes start their work. If the left toe tickles then the user
    is supposed to take a left turn; if the right toe tickles then the user has to
    take a right turn. When there is no tickle, then the user has to continue straight.
    If both tickle repeatedly, the user has arrived at their destination.
  prefs: []
  type: TYPE_NORMAL
- en: Besides navigation, it can also recommend places of interest nearby; the user
    updates their likes and dislikes on the cloud. Based on the likes and dislikes
    of the user, SuperShoe also gives an indication (both toes tickle once) when the
    user is near a recommended place of interest. Another interesting feature of SuperShoes
    is that it can give reminders as well; it can remind you if you have a task on
    a location nearby.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hardware required to make this shoe is very simple, it requires the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Three vibrotactile ticklers to tickle the toes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A capacitive pad to sense the walk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A microcontroller that takes the commands from the app, and accordingly, controls
    the ticklers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Bluetooth device to connect with the smartphone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batteries to power the entire system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The magic is performed by the software coded into the app. You can learn more
    about the SuperShoes at this website: [http://dhairyadand.com/works/supershoes](http://dhairyadand.com/works/supershoes).'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous glucose monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A major application of AI has been in IoT for healthcare, with one of the most
    successful commercial applications being continuous monitoring of the body's glucose
    level. Abbott's FreeStyle CGM, DexCom CGM, and Medtronic CGM are some of the commercially
    available brands.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous glucose monitoring** (**CGM**) allows people suffering from diabetes
    to check their body''s glucose level in real time. It helps them in monitoring
    the readings over a period of time, and the data can also be used to prediction
    of future glucose level, thus helping them to deal with conditions like hypoglycemia.'
  prefs: []
  type: TYPE_NORMAL
- en: In CGM, normally a sensor is placed either under the skin of the belly or adhered
    to the back of your arm. The sensor sends the readings to a connected pager/smartphone
    app. The app has additional AI-based algorithms that can inform the user of any
    clinically relevant glucose patterns. The availability of this data not only helps
    the user to proactively manage their glucose highs and lows, but additionally,
    it can also provide an insight into the impact that meals, exercise, or illness
    may have on a person's glucose levels.
  prefs: []
  type: TYPE_NORMAL
- en: The sensors have a lifespan ranging from 7 to 14 days, normally this time is
    sufficient for a medical practitioner to understand the person's lifestyle, and
    accordingly, suggest changes.
  prefs: []
  type: TYPE_NORMAL
- en: Hypoglycemia prediction using CGM data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once a person has CGM data, it can be analyzed using AI/ML to gather more information
    or to make a prediction about hypoglycemia. In this section, we see how we can
    use the algorithms we had learned in the previous chapters to make a glucose-predictor
    system.
  prefs: []
  type: TYPE_NORMAL
- en: We will build our predictor based on the research paper *Glucose Concentration
    can be Predicted Ahead in Time From Continuous Glucose Monitoring sensor Time-Series*
    by Sparacino et al. ([10.1109/TBME.2006.889774](https://doi.org/10.1109/TBME.2006.889774)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the paper, the CGM time series glucose data is described by a times series
    model; the paper considered two models, one a simple first order polynomial and
    second a first order autoregressive model. The model parameters are fitted at
    each sampling time, *t[s]*, against the past glucose data. Here, we will implement
    the simple first order polynomial using scikit linear regressor that we learned
    about in [Chapter 3](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml), *Machine Learning
    for IoT*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the modules pandas to read the `csv` file, NumpPy for data processing,
    Matplolib for plotting, and scikit-learn for the linear regressor, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the data obtained from your CGM in the data folder and read it. We require
    two values, the glucose reading and its time. The data that we are using has these
    available in two CSV files, `ys.csv` and `ts.csv`. The first one contains the
    glucose value and the second one contains the corresponding time, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'According to the paper, we define two parameters of the predictive model `ph`,
    the prediction `horizon`, and `mu` the forgetting factor. Please refer to the
    we mentioned earlier paper for more details on these two parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the arrays to hold our predicted values, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We now read the CGM data simulating the real-time acquisition and predict the
    glucose level `ph` minutes forward. All the past data is used to determine the
    model parameters, however, each has a different contribution decided by the individual
    weight assigned to it `mu^k` (to the sample taken `k` instants before the actual
    sampling time):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the prediction is lagging behind the actual. The normal glucose
    level lies in the range `70` to `180`. Below `70`, the patient can suffer from
    hypoglycemia and above `180` it can lead to hyperglycemia. Let us see the plot
    of our predicted data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/df2045fe-1257-4d6e-84e0-713cd388dea0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The RMSE error will be 27 for the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The code is located at `Chapter09/Hypoglycemia_Prediction.ipynb` notebook. The
    glucose-prediction system is available in many commercial products. You can make
    one too, based on the model that we just made. You can also use an artificial
    neural network to make a similar prediction with better results (refer to  [https://www.ncbi.nlm.nih.gov/pubmed/20082589](https://www.ncbi.nlm.nih.gov/pubmed/20082589)).
  prefs: []
  type: TYPE_NORMAL
- en: Heart monitor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another very useful personal application of AI in IoT is in the detection of
    heart disease. A large number of wearables exist that can be used to monitor and
    record heart rate. The data can be used to predict any harmful heart condition.
    Here, we will employ AI/ML tools to predict cardiac **arrhythmia**, a group of
    conditions where the heart rate is irregular; it can be either too fast (above
    100 beats per minute) or too slow (below 60 beats per minute). The data used is
    taken from the *UCI Machine learning R**epository* dataset: [https://archive.ics.uci.edu/ml/datasets/heart+Disease](https://archive.ics.uci.edu/ml/datasets/heart+Disease).
    The dataset consists of 76 attributes, not all required for prediction of the
    presence of disease; the dataset has a goal field associated with each data row.
    It has five possible values 0–4, the value 0 indicates a healthy heart, any other
    value means there is a disease. The problem can be broken into a binary classification
    problems for better accuracy. The code is inspired from the GitHub link of Mohammed
    Rashad, it is shared under the GNU GPL 3.0 license: [https://github.com/MohammedRashad/Deep-Learning-and-Wearable-IoT-to-Monitor-and-Predict-Cardiac-Arrhytmia](https://github.com/MohammedRashad/Deep-Learning-and-Wearable-IoT-to-Monitor-and-Predict-Cardiac-Arrhytmia).
    The complete code can be accessed from GitHub repository under `Chapter09/Heart_Disease_Prediction.ipynb`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step as always is to import the necessary modules. Since we are now
    classifying the patients as suffering from heart disease or not, we will need
    a classifier. Here for simplicity, we use the `SVC` classifier. You can experiment
    with the MLP classifier, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, read the dataset, preprocess the dataset to select the attributes you
    will be considering. We chose 13 attributes from 76, and then we convert the target
    from a multi-class value to binary class. Finally, the data is split into the
    train and test dataset, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define the model to be used. Here we are using a support vector classifier,
    using the `fit` function to train the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us see its performance on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that it provides an accuracy of 74%, using MLP, we can increase
    it further. But do remember to normalize all the input features before using the
    MLP classifier. Following is the confusion matrix of our trained support vector
    classifier on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows the confusion matrix for the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe756965-f750-4f3a-a070-01330b4ac16d.png)'
  prefs: []
  type: TYPE_IMG
- en: You can train your model on the same dataset and use your trained model to predict
    heart conditions for your friends, family, or clients.
  prefs: []
  type: TYPE_NORMAL
- en: Digital assistants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Digital assistants are one of the oldest conceived AI applications. Initial
    attempts at digital assistants never really took off. But with the advent and
    mass spread of smartphones, today we have a large number of digital assistants
    providing services such as dialing a phone number, writing a text message, scheduling
    appointments, or even searching the internet for you. You can ask them for recommendations
    for nearby restaurants and bars or any other similar thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the following are popular digital assistants:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Siri**: Developed by Apple, it allows the user to send/make calls, add appointments
    in the calendar, play music or video, and even send a text. Today, a voice-activated
    interface is available on almost all Apple products.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cortana**: Created by Microsoft, it helps you to stay on schedule by reminding
    you to do things based on time, place, or even people. You can ask Cortana to
    order lunch for you or use any other app it partners with. It comes integrated
    with Edge  and invokes a voice-activated speaker featuring Cortana.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alexa**: Developed by Amazon, this is available with Amazon Echo smart speakers.
    It can play music, make a to-do list, set alarms for you, play audio books, and
    provide real-time information on stocks, weather, and more. It is also capable
    of voice interaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Assistant**: This is a voice-controlled smart assistant. It provides
    continued conversation, that is you don''t have to say *Hey Google* for any follow-up
    requests, once you start talking, it listens for a response without needing the
    triggering phrase. It can also recognize the voice profiles for different people
    and can tailor its response according to the personal likes and dislikes of that
    person. It is available not only on Android smartphones but also on Google Home.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2018, Google went even further, releasing Google Duplex, an assistant that
    can make calls for you and book your appointments. It talks like a human, and
    also understands the context when speaking.
  prefs: []
  type: TYPE_NORMAL
- en: IoT and smart homes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A close friend of mine was always worried about his ageing mother, she was left
    home alone, while he, his wife, and kids were out. As her health started deteriorating,
    he asked for a piece of advice. The solution was simple; he installed CCTV cameras
    in all the rooms, interfaced with a mobile app. The cameras were connected to
    the internet, now, no matter where he is, he can check-in home to ensure the well-being
    of his mother.
  prefs: []
  type: TYPE_NORMAL
- en: CCTV, smart lightning, smart speakers, and so on connected to the internet help
    automate a lot of tasks at home, and what you get is a smart home. Most smart
    home systems at present work through a voice command interface, where you can
    use a set of commands to control the particular device. For example, in Amazon's
    Echo Dot, you can ask it to search or play a particular song. You can ask Apple's
    Siri to use your phone to call a friend, all by simple voice interface. Most of
    these devices are using AI/ML in some form, but home automation can be further
    advanced by employing AI/ML. For example, in the case of my friend, an AI system
    can be trained to identify activity from video, or to detect intrusion in the
    home. The possibilities are infinite. With the right data and sufficient computing
    power, you are limited only by your imagination.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will see some existing home automation products, and see
    how we can further use AI to augment the automation.
  prefs: []
  type: TYPE_NORMAL
- en: Human activity recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most researched smart home application is **human activity recognition**
    (**HAR**). There are many companies trying to develop apps that keep track of
    physical activity and its corresponding calorie burn count. Health and fitness
    no doubt is big business. Besides its application in fitness and health, HAR can
    also be useful in elder care or rehabilitation centers. There have been many approaches
    to perform HAR, two of which as the follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Use cameras (or radar or similar devices) to record human activity and classify
    it using a DL approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The individuals use wearable sensors (similar to accelerometers in smartphones)
    whose data is recorded and used to predict the activity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both approaches have their pros and cons. We will go through them in further
    detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: HAR using wearable sensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A large number of vendors have wearable watches and bracelets with fitness trackers.
    These watches and bracelets have GPS, accelerometer, gyroscope, heart rate sensor,
    and/or ambient light sensors. Employing **sensor fusion**, they combine the output
    of these sensors to make a prediction about the activity. Due to the temporal
    nature of the data, it is a challenging time series classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fitbit ([https://www.fitbit.com/smarttrack](https://www.fitbit.com/smarttrack)),
    a premier company in the field of fitness trackers, use a technology it calls
    **SmartTrack**, which recognizes activities with continuous movement or light
    movement. It uses the intensity and patterns of the movement to classify the activity.
    It classifies the activity in seven classes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Walking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aerobic workout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elliptical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outdoor bike
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swimming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apple Watch ([https://www.apple.com/in/apple-watch-series-4/workout/](https://www.apple.com/in/apple-watch-series-4/workout/))
    offers tough competition to Fitbit. Working on an iOSoperating system, it comes
    with fall detection, along with many other health tracking features. By analyzing
    the wrist trajectory and impact acceleration, it detects if the person is falling
    and can also initiate an emergency call. The Apple watch, by default, classifies
    activities into three groups: walking, exercise, and standing. The exercise (workouts)
    are further classified in another domain, such as indoor run, outdoor run, skiing,
    snowboarding, yoga, and even hiking.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to try making a similar app using your smartphone sensors, the
    first thing you will need is data. Following, we present an implementation of
    HAR using random forest, the code has been adapted from the GitHub link of Nilesh
    Patil, Data Scientist at the University of Rochester: [https://github.com/nilesh-patil/human-activity-recognition-smartphone-sensors](https://github.com/nilesh-patil/human-activity-recognition-smartphone-sensors).'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is from the paper *Davide Anguita, Alessandro Ghio, Luca Oneto,
    Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity
    Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks,
    Computational Intelligence and Machine Learning, ESANN 2013\. Bruges, Belgium
    24-26 April 2013.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Available at the UCI ML website: [https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones#](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones#).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each record in the dataset contains:'
  prefs: []
  type: TYPE_NORMAL
- en: Triaxial acceleration from the accelerometer (total acceleration) and the estimated
    body acceleration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triaxial angular velocity from the gyroscope
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 561-feature vector with time and frequency domain variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its activity label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An identifier of the subject who carried out the experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data is classified into six categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Laying
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walk-down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walk-up
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we use random forest classifier of the scikit-learn to classify the data.
    The necessary modules needed for the implementation are imported in the first
    step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We read the data and divide it into `train` and `test` datasets, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The data consists of 561 features however, not all are equally important. We
    can choose the more important features by making a simple random forest classifier,
    and choosing only the most important ones. In this implementation, it is done
    using two steps. Initially, we get the list of important features and arrange
    them in descending order of importance. Then the number and the features are found
    by grid hypertuning. The results of hypertuning are shown in the curve. We can
    see that after about 20 features, there is no significant improvement in **out
    of bag** (**OOB**) accuracy using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/192c852f-e7d6-4b50-a10e-07a0fbe2b223.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also see the average importance of the top 25 features in the following
    diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a50b2557-fe4d-4eec-a2b8-ae21cbf41606.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the same manner, we can hypertune the number of tree parameter. Here, we
    restricted ourselves to four important features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/8ca940bc-92b6-464e-953f-1f6c67d35399.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can see that a random forest with about four important features and
    `50` trees can give a good OOB accuracy. Hence our final model is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in a test data accuracy of 94%. Following you can see the confusion
    matrix for the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/5b71c386-b294-4b2f-8887-92cf66ef4cc7.png)'
  prefs: []
  type: TYPE_IMG
- en: The complete code, along with the data exploration, is available at the GitHub
    repository, `Chapter09/Human_activity_recognition_using_accelerometer.ipynb`.
    The advantage of using accelerometer data is that it is gathered from wearable
    devices, and hence, requires no installation on the premises. Another advantage
    is that it is textual data, hence, requires fewer computation resources than video
    data.
  prefs: []
  type: TYPE_NORMAL
- en: HAR from videos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another way to detect human activity is through videos. In this case, we will
    have to use a DL model such as CNN to get good results. A good dataset for classified
    videos is available from Ivan Laptev and Barbara Caputo ([http://www.nada.kth.se/cvap/actions/](http://www.nada.kth.se/cvap/actions/)).
    It contains six types of action: walking, jogging, running, boxing, hand waving,
    and hand clapping, in different scenarios. Each video has been recorded using
    a camera with 25 fps. The spatial resolution is 160 × 120, and of an average length
    of four seconds. It has in total 599 videos with about 100 for each of the six
    categories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the problems with video data is that it is computationally expensive,
    thus it will be important to reduce the dataset, and a few ways of doing this
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Since color has no role in the activity, the images can be converted from three-
    channel color images to two-dimensional grayscale images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The video is of four seconds at 25 frames in one second, most of these frames
    contain redundant data, so instead of (25 × 4 = 100) frames corresponding to one
    data row, we can reduce the number of frames to say 5 frames per second resulting
    in 20 frames. (It would be best if the total number of frames extracted per video
    is fixed).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the spatial resolution of individual frames from 160 × 120.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, when it comes to modeling, we should be using three-dimensional convolutional
    layers. So let us say if have taken only 20 frames per video, and reduced the
    size of each frame to 128 × 128, then a single sample will be: 20 × 128 × 128
    × 1, this corresponds to the volume of 20 × 128 × 128 with a single channel.'
  prefs: []
  type: TYPE_NORMAL
- en: Smart lighting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first home automation application that comes to mind when talking about
    smart homes is using smart light. Most of the smart lighting systems that exist
    at present offer an option to control the switching on and off of the lights,
    as well as their intensity, using an app on your smartphone or via the internet.
    Some also allow you to change the color/hue. Motion detecting lights, which automatically
    switch on after detecting any motion, are part of almost all households today:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45ffbdaf-d76a-455d-a42c-f3c4ca885eb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Smart light for people with hearing impairments changes color based on the situation
  prefs: []
  type: TYPE_NORMAL
- en: Using AI, we can make these smart lights even smarter. In case of emergency,
    they can be programmed to work in collaboration and guide you to the right exit.
    For people with hearing impairments, the smart lights can be used instead of alarms,
    for example, a red light when the fire alarm goes off, but an orange light when
    there's a burglar, and a welcoming green when someone rings the door bell. With
    the help of services such as **If This Then That** (**IFTTT**) you can set up
    smarter and more complex support systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The IFTTT provides a free service to control your devices. An action by one
    device (or service) can trigger one or more other devices. It is very simple to
    use, you just create an applet at the IFTTT website: [https://ifttt.com](https://ifttt.com),
    you select the device (point and click) or service you want to use as a trigger,
    and link it with your IFTTT account. Next you select (point and click) the service
    or the device you want should act when the trigger is activated. The site contains
    thousands of pre-made applets to make your job even easier.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84e7219c-8800-479b-af5b-65fa2780892d.png)'
  prefs: []
  type: TYPE_IMG
- en: Algorithm for a personalized smart light system
  prefs: []
  type: TYPE_NORMAL
- en: 'These are just some examples of what you can do with existing smart lights.
    But if you are adventurous and are ready to interface new sensors with these smart
    lights, you can build a personal light for yourself, one that changes its hue/intensity
    based on the mental activity of the person. It gets dim when you feel sleepy,
    and is full intensity when you are working, but when you are talking and spending
    time with friends, it simply provides a pleasant hue. Sounds far-fetched? Not
    really, you can first use an AI algorithm that detects human activity from video
    (or wearable fitness trackers) and classifies it into three classes: work, leisure,
    and sleep, and then use its output to control the smart light hue/intensity.'
  prefs: []
  type: TYPE_NORMAL
- en: Home surveillance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Home surveillance is a very useful and much-needed application. With single
    parents and the elderly population on the rise, the need for security and surveillance,
    not just on the outer premises, but inside homes, is also needed. Many companies
    are trying to provide in-home surveillance using videos. One of the successful
    implementations is by a company named DeepSight AILabs ([http://deepsightlabs.com](http://deepsightlabs.com)),
    they have developed proprietary software **SuperSecure**; a universally compatible
    retrofit solution that works with any CCTV system, any camera, any resolution,
    and turns it into an **AI-powered smart surveillance solution** to detect potential
    threats with high accuracy, and trigger instant alerts to save lives and protect
    assets.
  prefs: []
  type: TYPE_NORMAL
- en: When you try your own implementation of home surveillance, the points we discussed
    in the implementation of HAR using videos will be useful here as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Smart homes are still in their infancy, the major reason being that they involve
    a high cost of ownership and the inflexibility of the interconnected devices.
    Normally, one particular system is managed completely by one company. If for some
    reason the company is closed, the consumer is left in the lurch. The solution
    would be to allow open source home automation hardware and software. An interesting
    read on the challenges and opportunities in the field of home automation is an
    article by Microsoft Research, *Home Automation in the Wild: Challenges and Opportunities*
    ([https://www.microsoft.com/en-us/research/publication/home-automation-in-the-wild-challenges-and-opportunities/](https://www.microsoft.com/en-us/research/publication/home-automation-in-the-wild-challenges-and-opportunities/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The focus of this chapter was personal and home AI-powered IoT solutions. The
    large-scale use of smartphones has brought wearable sensors to every person's
    reach, resulting in a plethora of personal apps. In this chapter, we explored
    and implemented some of the successful personal and home AI-powered IoT solutions.
    We learned about SuperShoes by MIT, shoes that can find their own path to the
    destination. We learned about CGM systems and implemented code to predict hyperglycemia.
    This chapter also demonstrated how personalized heart monitors can be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: While smart homes are still in their infancy, the chapter explored some of the
    most popular and useful smart home solutions. HAR, an application that exists
    at the boundary of smart homes and personal IoT, was introduced. We wrote some
    code using scikit-learn to classify the activity from data obtained using accelerometers.
    The chapter introduced some cool smart lighting applications and talked about
    home surveillance using videos.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at some case studies where the data obtained
    from IoT sensors is used to improve production and efficiency in industries.
  prefs: []
  type: TYPE_NORMAL
