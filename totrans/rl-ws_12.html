<html><head></head><body>
		<div>
			<div id="_idContainer775" class="Content">
			</div>
		</div>
		<div id="_idContainer776" class="Content">
			<h1 id="_idParaDest-315"><a id="_idTextAnchor359"/>12. Evolutionary Strategies for RL</h1>
		</div>
		<div id="_idContainer828" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will be identifying the limitations of gradient-based methods and the motivation for evolutionary strategies. We will break down the components of genetic algorithms and implement them in <strong class="bold">Reinforcement Learning</strong> (<strong class="bold">RL</strong>). By the end of this chapter, you will be able to combine evolutionary strategies with traditional machine learning methods, specifically in the selection of neural network hyperparameters, and also identify the limitations of these evolutionary methods.</p>
			<h1 id="_idParaDest-316"><a id="_idTextAnchor360"/>Introduction</h1>
			<p>In the previous chapter, we looked at various policy-based methods and their advantages. In this chapter, we are going to learn about gradient-free methods, namely genetic algorithms; develop these algorithms step by step; and use them to optimize neural networks and RL-based algorithms. This chapter discusses the limitations of gradient-based methods, such as getting stuck at local optima and slower convergence when dealing with noisy input. This chapter presents an alternative optimization solution to gradient methods through genetic algorithms, as they ensure global optimum convergence. You will examine and implement the structure of genetic algorithms and implement them through hyperparameter selection for neural networks and evolving network topologies, as well as using them in combination with RL for a cart-pole balancing activity. Hybrid neural networks that use genetic algorithms are used to solve complex problems, such as modeling plasma chemical reactors, designing fractal frequency selective surfaces, or optimizing production processes. In the following section, you will be examining the problems posed by gradient-based methods.</p>
			<h1 id="_idParaDest-317"><a id="_idTextAnchor361"/>Problems with Gradient-Based Methods</h1>
			<p>In this section, you will learn about the differences between value-based and policy-based methods and the use of gradient-based methods in policy search algorithms. You will then examine the advantages and disadvantages of using gradient-based methods in policy-based approaches and implement stochastic gradient descent using TensorFlow to solve a cubic function with two unknowns.</p>
			<p>There are two approaches when doing RL: value-based and policy-based. These approaches are used to solve complex decision problems related to <strong class="bold">Markov Decision</strong> <strong class="bold">Processes</strong> (<strong class="bold">MDPs</strong>) and <strong class="bold">Partially Observable Markov Decision Processes</strong> (<strong class="bold">POMDPs</strong>). Value-based approaches rely on identifying and deriving the optimal policy based on the identification of the optimal value function. Algorithms such as Q-learning or SARSA(λ) are included within this category, and for tasks involving lookup tables, their implementation leads to convergence on a return that is optimal, globally. As the algorithms rely on a known model of the environment, for partially observable or continuous spaces, there are no guarantees for convergence on a solution that is optimal using these value search methods.</p>
			<p>Conversely, policy-based approaches, instead of relying on the value function to maximize the return, use gradient methods (stochastic optimization) to explore the policy space. Gradient-based methods or policy gradient methods map the parametrized space (environment) to the policy space using loss functions, thus enabling the RL agent to explore directly the entirety, or a portion, of the policy space. One of the most widely used methods (which is going to be implemented in this section) is gradient descent.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For further reading on gradient descent, please refer to the technical paper by <em class="italic">Marbach, 2001</em>, at the following link: https://link.springer.com/article/10.1023/A:1022145020786.</p>
			<p>The advantages of the gradient approaches (stochastic gradient descent or ascent) are that they are suitable for POMDPs or non-MDPs, especially for solving robotics problems with multiple constraints. However, there are several disadvantages to employing gradient-based methods. The most notable one is that algorithms such as <strong class="bold">REINFORCE</strong> and <strong class="bold">DPG</strong> determine a local optimum of the expected reward. As the local optimum is found, the RL agent does not expand its search globally. For example, a robot solving a maze problem will get stuck in a corner and will continuously try to move in the same location. Additionally, when dealing with high return variance or noisy input data, algorithm performance is affected as they converge slower. This happens when, for instance, a robotic arm is programmed to pick up and place a blue component in a tray, but the table has blue hues to its color, which interferes with the detection of the component through the sensors (such as a camera).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For further reading on the <strong class="bold">REINFORCE</strong> algorithm, please refer to the technical paper by <em class="italic">Williams, 1992</em>, at the following link: https://link.springer.com/article/10.1007/BF00992696.</p>
			<p class="callout">Similarly, please read about the <strong class="bold">DPG</strong> algorithm by <em class="italic">Silvester, 2014</em>, at the following link: http://proceedings.mlr.press/v32/silver14.pdf.</p>
			<p>An alternative to gradient-based methods is the use of gradient-free methods, which rely on evolutionary algorithms to achieve a global optimum for the return.</p>
			<p>The following exercise will enable you to understand the potential of gradient methods for converging on optimal solutions and the lengthy process that is undertaken as the method searches step by step for the optimal solution. You will be presented with a mathematical function (loss function) that maps the input values, <img src="image/B16182_12_00a.png" alt="1"/>, to an output value, <img src="image/B16182_12_00b.png" alt="2"/>. The goal is to identify the optimal values of the inputs that lead to the lowest value of the output; however, this is step-dependent and is at risk of staying at a local optimum. We will be using the <strong class="source-inline">GradientTape()</strong> function to calculate the gradients, which are nothing but differentiation solutions. This will help you understand the limitations of such optimization strategies.</p>
			<h2 id="_idParaDest-318"><a id="_idTextAnchor362"/>Exercise 12.01: Optimization Using Stochastic Gradient Descent</h2>
			<p>This exercise aims to enable you to apply gradient methods, most notably <strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>), available in TensorFlow by following the steps required to converge on an optimal solution.</p>
			<p>The following loss function has two unknowns, <img src="image/B16182_12_00c.png" alt="3"/>:</p>
			<div>
				<div id="_idContainer780" class="IMG---Figure">
					<img src="image/B16182_12_01.jpg" alt="Figure 12.1: Sample loss function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1: Sample loss function</p>
			<p>Find the optimum values for <img src="image/B16182_12_01a.png" alt="4"/> within 100 steps with a learning rate of <strong class="source-inline">0.1</strong>.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li>Create a new Jupyter Notebook.</li>
				<li>Import the <strong class="source-inline">tensorflow</strong> package as <strong class="source-inline">tf</strong>:<p class="source-code">import tensorflow as tf</p></li>
				<li>Define a function that outputs <img src="image/B16182_12_01b.png" alt="5"/>  :<p class="source-code">def funct(x,y):</p><p class="source-code">    return x**2-8*x+y**2+3*y</p></li>
				<li>Define a function for initializing the <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> variables and initialize them with the values <strong class="source-inline">5</strong> and <strong class="source-inline">10</strong>:<p class="source-code">def initialize():</p><p class="source-code">    x = tf.Variable(5.0)</p><p class="source-code">    y = tf.Variable(10.0)</p><p class="source-code">    return x, y</p><p class="source-code">x, y= initialize()</p></li>
				<li>In the preceding code snippet, we have used decimal format for the values assigned to <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> to start the optimization process, as the <strong class="source-inline">Variable()</strong> constructor needs to have a tensor type of <strong class="source-inline">float32</strong>.</li>
				<li>Instantiate the optimizer by selecting <strong class="source-inline">SGD</strong> from <strong class="source-inline">keras</strong> in TensorFlow and input the learning rate of 0.1:<p class="source-code">optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)</p></li>
				<li>Set a loop of <strong class="source-inline">100</strong> steps, where you calculate the loss, use the <strong class="source-inline">GradientTape()</strong> function for automatic differentiations, and process the gradients:<p class="source-code">for i in range(100):</p><p class="source-code">    with tf.GradientTape() as tape:</p><p class="source-code">        # Calculate loss function using x and y values</p><p class="source-code">        loss= funct(x,y)</p><p class="source-code">        # Get gradient values</p><p class="source-code">        gradients = tape.gradient(loss, [x, y])</p><p class="source-code">        # Save gradients in array without altering them</p><p class="source-code">        p_gradients = [grad for grad in gradients]</p><p>In the preceding code, we have used <strong class="source-inline">GradientTape()</strong> from TensorFlow to calculate the gradients (which essentially are differentiation solutions). We created a loss parameter that stores the <img src="image/B16182_12_01c.png" alt="37"/> value when calling the function. <strong class="source-inline">GradientTape()</strong> is activated when calling the <strong class="source-inline">gradient()</strong> method, which essentially is used to compute multiple gradients in a single computation. The gradients are stored in a <strong class="source-inline">p_gradients</strong> array.</p></li>
				<li>Use the <strong class="source-inline">zip()</strong> function to aggregate the gradients to the values:<p class="source-code">        ag = zip(p_gradients, [x,y])</p></li>
				<li>Print the step and the values of <img src="image/B16182_12_01d.png" alt="6"/>:<p class="source-code">        print('Step={:.1f} , z ={:.1f},x={:.1f},y={:.1f}'\</p><p class="source-code">              .format(i, loss.numpy(), x.numpy(), y.numpy()))</p></li>
				<li>Apply the optimizer using the gradients that were processed:<p class="source-code">        optimizer.apply_gradients(ag)</p></li>
				<li>Run the application.<p>You will get the following output:</p><div id="_idContainer785" class="IMG---Figure"><img src="image/B16182_12_02.jpg" alt="Figure 12.2: Step-by-step optimization using SGD&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 12.2: Step-by-step optimization using SGD</p>
			<p>You can observe in the output that from <strong class="source-inline">Step=25</strong> onward, the <img src="image/B16182_12_02a.png" alt="36"/> values do not change; therefore, they are considered to be the optimum values for the respective loss function.</p>
			<p>By printing the steps and values of the inputs and outputs, you can observe that the algorithm converges before the termination of the 100 steps to the optimal values of <img src="image/B16182_12_02a.png" alt="35"/>. However, you can observe that the problem is step-dependent: if the optimization is stopped before global optimum convergence, the solution would be sub-optimal. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2C10rXD">https://packt.live/2C10rXD</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2DIWSqc">https://packt.live/2DIWSqc</a>.</p>
			<p>This exercise helped your understanding and application of SGD when solving a loss function, developing your analysis skills as well as your skills in programming using TensorFlow. This will help you in your choice of optimization algorithm, giving you an understanding of the limitations of gradient-based methods.</p>
			<p>In this section, we have explored the benefits and disadvantages of gradient methods with respect to RL algorithms, identifying the types of problems that they are suitable for within the context of decision-making processes. The example offered a simple application of gradient descent, where the optimal solution for two unknowns was identified using SGD optimization in TensorFlow. In the next section, we will be exploring an optimization alternative that is gradient-free: genetic algorithms.</p>
			<h1 id="_idParaDest-319"><a id="_idTextAnchor363"/>Introduction to Genetic Algorithms</h1>
			<p>As the problem with gradient methods is that the solution can get stuck at a single local optimum, other methods, such as gradient-free algorithms, can be considered as alternatives. In this section, you will learn about gradient-free methods, specifically evolutionary algorithms (for example, genetic algorithms). This section provides an overview of the steps taken for the implementation of genetic algorithms and exercises on how to implement an evolutionary algorithm to solve the loss function given in the previous section.</p>
			<p>When multiple local optima exist or function optimization is required, gradient-free methods are recommended. These methods include evolutionary algorithms and particle swarm optimizations. A characteristic of these methods is that they rely on sets of optimization solutions that are commonly referred to as populations. The methods rely on iteratively searching for a good solution or a distribution that can solve a problem or a mathematical function. The search pattern for the optimal solution is modeled based on Darwin's natural selection paradigm and the biological phenomenon of genetic evolution. Evolutionary algorithms draw inspiration from biological evolution patterns such as mutation, reproduction, recombination, and selection. Particle swarm algorithms are inspired by group social behavior, such as a beehive organization or ant farms, where single solutions are termed as particles that can evolve over time.</p>
			<p>Natural selection stems from the premise that genetic material (the chromosome) encodes the survival of a species, in a certain way. The evolution of the species relies on how well it adapts to its external environment and the information passed from parents to children. In genetic material, there are variations (mutations) between generations that can lead to successful or unsuccessful adaptation to the environment (especially in dire conditions). Therefore, there are three steps to genetic algorithms: selection, reproduction (crossover), and mutation.</p>
			<p>Evolutionary algorithms go about things by creating an original population of solutions, selecting a sub-set, and using recombination or mutation to obtain different solutions. This new set of solutions can replace, partly or fully, the original set. For the replacement to take place, the solutions go through a selection process that relies on analyzing their fitness. This increases the chances of solutions that are more suited to being utilized to develop a new set of solutions.</p>
			<p>Other than the development of solutions, evolutionary algorithms can be used for parameter adaptation, using probability distributions. A population is still generated; however, a fitness method is used to select the parameters of the distribution instead of the actual solutions. After the new parameters are identified, the new distribution is used to generate a new set of solutions. Some strategies of parameter selection include the following:</p>
			<ul>
				<li>Using natural gradient ascent after the gradients of the parameters are estimated from the original population, also known as <strong class="bold">Natural Evolutionary Strategies</strong> (<strong class="bold">NESes</strong>)</li>
				<li>Selecting solutions with a specific parameter and using the mean of this sub-set to find a new distribution mean, known as <strong class="bold">Cross-Entropy Optimization</strong> (<strong class="bold">CEO</strong>)</li>
				<li>Attributing a weight to each solution based on its fitness, using the weighted average as a new distribution mean – <strong class="bold">Covariance Matrix Adaptation Evolution Strategies</strong> (<strong class="bold">CMAESes</strong>)</li>
			</ul>
			<p>One of the major problems identified with evolutionary strategies is that achieving solution fitness can be computationally expensive and noisy.</p>
			<p><strong class="bold">Genetic Algorithms</strong> (<strong class="bold">GAs</strong>) keep the solution population and conduct searches in multiple directions (through the chromosomes), furthering the exchange of information in these directions. The algorithms are most notably implemented on strings, which are either binary or character-based. The two main operations performed are mutation and crossover. The selection of the progenies is based on how close the solution is to the target (objective function), which denotes their fitness.</p>
			<p>As an overview, GAs have the following steps:</p>
			<ol>
				<li value="1">Population creation.</li>
				<li>Fitness score creation and assignment to each solution of the population.</li>
				<li>The selection of two parents to reproduce based on the fitness scores (potentially the solutions with the best performance).</li>
				<li>The creation of the two child solutions by combining and re-organizing the code of the two parents.</li>
				<li>The application of a random mutation.</li>
				<li>Child generation is repeated until the new population size is achieved and weights (fitness scores) for the population are assigned.</li>
				<li>The process is repeated until the maximum number of generations is reached or the target performance is achieved.</li>
			</ol>
			<p>We will be looking at each of these steps in detail further in this chapter.</p>
			<p>Among the many differences between gradient-based algorithms and GAs, one difference is the process of development. Gradient-based algorithms rely on differentiation, whereas GAs use the genetic processes of selection, reproduction, and mutation. The following exercise will enable you to implement GAs and evaluate their performance. You will be using a simple genetic algorithm in TensorFlow to identify the GA hyperparameter optimization for finding the optimal solution for <strong class="bold">Recurrent Neural Network</strong> (<strong class="bold">RNN</strong>) training for the same loss function as for the SGD method. To identify the optimal values, you will need to implement an evolutionary algorithm called <strong class="bold">Differential Evolution</strong> (<strong class="bold">DE</strong>), available in the <strong class="source-inline">tensorflow_probability</strong> package. </p>
			<h2 id="_idParaDest-320"><a id="_idTextAnchor364"/>Exercise 12.02: Implementing Fixed-Value and Uniform Distribution Optimization Using GAs</h2>
			<p>In this exercise, you will still need to solve the following function, as in the previous exercise:</p>
			<div>
				<div id="_idContainer788" class="IMG---Figure">
					<img src="image/B16182_12_03.jpg" alt="Figure 12.3: Sample loss function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3: Sample loss function</p>
			<p>Find the optimum values for <img src="image/B16182_12_03a.png" alt="7"/> for a population size of 100, starting from <img src="image/B16182_12_03b.png" alt="8"/> initialized to 5 and 10, and then extending to random samples from a distribution similar to the gradient-based method.</p>
			<p>The goal of this exercise is to enable you to analyze the differences in applying GAs and gradient-descent methods, by starting from a single pair of variables and a variety of potential solutions. The algorithm aids in optimization problems by applying selection, crossover, and mutation to reach an optimal or nearly optimal solution. Additionally, you will sample the values <img src="image/B16182_12_00c.png" alt="9"/> from a uniform distribution for a population of 100. By the end of this exercise, you will have evaluated the differences between starting from a fixed variable and sampling from a distribution:</p>
			<ol>
				<li value="1">Create a new Jupyter Notebook.</li>
				<li>Import the <strong class="source-inline">tensorflow</strong> package and download and import <strong class="source-inline">tensorflow_probability</strong>:<p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_probability as tfp</p></li>
				<li>Define a function that outputs <img src="image/B16182_12_03d.png" alt="10"/>:<p class="source-code">def funct(x,y):</p><p class="source-code">    return x**2-8*x+y**2+3*y</p></li>
				<li>Identify the initial step by defining the <img src="image/B16182_12_03e.png" alt="11"/> variables with values of 5 and 10:<p class="source-code">initial_position = (tf.Variable(5.0), tf.Variable(10.0))</p></li>
				<li>Instantiate the optimizer by selecting the <strong class="source-inline">tensorflow_probability</strong> optimizer named <strong class="source-inline">differential_evolution_minimize</strong>:<p class="source-code">optimizer1 = tfp.optimizer.differential_evolution_minimize\</p><p class="source-code">             (funct, initial_position = initial_position, \</p><p class="source-code">              population_size = 100, \</p><p class="source-code">              population_stddev = 1.5, seed = 879879)</p></li>
				<li>Print the final values of <img src="image/B16182_12_03f.png" alt="34"/>, by using the <strong class="source-inline">objective_value</strong> and <strong class="source-inline">position</strong> functions:<p class="source-code">print('Final solution: z={:.1f}, x={:.1f}, y={:.1f}'\</p><p class="source-code">      .format(optimizer1.objective_value.numpy(),\</p><p class="source-code">      optimizer1.position[0].numpy(), \</p><p class="source-code">      optimizer1.position[1].numpy()))</p></li>
				<li>Run the application. You will get the following output. You can observe that the final values are identical to the <strong class="source-inline">Step=25.0</strong> value in <em class="italic">Figure 12.2</em>:<p class="source-code">Final solution: z=-18.2, x=4.0, y=-1.5</p><p>In this exercise, the final optimal solution will be displayed. There are no additional optimization steps needed to reach the same solution as the gradient-based method. You can see that you are using fewer lines of code and that the time taken for the algorithm to converge is shorter.</p><p>For uniform optimization, the steps to modify the code are as follows:</p></li>
				<li>Import the <strong class="source-inline">random</strong> package:<p class="source-code">import random</p></li>
				<li>Initialize the population size and create the initial population sampling the <img src="image/B16182_12_03h.png" alt="33"/> variables from a random uniform distribution of the population size:<p class="source-code">size = 100</p><p class="source-code">initial_population = (tf.random.uniform([size]), \</p><p class="source-code">                      tf.random.uniform([size]))</p></li>
				<li>Use the same optimizer, change the <strong class="source-inline">initial_position</strong> parameter to <strong class="source-inline">initial_population</strong>; use the same seed:<p class="source-code">optimizer2 = tfp.optimizer.differential_evolution_minimize\</p><p class="source-code">             (funct, initial_population= initial_population,\</p><p class="source-code">              seed=879879)</p></li>
				<li>Print the final values of <img src="image/B16182_12_03i.png" alt="12"/>, by using the <strong class="source-inline">objective_value</strong> and <strong class="source-inline">position</strong> functions:<p class="source-code">print('Final solution: z={:.1f}, x={:.1f}, y={:.1f}'\</p><p class="source-code">      .format(optimizer2.objective_value.numpy(),\</p><p class="source-code">      optimizer2.position[0].numpy(),\</p><p class="source-code">      optimizer2.position[1].numpy()))</p><p>The output will be as follows:</p><p class="source-code">Final solution: z=-18.2, x=4.0, y=-1.5</p></li>
			</ol>
			<p>You will get the same result despite the variation in values. This means that we can randomly sample or choose a specific set of initial values, and the GA will still converge to the optimal solution faster, meaning we can improve our code by using fewer lines of code than if we'd used a gradient-based method.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2MQmlPr">https://packt.live/2MQmlPr</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2zpH6hJ">https://packt.live/2zpH6hJ</a>.</p>
			<p>The solution will converge to the optimal values irrespective of the initial starting point, whether using a fixed value for the inputs or a random sampling of the population of chromosomes.</p>
			<p>This section offered a general overview of evolutionary algorithms, explaining the differences between evolutionary strategies and GAs. You've had the opportunity to implement differential evolution using the <strong class="source-inline">tensorflow_probabilities</strong> package to optimize the solution of a loss function, analyzing the implementation of two different techniques: starting from fixed input values and using random sampling for the input values. You also had the opportunity to evaluate the implementation of GAs compared to gradient descent methods. GAs can use independent starting values and their convergence to a global optimum is faster and less prone to disturbances that gradient descent methods, whereas gradient descent is step-dependent and has higher sensitivity to the input variable.</p>
			<p>In the following section, we will build on the principles of developing GAs, starting with a look at population creation.</p>
			<h2 id="_idParaDest-321"><a id="_idTextAnchor365"/>Components: Population Creation</h2>
			<p>In the previous section, you were introduced to evolutionary methods for function optimization. In this section, we will concentrate on population creation, fitness score creation, and the task of creating the genetic algorithm.</p>
			<p>The population, <img src="image/B16182_12_03g.png" alt="32"/>, is identified as a group of individuals or chromosomes:</p>
			<div>
				<div id="_idContainer798" class="IMG---Figure">
					<img src="image/B16182_12_04.jpg" alt="Figure 12.4: Expression for the population&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.4: Expression for the population</p>
			<p>Here, <strong class="source-inline">s</strong> represents the total number of chromosomes (population size) and <strong class="source-inline">i</strong> is the iteration. Each chromosome is a possible solution to the presented problem in an abstract form. For a binary problem, the population can be a matrix with randomly generated ones and zeros.</p>
			<p>The chromosome is a combination of input variables (genes):</p>
			<div>
				<div id="_idContainer799" class="IMG---Figure">
					<img src="image/B16182_12_05.jpg" alt="Figure 12.5: Expression for the chromosome&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.5: Expression for the chromosome</p>
			<p>Here, <strong class="source-inline">m</strong> is the maximum number of genes (or variables).</p>
			<p>When translated to code, population creation can be demonstrated as follows:</p>
			<p class="source-code">    population =  np.zeros((no_chromosomes, no_genes))</p>
			<p class="source-code">    for i in range(no_chromosomes):</p>
			<p class="source-code">        ones = random.randint(0, no_genes)</p>
			<p class="source-code">        population[i, 0:ones] = 1</p>
			<p class="source-code">        np.random.shuffle(population[i])</p>
			<p>Each chromosome is then compared using a fitness function:</p>
			<div>
				<div id="_idContainer800" class="IMG---Figure">
					<img src="image/B16182_12_06.jpg" alt="Figure 12.6: Fitness function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.6: Fitness function</p>
			<p>The fitness function can be translated to code as follows:</p>
			<p class="source-code">identical_to_target = population == target</p>
			<p>The output of the function is a score indicating how close the chromosome is to the target (optimal solution). The target is represented by the maximization of the fitness function. There cases where the optimization problem relies on minimizing a cost function. The function can be a mathematical one, a thermodynamic model, or a computer game. This can be done either by considering the chromosomes with low weightings (scores) or by adapting the cost function into a fitness one.</p>
			<p>Once the fitness function is identified and defined, the evolution process can start. The initial population is generated. A characteristic of the initial population is diversity. To offer this diversity, the elements can be randomly generated. To make the population evolve, the iterative process starts by selecting the parents that offer the best fit to start the reproduction process.</p>
			<h2 id="_idParaDest-322"><a id="_idTextAnchor366"/>Exercise 12.03: Population Creation</h2>
			<p>In this exercise, we will be creating an original population of binary chromosomes of length 5. Each chromosome should have eight genes. We will define a target solution and output the similarity of each chromosome to it. This exercise aims to allow you to design and establish the first set of steps for a GA and find the binary solution that fits the target. The exercise is similar to matching the output of a control system with a target:</p>
			<ol>
				<li value="1">Create a new Jupyter Notebook. Import the <strong class="source-inline">random</strong> and <strong class="source-inline">numpy</strong> libraries:<p class="source-code">import random</p><p class="source-code">import numpy as np</p></li>
				<li>Create a function for the random population:<p class="source-code"># create function for random population</p><p class="source-code">def original_population(chromosomes, genes):</p><p class="source-code">    #initialize the population with zeroes</p><p class="source-code">    population =  np.zeros((chromosomes, genes))</p><p class="source-code">    #loop through each chromosome</p><p class="source-code">    for i in range(chromosomes):</p><p class="source-code">        #get random no. of ones to be created</p><p class="source-code">        ones = random.randint(0, genes)</p><p class="source-code">        #change zeroes to ones</p><p class="source-code">        population[i, 0:ones] = 1</p><p class="source-code">        #shuffle rows</p><p class="source-code">        np.random.shuffle(population[i])</p><p class="source-code">    return population</p></li>
				<li>Define a function for creating the target solution:<p class="source-code">def create_target_solution(gene):</p><p class="source-code">    #assume that there is an equal number of ones and zeroes</p><p class="source-code">    counting_ones = int(gene/2)</p><p class="source-code">    # build array with equal no. of ones and zeros</p><p class="source-code">    target = np.zeros(gene)</p><p class="source-code">    target[0:counting_ones] = 1</p><p class="source-code">    # shuffle the array to mix zeroes and ones</p><p class="source-code">    np.random.shuffle(target)</p><p class="source-code">    return target</p></li>
				<li>Define a function for calculating the fitness weighting for each chromosome:<p class="source-code">def fitness_function(target,population):</p><p class="source-code">    #create an array of true/false compared to the reference</p><p class="source-code">    identical_to_target = population == target</p><p class="source-code">    #sum no. of genes that are identical</p><p class="source-code">    fitness_weights = identical_to_target.sum(axis = 1)</p><p class="source-code">    return fitness_weights</p><p>In the preceding code, you are comparing each chromosome of the population with the target and cataloging the similarity as a Boolean – <strong class="source-inline">True</strong> if similar or <strong class="source-inline">False</strong> if different – in the matrix called <strong class="source-inline">identical_to_target</strong>. Count all the elements that are true and output them as the weights.</p></li>
				<li>Initialize the population with <strong class="source-inline">5</strong> chromosomes and <strong class="source-inline">8</strong> genes and calculate <strong class="source-inline">weights</strong>:<p class="source-code">#population of 5 chromosomes, each having 8 genes</p><p class="source-code">population = original_population(5,8)</p><p class="source-code">target = create_target_solution(8)</p><p class="source-code">weights = fitness_function(target,population)</p><p>In the preceding code, we calculate <strong class="source-inline">population</strong>, <strong class="source-inline">target</strong>, and <strong class="source-inline">weights</strong> based on the three developed functions.</p></li>
				<li>Print the target solution, the index of the chromosome, the chromosome, and the weight using a <strong class="source-inline">for</strong> loop:<p class="source-code">print('\n target:', target)</p><p class="source-code">for i in range(len(population)):</p><p class="source-code">    print('Index:', i, '\n chromosome:', population[i],\</p><p class="source-code">          '\n similarity to target:', weights[i])</p></li>
				<li>Run the application. You will get a similar output to this, as the population elements are randomized:<p class="source-code">target: [0. 0. 1. 1. 1. 0. 0. 1.]</p><p class="source-code">Index: 0 </p><p class="source-code"> chromosome: [1. 1. 1. 1. 1. 0. 1. 1.] </p><p class="source-code"> similarity to target: 5</p><p class="source-code">Index: 1 </p><p class="source-code"> chromosome: [1. 0. 1. 1. 1. 0. 0. 0.] </p><p class="source-code"> similarity to target: 6</p><p class="source-code">Index: 2 </p><p class="source-code"> chromosome: [1. 0. 0. 0. 0. 0. 0. 0.] </p><p class="source-code"> similarity to target: 3</p><p class="source-code">Index: 3 </p><p class="source-code"> chromosome: [0. 0. 0. 1. 1. 0. 1. 0.] </p><p class="source-code"> similarity to target: 5</p><p class="source-code">Index: 4 </p><p class="source-code"> chromosome: [1. 0. 0. 1. 1. 1. 0. 1.] </p><p class="source-code"> similarity to target: 5</p></li>
			</ol>
			<p>You will notice that each chromosome is compared to the target and the similarity (based on the fitness function) is printed out.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2zrjadT">https://packt.live/2zrjadT</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2BSSeEG">https://packt.live/2BSSeEG</a>.</p>
			<p>This section showcased the first steps of genetic algorithm development: the generation of a random population, fitness score assignment for each element of the population (chromosome), and getting the number of elements that are the best fit compared to the target (in this case have the highest similarity with an optimal solution). The following sections will expand on the code generation that occurs until the optimal solution is reached. To do this, in the next section, you will explore the selection of the parents for the reproduction process.</p>
			<h2 id="_idParaDest-323"><a id="_idTextAnchor367"/>Components: Parent Selection</h2>
			<p>The previous section showcased the concepts of populations; we looked at creating a target solution and comparing that solution with the elements (chromosomes) of the population. These concepts were implemented in an exercise that will be continued in this section. In this section, you will explore the concept of selection and implement two selection strategies.</p>
			<p>For the reproduction process (which is the quintessential part of GAs, as they rely on creating future generations of stronger chromosomes), there are three steps:</p>
			<ol>
				<li value="1">Parent selection</li>
				<li>Mixing the parents to create new children (crossover)</li>
				<li>Replacing them with the children in the population</li>
			</ol>
			<p>Selection essentially consists of choosing two or more parents for the mixing process. Once a fitness criterion is selected, the way in which the selection of the parents will be performed needs to be chosen, as does how many children will come from the parents. Selection is a vital step in performing genetic evolution, as it involves determining the children with the highest fitness. The most common way to select the best individuals is by the "survival of the fittest." This means the algorithm will improve the population in a step-by-step manner. The convergence of the GA is dependent upon the degree to which chromosomes with higher fitness are chosen. Therefore, the convergence speed is highly dependent on the successful selection of chromosomes. If the chromosomes with the highest fitness are prioritized, there is a chance that a sub-optimal solution will be found; if the candidates have consistently low fitness, then convergence will be extremely slow.</p>
			<p>The available selection methods are as follows:</p>
			<ul>
				<li><strong class="bold">Top-to-bottom pairing</strong>: This refers to creating a list of chromosomes and pairing them two by two. The chromosomes with odd indexes are paired with the even chromosomes, thus generating mother-father couples. The chromosomes at the top of the list are selected.</li>
				<li><strong class="bold">Random Selection</strong>: This involves using a uniform number generator to select the parents.</li>
				<li><strong class="bold">Random Weighted Selection or a Roulette Wheel</strong>: This involves calculating the probability of the suitability of a chromosome compared to the entire population. The selection of the parent is done randomly. The probability (weight) can be determined either by rank or fitness. The first approach (see <em class="italic">Figure 12.7</em>) relies on the rank of the chromosome <img src="image/B16182_12_06a.png" alt="31"/>, which can constitute the index of the chromosome in the population list, and <img src="image/B16182_12_06b.png" alt="30"/> represents the number of required chromosomes (parents):<div id="_idContainer803" class="IMG---Figure"><img src="image/B16182_12_07.jpg" alt="Figure 12.7: Probability using rank &#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 12.7: Probability using rank </p>
			<p>The second approach (see <em class="italic">Figure 12.8</em>) relies on the fitness of the chromosome (<img src="image/B16182_12_07a.png" alt="29"/>) compared to the sum of the fitness of the entire population ( <img src="image/B16182_12_07b.png" alt="28"/> ):</p>
			<div>
				<div id="_idContainer806" class="IMG---Figure">
					<img src="image/B16182_12_08.jpg" alt="Figure 12.8: Probability using chromosome fitness&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.8: Probability using chromosome fitness</p>
			<p>As an alternative, the probability (see <em class="italic">Figure 12.9</em>) can also be calculated based on the fitness of the chromosome (<img src="image/B16182_12_08a.png" alt="13"/>) compared with the highest fitness of the population <img src="image/B16182_12_08b.png" alt="14"/>. In all of the cases, the probabilities are compared to the randomly selected numbers to identify the parents with the best weights:</p>
			<div>
				<div id="_idContainer809" class="IMG---Figure">
					<img src="image/B16182_12_09.jpg" alt="Figure 12.9: Probability using the highest fitness of the population&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.9: Probability using the highest fitness of the population</p>
			<ul>
				<li><strong class="bold">Selection by Tournament</strong>: This method is based on the random selection of a subset of chromosomes, out of which the chromosome with the highest fitness is selected as a parent. This repeats until the required number of parents is identified.</li>
			</ul>
			<p>The roulette wheel and tournament techniques are among the most popular selection methods implemented in GAs, as they are inspired by biological processes. The problem with the roulette technique is that it can be noisy, and depending on which type of selection is used, the convergence rate can be affected. A benefit of the tournament method is that it can deal with large populations, leading to smoother convergence. The roulette wheel method is used to include random elements in the population, whereas when you are aiming to identify the parents with the highest similarity to the target, you use the tournament method. The following exercise will enable you to implement the tournament and roulette wheel techniques and evaluate your understanding of them.</p>
			<h2 id="_idParaDest-324"><a id="_idTextAnchor368"/>Exercise 12.04: Implementing the Tournament and Roulette Wheel Techniques</h2>
			<p>In the exercise, you will implement the tournament and roulette wheel methods for the population of binary chromosomes of <em class="italic">Exercise 12.02, Implementing Fixed Value and Uniform Distribution Optimization Using GAs</em>. Each chromosome should have eight genes. We will define a target solution and print two sets of parents: one based on the tournament method and the other by roulette from the remaining population. Once each parent is chosen, set the fitness rank to the minimum:</p>
			<ol>
				<li value="1">Create a new Jupyter Notebook. Import the <strong class="source-inline">random</strong> and <strong class="source-inline">numpy</strong> libraries:<p class="source-code">import random</p><p class="source-code">import numpy as np</p></li>
				<li>Create a function for the random population:<p class="source-code"># create  function for random population</p><p class="source-code">def original_population(chromosomes, genes):</p><p class="source-code">    #initialize the population with zeroes</p><p class="source-code">    population =  np.zeros((chromosomes, genes))</p><p class="source-code">    #loop through each chromosome</p><p class="source-code">    for i in range(chromosomes):</p><p class="source-code">        #get random no. of ones to be created</p><p class="source-code">        ones = random.randint(0, genes)</p><p class="source-code">        #change zeroes to ones</p><p class="source-code">        population[i, 0:ones] = 1</p><p class="source-code">        #shuffle rows</p><p class="source-code">        np.random.shuffle(population[i])</p><p class="source-code">    return population</p></li>
				<li>Define a function for creating the target solution:<p class="source-code">def create_target_solution(gene):</p><p class="source-code">    #assume that there is an equal number of ones and zeroes</p><p class="source-code">    counting_ones = int(gene/2)</p><p class="source-code">    # build array with equal no. of ones and zeros</p><p class="source-code">    target = np.zeros(gene)</p><p class="source-code">    target[0:counting_ones] = 1</p><p class="source-code">    # shuffle the array to mix zeroes and ones</p><p class="source-code">    np.random.shuffle(target)</p><p class="source-code">    return target</p></li>
				<li>Define a function for calculating the fitness weighting for each chromosome:<p class="source-code">def fitness_function(target,population):</p><p class="source-code">    #create an array of true/false compared to the reference</p><p class="source-code">    identical_to_target = population == target</p><p class="source-code">    #sum no. of genes that are identical</p><p class="source-code">    fitness_weights = identical_to_target.sum(axis = 1)</p><p class="source-code">    return fitness_weights</p></li>
				<li>Define a function for selecting the pair of parents with the highest weighting (the highest fitness score). Since the population is reduced, the chromosomes are competing more. This method is also known as tournament selection:<p class="source-code"># select the best parents</p><p class="source-code">def select_parents(population, weights):</p><p class="source-code">    #identify the parent with the highest weight</p><p class="source-code">    parent1 = population[np.argmax(weights)]</p><p class="source-code">    #replace weight with the minimum number</p><p class="source-code">    weights[np.argmax(weights)] = 0</p><p class="source-code">    #identify the parent with the second-highest weight</p><p class="source-code">    parent2 = population[np.argmax(weights)]</p><p class="source-code">    return parent1, parent2</p></li>
				<li>Create a function for the roulette wheel by selecting a random number from a uniform distribution:<p class="source-code">def choice_by_roulette(sorted_population, fitness):</p><p class="source-code">    normalised_fitness_sum = 0</p><p class="source-code">    #get a random draw probability</p><p class="source-code">    draw = random.uniform(0,1)</p><p class="source-code">    prob = []</p></li>
				<li>In the function, calculate the sum of all the fitness scores:<p class="source-code">    for i in range(len(fitness)):</p><p class="source-code">        normalised_fitness_sum += fitness[i]</p></li>
				<li>Calculate the probability of the fitness of the chromosome compared to the sum of all fitness scores and compared to the chromosome with the highest fitness score:<p class="source-code">    ma = 0</p><p class="source-code">    n = 0</p><p class="source-code"># calculate the probability of the fitness selection</p><p class="source-code">    for i in range(len(sorted_population)):</p><p class="source-code">           probability = fitness[i]/normalised_fitness_sum</p><p class="source-code">           #compare fitness to the maximum fitness and track it</p><p class="source-code">           prob_max = fitness[i]/np.argmax(fitness)</p><p class="source-code">           prob.append(probability)</p><p class="source-code">            if ma &lt; prob_max:</p><p class="source-code">                ma = prob_max</p><p class="source-code">                n = i</p></li>
				<li>Run through all the chromosomes and select the parent that has a higher fitness probability compared to the sum of fitness scores higher than the <strong class="source-inline">draw</strong>, or the parent with the highest probability compared to the maximum fitness score:<p class="source-code">      for i in range(len(sorted_population)):</p><p class="source-code">            if draw &lt;= prob[i]:</p><p class="source-code">                fitness[i] = 0</p><p class="source-code">                return sorted_population[i], fitness</p><p class="source-code">            else:</p><p class="source-code">                fitness[n] = 0</p><p class="source-code">                return sorted_population[n], fitness</p></li>
				<li>Initialize <strong class="source-inline">population</strong>, calculate <strong class="source-inline">target</strong> and the fitness scores, and print the scores and <strong class="source-inline">target</strong>:<p class="source-code">population = original_population(5,8)</p><p class="source-code">target = create_target_solution(8)</p><p class="source-code">weights = fitness_function(target,population)</p><p class="source-code">print(weights)</p><p class="source-code">print('\n target:', target)</p><p>You will get a similar output to this:</p><p class="source-code">[5 1 5 3 4]</p></li>
				<li>Apply the first selection method and print out the parents and the new scores:<p class="source-code">print('\n target:', target)</p><p class="source-code">parents = select_parents(population,weights)</p><p class="source-code">print('Parent 1:', parents[0],'\nParent 2:', parents[1])</p><p class="source-code">print(weights)</p><p>You will get a similar output to this for the tournament selection process:</p><p class="source-code">target: [0. 1. 1. 1. 1. 0. 0. 0.]</p><p class="source-code">Parent 1: [1. 1. 1. 1. 1. 0. 1. 1.] </p><p class="source-code">Parent 2: [1. 1. 1. 1. 1. 1. 1. 0.]</p><p class="source-code">[0 1 5 3 4]</p><p>You can observe that for parent 1, the score has been replaced with <strong class="source-inline">0</strong>. For parent 2, the score stays the same.</p></li>
				<li>Use the roulette function to select the next two parents and print out the parents and the weights:<p class="source-code">parent3, weights = choice_by_roulette(population, weights)</p><p class="source-code">print('Parent 3:', parent3, 'Weights:', weights)</p><p class="source-code">parent4, weights = choice_by_roulette(population, weights)</p><p class="source-code">print('Parent 4:', parent4,'Weights:', weights)</p><p>You will have a similar output to this:</p><p class="source-code">0.8568696148662779</p><p class="source-code">[0.0, 0.07692307692307693, 0.38461538461538464, </p><p class="source-code"> 0.23076923076923078, 0.3076923076923077]</p><p class="source-code">Parent 3: [1. 1. 1. 1. 1. 1. 1. 0.] Weights: [0 1 0 3 4]</p><p class="source-code">0.4710306341255527</p><p class="source-code">[0.0, 0.125, 0.0, 0.375, 0.5]</p><p class="source-code">Parent 4: [0. 0. 1. 0. 1. 1. 1. 0.] Weights: [0 1 0 3 0]</p></li>
			</ol>
			<p>You can see that parents 2 and 3 are the same. This time, the weight for the respective parent is changed to 0. Additionally, parent 4 is selected and has its weighting changed to 0.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2MTsKJO">https://packt.live/2MTsKJO</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YrwMhP">https://packt.live/2YrwMhP</a>.</p>
			<p>With this exercise, you have implemented a tournament-like method, by selecting the parents with the highest scores, and the roulette wheel selection technique. Also, you have developed a method of avoiding the double-selection of the same chromosome. The first set of parents was chosen using the first method, whereas the second method was used in selecting the second set of parents. We have also identified a need for a method of replacing indexes to avoid double-selection of the same chromosome, which is one of the pitfalls of the selection process. This helped you to understand the differences between the two methods and allowed you to put into practice GA-related methods from population generation to selection.</p>
			<h2 id="_idParaDest-325"><a id="_idTextAnchor369"/>Components: Crossover Application</h2>
			<p>This section expands on recombining the genetic code of the parents by means of crossover into children (that is, the creation of the two child solutions by combining and re-organizing the code of the two parents). Various techniques can be used to create new solutions for generating a new population. The binary information of two viable solutions in machine learning can be recombined by a process called crossover, which is similar to biological genetic exchange, where genetic information is transmitted from parents to children. Crossover ensures that the genetic material of a solution is transmitted to the next generation.</p>
			<p>Crossover is the most common form of reproduction technique, or mating. Between the first and last bits of the parents (selected chromosomes), the crossover point represents the splitting point of the binary code that will be passed onto the children (offspring): the part to the left of the crossover point of the first parent will be inherited by the first child, and everything to the right side of the crossover point of the second parent will become the part of the first child. The left side of the second parent combined with the right side of the first parent results in the second child:</p>
			<p class="source-code">child1 = np.hstack((parent1[0:p],parent2[p:]))</p>
			<p class="source-code">child2 = np.hstack((parent2[0:p], parent1[p:]))</p>
			<p>There are multiple crossover techniques, as listed follows:</p>
			<ul>
				<li>Single-point crossover (which you can see in the preceding code) involves splitting the genetic code of the parents at one point and passing the first part to the first child, and the second part to the second child. It is used by traditional GAs; the crossover point is identical for both chromosomes and is selected randomly.</li>
				<li>Two-point crossover involves two crossover points impacting the gene exchange between the two parents. The more crossover points are introduced, the more the performance of the GA can be reduced as the genetic makeup is lost. However, introducing two-point crossover can lead to a better exploration of the state or parameter space.</li>
				<li>Multi-point crossover involves a number of splits. If the number of splits is even, then the splits are selected randomly and the sections in the chromosome are exchanged. If the number is odd, then the splits are alternating the exchanges of section.</li>
				<li>Uniform crossover involves the random selection (as in a coin toss) of the parent that will provide an element of the chromosome (gene).</li>
				<li>Three-parent crossover entails the comparison of each gene between two parents. If they have the same value, the child inherits the gene; if not, the child inherits the gene from the third parent.</li>
			</ul>
			<p>Consider the following code example:</p>
			<p class="source-code">def crossover_reproduction(parents, population):</p>
			<p class="source-code">    #define parents separately</p>
			<p class="source-code">    parent1 = parents[0]</p>
			<p class="source-code">    parent2 = parents[1]</p>
			<p class="source-code">    #randomly assign a point for cross-over</p>
			<p class="source-code">    p = random.randrange(0, len(population))</p>
			<p class="source-code">    print("Crossover point:", p)</p>
			<p class="source-code">    #create children by joining the parents at the cross-over point</p>
			<p class="source-code">    child1 = np.hstack((parent1[0:p],parent2[p:]))</p>
			<p class="source-code">    child2 = np.hstack((parent2[0:p], parent1[p:]))</p>
			<p class="source-code">    return child1, child2</p>
			<p>In the preceding code, we define the crossover function between two parents. We have defined the parents separately and then randomly assigned a certain point for crossover. Then, we have defined the children to be created by joining the parents at the defined crossover point.</p>
			<p>In the following exercise, you will continue the process of implementing the components of GAs to create child chromosomes. </p>
			<h2 id="_idParaDest-326"><a id="_idTextAnchor370"/>Exercise 12.05: Crossover for a New Generation</h2>
			<p>In this exercise, we will be implementing crossover between two parents, for a new generation. Following the steps from <em class="italic">Exercise 12.04, Implementing Tournament and Roulette Wheel</em>, and using the chromosomes with the highest weight, we will apply single-point crossover to create the first new set of children:</p>
			<ol>
				<li value="1">Create a new Jupyter Notebook. Import the <strong class="source-inline">random</strong> and <strong class="source-inline">numpy</strong> libraries:<p class="source-code">import random</p><p class="source-code">import numpy as np</p></li>
				<li>Create the function for a random population:<p class="source-code">def original_population(chromosomes, genes):</p><p class="source-code">    #initialize the population with zeroes</p><p class="source-code">    population =  np.zeros((chromosomes, genes))</p><p class="source-code">    #loop through each chromosome</p><p class="source-code">    for i in range(chromosomes):</p><p class="source-code">        #get random no. of ones to be created</p><p class="source-code">        ones = random.randint(0, genes)</p><p class="source-code">        #change zeroes to ones</p><p class="source-code">        population[i, 0:ones] = 1</p><p class="source-code">        #shuffle rows</p><p class="source-code">        np.random.shuffle(population[i])</p><p class="source-code">    return population</p><p>As you can see in the previous code, we have created a <strong class="source-inline">population</strong> function.</p></li>
				<li>Define a function to create the target solution:<p class="source-code">def create_target_solution(gene):</p><p class="source-code">    #assume that there is an equal number of ones and zeroes</p><p class="source-code">    counting_ones = int(gene/2)</p><p class="source-code">    # build array with equal no. of ones and zeros</p><p class="source-code">    target = np.zeros(gene)</p><p class="source-code">    target[0:counting_ones] = 1</p><p class="source-code">    # shuffle the array to mix zeroes and ones</p><p class="source-code">    np.random.shuffle(target)</p><p class="source-code">    return target</p></li>
				<li>Define a function for calculating the fitness weighting for each chromosome:<p class="source-code">def fitness_function(target,population):</p><p class="source-code">    #create an array of true/false compared to the reference</p><p class="source-code">    identical_to_target = population == target</p><p class="source-code">    #sum no. of genes that are identical</p><p class="source-code">    fitness_weights = identical_to_target.sum(axis = 1)</p><p class="source-code">    return fitness_weights</p></li>
				<li>Define a function for selecting the pair of parents with the highest weighting (highest fitness score). Since the population is smaller, the chromosomes are competing more. This method is also known as tournament selection:<p class="source-code"># select the best parents</p><p class="source-code">def select_parents(population, weights):</p><p class="source-code">    #identify the parent with the highest weight</p><p class="source-code">    parent1 = population[np.argmax(weights)]</p><p class="source-code">    #replace weight with the minimum number</p><p class="source-code">    weights[np.argmax(weights)] = 0</p><p class="source-code">    #identify the parent with the second-highest weight</p><p class="source-code">    parent2 = population[np.argmax(weights)]</p><p class="source-code">    return parent1, parent2</p></li>
				<li>Define a function for crossover by using a randomly selected crossover point:<p class="source-code">def crossover_reproduction(parents, population):</p><p class="source-code">    #define parents separately</p><p class="source-code">    parent1 = parents[0]</p><p class="source-code">    parent2 = parents[1]</p><p class="source-code">    #randomly assign a point for cross-over</p><p class="source-code">    p = random.randrange(0, len(population))</p><p class="source-code">    print("Crossover point:", p)</p><p class="source-code">    #create children by joining the parents at the cross-over point</p><p class="source-code">    child1 = np.hstack((parent1[0:p],parent2[p:]))</p><p class="source-code">    child2 = np.hstack((parent2[0:p], parent1[p:]))</p><p class="source-code">    return child1, child2</p></li>
				<li>Initialize the population with <strong class="source-inline">5</strong> chromosomes and <strong class="source-inline">8</strong> genes and calculate <strong class="source-inline">weights</strong>:<p class="source-code">population = original_population(5,8)</p><p class="source-code">target = create_target_solution(8)</p><p class="source-code">weights = fitness_function(target,population)</p></li>
				<li>Print the <strong class="source-inline">target</strong> solution:<p class="source-code">print('\n target:', target)</p><p>The output will be as follows:</p><p class="source-code">target: [1. 0. 0. 1. 1. 0. 1. 0.]</p></li>
				<li>Select the parents with the highest weight and print the final selection:<p class="source-code">parents = select_parents(population,weights)</p><p class="source-code">print('Parent 1:', parents[0],'\nParent 2:', parents[1])</p><p>The output will be as follows:</p><p class="source-code">Parent 1: [1. 0. 1. 1. 1. 0. 1. 1.] </p><p class="source-code">Parent 2: [1. 0. 0. 0. 0. 0. 0. 0.]</p></li>
				<li>Apply the <strong class="source-inline">crossover</strong> function and print the children:<p class="source-code">children = crossover_reproduction(parents,population)</p><p class="source-code">print('Child 1:', children[0],'\nChild 2:', children[1])</p><p>The output will be as follows:</p><p class="source-code">Crossover point: 4 </p><p class="source-code">Child 1: [1. 0. 1. 1. 0. 0. 0. 0.] </p><p class="source-code">Child 2: [1. 0. 0. 0. 1. 0. 1. 1.]</p></li>
				<li>Run the application.<p>You will get a similar output to that shown in the following snippet. As you can see, the population elements are randomized. Check that the elements of <strong class="source-inline">Child 1</strong> and <strong class="source-inline">Child 2</strong> are the same as those of <strong class="source-inline">Parent 1</strong> and <strong class="source-inline">Parent 2</strong>:</p><p class="source-code">target: [1. 0. 1. 1. 0. 0. 1. 0.]</p><p class="source-code">. . .</p><p class="source-code">Parent 1: [1. 0. 1. 1. 1. 0. 1. 1.]</p><p class="source-code">Parent 2: [0. 0. 1. 1. 0. 1. 0. 0.]</p><p class="source-code">. . .</p><p class="source-code">Crossover point: 1</p><p class="source-code">Child 1: [1. 0. 1. 1. 0. 1. 0. 0.]</p><p class="source-code">Child 2: [0. 0. 1. 1. 1. 0. 1. 1.]. . .</p></li>
			</ol>
			<p>You can check that the starting elements from the crossover point in the array of <strong class="source-inline">Child 1</strong> have the same array elements as <strong class="source-inline">Parent 2</strong>, and that <strong class="source-inline">Child 2</strong> has the same array elements as <strong class="source-inline">Parent 1</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30zHbup">https://packt.live/30zHbup</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fueZxx">https://packt.live/3fueZxx</a>.</p>
			<p>In this section, we identified the various strategies for the recombination technique known as crossover. A basic implementation of single-point crossover, where the crossover point is randomly generated, was represented. In the following section, we will examine the last element of GA design: population mutation. </p>
			<h2 id="_idParaDest-327"><a id="_idTextAnchor371"/>Components: Population Mutation</h2>
			<p>In the previous sections, you have implemented population generation, parent selection, and crossover reproduction. This section will concentrate on the application of random mutation and the repetition of child generations until a new population size is achieved and weights (fitness scores) for the population of the genetic algorithm are assigned. This section will include an explanation of the mutation technique. This will be followed by a presentation of the available mutation techniques as well as a discussion about population replacement. Finally, an exercise implementing mutation techniques will be presented.</p>
			<p>A caveat of gradient methods is that the algorithms can stop at a local optimum solution. To prevent this from happening, mutations can be introduced to the population of solutions. Mutation generally occurs after the crossover process. Mutation relies on randomly assigning binary information in either a set of chromosomes or in the entire population. Mutation provides an avenue of problem space exploration by introducing a random change in the population. This technique prevents rapid convergence and encourages the exploration of new solutions. In the final steps (the last generations) or when the optimal solution is reached, mutation ceases to be applied.</p>
			<p>There are various mutation techniques, as follows:</p>
			<ul>
				<li>Single-point mutation (flipping) involves randomly selecting genes from different chromosomes and changing their binary values to their opposites (from 0 to 1 and 1 to 0).</li>
				<li>Interchanging involves selecting two sections of the chromosome of one parent and swapping them, thus generating a new child.</li>
				<li>You can also reverse a randomly selected segment within the parent or the population of chromosomes, and all the binary values are changed to their opposites.</li>
			</ul>
			<p>The occurrence of a mutation is determined by its probability. The probability defines the frequency at which mutations occur within the population. If the probability is 0%, then after the crossover, the children are unaltered; if a mutation occurs, one or more parts of the chromosome or the population are changed. If the probability is 100%, then the entire chromosome is changed.</p>
			<p>After the mutation process occurs, the fitness of the new children is calculated, and the population is altered to include them. This leads to a new generation of the population. Depending on the strategy used, the parents with the lowest fitness scores are discarded to leave room for the newly generated children.</p>
			<h2 id="_idParaDest-328"><a id="_idTextAnchor372"/>Exercise 12.06: New Generation Development Using Mutation</h2>
			<p>In this exercise, we will be focusing on the development of a new generation. We will again create a new population, select two parent chromosomes, and use crossover to develop two children. We will then add the two new chromosomes to the population and mutate the entire population with a probability of 0.05:</p>
			<ol>
				<li value="1">Create a new Jupyter Notebook. Import the <strong class="source-inline">random</strong> and <strong class="source-inline">numpy</strong> libraries:<p class="source-code">import random</p><p class="source-code">import numpy as np</p></li>
				<li>Create a function for the random population:<p class="source-code">def original_population(chromosomes, genes):</p><p class="source-code">    #initialize the population with zeroes</p><p class="source-code">    population =  np.zeros((chromosomes, genes))</p><p class="source-code">    #loop through each chromosome</p><p class="source-code">    for i in range(chromosomes):</p><p class="source-code">        #get random no. of ones to be created</p><p class="source-code">        ones = random.randint(0, genes)</p><p class="source-code">        #change zeroes to ones</p><p class="source-code">        population[i, 0:ones] = 1</p><p class="source-code">        #shuffle rows</p><p class="source-code">        np.random.shuffle(population[i])</p><p class="source-code">    return population</p></li>
				<li>Define a function to create the target solution:<p class="source-code">def create_target_solution(gene):</p><p class="source-code">    #assume that there is an equal number of ones and zeroes</p><p class="source-code">    counting_ones = int(gene/2)</p><p class="source-code">    # build array with equal no. of ones and zeros</p><p class="source-code">    target = np.zeros(gene)</p><p class="source-code">    target[0:counting_ones] = 1</p><p class="source-code">    # shuffle the array to mix zeroes and ones</p><p class="source-code">    np.random.shuffle(target)</p><p class="source-code">    return target</p></li>
				<li>Define a function to calculate the fitness weighting for each chromosome:<p class="source-code">def fitness_function(target,population):</p><p class="source-code">    #create an array of true/false compared to the reference</p><p class="source-code">    identical_to_target = population == target</p><p class="source-code">    #sum no. of genes that are identical</p><p class="source-code">    fitness_weights = identical_to_target.sum(axis = 1)</p><p class="source-code">    return fitness_weights</p></li>
				<li>Define a function to select the pair of parents with the highest weighting (highest fitness score). Since the population is small, the chromosomes are competing more. This method is also known as tournament selection:<p class="source-code"># select the best parents</p><p class="source-code">def select_parents(population, weights):</p><p class="source-code">    #identify the parent with the highest weight</p><p class="source-code">    parent1 = population[np.argmax(weights)]</p><p class="source-code">    #replace weight with the minimum number</p><p class="source-code">    weights[np.argmax(weights)] = 0</p><p class="source-code">    #identify the parent with the second-highest weight</p><p class="source-code">    parent2 = population[np.argmax(weights)]</p><p class="source-code">    return parent1, parent2</p></li>
				<li>Define a function for crossover by using a randomly selected crossover point:<p class="source-code">def crossover_reproduction(parents, population):</p><p class="source-code">    #define parents separately</p><p class="source-code">    parent1 = parents[0]</p><p class="source-code">    parent2 = parents[1]</p><p class="source-code">    #randomly assign a point for cross-over</p><p class="source-code">    p = random.randrange(0, len(population))</p><p class="source-code">    print("Crossover point:", p)</p><p class="source-code">    #create children by joining the parents at the cross-over point</p><p class="source-code">    child1 = np.hstack((parent1[0:p],parent2[p:]))</p><p class="source-code">    child2 = np.hstack((parent2[0:p], parent1[p:]))</p><p class="source-code">    return child1, child2</p></li>
				<li>Define a function for a mutation that uses the probability and the population as inputs:<p class="source-code">def mutate_population(population, mutation_probability):</p><p class="source-code">    #create array of random mutations that uses the population</p><p class="source-code">    mutation_array = np.random.random(size = (population.shape))</p><p class="source-code">    """</p><p class="source-code">    compare elements of the array with the probability and </p><p class="source-code">    put the results into an array</p><p class="source-code">    """</p><p class="source-code">    mutation_boolean = mutation_array \</p><p class="source-code">                       &gt;= mutation_probability</p><p class="source-code">    """</p><p class="source-code">    convert boolean into binary and store to create a new </p><p class="source-code">    array for the population</p><p class="source-code">    """</p><p class="source-code">    population[mutation_boolean] = np.logical_not\</p><p class="source-code">                                   (population[mutation_boolean])</p><p class="source-code">    return population</p><p>In the preceding code snippet, the condition set for the mutation selection is to check that each element of the array is higher than the mutation probability which acts as a threshold. If the element is higher than the threshold, mutation is applied.</p></li>
				<li>Append the array of <strong class="source-inline">children</strong> to the original population, creating a new crossover <strong class="source-inline">population</strong>, and use the <strong class="source-inline">print()</strong> function to display it:<p class="source-code">population = original_population(5,8)</p><p class="source-code">target = create_target_solution(8)</p><p class="source-code">weights = fitness_function(target,population)</p><p class="source-code">parents = select_parents(population,weights)</p><p class="source-code">children = crossover_reproduction(parents,population)</p><p>The output will be as follows:</p><p class="source-code">Crossover point: 3</p></li>
				<li>Next, append <strong class="source-inline">population</strong> with <strong class="source-inline">children</strong>:<p class="source-code">population_crossover = np.append(population, children, axis= 0)</p><p class="source-code">print('\nPopulation after the cross-over:\n', \</p><p class="source-code">      population_crossover)</p><p>The population will be as follows:</p><p class="source-code">Population after the cross-over:</p><p class="source-code"> [[0. 1. 0. 0. 0. 0. 1. 0.]</p><p class="source-code"> [0. 0. 0. 0. 0. 1. 0. 0.]</p><p class="source-code"> [1. 1. 1. 1. 1. 0. 0. 1.]</p><p class="source-code"> [1. 1. 1. 0. 1. 1. 1. 1.]</p><p class="source-code"> [0. 1. 1. 1. 1. 0. 0. 0.]</p><p class="source-code"> [1. 1. 1. 1. 1. 0. 0. 1.]</p><p class="source-code"> [1. 1. 1. 0. 1. 1. 1. 1.]]</p></li>
				<li>Use the crossover population and the mutation probability of <strong class="source-inline">0.05</strong> to create a new population and display the mutated population:<p class="source-code">mutation_probability = 0.05</p><p class="source-code">new_population = mutate_population\</p><p class="source-code">                 (population_crossover,mutation_probability)</p><p class="source-code">print('\nNext generation of the population:\n',\</p><p class="source-code">      new_population)</p><p>As you can see, the threshold(mutation_probability) is 0.05. Hence, if the elements are higher than this threshold, they will incur a mutation (so there is a 95% chance of the mutation occurring to the gene).</p><p>The output will be as follows:</p><p class="source-code">Next generation of the population:</p><p class="source-code"> [[1. 0. 1. 1. 1. 1. 0. 1.]</p><p class="source-code"> [1. 0. 1. 1. 1. 0. 1. 1.]</p><p class="source-code"> [1. 0. 0. 0. 0. 1. 1. 0.]</p><p class="source-code"> [0. 0. 0. 1. 0. 0. 0. 0.]</p><p class="source-code"> [1. 0. 0. 0. 1. 1. 1. 1.]</p><p class="source-code"> [0. 0. 0. 0. 0. 1. 1. 0.]</p><p class="source-code"> [0. 0. 0. 1. 0. 1. 0. 1.]]</p></li>
			</ol>
			<p>You will get a similar output as the population elements are randomized. You can see that the chromosomes resulting from crossover are added to the original population and that after mutation, the population has the same number of chromosomes, but the genes are different. The crossover and mutation steps can be repeated until the target solution is reached by looping the functions. These cycles are also known as generations.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3dXaBqi">https://packt.live/3dXaBqi</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Ysc5Cl">https://packt.live/2Ysc5Cl</a>.</p>
			<p>In this section, mutation was described. The benefit of mutation is that it introduces random variation to chromosomes, encouraging exploration and helping to avoid local optima. Various mutation techniques were presented. The example we used showed the impact of mutation probability by implementing reverse mutation on a population after the crossover process was finalized.</p>
			<h2 id="_idParaDest-329"><a id="_idTextAnchor373"/>Application to Hyperparameter Selection </h2>
			<p>In this section, we will explore the use of GAs for parameter selection, especially when using neural networks. GAs are widely used for optimization problems in scheduling in both production and railway management. The solutions to these types of problems rely on creating a combination of neural networks and GAs as function optimizers.</p>
			<p>The exercise in this section provides a platform for tuning hyperparameters for a neural network to predict wind flow patterns. You will apply a simple genetic algorithm to optimize the values of the hyperparameters used to train a neural network.</p>
			<p><strong class="bold">Artificial Neural Networks</strong> (<strong class="bold">ANNs</strong>) model the biological processes and structures of neurons in the brain. The neurons in ANNs rely on a combination of input information (parameters) and weights. The product (which has an added bias) passes through a transfer function, which is a set of neurons arranged in parallel with each other to form a layer.</p>
			<p>For weight and bias optimization, ANNs use gradient descent methods for their training processes and backpropagation processes. This impacts the development of the neural network, as before training even commences, the neural network topology needs to be fully designed. Because the design is pre-set, some neurons may not be used in the training process, but they may still be active, therefore making them redundant. Additionally, neural networks using gradient methods can become stuck at a local optimum, and therefore need to rely on alternative methods to help them continue their processes, such as regularization, ridge regression, or lasso regression. ANNs are widely used in speech recognition, feature detection (whether for image, topology, or signal processing), and disease detection.</p>
			<p>To prevent these problems and enhance the training of neural networks, GAs can be implemented. GAs are used for function optimization, while crossover and mutation techniques help with problem space exploration. Initially, GAs were used to optimize the weights and number of nodes of neural networks. For this, the chromosomes of the GA are encoded with possible variations of weights and nodes. The fitness function generated by the ANN relies on the mean squared error of the potential values and the exact values of the parameters.</p>
			<p>However, research has expanded to implementations of <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>) and combining them with RL, aiming towards multi-processor performance. An RNN is a type of ANN that produces outputs that are not only a result of the weighting process of the input but also of a vector containing previous input and outputs. This enables the neural network to maintain prior knowledge of previous training instances.</p>
			<p>GAs serve in expanding the topology of the neural networks beyond weighting adjustments. One example is EDEN, whereby encoding is done within the chromosome and the architecture of the network, and the learning rate achieves high accuracy rates on multiple TensorFlow datasets. One of the most challenging problems in training neural networks is the quality of the features (or input hyperparameters) that are fed to the network. If the parameters are not appropriate, the mapping of inputs and outputs will be erroneous. Therefore, GAs can act as a wrapper alternative to the ANNs by optimizing the selection of features.</p>
			<p>The following exercise will teach you how to apply a simple genetic algorithm to identify the optimal parameters (window size and number of units) for an RNN. The genetic algorithm implemented is using the <strong class="source-inline">deap</strong> package, through the <strong class="source-inline">eaSimple()</strong> function, which enables you to create, using toolbox-based code, a simple GA that includes population creation, selection through the <strong class="source-inline">selRandom()</strong> function, reproduction through the <strong class="source-inline">cxTwoPoint()</strong> function, and mutation through the <strong class="source-inline">mutFlipBit()</strong> function. For comparing and hyperparameter selection, the <strong class="source-inline">selBest()</strong> function is used.</p>
			<h2 id="_idParaDest-330"><a id="_idTextAnchor374"/>Exercise 12.07: Implementing GA Hyperparameter Optimization for RNN Training</h2>
			<p>Our goal in this exercise is to identify the best hyperparameters to use for an RNN using a simple genetic algorithm. In this exercise, we are using a dataset that was part of a weather forecasting challenge in 2012. A single feature, <strong class="source-inline">wp2</strong>, is used in the training and validation of the parameters. The two hyperparameters used are the number of units and the window size. These hyperparameters represent the genetic material for the chromosome:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be found in the GitHub repository at the following link: https://packt.live/2Ajjz2F.</p>
			<p class="callout">The original dataset can be found at the following link: https://www.kaggle.com/c/GEF2012-wind-forecasting/data.</p>
			<ol>
				<li value="1">Create a new Jupyter Notebook. Import the <strong class="source-inline">pandas</strong> and <strong class="source-inline">numpy</strong> libraries and functions:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.metrics import mean_squared_error</p><p class="source-code">from sklearn.model_selection import train_test_split as split</p><p class="source-code">from tensorflow.keras.layers import SimpleRNN, Input, Dense</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from deap import base, creator, tools, algorithms</p><p class="source-code">from scipy.stats import bernoulli</p><p class="source-code">from bitstring import BitArray</p><p>From the <strong class="source-inline">sklearn</strong> package, import <strong class="source-inline">mean_squared_error</strong> and <strong class="source-inline">train_test_split</strong>. Also, from the <strong class="source-inline">tensorflow</strong> and <strong class="source-inline">keras</strong> packages, import <strong class="source-inline">SimpleRNN</strong>, <strong class="source-inline">Input</strong>, <strong class="source-inline">Dense</strong> (from the <strong class="source-inline">layers</strong> folder), and the model (from the <strong class="source-inline">Model</strong> class). To create the GA, it is necessary to call from the <strong class="source-inline">deap</strong> package <strong class="source-inline">base</strong>, <strong class="source-inline">creator</strong>, <strong class="source-inline">tools</strong>, and <strong class="source-inline">algorithms</strong>. For statistics, we are using the Bernoulli equation; therefore, we will call <strong class="source-inline">bernoulli</strong> from the <strong class="source-inline">scipy.stats</strong> package. From <strong class="source-inline">bitstrings</strong>, we will call <strong class="source-inline">BitArray</strong>.</p></li>
				<li>Use a random seed for model development; <strong class="source-inline">998</strong> is an initialization number for the seed:<p class="source-code">np.random.seed(998)</p></li>
				<li>Load data from the <strong class="source-inline">train.csv</strong> file, use <strong class="source-inline">np.reshape()</strong> to modify the data into an array that only contains column <strong class="source-inline">wp2</strong>, and select the first 1,501 elements:<p class="source-code">#read data from csv</p><p class="source-code">data = pd.read_csv('../Dataset/train.csv')</p><p class="source-code">#use column wp2</p><p class="source-code">data = np.reshape(np.array(data['wp2']), (len(data['wp2']), 1))</p><p class="source-code">data = data[0:1500]</p></li>
				<li>Define a function to split the dataset based on window size:<p class="source-code">def format_dataset(data, w_size):</p><p class="source-code">    #initialize as empty array</p><p class="source-code">    X, Y = np.empty((0, w_size)), np.empty(0)</p><p class="source-code">    """</p><p class="source-code">    depending on the window size the data is separated in </p><p class="source-code">    2 arrays containing each of the sizes</p><p class="source-code">    """</p><p class="source-code">    for i in range(len(data)-w_size-1):</p><p class="source-code">        X = np.vstack([X,data[i:(i+w_size),0]])</p><p class="source-code">        Y = np.append(Y, data[i+w_size,0])</p><p class="source-code">    X = np.reshape(X,(len(X),w_size,1))</p><p class="source-code">    Y = np.reshape(Y,(len(Y), 1))</p><p class="source-code">    return X, Y</p></li>
				<li>Define a function to train the RNN to identify the optimal hyperparameters using a simple genetic algorithm:<p class="source-code">def training_hyperparameters(ga_optimization):</p><p class="source-code">    """</p><p class="source-code">    decode GA solution to integer window size and number of units</p><p class="source-code">    """</p><p class="source-code">    w_size_bit = BitArray(ga_optimization[0:6])</p><p class="source-code">    n_units_bit = BitArray(ga_optimization[6:])</p><p class="source-code">    w_size = w_size_bit.uint</p><p class="source-code">    n_units = n_units_bit.uint</p><p class="source-code">    print('\nWindow Size: ', w_size, \</p><p class="source-code">          '\nNumber of units: ',n_units)</p><p class="source-code">    """</p><p class="source-code">    return fitness score of 100 if the size or the units are 0</p><p class="source-code">    """</p><p class="source-code">    if w_size == 0 or n_units == 0:</p><p class="source-code">        return 100</p><p class="source-code">    """</p><p class="source-code">    segment train data on the window size splitting it into </p><p class="source-code">    90 train, 10 validation</p><p class="source-code">    """</p><p class="source-code">    X,Y = format_dataset(data, w_size)</p><p class="source-code">    X_train, X_validate, Y_train, Y_validate = \</p><p class="source-code">    split(X, Y, test_size= 0.10, random_state= 998)</p><p>The first step is identifying the sections of the chromosome pertaining to window size and the number of units. The next step is to return an extremely high fitness score, if there are no window sizes or the number of units. Split the two arrays into training and validation arrays with a 90:10 split. </p></li>
				<li>Initialize the input features, and use the <strong class="source-inline">SimpleRNN</strong> model with the training dataset. For optimization, use the Adam algorithm with mean squared error as the loss function. To train the model, use the <strong class="source-inline">fit</strong> function with <strong class="source-inline">5</strong> for <strong class="source-inline">epochs</strong> and a batch size of <strong class="source-inline">4</strong>. To generate the predicted values, use the input values stored in <strong class="source-inline">X_validate</strong> in the <strong class="source-inline">predict</strong> function for the model. Calculate the <strong class="bold">Root Mean Squared Error</strong> (<strong class="bold">RMSE</strong>) between the validation set and predicted set of output variables. Return <strong class="source-inline">RMSE</strong>:<p class="source-code">    input_features = Input(shape=(w_size,1))</p><p class="source-code">    x = SimpleRNN(n_units,input_shape=(w_size,1))(input_features)</p><p class="source-code">    output = Dense(1, activation='linear')(x)</p><p class="source-code">    rnnmodel = Model(inputs=input_features, outputs = output)</p><p class="source-code">    rnnmodel.compile(optimizer='adam', \</p><p class="source-code">                     loss = 'mean_squared_error')</p><p class="source-code">    rnnmodel.fit(X_train, Y_train, epochs=5, \</p><p class="source-code">                 batch_size=4, shuffle = True)</p><p class="source-code">    Y_predict = rnnmodel.predict(X_validate)</p><p class="source-code">    # calculate RMSE score as fitness score for GA</p><p class="source-code">    RMSE = np.sqrt(mean_squared_error(Y_validate, Y_predict))</p><p class="source-code">    print('Validation RMSE: ', RMSE, '\n')</p><p class="source-code">    return RMSE,</p></li>
				<li>Instantiate the population size, the number of generations used for the genetic algorithm, and the length of the gene with <strong class="source-inline">4</strong>, <strong class="source-inline">5</strong>, and <strong class="source-inline">10</strong>, respectively:<p class="source-code">population_size = 4</p><p class="source-code">generations = 5</p><p class="source-code">gene = 10</p></li>
				<li>Use the toolbox available in the <strong class="source-inline">deap</strong> package to instantiate the genetic algorithm, <strong class="source-inline">eaSimple()</strong>. To do this, use the creator tool to instantiate the fitness function as <strong class="source-inline">RMSE</strong>:<p class="source-code">creator.create('FitnessMax', base.Fitness, weights= (-1.0,))</p><p class="source-code">creator.create('Individual', list, fitness = creator.FitnessMax)</p><p class="source-code">toolbox = base.Toolbox()</p><p class="source-code">toolbox.register('bernoulli', bernoulli.rvs, 0.5)</p><p class="source-code">toolbox.register('chromosome', tools.initRepeat, \</p><p class="source-code">                 creator.Individual, toolbox.bernoulli, n = gene)</p><p class="source-code">toolbox.register('population', tools.initRepeat, \</p><p class="source-code">                 list, toolbox.chromosome)</p><p class="source-code">toolbox.register('mate', tools.cxTwoPoint)</p><p class="source-code">toolbox.register('mutate', tools.mutFlipBit, indpb = 0.6)</p><p class="source-code">toolbox.register('select', tools.selRandom)</p><p class="source-code">toolbox.register('evaluate', training_hyperparameters)</p><p class="source-code">population = toolbox.population(n = population_size)</p><p class="source-code">algo = algorithms.eaSimple(population,toolbox,cxpb=0.4, \</p><p class="source-code">                           mutpb=0.1, ngen=generations, \</p><p class="source-code">                           verbose=False)</p><p>The last few lines of the output will be as follows:</p><p class="source-code">Window Size:  48 </p><p class="source-code">Number of units:  15</p><p class="source-code">Train on 1305 samples</p><p class="source-code">Epoch 1/5</p><p class="source-code">1305/1305 [==============================] - 3s 2ms/sample </p><p class="source-code">- loss: 0.0106</p><p class="source-code">Epoch 2/5</p><p class="source-code">1305/1305 [==============================] - 3s 2ms/sample </p><p class="source-code">- loss: 0.0066</p><p class="source-code">Epoch 3/5</p><p class="source-code">1305/1305 [==============================] - 3s 2ms/sample </p><p class="source-code">- loss: 0.0057</p><p class="source-code">Epoch 4/5</p><p class="source-code">1305/1305 [==============================] - 3s 2ms/sample </p><p class="source-code">- loss: 0.0051</p><p class="source-code">Epoch 5/5</p><p class="source-code">1305/1305 [==============================] - 3s 2ms/sample </p><p class="source-code">- loss: 0.0049</p><p class="source-code">Validation RMSE:  0.05564985152918074</p><p>The lower the <strong class="source-inline">RMSE</strong> value, the better the hyperparameters. The Bernoulli distribution serves to randomly initialize the chromosome genes. Based on the chromosome, the population is initialized. Within the toolbox, there are four steps for creating a new population: <strong class="bold">mate</strong> (this refers to the crossover process: <strong class="source-inline">cxTwoPoint()</strong> refers to the parents crossing information at two points in a crossover), <strong class="bold">mutate</strong> (this refers to the mutation process: <strong class="source-inline">mutFlipBit()</strong> will only mutate one of the elements of the chromosome with a <strong class="source-inline">0.6</strong> probability of occurrence), <strong class="bold">select</strong> (selection of the parents happens randomly through the <strong class="source-inline">selRandom()</strong> function), <strong class="bold">evaluate</strong> (this uses the RNN training function from <em class="italic">Step 6</em> and <em class="italic">Step 7</em>).</p></li>
				<li>Use the <strong class="source-inline">selBest()</strong> function for a single optimal solution, <strong class="source-inline">k=1</strong>, compare the solutions to the fitness function, and select the one with the highest similarity. To get the optimal window size and number of units, loop through the chromosome and convert the bit values to unsigned integers and print the optimal hyperparameters:<p class="source-code">optimal_chromosome = tools.selBest(population, k = 1)</p><p class="source-code">optimal_w_size = None</p><p class="source-code">optimal_n_units = None</p><p class="source-code">for op in optimal_chromosome:</p><p class="source-code">    w_size_bit = BitArray(op[0:6])</p><p class="source-code">    n_units_bit = BitArray(op[6:])</p><p class="source-code">    optimal_w_size = w_size_bit.uint</p><p class="source-code">    optimal_n_units = n_units_bit.uint</p><p class="source-code">    print('\nOptimal window size:', optimal_w_size, \</p><p class="source-code">          '\n Optimal number of units:', optimal_n_units)</p><p>The output will be as follows:</p><p class="source-code">Optimal window size: 48 </p><p class="source-code">Optimal number of units: 15</p></li>
				<li>Run the application. You will get a similar output to what you see here. The initial values for the window size and number of units will be displayed. The GA will run using the RNN for the total number of epochs. At the end of each epoch, the <strong class="source-inline">RMSE</strong> value is displayed. Once all the epochs have executed, the optimal values are displayed:<div id="_idContainer810" class="IMG---Figure"><img src="image/B16182_12_10.jpg" alt="Figure 12.10: Optimization of the window size and number of units using GA&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 12.10: Optimization of the window size and number of units using GA</p>
			<p>We started with an initial window size of <strong class="source-inline">51</strong> and <strong class="source-inline">15</strong> units; the optimal window size is reduced to <strong class="source-inline">28</strong>, and the number of units to <strong class="source-inline">4</strong>. The difference between the parameters based on <strong class="source-inline">RMSE</strong> is reduced to <strong class="source-inline">0.05</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/37sgQA6">https://packt.live/37sgQA6</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/30AOKRK">https://packt.live/30AOKRK</a>.</p>
			<p>This section has covered combining GAs with neural networks as an alternative to using gradient descent methods. GAs mainly served in optimizing the number of neurons and weights for the neural networks, but their use can be expanded, through hybridization, to optimizing the structure of the network and hyperparameter selection. This exercise tested your ability to apply a genetic algorithm to find the optimal values of two features related to a weather forecasting problem. The features were used to train an RNN to estimate wind flow using RMSE values. In the following section, you will expand your knowledge of hybrid optimization techniques for the entire architecture of a neural network using NEAT.</p>
			<h2 id="_idParaDest-331"><a id="_idTextAnchor375"/>NEAT and Other Formulations</h2>
			<p>Neuroevolution is a term that refers to evolving neural networks using GAs. This branch of machine learning is shown to outperform RL in various problems and can be coupled with RL, as it is a method for unsupervised learning. As mentioned in the previous section, neuroevolution systems concentrate on changing the weights, the number of neurons (in the hidden layers), and the topology of ANNs.</p>
			<p><strong class="bold">Neuroevolution of Augmented Topologies</strong> (<strong class="bold">NEAT</strong>) focuses on topology evolution for ANNs. It involves training a simple ANN structure, consisting of input and output neurons and units to represent the bias, but no hidden layers. Each ANN structure is encoded within a chromosome that contains node genes and connection genes (the mapping or link between two node genes). Each connection specifies the input, output, weight node, activation of the connection, and innovation number that serves as a link between genes for the crossover process.</p>
			<p>Mutations relate to the weights of the connections or the structure of the full system. Structural mutations can appear either by including a connection between two nodes that are not linked or by including a new node to a pre-existing connection, which causes two new connections to be built (one between the existing pair of nodes and one that includes the newly created node).</p>
			<p>The crossover process entails the identification of common genes between different chromosomes within the population. This relies on the historical information about gene derivation, using a global innovation number. The genes resulting from the mutation receive incremented numbers from the gene they mutated, whereas through crossover, the genes keep their original numbers. This technique helps in solving the problems with gene matching that cause issues for neural network topologies. The genes that do not have the same innovation number are selected from the parent with the highest fitness. If both parents have the same fitness, the genes are selected randomly from each of the parents.</p>
			<p>Chromosomes that have similar topologies are grouped based on how far apart they are <img src="image/B16182_12_10a.png" alt="15"/>; individuals are therefore evaluated based on the genes that are different <img src="image/B16182_12_10b.png" alt="16"/>, supplementary genes <img src="image/B16182_12_10c.png" alt="17"/> , and the differences in weight <img src="image/B16182_12_10d.png" alt="18"/>  for the similar genes compared to the average number of genes <img src="image/B16182_12_10e.png" alt="20"/>. Each of the coefficients <img src="image/B16182_12_10f.png" alt="19"/> acts as a weight that highlights the significance of each parameter:</p>
			<div>
				<div id="_idContainer817" class="IMG---Figure">
					<img src="image/B16182_12_11.jpg" alt="Figure 12.11: Topology distance calculation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.11: Topology distance calculation</p>
			<p>To categorize the chromosomes into species, the distance <img src="image/B16182_12_11a.png" alt="21"/> is compared with a threshold <img src="image/B16182_12_11b.png" alt="22"/>. If <img src="image/B16182_12_11c.png" alt="23"/>, then the chromosome belongs to the first species where this condition is fulfilled. To prevent species dominance, all the elements of the species need to have the same fitness level, which is calculated based on the number of members in the species. The evolution of the species (how many new chromosomes are included, <img src="image/B16182_12_11d.png" alt="24"/> depends on the comparison between the fitness of the species, <img src="image/B16182_12_11e.png" alt="25"/>, and the average fitness of the population, <img src="image/B16182_12_11f.png" alt="26"/>:</p>
			<div>
				<div id="_idContainer824" class="IMG---Figure">
					<img src="image/B16182_12_12.jpg" alt="Figure 12.12: Calculation of the number of new chromosomes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.12: Calculation of the number of new chromosomes</p>
			<p>The advantage of NEAT is that, unlike neuroevolution algorithms that have a random set of topology parameters, it starts with the simplest topological form of a neural network and progressively evolves it to find the optimal solution, significantly reducing the number of used generations.</p>
			<p>Evolving topology algorithms are categorized as <strong class="bold">Weight Evolving Artificial Neural Networks</strong> (<strong class="bold">TWEANNs</strong>), which include EDEN, <strong class="bold">Cellular Encoding</strong> (<strong class="bold">CE</strong>), <strong class="bold">Enforced Subpopulations</strong> (<strong class="bold">SE</strong>) – a fixed topology system (out of which NEAT outperforms the latter two on CartPole) – <strong class="bold">Parallel Distributed Genetic Programming</strong> (<strong class="bold">PDGP</strong>), and <strong class="bold">Generalized Acquisition of Recurrent Links</strong> (<strong class="bold">GNARL</strong>).</p>
			<p>We will now see an exercise on applying NEAT to solve a simple XNOR gate, a logic gate that has a binary output. The binary inputs and output are quantified using a truth table, which is a representation of the sets of the functional values of Boolean logic expressions showcasing the combination of the logical values.</p>
			<h2 id="_idParaDest-332"><a id="_idTextAnchor376"/>Exercise 12.08: XNOR Gate Functionality Using NEAT</h2>
			<p>In the exercise, you will see the impact that NEAT has on solving a simple Boolean algebra problem. The problem involves implementing the NEAT algorithm to identify the optimal neural network topology for reproducing the binary output of an exclusive NOR (XNOR) gate. This is a type of logic gate where, when both inputs have the same signal (either 0 or 1 – equivalent to off and on, respectively), the output of the logic gate will be 1 (on), whereas when one of the inputs is high (1) and the other is low (0), the output will be 0 (off).</p>
			<p>We have the following truth table for the XNOR logic gate:</p>
			<div>
				<div id="_idContainer825" class="IMG---Figure">
					<img src="image/B16182_12_13.jpg" alt="Figure 12.13: Truth table for the XNOR gate&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.13: Truth table for the XNOR gate</p>
			<p>Use the NEAT algorithm to create a feedforward neural network that can mimic the output of an XNOR gate.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">In your Anaconda environment, execute the following command:<p class="source-code">conda install neat</p></li>
				<li>Create a new Jupyter Notebook.</li>
				<li>Import <strong class="source-inline">print_function</strong> from the <strong class="source-inline">__future__</strong> file, and import the <strong class="source-inline">neat</strong> and <strong class="source-inline">os</strong> packages:<p class="source-code">from __future__ import print_function</p><p class="source-code">import os</p><p class="source-code">import neat</p></li>
				<li>Initialize the inputs and the output of the XNOR gate based on the truth table:<p class="source-code">xnor_inputs = [(0.0, 0.0), (0.0, 1.0), (1.0, 0.0), (1.0, 1.0)]</p><p class="source-code">xnor_output = [(1.0,),(0.0,),(0.0,),(1.0,)]</p></li>
				<li>Create a fitness function that uses the squared difference between the actual output and the output of a feedforward neural network using NEAT:<p class="source-code"> def fitness_function(chromosomes, configuration):</p><p class="source-code">    for ch_id, chromosome in chromosomes:</p><p class="source-code">        chromosome.fitness = 4.0</p><p class="source-code">        neural_net = neat.nn.FeedForwardNetwork.create\</p><p class="source-code">                     (chromosome, configuration)</p><p class="source-code">        for xnor_i,xnor_o in zip(xnor_inputs, xnor_output):</p><p class="source-code">            output = neural_net.activate(xnor_i)</p><p class="source-code">            squared_diff = (output[0] - xnor_o[0])**2</p><p class="source-code">            chromosome.fitness -= squared_diff</p></li>
				<li>Create a new text file with the name <strong class="source-inline">config-feedforward-xnor</strong>. Include in the file the following parameters for the NEAT algorithm. For the fitness function, select the maximal value, with a threshold close to <strong class="source-inline">4</strong> and a population size of <strong class="source-inline">200</strong>:<p class="source-code">[NEAT]</p><p class="source-code">fitness_criterion    = max</p><p class="source-code">fitness_threshold    = 3.9</p><p class="source-code">pop_size             = 200</p><p class="source-code">reset_on_extinction   = False</p></li>
				<li>In the same <strong class="source-inline">config-feedforward-xnor</strong> file, include the <strong class="source-inline">sigmoid</strong> function for node activation with a mutation rate of <strong class="source-inline">0.01</strong>. The aggregation options are mostly about adding the values, with a mutation rate of 0 for aggregation:<p class="source-code">[DefaultGenome]</p><p class="source-code"># activation options of the nodes</p><p class="source-code">activation_default      = sigmoid</p><p class="source-code">activation_mutate_rate  = 0.01</p><p class="source-code">activation_options      = sigmoid</p><p class="source-code"># aggregation options for the node</p><p class="source-code">aggregation_default    = sum</p><p class="source-code">aggregation_mutate_rate = 0.0</p><p class="source-code">aggregation_options    = sum</p></li>
				<li>Set the <strong class="source-inline">bias</strong> parameters for the algorithm:<p class="source-code"># bias options for the node</p><p class="source-code">bias_init_mean          = 0.0</p><p class="source-code">bias_init_stdev         = 0.05</p><p class="source-code">bias_max_value          = 30.0</p><p class="source-code">bias_min_value          = -30.0</p><p class="source-code">bias_mutate_power       = 0.5</p><p class="source-code">bias_mutate_rate        = 0.8</p><p class="source-code">bias_replace_rate       = 0.1</p><p>For the bias, the minimum and maximum values are <strong class="source-inline">-30</strong> and <strong class="source-inline">30</strong>. Set the initial standard deviation at <strong class="source-inline">0.05</strong>, as low as possible, with a power of <strong class="source-inline">0.5</strong>, a mutation rate of <strong class="source-inline">0.8</strong>, and a replacement rate of <strong class="source-inline">0.1</strong>. These values are essential for implementing the genetic algorithm optimization.</p></li>
				<li>Define the coefficients <img src="image/B16182_12_13a.png" alt="27"/>, as we are only considering the difference between the genes (how disjointed they are) and the difference in weights:<p class="source-code"># compatibility options for the genes in the chromosome</p><p class="source-code">compatibility_disjoint_coefficient = 1.0</p><p class="source-code">compatibility_weight_coefficient   = 0.5</p></li>
				<li>Include the information about topology, connection, and node inclusion or removal-related parameters:<p class="source-code"># add/remove rates for connections between nodes</p><p class="source-code">conn_add_prob           = 0.5</p><p class="source-code">conn_delete_prob        = 0.5</p><p class="source-code"># connection enable options</p><p class="source-code">enabled_default         = True</p><p class="source-code">enabled_mutate_rate     = 0.01</p><p class="source-code">feed_forward            = True</p><p class="source-code">initial_connection      = full</p><p class="source-code"># add/remove rates for nodes</p><p class="source-code">node_add_prob           = 0.2</p><p class="source-code">node_delete_prob        = 0.2</p></li>
				<li>Start with a simple network without any hidden layers and set the response parameters for the nodes and connections:<p class="source-code"># network parameters</p><p class="source-code">num_hidden              = 0</p><p class="source-code">num_inputs              = 2</p><p class="source-code">num_outputs             = 1</p><p class="source-code"># node response options</p><p class="source-code">response_init_mean      = 1.0</p><p class="source-code">response_init_stdev     = 0.0</p><p class="source-code">response_max_value      = 30.0</p><p class="source-code">response_min_value      = -30.0</p><p class="source-code">response_mutate_power   = 0.0</p><p class="source-code">response_mutate_rate    = 0.0</p><p class="source-code">response_replace_rate   = 0.0</p><p class="source-code"># connection weight options</p><p class="source-code">weight_init_mean        = 0.0</p><p class="source-code">weight_init_stdev       = 1.0</p><p class="source-code">weight_max_value        = 30</p><p class="source-code">weight_min_value        = -30</p><p class="source-code">weight_mutate_power     = 0.5</p><p class="source-code">weight_mutate_rate      = 0.9</p><p class="source-code">weight_replace_rate     = 0.15</p></li>
				<li>Select the default parameters for the distance threshold, species fitness function, and parent selection. This is the final set of parameters to be included in the <strong class="source-inline">config-feedforward-xnor</strong> file:<p class="source-code">[DefaultSpeciesSet]</p><p class="source-code">compatibility_threshold = 3.0</p><p class="source-code">[DefaultStagnation]</p><p class="source-code">species_fitness_func = max</p><p class="source-code">max_stagnation       = 20</p><p class="source-code">species_elitism      = 2</p><p class="source-code">[DefaultReproduction]</p><p class="source-code">Elitism            = 2</p><p class="source-code">survival_threshold = 0.2</p></li>
				<li>Now, in the main code file, use the <strong class="source-inline">config-feedforward-xnor</strong> file to configure the NEAT formulation of the neural network and output each configuration of the network within <strong class="source-inline">Exercise 12.08</strong>:<p class="source-code">#load configuration</p><p class="source-code">configuration = neat.Config(neat.DefaultGenome, \</p><p class="source-code">                            neat.DefaultReproduction, \</p><p class="source-code">                            neat.DefaultSpeciesSet, \</p><p class="source-code">                            neat.DefaultStagnation,\</p><p class="source-code">                            "../Dataset/config-feedforward-xnor")</p><p class="source-code">print("Output of file configuration:", configuration)</p><p>The output will be as follows:</p><p class="source-code">Output of file configuration: &lt;neat.config.Config object at </p><p class="source-code">0x0000017618944AC8&gt;</p></li>
				<li>Get the population based on the configuration of the NEAT algorithm and include the progress to the terminal to monitor the statistical differences:<p class="source-code">#load the population size</p><p class="source-code">pop = neat.Population(configuration)</p><p class="source-code">#add output for progress in terminal</p><p class="source-code">pop.add_reporter(neat.StdOutReporter(True))</p><p class="source-code">statistics = neat.StatisticsReporter()</p><p class="source-code">pop.add_reporter(statistics)</p><p class="source-code">pop.add_reporter(neat.Checkpointer(5))</p></li>
				<li>Run the algorithm for <strong class="source-inline">200</strong> generations and select the best solution for the neural network topology:<p class="source-code">#run for 200 generations using</p><p class="source-code">best = pop.run(fitness_function, 200)</p><p class="source-code">#display the best chromosome</p><p class="source-code">print('\n Best chromosome:\n{!s}'.format(best))</p><p>The output will be similar to the following:</p><p class="source-code">****** Running generation 0 ****** </p><p class="source-code">Population's average fitness: 2.45675 stdev: 0.36807</p><p class="source-code">Best fitness: 2.99412 - size: (1, 2) - species 1 - id 28</p><p class="source-code">Average adjusted fitness: 0.585</p><p class="source-code">Mean genetic distance 0.949, standard deviation 0.386</p><p class="source-code">Population of 200 members in 1 species:</p><p class="source-code">   ID   age  size  fitness  adj fit  stag</p><p class="source-code">  ====  ===  ====  =======  =======  ====</p><p class="source-code">     1    0   200      3.0    0.585     0</p><p class="source-code">Total extinctions: 0</p><p class="source-code">Generation time: 0.030 sec</p><p class="source-code"> ****** Running generation 1 ****** </p><p class="source-code">Population's average fitness: 2.42136 stdev: 0.28774</p><p class="source-code">Best fitness: 2.99412 - size: (1, 2) - species 1 - id 28</p><p class="source-code">Average adjusted fitness: 0.589</p><p class="source-code">Mean genetic distance 1.074, standard deviation 0.462</p><p class="source-code">Population of 200 members in 1 species:</p><p class="source-code">   ID   age  size  fitness  adj fit  stag</p><p class="source-code">  ====  ===  ====  =======  =======  ====</p><p class="source-code">     1    1   200      3.0    0.589     1</p><p class="source-code">Total extinctions: 0</p><p class="source-code">Generation time: 0.032 sec (0.031 average)</p></li>
				<li>Use functions to compare the output of the neural network with the desired output:<p class="source-code">#show output of the most fit chromosome against the data</p><p class="source-code">print('\n Output:')</p><p class="source-code">best_network = neat.nn.FeedForwardNetwork.create\</p><p class="source-code">               (best, configuration)</p><p class="source-code">for xnor_i, xnor_o in zip(xnor_inputs, xnor_output):</p><p class="source-code">    output = best_network.activate(xnor_i)</p><p class="source-code">    print("input{!r}, expected output {!r}, got: {:.1f}"\</p><p class="source-code">          .format(xnor_i,xnor_o,output[0]))</p><p>The output will be as follows:</p><p class="source-code">Output:</p><p class="source-code">input(0.0, 0.0), expected output (1.0,), got: 0.9</p><p class="source-code">input(0.0, 1.0), expected output (0.0,), got: 0.0</p><p class="source-code">input(1.0, 0.0), expected output (0.0,), got: 0.2</p><p class="source-code">input(1.0, 1.0), expected output (1.0,), got: 0.9</p></li>
				<li>Run the code and you will get a similar output to what you see here. As the chromosomes are populated randomly, the algorithm will converge to a nearly optimal solution in a different number of generations for you:<p class="source-code">****** Running generation 41 ******</p><p class="source-code">Population's average fitness: 2.50036 stdev: 0.52561</p><p class="source-code">Best fitness: 3.97351 - size: (8, 16) - species 2 - id 8095</p><p class="source-code">Best individual in generation 41 meets fitness threshold \</p><p class="source-code">- complexity: (8, 16)</p><p class="source-code">Best chromosome:</p><p class="source-code">Key: 8095</p><p class="source-code">Fitness: 3.9735119749933214</p><p class="source-code">Nodes:</p><p class="source-code">    0 DefaultNodeGene(key=0, bias=-0.02623087593563278, \</p><p class="source-code">                      response=1.0, activation=sigmoid, \</p><p class="source-code">                      aggregation=sum)</p><p class="source-code">    107 DefaultNodeGene(key=107, bias=-1.5209385195946818, \</p><p class="source-code">                        response=1.0, activation=sigmoid, \</p><p class="source-code">                        aggregation=sum)[…]</p><p class="source-code">    </p><p class="source-code">Connections:</p><p class="source-code">    DefaultConnectionGene(key=(-2, 107), \</p><p class="source-code">                          weight=1.8280370376000628, \</p><p class="source-code">                          enabled=True)</p><p class="source-code">    DefaultConnectionGene(key=(-2, 128), \</p><p class="source-code">                          weight=0.08641968818530771, \</p><p class="source-code">                          enabled=True)</p><p class="source-code">    DefaultConnectionGene(key=(-2, 321), \</p><p class="source-code">                          weight=1.2366021868005421, \</p><p class="source-code">                          enabled=True)[…]</p></li>
			</ol>
			<p>By running this experiment, you can see that the conversion to a nearly optimal solution happened in less than the maximum number of generations (<strong class="source-inline">200</strong>). The output of the feedforward neural network is nearly optimal, as the values are integers. Their values are close to 1 and 0. You can also observe that from a neural network with no hidden layers, the ANN has evolved to have <strong class="source-inline">1149</strong> nodes with various connections.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to https://packt.live/2XTBs0M.</p>
			<p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p>
			<p>In this section, the NEAT algorithm, a neuroevolution algorithm that varies the topology of neural networks, was presented. What sets the NEAT algorithm apart from alternative TWEANNs is the way in which mutation, crossover, and selection take place to optimize the structure of the neural network, starting from a simple network with no hidden layers and evolving into a more complex one with an increased number of nodes and connections.</p>
			<p>This exercise, which involved implementing NEAT to reproduce the output of an XNOR logic gate, enabled you to understand the structure of the NEAT algorithm and analyze the benefits and implications of applying neuroevolutionary techniques as alternatives to simple electronic problems. In the next section, you will test your programming abilities and your knowledge of GAs by solving the cart-pole problem.</p>
			<h2 id="_idParaDest-333"><a id="_idTextAnchor377"/>Activity 12.01: Cart-Pole Activity</h2>
			<p>Automatic control is a challenge, especially when operating specific equipment using robotic arms or carts that are transporting equipment on a shop floor. This problem is often generalized as the cart-pole problem. You are going to program an automated cart to balance a pole. The goal is to maximize the time that the pole is balanced for. To solve this problem, an agent can use a neural network for the state-action mapping. The challenge lies in identifying the structure of the neural network and a solution for determining the optimal values for the weights, bias, and number of neurons for each layer of the neural network. We will use a GA to identify the best values for these parameters. </p>
			<p>This activity aims to implement a GA for parameter selection for an ANN that, after 20 generations, can obtain a high average score for 500 trials. You will output the average scores for both the generations and the episodes, and you will monitor the convergence to an optimal policy by tuning the parameters of the neural network using a genetic algorithm in the form of a graph. This activity has the purpose of testing your programming abilities by implementing concepts from previous chapters and the current one. The following are the steps needed to implement this activity:</p>
			<ol>
				<li value="1">Create a Jupyter Notebook file and import the appropriate packages as follows: <p class="source-code">import gym </p><p class="source-code">import numpy as np </p><p class="source-code">import math </p><p class="source-code">import tensorflow as tf</p><p class="source-code">from matplotlib import pyplot as plt</p><p class="source-code">from random import randint</p><p class="source-code">from statistics import median, mean</p></li>
				<li>Initialize the environment and the state and action space shapes.</li>
				<li>Create a function to generate randomly selected initial network parameters.</li>
				<li>Create a function to generate the neural network using the set of parameters.</li>
				<li>Create a function to get the total reward for 300 steps when using the neural network.</li>
				<li>Create a function to get the fitness scores for each element of the population when running the initial random selection.</li>
				<li>Create a mutation function.</li>
				<li>Create a single-point crossover function.</li>
				<li>Create a function for the next-generation creation by selecting the pair with the highest rewards.</li>
				<li>Select the parameters within the function to construct the neural network that adds the parameters.</li>
				<li>Build the neural network using the identified parameters and obtain a new reward based on the constructed neural network.</li>
				<li>Create a function to output the convergence graph.</li>
				<li>Create a function for the genetic algorithm that outputs the parameters of the neural network based on the highest average reward.</li>
				<li>Create a function that decodes the array of parameters to each neural network parameter.</li>
				<li>Set the generations to 50, the number of trial tests to 15, and the number of steps and trials to 500. You will get a similar output to this (only the first few lines are displayed here):<p class="source-code">Generation:1, max reward:11.0</p><p class="source-code">Generation:2, max reward:11.0</p><p class="source-code">Generation:3, max reward:10.0</p><p class="source-code">Generation:4, max reward:10.0</p><p class="source-code">Generation:5, max reward:11.0</p><p class="source-code">Generation:6, max reward:10.0</p><p class="source-code">Generation:7, max reward:10.0</p><p class="source-code">Generation:8, max reward:10.0</p><p class="source-code">Generation:9, max reward:11.0</p><p class="source-code">Generation:10, max reward:10.0</p><p class="source-code">Generation:11, max reward:10.0</p><p class="source-code">Generation:12, max reward:10.0</p><p class="source-code">Generation:13, max reward:10.0</p><p class="source-code">Generation:14, max reward:10.0</p><p class="source-code">Generation:15, max reward:10.0</p><p class="source-code">Generation:16, max reward:10.0</p><p class="source-code">Generation:17, max reward:10.0</p><p class="source-code">Generation:18, max reward:10.0</p><p class="source-code">Generation:19, max reward:11.0</p><p class="source-code">Generation:20, max reward:11.0</p><p>The plot of rewards against generations will be similar to the following:</p><div id="_idContainer827" class="IMG---Figure"><img src="image/B16182_12_14.jpg" alt="Figure 12.14: Rewards obtained over the generations&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 12.14: Rewards obtained over the generations</p>
			<p>The output for the average rewards (just the last few lines are shown here) will be similar to the following:</p>
			<p class="source-code">Trial:486, total reward:8.0</p>
			<p class="source-code">Trial:487, total reward:9.0</p>
			<p class="source-code">Trial:488, total reward:10.0</p>
			<p class="source-code">Trial:489, total reward:10.0</p>
			<p class="source-code">Trial:490, total reward:8.0</p>
			<p class="source-code">Trial:491, total reward:9.0</p>
			<p class="source-code">Trial:492, total reward:9.0</p>
			<p class="source-code">Trial:493, total reward:10.0</p>
			<p class="source-code">Trial:494, total reward:10.0</p>
			<p class="source-code">Trial:495, total reward:9.0</p>
			<p class="source-code">Trial:496, total reward:10.0</p>
			<p class="source-code">Trial:497, total reward:9.0</p>
			<p class="source-code">Trial:498, total reward:10.0</p>
			<p class="source-code">Trial:499, total reward:9.0</p>
			<p class="source-code">Average reward: 9.384</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 774.</p>
			<h1 id="_idParaDest-334"><a id="_idTextAnchor378"/>Summary</h1>
			<p>In this chapter, you have explored gradient-based and gradient-free methods of algorithm optimization, with an emphasis on the potential of evolutionary algorithms – in particular, GAs – to solve optimization problems, such as sub-optimal solutions, using a nature-inspired approach. GAs consist of specific elements, such as population generation, parent selection, parent reproduction or crossover, and finally mutation occurrence, which they use to create a binary optimal solution. </p>
			<p>Then, the use of GAs for hyperparameter tuning and selection for neural networks was explored, helping us to find the most suitable window size and unit number. We saw implementations of state-of-the-art algorithms that combined deep neural networks and evolutionary strategies, such as NEAT for XNOR output estimation. Finally, you had a chance to implement what was studied in this chapter through an OpenAI Gym cart-pole simulation, where we examined the application of GAs for parameter tuning with action selection using a deep neural network.</p>
			<p>The development of hybrid methods in RL systems is one of the most recent optimization developments. You have developed and implemented optimization methods for model-free RL systems. In the bonus chapter (which is available on the interactive version of the workshop at <a href="http://courses.packtpub.com">courses.packtpub.com</a>), you will be exploring model-based RL methods and state-of-the-art advances in deep RL for control systems that can be applied in the robotics, manufacturing, and transportation fields.</p>
			<p>You are now capable of applying the concepts that you learned about in this book using various coding techniques and various models that can help further enhance your field of expertise and potentially bring new changes and advancements. Your journey has just begun – you have taken the first steps to deciphering the world of RL, and you now have the tools to enhance your Python programming skills for RL, all of which you can independently apply.</p>
		</div>
	</body></html>