<html><head></head><body>
		<div>
			<div id="_idContainer448" class="Content">
			</div>
		</div>
		<div id="_idContainer449" class="Content">
			<h1 id="_idParaDest-181"><a id="_idTextAnchor221"/>6. Monte Carlo Methods</h1>
		</div>
		<div id="_idContainer464" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, you will learn about the various types of Monte Carlo methods, including the first visit and every visit techniques. In the case, if the model of the environment is not known, you can use Monte Carlo methods to learn the environment by generating experience samples or by simulation. This chapter teaches you importance sampling and how to apply Monte Carlo methods to solve the frozen lake problem. By the end of this chapter, you will be able to identify problems where Monte Carlo methods of reinforcement learning can be applied. You will be able to solve prediction, estimation, and control problems using Monte Carlo reinforcement learning.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor222"/>Introduction</h1>
			<p>In the previous chapter, we learned about dynamic programming. Dynamic programming is a way of doing reinforcement learning where the model of the environment is known beforehand. Agents in reinforcement learning can learn a policy, value function, and/or model. Dynamic programming helps solve a known <strong class="bold">Markov Decision Process</strong> (<strong class="bold">MDP</strong>). The probabilistic distribution for all possible transitions is known in an MDP and is required for dynamic programming.</p>
			<p>But what happens when the model of the environment is not known? In many real-life situations, the model of the environment is not known beforehand. Can the algorithm learn the model of the environment? Can the agents in reinforcement learning still learn to make good decisions?</p>
			<p>Monte Carlo methods are a way of learning when the model of the environment is not known and so they are called model-free learning. We can make a model-free prediction that estimates the value function of an unknown MDP. We can also use model-free control, which optimizes the value functions of an unknown MDP. Monte Carlo methods can also handle non-Markovian domains too.</p>
			<p>The transition probabilities between one state and another are not known in many cases. You need to play around and get a sense of the environment before learning how to play the game well. Monte Carlo methods can learn a model of an environment from experiencing the environment. Monte Carlo methods take actual or stochastically simulated scenarios and get an average of the sample returns. By using the sample sequence of states, actions, and rewards from actual or simulated interactions with the environment, Monte Carlo methods can learn from experience. A well-defined set of rewards is needed for Monte Carlo methods to work. This criterion is met only for episodic tasks, where experience is divided into clearly defined episodes, and episodes eventually terminate irrespective of the action selected. An example application is AlphaGo, which is one of the most complex games; the number of possible moves in any state is over 200. One of the key algorithms used to solve it was a tree search based on Monte Carlo.</p>
			<p>In this chapter, we will first understand Monte Carlo methods of reinforcement learning. We will apply them to the Blackjack environment in OpenAI. We will learn about various methods, such as the first visit method and every visit method. We will also learn about importance sampling and, later in the chapter, revisit the frozen lake problem. In the next section, we will introduce the basic workings of Monte Carlo methods.</p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor223"/>The Workings of Monte Carlo Methods</h1>
			<p>Monte Carlo methods solve reinforcement problems by averaging the sample returns for each state-action pair. Monte Carlo methods work only for episodic tasks. This means the experience is split into various episodes and all episodes finally terminate. Only after the episode is complete are the value functions recalculated. Monte Carlo methods can be incrementally optimized episode by episode but not step by step.</p>
			<p>Let's take the example of a game like Go. This game has millions of states; it is going to be difficult to learn all of those millions of states and their transition probabilities beforehand. The other approach would be to play the game of Go repeatedly and assign a positive reward for winning and a negative reward for losing.</p>
			<p>As we don't have information about the policy of the model, we need to use experience samples to learn. This technique is also a sample-based model. We call this direct sampling of episodes in Monte Carlo.</p>
			<p>Monte Carlo is model-free. As no knowledge of MDP is required, the model is inferred from the samples. You can perform model-free prediction or model-free estimation. We can perform an evaluation, also called a prediction, on a policy. We can also evaluate and improve a policy, which is often called control or optimization. Monte Carlo reinforcement learning can learn only from episodes that terminate.</p>
			<p>For example, if you have a game of chess, played by a set of rules or policies, that would be playing several episodes according to those rules or policies and evaluating the success rate of the policy. If we are playing a game according to a policy and modifying the policy based on the game, then it would be a policy improvement, optimization, or control.</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor224"/>Understanding Monte Carlo with Blackjack</h1>
			<p>Blackjack is a simple card game that is quite popular in casinos. It is a great game, as it is simple to simulate and take samples, and lends itself to Monte Carlo methods. Blackjack is also available as part of the OpenAI framework. Players and the dealer are dealt two cards each. The dealer shows one card face up and lays the other card face down. The players and the dealer have a choice of whether to be dealt additional cards or not:</p>
			<ul>
				<li><strong class="bold">The aim of the game</strong>: To obtain cards whose sum is close to or equal to 21 but not greater than 21.</li>
				<li><strong class="bold">Players</strong>: There are two players, called the player and the dealer.</li>
				<li><strong class="bold">The start of the game</strong>: The player is dealt with two cards. The dealer is also dealt with two cards, and the rest of the cards are pooled into a stack. One of the dealer's cards is shown to the player.</li>
				<li><strong class="bold">Possible actions</strong> – <strong class="bold">stick or hit</strong>: "Stick" is to stop asking for more cards. "Hit" is to ask for more cards. The player will choose "Hit" if the sum of their cards is less than 17. If the sum of their cards is greater than or equal to 17, the player will stick. This threshold of 17 to decide whether to hit or stick can be changed if needed in various versions of Blackjack. In this chapter, we will consistently keep the threshold at 17 to decide whether to hit or stick.</li>
				<li><strong class="bold">Rewards</strong>: +1 for a win, -1 for a loss, and 0 for a draw.</li>
				<li><strong class="bold">Strategy</strong>: The player has to decide whether to stick or hit by looking at the dealer's cards. The ace can be considered to be 1 or 11, based on the value of the other cards.</li>
			</ul>
			<p>We will explain the game of Blackjack in the following table. The table has the following columns:</p>
			<ul>
				<li><strong class="bold">Game</strong>: The game number and the sub-state of the game: i, ii, or iii</li>
				<li><strong class="bold">Player Cards</strong>: The cards the player has; for example, K♣, 8♦ means the player has the King of clubs and the eight of diamonds.</li>
				<li><strong class="bold">Dealer Cards</strong>: The cards the dealer gets. For example, 8♠, Xx means the dealer has the eight of spades and a hidden card.</li>
				<li><strong class="bold">Action</strong>: This is the action the player decides to choose.</li>
				<li><strong class="bold">Result</strong>: The result of the game based on the player's actions and the cards the dealer gets.</li>
				<li><strong class="bold">Sum of Player Cards</strong>: The sum of the player's two cards. Please note that the King (K), Queen (Q), and Jack (J) face cards are scored as 10.</li>
				<li><strong class="bold">Comments</strong>: An explanation of why a particular action was taken or a result was declared.</li>
			</ul>
			<p>In game 1, the player decided to stick as the sum of the cards was 18. "Stick" means the player will no longer receive cards. Now the dealer shows the hidden card. It is a draw as both the dealer's and player's cards sum 18. In game 2, the player's cards sum 15, which is less than 17. The player hits and gets another card, which takes the sum to 17. The player then sticks, which means the player will no longer receive cards. The dealer shows the cards and as the sum of the cards is less than 17, gets another card. With the dealer's new card, the sum is 25, which is greater than 21. The game aims to get close to or equal to 21 without the score becoming greater than 21. The dealer loses and the player wins the second game. The following figure presents a summary of this game:</p>
			<div>
				<div id="_idContainer450" class="IMG---Figure">
					<img src="image/B16182_06_01.jpg" alt="Figure 6.1: Explanation of a Blackjack game&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1: Explanation of a Blackjack game</p>
			<p>Next, we will be implementing the game of Blackjack using the OpenAI framework. This will serve as a foundation for the simulation and application of Monte Carlo methods.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor225"/>Exercise 6.01: Implementing Monte Carlo in Blackjack</h2>
			<p>We will learn how to use the OpenAI framework for Blackjack, and get to know about observation space, action space, and generating an episode. The goal of this exercise is to implement Monte Carlo techniques in the game of Blackjack.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li>Import the necessary libraries:<p class="source-code">import gym</p><p class="source-code">import numpy as np</p><p class="source-code">from collections import defaultdict</p><p class="source-code">from functools import partial</p><p><strong class="source-inline">gym</strong> is the OpenAI framework, <strong class="source-inline">numpy</strong> is the framework for data processing, and <strong class="source-inline">defaultdict</strong> is for dictionary support.</p></li>
				<li>We start the <strong class="source-inline">Blackjack</strong> environment with <strong class="source-inline">gym.make()</strong> and assign it to <strong class="source-inline">env</strong>:<p class="source-code">#set the environment as blackjack</p><p class="source-code">env = gym.make('Blackjack-v0')</p><p>Find the number of observation spaces and action spaces:</p><p class="source-code">#number of observation space value</p><p class="source-code">print(env.observation_space)</p><p class="source-code">#number of action space value</p><p class="source-code">print(env.action_space)    </p><p>You will get the following output:</p><p class="source-code">Tuple(Discrete(32), Discrete(11), Discrete(2))</p><p class="source-code">Discrete(2)</p><p>The number of observation spaces is the number of states. The number of action spaces is the number of actions possible in each state. The output shows as discrete, as the observation and action space in a Blackjack game is not continuous. For example, there are other games in OpenAI, such as balancing a CartPole and pendulum where the observation and action spaces are continuous.</p></li>
				<li>Write a function to play the game. If the sum of the player's cards is more than or equal to 17, stick (don't choose more cards); otherwise, hit (choose more cards), as shown in the following code:<p class="source-code">def play_game(state):</p><p class="source-code">    player_score, dealer_score, usable_ace = state </p><p class="source-code">    #if player_score is greater than 17, stick</p><p class="source-code">    if (player_score &gt;= 17):</p><p class="source-code">        return 0 # don't take any cards, stick</p><p class="source-code">    else:</p><p class="source-code">        return 1 # take additional cards, hit</p><p>Here, we are initializing the episode, choosing the initial state, and assigning it to <strong class="source-inline">player_score</strong>, <strong class="source-inline">dealer_score,</strong> and <strong class="source-inline">usable_ace</strong>.</p></li>
				<li>Add a dictionary, <strong class="source-inline">action_text</strong>, that has a key-value mapping for two action integers to action text. Here's the code to convert the integer value of the action into text format:<p class="source-code">for game_num in range(100):</p><p class="source-code">    print('***Start of Game:', game_num)</p><p class="source-code">    state = env.reset()</p><p class="source-code">    action_text = {1:'Hit, Take more cards!!', \</p><p class="source-code">                   0:'Stick, Dont take any cards' }</p><p class="source-code">    player_score, dealer_score, usable_ace = state</p><p class="source-code">    print('Player Score=', player_score,', \</p><p class="source-code">          Dealer Score=', dealer_score, ', \</p><p class="source-code">          Usable Ace=', usable_ace)</p></li>
				<li>Play the game in batches of 100 and calculate <strong class="source-inline">state</strong>, <strong class="source-inline">reward</strong>, and <strong class="source-inline">action</strong>:<p class="source-code">    for i in range(100):</p><p class="source-code">        action = play_game(state)</p><p class="source-code">        state, reward, done, info = env.step(action)</p><p class="source-code">        player_score, dealer_score, usable_ace = state</p><p class="source-code">        print('Action is', action_text[action])</p><p class="source-code">        print('Player Score=', player_score,', \</p><p class="source-code">              Dealer Score=', dealer_score, ', \</p><p class="source-code">              Usable Ace=', usable_ace, ', Reward=', reward)</p><p class="source-code">        if done:</p><p class="source-code">            if (reward == 1):</p><p class="source-code">                print('***End of Game:', game_num, \</p><p class="source-code">                      ' You have won Black Jack!\n')</p><p class="source-code">            elif (reward == -1):</p><p class="source-code">                print('***End of Game:', game_num, \</p><p class="source-code">                      ' You have lost Black Jack!\n')</p><p class="source-code">            elif (reward ==0):</p><p class="source-code">                print('***End of Game:', game_num, \</p><p class="source-code">                      ' The game is a Draw!\n') </p><p class="source-code">            break</p><p>You will get the following output:</p><div id="_idContainer451" class="IMG---Figure"><img src="image/B16182_06_02.jpg" alt="Figure 6.2: The output is the episode of the Blackjack game in progress&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.2: The output is the episode of the Blackjack game in progress</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Monte Carlo technique is based on generating random samples. As such, two executions of the same code will not match in values. So, you might have a similar output but not the same for all the exercises and activities.</p>
			<p>In the code, <strong class="source-inline">done</strong> has the value of <strong class="source-inline">True</strong> or <strong class="source-inline">False</strong>. If <strong class="source-inline">done</strong> is <strong class="source-inline">True</strong>, the game stops, we note the value of the rewards and print the game result. In the output, we simulated the game of Blackjack using the Monte Carlo method and noted the various actions, states, and game completion. We were also able to simulate the rewards when the game ends.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2XZssYh">https://packt.live/2XZssYh</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2Ys0cMJ">https://packt.live/2Ys0cMJ</a>.</p>
			<p>Next, we will describe the different types of Monte Carlo methods, namely, the first visit and every visit method, which will be used to estimate the value function.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor226"/>Types of Monte Carlo Methods</h1>
			<p>We have implemented the game of Blackjack using Monte Carlo. Typically, a trajectory of Monte Carlo is a sequence of state, action, and reward. In several episodes, it is possible that the state repeats. For example, the trajectory could be S0, S1, S2, S0, S3. How do we handle the calculation of the reward function when we have multiple visits to the states?</p>
			<p>Broadly, this highlights that there are two types of Monte Carlo methods – first visit and every visit. We will understand the implications of both methods.</p>
			<p>As stated previously, in Monte Carlo methods, we approximate the value function by averaging the rewards. In the first visit Monte Carlo method, only the first visit to a state in an episode is included to calculate the average reward. For example, in a given game of traversing a maze, you could make several visits to the sample place. In the first visit Monte Carlo method, only the first visit is used for the calculation of the reward. When the agent revisits the same state in the episode, the reward is not included for the calculation of the average reward.</p>
			<p>In every visit Monte Carlo, every time the agent visits the same state, the rewards are included in the calculation of the average return. For example, let's use the same game of maze. Every time the agent comes to the same point in the maze, we include the rewards earned in that state for the calculation of the reward function.</p>
			<p>Both first visit and every visit converge to the same value function. For a smaller number of episodes, the choice between the first visit and every visit is based on the particular game and the rules of the game.</p>
			<p>Let's understand the pseudocode for first visit Monte Carlo prediction.</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor227"/>First Visit Monte Carlo Prediction for Estimating the Value Function</h2>
			<p>In the pseudocode for first visit Monte Carlo prediction for estimating the value function, the key is to calculate the value function <em class="italic">V(s)</em>. Gamma is the discount factor. The discount factor is used to reward future rewards less than immediate rewards:</p>
			<div>
				<div id="_idContainer452" class="IMG---Figure">
					<img src="image/B16182_06_03.jpg" alt="Figure 6.3: Pseudocode for first visit Monte Carlo prediction&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3: Pseudocode for first visit Monte Carlo prediction</p>
			<p>What we have done in the first visit is to generate an episode, calculate the result value, and append the result to the rewards. We then calculate the average returns. In the upcoming exercise, we will apply the first visit Monte Carlo prediction to estimate the value function by following the steps detailed in the pseudocode. The key block of code for the first visit algorithm is navigating the states only through the first visit:</p>
			<p class="source-code">if current_state not in states[:i]:</p>
			<p>Consider the <strong class="source-inline">states</strong> that have not been visited. We increase the count for the number of <strong class="source-inline">states</strong> by <strong class="source-inline">1</strong>, calculate the value function with the incremental method, and return the value function. This is implemented as follows:</p>
			<p class="source-code">"""</p>
			<p class="source-code">only include the rewards of the states that have not been visited before</p>
			<p class="source-code">"""</p>
			<p class="source-code">            if current_state not in states[:i]:</p>
			<p class="source-code">                #increasing the count of states by 1</p>
			<p class="source-code">                num_states[current_state] += 1</p>
			<p class="source-code">                </p>
			<p class="source-code">                #finding the value_function by incremental method</p>
			<p class="source-code">                value_function[current_state] \</p>
			<p class="source-code">                += (total_rewards - value_function[current_state]) \</p>
			<p class="source-code">                / (num_states[current_state])</p>
			<p class="source-code">      return value_function</p>
			<p>Let's understand it better through the next exercise.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor228"/>Exercise 6.02: First Visit Monte Carlo Prediction for Estimating the Value Function in Blackjack</h2>
			<p>This exercise aims to understand how to apply first visit Monte Carlo prediction to estimate the value function in the game of Blackjack. We will apply the steps outlined in the pseudocode step by step.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the necessary libraries:<p class="source-code">import gym</p><p class="source-code">import numpy as np</p><p class="source-code">from collections import defaultdict</p><p class="source-code">from functools import partial</p><p><strong class="source-inline">gym</strong> is the OpenAI framework, <strong class="source-inline">numpy</strong> is the framework for data processing, and <strong class="source-inline">defaultdict</strong> is for dictionary support.</p></li>
				<li>Select the environment as <strong class="source-inline">Blackjack</strong> in OpenAI:<p class="source-code">env = gym.make('Blackjack-v0')</p></li>
				<li>Write the <strong class="source-inline">policy_blackjack_game</strong> function, which takes the state as input and returns the action <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong> based on <strong class="source-inline">player_score</strong>:<p class="source-code">def policy_blackjack_game(state):</p><p class="source-code">    player_score, dealer_score, usable_ace = state</p><p class="source-code">    if (player_score &gt;= 17):</p><p class="source-code">        return 0 # don't take any cards, stick</p><p class="source-code">    else:</p><p class="source-code">        return 1 # take additional cards, hit</p><p>In the function, if the player score is greater than or equal to <strong class="source-inline">17</strong>, it does not take more cards. But if <strong class="source-inline">player_score</strong> is less than 17, it takes additional cards.</p></li>
				<li>Write a function to generate a Blackjack episode. Initialize <strong class="source-inline">episode</strong>, <strong class="source-inline">states</strong>, <strong class="source-inline">actions</strong>, and <strong class="source-inline">rewards</strong>:<p class="source-code">def generate_blackjack_episode():</p><p class="source-code">    #initializing the value of episode, states, actions, rewards</p><p class="source-code">    episode = []</p><p class="source-code">    states = []</p><p class="source-code">    actions = []</p><p class="source-code">    rewards = []</p></li>
				<li>Reset the environment and set the <strong class="source-inline">state</strong> value to <strong class="source-inline">player_score</strong>, <strong class="source-inline">dealer_score</strong>, and <strong class="source-inline">usable_ace</strong>:<p class="source-code">   #starting the environment</p><p class="source-code">    state = env.reset()</p><p class="source-code">    </p><p class="source-code">    """</p><p class="source-code">    setting the state value to player_score, </p><p class="source-code">    dealer_score and usable_ace</p><p class="source-code">    """</p><p class="source-code">    player_score, dealer_score, usable_ace = state</p></li>
				<li>Write a function that generates the action from the state. We then step through the action and find <strong class="source-inline">next_state</strong> and <strong class="source-inline">reward</strong>:<p class="source-code">    while (True):</p><p class="source-code">        #finding the action by passing on the state</p><p class="source-code">        action = policy_blackjack_game(state)</p><p class="source-code">        next_state, reward, done, info = env.step(action)</p></li>
				<li>Create a list of <strong class="source-inline">episode</strong>, <strong class="source-inline">state</strong>, <strong class="source-inline">action,</strong> and <strong class="source-inline">reward</strong> by appending them to the existing list:<p class="source-code">        #creating a list of episodes, states, actions, rewards</p><p class="source-code">        episode.append((state, action, reward))</p><p class="source-code">        states.append(state)</p><p class="source-code">        actions.append(action)</p><p class="source-code">        rewards.append(reward)</p><p>If the episode is complete (<strong class="source-inline">done</strong> is true), we <strong class="source-inline">break</strong> the loop. If not, we update <strong class="source-inline">state</strong> to <strong class="source-inline">next_state</strong> and repeat the loop:</p><p class="source-code">        if done:</p><p class="source-code">            break</p><p class="source-code">        state = next_state</p></li>
				<li>We return <strong class="source-inline">episodes</strong>, <strong class="source-inline">states</strong>, <strong class="source-inline">actions</strong>, and <strong class="source-inline">rewards</strong> from the function:<p class="source-code">    return episode, states, actions, rewards</p></li>
				<li>Write the function for calculating the value function for Blackjack. The first step is to initialize the value of <strong class="source-inline">total_rewards</strong>, <strong class="source-inline">num_states</strong>, and <strong class="source-inline">value_function</strong>:<p class="source-code">def black_jack_first_visit_prediction(policy, env, num_episodes):</p><p class="source-code">    """</p><p class="source-code">    initializing the value of total_rewards, </p><p class="source-code">    number of states, and value_function</p><p class="source-code">    """</p><p class="source-code">    total_rewards = 0</p><p class="source-code">    num_states = defaultdict(float)</p><p class="source-code">    value_function = defaultdict(float)</p></li>
				<li>Generate an <strong class="source-inline">episode</strong>, and for an <strong class="source-inline">episode</strong>, we find the total <strong class="source-inline">rewards</strong> for all the <strong class="source-inline">states</strong> in reverse order in the <strong class="source-inline">episode</strong>:<p class="source-code">    for k in range (0, num_episodes):</p><p class="source-code">        episode, states, actions, rewards = \</p><p class="source-code">        generate_blackjack_episode()</p><p class="source-code">        total_rewards = 0</p><p class="source-code">        for i in range(len(states)-1, -1,-1):</p><p class="source-code">            current_state = states[i]</p><p class="source-code">            #finding the sum of rewards</p><p class="source-code">            total_rewards += rewards[i]</p></li>
				<li>Consider the <strong class="source-inline">states</strong> that have not been visited. We increase the count for the number of <strong class="source-inline">states</strong> by <strong class="source-inline">1</strong> and calculate the value function using the incremental method, and return the value function:<p class="source-code">            """</p><p class="source-code">            only include the rewards of the states that </p><p class="source-code">            have not been visited before</p><p class="source-code">            """</p><p class="source-code">            if current_state not in states[:i]:</p><p class="source-code">                #increasing the count of states by 1</p><p class="source-code">                num_states[current_state] += 1</p><p class="source-code">                </p><p class="source-code">                #finding the value_function by incremental method</p><p class="source-code">                value_function[current_state] \</p><p class="source-code">                += (total_rewards \</p><p class="source-code">                - value_function[current_state]) \</p><p class="source-code">                / (num_states[current_state])</p><p class="source-code">    return value_function</p></li>
				<li>Now, execute first visit prediction 10,000 times:<p class="source-code">black_jack_first_visit_prediction(policy_blackjack_game, env, 10000)</p><p>You will get the following output:</p><div id="_idContainer453" class="IMG---Figure"><img src="image/B16182_06_04.jpg" alt="Figure 6.4: First visit value function&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.4: First visit value function</p>
			<p>The value function for the first visit is printed. For all the states, a combination of <strong class="source-inline">player_score</strong>, <strong class="source-inline">dealer_score,</strong> and <strong class="source-inline">usable_space</strong> has a value function value from the first visit evaluation. Take the example output of <strong class="source-inline">(16, 3, False): -0.625</strong>. This means that the value function for the state with player score <strong class="source-inline">16</strong>, dealer score <strong class="source-inline">3</strong>, and a reusable ace as <strong class="source-inline">False</strong> is <strong class="source-inline">-0.625</strong>. The number of episodes and batches are configurable.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/37zbza1">https://packt.live/37zbza1</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2AYnhyH">https://packt.live/2AYnhyH</a>.</p>
			<p>We have covered the first visit Monte Carlo in this section. In the next section, we will understand every visit Monte Carlo prediction for estimating the value function.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor229"/>Every Visit Monte Carlo Prediction for Estimating the Value Function</h2>
			<p>In every visit Monte Carlo prediction, every visit to the state is used for the reward calculation. We have a gamma factor that is the discount factor, which enables us to discount the rewards in the far future relative to rewards in the immediate future:</p>
			<div>
				<div id="_idContainer454" class="IMG---Figure">
					<img src="image/B16182_06_05.jpg" alt="Figure 6.5: Pseudocode for every visit Monte Carlo prediction&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5: Pseudocode for every visit Monte Carlo prediction</p>
			<p>The difference is primarily visiting every step instead of just the first to calculate the rewards. The code remains similar to the first visit exercise, except for the Blackjack prediction function where the rewards are calculated.</p>
			<p>The following line in the first visit implementation checks if the current state has not been traversed before. This line is no longer in every visit algorithm:</p>
			<p class="source-code">if current_state not in states[:i]:</p>
			<p>The code for the calculation of the value function is as follows:</p>
			<p class="source-code">            #all the state values of every visit are considered</p>
			<p class="source-code">            #increasing the count of states by 1</p>
			<p class="source-code">            num_states[current_state] += 1</p>
			<p class="source-code">            </p>
			<p class="source-code">            #finding the value_function by incremental method</p>
			<p class="source-code">            value_function[current_state] \</p>
			<p class="source-code">            += (total_rewards - value_function[current_state]) \</p>
			<p class="source-code">            / (num_states[current_state])</p>
			<p class="source-code">    return value_function</p>
			<p>In this exercise, we will use every visit Monte Carlo method to estimate the value function.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor230"/>Exercise 6.03: Every Visit Monte Carlo Prediction for Estimating the Value Function </h2>
			<p>This exercise aims to understand how to apply every visit Monte Carlo prediction to estimate the value function. We will apply the steps outlined in the pseudocode step by step. Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the necessary libraries:<p class="source-code">import gym</p><p class="source-code">import numpy as np</p><p class="source-code">from collections import defaultdict </p><p class="source-code">from functools import partial</p></li>
				<li>Select the environment as <strong class="source-inline">Blackjack</strong> in OpenAI:<p class="source-code">env = gym.make('Blackjack-v0')</p></li>
				<li>Write the <strong class="source-inline">policy_blackjack_game</strong> function that takes the state as input and returns the action <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong> based on <strong class="source-inline">player_score</strong>:<p class="source-code">def policy_blackjack_game(state):</p><p class="source-code">    player_score, dealer_score, usable_ace = state </p><p class="source-code">    if (player_score &gt;= 17):</p><p class="source-code">        return 0 # don't take any cards, stick</p><p class="source-code">    else:</p><p class="source-code">        return 1 # take additional cards, hit</p><p>In the function, if the player score is greater than or equal to <strong class="source-inline">17</strong>, it does not take more cards. But if <strong class="source-inline">player_score</strong> is less than <strong class="source-inline">17</strong>, it takes additional cards.</p></li>
				<li>Write a function to generate a Blackjack episode. Initialize <strong class="source-inline">episode</strong>, <strong class="source-inline">states</strong>, <strong class="source-inline">actions</strong>, and <strong class="source-inline">rewards</strong>:<p class="source-code">def generate_blackjack_episode():</p><p class="source-code">    #initializing the value of episode, states, actions, rewards</p><p class="source-code">    episode = []</p><p class="source-code">    states = []</p><p class="source-code">    actions = []</p><p class="source-code">    rewards = []</p></li>
				<li>We reset the environment and set the value of <strong class="source-inline">state</strong> to <strong class="source-inline">player_score</strong>, <strong class="source-inline">dealer_score</strong>, and <strong class="source-inline">usable_ace</strong>, as shown in the following code:<p class="source-code">    #starting the environment</p><p class="source-code">    state = env.reset()</p><p class="source-code">    """</p><p class="source-code">    setting the state value to player_score, dealer_score and </p><p class="source-code">    usable_ace</p><p class="source-code">    """</p><p class="source-code">    player_score, dealer_score, usable_ace = state</p></li>
				<li>Write a function that generates <strong class="source-inline">action</strong> from <strong class="source-inline">state</strong>. We then step through <strong class="source-inline">action</strong> and find <strong class="source-inline">next_state</strong> and <strong class="source-inline">reward</strong>:<p class="source-code">    while (True):</p><p class="source-code">        #finding the action by passing on the state</p><p class="source-code">        action = policy_blackjack_game(state)       </p><p class="source-code">        next_state, reward, done, info = env.step(action)</p></li>
				<li>Create a list of <strong class="source-inline">episode</strong>, <strong class="source-inline">state</strong>, <strong class="source-inline">action</strong>, and <strong class="source-inline">reward</strong> by appending them to the existing list:<p class="source-code">        #creating a list of episodes, states, actions, rewards</p><p class="source-code">        episode.append((state, action, reward))</p><p class="source-code">        states.append(state)</p><p class="source-code">        actions.append(action)</p><p class="source-code">        rewards.append(reward)</p></li>
				<li>If the episode is complete (<strong class="source-inline">done</strong> is true), we <strong class="source-inline">break</strong> the loop. If not, we update <strong class="source-inline">state</strong> to <strong class="source-inline">next_state</strong> and repeat the loop:<p class="source-code">        if done:</p><p class="source-code">            break</p><p class="source-code">        state = next_state</p></li>
				<li>We return <strong class="source-inline">episodes</strong>, <strong class="source-inline">states</strong>, <strong class="source-inline">actions</strong>, and <strong class="source-inline">rewards</strong> from the function:<p class="source-code">    return episode, states, actions, rewards</p></li>
				<li>Write the function for calculating the value function for Blackjack. The first step is to initialize the values of <strong class="source-inline">total_rewards</strong>, <strong class="source-inline">num_states</strong>, and <strong class="source-inline">value_function</strong>:<p class="source-code">def black_jack_every_visit_prediction\</p><p class="source-code">(policy, env, num_episodes):</p><p class="source-code">    """</p><p class="source-code">    initializing the value of total_rewards, number of states, </p><p class="source-code">    and value_function</p><p class="source-code">    """</p><p class="source-code">    total_rewards = 0</p><p class="source-code">    num_states = defaultdict(float)</p><p class="source-code">    value_function = defaultdict(float)</p></li>
				<li>Generate an <strong class="source-inline">episode</strong> and for the <strong class="source-inline">episode</strong>, we find the total <strong class="source-inline">rewards</strong> for all the <strong class="source-inline">states</strong> in reverse order in the <strong class="source-inline">episode</strong>:<p class="source-code">    for k in range (0, num_episodes):</p><p class="source-code">        episode, states, actions, rewards = \</p><p class="source-code">        generate_blackjack_episode() </p><p class="source-code">        total_rewards = 0</p><p class="source-code">        for i in range(len(states)-1, -1,-1):</p><p class="source-code">            current_state = states[i]</p><p class="source-code">            #finding the sum of rewards</p><p class="source-code">            total_rewards += rewards[i]</p></li>
				<li>Consider every <strong class="source-inline">state</strong> visited. We increase the count for the number of <strong class="source-inline">states</strong> by <strong class="source-inline">1</strong> and calculate the value function with the incremental method and return the value function:<p class="source-code">            #all the state values of every visit are considered</p><p class="source-code">            #increasing the count of states by 1</p><p class="source-code">            num_states[current_state] += 1</p><p class="source-code">            #finding the value_function by incremental method</p><p class="source-code">            value_function[current_state] \</p><p class="source-code">            += (total_rewards - value_function[current_state]) \</p><p class="source-code">            / (num_states[current_state])</p><p class="source-code">    return value_function</p></li>
				<li>Now, execute every visit prediction 10,000 times:<p class="source-code">black_jack_every_visit_prediction(policy_blackjack_game, \</p><p class="source-code">                                  env, 10000)</p><p>You will get the following output:</p><div id="_idContainer455" class="IMG---Figure"><img src="image/B16182_06_06.jpg" alt="Figure 6.6: Every visit value function &#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.6: Every visit value function </p>
			<p>The value function for every visit is printed. For all the states, a combination of <strong class="source-inline">player_score</strong>, <strong class="source-inline">dealer_score,</strong> and <strong class="source-inline">usable_space</strong> has a value function value from every visit evaluation. We can increase the number of episodes and run this again too. As the number of episodes is made larger and larger, the first visit and every visit functions will converge.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2C0wAP4">https://packt.live/2C0wAP4</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2zqXsH3">https://packt.live/2zqXsH3</a>.</p>
			<p>In the next section, we will talk about a key concept of Monte Carlo reinforcement learning, which is the need to balance exploration and exploitation. This is also the basis of the greedy epsilon technique of the Monte Carlo method. Balancing exploration and exploitation helps us to improve the policy function.</p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor231"/>Exploration versus Exploitation Trade-Off</h1>
			<p>Learning happens by exploring new things and exploiting or applying what has been learned before. The right combination of these is the essence of any learning. Similarly, in the context of reinforcement learning, we have exploration and exploitation. <strong class="bold">Exploration</strong> is trying out different actions, while <strong class="bold">exploitation</strong> is following an action that is known to have a good reward.</p>
			<p>Reinforcement learning has to balance between exploration and exploitation. Every agent can learn only from the experience of trying an action. Exploration helps try new actions that might enable the agent to make better decisions in the future. Exploitation is choosing actions that yield good rewards based on experience. The agent needs to trade off gaining rewards by exploitation by experimenting in exploration. If an agent exploits more, the agent might miss learning about other policies with even greater rewards. If the agent explores more, the agent might miss the opportunity to exploit a known path and lose out on rewards.</p>
			<p>For example, think of a student who is trying to maximize their grades in college. The student can either "explore" by taking courses with new subjects or "exploit" by taking courses in their favorite subjects. If the student indexes towards "exploitation," the student might miss out on both getting good grades in new unexplored courses and the overall learning. If the student explores too many diverse subjects by taking courses in them, this might impact their grades and might make the learning too broad.</p>
			<p>Similarly, if you choose to read books, you could exploit by reading books belonging to the same genre or author or explore by reading books across different genres and authors. Similarly, while driving from one place to another, you could exploit by following the same known route based on past experience or explore by taking different routes. In the next section, we will get an understanding of the techniques of on-policy and off-policy learning. We will then get an understanding of a key factor called importance sampling, for off-policy learning.</p>
			<p>Exploration and exploitation are techniques used in reinforcement learning. In off-policy learning, you can have an exploitation technique as the target policy and an exploration technique as the behavior policy. We could have a greedy policy as the exploitation technique and a random policy as the exploration technique.</p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor232"/>Importance Sampling</h1>
			<p>Monte Carlo methods can be on-policy or off-policy. In <strong class="bold">on-policy</strong> learning, we learn from the agent experience of the following policy. In <strong class="bold">off-policy</strong> learning, we learn how to estimate a target policy from the experience of following a different behavioral policy. Importance sampling is a key technique for off-policy learning. The following figure compares on-policy and off-policy learning:</p>
			<div>
				<div id="_idContainer456" class="IMG---Figure">
					<img src="image/B16182_06_07.jpg" alt="Figure 6.7: On-Policy versus Off-Policy comparison&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7: On-Policy versus Off-Policy comparison</p>
			<p>You might think that on-policy learning is learning while playing, while off-policy learning is learning by watching someone else play. You could improve your cricket game by playing cricket yourself. This will help you learn from your mistakes and best actions. That would be on-policy learning. You could also learn by watching others play the game of cricket and learning from their mistakes and best actions. That would be off-policy learning.</p>
			<p>Human beings typically do both on-policy and off-policy learning. For example, cycling is primarily on-policy learning. We learn to cycle by learning to balance ourselves while cycling. Dancing is a kind of off-policy learning; you watch someone else dance and learn the dance steps.</p>
			<p>On-policy methods are simple compared to off-policy methods. Off-policy methods are more powerful due to the "transfer learning" effect. In off-policy methods, you are learning from a different policy, the convergence is slower, and the variance is higher.</p>
			<p>The advantage of off-policy learning is that the behavior policy can be very exploratory in nature, while the target policy can be deterministic and greedily optimize the rewards.</p>
			<p>Off-policy reinforcement methods are based on a concept called importance sampling. This methodology helps estimate values under one policy probability distribution given samples from another policy probability distribution. Let's understand Monte Carlo off-policy evaluation by detailing the pseudocode. We'll then apply it to the Blackjack game in the OpenAI framework.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor233"/>The Pseudocode for Monte Carlo Off-Policy Evaluation</h2>
			<p>What we see in the following figure is that we are estimating <strong class="source-inline">Q(s,a)</strong> by learning from behavior policy <strong class="source-inline">b</strong>.</p>
			<div>
				<div id="_idContainer457" class="IMG---Figure">
					<img src="image/B16182_06_08.jpg" alt="Figure 6.8: Pseudocode for Monte Carlo off-policy evaluation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8: Pseudocode for Monte Carlo off-policy evaluation</p>
			<p>The target policy is a greedy policy; hence we choose the action with the maximum rewards by using <strong class="source-inline">argmax Q(s,a)</strong>. Gamma is the discount factor that allows us to discount rewards in the distant future compared to immediate rewards in the future. The cumulative value function <strong class="source-inline">C(s,a)</strong> is calculated by incrementing it with weight <strong class="source-inline">W</strong>. Gamma is used to discount the rewards.</p>
			<p>The essence of off-policy Monte Carlo is the loop through every episode:</p>
			<p class="source-code">for step in range(len(episode))[::-1]:</p>
			<p class="source-code">            state, action, reward = episode[step]</p>
			<p class="source-code">            #G &lt;- gamma * G + Rt+1</p>
			<p class="source-code">            G = discount_factor * G + reward    </p>
			<p class="source-code">            # C(St, At) = C(St, At) + W</p>
			<p class="source-code">            C[state][action] += W</p>
			<p class="source-code">            #Q (St, At) &lt;- Q (St, At) + W / C (St, At)</p>
			<p class="source-code">            Q[state][action] += (W / C[state][action]) \</p>
			<p class="source-code">            * (G - Q[state][action])</p>
			<p class="source-code">            """</p>
			<p class="source-code">            If action not equal to argmax of target policy </p>
			<p class="source-code">            proceed to next episode</p>
			<p class="source-code">            """</p>
			<p class="source-code">            if action != np.argmax(target_policy(state)):</p>
			<p class="source-code">                break</p>
			<p class="source-code">            # W &lt;- W * Pi(At/St) / b(At/St)</p>
			<p class="source-code">            W = W * 1./behaviour_policy(state)[action]</p>
			<p>Let's understand the implementation of the off-policy method of Monte Carlo by using importance sampling. This exercise will help us learn how to set the target policy and the behavior policy, and learn the target policy from the behavior policy.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor234"/>Exercise 6.04: Importance Sampling with Monte Carlo</h2>
			<p>This exercise will aim to do off-policy learning by using a Monte Carlo method. We have chosen a greedy target policy. We also have a behavior policy, which is any soft, non-greedy policy. By learning from the behavior policy, we will estimate the value function of the target policy. We will apply this technique of importance sampling to the Blackjack game environment. We will apply the steps outlined in the pseudocode step by step.</p>
			<p>Perform the following steps to complete the exercise:</p>
			<ol>
				<li value="1">Import the necessary libraries:<p class="source-code">import gym</p><p class="source-code">import numpy as np</p><p class="source-code">from collections import defaultdict</p><p class="source-code">from functools import partial</p></li>
				<li>Select the environment as <strong class="source-inline">Blackjack</strong> in OpenAI by using <strong class="source-inline">gym.make</strong>:<p class="source-code">env = gym.make('Blackjack-v0')</p></li>
				<li>Create two policy functions. One of them is a random policy. The random policy chooses a random action, which is a list of size n with 1/n probability where n is the number of actions:<p class="source-code">"""</p><p class="source-code">creates a random policy which is a linear probability distribution</p><p class="source-code">num_Action is the number of Actions supported by the environment</p><p class="source-code">"""</p><p class="source-code">def create_random_policy(num_Actions): </p><p class="source-code">#Creates a list of size num_Actions, with a fraction 1/num_Actions.</p><p class="source-code">#If 2 is numActions, the array value would [1/2, 1/2]</p><p class="source-code">    Action = np.ones(num_Actions, dtype=float)/num_Actions</p><p class="source-code">    def policy_function(observation):</p><p class="source-code">        return Action</p><p class="source-code">    return policy_function</p></li>
				<li>Write a function to create a greedy policy:<p class="source-code">#creates a greedy policy,</p><p class="source-code">"""</p><p class="source-code">sets the value of the Action at the best_possible_action, </p><p class="source-code">that maximizes the Q, value to be 1, rest to be 0</p><p class="source-code">"""</p><p class="source-code">def create_greedy_policy(Q):</p><p class="source-code">    def policy_function(state):</p><p class="source-code">        #Initializing with zero the Q</p><p class="source-code">        Action = np.zeros_like(Q[state], dtype = float)</p><p class="source-code">        #find the index of the max Q value </p><p class="source-code">        best_possible_action = np.argmax(Q[state])</p><p class="source-code">        #Assigning 1 to the best possible action</p><p class="source-code">        Action[best_possible_action] = 1.0</p><p class="source-code">        return Action</p><p class="source-code">    return policy_function</p><p>The greedy policy chooses an action that maximizes the rewards. We first identify the <strong class="source-inline">best_possible_action</strong>, that is, the maximum value of <strong class="source-inline">Q</strong> across states. We then assign a value to the <strong class="source-inline">Action</strong> corresponding to the <strong class="source-inline">best_possible_action</strong>.</p></li>
				<li>Define a function for Blackjack importance sampling that takes <strong class="source-inline">env</strong>, <strong class="source-inline">num_episodes</strong>, <strong class="source-inline">behaviour_policy</strong>, and <strong class="source-inline">discount_factor</strong> as arguments:<p class="source-code">def black_jack_importance_sampling\</p><p class="source-code">(env, num_episodes, behaviour_policy, discount_factor=1.0):</p><p class="source-code">        #Initialize the value of Q</p><p class="source-code">        Q = defaultdict(lambda: np.zeros(env.action_space.n))</p><p class="source-code">        #Initialize the value of C</p><p class="source-code">        C = defaultdict(lambda: np.zeros(env.action_space.n))</p><p class="source-code">        #target policy is the greedy policy</p><p class="source-code">        target_policy = create_greedy_policy(Q)</p><p>We initialize the value of <strong class="source-inline">Q</strong> and <strong class="source-inline">C</strong>, and set the target policy as a greedy policy.</p></li>
				<li>We loop for the number of episodes, initialize the episodes list, and state the initial set by doing <strong class="source-inline">env.reset()</strong>:<p class="source-code">        for i_episode in range(1, num_episodes + 1):</p><p class="source-code">            episode = []</p><p class="source-code">            state = env.reset()</p></li>
				<li>For a batch of 100, apply the behavior policy on a state to calculate the probability:<p class="source-code">            for i in range(100):</p><p class="source-code">                probability = behaviour_policy(state)</p><p class="source-code">                action = np.random.choice\</p><p class="source-code">                         (np.arange(len(probability)), p=probability)</p><p class="source-code">                next_state, reward, done, info = env.step(action)</p><p class="source-code">                episode.append((state, action, reward))</p><p>We pick a random action from that list. The step is taken with the random action, returning <strong class="source-inline">next_state</strong> and <strong class="source-inline">reward</strong>. The episode list is appended with the <strong class="source-inline">state</strong>, <strong class="source-inline">action</strong>, and <strong class="source-inline">reward</strong>.</p></li>
				<li>If the <strong class="source-inline">episode</strong> is completed, we break the loop and assign <strong class="source-inline">next_state</strong> to <strong class="source-inline">state</strong>:<p class="source-code">                if done:</p><p class="source-code">                    break</p><p class="source-code">                state = next_state </p></li>
				<li>Initialize <strong class="source-inline">G</strong>, the results as <strong class="source-inline">0</strong> and <strong class="source-inline">W</strong>, and the weight as <strong class="source-inline">1</strong>:<p class="source-code">               # G &lt;- 0</p><p class="source-code">                     G = 0.0</p><p class="source-code">                     # W &lt;- 0</p><p class="source-code">                     W = 1.0  </p></li>
				<li>Perform the steps detailed in the pseudocode using a <strong class="source-inline">for</strong> loop, as shown in the following code:<p class="source-code">            """</p><p class="source-code">            Loop for each step of episode t=T-1, T-2,...,0 </p><p class="source-code">            while W != 0</p><p class="source-code">            """</p><p class="source-code">            for step in range(len(episode))[::-1]:</p><p class="source-code">                state, action, reward = episode[step]</p><p class="source-code">                #G &lt;- gamma * G + Rt+1</p><p class="source-code">                G = discount_factor * G + reward</p><p class="source-code">                # C(St, At) = C(St, At) + W</p><p class="source-code">                C[state][action] += W</p><p class="source-code">                #Q (St, At) &lt;- Q (St, At) + W / C (St, At)</p><p class="source-code">                Q[state][action] += (W / C[state][action]) \</p><p class="source-code">                * (G - Q[state][action])</p><p class="source-code">                """</p><p class="source-code">                If action not equal to argmax of target policy </p><p class="source-code">                proceed to next episode</p><p class="source-code">                """</p><p class="source-code">                if action != np.argmax(target_policy(state)):</p><p class="source-code">                    break</p><p class="source-code">                # W &lt;- W * Pi(At/St) / b(At/St)</p><p class="source-code">                W = W * 1./behaviour_policy(state)[action]</p></li>
				<li>Return <strong class="source-inline">Q</strong> and <strong class="source-inline">target_policy</strong>:<p class="source-code">        return Q, target_policy </p></li>
				<li>Create a random policy:<p class="source-code">#create random policy</p><p class="source-code">random_policy = create_random_policy(env.action_space.n)</p><p class="source-code">"""</p><p class="source-code">using importance sampling evaluates the target policy </p><p class="source-code">by learning from the behaviour policy</p><p class="source-code">"""</p><p class="source-code">Q, policy = black_jack_importance_sampling\</p><p class="source-code">            (env, 50000, random_policy)</p><p>The random policy is used as a behavior policy. We pass the behavior policy and using the importance sampling method, get the <strong class="source-inline">Q</strong> value function or the target policy.</p></li>
				<li>Iterate through the items in <strong class="source-inline">Q</strong> and then find the action that has the maximum value. This is then stored as the value function for the corresponding state:<p class="source-code">valuefunction = defaultdict(float)</p><p class="source-code">for state, action_values in Q.items():</p><p class="source-code">    action_value = np.max(action_values)</p><p class="source-code">    valuefunction[state] = action_value</p><p class="source-code">    print("state is", state, "value is", valuefunction[state])</p><p>You will get the following output:</p><div id="_idContainer458" class="IMG---Figure"><img src="image/B16182_06_09.jpg" alt="Figure 6.9: Output of off-policy Monte Carlo evaluation&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.9: Output of off-policy Monte Carlo evaluation</p>
			<p>The off-policy evaluation has calculated and returned the value function for every state-action pair. In this exercise, we have applied the concept of importance sampling using a behavior policy and applied the learning to a target policy. The output is provided for every combination state and action pair. It has helped us understand off-policy learning. We had two policies – a behavior policy and a target policy. We learned the target policy by following the behavior policy.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3hpOOKa">https://packt.live/3hpOOKa</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2B1GQGa">https://packt.live/2B1GQGa</a>.</p>
			<p>In the next section, we will learn how to solve the frozen lake problem, available in the OpenAI framework, using Monte Carlo techniques.</p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor235"/>Solving Frozen Lake Using Monte Carlo</h1>
			<p>Frozen Lake is another simple game found in the OpenAI framework. This is a classic game where you can do sampling and simulations for Monte Carlo reinforcement learning. We have already described and used the Frozen Lake environment in <em class="italic">Chapter 05</em>, <em class="italic">Dynamic Programming</em>. Here we shall quickly revise the basics of the game so that we can solve it using Monte Carlo methods in the upcoming activity. </p>
			<p>We have a 4x4 grid of cells, which is the entire frozen lake. It contains 16 cells (a 4x4 grid). The cells are marked as <strong class="source-inline">S</strong> – Start, <strong class="source-inline">F</strong> – Frozen, <strong class="source-inline">H</strong> – Hole, and <strong class="source-inline">G</strong> – Goal. The player needs to move from the Start cell, <strong class="source-inline">S</strong>, to the Goal cell, along with the Frozen areas (<strong class="source-inline">F</strong> cells), without falling into Holes (<strong class="source-inline">H</strong> cells). The following figure visually presents the aforementioned information:</p>
			<div>
				<div id="_idContainer459" class="IMG---Figure">
					<img src="image/B16182_06_10.jpg" alt="Figure 6.10: The Frozen Lake game&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10: The Frozen Lake game</p>
			<p>Here are some basic details of the game:</p>
			<ul>
				<li><strong class="bold">The aim of the game</strong>: The aim of the game is to move from the Start (cell <strong class="source-inline">S</strong>) to the Goal (cell <strong class="source-inline">G</strong>).</li>
				<li><strong class="bold">States</strong> = 16</li>
				<li><strong class="bold">Actions</strong> = 4</li>
				<li><strong class="bold">Total state-action pairs</strong> = 64</li>
				<li><strong class="bold">Strategy</strong>: The player should move along the Frozen cells (cells <strong class="source-inline">F</strong>) without falling into the Holes in the lake (cells <strong class="source-inline">H</strong>). Reaching the Goal (cell <strong class="source-inline">G</strong>) or falling into any Hole (cells <strong class="source-inline">H</strong>) ends the game.</li>
				<li><strong class="bold">Actions</strong>: The actions that can be performed in any cell are left, down, right, and up.</li>
				<li><strong class="bold">Players</strong>: It is a single-player game.</li>
				<li><strong class="bold">Rewards</strong>: The reward for moving into a Frozen cell is 0 (cells <strong class="source-inline">F</strong>), +1 for reaching the Goal (cell <strong class="source-inline">G</strong>), and 0 for falling into a Hole (cells <strong class="source-inline">H</strong>).</li>
				<li><strong class="bold">Configuration</strong>: You can configure the frozen lake to be slippery or non-slippery. If the frozen lake is slippery, then the intended action and the actual action can vary, so if someone wants to move left, they might end up moving right or down or up. If the frozen lake is non-slippery, the intended action and the actual action are always aligned. The grid has 16 possible cells where the agent can be at any point in time. The agent can take 4 possible actions in each of these cells. So, there are 64 possibilities in the game, whose likelihood is updated based on the learning. In the next activity, we will learn more about the Frozen Lake game, and understand the various steps and actions.</li>
			</ul>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor236"/>Activity 6.01: Exploring the Frozen Lake Problem – the Reward Function</h2>
			<p>Frozen Lake is a game in OpenAI Gym that's helpful to apply learning and reinforcement techniques. In this activity, we will solve the Frozen Lake problem and determine the various states and actions using Monte Carlo methods. We will track the success rate through batches of episodes.</p>
			<p>Perform the following steps to complete the activity:</p>
			<ol>
				<li value="1">We import the necessary libraries: <strong class="source-inline">gym</strong> for the OpenAI Gym framework, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">defaultdict</strong> is required to process dictionaries.</li>
				<li>The next step is to select the environment as <strong class="source-inline">FrozenLake</strong>. <strong class="source-inline">is_slippery</strong> is set to <strong class="source-inline">False</strong>. The environment is reset with the line <strong class="source-inline">env.reset()</strong> and rendered with the line <strong class="source-inline">env.render()</strong>.</li>
				<li>The number of possible values in the observation space is printed with <strong class="source-inline">print(env.observation_space)</strong>. Similarly, the number of action values is printed with the <strong class="source-inline">print(env.action_space)</strong> command.</li>
				<li>The next step is to define a function to generate a frozen lake <strong class="source-inline">episode</strong>. We initialize the episodes and the environment.</li>
				<li>We simulate various episodes by using a Monte Carlo method. We then navigate step by step and store <strong class="source-inline">episode</strong> and return <strong class="source-inline">reward</strong>. The action is obtained with <strong class="source-inline">env.action_space.sample()</strong>. <strong class="source-inline">next_state</strong>, <strong class="source-inline">action</strong>, and <strong class="source-inline">reward</strong> are obtained by calling the <strong class="source-inline">env_step(action)</strong> function. They are then appended to an episode. The episode is now a list of states, actions, and rewards.</li>
				<li>The key is now to calculate the success rate, which is the likelihood of success for a batch of episodes. The way we do this is by calculating the total number of attempts in a batch of episodes. We calculate how many of them successfully reached the goal. The ratio of the agent successfully reaching the goal to the number of attempts made by the agent is the success ratio. First, we initialize the total rewards.</li>
				<li>We generate <strong class="source-inline">episode</strong> and <strong class="source-inline">reward</strong> for every iteration and calculate the total <strong class="source-inline">reward</strong>.</li>
				<li>The success ratio is calculated by dividing <strong class="source-inline">total_reward</strong> by <strong class="source-inline">100</strong> and is printed.</li>
				<li>The frozen lake prediction is calculated using the <strong class="source-inline">frozen_lake_prediction</strong> function. The final output will demonstrate the default success ratio of the game without any reinforcement learning when the game is played randomly.<p>You will get the following output:</p><div id="_idContainer460" class="IMG---Figure"><img src="image/B16182_06_11.jpg" alt="Figure 6.11: Output of Frozen Lake without learning&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.11: Output of Frozen Lake without learning</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 719.</p>
			<p>In the next section, we detail how we can enable improvement by balancing exploration and exploitation, by using the epsilon soft policy and greedy policy. This ensures that we balance exploration and exploitation.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor237"/>The Pseudocode for Every Visit Monte Carlo Control for Epsilon Soft</h2>
			<p>We have previously implemented every visit Monte Carlo algorithm for estimating the value function. In this section, we will briefly describe every visit Monte Carlo control for epsilon soft so that we can use this in our final activity of this chapter. The following figure shows the pseudo-code for every visit for Epsilon soft by balancing exploration and exploitation: </p>
			<div>
				<div id="_idContainer461" class="IMG---Figure">
					<img src="image/B16182_06_12.jpg" alt="Figure 6.12: Pseudocode for Monte Carlo every visit for epsilon soft&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12: Pseudocode for Monte Carlo every visit for epsilon soft</p>
			<p>The following code picks a random action with epsilon probability and picks an action that has a maximum <strong class="source-inline">Q(s,a)</strong> with 1-epsilon probability. So, we can choose between exploration with epsilon probability and exploitation with 1-epsilon probability:</p>
			<p class="source-code">while not done:</p>
			<p class="source-code">        </p>
			<p class="source-code">        #random action less than epsilon</p>
			<p class="source-code">        if np.random.rand() &lt; epsilon:</p>
			<p class="source-code">            #we go with the random action</p>
			<p class="source-code">            action = env.action_space.sample()</p>
			<p class="source-code">        else:</p>
			<p class="source-code">            """</p>
			<p class="source-code">            1 - epsilon probability, we go with the greedy algorithm</p>
			<p class="source-code">            """</p>
			<p class="source-code">            action = np.argmax(Q[state, :])</p>
			<p>In the next activity, we will evaluate and improve the policy for Frozen Lake by implementing the Monte Carlo control every visit for the epsilon soft method.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor238"/>Activity 6.02 Solving Frozen Lake Using Monte Carlo Control Every Visit Epsilon Soft </h2>
			<p>The activity aims to evaluate and improve the policy for the Frozen Lake problem by using every visit epsilon soft method.</p>
			<p>You can launch the Frozen Lake game by importing <strong class="source-inline">gym</strong> and executing <strong class="source-inline">gym.make()</strong>:</p>
			<p class="source-code">import gym</p>
			<p class="source-code">env = gym.make("FrozenLake-v0", is_slippery=False)</p>
			<p>Perform the following step to complete the activity:</p>
			<ol>
				<li value="1">Import the necessary libraries.</li>
				<li>Select the environment as <strong class="source-inline">FrozenLake</strong>. <strong class="source-inline">is_slippery</strong> is set to <strong class="source-inline">False</strong>.</li>
				<li>Initialize the <strong class="source-inline">Q</strong> value and <strong class="source-inline">num_state_action</strong> to zeros.</li>
				<li>Set the value of <strong class="source-inline">num_episodes</strong> to <strong class="source-inline">100000</strong> and create <strong class="source-inline">rewardsList</strong>. Set epsilon to <strong class="source-inline">0.30</strong>.</li>
				<li>Run the loop till <strong class="source-inline">num_episodes</strong>. Initialize the environment, <strong class="source-inline">results_List</strong>, and <strong class="source-inline">result_sum</strong> to zero. Also, reset the environment.</li>
				<li>We need to now have both exploration and exploitation. Exploration will be a random policy with epsilon probability and exploitation will be a greedy policy with 1-epsilon. We start a <strong class="source-inline">while</strong> loop and check whether we need to pick a random value with probability epsilon or a greedy policy with a probability of 1-epsilon.</li>
				<li>Step through the <strong class="source-inline">action</strong> and get <strong class="source-inline">new_state</strong> and <strong class="source-inline">reward</strong>.</li>
				<li>The result list is appended, with the state and action pair. <strong class="source-inline">result_sum</strong> is incremented by the value of the result.</li>
				<li><strong class="source-inline">new_state</strong> is assigned to <strong class="source-inline">state</strong> and <strong class="source-inline">result_sum</strong> is appended to <strong class="source-inline">rewardsList</strong>.</li>
				<li>Calculate <strong class="source-inline">Q[s,a]</strong> using the incremental method, as <strong class="source-inline">Q[s,a] + (result_sum – Q[s,a]) / N(s,a)</strong>.</li>
				<li>Print the value of the success rate in batches of <strong class="source-inline">1000</strong>.</li>
				<li>Print the final success rate.<p>You will get the following output initially: </p><div id="_idContainer462" class="IMG---Figure"><img src="image/B16182_06_13.jpg" alt="Figure 6.13: Initial output of the Frozen Lake success rate &#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 6.13: Initial output of the Frozen Lake success rate </p>
			<p>You will get the following output finally:</p>
			<div>
				<div id="_idContainer463" class="IMG---Figure">
					<img src="image/B16182_06_14.jpg" alt="Figure 6.14: Final output of the Frozen Lake success rate &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.14: Final output of the Frozen Lake success rate </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 722.</p>
			<h1 id="_idParaDest-199"><a id="_idTextAnchor239"/>Summary</h1>
			<p>Monte Carlo methods learn from experience in the form of sample episodes. Without having a model of the environment, by interacting with the environment, the agent can learn a policy. In several cases of simulation or sampling, an episode is feasible. We learned about the first visit and every visit evaluation. Also, we learned about the balance between exploration and exploitation. This is achieved by having an epsilon soft policy. We then learned about on-policy and off-policy learnings, and how importance sampling plays a key role in off-policy methods. We learned about the Monte Carlo methods by applying them to Blackjack and the Frozen Lake environment available in the OpenAI framework.</p>
			<p>In the next chapter, we will learn about temporal learning and its applications. Temporal learning combines the best of dynamic programming and the Monte Carlo methods. It can work where the model is not known, like the Monte Carlo methods, but can provide incremental learning instead of waiting for the episode to end.</p>
		</div>
	</body></html>