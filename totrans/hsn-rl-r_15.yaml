- en: Exploring Deep Reinforcement Learning Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Neural networks** (**NNs**) are exceptionally effective at getting good characteristics
    for highly structured data. We could then represent our Q function with a neural
    network, which takes the status and action as input and outputs (gives) the corresponding
    Q value. **Deep reinforcement learning** (**DRL**) methods use deep neural networks
    to approximate any of the following reinforcement learning components: value function,
    policy, and model.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will deal with DRL gradually. First, we will learn the basic
    concepts of artificial neural networks and see how to apply them by taking a practical
    example. Later, we will see how to apply these concepts to reinforcement learning
    to improve the performance of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we will learn the fundamentals of artificial neural
    networks, how to apply feedforward neural network methods to your data, and how
    neural network algorithms work. We will understand the basic concepts that deep
    neural networks use to approximate reinforcement learning components and we will
    learn how to implement a deep Q network using R. Finally, we will learn how to
    implement a deep recurrent Q network using R.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing neural network basic concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing feed-forward neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approaching DRL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep recurrent Q-networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/35szk1D](http://bit.ly/35szk1D)'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing neural network basic concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial neural networks** (**ANNs**) are mathematical models whose purpose
    is to try to simulate some typical human brain activities such as pattern recognition,
    language comprehension, image perception, and so on. The architecture of an ANN
    is composed of a system of nodes, which refer to the neurons of a human brain,
    interconnected between them by weighted connections, which simulate synapses between
    neurons. The output of the network is updated iteratively through the link weights
    up to the convergence. The data collected in the experimental fields are provided
    at the input level and the network result is provided by the output level. The
    input nodes represent the independent or predictive variables necessary to predict
    the dependent variables that represent the output neurons.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks offer a very powerful set of tools that can solve problems in
    the field of classification, regression, and non-linear control. In addition to
    having a high processing speed, neural networks can learn the solution from a
    certain set of examples. In many applications, this allows us to circumvent the
    need to develop a model of the physical processes underlying the problem, which
    can often be difficult, if not impossible, to find.
  prefs: []
  type: TYPE_NORMAL
- en: ANNs try to emulate the behavior of biological neurons. Let's see how.
  prefs: []
  type: TYPE_NORMAL
- en: Biological neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The inspiration for neural networks derives from studies on information-processing
    mechanisms in the biological nervous system, the human brain; in fact, much of
    the research on neural networks has precisely the purpose of investigating these
    mechanisms. An artificial neural network is made up of many neurons or simple
    processors. An artificial neuron mimics the characteristics of a biological neuron—every
    cell in the human nervous system can receive, process, and transmit electrical
    signals.
  prefs: []
  type: TYPE_NORMAL
- en: 'It consists of four basic parts, namely, the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Body cell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synapses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Axon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dendrites
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dendrites receive electrical information from other neurons through the
    synapses and transmit them to the body of the cell. Here, they are added together
    and, if the total excitation exceeds a threshold limit, the cell reacts by passing
    the signal to another cell through the axon.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the structure of a biological neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73296bdf-c324-4417-a0c0-dc8611f99d3a.png)'
  prefs: []
  type: TYPE_IMG
- en: When the signal reaches the synapse, it causes the release of chemicals called
    neurotransmitters, which enter the bodies of other neurons. Depending on the type
    of synapse, which can be excitatory or inhibitory, these substances respectively
    increase or decrease the probability that the next neuron becomes active. At each
    synapse, a weight is associated, which determines the type and magnitude of the
    exciter or inhibitor effect. Hence, each neuron carries out a weighted sum of
    the inputs coming from the other neurons and, if this sum exceeds a certain threshold,
    the neuron is activated.
  prefs: []
  type: TYPE_NORMAL
- en: Each operation performed by the neuron has a millisecond duration, so it represents
    a relatively slow processing system. However, the entire network has a very large
    number of neurons and synapses that can operate in parallel and simultaneously,
    making the actual processing power very high. Furthermore, the biological neural
    network has a high tolerance to inaccurate or even wrong information; it has the
    capacity for learning and generalization, which makes it so efficient in identification
    and classification operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The functioning of neurons regulates the activities of the brain, which is
    a naturally optimized machine for solving complex problems. Its structure, made
    of simple elements, has evolved over time in the direction of improving its capabilities:
    there is no central control, and all areas of the brain contribute together to
    the realization of a task or the solution of a problem in a contributory way.
    If one part of the brain stops working, it continues to perform its tasks, perhaps
    not with the same performance. The brain is fault-tolerant; its performance slowly
    degrades in proportion to the destruction of its neurons.'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a similar way to a biological neuron, an artificial neuron receives various
    stimuli in input, each of which is the output of another neuron. Each input is
    then multiplied by a corresponding weight and added to the others to determine
    the level of neuron activation by another function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of a neural network is characterized by the distinction between
    input neurons and output neurons, the number of layers of synapses (or neurons),
    and the presence of feedback connections, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f591bb8-d3f4-4be8-bbb0-be0b3f1a1306.png)'
  prefs: []
  type: TYPE_IMG
- en: When an input vector (stimulus) is applied to the input neurons of the neural
    network, the signals travel in parallel along the connections through the internal
    nodes, up to the output and hence produce the response of the neural network.
    In the simplest formulation, each node processes only the local information, does
    not know the overall purpose of the processing, and has no memory. The response
    and behavior of the network depend intrinsically on its architecture and the value
    of artificial synapses.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, a single layer of synapses is not sufficient to learn the desired
    association between input and output patterns: in these cases, it is necessary
    to use multi-layer networks that possess internal neurons and more than one layer
    of synapses. These networks are called deep neural networks. The response of such
    a network is obtained by calculating the activation of a layer of neurons at a
    time proceeding gradually from the internal nodes toward the exit nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An artificial neural network goal is simply the computation of the outputs
    of all of the neurons, through a deterministic calculation. Basically, ANN is
    a set of mathematical function approximations. The following elements are essential
    in an ANN architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will dive deeper into these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Layers types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We already introduced the architecture of an artificial neural network in the
    *Artificial neural networks* section, and we have been able to analyze a scheme
    in which different types of neurons were highlighted. In that scheme, it is possible
    to identify a structure in layers. In fact, we can easily identify an input layer,
    a middle layer (named hidden layer), and an output layer, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/625dcdbc-3f1d-4829-b703-2cbfaf8b798c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous diagram, it is possible to identify the simplest of architectures
    that includes an input layer, a single hidden layer, and an output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Each layer has its own task that it performs through the action of the neurons
    it contains. The input layer is intended to introduce the initial data into the
    system for further processing by the subsequent layers. From the input level,
    the workflow of the artificial neural network begins.
  prefs: []
  type: TYPE_NORMAL
- en: In the input layer, artificial neurons have a different role to play in some
    *passive* way because they do not receive information from previous levels. In
    general, they receive a series of inputs and introduce the information into the
    system for the first time. This level then sends the data to the next levels,
    where the neurons receive weighted inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The hidden layer in an artificial neural network is interposed between input
    levels and output levels. The neurons of the hidden layer receive a set of weighted
    inputs and produce an output according to the indications received from an activation
    function. It represents the essential part of the entire network, as it is here
    that the magic of transforming the input data into output responses takes place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hidden levels can operate in many ways. In some cases, the inputs are weighted
    randomly; in others they are calibrated through an iterative process. In general,
    the neuron of the hidden layer functions as a biological neuron in the brain:
    it takes its probabilistic input signals, processes them, and converts them into
    an output corresponding to the axon of the biological neuron.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the output layer produces certain outputs for the model. Although they
    are made in a way very similar to other artificial neurons in the neural network,
    the type and number of neurons in the output layer depend on the type of response
    the system must provide. For example, if we are designing a neural network for
    the classification of an object, the output layer will consist of a single node
    that will provide us with this value. In fact, the output of this node must simply
    provide a positive or negative indication of the presence or absence of the target
    in the input data.
  prefs: []
  type: TYPE_NORMAL
- en: Weights and biases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an artificial neural network, the conversion of an input into an output takes
    place thanks to the contribution of the weights of the connections. In linear
    regression, the slope is multiplied by the input to provide the output. The same
    argument can be made for weights in a neural network. In fact, they represent
    numerical parameters that specify the contribution of each neuron to the final
    result. For example, if the inputs are *x[1]*, *x[2]*, and *x[3]*, the synaptic
    weights to be applied to these are indicated as *w[1]*, *w[2]*, and *w[3]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this assumption, we can represent the output returned by the neuron through
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4256d04d-bce5-4e1b-88ed-11f17e638b6d.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous formula, *i* is the number of inputs.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding formula, the matrix multiplication defines a weighted sum.
    To this weighted sum, it is necessary to add the bias that can be compared to
    the added intercept in a linear equation. The bias is, therefore, an additional
    parameter that is used to adjust the output of each neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'The processing done by a neuron is hence denoted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0f19659-6821-499f-8683-42de46ad1f66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output is adjusted by the activation function. The output of neurons in
    a level will represent the input of the next level, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29ed19a0-afef-442c-8bab-2f7cb2683f12.png)'
  prefs: []
  type: TYPE_IMG
- en: The meaning of this scheme is that we are giving the input signal (*x[i]*) a
    weight (*w[i]*), which is a real number that reproduces the natural synapse. When
    the value *w[i]* is greater than zero, the channel is called **excitatory**; if
    the value is less than zero, the channel is inhibitory. The absolute value of
    *w[i]* represents the strength of the connection.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The activation function plays a crucial role in processing system output. The
    activation function represents a mathematical function that converts the input
    into output and defines the process based on neural networks. Without the contribution
    of the activation function, a neural network is trivialized to a simple linear
    function. In a linear function, the conversion from input to output is realized
    through a direct proportionality, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa07f9f6-1e5d-4eda-9545-da04c1ead7e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Simply, a linear function is a polynomial of the first degree, then a straight
    line. In the real world, most problems are non-linear and complex in nature. To
    deal with non-linear problems, it is necessary to use the activation functions.
    Nonlinear functions are high degree polynomial functions, as shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66702e7f-863e-4487-aa2f-756cbdbf85f8.png)'
  prefs: []
  type: TYPE_IMG
- en: It is a non-linear function that contains a factor of complexity. The activation
    functions add the non-linearity property to neural networks and characterize them
    as approximators of universal functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many activation functions available for a neural network to use.
    The following are the most used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sigmoid**: This function is represented by a sigmoid curve, typical for its
    S shape. This is the most used activation function. Its action is to transform
    the input into a value between 0 and 1\. In this way, the model takes on a logistical
    nature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unit step**: This function transforms the input into 0 if the argument is
    negative and 1 if the argument is positive. In this way, the output takes on a
    binary nature. These activation functions are used for binary schemes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperbolic tangent**: It is a non-linear function, defined in the range of
    values (-1, 1). These functions are interesting because they allow the neuron
    to have a continuous output, which allows a probabilistic interpretation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rectified Linear Unit** (**ReLU**): It is a function with linear characteristics
    for parts of the existence domain that will output the input directly if is positive;
    otherwise, it will output zero. The range of output is between 0 and infinity.
    ReLU finds applications in computer vision and speech recognition using deep neural
    networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing feedforward neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the flow passes from the input layer to the hidden layers and therefore
    to the output layer, we talk about feed-forward propagation. In this case, the
    transfer function is applied to each hidden level. Hence, the value of the activation
    function is propagated to the next level. The next layer can be another hidden
    layer or the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: The term **feedforward** is used to indicate the networks in which each node
    receives connections only from the lower layers. These networks emit a response
    for each input pattern but fail to capture the possible temporal structure of
    the input information or to exhibit endogenous temporal dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s move on to a crucial topic for neural networks: neural network training.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural network training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To choose the input values for which a neuron turns on or off, the network
    is trained. This is a crucial step in the realization of the model, which consists
    of training the neural network to generalize the information, starting from a
    set of inputs corresponding to known outputs. The performances of the network
    depend very much on the information presented to them: they must be representative
    of what the network must learn. Training is a fundamental part of building a neural
    network and the examples to be used (training set) must be carefully chosen.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We shall take a step-by-step approach to understand the neural network training
    with a single hidden layer. Let''s take the input layer has one neuron and the
    output will solve a binary classification problem (predict 0 or 1). Here''s a
    list of all the steps for training a network:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the input as a matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use random values to initialize weights and biases. This step must be done only
    at the beginning, then just update them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the following steps from 4 to 9 for each epoch, until convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Send the inputs to the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate the output from the input layer, through the hidden layer(s), to the
    output layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate the error at the outputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adopt the output error to calculate error signals for previous layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adopt the error signals to calculate weight changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the weight changes to update them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Steps 4* and *5* are forward propagation and *steps 6* through *9* are backpropagation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the most used method to teach a network to generalize through the
    adjustment of neuron weights (*w[i]*) is to follow the **delta rule**, which consists
    of comparing the network outputs with the desired values: subtract the two values
    and the difference is used to update all of the weights of the inputs that have
    different values of zero.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is iterated until convergence is reached. The following diagram
    shows a graph of the net weight adjustment procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82ab85ec-fe9a-4657-8e9c-216e5cc8a8af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In practice, the algorithm compares the inputs with the outputs: the difference
    between the weighted input values and the output or expected values is calculated
    and the difference (error) is used to recalculate all the input weights. The procedure
    is repeated until the error between input and output becomes close to zero.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will apply neural networks to solve a regression
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression analysis is the starting point in data science; in fact, they are
    the most well-understood models in numerical simulation. Regression models are
    easily interpreted as they are based on solid mathematical bases—think of matrix
    algebra. Linear regression allows us to derive a mathematical formula representative
    of the corresponding model. Therefore, these techniques are extremely easy to
    understand.
  prefs: []
  type: TYPE_NORMAL
- en: Regression analysis is a statistical process aimed at identifying the relationship
    between a set of independent variables (explanatory variables) and the dependent
    variable (response variable). With this technique, it is possible to establish
    how the value of the response variable changes when the explanatory variable is
    varied.
  prefs: []
  type: TYPE_NORMAL
- en: In the following paragraphs, an example of a regression predictive modeling
    problem is proposed to understand how to solve it with neural networks. The Boston
    dataset will be used as a data source; the median values of owner-occupied homes
    are predicted for the test data. The dataset describes 12 numerical properties
    of houses in Boston suburbs and is concerned with modeling the price of houses
    in those suburbs in thousands of dollars. As such, this is a regression predictive
    modeling problem because the output is a continuous variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that regression and classification are both related to the forecast:
    in classification, we try to predict the output by grouping it into classes (categorical
    variable) while, in regression, we try to predict the output value in a continuous
    way (continuous variable).'
  prefs: []
  type: TYPE_NORMAL
- en: The Boston dataset input attributes include features such as crime rate, the
    proportion of non-retail business acres, and chemical concentrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the data, we draw on the large collection of data available in the UCI
    Machine Learning Repository at the following link: [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of instances and the number of variables are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of instances: 506'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of variables: 13 continuous variables (including the class attribute,
    `medv`) and 1 binary-valued attribute'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All the variables are shown in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`crimper`: Capita crime rate by town'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zn`: Proportion of residential land zoned for lots over 25,000 square feet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indus`: Proportion of non-retail business acres per town'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chas`: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nox`: Nitric oxides concentration (parts per 10 million)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rm`: Average number of rooms per dwelling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`age`: Proportion of owner-occupied units built before 1940'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dis`: Weighted distances to five Boston employment centers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rad`: Index of accessibility to radial highways'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tax`: Full-value property-tax rate per $10,000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ptratio`: Pupil-teacher ratio by town'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lstat`: Percent lower status of the population'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`medv`: Median value of owner-occupied homes in $1,000s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous list, `medv` represents the response variable, and the other
    thirteen variables are the predictors. Our goal is to develop a regression model
    that simulates the variation of the `medv` value. The model should be able to
    identify the relationship between the first thirteen columns and the response
    variable `medv`, if it exists.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is already provided with R libraries (MASS), so we do not have
    to worry about retrieving the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to get the data. To do this, as we said, we can use the MASS
    libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To install a new library, you need to use the `install.packages()` function.
    This feature installs the packages. It is necessary to pass a vector of names
    and a destination library, after which the command downloads the packages from
    the repositories and installs them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now concern ourselves with making the experiment reproducible:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `set.seed()` command makes the example reproducible, in the sense that all
    of the random numbers generated will always be the same, for each simulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will only use the variables necessary for our analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The dataframe also includes the names of the variables as they are in the original
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by taking a look at the data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now perform an exploratory analysis to see how the data is distributed and
    to extract the preliminary knowledge. Let's start by checking the dataset using
    the `str()` function. This function returns a tight summary of the internal structure
    of an R object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, only one line for each basic structure is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we got the confirmation that it was 506 observations of 13 variables: 11
    numericals and 2 integers. Now, to obtain a brief summary of the dataset, we can
    use the `summary()` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `summary()` function returns a series of data statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1d52e33-2f77-4138-9fae-ba4d6be8d2ce.png)'
  prefs: []
  type: TYPE_IMG
- en: The analysis of the results shows that the variables have different intervals.
    When the predictors have very different extreme values, the weight on the response
    variables by the character with extreme values may be prevalent. This can affect
    the accuracy of the forecast. Hence, we may need to scale values under different
    features such that they fall under a common range. Through this statistical procedure,
    it is possible to compare identical variables belonging to different distributions
    and also different variables or variables expressed in different units.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, it is good practice to rescale the data before training a regression
    algorithm. Using the rescaling technique, data units are eliminated; this allows
    us to easily compare data from different locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To rescale the data, we will use the min-max method to get all the scaled data
    in the range [0, 1]. The formula to achieve this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/312c9d15-12ff-45cc-940a-992681a690fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we need to calculate the minimum and maximum values of each column in
    the database. We will use the `apply()` function that applies a function to the
    values of a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Three arguments have been passed: the first specifies the data set on which
    to apply the function (`InputData`). The second argument specifies the indexes
    on which the function (2) will be applied. Being a matrix, 1 specifies the rows
    and 2 specifies the columns. The third argument specifies the function to be applied,
    in our case, the `max()` function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will calculate the minimums for each column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to apply the `scale ()` function to normalize the data. The `scale
    ()` function centers and/or resizes the columns of a numeric matrix, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm the normalization of the data, let''s apply the `summary()` function
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/907c7fbc-9270-456b-81ff-41f777c3e596.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s go into our exploratory analysis. We can do it by making a boxplot of
    the variables, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot is printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e7c11ff-e4e7-448c-bea5-079223d09348.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous diagram clearly shows that some variables have anomalous values.
    For example, the variable crim shows the greatest number of outliers. Outliers
    are numerically different from the rest of the collected data. Statistics obtained
    from variables containing anomalous values may return incorrect information.
  prefs: []
  type: TYPE_NORMAL
- en: Training the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before training the network, we must split the data. We will start with data
    splitting, subdividing data into exactly two subsets of a specified ratio for
    training and validation. This technique is particularly useful when you have a
    very large dataset. In this case, the dataset is divided into two partitions:
    training and test. The training set is used to train the model, while the test
    set will provide us with a significant performance estimate. This method is very
    advantageous when using slow methods and needing a quick approximation of performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example divides the dataset so that 70 percent is used to train
    a neural network model and the remaining 30 percent is used to evaluate model
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The code in the first line subdivides the 70:30 data, meaning to use 70 percent
    of the data to train the network and the remaining 30 percent to test the network.
    In the second and third row, the data of the dataframe named `DataScaled` is subdivided
    into two new dataframes called `TrainData` and `TestData`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to set the formula we will use to build the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the previous piece of code, we first retrieve all variable names through
    the `names()` function. Next, we create the formula we will use to build the network.
  prefs: []
  type: TYPE_NORMAL
- en: The `neuralnet()` function uses formulas in a compact symbolic form. The `~`
    operator defines the model. For example, the **formula y ~** model is interpreted
    as meaning that the answer y is modeled by a predictor specified symbolically
    by the model. This model consists of a series of terms separated by `+` operators.
    Each term is a variable name separated from others by `:` operators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will use the `neuralnet` library to build and train the network. Let''s
    load the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `neuralnet` library is used to train neural networks using backpropagation,
    resilient backpropagation (RPROP) with or without weight backtracking, or the
    modified globally convergent version (GRPROP). The following table gives some
    information about this package:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Package | `neuralnet` |'
  prefs: []
  type: TYPE_TB
- en: '| Date | 2019-02-07 |'
  prefs: []
  type: TYPE_TB
- en: '| Version | 1.44.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Title | Training of Neural Networks |'
  prefs: []
  type: TYPE_TB
- en: '| Author | Stefan Fritsch, Frauke Guenther, Marvin N. Wright, Marc Suling,
    Sebastian M. Mueller |'
  prefs: []
  type: TYPE_TB
- en: 'The following lists the most useful functions contained in this package:'
  prefs: []
  type: TYPE_NORMAL
- en: '`neuralnet`: Training of neural networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`compute`: Computation of a given neural network for given covariate vectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prediction`: Summarizes the output of the neural network, the data and the
    fitted values of `glm` objects (if available)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`plot.nn`: The plot method for neural networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can build and train the network. At first, we have to choose the number
    of neurons, and to do this, we need to know the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The choice of a layer on a few neurons will cause a high error; this is because
    the predictive factors could be too complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the contrary, too many neurons overload training data and do not allow generalization.
    The number of neurons in each hidden layer should be a number between the input
    size and the output layer, for example, an average.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of neurons in each hidden layer should not exceed twice the number
    of incoming neurons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We choose to set ten neurons in the hidden layer. Do not worry—the best choice
    is obtained with experience:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The hidden argument specifies the number of neurons for each hidden layer. The
    `linear.output` argument performs a regression if `linear.output=TRUE` or a classification
    if `linear.output=FALSE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To produce result summaries of the results of the model, we use the `summary()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Three features are displayed for each component of the neural network model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Length:** This feature specifies how many elements of this type are contained
    in it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Class:** This feature returns a specific indication on the component class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mode:** This feature describes the type of component (numeric, list, function,
    logical, and so on).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `plot()` function draws a graph indicating the neural network architecture
    with layers, nodes, and weights on each connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The neural network plot is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40121ab6-a594-487d-b78a-296fd0071878.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous diagram, the black lines represent the connections between each
    layer; also, the weight values on each connection are printed. The blue lines
    show the added bias in each step.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we can use the network to make predictions. For this, we had set aside
    30% of the data in the `TestData` dataframe. It is time to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How can we figure whether the forecasts performed by the network are accurate?
    We can use the **mean squared error** (**MSE**) as a measure of how far our predictions
    are from the real data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first part of the algorithm, we have normalized the data. To compare
    the data we need to step back and return to the original data. Once the values
    of the dataset are restored, we can calculate the MSE through the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5799058f-14d5-4c07-a5d0-9a460b9f8f81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code performs an MSE calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have results, but what do we compare them with? To compare the results
    with another model, we can construct a linear regression model. Then, we elaborate
    on a linear regression model by applying the `lm()` function. This function is
    used to process linear regression models, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To produce a summary of the results of the model, we can use once again the
    `summary()` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f091fae-3580-4f36-b6ea-949a7583f484.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will calculate the MSE for the model based on multiple regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can compare the results of both models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: From the comparison between the two models (neural network model versus linear
    regression model), the neural network wins (19.4 versus 34.8).
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will see how it is possible to develop a DRL model.
  prefs: []
  type: TYPE_NORMAL
- en: Approaching DRL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 7](9a0709b1-fdad-4fba-8a06-30d68361b3b2.xhtml), *Temporal Difference
    Learning*, we addressed a practical example of using Q-learning to address a vehicle
    routing problem. In that case, the estimates of the value function were made using
    a table, in which each box represents a state or a state-action pair. The use
    of a table to represent the value function allows the creation of simple algorithms.
    Under Markovian environmental conditions, this table allows us to accurately estimate
    the value function since it assigns the expected performance during the iterations
    of the policies to every possible configuration from the environment. The use
    of the table, however, also leads to limitations. These methods apply only to
    environments with a reduced number of states and actions. The problem is not limited
    to the large amount of memory required to store the table, but above all, to the
    large amount of data and time required to accurately estimate each state-action
    pair. In other words, the main problem is generalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, we can adopt a method based on the combination of reinforcement
    learning methods with function approximation methods. The following diagram shows
    a deep Q-learning scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0470eccc-f797-4e9e-af06-899bceb5684b.png)'
  prefs: []
  type: TYPE_IMG
- en: The term deep Q-learning refers to a method of reinforcement learning that adopts
    a neural network as an approximation of a value function. It, therefore, represents
    an evolution of the basic Q-learning method, since the action-state table is replaced
    by a neural network, to approximate the optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: This is an innovative approach compared to those seen in the previous chapters.
    So far, the input of the algorithm has provided both the state and the action
    to provide the expected return. Deep Q-learning revolutionizes the structure,
    as it only requires the state of the environment as an input and provides all
    of the status-action values, as there are actions that can be performed in the
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning is an algorithm widely used in reinforcement learning. Initially,
    it was considered an unstable algorithm when used with neural networks and therefore
    its use was limited to tasks and problems that involved limited dimensional spaces
    of states. The Q-learning algorithms and techniques can be used with DNNs. These
    algorithms have shown excellent performance.
  prefs: []
  type: TYPE_NORMAL
- en: The Deep Q-learning or Deep Q-Network (DQN) is a reinforcement learning method
    for the approximation of the function. It represents an evolution of the Q-learning
    method where the action-state table is replaced by a neural network. In this algorithm,
    therefore, the learning does not consist of updating the table but consists of
    adjusting the weights of the neurons that make up the network. This update takes
    place using the backpropagation technique, which we have had the opportunity to
    learn more about in the *Neural network training* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning of the value function is therefore based on the modification of
    the weights using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4da55eb9-4a0c-4890-b0eb-9912439c6861.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous equation, the two terms take on the following meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*L[t]* is the loss function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b29ae214-7f75-417a-93ee-075f338d8a92.png)is the optimal expected return.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/bbc4d93f-23fa-43a5-8ab4-56bdc487feb9.png)is the estimated value from
    the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The errors calculated by the loss function will be propagated backward in the
    network using a backward step (backpropagation), following the gradient descent
    logic. In fact, the gradient indicates the direction of the greatest growth of
    a function; moving in the opposite direction, we reduce the error to the maximum.
    Policy behavior is given by an e-greedy approach to ensure enough exploration.
    The key aspect of DQN is the use of the experience replay. With this technique,
    the agent's experience is taken at every time step *t* and saved in a dataset
    called replay memory.
  prefs: []
  type: TYPE_NORMAL
- en: The training is carried out through a mini-batch technique, that is, by taking
    a sub-set of samples of experiences randomly extracted from the replay memory.
    In this way, past experiences are used to update the network. Furthermore, the
    sub-set chosen randomly by the replay memory allows interrupting the strong correlation
    between successive experiences, hence reducing the variance between updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the algorithm in pseudo-code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This algorithm can be implemented using R and the libraries available for neural
    networks and reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now see an advanced example of DRL.
  prefs: []
  type: TYPE_NORMAL
- en: Deep recurrent Q-networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last section, *Approaching DRL*, we have already said that deep Q-learning
    adopts a neural network as an approximation of a value function. However, this
    method has limited memory and relies on the possibility of perceiving the state
    of the environment at each decision point. To overcome this problem, we can add
    recurrence to a **deep Q-network** (**DQN**) by replacing the first level fully
    connected neural network with a recurring LSTM. In this way, the **deep recurrent
    Q-network** (**DRQN**) model is obtained.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **recurrent neural network** (**RNN**) is a neural model in which a bidirectional
    flow of information is present. In other words, while the propagation of signals
    in feedforward networks takes place only in a continuous manner in one direction
    from inputs to outputs, recurrent networks are different. In recurrent networks,
    this propagation can also occur from a neural layer following a previous one,
    between neurons belonging to the same layer, or even between a neuron and itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'A recurring network will decide things at a particular time which will affect
    the decision it will take immediately after. Recurrent networks have two sources
    of input: the present and the recent past. This information is combined to determine
    how to respond to the new data. Recurrent networks differ from feedforward networks
    in that they add feedback linked to past decisions. This functionality gives the
    recurring networks a memory to perform tasks that feedforward networks cannot
    do.'
  prefs: []
  type: TYPE_NORMAL
- en: Access to memory occurs through the content rather than by address or location.
    One approach to this is that the memory content is the pattern of activations
    on the nodes of an RNN. The idea is to start the network with an activation scheme
    that is a partial or noisy representation of the requested memory content and
    that the network stabilizes on the required content.
  prefs: []
  type: TYPE_NORMAL
- en: 'An RNN is a class of neural networks where there is at least one feedback connection
    between neurons that form a directed cycle. A typical RNN with connections between
    the output layer and the hidden layer is represented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7b4b40d-7067-4397-942b-a9e87dae5960.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the recurring network shown in the preceding diagram, both the input level
    and the output level are used to define the weights of the hidden level. Ultimately,
    we can think of RNNs as a variant of ANNs: these variants can be characterized
    by a different number of hidden levels and different trends of the data flow.
    RNNs are characterized by different trends in the flow of data, in fact, the connections
    between the neurons form a cycle. Recurrent neural networks can use internal memory
    for their processing, as they have connections between hidden levels that propagate
    over time to learn sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the world of DRL. To start, we learned the basic
    concepts of neural networks. We understood the concepts of layers, nodes, biases
    and transfer functions. In a nutshell, we understood how the architecture of a
    fully connected neural network is structured. Later, we applied the acquired skills,
    building a neural network to solve a regression problem. Then, we learned what
    is meant by DRL and how neural networks are used to approximate the value function.
    Finally, we analyzed a further form of DRL in which the neural network is replaced
    by a recurring network. These are the DRQNs that have proven to be particularly
    efficient.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore the Keras model using TensorFlow as a backend
    engine. We will learn how to use Keras to set a multilayer perceptron model. Then,
    we will learn how to use DRL to balance a cart pole system.
  prefs: []
  type: TYPE_NORMAL
