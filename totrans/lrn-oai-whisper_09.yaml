- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Harnessing Whisper for Personalized Voice Synthesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to [*Chapter 9*](B21020_09.xhtml#_idTextAnchor207), where we’ll delve
    into **personalized voice synthesis** (**PVS**). This field encompasses various
    applications and technologies that create synthetic voices tailored to individual
    preferences or needs. PVS is a versatile process that can be customized for various
    purposes, including assistive technologies, virtual assistant development, and
    digital content creation. In this context, OpenAI’s Whisper tool enables voice
    synthesis by providing accurate speech data transcriptions during preprocessing
    and registration.
  prefs: []
  type: TYPE_NORMAL
- en: As we begin, it’s crucial to distinguish between voice cloning and PVS. Voice
    cloning involves creating a digital replica of a natural person’s voice. While
    this technology has valid applications, it also raises significant ethical concerns.
    PVS, however, focuses on creating unique voices inspired by specific characteristics
    without directly copying an individual’s voice. This distinction is vital in discussions
    about the ethical use of voice synthesis technologies. In this chapter, we will
    guide you on harnessing Whisper’s power to create PVS models, ensuring you have
    the knowledge to use this technology responsibly.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll begin by exploring **speech synthesis** and **text-to-speech** (**TTS**)
    fundamentals. You will gain insights into the role of neural networks, audio processing,
    and voice synthesis in this domain. Building on this foundation, we will guide
    you through converting audio files to the **LJSpeech** format, a standardized
    dataset structure commonly used in TTS tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will introduce you to the **Deep Learning Art School** (**DLAS**) toolkit,
    a robust framework for fine-tuning PVS models. This is where your learning journey
    will truly begin. You will discover how to set up the training environment, prepare
    the dataset, and configure the model architecture. By leveraging the power of
    Whisper’s accurate transcriptions, you can align audio segments with their corresponding
    text, creating a dataset suitable for training PVS models. This tutorial is not
    just a guide but your gateway to mastering the art of PVS with Whisper. Get ready
    to be inspired and motivated!
  prefs: []
  type: TYPE_NORMAL
- en: Hands-on examples and code snippets will give you practical experience fine-tuning
    a pre-trained PVS model using your LJSpeech dataset. You will discover how to
    customize the training process, select appropriate hyperparameters, and evaluate
    the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will test your fine-tuned PVS model by synthesizing realistic and
    expressive speech. You will learn how to generate natural-sounding speech by providing
    text input to the model, bringing the PVS voice to life.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding TTS in PVS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting audio files into LJSpeech format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning a PVS model using the DLAS toolkit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synthesizing speech using a fine-tuned PVS model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a comprehensive understanding of how
    to utilize Whisper for PVS. You will possess the knowledge and skills to preprocess
    audio data, fine-tune voice models, and generate realistic speech using PVS frameworks.
    Whether you are a researcher, developer, or enthusiast in speech technology, this
    chapter will equip you with valuable insights and practical techniques to unlock
    the potential of PVS using OpenAI’s Whisper.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To harness the capabilities of OpenAI’s Whisper for advanced applications, this
    chapter leverages Python and Google Colab for ease of use and accessibility. The
    Python environment setup includes the Whisper library for transcription tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Key requirements**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Colab notebooks**: The notebooks are set to run our Python code with
    the minimum required memory and capacity. If the **T4 GPU** runtime type is available,
    select it for better performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Python environment**: Each notebook contains directives to load the required
    Python libraries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hugging Face account**: Some notebooks require a Hugging Face account and
    login API key. The Colab notebooks include information about this topic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Audacity**: Audacity is a free and open source digital audio editor and recording
    application available for Windows, macOS, Linux, and other Unix-like operating
    systems. It is an excellent choice if you want to synthesize your voice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microphone and speakers**: Some notebooks implement audio with voice recording
    and audio playback. A microphone and speakers connected to your computer might
    help you experience the interactive voice features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitHub repository access**: All Python code, including examples, is available
    in this chapter’s GitHub repository ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter09](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter09)).
    These Colab notebooks are ready to run, providing a practical and hands-on approach
    to learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By meeting these technical requirements, you will be prepared to explore Whisper
    in different contexts while enjoying the streamlined experience of Google Colab
    and the comprehensive resources available on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding text-to-speech in voice synthesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TTS is a crucial component in the voice synthesis process, enabling speech
    to be generated from written text using the synthesized voice. Understanding the
    fundamentals of TTS is essential to grasp how voice synthesizing works and how
    it can be applied in various scenarios. *Figure 9**.1* illustrates a high-level
    overview of how TTS works in the context of voice synthesis without delving too
    deeply into technical specifics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – The TTS voice synthesis pipeline](img/Figure_9.1_B21020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – The TTS voice synthesis pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'There are five components in the TTS voice synthesis pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text preprocessing**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The input text is first normalized and preprocessed.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Numbers, abbreviations, and special characters are expanded into full words.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The text is divided into individual sentences, words, and phonemes (distinct
    sound units).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Text-to-spectrogram**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The normalized text is converted into a sequence of linguistic features and
    encoded into a vector representation.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A spectrogram generator model, usually a deep learning model, takes this encoded
    text and generates a spectrogram.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The spectrogram visually represents the frequencies and intensities of the speech
    sounds over time.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Spectrogram-to-waveform**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The spectrogram is then fed into a vocoder model. The vocoder is a generative
    model trained to convert spectrograms into audible waveforms. It reconstructs
    the speech signal from the frequency information in the spectrogram.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Voice synthesis**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To synthesize a specific person’s voice, the TTS models are fine-tuned on a
    dataset of that person’s speech. This allows the models to learn their voices’
    unique characteristics, tone, and prosody. With sufficient training data, the
    generated speech will mimic the target voice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Synthesis**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the generated waveform is output as audible synthetic speech. The result
    is a synthesized voice that speaks the original input text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Modern TTS systems can produce highly natural-sounding speech with appropriate
    intonation and expressiveness. The TTS pipeline, with its complex interplay of
    text processing, acoustic modeling, and speech synthesis, forms the foundation
    of the PVS transformative technology. As we explore the intricacies of voice synthesis,
    it is essential to understand how TTS systems can be leveraged to create personalized
    voices.
  prefs: []
  type: TYPE_NORMAL
- en: One such robust TTS implementation is **TorToiSe-TTS-Fast**, a high-performance
    TTS system that harnesses the power of neural networks to generate realistic and
    expressive speech. The following sections will delve into TorToiSe-TTS-Fast’s
    capabilities and demonstrate how it can synthesize voices with remarkable accuracy
    and naturalness.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TorToiSe-TTS-Fast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 5*](B21020_05.xhtml#_idTextAnchor142), we used the gTTS Python
    library, an interface to Google Translate’s TTS API. gTTS lets you generate spoken
    audio from text using Google’s TTS engine. This time, we will explore the TorToiSe-TTS-Fast
    project, a high-performance TTS system that leverages neural networks to synthesize
    realistic speech without fine-tuning. Next, we will learn how to initialize the
    `TextToSpeech` model, which is the core component of the TTS system. We will explore
    the `TextToSpeech` class and understand its role in converting text into speech.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the exciting features of the TorToiSe-TTS-Fast project is its ability
    to generate speech using different audio clip samples of a given voice. The project
    provides a collection of pre-packaged voices as audio clips organized in separate
    folders. These audio clips are used to determine many properties of the voice
    synthesized output, such as the pitch and tone of the voice, speaking speed, and
    even speaking defects, such as a lisp or stuttering. We will delve into selecting
    a voice from that collection of pre-existing voice samples. *Figure 9**.2* shows
    the TorToiSe-TTS-Fast voice processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – TorToiSe-TTS-Fast voice processing pipeline](img/Figure_9.2_B21020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – TorToiSe-TTS-Fast voice processing pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'By following the steps in *Figure 9**.2*, you can incorporate additional voices
    into TorToiSe and enhance its versatility:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect audio samples featuring the desired voice(s). Interviews on YouTube
    (which can be downloaded using `youtube-dl` or the `pytube` Python library, as
    we did in [*Chapter 6*](B21020_06.xhtml#_idTextAnchor160)), audiobooks, and podcasts
    are excellent sources. I recommend the **Audacity** tool as a viable option for
    recording your voice and processing audio files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the collected audio into segments of approximately 10 seconds each. A
    minimum of 3 clips is required, but more clips are recommended for better results.
    During testing, I experimented with up to 5 clips.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the audio segments into WAV format with floating-point encoding and
    a sample rate of 22,050 Hz.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you run the `LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb`
    notebook later in this chapter, you will see a directory structure called `/tortoise/voices/`
    with audio clip samples in it. This is the default folder TorToiSe uses to store
    and retrieve audio samples. If you create your samples, create a folder in that
    `/tortoise/voices/` directory and save your files there. For example, I made the
    `/tortoise/voices/josue` folder to store my audio files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transfer the processed audio segments into the newly created subdirectory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To utilize the new voice, execute the `tortoise` utilities with the `--voice`
    flag, followed by the name of your subdirectory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After exploring the TorToiSe-TTS-Fast pipeline, it should be clear that high-quality
    audio data is foundational to creating convincing, natural-sounding synthesized
    voices. Preparing this audio data involves creating new recordings or manipulating
    existing audio files to ensure they are suitable for voice synthesis. This is
    where Audacity comes into play as a powerful tool for audio creation, editing,
    and refinement. Of course, I encourage you to use other tools you are already
    using for audio processing; Audacity and creating an audio file is optional.
  prefs: []
  type: TYPE_NORMAL
- en: Audacity is a versatile tool for creating, editing, and manipulating audio files,
    an essential step in the voice synthesis pipeline. It allows you to record your
    voice samples, trim and split audio clips, adjust audio properties such as pitch
    and speed, and export files in various formats compatible with voice synthesis
    tools. By leveraging Audacity’s capabilities, you can prepare high-quality audio
    data tailored to your voice synthesis requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Using Audacity for audio processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At its core, Audacity is a multitrack audio editor and recorder that supports
    many operating systems, including Windows, macOS, GNU/Linux, and other Unix-like
    systems. Its open source nature ensures it remains free for all users and fosters
    a vibrant community of developers and audio enthusiasts who continuously contribute
    to its development and enhancement. This collaborative effort has equipped Audacity
    with various capabilities, from basic recording and editing to more advanced features
    such as noise reduction, spectral analysis, and support for different audio formats.
    If you prefer another audio editor, go for it. The use of Audacity is optional.
    If you want to install it, here is a step-by-step guide.
  prefs: []
  type: TYPE_NORMAL
- en: The installation process of Audacity is straightforward, regardless of your
    operating system. Detailed instructions are available on the Audacity website
    ([https://support.audacityteam.org/basics/downloading-and-installing-audacity](https://support.audacityteam.org/basics/downloading-and-installing-audacity)).
    Here, we’ll cover the basic steps to get Audacity up and running on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Audacity for Windows
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Download the installer**: Navigate to the official Audacity website ([https://www.audacityteam.org/](https://www.audacityteam.org/))
    and click on the download link for the Windows version. The site will automatically
    detect your operating system, but you can manually select the version if needed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Downloads` folder) and double-click to initiate installation. You might encounter
    a security prompt asking for permission to allow the installer to change your
    system; click **Yes** to proceed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Follow the installation wizard**: The installer will guide you through several
    steps. You’ll select your preferred language, agree to the license terms, choose
    the installation directory, and decide on additional tasks, such as creating a
    desktop shortcut.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Complete the installation**: After configuring your preferences, click **Install**
    to begin the installation. Once completed, you can launch Audacity directly from
    the installer or find it in your Start menu.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Audacity for macOS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Download the DMG file**: Visit the Audacity website and download the macOS
    version. The site should automatically provide the correct version of your system.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Applications` folder to install the software. You might need to authenticate
    using your administrator password.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Applications` folder. macOS might prompt you to confirm that you trust the
    application, especially if you’re running it for the first time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installing Audacity for Linux
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linux users can download AppImage from the Audacity website or install Audacity
    using their distribution’s package manager. For AppImage, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Make the AppImage executable**: After downloading, right-click the file,
    navigate to **Properties** | **Permissions**, and check the option to make the
    file executable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Run Audacity**: Double-click AppImage to launch Audacity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alternatively, use commands such as `sudo apt install audacity` for Debian-based
    distributions or `sudo yum install audacity` for Fedora/RHEL to install Audacity
    through the Terminal.
  prefs: []
  type: TYPE_NORMAL
- en: Running the notebook with TorToiSe-TTS-Fast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a more detailed understanding of Audacity as an audio creation, manipulation,
    and management tool, let’s do some hands-on work with TorToiSe-TTS-Fast. Please
    find and open the Colab notebook called `LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb`
    ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_1_Synthesizing_voices_with_tortoise_tts_fast.ipynb)).
    This notebook is based on the TorToiSe-TTS-Fast ([https://github.com/152334H/tortoise-tts-fast](https://github.com/152334H/tortoise-tts-fast))
    TTS project, which drastically boosts the performance of `TorToiSe` ([https://github.com/neonbjb/tortoise-tts](https://github.com/neonbjb/tortoise-tts)),
    without modifying the base models.
  prefs: []
  type: TYPE_NORMAL
- en: Using the notebook, we will develop speech from a given text with the `TextToSpeech`
    model initialized and a chosen voice. Furthermore, we will investigate the flexibility
    of the TorToiSe-TTS-Fast project by generating speech using random and even custom
    voices. We can create personalized voices for speech synthesis by uploading and
    preprocessing our WAV files.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we will explore the fascinating capability of combining multiple voices
    to generate speech with blended characteristics. We can create unique and intriguing
    voice combinations by loading voice samples and conditioning latents from different
    voices.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this section, you will have a solid understanding of TTS in the
    context of voice synthesis. You will have the knowledge and practical skills to
    set up the environment, initialize the `TextToSpeech` model, select voices, generate
    speech, and create custom and combined voices using the TorToiSe-TTS-Fast project.
    This understanding will serve as a foundation for further exploring the potential
    of voice synthesis and its applications in various domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s open the notebook and run the cells to better understand the voice synthesis
    pipeline in the TorToiSe-TTS-Fast project:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Setting up the environment**: Here, we will install and instantiate several
    libraries, each serving a distinct purpose in the project setup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s briefly review each library:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`torch`: This is the PyTorch library, a popular open source machine learning
    library for computer vision and natural language processing applications. In the
    context of this project, PyTorch provides the foundational framework for building
    and training the neural networks that underpin the voice synthesis capabilities
    of TorToiSe-TTS-Fast.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torchaudio`: As an extension to PyTorch, `torchaudio` offers easy access to
    audio processing tools within the PyTorch framework. It is used for loading and
    saving audio files and performing transformations and augmentations on audio data,
    which are essential tasks in voice synthesis.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`huggingface_hub`: This library from Hugging Face allows users to easily download
    and upload models and other files to the Hugging Face Hub, which may include the
    pre-trained models or components required by the `TextToSpeech` class for voice
    synthesis. The `huggingface_hub` library also provides a function for authenticating
    with the Hugging Face Hub via `notebook_login()` and managing user information
    via `whoami()`, facilitating access to the models and resources stored on the
    Hub required for voice synthesis.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers (version 4.29.2)`: The `transformers` library, also from Hugging
    Face, provides thousands of pre-trained models for various natural language processing
    tasks, including TTS. This library supports the underlying NLP and TTS functionalities
    of the TorToiSe-TTS-Fast project by providing access to state-of-the-art models
    and utilities.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`voicefixer (version 0.1.2)`: This tool is designed for repairing and enhancing
    human voice recordings. `voicefixer` improves the quality of voice samples before
    they are processed by the voice synthesis system, ensuring higher fidelity in
    the synthesized voices.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BigVGAN`: The TTS model uses a `BigVGAN` library plays a role in the voice
    synthesis process, enhancing the realism or quality of the generated voices.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these libraries contributes to the overall functionality of the TorToiSe-TTS-Fast
    project by providing essential tools and frameworks for machine learning, audio
    processing, model management, and voice enhancement. These enable the efficient
    and practical synthesis of human voices.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`TextToSpeech` model that will be used for synthesizing voices. The steps involved
    in this section are designed to initialize the TTS model so that it is ready to
    process text input and generate corresponding speech output using the selected
    voice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the steps that are outlined in the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`TextToSpeech``TextToSpeech` class from the `tortoise.api` module. This class
    is the primary interface for the TTS functionality provided by the TorToiSe-TTS-Fast
    project.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TextToSpeech`: After importing the class, an instance of `TextToSpeech` is
    created by simply calling the class constructor without any arguments. This instance
    is assigned to the `tts` variable.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TextToSpeech`, the necessary models are automatically downloaded from the
    Hugging Face Hub. This step ensures that all the required components for voice
    synthesis are available locally.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `TextToSpeech` class encapsulates the functionality needed to convert text
    into speech. Initializing it is critical in preparing the system for voice synthesizing
    tasks. Once the model has been initialized, it can be used in subsequent steps
    to generate speech from text using various voices.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Selecting a voice**: This section is crucial for personalizing the voice
    synthesis process by allowing you to choose from various pre-existing voice samples
    or uploaded voice clips:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The steps involved in guiding you through the process of selecting a specific
    voice to synthesize are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`os` module to interact with the operating system and lists all the available
    voice folders in the `tortoise/voices` directory. This step is essential for identifying
    which voices are available for synthesis.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dropdown` class from the `ipywidgets` library. This widget allows you to select
    a voice folder from the list of available folders. The `Dropdown` widget is configured
    with options populated from the list of voice folders, a description prompt (`"Select
    a voice:"`), and other settings to ensure usability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`os.listdir` function, and the `Dropdown` widget is again used to present these
    options to you.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IPython.display.Audio` class plays the voice file chosen directly in the Colab
    notebook. This feature provides immediate auditory feedback, enabling you to confirm
    that the selected voice is the desired one for synthesis.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These steps collectively enable a user-friendly and interactive approach to
    selecting a voice for synthesis. They ensure that you can easily navigate the
    available options and make an informed choice based on your preferences or project
    requirements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Generating speech with a selected voice**: This section of the notebook generates
    speech audio from text using the voice previously specified by you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The steps in this section are designed to convert your input text into spoken
    words in the chosen voice’s style. Here are the steps outlined in the code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`text`. This text will be synthesized into speech.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"ultra_fast"`, `"fast"`, `"standard"`, and `"high_quality"`. This determines
    the trade-off between generation speed and audio quality.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_voice` function from `tortoise.utils.audio` is used to load the selected
    voice. This function returns two items: `voice_samples` and `conditioning_latents`.
    These condition the TTS model to generate speech in the selected voice’s style.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tts_with_preset` method of the `tts` object (an instance of the `TextToSpeech`
    class) is called with the text, voice samples, conditioning latents, and preset.
    This method synthesizes the speech based on the given parameters.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torchaudio.save` function. The file is then played using `IPython.display.Audio`
    to allow you to hear the synthesized speech.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These steps enable you to create a spoken version of their text using the specific
    characteristics of the chosen voice, effectively using the PVS model for speech
    synthesis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`voice_samples` and `conditioning_latents` set to `None`, which generates speech
    using a random voice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Using a custom voice**: The following code allows users to upload their WAV
    files (6-10 seconds long) to create a custom voice:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It creates a custom voice folder using `os.makedirs` and saves the uploaded
    files in that folder. The custom voice is then loaded and used to generate speech,
    similar to *steps 4* and *5*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`load_voices` function loads multiple voices (in this case, `''pat''` and `''william''`).
    The `tts_with_preset` method combines voice samples and conditioning latents to
    generate speech with traits from both voices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Having gained a foundational understanding of TTS in voice synthesis and explored
    the powerful capabilities of the TorToiSe-TTS-Fast project, we’ll turn our attention
    to a crucial step in preparing our data for the voice synthesizing process: converting
    audio files into the LJSpeech format.'
  prefs: []
  type: TYPE_NORMAL
- en: PVS step 1 – Converting audio files into LJSpeech format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section and the accompanying notebook, `LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb`,
    represent the initial step in the three-step PVS process outlined in this chapter.
    This step takes an audio sample of the target voice as input and processes it
    into the LJSpeech dataset format. The notebook demonstrates using the OZEN Toolkit
    and OpenAI’s Whisper to extract speech, transcribe it, and organize the data according
    to the LJSpeech structure. The resulting LJSpeech-formatted dataset, consisting
    of segmented audio files and corresponding transcriptions, serves as the input
    for the second step, *PVS step 2 – Fine-tuning a discrete variational autoencoder
    using the DLAS toolkit*, where a PVS model will be fine-tuned using this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: An LJSpeech-formatted dataset is crucial in TTS models as it provides a standardized
    structure for organizing audio files and their corresponding transcriptions. By
    following the LJSpeech format, researchers and developers can ensure compatibility
    with various TTS tools and facilitate training.
  prefs: []
  type: TYPE_NORMAL
- en: 'An LJSpeech-formatted dataset refers to a specific structure and organization
    of audio files and their corresponding transcriptions modeled after the **LJSpeech
    dataset** ([https://keithito.com/LJ-Speech-Dataset/](https://keithito.com/LJ-Speech-Dataset/)).
    The LJSpeech dataset is a public domain speech dataset that includes 13,100 short
    audio clips of a single speaker reading passages from seven non-fiction books,
    with a transcription of each clip. The audio clips vary in length and have a total
    duration of approximately 24 hours. When formatting a dataset for training a TTS
    model in the style of LJSpeech, the following structure is recommended:'
  prefs: []
  type: TYPE_NORMAL
- en: Audio clips should be divided into separate files with a corresponding transcription.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The WAV file format should be used for the audio to avoid compression artifacts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The audio clips and their transcriptions are collected in a folder named `wavs`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A metadata text file maps each audio clip to its transcription. This file should
    have columns delimited by a special character, typically a pipe (`|`), to separate
    the audio filename, the transcription, and the normalized transcription.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The delimiter used in the metadata file should not appear in the transcription
    text itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If normalized transcriptions are unavailable, the same transcription can be
    used for both columns, with normalization applied later in the pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The folder structure for an LJSpeech-formatted dataset would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the text files, entries would be formatted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The LJSpeech format is widely used because it is supported by various TTS tools,
    such as TorToiSe, which provides tooling for the LJSpeech dataset. Formatting
    a dataset this way allows for immediate model training without additional formatting
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the LJSpeech format and why it’s used, let’s convert
    our audio files into this format. By doing so, we’ll ensure that our dataset is
    compatible with various TTS tools and ready for training our PVS models.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a recording of the voice you would like to synthesize, the next
    step is to preprocess the audio files using the OZEN Toolkit. This toolkit simplifies
    extracting speech, transcribing it using Whisper, and saving the results in the
    LJSpeech format. It can handle both single audio files and entire folders of audio
    files.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the OZEN Toolkit and Whisper allows us to efficiently convert our
    audio data into the LJSpeech format. The toolkit automates segmenting audio files,
    generating corresponding WAV files, and creating the necessary metadata files
    (`train.txt` and `valid.txt`) that map the audio files to their transcriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Converting audio files into the LJSpeech format is a crucial skill in the voice
    synthesis pipeline as it ensures data compatibility and facilitates the training
    process. Mastering this technique will prepare you to tackle the subsequent steps,
    such as fine-tuning PVS models and synthesizing speech.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please find and open the Colab notebook called `LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb`
    ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb)).
    This notebook is based on the OZEN Toolkit project ([https://github.com/devilismyfriend/ozen-toolkit](https://github.com/devilismyfriend/ozen-toolkit)).
    Given a folder of files or a single audio file, it will extract the speech, transcribe
    using Whisper, and save it in LJ format (segmented audio files in WAV format go
    in the `wavs` folder, while transcriptions go in the `train` and `valid` folders).
    Let’s walk through the code while explaining the steps and providing code samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloning the OZEN Toolkit repository**: The following command clones the OZEN
    Toolkit repository from GitHub, which contains the necessary scripts and utilities
    for processing audio files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Installing the required libraries**: The following commands install the necessary
    libraries for audio processing, speech recognition, and text formatting. After
    installing the dependencies, restarting the session is recommended to ensure the
    installed packages are initialized adequately:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '%cd ozen-toolkit'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Downloading a sample audio file**: If you do not have an audio file for synthesis,
    this command downloads a sample audio file from the specified URL for demonstration
    purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import os
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: from google.colab import files
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: custom_voice_folder = "./myaudiofile"
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: os.makedirs(custom_voice_folder, exist_ok=True)  # Create the directory if it
    doesn't exist
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'for filename, file_data in files.upload().items():'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'with open(os.path.join(custom_voice_folder, filename), ''wb'') as f:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: f.write(file_data)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '%ls -l "$PWD"/{*,.*}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`"config.ini"` using the `configparser` library. It defines various settings,
    such as the Hugging Face API key, Whisper model, device, diarization and segmentation
    models, validation ratio, and segmentation parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`ozen.py` script with the sample audio file as an argument (or the file you
    uploaded):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 9.3 – The pyannote/segmentation gated model on Hugging Face](img/Figure_9.3_B21020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – The pyannote/segmentation gated model on Hugging Face
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the cell after ensuring you have access to the pyannote/segmentation model.
    The `ozen.py` script processes the audio file, extracts speech, transcribes it
    using Whisper, and saves the output in the LJSpeech format. The script saves the
    DJ format files in a folder called `ozen-toolkit/output/<audio file name + timestamp>/`.
    Here is an example of the expected file structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Mounting Google Drive**: The following lines mount your Google Drive to the
    Colab environment, allowing access to the drive for saving checkpoints and loading
    datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`ozen-toolkit/output` directory to your Google Drive. After running the cell,
    go to your Google Drive using a web browser, as shown in *Figure 9**.4*; you will
    see a directory called `output` with the DJ format dataset files in it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Identifying the location of the DJ format files from the ozen.py
    script](img/Figure_9.4_B21020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Identifying the location of the DJ format files from the ozen.py
    script
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the cell, go to your Google Drive using a web browser; you will
    see a directory called `output` with the DJ format dataset files in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Example of the DJ format output folder in Google Colab](img/Figure_9.5_B21020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Example of the DJ format output folder in Google Colab
  prefs: []
  type: TYPE_NORMAL
- en: The Python code in the `LOAIW_ch09_2_Processing_audio_to_LJ_format_with_Whisper_OZEN.ipynb`
    notebook demonstrates setting up the environment, installing dependencies, configuring
    the OZEN Toolkit, processing audio files using Whisper, and saving the output
    in the LJSpeech format. It provides a streamlined workflow for preparing audio
    data for further analysis or use in downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our audio data now converted into the LJSpeech format, we are well-prepared
    to embark on the following critical stage of the voice synthesis journey: fine-tuning
    a PVS model using the powerful DLAS toolkit. The `LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb`
    notebook will cover this process in detail in the next section. By leveraging
    the DLAS toolkit’s comprehensive features and the structured LJSpeech dataset,
    we can create a personalized voice model that captures the unique characteristics
    of our target voice with remarkable accuracy and naturalness.'
  prefs: []
  type: TYPE_NORMAL
- en: PVS step 2 – Fine-tuning a PVS model with the DLAS toolkit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning a PVS model is a critical step in creating personalized voices that
    capture the unique characteristics of a voice. To achieve high-quality results,
    utilizing a robust framework that leverages state-of-the-art techniques and provides
    flexibility in customizing the training process is essential. The DLAS toolkit
    emerges as a comprehensive solution for fine-tuning PVS models, offering a range
    of features and capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting the fine-tuning process, ensuring that the necessary components
    and resources are in place is crucial. This includes setting up a suitable training
    environment, such as Google Colab, which provides access to powerful GPUs and
    sufficient RAM to handle the computational demands of PVS models. Checking the
    availability and compatibility of NVIDIA GPUs is vital to ensuring optimal performance
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset preparation phase is another essential aspect of fine-tuning a PVS
    model. The DLAS toolkit requires a specific repository structure and dependencies,
    which must be cloned and installed before proceeding. Additionally, pre-trained
    model checkpoints, such as the **discrete variational autoencoder** (**dVAE**),
    play a crucial role in learning a discrete latent representation of the speech
    data. Verifying the integrity of these checkpoints is necessary to accelerate
    the fine-tuning process and achieve better results.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting appropriate hyperparameters based on the dataset’s size is critical
    in fine-tuning a PVS model. The DLAS toolkit offers intelligent suggestions for
    hyperparameters, such as batch sizes, learning rate decay steps, and validation
    frequencies, all of which consider the specific characteristics of the dataset.
    Understanding how these hyperparameters are calculated and their impact on the
    training process is essential for achieving optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: Customization is another critical aspect of fine-tuning a PVS model using the
    DLAS toolkit. Researchers and developers often have specific requirements and
    preferences for training settings, such as experiment names, dataset names, and
    turning certain features on or off. The DLAS toolkit provides flexibility in modifying
    these settings, allowing for tailored fine-tuning processes that align with specific
    needs and goals.
  prefs: []
  type: TYPE_NORMAL
- en: The DLAS toolkit utilizes a YAML configuration file to ensure the fine-tuning
    process is configured according to the desired specifications. This file serves
    as a blueprint for the training process, specifying various parameters and settings.
    The toolkit applies the customized training settings to the YAML file using sophisticated
    `sed` commands, ensuring that the fine-tuning process is tailored to the specific
    requirements and enables the reproducibility of the experiments (`sed` stands
    for Stream Editor, a powerful command-line utility that’s used for parsing and
    transforming text using a simple, compact programming language).
  prefs: []
  type: TYPE_NORMAL
- en: With the configuration file ready, the training process can be initiated by
    running the `train.py` script provided by the DLAS toolkit. This script leverages
    the power of GPUs to efficiently fine-tune the PVS model, utilizing optimization
    algorithms and loss functions to guide the learning process. Monitoring the training
    progress and evaluating the model’s performance using appropriate metrics is crucial
    for ensuring the quality of the fine-tuned PVS model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, saving and exporting the fine-tuned PVS model is essential for future
    use and deployment. The DLAS toolkit provides convenient methods to store the
    trained model checkpoints and experiment files, ensuring data persistence and
    facilitating research collaboration. Proper management and organization of the
    fine-tuned models are critical for seamless integration into various applications,
    such as virtual assistants, audiobook narration, and personalized voice interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers and developers can create personalized voices that capture the nuances
    and characteristics by understanding the components, processes, and considerations
    involved in fine-tuning a PVS model using the DLAS toolkit. The ability to customize
    the training process, select appropriate hyperparameters, and leverage pre-trained
    checkpoints empowers users to achieve high-quality results and explore exciting
    possibilities in voice synthesis.
  prefs: []
  type: TYPE_NORMAL
- en: Please find and open the Colab notebook called `LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb`
    ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb)).
  prefs: []
  type: TYPE_NORMAL
- en: This notebook fine-tunes a PVS model using the DLAS toolkit. It is based on
    the *TorToiSe fine-tuning with DLAS* project by James Betker ([https://github.com/152334H/DL-Art-School](https://github.com/152334H/DL-Art-School)).
    I cloned and modified the code to run on Google Colab and leveraged an NVIDIA
    GPU for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s walk through the steps in the `LOAIW_ch09_3_Fine-tuning_PVS_models_with_DLAS.ipynb`
    notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nvidia-smi` command. It prints out the GPU’s information if it’s connected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`psutil` library. It prints a message if it’s using a high-RAM runtime:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Mounting Google Drive**: The following code mounts your Google Drive to save
    trained checkpoints and load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Installing the requirements**: It clones the DLAS repository, downloads pre-trained
    model checkpoints, and installs the required dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 9.6 – Example of searching for the DJ format dataset in the output
    directory created in the previous notebook](img/Figure_9.6_B21020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Example of searching for the DJ format dataset in the output directory
    created in the previous notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the entire script that performs the hyperparameter calculation.
    An explanation of how it works is provided after the code listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break down the purpose and steps in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code imports the necessary libraries: `Path` from `pathlib` to work with
    files and directories, and `ceil`, a function in the built-in `math` module for
    rounding numbers up to the nearest integer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It defines default training and validation batch size values: `DEFAULT_TRAIN_BS
    = 64 and DEFAULT_VAL_BS =` `32`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You are prompted to provide the paths for the training and validation datasets:
    `Dataset_Training_Path` and `ValidationDataset_Training_Path`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code checks whether the training and validation dataset paths are identical.
    If they are, it prints a warning message indicating that validation metrics will
    be useless and the training rate will be substantially slowed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code defines a helper function, `txt_file_lines`, that takes a file path
    as input and returns the number of lines in the file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It calculates the training and validation samples by calling `txt_file_lines`
    with the respective dataset paths.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code prints warning messages if the dataset sizes are small: less than
    128 for training and less than 20 for validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It defines a helper function called `div_spillover` that takes the number of
    samples (`n`) and batch size (`bs`) as input and returns an adjusted batch size
    to minimize the number of leftover samples in each epoch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code calculates the training batch size (`train_bs`) based on the number
    of training samples. If the number of training samples is smaller than `DEFAULT_TRAIN_BS`,
    it sets `train_bs` to the number of training samples and prints a warning message.
    Otherwise, it calls `div_spillover` with the number of training samples and `DEFAULT_TRAIN_BS`
    to calculate an adjusted batch size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, it calculates the validation batch size (`val_bs`) based on the number
    of validation samples. If the number of validation samples is smaller than `DEFAULT_VAL_BS`,
    it sets `val_bs` to the number of validation samples. Otherwise, it calls `div_spillover`
    with the number of validation samples and `DEFAULT_VAL_BS` to calculate an adjusted
    batch size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code calculates the number of steps per epoch (`steps_per_epoch`) by dividing
    the number of training samples by the training batch size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It defines the epochs at which learning rate decay should occur via `lr_decay_epochs
    = [20, 40,` `56, 72]`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code calculates the corresponding steps for learning rate decay (`lr_decay_steps`)
    by multiplying `steps_per_epoch` with each value in `lr_decay_epochs`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It calculates the frequency for printing training progress (`print_freq`) based
    on the number of steps per epoch, with a minimum of 20 and a maximum of 100.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code sets the frequency for validation and saving checkpoints (`val_freq`
    and `save_checkpoint_freq`) to three times the `print_freq` value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, it prints the calculated settings: `train_bs`, `val_bs`, `val_freq`,
    `lr_decay_steps`, `print_freq`, and `save_checkpoint_freq`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the creator of the DLAS trainer, the values of `print_freq`, `val_freq`,
    and `save_checkpoint_freq` should all be adjusted to the dataset’s size. The Python
    code states the recommended values: `val_freq == save_checkpoint_freq == print_freq*3`;
    `print_freq == min(epoch_steps,100)`. Again, these are recommendations; I encourage
    you to experiment with different ones and compare results for optimal hyperparameter
    settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By calculating these hyperparameters, the code aims to provide reasonable default
    values that can be used to train the PVS model. However, we can override these
    calculated values in the subsequent sections if needed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training settings**: This section allows us to customize the training settings
    according to their requirements and available resources. It provides flexibility
    in naming the experiment, specifying dataset names, turning certain features on
    or off, and overriding calculated settings. The code also includes notes and warnings
    to guide you in making appropriate choices based on the system’s storage and computational
    capabilities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s break down the purpose and steps in this section:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can specify the following training settings:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Experiment_Name`: A string to name the experiment.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dataset_Training_Name`: A string to name the training dataset.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ValidationDataset_Name`: A string to name the validation dataset.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SaveTrainingStates`: A Boolean to indicate whether to save training states.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Keep_Last_N_Checkpoints`: An integer slider to specify the number of checkpoints
    to keep. Setting it to 0 means keeping all saved models.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code provides notes and warnings:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It mentions that keeping all saved models (setting `Keep_Last_N_Checkpoints`
    to 0) could potentially cause out-of-storage issues.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Without training states, `Fp16`: A Boolean to turn 16-bit floating-point precision
    on or off.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Use8bit`: A Boolean to turn 8-bit precision on or off.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TrainingRate`: A string to specify the learning rate.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TortoiseCompat`: A Boolean to turn compatibility with the TorToiSe model on
    or off. Enabling it to introduce breaking changes to the training process and
    then disabling it is recommended to reproduce older models.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calculated settings override:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can manually override the calculated settings from the previous cell by
    specifying values for `TrainBS`, `ValBS`, `ValFreq`, `LRDecaySteps`, `PrintFreq`,
    and `SaveCheckpointFreq`
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If left blank, the calculated defaults from the previous cell will be used
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The code defines a `take` function to override the calculated settings. If the
    override is an empty string, it returns the original value; otherwise, it returns
    the overridden value.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code assigns the overridden or default values to the corresponding variables:
    `train_bs`, `val_bs`, `val_freq`, `lr_decay_steps`, `print_freq`, and `save_checkpoint_freq`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the code prompts you to run the cell after editing the settings.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sed` commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s break down the purpose and steps in this section:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The code changes the current directory to `/content/DL-Art-School` using the
    `%cd` magic command
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It downloads a fresh YAML configuration file named `EXAMPLE_gpt.yml` from the
    GitHub repository, `152334H/DL-Art-School`, using the `wget` command and saves
    it in the `experiments` directory
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code then uses a series of `sed` commands to modify the values in the `EXAMPLE_gpt.yml`
    file based on the user-defined settings:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It replaces the `batch_size` values for training and validation with the values
    stored in the `$train_bs` and `$val_bs` variables, respectively
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It updates the `val_freq` value with the value stored in `$val_freq`
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It replaces the learning rate decay steps with the values stored in `$gen_lr_steps`
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It updates the `print_freq` and `save_checkpoint_freq` values with the corresponding
    values stored in the `$print_freq` and `$``save_checkpoint_freq` variables
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The code replaces placeholders in the YAML file with the user-defined values
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, if `TrainingRate` is not equal to the default value of `1e-5`, the
    code uses `sed` to replace the placeholder of `CHANGEME:` with the user-defined
    `TrainingRate` value in the YAML file
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The training process can be customized according to your requirements by modifying
    the YAML file with the user-specified values. This ensures the training process
    is configured based on your preferences and dataset specifications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`train.py` script with the configured YAML file. Press the stop button for
    this cell when you are satisfied with the results and have seen the following
    output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Figure 9**.7* shows an example of the output and the `60_gtp.pht` checkpoint
    as it looks in Google Colab’s **Files** interface:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Example of the checkpoint file at the 60-epoch mark](img/Figure_9.7_B21020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Example of the checkpoint file at the 60-epoch mark
  prefs: []
  type: TYPE_NORMAL
- en: 'If your training run saves many models, you might exceed the storage limits
    on the Google Colab runtime. To prevent this, try to delete old checkpoints in
    `/content/DL-Art-School/experiments/$Experiment_Name/(models|training_state)/`
    via the file explorer panel as the training runs. Resuming training after a crash
    requires config editing, so try not to let that happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`experiments` folder to Google Drive for persistence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 9.8 – Example of the folder in Google Drive containing DLAS checkpoint
    files](img/Figure_9.8_B21020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Example of the folder in Google Drive containing DLAS checkpoint
    files
  prefs: []
  type: TYPE_NORMAL
- en: 'You will find the model checkpoints in the `<Experiment_Name>/models` folder
    – that is, the files with the `.``pth` extension:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Example of the DLAS checkpoint file. In the next step, you will
    need a checkpoint file to synthesize a fine-tuned PVS model](img/Figure_9.9_B21020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Example of the DLAS checkpoint file. In the next step, you will
    need a checkpoint file to synthesize a fine-tuned PVS model
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our overview of the PVS fine-tuning process using the DLAS toolkit.
    That `.pth` file is the fine-tuned PVS model we just created with DLAS. In the
    next step, we will use that file to synthesize the voice using TorToiSe-TTS-Fast.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning PVS models – Hyperparameters versus dataset size
  prefs: []
  type: TYPE_NORMAL
- en: When fine-tuning PVS models, it’s essential to consider the relationship between
    hyperparameters and dataset size. The Google Colab training notebook automatically
    suggests appropriate hyperparameters based on the provided dataset size to ensure
    optimal performance and results.
  prefs: []
  type: TYPE_NORMAL
- en: One key aspect to remember is that the number of steps per epoch (`epoch_steps`)
    is calculated as `dataset_size // batch_size`. The trainer discards partial batches,
    so selecting a batch size that evenly divides the dataset is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your dataset is relatively small (50-500 samples), consider making the following
    adjustments to the hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**- Reduce the batch size**: To minimize discarded samples, choose a batch
    size that is a clean divisor of your dataset size.'
  prefs: []
  type: TYPE_NORMAL
- en: '`gen_lr_steps``gen_lr_steps` should be less than `epoch_steps * 10`. Experiments
    show that the loss may increase if there is no decay by epoch 20.'
  prefs: []
  type: TYPE_NORMAL
- en: '`print_freq`, `val_freq`, and `save_checkpoint_freq` based on the dataset’s
    size. A recommended setting is `val_freq == save_checkpoint_freq == print_freq*3`;
    `print_freq ==` `min(epoch_steps,100)`.'
  prefs: []
  type: TYPE_NORMAL
- en: By carefully tuning these hyperparameters according to your dataset’s size,
    you can optimize the fine-tuning process and achieve better results with your
    PVS models.
  prefs: []
  type: TYPE_NORMAL
- en: Having successfully fine-tuned our PVS model using the DLAS toolkit, we are
    now ready to test our personalized voice by synthesizing speech that captures
    the essence of our target voice. In the next section, we will explore generating
    realistic and expressive speech using our fine-tuned model, bringing the synthesized
    voice to life and unlocking the potential for a wide range of exciting applications.
  prefs: []
  type: TYPE_NORMAL
- en: PVS step 3 – Synthesizing speech using a fine-tuned PVS model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Synthesizing speech using a fine-tuned PVS model is the culmination of the voice
    synthesizing process, where the personalized voice is brought to life. It is the
    stage where the fine-tuned model is tested, generating realistic and natural-sounding
    speech. The ability to synthesize speech using a fine-tuned PVS model opens up
    various applications, from creating virtual assistants and audiobook narration
    to personalized voice interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Several key components and considerations come into play when embarking on the
    journey of speech synthesis. Firstly, it is essential to have a suitable computing
    environment that can handle the computational demands of speech synthesis. This
    often involves leveraging the power of GPUs, particularly NVIDIA GPUs, which can
    significantly accelerate the synthesis process. Checking the availability and
    compatibility of the GPU is crucial to ensure smooth and efficient speech generation.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the hardware requirements, the speech synthesis process relies
    on a robust software stack. The TorToiSe-TTS-Fast project, a high-performance
    TTS system, emerges as a powerful tool. To utilize TorToiSe-TTS-Fast, it is necessary
    to clone the project repository and install the required dependencies, ensuring
    that all the necessary libraries and packages are available.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the fine-tuned PVS model is a critical step in speech synthesis. During
    the fine-tuning phase, we stored the model in **Google Drive** as a checkpoint
    file or a serialized model object. The location and format of the fine-tuned model
    may vary depending on the specific locations you used during the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: With the fine-tuned PVS model loaded, the next step is to prepare the text input
    that will be synthesized into speech. Depending on the desired output, this text
    input can be a single sentence, a paragraph, or a script. It is essential to ensure
    that the text input is formatted correctly and free of any errors or inconsistencies
    that could impact the quality of the synthesized speech.
  prefs: []
  type: TYPE_NORMAL
- en: The speech synthesis process involves feeding the text input into the fine-tuned
    PVS model and generating the corresponding audio output. This is where the magic
    happens as the model applies its learned knowledge to convert the text into speech
    that mimics the voice we used as the foundation. The synthesis process may involve
    various techniques, such as neural vocoding, to generate high-quality audio waveforms.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the specific requirements and preferences, various parameters and
    settings can be adjusted during speech synthesis. These can include factors such
    as the speaking rate, pitch, volume, and emotional tone of the generated speech.
    Fine-tuning these parameters allows greater control over the final output and
    can help achieve the desired expressiveness and naturalness in the synthesized
    speech.
  prefs: []
  type: TYPE_NORMAL
- en: Once the speech synthesis process is complete, the generated audio can be saved
    to a file format suitable for playback, such as WAV or MP3\. This allows us to
    integrate synthesized speech into various applications and platforms easily. Consider
    the desired audio quality and compatibility when choosing the output file format
    – that is, `"ultra_fast"`, `"fast" (default`), `"standard"`, or `"high_quality"`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, evaluating the quality and naturalness of the synthesized speech is
    a crucial step in the process. This can involve subjective assessments, such as
    listening tests conducted by human evaluators, and objective metrics that measure
    various aspects of the generated speech, such as intelligibility, naturalness,
    and similarity to the target voice. Iterative refinement and fine-tuning based
    on the evaluation results can help improve the overall quality of the synthesized
    speech.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers and developers can unlock the potential of personalized speech generation
    by understanding the components, considerations, and steps involved in synthesizing
    speech using a fine-tuned PVS model. The ability to create realistic and expressive
    speech that captures the essence of a voice opens exciting possibilities in various
    domains, from entertainment and education to accessibility and beyond. With the
    right tools, techniques, and attention to detail, the speech synthesis process
    using fine-tuned PVS models can be a powerful and transformative technology in
    speech and audio processing.
  prefs: []
  type: TYPE_NORMAL
- en: Please find and open the Colab notebook called `LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb`
    ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter09/LOAIW_ch09_4_Synthesizing_speech_using_fine-tuned_PVS_models.ipynb)).
  prefs: []
  type: TYPE_NORMAL
- en: 'This notebook demonstrates how to check the GPU and RAM, install the necessary
    libraries, load a fine-tuned PVS model, synthesize speech using the model, and
    play the generated audio. The code relies on the tortoise-TTS-Fast project to
    achieve high-performance voice synthesis. Let’s walk through the code in the `LOAIW_ch09_4_Synthetizing_speech_using_fine-tuned_PVS_models.ipynb`
    file, with explanations and code samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nvidia-smi` command. It prints the GPU information if connected. Otherwise,
    it indicates that no GPU is connected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`psutil` library. It prints the amount of available RAM in GB and indicates
    if a high-RAM runtime is being used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`tortoise-tts-fast` repository from GitHub and installs the required dependencies
    using `pip3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '!pip3 install transformers==4.29.2'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '!pip3 uninstall voicefixer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '!pip3 install voicefixer==0.1.2'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '!pip3 install git+https://github.com/152334H/BigVGAN.git'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Mounting Google Drive**: We must mount Google Drive to load the fine-tuned
    PVS model we created earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`gpt_path`) and the text to be synthesized (`text`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 9.10 – Example of a checkpoint file created by DLAS in Google Colab](img/Figure_9.10_B21020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Example of a checkpoint file created by DLAS in Google Colab
  prefs: []
  type: TYPE_NORMAL
- en: '`tortoise_tts.py` script with the specified arguments, including the `--preset`
    option for inference speed, the `--ar_checkpoint` option for the fine-tuned model
    path, the `-o` option for an output filename, and the text to be synthesized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import IPython
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IPython.display.Audio('/content/tortoise-tts-fast/scripts/results/random_00_00.wav')
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 9.11 – Example of audio files created with TorToiSe-TTS-Fast using
    a fine-tuned PVS model’s checkpoint .pth file](img/Figure_9.11_B21020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Example of audio files created with TorToiSe-TTS-Fast using a
    fine-tuned PVS model’s checkpoint .pth file
  prefs: []
  type: TYPE_NORMAL
- en: I encourage you to examine and run all the notebooks in this chapter. They provide
    an end-to-end practical understanding of leveraging fine-tuned PVS models for
    high-quality, efficient TTS synthesis. This knowledge will enable you to create
    PVS applications and explore the potential of personalized speech generation using
    the TorToiSe-TTS-Fast project in conjunction with the techniques you’ve learned
    throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored PVS using OpenAI’s Whisper. We discovered how to
    harness its power to create customized voice models that capture the unique characteristics
    of a voice or entirely new voices, opening a wide range of exciting applications.
  prefs: []
  type: TYPE_NORMAL
- en: We began by exploring the fundamentals of TTS in voice synthesis, gaining insights
    into the role of neural networks, audio processing, and voice synthesis. We learned
    how to convert audio files into the LJSpeech format, a standardized dataset structure
    commonly used in TTS tasks, using the OZEN Toolkit and Whisper. This hands-on
    experience provided a solid foundation for the subsequent steps in the voice synthesizing
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we delved into the DLAS toolkit, a robust framework for fine-tuning PVS
    models. We learned how to set up the training environment, prepare the dataset,
    and configure the model architecture. By leveraging Whisper’s accurate transcriptions,
    we aligned audio segments with their corresponding text, creating a dataset suitable
    for training personalized PVS models.
  prefs: []
  type: TYPE_NORMAL
- en: Through practical examples and code snippets, we gained hands-on experience
    fine-tuning a pre-trained PVS model using our LJSpeech dataset. We discovered
    how to customize the training process, select appropriate hyperparameters, and
    evaluate the model’s performance. This experience gave us the knowledge and skills
    to create high-quality personalized PVS models.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we tested our fine-tuned PVS model by synthesizing realistic and expressive
    speech. We learned how to generate natural-sounding speech by providing text input
    to the model, bringing our synthesized voice to life. The ability to create personalized
    speech opened a wide range of applications, from virtual assistants and audiobook
    narration to personalized voice interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: As we conclude this chapter, we look ahead to [*Chapter 10*](B21020_10.xhtml#_idTextAnchor221),
    *Shaping the Future with Whisper*. In this final chapter, we will explore the
    evolving landscape of ASR and Whisper’s role in shaping its future. We will delve
    into upcoming trends, anticipated features, ethical considerations, and the general
    direction of voice technologies, including advanced voice TTS fine-tuning techniques.
    This forward-looking perspective will provide us with the knowledge and foresight
    to prepare for and adapt to the future of ASR and voice technology.
  prefs: []
  type: TYPE_NORMAL
