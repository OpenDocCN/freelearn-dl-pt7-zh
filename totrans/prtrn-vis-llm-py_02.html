<html><head></head><body>
		<div id="_idContainer016">
			<h1 id="_idParaDest-29" class="chapter-number"><a id="_idTextAnchor034"/>2</h1>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor035"/>Dataset Preparation: Part One</h1>
			<p>In this chapter, we will begin to discuss what you’ll need in your dataset to start a meaningful pretraining project. This is the first of two parts on dataset preparation. It opens with some business guidance on finding a good use case for foundation modeling, where the data becomes instrumental. Then, focusing on the content of your dataset, we use qualitative and quantitative measures to compare it with datasets used to pretrain other top models. You’ll learn how to determine whether your datasets are “large enough” and “good enough” to boost accuracy while pretraining. We discuss bias identification and mitigation, along with multilingual and <span class="No-Break">multimodal solutions.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>A business-level discussion on finding datasets and use cases for <span class="No-Break">foundation modeling</span></li>
				<li>Evaluating your dataset by comparing it to ones available in the open source <span class="No-Break">research community</span></li>
				<li>Using scaling laws to size your <span class="No-Break">dataset appropriately</span></li>
				<li>Bias detection <span class="No-Break">and mitigation</span></li>
				<li>Dataset enhancements – multilingual <span class="No-Break">and augmentation</span></li>
			</ul>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor036"/>Finding a dataset and use case for foundation modeling</h1>
			<p>Datasets – we<a id="_idIndexMarker074"/> love them, we struggle with them, we rely on them, and we ignore them, oftentimes all at once. Every transaction, every digital moment, every archive, and every snapshot is a candidate for inclusion in a dataset. If your organization has already gone through a digital transformation, or if you’re digitally native, odds are you are already heavily invested in some data storage solution. Whether it’s on-premises or in the cloud, every organization needs a secure, reliable, operational, and robust solution to store countless types of data. The major question for you right now is, how can I monetize that? How can I lean into what is most unique about my organization’s history and strengths, and capitalize on it to develop net-new capabilities that further my own <span class="No-Break">competitive advantage?</span></p>
			<p>For companies that already have machine learning models deployed into production applications, an easy way to find a candidate dataset for a foundation modeling project is to ask yourself, what is the single common denominator in all of my models? What domains, what modalities, and what signals do those models rely on, and what resources can I draw on to increase the overall intelligence of <span class="No-Break">these models?</span></p>
			<p>One common mental exercise is to consider the interactions that are core to your business. From search to customer support, supply chain, product development and maintenance, marketing, and so on, each of your lines of business involve decision making on a regular basis. Now, ask yourself <span class="No-Break">the following:</span></p>
			<ul>
				<li>What if I could improve the accuracy of this decision making <span class="No-Break">by 1%?</span></li>
				<li>What if I could increase my marketing lead conversion <span class="No-Break">by 1%?</span></li>
				<li>What if I could recommend better content to customers <span class="No-Break">by 1%?</span></li>
				<li>What if I could increase my operational efficiency <span class="No-Break">by 1%?</span></li>
				<li>What if I could answer questions more accurately <span class="No-Break">by 1%?</span></li>
				<li>What if I could deliver my products faster <span class="No-Break">by 1%?</span></li>
			</ul>
			<p>Once you’ve<a id="_idIndexMarker075"/> found a certain aspect of your business that’s most<a id="_idIndexMarker076"/> interesting to you or where you think the impact of your investment could be the highest, try to quantify this number. Will an increase in accuracy by 1% give you $50,000? What about $500,000? Maybe even $1,000,000? Many multiples of that? I’m stating the obvious here, but all things being equal, higher is clearly better. You want to pick an area you think will have the absolute maximum return on <span class="No-Break">your investment.</span></p>
			<p>Now, once you have that area of your organization identified, take 10% of the total estimated earnings, or some other low percentage you feel more comfortable with. That is your maximum compute budget. Now, don’t worry – we’re not going to blow through that all at once. You may not even need to spend it all. As we step through this book, I’ll help you figure out how to get early signals that your project is going to be successful, such as training on 1% of your data to ensure the performance is better than open source models. You want to hit key milestones throughout your pretraining project, and as you hit those milestones, you will get closer to achieving your end goal. That overall goal is also the same number you’ll use to figure out how much time it’s worth for you to spend on this project, how many people you’ll want to pull in, how many sprints to use, and <span class="No-Break">so on.</span></p>
			<p>Once you have this target application in mind, along with both the estimated return and costs, you are ready to start bringing it to life! Start listing any datasets your organization already stores that are related to the application you want to build. Do you have relational databases with transactions relevant to this? Customer history? Click-stream data? Search results? What about images? Do you have any videos? Any audio files? Any experimental results? Push yourself to be creative in listing as many candidate datasets as you have. Consider taking a solid hour just to look around and see what your organization already has stored around this candidate application area. If it’s already mission-critical, then most likely you have quite a bit <span class="No-Break">stored already.</span></p>
			<p>If you don’t <a id="_idIndexMarker077"/>already have at least a few GB of data or, even better, a few 10s of GB, then you might want to consider gathering a new dataset from open source solutions. These might include any combination of over 6,000 datasets available through the <em class="italic">Papers With Code</em> site <em class="italic">(1)</em>. You can also look at the 8,000 datasets available from the <em class="italic">Hugging Face Hub</em> <em class="italic">(2)</em>. Remember, these datasets are available at no cost! Open source datasets are an excellent way to start proving the concept of your idea. Common datasets for language pretraining are <em class="italic">The Pile</em>,<em class="italic"> Common Crawl</em>,<em class="italic"> Wikipedia</em>,<em class="italic"> CodeParrot</em>, and so on <em class="italic">(3)</em>. You can also look at the OSCAR corpus for multimodal<a id="_idIndexMarker078"/> pretraining. The <strong class="bold">Vision Transformer</strong> (<strong class="bold">ViT</strong>) <em class="italic">(4)</em> model was trained from scratch on ImageNet. You have plenty of options! You<a id="_idIndexMarker079"/> can also use all of these open source datasets to enhance your original dataset, using the best of both open source and <span class="No-Break">proprietary options.</span></p>
			<p>Something else to remember is that <em class="italic">pretraining explicitly benefits from unlabeled data</em>. The best pretraining projects happen when they leverage large volumes of unlabeled data and smaller volumes of labeled data. This is largely why pretraining foundation models is popular – most data in the world isn’t labeled. However, when we use a pretraining objective, as we learned about in the first chapter, we can easily train models to learn about this. Then, we fine-tune them using supervised data, which is usually smaller <span class="No-Break">in quantity.</span></p>
			<p>So, if you find yourself in <a id="_idIndexMarker080"/>a scenario with multiple hundreds of GBs of<a id="_idIndexMarker081"/> data, such as images, files, records, transactions, metadata, and time-series data, you may want to consider that as a top candidate for <span class="No-Break">custom pretraining.</span></p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor037"/>Top pretraining use cases by industry</h2>
			<p>Now, let’s highlight a few top use cases to <a id="_idIndexMarker082"/>pretrain custom foundational models by industry. These are areas in our world where pretraining and finetuning are already having an <span class="No-Break">impact today:</span></p>
			<ul>
				<li><strong class="bold">Software </strong><span class="No-Break"><strong class="bold">and internet</strong></span><span class="No-Break">:</span><ul><li>Search <span class="No-Break">and discovery</span></li><li>Generating documentation <span class="No-Break">and code</span></li><li><span class="No-Break">Questioning/answering</span></li></ul></li>
				<li><strong class="bold">Hospitality </strong><span class="No-Break"><strong class="bold">and travel</strong></span><span class="No-Break">:</span><ul><li>Customer support <span class="No-Break">and service</span></li><li><span class="No-Break">Booking recommendations</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Consumer electronics</strong></span><span class="No-Break">:</span><ul><li><span class="No-Break">Design automation</span></li><li>Fashion <span class="No-Break">design automation</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Financial services</strong></span><span class="No-Break">:</span><ul><li>Document summarization <span class="No-Break">and generation</span></li><li><span class="No-Break">Multimodal forecasts</span></li></ul></li>
				<li><strong class="bold">Media </strong><span class="No-Break"><strong class="bold">and entertainment</strong></span><span class="No-Break">:</span><ul><li><span class="No-Break">Creativity enhancement</span></li><li>Speeding up the generation of creatives (new images, new movies, better artifacts, and <span class="No-Break">so on)</span></li><li>Identifying the best creative to move to finals (best shot, best sequence, best melody, and <span class="No-Break">so on)</span></li></ul></li>
				<li><strong class="bold">Health care and </strong><span class="No-Break"><strong class="bold">life sciences</strong></span><span class="No-Break">:</span><ul><li>Protein modeling, drug discovery, and <span class="No-Break">experimental prioritization</span></li><li>Notes synthesis and <span class="No-Break">diagnosis confirmation</span></li><li>Visual results confirmation and experiment <span class="No-Break">result prioritization</span></li><li>Scientific literature synthesis and experimental <span class="No-Break">design suggestion</span></li></ul></li>
				<li><strong class="bold">Manufacturing </strong><span class="No-Break"><strong class="bold">and agriculture</strong></span><span class="No-Break">:</span><ul><li>Part defects and building <span class="No-Break">error detection</span></li><li>Overall design automation of parts <span class="No-Break">and products</span></li><li>Autonomous <span class="No-Break">product design</span></li></ul></li>
				<li><strong class="bold">Public </strong><span class="No-Break"><strong class="bold">sector governance</strong></span><span class="No-Break">:</span><ul><li>Policy impact <span class="No-Break">analysis automation</span></li><li>Policy <span class="No-Break">suggestion automation</span></li><li>Political and philosophical <span class="No-Break">difference reconciliation</span></li><li>Budget <a id="_idIndexMarker083"/>impact assessment and <span class="No-Break">analysis automation</span></li></ul></li>
			</ul>
			<p>Let’s now move on to see how different your <span class="No-Break">dataset is.</span></p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor038"/>Delta – how different is your dataset?</h1>
			<p>Now that you have some idea of what use case you are most interested in, and what datasets will give your organization the most value, it’s time to understand how unique your dataset<a id="_idIndexMarker084"/> is. This analysis matters because it will answer <span class="No-Break">two questions:</span></p>
			<ol>
				<li>First, which<a id="_idIndexMarker085"/> models are already on the table for you to use, due to having been trained on <span class="No-Break">similar data?</span></li>
				<li>Second, how well have those <span class="No-Break">models performed?</span></li>
			</ol>
			<p>This insight will start to give you a clue toward what performance you can hope to achieve on your datasets as a best-case scenario. Then, we’ll plug that expected performance number back into our total project value and make sure we’re still on track. The next chapter is completely dedicated to answering those questions. Here, we’ll learn how to pick apart your dataset. This is a good section for those who are new to <span class="No-Break">data analysis.</span></p>
			<p>First, it’s always a good idea to spend time really analyzing any dataset you’re touching. Whether you are starting from something custom in your own database, or working with an open source option, anticipate spending at least a few hours getting to know it in little detail. Probably the best phrase I’ve heard to inspire this process is, <em class="italic">a good data science team asks more questions than they answer</em>. This is because <em class="italic">the act of analyzing a dataset is a living process, not a finite state</em>. Getting to know a dataset is a little bit like getting to know a person; it’s just that the way you ask questions and make observations is <span class="No-Break">totally different.</span></p>
			<p>Start by verbally describing the contours of your dataset. How many rows does it have? How many columns? How large are the images? How many tokens does it have? What features does it have? Are they numeric or categorical? Is it based on time? What metadata does it have? Make sure you have a good picture in your mind of what this dataset looks like. Talk with other people on your team about it until you feel confident and can answer questions quickly about the basics of your dataset composition. Use common data analysis techniques, such as Jupyter notebooks, to produce summary statistics and charts, and perform exploratory <span class="No-Break">data analysis.</span></p>
			<p>Critically, ask yourself, what real-world process was this dataset drawn from? How was this dataset acquired? We call this a sampling mechanism. If you are new to data analysis, and especially new to data analysis in applied settings outside of theoretical research, the first thing you’ll need to understand is that “not all sampling mechanisms are perfect.” To put it another way, you should get into the practice of assuming that there may be something wrong with your dataset. You need to critically evaluate the way your dataset was developed. Was it randomly collected? Or does all of the data have some underlying similarities? Any errors? The most important part of your data analysis process is to disabuse yourself of any underlying errors, inconsistencies, oddities, and faults in the raw data itself. You need to gain certainty that the data itself is indeed valid and reliable. Why? Because this certainty serves as a fundamental guarantee for everything you produce from this dataset. If the data isn’t reliable, your work can never <span class="No-Break">be reliable.</span></p>
			<p>When you have an idea<a id="_idIndexMarker086"/> about your dataset, before that idea is proven true by the results you empirically observe, it’s called a <strong class="bold">hypothesis</strong>. A hypothesis<a id="_idIndexMarker087"/> is a concept you believe may be true about your dataset, or about any real-world process. However, because you currently lack empirical evidence validating the certainty of this hypothesis, you can’t state at the current time that it is objectively true. That’s why we call it <span class="No-Break">a hypothesis!</span></p>
			<p>A core part of the scientific process, and as a corollary, your own development in machine learning is learning how to state this hypothesis clearly. You can phrase it as a simple question, something as basic as “which model solves this problem the best?”, “what does it mean to solve this type of problem optimally?”, or even, “how can we improve upon the state of the art in <span class="No-Break">this area?”</span></p>
			<p>Once you have a hypothesis, also called<a id="_idIndexMarker088"/> a <strong class="bold">research objective</strong>, clearly stated, you then want to learn “how to design experiments that answer this question.” Experimental design is a surprisingly challenging skill! This includes the work of evaluating current research in certain areas, considering open questions and results others have demonstrated, and attempting to build upon them empirically. At the end of your project, you want to have clear empirical results you can point to that validate your work. We’ll discuss this more in the following chapters on model evaluation, but it’s a <a id="_idIndexMarker089"/>critical topic to keep <span class="No-Break">in mind.</span></p>
			<p>Next, let’s learn about sizing <span class="No-Break">our datasets.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor039"/>Use the scaling laws to size your datasets</h2>
			<p>At <a id="_idIndexMarker090"/>this point, you <a id="_idIndexMarker091"/>should have identified your datasets, have a basic<a id="_idTextAnchor040"/> understanding of them, and be able to desc<a id="_idTextAnchor041"/>ribe how they are similar to and different from previous datasets and research work in your chosen domain. It’s helpful to have at least a handful of papers to refer to so that you can do so when <span class="No-Break">you’re stuck.</span></p>
			<p>In this section, we’ll explore how large your <a id="_idTextAnchor042"/>dataset should be in order to produce the expected results on a pretraining or fine-tuning project, which clearly validates the time and compute expenses you’ll be racking up. We’ll also discuss certain characteristics you’ll want this dataset to have, such as sufficient variety, quality, and lack of duplicates. The entirety of the next chapter is dedicated to picking the right model, including the size and scope, but for now, we’ll focus on <span class="No-Break">the dataset.</span></p>
			<p>First, it’s helpful to know that there is a very large gray area between so-called large and small models and the corresponding size in datasets that they tend to run on. Under no circumstances should you think that you only need multiple terabytes and/or petabytes to think about pretraining, or even models that don’t fit on a single GPU. You can produce meaningful results with unsupervised data simply by continuing to pretrain your model, rather than necessarily starting pretraining from scratch, and still hit your business and intellectual goals. Depending on your project, and how niche and interesting it may be, you can easily showcase some useful work on just under 1 GB of data. So, don’t hesitate just because you aren’t sitting on the Fort Knox of all web data; just start from where <span class="No-Break">you are!</span></p>
			<p>Next, you’ll<a id="_idIndexMarker092"/> need to understand something called the scaling laws <em class="italic">(6)</em>. These are a set of theories and formulas about how large models behave at different scales, notably as power laws. These formulas themselves are derived from empirical behavior at varying scales. You can use them to determine what model and dataset sizes are optimal for a given compute budget, and vice versa. To some degree, these laws, and their updated versions as presented in <em class="italic">Chinchilla</em>, are independent of the model architecture<a id="_idIndexMarker093"/> itself. This implies that the biggest way to improve model accuracy is scaling up the size, rather than alterations in the model architecture itself. Kaplan originally presented scaling laws explicitly within the context of language models leveraging the transformer model architecture. However, given the 10x increase in accuracy that this validated hypothesis gave rise to in the GPT-3 paper, I and many others <em class="italic">(7)</em> believe there is a reason to explore this basic relationship outside of <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>), including <a id="_idIndexMarker094"/>vision and <span class="No-Break">multimodal especially.</span></p>
			<p>You might be thinking, so what? Why is this such a big deal? Obviously, there’s some balance you’d want to achieve across your dataset, compute size, and model, so what gives? The reason Kaplan’s work was such a breakthrough is that <em class="italic">having a valid formula to <a id="_idTextAnchor043"/>quantify the optimal compute, data, and model values lets you estimate what range of loss your model might achieve</em>. To put it another way, now that we have scaling laws, we can figure out mathematically what loss we should expect at the end of our model training run, within a given range. And for training runs that can send compute costs into the hundreds of thousands of dollars, if not millions, this knowledge is incredibly valuable. OpenAI has validated this in its GPT-4 technical report, claiming to be able to accurately forecast its model’s loss given changes <span class="No-Break">in scale.</span></p>
			<p>This opens a new area of questions. What other aspects of machine learning have empirically observable laws? In what other ways can we be inspired by physics to discover formulaic patterns that rely on mathematical relationships, beyond the inner workings of the model itself? This matters because, today, the vast majority of machine learning is trial and error. We hope something works, we try it out, learn from our experiment, and then take another step. However, I believe scaling laws point to a future where <a id="_idIndexMarker095"/>machine learning is increasingly enhanced with simple, efficient, and fast checks, rather <a id="_idIndexMarker096"/>than long-running computational experiments. What if we’ve simply been thinking about this in the wrong way <span class="No-Break">for decades?</span></p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor044"/>Fundamentals – scaling laws of neural language models</h2>
			<p>If you<a id="_idIndexMarker097"/> take a look at the original paper: <em class="italic">Scaling Laws of Neural Language Laws</em>, you’ll find out that one core <a id="_idIndexMarker098"/>concept is central to their analysis – proportionality. Kaplan et al here argue that changes in your dataset size or model size should be accompanied by proportional changes in the companion quantity. To put it another way, if you use a bigger dataset, you should use a bigger model, and vice versa. Now, exactly how strong this relationship is, what describes it, what constants are involved, and precisely how much scaling should be undertaken up or down is entirely at the heart of <span class="No-Break">their paper.</span></p>
			<p>While it is helpful to know that a relationship is proportional, it is insufficient. Kaplan et al suggest and find empirically that the optimal scaling of neural language models follows a power law. Power laws are actually quite simple; they’re just about exponents at the end of the day. If two quantities follow a power law, you can assume that one side of the equation follows <span class="No-Break">exponential change:</span></p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B18942_02_001.jpg" alt=""/>
				</div>
			</div>
			<p>To estimate the early-stopped test loss of a transformer-based training regime, given the size of both a dataset and a model, Kaplan et al suggest <span class="No-Break">the following.</span></p>
			<p>Let’s try to unpack this in very simple terms – first, the <span class="No-Break">left-hand side:</span></p>
			<ul>
				<li><strong class="bold">L</strong> = the final loss of your model, stopped early, on your <span class="No-Break">test set</span></li>
				<li><strong class="bold">N</strong> = the number of trainable parameters in <span class="No-Break">your model</span></li>
				<li><strong class="bold">D</strong> = the size of your dataset in tokens (<span class="No-Break">for language)</span></li>
			</ul>
			<p>Now, you understand that the entire equation is about computing the potential loss of your model, which would be great to know ahead of time! The rest of the terms on the right-hand side <a id="_idIndexMarker099"/>are about how to get there. All four of <img src="image/B18942_02_002.png" alt=""/>, <img src="image/B18942_02_003.png" alt=""/>, <img src="image/B18942_02_004.png" alt=""/>, and <img src="image/B18942_02_005.png" alt=""/> describe constants that must be discovered from the dataset <a id="_idIndexMarker100"/>and training regime. Think of these as hyperparameters; we want to find some constant terms that describe our specific dataset and model. In many cases, however, we can simply use the constants as presented in <span class="No-Break">their work.</span></p>
			<p>Kaplan et al found constant values for each of these in their training runs by fitting the loss curves with their scaling law functions. Using mathematical extensions of their core equations, they were able to accurately fit their learning curves. Making that fit helped them discover constants that proved useful throughout the rest of <span class="No-Break">their studies.</span></p>
			<p>In the real world, once you’ve performed some preliminary data analysis and have a good idea of what characteristics you’ll need to train an adequate model, most data science teams will immediately move on to training your first model. This is because the machine learning process is generally iterative; you’ll test a variety of methods, see which ones are the most promising at a given point in time, scale and evaluate, and then try again. For the purposes of a larger book on the topic, I’ll go into more detail on two key topics that can help you<a id="_idIndexMarker101"/> improve your dataset. These are steps you<a id="_idIndexMarker102"/> probably wouldn’t implement right at the beginning of your data science journey but that you should come back to over time to increase the overall quality of your work. The first is bias detection and mitigation, and the second is <span class="No-Break">dataset enhancements.</span></p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor045"/>Bias detection and mitigation</h1>
			<p>The trajectory of the word “bias” is interesting in that, in the last 15 years, it’s come full circle. Originally, <em class="italic">bias</em> was<a id="_idIndexMarker103"/> arguably a statistical term. Formally, it implied that a sample size was improperly constructed, giving excessive weight to certain variables. Statisticians developed numerous methods to identify and reduce bias to evaluate studies properly, such as those used in randomized control trials in public health or policy evaluations in econometrics. Basic tactics include making sure that the treatment and control groups are roughly the same size and have roughly the same characteristics. Without a guarantee of that basic mathematical equivalence, or more realistically as close to it as the research team can get, it’s difficult to trust that the results of a study are truly valid. The results themselves are subject to bias, simply indicating the presence or absence of basic characteristics, rather than implying anything meaningful about the <span class="No-Break">treatment itself.</span></p>
			<p>In the last 5 years, however, numerous <a id="_idIndexMarker104"/>studies have demonstrated the inability of machine<a id="_idIndexMarker105"/> learning models to perform adequately for certain groups of people under certain scenarios. The most egregious examples include facial recognition, image detection, employment, judicial decision-making, and countless others. Large technology companies have been the first to come under fire here, with financial institutions and even public policy organizations also coming in tow. These accusations are valid. While bias in datasets has always been a big problem in machine learning, the impact on human lives across the world is now so obvious that it deserves significant dialogue, discussion, solutioning, and monitoring. If bias is present in any dataset, it is almost certain to creep into the model itself. Models certainly are not objective; they are effectively children of the datasets they were trained on. Bias has now come full circle, starting in statistics, resonating with human rights, and now driving machine <span class="No-Break">learning research.</span></p>
			<p><em class="italic">The word bias has now come full circle; starting in statistics, resonating with human rights, and now driving machine </em><span class="No-Break"><em class="italic">learning research.</em></span></p>
			<p>For the purposes of developing and attempting to deploy a machine learning model, and especially a large one with its own pretraining regime, you need to know a few things. First, the most reliable way to mitigate bias is by increasing and decreasing the different aspects of your datasets. This is especially obvious in computer vision. If you add more images of certain groups – for example, African Americans – your model will be able to recognize them. If you don’t have those images in sufficient numbers, your model won’t be able to recognize them <span class="No-Break">in applications.</span></p>
			<p>For natural language, this question ends up being even more challenging. This is because most of the data in language isn’t already tabulated into different social categories, such as gender, race, religion, and sexuality.<a id="_idTextAnchor046"/> For all of those types that we care about and know we want to protect, we need to introduce our own methods to identify, compare, and synthesize them across our datasets. Just doing this alone is tough, as you <span class="No-Break">can imagine.</span></p>
			<p>Identifying bias is the <a id="_idIndexMarker106"/>first critical<a id="_idIndexMarker107"/> step in your journey toward responsible ML. Right at the beginning, you need to be able to answer two critical questions about <span class="No-Break">your dataset:</span></p>
			<ul>
				<li>First, what types of bias are present in my <span class="No-Break">dataset currently?</span></li>
				<li>Second, how much risk does this bias expose to <span class="No-Break">my organization?</span></li>
			</ul>
			<p>Think about risk in terms of impact on your customers, particularly the predictions from a biased ML model. If your model has the potential to cause harm to your customers, such as denying a loan, downgrading an employment submission, recommending harmful content, or even denying bail or other legal sentencing, then by all means make bias detection and mitigation your <span class="No-Break">highest priority!</span></p>
			<p>While there are a variety of frameworks concerning responsible AI, I like to boil these down to four key actions to take. In terms of bias in ML models trained on biased datasets, your four key steps are <strong class="bold">expect</strong>, <strong class="bold">identify</strong>, <strong class="bold">mitigate</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">monitor</strong></span><span class="No-Break">:</span></p>
			<ol>
				<li><strong class="bold">Expect</strong>: When picking ML projects and datasets, expect that every dataset will have some type of bias at the root. Ask yourself, what problem is my ML model trying to solve, and what issues will I run into if I don’t have enough of certain types <span class="No-Break">of data?</span></li>
				<li><strong class="bold">Identify</strong>: Then, use a variety of techniques to identify the bias present in your dataset. Make sure you know right at the outset how many attributes within certain groups you do or do not have. Keep working on this until you can quantify at least a handful of different types of bias metrics. See the following note box for some suggestions on how to <span class="No-Break">identify bias.</span></li>
				<li><strong class="bold">Mitigate</strong>: Once you’ve identified the bias in your dataset numerically, take steps to mitigate the bias. Increase or decrease certain aspects of your dataset. Use augmentation, up- or down-sampling, and data transformations to drive down your bias metrics until they hit a less <span class="No-Break">dangerous threshold.</span></li>
				<li><strong class="bold">Monitor</strong>: Once you’ve<a id="_idIndexMarker108"/> deployed your ML model, the adventure continues. You can use the same bias detection methods you leveraged in <em class="italic">step 2</em> to monitor the model deployed in your application. Ensure that your application and overall system design include statistical monitoring and set thresholds for acceptable statistical levels. When the model starts to meet or exceed your thresholds, start manually reviewing the model predictions and initiate your training pipeline. Keeping <a id="_idIndexMarker109"/>humans in the loop, particularly those who are both knowledgeable and caring, is the best way to reduce the risk of <span class="No-Break">biased predictions.</span></li>
			</ol>
			<p class="callout-heading">How to do bias detection and monitoring</p>
			<p class="callout">Now that we know bias is important, how do we find it mathematically? And once we’ve done that, how do we mitigate and monitor? There are many ways of doing this, and we can categorize these in their <span class="No-Break">respective domains:</span></p>
			<p class="callout"><strong class="bold">Tabular</strong>: Detecting bias in tabular data amounts to computing some statistics. First, you’ll need to have some ground truth label in your dataset, indicating the status inside or outside of a certain group. Notably, for many teams, this alone presents a sizeable problem. However, the logical counter to this is simple. Expect your data to be biased, regardless of whether or not you have a column labeling it as members of certain groups. Introducing this label <em class="italic">is the only way to identify bias intrinsic to your dataset and, ultimately, remove it</em>. Otherwise, try to use a proxy, although these are known to <span class="No-Break">be faulty.</span></p>
			<p class="callout">Assuming you have a label, such as gender or race, then you have two types of metrics – pretraining and post-training metrics. One simple pretraining statistic is <strong class="bold">class imbalance</strong>. Class imbalance is<a id="_idIndexMarker110"/> simply the number of observations from your advantaged group, minus the number in the disadvantaged group, divided by your overall dataset size. If your class imbalance is too high, your dataset and subsequent model are certain to <span class="No-Break">be biased.</span></p>
			<p class="callout">One common post-training metric is disparate impact, which is defined simply as the number of positive predicted labels in your disadvantaged group, divided by the same in your advantaged group. Intuitively, this measures your model’s likelihood of predicting positive for different groups, which as you can imagine is critical in certain domains such as employment or law. There is some legal precedent for using 4/5, or 80%, as the lower <span class="No-Break">threshold here.</span></p>
			<p class="callout"><strong class="bold">Vision and language</strong>: Lastly, both <a id="_idIndexMarker111"/>vision and language have different approaches. In language, it’s <a id="_idIndexMarker112"/>common to evaluate a language model’s learned preference to suggest a given category under certain conditions, such as placing “he” or “her” under some <span class="No-Break">employment criteria.</span></p>
			<p>With vision, you might use a pretrained text classifier to ensure that datasets are balanced before training. Also, you can clearly indicate a model’s poor behavior in detecting certain classes – for example, certain groups in <span class="No-Break">image recognition.</span></p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor047"/>Enhancing your dataset – multilingual, multimodal, and augmentations</h1>
			<p>Finally, now that you’ve<a id="_idIndexMarker113"/> learned how to pick a dataset, compare it with research datasets, determine the right approximate size, and evaluate bias, let’s dive into enhancing the dataset. In particular, we’ll look at a few dimensions – <strong class="bold">multilingual</strong>, <strong class="bold">multimodal</strong>, and <strong class="bold">augmentations</strong>. All three of these typically come a bit later in your ML projects, especially <a id="_idIndexMarker114"/>after the first few versions of your models have been trained and you’re looking for the next idea to give you <span class="No-Break">a boost.</span></p>
			<p>Personally, I think there are few applications in the world where multilingually <em class="italic">isn’t</em> a strong added value. <em class="italic">Multilingual</em> just<a id="_idIndexMarker115"/> means multiple languages. While many of the state-of-the-art language models were originally trained on English-only text, researchers in the last few years have made strong efforts to increase the lingual diversity of these corpora. That means they’re adding support for a lot of languages. In 2022, Hugging Face led a massive worldwide effort to democratize the creation of large language models, calling their program <em class="italic">Big Science</em> <em class="italic">(8)</em>. This led to the creation of a novel model they named<a id="_idIndexMarker116"/> the <strong class="bold">BigScience Open-Science Open-Access Multilingual Language Model</strong> (<strong class="bold">BLOOM</strong>). Hugging Face hopes to improve upon the state of the art in multilingual use cases especially, such as zero-shot language translation. However, the model was shown to perform worse than GPT-3 in many cases, leading us to believe that the best models may be <span class="No-Break">single-language only.</span></p>
			<p>Frankly, being multilingual is just good business. For any product you develop, any program you run, and any service you offer, you are limited in interacting with your potential consumer through language at the end of the day. Think of a language as a market. While you’re developing your product, you want to bring it to as many markets as you can. Ultimately, that means as many languages as you can. For this reason, I’m optimistic that the industry will find a better way to incorporate multiple languages in possibly the same model without worsening results. Perhaps this is as simple as formatting a dataset appropriately, as in the case of chain-of-thought or <span class="No-Break">instruction tuning.</span></p>
			<p>Briefly, let’s explore adding<a id="_idIndexMarker117"/> additional modalities. Simply put, this means different types of datasets, such as adding vision to text, or vice versa. I introduced this concept in more detail at the close of <a href="B18942_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. Here, I’d like to simply point out that <em class="italic">if you have text with images, or images with your text, try to use it</em>. Once you’re invested in a project, with many hours spent on analyzing the data, training, evaluating models, deploying these, and so on, why<a id="_idIndexMarker118"/> would you not go the extra mile to explore adding other modalities? Particularly when it has the potential of raising accuracy, which it does. From the perspective of a model, another modality is just another type of embedding. You’ll most likely want to use some model pretrained elsewhere to convert a raw image into embeddings – that is, before adding them as another input to <span class="No-Break">your model.</span></p>
			<p>There are trade-offs <a id="_idIndexMarker119"/>here; increasing the size of your model will increase its runtime. Increasing your dataset also increases your data transformation costs. Adding another step in the data transformation makes hosting more complex, meaning you may need to revisit the system design to deploy your model. All of these trade-offs are worthy of discussion, and ultimately, you’ll need to prioritize the projects that add the most value for your teams and your customers, which could very well include <span class="No-Break">language-only models.</span></p>
			<p>The other reason I’m optimistic about multimodal projects generally, as opposed to language-only projects, is that the visual domain carries so much information to humans. Humans learn to see before they learn to speak, and so many of our experiences and knowledge are gathered visually. For this reason, I believe foundation models will continue to converge around joint vision and <span class="No-Break">language tasks.</span></p>
			<p>Finally, data<a id="_idIndexMarker120"/> augmentation is a simple and easy step to improve the accuracy of your models without adding a ton of extra work to get it. The core idea is that you’re adding some degree of variety in your dataset and slight changes in the provided samples, which will help your model learn the difference between signal and noise. Both text and vision have well-tested methods for augmentation. With vision, this is frequently as simple as pixel manipulations, light color manipulations, or <span class="No-Break">image rotations.</span></p>
			<p>With text, this can be substituting synonyms, sentence-level reconstruction, or lightweight punctuation modifications. The trick is that you don’t want to change the basic mechanism you are trying to learn. If you’re training an image detection model, don’t modify any of the images so that you can’t detect the images. If you’re training a text classifier, don’t alter the text so much that it moves into a <span class="No-Break">different class.</span></p>
			<p>Augmentation is usually less of an issue in large-scale pretraining, where most datasets are so large, as they already include more than enough noise and variation. It does, however, seem like a promising avenue for bias reduction especially. Another key technique for pretraining is reducing duplicate text. This is especially key in web data, where memes, comments, and threads can easily render the same text many hundreds of times across platforms <a id="_idIndexMarker121"/><span class="No-Break">and users.</span></p>
			<p>Now that you’ve learned all about the early stages of preparing your data, let’s do a quick recap of what you just learned before we move on to preparing <span class="No-Break">your model!</span></p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor048"/>Summary</h1>
			<p>In this chapter, we introduced a wide variety of use cases for foundation modeling, encompassing scenarios where you can fine-tune an existing foundation model and where pretraining itself is competitive. We provided a simple economics framework to help you make the case for your pretraining project, notably by tying it to how much you expect your business to increase based on a more accurate model. After that, we talked about evaluating your dataset, comparing it to research datasets, and learning how to think critically about its sampling mechanism. We set up some basic ideas to use this critical thinking for framing experiments, which we’ll continue in the next chapter. We learned about the scaling laws and presented an open source notebook you can use to find which dataset size will help you hit performance levels, given fixed model and compute budgets. We talked about detecting and mitigating bias in your datasets, along with enhancing these with augmentation, modalities, <span class="No-Break">and languages.</span></p>
			<p>Next up is <span class="No-Break">model preparation!</span></p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor049"/>References</h1>
			<p>Please go through the following content for more information on a few of the topics covered in <span class="No-Break">this chapter:</span></p>
			<ol>
				<li><em class="italic">Papers With </em><span class="No-Break"><em class="italic">Code</em></span><span class="No-Break">: </span><a href="https://paperswithcode.com/datasets"><span class="No-Break">https://paperswithcode.com/datasets</span></a><span class="No-Break">.</span></li>
				<li><em class="italic">Hugging Face </em><span class="No-Break"><em class="italic">Hub</em></span><span class="No-Break">: </span><a href="https://huggingface.co/datasets"><span class="No-Break">https://huggingface.co/datasets</span></a></li>
				<li><em class="italic">Hugging </em><span class="No-Break"><em class="italic">Face</em></span><span class="No-Break">:</span><span class="No-Break"> </span><a href="https://huggingface.co/datasets?task_ids=task_ids:language-modeling&amp;sort=downloads"><span class="No-Break">https://huggingface.co/datasets?task_ids=task_ids:language-modeling&amp;sort=downloads</span></a></li>
				<li><em class="italic">AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT </em><span class="No-Break"><em class="italic">SCALE</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2010.11929.pdf"><span class="No-Break">https://arxiv.org/pdf/2010.11929.pdf</span></a></li>
				<li>Scaling Laws for Neural Language Models: <a href="https://arxiv.org/pdf/2001.08361.pdf">https://arxiv.org/pdf/2001.08361.pdf</a> </li>
				<li>Training Compute-Optimal Large Language Models: <a href="https://arxiv.org/pdf/2203.15556.pdf">https://arxiv.org/pdf/2203.15556.pdf</a> </li>
				<li><em class="italic">BigScience Episode #5 – Challenges &amp; Perspectives in Creating Large Language </em><span class="No-Break"><em class="italic">Models</em></span><span class="No-Break">: </span><a href="https://bigscience.huggingface.co/acl-2022"><span class="No-Break">https://bigscience.huggingface.co/acl-2022</span></a></li>
			</ol>
		</div>
	</body></html>