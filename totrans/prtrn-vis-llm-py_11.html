<html><head></head><body>
		<div id="_idContainer096">
			<h1 id="_idParaDest-146" class="chapter-number"><a id="_idTextAnchor167"/>11</h1>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor168"/>Detecting, Mitigating, and Monitoring Bias</h1>
			<p>In this chapter, we’ll analyze leading bias identification and mitigation strategies for large vision, language, and multimodal models. You’ll learn about the concept of bias, both in a statistical sense and how it impacts human beings in critical ways. You’ll understand key ways to quantify and remedy this in vision and language models, eventually landing on monitoring strategies that enable you to reduce any and all forms of harm when applying your <strong class="bold">machine learning</strong> (<span class="No-Break"><strong class="bold">ML</strong></span><span class="No-Break">) models.</span></p>
			<p>We will cover the following topics in <span class="No-Break">the chapter:</span></p>
			<ul>
				<li>Detecting bias in <span class="No-Break">ML models</span></li>
				<li>Mitigating bias in vision and <span class="No-Break">language models</span></li>
				<li>Monitoring bias in <span class="No-Break">ML models</span></li>
				<li>Detecting, mitigating, and monitoring bias with <span class="No-Break">SageMaker Clarify</span></li>
			</ul>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor169"/>Detecting bias in ML models</h1>
			<p>At this point in the book, we’ve covered many of the useful, interesting, and impressive aspects of large <a id="_idIndexMarker545"/>vision and language models. Hopefully, some <a id="_idIndexMarker546"/>of my passion for this space has started rubbing off on you, and you’re beginning to realize why this is as much of an art as it is a science. Creating cutting-edge ML models takes courage. Risk is inherently part of the process; you hope a given avenue will pay off, but until you’ve followed the track all the way to the end, you can’t be positive. Study helps, as does discussion with experts to try to validate your designs ahead of time, but personal experience ends up being the most successful tool in <span class="No-Break">your toolbelt.</span></p>
			<p>This entire <a id="_idIndexMarker547"/>chapter is dedicated to possibly the most significant Achilles heel <a id="_idIndexMarker548"/>in ML and <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>): <strong class="bold">bias</strong>. Notably, here we are most interested in bias toward and against specific groups of human beings. You’ve probably already heard about <strong class="bold">statistical bias</strong>, an undesirable scenario where a given <a id="_idIndexMarker549"/>model has a statistical preference for part of the dataset, and thereby naturally against another part. This is an inevitable stage of every data science project: you need to carefully consider which datasets you are using and grapple with how that represents the world to your model. If you are over- or under-representing any facets of your datasets, this will invariably impact your model’s behavior. In the previous chapter, we looked at an example of credit card fraud and began to understand how the simple act of extracting and constructing your dataset can lead you in completely the wrong direction. Now, we’ll follow a similar exercise but focus <span class="No-Break">on people.</span></p>
			<p>When ML and data science started becoming popular in business leadership circles, as with any new phenomena, there were naturally a few misunderstandings. A primary one of these in terms of ML was the mistaken belief that computers would naturally have fewer biased <a id="_idIndexMarker550"/>preferences than people. More <a id="_idIndexMarker551"/>than a few projects were inspired by this falsity. From hiring to performance evaluations, credit applications to background checks, and even for sentencing in the criminal justice system, countless data science projects were started with an intention of reducing biased outcomes. What these projects failed to realize is that <em class="italic">every dataset is limited by historical records</em>. When we train ML models on these records naively, we necessarily introduce those same limitations on the output space of <span class="No-Break">the model.</span></p>
			<p>This means that records from criminal justice to human resources, financial services, and imaging systems when naively used to train ML models codified that bias and presented it in a digital format. When used at scale—for example, to make hundreds to thousands of digital decisions—this actually increases the scale of biased decision-making rather than reduces it. Classic examples of this include large-scale image classification systems failing to detect African Americans <em class="italic">(1)</em> or resume screening systems that developed a bias against anything female <em class="italic">(2)</em>. While all of these organizations immediately took action to right their wrongs, the overall problem was still shockingly public for the entire world <span class="No-Break">to watch.</span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor170"/>Detecting bias in large vision and language models</h2>
			<p>As you may <a id="_idIndexMarker552"/>already suspect, large models trained on massive datasets <a id="_idIndexMarker553"/>from internet crawls are ripe with bias. This includes everything from types of people more likely to produce or not to produce <a id="_idIndexMarker554"/>content on the internet, to languages and styles, topics, the accuracy of the content, depth of analysis, personalities, backgrounds, histories, interests, professions, educational levels, and more. It includes visual representations of other people, modes, cultural styles, places, events, perspectives, objects, sexuality, preferences, religion—the list goes on <span class="No-Break">and on.</span></p>
			<p>In most projects, to use a phrase from Amazon, I find it helpful to <em class="italic">work backward</em> from my final application. Here, that refers to some large vision-language model—for example, <strong class="bold">Stable Diffusion</strong>. Then, I’ll ask myself: <em class="italic">Who</em> is likely to use this model, and how? Try to write down a list of types of people you think might use your model; in a bias context, you need to push yourself to think outside of your comfort zone. This is another place where having diverse teams is incredibly useful; ideally, ask someone with a background different from yours about <span class="No-Break">their perspective.</span></p>
			<p>Once you have a target list of types of people who might use your model, think to yourself: Are these <a id="_idIndexMarker555"/>people represented in my dataset? How are <a id="_idIndexMarker556"/>they represented? Are they represented in a full <a id="_idIndexMarker557"/>spectrum of different emotional states and outcomes, or are they only represented in a tiny slice of humanity? If my dataset were the sole input to a computational process designed to learn patterns—that is, an ML algorithm—would many people from this group consider it a fair and accurate representation? Or, would they get angry at me and say: That’s <span class="No-Break">so biased!?</span></p>
			<p>You can get started with even one or two groups of people, and usually, you want to think about certain scenarios where you know there’s a big gap in your dataset. For me, I tend to look right at gender and employment. You can also look at religion, race, socioeconomic status, sexuality, age, and so on. Try to stretch yourself and find an intersection. An intersection would be a place where someone from this group is likely to be, or not to be, within another category. This second category can be employment, education, family life, ownership of specific objects, accomplishments, criminal history, medical status, and <span class="No-Break">so on.</span></p>
			<p>A model is biased when it exhibits a clear “preference”, or measurable habit, of placing or not placing certain types of people in certain types of groups. Bias shows up when your model empirically places or does not place people from one of your <em class="italic">A</em> categories into one of your <span class="No-Break"><em class="italic">B</em></span><span class="No-Break"> categories.</span></p>
			<p>For example, let’s say you’re using a text generation model in the GPT family. You might send your GPT-based model a prompt such as, “Akanksha works really hard as a …”. A biased model might fill in the blank as “nurse”, “secretary”, “homemaker”, “wife”, or “mother”. An unbiased model might fill in the blank as “doctor”, “lawyer”, “scientist”, “banker”, “author”, or “entrepreneur”. Imagine using that biased model for a resume-screening classifier, an employment chat helpline, or a curriculum-planning assistant. It would unwittingly, but very measurably, continue to make subtle recommendations against certain careers for women! Let’s look at a few more examples in the context <span class="No-Break">of language:</span></p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/Figure_11.1_B18942.jpg" alt="Figure 11.1 – Biased inference results from GPT-J 6B"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Biased inference results from GPT-J 6B</p>
			<p>Here, I simply used my own name as a prompt into the GPT-J 6B model, and it thought I was a makeup artist. Alternatively, if I used the name “John”, it thought I was a <span class="No-Break">software developer:</span></p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/Figure_11.2_B18942.jpg" alt="Figure 11.2 – Biased inference results from GPT-J 6B (continued)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Biased inference results from GPT-J 6B (continued)</p>
			<p>Once you try this again, however, the responses obviously change. This is because the random seed isn’t set in the Hugging Face model playground, so the output from the <strong class="bold">neural network</strong> (<strong class="bold">NN</strong>) can change. When I tried it again for John, it still gave a response of “software developer”. When I tried it again for myself, it responded with “a freelance social <span class="No-Break">media consultant”.</span></p>
			<p>Some of you might be wondering: Why is this biased? Isn’t this just holding to the statistical representation in the dataset? The answer is that the dataset itself is biased. There are more examples of male software engineers, fewer examples of female entrepreneurs, and so on. When we train AI/ML models on these datasets, we bring that bias directly into our applications. This means that if we use a biased model to screen resumes, suggest promotions, stylize text, assign credit, predict healthy indicators, determine criminal likelihoods, and more, we perpetuate that same bias systematically. And that is a big problem—one we need to actively <span class="No-Break">fight against.</span></p>
			<p>This guess-and-check process <a id="_idIndexMarker558"/>we’re doing right now with the pretrained model <a id="_idIndexMarker559"/>is called <strong class="bold">detecting bias</strong> or <strong class="bold">identifying bias</strong>. We are taking a pretrained model and providing it with specific scenarios at the intersection of our groups of interest defined previously, to empirically determine how well it performs. Once you’ve found a few empirical examples of bias, it’s helpful to also run summary statistics on this to understand how regularly this occurs in your dataset. Amazon researchers proposed a variety of metrics to do so <span class="No-Break">here </span><span class="No-Break"><em class="italic">(3)</em></span><span class="No-Break">.</span></p>
			<p>You might do a similar process with a pretrained vision model, such as <strong class="bold">Stable Diffusion</strong>. Ask your Stable Diffusion model to generate images of people, of workers, of different scenarios in life. Try <a id="_idIndexMarker560"/>phrasing your prompt to force the model to categorize a person around one of the points of intersection, and these days you are almost guaranteed to find empirical evidence <span class="No-Break">of bias.</span></p>
			<p>Fortunately, more <a id="_idIndexMarker561"/>models are using “safety filters”, which explicitly <a id="_idIndexMarker562"/>bar the model from being able to produce <a id="_idIndexMarker563"/>violent or explicit content, but as you’ve learned in this chapter, that is far from being <span class="No-Break">without bias.</span></p>
			<p>By now, you should have a good idea of what bias means in the context of your application. You should know for which groups of people you want to design, and in which categories you want to improve your model’s performance. Make sure you spend a decent amount of time empirically evaluating bias in your model, as this will help you demonstrate that the following techniques actually improve the outcomes you <span class="No-Break">care about.</span></p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor171"/>Mitigating bias in vision and language models</h1>
			<p>Now that you’ve learned about detecting bias in your vision and language models, let’s explore methods <a id="_idIndexMarker564"/>to mitigate this. Generally, this revolves around <a id="_idIndexMarker565"/>updating your dataset in various ways, whether <a id="_idIndexMarker566"/>through sampling, augmentation, or <a id="_idIndexMarker567"/>generative methods. We’ll also look at some techniques to use during the training process itself, including the concept of fair loss functions and <span class="No-Break">other techniques.</span></p>
			<p>As you are well aware by now, there <a id="_idIndexMarker568"/>are two key training phases to stay on top of. The first is the <strong class="bold">pretraining process</strong>, and the second is the <strong class="bold">fine-tuning</strong> or <strong class="bold">transfer</strong> <strong class="bold">learning</strong> (<strong class="bold">TL</strong>). In terms <a id="_idIndexMarker569"/>of bias, a critical point is how much bias transfer your models exhibit. That is to say, if your pretrained model was built on a <a id="_idIndexMarker570"/>dataset with bias, does that bias then transfer into your new model after you’ve done <span class="No-Break">some fine-tuning?</span></p>
			<p>A research team out of MIT delivered an interesting study on the effects of bias transfer in vision as recently as 2022 <em class="italic">(4)</em>, where they concluded: “<em class="italic">bias in pretrained models remained present even after fine-tuning these models on downstream target tasks. Crucially these biases can persist even when the target dataset used for fine-tuning did not contain such biases.</em>” This indicates that in vision, it is critical to ensure that your upstream pretrained dataset is bias-free. They found that the bias carried through into the <span class="No-Break">downstream task.</span></p>
			<p>A similar study for language found exactly the opposite of this! (<em class="italic">11</em>) Using a regression analysis in their work, the researchers realized that a better explanation for the presence of bias <em class="italic">was in the fine-tuned dataset</em> rather than the pretrained one. They concluded: “<em class="italic">attenuating downstream bias via upstream interventions—including embedding-space bias mitigation—is mostly futile.</em>” In language, the recommendation is to mostly mitigate bias in your downstream task, rather <span class="No-Break">than upstream.</span></p>
			<p>How interesting is this?! Similar work in two different domains reached opposite conclusions about <a id="_idIndexMarker571"/>the most effective place to focus for <a id="_idIndexMarker572"/>mitigating bias. This means if you are working on a <a id="_idIndexMarker573"/>vision scenario, you should spend your time optimizing <a id="_idIndexMarker574"/>your pretrained dataset to remove bias. Alternatively, if you’re on a language project, you should focus on reducing bias in the <span class="No-Break">fine-tuning dataset.</span></p>
			<p>Perhaps this means that vision models on average carry more context and background knowledge into their downstream performance, such as correlating objects with nearby objects and patterns as a result of the convolution, while language only applies this context learning at a much smaller <span class="No-Break">sentence-level scope.</span></p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor172"/>Bias mitigation in language – counterfactual data augmentation and fair loss functions</h2>
			<p>In language, many bias mitigation techniques center on <em class="italic">creating counterfactuals</em>. Remember—a counterfactual is a hypothetical scenario that did not happen in the real world but could <a id="_idIndexMarker575"/>have. For example, this morning you had many options for <a id="_idIndexMarker576"/>eating breakfast. You might <a id="_idIndexMarker577"/>have had coffee with a muffin. You also might have had breakfast cereal with orange juice. You might have gone out to a restaurant for breakfast with a friend, or maybe you skipped breakfast entirely. One of them really happened to you, but the other ones are completely fabricated. Possible, but fabricated. Each of these different scenarios could be considered <em class="italic">counterfactual</em>. They represent different scenarios and chains of events that did not actually happen but are reasonably likely <span class="No-Break">to occur.</span></p>
			<p>Now, consider this: what if you wanted to represent each scenario as being equally likely to occur? In the dataset of your life, you’ve established certain habits of being. If you wanted to train a model to consider all habits as equally likely, you’d need to create counterfactuals to equally represent all other possible outcomes. This type of dataset-hacking is exactly what we’re doing when we try to augment a dataset to debias it, or to mitigate the <a id="_idIndexMarker578"/>bias. First, we identify <a id="_idIndexMarker579"/>the top ways that bias creeps into our models and datasets, and then we mitigate that bias by creating more examples of what we don’t have enough examples for, <span class="No-Break">creating counterfactuals.</span></p>
			<p>One study presenting these methods is available in reference <em class="italic">(5)</em> in the <em class="italic">References</em> section—it includes researchers from Amazon, UCLA, Harvard, and more. As mentioned previously, they focused on the intersection of gender and employment. Let’s take a look at <span class="No-Break">an example:</span></p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/Figure_11.3_B18942.jpg" alt="Figure 11.3 – Comparing responses from normal and debiased models"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – Comparing responses from normal and debiased models</p>
			<p>To generate counterfactual samples for their fine-tuning dataset, the researchers used a common technique of switching pronouns. In particular, they “<em class="italic">use a curated dictionary of gender words with male &lt;-&gt; female mapping, for instance, father -&gt; mother, she-&gt; he, him-&gt; her, and so on</em>”. With this pronoun dictionary, they generated new sequences and included these in the <span class="No-Break">fine-tuning dataset.</span></p>
			<p>They also defined a fair knowledge distillation loss function. We’ll learn all about knowledge distillation in the upcoming chapter, but at a high level, what you need to know is that it’s the <a id="_idIndexMarker580"/>process of training a <a id="_idIndexMarker581"/>smaller model to mimic the performance of a larger model. Commonly this is done to shrink model sizes, giving you ideally the same performance as your large model, but on something much smaller you can use to deploy in <span class="No-Break">single-GPU environments.</span></p>
			<p>Here, the researchers developed a novel distillation strategy to <em class="italic">equalize probabilities</em>. In generic distillation, you want the student model to learn the same probabilities for a <span class="No-Break">given pattern:</span></p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/Figure_11.4_B18942.jpg" alt="Figure 11.4 – Equalizing distributions through distillation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – Equalizing distributions through distillation</p>
			<p>Here, the researchers knew this would lead to the student model learning exactly the same biased behavior they wanted to avoid. In response, they developed a novel distillation loss function to weight both the original and the counterfactual distributions as the same. This equalizing loss function helped their model learn to see both outcomes as <a id="_idIndexMarker582"/>equally likely and <a id="_idIndexMarker583"/>enabled the fair prompt responses you just saw! Remember—in order to build AI/ML applications that do not perpetuate the bias inherent in the datasets, we need to equalize how people are treated in the <span class="No-Break">model itself.</span></p>
			<p>Now that we’ve learned about a few ways to overcome bias in language, let’s do the same <span class="No-Break">for vision.</span></p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor173"/>Bias mitigation in vision – reducing correlation dependencies and solving sampling issues</h2>
			<p>In vision scenarios, you <a id="_idIndexMarker584"/>have at least two big problems to tackle, <span class="No-Break">as follows:</span></p>
			<ul>
				<li>First, not <a id="_idIndexMarker585"/>having enough pictures of under-represented groups <span class="No-Break">of people</span></li>
				<li>Second, realizing after it’s too late that your pictures are all correlated with underlying objects <span class="No-Break">or styles</span></li>
			</ul>
			<p>In the first scenario, your model is likely to not learn that class at all. In the second scenario, your model learns a correlated confounding factor. It might learn more about objects in the background, overall colors, the overall style of the image, and so much more than it does about the objects you think it’s detecting. Then, it continues to use those background objects or traces to make classification guesses, where it clearly underperforms. Let’s explore a 2021 study <em class="italic">(6)</em> from Princeton to learn more about <span class="No-Break">these topics:</span></p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/Figure_11.5_B18942.jpg" alt="Figure 11.5 – Correct and incorrect vision classifications"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Correct and incorrect vision classifications</p>
			<p>Fundamentally, what these images show is the correlation problem in computer vision. Here the model is simply trying to classify males and females in images. However, due to underlying correlations in these datasets, the model makes basic mistakes. In terms of sports uniforms, the researchers found that “<em class="italic">males tend to be represented as playing outdoor sports like baseball, while females tend to be portrayed as playing an indoor sport like basketball or in a swimsuit</em>”. This means the model thought <a id="_idIndexMarker586"/>everyone wearing a <a id="_idIndexMarker587"/>sports uniform indoors was female, and everyone wearing a sports uniform outdoors was male! Alternatively, for flowers, the researchers found a “<em class="italic">drastic difference in how males and females are portrayed, where males pictured with a flower are in formal, official settings, whereas females are in staged settings or paintings</em>." Hopefully, you can immediately see why this is a problem; even the model thinks that everyone in a formal setting is male, simply due to a lack of available <span class="No-Break">training data!</span></p>
			<p>How do we solve this problem? One angle the researchers explored was geography. They realized that—consistent with previous analyses—the images’ countries of origin were overwhelmingly the United States and European nations. This was true across the multiple datasets they analyzed that are common in vision research. In the following screenshot, you can see the model learns an association of the word “dish” with food items from Eastern Asia while failing to detect plates or satellite dishes, which were more common images from <span class="No-Break">other regions:</span></p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/Figure_11.6_B18942.jpg" alt="Figure 11.6 – Visual bias in the meaning of “dish” geographically"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Visual bias in the meaning of “dish” geographically</p>
			<p>The Princeton team developed and open sourced a tool called <em class="italic">REVISE: REvealing VIsual biaSEs</em> <em class="italic">(7)</em> that any Python developer can use to analyze their own visual datasets and identify candidate objects and problems that will cause correlation issues. This actually <a id="_idIndexMarker588"/>uses Amazon’s Rekognition service in the backend to run large-scale classification and object detection on the <a id="_idIndexMarker589"/>dataset! You can, however, modify it to use open source classifiers if you prefer. The tool automatically suggests actions to take to reduce bias, many of which revolve around searching for additional datasets to increase the learning for that specific class. The suggested actions can also include adding extra tags, reconciling duplicate annotations, <span class="No-Break">and more.</span></p>
			<p>Now that we’ve learned about multiple ways to mitigate bias in vision and language models, let’s explore ways to monitor this in <span class="No-Break">your applications.</span></p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor174"/>Monitoring bias in ML models</h1>
			<p>At this point in the book, for beginners, you are probably starting to realize that in fact, we are just at <a id="_idIndexMarker590"/>the tip of the iceberg in terms of identifying and <a id="_idIndexMarker591"/>solving bias problems. Implications for this range from everything from poor model performance to actual harm to humans, especially in domains such as hiring, criminal justice, financial services, and more. These are some of the reasons Cathy O’Neil raised these important issues in her 2016 book, <em class="italic">Weapons of Math Destruction</em> <em class="italic">(8)</em>. She argues that while ML models can be useful, they can also be quite harmful to humans when designed and <span class="No-Break">implemented carelessly.</span></p>
			<p>This raises core issues about ML-driven innovation. How good is good enough in a world full of biases? As an ML practitioner myself who is passionate about large-scale innovation, and also as a woman who is on the negative end of some biases, while certainly on the positive side of others, I grapple with these questions <span class="No-Break">a lot.</span></p>
			<p>Personally, there are some data science projects I just refuse to work on because of bias. For me, this includes at least hiring and resume screening, performance reviews, criminal justice, and some financial applications. Maybe someday we’ll have balanced data and truly unbiased models, but based on what I can see, we are far away from that. I encourage every ML practitioner to develop a similar personal ethic about projects that have the potential for a negative impact on humans. You can imagine that even something as seemingly innocuous as online advertising can lead to large-scale discrepancies among people. Ads for everything from jobs to education, networking to personal growth, products to financial tools, psychology, and business advice, can in fact perpetuate large-scale <span class="No-Break">social biases.</span></p>
			<p>At a higher level, I believe as an industry we can continue to evolve. While some professions require third-party certification, such as medical, legal, and education experts, ours still does not. Some service providers provide ML-specific certification, which is certainly a step in the right direction but doesn’t fully address the core dichotomy between <a id="_idIndexMarker592"/>pressure to deliver results from your employer <a id="_idIndexMarker593"/>with potential unknown and unidentified harm to your customers. Certainly, I’m not claiming to have any answers here; I can see the merits of both sides of this argument and can sympathize with the innovators just as well as with the end consumers. I’m just submitting that this is in fact a massive challenge for the entire industry, and I hope we can develop better mechanisms for this in <span class="No-Break">the future.</span></p>
			<p>On a more immediately actionable note, for those of you who have a project to deliver on in the foreseeable future, I would suggest the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Identify a broad picture of your customer base, ideally with the help of a <span class="No-Break">diverse team.</span></li>
				<li>Identify what outcomes your model will impose on your customer; push yourself to think beyond the immediate impact on your business and your team. To use a phrase from Amazon, <span class="No-Break">think big!</span></li>
				<li>Try to find empirical examples of your best- and worst-case scenarios—best case being where your model leads to win-wins, and worst case where it leads <span class="No-Break">to lose-lose.</span></li>
				<li>Use the techniques we learned throughout this book to make the win-wins more common, and the lose-lose as infrequent as you can. Remember—this usually comes down to analyzing your data, learning about its flaws and inherent perspectives, and remedying these either through the data itself or through your model and <span class="No-Break">learning process.</span></li>
				<li>Add transparency. As O’Neil points out in her book, part of the industry-wide problem is <a id="_idIndexMarker594"/>applications that impact humans in major <a id="_idIndexMarker595"/>ways not explaining which features actually drive their final classification. To solve this, you can add simple feature importance testing through LIME <em class="italic">(9)</em>, or pixel and token mapping, as we’ll see next with <span class="No-Break">SageMaker Clarify.</span></li>
				<li>Try to develop quantitative measures for especially your worst-case scenario outcomes and monitor these in your <span class="No-Break">deployed application.</span></li>
			</ol>
			<p>As it turns out, one way to detect, mitigate, and monitor bias in your models is <span class="No-Break">SageMaker Clarify!</span></p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor175"/>Detecting, mitigating, and monitoring bias with SageMaker Clarify</h1>
			<p>SageMaker Clarify is <a id="_idIndexMarker596"/>a feature within the SageMaker service <a id="_idIndexMarker597"/>you can use for bias and explainability <a id="_idIndexMarker598"/>across your ML workflows. It <a id="_idIndexMarker599"/>has a nice integration with SageMaker’s <a id="_idIndexMarker600"/>Data Wrangler, a fully managed UI for <a id="_idIndexMarker601"/>tabular data analysis and exploration. This includes nearly 20 bias metrics, statistical terms you can study and use to get increasingly more precise about how your model interacts with humanity. I’ll spare you the mathematics here, but feel free to read more about them in my blog post on the topic here: <a href="https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72"><span class="No-Break">https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72</span></a><span class="No-Break"> </span><span class="No-Break"><em class="italic">(10)</em></span><span class="No-Break">!</span></p>
			<p>Arguably more relevant for this book are Clarify’s <em class="italic">vision and language features</em>! This includes explaining image classification and object detection, along with language classification and regression. This should help you immediately understand what is driving your discriminative models’ output, and help you take steps to rectify any <span class="No-Break">biased decisioning.</span></p>
			<p>Actually, the modularity of large pretrained models in combination with smaller outputs, such as <a id="_idIndexMarker602"/>using Hugging Face to easily add a classification output to a pretrained <strong class="bold">large language model</strong> (<strong class="bold">LLM</strong>), might be a way we could debias pretrained models using Clarify, ultimately using them for generation. A strong reason to use Clarify is that you can monitor both bias metrics and <span class="No-Break">model explainability!</span></p>
			<p>In the next <a id="_idIndexMarker603"/>section of the book, <em class="italic">Part 5</em>, we will dive into key <a id="_idIndexMarker604"/>questions around deployment. Particularly, <a id="_idIndexMarker605"/>in <a href="B18942_14.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 14</em></span></a>, we’ll dive <a id="_idIndexMarker606"/>into ongoing operations, monitoring, and <a id="_idIndexMarker607"/>maintenance of models deployed <a id="_idIndexMarker608"/>into production. We’ll cover SageMaker Clarify’s monitoring features extensively there, especially discussing how you can connect these both to audit teams and automatic <span class="No-Break">retraining workflows.</span></p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor176"/>Summary</h1>
			<p>In this chapter, we dove into the concept of bias in ML, and especially explored angles in vision and language. We opened with a general discussion of human bias and introduced a few ways these empirically manifest in technology systems, frequently without intention. We introduced the concept of “intersectional bias”, and how commonly your first job in detecting bias is listing a few common types of intersections you want to be wary of, including gender or race and employment, for example. We demonstrated how this can easily creep into large vision and language models trained on datasets crawled from the internet. We also explored methods to mitigate bias in ML models. In language, we presented counterfactual data augmentation along with fair loss functions. In vision, we learned about the problem of correlational dependencies, and how you can use open source tools to analyze your vision dataset and solve <span class="No-Break">sampling problems.</span></p>
			<p>Finally, we learned about monitoring bias in ML models, including a large discussion about both personal and professional ethics, and actional steps for your projects. We closed out with a presentation of SageMaker Clarify, which you can use to detect, mitigate, and monitor bias in your <span class="No-Break">ML models.</span></p>
			<p>Now, let’s dive into <em class="italic">Part Five: Deployment!</em>. In the next chapter, we will learn about how to deploy your model <span class="No-Break">on SageMaker.</span></p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor177"/>References</h1>
			<p>Please go through the following content for more information on a few topics covered in <span class="No-Break">the chapter:</span></p>
			<ol>
				<li><em class="italic">Google apologises for Photos app’s racist </em><span class="No-Break"><em class="italic">blunder</em></span><span class="No-Break">: </span><a href="https://www.bbc.com/news/technology-33347866"><span class="No-Break">https://www.bbc.com/news/technology-33347866</span></a></li>
				<li><em class="italic">Amazon scraps secret AI recruiting tool that showed bias against </em><span class="No-Break"><em class="italic">women</em></span><span class="No-Break">: </span><a href="https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G"><span class="No-Break">https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G</span></a></li>
				<li><em class="italic">BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation</em>: <span class="hidden"> </span><a href="https://assets.amazon.science/bd/b6/db8abad54b3d92a2e8857a9a543c/bold-dataset-and-metrics-for-measuring-biases-in-open-ended-language-generation.pdf"><span class="No-Break">https://assets.amazon.science/bd/b6/db8abad54b3d92a2e8857a9a543c/bold-dataset-and-metrics-for-measuring-biases-in-open-ended-language-generation.pdf</span></a></li>
				<li><em class="italic">When does Bias Transfer in Transfer </em><span class="No-Break"><em class="italic">Learning?</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2207.02842.pdf"><span class="No-Break">https://arxiv.org/pdf/2207.02842.pdf</span></a></li>
				<li><em class="italic">Mitigating Gender Bias in Distilled Language Models via Counterfactual Role </em><span class="No-Break"><em class="italic">Reversal</em></span><span class="No-Break">: </span><a href="https://aclanthology.org/2022.findings-acl.55.pdf"><span class="No-Break">https://aclanthology.org/2022.findings-acl.55.pdf</span></a></li>
				<li><em class="italic">REVISE: A Tool for Measuring and Mitigating Bias in Visual </em><span class="No-Break"><em class="italic">Datasets</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/2004.07999.pdf"><span class="No-Break">https://arxiv.org/pdf/2004.07999.pdf</span></a></li>
				<li><span class="No-Break"><em class="italic">princetonvisualai/revise-tool</em></span><span class="No-Break">: </span><a href="https://github.com/princetonvisualai/revise-tool"><span class="No-Break">https://github.com/princetonvisualai/revise-tool</span></a></li>
				<li><em class="italic">Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy Hardcover – September 6, </em><span class="No-Break"><em class="italic">2016</em></span><span class="No-Break">: </span><a href="https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815"><span class="No-Break">https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815</span></a></li>
				<li><em class="italic">Why Should I Trust You?” Explaining the Predictions of Any </em><span class="No-Break"><em class="italic">Classifier</em></span><span class="No-Break">: </span><a href="https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf"><span class="No-Break">https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf</span></a></li>
				<li><em class="italic">Dive into Bias Metrics and Model Explainability with Amazon SageMaker </em><span class="No-Break"><em class="italic">Clarify</em></span><span class="No-Break">: </span><a href="https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72"><span class="No-Break">https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72</span></a></li>
				<li><em class="italic">Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language </em><span class="No-Break"><em class="italic">Models: </em></span><a href="https://aclanthology.org/2022.acl-long.247.pdf"><span class="No-Break">https://aclanthology.org/2022.acl-long.247.pdf</span></a></li>
			</ol>
		</div>
	</body></html>