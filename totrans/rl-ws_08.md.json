["```py\nfrom utils import Bandit\n```", "```py\nmy_bandit = Bandit()\n```", "```py\nreward = my_bandit.pull(0)\nreward\n```", "```py\nreward = my_bandit.pull(1)\nreward\n```", "```py\nrunning_rewards = [[], []]\nfor _ in range(10):\n    running_rewards[0].append(my_bandit.pull(0))\n    running_rewards[1].append(my_bandit.pull(1))\n\nrunning_rewards\n```", "```py\n[[1, 1, 1, 0, 0, 1, 1, 1, 0, 0], [0, 0, 1, 0, 0, 1, 0, 1, 1, 1]]\n```", "```py\nrounds = [i for i in range(1, 11)]\nplt.plot(rounds, np.cumsum(running_rewards[0]),\\\n         label='Cumulative reward from arm 0')\nplt.plot(rounds, np.cumsum(running_rewards[1]), \\\n         label='Cumulative reward from arm 1')\nplt.legend()\nplt.show()\n```", "```py\nclass Greedy:\n    def __init__(self, n_arms=2):\n        self.n_arms = n_arms\n        self.reward_history = [[] for _ in range(n_arms)]\n```", "```py\nclass Greedy:\n    ...\n    def update(self, arm_id, reward):\n        self.reward_history[arm_id].append(reward)\n```", "```py\ndef decide(self):\n        for arm_id in range(self.n_arms):\n            if len(self.reward_history[arm_id]) == 0:\n                return arm_id\n        mean_rewards = [np.mean(history) for history in self.reward_history]\n        return int(np.random.choice\\\n                  (np.argwhere(mean_rewards == np.max(mean_rewards))\\\n                  .flatten()))\n```", "```py\nN_ARMS = 3\nbandit = Bandit(optimal_arm_id=0,\\\n                n_arms=3,\\\n                reward_dists=[np.random.binomial \\\n                              for _ in range(N_ARMS)],\\\n                reward_dists_params=[(1, 0.9), (1, 0.8), (1, 0.7)])\n```", "```py\ngreedy_policy = Greedy(n_arms=N_ARMS)\nhistory, rewards, optimal_rewards = bandit.automate\\\n                                    (greedy_policy, n_rounds=500,\\\n                                     visualize_regret=True)\n```", "```py\nprint(*history)\n```", "```py\n0 1 2 0 1 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n```", "```py\nregrets = bandit.repeat(Greedy, [N_ARMS], n_experiments=100, \\\n                        n_rounds=300, visualize_regret_dist=True)\n```", "```py\nnp.mean(regrets), np.max(regrets)\n```", "```py\n(8.66, 62)\n```", "```py\ndef decide(self):\n        for arm_id in range(self.n_arms):\n            if len(self.reward_history[arm_id]) < T:\n                return arm_id\n\n        mean_rewards = [np.mean(history) \\\n                        for history in self.reward_history]\n        return int(np.random.choice\\\n               (np.argwhere(mean_rewards == np.max(mean_rewards))\\\n               .flatten()))\n```", "```py\ndef decide(self):\n        ...\n        if np.random.rand() < self.e:\n            return np.random.randint(0, self.n_arms)\n        ...\n```", "```py\n    import numpy as np\n    np.random.seed(0)\n    import matplotlib.pyplot as plt\n    from utils import Bandit\n    ```", "```py\n    class eGreedy:\n        def __init__(self, n_arms=2, e=0.01):\n            self.n_arms = n_arms\n            self.e = e\n            self.reward_history = [[] for _ in range(n_arms)]\n    ```", "```py\n        def decide(self):\n            for arm_id in range(self.n_arms):\n                if len(self.reward_history[arm_id]) == 0:\n                    return arm_id\n\n            if np.random.rand() < self.e:\n                return np.random.randint(0, self.n_arms)\n\n            mean_rewards = [np.mean(history) \\\n                            for history in self.reward_history]\n\n            return int(np.random.choice(np.argwhere\\\n                      (mean_rewards == np.max(mean_rewards))\\\n                      .flatten()))\n    ```", "```py\n        def update(self, arm_id, reward):\n            self.reward_history[arm_id].append(reward)\n    ```", "```py\n    N_ARMS = 3\n    bandit = Bandit(optimal_arm_id=0, \\\n                    n_arms=3,\\\n                    reward_dists=[np.random.binomial \\\n                                  for _ in range(N_ARMS)],\\\n                                  reward_dists_params=[(1, 0.9), \\\n                                                       (1, 0.8), \\\n                                                       (1, 0.7)])\n    egreedy_policy = eGreedy(n_arms=N_ARMS)\n    history, rewards, optimal_rewards = bandit.automate\\\n                                        (egreedy_policy, \\\n                                         n_rounds=500, \\\n                                         visualize_regret=True)\n    ```", "```py\n    print(*history)\n    ```", "```py\n    0 1 2 1 2 1 0 0 1 2 1 0 0 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n    ```", "```py\n    egreedy_policy_v2 = eGreedy(n_arms=N_ARMS, e=0.1)\n    history, rewards, optimal_rewards = bandit.automate\\\n                                        (egreedy_policy_v2, \\\n                                         n_rounds=500, \\\n                                         visualize_regret=True)\n    ```", "```py\n    print(*history)\n    ```", "```py\n    0 1 2 2 0 1 0 1 2 2 0 2 2 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1 2 0 1 0 \n    0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 \n    0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 \n    0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 2 0 0 \n    0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 \n    0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n    ```", "```py\n    regrets = bandit.repeat(eGreedy, [N_ARMS, 0.03], \\\n                            n_experiments=100, n_rounds=300,\\\n                            visualize_regret_dist=True)\n    ```", "```py\n    np.mean(regrets), np.max(regrets)\n    ```", "```py\n    (9.95, 64)\n    ```", "```py\ndef decide(self):\n        for arm_id in range(self.n_arms):\n            if len(self.reward_history[arm_id]) == 0:\n                return arm_id\n        conf_bounds = [np.mean(history) \\\n                       + np.sqrt(2 * np.log(self.t) / len(history))\\\n                       for history in self.reward_history]\n        return int(np.random.choice\\\n                  (np.argwhere(conf_bounds == np.max(conf_bounds))\\\n                  .flatten()))\n```", "```py\n    import numpy as np\n    np.random.seed(0)\n    import matplotlib.pyplot as plt\n    from utils import Bandit\n    ```", "```py\n    class UCB:\n        def __init__(self, n_arms=2):\n            self.n_arms = n_arms\n            self.reward_history = [[] for _ in range(n_arms)]\n            self.t = 0\n    ```", "```py\n        def decide(self):\n            for arm_id in range(self.n_arms):\n                if len(self.reward_history[arm_id]) == 0:\n                    return arm_id\n\n            conf_bounds = [np.mean(history) \\\n                           + np.sqrt(2 * np.log(self.t) \\\n                                     / len(history))\\\n                           for history in self.reward_history]\n            return int(np.random.choice\\\n                      (np.argwhere\\\n                      (conf_bounds == np.max(conf_bounds))\\\n                      .flatten()))\n    ```", "```py\n        def update(self, arm_id, reward):\n            self.reward_history[arm_id].append(reward)\n            self.t += 1\n    ```", "```py\n    N_ARMS = 3\n    bandit = Bandit(optimal_arm_id=0,\\\n                    n_arms=3,\\\n                    reward_dists=[np.random.binomial \\\n                                  for _ in range(N_ARMS)],\\\n                    reward_dists_params=[(1, 0.9), (1, 0.8), \\\n                                         (1, 0.7)])\n    ucb_policy = UCB(n_arms=N_ARMS)\n    history, rewards, optimal_rewards = bandit.automate\\\n                                        (ucb_policy, n_rounds=500, \\\n                                         visualize_regret=True)\n    ```", "```py\n    print(*history)\n    ```", "```py\n    0 1 2 1 0 2 0 1 1 0 1 0 2 0 2 0 0 1 0 1 0 1 0 1 2 0 1 0 1 0 0 \n    1 0 1 0 1 0 0 0 2 2 1 1 0 1 0 1 0 1 0 1 1 1 2 2 2 2 0 2 0 2 0 \n    1 1 1 1 1 0 0 0 0 0 2 2 0 0 1 0 1 0 0 0 0 0 1 0 2 2 2 0 0 0 0 \n    0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 2 2 2 0 0 0 0 0 0 0 0 \n    0 1 1 0 1 0 0 0 0 0 0 2 2 2 2 2 0 1 0 1 1 0 1 0 0 0 0 0 0 2 2 \n    2 2 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 2 1 1 0 1 0 \n    1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 2 2 2 0 0 0 1 1 1 0 0 0 0 \n    0 0 2 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 \n    0 0 0 0 0 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 2 2 2 2 1 1 1 1 1 1 0 \n    0 0 0 0 0 1 1 1 1 1 1 1 1 1 2 2 2 2 0 0 0 0 0 0 1 1 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n    1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 1 \n    1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 1 \n    1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 2 2 2 2 2 0 0 1 1 0 0 0 0\n    ```", "```py\n    regrets = bandit.repeat(UCB, [N_ARMS], n_experiments=100, \\\n                            n_rounds=300, visualize_regret_dist=True)\n    ```", "```py\n    np.mean(regrets), np.max(regrets)\n    ```", "```py\n    (18.78, 29)\n    ```", "```py\ndef decide(self):\n        for arm_id in range(self.n_arms):\n            if len(self.reward_history[arm_id]) == 0:\n                return arm_id\n        draws = [np.random.beta(alpha, beta, size=1)\\\n                 for alpha, beta in self.temp_beliefs]\n        return int(np.random.choice\\\n                  (np.argwhere(draws == np.max(draws)).flatten()))\n```", "```py\n    import numpy as np\n    np.random.seed(0)\n    import matplotlib.pyplot as plt\n    from utils import Bandit\n    ```", "```py\n    class BernoulliThompsonSampling:\n        def __init__(self, n_arms=2):\n            self.n_arms = n_arms\n            self.reward_history = [[] for _ in range(n_arms)]\n            self.temp_beliefs = [(1, 1) for _ in range(n_arms)]\n    ```", "```py\n        def decide(self):\n            for arm_id in range(self.n_arms):\n                if len(self.reward_history[arm_id]) == 0:\n                    return arm_id\n            draws = [np.random.beta(alpha, beta, size=1)\\\n                     for alpha, beta in self.temp_beliefs]\n            return int(np.random.choice\\\n                      (np.argwhere(draws == np.max(draws)).flatten()))\n    ```", "```py\n        def update(self, arm_id, reward):\n            self.reward_history[arm_id].append(int(reward))\n            # Update parameters according to Bayes rule\n            alpha, beta = self.temp_beliefs[arm_id]\n            alpha += reward\n            beta += 1 - reward\n            self.temp_beliefs[arm_id] = alpha, beta\n    ```", "```py\n    N_ARMS = 3\n    bandit = Bandit(optimal_arm_id=0,\\\n                    n_arms=3,\\\n                    reward_dists=[np.random.binomial \\\n                                  for _ in range(N_ARMS)],\\\n                    reward_dists_params=[(1, 0.9), (1, 0.8), \\\n                                         (1, 0.7)])\n    ths_policy = BernoulliThompsonSampling(n_arms=N_ARMS)\n    history, rewards, optimal_rewards = bandit.automate\\\n                                        (ths_policy, n_rounds=500, \\\n                                         visualize_regret=True)\n    ```", "```py\n    print(*history)\n    ```", "```py\n    0 1 2 0 0 2 0 0 0 1 0 2 2 0 0 0 2 0 2 2 0 0 0 2 2 0 0 0 0 0 0 \n    0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 \n    0 1 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 2 1 1 0 2 \n    0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 1 1 0 0 0 1 \n    0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 2 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 0 2 \n    0 0 0 0 0 0 0 2 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 \n    0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 \n    0 0 0 0 2 0 0 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n    1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n    ```", "```py\n    regrets = bandit.repeat(BernoulliThompsonSampling, [N_ARMS], \\\n                            n_experiments=100, n_rounds=300,\\\n                            visualize_regret_dist=True)\n    ```", "```py\n    np.mean(regrets), np.max(regrets)\n    ```", "```py\n    (4.03, 10)\n    ```", "```py\nqueue_bandit = QueueBandit(filename='../data.csv')\n```", "```py\ncumulative_times = queue_bandit.repeat\\\n                   ([ALG NAME], [ANY ALG ARGUMENTS], \\\n                    visualize_cumulative_times=True)\n```", "```py\n    (1218887.7924350922, 45155.236786598274)\n    ```", "```py\n    (1238591.3208636027, 45909.77140562623)\n    ```", "```py\n    (1218887.7924350922, 45129.343871806814)\n    ```", "```py\n(1218887.7924350922, 45093.244027644556)\n```"]