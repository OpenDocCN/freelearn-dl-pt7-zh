<html><head></head><body>
		<div id="_idContainer061">
			<h1 id="_idParaDest-97" class="chapter-number"><a id="_idTextAnchor116"/>7</h1>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor117"/>Finding the Right Hyperparameters</h1>
			<p>In this chapter, you’ll dive into the key hyperparameters that govern performance for top vision and language models, such as batch size, learning rate, and more. First, we’ll start with a quick overview of hyperparameter tuning for those who are new or need a light refresh, including key examples in vision and language. Then, we’ll explore hyperparameter tuning in foundation models, both what is possible today and where trends might emerge. Finally, we’ll learn how to do this on Amazon SageMaker, taking incremental steps in a cluster size and changing each hyperparameter as we do. In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Hyperparameters – batch size, learning rate, <span class="No-Break">and more</span></li>
				<li><span class="No-Break">Tuning strategies</span></li>
				<li>Tuning for <span class="No-Break">foundation models</span></li>
				<li>Scaling up as a function of world size <span class="No-Break">with SageMaker</span></li>
			</ul>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor118"/>Hyperparameters – batch size, learning rate, and more</h1>
			<p><strong class="bold">Hyperparameters</strong> determine <a id="_idIndexMarker362"/>a huge majority of critical decision points <a id="_idIndexMarker363"/>in deep learning. They operate like an intermediary between you, your model, your dataset, and your overall compute environment. You’ll pick up terms such as batch size, learning rate, number of attention heads, and more to balance your overall solution to the problem at hand, balance costs, and ensure optimal performance of your model during both training <span class="No-Break">and inference.</span></p>
			<p><strong class="bold">Batch size</strong> tells <a id="_idIndexMarker364"/>your training algorithm literally how many objects from your dataset to pick up into memory for each training step. Basic physics tells us that if you pick up more objects than your GPU can hold in memory at a single time, you’ll hit an <strong class="source-inline">Out of Memory</strong> error. A large batch size helps you step through your training loop quickly but runs the risk of failing to capture all the variation in your dataset if you do not run the optimizer frequently enough. This core trade-off is one you want to get familiar with and learn methods to solve. Hint, the entire next section is dedicated to a method<a id="_idIndexMarker365"/> called <span class="No-Break"><strong class="bold">hyperparameter tuning</strong></span><span class="No-Break">.</span></p>
			<p><strong class="bold">Learning rate</strong> operates <a id="_idIndexMarker366"/>almost like a steering wheel for the entire learning process the gradient descent optimization. Literally, it’s an amount you use that parameterizes how much to update your trainable weights. A small learning rate means you take small steps down the gradient, ideally down your loss curve, but it can slow down your job considerably. A large learning rate means you are taking large steps down the loss curve, which can speed up your job but runs the risk of getting stuck in what’s<a id="_idIndexMarker367"/> called a <strong class="bold">local minimum</strong>, or a <em class="italic">small valley in the gradient descent function</em>. Basically, that means your model underfits; the loop will think it’s completed because the optimizer indicates that loss has plateaued, but the model is just stuck in a small loss valley. As before, this introduces another core trade-off that hyperparameter tuning is well suited to help us solve. Learning rate schedulers are one step towards solving this problem, letting<a id="_idIndexMarker368"/> you pick a large enough value at the beginning of the<a id="_idIndexMarker369"/> loop and decreasing <span class="No-Break">this throughout.</span></p>
			<p>Let’s look at a variety of important hyperparameters that determine performance across notable vision and <span class="No-Break">language models.</span></p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor119"/>Key hyperparameters in vision and language</h2>
			<p>Let’s look at a few <a id="_idIndexMarker370"/>key hyperparameters in vision <span class="No-Break">and language:</span></p>
			<ul>
				<li><strong class="bold">Batch size</strong> – The number of objects your model picks up into GPU memory during each training step. A high number speeds up the job, while a lower number can increase generalization performance. Gradient noise scales seem a promising avenue for predicting the largest possible <span class="No-Break">batch size.</span></li>
				<li><strong class="bold">Learning rate</strong> – A term used to determine how much the trainable weights should be updated during the gradient descent optimization process.<a id="_idTextAnchor120"/> A large number speeds up the job and may overfit, while a lower number can underfit and fail to adequately learn the training data. As mentioned previously, these are typically paired <span class="No-Break">with schedulers.</span></li>
				<li><strong class="bold">Number of epochs</strong> – The number of total passes to make through your entire dataset. A small number decreases the total runtime of your training job, while a large number can increase accuracy. However, setting this number too large can be wasteful and cause overfitting. In <a href="B18942_09.xhtml#_idTextAnchor138"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, we learn how to use the scaling laws to <span class="No-Break">solve this.</span></li>
				<li><strong class="bold">Number of attention heads</strong> – The total number of self-attention heads used in your model. This is a big determining factor in the overall size of your model in trainable parameters. When you see the 10x jump in size from GPT-2 to GPT-3, more often than not, it’s due to both more attention heads and <span class="No-Break">larger heads.</span></li>
				<li><strong class="bold">Sequence length</strong> – The total number of tokens used as one object during the training loop. This is especially relevant in language models, where each step in the training loop uses some part of the sentence to predict another part. Tokens map, roughly speaking, to words. This means that sequence length can almost be generally interpreted as the number of words in each prediction step. This has a direct impact on both training and inference. For inference, it means this is the maximum number of words this model can use. For training, it can directly increase or decrease your GPU <span class="No-Break">memory footprint.</span></li>
			</ul>
			<p>There are countless more hyperparameters. You’ll see hyperparameters that are specific to different SDKs, such as Hugging Face’s <strong class="source-inline">transformers</strong> or <strong class="source-inline">diffusers</strong>, which let you define aspects of your training job such as your base model name, dataset name, data type, and <a id="_idIndexMarker371"/>more. SageMaker has hyperparameters for our distributed training libraries, such as optimizer state sharding, tensor parallelism, activation checkpointing and offloading, and more. In the SageMaker Training Job API, you can also define and bring any arbitrary hyperparameter <span class="No-Break">you like.</span></p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor121"/>Tuning strategies</h1>
			<p>In some sense, hyperparameter <a id="_idIndexMarker372"/>tuning<a id="_idIndexMarker373"/> is the art and science of <em class="italic">guessing and checking at scale</em>. Using sophisticated algorithms and strategies, you can actually train <em class="italic">whole fleets of models</em> to test entire ranges of hyperparameters in a huge variety of configurations. Your <em class="italic">tuning strategy</em> will then help you find the best models in the end, eventually identifying critical hyperparameters to use at larger scales. I’ve seen hyperparameter tuning help customers get boosts in accuracy of anywhere from less than 1 all the way up to over 15 percentage points. If that’s a direct translation into business returns, you can see why it’s an <span class="No-Break">attractive proposition.</span></p>
			<p>There are many strategies and technical solutions for hyperparameter tuning. These are all similar in that you, as the end user, will need to pick hyperparameters and ranges for these that you’d like to test. Many hyperparameter tuning solutions will provide defaults for you as a starting point, in addition to relevant documentation. As you progress toward using your preferred hyperparameter tuning solution, I’d suggest you budget a healthy amount of time researching at least some of these hyperparameters in some detail. If you plan on interviewing for a data science position someday, then spend a great amount of time understanding these, and especially how they impact <span class="No-Break">model performance.</span></p>
			<p>Most hyperparameter tuning solutions will actually train anywhere from a few to a few hundred models for you, each with slightly different hyperparameter configurations. At the end of the tuning process, you should see an improvement in your preferred metric, such as a decrease in your loss or an increase in your accuracy. How do these solutions find and pick the optimal hyperparameters, you ask? Through a <span class="No-Break"><strong class="bold">tuning strategy</strong></span><span class="No-Break">.</span></p>
			<p>A tuning strategy is an optimization method that tests a variety of configurations and evaluates each<a id="_idIndexMarker374"/> based<a id="_idIndexMarker375"/> on a pre-defined performance metric. These vary along a few dimensions. Some of them are simply random guesses, others try to logically fill a space, some use basic machine learning, and some use extremely sophisticated machine learning algorithms. In addition to the search method, they will also vary in the timing of their search. Some tuning methods run all experiments at the same time, or concurrently, which is valuable because the overall job will complete more quickly. Others run sequentially, testing some configurations and running another set after these are completed. This is valuable because you may ultimately hit a higher accuracy, but it comes with the downside of a longer overall runtime. Let’s look at some common hyperparameter tuning strategies, along with <span class="No-Break">their trade-offs.</span></p>
			<p>Here are some common hyperparameter <span class="No-Break">tuning strategies:</span></p>
			<ul>
				<li><strong class="bold">Random search</strong> – Just as it<a id="_idIndexMarker376"/> sounds, random search is a tuning strategy that will simply use randomness to evaluate your search space. For example, let’s say you want to explore batch sizes from 2 to 26, and using random search, you indicate that you want a total of 4 jobs to be run. You’d probably have 4 jobs run at the same time, each with a randomly selected batch size, for example, 4, 7, 17, and 22. Your tuning <a id="_idIndexMarker377"/>solution should tell you which job gives you the best performance on your <span class="No-Break">preferred metric.</span></li>
				<li><strong class="bold">Grid search</strong> – In contrast to<a id="_idIndexMarker378"/> random search, grid search will establish an orderly set of experiments to run that balances your available search space. For example, using the same configuration as the previous on batch size, but using grid search, you might end up running 4 jobs at the same time with 8, 14, 20, and 26. Like last time, these jobs will run at the same time and give you the <span class="No-Break">best-performing model.</span></li>
				<li><strong class="bold">Bayesian search</strong> – Bayesian <a id="_idIndexMarker379"/>search flips this basic idea on its head in <span class="No-Break">two ways:</span><ul><li>First, it’s a sequential tuning strategy. This means it runs a few jobs at the same time, then evaluates the results of them, and initiates another set of experiments <span class="No-Break">to run.</span></li><li>Second, it’s <a id="_idIndexMarker380"/>actually <a id="_idIndexMarker381"/>using a machine learning algorithm to select the hyperparameters. Commonly, this is a simple logistic regression; using the metadata of the model as an input, it predicts the values for the next hyperparameters to evaluate. We’ve had this available in Amazon SageMaker since at least 2018! For the record, SageMaker also has random and <span class="No-Break">grid search.</span></li></ul></li>
				<li><strong class="bold">Hyperband</strong> – Presented by a <em class="italic">research team</em> <em class="italic">(1)</em> in 2018, the Hyperband strategy<a id="_idIndexMarker382"/> actually focuses on optimizing the random search. They developed an infinite-armed bandit to adaptively explore and select tuning configurations, using predefined preferences. This is very similar to reinforcement learning! At AWS, we’ve combined this with a massively parallel strategy known as <strong class="bold">Asynchronous Successive Halving Algorithm</strong> (<strong class="bold">ASHA</strong>) <em class="italic">(2)</em>, which exploits<a id="_idIndexMarker383"/> parallelism and aggressive early stopping. We’ve shown that these solutions together enable large-scale tuning, such <a id="_idIndexMarker384"/>as for vision and language models. Our <em class="italic">tests</em> <em class="italic">(3)</em> demonstrate a ~5x and ~4.5x speedup relative to random and Bayesian <span class="No-Break">strategies, respectively.</span></li>
			</ul>
			<p>As you may have noticed, I’ve essentially listed the search strategies from simple to complex. In your learning journey, you can consider that a good target path for yourself as well. Start with the easiest and most simple tuning strategies, then slowly build yourself up to ones that are more complex <span class="No-Break">over time.</span></p>
			<p>Another theme you may have already guessed by now is the critical need to <em class="italic">balance cost with accuracy gains</em>. Take a look at where and how your search strategy is running; are they using compute efficiently or inefficiently? Will they continue running jobs and experiments for you even after the gains have slowed down, or will they aggressively shut down your resources when the <span class="No-Break">gains stop?</span></p>
			<p>Lastly, it’s helpful to know that when you have a full application built around your model, you will most likely want to integrate a tuning strategy in your retraining pipeline. Say you have a vision or language model deployed on SageMaker; when new data arrives, you <a id="_idIndexMarker385"/>trigger a<a id="_idIndexMarker386"/> pipeline that retrains your model. A great asset in that pipeline would be tuning your model to ensure you have the highest possible accuracy. More on that in <a href="B18942_14.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 14</em></span></a><span class="No-Break">!</span></p>
			<p>For now, let’s explore unique challenges and approaches for tuning hyperparameters in <span class="No-Break">foundation models.</span></p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor122"/>Hyperparameter tuning for foundation models</h1>
			<p>Foundation models <a id="_idIndexMarker387"/>present some unique challenges for hyperparameter tuning. Let’s try to <span class="No-Break">understand them:</span></p>
			<ul>
				<li><strong class="bold">Model size</strong> – Possibly the largest obstacle to tuning foundation models is their sheer size. Many of the classic tuning strategies we looked at previously <em class="italic">rely on training the model as many times as possible</em>. When simply holding one copy of the model in memory requires tens of accelerators, the economics around this approach <span class="No-Break">fall apart.</span></li>
				<li><strong class="bold">Volume of downstream tasks</strong> – As we’ve seen throughout the book, the sheer volume of candidate downstream tasks for foundation models is enormous. This makes hyperparameter tuning much more complex because the objective metrics for each of these tasks are unique. Picking the right downstream task itself could be a kind of <span class="No-Break">tuning challenge!</span></li>
				<li><strong class="bold">Variety of hyperparameters</strong> – At these scales, the relevant hyperparameters aren’t just indicators of the training procedure. They are also about the distribution techniques, such as model and data parallel, as we learned <span class="No-Break">about previously.</span></li>
			</ul>
			<p>How should we think about overcoming these challenges? One approach is suggested here; basically, you can tune hyperparameters efficiently on a tiny sample of your dataset. This means you can do large-scale searches for hyperparameters using 1% of your data, and this helps you find the <a id="_idIndexMarker388"/>right settings at the start of <span class="No-Break">your job.</span></p>
			<p>Another approach, as we mentioned previously, is a much more efficient tuning strategy called <strong class="bold">Hyberband</strong>. Following <a id="_idIndexMarker389"/>this example post <em class="italic">(4)</em>, you can see how to integrate this with SageMaker Training, with an example <span class="No-Break">for Cifar10.</span></p>
			<p>Is that the end of the story? I don’t think so. Today, so much of the foundation model development world relies on following work that others have done, including reusing their exact same hyperparameters or running very lightweight experiments on massive datasets and accelerator scales; it seems only natural to me that this will converge with hyperparameter tuning<a id="_idIndexMarker390"/> strategies someday. Also, given some of the parameter efficient fine-tuning strategies we’ll learn about in Chapters 10 and 15, we may see hyperparameter tuning become even more relevant when adapting a model after the pretraining process. We will also look at strategies for <em class="italic">tuning inference requests</em> in <a href="B18942_13.xhtml#_idTextAnchor198"><span class="No-Break"><em class="italic">Chapter 13</em></span></a><span class="No-Break">.</span></p>
			<p>Next, let’s look at how we think about scaling up our tuning experiments to handle large models and datasets, in addition to working with hyperparameter tuning <span class="No-Break">on SageMaker.</span></p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor123"/>Scaling up as a function of world size with SageMaker</h1>
			<p>In this section, we’ll break<a id="_idIndexMarker391"/> down two critical concepts that you need to master hyperparameter tuning, especially in the context of distributed training. The first one is the concept of scaling, especially using hyperparameter tuning as a method to run smaller experiments before ultimately running your large training job. The second is using tips and tricks available on SageMaker for hyperparameter <span class="No-Break">tuning generally.</span></p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor124"/>Tuning on a sample of your data and updating based on world size</h2>
			<p>As you’ve <a id="_idIndexMarker392"/>learned in<a id="_idIndexMarker393"/> this chapter, hyperparameter tuning is a great way to eke out performance gains, but it can require intensive compute that executes a large number of experiments. You might be wondering, <em class="italic">How do I easily apply this to my use case with a dataset size of at least a few hundred GB, and maybe a few TB or more?</em> The answer is just to start with a <span class="No-Break">tiny sample!</span></p>
			<p>The goal of tuning at a tiny fraction of your dataset is <em class="italic">to see how sensitive your model is to changes in its key hyperparameters</em>. At a 1% sample, you might be interested in core algorithmic settings such as the number of attention heads, variations in the optimizer or layer operations, sequence length, and any other critical settings in your overall training loop. If you see a big boost, that’s a signal that you may want to pay more attention to this hyperparameter and either integrate tuning into your training job directly or simply add a check to determine what setting will give you the best performance at scale. You might also tune the batch size and learning rate, including its warm-up, to see which <span class="No-Break">performs best.</span></p>
			<p>As I hope you are thinking already, if you’re tuning on only 1% of your entire dataset, then you’ll only need a tiny fraction of your overall compute! That means you can and should plan on using very small instances for hyperparameter tuning, such as <strong class="source-inline">ml.g5.12xlarge</strong> or smaller. Once you’re ready to move up to more instances, however, you’ll want to update your key hyperparameters as a function of your overall <span class="No-Break">world size.</span></p>
			<p>Remember, the world size is just a phrase for counting all of the GPUs or accelerators available in your training cluster. If you have 2 instances with 8 GPUs each, that means your overall world size is 16 GPUs. Some of your hyperparameters should be updated every time you change your world size because they govern how your model interacts with the training environment. These are especially batch size and <span class="No-Break">learning rate.</span></p>
			<p>For example, in our previous 16 GPU example, let’s say you used hyperparameter tuning to find a good per-device batch size of 22 and a learning rate of 5e-5. Next, maybe you want to move up to 4 instances, each with 8 GPUs, giving you a total world size of 32 GPUs. That jump from 16 to 32 is clearly a doubling, increasing the number of accelerators by a multiple of 2. We’ll <a id="_idIndexMarker394"/>apply <a id="_idIndexMarker395"/>this same factor to the global batch size and learning rate, so they scale up to the same degree as the <span class="No-Break">world size.</span></p>
			<h3>Simple hyperparameter tuning scale-up example</h3>
			<p>Let’s go<a id="_idIndexMarker396"/> through the examples <span class="No-Break">as follows:</span></p>
			<p><span class="No-Break"><em class="italic">Original</em></span><span class="No-Break">:</span></p>
			<ul>
				<li>Two instances, each with <span class="No-Break">eight GPUS</span></li>
				<li>World size = <span class="No-Break">16</span></li>
				<li>Per device batch size = <span class="No-Break">22</span></li>
				<li>Learning rate = <span class="No-Break">5e-5</span></li>
			</ul>
			<p><em class="italic">Increased </em><span class="No-Break"><em class="italic">cluster size</em></span><span class="No-Break">:</span></p>
			<ul>
				<li>Four instances, each with <span class="No-Break">eight GPUs</span></li>
				<li>World size = <span class="No-Break">32</span></li>
				<li>Per device batch size = <span class="No-Break">22</span></li>
			</ul>
			<p>Notice that the <em class="italic">per-device</em> batch size won’t necessarily change as you increase the world size. Depending on your script, however, make sure you know whether you are supplying the per-device or the global batch size. The global batch size will, of course, need to <span class="No-Break">be changed!</span></p>
			<p><em class="italic">Learning rate = 5e-5 * 2 = </em><span class="No-Break"><em class="italic">0.0001</em></span></p>
			<p>On top of updating the batch size and learning rate as you scale up in your overall world size, make sure you are <em class="italic">also considering the size of the model itself</em>. This might include adding more parameters, more attention heads, more layers, and so on. As we’ve seen, this is a strong indicator of ultimately getting a more accurate model. For example, in our public examples <em class="italic">(4)</em> of training large-scale GPT-2 models, we provide three different<a id="_idIndexMarker397"/> configurations of the model, with hyperparameters selected for each <span class="No-Break">model size:</span></p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B18942_Figure_7.01.jpg" alt="Figure 7.1 – Model config parameters"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Model config parameters</p>
			<p>You can see that for a model size of <strong class="source-inline">30b</strong> parameters, we’d suggest setting the number of heads to <strong class="source-inline">64</strong>, layers to <strong class="source-inline">48</strong>, hidden width to <strong class="source-inline">7168</strong>, and train batch size <span class="No-Break">to </span><span class="No-Break"><em class="italic">5</em></span><span class="No-Break">.</span></p>
			<p>However, for a much smaller model size of only <strong class="source-inline">1.5b</strong> parameters, we set the number of heads to <strong class="source-inline">24</strong>, hidden width to <strong class="source-inline">1536</strong>, and a train batch size of <strong class="source-inline">2</strong>. Why would a smaller model use a smaller batch size, you ask? Isn’t it somewhat counterintuitive, since a smaller model should have a smaller GPU footprint, allowing you to increase <span class="No-Break">batch size?</span></p>
			<p>The answer to that question is yes, theoretically, a smaller model should have a <em class="italic">larger</em> batch size, but in this case, because we implement <em class="italic">significant model parallelism</em> on the large model, it actually counteracts this influence and has a net smaller GPU <span class="No-Break">memory footprint.</span></p>
			<p>The hidden width parameter, if you’re curious, is simply the size of the inner layers in your neural networks. We call them hidden because they are inside the black box; one step after the input layer and one step before the output layer. This very logically follows from the <a id="_idIndexMarker398"/>overall model size; a larger model in parameter count should absolutely have a larger <span class="No-Break">hidden width.</span></p>
			<p>Lastly, let’s take a quick look at hyperparameter tuning <span class="No-Break">on SageMaker.</span></p>
			<h3>Tuning with Amazon SageMaker</h3>
			<p>Using SageMaker<a id="_idIndexMarker399"/> for hyperparameter<a id="_idIndexMarker400"/> tuning is quite simple, and you have a few options. First, as always, you can simply add any tuning strategy directly inside your script and execute it on the training cluster directly. You might do this with grid search, simply bringing your own scripts in and <span class="No-Break">running them.</span></p>
			<p>As you scale, however, and especially as you build tuning into your retrain pipeline, you may want to eventually use our fully managed <strong class="source-inline">HyperparameterTuner</strong>. This is essentially an object that takes your training job, in the form of a prebuilt estimator, along with a few <span class="No-Break">other specifications:</span></p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B18942_Figure_7.02.jpg" alt="Figure 7.2 – Defining the tuning object"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Defining the tuning object</p>
			<p>You define your objective metric, your hyperparameters, and their ranges, along with the total number of jobs you’d like to run and the ones in parallel. These default settings will then use Bayesian optimization. In fact, in this example, they’d spin up a maximum of three instances at the same time, reusing them to run up to nine <span class="No-Break">total jobs.</span></p>
			<p>You might enhance this with early stopping, or with the <strong class="bold">Hyperband</strong> algorithm we learned about earlier. You can point to the strategy just by adding it as another argument to this <span class="No-Break">function. </span><span class="No-Break"><em class="italic">(5)</em></span></p>
			<p>In terms of the objective metric, you’ll be happy to know that you can bring whatever you want. What<a id="_idIndexMarker401"/> we<a id="_idIndexMarker402"/> do for that is walk through the CloudWatch logs for your job and look for the metric definition. Honestly, I’ve found this to be the single hardest part of the process: matching your regex string exactly to what’s coming out of your training job. In a PyTorch MNIST example, here’s what this <span class="No-Break">looks like:</span></p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B18942_Figure_7.03.jpg" alt="Figure 7.3 – Defining a tuning metric for the job config"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Defining a tuning metric for the job config</p>
			<p>You can see we’re asking you to write a regex string and supply that in this object. This then should directly match what is defined in the training <span class="No-Break">script here:</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B18942_Figure_7.04.jpg" alt="Figure 7.4 – Defining a tuning metric in your training script"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Defining a tuning metric in your training script</p>
			<p>For good measure, here’s a visual of the hyperparameter ranges so you can see how <span class="No-Break">they’re defined:</span></p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B18942_Figure_7.05.jpg" alt="Figure 7.5 – Defining hyperparameter ranges"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Defining hyperparameter ranges</p>
			<p>We have another method I think is particularly well designed. It lets you run analytics on your tuning job by porting it into a pandas DataFrame! The notebook for this is <span class="No-Break">here </span><span class="No-Break"><em class="italic">(6)</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B18942_Figure_7.06.jpg" alt="Figure 7.6 – Calling tuner.dataframe()"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Calling tuner.dataframe()</p>
			<p>And that’s <a id="_idIndexMarker403"/>a <a id="_idIndexMarker404"/>wrap! Let’s quickly take a look at everything you learned in <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor125"/>Summary</h1>
			<p>In this chapter on hyperparameter tuning, you learned about what hyperparameters are, including batch size, learning rate, number of epochs, number of attention heads, sequence length, and more. You learned how to use hyperparameter tuning to improve the performance of your model, along with top strategies for doing so. You learned how to scale up your tuning, starting at 1% of your dataset, then modifying your key hyperparameters as a function of your overall GPU world size. Finally, you learned about key features for doing all of this on <span class="No-Break">Amazon SageMaker.</span></p>
			<p>In the next chapter, we’ll learn about large-scale <span class="No-Break">distributed training!</span></p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor126"/>References</h1>
			<ol>
				<li><em class="italic">Hyperband: A Novel Bandit-Based Approach to Hyperparameter </em><span class="No-Break"><em class="italic">Optimization</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/1603.06560.pdf"><span class="No-Break">https://arxiv.org/pdf/1603.06560.pdf</span></a></li>
				<li><em class="italic">A SYSTEM FOR MASSIVELY PARALLEL HYPERPARAMETER </em><span class="No-Break"><em class="italic">TUNING</em></span><span class="No-Break">: </span><a href="https://arxiv.org/pdf/1810.05934.pdf"><span class="No-Break">https://arxiv.org/pdf/1810.05934.pdf</span></a></li>
				<li><em class="italic">Amazon SageMaker Automatic Model Tuning now provides up to three times faster hyperparameter tuning with </em><span class="No-Break"><em class="italic">Hyperband</em></span><span class="No-Break">:</span><span class="No-Break"><em class="italic"> </em></span><a href="https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-provides-up-to-three-times-faster-hyperparameter-tuning-with-hyperband/"><span class="No-Break">https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-provides-up-to-three-times-faster-hyperparameter-tuning-with-hyperband/</span></a></li>
				<li><span class="No-Break"><em class="italic">amazon-sagemaker-examples</em></span><span class="No-Break">: </span><a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt2/smp-train-gpt-simple.ipynb"><span class="No-Break">https://github.com/aws/amazon-sagemaker-examples/blob/main/training/distributed_training/pytorch/model_parallel/gpt2/smp-train-gpt-simple.ipynb</span></a></li>
				<li><span class="No-Break"><em class="italic">HyperparameterTuner</em></span><span class="No-Break">: </span><a href="https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html"><span class="No-Break">https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html</span></a></li>
				<li><span class="No-Break"><em class="italic">amazon-sagemaker-examples</em></span><span class="No-Break">: </span><a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb"><span class="No-Break">https://github.com/aws/amazon-sagemaker-examples/blob/main/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb</span></a></li>
			</ol>
		</div>
	</body></html>